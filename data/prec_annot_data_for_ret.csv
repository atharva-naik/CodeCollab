id,query,doc,score,is_rel
1568,prepping the data,"def preparedata():
    # Drop date variable
    data = data.drop(['DATE'], 1)
    n = data.shape[0]
    # Dimensions of dataset
    p = data.shape[1]
    # Make data a np.array
    data = data.values
preparedata():
print(""FINISHED PREPARING THE DATA"")",0.4391829371,1
1568,prepping the data,"def preprocess_data(data):
    # This function flattens the data along the spatial dimensions. So size of data will
    # change from (numx, numy, numz, numt) to (numx*numy*numz, numt)
    
    #Parameters:
    #   1. The data to be flattened
    
    # Returns: Flattened data
    flattened_data = np.reshape(data,(np.prod(data.shape[:-1]), data.shape[3]))
    return flattened_data

def PCA_decomp(data, 
               pca_results_path,
               indices_for_windows,
               min_variance_explained=0.8):
    # This function is called to do PCA decomposition. This checks if the PCA has already
    # been done by checking if a pickled file exists in pca_results_path. If this file
    # exists, it just loads those results. Otherwise, it performs the PCA decomposition.
    
    #Parameters:
    #   1. The data to perform PCA on. Shape: (numx, numy, numz, numt)
    #   2. pca_results_path: Path to where the PCA result would have been stored
    #                        if PCA has already been performed for this data.
    #   3. The minimum amount of variance that should be explained. The number of 
    #      PCs stored will be determined by this number.
    
    # Returns: Data in PCA space
    
    if os.path.isfile(pca_results_path): #has PCA already been done?
        return transform_from_loadedpca(data, pca_results_path)
    else:
        return perform_PCA_decomp(data, pca_results_path, indices_for_windows, min_variance_explained)

def transform_from_loadedpca(data, 
                             pca_results_path):
    # This function transforms inputted data based on stored PCA results for that data.
    
    #Parameters:
    #   1. The data on which PCA was performed.
    #   2. pca_results_path: Path to where the PCA result would have been stored
    #                        if PCA has already been performed for this data.
    
    # Returns: Data in PCA space
    
    flattened_data = preprocess_data(data)
    pca = load_calculated_pca(pca_results_path)
    compressed_data = pca.transform(flattened_data.T).T #transform back to shape n_components x n_timepoints
    return compressed_data

def perform_PCA_decomp(data, 
                       pca_results_path,
                       indices_for_windows,
                       min_variance_explained=0.8):
    # This function performs the PCA decomposition. This is called only if 
    # there aren't any results from a previous run stored in pca_results_path.
    
    #Parameters:
    #   1. The data to perform PCA on. Shape: (numx, numy, numz, numt)
    #   2. pca_results_path: Path to where the PCA result should be stored.
    #   3. The minimum amount of variance that should be explained. The number of 
    #      PCs stored will be determined by this number.
    
    # Returns: Data in PCA space
    
    flattened_data = preprocess_data(data)
    pca = PCA(n_components=min_variance_explained)
    pca.fit(flattened_data.T) 
    compressed_data = pca.transform(flattened_data.T).T #transform back to shape n_components x n_timepoints
    pca, compressed_data = standardize_pca_sign(pca, compressed_data,indices_for_windows)
    joblib.dump(pca, pca_results_path)
    return compressed_data

def load_calculated_pca(pca_results_path):
    # This function loads the calculated PCA object.
    
    #Parameters:
    #   1. pca_results_path: Path to where the PCA result would have been stored
    #                        if PCA has already been performed for this data.
    
    # Returns: sklearn PCA object
    pca = joblib.load(pca_results_path)
    return pca

def standardize_pca_sign(pca, 
                         compressed_data,
                         indices_for_windows,
                         criterion='positivestimresponse'):
    # The PCs are only defined upto a negative sign, i.e. a 180 degree rotated PC vector
    # is equivalently a PC vector. This function prevents this ambiguity by enforcing
    # the sign of each PC to be such that the derivative of the trace is positive at the 
    # onset of the first stimulation. Other forms of standardization could also be used.  
    # Obviously, this works only for this current experiment with stimulation. In case
    # there is no stimulation, you could set the criterion to ""positiveslope"" in which 
    # case the function will ensure that the PC's trace has a positive linear trend 
    # through the recording.
    
    #Parameters:
    #   1. The sklearn PCA object.
    #   2. compressed_data: Data in PCA space
    #   3. indices for windows. Explained above
    #   4. criterion for standardization. Set to positivestimresponse to ensure that
    #      the PCs have a positive stimulation response.
    
    # Returns: Sign standardized input parameters
    for pc in range(compressed_data.shape[0]):
        trace = compressed_data[pc,:].T
        if criterion=='positivestimresponse':
            if np.diff(trace)[indices_for_windows[1]]<0:
                compressed_data[pc,:] = -compressed_data[pc,:]
                pca.components_[pc,:] = -pca.components_[pc,:]  
        elif criterion=='positiveslope':
            time = np.arange(trace.shape[0]).T
            time = sm.add_constant(time)
            lm = sm.OLS(trace, time).fit()
            if lm.params[1] < 0: #if slope < 0, flip the PC vector and the trace
                compressed_data[pc,:] = -compressed_data[pc,:]
                pca.components_[pc,:] = -pca.components_[pc,:]
    return pca, compressed_data

def extract_pc_vectors(pca_results_path, 
                       (numx, numy, numz)):
    # This function is used to extract the PC vectors from the stored results 
    # in pca_results_path and reshapes them to the original voxel tiling.
    
    #Parameters:
    #   1. PCA results path
    
    # Returns: pca_vectors
    
    pca = load_calculated_pca(pca_results_path)
    pca_vectors = np.reshape(pca.components_.T, (numx,numy,numz,pca.components_.shape[0]))
    return pca_vectors

def plot_variance_explained_per_pc(pca_results_path,
                                   fig=None,
                                   ax=None,
                                   label='',
                                   numpcs=None):
    # This function plots the % of variance explained by each PC.
    
    #Parameters:
    #   1. PCA results path
    #   2. Figure handle. Optional. Useful if you want to layer plots
    #      across all conditions
    #   3. Axis handle. Optional. Same as above.
    #   4. Label for the plot. Will be set to a condition when called later.
    #   5. Number of PCs to show in the plot. All if set to None
    
    # Returns: Figure and axis handle to the plot   
    pca = load_calculated_pca(pca_results_path)
    if fig is None or ax is None:
        fig,ax = plt.subplots()
    if numpcs is None:
        temp = 100*pca.explained_variance_ratio_
    else:
        temp = 100*pca.explained_variance_ratio_[:numpcs]
    ax.plot(temp, '.-', label=label)
    ax.set_ylabel('% of variance explained')
    ax.set_xlabel('PC number')
    ax.set_ylim((0,7))
    return fig, ax

def plot_pc_vectors(pca_results_path,
                    (numx, numy, numz),
                    pc_of_interest):
    # This function plots the PC vector for pc_of_interest based on
    # the results stored in pca_results_path.
    # It returns a figure handle for this plot. One could then iterate over all PCs
    # to save the figures to your path of choice.
    
    #Parameters:
    #   1. PCA results path
    #   2. Spatial shape of the data
    #   3. pc_of_interest
    
    # Returns: Figure handle to the plot.
    
    pca_vectors = extract_pc_vectors(pca_results_path, (numx, numy, numz))
    
    fig, axs = plt.subplots(3, 4)
    vmax = np.amax(pca_vectors[:,:,:,pc_of_interest])
    vmin = np.amin(pca_vectors[:,:,:,pc_of_interest])
    vmaxsymmetric = np.maximum(np.abs(vmax),np.abs(vmin))
    vminsymmetric = -np.maximum(np.abs(vmax),np.abs(vmin))

    temp = np.swapaxes(pca_vectors,0,1) #For making the plot
    for ax, zplane in zip(axs.flat, range(0,numz)):
        ax.matshow(temp[:,:,zplane,pc_of_interest],
                   vmin=vminsymmetric,
                   vmax=vmaxsymmetric,
                   cmap=plt.get_cmap('seismic'))
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
    fig.tight_layout()
        
    return fig

        
def plot_pc_traces(data_in_pcaspace, pc_of_interest):  
    # This function plots the PC trace associated with pc_of_interest
    # given the data in pca space.
    # Calling PCA_decomp() returns the data in pca space. 
    # This function calculates a z-score of all the traces.
    # So each trace is normalized within itself.
    # Hence, note that this function wouldn't be appropriate to compare two traces
    # since their magnitudes are normalized within themselves, rather than between
    # them.
    
    #Parameters:
    #   1. Data in PCA space
    
    # Returns: figure handle to the plot.
    
    temp = (data_in_pcaspace[pc_of_interest,:])
    baseline = np.mean(temp[:indices_for_windows[1]])
    ztrace = temp
    fig, ax = plt.subplots()
    sns.tsplot(ztrace, ax=ax)
    ax.set_ylabel('PC signal (score)')
    ax.set_xlabel('Time (s)')
    fig.tight_layout()
        
    return fig",0.4215570092,1
1568,prepping the data,"def preprocess_data(data):
    # This function flattens the data along the spatial dimensions. So size of data will
    # change from (numx, numy, numz, numt) to (numx*numy*numz, numt)
    
    #Parameters:
    #   1. The data to be flattened
    
    # Returns: Flattened data
    flattened_data = np.reshape(data,(np.prod(data.shape[:-1]), data.shape[3]))
    return flattened_data

def PCA_decomp(data, 
               pca_results_path, 
               indices_for_windows,
               min_variance_explained=0.8):
    # This function is called to do PCA decomposition. This checks if the PCA has already
    # been done by checking if a pickled file exists in pca_results_path. If this file
    # exists, it just loads those results. Otherwise, it performs the PCA decomposition.
    
    #Parameters:
    #   1. The data to perform PCA on. Shape: (numx, numy, numz, numt)
    #   2. pca_results_path: Path to where the PCA result would have been stored
    #                        if PCA has already been performed for this data.
    #   3. The minimum amount of variance that should be explained. The number of 
    #      PCs stored will be determined by this number.
    
    # Returns: Data in PCA space
    
    if os.path.isfile(pca_results_path): #has PCA already been done?
        return transform_from_loadedpca(data, pca_results_path)
    else:
        return perform_PCA_decomp(data, pca_results_path, indices_for_windows, min_variance_explained)

def transform_from_loadedpca(data, 
                             pca_results_path):
    # This function transforms inputted data based on stored PCA results for that data.
    
    #Parameters:
    #   1. The data on which PCA was performed.
    #   2. pca_results_path: Path to where the PCA result would have been stored
    #                        if PCA has already been performed for this data.
    
    # Returns: Data in PCA space
    transformed_data_path = os.path.join(results_directory, 'transformed_data.h5')
    if not os.path.isfile(transformed_data_path):
        transformed_data_handle = h5py.File(transformed_data_path,'x')
    transformed_data_handle = h5py.File(transformed_data_path,'r')
    if pca_results_path in transformed_data_handle:
        return transformed_data_handle[pca_results_path]
    else:
        transformed_data_handle.close()
        transformed_data_handle = h5py.File(transformed_data_path,'a')
        return perform_transformation(data, pca_results_path, transformed_data_handle)    
    
def perform_transformation(data,
                           pca_results_path,
                           transformed_data_handle):
    flattened_data = preprocess_data(data)
    pca = load_calculated_pca(pca_results_path)
    compressed_data = pca.transform(flattened_data.T).T #transform back to shape n_components x n_timepoints
    transformed_data_handle.create_dataset(pca_results_path, data=compressed_data)
    transformed_data_handle.close()
    return compressed_data

def perform_PCA_decomp(data, 
                       pca_results_path, 
                       indices_for_windows,
                       min_variance_explained=0.8):
    # This function performs the PCA decomposition. This is called only if 
    # there aren't any results from a previous run stored in pca_results_path.
    
    #Parameters:
    #   1. The data to perform PCA on. Shape: (numx, numy, numz, numt)
    #   2. pca_results_path: Path to where the PCA result should be stored.
    #   3. The minimum amount of variance that should be explained. The number of 
    #      PCs stored will be determined by this number.
    
    # Returns: Data in PCA space
    
    flattened_data = preprocess_data(data)
    pca = PCA(n_components=min_variance_explained)
    pca.fit(flattened_data.T) 
    compressed_data = pca.transform(flattened_data.T).T #transform back to shape n_components x n_timepoints
    pca, compressed_data = standardize_pca_sign(pca, compressed_data, indices_for_windows)
    joblib.dump(pca, pca_results_path)
    return compressed_data

def load_calculated_pca(pca_results_path):
    # This function loads the calculated PCA object.
    
    #Parameters:
    #   1. pca_results_path: Path to where the PCA result would have been stored
    #                        if PCA has already been performed for this data.
    
    # Returns: sklearn PCA object
    pca = joblib.load(pca_results_path)
    return pca

def standardize_pca_sign(pca, 
                         compressed_data,
                         indices_for_windows,
                         criterion='positivestimresponse'):
    # The PCs are only defined upto a negative sign, i.e. a 180 degree rotated PC vector
    # is equivalently a PC vector. This function prevents this ambiguity by enforcing
    # the sign of each PC to be such that the derivative of the trace is positive at the 
    # onset of the first stimulation. Other forms of standardization could also be used.  
    # Obviously, this works only for this current experiment with stimulation. In case
    # there is no stimulation, you could set the criterion to ""positiveslope"" in which 
    # case the function will ensure that the PC's trace has a positive linear trend 
    # through the recording.
    
    #Parameters:
    #   1. The sklearn PCA object.
    #   2. compressed_data: Data in PCA space
    #   3. indices for windows. Explained above
    #   4. criterion for standardization. Set to positivestimresponse to ensure that
    #      the PCs have a positive stimulation response.
    
    # Returns: Sign standardized input parameters
    for pc in range(compressed_data.shape[0]):
        trace = compressed_data[pc,:].T
        if criterion=='positivestimresponse':
            if np.diff(trace)[indices_for_windows[1]]<0:
                compressed_data[pc,:] = -compressed_data[pc,:]
                pca.components_[pc,:] = -pca.components_[pc,:]  
        elif criterion=='positiveslope':
            time = np.arange(trace.shape[0]).T
            time = sm.add_constant(time)
            lm = sm.OLS(trace, time).fit()
            if lm.params[1] < 0: #if slope < 0, flip the PC vector and the trace
                compressed_data[pc,:] = -compressed_data[pc,:]
                pca.components_[pc,:] = -pca.components_[pc,:]
    return pca, compressed_data

def extract_pc_vectors(pca_results_path, 
                       (numx, numy, numz)):
    # This function is used to extract the PC vectors from the stored results 
    # in pca_results_path and reshapes them to the original voxel tiling.
    
    #Parameters:
    #   1. PCA results path
    
    # Returns: pca_vectors
    
    pca = load_calculated_pca(pca_results_path)
    pca_vectors = np.reshape(pca.components_.T, (numx,numy,numz,pca.components_.shape[0]))
    return pca_vectors

def plot_variance_explained_per_pc(pca_results_path,
                                   fig=None,
                                   ax=None,
                                   label=''):
    # This function plots the % of variance explained by each PC.
    
    #Parameters:
    #   1. PCA results path
    #   2. Figure handle. Optional. Useful if you want to layer plots
    #      across all conditions
    #   3. Axis handle. Optional. Same as above.
    #   4. Label for the plot. Will be set to a condition when called later.
    
    # Returns: Figure and axis handle to the plot   
    pca = load_calculated_pca(pca_results_path)
    if fig is None or ax is None:
        fig,ax = plt.subplots()
    ax.plot(100*pca.explained_variance_ratio_, '.-', label=label)
    ax.set_ylabel('% of variance explained')
    ax.set_xlabel('PC number')
    return fig, ax

def plot_pc_vectors(pca_results_path,
                    (numx, numy, numz),
                    pc_of_interest):
    # This function plots the PC vector for pc_of_interest based on
    # the results stored in pca_results_path.
    # It returns a figure handle for this plot. One could then iterate over all PCs
    # to save the figures to your path of choice.
    
    #Parameters:
    #   1. PCA results path
    #   2. Spatial shape of the data
    #   3. pc_of_interest
    
    # Returns: Figure handle to the plot.
    
    pca_vectors = extract_pc_vectors(pca_results_path, (numx, numy, numz))
    
    fig, axs = plt.subplots(3, 4)
    vmax = np.amax(pca_vectors[:,:,:,pc_of_interest])
    vmin = np.amin(pca_vectors[:,:,:,pc_of_interest])
    vmaxsymmetric = np.maximum(np.abs(vmax),np.abs(vmin))
    vminsymmetric = -np.maximum(np.abs(vmax),np.abs(vmin))

    temp = np.swapaxes(pca_vectors,0,1) #For making the plot
    for ax, zplane in zip(axs.flat, range(0,numz)):
        ax.matshow(temp[:,:,zplane,pc_of_interest],
                   vmin=vminsymmetric,
                   vmax=vmaxsymmetric,
                   cmap=plt.get_cmap('seismic'))
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
    fig.tight_layout()
        
    return fig

        
def plot_pc_traces(data_in_pcaspace, pc_of_interest):  
    # This function plots the PC trace associated with pc_of_interest
    # given the data in pca space.
    # Calling PCA_decomp() returns the data in pca space. 
    # This function calculates a z-score of all the traces.
    # So each trace is normalized within itself.
    # Hence, note that this function wouldn't be appropriate to compare two traces
    # since their magnitudes are normalized within themselves, rather than between
    # them.
    
    #Parameters:
    #   1. Data in PCA space
    
    # Returns: figure handle to the plot.
    
    temp = (data_in_pcaspace[pc_of_interest,:])
    baseline = np.mean(temp[:indices_for_windows[1]])
    ztrace = (temp-np.mean(temp))/np.std(temp)#-np.log(temp/baseline) #=R2* multiplied by TE
    fig, ax = plt.subplots()
    sns.tsplot(ztrace, ax=ax)
    ax.set_ylabel('z-score PC signal (score)')
    ax.set_xlabel('Time (s)')
    fig.tight_layout()
        
    return fig",0.4215570092,1
1568,prepping the data,"if not params[""load_cov""]:
  data = dp.get_data(params[""data_type""], params)                                 
  params[""input_shape""] = [                                                       
    data[""train""].num_rows*data[""train""].num_cols*data[""train""].num_channels]",0.4168394804,0
1568,prepping the data,"def prepdata(...): 
    '''Function to prepare train, test and validation data given X and Y data set given
    '''
    ...
    
...",0.4150904715,1
1568,prepping the data,"predictions, historical = support.data_prep(predictions, historical)",0.4145896435,0
1568,prepping the data,"def prep_data(data):
    randomized_data = copy.deepcopy(data);
    splitIndex = len(randomized_data)/2;
    set1 = {};
    set2 = {};
    for d in randomized_data:
        if d < splitIndex:
            set1[d] = randomized_data[d];
        else:
            set2[d] = randomized_data[d];
    return (set1, set2);

(set1, set2) = prep_data(data);
print set1.get(0);
print ""Number in data sets:"", len(set1), len(set2);",0.4108049273,1
1568,prepping the data,"pre_data = [[], [], [], []]

def process_image(image):
    global pre_data
    #### highlight lanes in selected region ####
    regular_median_color = 80    # median color for a image with excellent visibility conditions
    d_factor = 25                # factor to decide the effect of overall median color on the thresholds
    red_threshold = 220 - (regular_median_color - np.median(image[:,:, 0]))/d_factor
    green_threshold = 220 - (regular_median_color - np.median(image[:,:, 1]))/d_factor
    blue_threshold = 220 - (regular_median_color - np.median(image[:,:, 2]))/d_factor

    color_thresholds = (image[:,:, 0] > red_threshold)\
                    | (image[:,:, 1] > green_threshold)\
                    | (image[:,:, 2] > blue_threshold)
    
    #### create a mask for selected region ####
    # Pull out the x and y sizes
    ysize = image.shape[0]
    xsize = image.shape[1]

    # Define a triangle region of interest
    left_bottom = [50, ysize-20]
    right_bottom = [xsize-50, ysize-20]
    apex = [xsize//2, ysize//2]

    # Perform a linear fit (y=Ax+B) to each of the three sides of the triangle
    # np.polyfit returns the coefficients [A, B] of the fit
    fit_left = np.polyfit((left_bottom[0], apex[0]), (left_bottom[1], apex[1]), 1)
    fit_right = np.polyfit((right_bottom[0], apex[0]), (right_bottom[1], apex[1]), 1)
    fit_bottom = np.polyfit((left_bottom[0], right_bottom[0]), (left_bottom[1], right_bottom[1]), 1)

    # Find the region inside the lines
    XX, YY = np.meshgrid(np.arange(0, xsize), np.arange(0, ysize))
    region_thresholds = (YY > (XX*fit_left[0] + fit_left[1])) & \
                        (YY > (XX*fit_right[0] + fit_right[1])) & \
                        (YY < (XX*fit_bottom[0] + fit_bottom[1]))
    
    lane_select = np.copy(image)
    lane_select[~color_thresholds | ~region_thresholds] = [0, 0, 0]
    
    #### soften this image ####
    kernel_size = 11
    blur_image = gaussian_blur(lane_select, kernel_size)
    edges = auto_threshold_canny(image, blur_image)
    
    #### apply hough transform on edge image to get highlighted lane lines for selected region ####
    rho = 1  # distance resolution in pixels of the hough grid
    theta = np.pi/180  # angular resolution in hough grid
    threshold = 20     # minimum number of votes (intersections in hough grid)
    min_line_len = 10  # minimum number of pixels making up a line
    max_line_gap = 250    # maximum gap in pixels between connectable line segments
    line_image = np.copy(image)   
    line_only_image = np.copy(image)*0   # creating a blank to draw lines on
    # Output ""lines"" containing endpoints of detected line segments
    lines = cv2.HoughLinesP(edges, rho, theta, threshold, np.array([]), min_line_len, max_line_gap)

    #### mask out all the colors from the hough transformed image except lane lines ####
    # uncomment below line to see raw lines draw on orignal video
    #return draw_lines(line_image, lines)
    # find positive and negative slope lines and get average positive and negative slopes respectively
    pm = []    # list of positive slopes
    pc = []    # list of intercepts of lines with positive slopes
    pl = []    # list of lengths of lines with positive slopes
    nm = []    # list of negative slopes
    nc = []    # list of intercepts of lines with negative slopes
    nl = []    # list of lengths of lines with negative slopes

    # get line details in lists
    for line in lines:
        for x1, y1, x2, y2 in line:
            m = (y2-y1)/(x2-x1)
            c = y1 - m*x1
            l = math.sqrt(((y2-y1)**2) + ((x2-x1)**2))
            if m < 0:
                nm.append(m)
                nc.append(c)
                nl.append(l)
            elif m > 0:
                pm.append(m)
                pc.append(c)
                pl.append(l)

    # get the average of slopes and intercepts weighted by lengths
    if sum(pl) != 0:
        a_pm = np.average(pm, weights=pl)
        a_pc = np.average(pc, weights=pl)
    else:
        a_pm = np.average(pm)
        a_pc = np.average(pc)
    
    if sum(nl) != 0:
        a_nm = np.average(nm, weights=nl)
        a_nc = np.average(nc, weights=nl)
    else:
        a_nm = np.average(nm)
        a_nc = np.average(nc)
    # create two extrapolated lines
    lines = []
    #check if current data is valid
    data_valid_flag = True
    if math.isnan(a_pm) | math.isnan(a_pc) | math.isnan(a_nm) | math.isnan(a_nc):
        data_valid_flag = False
    
    if data_valid_flag:
        if len(pre_data[0]) > 5:
            pre_data[0].pop(0)
            pre_data[1].pop(0)
            pre_data[2].pop(0)
            pre_data[3].pop(0)

        pre_data[0].append(a_pm)
        pre_data[1].append(a_pc)
        pre_data[2].append(a_nm)
        pre_data[3].append(a_nc)
    #calculate weights for previous frames
    wt = []
    for v in range(1, len(pre_data[0])+1):
        wt.append(v)
    #wt[-1] = wt[-1]*2
    # prepare data
    line_data = [[np.average(pre_data[0], weights=wt), np.average(pre_data[1], weights=wt)],\
                 [np.average(pre_data[2], weights=wt), np.average(pre_data[3], weights=wt)]]
     
    for i in range(2):
        m = line_data[i][0]
        c = line_data[i][1]
        y1 = ysize
        x1 = int((y1 - c)/m)
        y2 = y1-(ysize//3)-30
        x2 = int((y2 - c)/m)
        line = [[x1, y1, x2, y2]]
        lines.append(line)
    
    # draw lines on original image
    draw_lines(line_only_image, lines, thickness=15)
    result = weighted_img(image, line_only_image)
    return result",0.406324625,1
1568,prepping the data,"def preprocess_data(data, labels):
    """"""
    Flattens the each image in the data to be a one-dimensional feature vector and encodes the labels in one-hot encoding.

    Parameters
    ----------
    data : np.array[samples, width, height]
        the image dataset
    labels : np.array[samples]
        the corresponding labels

    Returns
    -------
    data_and_labels : tuple(np.array[samples, width * height], np.array[samples, classes])
        a tuple with two numpy array containing the flattened data and one-hot encoded labels
    """"""
    ##############
    return the flattened images and labels
##############

preprocessed_data, preprocessed_labels = preprocess_data(data, labels)",0.4061832428,1
1568,prepping the data,"%%time
if cfg.redo_preprocessing:
    # featurized data gets written here
    feature_data_split = os.path.join(cfg.data_dir, ""feature_data_split"")

    threads = []
    for x in [""training"", ""validation"", ""test""]:
        # create_local_dir(feature_data_split, x)
        for word in cfg.words_core + cfg.words_negative:
            # folder = create_local_dir(feature_data_split, x, word)
            print (""Feature engineering for set <%s> word <%s>"" % (x, word))
            wav_dir = ""/"".join([raw_data_split,     x, word])
            # full path gets created automatically inside extract_features function
            out_dir = ""/"".join([feature_data_split, x, word])
            thread = threading.Thread(name = x + ' ' + word, args = (wav_dir, out_dir, ), target = extract_features)
            threads += [thread]
            thread.start()

    # block the calling thread (this notebook cell) until all threads in the list complete
    for thread in threads:
        thread.join()",0.4025538862,1
1829,"rounding, overflow, linear algebra","sum(4*l**2 - 6*(l-1) for l in xrange(3, 1002, 2)) + 1",0.5149160028,1
1829,"rounding, overflow, linear algebra","solve([3*x-7*y-3,x+3*y+4*z-1])",0.5121361017,1
1829,"rounding, overflow, linear algebra","list(filter(lambda y: y < 25, map(lambda x: x**2, range(10))))",0.5097714663,1
1829,"rounding, overflow, linear algebra","solve([3*x-7*y-3,2*x+2*y+5*z-2])",0.5094641447,1
1829,"rounding, overflow, linear algebra","print(decompose(1 + x + x**2 + x*y))
print(decompose(x**2 + x + y + 1 + x**3 + x**2*y + y**4 + x**3*y + y**2*x**2))
print(decompose(x**2 + x + y + 1 + x**3 + x**2*y + y**4 + x**3*y + y**2*x**2, 1))",0.508669734,1
1829,"rounding, overflow, linear algebra",print((lambda x: x**2 + 5*x + 4) (-4)),0.508466959,1
1829,"rounding, overflow, linear algebra","from itertools import count

# Find the first n that fail make 100n^2 < 2^n
next(n for n in count(1) if 100*n**2 < 2**n)",0.5079895854,1
1829,"rounding, overflow, linear algebra","square_wave = Piecewise((1, t <= Rational(1, 2)), (-1, t > Rational(1, 2)))
square_wave",0.5074080229,1
1829,"rounding, overflow, linear algebra","is_independent([list2vec([one,zero,one,zero]),list2vec([zero,one,zero,zero]),list2vec([one,one,one,one]),list2vec([one,zero,zero,one])])",0.5070511103,1
1829,"rounding, overflow, linear algebra","solve([2*x+y-5*z-25,7*x+10*y+2*z-20])",0.5068932772,1
1594,build a classification model,"if __name__ == '__main__':
    # Posture Classification
    classifiers = []
    Accuracy=[]
    Final_results = {}
    for model in gumpy.classification.available_classifiers:
        print (model)
        feature_idx, cv_scores, algorithm, sfs, clf = gumpy.features.sequential_feature_selector(features, labels,model,(6, 25), 3, 'SFFS')
        classifiers.append(model)
        Accuracy.append (cv_scores*100) 
        Final_results[model]= cv_scores*100
        print (Final_results)",0.5218045115,1
1594,build a classification model,"if __name__ == '__main__':
    # Posture Classification
    classifiers = []
    Accuracy=[]
    Final_results = {}
    for model in gumpy.classification.available_classifiers:
        print (model)
        feature_idx, cv_scores, algorithm = gumpy.features.sequential_feature_selector(X_tot, y_tot, model,(6, 25), 10, 'SFFS')
        classifiers.append(model)
        Accuracy.append (cv_scores*100) 
        Final_results[model]= cv_scores*100
print (Final_results)",0.5216577053,1
1594,build a classification model,"class NBClassification(object):
    """"""
    Class to denote a classification result.
    """"""
    
    def __init__(self, label: str, value: float = 0.0):
        self.label: str = label
        self.value: float = value

    def __repr__(self):
        return ""{0}<{1}>"".format(self.label, self.value)


class NBTerm(object):
    """"""
    Class to denote a term.
    """"""

    def __init__(self, term: str, likelihood: float = 0.0):
        self.term: str = term.lower().strip()
        self.likelihood: float = likelihood

    def __repr__(self):
        return ""{0}<{1}>"".format(self.term, self.likelihood)


class NBDocument(object):
    """"""
    Class to denote a document.
    """"""
    
    USE_FILTERED: bool = False
    """"""
    boolean: Discontinued option to enable the use of stopwords.
    """"""

    def __init__(self, raw_terms: [NBTerm], filtered_terms: [NBTerm]):
        self.raw_terms: [NBTerm] = raw_terms  # stopwords included
        self.filtered_terms: [NBTerm] = filtered_terms  # stopwords removed

    def __repr__(self):
        str = ""\t\t\tTerms: {}\n"".format(len(self.get_terms()))
        for t in self.get_terms():
            str += ""\t\t\t{}\n"".format(t)

        return str

    def get_terms(self):
        """"""
        Retrieves all terms in a document.
        
        :return: A List containing ALL terms in a document, including duplicates.
        """"""
        if NBDocument.USE_FILTERED:
            return self.filtered_terms
        else:
            return self.raw_terms


class NBClass(object):
    """"""
    Class to denote a classification class.
    """"""
    
    def __init__(self, label: str):
        self.label: str = label
        self.documents: [NBDocument] = []
        self.prior: float = 0.0
        self.likelihoods: [NBTerm] = []
        self.name: str = """"
        if self.label == '0':
            self.name = 'Wise Saying'
        elif self.label == '1':
            self.name = 'Future'

    def __repr__(self):
        str = ""\tClass Label: {}\n"".format(self.label)
        str += ""\tDocuments: {}\n"".format(len(self.documents))
        for d in self.documents:
            str += ""\t\t{}\n"".format(d)
        str += ""\tPrior: {}\n"".format(self.prior)
        str += ""\tLikelihoods: {}\n"".format(len(self.likelihoods))
        for l in self.likelihoods:
            str += ""\t\t{}\n"".format(l)

        return str

    def add_create_document(self, message: str) -> None:
        """"""
        Create and add a document to this class.
        
        :param message: The message/document to parse and add to this class.
        
        :return: None
        """"""
        terms = message.split(' ')  # break the document into terms
        raw_terms = [NBTerm(term=t) for t in terms]
        filtered_terms = raw_terms  # legacy, no use
        self.documents.append(NBDocument(raw_terms=raw_terms, filtered_terms=filtered_terms))

    def compute_likelihood(self, lexicon: [str]) -> None:
        """"""
        Compute the likelihood for ALL terms in this class and then also for the terms
        that are not in this class, assigning to them a zero-frequency score. For this
        we use the lexicon, which contains UNIQUE terms in all classes.
        
        :param lexicon: A List containing ALL UNIQUE terms in all classes. No duplicates \
            are allowed.
            
        :return: None
        """"""
        # this will include ALL terms in the class, INCLUDED repeated terms!!!
        class_terms = [t.term for d in self.documents for t in d.get_terms()]  # ALL TERMS!!!

        # now for each term in lexicon compute its likelihood and add to the list of likelihoods
        # likelihood = occurrences of term / all terms
        for t in lexicon:
            # compute numerator. add 1 to avoid the zero-frequency problem
            numerator = class_terms.count(t) + 1
            # compute denominator. add count of lexicon to avoid zero-frequency problem
            denominator = len(class_terms) + len(lexicon)
            # add to the likelihood list IF not present
            flag = False
            for e in self.likelihoods:
                if e.term == t:
                    flag = True

            if not flag:
                self.likelihoods.append(NBTerm(term=t, likelihood=(numerator / denominator)))

    def get_likelihood(self, term: str) -> None:
        """"""
        Returns the likelihood for a particular term.
        
        :param term: The needle.
        
        :return: None if needle is not found, likelihood as a float if found.
        """"""
        for e in self.likelihoods:
            if e.term == term:
                return e.likelihood

    def get_class_lexicon(self) -> [str]:
        """"""
        Returns the lexicon for a particular class.
        
        :return: A List of strings containing the lexicon for a class. Remember that in the lexicon \
            the terms are UNIQUE.
        """"""
        lexicon = []
        for d in self.documents:
            for t in d.get_terms():
                if t.term not in lexicon:
                    lexicon.append(t.term)

        return lexicon

    @staticmethod
    def get_class_name(label: str):
        """"""
        Returns the name of a class.
        
        :return: A string containing the name of the class.
        """"""
        if label == '0':
            return 'Wise Saying'
        elif label == '1':
            return 'Future'

        return 'None'


class NBModel(object):
    """"""
    Class to denote a model.
    
    Diagram of a model using encapsulation:
    MODEL
    |-- CLASS 1
    |   |-- DOCUMENT 1
    |   |   |-- TERM 1
    |   |   |-- ...
    |   |   |-- TERM N
    |   |
    |   |-- DOCUMENT N
    |
    |-- CLASS N
    
    The model was built using encapsulation/objects and lists.
    """"""
    
    DEBUG = False
    """"""
    boolean: Enable/Disable debug info.
    """"""

    def __init__(self):
        self.classes: [NBClass] = []
        self.lexicon: [str] = []  # vocabulary of UNIQUE words in ALL documents

    def __repr__(self):
        str = ""Classes: {}\n"".format(len(self.classes))
        for c in self.classes:
            str += ""{}\n"".format(c)
        str += ""Lexicon: {}\n"".format(len(self.lexicon))
        str += ""{}"".format(sorted(self.lexicon))

        return str

    def get_class(self, label: str) -> NBClass:
        """"""
        Return a particular class from the model.
        
        :param label: The label of the class.
        
        :return: A class object matching the label. None if no class is found.
        """"""
        for c in self.classes:
            if c.label == label:
                return c

        return None

    def calculate_and_update_prior(self, label: str) -> None:
        """"""
        Compute and update the PRIOR probabilities for a particular class.
        
        :param label: The label of the class.
        
        :return: None
        """"""
        N_c = float(len(self.get_class(label=label).documents))  # number of docs in class
        N = 0.0  # number of docs in all classes
        for c in self.classes:
            N += len(c.documents)

        # update prior
        self.get_class(label=label).prior = N_c / N

        # +++ DEBUG
        if NBModel.DEBUG:
            print(""PRIOR for class {0} is {1}."".format(label, N_c / N))
            print(""N_c: {0}, N: {1}"".format(N_c, N))

    def compute_lexicon(self) -> None:
        """"""
        Create the lexicon for this class.
        
        :return: None
        """"""
        # vocabulary should NOT contain duplicates
        for c in self.classes:
            for d in c.documents:
                for t in d.get_terms():
                    if t.term not in self.lexicon:
                        self.lexicon.append(t.term)

    def compute_likelihood(self) -> None:
        """"""
        Wrapper function to compute likelihoods. Calls the compute_likelihood() function for each class.
        
        :return: None
        """"""
        for c in self.classes:
            c.compute_likelihood(lexicon=self.lexicon)",0.50458318,1
1594,build a classification model,"class NaiveBayesClassifier:
    """""" Naive Bayes classifier for a mixture of continuous and categorical attributes.
        We use GaussianPredictor for continuous attributes and MultinomialPredictor for categorical ones.
        Attributes:
            predictor (dict): model for P(X_i|Y) for each i
            log_prior (array_like): log P(Y)
    """"""
    # feel free to define and use any more attributes, e.g., number of classes, etc
    def __init__(self, df, alpha=1):
        """"""initializes predictors for each feature and computes class prior
        Inputs:
            df (pd.DataFrame): processed dataframe, without any missing values.
        """"""
        self.u=list(df.columns)
        self.predictor={}
        for i in self.u:
            if i!='label':
                
                if (df[i]).dtypes=='int64':
                    self.predictor[i]=GaussianPredictor(df[i], df['label'])
                else:
                    self.predictor[i]=CategoricalPredictor(df[i], df['label'], alpha)

        self.g=list(np.unique(df['label']))
        self.l=len(self.g)
        self.d=len(df)
        #self.no=np.zeros(self.l)
        self.log_prior=np.zeros(self.l)
        for j in range(self.l):
            self.counter=0
            for i in (df['label']):
                if i==self.g[j]:
                    self.counter=self.counter+1
            self.log_prior[j]=(self.log_prior[j]+self.counter)
        
        for f in range(len(self.log_prior)):
            self.log_prior[f]=np.log((self.log_prior[f]+alpha)/(self.d+alpha*self.l))
        


    def predict(self, x):
        """""" predicts label for input instances from log_prior and partial_log_likelihood of feature predictors
        Inputs:
            x (pd.DataFrame): processed dataframe, without any missing values and without class label.
        Outputs:
            (array_like): array of predicted class labels (0,..,k-1)
        """"""
        self.le=len(x)
        self.predicted=np.zeros(self.le)
        self.sum_pll = np.zeros((self.le,len(self.g)))
        self.io=[]
        for j in x.columns:
            if j!='label':
                self.partial_log_likelihood = self.predictor[j].partial_log_likelihood(x[j])
                self.sum_pll=self.sum_pll+self.partial_log_likelihood
        self.sum_pll=self.sum_pll+self.log_prior
        self.index=np.argmax(self.sum_pll,axis=1)
        for k in range(len(x)):
                       
            self.predicted[k]=(self.g[self.index[k]])
        #self.io.append(self.index)
        self.predicted=self.predicted.astype('int64')
        return self.predicted
        #self.predicted=(self.g[self.index])
#         #return self.io
#         self.predicted=self.predicted.astype('int64')
# #                 
#         return self.predicted

#         for k in range(self.le):
            
#             for j in x.columns:
#                 if j!='label':
#                     self.partial_log_likelihood = self.predictor[j].partial_log_likelihood(x[k][j])
#                     self.sum_pll=self.sum_pll+self.partial_log_likelihood
#                 self.sum_pll=self.sum_pll+self.log_prior
#                 self.index=np.argmax(self.sum_pll)
#                 #self.io.append(self.index)        
#                 self.predicted[k]=(self.g[self.index])
#         #return self.io
#         self.predicted=self.predicted.astype('int64')
        
        
        
        
#         return self.predicted

#         for k in range(self.le):
#             self.sum=0
#             for j in x.columns:
#                 if j!='label':
#                     if x[j].dtype=='object':
#                         self.partial_log_likelihood=np.log(self.predictor[j].p[x[j][k]])
#                         self.sum=self.sum+self.partial_log_likelihood
#                     else:
#                         self.partial_log_likelihood=np.log(1/(np.sqrt(2*np.pi)*(self.predictor[j].sigma))*np.exp(-((self.predictor[j].mu-x[j][k])**2/(2*(self.predictor[j].sigma**2)))))
# #                         self.partial_log_likelihood=np.log(scipy.stats.norm(self.predictor[j].mu,self.predictor[j].sigma).pdf(x[j][k]))
#                         self.sum=self.sum+self.partial_log_likelihood
                        
#                 self.sum=self.sum+self.log_prior
#                 self.index=np.argmax(self.sum)
                        
#             self.predicted[k]=(self.g[self.index])
        
#         self.predicted=self.predicted.astype('int64')
        
        
        
        
        #return self.predicted

# AUTOLAB_IGNORE_START
c = NaiveBayesClassifier(df, 0)
y_pred = c.predict(df)
# AUTOLAB_IGNORE_STOP",0.4987077117,1
1594,build a classification model,"if False:
    Results = []
    classifiers = []
    Accuracy=[]
    Final_results = {}
    for model in gumpy.classification.available_classifiers:
        print (model)
        feature_idx, cv_scores, algorithm,sfs, clf = gumpy.features.sequential_feature_selector(X_train, Y_train, model,(6, 10), 5, 'SFFS')
        classifiers.append(model)
        Accuracy.append (cv_scores*100)
        Final_results[model]= cv_scores*100
        print (Final_results)",0.4979709089,1
1594,build a classification model,"def predict(models, X):
    """"""
    Voting classifier that works by averaging predicted probabilities among models.
    Reminder: you can use model.predict_proba(X) to get probabilities for each class
    """"""

    <implement prediction by voting among models>
    
    return <your code: array of averaged probabilities>
    
prediction = predict(models,X_test)

assert prediction.ndim==2 and prediction.shape[1]==2, ""Predicted probabilities must be a matrix[n,2]""
assert np.allclose(prediction.sum(axis=-1),1)",0.497110188,0
1594,build a classification model,"for i in range (1,3):
    le = LabelEncoder()
    le.fit(Data[:,i])
    Data[:,i] = le.transform(Data[:,i])
    
Data[:,range(1,3)]",0.4969923198,0
1594,build a classification model,"from lasagne.init import Normal #since bad init can blow the network up =c
class discriminator:
    
    #image: real or generated
    l_img = InputLayer((None, 1, IMG_SHAPE[0], IMG_SHAPE[1]),inputX,
                       name='d_input image')
    l_conv0 = Conv2DLayer(l_img, 32, 5)
    l_pooling0 = Pool2DLayer(l_conv0, 3)
    l_conv1 = Conv2DLayer(l_pooling0, 64, 5)
    l_dense0a = DenseLayer(l_conv1, 128,nonlinearity=T.tanh)
    
    #class: from data or from generator input
    l_class = InputLayer((None,N_CLASSES),inputY,
                         name='d_input condition (class)')
    
    l_dense0b = DenseLayer(l_class, 128,nonlinearity=T.tanh)
    
    #concatenate the two branches
    l_dense1 = DenseLayer(concat([l_dense0a,l_dense0b]),256)
    
    #predicted probability of image being REAL
    l_prob = DenseLayer(l_dense1,1,W = Normal(1e-3),
                        nonlinearity=T.nnet.sigmoid) #P(real|x)   # If about 1, then probably a real image
    
    #auxilary stuff
    regularization = regularize_layer_params(l_prob, l2)*1e-4
    
    weights = get_all_params(l_prob, trainable=True)
    
    
    predict = theano.function([inputX,inputY],get_output(l_prob))",0.4953193069,1
1594,build a classification model,"# Majority Rule (hard) Voting

mv_clf = MajorityVoteClassifier(
                classifiers=[pipe1, clf2,pipe3])

clf_labels = ['Logistic Regression', 'Decision Tree', 'K Nearest Neighbours', 'Majority Voting'] 
all_clf = [pipe1, clf2, pipe3, mv_clf]

for clf, label in zip(all_clf, clf_labels):
    scores = cross_val_score(estimator=clf, 
                             X=X_train, 
                             y=y_train, 
                             cv=10, 
                             scoring='roc_auc')
    print(""ROC AUC: %0.2f (+/- %0.2f) [%s]"" 
               % (scores.mean(), scores.std(), label))",0.4950838983,1
1594,build a classification model,"if __name__ == ""__main__"":
    # make triplets: (subject, actual is_spam, predicted is_spam prob)
    classified = [(subject, is_spam, classifier.classify(subject))
                  for subject, is_spam in test_data]
    
    # assume 0.5 probability cutoff, count combinations of (is_spam, pred_is_spam)
    counts = Counter((is_spam, spam_prob > 0.5) 
                     for _, is_spam, spam_prob in classified)
    
    print(counts)",0.494378984,0
2503,vector,"def vortex_contribution_normal(panels):
    A = numpy.empty((panels.size, panels.size), dtype=float)
    numpy.fill_diagonal(A, 0.0)
    for i, panel_i in enumerate(panels):
        for j, panel_j in enumerate(panels):
            if i != j:
                A[i, j] = -0.5 / numpy.pi * integral(panel_i.xc, panel_i.yc, 
                                                     panel_j,
                                                     numpy.sin(panel_i.beta),
                                                     -numpy.cos(panel_i.beta))
    return A",0.4788439274,
2503,vector,"def source_contribution_normal(panels):
  
    A = np.zeros((panels.size, panels.size), dtype=float)
    # source contribution on a panel from itself
    np.fill_diagonal(A, 0.5)
    # source contribution on a panel from others
    for i, panel_i in enumerate(panels):
        for j, panel_j in enumerate(panels):
            if i != j:
                A[i, j] = 0.5 / np.pi * integral(panel_i.xc, panel_i.yc, 
                                                    panel_j,
                                                    np.cos(panel_i.beta),
                                                    np.sin(panel_i.beta))
    return A",0.4759396315,
2503,vector,"# INPUT: V - np.array(N_films, k), test_data - scipy.sparse.csr_matrix (N_train x N_films)
# OTPUT: total_score - integer
def total_score_folding(V, test_data): # 8 pts
    # enter you code here
    total_score = 0
    test_data[:,-3:]=scipy.sparse.csr_matrix(np.zeros((test_data.shape[0],3)))
    for user in test_data:
        rec = V@user.T
        rec = scipy.sparse.csr_matrix(V.T@rec)
        total_score+=np.intersect1d(top_n(user,3), top_n(rec,3)).shape[0]
    return total_score",0.475308001,
2503,vector,"def _make_transparent(cmap):
    # custom colormaps that are transparent at low values
    my_cmap = cmap(np.arange(cmap.N))
    my_cmap[:, -1] = np.zeros_like(my_cmap[:, -1])
    my_cmap[-1:, -1] = 0.75
    my_cmap = ListedColormap(my_cmap)
    return my_cmap",0.4722653031,
2503,vector,"def source_matrix(panels):
    A = numpy.empty((panels.size, panels.size), dtype=float)
    numpy.fill_diagonal(A, 0.5)
    
    for i, p_i in enumerate(panels):
        for j, p_j in enumerate(panels):
            if i != j:
                A[i,j] = 0.5/numpy.pi*integral(p_i.xc, p_i.yc, p_j, numpy.cos(p_i.beta), numpy.sin(p_i.beta))
    
    return A",0.4714748263,1
2503,vector,"def source_contribution_normal(panels):
    A = numpy.empty((panels.size, panels.size), dtype=float)
    numpy.fill_diagonal(A, 0.5)
    for i, panel_i in enumerate(panels):
        for j, panel_j in enumerate(panels):
            if i != j:
                A[i, j] = 0.5 / numpy.pi * integral(panel_i.xc, panel_i.yc, 
                                                    panel_j,
                                                    numpy.cos(panel_i.beta),
                                                    numpy.sin(panel_i.beta))
    return A",0.4714748263,
2503,vector,"def BGR2RGB(Input):
   output = np.zeros(Input.shape);
   output[:,:,0] = Input[:,:,2] 
   output[:,:,1] = Input[:,:,1] 
   output[:,:,2] = Input[:,:,0]
   output = output.astype('uint8')
   return output",0.4671202302,
2503,vector,"def cosine_similarity(model):
    sim = model.item_vecs.dot(model.item_vecs.T)
    norms = np.array([np.sqrt(np.diagonal(sim))])
    return sim / norms / norms.T

als_sim = cosine_similarity(best_als_model)
sgd_sim = cosine_similarity(best_sgd_model)",0.4671061635,1
2503,vector,"def BGR2RGB(Input):
   output = np.zeros(Input.shape);
   output[:,:,0] = Input[:,:,2] 
   output[:,:,1] = Input[:,:,1] 
   output[:,:,2] = Input[:,:,0]
   output = output.astype('uint8')
   return output
pic_ori = cv2.imread('./images/origin/Office.png')
pic_bil = cv2.imread('./Bi_Office.png')
pic_pro = cv2.imread('./Pr_Office.png')
# plot the image out
plt.figure(figsize=(50,32))
plt.subplot(311),plt.imshow(BGR2RGB(pic_ori))
plt.title('Original Image'), plt.xticks([]), plt.yticks([])
plt.subplot(312),plt.imshow(BGR2RGB(pic_bil))
plt.title('Bilinear Image'), plt.xticks([]), plt.yticks([])
plt.subplot(313),plt.imshow(BGR2RGB(pic_pro))
plt.title('Proposed Image'), plt.xticks([]), plt.yticks([])",0.4668260515,
2503,vector,"def vortex_contribution_normal(panels):
    """"""
    Builds the vortex contribution matrix for the normal velocity.
    
    Parameters
    ----------
    panels: 1D array of Panel objects
        List of panels.
    
    Returns
    -------
    A: 2D Numpy array of floats
        Vortex contribution matrix.
    """"""
    A = np.zeros((panels.size, panels.size), dtype=float)
    # vortex contribution on a panel from itself
    np.fill_diagonal(A, 0.0)
    # vortex contribution on a panel from others
    for i, panel_i in enumerate(panels):
        for j, panel_j in enumerate(panels):
            if i != j:
                A[i, j] = -0.5 / np.pi * integral(panel_i.xc, panel_i.yc, 
                                                     panel_j,
                                                     np.sin(panel_i.beta),
                                                     -np.cos(panel_i.beta))
    return A",0.4659152329,
2338,the effect of nan s in arithmetic operations,"0 * np.nan
np.nan == np.nan
np.inf > np.nan
np.nan - np.nan
0.3 == 3 * 0.1",0.5331114531,1
2338,the effect of nan s in arithmetic operations,"0 * np.nan           ## np.nan
np.nan == np.nan     ## False
np.inf > np.nan      ## False
np.nan - np.nan      ## np.nan
0.3 == 3 * 0.1       ## false",0.5331114531,1
2338,the effect of nan s in arithmetic operations,"0 * np.nan
np.nan == np.nan
np.inf > np.nan
np.nan - np.nan 
0.3 == 3 * 0.1",0.5331114531,1
2338,the effect of nan s in arithmetic operations,np.nan - np.nan # np.nan,0.5287275314,1
2338,the effect of nan s in arithmetic operations,Z <- Z,0.5236598849,0
2338,the effect of nan s in arithmetic operations,"np.nan_to_num(normmat, 0.0)",0.516269803,1
2338,the effect of nan s in arithmetic operations,"movie <- ""Toy Story""
movie",0.5161467195,0
2338,the effect of nan s in arithmetic operations,"%%R
myString <- ""Hello, this is R""
print ( myString)",0.5143620968,0
2338,the effect of nan s in arithmetic operations,"a<- ""this is R!""; cat(a)",0.5143620968,0
2338,the effect of nan s in arithmetic operations,1 + np.nan,0.5098353624,1
1381,sum of squares,"def f(invars):
    x = invars[0]
    y = invars[1]
    
    return (x-3)**2 + (y-4)**2 + 1",0.5180427432,1
1381,sum of squares,"def calc_squared_error(line):
    actual = line[0]
    prediction = line[1]
    return (prediction - actual)**2",0.5158830881,1
1381,sum of squares,"def cons(invars):
    x = invars[0]
    y = invars[1]
    
    # Optimizer will ensure return value is >= 0
    return 1-x**2-y**2",0.5125323534,1
1381,sum of squares,"def inverse_fit_func(y,par):
    #a*np.power(x,b)+c
    return np.power((y-par[2])/par[0],1/par[1])",0.5090507269,0
1381,sum of squares,"from operator import add

# Evaluate clustering by computing Within Set Sum of Squared Errors
def error(clusters, point):
    closest_center = clusters.centers[clusters.predict(point)]
    return np.power(euclidean_distance(closest_center, point), 2)",0.5062428117,1
1381,sum of squares,"#(<x,y>+1)^5
#x1^10 + 5 x1^8 x2^2 + 5 x1^8 + 10 x1^6 x2^4 + 20 x1^6 x2^2 + 10 x1^6 + 10 x1^4 x2^6 + 30 x1^4 x2^4 + 30 x1^4 x2^2 + 10 x1^4 + 5 x1^2 x2^8 + 20 x1^2 x2^6 + 30 x1^2 x2^4 + 20 x1^2 x2^2 + 5 x1^2 + x2^10 + 5 x2^8 + 10 x2^6 + 10 x2^4 + 5 x2^2 + 1
def phi3(x):
    x1 = x[0]
    x2 = x[1]
    return np.array([x1**5, np.sqrt(5)*(x1**4)*(x2), np.sqrt(5)*(x1**4), np.sqrt(10)*(x1**3)*(x2**2), 
                     np.sqrt(20)*(x1**3)*x2, np.sqrt(10)*(x1**3), np.sqrt(10)*(x1**2)*(x2**3), 
                     np.sqrt(30)*(x1**2)*(x2**2), np.sqrt(30)*(x1**2)*x2 , np.sqrt(10)*(x1**2), 
                     np.sqrt(5)*x1*(x2**4), np.sqrt(20)*x1*(x2**3), np.sqrt(30)*x1*(x2**2), np.sqrt(20)*x1*x2, 
                     np.sqrt(5)*x1, x2**5, np.sqrt(5)*(x2**4), np.sqrt(10)*(x2**3), np.sqrt(10)*(x2**2), np.sqrt(5)*x2 , 1])",0.5053143501,0
1381,sum of squares,"def g(invars):
    x = invars[0]
    y = invars[1]
    return (x-1)**2 + (y+1)**4 + 3

x0 = np.array([0.,0.])
#g(x0)
#x = x0[0]
#y = x0[1]
#print g(x0)
res = minimize(g,x0)
print res",0.5032632947,1
1381,sum of squares,"def price_predict(x):
    w=best_w
    return w[0]+w[1]*x[0]+w[2]*x[1]+w[3]*x[2]+w[4]*x[3]",0.5031551123,0
1381,sum of squares,"def func(x):
    x1 = x[0]
    x2 = x[1]
    x3 = x[2]
    return x1**2 + x2**2 + x3**2
def newcon1(x):
    return x[0] + x[1] - 6
def newcon2(x):
    return x[2] + 2*x[1] - 4",0.5025279522,1
1381,sum of squares,"#(<x,y>)^2
def phi2(x):
    return np.array([x[0]**2, np.sqrt(2)*x[0]*x[1], x[1]**2])",0.5021484494,1
2594,stock correlation analysis,"def employment_rate(df):
    
    data = [(100*(df[""TotalWarehousingEmployment""].iloc[df[""YearTreated""].iloc[0] - 2004] - df[""TotalWarehousingEmployment""].iloc[df[""YearTreated""].iloc[0] - 2002]) / df[""TotalWarehousingEmployment""].iloc[df[""YearTreated""].iloc[0] - 2002]) + 100,
            (100*(df[""TotalWarehousingEmployment""].iloc[df[""YearTreated""].iloc[0] - 2003] - df[""TotalWarehousingEmployment""].iloc[df[""YearTreated""].iloc[0] - 2002]) / df[""TotalWarehousingEmployment""].iloc[df[""YearTreated""].iloc[0] - 2002]) + 100,
            (100*(df[""TotalWarehousingEmployment""].iloc[df[""YearTreated""].iloc[0] - 2002] - df[""TotalWarehousingEmployment""].iloc[df[""YearTreated""].iloc[0] - 2002]) / df[""TotalWarehousingEmployment""].iloc[df[""YearTreated""].iloc[0] - 2002]) + 100,
            (100*(df[""TotalWarehousingEmployment""].iloc[df[""YearTreated""].iloc[0] - 2001] - df[""TotalWarehousingEmployment""].iloc[df[""YearTreated""].iloc[0] - 2002]) / df[""TotalWarehousingEmployment""].iloc[df[""YearTreated""].iloc[0] - 2002]) + 100,
            (100*(df[""TotalWarehousingEmployment""].iloc[df[""YearTreated""].iloc[0] - 2000] - df[""TotalWarehousingEmployment""].iloc[df[""YearTreated""].iloc[0] - 2002]) / df[""TotalWarehousingEmployment""].iloc[df[""YearTreated""].iloc[0] - 2002]) + 100]
    
    data = pd.DataFrame(data).T
    
    data.columns = [""-2"", ""-1"", ""0"", ""1"", ""2""]
    
    return data

def income_rate(df):
    
    data = [(100*(df[""AverageWarehousingIncome""].iloc[df[""YearTreated""].iloc[0] - 2004] - df[""AverageWarehousingIncome""].iloc[df[""YearTreated""].iloc[0] - 2002]) / df[""AverageWarehousingIncome""].iloc[df[""YearTreated""].iloc[0] - 2002]) + 100,
            (100*(df[""AverageWarehousingIncome""].iloc[df[""YearTreated""].iloc[0] - 2003] - df[""AverageWarehousingIncome""].iloc[df[""YearTreated""].iloc[0] - 2002]) / df[""AverageWarehousingIncome""].iloc[df[""YearTreated""].iloc[0] - 2002]) + 100,
            (100*(df[""AverageWarehousingIncome""].iloc[df[""YearTreated""].iloc[0] - 2002] - df[""AverageWarehousingIncome""].iloc[df[""YearTreated""].iloc[0] - 2002]) / df[""AverageWarehousingIncome""].iloc[df[""YearTreated""].iloc[0] - 2002]) + 100,
            (100*(df[""AverageWarehousingIncome""].iloc[df[""YearTreated""].iloc[0] - 2001] - df[""AverageWarehousingIncome""].iloc[df[""YearTreated""].iloc[0] - 2002]) / df[""AverageWarehousingIncome""].iloc[df[""YearTreated""].iloc[0] - 2002]) + 100,
            (100*(df[""AverageWarehousingIncome""].iloc[df[""YearTreated""].iloc[0] - 2000] - df[""AverageWarehousingIncome""].iloc[df[""YearTreated""].iloc[0] - 2002]) / df[""AverageWarehousingIncome""].iloc[df[""YearTreated""].iloc[0] - 2002]) + 100]
    
    data = pd.DataFrame(data).T
    
    data.columns = [""-2"", ""-1"", ""0"", ""1"", ""2""]
    
    return data",0.4752085805,0
2594,stock correlation analysis,"def employment_rate_county(df):
    
    data = [(100*(df[""TotalWarehousingEmployment_x""].iloc[df[""YearTreated""].iloc[0] - 2004] - df[""TotalWarehousingEmployment_x""].iloc[df[""YearTreated""].iloc[0] - 2002]) / df[""TotalWarehousingEmployment_x""].iloc[df[""YearTreated""].iloc[0] - 2002]) + 100,
            (100*(df[""TotalWarehousingEmployment_x""].iloc[df[""YearTreated""].iloc[0] - 2003] - df[""TotalWarehousingEmployment_x""].iloc[df[""YearTreated""].iloc[0] - 2002]) / df[""TotalWarehousingEmployment_x""].iloc[df[""YearTreated""].iloc[0] - 2002]) + 100,
            (100*(df[""TotalWarehousingEmployment_x""].iloc[df[""YearTreated""].iloc[0] - 2002] - df[""TotalWarehousingEmployment_x""].iloc[df[""YearTreated""].iloc[0] - 2002]) / df[""TotalWarehousingEmployment_x""].iloc[df[""YearTreated""].iloc[0] - 2002]) + 100,
            (100*(df[""TotalWarehousingEmployment_x""].iloc[df[""YearTreated""].iloc[0] - 2001] - df[""TotalWarehousingEmployment_x""].iloc[df[""YearTreated""].iloc[0] - 2002]) / df[""TotalWarehousingEmployment_x""].iloc[df[""YearTreated""].iloc[0] - 2002]) + 100,
            (100*(df[""TotalWarehousingEmployment_x""].iloc[df[""YearTreated""].iloc[0] - 2000] - df[""TotalWarehousingEmployment_x""].iloc[df[""YearTreated""].iloc[0] - 2002]) / df[""TotalWarehousingEmployment_x""].iloc[df[""YearTreated""].iloc[0] - 2002]) + 100]
    
    data = pd.DataFrame(data).T
    
    data.columns = [""-2"", ""-1"", ""0"", ""1"", ""2""]
    
    return data

def employment_rate_us(df):
    
    data = [(100*(df[""TotalWarehousingEmployment_y""].iloc[df[""YearTreated""].iloc[0] - 2004] - df[""TotalWarehousingEmployment_y""].iloc[df[""YearTreated""].iloc[0] - 2002]) / df[""TotalWarehousingEmployment_y""].iloc[df[""YearTreated""].iloc[0] - 2002]) + 100,
            (100*(df[""TotalWarehousingEmployment_y""].iloc[df[""YearTreated""].iloc[0] - 2003] - df[""TotalWarehousingEmployment_y""].iloc[df[""YearTreated""].iloc[0] - 2002]) / df[""TotalWarehousingEmployment_y""].iloc[df[""YearTreated""].iloc[0] - 2002]) + 100,
            (100*(df[""TotalWarehousingEmployment_y""].iloc[df[""YearTreated""].iloc[0] - 2002] - df[""TotalWarehousingEmployment_y""].iloc[df[""YearTreated""].iloc[0] - 2002]) / df[""TotalWarehousingEmployment_y""].iloc[df[""YearTreated""].iloc[0] - 2002]) + 100,
            (100*(df[""TotalWarehousingEmployment_y""].iloc[df[""YearTreated""].iloc[0] - 2001] - df[""TotalWarehousingEmployment_y""].iloc[df[""YearTreated""].iloc[0] - 2002]) / df[""TotalWarehousingEmployment_y""].iloc[df[""YearTreated""].iloc[0] - 2002]) + 100,
            (100*(df[""TotalWarehousingEmployment_y""].iloc[df[""YearTreated""].iloc[0] - 2000] - df[""TotalWarehousingEmployment_y""].iloc[df[""YearTreated""].iloc[0] - 2002]) / df[""TotalWarehousingEmployment_y""].iloc[df[""YearTreated""].iloc[0] - 2002]) + 100]
    
    data = pd.DataFrame(data).T
    
    data.columns = [""-2"", ""-1"", ""0"", ""1"", ""2""]
    
    return data",0.4752085805,0
2594,stock correlation analysis,"def income_rate_county(df):
    
    data = [(100*(df[""AverageWarehousingIncome_x""].iloc[df[""YearTreated""].iloc[0] - 2004] - df[""AverageWarehousingIncome_x""].iloc[df[""YearTreated""].iloc[0] - 2002]) / df[""AverageWarehousingIncome_x""].iloc[df[""YearTreated""].iloc[0] - 2002]) + 100,
            (100*(df[""AverageWarehousingIncome_x""].iloc[df[""YearTreated""].iloc[0] - 2003] - df[""AverageWarehousingIncome_x""].iloc[df[""YearTreated""].iloc[0] - 2002]) / df[""AverageWarehousingIncome_x""].iloc[df[""YearTreated""].iloc[0] - 2002]) + 100,
            (100*(df[""AverageWarehousingIncome_x""].iloc[df[""YearTreated""].iloc[0] - 2002] - df[""AverageWarehousingIncome_x""].iloc[df[""YearTreated""].iloc[0] - 2002]) / df[""AverageWarehousingIncome_x""].iloc[df[""YearTreated""].iloc[0] - 2002]) + 100,
            (100*(df[""AverageWarehousingIncome_x""].iloc[df[""YearTreated""].iloc[0] - 2001] - df[""AverageWarehousingIncome_x""].iloc[df[""YearTreated""].iloc[0] - 2002]) / df[""AverageWarehousingIncome_x""].iloc[df[""YearTreated""].iloc[0] - 2002]) + 100,
            (100*(df[""AverageWarehousingIncome_x""].iloc[df[""YearTreated""].iloc[0] - 2000] - df[""AverageWarehousingIncome_x""].iloc[df[""YearTreated""].iloc[0] - 2002]) / df[""AverageWarehousingIncome_x""].iloc[df[""YearTreated""].iloc[0] - 2002]) + 100]
    
    data = pd.DataFrame(data).T
    
    data.columns = [""-2"", ""-1"", ""0"", ""1"", ""2""]
    
    return data

def income_rate_us(df):
    
    data = [(100*(df[""AverageWarehousingIncome_y""].iloc[df[""YearTreated""].iloc[0] - 2004] - df[""AverageWarehousingIncome_y""].iloc[df[""YearTreated""].iloc[0] - 2002]) / df[""AverageWarehousingIncome_y""].iloc[df[""YearTreated""].iloc[0] - 2002]) + 100,
            (100*(df[""AverageWarehousingIncome_y""].iloc[df[""YearTreated""].iloc[0] - 2003] - df[""AverageWarehousingIncome_y""].iloc[df[""YearTreated""].iloc[0] - 2002]) / df[""AverageWarehousingIncome_y""].iloc[df[""YearTreated""].iloc[0] - 2002]) + 100,
            (100*(df[""AverageWarehousingIncome_y""].iloc[df[""YearTreated""].iloc[0] - 2002] - df[""AverageWarehousingIncome_y""].iloc[df[""YearTreated""].iloc[0] - 2002]) / df[""AverageWarehousingIncome_y""].iloc[df[""YearTreated""].iloc[0] - 2002]) + 100,
            (100*(df[""AverageWarehousingIncome_y""].iloc[df[""YearTreated""].iloc[0] - 2001] - df[""AverageWarehousingIncome_y""].iloc[df[""YearTreated""].iloc[0] - 2002]) / df[""AverageWarehousingIncome_y""].iloc[df[""YearTreated""].iloc[0] - 2002]) + 100,
            (100*(df[""AverageWarehousingIncome_y""].iloc[df[""YearTreated""].iloc[0] - 2000] - df[""AverageWarehousingIncome_y""].iloc[df[""YearTreated""].iloc[0] - 2002]) / df[""AverageWarehousingIncome_y""].iloc[df[""YearTreated""].iloc[0] - 2002]) + 100]
    
    data = pd.DataFrame(data).T
    
    data.columns = [""-2"", ""-1"", ""0"", ""1"", ""2""]
    
    return data",0.4752085805,0
2594,stock correlation analysis,"# plt.matshow(Triangle.T.corr(method='pearson', min_periods=1))
def plot_corr(df,size=10):
    '''Function plots a graphical correlation matrix for each pair of columns in the dataframe.

    Input:
        df: pandas DataFrame
        size: vertical and horizontal size of the plot'''

    corr = df.corr(method='pearson', min_periods=1)
    fig, ax = plt.subplots(figsize=(size, size))
    ax.matshow(corr)
    plt.xticks(range(len(corr.columns)), corr.columns);
    plt.yticks(range(len(corr.columns)), corr.columns);
    return corr
corr = plot_corr(Triangle.T)",0.4720790386,1
2594,stock correlation analysis,"def person_plotA(person_name):
    ix_check = participant_y_pred(person_name) > 10
    _ = plt.hist(participant_y_pred(person_name)[~ix_check], range=[0, x_lim], bins=x_lim, histtype='stepfilled', label='<10 seconds')
    _ = plt.hist(participant_y_pred(person_name)[ix_check], range=[0, x_lim], bins=x_lim, histtype='stepfilled', label='>10 seconds')
    _ = plt.title('Posterior predictive \ndistribution for %s' % person_name)
    _ = plt.xlabel('Response time')
    _ = plt.ylabel('Frequency')
    _ = plt.legend()

def person_plotB(person_name):
    x = np.linspace(1, 60, num=60)
    num_samples = float(len(participant_y_pred(person_name)))
    prob_lt_x = [100*sum(participant_y_pred(person_name) < i)/num_samples for i in x]
    _ = plt.plot(x, prob_lt_x, color=colors[4])
    _ = plt.fill_between(x, prob_lt_x, color=colors[4], alpha=0.3)
    _ = plt.scatter(10, float(100*sum(participant_y_pred(person_name) < 10))/num_samples, s=180, c=colors[4])
    _ = plt.title('Probability of responding \nto %s before time (t)' % person_name)
    _ = plt.xlabel('Response time (t)')
    _ = plt.ylabel('Cumulative probability t')
    _ = plt.ylim(ymin=0, ymax=100)
    _ = plt.xlim(xmin=0, xmax=60)

fig = plt.figure(figsize=(11,6))
_ = fig.add_subplot(221)
person_plotA('Michael Melchger')
_ = fig.add_subplot(222)
person_plotB('Michael Melchger')

_ = fig.add_subplot(223)
person_plotA('Patrick Oswald')
_ = fig.add_subplot(224)
person_plotB('Patrick Oswald')

plt.tight_layout()",0.4684532881,1
2594,stock correlation analysis,"def compute_corr(k):
    return prot_data_log2.iloc[k][""prot""].corr(mRNA_data_log2.iloc[k][""mRNA""], method='pearson')",0.4664903283,1
2594,stock correlation analysis,"games = []

def merge_cp(data, country):
    '''Returns cleaned dataset for total tourism'''

    #Only keep relevant variables, rename, and transform the units.
    copy = data.iloc[[4, 5, 6, 54]]
    copy = copy.drop(['Cod.', 'Notes', 'Units'], axis=1)
    copy.iloc[0,0] = 'tourist_total'
    copy.iloc[1,0] = 'overnight_visitors'
    copy.iloc[2,0] = 'same_day_visitors'
    copy.iloc[3,0] = 'length_of_stay'
    copy.iloc[:-1,1:] = copy.iloc[:-1,1:]*1000

    #Reshape by years, then make separate columns
    copy = pd.melt(copy, id_vars=['Basic data and indicators'], var_name='year_id', value_name='data')
    copy = copy.pivot(index='year_id', columns='Basic data and indicators', values='data')
    copy['location_name'] = country
    copy['year_id'] = copy.index

    #Replace almost all missing values with next best estimates
    copy = next_best(copy)
    return copy

def next_best(data):
    '''Replaces tourist total with next best estimate if tourist total is missing'''

    if np.isnan(data['tourist_total'].values).sum() >= 19:
        data['tourist_total'] = data['overnight_visitors']
    if np.isnan(data['tourist_total'].values).sum() >= 19:
        data['tourist_total'] = data['same_day_visitors']
    return data

for country, data in filtered_by_cp.items():
    games.append(merge_cp(data, country))

#Merge on stgpr template
tourist_total = pd.concat(games, ignore_index=True)
tourist_total = fix_locations(tourist_total)
tourist_total_gpr = pd.merge(tourist_total, template, how='right', on=['location_name', 'year_id'])
tourist_total_gpr['year_id'] = tourist_total_gpr['year_id'].astype(int)",0.4645704627,
2594,stock correlation analysis,"def strategy_moving_average_crossover(bars, short_window = 100, long_window = 400):
    """"""    
    Return the DataFrame of symbols containing the signals to go long, short or hold (1, -1 or 0).

    Requires:
    bars - A DataFrame of bars for the above symbol.
    short_window - Lookback period for short moving average.
    long_window - Lookback period for long moving average.
    """"""

    signals = pd.DataFrame(index = bars.index)
    signals['invested'] = 0.

    # Create the set of short and long simple moving averages over the respective periods
    signals['short_mavg'] = pd.rolling_mean(bars, short_window, min_periods = 1)
    signals['long_mavg'] = pd.rolling_mean(bars, long_window, min_periods = 1)

    # Assign 1. when the short moving average is above the long moving average (else 0.)
    signals['invested'] = np.where(signals['short_mavg'] > signals['long_mavg'], 1., 0.)
    # Wait a little before investing
    signals['invested'][:long_window] = 0.
    
    # Take the difference in order to generate buy/sell signal
    signals['signal'] = signals['invested'].diff()

    return signals['signal']",0.4637563825,
2594,stock correlation analysis,"def compute_corr(k):
    """"""Compute correlation between prot k and ribo k for log2 data""""""
    return prot_data_geom.iloc[k][""prot""].corr(ribo_data_geom.iloc[k][""ribo""], method='pearson')",0.4630548358,1
2594,stock correlation analysis,"def CAGR(returns):
    cumulative_returns = returns.cumsum().iloc[-1]   
    period_in_days = len(returns)
    return 100*((cumulative_returns+1)**(252.0/period_in_days)-1)",0.4623790681,
409,data manipulation and plotting review,"PCAmodelAnalytical = nPYc.multivariate.exploratoryAnalysisPCA(tData, withExclusions=True, scaling=1.0)

nPYc.reports.multivariateReport.multivariateQCreport(tData, reportType='analytical', withExclusions=True)",0.3655978441,0
409,data manipulation and plotting review,"#A new dataset
xy3 = q.XYDataSet( xdata = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
                  ydata = [0.9, 1.4, 2.5, 4.2, 5.7, 6., 7.3, 7.1, 8.9, 10.8],
                  yerr = 0.5,
                  xname = 'length', xunits='m',
                  yname = 'force', yunits='N',
                  data_name = 'xydata3')

fig1.add_dataset(xy3)
fig1.add_residuals()
fig1.show()",0.3492957354,1
409,data manipulation and plotting review,"plt.scatter(dg.reanalysis_avg_temp_k,dg.reanalysis_air_temp_k)",0.3474496603,1
409,data manipulation and plotting review,"#A third option, is to use an XYDataSet to represent our paired set of 
#x and y data. In fact, the XYDataSet is how QExPy internally holds
#the data that are passed when calling MakePlot()

#The datasets can be built either directly from the data or from 
#two measurement arrays, inmuch the same way as we passed the data
#directly to the plots


xy3 = q.XYDataSet( xdata = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
                  ydata = [0.6, 1.6, 3.5, 4.1, 4.6, 5.6, 6.1, 7.9, 8.7, 9.8],
                  yerr = 0.5,
                  xname = 'length', xunits='m',
                  yname = 'force', yunits='N',
                  data_name = 'April22ndData')

#Let's print out the dataset to see what it looks like:
print(xy3)

#We can now build the figure from the dataset directly:
fig3 = q.MakePlot(xy3)
fig3.show()",0.3444262445,1
409,data manipulation and plotting review,"#initialize the dataset
xy2 = q.XYDataSet(xdata = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
                  ydata = [2., 2.3, 2.8, 3.8, 5.2, 5.9, 7.8, 7.7, 8.8, 10.1],
                  yerr = [0.4, 0.6, 0.5, 0.4, 0.4, 0.5, 0.5, 0.5, 0.6, 0.5],
                  xname = 'length', xunits='m',
                  yname = 'force', yunits='N',
                  data_name = 'xydata2')

#create the plot object
fig2 = q.MakePlot(xy2)
#show the figure, it will have no fit:
fig2.show()",0.3435669243,1
409,data manipulation and plotting review,"self.markov = {0 : [0, 0.33, 0.18, 0, 0.143, 0.2, 0.143],
               1 : [0.3, 0, 0.6, 0, 0.1, 0, 0],
               2 : [0, 0.1, 0.33, 0.3, 0, 0.28, 0],
               3 : [0.2, 0, 0.2, 0.33, 0.15, 0.12, 0],
               4 : [0.24, 0.3, 0, 0.2, 0.26, 0, 0],
               5 : [0.2, 0, 0.32, 0, 0.48, 0, 0],
               6 : [0, 0, 0.25, 0, 0.25, 0, 0.5]
               }",0.3433355689,0
409,data manipulation and plotting review,"#Initialize the data set:
xy1 = q.XYDataSet( xdata = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
                  ydata = [0.6, 1.6, 3.5, 4.1, 4.6, 5.6, 6.1, 7.9, 8.7, 9.8],
                  yerr = 0.5,
                  xname = 'length', xunits='m',
                  yname = 'force', yunits='N',
                  data_name = 'xydata')

results = xy1.fit(""linear"")",0.3412072062,0
409,data manipulation and plotting review,systems[2].m.plot_plane('z'),0.3389235735,1
409,data manipulation and plotting review,"xy7 = q.XYDataSet(xdata = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
                  xerr = [0.5, 0.6, 0.5, 0.6, 0.5, 0.6, 0.5, 0.5, 0.6, 0.4],
                  ydata = [2., 2.3, 2.8, 3.8, 5.2, 5.9, 7.8, 7.7, 8.8, 10.1],
                  yerr = [0.4, 0.6, 0.5, 0.4, 0.4, 0.5, 0.5, 0.5, 0.6, 0.5],
                  xname = 'length', xunits='m',
                  yname = 'force', yunits='N',
                  data_name = 'April25thData')

fig7 = q.MakePlot(xy7)
fig7.show()",0.3377826512,1
409,data manipulation and plotting review,ACSpipe1.plot_MCMCresults('2'),0.3371994793,1
507,drop the rows with nan values,"def cols_with_missings(data_df):

    return [col for col in data_df.columns if data_df[col].isnull().any()]",0.5870018005,0
507,drop the rows with nan values,"#function to check if there are any null values in any column
def null_columns(x):
    y = x.columns[x.isnull().any()]
    return y",0.5427462459,0
507,drop the rows with nan values,"def get_col_null_counts(df):
    return pd.DataFrame(sorted({c: df[c].isnull().sum() for c in df.columns}.items(), key=lambda x: x[1], 
                               reverse=True), columns=['Column', 'Null Count'])",0.5401771665,0
507,drop the rows with nan values,"def show_null_samples(frame):
    print(frame[frame.isnull().any(axis=1)])",0.5339337587,0
507,drop the rows with nan values,"def remove_nan_rows(dataframe):
    for column in dataframe.columns:
        if dataframe[column].isnull().sum() > 10000:
            del dataframe[column]
    return dataframe",0.5336865187,1
507,drop the rows with nan values,"def to_drop(df):
    df.drop([c for c in df.columns if 'target_mean' in c or 'freq' in c or c in ['day', 'next_click_shift']],
            axis=1, inplace=True)
    return df",0.533570528,0
507,drop the rows with nan values,"# this function remove all rows which have some fields empty, it takes the list of important columns names such as 'pickup_longitude' and 'pickup_latitude'. 
#Then it checks these columns if it had any empty fields, this function will remove this row.
def removeRow_if_some_fields_empty(df,list_column_name):
    for c in list_column_name:
        df.dropna(subset=[c], inplace=True)
    return df",0.5334785581,1
507,drop the rows with nan values,"def drop_null_rows_and_columns(dataframe):
    dataframe.dropna(how = ""all"", axis =1, inplace = True)
    dataframe.dropna(how = ""all"", axis =0, inplace = True)
    return dataframe",0.5333256721,1
507,drop the rows with nan values,"def clean_data(data):
    data.dropna(inplace=True)
    return data",0.5315004587,1
507,drop the rows with nan values,"def missing_value(df):
    col = df.columns
    for i in col:
        if df[i].isnull().sum()>0:
            df[i].fillna(df[i].mode()[0],inplace=True)",0.5309422612,0
1242,multivariate regression,"if Test_PCA:
    # INFO: PCA is used *before* KNeighbors to simplify your high dimensionality
    # image samples down to just 2 principal components! A lot of information
    # (variance) is lost during the process, as I'm sure you can imagine. But
    # you have to drop the dimension down to two, otherwise you wouldn't be able
    # to visualize a 2D decision surface / boundary. In the wild, you'd probably
    # leave in a lot more dimensions, which is better for higher accuracy, but
    # worse for visualizing the decision boundary;
    #
    # Your model should only be trained (fit) against the training data (data_train)
    # Once you've done this, you need use the model to transform both data_train
    # and data_test from their original high-D image feature space, down to 2D
    
    from sklearn.decomposition import PCA
    pca_scaler = PCA(n_components=2, svd_solver='full', random_state=7).fit(X_train)
    X_train = pca_scaler.transform(X_train)
    X_test = pca_scaler.transform(X_test)

else:
    # INFO: Isomap is used *before* KNeighbors to simplify your high dimensionality
    # image samples down to just 2 components! A lot of information has been is
    # lost during the process, as I'm sure you can imagine. But if you have
    # non-linear data that can be represented on a 2D manifold, you probably will
    # be left with a far superior dataset to use for classification. Plus by
    # having the images in 2D space, you can plot them as well as visualize a 2D
    # decision surface / boundary. In the wild, you'd probably leave in a lot more
    # dimensions, which is better for higher accuracy, but worse for visualizing the
    # decision boundary;
    
    # Your model should only be trained (fit) against the training data (data_train)
    # Once you've done this, you need use the model to transform both data_train
    # and data_test from their original high-D image feature space, down to 2D

    
    from sklearn.manifold import Isomap
    iso_scaler = Isomap(n_neighbors=5, n_components=2).fit(X_train)
    X_train = iso_scaler.transform(X_train)
    X_test = iso_scaler.transform(X_test)",0.5392035842,0
1242,multivariate regression,"from sklearn.svm import SVC

def answer_seven():
    vect = TfidfVectorizer(min_df=5).fit(X_train)#min_df=5
    X_train_vectorized = vect.transform(X_train)
    X_test_vectorized = vect.transform(X_test)
    
    X_train_len=X_train.apply(lambda x:len(x))
    X_test_len=X_test.apply(lambda x:len(x))
    
    X_train_vectorized=add_feature(X_train_vectorized, X_train_len)
    X_test_vectorized=add_feature(X_test_vectorized, X_test_len)
    #X_train_vectorized
    model = SVC(C=10000)
    model.fit(X_train_vectorized, y_train)
    predictions = model.predict(X_test_vectorized)

    #print('AUC: ', roc_auc_score(y_test, predictions))
    
    return roc_auc_score(y_test, predictions)",0.5376688242,0
1242,multivariate regression,"from sklearn.svm import SVC

def answer_seven():
    
    vect = TfidfVectorizer(min_df=5).fit(X_train)
    X_tr_v = vect.transform(X_train)
    
    X_tr = add_feature(X_tr_v, X_train.str.len())
    X_te = add_feature(vect.transform(X_test), X_test.str.len())
    
    svc = SVC(C=10000).fit(X_tr, y_train)
    preds = svc.predict(X_te)
    
    return roc_auc_score(y_test, preds)",0.5339833498,0
1242,multivariate regression,"from sklearn.svm import SVC

def answer_seven():
    vect = TfidfVectorizer(min_df=5).fit(X_train)
    X_train_vectorized = vect.transform(X_train)
    X_train_vectorized = add_feature(X_train_vectorized, X_train.str.len())
    X_test_vectorized = vect.transform(X_test)
    X_test_vectorized = add_feature(X_test_vectorized, X_test.str.len())
    model = SVC(C=10000)
    model.fit(X_train_vectorized, y_train)
    predictions = model.predict(X_test_vectorized)    
    return roc_auc_score(y_test, predictions)",0.5337875485,0
1242,multivariate regression,"from sklearn.linear_model import LogisticRegression as lr
def logistic_model():
    logit_model = lr()
    logit_model = logit_model.fit(train[range(1,data.shape[1])],train[0])
    pred_y = logit_model.predict(test[range(1,data.shape[1])])
    return accuracy_score(test[0], pred_y)
print logistic_model()",0.5335804224,1
1242,multivariate regression,"# Defining the regression models 

def linear_regression(X_train, y_train, X_test, y_test, plot=True):
    regr = LinearRegression()
    regr.fit(X_train, y_train)

    # Make predictions using the testing set
    y_pred = regr.predict(X_test)

    # Error
    rms_r2_score(y_test, y_pred)
    
    if plot:
        # Plot outputs
        #plot_task1(X_test, y_test, y_pred, 'Linear Regression')
        plot_powergeneration(y_test, y_pred, 'Linear Regression')
    return y_pred
        

def k_nearest_neighbors(X_train, y_train, X_test, y_test, plot=True):
    neigh = KNeighborsRegressor(n_neighbors=800)
    neigh.fit(X_train_selected, y_train) 
    y_pred = neigh.predict(X_test)

    # Error
    rms_r2_score(y_test, y_pred)
    
    if plot:
        # Plot outputs
        #plot_task1(X_test, y_test, y_pred, 'KNN')
        plot_powergeneration(y_test, y_pred, 'KNN')

#def knn_crossval(X,y,n_folds=10):
#    num_neighbors = [1, 5, 20, 50, 100, 500, 800, 1000]
#    #leaf_size = [10, 30, 50, 100, 200, 500]
#    param_grid = [{'n_neighbors': num_neighbors,
#                   'weights':['uniform'],
#                 #  'leaf_size':leaf_size
#                  },
#                 {'n_neighbors': num_neighbors,
#                  'weights':['distance'],
#               #   'leaf_size':leaf_size
#                 }]
#    grid_search = GridSearchCV(KNeighborsRegressor(),
#                                   param_grid,
#                                   cv=n_folds,
#                                   n_jobs=-1)
#    grid_search.fit(X, y)
#    grid_search.best_params_
#    return grid_search.best_params_    
#    
#def k_nearest_neighbors(X_train, y_train, X_test, y_test, plot=True):
#    
#    best_params = knn_crossval(X_train, y_train)    
#    neigh = KNeighborsRegressor().set_params(**best_params)
#    
#    neigh.fit(X_train_selected, y_train) 
#    y_pred = neigh.predict(X_test)
#
#    # The Root mean squared error
#    print(""Root Mean squared error: %.4f""
#          % np.sqrt(mean_squared_error(y_test, y_pred)))
#    # Explained variance score: 1 is perfect prediction
#    print('Variance score: %.2f' % r2_score(y_test, y_pred))
#    
#    if plot:
#        # Plot outputs
#        #plot_task1(X_test, y_test, y_pred, 'KNN')
#        plot_powergeneration(y_test, y_pred, 'KNN')


def support_vector_regression(X_train, y_train, X_test, y_test, plot=True):

    svr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)
    y_pred = svr_rbf.fit(X_train_selected, y_train).predict(X_test_selected)

    # Error
    rms_r2_score(y_test, y_pred)
    
    if plot:
        #plot_task1(X_test, y_test, y_pred, 'SVR')
        plot_powergeneration(y_test, y_pred, 'SVR')
        
        

def ann_model(X_train, y_train, X_test, y_test, plot=True):
    '''
    Trains an artificial neural network. n-input channels and one 
    output(the predicted power).
    '''
    input_shape = X_train.shape[1]
    output_shape = 1
    model = Sequential()

    model.add(InputLayer(input_shape=(input_shape,)))
    model.add(Dense(6, kernel_initializer='lecun_normal',
                    bias_initializer='ones',activation='selu'))
    model.add(Dropout(0.3))
    model.add(Dense(8, kernel_initializer='lecun_normal',
                    bias_initializer='ones',activation='softmax'))
    model.add(Dense(output_shape))

    model.compile(optimizer='rmsprop',
                loss='mean_squared_error')  #batch_size=2000, epochs=100,

    model.fit(X_train, y_train,epochs=10,  verbose=1)


    y_pred = model.predict(X_test)
    
    # Error
    rms_r2_score(y_test, y_pred)

    if plot:
        plot_powergeneration(y_test, y_pred, model='ANN')
 
def rms_r2_score(y_test, y_pred):
    # The Root mean squared error
    print(""Root Mean squared error: %.4f""
          % np.sqrt(mean_squared_error(y_test, y_pred)))
    # Explained variance score: 1 is perfect prediction
    print('R^2: %.2f' % r2_score(y_test, y_pred))

# Store predictions to file function        
def store_predictions_to_file(y_pred, model=None, task=1 , 
                              template='ForecastTemplate.csv'):
    pred = pd.read_csv(template)
    pred['FORECAST'] = y_pred[:len(pred)]
    pred.to_csv('ForecastTemplate{1}-{0}.csv'.format(model,task), index=False)
        
        
        
# Plotting function

def plot_powergeneration(y_test, y_pred, model=None):
    
    plt.figure(figsize=(15,5))
   
    plt.plot(y_test.values, color='darkorange', label='Real')
    
    plt.plot(y_pred, color='navy', label='Predicted')
    
    plt.xlabel('Time')
    plt.ylabel('Wind Power')
    plt.title(model)
    plt.legend()
    #plt.ylim(-0.1,y_test.max().all()+0.1)
    plt.show()
    

def plot_task1(X_test, y_test, y_pred, model=None):
    plt.scatter(X_test, y_test, color='darkorange', 
            marker='.', label='Real', linewidth=0.1)
    
    plt.scatter(X_test, y_pred, color='navy', 
                marker='.', label='Predicted', linewidth=0.1)
    
    plt.xlabel('Wind speed')
    plt.ylabel('Wind Power')
    plt.title(model)
    plt.legend()
    plt.ylim(-0.1,y_test.max().all()+0.1)
    plt.show()",0.5325804949,1
1242,multivariate regression,"from sklearn.linear_model import LinearRegression

def helper(x):
    """"""
    1. Train and test the simple linear regression model with feature x
    2. Plot the model on the testing set
    3. Print the slope and intercept
    
    Parameters
    ----------
    x : a feature
    
    Returns
    ----------
    y_pred : the predicted value of the target
    """"""

    # Declare the linear regression model
    slr = LinearRegression()
    
    # Get the index of feature x
    idx = features.index(x)
    
    # Train the model on the training set
    slr.fit(X_train[:, idx].reshape(-1, 1), y_train)
    
    # Test the model on the testing set
    # Implement me

    y_pred =     

    # Plot the model on the testing set
    plt.scatter(X_test[:, idx], y_test, c='steelblue', edgecolor='white', s=70)
    plt.plot(X_test[:, idx], y_pred, color='black', lw=2)  
    plt.xlabel(x)
    plt.ylabel(target)
    plt.show()
    
    # Print Slope and Intercept
    print('Slope for model with feature ' + x + ': %.3f' % slr.coef_[0])
    print('Intercept for model with feature ' + x + ': %.3f' % slr.intercept_)
    
    return y_pred",0.5324105024,1
1242,multivariate regression,"from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score
def linear_reg(features, label):
    clf = linear_model.LinearRegression()
    clf.fit(df_train[features],df_train[label])
    # Test
    # Make predictions using the testing set
    pred = clf.predict(df_test[features])
    
    # The coefficients
    print('Coefficients: \n', features, '\n', clf.coef_)

    # The mean squared error
    print(""Mean squared error: %.2f""
          % mean_squared_error(df_test[label], pred))
    # Explained variance score: 1 is perfect prediction
    print('Variance score: %.2f \n' % r2_score(df_test[label], pred))",0.5310631394,1
1242,multivariate regression,"from sklearn import linear_model
import numpy as np
def linearModel(faelle):
    try:
        # Create linear regression object
        regr = linear_model.LinearRegression()

        # Train the model using the training sets
        regr.fit(np.arange(len(faelle)).reshape(-1,1), faelle.values)
        
        return max(0, (regr.coef_*len(faelle)+regr.intercept_)[0])
    
    except ValueError:
        return None",0.5281732082,1
1242,multivariate regression,"import numpy as np
from sklearn import linear_model
def lin_reg(x,y):
    linreg=linear_model.LinearRegression(fit_intercept=True)
    linreg.fit([[x_] for x_ in x],y)
    slope=linreg.coef_[0]
    intercept=np.mean(x)
    return (""Y={a}*n + {b}"".format(a=slope,b=intercept))",0.5280357599,1
2178,test the model for accuracy,"## TODO: Create train and evaluate function using tf.estimator
def train_and_evaluate(output_dir, num_train_steps):
  #ADD CODE HERE
  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)",0.5668202639,1
2178,test the model for accuracy,"# ALL section to do. unuasable
def validate(val_loader, model, criterion, epoch):
    batch_time = AverageMeter()
    losses = AverageMeter()
    top1 = AverageMeter()

    # switch to evaluate mode
    model.eval()

    end = time.time()
    for i, (input, target) in enumerate(val_loader):
        target = target.cuda(async=True)
        
        with torch.set_grad_enabled(False):
            input_var = torch.autograd.Variable(input)
            target_var = torch.autograd.Variable(target)
    
            # compute output
            output = model(input_var)
            loss = criterion(output, target_var)
    
            # measure accuracy and record loss
            prec1 = accuracy(output.data, target, topk=(1, 1))
            losses.update(loss.data[0], input.size(0))
            top1.update(prec1[0], input.size(0))
    
            # measure elapsed time
            batch_time.update(time.time() - end)
            end = time.time()
    
            if i % 10 == 0:
                print('Validation: [{0}/{1}]\t'
                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
                      'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
                      'Accuracy {top1.val:.3f} ({top1.avg:.3f})\t'.format(
                       i, len(val_loader), batch_time=batch_time, loss=losses,
                       top1=top1))
    print(' * Accuracy {top1.avg:.3f}'
          .format(top1=top1))
    writer.add_scalar('Validation/Loss', losses.avg, epoch)
    writer.add_scalar('Validation/Accuracy', top1.avg, epoch)

    return top1.avg",0.5598962307,1
2178,test the model for accuracy,"def full_eval(network, baseline_mean, baseline_std):
    # Check performance at the point in training with the best dev performance
    network.restore(network.best_dev_tup)
    print 'At best Dev performance...'
    print '\nTrain accuracy (appoximate): '
    network.check_accuracy(data_dict['train'], 2000);
    print '\nDev accuracy: '
    network.check_accuracy(data_dict['dev']);
    print '\nTest accuracy: '
    network.check_accuracy(data_dict['test']);
    
    # Check the performance at the termination of training
    network.restore(network.end_of_train_tup)
    print '\n\nAt end of training...'
    print '\nTrain accuracy (appoximate): '
    network.check_accuracy(data_dict['train'], 2000);
    print '\nDev accuracy: '
    network.check_accuracy(data_dict['dev']);
    print '\nTest accuracy: '
    a = 100.*network.check_accuracy(data_dict['test']);

    # Compare to the goal
    print '\n\nProbability this score beats Sheng Tai baseline:'
    print '{}%'.format(
        100.*np.round(1000.*np.mean(np.random.normal(baseline_mean, baseline_std, 10000) < a))/1000.)",0.5587562919,1
2178,test the model for accuracy,"def validate(val_loader, model, criterion, epoch):
    batch_time = AverageMeter()
    losses = AverageMeter()
    acc = AverageMeter()

    # switch to evaluate mode
    model.eval()

    end = time.time()
    for i, (images, labels) in enumerate(val_loader):
        labels = labels.cuda(async=True)
        image_var = torch.autograd.Variable(images, volatile=True)
        label_var = torch.autograd.Variable(labels, volatile=True)

        # compute y_pred
        y_pred = model(image_var)
        loss = criterion(y_pred, label_var)

        # measure accuracy and record loss
        prec1, temp_var = accuracy(y_pred.data, labels, topk=(1, 1))
        losses.update(loss.data[0], images.size(0))
        acc.update(prec1[0], images.size(0))

        # measure elapsed time
        batch_time.update(time.time() - end)
        end = time.time()

    print('   * EPOCH {epoch} | Accuracy: {acc.avg:.3f} | Loss: {losses.avg:.3f}'.format(epoch=epoch,
                                                                                         acc=acc,
                                                                                         losses=losses))

    return acc.avg",0.5532014966,1
2178,test the model for accuracy,"def test(epoch):
    global best_acc
    net.eval()
    # Declaring the variables
    test_loss = 0
    correct = 0
    total = 0
    # Looping over the test data
    for batch_idx, (inputs, targets) in enumerate(testloader):
        # Checking for GPU instance
        if use_cuda:
            inputs, targets = inputs.cuda(), targets.cuda()
        ## Coverting inputs and targets intp pytorch variables
        inputs, targets = Variable(inputs, volatile=True), Variable(targets)
        # Forward pass
        outputs = net(inputs)
        # Storing the size of outputs
        size_ = outputs.size()
        # Reducing the dimenssion
        outputs_ = outputs.view(size_[0], num_classes)
        # Calculating the loss
        loss = criterion(outputs_, targets)
        # Calculating the test loss
        test_loss += loss.data[0]
        # Predicted values
        _, predicted = torch.max(outputs_.data, 1)
        # Storing the size of targets
        total += targets.size(0)
        # Calculating the correct values
        correct += predicted.eq(targets.data).cpu().sum()
        # Printing the data
        if batch_idx%30 == 0 or batch_idx == len(testloader)-1:
            # printing the progress bar
            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'
                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))
    # Printing the validation loss 
    print('val_loss: ',  test_loss/len(testloader), 'accuracy: ', 100.0*correct/total)
    # Storing number of epoch,loss and acuracy in a file
    val_loss_file.write('%d %.3f %.3f\n' %(epoch,  test_loss/len(testloader), 100.*correct/total))

    # Save checkpoint.
    acc = 100.*correct/total
    # Checking for best accuracy
    if acc > best_acc:
        print('Saving..')
        state = {
            'net': net,
            'acc': acc,
            'epoch': epoch,
        }
        # Checking for the directory
        if not os.path.isdir('../checkpoint'):
            # creating the directory
            os.mkdir('../checkpoint')
        # saving the data
        torch.save(state, '../checkpoint_ckpt.t7')
        # Storing the accuracy value
        best_acc = acc",0.5525250435,1
2178,test the model for accuracy,"def test(epoch):
    global best_acc
    net.eval()
    # Declaring the values
    test_loss = 0
    correct = 0
    total = 0
    # Looping over the test data
    for batch_idx, (inputs, targets) in enumerate(testloader):
        # Checking for GPU instance
        if use_cuda:
            inputs, targets = inputs.cuda(), targets.cuda()
        # Converting inputs and targets into pytorch variables 
        inputs, targets = Variable(inputs, volatile=True), Variable(targets)
        # Forward Pass
        outputs = net(inputs)
        # Storing the outputs size
        size_ = outputs.size()
        # Reducing the dimenssion
        outputs_ = outputs.view(size_[0], num_classes)
        # Calculating the loss
        loss = criterion(outputs_, targets)
        # Storing the sum of loss 
        test_loss += loss.data[0]
        # Storing the predicted values
        _, predicted = torch.max(outputs_.data, 1)
        # Storing the targets size
        total += targets.size(0)
        # Calcualting the correct values
        correct += predicted.eq(targets.data).cpu().sum()
        # Printing the data
        if batch_idx%30 == 0 or batch_idx == len(testloader)-1:
            # Printing the progress bar
            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'
                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))
    # Printing the validation loss
    print('val_loss: ',  test_loss/len(testloader), 'accuracy: ', 100.0*correct/total)
    # Storing epoch,loss and accuracy into a file
    val_loss_file.write('%d %.3f %.3f\n' %(epoch,  test_loss/len(testloader), 100.*correct/total))

    # Save checkpoint.
    acc = 100.*correct/total
    # Checking for best accuracy
    if acc > best_acc:
        print('Saving..')
        state = {
            'net': net,
            'acc': acc,
            'epoch': epoch,
        }
        # Checking whether its a directory or not
        if not os.path.isdir('../checkpoint'):
            # Creating a directory
            os.mkdir('../checkpoint')
        # Saving the data
        torch.save(state, '../checkpoint_ckpt.t7')
        # Storing the accuracy
        best_acc = acc",0.5503125191,1
2178,test the model for accuracy,"def train(epoch, loader):
    loader = ellipse_loader
    model.train()
    train_loss = 0
    for batch_idx, (data, _) in enumerate(loader):
        #just need the data, don't need the dummy label for VAE :)
        data = Variable(data)
        optimizer.zero_grad()
        (mu_x, logvar_x), mu_z, logvar_z = model(data)
        loss = loss_function_Gaussian(mu_x, logvar_x, data, mu_z, logvar_z) #minimize the - loss = maximize lower bound
        loss.backward()
        train_loss += loss.data[0]
        optimizer.step()
        if batch_idx % log_interval == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(loader.dataset),
                100. * batch_idx / len(loader),
                loss.data[0] / len(data)))
            

#     print('====> Epoch: {} Average loss: {:.4f}'.format(
#           epoch, train_loss / len(loader.dataset)))
    return (-train_loss/len(loader.dataset))  #record average loss at current iteration",0.5434725285,0
2178,test the model for accuracy,"def objective_function(theta):
    program = ansatz(theta[0])
    bitstrings = qc.run_and_measure(program, trials=1000)
    result = np.mean(bitstrings[0])
    return -result

import scipy.optimize
res = scipy.optimize.minimize(objective_function, x0=[0.1], method='COBYLA')
res",0.5430921316,0
2178,test the model for accuracy,"def test(epoch):
    model.eval()
    test_loss = 0
    correct = 0
    total = 0    
    with torch.no_grad():
        for batch_idx, (images, labels) in enumerate(test_loader):
            outputs = model(images.view(-1, seq_dim, num_features))
            test_loss += loss_function(outputs, labels).item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum()
        accuracy = 100 * correct / total

    test_loss /= len(test_loader.dataset)
    print('====> Test set loss: {:.4f}. Accuracy: {}'.format(test_loss, accuracy))",0.5428224206,1
2178,test the model for accuracy,"def check_accuracy_on_test(loader): 
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in loader:
            images, labels = Variable(images.to(device)), Variable(labels.to(device))
            outputs = model.forward(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))
    
check_accuracy_on_test(loaders['test'])",0.539234519,1
1233,multi step jobs,"def knn_grid(params):
    start = time()
    clf = KNeighborsClassifier(n_jobs=-1)
    knn_results_df = pd.DataFrame(dtype = 'float64')
    count = 0
    for k, v in params.items():
        for val in v:
            clf.set_params(**{k: val})
            knn_results_df.loc[count, k] = val
            clf.fit(x_training, y_training)
            knn_results_df.loc[count, 'accuracy'] = accuracy_score(y_validation, clf.predict(x_validation))
            count += 1
    print(""Grid traversal took %.2f seconds for %d candidates."" % ((time() - start), count))            
    return knn_results_df",0.5024704933,0
1233,multi step jobs,"# Function to Perform a Splunk search
def execute_query():
    
    # Execute Search
    job = service.jobs.create(splunk_query, **kwargs)

    # A normal search returns the job's SID right away, so we need to poll for completion
    while True:
        while not job.is_ready():
            pass
        stats = {""isDone"": job[""isDone""],
                 ""doneProgress"": float(job[""doneProgress""])*100,
                 ""scanCount"": int(job[""scanCount""]),
                 ""eventCount"": int(job[""eventCount""]), 
                 ""resultCount"": int(job[""resultCount""])}
        
        status = (""\r%(doneProgress)03.1f%%   %(scanCount)d scanned   "" 
                  ""%(eventCount)d matched   %(resultCount)d results"") % stats

        sys.stdout.write(status)
        sys.stdout.flush()
        if stats[""isDone""] == ""1"":
            print ""\nSearch duration: "", job[""runDuration""], ""seconds""
            sys.stdout.write(""\n***DONE***"")
            break
        time.sleep(0.5)

    # Get the results and display them
    csv_results = job.results(**kwargs_options).read()
    job.cancel()
    return csv_results",0.4964971542,
1233,multi step jobs,"def objective(params):
    params = {'n_estimators': int(params['n_estimators']), 'max_depth': int(params['max_depth'])}
    clf = RandomForestClassifier(n_jobs=4, class_weight='balanced', **params)
    score = cross_val_score(clf, X, Y, scoring=gini_scorer, cv=StratifiedKFold()).mean()
    print(""Gini {:.3f} params {}"".format(score, params))
    return score

space = {
    'n_estimators': hp.quniform('n_estimators', 25, 500, 25),
    'max_depth': hp.quniform('max_depth', 1, 10, 1)
}

best = fmin(fn=objective,
            space=space,
            algo=tpe.suggest,
            max_evals=10)",0.4946681857,0
1233,multi step jobs,"def LeNet_Model():
    # creating a sequential model
    model = Sequential()
    
    # adding first set of CONV -> RELU -> POOL
    model.add(Convolution2D(20, 5, 5, border_mode=""same"",input_shape=(60, 60,3))) # 3,60,60 tha pehle
    model.add(Dropout(0.2))
    model.add(Activation(""relu""))
    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))

    # adding second set of CONV -> RELU -> POOL
    model.add(Convolution2D(50, 5, 5, border_mode=""same""))
    model.add(Dropout(0.2))
    model.add(Activation(""relu""))
    model.add(MaxPooling2D(pool_size=(2,2),strides=(2, 2),dim_ordering='th')) # specifying the order of dimensions

    # adding set of FC -> RELU layers
    model.add(Flatten())
    model.add(Dense(500))
    model.add(Dropout(0.2))
    model.add(Activation(""relu""))
 
    # adding softmax classifier
    model.add(Dense(7))          # 7 categories
    model.add(Dropout(0.2))
    model.add(Activation(""softmax""))
        
    return model",0.4853518605,0
1233,multi step jobs,"def splunkQuery(aQuery):
    try:
        jobs = session.jobs
        job = jobs.create(aQuery)
        aResult = []
        while len(aResult) < 1:
            for oDict in results.ResultsReader(job.results()):
                aResult.append({ ""_indextime"": oDict['_indextime'], ""_raw"": oDict['_raw']})
        return aResult
    except:
        return (""Query failed. Validate connection, session and syntax."")",0.4817406237,1
1233,multi step jobs,"def mlp_model():
    model = Sequential()
    
    model.add(Dense(50, input_shape = (784, )))
    model.add(Activation('sigmoid'))    
    model.add(Dense(50))
    model.add(Activation('sigmoid'))  
    model.add(Dense(50))
    model.add(Activation('sigmoid'))    
    model.add(Dense(50))
    model.add(Activation('sigmoid'))    
    model.add(Dense(10))
    model.add(Activation('softmax'))
    
    adam = optimizers.Adam(lr = 0.001)                     # use Adam optimizer
    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])
    
    return model",0.4794971347,0
1233,multi step jobs,"# My CNN model 2
def get_model_2():
    row, col, ch = 66, 200, 3  # camera format

    model = Sequential()
    model.add(Lambda(lambda x: x/127.5 - 1.,
            input_shape=(row, col, ch),
            output_shape=(row, col, ch)))
    model.add(Convolution2D(24, 5, 5, subsample=(2, 2), border_mode=""valid""))
    model.add(Activation('relu'))
    model.add(Convolution2D(36, 5, 5, subsample=(2, 2), border_mode=""valid""))
    model.add(Activation('relu'))
    model.add(Convolution2D(48, 5, 5, subsample=(2, 2), border_mode=""valid""))
    model.add(Activation('relu'))
    model.add(Convolution2D(64, 3, 3, border_mode=""valid""))
    model.add(Activation('relu'))
    model.add(Convolution2D(64, 3, 3, border_mode=""valid""))
    model.add(Flatten())
    model.add(Dropout(.2))
    model.add(ELU())
    model.add(Dense(100))
    model.add(Activation('relu'))
    model.add(Dense(50))
    model.add(Activation('relu'))
    model.add(Dense(10))
    model.add(Dropout(.5))
    model.add(Activation('relu'))
    model.add(Dense(1))

    model.compile(optimizer=""adam"", loss=""mse"")

    return model",0.4782434702,0
1233,multi step jobs,"# My ConvNet model 0
def get_model_0():
    row, col, ch = 66, 200, 3  # camera format

    model = Sequential()
    model.add(Lambda(lambda x: x/127.5 - 1.,
            input_shape=(row, col, ch),
            output_shape=(row, col, ch)))
    model.add(Convolution2D(24, 5, 5, subsample=(2, 2), border_mode=""valid""))
    model.add(Activation('relu'))
    model.add(Convolution2D(36, 5, 5, subsample=(2, 2), border_mode=""valid""))
    model.add(Activation('relu'))
    model.add(Convolution2D(48, 5, 5, subsample=(2, 2), border_mode=""valid""))
    model.add(Activation('relu'))
    model.add(Convolution2D(64, 3, 3, border_mode=""valid""))
    model.add(Activation('relu'))
    model.add(Convolution2D(64, 3, 3, border_mode=""valid""))
    model.add(Flatten())
    model.add(Dropout(.2))
    model.add(ELU())
    model.add(Dense(256))
    model.add(Activation('relu'))
    model.add(Dense(64))
    model.add(Activation('relu'))
    model.add(Dense(32))
    model.add(Dropout(.5))
    model.add(Activation('relu'))
    model.add(Dense(1))

    model.compile(optimizer=""adam"", loss=""mse"")

    return model",0.4782434702,0
1233,multi step jobs,"# My CNN model 1
def get_model_1():
    row, col, ch = 66, 200, 3

    model = Sequential()
    # normalize input image.
    model.add(Lambda(lambda x: x/127.5 - 1.,
                     input_shape=(row, col, ch), output_shape=(row, col, ch)))

    # convolution layers
    model.add(Convolution2D(24, 5, 5, subsample=(2, 2), border_mode=""valid""))
    model.add(Activation('relu'))
    model.add(Convolution2D(36, 5, 5, subsample=(2, 2), border_mode=""valid""))
    model.add(Activation('relu'))
    model.add(Convolution2D(48, 5, 5, subsample=(2, 2), border_mode=""valid""))
    model.add(Activation('relu'))
    model.add(Convolution2D(64, 3, 3, border_mode=""valid""))
    model.add(Activation('relu'))
    model.add(Convolution2D(64, 3, 3, border_mode=""valid""))

    # flatten CNN layer for fc layers
    model.add(Flatten())
    # dropout for regularization usage
    model.add(Dropout(.2))
    model.add(ELU())

    # fc layers for regression
    model.add(Dense(100))
    model.add(Activation('relu'))
    model.add(Dense(50))
    model.add(Activation('relu'))
    model.add(Dense(10))
    model.add(Dropout(.5))
    model.add(Activation('relu'))
    model.add(Dense(1))

    model.compile(optimizer=""adam"", loss=""mse"")

    return model",0.4782434702,0
1233,multi step jobs,"for i in range(20):
    job_cfg_2 = { 'descr' : str(i),  'estimated_time':120,#a description for the example job 
    }
    job = xp_man.job.ExampleJob(**job_cfg_2)
    jq.add_job(job)
print(jq.get_status_string())",0.4769067168,1
729,generating a linearly separable dataset,"plt.pcolormesh(grid.Y/1e3,grid.Z,np.squeeze(np.mean(BuoyancyFlux[120:-1,:,:],axis=0)),vmin=-0.05,vmax=0.05,cmap='RdBu_r')
CenterPlot(15)
plt.title('Average Buoyancy Flux Density')
plt.colorbar(label=r'$\rho'' g w$ [W m$^{-3}$]')
plt.plot(grid.Y/1e3,-grid.Depth,'k');",0.5133676529,0
729,generating a linearly separable dataset,"from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC

X, y = mglearn.datasets.make_forge()

fig, axes = plt.subplots(1, 2, figsize=(12, 5))

for model, ax in zip([LogisticRegression(), LinearSVC()], axes):
    clf = model.fit(X, y)
    
    mglearn.plots.plot_2d_separator(clf, X, fill=False, eps=0.5, ax=ax, alpha=0.7)
    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)
    
    ax.set_xlabel(""Feature 0"")
    ax.set_ylabel(""Feature 1"")
    ax.legend()
    ax.set_title(""{}"".format(clf.__class__.__name__))",0.5058630705,1
729,generating a linearly separable dataset,"from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC

X, y = mglearn.datasets.make_forge()

fig, axes = plt.subplots(1, 2, figsize=(10, 3))

for model, ax in zip([LinearSVC(), LogisticRegression()], axes):
    clf = model.fit(X, y)
    mglearn.plots.plot_2d_separator(clf, X, fill=False, eps=0.5,
                                    ax=ax, alpha=.7)
    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)
    ax.set_title(""{}"".format(clf.__class__.__name__))
    ax.set_xlabel(""Feature 0"")
    ax.set_ylabel(""Feature 1"")
axes[0].legend()",0.5050925612,1
729,generating a linearly separable dataset,"from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC

X, y = mglearn.datasets.make_forge()

fig, axes = plt.subplots(1, 2, figsize=(10, 3))

for model, ax in zip([LinearSVC(), LogisticRegression()], axes):
    clf = model.fit(X, y)
    mglearn.plots.plot_2d_separator(clf, X, fill=False, eps=0.5,
                                    ax=ax, alpha=.7)
    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)
    ax.set_title(""{}"".format(clf.__class__.__name__))
    ax.set_xlabel(""Feature 0"")
    ax.set_ylabel(""Feature 1"")
axes[0].legend();",0.5050925612,1
729,generating a linearly separable dataset,"from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC

X, y = mglearn.datasets.make_forge()

fig, axes = plt.subplots(1, 2, figsize=(10,3))

for model, ax in zip([LinearSVC(), LogisticRegression()], axes):
    clf = model.fit(X, y)
    mglearn.plots.plot_2d_separator(clf, X, fill=False, eps=0.5, ax=ax, alpha=0.7)
    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)
    ax.set_title('{}'.format(clf.__class__.__name__))
    ax.set_xlabel('Feature 0')
    ax.set_ylabel('Feature 1')
    
axes[0].legend();",0.5050925612,1
729,generating a linearly separable dataset,"s = plt.pcolormesh(rs/1000.,
                   np.hstack((alts-250.,np.max(alts)+250.))/1000.,
                   radial_ave[:,2,:].T/1000.)
plt.plot(rs/1000.,projl.fromECEF(rs_ecef[0], rs_ecef[1], rs_ecef[2])[2]/1000., color='0.8')
plt.clim(vmin=-0.25,vmax=2.5)
plt.ylim(0.25,20)
plt.xlabel('Range')
plt.ylabel('Altitude')
plt.colorbar(label='Average Altitude Error (km)')
plt.colorbar()
plt.tight_layout()
plt.show()",0.5029155016,0
729,generating a linearly separable dataset,"decorrResults = [doDecorr(sciImg, templImg, im1, spatialKernel, xcen=xy[0], ycen=xy[1], svar=v2, tvar=v1) for \
    #xy in [(None, None)] for v1 in (58., None) for v2 in (64.5, None)]
    xy in [(1023,2047), (0,4094), (2046,0), (None, None)] for v1 in (60., 58.9, 61.3, None) for v2 in (62.7, 61.5, 64.1, None)]",0.4980814457,
729,generating a linearly separable dataset,"# plot comparison
xx, yy = np.meshgrid(
    np.linspace(X[:, 0].min(), X[:, 0].max(), 1200).reshape(-1, 1),
    np.linspace(X[:, 1].min(), X[:, 1].max(), 1000).reshape(-1, 1),
)
X_new = np.c_[xx.ravel(), yy.ravel()]
fig, axes = plt.subplots(2, 2, figsize=(15, 10))
param_grid = [[[0.1, 0.001], [0.1, 1000]], [[5, 0.001], [5, 1000]]]
for i in range(2):
    for j in range(2):
        rbf_clf = Pipeline([
            (""scaler"", StandardScaler()),
            (""svm_clf"", SVC(kernel=""rbf"", gamma=param_grid[i][j][0], C=param_grid[i][j][1]))
        ])
        rbf_clf.fit(X, y)
        y_predict = rbf_clf.predict(X_new)
        axes[i, j].contourf(xx, yy, y_predict.reshape(xx.shape), cmap=custom_cmap)
        axes[i, j].plot(X[rbf_clf.predict(X)==0, 0], X[rbf_clf.predict(X)==0, 1], ""bs"")
        axes[i, j].plot(X[rbf_clf.predict(X)==1, 0], X[rbf_clf.predict(X)==1, 1], ""r^"")
        axes[i, j].set_xlabel(""$x_1$"")
        axes[i, j].set_ylabel(""$x_2$"")
        axes[i, j].set_title(""$\gamma$=""+str(param_grid[i][j][0])+"",C=""+str(param_grid[i][j][1]))
plt.show()",0.4973734021,1
729,generating a linearly separable dataset,"# plot a comparison
xx, yy = np.meshgrid(
    np.linspace(X[:, 0].min(), X[:, 0].max(), 1200).reshape(-1, 1),
    np.linspace(X[:, 1].min(), X[:, 1].max(), 1000).reshape(-1, 1),
)
X_plot = np.c_[xx.ravel(), yy.ravel()]
poly1 = Pipeline([
    (""scaler"", StandardScaler()),
    (""svm_clf"", SVC(kernel=""poly"", degree=3, coef0=1, C=5))
])
poly2 = Pipeline([
    (""scaler"", StandardScaler()),
    (""svm_clf"", SVC(kernel=""poly"", degree=10, coef0=1, C=5))
])
poly1.fit(X, y)
poly2.fit(X, y)
y_predict1 = poly1.predict(X_plot)
y_predict2 = poly2.predict(X_plot)
fig, axes = plt.subplots(1, 2, figsize=(12, 5))
axes[0].contourf(xx, yy, y_predict1.reshape(xx.shape), cmap=custom_cmap)
axes[1].contourf(xx, yy, y_predict2.reshape(xx.shape), cmap=custom_cmap)
axes[0].plot(X[poly1.predict(X)==0, 0], X[poly1.predict(X)==0, 1], ""bs"")
axes[0].plot(X[poly1.predict(X)==1, 0], X[poly1.predict(X)==1, 1], ""r^"")
axes[1].plot(X[poly2.predict(X)==0, 0], X[poly2.predict(X)==0, 1], ""bs"")
axes[1].plot(X[poly2.predict(X)==1, 0], X[poly2.predict(X)==1, 1], ""r^"")
axes[0].set_xlabel(""$x_1$"")
axes[1].set_xlabel(""$x_1$"")
axes[0].set_ylabel(""$x_2$"")
axes[1].set_ylabel(""$x_2$"")
axes[0].set_title(""d=3,r=1,C=5"")
axes[1].set_title(""d=10,r=1,C=5"")
plt.show()",0.4972449541,1
729,generating a linearly separable dataset,"# We are going to pretend we don't know h and make a search for h values by settting up 
# a range of potential values for h1 and h2

h1, h2 = np.meshgrid(np.arange(-1, 10, .2), np.arange(-1, 10, .2))
hs = np.vstack([h1.ravel(), h2.ravel()])

# get responses from each set of weights
ys = np.dot(x.T, hs)

# calculate error between the response, y, and each of the possible responses, ys.  
err = np.sum((y - ys) ** 2, 0)

# reshape for plotting
err = err.reshape(h1.shape)",0.4958877265,
723,gaussian kernels,"def get_gaussian_filter2d(kernel_size, sigma=1):
    gaussian_filter = cv2.getGaussianKernel(kernel_size, sigma).flatten()
    
    gaussian_filter = gaussian_filter.reshape((len(gaussian_filter), 1)) * gaussian_filter
    
    gaussian_filter /= np.sum(gaussian_filter)
    return gaussian_filter",0.5919042826,1
723,gaussian kernels,"def func_kernel2(im,sigma=-1):
    
    #kernel 1D
    gaus1D_1 = cv2.getGaussianKernel(im.shape[0],sigma)
    gaus1D_2 = cv2.getGaussianKernel(im.shape[1],sigma)
    
    #kernel 2D
    gaus2D = np.dot(gaus1D_1,gaus1D_2.T)

    return np.sum(im*gaus2D)",0.5853060484,1
723,gaussian kernels,"def getDensity(df):
    data = df#_log_prob['log.prior']
    density = scipy.stats.gaussian_kde(data)
    width = np.max(data) - np.min(data)
    xs = np.linspace(np.min(data)-width/5, np.max(data)+width/5,600)
    density.covariance_factor = lambda : .25
    density._compute_covariance()
    return xs, density(xs)",0.5833412409,1
723,gaussian kernels,"def get_probarr(elec):
    kernel = stats.gaussian_kde(elec)
    return np.reshape(kernel(elec).T, elec.shape)",0.5823360085,1
723,gaussian kernels,"def kde_plot(x):
        from scipy.stats.kde import gaussian_kde
        kde = gaussian_kde(x)
        positions = np.linspace(x.min(), x.max())
        smoothed = kde(positions)
        plt.plot(positions, smoothed)

def qq_plot(x):
    from scipy.stats import probplot
    probplot(x, dist='norm', plot=plt)",0.5700438023,1
723,gaussian kernels,"# Numpy implementation
def compute_pi_v2(n):
    series = 4.0*np.arange(1,n)**2
    series /= (series-1)
    return 2.0*series.prod()",0.5661482215,0
723,gaussian kernels,"from scipy.stats.kde import gaussian_kde
def kde_plot(x):   
    kde = gaussian_kde(x)
    positions = np.linspace(x.min(), x.max())
    smoothed = kde(positions)
    plt.plot(positions, smoothed)
    
def kde_values(x):   
    kde = gaussian_kde(x)
    positions = np.linspace(x.min(), x.max())
    smoothed = kde(positions)
    return positions, smoothed

x=x.dropna()
kde_plot(x)
plt.title(""Distribution of mean prices"")
plt.show()",0.5661156178,1
723,gaussian kernels,"def kde(x):
    # Kernel density estimation, to get P(dI/dx | on edge) and P(dI/dx | off edge) from data
    from scipy.stats import gaussian_kde
    f = gaussian_kde(x, bw_method=0.01 / x.std(ddof=1))
    return f
    
def ponEdge(im, edgeMap):
    # Compute on edge histogram
    # im is filtered image
    
    # Convert edge map to pixel index
    flattenEdgeMap = edgeMap.flatten()
    edgeIdx = [i for i in range(len(flattenEdgeMap)) if flattenEdgeMap[i]]
    
    # find edge pixel in 3x3 region, shift the edge map a bit, in case of inaccurate boundary labeling
    [offx, offy] = np.meshgrid(np.arange(-1,2), np.arange(-1,2)); offx = offx.flatten(); offy = offy.flatten()
    maxVal = np.copy(im)
    for i in range(9):
        im1 = np.roll(im, offx[i], axis=1) # x axis
        im1 = np.roll(im1, offy[i], axis=0) # y axis    
        maxVal = np.maximum(maxVal, im1)

    vals = maxVal.flatten()
    onEdgeVals = vals[edgeIdx]
    
    bins = np.linspace(0,0.5, 100)
    [n, bins] = np.histogram(onEdgeVals, bins=bins)
    # n = n+1 # Avoid divide by zero

    pon = kde(onEdgeVals)

    return [n, bins, pon]


def poffEdge(im, edgeMap):
    flattenEdgeMap = edgeMap.flatten()
    noneEdgeIdx = [i for i in range(len(flattenEdgeMap)) if not flattenEdgeMap[i]]
    
    vals = im.flatten()
    offEdgeVals = vals[noneEdgeIdx] 

    bins = np.linspace(0,0.5, 100)
    n, bins = np.histogram(offEdgeVals, bins=bins)

    # n = n+1
    # p = n / sum(n)
    
    poff = kde(offEdgeVals)
    
    return [n, bins, poff]

dx = dIdx(im)
[n1, bins, pon] = ponEdge(dx, edgeMap)
[n2, bins, poff] = poffEdge(dx, edgeMap)

plt.figure(); # Plot on edge
# title('(Normalized) Histogram of on/off edge pixels')
plt.plot((bins[:-1] + bins[1:])/2, n1.astype(float)/sum(n1), '-', lw=2, label=""p(f|y=1)"")
plt.plot((bins[:-1] + bins[1:])/2, n2.astype(float)/sum(n2), '--', lw=2, label=""p(f|y=-1)"")
plt.legend()

plt.figure()
# title('Density function of on/off edge pixels')
plt.plot(bins, pon(bins), '-', alpha=0.5, lw=3, label=""p(f|y=1)"")
plt.plot(bins, poff(bins), '--', alpha=0.5, lw=3, label=""p(f|y=-1)"")
plt.legend()",0.5638000965,1
723,gaussian kernels,"def atg(x, n):
    return sum([1 * x] + [((-1.)**(i)) * ((1. * (x**((i*2) + 1)))/((i*2.) + 1.)) for i in xrange(1, n)])",0.5579457879,
723,gaussian kernels,"from scipy.ndimage.filters import gaussian_filter
def P6():
    
### STUDENT START ###
    blurred_train = gaussian_filter(train_data, sigma=0.25)
    blurred_dev = gaussian_filter(dev_data, sigma=0.25)

    placeholder = P5(
        blurred_train, train_labels, dev_data, dev_labels, ip_suppress_cm=""y"")
    placeholder = P5(
        train_data, train_labels, blurred_dev, dev_labels, ip_suppress_cm=""y"")
    placeholder = P5(
        blurred_train, train_labels, blurred_dev, dev_labels, ip_suppress_cm=""y"")
### STUDENT END ###

P6()",0.5573644638,1
2611,working with geometric objects,"for c in cubelist:
    c.coord('grid_longitude').guess_bounds()
    c.coord('grid_latitude').guess_bounds()",0.4609850049,1
2611,working with geometric objects,"phi.data = np.sin(mesh.coords[:,0])**2.0 
psi.data = np.cos(mesh.coords[:,0])**2.0 * np.sin(mesh.coords[:,1])**2.0",0.4607131183,1
2611,working with geometric objects,"# 2D Points have a few use methods and attributes including: coordinates, geom_type, distance
point1.x, point1.y, point1.coords.xy",0.4487324953,1
2611,working with geometric objects,"fig = bk.figure(title='Automobile mileage by year and country')

fig.quad(left=years-0.4, right=years+0.4, bottom=mpg_avg-mpg_std, top=mpg_avg+mpg_std, fill_alpha=0.4)",0.447322011,0
2611,working with geometric objects,"%%sql
explain analyze 
select * 
from cube where ST_3DDWithin(obj_geom, ST_MakePoint(25000, 25000, 25000),1000)
and obj_geom &&& ST_MakeLine(ST_MakePoint(24000, 24000, 24000), ST_MakePoint(26000, 26000, 26000));",0.4470897317,1
2611,working with geometric objects,Polygon(poly_2.exterior.coords).area,0.4455289841,1
2611,working with geometric objects,"# List the exterior coordinates of the annotation
# Expressed in latitude and longitude point pairs
list(p.exterior.coords)",0.4447691441,1
2611,working with geometric objects,"#since they are spatial objects, let's use their features
# e.g. coordinates
print domm.coords.xy
print ""Milan's Duomo is located at lat {} lon {}"".format(domm.x, domm.y)",0.4444707632,1
2611,working with geometric objects,"#since these features are plain python objects, they come with properties and methods
# e.g. coordinates
print domm.coords.xy
print ""Milan's Duomo is located at lon {} lat {}"".format(domm.x, domm.y)",0.4444707632,1
2611,working with geometric objects,"#primalDomain
#fineDomain

#primalBasis
#optiBasis

# construct residual
# Element integrals
# Diffusive part and RHS
res = fineDomain.integral('k primalBasis_n,0 u_,0 - primalBasis_n f' @ ns, geometry=ns.x, degree=fineDegree*3)
## Advective part
res += fineDomain.integral('-vel primalBasis_n,0 u' @ ns, geometry=ns.x, degree=fineDegree*3)
## SUPG stabilized part
#res += domain.integral(' tau v basis_n,0 (v u_,0 - k (u_,0)_,0 - f )' @ ns, geometry=ns.x, degree=5)

## Interface integrals
## Diffusive part
##    part one                      |
##                                  v  minus might be necessary here because of jump definitions (likewise on second +)
#res += domain.interfaces.integral(' [basis_n] n_0 k {u_,0} + beta k {basis_n,0} [u] n_0' @ ns, geometry=ns.x, degree=degree*2)
##    part two (stabilized)
#res += domain.interfaces.integral(' tauA [basis_n] n_0 [u] n_0' @ ns, geometry=ns.x, degree=degree*2)
## Advective part
#res += domain.interfaces.integral('- v [basis_n] n_0 ({u} - .5 C [u] n_0)' @ ns, geometry=ns.x, degree=5)

## Boundary integrals LHS
##    part one
#res += domain.boundary.integral('- k basis_n u_,0 + beta k basis_n,0 u ' @ ns, geometry=ns.x, degree=degree*2)
##    part two (stabilized)
#res += domain.boundary['left'].integral(' tauZero basis_n u ' @ ns, geometry=ns.x, degree=degree*2)
#res += domain.boundary['right'].integral(' tauN basis_n u ' @ ns, geometry=ns.x, degree=degree*2)

## Boundary integrals RHS (weakly imposed boundary conditions) (including stabilization)
## Diffusive part
#res -= domain.boundary['left'].integral(' ( beta k basis_n,0 + tauZero basis_n) gl ' @ ns, geometry=ns.x, degree=degree*2)
#res -= domain.boundary['right'].integral(' ( beta k basis_n,0 + tauN basis_n) gr ' @ ns, geometry=ns.x, degree=degree*2)
## Advective part
#res += domain.boundary['left'].integral('v basis_n n_0 (0.5 (gl + u) - .5 C (gl - u) n_0)' @ ns, geometry=ns.x, degree=degree*2)
#res += domain.boundary['right'].integral('v basis_n n_0 (0.5 (gr + u) - .5 C (gr - u) n_0)' @ ns, geometry=ns.x, degree=degree*2)

sqr = fineDomain.boundary['left'].integral('(u - gl)^2' @ ns, geometry=ns.x, degree=fineDegree*3)
sqr += fineDomain.boundary['right'].integral('(u - gr)^2' @ ns, geometry=ns.x, degree=fineDegree*3)
cons = solver.optimize('lhs', sqr, droptol=1e-15)",0.4441106319,1
69,array datatypes,"ptr.__array_interface__ = {
    ""version"": 3,
    ""typestr"": numpy.ctypeslib._dtype(type(ptr.contents)).str,
    ""data"": (ctypes.addressof(ptr.contents), False),
    ""shape"": (4096,)
}
array = numpy.array(ptr, copy=False)
array",0.5112776756,1
69,array datatypes,"class matrix_t(ct.Structure) :
    _fields_ = \
        [
            (""xx"", ct.c_double),
            (""yx"", ct.c_double),
            (""xy"", ct.c_double),
            (""yy"", ct.c_double),
            (""x0"", ct.c_double),
            (""y0"", ct.c_double),
        ]

    def __repr__(self) :
        return ""["" + "", "".join(repr(getattr(self, f[0])) for f in matrix_t._fields_) + ""]""
    #end __repr__
#end matrix_t",0.5055693984,0
69,array datatypes,"struct2 = np.dtype([('jd', np.double),  ('sig', np.float32), ('sig_noise', np.float32), 
                   ('dir', np.dtype('S1')),    ('pdb',np.ubyte), ('azcorr_flag', 
                                                                  np.dtype([('f', np.ubyte),  
                                                                            ('m',np.ubyte),  
                                                                            ('a', np.ubyte)]))])",0.5036939383,1
69,array datatypes,"import array
array.array('I', (ord(symbol) for symbol in symbols))",0.4934924245,1
69,array datatypes,"# Convert object Attributes to Category
for column in data:
    if data[column].dtype == object:
        data[column] = data[column].astype('category')",0.4914848804,0
69,array datatypes,"def df_rows_to_np_matrix(window):
    return np.array([unpack_row_vals(row) for row in window])",0.4914466739,1
69,array datatypes,"def preprocess(data):
    return numpy.array([parse(line) for line in data])",0.4914466739,0
69,array datatypes,"def shift_grid(grid):
    return np.array([shift_line(i) for i in grid])",0.4914466739,0
69,array datatypes,"import numpy as np

def Psat(species,T):
    if isinstance(T,np.ndarray):
        return [Psat(species,t) for t in T]
    for Tmin,Tmax in adb.ix[species.lower()].index:
        if (T >= Tmin) & (T <= Tmax):
            A = adb.ix[species.lower(),Tmin,Tmax]['A']
            B = adb.ix[species.lower(),Tmin,Tmax]['B']
            C = adb.ix[species.lower(),Tmin,Tmax]['C']
            return 10.0**(A - B/(T + C) )
    else:
        return float('nan')",0.4903326631,0
69,array datatypes,"field_dtype = np.dtype([('label', np.object),
                        ('ra', np.float64),
                          ('dec', np.float64),
                          ('nepochs', np.int32),
                          ('nexp', np.int32, 400),
                          ('lunation', np.float32, 400)])",0.4867107868,1
863,i regularization,"def neural_network(X):
  h = tf.tanh(tf.matmul(X, W_0) + b_0)
  h = tf.tanh(tf.matmul(h, W_1) + b_1)
  h = tf.matmul(h, W_2) + b_2
  return tf.reshape(h, [-1])",0.3858528137,0
863,i regularization,"def network(input_tensor):
    # Sigmoid fits modified data well
    layer1 = tf.nn.sigmoid(tf.matmul(input_tensor, weight_1_node) + biases_1_node)
    # Dropout prevents model from becoming lazy and over confident
    layer2 = tf.nn.dropout(tf.nn.sigmoid(tf.matmul(layer1, weight_2_node) + biases_2_node), 0.85)
    # Softmax works very well with one hot encoding which is how results are outputted
    layer3 = tf.nn.softmax(tf.matmul(layer2, weight_3_node) + biases_3_node)
    return layer3",0.3798757195,1
863,i regularization,"def forward_propagation(x):
    hlout_1 = tf.nn.sigmoid(tf.matmul(x, hlw_1) + hlb_1) # input multiplied by hidden layer 1 weights. Add bias. Apply sigmoid
    output = tf.nn.sigmoid(tf.matmul(hlout_1, outw) + outb) # hidden layer 2 output multiplied by output layer weights. add bias. DON'T add sigmoid
    return output",0.3796371222,0
863,i regularization,"def plot_lstsq_lad_regressions(x, y, sq_model, lad_model):
    
    sq_intercept = sq_model.params[0]
    sq_slope = sq_model.params[1]
    
    lad_intercept = lad_model.params[0]
    lad_slope = lad_model.params[1]
    
    plt.figure(figsize=(10,8))
    
    axes = plt.gca()
        
    plt.scatter(x, y, s=70, c='steelblue')
    
    x_points = np.linspace(axes.get_xlim()[0], axes.get_xlim()[1], 100)
    
    regline_x = x_points
    sq_regline_y = x_points*sq_slope + sq_intercept
    lad_regline_y = x_points*lad_slope + lad_intercept

    plt.plot(regline_x, sq_regline_y, c='red', lw=3)
    plt.plot(regline_x, lad_regline_y, c='green', lw=3)

    plt.show()",0.3771404624,0
863,i regularization,"def f_model(x1, x2, model):
    ''' Must feed in x1 = 'alcohol consumption'
        and x2 = 'tobacco consumption'. Returns the
        2D hyperplane (a regular plane) of best fit:
        f = m_0 + m_1 x_1 + m_2 x_2 '''
    f = model.params[0] + model.params[1]*x1 + model.params[2]*x2
    return np.float64(f)

def plot_plane(x_points, y_points, z_points, model,
               weights, labels, view):
    fig = plt.figure(figsize=(14,14))
    ax = fig.gca(projection='3d')
    
    pad = 0
    x1, x2 = min(x_points)-pad, max(x_points)+pad
    y1, y2 = min(y_points)-pad, max(y_points)+pad
    
    # set up a meshgrid - like labeling (x,y) coordinates
    # for each vertex on a piece of graph paper
    dx = 0.1
    x = np.arange(x1, x2, dx)
    y = np.arange(y1, y2, dx)
    X, Y = np.meshgrid(x, y)
    
    # define the vertical values Z
    # as the model plane of best fit,
    # Z ends up as a numpy array
    Z = f_model(X, Y, model)
    
    # plot
    ax.plot_wireframe(X, Y, Z, rstride=5, cstride=7,
                      color=colors[-2], alpha=0.7)
    ax.scatter(x_points, y_points, z_points,
               s=[w*3 for w in weights])
    
    ax.set_xlim(0.8, 4.2)
    ax.set_ylim(0.8, 4.2)
    ax.set_zlim(-0.02, 1.02)
    
    ax.set_xlabel(labels[0], labelpad=30, fontsize=20)
    ax.set_ylabel(labels[1], labelpad=30, fontsize=20)
    ax.set_zlabel(labels[2], labelpad=25, fontsize=20)
    
    ax.set_xticks([1, 2, 3, 4])
    ax.set_xticklabels(alcgp.values())
    ax.set_yticks([1, 2, 3, 4])
    ax.set_yticklabels(tobgp.values())
    
    ax.view_init(elev=view[0], azim=view[1])

    
plot_plane(df.alcgp, df.tobgp, df.positive_frac, reg, df.ncontrols,
           labels=['Alcohol consumption', 'Tobacco consumption', 'Positive diagnosis percentage'],
           view=[30, 100])
plt.savefig('figures/two_feature_model_3d_1.png', bbox_inches='tight', dpi=144)
plt.show()

plot_plane(df.alcgp, df.tobgp, df.positive_frac, reg, df.ncontrols,
           labels=['Alcohol consumption', 'Tobacco consumption', 'Positive diagnosis percentage'],
           view=[30, 145])
plt.gcf().subplots_adjust(right=1.08)
plt.savefig('figures/two_feature_model_3d_2.png', bbox_inches='tight', dpi=144)
plt.show()

plot_plane(df.alcgp, df.tobgp, df.positive_frac, reg, df.ncontrols,
           labels=['Alcohol consumption', 'Tobacco consumption', 'Positive diagnosis percentage'],
           view=[10, 185])
plt.savefig('figures/two_feature_model_3d_3.png', bbox_inches='tight', dpi=144)
plt.show()",0.3751192689,0
863,i regularization,"def sin_approximation(theta, a, b, c, d, e, f):
    return (a * np.power(theta, 2.0) + b * theta + c)/(d * np.power(theta, 2.0) + e * theta + f)

popt, pcov = curve_fit(sin_approximation, theta, sin_theta)
print(popt)",0.3749681711,0
863,i regularization,"def tan_approximation(theta, a, b, c, d, e, f):
    return (a * np.power(theta, 2.0) + b * theta + c)/(d * np.power(theta, 2.0) + e * theta + f)

popt, pcov = curve_fit(tan_approximation, theta, tan_theta)
print(popt)",0.3749681711,0
863,i regularization,"def arcsin_approximation(x, a, b, c, d, e, f):
    return (a * np.power(x, 2.0) + b * x + c)/(d * np.power(x, 2.0) + e * x + f)

popt, pcov = curve_fit(arcsin_approximation, x, arcsin_x)
print(popt)",0.3749681711,0
863,i regularization,"def cos_approximation(theta, a, b, c, d, e, f):
    return (a * np.power(theta, 2.0) + b * theta + c)/(d * np.power(theta, 2.0) + e * theta + f)

popt, pcov = curve_fit(cos_approximation, theta, cos_theta)
print(popt)",0.3749681711,0
863,i regularization,"def VDSR(h0):
    print('Generator input shape: ', h0.shape)
    
    with C.layers.default_options(init = C.he_normal(), activation = C.relu, bias = False):
        model = C.layers.Sequential([
            C.layers.For(range(18), lambda :
                C.layers.Convolution((3, 3), 64, pad = True)),
            C.layers.Convolution((3, 3), 3, activation = None, pad = True)
        ])
    
    return model(h0)",0.3748664856,0
1487,periodic signals & the lomb scargle periodogram,"def plot_signal_price(df_pass):
    plt.figure(figsize=(15, 5));
    plt.plot(df_pass.signal.values, color='blue', label='SIGNAL')
    plt.plot(df_pass.spy_close_price.values, color='red', label='SPY_CLOSE_PRICE')

    plt.title('Normalization plot of both Signal and Spy close price')
    plt.xlabel('time[data entries]')
    plt.ylabel('normalized value')
    plt.legend(loc='best')
    plt.show()",0.4867633879,
1487,periodic signals & the lomb scargle periodogram,"def blr(fee, wf):
    coef = fee.freq_LHPFd*np.pi
    signal_out_cf = FE.signal_clean(fee, wf.q, -1)
    signal_r, _ = deconv_simple(signal_out_cf, coef)
    return Waveform(wf.t, signal_r)",0.4860947728,
1487,periodic signals & the lomb scargle periodogram,"def construct_signal(self, spot_df, spot_df2, tech_params, br):

        ##### FILL IN WITH YOUR OWN SIGNALS

        # use technical indicator to create signals
        # (we could obviously create whatever function we wanted for generating the signal dataframe)
        tech_ind = TechIndicator()
        tech_ind.create_tech_ind(spot_df, 'SMA', tech_params); signal_df = tech_ind.get_signal()

        return signal_df
    
    StrategyFXCTA_Example.construct_signal = construct_signal",0.4721484184,
1487,periodic signals & the lomb scargle periodogram,"class heart_signal(object):
    """"""
    Create a class to store groups of .wav files 
    We can convert data into a dataframe and perform lowpass filtering, perform smoothing, mapping S1/S2 locations
    """"""    
    
    def __init__(self, folder_path=None, files=None, autoread_files=False):
        """"""
        when initialized just store the location of all the data files containing .wav files to read in
        """"""
        self.provided_files = []
        self.autoread_files = autoread_files
        if not(folder_path is None):
            for r, dlist, flist in os.walk(path):
                """"""
                read in all files provided by folder
                """"""
                for f in flist:
                    if not(f.endswith('.wav')):
                        continue
                    name = os.path.join(r, f)
                    self.provided_files.append(name)
        if not (files is None):
            for name in files:                
                if os.path.isfile(name) and name.endswith('.wav'):
                    self.provided_files.append(name)
                else:
                    print('Invalid file {0}'.format(name))
        
        if len(self.provided_files) == 0:
            raise Exception
        
        if self.autoread_files:
            self.create_df()
        else:
            self.signal_df = None
        
        self.nan_free = None
        self.s1 = None
        self.s2 = None
                
    
    def create_df(self):
        """"""
            Read the .wav data into a dataframe where we collect the signal/sound data at the provided sampling rate
        """"""
        _, self.samplerate = sf.read(self.provided_files[0])
        
        signal_array = []
        data = []
        for name in self.provided_files:            
            signal, samplerate = sf.read(name)
            signal_array.append(list(signal))
            data.append({'name': name, 'basename': os.path.basename(name), 'samplerate': samplerate})
        
        self.data_df = pd.DataFrame(data)
        self.signal_df = pd.DataFrame(signal_array, index=self.data_df['basename'])
        
        if len(self.data_df['samplerate'].unique()) != 1:
            print('The sample rate seems to be different for the provided files!')
        else:
            # we can actually assign ""time data"" to the columns of the dataframe
            # the ""time"" will be given in intervals defined by the sampling rate
            time_values = np.arange(0, (self.signal_df.shape[1]*1.0)/self.samplerate, self.samplerate)
            # self.signal_df.rename(columns=lambda x: (1.0 * x)/self.samplerate, inplace=True)
            
        self.filtered_signal_df = self.signal_df
            
    def get_signal_length_distribution(self, nbins=None):
        if self.signal_df is None:
            print('You must create_df() first!')
            return
        
        if nbins:
            return np.histogram(
                self.signal_df.apply(lambda x: len(x.dropna())/self.samplerate, axis=1)
            )
        else:
            return np.histogram(self.signal_df.apply(lambda x: len(x.dropna()), axis=1))
        
    def identify_null_values(self):
        """"""
        It makes sense for the data to have NaN at the end of the signal. This is because each .wav file is a different 
        length. But not sure if there is any NaN data in the middle of the file. So this makes sure there is only
        one spoint in the data that has Nan values (i.e. doesnt do 0 NaN 1 Nan 2 Nan Nan; but it can do 0 1 2 NaN NaN NaN)
        """"""
        check_for_nan = (normal_group.signal_df.isnull().diff(axis=1).iloc[:, 1:] == True).sum(axis=1)
        has_unusual_data = check_for_nan[check_for_nan>1]
        if has_unusual_data.shape[0] > 0:            
            self.nan_free = False
            print('WE found files with NaN in them')
        else:
            self.nan_free = True
            print('Files look good (no NaN in the middle)')
        return has_unusual_data.index
        
    def get_some_data(self, index, use_iloc=False):
        """"""
        return a copy of the signals
        """"""
        if use_iloc:            
            return self.signal_df.iloc[index].copy()
        else:            
            return self.signal_df.loc[index].copy()
        
    
    def pad_zero(self, check_nan_free = True):
        if check_nan_free and (self.nan_free is None or check_nan_free is False):
            print('Sorry I cannot do this function until I know all nan values have been handled correctly!')
            raise Exception
            return
            
        if self.nan_free is None:
            print(""Warning: Im replacing all NaN points in the table with zeros but I havent checked if there are any Nan values in the middle of the dataset...this could screw up the results"")
        elif self.nan_free is False:
            print('Warning: Im replacing all Nan points in the table with zeros but some datapoints with Nan are not at the end of the signal and might yield incorrect results')
        
        self.signal_df.fillna(0, inplace=True)
        
    def low_pass_filter(self, freq, limit=None, inplace=False):
        """"""
        Applies a low pass filter to the signal dataframe
        
        freq: low-pass filter cutoff
        
        returns:
            df (signal df)
        """"""
        if limit:
            subsample = self.signal_df.iloc[:limit]
        else:
            subsample = self.signal_df
                                           
        new_data = [filter_noise(d, freq, self.samplerate) for d in subsample.values]
    
        new_data_df = pd.DataFrame(new_data, index=subsample.index)
        time_values = np.arange(0, (new_data_df.shape[1]*1.0)/self.samplerate, self.samplerate)
        # new_data_df.rename(columns=lambda x: (1.0 * x)/self.samplerate, inplace=True)
        
        if inplace:            
            self.filtered_signal_df = new_data_df
        else:
            return new_data_df
            
    def heart_beat_rate_to_hz(self, rate_per_min=60):
        # rate_per_min = total (lub/dub) beats per minute
        beats_per_sec = (rate_per_min*1.0)/60
        return beats_per_sec
    
    def heart_beat_rate_to_sampling_rate(self, rate_per_min=60):
        """"""
        Given a heart beat rate, return the number of ""frames"" in 
        the wav file that captures a single beat within a window size
        """"""
        # rate_per_min = total (lub/dub) beats per minute
        beat_hz = self.heart_beat_rate_to_hz(rate_per_min)
        # samplerate => frames/second
        # beat_hz => beats/secon, inverse = seconds/beats
        # frames_per_heartbeat = (frames/second) * (second/beats) = frames/second
        frames_per_heartbeat = self.samplerate*(1/beat_hz)
        return frames_per_heartbeat
    
    def spectrogram(self, window_size, noverlap=None, additional_kwargs={}, 
        limit=None, randomly_select=None, keep_time_unit=False,
        use_filtered_data = True):
        
        random_signals = self.filtered_signal_df if use_filtered_data else self.signal_df
        
        if randomly_select:
            random_signals.subsample(randomly_select)        
            
        if limit:
            limit = min(limit, random_signals.shape[0])
            random_signals = random_signals.iloc[:limit]
        
        freqs_sampled, average_time_window, Sxx = signal.spectrogram(
            random_signals.values, self.samplerate, 
            nperseg=int(window_size), 
            noverlap=int(noverlap), 
            **additional_kwargs
        )
        
        if keep_time_unit is False:
            average_time_window *= np.round(self.samplerate)
        
        return freqs_sampled, average_time_window, \
                pd.Panel(Sxx, items=random_signals.index, major_axis=freqs_sampled, minor_axis=average_time_window)
    
    def return_S1_S2_position(self, heart_calls_df, inplace=True):
        """"""
            Given the dataframe where the S1/S2 or lub/dub demarcations have been called out already, 
            return the original data where 'S1' is filled in the proper index and 'S2' is filled in the proper indexes            
        """"""
        
        # isolate only the rows listed in heart_calls_df
        subset = self.signal_df.loc[normal_group.signal_df.index.intersection(heart_calls_df.index)]
        # make a dummy dataframe to store results
        result_calls = pd.DataFrame(np.nan, index=subset.index, columns=subset.columns)
        df = heart_calls_df.loc[self.signal_df.index.intersection(heart_calls_df.index)]
        
        # isolate the 'S1' columns from the test set (lub)
        s1_columns = [s for s in df.columns if s.startswith('S1')]
        
        # isolate the 'S2' columns from the test set (dub)
        s2_columns = [s for s in df.columns if s.startswith('S2')]
        
        # really slow and hacky code, but go through each row and find the frames that refer to S1 or S2
        # LABEL THAT ROW/COLUMN COMBINATION in the dummy dataframe
        for i, (r, v) in enumerate(df.iterrows()):
            s1_col = v[s1_columns].dropna().values
            result_calls.ix[i, s1_col] = 'S1'
            s2_col = v[s2_columns].dropna().values
            result_calls.ix[i, s2_col] = 'S2'
            
        if inplace:
            self.S1_S2_demarcation = result_calls
        else:
            return result_calls
    
    def return_S1_S2_noise_position(self, heart_calls_df, inplace=True):
        """"""
            Given the dataframe where the S1/S2 or lub/dub demarcations have been called out already, 
            return the original data where 'S1' is filled in the proper index and 'S2' is filled in the proper indexes            
        """"""
        
        # isolate only the rows listed in heart_calls_df
        subset = self.signal_df.loc[normal_group.signal_df.index.intersection(heart_calls_df.index)]
        # make a dummy dataframe to store results
        result_calls = pd.DataFrame(np.nan, index=subset.index, columns=subset.columns)
        df = heart_calls_df.loc[self.signal_df.index.intersection(heart_calls_df.index)]
        
        # isolate the 'S1' columns from the test set (lub)
        s1_columns = [s for s in df.columns if s.startswith('S1')]
        
        # isolate the 'S2' columns from the test set (dub)
        s2_columns = [s for s in df.columns if s.startswith('S2')]
        
        noise_columns = [s for s in df.columns if s.lower() == 'noise']
        
        # really slow and hacky code, but go through each row and find the frames that refer to S1 or S2
        # LABEL THAT ROW/COLUMN COMBINATION in the dummy dataframe
        for i, (r, v) in enumerate(df.iterrows()):
            s1_col = v[s1_columns].dropna().values
            result_calls.ix[i, s1_col] = 'S1'
            s2_col = v[s2_columns].dropna().values
            result_calls.ix[i, s2_col] = 'S2'
            noise_col = v[noise_columns].dropna().values 
            result_calls.ix[i, noise_col] = 'N'

            
        if inplace:
            self.S1_S2_demarcation = result_calls
        else:
            return result_calls
        
    def return_event_position(self, heart_calls_df, lambda_fxn=lambda x: True, label='label'):
        """"""
            Given the dataframe where the S1/S2 or lub/dub demarcations have been called out already, 
            return the original data where 'S1' is filled in the proper index and 'S2' is filled in the proper indexes            
        """"""
        
        # isolate only the rows listed in heart_calls_df
        subset = self.signal_df.loc[normal_group.signal_df.index.intersection(heart_calls_df.index)]
        # make a dummy dataframe to store results
        result_calls = pd.DataFrame(np.nan, index=subset.index, columns=subset.columns)
        df = heart_calls_df.loc[self.signal_df.index.intersection(heart_calls_df.index)]
        
        # isolate the columns which return true for the lambda_fxn
        return_columns = [s for s in df.columns if lambda_fxn(s)]
         
        # really slow and hacky code, but go through each row and find the frames that refer to the lambda_fxn
        # LABEL THAT ROW/COLUMN COMBINATION in the dummy dataframe
        for i, (r, v) in enumerate(df.iterrows()):
            s1_col = v[return_columns].dropna().values
            result_calls.ix[i, s1_col] = label
            
        return result_calls
    
    def bin_signal(self, nperseg, noverlap, fs=1.0):
        """"""
            given a window, return the indicies of the dataframe where data should be binned into it            
        """"""
        col_as_array = self.signal_df.columns
        skip = int(nperseg)
        overlap=int(noverlap)
        bins = OrderedDict()
        for c in np.arange(
            0,
            col_as_array.max(),
            skip - overlap
        ):
            p1 = c
            p2 = (c + skip)
            bins[(c + (skip)/2.0)/fs] = list(col_as_array[p1:p2])
        return bins
    
    def get_average_signal_in_bin(self, nperseg, noverlap, fs=1.0, method='mean'):
        binned_data = self.bin_signal(nperseg, noverlap, fs)
        x_vals = np.array(binned_data.keys())
        new_df = {}
        for k, val in binned_data.iteritems():
            if method == 'mean':
                new_df[k] = self.filtered_signal_df[val].mean(axis=1)
            elif method == 'max':
                # returns either the maximum or minimum value in a group
                new_df[k] = self.filtered_signal_df[val].apply(
                    lambda x: x[np.abs(x.dropna()).idxmax()] if len(x.dropna())>0 else np.nan,
                    axis=1,
                )  
                
            else:
                print('performing median')
                new_df[k] = self.filtered_signal_df[val].median(axis=1)
        return pd.DataFrame(new_df)
    
    def return_s1_s2_binned_data(self, s1_s2_df, nperseg, noverlap, fs=1.0):
        binned_data = self.bin_signal(nperseg, noverlap, fs)
        s1_s2_bins = self.return_S1_S2_position(s1_s2_df, inplace=False)
        
        new_df = {}
        for k, val in binned_data.iteritems():
            new_df[k] = s1_s2_bins[val].apply(
                lambda x: np.array(','.join(sorted(list(x.dropna().unique())))) if len(x.dropna()) > 0 else '',
                axis=1
            )
        return pd.DataFrame(new_df)
    
    def get_average_freq_contribution(self, frame_cutoff=1000, window=100):
        """"""
            1. Convert signal to frequency contributes
            2. Only consider frequencies within the first [:frame_cutoff] indexes
            3. Smooth out the signal/get an average coefficient of frequencies defined by the window
            4. only return coefficients in multiples of the window
        """"""
        
        def smoothed_coefficients(signal):
            fft_signal = fftpack.fft(signal.dropna())
            fft_freqs = fftpack.fftfreq(fft_signal.shape[0], 1.0/self.samplerate)
            lim = range(frame_cutoff)
            abs_sign = pd.Series(np.abs(fft_signal[lim]), index=fft_freqs[lim])
            abs_sign =  abs_sign.rolling(window=window).mean().iloc[range(0, abs_sign.shape[0], window)].dropna()
            return abs_sign/abs_sign.max()
            
        return self.filtered_signal_df.apply(smoothed_coefficients, axis=1).fillna(0)",0.4647653103,
1487,periodic signals & the lomb scargle periodogram,"flP.plotOne(flP.petal); 
    flP.addOne(flP.spots);
    flP.addOne(flP.center, col='white', a=0.5)
    plt.plot(flP.petal.centroid.x,flP.petal.centroid.y, 'b+', markersize=15, mew = 3)",0.4628772736,
1487,periodic signals & the lomb scargle periodogram,"def handle_data(context, data):
    '''
    Called when a market event occurs for any of the algorithm's 
    securities. 

    Parameters

    data: A dictionary keyed by security id containing the current 
          state of the securities in the algo's universe.

    context: The same context object from the initialize function.
             Stores the up to date portfolio as well as any state 
             variables defined.

    Returns None
    '''
    # Allow history to accumulate 100 days of prices before trading
    # and rebalance every day thereafter.
    context.tick += 1
    if context.tick < 100:
        return
    # Get rolling window of past prices and compute returns
    
    #rebalance only every i days
    i = 60
    if (context.tick % i) != 0:
        return
    
    prices = history(100, '1d', 'price').dropna()
    returns = prices.pct_change().dropna()
    try:
        # Perform Markowitz-style portfolio optimization
        weights, _, _ = optimal_portfolio(returns.T)
        weights = np.around(weights)
        # Rebalance portfolio accordingly
        for stock, weight in zip(prices.columns, weights):
            order_target_percent(stock, weight)
    except ValueError as e:
        # Sometimes this error is thrown
        # ValueError: Rank(A) < p or Rank([P; A; G]) < n
        pass",0.4606952071,
1487,periodic signals & the lomb scargle periodogram,"def update_hist_exp(curr):
    # check if animation is at the last frame, and if so, stop the animation a
    if curr == n: 
        a.event_source.stop()
    plt.cla()
    #bins = np.arange(-7,1, 0.3)
    plt.hist(x3[:curr], bins=20, alpha=0.5, color = ""#c0737a"")
    plt.axis([5,21,0,20])
    plt.gca().set_title('Exponential (x3)')
    #plt.gca().set_ylabel('Frequency')
    #plt.gca().set_xlabel('Value')
    plt.annotate('n = {}'.format(curr), [3,27])",0.4604123533,
1487,periodic signals & the lomb scargle periodogram,"def update_hist_gamma(curr):
    # check if animation is at the last frame, and if so, stop the animation a
    if curr == n: 
        a.event_source.stop()
    plt.cla()
    #bins = np.arange(-7,1, 0.3)
    plt.hist(x2[:curr], bins=20, alpha=0.5, color = ""#ff796c"")
    plt.axis([-1,14,0,20])
    plt.gca().set_title('Gamma (x2)')
    #plt.gca().set_ylabel('Frequency')
    #plt.gca().set_xlabel('Value')
    plt.annotate('n = {}'.format(curr), [3,27])",0.4604123533,
1487,periodic signals & the lomb scargle periodogram,"def update_hist_uni(curr):
    # check if animation is at the last frame, and if so, stop the animation a
    if curr == n: 
        a.event_source.stop()
    plt.cla()
    #bins = np.arange(-7,1, 0.3)
    plt.hist(x4[:curr], bins=20, alpha=0.5, color = ""#88b378"")
    plt.axis([13,21,0,20])
    plt.gca().set_title('Uniform (x4)')
    #plt.gca().set_ylabel('Frequency')
    #plt.gca().set_xlabel('Value')
    plt.annotate('n = {}'.format(curr), [3,27])",0.4604123533,
1487,periodic signals & the lomb scargle periodogram,"def plot_history(history):
    # plot of loss function
    plt.figure(figsize=(13,7))
    plt.plot(history.history['loss'],""o-"",label=""loss"",)
    plt.plot(history.history['val_loss'],""o-"",label=""val_loss"")
    plt.title('model loss')
    plt.xlabel('epoch')
    plt.ylabel('loss')
    plt.legend(loc='upper right')
    plt.show()

    # plot of accuracy
    plt.figure(figsize=(13,7))
    plt.plot(history.history['acc'],""o-"",label=""accuracy"")
    plt.plot(history.history['val_acc'],""o-"",label=""val_acc"")
    plt.title('model accuracy')
    plt.xlabel('epoch')
    plt.ylabel('accuracy')
    plt.legend(loc=""lower right"")
    plt.show()

plot_history(history)",0.4560183883,
287,confusion matrix,"def tmatrix(N=10):
    twoN = 2*N
    states = twoN + 1
    t = np.zeros((states, states))
    j = range(0, states)
    for i in j:
        p = i/twoN
        t[i,] = binom.pmf(j, twoN, p)       
    return t",0.4760795236,
287,confusion matrix,"def diff(n, k=1):
    """""" Construct kth order difference matrix with shape (n-k, n) 
    """"""
    D = np.diag(-1*np.ones(n)) + np.diag(np.ones(n-1),1)
    D = D[:-1,:]
    if k > 1:
        return diff(n-1,k-1).dot(D)
    else:
        return D",0.4743890762,
287,confusion matrix,"def make_kernel(size=3):
    # make impulse
    kernel = np.zeros((size, size))
    kernel[size//2, size//2] = 2
    # substract unsharp
    kernel -= np.ones((size, size)) / size ** 2
    return kernel

crop = gray[600:1000, 600:1000]

fig = plt.figure(figsize=(8, 8))
fig.add_subplot(1, 2, 1)
plt.imshow(crop, cmap='gray')
plt.xlabel(""Original"")

unsharp_kernel = make_kernel()
fig.add_subplot(1, 2, 2)
unsh = cv2.filter2D(crop, -1, unsharp_kernel)
plt.imshow(unsh, cmap='gray')
plt.xlabel(""Unsharp mask"")",0.4733687043,
287,confusion matrix,"def plot_confusion_matrix(cls_pred):
    ut.plot_confusion_matrix(cls_pred, cls_test, num_classes, class_names)",0.4720308781,
287,confusion matrix,"# Construct kth order difference matrix with shape (n-k, n) 
def diff(n, k=1):
    D = np.diag(-1*np.ones(n)) + np.diag(np.ones(n-1),1)
    D = D[:-1,:]
    if k > 1:
        return diff(n-1,k-1).dot(D)
    else:
        return D",0.4639958143,
287,confusion matrix,"def confusion(y_true, y_pred):
    cm = metrics.confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8,4))
    sns.heatmap(cm, annot=True, fmt=""d"", xticklabels=['non-default', 'default'], yticklabels=['non-default','default'])
    plt.ylabel(""Real value"")
    plt.xlabel(""Predicted value"")
    
    d={'True Negative':cm[0,0], 'False Positive':cm[0,1], 'False Negative':cm[1,0], 'True Positive':cm[1,1]}

    return(d)",0.4593346715,1
287,confusion matrix,"def getX(n):
    """""" 
    Creates nx2 data matrix for Exercise 3-2
    """"""
    X = np.zeros((n,2))
    X[:,0] = np.linspace(-10, 10, num=n)
    X[:,1] = X[:,0]**2
    return X

def getY(X, true_w, true_b):
    """""" 
    Creates nx1 matrix for target of X
    with true weights, w = [w1, w2] and b
    """"""
    w = np.r_[true_b, true_w]
    X = np.c_[np.ones((X.shape[0], 1)), X]
    return np.dot(X, w).reshape((X.shape[0],-1))",0.4575753212,
287,confusion matrix,"def bin_by_year(nbins = 9):
    bins = np.mgrid[min_year:max_year:nbins*1j]
    min_sqf, max_sqf = bin_extrema(buildings_20, bins,
                               'Year Acquired', 'Square Footage')
    plt.semilogy(bins[:-1], min_sqf, '-r', drawstyle='steps-post')
    plt.semilogy(bins[:-1], max_sqf, '-r', drawstyle='steps-post')
    plt.plot(buildings_20['Year Acquired'],
             buildings_20['Square Footage'],
             '.g')",0.4568082392,
287,confusion matrix,"def diagonalDelta():
    numbers = numpy.random.rand(2)*2 - 1 + 1j*(numpy.random.rand(2)*2 -1)
    Delta = numpy.asmatrix(numpy.diag(numbers))
    return Delta",0.453851223,
287,confusion matrix,"# some helper functions we will need for making the data
def make_independent_data(dimension=3, n=1000):
    '''
    Makes independent data in ""dimension"" dimensions
    '''
    covar = np.diag([1 for i in range(dimension)])
    dat_i = np.random.multivariate_normal([0 for i in range(dimension)], 
                                          covar, n)
    
    return dat_i, covar

def make_correlated_data(dimension, n=1000, phi=0.5):
    '''
    Makes dependent data in ""dimension"" dimensions, correlation between
    index-adjacent dimensions controlled with ""phi""
    NOTE: Phi must be between 0 and 1
    '''
    
    covar = np.zeros((dimension, dimension))
    
    for ii in range(dimension):
        for jj in range(dimension):
            covar[ii, jj] = np.power(phi, np.abs(ii - jj))
            
    dat = np.random.multivariate_normal([0 for i in range(dimension)], 
                                          covar, n)
    
    return dat, covar",0.4517041445,
2552,what do you notice about the income data after you replot it?,"df = data[['per_fisc_year','tot_revnu','cost_good_sold','gross_profit','tot_oper_exp','oper_income']]
df['op_profit_margin_pct'] = df.oper_income / df.tot_revnu
df",0.4665846229,
2552,what do you notice about the income data after you replot it?,"# Assign outcome as 0 if income <=50K and as 1 if income >50K
df['income'] = [0 if x == '<=50K' else 1 for x in df['income']]

# Assign X as a DataFrame of features and y as a Series of the outcome variable
X = df.drop('income', 1)
y = df.income",0.4650769234,
2552,what do you notice about the income data after you replot it?,"dta.income.replace({""<=50K"": 0, "">50K"": 1})",0.4547773004,
2552,what do you notice about the income data after you replot it?,"dta.income.replace({'<=50K': 0, '>50K': 1})",0.4547773004,
2552,what do you notice about the income data after you replot it?,"#Define covariates in X and dependent variable in y
X = adults2[['age','workclass_enc','education.num','marital.status_enc','occupation_enc',
            'race_enc','sex_enc','relationship_enc','capital.gain','capital.loss',
            'hours.per.week','native.country_enc']]
y = adults2.income_enc",0.4480867982,
2552,what do you notice about the income data after you replot it?,"df = data[['per_fisc_year',
           'pre_tax_income',
           'tot_provsn_income_tax']]


df['effective_tax_rate'] = df.tot_provsn_income_tax / df.pre_tax_income
df",0.4465018809,
2552,what do you notice about the income data after you replot it?,"#setting dataframes for the axis
X = df.drop('income',1)
Y = df.income",0.4461190701,
2552,what do you notice about the income data after you replot it?,"# Split into features and target variable
X_unprocessed = df_unprocessed.drop('income', 1)
y_unprocessed = df_unprocessed.income",0.4461190701,
2552,what do you notice about the income data after you replot it?,"df = data[['per_fisc_year',
           'tot_revnu',
           'cost_good_sold',
           'tot_oper_exp',
           'tot_non_oper_income_exp',
           'tot_provsn_income_tax',
           'income_cont_oper',
           'income_discont_oper',
           'consol_net_income_loss']]

df['net_margin_pct'] = df.consol_net_income_loss / df.tot_revnu
df",0.4461100101,
2552,what do you notice about the income data after you replot it?,"df['income'] = df.income.replace({'<=50K': 0.0, '>50K' : 1.0})",0.4453092813,
516,early stopping methods for decision trees,"class decision_tree:
    def __init__(self, data, labels, eita=5, pi=0.9, show_score=False):
        self.eita = eita
        self.pi = pi
        num_of_classes = len(set(labels))
        self.root = self.build_decision_tree(data, labels, num_of_classes, eita, pi, show_score)
    
    def classify(self, data_point):
        return self.recursive_classify(self.root, data_point)
    
    def recursive_classify(self, root, data_point):
        if isinstance(root, leaf_node):
            return root.label
        if root.classify(data_point):
            return self.recursive_classify(root.left, data_point)
        return self.recursive_classify(root.right, data_point)
        
    def build_decision_tree(self, data, labels, num_of_classes, eita, pi, show_score):
        assert len(data) == len(labels)
        n = len(data)

        class_data = list_to_dict(labels)
        for i in range(len(data)):
            label = labels[i]
            class_data[label].append(data[i])

        # Convert class-specific subsets to np matrix
        for label in class_data:
            temp = class_data[label]
            class_data[label] = np.array(temp).reshape(len(temp), len(temp[0]))

        # find class with max purity
        purity = 0
        purest_class = -1
        for label in class_data:
            if len(class_data[label]) > purity:
                purity = len(class_data[label])
                purest_class = label
        if n == 0: # shouldn't happen unless a split point didn't split anything
            return leaf_node(-1, 0)
        purity = purity / n

        # stopping condition / base case
        if eita >= n or purity > pi:
            leaf = leaf_node(purest_class, purity)
            return leaf

        split_attr, split_point = -1,-1
        best_score = -1

        # for each attribute
        for attr in range(len(data[0])):
            if type(data[0][attr]) is 'bool':
                continue
            else:
                split, score = evaluate_numeric_attr(data, labels, num_of_classes, attr)
                if show_score:
                    print (""attr: {}, score: {}"".format(attr, score))
                if score > best_score:
                    split_attr, split_point = attr, split
                    best_score = score

        data_y = data[data[:,split_attr] < split_point]
        data_n = data[data[:,split_attr] >= split_point]

        label_y = labels[data[:,split_attr] < split_point]
        label_n = labels[data[:,split_attr] >= split_point]

        int_node = internal_node(purity)
        int_node.set_splitter(split_attr, split_point)
        int_node.set_left_child(self.build_decision_tree(data_y, label_y, num_of_classes, eita, pi, show_score))
        int_node.set_right_child(self.build_decision_tree(data_n, label_n, num_of_classes, eita, pi, show_score))

        return int_node",0.4616602659,1
516,early stopping methods for decision trees,"def lasso_cyclical_coordinate_descent(feature_matrix, output, initial_weights, l1_penalty, tolerance):
    weights = initial_weights
    max_delta = tolerance + 0.1 #initializing a max_delta that's greater than tolerance
    
    while max_delta > tolerance:
        for i in range(len(weights)):
            old_weights_i = weights[i]
            weights[i] = lasso_coordinate_descent_step(i, feature_matrix, output, weights, l1_penalty)
            delta_i = np.absolute(old_weights_i - weights[i])
            
            if (i == 0) or (delta_i > max_delta): #first feature update
                max_delta = delta_i
                
    return weights",0.4550530314,
516,early stopping methods for decision trees,"def criteria(p):
    if p.visits[0].age < 19: # the patient is younger than 19 years during first visit recorded for the patient 
        for v in p.visits: # iterate over all visits
            for pr in v.prs: # iterate over all procedures
                if pr.pcode == code and v.year == 2006: # if there is a procedure during 2006 return True
                    return True
    return False
P022_in_2006 = filter(criteria,patients) # filter list of patients using above criteria
print ""Number of patients who underwent {} during 2006, at age < 19 or younger is {}"".format(code,len(P022_in_2006))",0.4528716207,
516,early stopping methods for decision trees,"###### student code goes here ######
def KFSample(mean0, var0, a, b, c, d, t):
    X = -np.zeros(t+1)
    Y = -np.zeros(t+1)
    X[0] = norm.rvs(mean0, var0)
    Y[0] = np.nan
    for s in xrange(1, t+1):
        X[s] = a * X[s-1] + b * norm.rvs(0, 1)
        Y[s] = c * X[s] + d * norm.rvs(0, 1)
    return X, Y",0.4522571564,
516,early stopping methods for decision trees,"def optimize(num_iterations):
    ut.optimize(num_iterations, transfer_values_train, train_batch_size, 
                labels_train, session, global_step, optimizer, accuracy, x, y_true)",0.4518017471,
516,early stopping methods for decision trees,"def fitandplot2(X_train, X_val, y_train, y_val, clf):
    clf.fit(X = X_train.values, y = y_train.values, eval_metric='error', early_stopping_rounds=30, 
                    eval_set = [(X_val.values, y_val.values)], verbose=True)
    print(min(clf.evals_result()['validation_0']['error']))

    importances = clf.feature_importances_
    indices = np.argsort(importances)[::-1]
    plt.rcParams['text.usetex']=False
    plt.figure()
    plt.title(""Feature importances"")
    plt.bar(range(X_train.shape[1]), importances[indices],
           color=""r"", align=""center"")
    plt.xticks(range(X_train.shape[1]), X_train.columns.values[indices], rotation=25)
    plt.xlim([-1, X_train.shape[1]])
    plt.show()

param={'colsample_bylevel': 0.7501177019888027, 'colsample_bytree': 0.8291070503829303, 'gamma': 0.3712264689410901, 'learning_rate': 0.07820296189339045, 'max_depth': 17, 'min_child_weight': 5.0, 'reg_alpha': 0.9998518175523757, 'reg_lambda': 0.5238219352181274, 'subsample': 0.6257283056660621}
clf2 = XGBClassifier(**param, nthread = -1, n_estimators=1000)   
fitandplot2(X_train2.drop(['tarray','xybinarray'],axis=1), X_val2.drop(['tarray','xybinarray'],axis=1), 
            y_train2, y_val2, clf2)",0.4517319202,
516,early stopping methods for decision trees,"def cd(x0, x_true, f, grad_f, g, prox_g, eps=1e-30, n_iter=50, callback=None):
    x = x0.copy()
    x_new = x0.copy()
    n_samples, n_features = K.shape
    L = K
    grad = grad_f(x)
    
    for k in range(n_iter):
        if norm(x - x_true) <= eps and callback is not None: 
            callback(x)
            break
        i = np.random.randint(0, n_features)
        x_new[i] = proxi((x[i] - grad[i]/L[i,i]))
        grad = grad - K[:,i]*x[i] + K[:,i]*x_new[i] # smart update
        x[i] = x_new[i]
        
        # Update metrics after each iteration.
        if k % n_features == 0 and callback is not None: 
            callback(x)
    return x",0.4498067498,
516,early stopping methods for decision trees,"if RUN_ANALYSES:
    analyses.trial_time_means.run(exp_data, RESULTS_PATH, SEED)
    analyses.trial_time_corrs.run(exp_data, RESULTS_PATH, SEED)
pd.read_csv(RESULTS_PATH.joinpath(analyses.trial_time_corrs.filename))",0.4461746812,
516,early stopping methods for decision trees,"def warped_back(binary_warped,left_fitx, right_fitx, ploty, Minv, img):
    newwartp = None
    bi_warp_zero = np.zeros_like(binary_warped).astype(np.uint8)
    color_warp = np.dstack((bi_warp_zero, bi_warp_zero, bi_warp_zero))*255

    # Recast the x and y points into usable format for cv2.fillPoly()
    pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])
    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])
    pts = np.hstack((pts_left, pts_right))

    # Draw the lane onto the warped blank image
    cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))
#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    # Warp the blank back to original image space using inverse perspective matrix (Minv)
    newwarp = cv2.warpPerspective(color_warp, Minv, image_size)
    # Combine the result with the original image
    result = cv2.addWeighted(img, 1, newwarp, 0.3, 0)
    
    return result",0.4460883737,
516,early stopping methods for decision trees,"# Likelihood function used to compute P(X | Z, sigma_X, sigma_A)
def likelihood(X, Z, M, sigma_A, sigma_X, K_plus, num_objects, object_dim):
    part1 = (-1)*num_objects*(0.5*object_dim)*np.log(2*np.pi)
    part2 = (-1)*(num_objects-K_plus)* object_dim *np.log(sigma_X) 
    part3 = (-1)*object_dim*K_plus*np.log(sigma_A) 
    part4 = (-1)*(0.5*object_dim)* np.log(np.linalg.det((np.dot(Z.T, Z) + (sigma_X**2/sigma_A**2)*np.eye(K_plus)))) 
    part5 = (-1/(2*sigma_X**2)) * np.trace(np.dot(np.dot(X.T,(np.identity(num_objects) - np.dot(np.dot(Z,M),Z.T))),X))
    total = part1+part2+part3+part4+part5
    return(total)",0.4450092316,
159,calculate norm,"def euclidean_distance(x1, x2):
    return np.linalg.norm(np.subtract(x1,x2))",0.4760505855,
159,calculate norm,"def distance_between_leaf_and_projection(leaf, n_dimensions=8):
    return np.linalg.norm(leaf - svd_shadow_cast_by(leaf, n_dimensions)) / float(len(leaf))

distances = [distance_between_leaf_and_projection(leaf) for leaf in A.T]",0.4740537405,
159,calculate norm,"def closestPrototype(x):
    # First, 
    distances = map(lambda kp: np.linalg.norm(x-kp), kPrototypes)
    distances = np.array(distances)
    return(np.ndarray.argmin(distances))",0.4731626511,
159,calculate norm,"def rmse(predictions, true):
    """"""Calculate the mean squared error for a prediction
    
    Parameters
    ----------
    predictions : numpy.ndarray
    true: numpy.ndarray
    
    Returns
    -------
    float
    
    Raises
    ------
    ValueError
        If `predictions` and `true` are not the same length
    
    Examples
    --------
    >>> a = np.arange(5)
    >>> rmse(a, a+1)
    2.2360679774997898
    
    >>> rmse(np.ones(4), np.ones(4))
    0.0
    """"""
    return np.linalg.norm(predictions - true)",0.4722700715,
159,calculate norm,"def euclidean_distance(x1, x2):
    """"""Compute Euclidean distance between two data points.
    
    Parameters
    ----------
    x1 : array, shape (4)
        First data point.
    x2 : array, shape (4)
        Second data point.
    
    Returns
    -------
    distance : float
        Euclidean distance between x1 and x2.
    """"""
    # TODO
    return np.linalg.norm(x1 - x2)",0.4722700715,
159,calculate norm,"def dist(x):
    ''' Return the distance of point x from the origin <0, 0, .., 0>.
    
        Uses the linear algebra concept of a norm to calculate 
        Euclidean distance.
        
        Examples:
            Given two vectors A = <0,3,8> and B = <0,3,9>
                >>> A = np.asarray([0,3,8])
                >>> B = np.asarray([0,3,9])
        
            Find the length of A.
                >>> dist(A)

            Find the distance between A and B.
                >>> dist(A - B)
                
            Given a list of vectors C = [<1,1,1>, <2,3,5>, <8,3,7>].
                >>> C = np.asarray([[1,1,1], [2,3,5], [8,3,7]])
                
            Find the distances from the origin, O.
                >>> dist(C)
                
            Find the distances from point B.
                >>> dist(C - B)
            
        Args:
            x (np.array):   point or list of points.
            
        Returns:
            np.array: scalar distance or list of scalar distances.
    '''
    
    return np.linalg.norm(x, axis=(x.ndim - 1))",0.4713137448,
159,calculate norm,"def sse(x1, x2):
    """""" returns the sum-squared-error between two points. """"""
    return np.linalg.norm(x1 - x2, 2, axis=0)",0.4705189168,
159,calculate norm,"def vector2length(x):
    # x : [B, 2] where B is number of samples.
    return np.sqrt(np.sum(x ** 2, axis=1, keepdims=True))",0.469861567,
159,calculate norm,"def _a(mu, v_inf):
    return - (mu)/(np.linalg.norm(np.square(v_inf)))",0.4684973955,
159,calculate norm,"def max_synthetic_likelihood(d0, N):
    sigma_g=1/np.sqrt(N*batch_size)
    return ss.norm(d0, sigma_g)",0.4683126509,
633,final mask,"def rle_encode(mask_image):
    pixels = mask_image.flatten()
    # We avoid issues with '1' at the start or end (at the corners of 
    # the original image) by setting those pixels to '0' explicitly.
    # We do not expect these to be non-zero for an accurate mask, 
    # so this should not harm the score.
    pixels[0] = 0
    pixels[-1] = 0
    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2
    runs[1::2] = runs[1::2] - runs[:-1:2]
    return runs

def rle_to_string(runs):
    return ' '.join(str(x) for x in runs)

file_name = get_car_image_files(image_id)[0].split(""/"")[-1]
mask_rle = train_masks_df[train_masks_df['img'] == file_name][""rle_mask""].iloc[0]
assert rle_to_string(rle_encode(mask)) == mask_rle, ""Mask rle don't match""
print(""Mask rle match!"")",0.4024935365,1
633,final mask,"def inc_brightness(array):
    array2 = array.copy()
    array2[array2>127] = 255
    array2[array2!=255] *= 2
    return array2

bright_image = np.apply_along_axis(inc_brightness, 2, img)

display_image(bright_image)",0.4007992744,
633,final mask,"def signum(u):
    return Vec(u.D, {k: 1 if v >= 0 else -1 for k, v in u.f.items()})",0.389325738,
633,final mask,"def preprocess(img):
    res = np.copy(img)
    # lane to road
    res[res==6] = 7
    # set all zero except road and vehicle
    res[(res!=7)&(res!=10)] = 0
    # Hood to zero
    res[490:,:,:][res[490:,:,:]==10] = 0
    return res

plt.imshow(preprocess(label_img)[:,:,0])",0.3892453909,1
633,final mask,"def pos_nes_chopper(s):
    p = s.copy()
    p[p<0] = 0
    n = s.copy()
    n[n>0] = 0
    return p,n",0.3886879683,1
633,final mask,"def classified_band_from_y(y, band_mask):
    class_band = np.ma.array(np.zeros(band_mask.shape),
                             mask=band_mask.copy())
    class_band[~class_band.mask] = y
    return class_band

pred_band = classified_band_from_y(y_pred, get_mask(refl_bands))",0.38606745,1
633,final mask,"def cast_non_zero_to_one(var):
    """"""
    Cast non-zero values to one
    
    The input variable will be copied prior to transformation
    
    Parameters
    ----------
    var : array-like
        The variable to make the transformation on
    
    Returns
    -------
    binary : array-like
        The transformed variable
    """"""
    
    binary = var.copy()
    
    binary[binary != 0] = 1
    
    return binary",0.383746773,1
633,final mask,"def bound_prediction_values(predictions):
    x = predictions.copy()
    x[x < 0] = 0
    x[x > 1] = 1
    return x",0.3835615218,1
633,final mask,"def makeMatrixMask(matrixDf):
    mask = matrixDf.mask(np.triu(np.ones(matrixDf.shape)).astype(np.bool))
    mask[abs(mask)>0] = False
    mask[abs(mask)!=0.] = True
    return abs(1-np.array(mask).T)",0.3822554946,1
633,final mask,"# INPUT: V - np.array(N_films, k), test_data - scipy.sparse.csr_matrix (N_train x N_films)
# OTPUT: total_score - integer
def total_score_folding(V, test_data): # 8 pts
    # enter you code here
    total_score = 0
    test_data[:,-3:]=scipy.sparse.csr_matrix(np.zeros((test_data.shape[0],3)))
    for user in test_data:
        rec = V@user.T
        rec = scipy.sparse.csr_matrix(V.T@rec)
        total_score+=np.intersect1d(top_n(user,3), top_n(rec,3)).shape[0]
    return total_score",0.379150629,
1965,split,"def longest_word(text):
    longest = max(text.split(), key = len) 
    return longest, len(longest)",0.4598654509,
1965,split,"# Util functions
def get_typle_family_name_and_title(str_name):
    idx_comma = str_name.find(',')
    idx_dot = str_name.find('.')
    return (str_name[:idx_comma], str_name[idx_comma+2: idx_dot]) 

def discretization_eq_interval_column(df, column_name, num_intervals):
    min_ = int(df[column_name].min())
    max_ = int(df[column_name].max())
    num = num_intervals+1
    return np.linspace(min_,max_,num,endpoint=True)
    

def discretization_lenght_columns(df, column_names, sufx='len'):
    
    for col in column_names:
        x_len = df[col].apply(lambda x: ('len_%s') % len(x))
        
        # applying the sufx
        if not pd.isnull(sufx):
            df[col + '_' + sufx] = x_len
        else:
            df[col] = x_len
    
    return df
    
    
    
def normalize_float_columns(df, columns_name, imputer, scaler, sufx='normalized'):
    for idx, col in enumerate(columns_name):
        #returns a numpy array
        x_col = df[col].values 

        #  reshape numpy array
        #  if you have more than one feature: => temp = temp.reshape(1,-1) 
        #  for one feature: => temp = temp.reshape(-1,1)
        x_col = x_col.reshape(-1,1)
        
        # for replacing of nan values
        x_col = imputer.fit_transform(x_col)
        
        # normalize
        x_scaled = min_max_scaler.fit_transform(x_col)
        
        # applying the sufx
        if not pd.isnull(sufx):
            df[col + '_' + sufx] = x_scaled
        else:
            df[col] = x_scaled
    
    # returning the dataframe
    return df


def discretize_float_columns(df, columns_name, dim_bins, sufx='discretized'):   
    
    for idx, col in enumerate(columns_name):
        df_col = df[col]
        # bins = dim_bins[idx]
        # get the intervals
        bins = discretization_eq_interval_column(df, col, dim_bins[idx])
        
        labels = [ ""%s %s %s "" % (col, (idx_b+1), len(bins)-1) for (idx_b, x) in enumerate(bins)]
        # must be one less 
        labels = labels[:-1] # dropping the last 
        
#        print bins, labels
        
#         df_col = pd.cut(df[col], bins=bins, labels=labels).values.add_categories('missing')
        df_col = pd.cut(df[col], bins=bins, labels=labels, include_lowest=True)
#         df_col = df_col.fillna('missing')
        
        if not pd.isnull(sufx):
            df[col + '_' + sufx] = df_col
        else:
            df[col] = df_col
    # return dataframe
    return df


def categorize_float_columns(df, columns_name, sufx='category'):   
    
    for idx, col in enumerate(columns_name):
        df_col = df[col].tolist()
        
        values = [str(x) for x in df_col]
        
        if not pd.isnull(sufx):
            df[col + '_' + sufx] = values
        else:
            df[col] = values
    # return dataframe
    return df
        
        
# Utility function to report best scores
def report(results, n_top=3):
    for i in range(1, n_top + 1):
        candidates = np.flatnonzero(results['rank_test_score'] == i)
        for candidate in candidates:
            print(""Model with rank: {0}"".format(i))
            print(""Mean validation score: {0:.3f} (std: {1:.3f})"".format(
                  results['mean_test_score'][candidate],
                  results['std_test_score'][candidate]))
            print(""Parameters: {0}"".format(results['params'][candidate]))
            print("""")


# save log results for the gridserach nn            
def save_df_result_dataframe(df_result):
    file_name = r""result/result_log_%s.csv"" % time.strftime(""%Y-%m-%d %H:%M"")
    with open(file_name, 'wb') as f:
        df_result.to_csv(f, sep=b'\t')



def save_log_result_nn(grid_result):    
    means = grid_result.cv_results_['mean_test_score']
    stds = grid_result.cv_results_['std_test_score']
    params = grid_result.cv_results_['params']
    
    file_name = r""result/result_log_%s.txt"" % time.strftime(""%Y-%m-%d %H:%M"")
    path = os.getcwd()
    fullpath = os.path.join(path, file_name)
    
    with open(fullpath, ""w"") as text_file:
        # print(""Best: %f using %s"" % (grid_result.best_score_, grid_result.best_params_))
        text_file.write(""Best: %f using %s"" % (grid_result.best_score_, grid_result.best_params_))
        
        for mean, stdev, param in zip(means, stds, params):
            # print(""%f (%f) with: %r"" % (mean, stdev, param))
            text_file.write(""%f (%f) with: %r"" % (mean, stdev, param))",0.4392425418,1
1965,split,"def split_tree_by(tree, attr):
  tree.split = attr
  children = {}
  values = set(d[attr] for d, _ in tree.data)
  for value in values:
    children[value] = Tree([record
      for record in tree.data
        if record[0][attr] == value])
  tree.children = children",0.4377244711,
1965,split,"def split_traffic_data(x):
    temp = x.split("";"")
    return (temp[1], [temp[0]] + temp[2:-1])",0.4336189032,
1965,split,"# $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
# $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
# MODIFY THIS METHOD TO WIN
# $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
# $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
#HINT: stopwords, lemmatization, stemming, named entity, lowercase, word combination (e.g, 'not good'), adjectives, etc. 
# $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
# $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

# extract features from a text
def extractTokens(strText):
    result = []
    # features = tokenizer.tokenize(strText)
    result = re.split('\s', strText)
    return result",0.4306647182,
1965,split,"# split the given strings from a string list based on given separator and outpu a list of the splitted strings
def extractString(strInputList, separator):
    strOutputList = []
    for si in strInputList:
        splitted = si.split(separator)
        strOutputList.extend(splitted)
    return strOutputList",0.4282001257,1
1965,split,"def tokenize(sentence):
    # tokenize words
    return [x.strip() for x in re.split('(\W+)', sentence) if x.strip()]",0.4259508252,1
1965,split,"def tokenize(sent):
    return [x.strip() for x in re.split('(\W+)?', sent) if x.strip()]",0.4259508252,1
1965,split,"def tokenize(sent):
    return [ x.strip() for x in re.split('(\W+)?', sent) if x.strip()]",0.4259508252,1
1965,split,"def split(string, char):
    """""" Split the string at the given character """"""
    
    position = string.find(char)
    
    if  position > 0:
        return string[:position+1], string[position+1:]
    else:
        return string, ''",0.4245211482,1
1830,rug plots,"for r in rundb:
    plotFiles = rundb[r].getPlotFiles(mIds[r])
    summaryStats = rundb[r].getSummaryStats(mIds[r])
    print ""Run %s"" %r
    print plotFiles['plotFile']  # this is a numpy array with the metric information + plot file name
    print summaryStats
    print ''",0.4831933975,
1830,rug plots,"def plot_variant(chrom, x1=526000, x2=538000):
    clf()    
    chrom.plot_rois(x1, x2)
    roi = chrom.rois
    low = roi.get_low_rois()
    high = roi.get_high_rois()
    chr = chrom
    highT = (chr.thresholds.high * chr.best_gaussian[""sigma""] +
         chr.best_gaussian[""mu""]) * chr.df[""rm""]
    lowT = (chr.thresholds.low * chr.best_gaussian[""sigma""] +
        chr.best_gaussian[""mu""]) * chr.df[""rm""]

    for k,v in low.iterrows():        
        Y1 = chr.df['cov'].loc[v.start:v.end]
        Y2 = lowT.loc[v.start:v.end] 
        Y1 = Y1.combine(Y2, max) *0
        #if v.start > x1 and v.end<x2:
        #    try:fill_between(range(v.start, v.end+1), Y1, Y2, 
        #                     alpha=0.3, color=""green"")
        #    except:pass   
    # CNVntator 
    xlim([x1, x2])
    x1, x2 = xlim()
    L = x2-x1
    for i in range(len(cnvnator)):
        this = cnvnator.loc[i]
        axhline(chrom.df['cov'].mean() * this[3], (this.start-x1)/L, 
                (this.end-x1)/L, lw=8, color=""yellow"", alpha=0.5)
    xlabel(""Base position (bp)"")
    
plot_variant(chrom, 374000,400000)
pylab.xlabel(""Base position (bp)"", fontsize=18)
pylab.ylabel(""Coverage"", fontsize=18)
pylab.savefig(""comp_cnv_bin100_2.png"", dpi=200)",0.4728558064,
1830,rug plots,"def plotGraphs():
    import matplotlib.pyplot as plt
    from sklearn import preprocessing
    x = original_train_data[""free sulfur dioxide""]
    x1 = original_train_data[""total sulfur dioxide""]


    plt.plot(1/(x1[0:100]/x[0:100]),color=""g"")
    plt.ylabel('some numbers')
    plt.show()
    return 
plotGraphs()",0.4683311582,
1830,rug plots,"def experiment(pca_comp, gmm_comp, cov_type):
    pca = PCA(n_components=pca_comp)
    pca.fit(df_ptrainf) 
    dim_reduc = pca.transform(df_ptrainf) 
    gmm = []
    lp_dev = []
    lp_test = []
    label_set = np.unique(df_ptrain['Activity'])
    for i in range(0,len(label_set)):
        one_label = dim_reduc[df_ptrain['Activity'] == label_set[i]]
        gmm.extend([GaussianMixture(n_components=gmm_comp, covariance_type=cov_type)])
        gmm[i].fit(one_label)
        lp_dev.extend([gmm[i].score_samples(pca.transform(df_pdevf))]) # weighted log probabilities
        lp_test.extend([gmm[i].score_samples(pca.transform(df_ptestf))]) 
    pred_dev = []
    pred_test = []
    for j in range(0,lp_dev[0].shape[0]):
        col = [row[j] for row in lp_dev]
        pred_dev.extend([label_set[np.argmax(col)]])
    for j in range(0,lp_test[0].shape[0]):
        col = [row[j] for row in lp_test]
        pred_test.extend([label_set[np.argmax(col)]]) 
    means = gmm[0].means_.size
    covs = gmm[0].covariances_.size
    ppc = means + covs # parameters per class
    dev_acc = round(metrics.accuracy_score(df_pdev['Activity'],pred_dev)*100,1)
    test_acc = round(metrics.accuracy_score(df_ptest['Activity'],pred_test)*100,1)
    tab_cells.append([means, covs, ppc, dev_acc, test_acc])
    tab_rows.extend([""%d-component PCA with %d-component %s covariance GMM"" 
                     % (pca_comp, gmm_comp, cov_type)])

tab_cols = ['means', 'covariances', 'params per class', 'dev accuracy', 'test accuracy']
tab_rows = []
tab_cells = []
ppc_lim = 50 # arbitrary limit on parameters per class
covs = ['spherical', 'diag', 'tied', 'full'] 

gmm_comp = np.unique(df_ptrain['Activity']).shape[0] # setting to class count for simplicity
for cov in covs:
    if cov == 'spherical':
        pca_comp = 5 # each component has its own single variance
    elif cov == 'diag':
        pca_comp = 3 # each component has its own diagonal covariance matrix
    elif cov == 'tied':
        pca_comp = 4 # all components share the same general covariance matrix
    elif cov == 'full':
        pca_comp = 2 # each component has its own general covariance matrix
    else:
        print(""error: bad cov"")
    if gmm_comp > 0:
        experiment(pca_comp, gmm_comp, cov)
            
tab_cells = np.array(tab_cells)
print(""Accuracy varies somewhat between runs due to random initilization."")
print(""Reasonably close agreement in accuracy against dev and test suggests overfitting isn't excessive."")

plt.figure(figsize=(10, 1)) 
tab = plt.table(cellText=tab_cells,rowLabels=tab_rows, colLabels=tab_cols, 
                cellLoc='center', loc='upper left')
for key, cell in tab.get_celld().items():
    row, col = key
plt.axis('off')
tab.scale(xscale=1, yscale=1.5) # for row spacing relative to text
plt.show()",0.4661109447,
1830,rug plots,"# Shows the Sun image with a plot (x,y).
def display_wave(im, i=[], j=[], style='go', i1=[], j1=[]):
    if im == 'A':
        image = dataA
        a, b = 525, 350
    elif im == 'B':
        image = dataB
        a, b = 575, 800
    else:
        return
    colormap = plt.cm.magma
    normalize = matplotlib.colors.Normalize(vmin=-40, vmax=40) # parameters here were picked up for better visualization
    plt.figure(figsize=(12,12))
    plt.plot(np.array(j), np.array(i), style)
    plt.plot(np.array(j1), np.array(i1), style[0] + ""X"")
    plt.imshow(image, cmap=colormap, norm=normalize)
    plt.ylim(a+350,a)
    plt.xlim(b,b+350)
    #plt.gca().invert_yaxis()
    plt.grid()
    plt.show()",0.4648055434,
1830,rug plots,"# from https://github.com/egabrielsen/MachineLearning/blob/master/Lab08/lab8-Danh-Copy1.ipynb

def plot_roc(probas, y_true):
    plt.figure(figsize=(15,5))
    mean_tpr = 0.0
    mean_fpr = np.linspace(0, 1, 100)
    all_tpr = []

    classes = np.unique(y_true)
    perclass_mean_tpr = 0.0
    roc_auc = 0
    for j in classes:
        fpr, tpr, thresholds = mt.roc_curve(y_true, probas[:, j], pos_label=j)
        perclass_mean_tpr += interp(mean_fpr, fpr, tpr)
        perclass_mean_tpr[0] = 0.0
        roc_auc += mt.auc(fpr, tpr)
        plt.plot(fpr,tpr,'--',lw=.5,label='Class ROC for ensemble, AUC=%0.4f'
                     %(mt.auc(fpr, tpr)) )

    perclass_mean_tpr /= len(classes)
    roc_auc /= len(classes)
    mean_tpr += perclass_mean_tpr

    plt.plot(mean_fpr,perclass_mean_tpr,'-',lw=2,label='Mean Class ROC for ensemble, AUC=%0.4f'
                     %(roc_auc))
    plt.legend(loc='best')
    plt.xlabel('false positive rate')
    plt.ylabel('true positive rate')
    plt.title('ROC Curve')",0.4633380473,
1830,rug plots,"def display_vehicle_likelihoods():
    identify = Identifier(model)
    shape = params.inputs.shape
    (a, b) = params.inputs.roi

    for path in find_images('data/project_video_frames/*.png'):
        image = load_image(path, shape)

        mask = np.zeros(image.shape[:2], dtype=np.float64)
        mask[a:b] += identify(image)
        mask = np.uint8(mask * 127.0)
        k = (mask > 0)

        mask_bgr = np.zeros_like(image)
        mask_bgr[..., 1] = mask

        annotated = np.copy(image)
        annotated[k] = np.uint8(0.5 * image[k] + mask_bgr[k])

        display(image, annotated)

        
display_vehicle_likelihoods()",0.4624695778,
1830,rug plots,"def display_vehicle_tracking():
    track = Tracker(model)
    shape = params.inputs.shape
    (a, b) = params.inputs.roi

    for path in find_images('data/project_video_frames/*.png'):
        image = load_image(path, shape)
        annotated = track(image)
        display(image, annotated)

        
display_vehicle_tracking()",0.4590724111,
1830,rug plots,"def get_params():
    
    #PC components 
    components = [5,8,10]
    
    #Random Forest
    RF_params = dict(pca__n_components=components,RandomForest__n_estimators=[100,250,500])

    #Logistic Regression
    LR_params = dict(pca__n_components=components)    
    
    return(RF_params,LR_params)

def pipeline_gridsearch_(X,y):
    
    results = defaultdict()
    
    clf_pipes = [(clf,params) for clf,params in zip(make_pipeline(),get_params())]
    for clf in clf_pipes:
        
        model = clf[0].steps[1][0]
        print('================ Model: {} ================ '.format(model))
        
        clf = GridSearchCV(clf[0],param_grid=clf[1],cv=10,scoring='accuracy')
        clf.fit(X,y)
        
        print('Fold {0} -- Max Cross Validation Score of {1:.3f}'.format(clf.best_index_,clf.best_score_*100))
        print('Mean Test Score: {0:.3f} +- {1:.3f}'.format(float(np.mean(clf.cv_results_['mean_test_score'])*100),
                                            float(np.mean(clf.cv_results_['std_test_score'])*100)))

        #Cross-Validation Keys of Interest:
        k_cv = clf.cv_results_
        filt = ['std_score_time','mean_fit_time','mean_train_score','mean_test_score','param_pca__n_components']
        
        filtered_cv = dict((k, k_cv[k]) for k in filt if k in k_cv)
        results[model] = filtered_cv
        
    return(results)
        
if __name__ == '__main__':
    
    results = pipeline_gridsearch_(X,y)",0.4588832259,
1830,rug plots,"def frame_processor(img, color_space):
    """"""
    Detects and vehicles in input image.
        Parameters:
            img: Input image.
            color_space: The color space used by the classifier.
    """"""
    global heat_p, boxes_p, n_count
    heat = np.zeros_like(img[:,:,0]).astype(np.float)
    boxes = []
    boxes = find_cars(img, 400, 650, 950, 1280, 2.0, 2, color_space)
    boxes += find_cars(img, 400, 500, 950, 1280, 1.5, 2, color_space)
    boxes += find_cars(img, 400, 650, 0, 330, 2.0, 2, color_space)
    boxes += find_cars(img, 400, 500, 0, 330, 1.5, 2, color_space)
    boxes += find_cars(img, 400, 460, 330, 950, 0.75, 3, color_space)
    for track in track_list:
        y_loc = track[1]+track[3]
        lane_w = (y_loc*2.841-1170.0)/3.0
        if lane_w < 96:
            lane_w = 96
        lane_h = lane_w/1.2
        lane_w = max(lane_w, track[2])
        xs = track[0]-lane_w
        xf = track[0]+lane_w
        if track[1] < Y_MIN:
            track[1] = Y_MIN
        ys = track[1]-lane_h
        yf = track[1]+lane_h
        if xs < 0: xs=0
        if xf > 1280: xf=1280
        if ys < Y_MIN - 40: ys=Y_MIN - 40
        if yf > 720: yf=720
        size_sq = lane_w / (0.015*lane_w+0.3)
        scale = size_sq / 64.0
        # Apply multi scale image windows 
        boxes+=find_cars(img, int(ys), int(yf), int(xs), int(xf), scale, 2, color_space)
        boxes+=find_cars(img, int(ys), int(yf), int(xs), int(xf), scale*1.25, 2, color_space)
        boxes+=find_cars(img, int(ys), int(yf), int(xs), int(xf), scale*1.5, 2, color_space)
        boxes+=find_cars(img, int(ys), int(yf), int(xs), int(xf), scale*1.75, 2, color_space)
    heat = add_heat(heat, boxes)
    heat_l = heat_p + heat
    heat_p = heat
    heat_l = apply_threshold(heat_l, THRES)
    heatmap = np.clip(heat_l, 0, 255)
    labels = label(heatmap)
    cars_boxes = draw_labeled_bboxes(labels)
    boxes_p = cars_boxes 
    imp = draw_boxes(np.copy(img), cars_boxes, color=(0, 0, 255), thick=6)
    n_count += 1
    return imp",0.4578122497,
2177,step test results,"def objective(x):
    vals = vision_benchmarks_final.Vision_Model().run(
        result_feedback = x['result_feedback']
        #compare_to_result_strength = x['compare_to_result_strength']
    )
    return {
        'loss': 
            abs(vals['AIT_to_PFC'] - 0.03) + abs(vals['PFC_to_PMC'] - 0.02) + abs(vals['PMC_to_MC'] - 0.025),
        
        'status': hyperopt.STATUS_OK,
    }
trials = Trials()
best = fmin(objective,
            space = {#'compare_to_result_strength': hp.uniform('compare_to_result_strength', 0, 1),
                     'result_feedback': hp.uniform('result_feedback', 0, 1)
                    },
            algo = rand.suggest,
            max_evals = 100,
            trials = trials
           )
pickle.dump({'Trials': trials, 'Best': best}, open ('result_feedback_data_postmotor', 'w'))",0.5260627866,
2177,step test results,"def present_test_result(fitted_model, X_test):
    """"""
    Presenting prediction performance on test data using the fitted model
    """"""
    y_predicted = fitted_model.predict(X_test)
    print ""Accuracy:"", metrics.accuracy_score(y_test, y_predicted)
    print 
    print metrics.classification_report(y_test, y_predicted)
    print
    print ""Confusion Matrix:""
    print
    print pd.DataFrame(metrics.confusion_matrix(y_test, y_predicted))",0.5189282298,
2177,step test results,"def save_results():
    '''
    <ARGUMENTS>
        # NONE
        
    <OUTPUT>
        # Training Data
          ==> loss_results, train_accuracy_results, valid_accuracy_results
          
        # Testing Data
          ==> test_accuracy_results 
          
    [Save Files - Training]
     - Loss, Accuracies
     1. If there's no result directory, then make it!
     2. If it is training_set, make csv files of loss and accuracies.
     3. If there's past results, delete it. 
     4. Write down the loss and accuracies header on csv files that made it before.
    '''

    # 1 
    if not exists(cfg.results):
        mkdir(cfg.results)

    # 2 ( When Training )
    if cfg.is_training:
        loss = cfg.results + '/loss.csv'
        train_accuracy = cfg.results + '/train_accuracy.csv'
        valid_accuracy = cfg.results + '/valid_accuracy.csv'

    # 3     
        if exists(loss):
            remove(loss)
            
        if exists(train_accuracy):
            remove(train_accuracy)    
        
        if exists(valid_accuracy):
            remove(valid_accuracy)
            
    # 4
        loss_results = open(loss, 'w')
        loss_results.write('step,Loss\n')
        
        train_accuracy_results = open(train_accuracy, 'w')
        train_accuracy_results.write('step,Train_Accuracy\n')
        
        valid_accuracy_results = open(valid_accuracy, 'w')
        valid_accuracy_results.write('step,Valid_Accuracy\n')
        
        return loss_results, train_accuracy_results, valid_accuracy_results
    
    else: # ( When Testing )
        '''
        [Save Files - Testing]
         - Loss, Accuracies
         1. If it is testing_set, make csv files of loss and accuracies.
         2. If there's past results, delete it. 
         3. Write down the loss and accuracies header on csv files that made it before.
        '''

        # 1
        test_accuracy = cfg.results + '/test_accuracy.csv'
        
        # 2
        if exists(test_accuracy):
            remove(test_accuracy)
            
        # 3    
        test_accuracy_results = open(test_accuracy, 'w')
        test_accuracy_results.write('test_accuracy\n')
        
        return test_accuracy_results",0.5187610984,
2177,step test results,"def episode_finished(r):
    print(""Finished episode {ep} after {ts} timesteps (reward: {reward})"".format(ep=r.episode, ts=r.episode_timestep,
                                                                                 reward=r.episode_rewards[-1]))
    return True",0.5167921782,
2177,step test results,"def cuisines(df):
    """"""
    takes a dataframe as input and outputs
    a> a dictionary of all the cuisine names with frequency of occurance
    b> a numpy array labeling the cuisine names with integer values 0,1,2...
    """"""    
    # number of labels or number of different cuisine types
    cuisine_count = df['cuisine'].describe()[1]
    
    # labeling the differet cuisines as 0,1,2...
    # which will be necessary during the classification problem
    # cuisine_labels = np.arange(cuisine_count)
    
    # determining the name of the cuisine types in the train data set
    cuisine_names = []
    for row in df.itertuples():
        cuisine = row[1]
        if cuisine not in cuisine_names:
            cuisine_names.append(cuisine)
    
    # determining the number of occurence of each cuisine type 
    # in the train data set
    count = np.zeros(cuisine_count,float)
    # we create a dictionary of cuisine names and corresponding count
    cuisines = {}
    for row in df.itertuples():
        for i in xrange(cuisine_count):
            if row[1] == cuisine_names[i]:
                count[i] += 1.0
                cuisines[row[1]] = count[i] 
    
    return cuisines, cuisines.keys()",0.5136291981,
2177,step test results,"# print classification results

def test_results(y_test, y_predicted):

    print '\nThe accuracy is: {0:4.2} ' \
    .format(accuracy_score(y_test, y_predicted))

    print '\nThe confusion matrix: '
    cm = confusion_matrix(y_test, y_predicted)
    print cm

    print '\nThe True Positive rate is: {0:4.2}' \
    .format(float(cm[1][1])/np.sum(cm[1]))

    print '\nThe Matthews correlation coefficient: {0:4.2f} \n' \
    .format(matthews_corrcoef(y_test, y_predicted))

    print(classification_report(y_test, y_predicted))",0.5132901073,
2177,step test results,"def evaluate_accuracy(model):
    global test_set
    
    # Allocate a Distributed Keras Accuracy evaluator.
    evaluator = AccuracyEvaluator(prediction_col=""prediction_index"", label_col=""label_index"")
    # Clear the prediction column from the testset.
    test_set = test_set.select(""features_normalized"", ""label_index"", ""label"")
    # Apply a prediction from a trained model.
    predictor = ModelPredictor(keras_model=trained_model, features_col=""features_normalized"")
    test_set = predictor.predict(test_set)
    # Allocate an index transformer.
    index_transformer = LabelIndexTransformer(output_dim=nb_classes)
    # Transform the prediction vector to an indexed label.
    test_set = index_transformer.transform(test_set)
    # Fetch the score.
    score = evaluator.evaluate(test_set)
    
    return score",0.5098130703,
2177,step test results,"def full_eval(network, baseline_mean, baseline_std):
    # Check performance at the point in training with the best dev performance
    network.restore(network.best_dev_tup)
    print 'At best Dev performance...'
    print '\nTrain accuracy (appoximate): '
    network.check_accuracy(data_dict['train'], 2000);
    print '\nDev accuracy: '
    network.check_accuracy(data_dict['dev']);
    print '\nTest accuracy: '
    network.check_accuracy(data_dict['test']);
    
    # Check the performance at the termination of training
    network.restore(network.end_of_train_tup)
    print '\n\nAt end of training...'
    print '\nTrain accuracy (appoximate): '
    network.check_accuracy(data_dict['train'], 2000);
    print '\nDev accuracy: '
    network.check_accuracy(data_dict['dev']);
    print '\nTest accuracy: '
    a = 100.*network.check_accuracy(data_dict['test']);

    # Compare to the goal
    print '\n\nProbability this score beats Sheng Tai baseline:'
    print '{}%'.format(
        100.*np.round(1000.*np.mean(np.random.normal(baseline_mean, baseline_std, 10000) < a))/1000.)",0.5097836256,
2177,step test results,"def print_examples(preds):
    print(""pronunciation"".ljust(40), ""real spelling"".ljust(17), 
          ""model spelling"".ljust(17), ""is correct"")

    for index in range(20):
        ps = ""-"".join([phonemes[p] for p in input_test[index]]) 
        real = [letters[l] for l in labels_test[index]] 
        predict = [letters[l] for l in preds[index]]
        print (ps.split(""-_"")[0].ljust(40), """".join(real).split(""_"")[0].ljust(17),
            """".join(predict).split(""_"")[0].ljust(17), str(real == predict))",0.5096934438,
2177,step test results,"##Measure accuracy of the net
def print_test_accuracy(test_batch_size, mnist):
    data=mnist 
    data.test.cls = np.argmax(data.test.labels, axis=1)
    #data.validation.cls = np.argmax(data.test.labels, axis=1)
    #data.train.cls = np.argmax(data.train.labels, axis=1)
    # Numero de imagenes en test-set.
    num_test = len(data.test.images)

    # Crea arreglo para guardar clases predichas.
    cls_pred = np.zeros(shape=num_test, dtype=np.int)

    # Calcular clases predichas.
    i = 0
    while i < num_test:
        
        j = min(i + test_batch_size, num_test)
        images = data.test.images[i:j, :]
        images = (2*images)-1
        labels = data.test.labels[i:j, :]
        dp=1
        feed_dict = {x: images,
                     y_: labels, keep_prob: dp}

        cls_pred[i:j] = sess.run(y_pred_cls, feed_dict=feed_dict)
        i = j
    
    # Labels reales.
    cls_true = data.test.cls

    # Arreglo booleano de clasificaciones correctas.
    correct = (cls_true == cls_pred)
    
    #Numero de clasificaciones correctas.
    correct_sum = correct.sum()

    # Accuracy
    acc = float(correct_sum) / num_test
    msg = ""Accuracy on Test-Set: {0:.1%} ({1} / {2})""
    print(msg.format(acc, correct_sum, num_test))
    return acc, cls_true,  cls_pred",0.509599328,
1398,part data pre processing,"def preparedata():
    # Drop date variable
    data = data.drop(['DATE'], 1)
    n = data.shape[0]
    # Dimensions of dataset
    p = data.shape[1]
    # Make data a np.array
    data = data.values
preparedata():
print(""FINISHED PREPARING THE DATA"")",0.4478774071,
1398,part data pre processing,"def preprocess_data(data):
    # This function flattens the data along the spatial dimensions. So size of data will
    # change from (numx, numy, numz, numt) to (numx*numy*numz, numt)
    
    #Parameters:
    #   1. The data to be flattened
    
    # Returns: Flattened data
    flattened_data = np.reshape(data,(np.prod(data.shape[:-1]), data.shape[3]))
    return flattened_data

def PCA_decomp(data, 
               pca_results_path, 
               indices_for_windows,
               min_variance_explained=0.8):
    # This function is called to do PCA decomposition. This checks if the PCA has already
    # been done by checking if a pickled file exists in pca_results_path. If this file
    # exists, it just loads those results. Otherwise, it performs the PCA decomposition.
    
    #Parameters:
    #   1. The data to perform PCA on. Shape: (numx, numy, numz, numt)
    #   2. pca_results_path: Path to where the PCA result would have been stored
    #                        if PCA has already been performed for this data.
    #   3. The minimum amount of variance that should be explained. The number of 
    #      PCs stored will be determined by this number.
    
    # Returns: Data in PCA space
    
    if os.path.isfile(pca_results_path): #has PCA already been done?
        return transform_from_loadedpca(data, pca_results_path)
    else:
        return perform_PCA_decomp(data, pca_results_path, indices_for_windows, min_variance_explained)

def transform_from_loadedpca(data, 
                             pca_results_path):
    # This function transforms inputted data based on stored PCA results for that data.
    
    #Parameters:
    #   1. The data on which PCA was performed.
    #   2. pca_results_path: Path to where the PCA result would have been stored
    #                        if PCA has already been performed for this data.
    
    # Returns: Data in PCA space
    transformed_data_path = os.path.join(results_directory, 'transformed_data.h5')
    if not os.path.isfile(transformed_data_path):
        transformed_data_handle = h5py.File(transformed_data_path,'x')
    transformed_data_handle = h5py.File(transformed_data_path,'r')
    if pca_results_path in transformed_data_handle:
        return transformed_data_handle[pca_results_path]
    else:
        transformed_data_handle.close()
        transformed_data_handle = h5py.File(transformed_data_path,'a')
        return perform_transformation(data, pca_results_path, transformed_data_handle)    
    
def perform_transformation(data,
                           pca_results_path,
                           transformed_data_handle):
    flattened_data = preprocess_data(data)
    pca = load_calculated_pca(pca_results_path)
    compressed_data = pca.transform(flattened_data.T).T #transform back to shape n_components x n_timepoints
    transformed_data_handle.create_dataset(pca_results_path, data=compressed_data)
    transformed_data_handle.close()
    return compressed_data

def perform_PCA_decomp(data, 
                       pca_results_path, 
                       indices_for_windows,
                       min_variance_explained=0.8):
    # This function performs the PCA decomposition. This is called only if 
    # there aren't any results from a previous run stored in pca_results_path.
    
    #Parameters:
    #   1. The data to perform PCA on. Shape: (numx, numy, numz, numt)
    #   2. pca_results_path: Path to where the PCA result should be stored.
    #   3. The minimum amount of variance that should be explained. The number of 
    #      PCs stored will be determined by this number.
    
    # Returns: Data in PCA space
    
    flattened_data = preprocess_data(data)
    pca = PCA(n_components=min_variance_explained)
    pca.fit(flattened_data.T) 
    compressed_data = pca.transform(flattened_data.T).T #transform back to shape n_components x n_timepoints
    pca, compressed_data = standardize_pca_sign(pca, compressed_data, indices_for_windows)
    joblib.dump(pca, pca_results_path)
    return compressed_data

def load_calculated_pca(pca_results_path):
    # This function loads the calculated PCA object.
    
    #Parameters:
    #   1. pca_results_path: Path to where the PCA result would have been stored
    #                        if PCA has already been performed for this data.
    
    # Returns: sklearn PCA object
    pca = joblib.load(pca_results_path)
    return pca

def standardize_pca_sign(pca, 
                         compressed_data,
                         indices_for_windows,
                         criterion='positivestimresponse'):
    # The PCs are only defined upto a negative sign, i.e. a 180 degree rotated PC vector
    # is equivalently a PC vector. This function prevents this ambiguity by enforcing
    # the sign of each PC to be such that the derivative of the trace is positive at the 
    # onset of the first stimulation. Other forms of standardization could also be used.  
    # Obviously, this works only for this current experiment with stimulation. In case
    # there is no stimulation, you could set the criterion to ""positiveslope"" in which 
    # case the function will ensure that the PC's trace has a positive linear trend 
    # through the recording.
    
    #Parameters:
    #   1. The sklearn PCA object.
    #   2. compressed_data: Data in PCA space
    #   3. indices for windows. Explained above
    #   4. criterion for standardization. Set to positivestimresponse to ensure that
    #      the PCs have a positive stimulation response.
    
    # Returns: Sign standardized input parameters
    for pc in range(compressed_data.shape[0]):
        trace = compressed_data[pc,:].T
        if criterion=='positivestimresponse':
            if np.diff(trace)[indices_for_windows[1]]<0:
                compressed_data[pc,:] = -compressed_data[pc,:]
                pca.components_[pc,:] = -pca.components_[pc,:]  
        elif criterion=='positiveslope':
            time = np.arange(trace.shape[0]).T
            time = sm.add_constant(time)
            lm = sm.OLS(trace, time).fit()
            if lm.params[1] < 0: #if slope < 0, flip the PC vector and the trace
                compressed_data[pc,:] = -compressed_data[pc,:]
                pca.components_[pc,:] = -pca.components_[pc,:]
    return pca, compressed_data

def extract_pc_vectors(pca_results_path, 
                       (numx, numy, numz)):
    # This function is used to extract the PC vectors from the stored results 
    # in pca_results_path and reshapes them to the original voxel tiling.
    
    #Parameters:
    #   1. PCA results path
    
    # Returns: pca_vectors
    
    pca = load_calculated_pca(pca_results_path)
    pca_vectors = np.reshape(pca.components_.T, (numx,numy,numz,pca.components_.shape[0]))
    return pca_vectors

def plot_variance_explained_per_pc(pca_results_path,
                                   fig=None,
                                   ax=None,
                                   label=''):
    # This function plots the % of variance explained by each PC.
    
    #Parameters:
    #   1. PCA results path
    #   2. Figure handle. Optional. Useful if you want to layer plots
    #      across all conditions
    #   3. Axis handle. Optional. Same as above.
    #   4. Label for the plot. Will be set to a condition when called later.
    
    # Returns: Figure and axis handle to the plot   
    pca = load_calculated_pca(pca_results_path)
    if fig is None or ax is None:
        fig,ax = plt.subplots()
    ax.plot(100*pca.explained_variance_ratio_, '.-', label=label)
    ax.set_ylabel('% of variance explained')
    ax.set_xlabel('PC number')
    return fig, ax

def plot_pc_vectors(pca_results_path,
                    (numx, numy, numz),
                    pc_of_interest):
    # This function plots the PC vector for pc_of_interest based on
    # the results stored in pca_results_path.
    # It returns a figure handle for this plot. One could then iterate over all PCs
    # to save the figures to your path of choice.
    
    #Parameters:
    #   1. PCA results path
    #   2. Spatial shape of the data
    #   3. pc_of_interest
    
    # Returns: Figure handle to the plot.
    
    pca_vectors = extract_pc_vectors(pca_results_path, (numx, numy, numz))
    
    fig, axs = plt.subplots(3, 4)
    vmax = np.amax(pca_vectors[:,:,:,pc_of_interest])
    vmin = np.amin(pca_vectors[:,:,:,pc_of_interest])
    vmaxsymmetric = np.maximum(np.abs(vmax),np.abs(vmin))
    vminsymmetric = -np.maximum(np.abs(vmax),np.abs(vmin))

    temp = np.swapaxes(pca_vectors,0,1) #For making the plot
    for ax, zplane in zip(axs.flat, range(0,numz)):
        ax.matshow(temp[:,:,zplane,pc_of_interest],
                   vmin=vminsymmetric,
                   vmax=vmaxsymmetric,
                   cmap=plt.get_cmap('seismic'))
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
    fig.tight_layout()
        
    return fig

        
def plot_pc_traces(data_in_pcaspace, pc_of_interest):  
    # This function plots the PC trace associated with pc_of_interest
    # given the data in pca space.
    # Calling PCA_decomp() returns the data in pca space. 
    # This function calculates a z-score of all the traces.
    # So each trace is normalized within itself.
    # Hence, note that this function wouldn't be appropriate to compare two traces
    # since their magnitudes are normalized within themselves, rather than between
    # them.
    
    #Parameters:
    #   1. Data in PCA space
    
    # Returns: figure handle to the plot.
    
    temp = (data_in_pcaspace[pc_of_interest,:])
    baseline = np.mean(temp[:indices_for_windows[1]])
    ztrace = (temp-np.mean(temp))/np.std(temp)#-np.log(temp/baseline) #=R2* multiplied by TE
    fig, ax = plt.subplots()
    sns.tsplot(ztrace, ax=ax)
    ax.set_ylabel('z-score PC signal (score)')
    ax.set_xlabel('Time (s)')
    fig.tight_layout()
        
    return fig",0.441185534,
1398,part data pre processing,"def preprocess_data(data):
    # This function flattens the data along the spatial dimensions. So size of data will
    # change from (numx, numy, numz, numt) to (numx*numy*numz, numt)
    
    #Parameters:
    #   1. The data to be flattened
    
    # Returns: Flattened data
    flattened_data = np.reshape(data,(np.prod(data.shape[:-1]), data.shape[3]))
    return flattened_data

def PCA_decomp(data, 
               pca_results_path,
               indices_for_windows,
               min_variance_explained=0.8):
    # This function is called to do PCA decomposition. This checks if the PCA has already
    # been done by checking if a pickled file exists in pca_results_path. If this file
    # exists, it just loads those results. Otherwise, it performs the PCA decomposition.
    
    #Parameters:
    #   1. The data to perform PCA on. Shape: (numx, numy, numz, numt)
    #   2. pca_results_path: Path to where the PCA result would have been stored
    #                        if PCA has already been performed for this data.
    #   3. The minimum amount of variance that should be explained. The number of 
    #      PCs stored will be determined by this number.
    
    # Returns: Data in PCA space
    
    if os.path.isfile(pca_results_path): #has PCA already been done?
        return transform_from_loadedpca(data, pca_results_path)
    else:
        return perform_PCA_decomp(data, pca_results_path, indices_for_windows, min_variance_explained)

def transform_from_loadedpca(data, 
                             pca_results_path):
    # This function transforms inputted data based on stored PCA results for that data.
    
    #Parameters:
    #   1. The data on which PCA was performed.
    #   2. pca_results_path: Path to where the PCA result would have been stored
    #                        if PCA has already been performed for this data.
    
    # Returns: Data in PCA space
    
    flattened_data = preprocess_data(data)
    pca = load_calculated_pca(pca_results_path)
    compressed_data = pca.transform(flattened_data.T).T #transform back to shape n_components x n_timepoints
    return compressed_data

def perform_PCA_decomp(data, 
                       pca_results_path,
                       indices_for_windows,
                       min_variance_explained=0.8):
    # This function performs the PCA decomposition. This is called only if 
    # there aren't any results from a previous run stored in pca_results_path.
    
    #Parameters:
    #   1. The data to perform PCA on. Shape: (numx, numy, numz, numt)
    #   2. pca_results_path: Path to where the PCA result should be stored.
    #   3. The minimum amount of variance that should be explained. The number of 
    #      PCs stored will be determined by this number.
    
    # Returns: Data in PCA space
    
    flattened_data = preprocess_data(data)
    pca = PCA(n_components=min_variance_explained)
    pca.fit(flattened_data.T) 
    compressed_data = pca.transform(flattened_data.T).T #transform back to shape n_components x n_timepoints
    pca, compressed_data = standardize_pca_sign(pca, compressed_data,indices_for_windows)
    joblib.dump(pca, pca_results_path)
    return compressed_data

def load_calculated_pca(pca_results_path):
    # This function loads the calculated PCA object.
    
    #Parameters:
    #   1. pca_results_path: Path to where the PCA result would have been stored
    #                        if PCA has already been performed for this data.
    
    # Returns: sklearn PCA object
    pca = joblib.load(pca_results_path)
    return pca

def standardize_pca_sign(pca, 
                         compressed_data,
                         indices_for_windows,
                         criterion='positivestimresponse'):
    # The PCs are only defined upto a negative sign, i.e. a 180 degree rotated PC vector
    # is equivalently a PC vector. This function prevents this ambiguity by enforcing
    # the sign of each PC to be such that the derivative of the trace is positive at the 
    # onset of the first stimulation. Other forms of standardization could also be used.  
    # Obviously, this works only for this current experiment with stimulation. In case
    # there is no stimulation, you could set the criterion to ""positiveslope"" in which 
    # case the function will ensure that the PC's trace has a positive linear trend 
    # through the recording.
    
    #Parameters:
    #   1. The sklearn PCA object.
    #   2. compressed_data: Data in PCA space
    #   3. indices for windows. Explained above
    #   4. criterion for standardization. Set to positivestimresponse to ensure that
    #      the PCs have a positive stimulation response.
    
    # Returns: Sign standardized input parameters
    for pc in range(compressed_data.shape[0]):
        trace = compressed_data[pc,:].T
        if criterion=='positivestimresponse':
            if np.diff(trace)[indices_for_windows[1]]<0:
                compressed_data[pc,:] = -compressed_data[pc,:]
                pca.components_[pc,:] = -pca.components_[pc,:]  
        elif criterion=='positiveslope':
            time = np.arange(trace.shape[0]).T
            time = sm.add_constant(time)
            lm = sm.OLS(trace, time).fit()
            if lm.params[1] < 0: #if slope < 0, flip the PC vector and the trace
                compressed_data[pc,:] = -compressed_data[pc,:]
                pca.components_[pc,:] = -pca.components_[pc,:]
    return pca, compressed_data

def extract_pc_vectors(pca_results_path, 
                       (numx, numy, numz)):
    # This function is used to extract the PC vectors from the stored results 
    # in pca_results_path and reshapes them to the original voxel tiling.
    
    #Parameters:
    #   1. PCA results path
    
    # Returns: pca_vectors
    
    pca = load_calculated_pca(pca_results_path)
    pca_vectors = np.reshape(pca.components_.T, (numx,numy,numz,pca.components_.shape[0]))
    return pca_vectors

def plot_variance_explained_per_pc(pca_results_path,
                                   fig=None,
                                   ax=None,
                                   label='',
                                   numpcs=None):
    # This function plots the % of variance explained by each PC.
    
    #Parameters:
    #   1. PCA results path
    #   2. Figure handle. Optional. Useful if you want to layer plots
    #      across all conditions
    #   3. Axis handle. Optional. Same as above.
    #   4. Label for the plot. Will be set to a condition when called later.
    #   5. Number of PCs to show in the plot. All if set to None
    
    # Returns: Figure and axis handle to the plot   
    pca = load_calculated_pca(pca_results_path)
    if fig is None or ax is None:
        fig,ax = plt.subplots()
    if numpcs is None:
        temp = 100*pca.explained_variance_ratio_
    else:
        temp = 100*pca.explained_variance_ratio_[:numpcs]
    ax.plot(temp, '.-', label=label)
    ax.set_ylabel('% of variance explained')
    ax.set_xlabel('PC number')
    ax.set_ylim((0,7))
    return fig, ax

def plot_pc_vectors(pca_results_path,
                    (numx, numy, numz),
                    pc_of_interest):
    # This function plots the PC vector for pc_of_interest based on
    # the results stored in pca_results_path.
    # It returns a figure handle for this plot. One could then iterate over all PCs
    # to save the figures to your path of choice.
    
    #Parameters:
    #   1. PCA results path
    #   2. Spatial shape of the data
    #   3. pc_of_interest
    
    # Returns: Figure handle to the plot.
    
    pca_vectors = extract_pc_vectors(pca_results_path, (numx, numy, numz))
    
    fig, axs = plt.subplots(3, 4)
    vmax = np.amax(pca_vectors[:,:,:,pc_of_interest])
    vmin = np.amin(pca_vectors[:,:,:,pc_of_interest])
    vmaxsymmetric = np.maximum(np.abs(vmax),np.abs(vmin))
    vminsymmetric = -np.maximum(np.abs(vmax),np.abs(vmin))

    temp = np.swapaxes(pca_vectors,0,1) #For making the plot
    for ax, zplane in zip(axs.flat, range(0,numz)):
        ax.matshow(temp[:,:,zplane,pc_of_interest],
                   vmin=vminsymmetric,
                   vmax=vmaxsymmetric,
                   cmap=plt.get_cmap('seismic'))
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
    fig.tight_layout()
        
    return fig

        
def plot_pc_traces(data_in_pcaspace, pc_of_interest):  
    # This function plots the PC trace associated with pc_of_interest
    # given the data in pca space.
    # Calling PCA_decomp() returns the data in pca space. 
    # This function calculates a z-score of all the traces.
    # So each trace is normalized within itself.
    # Hence, note that this function wouldn't be appropriate to compare two traces
    # since their magnitudes are normalized within themselves, rather than between
    # them.
    
    #Parameters:
    #   1. Data in PCA space
    
    # Returns: figure handle to the plot.
    
    temp = (data_in_pcaspace[pc_of_interest,:])
    baseline = np.mean(temp[:indices_for_windows[1]])
    ztrace = temp
    fig, ax = plt.subplots()
    sns.tsplot(ztrace, ax=ax)
    ax.set_ylabel('PC signal (score)')
    ax.set_xlabel('Time (s)')
    fig.tight_layout()
        
    return fig",0.441185534,
1398,part data pre processing,"def preprocess_images(imgs): # works for either a single image or multiple images
    sample_img = imgs if len(imgs.shape) == 2 else imgs[0]
    assert sample_img.shape in [(28, 28, 1), (28, 28)], sample_img.shape # make sure images are 28x28 and single-channel (grayscale)
    return ??? # normalize to [0,1] here

train_images = ???
test_images = ???",0.4338543415,
1398,part data pre processing,"def process_data(data,var_id,plt_title):
    # This  function works with daily averaged data, preparing it for the h5 file
    # NOTE: for TAVG its assumes that TMIN and TMAX are not missing
    print plt_title
    var=data[var_id]
    plt.subplot(311)
    plt.hist(var); plt.title(plt_title)
    n=len(var)
    print 'data legth:',n
    print 'min:',min(var), ', max=', max(var), \
                    ', mean=', (""%0.2f""%np.mean(var))

    if var_id=='TAVG':
        #if histograms shows that we have -9999 for TAVG, then we substitute these numbers with (TMIN+TMAX)/2
        data.loc[data['TAVG']==-9999,'TAVG']= \
                           (data.loc[data['TAVG']==-9999,'TMIN'] \
                           +data.loc[data['TAVG']==-9999,'TMAX'])/2
        var=data['TAVG']
    else:
        #If data has a negative values we going to zero them out 
        data.loc[var<0,var_id]=0
         
    plt.subplot(312)
    plt.hist(var); plt.title('Corrected '+plt_title)
    
    #convert data units
    if var_id=='AWND':
        plt_title=r'Average wind [AWND] (m/s)'
        var=var*1609.34/3600 #mph to mps
    
    if var_id=='PRCP':
        plt_title=r'Precipitation (m/s)'
        var=var*0.0254/86400 #inch per day to mm per second    
        
    elif var_id=='TAVG':
        plt_title=r'TAVG (K)'
        var=(var + 459.67) * 5/9 # F to K
    
    elif var_id=='SNOW':
        plt_title=r'Snowfall (m SWE /s)'
        var=var*0.0254/864000 #inch per day to mm per second/ 10 SWE
         
    plt.subplot(313)
    time=range(0,n)
    plt.xlabel('Days'), plt.ylabel(plt_title)
    z = np.polyfit(time, var, 1); poly = np.poly1d(z); y = poly(time)    
    print 'slope:',(""%0.4f""%z[0]),', intersection:',(""%0.2f""%z[1])
    plt.plot(time,var,'b',time,y,'r'); plt.title(plt_title)
    plt.tight_layout()
    plt.show()
    
    return var",0.428684175,
1398,part data pre processing,"pre_data = [[], [], [], []]

def process_image(image):
    global pre_data
    #### highlight lanes in selected region ####
    regular_median_color = 80    # median color for a image with excellent visibility conditions
    d_factor = 25                # factor to decide the effect of overall median color on the thresholds
    red_threshold = 220 - (regular_median_color - np.median(image[:,:, 0]))/d_factor
    green_threshold = 220 - (regular_median_color - np.median(image[:,:, 1]))/d_factor
    blue_threshold = 220 - (regular_median_color - np.median(image[:,:, 2]))/d_factor

    color_thresholds = (image[:,:, 0] > red_threshold)\
                    | (image[:,:, 1] > green_threshold)\
                    | (image[:,:, 2] > blue_threshold)
    
    #### create a mask for selected region ####
    # Pull out the x and y sizes
    ysize = image.shape[0]
    xsize = image.shape[1]

    # Define a triangle region of interest
    left_bottom = [50, ysize-20]
    right_bottom = [xsize-50, ysize-20]
    apex = [xsize//2, ysize//2]

    # Perform a linear fit (y=Ax+B) to each of the three sides of the triangle
    # np.polyfit returns the coefficients [A, B] of the fit
    fit_left = np.polyfit((left_bottom[0], apex[0]), (left_bottom[1], apex[1]), 1)
    fit_right = np.polyfit((right_bottom[0], apex[0]), (right_bottom[1], apex[1]), 1)
    fit_bottom = np.polyfit((left_bottom[0], right_bottom[0]), (left_bottom[1], right_bottom[1]), 1)

    # Find the region inside the lines
    XX, YY = np.meshgrid(np.arange(0, xsize), np.arange(0, ysize))
    region_thresholds = (YY > (XX*fit_left[0] + fit_left[1])) & \
                        (YY > (XX*fit_right[0] + fit_right[1])) & \
                        (YY < (XX*fit_bottom[0] + fit_bottom[1]))
    
    lane_select = np.copy(image)
    lane_select[~color_thresholds | ~region_thresholds] = [0, 0, 0]
    
    #### soften this image ####
    kernel_size = 11
    blur_image = gaussian_blur(lane_select, kernel_size)
    edges = auto_threshold_canny(image, blur_image)
    
    #### apply hough transform on edge image to get highlighted lane lines for selected region ####
    rho = 1  # distance resolution in pixels of the hough grid
    theta = np.pi/180  # angular resolution in hough grid
    threshold = 20     # minimum number of votes (intersections in hough grid)
    min_line_len = 10  # minimum number of pixels making up a line
    max_line_gap = 250    # maximum gap in pixels between connectable line segments
    line_image = np.copy(image)   
    line_only_image = np.copy(image)*0   # creating a blank to draw lines on
    # Output ""lines"" containing endpoints of detected line segments
    lines = cv2.HoughLinesP(edges, rho, theta, threshold, np.array([]), min_line_len, max_line_gap)

    #### mask out all the colors from the hough transformed image except lane lines ####
    # uncomment below line to see raw lines draw on orignal video
    #return draw_lines(line_image, lines)
    # find positive and negative slope lines and get average positive and negative slopes respectively
    pm = []    # list of positive slopes
    pc = []    # list of intercepts of lines with positive slopes
    pl = []    # list of lengths of lines with positive slopes
    nm = []    # list of negative slopes
    nc = []    # list of intercepts of lines with negative slopes
    nl = []    # list of lengths of lines with negative slopes

    # get line details in lists
    for line in lines:
        for x1, y1, x2, y2 in line:
            m = (y2-y1)/(x2-x1)
            c = y1 - m*x1
            l = math.sqrt(((y2-y1)**2) + ((x2-x1)**2))
            if m < 0:
                nm.append(m)
                nc.append(c)
                nl.append(l)
            elif m > 0:
                pm.append(m)
                pc.append(c)
                pl.append(l)

    # get the average of slopes and intercepts weighted by lengths
    if sum(pl) != 0:
        a_pm = np.average(pm, weights=pl)
        a_pc = np.average(pc, weights=pl)
    else:
        a_pm = np.average(pm)
        a_pc = np.average(pc)
    
    if sum(nl) != 0:
        a_nm = np.average(nm, weights=nl)
        a_nc = np.average(nc, weights=nl)
    else:
        a_nm = np.average(nm)
        a_nc = np.average(nc)
    # create two extrapolated lines
    lines = []
    #check if current data is valid
    data_valid_flag = True
    if math.isnan(a_pm) | math.isnan(a_pc) | math.isnan(a_nm) | math.isnan(a_nc):
        data_valid_flag = False
    
    if data_valid_flag:
        if len(pre_data[0]) > 5:
            pre_data[0].pop(0)
            pre_data[1].pop(0)
            pre_data[2].pop(0)
            pre_data[3].pop(0)

        pre_data[0].append(a_pm)
        pre_data[1].append(a_pc)
        pre_data[2].append(a_nm)
        pre_data[3].append(a_nc)
    #calculate weights for previous frames
    wt = []
    for v in range(1, len(pre_data[0])+1):
        wt.append(v)
    #wt[-1] = wt[-1]*2
    # prepare data
    line_data = [[np.average(pre_data[0], weights=wt), np.average(pre_data[1], weights=wt)],\
                 [np.average(pre_data[2], weights=wt), np.average(pre_data[3], weights=wt)]]
     
    for i in range(2):
        m = line_data[i][0]
        c = line_data[i][1]
        y1 = ysize
        x1 = int((y1 - c)/m)
        y2 = y1-(ysize//3)-30
        x2 = int((y2 - c)/m)
        line = [[x1, y1, x2, y2]]
        lines.append(line)
    
    # draw lines on original image
    draw_lines(line_only_image, lines, thickness=15)
    result = weighted_img(image, line_only_image)
    return result",0.4283666313,
1398,part data pre processing,"def preprocess_data(data, labels):
    """"""
    Flattens the each image in the data to be a one-dimensional feature vector and encodes the labels in one-hot encoding.

    Parameters
    ----------
    data : np.array[samples, width, height]
        the image dataset
    labels : np.array[samples]
        the corresponding labels

    Returns
    -------
    data_and_labels : tuple(np.array[samples, width * height], np.array[samples, classes])
        a tuple with two numpy array containing the flattened data and one-hot encoded labels
    """"""
    ##############
    return the flattened images and labels
##############

preprocessed_data, preprocessed_labels = preprocess_data(data, labels)",0.4213905931,
1398,part data pre processing,"def crear_calendario(temp, path):
    
    global partidos_df # Use the defined empty dataframe
    page_soup = BS(uOpen(path).read(), 'html.parser')
    rounds = page_soup.find_all('div',{'class': 'jornada calendarioInternacional'})
    
    for r in rounds:
          
        rnd = r.caption.text        # Get the name of the round i.e. Jornada 1
        matches = r.findAll('tr')   # Find all the matches in that round
        
        for j, match in enumerate(matches[1:]):
            
            loc  = match.find('td', {'class': 'local'}).span.text
            away = match.find('td', {'class': 'visitante'}).span.text
            loc, away = limpiar_nombre(loc), limpiar_nombre(away)        
            loc, away = buscar_equivalencia(loc), buscar_equivalencia(away)
            
            res = pd.DataFrame([[rnd, j+1, loc, away]], columns=list(partidos_df))
            partidos_df = partidos_df.append(res)
        
    partidos_df = partidos_df.reset_index()
    partidos_df.drop('index', axis=1, inplace=True)
    

crear_calendario('2018-2019', m_url)",0.4204567075,
1398,part data pre processing,"def remove_stopword(self):
    """"""
    This function needs to import stopwords.
    Stopwords can be added or removed.
    """"""
    self.final_list = self.lower_corpus_list.copy()

    for word in self.lower_corpus_list:
        if word in stopwords:
            self.final_list.remove(word)

    return self.final_list",0.4198163748,
1398,part data pre processing,"def create_tfidf(data,my_preprocessor):
    print ""-----------------------------------""
    print ""#########  CREATE TFIDF MATRIX  ###### ""
    m = TfidfVectorizer(preprocessor=my_preprocessor)
    tfidf_matrix = m.fit_transform(data)
    tfidf_matrix = tfidf_matrix.toarray()
    print ""Taille de tfidf_matrix : "",tfidf_matrix.shape
    return tfidf_matrix",0.4187246561,
2059,step delete the column unnamed and id,"def drop_cols(colnames):
    for df in [X_train, X_val, X_test, X]:
        df.drop(colnames, axis=1, inplace=True)
drop_cols('customerID')
X_train.tail()",0.5048521757,
2059,step delete the column unnamed and id,"# reverse beta parameters for shock and loss to match gain
def reverseBeta(model_param_df):
    model_param_df.loc[(model_param_df.task == 'shock') & (model_param_df.parameter == 'mag_diff_rl'), 'beta'] = model_param_df.loc[(model_param_df.task == 'shock')& (model_param_df.parameter == 'mag_diff_rl'),'beta']*-1
    model_param_df.loc[(model_param_df.task == 'shock') & (model_param_df.parameter == 'prob_diff_rl'), 'beta'] = model_param_df.loc[(model_param_df.task == 'shock') & (model_param_df.parameter == 'prob_diff_rl'),'beta']*-1
    model_param_df.loc[(model_param_df.task == 'loss') & (model_param_df.parameter == 'prob_diff_rl'), 'beta'] = model_param_df.loc[(model_param_df.task == 'loss') & (model_param_df.parameter == 'prob_diff_rl'),'beta']*-1
    return(model_param_df)",0.5028107166,
2059,step delete the column unnamed and id,"def drop_columns(df):
    #drop categorical columns
    df.drop(['Sex','Embarked','Name','Ticket', 'Pclass'],axis=1,inplace=True)
    # PassengerId is just the index+1
    df.drop('PassengerId', axis=1, inplace=True)",0.4858961403,
2059,step delete the column unnamed and id,"# Remove the channels we don't want 
def Remove_Channels(df):
    df.drop(df.columns[[15,16,20,21,22,25,26,27,28,29,30,31,32]], axis=1, inplace=True)
    return df",0.4844681323,
2059,step delete the column unnamed and id,"def clean_titanic_df(df):
    # just to avoir issue
    if 'Ticket' in df.columns:
        df.drop(['PassengerId'], axis=1, inplace=True)
        
        # next keep Name ! Use Mrs. Miss. and Mr.
        df.loc[df['Name'].str.contains('Mrs.'), 'Name'] = 'Mrs'
        df.loc[df['Name'].str.contains('Mme.'), 'Name'] = 'Mrs'
        df.loc[df['Name'].str.contains('Mr.'), 'Name'] = 'Mr'
        df.loc[df['Name'].str.contains('Mlle.'), 'Name'] = 'Miss'
        df.loc[df['Name'].str.contains('Ms.'), 'Name'] = 'Miss'
        df.loc[df['Name'].str.contains('Miss.'), 'Name'] = 'Miss'
        
        # Important states ?
        df.loc[df['Name'].str.contains('Master.'), 'Name'] = 'Master'
        df.loc[df['Name'].str.contains('Rev.'), 'Name'] = 'Rev'
        df.loc[df['Name'].str.contains('Dr.'), 'Name'] = 'Dr'
        df.loc[df['Name'].str.contains('Don.'), 'Name'] = 'Don'
        df.loc[df['Name'].str.contains('Countess.'), 'Name'] = 'Countess'
        df.loc[df['Name'].str.contains('Jonkheer.'), 'Name'] = 'Jonkheer'
        df.loc[df['Name'].str.contains('Col.'), 'Name'] = 'Col'
        
        # Boat states ?
        df.loc[df['Name'].str.contains('Major.'), 'Name'] = 'Boat'
        df.loc[df['Name'].str.contains('Capt.'), 'Name'] = 'Boat'
        
    df['Embarked'].fillna(method='ffill', inplace=True)
    df['Age'].fillna(df['Age'].mean(), inplace=True)
    df['Fare'].fillna(df['Fare'].mean(), inplace=True)
    df['Cabin'].fillna('Unknown', inplace=True)
    df['Ticket'].fillna('Unknown', inplace=True)

clean_titanic_df(titanic_train)
clean_titanic_df(titanic_test)
clean_titanic_df(titanic)
titanic_train.isnull().any()",0.4835133851,
2059,step delete the column unnamed and id,"def top_k_users(user_number, k):
    # no need to transpose the matrix this time because the rows already represent users
    # remove the active user
    df_sim = dense_matrix.loc[dense_matrix.index != user_number]
    # calculate the distance for between the given user and each row
    df_sim[""distance""] = df_sim.apply(lambda x: euclidean(dense_matrix.loc[user_number], x), axis=1)
    # return the top k from the sorted distances
    return df_sim.sort(""distance"").head(k)[""distance""]",0.4834954441,
2059,step delete the column unnamed and id,"def drugname_lookup(ID):
    tmp = lookup.loc[lookup.ID == ID, 1].values
    if tmp.size > 0:
        drugname = tmp[0]
    else :
        drugname = np.nan
    return drugname",0.4834733009,
2059,step delete the column unnamed and id,"if train[""Fare""][0] != 0:
    train['Fare'].fillna(train['Fare'].dropna().mode()[0], inplace=True)
    fare = []
    for i in train[""Fare""]:
        if i <= 7.910400:
            fare = np.append(fare,0)
        elif i < 14.454200:
            fare = np.append(fare,1)
        elif i < 31.000000:
            fare = np.append(fare,2)
        else:
            fare = np.append(fare,3)
train[""Fare""] = fare = fare.astype(int)",0.4832573533,
2059,step delete the column unnamed and id,"def run_simulation(system):
    """"""Runs a proportional growth model.
    
    Adds TimeSeries to `system` as `results`.
    
    system: System object
    """"""
    results = TimeFrame(columns=system.pop0.index)
    results.loc[system.t0] = system.pop0
    
    
    temp = system.pop0
    
    for t in linrange(system.t0, system.t_end):
        maturations = system.mature_rate * temp.juveniles
        births = system.birth_rate * temp.adults
        deaths = system.death_rate * temp.adults
        
        if temp.adults > 30:
            market = temp.adults - 30
        else:
            market = 0
        
        temp.juveniles = temp.juveniles + births - maturations
        temp.adults = temp.adults + maturations - deaths - market
        
        results.loc[t+1] = temp     
    
    system.results = results",0.4828987718,
2059,step delete the column unnamed and id,"def missingEmbark(data):
    ModeEmb = data['Embarked'].mode()[0]
    data['Embarked'].loc[data['Embarked'].isnull()] = ModeEmb
    return(data)",0.4822001755,
268,"computing alpha, beta, and r squared in python","metrics.r2_score((x_q,y_q), (x_q,intercept + slope*x_q))",0.537470758,
268,"computing alpha, beta, and r squared in python",-f_star / (1 - lq.beta),0.5300936699,
268,"computing alpha, beta, and r squared in python","from scipy.special import beta as betafn
def beta(theta, alpha, beta):
    return (1/betafn(alpha, beta))*x**(alpha-1) * (1-x)**(beta-1)",0.5294340849,
268,"computing alpha, beta, and r squared in python","# plot the ACF for the betas and their residuals
DL.ACF_beta(beta_fits, residuals)",0.5256218314,
268,"computing alpha, beta, and r squared in python","from scipy.stats import beta

# probability of x less or equal 0.3
print(""P(X <0.3) = {:.3}"".format(beta.cdf(a=2, b=2, x=0.3)))

# probability of x in [-0.2, +0.2]
print(""P(-0.2 < X < 0.2) = {:.3}"".format(beta.cdf(a=2, b=2, x=0.2) - beta.cdf(a=2, b=2, x=-0.2)))",0.5252709389,
268,"computing alpha, beta, and r squared in python","stats.beta.cdf(0.2, a, b)",0.5224453211,
268,"computing alpha, beta, and r squared in python","print ""The market beta of the backtest is: "", empyrical.beta(bt_returns,benchmark_rets)",0.5195627809,
268,"computing alpha, beta, and r squared in python","# Using scipy
import scipy.stats as ss

n = 5
xs = [0.1, 0.5, 0.9]
rv = ss.beta(a=0.5, b=0.5)

print rv.pdf(xs) # equivalent of dbeta
print rv.cdf(xs) # equivalent of pbeta
print rv.ppf(xs) # equvialent of qbeta
print rv.rvs(n) # equivalent of rbeta",0.5180393457,
268,"computing alpha, beta, and r squared in python","a, b = 10, 10
prior = stats.beta(a, b)
post = stats.beta(heads+a, n-heads+b)
ci = post.interval(0.95)

xs = np.linspace(0, 1, 100)
plt.plot(prior.pdf(xs), label='Prior')
plt.plot(post.pdf(xs), label='Posterior')
plt.axvline(100*heads/n, c='red', alpha=0.4, label='MLE')
plt.xlim([0, 100])
plt.axhline(0.3, ci[0], ci[1], c='black', linewidth=2, label='95% CI');
plt.legend()
pass",0.5158128142,
268,"computing alpha, beta, and r squared in python","# Code up pdf function, log_likehood function and criterion function for optimization.
from scipy.special import beta
def gb2pdf(x ,a, b, p, q):
    return (a*(x**(a*p-1)))/(b**(a*p) * beta(p,q)*((1+(x/b)**a)**(p+q)))

def loglikegb2(xvals, a, b,p, q):
    pdf_vals = gb2pdf(xvals, a, b,p, q)
    log_pdf_vals = np.log(pdf_vals)
    log_lik_gamma = np.sum(log_pdf_vals)
    return log_lik_gamma

def crit3(params, xvals):
    a, b, p, q = params
    log_lik_val =  loglikegb2(xvals, a, b,p,q)
    neg_log_lik_val = - log_lik_val
    return neg_log_lik_val",0.5155084133,
310,convert text data into vector,"ratings = sc.textFile(""/home/saurabh/ml-1m/ratings.dat"").map(
                        lambda line: line.split(""::"")).map(lambda x: (int(x[0]), (int(x[1]), float(x[2]) )))",0.4727197587,
310,convert text data into vector,tokenizer.texts_to_sequences([x_train[15]]),0.4698423147,
310,convert text data into vector,"textFile1 = sc.textFile(""hdfs:///user/emma/svd/spark/BloomGridmetLeafGridmetCali3/U.csv"").map(lambda line: (line.split(','))).map(lambda m: [ float(i) for i in m]).collect()",0.4647399783,
310,convert text data into vector,"def split_string(verse):    
    return verse.split(' ')

lines = sc.textFile(""/user/pascepet/data/bible.txt"")
words = lines.flatMap(split_string)
pairs = words.map(lambda word: (word, 1))
counts = pairs.reduceByKey(lambda a, b: a + b)
counts.take(10)",0.4647040963,
310,convert text data into vector,"clayVecs = [w.vector for w in clayNouns]
clayLabels = [w.string.strip() for w in clayNouns]",0.4645026326,
310,convert text data into vector,"prideNounVecs = [word.vector for word in prideNouns]
prideNounLabels = [word.string.strip() for word in prideNouns]",0.4645026326,
310,convert text data into vector,"def process_tup(tup):
    return [float(ele) for ele in (tup.strip('()').split(','))]",0.4641312361,
310,convert text data into vector,"ratings = sc.textFile(""/home/saurabh/ml-1m/ratings.dat"").map(
                        lambda token: token.split(""::"")).map(lambda token: (
                                                            int(token[0]), int(token[1]), float(token[2])))
movies = sc.textFile(""/home/saurabh/ml-1m/movies.dat"").map(
                        lambda token: token.split(""::"")).map(lambda token: (token[0], token[1]))

ratings.take(3)",0.4638787508,
310,convert text data into vector,"trackfile = sc.textFile(""source_data/tracks.csv"")

def make_tracks_kv(str):
    l = str.split("","")
    return [l[1], [[int(l[2]), l[3], int(l[4]), l[5]]]]

# Make a k,v RDD out of the input data
tbycust = trackfile.map(lambda line: make_tracks_kv(line)).reduceByKey(lambda a,b: a+b)",0.46342206,
310,convert text data into vector,"# Load tracks.csv file
tracks_file = sc.textFile(""./raw/tracks.csv"")

def make_tracks_kv(str):
    line = str.split("","")
    # Create PairRDD with line is tuple data: key is line[1]: customer ID
    return [line[1], [[int(line[2]), line[3], int(line[4]), line[5]]]]

# Make a key,value RDD out of the input data
tracks_rdd = tracks_file.map(lambda line: make_tracks_kv(line)) \
                        .reduceByKey(lambda a,b: a+b)
tracks_rdd.first() 

#tracks_rdd = tracks_file.map(lambda line: line.split("","")) \
#                        .map(lambda p: [p[1], [[int(p[2]), p[3], int(p[4]), p[5]]]]) \
#                        .reduceByKey(lambda a,b: a+b)",0.46342206,
2202,step what is the number of observations in the dataset?,"def calculated_grossprofit_rdf(row):
    
    if row.Predictions_rdf == 1 and row.RESP==1:
        return 20
    elif row.Predictions_rdf == 1 and row.RESP==0:
        return -5
    elif row.Predictions_rdf == 0 and row.RESP==1:
        return 0
    else:
        return 0",0.4586341977,
2202,step what is the number of observations in the dataset?,"def predict(self, X):
    X_transformed = X
    for step in self.steps[:-1]:
        # iterate over all but the final step
        # transform the data
        X_transformed = step[1].transform(X_transformed)
    # fit the last step
    return self.steps[-1][1].predict(X_transformed)",0.458167851,
2202,step what is the number of observations in the dataset?,"def predict(self, X):
    X_transformed = X
    for step in self.steps[:-1]:
        # iterate over all but the final step
        # transform the data
        X_transformed = step[1].transform(X_transformed)
    # predict using the last step
    return self.steps[-1][1].predict(X_transformed)",0.458167851,
2202,step what is the number of observations in the dataset?,"def calculated_grossprofit_qda(row):
    
    if row.Predictions_qda == 1 and row.RESP==1:
        return 20
    elif row.Predictions_qda == 1 and row.RESP==0:
        return -5
    elif row.Predictions_qda == 0 and row.RESP==1:
        return 0
    else:
        return 0",0.4580158591,
2202,step what is the number of observations in the dataset?,"def get_mnist_image(ds):
    """"""
    Choose an image at random from the image dataset passed as parameter
    :param ds: dataset class (from MNIST input_data)
    :return ndarray [28, 28]
    """"""
    observation, label = ds.next_batch(1)
    return observation, label",0.4560734034,
2202,step what is the number of observations in the dataset?,"def gcv_score(results):
    X = results.model.exog[:-(q + 2), :]
    n = X.shape[0]
    y = results.model.endog[:n]
    
    y_hat = results.predict(X)
    hat_matrix_trace = results.get_influence().hat_matrix_diag[:n].sum()
    
    return n * np.power(y - y_hat, 2).sum() / np.power(n - hat_matrix_trace, 2)",0.4547813535,
2202,step what is the number of observations in the dataset?,"def visualizeBoundaryLinear(model):
    global ex1_df
    
    theta_0 = model.intercept_[0]
    theta_1 = model.coef_[0,0]
    theta_2 = model.coef_[0,1]
    x1 = ex1_df[""x1""]
    
    xRange = np.linspace(x1.min(), x1.max(), 100)
    decisionBoundary = -(theta_0 + theta_1*xRange) / theta_2
    
    # plotting
    plotData(ex1_df)
    plt.plot(xRange, decisionBoundary, color=""g"")
    plt.show()
    return

visualizeBoundaryLinear(linear_svc_c1)",0.4505448341,
2202,step what is the number of observations in the dataset?,"def show(batch, model):
    assert batch.batch_size == 1
    g_field = batch.dataset.fields['grapheme']
    p_field = batch.dataset.fields['phoneme']
    prediction = model(batch.grapheme).data.tolist()[:-1]
    grapheme = batch.grapheme.squeeze(1).data.tolist()[1:][::-1]
    phoneme = batch.phoneme.squeeze(1).data.tolist()[1:-1]
    print(""> {}\n= {}\n< {}\n"".format(
        ''.join([g_field.vocab.itos[g] for g in grapheme]),
        ' '.join([p_field.vocab.itos[p] for p in phoneme]),
        ' '.join([p_field.vocab.itos[p] for p in prediction])))",0.4504601955,
2202,step what is the number of observations in the dataset?,"# Counter for total number of iterations performed so far.
total_iterations = 0

def optimize(num_iterations):
    # Ensure we update the global variable rather than a local copy.
    global total_iterations

    for i in range(total_iterations,
                   total_iterations + num_iterations):

        # Get a batch of training examples.
        # x_batch now holds a batch of images and
        # y_true_batch are the true labels for those images.
        x_batch, y_true_batch = data.train.next_batch(train_batch_size)

        # Put the batch into a dict with the proper names
        # for placeholder variables in the TensorFlow graph.
        feed_dict_train = {x: x_batch,
                           y_true: y_true_batch}

        # Run the optimizer using this batch of training data.
        # TensorFlow assigns the variables in feed_dict_train
        # to the placeholder variables and then runs the optimizer.
        session.run(optimizer, feed_dict=feed_dict_train)

        # Print status every 100 iterations.
        if i % 100 == 0:
            # Calculate the accuracy on the training-set.
            acc = session.run(accuracy, feed_dict=feed_dict_train)

            # Message for printing.
            msg = ""Optimization Iteration: {0:>6}, Training Accuracy: {1:>6.1%}""

            # Print it.
            print(msg.format(i + 1, acc))

    # Update the total number of iterations performed.
    total_iterations += num_iterations",0.4496857226,
2202,step what is the number of observations in the dataset?,"def add_race(df):
    if (df.end_epoch >= race_start_dict[1]) and (df.end_epoch < race_start_dict[2]):
        return 1
    elif (df.end_epoch >= race_start_dict[2]) and (df.end_epoch < race_start_dict[3]):
        return 2
    elif (df.end_epoch >= race_start_dict[3]) and (df.end_epoch < race_start_dict[4]):
        return 3
    elif (df.end_epoch >= race_start_dict[4]) and (df.end_epoch < race_start_dict[5]):
        return 4
    elif (df.end_epoch >= race_start_dict[5]) and (df.end_epoch < 1448164800000):
        return 5
    else:
        return 0",0.4484687746,
1888,sentiment classification & how to frame problems for a neural network,"# Predictions for Train, Validation, and Test Sets
y_train_pred = model.predict_classes(X_train, verbose=0)
y_val_pred = model.predict_classes(X_val, verbose=0)
y_test_pred = model.predict_classes(X_test, verbose=0)
scores.append(['MLP (Multiple Hidden Layers)', 
               accuracy_score(y_true=y_train,y_pred=y_train_pred), 
               accuracy_score(y_true=y_val,y_pred=y_val_pred),
               accuracy_score(y_true=y_test,y_pred=y_test_pred)])

# Training Accuracy
print('Training Accuracy: %.3f'
      % accuracy_score(y_true=y_train,
                       y_pred=y_train_pred))

# Validation Accuracy
print('Validation Accuracy: %.3f'
      % accuracy_score(y_true=y_val,
                       y_pred=y_val_pred))

# Plotting confusion matrix
plot_confusion_matrix(labels=y_val, preds=y_val_pred)

# Plotting examples of correctly and incorrectly classified images
plot_pred_examples(imgs=X_val, labels=y_val, preds=y_val_pred)",0.6119096279,
1888,sentiment classification & how to frame problems for a neural network,"netc = nn.NeuralNetworkClassifier(X.shape[1], 0, len(np.unique(T)))
netc.train(Xtrain, Ttrain, 100)
print(netc)
print('T, Predicted')
Ytrain = netc.use(Xtrain)
Ytest = netc.use(Xtest)",0.6108072996,
1888,sentiment classification & how to frame problems for a neural network,"lr.fit(X_train.reshape(-1,1),y_train.reshape(len(y_train),))
B0, B1 = lr.intercept_[0], lr.coef_[0]",0.6092172861,
1888,sentiment classification & how to frame problems for a neural network,"# Predictions for Train, Validation, and Test Sets
y_train_pred = model.predict_classes(X_train, verbose=0)
y_val_pred = model.predict_classes(X_val, verbose=0)
y_test_pred = model.predict_classes(X_test, verbose=0)
scores.append(['MLP (Single Hidden Layer)', 
               accuracy_score(y_true=y_train,y_pred=y_train_pred), 
               accuracy_score(y_true=y_val,y_pred=y_val_pred),
               accuracy_score(y_true=y_test,y_pred=y_test_pred)])

# Training Accuracy
print('Training Accuracy: %.3f'
      % accuracy_score(y_true=y_train,
                       y_pred=y_train_pred))

# Validation Accuracy
print('Validation Accuracy: %.3f'
      % accuracy_score(y_true=y_val,
                       y_pred=y_val_pred))

# Plotting confusion matrix
plot_confusion_matrix(labels=y_val, preds=y_val_pred)

# Plotting examples of correctly and incorrectly classified images
plot_pred_examples(imgs=X_val, labels=y_val, preds=y_val_pred)",0.6070799828,
1888,sentiment classification & how to frame problems for a neural network,"#Now to compare my 3 classifiers
#Set up AdaBoostClassifier with the tuned parameter
AdaBoostCLF = AdaBoostClassifier(n_estimators = adavalue)
#use same training/validation set as used on Random forest
AdaBoostCLF.fit(fullData[:][-200:],targetData[:][-200:])
adaBoostScore = AdaBoostCLF.score(fullData[:][:200],targetData[:200])

#Now the same for K Nearest Neighbours with tuned parameter
KnnCLF = KNeighborsClassifier(n_neighbors = kNNvalue)
KnnCLF.fit(fullData[:][-200:],targetData[:][-200:])
knnScore = KnnCLF.score(fullData[:][:200],targetData[:200])

scoreList = [randomForestScore,AdaBoostScore,KnnScore]

#create a dataframe to compare results in a table
scoreTable = DataFrame(scoreList,['Random Forest','Ada Boost', 'K Nearest'],columns = ['Validation Score'])
print(scoreTable)",0.6064482331,
1888,sentiment classification & how to frame problems for a neural network,"# print intercept:
print(""Intercept: "",model.intercept_)",0.6064104438,
1888,sentiment classification & how to frame problems for a neural network,"print 'Estimated intercept coefficient:', lm.intercept_",0.6064104438,
1888,sentiment classification & how to frame problems for a neural network,"print(""intercept:"", model.intercept_)",0.6064104438,
1888,sentiment classification & how to frame problems for a neural network,"print('Estimated intercept coefficient:', lm.intercept_)",0.6064104438,
1888,sentiment classification & how to frame problems for a neural network,"print('Intercept:', lm.intercept_)",0.6064104438,
2041,step create a histogram of the top items bought,"def eval_model(model, data, start=0, end=12000):
    plt.plot(data.sqft_living, data.price, '.')
    mlg.add_to_plot(model, np.arange(start, end, 1))
    plt.title('Regression Model')
    plt.show()
eval_model(model, test)",0.5913984776,
2041,step create a histogram of the top items bought,"def engineer_features(df):
    price_max = np.percentile(df.price, 99)
    df.loc[df.price > price_max, 'price'] = price_max
    latitude_max = np.percentile(df.latitude, 99)
    latitude_min = np.percentile(df.latitude, 1)
    df.loc[df.latitude > latitude_max, 'latitude'] = latitude_max
    df.loc[df.latitude < latitude_min, 'latitude'] = latitude_min
    longitude_max = np.percentile(df.longitude, 99)
    longitude_min = np.percentile(df.longitude, 1)
    df.loc[df.longitude > longitude_max, 'longitude'] = longitude_max
    df.loc[df.longitude < longitude_min, 'longitude'] = longitude_min
    df[""num_photos""] = df.photos.apply(len)
    df[""num_features""] = df.features.apply(len)
    df[""num_description_words""] = df.description.apply(lambda x: len(x.split("" "")))
    df[""created""] = pd.to_datetime(df.created)
    df[""created_year""] = df.created.dt.year
    df[""created_month""] = df.created.dt.month
    df[""created_day""] = df.created.dt.day
    
    df['all_rooms'] = df['bathrooms'] + df['bedrooms']
    df['price_per_bed'] = df['price'] / df['bedrooms']    
    df['price_per_bath'] = df['price'] / df['bathrooms']
    df['price_per_room'] = df['price'] / df['all_rooms']

    df['price_per_bed'] = df['price_per_bed'].replace(np.Inf, 5000)
    df['price_per_bath'] = df['price_per_bath'].replace(np.Inf, 5000)
    df['price_per_room'] = df['price_per_room'].replace(np.Inf, 5000)
    
engineer_features(train_df)
engineer_features(to_test_df)",0.5575870872,
2041,step create a histogram of the top items bought,"def plotting(df):
    df.track_sold.plot.bar(title = ""Most Sold Genres in USA"")
    plt.show()
    df.percent_sold.plot.bar(title=""Most Sold Genres in USA by percentage"")
    plt.show()
    
plotting(best_sold_usa)",0.5575332642,
2041,step create a histogram of the top items bought,"# objective function
def Obj_fn(m):
    return sum((m.priceBuy[i]*m.posNetLoad[i]) + (m.priceSell[i]*m.negNetLoad[i]) for i in m.Time)  
m.total_cost = en.Objective(rule=Obj_fn,sense=en.minimize)",0.5523751378,
2041,step create a histogram of the top items bought,"def find_optimal(df):
    x = np.array(list(df.avg_unit_price))
    y = np.array(list(df.usd))
    x_df = pd.DataFrame(x)
    y_df = pd.DataFrame(y)
    degree = 3 # Set the degree of our polynomial
    # Generate the model type with make_pipeline. First step is to generate # degree polynomial features 
    # in the input features and then run a linear regression on the resulting features.
    est = make_pipeline(PolynomialFeatures(degree), LinearRegression())
    est.fit(x_df, y_df) # Fit our model to the training data
    range_min = x.mean() - 1*x.std()
    range_max = x.mean() + 1*x.std()
    x0 = np.arange(range_min, range_max, .05, dtype=None)
    x_df0 = pd.DataFrame(x0)
    pred0 = est.predict(x_df0)
    max_index = np.argmax(pred0)
    return x0[max_index]",0.5506049991,
2041,step create a histogram of the top items bought,"#print df    
    change_mean = Series(df['change']).mean()
    change_std = Series(df['change']).std()
    
    num_bins = 200
    #n, bins, patches = plt.hist(df['price'], num_bins, normed=1, facecolor='green', alpha=0.5)
    plt.hist(df['change'], num_bins, normed=1, facecolor='green', alpha=0.5)
    plt.title(""Percentage change histogram"")
    plt.ylabel(""Frequency"")
    plt.xlabel(""Normalized percentage change"")
    plt.show()",0.5460796356,
2041,step create a histogram of the top items bought,"def dfAggregate(df):
    """"""given a data frame with cycling data from .tcx file
    return the per lap average cadence, heart rate, power, and speed 
    as well as the distance traveled on that lap""""""
    return (df.dropna()
          .groupby('Lap')
          .agg({'Cadence (rev/min)':np.average,
                'Distance (mi)':np.ptp,
                'Heart Rate (bpm)':np.average,
                'Power (W)':np.average,
                'Speed (mph)':np.average})
            .rename(columns={'Distance (mi)':'Distance Traveled (mi)',
                             'Cadence (rev/min)':'Average Cadence (rev/min)',
                             'Heart Rate (bpm)':'Average Heart Rate (bpm)',
                             'Power (W)':'Average Power (W)',
                             'Speed (mph)':'Average Speed (mph)'}))",0.5436435342,
2041,step create a histogram of the top items bought,"def drop_outliers(df):
    print('Dropping outliers')
    print('- n (before) =', len(df))

    # TODO
    Q1 = df.SalePrice.quantile(.25)
    Q3 = df.SalePrice.quantile(.25)
    IQR = Q3 - Q1
    print(Q1)
    print(Q3)
    
    df.drop(df[(df.SalePrice > Q3 + 1.5 * IQR) |
               (df.SalePrice < Q1 - 1.5 * IQR)].index, inplace = True)
    print('- n (after)  =', len(df))
    
drop_outliers(df)",0.5427626371,
2041,step create a histogram of the top items bought,"def plot_class_broke_down_hist(df, var, xlog=False, ylog=False, **histkwargs):
    df[var][df.died_hosp == 0].hist(alpha=.5, label='survived', **histkwargs)
    df[var][df.died_hosp == 1].hist(alpha=.5, label='died', **histkwargs)
    plt.xlabel(var)
    if xlog:
        plt.xscale('log')
    if ylog:
        plt.yscale('log')
    plt.legend();",0.5396605134,
2041,step create a histogram of the top items bought,"def drop_outliers(df):
    print 'Dropping outliers'
    print '- n (before) =', df.shape[0]

    Q1 = df.SalePrice.quantile(.25)
    Q2 = df.SalePrice.quantile(.5)
    Q3 = df.SalePrice.quantile(.75)
    IQR = Q3 - Q1

    print '- Q1         =', Q1, '($M)'
    print '- Q2/Median  =', Q2, '($M)'
    print '- Q3         =', Q3, '($M)'

    df.drop(df[(df.SalePrice < Q1 - 1.5 * IQR) | (df.SalePrice > Q3 + 1.5 * IQR)].index, inplace = True)

    print '- n (after)  =', df.shape[0]

drop_outliers(df)",0.5372624993,
125,build a multi layer feedforward network,"def create_model(window_length):
    model = Sequential()
    model.add(Dense(window_length, activation='relu', input_shape=(window_length,)))
    model.add(Dense(200))
    model.add(Dense(200))
    model.add(Dense(100))
    model.add(Dense(1))
    model.compile(loss='mean_squared_error', optimizer='adam')    
    return model

def fit_model(model, train, batch_size, nb_epoch):
    X, y = train[:, 0:-1], train[:, -1]    
    model.fit(X, y, epochs=nb_epoch, batch_size=batch_size, verbose=0, shuffle=False)

def generate_new_window(train_scaled, predictions_scaled, window_length, index):
    # sufficient number of predictions exist
    if index - window_length >= 0:
        new_train = predictions_scaled[-window_length:]
        return np.asarray(new_train).reshape((1, window_length))
    
    # insufficient predictions, use some from last train sequence
    else:
        train_end = train_scaled[0][-(window_length - index):]
        new_train = np.concatenate((train_end, predictions_scaled))
        return new_train.reshape((1, window_length))
    
def make_window_predictions(model, train_scaled, scaler, raw_values, number_of_predictions, window_length):
    scaled_predictions = list()
    predictions = list()
    
    X_train = train_scaled[-1, -window_length:].reshape((1, window_length))    
    yhat = model.predict(X_train, 1)[0, 0]
    scaled_predictions.append(yhat)

    yhat = invert_scale(scaler, X_train[0], yhat)
    yhat = yhat + raw_values[-1]
    predictions.append(yhat)
    
    # Predict N steps into the FUTURE!
    for i in range(1, number_of_predictions):
        X = generate_new_window(X_train, scaled_predictions, window_length, i)
        
        yhat = model.predict(X, 1)[0, 0]
        scaled_predictions.append(yhat)
        
        yhat = invert_scale(scaler, X[0], yhat)
        yhat = yhat + predictions[-1]
        predictions.append(yhat)   
    return predictions

# Create sliding window dataset
def create_dataset(train_series, window_length):
    windowed_set = timeseries_to_supervised(train_series, window_length)
    windowed_set = windowed_set.iloc[:-window_length]
    return windowed_set

# predicts from single time series using categorical model
def predict_from_model(train_series, model, scaler, n_predictions=1, window_length=5, batch_size=4, nb_epoch=5):
    supervised_values = timeseries_to_supervised(train_series, window_length)
    supervised_values = supervised_values.iloc[:-window_length].values
    
    train_scaled = fit_existing_scaler(supervised_values, scaler)
    return make_window_predictions(model, train_scaled, scaler, train_series, n_predictions, window_length)
    
# fits or predicts, returns either predictions or nothing respectively
def fit_full_category(train_series, model, window_length=5, batch_size=4, nb_epoch=5):
    scaler, train_scaled = scale2(train_series.values)
    fit_model(model, train_scaled, batch_size, nb_epoch)
    return scaler",0.4943622053,
125,build a multi layer feedforward network,"def model(x, act=tf.nn.relu): 
    layer_1 = act(tf.add(tf.matmul(x, weights['h1']), biases['b1']))
    layer_2 = act(tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']))
    out_layer = tf.add(tf.matmul(layer_2, weights['out']), biases['out'], name=""absolute_output"")
    return out_layer

# Construct model
logits = model(X)",0.4888151586,
125,build a multi layer feedforward network,"def create_count_vector_nn_model():
    model = Sequential()
    # first and only hidden layer with 128 units
    model.add(Dense(128, activation='relu', input_shape=(max_features,)))

    # since we are classifying each label separately, we use a sigmoid function for the output layer and 
    # binary_crossentropy as the loss function
    model.add(Dense(6, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

model_count_vector_nn = create_count_vector_nn_model()

# add early stopping which checks validation loss to avoid overfitting. 
# if the validation loss does not decrease by more than 0.0001 for more than 2 epochs, training stops.
early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=2, verbose=1)

auroc = AUROC(validation_data=(x_validation, y_validation))

hist_count_vector_nn = model_count_vector_nn.fit(x_train, y_train, batch_size=500, epochs=10, 
                                                 validation_data=(x_validation, y_validation), 
                                                 callbacks=[early_stopping, auroc])

# save model in case it's needed later
model_count_vector_nn.save('models/count_vector_nn.h5')

# save model history in case it's needed later
with open('models/history/count_vector_nn_hist','wb') as f:
    pkl.dump(hist_count_vector_nn.history, f)",0.486123383,
125,build a multi layer feedforward network,"def generator(z):
    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)
    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2
    G_prob = tf.nn.sigmoid(G_log_prob)

    return G_prob


def discriminator(x):
    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)
    D_logit = tf.matmul(D_h1, D_W2) + D_b2
    D_prob = tf.nn.sigmoid(D_logit)

    return D_prob, D_logit",0.4860363901,
125,build a multi layer feedforward network,"def get_simple_model():
    model = Sequential()
    model.add(Dense(512, activation='relu', input_shape=(num_max,num_max)))
    model.add(Dropout(0.5))
    model.add(Dense(256, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(1, activation='sigmoid'))
    model.summary()
    model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['acc',keras.metrics.binary_accuracy])
    print('compile done')
    return model

def check_model(model,x,y,epochs=2):
    history=model.fit(x,y,batch_size=32,epochs=epochs,verbose=1,shuffle=True,validation_split=0.2,
              callbacks=[checkpointer, tensorboard]).history
    return history


def check_model2(model,x_train,y_train,x_val,y_val,epochs=10):
    history=model.fit(x_train,y_train,batch_size=64,
                      epochs=epochs,verbose=1,
                      shuffle=True,
                      validation_data=(x_val, y_val),
                      callbacks=[checkpointer, tensorboard]).history
    return history

# define checkpointer
checkpointer = ModelCheckpoint(filepath=model_save_path,
                               verbose=1,
                               save_best_only=True)    


# define tensorboard
tensorboard = TensorBoard(log_dir='./logs',
                          histogram_freq=0,
                          write_graph=True,
                          write_images=True)




# define the predict function for the deep learning model for later use
def predict(data):
    result=spam_model_dl.predict(data)
    prediction = [round(x[0]) for x in result]
    return prediction",0.4849546552,
125,build a multi layer feedforward network,"def generator(z):
    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)
    G_h2 = tf.nn.relu(tf.matmul(G_h1, G_W2) + G_b2)
    G_h3 = tf.nn.relu(tf.matmul(G_h2, G_W3) + G_b3)
    G_logit = tf.matmul(G_h3, G_W4) + G_b4
    return G_logit

def gan_discriminator(x_real, x_fake):
    """""" I.e. Regular GAN discriminator """"""
    D_h1_real = tf.nn.relu(tf.matmul(x_real / 4, D_W1) + D_b1)
    D_h2_real = tf.nn.relu(tf.matmul(D_h1_real, D_W2) + D_b2)
    D_h3_real = tf.nn.relu(tf.matmul(D_h2_real, D_W3) + D_b3)
    D_logit_real = tf.matmul(D_h3_real, D_W4) + D_b4
    D_prob_real = tf.nn.sigmoid(D_logit_real)

    D_h1_fake = tf.nn.relu(tf.matmul(x_fake / 4, D_W1) + D_b1)
    D_h2_fake = tf.nn.relu(tf.matmul(D_h1_fake, D_W2) + D_b2)
    D_h3_fake = tf.nn.relu(tf.matmul(D_h2_fake, D_W3) + D_b3)
    D_logit_fake = tf.matmul(D_h3_fake, D_W4) + D_b4
    D_prob_fake = tf.nn.sigmoid(D_logit_fake)    
    return D_prob_real, D_logit_real, D_prob_fake, D_logit_fake

def dan_s_discriminator(x_real, x_fake):
    """""" A.k.a Sample classifier for DAN_S""""""
    D_h1_real = tf.nn.relu(tf.matmul(x_real / 4, D_W1) + D_b1)
    D_h2_real = tf.reduce_mean(tf.nn.relu(tf.matmul(D_h1_real, D_W2) + D_b2), axis=0, keep_dims=True)
    D_h3_real = tf.nn.relu(tf.matmul(D_h2_real, D_W3) + D_b3)
    D_logit_real = tf.matmul(D_h3_real, D_W4) + D_b4
    D_prob_real = tf.nn.sigmoid(D_logit_real)

    D_h1_fake = tf.nn.relu(tf.matmul(x_fake / 4, D_W1) + D_b1)
    D_h2_fake = tf.reduce_mean(tf.nn.relu(tf.matmul(D_h1_fake, D_W2) + D_b2), axis=0, keep_dims=True)
    D_h3_fake = tf.nn.relu(tf.matmul(D_h2_fake, D_W3) + D_b3)
    D_logit_fake = tf.matmul(D_h3_fake, D_W4) + D_b4
    D_prob_fake = tf.nn.sigmoid(D_logit_fake)

    return D_prob_real, D_logit_real, D_prob_fake, D_logit_fake
                        
def dan_2s_discriminator(x_ori_1, x_ori_2, x_alter_1, x_alter_2):
    """""" A.k.a two-sample discriminator""""""
    D_h1_ori_1 = tf.nn.relu(tf.matmul(x_ori_1 / 4, D_W1) + D_b1)
    D_h2_ori_1 = tf.reduce_mean(tf.nn.relu(tf.matmul(D_h1_ori_1, D_W2) + D_b2), axis=0, keep_dims=True)
    D_h1_alter_1 = tf.nn.relu(tf.matmul(x_alter_1 / 4, D_W1) + D_b1)
    D_h2_alter_1 = tf.reduce_mean(tf.nn.relu(tf.matmul(D_h1_alter_1, D_W2) + D_b2), axis=0, keep_dims=True)
    D_h1_ori_2 = tf.nn.relu(tf.matmul(x_ori_2 / 4, D_W1) + D_b1)
    D_h2_ori_2 = tf.reduce_mean(tf.nn.relu(tf.matmul(D_h1_ori_2, D_W2) + D_b2), axis=0, keep_dims=True)
    D_h1_alter_2 = tf.nn.relu(tf.matmul(x_alter_2 / 4, D_W1) + D_b1)
    D_h2_alter_2 = tf.reduce_mean(tf.nn.relu(tf.matmul(D_h1_alter_2, D_W2) + D_b2), axis=0, keep_dims=True)

    D_h3_11 = tf.nn.relu(tf.matmul(tf.abs(D_h2_ori_2 - D_h2_ori_1), D_W3) + D_b3)
    D_h3_10 = tf.nn.relu(tf.matmul(tf.abs(D_h2_ori_2 - D_h2_alter_1), D_W3) + D_b3)
    D_h3_01 = tf.nn.relu(tf.matmul(tf.abs(D_h2_alter_2 - D_h2_ori_1), D_W3) + D_b3)
    D_h3_00 = tf.nn.relu(tf.matmul(tf.abs(D_h2_alter_2 - D_h2_alter_1), D_W3) + D_b3)

    D_logit_00 = tf.matmul(D_h3_00, D_W4) + D_b4
    D_logit_01 = tf.matmul(D_h3_01, D_W4) + D_b4
    D_logit_10 = tf.matmul(D_h3_10, D_W4) + D_b4
    D_logit_11 = tf.matmul(D_h3_11, D_W4) + D_b4
    return D_logit_00, D_logit_11, D_logit_01, D_logit_10",0.4838160276,
125,build a multi layer feedforward network,"class Network():
    def __init__(self):
        self._model = models.Sequential()
        self._loss = np.ones(500)
        self._mse = np.ones(500)
    def _compile(self):
        self._model.add(layers.Dense(1))
        self._model.compile(optimizer='rmsprop', loss='mse', metrics=['mse'])        
    def baseline(self):
        self._model.add(layers.Dense(64, activation='relu', input_shape=(train_data.shape[1],)))
        self._model.add(layers.Dense(64, activation='relu'))
        self._compile()
    def dropout(self):
        self._model.add(layers.Dense(64, activation='relu', input_shape=(train_data.shape[1],)))
        self._model.add(layers.Dropout(0.5))
        self._model.add(layers.Dense(64, activation='relu'))
        self._model.add(layers.Dropout(0.5))
        self._compile()
    def l1(self):
        self._model.add(layers.Dense(64, kernel_regularizer=l1(0.001), activation='relu', input_shape=(train_data.shape[1],)))
        self._model.add(layers.Dense(64, kernel_regularizer=l1(0.001), activation='relu'))
        self._compile()
    def l2(self):
        self._model.add(layers.Dense(64, kernel_regularizer=l2(0.001), activation='relu', input_shape=(train_data.shape[1],)))
        self._model.add(layers.Dense(64, kernel_regularizer=l2(0.001), activation='relu'))
        self._compile()    
    def layer3(self):
        self._model.add(layers.Dense(64, activation='relu', input_shape=(train_data.shape[1],)))
        self._model.add(layers.Dense(64, activation='relu'))
        self._model.add(layers.Dense(64, activation='relu'))
        self._compile()
    def layer2b(self):
        self._model.add(layers.Dense(200, activation='relu', input_shape=(train_data.shape[1],)))
        self._model.add(layers.Dense(200, activation='relu'))
        self._compile()        
    def kfold_validate(self):
        k = 10
        num_val_samples = len(train_data) // k
        num_epochs = 500
        all_mse_histories = []
        for i in range(k):
            print('processing fold #', i)
            val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]
            val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]
            partial_train_data = np.concatenate(
                [train_data[:i * num_val_samples],
                 train_data[(i + 1) * num_val_samples:]],
                axis=0)
            partial_train_targets = np.concatenate(
                [train_targets[:i * num_val_samples],
                 train_targets[(i + 1) * num_val_samples:]],
                axis=0)

            model = self._model
            history = model.fit(partial_train_data, partial_train_targets,
                                validation_data=(val_data, val_targets),
                                epochs=num_epochs, batch_size=128, verbose=0)
            mse_history = history.history['mean_squared_error']
            all_mse_histories.append(mse_history)
        self._histories = [np.mean([x[i] for x in all_mse_histories]) for i in range(num_epochs)]
    def history(self):
        return self._loss[1:,].mean(axis=0), self._mse[1:,].mean(axis=0)
    def scores(self):
        return self._histories
    @property
    def model(self):
        return self._model",0.4832389951,
125,build a multi layer feedforward network,"import layers

def getarch(FirstLayerType):
    h = 100
    return layers.Sequential([
        FirstLayerType(13,h,seed=0),
        layers.ReLU(),
        layers.Pooling(h)
    ])",0.4770392179,
125,build a multi layer feedforward network,"def build_model():
    model = Sequential()
    model.add(Dense(24, input_dim=10, init='uniform', activation='relu'))
    model.add(Dense(1))
    model.compile(loss='mse', optimizer='adam')
    return model

estimator = KerasRegressor(build_fn=build_model, epochs=1000, verbose=0)
cross_val_score(estimator, X_train, y_train, cv=5, scoring = 'neg_mean_squared_error')",0.476762861,
125,build a multi layer feedforward network,"def third_model():
    # create model
    model = Sequential()
    model.add(Conv2D(128, (3, 3), input_shape=(size_h, size_w, 3), activation='relu'))
    model.add(Conv2D(128, (3, 3), input_shape=(size_h, size_w, 3), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Conv2D(64, (3, 3), input_shape=(size_h, size_w, 3), activation='relu'))
    model.add(Conv2D(64, (3, 3), input_shape=(size_h, size_w, 3), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dense(2, activation='softmax'))
    
    # Compile model
    model.compile(Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Call model
myModel = third_model()

#Setup callbacks
tb=TensorBoard(log_dir='./logs/op3')
estop = EarlyStopping(monitor='val_acc', patience=3)

#train the model
model_out_3 = myModel.fit_generator(generator=train_batch, 
                      steps_per_epoch=trainStepEpoch,
                      validation_data=val_batch,
                      validation_steps=valStepEpoch, 
                      epochs=20,
                      callbacks=[tb,estop])",0.4761066437,
501,download the newsgroups dataset,"#Connect to SeatGeek API and determine number of pages of information
def get_SeatGeek_Pages():
    url = 'https://api.seatgeek.com/2/events?format=json'
    payload = {'per_page' : 1000,
               'taxonomies.name':'concert',
               'client_id':  config.sg_key,
              }
    r = requests.get(url, params=payload,verify=True)
    json_obj = json.loads(r.text)
    
    #Return the total number of JSON items divided by the number of page to get page count
    return math.ceil(json_obj['meta']['total']/json_obj['meta']['per_page'])

#Connect to SeatGeek API and download JSON data, format it into pandas dataframe
def get_SeatGeek_data(page=1):
    url = 'https://api.seatgeek.com/2/events?format=json'
    payload = {'per_page' : 1000,
               'page' : page,
               'taxonomies.name':'concert',
               'venue.country' : 'US',
               'client_id': config.sg_key,
              }
    r = requests.get(url,params=payload,verify=True)
    json_obj = json.loads(r.text)
    info_list = []
    for event in json_obj['events']:
        info_list.append(
         {'SG_event_id' : str(event.get('id',{})),
         'SG_listing_count' : str(event.get('stats',{}).get('listing_count',{})),
         'SG_average_price' : str(event.get('stats',{}).get('average_price',{})),
         'SG_min_price' : str(event.get('stats',{}).get('lowest_price',{})),
         'SG_max_price' : str(event.get('stats',{}).get('highest_price',{})),
         'SG_title' : str(event.get('title',{})),
         'SG_date' : str(event.get('datetime_local',{})),
         'SG_artists' : list(str(performer.get('name',{})) for performer in event.get('performers',{})),
         'SG_artists_score' : list(str(performer.get('score',{})) for performer in event.get('performers',{})),
         'SG_artists_id' : list(str(performer.get('id',{})) for performer in event.get('performers',{})),
         'SG_venue' : str(event.get('venue',{}).get('name',{})),
         'SG_venue_city' : str(event.get('venue',{}).get('city',{})),
         'SG_venue_state' : str(event.get('venue',{}).get('state',{})),
         'SG_venue_score' : str(event.get('venue',{}).get('score',{}))
                               })
    sg_df = pd.DataFrame(info_list)
    return sg_df

def localize_times(df, time_col, tz_col):
    df_list = []
    for tz in df[tz_col].unique():
        #Filter rows by timezone
        mask = (df[tz_col] == tz)
        df_local = df.loc[mask]
        df_local[time_col] = pd.to_datetime(df_local[time_col],
                                                    errors='coerce').dt.tz_localize(None).dt.tz_localize(tz)
        df_list.append(df_local)
        df = pd.concat(df_list, axis=0)
    return df",0.5653611422,
501,download the newsgroups dataset,"from tqdm import tqdm

    
def scrape_events_per_page(url):
    '''
    returns:
        A list of tuples of strings holding title, place, date, and price
        for concerts in Copenhagen scraped from Kulturnaut.dk
    '''
    res = requests.get(url)
    res.raise_for_status()
    soup = bs4.BeautifulSoup(res.text, ""html.parser"")

    event_cells = soup.find_all('td', {""width"": ""100%"", ""valign"" : ""top""})

    scraped_events_per_page = []
    for event_cell in event_cells:
        try:
            title = event_cell.find('b').text
            spans = event_cell.find_all('span')
            place = spans[1].text
            try:
                date, price = spans[0].text.splitlines()
            except ValueError, e:
                date = spans[0].text.splitlines()[0]
                price = ''
        except Exception, e:
            print(e)
            
        scraped_events_per_page.append((title, place, date, price))
        
    return scraped_events_per_page


scraped_events = []
for idx in tqdm(range(0, no_events, 20)):
    if idx > 0:
        idx += 1

    scrape_url = base_url.format(idx)
    scraped_events += scrape_events_per_page(scrape_url)",0.5568999052,
501,download the newsgroups dataset,"#loading notMNIST datasheet from http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html
if(not os.path.isfile(""./notMNIST_small.mat"")):
    #get the dataset
    sys.stdout.write(b""Fetching the dataset from the internet"")
    import requests
    s = requests.Session()
    while(True):
        r = s.get(""http://yaroslavvb.com/upload/notMNIST/notMNIST_small.mat"", stream=""True"")
        if(r.status_code == 200):
            f = open(""./notMNIST_small.mat"", 'wb')
            for chunk in r.iter_content(chunk_size=512 * 1024): 
                if chunk: # filter out keep-alive new chunks
                    f.write(chunk)
                    sys.stdout.write(b""."")
                    sys.stdout.flush()
            f.close()
            print(""Done"")
            break
        os.sleep(200) #do not dos the server :)
    del s #close sesion
    del requests
    
data = io.loadmat(""./notMNIST_small.mat"")

#preparation of the data
X = data['images']
y = data['labels']
resolution = 28
classes = 10

X = np.transpose(X, (2, 0, 1))

y = y.astype('int32')
X = X.astype('float32') / 255.

Y = np_utils.to_categorical(y,10)",0.5566259027,
501,download the newsgroups dataset,"# Get top @nmbr_of_coins coins on coinmarketcap
def get_top_coins(nmbr_of_coins = 9):
    # Get main ranking table
    page = requests.get('https://coinmarketcap.com/')
    soup = BeautifulSoup(page.content, 'html.parser')
    body = soup.find('body')
    container = body.find('div', {'class':'container'}, recursive=False)
    table = container.find('table', {'id':'currencies'})

    # Table body
    tbody = table.find('tbody')
    rows = tbody.find_all('tr')[:nmbr_of_coins]
    
    # Get the coin name and ticker from each row in table
    coins = []
    coin_names = []
    for row in rows:
        a = row.find('span', {'class':'currency-symbol'}).find('a')
        coins.append(a.get_text())                  # Get coin ticker
        coin_names.append(a['href'].split('/')[-2]) # Get coin name
        
    # Return dictionary sorted by coin ticker
    return dict([(coin, coin_name) for coin, coin_name in sorted(zip(coins, coin_names))])",0.5564956665,
501,download the newsgroups dataset,"#%% get raw data using quandl API and website
# https://www.quandl.com/data/BKRHUGHES-Baker-Hughes-Investor-Relations?page=4
def fetch_data():
    import quandl
    quandl_code = [""COM/WLD_CRUDE_WTI"", 'COM/PNGASUS_USD']
    og_price = quandl.get(quandl_code, authtoken=QUANDL_TOKEN)
    og_price.columns = ['WTI','NG']
    og_price.plot(title='quandl WTI NG', figsize=(8,3))
    quandl_code_rig = ['BKRHUGHES/RIGS_BY_STATE_TOTALUS_LAND', 
                       'BKRHUGHES/RIGS_BY_STATE_TEXAS_LAND']
    og_rig = quandl.get(quandl_code_rig,
                        authtoken=QUANDL_TOKEN)
    og_rig.columns = [s.split('_')[-2] for s in og_rig.columns]
    og_rig.plot(title='quandl rig', figsize=(8,3))
    quandl_code_future = ['EIA/STEO_NYWFUTR_M']
    (quandl.get(quandl_code_future,
               authtoken=QUANDL_TOKEN)
             .plot(figsize=(8,3),
                   title='EIA oil gas price short future'))
    
    # https://www.eia.gov/opendata/qb.php?sdid=AEO.2017.REF2017.PRCE_NOMP_TEN_NA_WTI_NA_USA_NDLRPBRL.A
    from EIAgov import EIAgov
    eia_future = ['AEO.2017.REF2017.PRCE_NOMP_TEN_NA_WTI_NA_USA_NDLRPBRL.A',
                  'AEO.2017.REF2017.PRCE_RLP_TEN_NA_WTI_NA_USA_Y13DLRPBBL.A'] # STEO.NYWSTEO.M
    eia = EIAgov(EIA_API, eia_future)
    og_future = eia.GetData().set_index('Date').sort_index()
    og_future.plot(title='EIA oil gas longterm future', figsize=(8,3))
    plt.legend(['Norminal price', 'Real price'])
    plt.show()
    return og_price, og_rig

#og_price, og_rig = fetch_data()

if __name__ == '__main__':
    og_price, og_rig = fetch_data()",0.5562108755,
501,download the newsgroups dataset,"# Helper functions

def load_revere_affiliation(
        url=""https://raw.githubusercontent.com/kjhealy/revere/master/data/PaulRevereAppD.csv""):
    data = urllib.request.urlopen(url)
    df = pd.read_csv(data).set_index(""Unnamed: 0"")
    people = list(df.index)
    groups = list(df.columns)
    #
    B = nx.Graph()
    for column in df.columns:
        for row in df[df[column] == 1].index:
            B.add_edge(column, row)
    return people, groups, B

def load_revere(
        url=""https://raw.githubusercontent.com/kjhealy/revere/master/data/PaulRevereAppD.csv"",
        threshold=None, dual=False):
    # Load the affiliation network
    people, groups, B = load_revere_affiliation(url)
    # Project the affiliation network onto the set of people
    if dual:
        G = nxalg.bipartite.projection.weighted_projected_graph(B, groups)
    else:
        G = nxalg.bipartite.projection.weighted_projected_graph(B, people)
    # Threshold
    if threshold is not None:
        for s, t, data in list(G.edges(data=True)):
            if data[""weight""] < threshold:
                G.remove_edge(s,t)
    # Copy ""weight"" data to ""value"" needed by visjs
    w = nx.get_edge_attributes(G, ""weight"")
    nx.set_edge_attributes(G, w, ""value"")
    return G",0.5532265902,
501,download the newsgroups dataset,"def getSynonyms():
    #servicePath = '/indexers/%s?api-version=%s' % (indexName, apiVersion)
    headers = {""Content-type"": ""application/json"", ""api-key"": apiKey}
    r = requests.get(""https://eyazuresearch2017.search.windows.net/synonymmaps/synonym-map?api-version=2016-09-01-Preview"", headers=headers)
    print(r, r.text)
    return r

getSynonyms()",0.5493137836,
501,download the newsgroups dataset,"# YZ
def readHTML(level=1):
    import urllib.request
    pre = ""http://www.sudoku-puzzles-online.com/cgi-bin/hexadoku/print-1-grid-hexadoku.cgi?""
    fp = urllib.request.urlopen(pre + str(level))
    my_bytes = fp.read()

    mystr = my_bytes.decode(""utf8"").splitlines()
    fp.close()

    myline = None
    for line in mystr:
        if line.startswith(""</div><table id='table0'><tr><td>""):
            myline = line

    start = myline.find(""<table id='grid'>"") + len(""<table id='grid'>"")
    end = myline.find(""</table>"")
    my_grid = myline[start:end]

    grid_lines = my_grid.split('<tr>')
    puzzle = ''
    for l in grid_lines[1:]:
        l = l.replace('&nbsp;', '0').replace('<td>', '')
        puzzle += clean(l)

    return puzzle.replace('0', '.').lower()


def clean(s):
    import re
    s = re.sub(""(<[^>]+>)"", '', s)
    return s


def get_list(n, level=1):
    list = []
    while len(list) is not n:
        puzzle = readHTML(level)
        if puzzle not in list:
            list.append(puzzle)
    return list


def write_file(file_name, num, level=1):
    file = open(file_name, 'w')
    for item in get_list(num, level):
        file.write(""%s\n"" % item)",0.5492713451,
501,download the newsgroups dataset,"def IsAlive():    
    url = 'https://hft-api.lykke.com/api/IsAlive'
    headers = {'Accept': 'application/json'}
    r = requests.get(url, headers=headers)
    return r",0.5483315587,
501,download the newsgroups dataset,"def currentwave():
       
    GC_soup = BeautifulSoup(GC_wavedata, ""lxml"")  #assign variable to BeautifulSoup parameters

    for element in GC_soup(attrs={'class' : 'rating-text text-dark'}):   #look for the class with the corresponding value
        print(""The current wave conditions at the Gold Coast are: {}."".format(element.text.strip()))  #print the text after the matching class & value
        
    for element in GC_soup(attrs={'class' : 'weather-icon weather-icon-10'}):   #look for the class with the corresponding value
        if element.text not in GC_soup:   #only add once / prevent dupplicates
            GC_soup.append(element.text.strip())
            print(""The current weather conditions at the Gold Coast are: {}."".format(element.text.strip()))   #print the text after the matching class & value
      
    print() # free line space
    
    SC_soup = BeautifulSoup(SC_wavedata, ""lxml"")  #assign variable to BeautifulSoup parameters

    for element in SC_soup(attrs={'class' : 'rating-text text-dark'}):   #look for the class with the corresponding value
        print(""The current wave conditions at the Sunshine Coast are: {}."".format(element.text.strip()))   #print the text after the matching class & value
    
    for element in SC_soup(attrs={'class' : 'weather-icon weather-icon-10'}):   #look for the class with the corresponding value
        if element.text not in SC_soup:   #only add once / prevent dupplicates
            SC_soup.append(element.text.strip())
            print(""The current weather conditions at the Sunshine Coast are: {}."".format(element.text.strip()))  #print the text after the matching class & value",0.5477837324,
2345,the history of regular expressions,"def plot_history(history):
    metrics = sorted(history.history.keys())
    metrics = metrics[:len(metrics)//2]
    for m in metrics:
        # summarize history for metric m
        plt.plot(history.history[m])
        plt.plot(history.history['val_' + m])
        plt.title(m)
        plt.ylabel(m)
        plt.xlabel('epoch')
        plt.legend(['train', 'test'], loc='upper right')
        plt.show()",0.428927213,
2345,the history of regular expressions,"def plot_AccLoss(his):
    for k, v in his.history.items():
        if (k == 'acc') | (k == 'val_acc'):
            plt.subplot(211)
            v = np.array(historyA.history[k])*100
            if k == 'acc': 
                label = 'Train Acc. = {}'.format(v[-1])
                plt.plot(range(len(v)), v, label=label, color='red')
            if k == 'val_acc': 
                label = 'Validation Acc. = {}'.format(v[-1])
                plt.plot(range(len(v)), v, label=label, color='green')
            plt.xlabel(""Epochs"")
            plt.ylabel(""Accuracy %"")
            plt.legend()
        if (k == 'loss') | (k == 'val_loss'):
            plt.subplot(212)
            v = historyA.history[k]
            if k == 'loss': 
                label = 'Train Loss. = {}'.format(v[-1])
                plt.plot(range(len(v)), v, label=label, color='red')
            if k == 'val_loss': 
                label = 'Validation Loss. = {}'.format(v[-1])
                plt.plot(range(len(v)), v, label=label, color='green')
            plt.xlabel(""Epochs"")
            plt.ylabel(""Loss %"")
            plt.legend()
        plt.tight_layout()",0.427837193,
2345,the history of regular expressions,"# list all data in history
print(history.history.keys())",0.4258989692,
2345,the history of regular expressions,print(hist.history.keys()),0.4258989692,
2345,the history of regular expressions,print(history.history.keys()),0.4258989692,
2345,the history of regular expressions,"# List all data in history
print(hist.history.keys())",0.4258989692,
2345,the history of regular expressions,print(r.history.keys()),0.4258989692,
2345,the history of regular expressions,print(model_history.history.keys()),0.4258989692,
2345,the history of regular expressions,print(training.history.keys()),0.4258989692,
2345,the history of regular expressions,print(histories.history.keys()),0.4258989692,
969,introduction to data science,"def survey_simulations(overwrite=False):
    """"""Run survey simulations.
    
    """"""
    if overwrite or not os.path.isfile(expfile):
        from desisurvey.progress import Progress
        from desisurvey.config import Configuration
        from surveysim.util import add_calibration_exposures
    
        Configuration.reset()
        config = Configuration(surveyconfigfile)

        survey_logname = os.path.join(surveydir, 'survey.log')
        print('Running survey simulations; logging to {}'.format(survey_logname))
        
        with open(survey_logname, 'w') as logfile:
            cmd = ""surveyinit --config-file {} --output-path {}"".format(surveyconfigfile, surveydir)
            err = subprocess.call(cmd.split(), stdout=logfile, stderr=logfile)
            assert err == 0

            # Use 0d fiber assignment delay to move on with mini sim quickly
            # Do we need new rules?!?
            cmd = ""surveyplan --config-file {} --output-path {} --create --fa-delay 0d"".format(
                surveyconfigfile, surveydir)
            err = subprocess.call(cmd.split(), stdout=logfile, stderr=logfile)
            assert err == 0

            cmd = ""surveysim --config-file {} --output-path {} --seed {}"".format(
                surveyconfigfile, surveydir, seed)
            err = subprocess.call(cmd.split(), stdout=logfile, stderr=logfile)
            assert err == 0

            # Do we need new rules?!?
            plan_cmd = 'surveyplan --config-file {} --output-path {} --fa-delay 0d'.format(
                surveyconfigfile, surveydir)
            sim_cmd = 'surveysim --resume --config-file {} --output-path {} --seed {}'.format(
                surveyconfigfile, surveydir, seed)
            while True:
                lastdate = open(os.path.join(surveydir, 'last_date.txt')).readline().strip()
                progress = Table.read(os.path.join(surveydir, 'progress.fits'), 1)
                ndone = np.count_nonzero(progress['status'] == 2)
                print('Starting {} with {}/{} tiles completed {}'.format(lastdate, ndone, len(progress), time.asctime()))
                if subprocess.call(plan_cmd.split(), stdout=logfile, stderr=logfile) != 0:
                    break
                if subprocess.call(sim_cmd.split(), stdout=logfile, stderr=logfile) != 0:
                    break

        # Make sure observing truly finished.
        progressfile = os.path.join(surveydir, 'progress.fits')
        if not os.path.exists(progressfile):
            print(""ERROR: Missing {}"".format(progressfile))
            print(""Check {} for what might have gone wrong"".format(survey_logname))
    
        print('Files in {}:\n'.format(surveydir))
        !ls $surveydir
        
        # convert progress.fits -> exposures.fits
        p = Progress(restore='progress.fits')
        explist = p.get_exposures()
        explist = add_calibration_exposures(explist)

        # Sanity check that all tiles in the subset were observed in the exposures list.
        if not np.all(np.in1d(tiles['TILEID'], explist['TILEID'])):
            print(""ERROR: some tiles weren't observed;\ncheck {} for failures"".format(survey_logname) )
            print(""Missing TILEIDs:"", set(tiles['TILEID']) - set(explist['TILEID']))
        else:
            print('All tiles in the subset were observed at least once.')
            explist.write(expfile, overwrite=True)
            print('Writing {}'.format(expfile))                

        # Optionally make a movie
        if False:
            cmd = ""surveymovie --config-file {} --output-path {}"".format(
                surveyconfigfile, surveydir)
            err = subprocess.call(cmd.split(), stdout=logfile, stderr=logfile)
            assert err == 0
    else:
        print('Simulated observing has been done.')
        explist = Table.read(expfile)
        print('Read {} exposures from {}'.format(len(explist), expfile))

    return explist",0.405562669,
969,introduction to data science,"def defineDataSets():
    
    global dataSets
    
    onStepStart('build data sets')
    # Define data folders to analyze
    dataSets = DATA_SETS

    # Read meta data
    for ds in dataSets:
        buildDataSet(ds)

        logger.info(""Dataset: %s\nversion: %d (%d), chunks: %.0f, totalLines: %.0f, parseErrors: %.0f, results: %s"" % (
            ds['id'],
            ds['meta']['script']['version']['major'],
            ds['meta']['script']['version']['minor'],
            len(ds['meta']['chunks']) if 'chunks' in ds['meta'] else np.NaN,
            ds['meta']['totalLines'] if 'totalLines' in ds['meta'] else np.NaN,
            len(ds['meta']['httpParseErrors']) if 'httpParseErrors' in ds['meta'] else np.NaN,
            str(True if 'results' in ds and len(ds['results']) > 0 else False)
        ))
    printMemory()
defineDataSets()",0.4037259817,
969,introduction to data science,"if cfg.redo_preprocessing:

    # trigger packing execution
    outdir = cfg.data_dir + ""/compressed_hdf5""
    create_local_dir(cfg.outdir)

    #def set_hdf5_worker(feature_data_split, x, outdir)
    #    pack_features_to_hdf5(feature_data_split, x, outdir + '/' + x + '.h5')

    threads = []
    for x in [""training"", ""validation"", ""test""]:
        thread = threading.Thread(name = x,\
                                  args = (feature_data_split, x, outdir + '/' + x + '.h5', ),\
                                  target = pack_features_to_hdf5)
        threads += [thread]
        thread.start()
        #pack_features_to_hdf5(feature_data_split, x, outdir + '/' + x + '.h5')

    # block and wait for all threads to complete
    for thread in threads:
        thread.join()",0.4025864303,
969,introduction to data science,"class DataGen:
    def __init__(self, sr=16000, batch_size=128):
        np.random.seed(1209)
        self.pitches = [440., 466.2, 493.8, 523.3, 554.4, 587.3,
                        622.3, 659.3, 698.5, 740., 784.0, 830.6]

        self.sr = sr
        self.n_class = len(self.pitches)  # 12 pitches
        self.secs = 1.
        self.batch_size = batch_size
        self.sins = []
        self.labels = np.eye(self.n_class)[range(0, self.n_class)]  # 1-hot-vectors

        for freq in self.pitches:
            cqt = librosa.cqt(sin_wave(self.secs, freq, self.sr, gain=0.5), sr=sr,
                              fmin=220, n_bins=36, filter_scale=2)[:, 1]  # use only one frame!
            cqt = librosa.amplitude_to_db(cqt, ref=np.min)
            cqt = cqt / np.max(cqt)
            self.sins.append(cqt)

        self.cqt_shape = cqt.shape  # (36, )

    def __next__(self):
        choice = np.random.choice(12, size=self.batch_size, # pick pitches for this batch
                                  replace=True)
        noise_gain = 0.1 * np.random.random_sample(1)  # a random noise gain 
        noise = whitenoise(noise_gain, self.cqt_shape)  # generate white noise
        xs = [noise + self.sins[i] for i in choice]  # compose a batch with additive noise
        ys = [self.labels[i] for i in choice] # corresponding labels

        return np.array(xs, dtype=np.float32), np.array(ys, dtype=np.float32)

    next = __next__",0.4017012417,
969,introduction to data science,"def get_network(initialization):
    network = []
    network.append(Dense(X_train.shape[1],100, initialization=initialization))
    network.append(ReLU())
    network.append(Dense(100,100, initialization=initialization))
    network.append(ReLU())
    network.append(Dense(100,100, initialization=initialization))
    network.append(ReLU())
    network.append(Dense(100,100, initialization=initialization))
    network.append(ReLU())
    network.append(Dense(100,100, initialization=initialization))
    network.append(ReLU())
    network.append(Dense(100,10, initialization=initialization))

    return network",0.4007775784,
969,introduction to data science,"# from desaster import config


def view_config():
    var = vars(config)
    conf = dir(config)
    result = {}
    for i in conf:
        if ""__"" not in i and ""random"" not in i:
            print (""{} = {}"".format(i, var[i]))
            #print (i)
            result[i] = var[i]
            
        else:
            pass
    return result

def imp_concat():
    input1 = pd.read_csv(""../inputs/Work out of County Households.csv"")
    input2 = pd.read_csv(""../inputs/Work in County Households.csv"")
    input3 = pd.read_csv(""../inputs/Middle High School Households.csv"")
    input4 = pd.read_csv(""../inputs/Elementary School Households.csv"")
    input5 = pd.read_csv(""../inputs/Grocery Households.csv"")
    all_inputs = [input1, input2, input3, input4, input5]
    data = pd.concat(all_total)
    return data

def get_names():
    lasts = pd.read_csv(""../inputs/last_names.csv"")
    data = lasts.iloc[0:1366]
    return data['name']",0.4005830288,
969,introduction to data science,"def _compute_network_model(self) : 
    """""" Build the network, loss, grad_loss and sgd_update theano functions.
        More work than is strictly nessecary is done here as the only thing
        that is really needed in order to run sgd (stochastic gradient 
        descent) is the sgd_update function. The network, loss and grad_loss
        functions are compiled since this is experimental code.
    """"""

    # build the network
    self.i = T.vector('i',dtype = self.int_dtype)

    self.network_outputs = compute_network_outputs(self.i,self.s0,self.V,
                                                   self.U,self.W,self.b)


    # build mean log likelyhood loss

    # variables for a batch of sentences
    self.I = T.matrix('I',dtype = self.int_dtype)
    self.J = T.matrix('J',dtype = self.int_dtype) # for embedding I = J

    self.loss_outputs = compute_mean_log_lklyhd_outputs(self.I,self.J,
                                                        self.s0,self.V,
                                                        self.U,self.W,
                                                        self.b)

    # set up the accumulator for computing the loss in batches

    n_minibatch = T.cast(self.I.shape[0],self.float_dtype)
    loss_accum_ipnm = self.loss_accum_i + n_minibatch

    self.loss_updates = ((self.loss_accum,
                          (self.loss_outputs*n_minibatch/loss_accum_ipnm
                           + (self.loss_accum 
                             * self.loss_accum_i/loss_accum_ipnm))),
                         (self.loss_accum_i,loss_accum_ipnm))

    # get the gradient of the loss

    (self.dV,
     self.dU,
     self.dW,
     self.db) = theano.grad(self.loss_outputs,
                            [self.V,self.U,self.W,self.b])

    # get the gradient magnitudes

    self.dV_mag = T.sqrt(T.sum(self.dV*self.dV))
    self.dU_mag = T.sqrt(T.sum(self.dU*self.dU))
    self.dW_mag = T.sqrt(T.sum(self.dW*self.dW))
    self.db_mag = T.sqrt(T.sum(self.db*self.db))

    # get the sgd update function

    # this is the learning parameter
    self.eta = T.scalar('eta',dtype = self.float_dtype)

    # also including a running average of the gradient magnitudes

    self.sgd_i = T.scalar('sgd_i',dtype = self.float_dtype)

    dV_mag_accum = (self.dV_mag/(self.sgd_i+1.)
                        + self.m_dV_mag*(self.sgd_i/(self.sgd_i+1.)))
    dU_mag_accum = (self.dU_mag/(self.sgd_i+1.) 
                        + self.m_dU_mag*(self.sgd_i/(self.sgd_i+1.)))
    dW_mag_accum = (self.dW_mag/(self.sgd_i+1.) 
                        + self.m_dW_mag*(self.sgd_i/(self.sgd_i+1.)))
    db_mag_accum = (self.db_mag/(self.sgd_i+1.) 
                        + self.m_db_mag*(self.sgd_i/(self.sgd_i+1.)))

    # adding here since we are taking a max of the loss - accumulators
    # do not include the latest values
    self.sgd_updates = ((self.V,self.V + self.eta*self.dV),
                        (self.U,self.U + self.eta*self.dU),
                        (self.W,self.W + self.eta*self.dW),
                        (self.b,self.b + self.eta*self.db),
                        (self.m_dV_mag,dV_mag_accum),
                        (self.m_dU_mag,dU_mag_accum),
                        (self.m_dW_mag,dW_mag_accum),
                        (self.m_db_mag,db_mag_accum))

    # pointers for the compiled functions
    self.network = None
    self.loss = None
    self.grad_loss = None
    self.sgd_update = None
    self.sgd_update_w_loss = None

# bind the method to the class - this is just so the class definition
# can be broken up across several Jupyter notebook cells
embedding_model._compute_network_model = _compute_network_model",0.3995211124,
969,introduction to data science,"# Get distances for each word to word combination
def tsne_distances():
    ""Get the distances between different points""
    words = sorted(model_words)[1:]
    distances = np.empty([len(words), len(words)])
    dist = lambda p1,p2: sqrt(((p2[0]-p1[0])**2)+((p2[1]-p1[1])**2))
    
    # For all the words, generate the the distance plot:
    for index, word in enumerate(words):
        for index2, word2 in enumerate(words):
            distances[index][index2] = dist(new_values[index], new_values[index2])
    
    return words, distances

def distance_plot(save=False):
    ""Create a heatmap correlation style map for a given set of words""
    words, distances = tsne_distances()
    fig = pyplot.figure(figsize=(16, 16))
    ax = fig.add_subplot(111)
    cax = ax.matshow(distances, vmin=np.min(distances), vmax=np.max(distances))
    fig.colorbar(cax)
    ticks = np.arange(0,len(words),1)
    ax.axis('off')
    #ax.set_xticks(ticks)
    #ax.set_yticks(ticks)
    #ax.set_xticklabels(words)
    #ax.set_yticklabels(words)
    pyplot.show()
    
    # Save a .png of the plot
    if save:
        fig.savefig(""output/closeness-map.png"")",0.3989983201,
969,introduction to data science,"from sklearn import preprocessing

# Train, Validation, Test Split
def loadDataSplit():
    df_train = pd.read_csv(BASE_FOLDER + TRAINING_DATA_CSV)
    # TOURNAMENT_DATA_CSV has both validation and test data provided by NumerAI
    df_test_valid = pd.read_csv(BASE_FOLDER + TOURNAMENT_DATA_CSV)

    answers_1_SINGLE = df_train[TARGET_VAR]
    df_train.drop(TARGET_VAR, axis=1,inplace=True)
    df_train.drop('id', axis=1,inplace=True)
    df_train.drop('era', axis=1,inplace=True)
    df_train.drop('data_type', axis=1,inplace=True)    
    
#     df_train=oneHOT(df_train)

    df_train.to_csv(BASE_FOLDER + TRAINING_DATA_CSV + 'clean.csv', header=False,  index = False)    
    df_train= pd.read_csv(BASE_FOLDER + TRAINING_DATA_CSV + 'clean.csv', header=None, dtype=np.float32)    
    df_train = pd.concat([df_train, answers_1_SINGLE], axis=1)
    feature_cols = list(df_train.columns[:-1])
#     print (feature_cols)
    target_col = df_train.columns[-1]
    trainX, trainY = df_train[feature_cols], df_train[target_col]
    
    
    # TOURNAMENT_DATA_CSV has both validation and test data provided by NumerAI
    # Validation set
    df_validation_set=df_test_valid.loc[df_test_valid['data_type'] == 'validation'] 
    df_validation_set=df_validation_set.copy(deep=True)
    answers_1_SINGLE_validation = df_validation_set[TARGET_VAR]
    df_validation_set.drop(TARGET_VAR, axis=1,inplace=True)    
    df_validation_set.drop('id', axis=1,inplace=True)
    df_validation_set.drop('era', axis=1,inplace=True)
    df_validation_set.drop('data_type', axis=1,inplace=True)
    
#     df_validation_set=oneHOT(df_validation_set)
    
    df_validation_set.to_csv(BASE_FOLDER + TRAINING_DATA_CSV + '-validation-clean.csv', header=False,  index = False)    
    df_validation_set= pd.read_csv(BASE_FOLDER + TRAINING_DATA_CSV + '-validation-clean.csv', header=None, dtype=np.float32)    
    df_validation_set = pd.concat([df_validation_set, answers_1_SINGLE_validation], axis=1)
    feature_cols = list(df_validation_set.columns[:-1])

    target_col = df_validation_set.columns[-1]
    valX, valY = df_validation_set[feature_cols], df_validation_set[target_col]
                            
    # Test set for submission (not labeled)    
    df_test_set = pd.read_csv(BASE_FOLDER + TOURNAMENT_DATA_CSV)
#     df_test_set=df_test_set.loc[df_test_valid['data_type'] == 'live'] 
    df_test_set=df_test_set.copy(deep=True)
    df_test_set.drop(TARGET_VAR, axis=1,inplace=True)
    tid_1_SINGLE = df_test_set['id']
    df_test_set.drop('id', axis=1,inplace=True)
    df_test_set.drop('era', axis=1,inplace=True)
    df_test_set.drop('data_type', axis=1,inplace=True)   
    
#     df_test_set=oneHOT(df_validation_set)
    
    feature_cols = list(df_test_set.columns) # must be run here, we dont want the ID    
#     print (feature_cols)
    df_test_set = pd.concat([tid_1_SINGLE, df_test_set], axis=1)            
    testX = df_test_set[feature_cols].values
        
    return trainX, trainY, valX, valY, testX, df_test_set",0.3987259865,
969,introduction to data science,"class DataAnalysis_GC:
    def __init__(self):
        self.measures_dict = dict()
        self.critical_threshold_dict = dict()
        self.critical_mean_degree_dict = dict()
    
    def load_data(self, data_path):
        measures_dict = self.measures_dict 
        for file in listdir(data_path):
            with open(data_path + file) as f:
                data = json.load(f)
            size = int(data['network_size'])
            covariance = float(data['covariance'])
            threshold = float(data['threshold'])
            #initialize dictionary if first appearance
            if not size in measures_dict:
                measures_dict[size] = dict()
            if not covariance in measures_dict[size]:
                measures_dict[size][covariance] = dict()
            if not threshold in measures_dict[size][covariance]:
                measures_dict[size][covariance][threshold] = dict()
            #get measures
            lc_size_list = [int(s) for s in data['lc_size_list']]
            sec_lc_size_list = [int(s) for s in data['sec_lc_size_list']]
            mean_sc_size_list = [float(s) for s in data['mean_sc_size_list']]
            avg_shortest_path_list = [float(l) for l in data['average_shortest_path_list']]
            measures_dict[size][covariance][threshold][""lc_size_dispersion""] = \
                np.var(lc_size_list)/np.mean(lc_size_list)
            measures_dict[size][covariance][threshold][""mean_sec_lc_size""] = np.mean(sec_lc_size_list)
            measures_dict[size][covariance][threshold][""mean_sc_size""] = np.mean(mean_sc_size_list)
            measures_dict[size][covariance][threshold][""average_shortest_path""] = np.mean(
                avg_shortest_path_list)
 
    @staticmethod
    def lorentzian(x,x0,a,b):
        return a*b/(2*np.pi*((x-x0)**2+(b/2)**2)) 
    
    def fit_lorentzian(self, xdata, ydata):
        x0,a,b = curve_fit(self.lorentzian,xdata,ydata)[0]
        return x0,a,b 
    
    def extract_peak(self, measure, covariance_list=None):
        critical_threshold_dict = self.critical_threshold_dict
        critical_mean_degree_dict = self.critical_mean_degree_dict
        for size, d1 in self.measures_dict.items():
            critical_threshold_dict[size] = dict()
            critical_mean_degree_dict[size] = dict()
            if covariance_list is None:
                covariance_list = d1.keys()
            for covariance in covariance_list:
                d2 = d1[covariance]
                threshold_list = [t for t in d2.keys()]
                measure_list = [d2[t][measure] for t in threshold_list]
                try:
                    x0,a,b = self.fit_lorentzian(threshold_list, measure_list)
                    critical_threshold_dict[size][covariance] = dict()
                    critical_threshold_dict[size][covariance][""peak""] = x0
                    critical_threshold_dict[size][covariance][""FWHM""] = b
                    critical_mean_degree_dict[size][covariance] = get_mean_degree(size, x0)
                except RuntimeError:
                    pass",0.3973629177,
2520,visualizing chipotle s data,"print(""RMSE: "",np.sqrt(hist_NN_100neurons.history['val_loss'][-1]) * 48)",0.4070611894,
2520,visualizing chipotle s data,"raw_means, pop_dict = Super_av_tag_cov(""/home/djeffrie/Data/RADseq/Lsphenosephalus/Incremental/Populations/after_sample_filtering/batch_1.vcf"", \
                                  popmap = ""/home/djeffrie/Data/RADseq/Lsphenosephalus/Incremental/Populations/after_sample_filtering/Sex_ID_info.txt"",\
                                  whitelist = ZW_whitelist)",0.4033508003,
2520,visualizing chipotle s data,"raw_means, pop_dict = Super_av_tag_cov(""/home/djeffrie/Data/RADseq/Lsphenosephalus/Incremental/Populations/after_sample_filtering/batch_1.vcf"", \
                                  popmap = ""/home/djeffrie/Data/RADseq/Lsphenosephalus/Incremental/Populations/after_sample_filtering/Sex_ID_info.txt"",\
                                  whitelist = XY_whitelist)",0.4033508003,
2520,visualizing chipotle s data,"news.target[0], news.data[0][1:100]",0.3990037441,
2520,visualizing chipotle s data,"#showing the ages of survivors vs the ages of those who perished
ax=sns.violinplot(x='Survival',y='Age',hue='Sex',palette='Set1', split=True,data=titanic_aged_df)",0.3953825831,
2520,visualizing chipotle s data,"ax=sns.violinplot(x='Pclass',y='Survival',hue='Sex',palette='Set1', split=True,data=titanic_df)",0.3953825831,
2520,visualizing chipotle s data,"def get_test_image():
    return vehicle_imgs[""GTI_Left""][274], non_vehicle_imgs[""Extras""][3546]",0.3946108818,
2520,visualizing chipotle s data,twothousands.geekscore.hist(bins=20),0.394176662,
2520,visualizing chipotle s data,"plot_hist(history_B.history, xsize=8, ysize=12)",0.3940517902,
2520,visualizing chipotle s data,"plot_hist(history_A.history, xsize=8, ysize=12)",0.3940517902,
437,dealing with frequency offsets and multipliers,"f = abs(fft.fft(sound)[0:size(sound)/2]) #the magnitude spectrum, from 0 to nyquist
freqs = (fft.fftfreq(size(sound), 1/44100))[0:(size(sound)/2)] #the frequency values for each bin, from 0 to nyquist
centroid = sum(freqs * f)/sum(f) # the weighted sum of magnitudes divided by the unweighted sum of magnitudes
print centroid #print it out",0.4301959276,
437,dealing with frequency offsets and multipliers,"w, v = sig.freqz(hbp)
plot(w, 20*log(abs(v)))",0.4210531116,
437,dealing with frequency offsets and multipliers,"w, v = sig.freqz(h)
plot(w,20*log(abs(v)))",0.4210531116,
437,dealing with frequency offsets and multipliers,"#wn, hn = sig.freqz(bn_e4/np.sum(bn_e4.astype(float))) # <= w <= frequencias, h <= resposta (numeros complexos)
w_e2, h_e2 = sig.freqz(b_e2)
w_a2, h_a2 = sig.freqz(b_a2)
w_d3, h_d3 = sig.freqz(b_d3)
w_g3, h_g3 = sig.freqz(b_g3)
w_b3, h_b3 = sig.freqz(b_b3)
w_e4, h_e4 = sig.freqz(b_e4)

plt.figure()
plt.axis([0,700,-1,2])
plt.title(""Absolute Value"")
plt.xlabel(""Frequency [Hz]"")
plt.ylabel(""Gain"")
plt.plot(w_e2*nyq/(np.pi), np.abs(h_e2), label=""E2"")
plt.plot(w_a2*nyq/(np.pi), np.abs(h_a2), label=""A2"")
plt.plot(w_d3*nyq/(np.pi), np.abs(h_d3), label=""D3"")
plt.plot(w_g3*nyq/(np.pi), np.abs(h_g3), label=""G3"")
plt.plot(w_b3*nyq/(np.pi), np.abs(h_b3), label=""B3"")
plt.plot(w_e4*nyq/(np.pi), np.abs(h_e4), label=""E4"")

leg = plt.legend(loc='best', ncol=2, shadow=False, fancybox=False)
leg.get_frame().set_alpha(0.5)

plt.show()

#plt.figure()
#plt.axis([0,1000,-140,50])
#plt.plot(w*nyq/(np.pi), 20*np.log10(np.abs(h))) # Plot em dB
#plt.plot(wn*nyq/(np.pi), 20*np.log10(np.abs(hn))) # Plot em dB
#plt.xlabel(""Frequency [Hz]"")
#plt.ylabel(""Gain [dB]"")
#plt.show()",0.4172914028,
437,dealing with frequency offsets and multipliers,"w, h = signal.freqz(b, a)",0.4166879654,
437,dealing with frequency offsets and multipliers,"R_periodic_random = np.fft.rfft(r_periodic_random[:(r_periodic_random.size//sum(repeat_sequence))])
f_periodic_random = np.fft.rfftfreq(r_periodic_random[:(r_periodic_random.size//sum(repeat_sequence))].size, d=dt)",0.4119572639,
437,dealing with frequency offsets and multipliers,"#  : Repeat using in the built-in methods for MLE probability:
# a, b = nltk.bigrams(sentence)
[cfd[a].freq(b) for (a,b) in nltk.bigrams(sentence)]
# type(cfd)

# cfdist = nltk.probability.ConditionalProbDist((len(word), word) for word in sentence)
# cfdist",0.41147089,
437,dealing with frequency offsets and multipliers,"for i in range(0,24):
    
    sp = np.fft.fft(readings[i,:])
    freq = np.fft.fftfreq(time.shape[-1], d = 0.1)
    plt.plot(freq, sp.real)
    axis = plt.gca()
    axis.set_ylim([-50000,50000])
    plt.show()",0.4093173444,
437,dealing with frequency offsets and multipliers,"kx = fft.fftshift(fft.fftfreq(n,pscale))
ky = fft.fftshift(fft.fftfreq(n,pscale))
mg = np.meshgrid(kx,ky)
kr = np.sqrt(np.sum((m**2 for m in mg)))",0.4033760726,
437,dealing with frequency offsets and multipliers,"##keep track of all the countries and attack counts
def parse_country_attacks(arr):
    results = stats.itemfreq(arr['country_txt'])
    return results
##display the counts of attacks using plotly https://www.plot.ly
country_attack_counts = parse_country_attacks(main_df)
_x = []
_y = []
for i in range(0, len(country_attack_counts)):
    _x.append(country_attack_counts[i][0])
    _y.append(country_attack_counts[i][1])

data = [go.Bar(
    x = _x, #names of attacks
    y = _y, #counts of attacks
)]
iplot(data, filename='basic-bar')",0.402233392,
2379,tokenization,"def corpus_tokenize(data):
    tokenize_title_text = [ret.tokenize(t) for t in data]
    tagged_flat_text_list = [pos_tag(word) for word in tokenize_title_text]
    flat_text_list = [item for sublist in tagged_flat_text_list for item in sublist]
    tagged_flat_text_list = [(word, get_wordnet_pos(tag)) for word, tag in flat_text_list]
    almost_complete = [lem.lemmatize(t.lower(), tag) for t, tag in tagged_flat_text_list if tag != ''] 
    complete = [t for t in almost_complete if t not in sw and len(t) > 1]
    
    return complete",0.4875448942,
2379,tokenization,"def speech_to_sentences( review, tokenizer):
    # Function to split a speech into parsed sentences. Returns a 
    # list of sentences, where each sentence is a list of words
    #
    # 1. Use the NLTK tokenizer to split the paragraph into sentences
    raw_sentences = tokenizer.tokenize(review.strip())
    #
    # 2. Loop over each sentence
    sentences = []
    for raw_sentence in raw_sentences:
        # If a sentence is empty, skip it
        if len(raw_sentence) > 0:
            # Otherwise, call speech_to_wordlist to get a list of words
            sentences.append(speech_to_wordlist( raw_sentence))
        
    #
    # Return the list of sentences (each sentence is a list of words,
    # so this returns a list of lists
    return sentences",0.4848963618,
2379,tokenization,"# Define a function to split a review into parsed sentences
def review_to_sentences( review, tokenizer ):
    # Function to split a review into parsed sentences. Returns a 
    # list of sentences, where each sentence is a list of words
    #
    # 1. Use the NLTK tokenizer to split the paragraph into sentences
    raw_sentences = tokenizer.tokenize(review.strip())
    #
    # 2. Loop over each sentence
    sentences = []
    for raw_sentence in raw_sentences:
        # If a sentence is empty, skip it
        if len(raw_sentence) > 0:
            # Otherwise, call review_to_wordlist to get a list of words
            sentences.append( raw_sentence_to_wordlist( raw_sentence ))
    #
    # Return the list of sentences (each sentence is a list of words,
    # so this returns a list of lists
    return sentences",0.4848963618,
2379,tokenization,"# Takes average of all query terms in 1 query, so the query is represented by 1 vector
def get_q_rep(index, query_str, w2vmodel):
    query_tokens = index.tokenize(query_str)
    vectors = []
    for token in query_tokens:
        vector = w2vmodel['token']
        vectors.append(vector)

    avg_query = np.mean(np.array(vectors), axis=0)
    return avg_query

# Takes average of all terms in 1 document, so the document is represented by 1 vector
def get_d_rep(document, w2vmodel):
    wordIDs = document[1][:]
    vectors = []
    for wordID in wordIDs:
        if wordID > 0:
            wordstring = id2token[wordID]
            if wordstring in w2vmodel:
                vector = w2vmodel[wordstring]
                vectors.append(vector)

    avg_doc = np.mean(np.array(vectors), axis=0)
    return avg_doc

#avg_d = get_d_rep(index.document(1),savedW2Vmodel)
#mutual_info_score(avg_q, avg_d)",0.4837619662,
2379,tokenization,"def lem(tweet, tknzr, stoplist):
    temp = tknzr.tokenize(tweet)
    result = []
    tweet_length = 0
    for x in temp:
        if x in string.punctuation:
            continue
        elif x in ['', '', """", ' ']:
            continue
        elif str('\n') in x:
            continue
        elif x in stoplist:
            continue
        else:
            result.append(x)
            tweet_length += 1
    if len(result) == 0:
        result.append('0')
    r = ' '.join(result)
    return r, tweet_length",0.4819589257,
2379,tokenization,"def token_lookup():

    # key : value
    token_lookup = {'.': '||Period||', ',':'||comma||', '""':'QuotationMark', ';':'Semicolon', '!':'ExclamationMark',
                    '?':'QuestionMark', '(':'LeftParentheses', ')':'RightParentheses', '--':'||Dash||', 
                    '\n':'||Return||'}
    
    return token_lookup

tests.test_tokenize(token_lookup)",0.4814550877,
2379,tokenization,"def get_q_rep(index, query_str, d2vmodel):
    query_tokens = index.tokenize(query_str) # "" hoi ik ben diede "" = ""hoi"", ""ik...
    vectors = []
    for token in query_tokens:
        vector = d2vmodel['token']
        vectors.append(vector)

    avg_query = np.mean(np.array(vectors), axis=0)
    return avg_query",0.4802485406,
2379,tokenization,"def ie_preprocess(document):
    sentences = [nltk.sent_tokenize(document)]
    sentences = [ntlk.word_tokenize(sent) for sent in sentences]
    sentences = [nltk.pos_tag(sent) for sent in sentences]",0.4793694317,
2379,tokenization,"def preprocess(document):
    sentences = nltk.sent_tokenize(document)
    sentences = [nltk.word_tokenize(sent) for sent in sentences]
    sentences = [nltk.pos_tag(sent) for sent in sentences]
    return sentences",0.4770340323,
2379,tokenization,"def ie_preprocess(document):
    sentences = nltk.sent_tokenize(document) # sentence segmenter
    sentences = [nltk.word_tokenize(sent) for sent in sentences] # word tokenizer
    sentences = [nltk.pos_tag(sent) for sent in sentences] # pos tagger",0.4770340323,
300,contrib learn,"def main():
    lines = load_lines(data_path)[1:]
    training_set_lines, validation_set_lines = train_test_split(lines, test_size=0.2)
    
    nb_training = len(training_set_lines)*6
    nb_validation = len(validation_set_lines)*6

    training_images, steering_angles = get_data_without_generator(data_path, lines[0:500])
    return (training_images, steering_angles)
data_path = ""data-from-udacity""
#main()",0.4706285596,
300,contrib learn,"def objective_function(theta):
    program = ansatz(theta[0])
    bitstrings = qc.run_and_measure(program, trials=1000)
    result = np.mean(bitstrings[0])
    return -result

import scipy.optimize
res = scipy.optimize.minimize(objective_function, x0=[0.1], method='COBYLA')
res",0.4653664827,
300,contrib learn,"def train():
    agent = MonteCarloAgent(epsilon=0.1)
    env = gym.make(""FrozenLakeEasy-v0"")
    agent.learn(env, episode_count=500)
    show_q_value(agent.Q)
    agent.show_reward_log()",0.4583240747,
300,contrib learn,"def evaluate(data_source):
    model.eval()
    total_loss = 0
    ntok = len(corpus.dictionary)
    hidden = model.init_hidden(evaluation_batch_size)
    
    for i in range(0, data_source.size(0) -1, sequence_length):
        data, targets = get_batch(data_source, i, evaluation=True)
        output, hidden = model(data, hidden)
        output_flat = output.view(-1, ntok)
        total_loss += len(data) * criterion(output_flat, targets).data
        hidden = repackage_hidden_states(hidden)
    return total_loss[0] / len(data_source)",0.4550590515,
300,contrib learn,"def predict(n):
    # Get the data from the test set
    x = test_data[n][0]

    # Print the prediction of the network
    print('Network output: \n' + str(np.round(net.feedforward(x), 2)) + '\n')
    print('Network prediction: ' + str(np.argmax(net.feedforward(x))) + '\n')
    print('Actual image: ')
    
    # Draw the image
    plt.imshow(x.reshape((28,28)), cmap='Greys')

# Replace the argument with any number between 0 and 9999
predict(273)",0.4506028295,
300,contrib learn,"nb_classes = 10
 
def load_dataset():
    (X_train, y_train), (X_test, y_test) = cifar10.load_data()
    print 'X_train shape:', X_train.shape
    print X_train.shape[0], 'train samples'
    print X_test.shape[0], 'test samples'
 
    Y_train = np_utils.to_categorical(y_train, nb_classes)
    Y_test = np_utils.to_categorical(y_test, nb_classes)
 
    X_train = X_train.astype('float32')
    X_test = X_test.astype('float32')
    X_train /= 255
    X_test /= 255
 
    return X_train, Y_train, X_test, Y_test

X_train, y_train, X_test, y_test = load_dataset()",0.4497393966,
300,contrib learn,"nb_classes = 10
 
def load_dataset():
    # the data, shuffled and split between train and test sets
    (X_train, y_train), (X_test, y_test) = cifar10.load_data()
    print 'X_train shape:', X_train.shape
    print X_train.shape[0], 'train samples'
    print X_test.shape[0], 'test samples'
 
    # convert class vectors to binary class matrices
    Y_train = np_utils.to_categorical(y_train, nb_classes)
    Y_test = np_utils.to_categorical(y_test, nb_classes)
 
    X_train = X_train.astype('float32')
    X_test = X_test.astype('float32')
    X_train /= 255
    X_test /= 255
 
    return X_train, Y_train, X_test, Y_test

X_train, y_train, X_test, y_test = load_dataset()",0.4497393966,
300,contrib learn,"def irisClassificationSepalPlot(data):
    X = iris.data[:, :2]
    Y = iris.target
    
    model = linear_model.LogisticRegression()
    model.fit(X, Y)
    
    h = .02
    
    # Plotting decision boundary for sepal plot 
    # Make sure we assign a color to each point in the mesh
    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + .5
    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + .5
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    
    Z = Z.reshape(xx.shape)
    plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)
    
    plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)
    plt.xlabel(""Sepal length"")
    plt.ylabel(""Sepal width"")
    plt.show()
    
    return model",0.4495754838,
300,contrib learn,"def test(epoch):
    global best_acc
    net.eval()
    # Declaring the values
    test_loss = 0
    correct = 0
    total = 0
    # Looping over the test data
    for batch_idx, (inputs, targets) in enumerate(testloader):
        # Checking for GPU instance
        if use_cuda:
            inputs, targets = inputs.cuda(), targets.cuda()
        # Converting inputs and targets into pytorch variables 
        inputs, targets = Variable(inputs, volatile=True), Variable(targets)
        # Forward Pass
        outputs = net(inputs)
        # Storing the outputs size
        size_ = outputs.size()
        # Reducing the dimenssion
        outputs_ = outputs.view(size_[0], num_classes)
        # Calculating the loss
        loss = criterion(outputs_, targets)
        # Storing the sum of loss 
        test_loss += loss.data[0]
        # Storing the predicted values
        _, predicted = torch.max(outputs_.data, 1)
        # Storing the targets size
        total += targets.size(0)
        # Calcualting the correct values
        correct += predicted.eq(targets.data).cpu().sum()
        # Printing the data
        if batch_idx%30 == 0 or batch_idx == len(testloader)-1:
            # Printing the progress bar
            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'
                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))
    # Printing the validation loss
    print('val_loss: ',  test_loss/len(testloader), 'accuracy: ', 100.0*correct/total)
    # Storing epoch,loss and accuracy into a file
    val_loss_file.write('%d %.3f %.3f\n' %(epoch,  test_loss/len(testloader), 100.*correct/total))

    # Save checkpoint.
    acc = 100.*correct/total
    # Checking for best accuracy
    if acc > best_acc:
        print('Saving..')
        state = {
            'net': net,
            'acc': acc,
            'epoch': epoch,
        }
        # Checking whether its a directory or not
        if not os.path.isdir('../checkpoint'):
            # Creating a directory
            os.mkdir('../checkpoint')
        # Saving the data
        torch.save(state, '../checkpoint_ckpt.t7')
        # Storing the accuracy
        best_acc = acc",0.4483867288,
300,contrib learn,"# TODO: Write a function that loads a checkpoint and rebuilds the model

#model = models.resnet152(pretrained=True)

#this version doesn't need optimizer to get loaded just rebuild it outside again
def load_checkpoint_new(filepath):   #this also work   may be add the name of model to dictionray and run for longer time?
    
       
    checkpoint = torch.load(filepath)
    
    model = models.resnet152(pretrained=True)


    for param in model.parameters():
        
        param.requires_grad = False
    
    model.class_to_idx=checkpoint['class_to_label']
    model.fc = checkpoint['model_classifier']

    model.load_state_dict(checkpoint['model_state_dict'])             #keeping weight and layers
    
    model=model.cuda()
    model=model.eval()
    return model,checkpoint",0.4483818412,
283,conditional statements,"if ...:
    if ...:
        ...
    elif ...:
        ...
    else ...:
        ...
elif ...:
    if ...:
        ...
    elif ...:
        ...
    else ...:
        ...
elif ...:
    ...
else:
    ...",0.449136138,
283,conditional statements,"# Define Prob(spam|comment) or Prob(ham|comment)
def conditionalComment(comment,label):
    
    # Initialize conditional probability
    prob_label_comment = 1.0
    
    # Split comments into list of words
    
    
    # Go through all words in comments
    for ...
        
        # Compute value proportional to P(label|comment)
        # Conditional indepdence is assumed here
        
    
    return prob_label_comment",0.4414697886,
283,conditional statements,"def add_statements(self, stmts):
    """"""Add INDRA Statements to the assembler's list of statements.

    Parameters
    ----------
    stmts : list[indra.statements.Statement]
        A list of :py:class:`indra.statements.Statement`
        to be added to the statement list of the assembler.
    """"""
    for stmt in stmts:
        self.statements.append(stmt)",0.4373092055,
283,conditional statements,"def standard_pos_tagger(text):
    if(isinstance(text[0],list)):
        tagged_POS = [nltk.pos_tag(sent) for sent in text]
    else:
        tagged_POS = nltk.pos_tag(text)
    return tagged_POS",0.436085999,
283,conditional statements,"# %load -s p_stmt code/cuppa1_frontend_gram.py
def p_stmt(p):
    '''
    stmt : ID '=' exp opt_semi
         | GET ID opt_semi
         | PUT exp opt_semi
         | WHILE '(' exp ')' stmt
         | IF '(' exp ')' stmt opt_else
         | '{' stmt_list '}'
    '''
    if p[2] == '=':
        p[0] = ('assign', p[1], p[3])
        state.symbol_table[p[1]] = None
    elif p[1] == 'get':
        p[0] = ('get', p[2])
        state.symbol_table[p[2]] = None
    elif p[1] == 'put':
        p[0] = ('put', p[2])
    elif p[1] == 'while':
        p[0] = ('while', p[3], p[5])
    elif p[1] == 'if':
        p[0] = ('if', p[3], p[5], p[6])
    elif p[1] == '{':
        p[0] = ('block', p[2])
    else:
        raise ValueError(""unexpected symbol {}"".format(p[1]))",0.4305349588,
283,conditional statements,"if condition:
    <statements when condition is True>
else:
    <statements when condition is False>",0.4305023849,
283,conditional statements,"# Define Prob(word|spam) and Prob(word|ham)
def conditionalWord(word,label):
   
    # Laplace smoothing parameter
    # remider: to have access to a global variable inside a function 
    # you have to specify it using the word 'global'
    
    
    # word in ham comment
    if(label == 0):
        # Compute Prob(word|ham)
        
    
    # word in spam comment
    else:
        
        # Compute Prob(word|ham)",0.4290984571,
283,conditional statements,"loops = []
def find_width8(op):
    """""" Find all the 'For' nodes whose extent can be divided by 8. """"""
    if isinstance(op, tvm.stmt.For):
        if isinstance(op.extent, tvm.expr.IntImm):
            if op.extent.value % 8 == 0:
                loops.append(op)",0.4284311831,
283,conditional statements,"def request_description(data):
    regex = compile(""\w+"")
    if data[1]:
        soup = parse_page(data[1])
        data = (data[0], reduce(lambda a, v: a + str(v), soup.find_all([""p"", ""h2""], string=regex), """"))
    return data",0.4268894792,
283,conditional statements,"def prime_factors_divmod(prime):
    factor = 2
    factors = []
    while prime > 1:
        p, r = divmod(prime, factor)
        if r:
            factor += 1
            continue
        factors.append(factor)
        prime = p

    return factors",0.4257128835,
575,exercises exploratory data analysis,"# SVD does not accept missing values
try:
    PCAmodelAnalytical = nPYc.multivariate.exploratoryAnalysisPCA(tData, withExclusions=True, scaling=1.0)
    nPYc.reports.multivariateReport.multivariateQCreport(tData, PCAmodelAnalytical, reportType='analytical', withExclusions=True)
except ValueError:
    print('Multivariate analysis is not currently possible with vaues <LLOQ or >ULOQ.')",0.4637009501,
575,exercises exploratory data analysis,"%%writefile /lustre/storeB/project/fou/kl/admaeolus/catalog.yaml
plugins:
  source:
    - module: intake_xarray

sources:

  adm_full:
    description: ""ADM aeolus SCA/MCA retrieval, full orbit""
    driver: netcdf
    direct_access: allow
    parameters:
      retrieval:
        description: ""aeolus retrieval""
        type: str
        default: ""mca""
        allowed: [ ""mca"", ""sca"" ]
      date:
        description: ""date ('YYYYMMDD' format)""
        type: str
        default: ""*""
      orbit:
        description: ""absolute orbit ('06d' format)""
        type: str
        default: ""*""
      version:
        description: ""file version ('04d' format)""
        type: str
        default: ""*""
        allowed: [ ""*"", ""0001"", ""0002"" ]
    args:
      concat_dim: point
      urlpath: '{{ CATALOG_DIR}}/data.rev.TD01/netcdf_{{ retrieval }}/AE_TD01_ALD_U_N_2A_{{ date }}T*_{{ orbit }}_{{ version }}.DBL.nc'
      path_as_pattern: 'AE_TD01_ALD_U_N_2A_{date:%Y%m%dT%H%M%S%f}_{time_len:09d}_{orbit:06d}_{version:04d}.DBL.nc'

  adm_cast:
    description: ""ADM aeolus SCA/MCA retrieval over the MACC14 domain""
    driver: netcdf
    direct_access: allow
    parameters:
      retrieval:
        description: ""aeolus retrieval""
        type: str
        default: ""mca""
        allowed: [ ""mca"", ""sca"" ]
      date:
        description: ""date ('YYYYMMDD' format)""
        type: str
        default: ""*""
      orbit:
        description: ""absolute orbit ('06d' format)""
        type: str
        default: ""*""
      version:
        description: ""file version ('04d' format)""
        type: str
        default: ""*""
        allowed: [ ""*"", ""0001"", ""0002"" ]
    args:
      concat_dim: point
      urlpath: '{{ CATALOG_DIR}}/data.rev.TD01/netcdf_emep_domain_{{ retrieval }}/AE_TD01_ALD_U_N_2A_{{ date }}T*_{{ orbit }}_{{ version }}.DBL.nc'
      path_as_pattern: 'AE_TD01_ALD_U_N_2A_{date:%Y%m%dT%H%M%S%f}_{time_len:09d}_{orbit:06d}_{version:04d}.DBL.nc'
        
  cwf_cast:
    description: ""EMEP 12ST forecast colocated""
    driver: netcdf
    direct_access: allow
    parameters:
      date:
        description: ""date ('YYYYMMDD' format)""
        type: str
        default: ""*""
      orbit:
        description: ""absolute orbit ('06d' format)""
        type: str
        default: ""*""
    args:
      concat_dim: point
      urlpath: '{{ CATALOG_DIR}}/EMEPmodel/cast-{{ date }}_{{ orbit }}.nc'
      path_as_pattern: 'cast-{date:%Y%m%d}_{orbit}.nc'

  cwf_full:
    description: ""EMEP 12ST forecast, full MACC14 domain""
    driver: netcdf
    direct_access: allow
    parameters:
      date:
        description: ""date ('YYYYMMDD' format)""
        type: str
        default: ""201812*""
    args:
      concat_dim: time
      urlpath: '{{ CATALOG_DIR}}/EMEPmodel/CWF_12ST-{{ date}}_hourInst.nc'
      path_as_pattern: 'CWF_12ST-{date:%Y%m%d}_hourInst.nc'

  topo:
    description: ""MACC14 domain topography""
    driver: netcdf
    direct_access: allow
    args:
      urlpath: '{{ CATALOG_DIR}}/EMEP.topo/MACC14_topo_ecdis.nc'",0.458070308,
575,exercises exploratory data analysis,"from nltk.sentiment.vader import SentimentIntensityAnalyzer
#function of getting the sentiment value
def get_sentiment(tbl,text):
    """"""
    description: analyze the sentiment of sentence
    input:sentence
    output:sentiment value   
    """"""
    compound = []
    neg = []
    neu = []
    pos = []
    for i in range(len(text)):
        analyzer = SentimentIntensityAnalyzer()
        answer = analyzer.polarity_scores(text[i])
        compound.append(answer['compound'])
        neg.append(answer['neg'])
        neu.append(answer['neu'])
        pos.append(answer['pos'])
    tbl['compound'] = compound
    tbl['negative'] = neg
    tbl['neutral'] = neu
    tbl['positive'] = pos
    return tbl",0.4376832843,
575,exercises exploratory data analysis,"def analyze(results):
    df = pd.DataFrame(data=results)
    df.columns.name = 'Batch size'
    df.index.name='Repetition'

    # Show raw results.
    print ('Experimental results: raw')
    display(df)

    df_stats = df.describe()
    df_stats.loc['mean per image'] = df_stats.ix['mean'] / df.columns
    df_stats.loc['std per image']  =  df_stats.ix['std'] / df.columns # FIXME: div by sqrt(n)?
    
    # Show stats.
    print ('Experimental results: stats')
    display(df_stats)

    # Show two plots side-by-side: mean time per batch and mean time per image.
    fig, axs = plt.subplots(1,2)
    df_stats.ix['mean'] \
        .plot(ax=axs[0],
            yerr=df_stats.ix['std'],
            title='Mean time per batch (ms)',
            kind='bar', grid=True, rot=0, figsize=[10, 4], colormap=cm.autumn_r
        )
    df_stats.ix['mean per image'] \
        .plot(ax=axs[1],
            yerr=df_stats.ix['std per image'],
            title='Mean time per image (ms)',
            kind='bar', grid=True, rot=0, figsize=[10, 4], colormap=cm.autumn
        )
    
    # Show batch size giving minimum time per image, mean and std.
    min_time_per_image_idx = df_stats.ix['mean per image'].idxmin()
    if not math.isnan(min_time_per_image_idx):
        print (
            'Minimum time per image: batch size = %d, mean = %.2f, std = %.2f' % (
                min_time_per_image_idx, 
                df_stats.ix['mean per image'][min_time_per_image_idx],
                df_stats.ix['std per image'][min_time_per_image_idx]
            )
        )
    else:
        print ('Minimum time per image: N/A')",0.4367529452,
575,exercises exploratory data analysis,"def create_LR_and_predict(penalty, c, stop_words = []):
    """"""
    Create Logistic Regression classifier, fit the train data and the plot chart of R vs. correct probability and run classification report
    """"""    

    # Analyzer to stem words in docs
    analyzer = TfidfVectorizer().build_analyzer()
    
    def better_analyzer(doc):        
        return (w for w in analyzer(doc) if not w in stop_words)

    t_cv = TfidfVectorizer(analyzer=better_analyzer)
    tfidf = t_cv.fit_transform(train_data)

    # Create LR classifier and fit to tfidf ( from train data )
    clf = LogisticRegression(penalty=penalty,C=c)
    clf.fit(tfidf,train_labels)

    # Create tfidf from dev_data and predict based on LR classifier
    tfidf_dev = t_cv.transform(dev_data)
    p = clf.predict_proba(tfidf_dev)

    # Create mask to extract probability for correct label
    mask = [ (i==0,i==1,i==2,i==3) for i in dev_labels ]
    correct_prob = np.extract(mask, p)
    Rc = list(zip(np.divide(p.max(axis=1), correct_prob),correct_prob))

    # Get top3 R values
    print('Top 3 R values for {2} ~ C={1} : {0}'.format(np.sort([x for x,y in Rc])[-3:],c,penalty))

    Rx = tfidf_dev.getnnz(axis=1)
    Ry = [ r[0] for r in Rc]

    # Plot R vs. # terms per document (scatter)
    plt.rcParams['figure.figsize'] = (5,5)
    plt.rcParams['figure.dpi'] = 150
    plt.scatter(Rx,Ry)
    plt.ylabel('R')
    plt.xlabel('Number of terms')
    plt.title('Plot of R vs. Number of terms for {1} ~ C = {0}'.format(c,penalty))
    plt.show()

    # Get classification report
    p2 = clf.predict(tfidf_dev)
    print (classification_report(dev_labels, p2))

    
for c in (0.1,0.5,0.9,1,5,10,20,30,50):
    create_LR_and_predict(""l1"", c)",0.4337416887,
575,exercises exploratory data analysis,"def run_analysis(project_file, annotation_file, pimp_file):
    
    # load m2lda object and do thresholding
    ms2lda = Ms2Lda.resume_from(project_file)
    ms2lda.do_thresholding(th_doc_topic=0.05, th_topic_word=0.01)

    # read the annotation file
    print
    print ""Annotation file""
    motif_annotation = {}
    motif_idx = {}
    i = 0
    for item in csv.reader(open(annotation_file), skipinitialspace=True):
        key = int(item[0])
        val = item[1]
        print str(key) + ""\t"" + val
        motif_annotation[key] = val
        motif_idx[key] = i
        i += 1

    motifs_of_interest = motif_annotation.keys()    
    norm = mpl.colors.Normalize(vmin=min(motif_idx.values()), vmax=max(motif_idx.values()))
    cmap = cm.gist_rainbow
    motif_colour = cm.ScalarMappable(norm=norm, cmap=cmap)    
    
    # get the network graph out for the motifs of interest
    G = get_network_graph(ms2lda, motifs_of_interest)
    print ""\n"" + nx.info(G)  
    print

    # print out some info
    ms1_count = 0
    nodes = G.nodes(data=True)
    for node_id, node_data in nodes:
        # 1 for doc, 2 for motif
        if node_data['group'] == 1: 
            ms1_count += 1
    print ""%d (out of %d) MS1 peaks found in the graph"" % (ms1_count, ms2lda.ms1.shape[0])    
    
    # load the pimp differential analysis file for matching
    de_peaks = []
    with open(pimp_file, ""rb"") as infile:
       reader = csv.reader(infile)
       next(reader, None)  # skip the headers
       for row in reader:
        PiMP_ID = int(row[0])
        polarity = row[1]
        mz = float(row[2])
        rt = float (row[3])
        mh_intensity = float(row[4])
        tup = (PiMP_ID, polarity, mz, rt, mh_intensity)
        de_peaks.append(tup)

    print
    print ""PiMP list: ""
    for tup in de_peaks:
        print tup
        
    # do the matching
    mass_tol = 3
    rt_tol = 12

    std = np.array(de_peaks)
    std_mz = np.array([x[2] for x in de_peaks])
    std_rt = np.array([x[3] for x in de_peaks])
    matches = {}

    ms1_label = {}
    for row in ms2lda.ms1.itertuples(index=True):
        peakid = row[1]
        mz = row[5]
        rt = row[4]

        # the following line is hacky for pos mode data
        mass_delta = mz*mass_tol*1e-6
        mass_start = mz-mass_delta
        mass_end = mz+mass_delta
        rt_start = rt-rt_tol
        rt_end = rt+rt_tol

        match_mass = (std_mz>mass_start) & (std_mz<mass_end)
        match_rt = (std_rt>rt_start) & (std_rt<rt_end)
        match = match_mass & match_rt

        res = std[match]
        if len(res) == 1:
            closest = tuple(res[0])
            matches[closest] = row
            ms1_label[row[1]] = closest[1]        
        elif len(res)>1:
            closest = None
            min_dist = sys.maxint
            for match_res in res:
                match_mz = float(match_res[2])
                match_rt = float(match_res[3])
                dist = math.sqrt((match_rt-rt)**2 + (match_mz-mz)**2)
                if dist < min_dist:
                    min_dist = dist
                    closest = match_res
            closest = tuple(closest)
            matches[closest] = row
            ms1_label[row[1]] = closest[1]

    print ""Matches found %d/%d"" % (len(matches), len(std))
    print

    ms1_list = []
    for match in matches:
        key = str(match)
        ms1_row = matches[match]
        value = str(ms1_row)
        pid = ms1_row[1]
        print ""Standard %s"" % key
        print ""MS1 %s"" % value
        print
        ms1_list.append(pid)
        
    # print the motifs and count their occurences
    m2m_list = motifs_of_interest
    word_map, motif_words = ms2lda.print_motif_features(quiet=True)

    c = Counter() # count the motif occurences
    for i in range(len(ms1_list)):

        ms1 = ms1_list[i]
        df = print_report(ms2lda, G, ms1, motif_annotation, motif_words, motif_colour, motif_idx, word_map, xlim_upper=770)
        if df is not None:

            # display(df) # show the table to see the mz, annotations, etc

            # get the motif ids in the dataframe
            fragment_motif_ids = df[['fragment_motif']].values.flatten()
            loss_motif_ids = df[['loss_motif']].values.flatten()

            # get rid of nan values
            fragment_motif_ids = fragment_motif_ids[~np.isnan(fragment_motif_ids)]
            loss_motif_ids = loss_motif_ids[~np.isnan(loss_motif_ids)]

            # store the unique counts
            combined = np.append(fragment_motif_ids, loss_motif_ids).astype(int)
            combined = set(combined.tolist())
            c.update(combined)
            
    return c",0.4331275225,
575,exercises exploratory data analysis,"PCAmodelAnalytical = nPYc.multivariate.exploratoryAnalysisPCA(tData, withExclusions=True, scaling=1.0)

nPYc.reports.multivariateReport.multivariateQCreport(tData, reportType='analytical', withExclusions=True)",0.4303715229,
575,exercises exploratory data analysis,"def grid_search(essays, expected_tags):

    rows_ana = []
    proc_essays = processed_essays_use_predicted_tag(essays=essays)

    metrics = get_metrics_raw(proc_essays, expected_tags=expected_tags,  micro_only=True)
    row = metrics[""MICRO_F1""]
    rows_ana.append(row)

    df_results = pd.DataFrame(rows_ana)
    return df_results",0.4287244678,
575,exercises exploratory data analysis,"from nltk.sentiment.vader import SentimentIntensityAnalyzer
def sentiment_value(paragraph):
    analyser = SentimentIntensityAnalyzer()
    result = analyser.polarity_scores(paragraph)
    score = result['compound']
    return round(score,1)",0.4264819622,
575,exercises exploratory data analysis,"def vader_comparison(texts):
    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
    headers = ['pos','neg','neu','compound']
    print(""Name\t"",'  pos\t','neg\t','neu\t','compound')
    analyzer = SentimentIntensityAnalyzer()
    for i in range(len(texts)):
        name = texts[i][0]
        sentences = sent_tokenize(texts[i][1])
        pos=compound=neu=neg=0
        for sentence in sentences:
            vs = analyzer.polarity_scores(sentence)
            pos+=vs['pos']/(len(sentences))
            compound+=vs['compound']/(len(sentences))
            neu+=vs['neu']/(len(sentences))
            neg+=vs['neg']/(len(sentences))
        print('%-10s'%name,'%1.2f\t'%pos,'%1.2f\t'%neg,'%1.2f\t'%neu,'%1.2f\t'%compound)",0.4247432351,
1589,problem,"def request_description(data):
    regex = compile(""\w+"")
    if data[1]:
        soup = parse_page(data[1])
        data = (data[0], reduce(lambda a, v: a + str(v), soup.find_all([""p"", ""h2""], string=regex), """"))
    return data",0.4019674063,
1589,problem,"# The function that displays
def display_freq_bases_v2 (counts):
    """"""
    displays the result of count_bases
    """"""
    # we extract the 5 values from count_bases
    nbTotal, nbA, nbC, nbG, nbT = counts
    # and we print them
    print(""Total sequence length"", nbTotal)
    print(""A = {:.2%}"".format(nbA / nbTotal))
    print(""C = {:.2%}"".format(nbC / nbTotal))
    print(""G = {:.2%}"".format(nbG / nbTotal))
    print(""T = {:.2%}"".format(nbT / nbTotal))
    # optionnally we could as well display
    # the proportions of CG and TA
    print(""CG = {:.2%}"".format((nbC + nbG) / nbTotal))
    print(""TA = {:.2%}"".format((nbT + nbA) / nbTotal))",0.3985564709,
1589,problem,"def my_print(input_tuple):          # return none
    """"""
    Take a 2 elements tuple as index and content.
    Print the index -> content
    """"""
    ind, val = input_tuple
    print(ind, '-->', val)",0.3952500224,
1589,problem,"def choice(symbol):
    global board,state,announce
    announce=''
    symbol=str(symbol)
    pinput(symbol)
    
    if win(board,symbol):
        clear_output()
        display()
        announce = symbol+""Wins.Congragulations""
        state= False
    clear_output()
    display()
    #tie Check
    if check_tie(board):
        announce =""Tie""
        state= False
    return state,announce",0.3923697174,
1589,problem,"def sortFunction(tuple):
    """""" Construct the sort string (does not perform actual sorting)
    Args:
        tuple: (rating, MovieName)
    Returns:
        sortString: the value to sort with, 'rating MovieName'
    """"""
    key = unicode('%.3f' % tuple[0])
    value = tuple[1]
    return (key + ' ' + value)


print oneRDD.sortBy(sortFunction, True).collect()
print twoRDD.sortBy(sortFunction, True).collect()",0.3919854164,
1589,problem,"def sortFunction(tuple):
    """""" Construct the sort string (does not perform actual sorting)
    Args:
        tuple: (rating, MovieName)
    Returns:
        sortString: the value to sort with, 'rating MovieName'
    """"""
    key = unicode('%.3f' % tuple[0])
    value = tuple[1]
    return (key + ' ' + value)",0.3909614682,
1589,problem,"def printState_8p(state):
    """"""Prints out the 3x3 state of the 8 puzzle""""""
    index = state.index(0)
    state[index] = ' '
    print('{} {} {}'.format(state[0],state[1],state[2]))
    print('{} {} {}'.format(state[3],state[4],state[5]))
    print('{} {} {}'.format(state[6],state[7],state[8]))
    print()
    state[index] = 0",0.3904207945,
1589,problem,"def build_model_lstm_oneHot(layers):
    """"""
    Given the layer settings, build the LSTM model
    @Parameters:
    layers: an array of the number of units on each layer
    """"""
    lstm_model = Sequential()

    lstm_model.add(LSTM(
        32,
        input_shape=(layers[1], layers[0]),
        return_sequences=True))
    lstm_model.add(Dropout(0.2))

    lstm_model.add(LSTM(
        32,
        return_sequences=False))
    lstm_model.add(Dropout(0.2))
    
    lstm_model.add(Dense(
        output_dim=layers[3]))
    lstm_model.add(Activation(""relu""))

    lstm_model.add(Dense(
    output_dim=layers[4]))
    lstm_model.add(Activation(""relu""))
    
    lstm_model.add(Dense(layers[5]))
    lstm_model.add(Activation(""softmax""))
    
    return lstm_model",0.3883623481,
1589,problem,"def pretty_print_compl(c):
    if p_imag(c) >= 0:
        rep_ext = str(p_real(c)) + '+' + str(p_imag(c)) + 'i'
    else:
        rep_ext = str(p_real(c)) + '-' + str(abs(p_imag(c))) + 'i'
    print(rep_ext)",0.3852331936,
1589,problem,"def max_diff_sorted(S):
    min = S[0]
    max = S[-1]
    
    print(""min = %s"" % min)
    print(""max = %s"" % max)
    print(""difference = %s"" % (max-min))
    
max_diff_sorted(sorted_S)",0.3845581114,
1966,split a dataframe into a testing and a test set,"df_full = pd.read_csv(""classification_full_dataset.csv"")

x_train, x_test, y_train, y_test = train_test_split(df_full.drop([""y""], axis=1), df_full[""y""],
                                                    test_size=0.3, random_state=1)
# Comparison
for name, clf in zip(names, classifiers):
    clf.fit(x_train.get_values(), y_train.get_values())
    print str(name) + "" "" + str(clf.score(x_test.get_values(),
                                          y_test.get_values()))",0.5280903578,
1966,split a dataframe into a testing and a test set,"# Split df into data and target

data = df.ix[:,'Pclass':].values
target = df['Survived'].values

# Split data randomly into training and test sets
X_train, X_test, y_train, y_test = cross_validation.train_test_split(
data, target, test_size=0.2)

print(""Training sets: "", X_train.shape, y_train.shape)
print(""Test sets: "",X_test.shape, y_test.shape)",0.521107018,
1966,split a dataframe into a testing and a test set,"#no x_var1 since not important
X = df_96on[['x_var2','x_var3',....]].values
y = df_96on['y_var'].values
X_train, X_test, y_train, y_test= train_test_split(X,y,test_size=0.3,random_state=1)",0.5208618641,
1966,split a dataframe into a testing and a test set,"X_train, X_test, y_train, y_test = cross_validation.train_test_split(train.ix[:, train.columns != 'TripType'],
                                                                     train.ix[:, train.columns == 'TripType'], 
                                                                     random_state=26L)",0.519818902,
1966,split a dataframe into a testing and a test set,"X1 = df_96on[['x_var1','x_var2',.....]].values
y1 = df_96on['y_var'].values
X_train1, X_test1, y_train1, y_test1= train_test_split(X1,y1,test_size=0.3,random_state=1)
forest1 = RandomForestRegressor<instert first part in exploratory analysis>
forest1.fit(X_train1, y_train1)
y_train_pred1= forest1.predict(X_train1)
y_test_pred1= forest1.predict(X_test1)
print('MSE train: %.3f, test: %.3f' % (mean_squared_error(y_train1, y_train_pred1),mean_squared_error(y_test1, y_test_pred1)))
print('R^2 train: %.3f, test: %.3f' % (r2_score(y_train1, y_train_pred1),r2_score(y_test1,y_test_pred1)))
print(forest1.feature_importances_)",0.518822968,
1966,split a dataframe into a testing and a test set,"pred_train, pred_test, tar_train, tar_test = train_test_split(df_notext.as_matrix(), df[y].as_matrix(),
                                                              test_size=.2, random_state=123)

t1 = time.time()
model=LassoLarsCV(cv=20, precompute=False).fit(pred_train, tar_train)
t_lasso_lars_cv = time.time() - t1
print 'train time (seconds):', t_lasso_lars_cv
print

# Number of coefs after fitting model
coefs = dict(zip(df_notext.columns, model.coef_))
print 'Initial number coefs:', df_notext.shape[1]
print 'Coefs after fitting model:', len(coefs)
print

# MSE and MAE
print_errors(model, pred_train, pred_test, tar_train, tar_test)",0.5187101364,
1966,split a dataframe into a testing and a test set,"from sklearn.cross_validation import train_test_split
texts_train, texts_test, y_train, y_test = train_test_split(imdb.review.values, imdb.sentiment.values)",0.5183836818,
1966,split a dataframe into a testing and a test set,"X = indeed_df_two_groups.ix[:, indeed_df_two_groups.columns != 'sal_group']
y = indeed_df_two_groups['sal_group']

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 35)

scaler = StandardScaler()
scaler.fit(X_train)

X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

LOGREG = LogisticRegression()
LOGREG.fit(X_train, y_train)

print {'Logistic Regression Train Accuracy Score': LOGREG.score(X_train, y_train),
       'Logistic Regression Test Accuracy Score': LOGREG.score(X_test, y_test)}",0.5174949169,
1966,split a dataframe into a testing and a test set,"import numpy as np

def train_test_split(df, train_size=0.7):
    """"""
    Split the data into training and testing sets.
    
    Parameters
    ----------
    df : data (dataframe) 
    train_size : the proportion of the original dataset to include in the training set,
                 0.7 by default
    
    Returns
    -------
    df_train : training set (dataframe)
    df_test : testing set (dataframe)
    """"""

    # Specify the seed of the pseudo random number generator
    np.random.seed(0)
    
    # Get the number of rows in the original dataset
    # Implement me
    row_num = 

    # Get the number of rows in the training set
    # Implement me
    row_num_train = 

    # Randomly choose row_num_train rows from the original dataset
    # These rows will be included into the training set
    row_idxs_train = np.random.choice(row_num, size=row_num_train, replace=False)

    # Get the remaining rows from the original dataset
    # These rows will be included into the testing set
    # Implement me
    row_idxs_test = 

    # Get the training set
    # Implement me
    df_train = 

    # Get the testing set
    # Implement me
    df_test = 
    
    # Implement me
    return",0.5161734223,
1966,split a dataframe into a testing and a test set,"X = indeed_df_two_groups.ix[:, indeed_df_two_groups.columns != 'parsed_salary']
y = indeed_df_two_groups['parsed_salary']

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 35)

scaler = StandardScaler()
scaler.fit(X_train)

X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

LR = LinearRegression()
LR.fit(X_train, y_train)

RD = Ridge()
RD.fit(X_train, y_train)

print {'LR Train R^2': LR.score(X_train, y_train),
       'LR Test R^2': LR.score(X_test, y_test),
       'RD Train R^2': RD.score(X_train, y_train),
       'RD Test R^2': RD.score(X_test, y_test)}",0.5155299902,
832,how does datashader work?,hd.datashade(points),0.4414494932,
832,how does datashader work?,"# extract events and convert them into dataframe 
dp_events_SB.dataprep()",0.4103288651,
832,how does datashader work?,import datashader as ds,0.4044498205,
832,how does datashader work?,noisy_data_z.catalog[0:2],0.3942382932,
832,how does datashader work?,ideal_data.catalog[0:5],0.3916819096,
832,how does datashader work?,import datamover as dm,0.3894566894,
832,how does datashader work?,import tridash,0.3893970251,
832,how does datashader work?,"#Imports the QTrader class that I have created
from q_trader import QTrader",0.3890529275,
832,how does datashader work?,"n, a, b = 10, -1, 1",0.3865125179,
832,how does datashader work?,tsv.delim2cx.process_row(),0.3854038119,
2468,"using a dataframe and matplotlib commands, we can get fancy","sns.pairplot(data=auto[['mpg','displacement','horsepower',
                        'weight','acceleration']]);",0.4878519177,
2468,"using a dataframe and matplotlib commands, we can get fancy","sns.pairplot(data=data_without_na[['btc_market_cap','btc_trade_volume','btc_cost_per_transaction']])",0.4848502874,
2468,"using a dataframe and matplotlib commands, we can get fancy","sns.pairplot(data=movie_data[['startYear', 'averageRating', 'numVotes']])",0.4848502874,
2468,"using a dataframe and matplotlib commands, we can get fancy","sns.pairplot(x_vars='sqft_living', y_vars='price', data=df)",0.4816870689,
2468,"using a dataframe and matplotlib commands, we can get fancy","#Data looks consistent, let's run the pair plot to see the patterns
sns.pairplot(x_vars=['Pressure'], y_vars=['PM-2.5 - level'], data=DataframeTest, size=6)",0.4765302539,
2468,"using a dataframe and matplotlib commands, we can get fancy","#Shows that PM Level is high in winters than summers. This pattern is intact with the fact
sns.pairplot(x_vars=['month'], y_vars=['Average_PM_Level'], data=Beijingdata, size=6)",0.4765302539,
2468,"using a dataframe and matplotlib commands, we can get fancy","#Shows that PM Level is high in winters than summers. This pattern is intact with the fact
sns.pairplot(x_vars=['hour'], y_vars=['Average_PM_Level'], data=Beijingdata, size=6)",0.4765302539,
2468,"using a dataframe and matplotlib commands, we can get fancy","sns.lmplot(x='X5', y = 'Y1', data = buildings), sns.lmplot(x='X5', y = 'Y2', data = buildings)",0.4743369818,
2468,"using a dataframe and matplotlib commands, we can get fancy",sns.pairplot(data=data[numerical_features + ['Price']]);,0.4742264748,
2468,"using a dataframe and matplotlib commands, we can get fancy","#First plot show the drugs with good to strong correlations
sns.pairplot(data=drug_use,
                  x_vars=['alcohol_use', 'cocaine_use','hallucinogen_use', 'pain_releiver_use', 'oxycontin_use','tranquilizer_use', 'stimulant_use'],y_vars=['marijuana_use'])",0.4731452465,
2589,which brand have more video games?,"# get movie, genre pairs from TMDb
latest_id = tmdb.Movies().latest()['id']

def worker(i):
    try:
        movie = tmdb.Movies(i).info()
    except:
        movie = """"
    return [i, movie]

ids = list(range(1,latest_id))
random.shuffle(ids)
ids = ids[1:25000] #take random sample of 25,000 IDs 

p = multiprocessing.Pool()
ans = p.map(worker,ids)

global movie_dictionary 
movie_dictionary = {}
for i in ans:
    if i[1]: #only add the non-deleted movies to movie dictionary
        movie_dictionary[i[0]] = i[1]",0.4155102372,
2589,which brand have more video games?,"p = darts_advisor.Player(a)
p.strategies.head()",0.4139790237,
2589,which brand have more video games?,"genres_dict_file = './dltutorial/data/genres_dict.pckl'

# Top 20 popular films
top_movies = tmdb.Movies().popular()['results']
# Create a TMDB genre object
genres = tmdb.Genres()
# Returns the genres list (ID/name)
genres_list = genres.movie_list()['genres']

# if genres dictionary does not exists
if not os.path.exists(genres_dict_file):
    # Create a genres dictionary mapping a genre ID to its name
    genres_dict = dict()
    for genre in genres_list:
        genres_dict[genre['id']] = genre['name']
    logging.debug(""Writing genres dict on hard disk..."")
    with open(genres_dict_file, 'wb') as f:
        pickle.dump(genres_dict, f)
    logging.debug(""Done saving genres dict in %s..."" %
                 genres_dict_file)
# Else load existing one
else:
    logging.debug(""Loading genres dict from hard disk..."")
    with open(genres_dict_file, 'rb') as f:
        genres_dict = pickle.load(f)
    logging.debug(""Done loading genres dict from %s..."" %
                 genres_dict_file)


# Print the genres of top 5 movies
for movie in top_movies[:5]:
    genre_ids = movie['genre_ids']
    genre_names = [genres_dict[genre_id] for genre_id in genre_ids]
    print(""%s\n%s\n"" % (movie['title'], genre_names))",0.4117695093,
2589,which brand have more video games?,"genres = tmdb.Genres().list()
for movie in top10:
    m = tmdb.Movies(movie['id']).info()
    genre_names = ' '.join([d['name'] for d in m['genres']])
    print movie['title'],"":"",genre_names",0.4095343947,
2589,which brand have more video games?,"#plays a fancy sound, customizable with other sounds in the fancy_sounds folder
import vlc
def play_sound_futur(future):
    folder_path = 'fancy_sounds/'
    music_path = 'zelda_small_item.wav'
    p = vlc.MediaPlayer(folder_path + music_path)
    p.play()
    return 0

#plays a other sound, when crash or have data from where we saved the data
def play_sound(music_path):
    folder_path = 'fancy_sounds/'
    p = vlc.MediaPlayer(folder_path + music_path)
    p.play()
    return 0",0.4093499482,
2589,which brand have more video games?,"bgs_vs = vs.bgs_vs()
bgs_vs.play_videos(df)",0.4085076153,
2589,which brand have more video games?,"genres = set()
genre_script_freq = defaultdict(int)
    
for key,value in happiness_score.items():
    # for each movie we add the sentimental value to all of the movies genres
    for genre in [genre['name'] for genre in tmdb_5000_movies[key]['genres']]:
        genre_script_freq[genre] += 1

genres = list(genre_script_freq.keys())

print (""{genre_s: <20} {nr_s: <14}"".format(
        genre_s =  ""Genre"",
        nr_s = ""Number of manuscripts""))

print(""------------------------------------------"")

for genre,nr in genre_script_freq.items():
    print (""{genre_s: <20} {nr_s: <14}"".format(
        genre_s =  genre,
        nr_s = nr
    ))",0.4039259553,
2589,which brand have more video games?,"duo_toi = {}
for team in pm2.get_teams(2015):
    for line in pm2.read_team_toi(team, 2015):
        if pm2.get_game(line) in games:
            ps = pm2.get_home_players(line)
            for p1 in ps:
                if p1 not in duo_toi:
                    duo_toi[p1] = {}
                for p2 in ps:
                    if p2 not in duo_toi[p1]:
                        duo_toi[p1][p2] = 0
                    duo_toi[p1][p2] += 1
    print('Done with', team)",0.4027234316,
2589,which brand have more video games?,"def get_playlist_tracks(name):
    pls = sp.user_playlists(me_id)
    res = [p for p in pls['items']
                 if p['name']==name]
    playlist_id = res[0]['id']
    ps = sp.user_playlist_tracks(
        ""124292901"", playlist_id=playlist_id)
    return ps['items']
    
    
ps_tracks = get_playlist_tracks('science')",0.4006505609,
2589,which brand have more video games?,"for n in range(100):
    engine = C4(gametype='setset', verbose=False, p1_name='Albert', p2_name='Paul')
    engine.play_game()
    engine.save_game('demo.csv', verbose=False)
    
data = pd.read_csv('demo.csv')
print('shape of database: {}'.format(data.shape))
data.head()",0.4002056718,
747,get the data,"def get_formation_energy_and_density(compound):
    # print 'retrieving', compound['compound_name']

	returned_compounds = m.get_data(compound['compound_name']) 
	# returns of a list which matches the given compound
	# multiple hits are returned, we can get the specific one we need by sorting using bandgap
	# we have the bandgap, compare it to each entry, and keep the one which is closest to the given bandgap

	band_gap_diffs = []
	for c in returned_compounds:
		# if c['band_gap'] != compound['band_gap']:
			# returned_compounds.remove(c)
		# decimal places are off.. ex. 2.918 vs 2.9180
		band_gap_diffs.append(abs(c['band_gap']-compound['band_gap']))

    # take care of case where compound does not exist in database..
	if band_gap_diffs == []:
		print ""Matching compounds for "" + compound['compound_name'] + "" does not exist. Replacing formation energy and density with 0.""
		return 0, 0
	ind = np.argmin(band_gap_diffs)
	matching_compound = returned_compounds[ind]

	# if len(returned_compounds) != 1:
	# 	sys.exit(""Matching compounds for "" + compound['compound_name'] + "" exceeds 1. Matches: "" + str(len(returned_compounds)))
		# print ""Matching compounds for "" + compound_name + "" is not 1. Matches: "" + len(returned_compounds)

	return pd.Series({'formation_energy':matching_compound['formation_energy_per_atom'], 'density':matching_compound['density']})",0.5475436449,
747,get the data,"def conv_plot(comp, label, grp, n):

    # Get data and sort by cycle number
    data = comp.data[n].get_data(label,grp)
    data = data[data[:,0].argsort()]
    # Get stdev
    std = np.sqrt(comp.data[n].get_var(label,grp))

    plt.figure(figsize=(12,9))
    plt.plot(data[:,0],data[:,1],'k.')

    fom_plot_setup(12,12)
    plt.title(comp.data[n].name)

    plt.axhline(data[-1,1] - std, ls='--', c='k')
    plt.axhline(data[-1,1] + std, ls='--', c='k')

    plt.show()",0.547052145,
747,get the data,"def get_historical_closes(ticker, start_date, end_date):
    # get the data for the tickers.  This will be a panel
    p = web.DataReader(ticker, ""yahoo"", start_date, end_date)    
    # convert the panel to a DataFrame and selection only Adj Close
    # while making all index levels columns
    d = p.to_frame()['Adj Close'].reset_index()
    # rename the columns
    d.rename(columns={'minor': 'Ticker', 'Adj Close': 'Close'}, inplace=True)
    # pivot each ticker to a column
    pivoted = d.pivot(index='Date', columns='Ticker')
    # and drop the one level on the columns
    pivoted.columns = pivoted.columns.droplevel(0)
    return pivoted",0.5383247137,
747,get the data,"## Helper function to get prices from Yahoo
def get_historical_closes(ticker, start_date, end_date):
    # get the data for the tickers.  This will be a panel
    p = wb.DataReader(ticker, ""yahoo"", start_date, end_date)    
    # convert the panel to a DataFrame and selection only Adj Close
    # while making all index levels columns
    d = p.to_frame()['Adj Close'].reset_index()
    # rename the columns
    d.rename(columns={'minor': 'Ticker', 'Adj Close': 'Close'}, inplace=True)
    # pivot each ticker to a column
    pivoted = d.pivot(index='Date', columns='Ticker')
    # and drop the one level on the columns
    pivoted.columns = pivoted.columns.droplevel(0)
    return pivoted",0.5383247137,
747,get the data,"def scatter_map(table, samples, seq):
    """"""Make dataframe of a given otu with metadata""""""
    otu_count = table.data(id=seq, axis='observation')
    df_otu = pd.DataFrame(data=otu_count, index=samples, columns=['otu'])
    df_otu_map = pd.merge(df_map, df_otu, left_index=True, right_index=True)
    # must remove zero counts otherwise scatter plot is misleading
    df_otu_map.otu = [np.nan if x==0 else x for x in df_otu_map.otu]
    return(df_otu_map)",0.5374078751,
747,get the data,"def blood_bank_setup_data():
    frame = pd.read_csv('transfusion.csv')
    frame = frame.dropna()
    frame = frame[[""Recency"",""Frequency"", ""Result""]]
    frame.Result = frame.Result.apply(lambda val : 1 if val == 1 else 0)
    return frame
bb_frame = blood_bank_setup_data()",0.532520175,
747,get the data,"def read_data():
    d = pandas.read_csv(_DATA_FILEPATH)

    return d
    
def process_data(d):
    # Group by organization.

    def sum_views(df):
        return sum(df['Views per Month'])

    g = d.groupby('Organization Name').apply(sum_views)

    # Sort by views (descendingly).

    g.sort(ascending=False)

    # Grab the first N to plot.

    items = g.iteritems()
    s = itertools.islice(items, 0, _ORGANIZATION_COUNT)

    s = list(s)

    # Sort them in ascending order, this time, so that the larger ones are on 
    # the right (in red) in the chart. This has a side-effect of flattening the 
    # generator while we're at it.
    s = sorted(s, key=lambda (n, v): v)

    # Truncate the names (otherwise they're unwieldy).

    distilled = []
    for (name, views) in s:
        if len(name) > (_MAX_LABEL_LENGTH - 3):
            name = name[:17] + '...'

        distilled.append((name, views))

    return distilled",0.5323609114,
747,get the data,"def loadTestData():
    mnist23_test = pickle.load( open( ""./datasets/mnist23.data"", ""rb"" ) )
    test_x = mnist23.data
    test_y = np.array([mnist23.target])
    return test_x,test_y",0.5317804813,
747,get the data,"def scrape_reviews(asins):
    ratings_dict = {}
    reviews_list = []
    reviews_df = pd.DataFrame()

    headers = {
        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.90 Safari/537.36'}
    XPATH_REVIEWS = '//div[@data-hook=""review""]'
    XPATH_REVIEW_RATING = './/i[@data-hook=""review-star-rating""]//text()'
    XPATH_REVIEW_HEADER = './/a[@data-hook=""review-title""]//text()'
    XPATH_REVIEW_AUTHOR = './/a[@data-hook=""review-author""]//text()'
    XPATH_REVIEW_DATE = './/span[@data-hook=""review-date""]//text()'
    XPATH_REVIEW_BODY = './/span[@data-hook=""review-body""]//text()'
    XPATH_REVIEW_HELPFUL = './/span[@data-hook=""helpful-vote-statement""]//text()'

    p_num = 0
    for asin in asins:
        while True:
            print 'Scraping review page nr. {}'.format(p_num)
            amazon_url = 'https://www.amazon.com/product-reviews/' + asin + '?pageNumber=' + str(p_num) + '&sortBy=recent'
            # Add some recent user agent to prevent amazon from blocking the request 
            # Find some chrome user agent strings  here https://udger.com/resources/ua-list/browser-detail?browser=Chrome
            page = requests.get(amazon_url, headers=headers)
            page_response = page.text.encode('utf-8')

            parser = html.fromstring(page_response)

            reviews = parser.xpath(XPATH_REVIEWS)
            if not len(reviews) > 0:
                break
            # Parsing individual reviews
            for review in reviews:
                raw_review_author = review.xpath(XPATH_REVIEW_AUTHOR)
                raw_review_rating = review.xpath(XPATH_REVIEW_RATING)
                raw_review_header = review.xpath(XPATH_REVIEW_HEADER)
                raw_review_date = review.xpath(XPATH_REVIEW_DATE)
                raw_review_body = review.xpath(XPATH_REVIEW_BODY)
                raw_review_helpful = review.xpath(XPATH_REVIEW_HELPFUL)

                review_dict = {
                    'review_text': raw_review_body,
                    'review_posted_date': raw_review_date,
                    'review_header': raw_review_header,
                    'review_rating': raw_review_rating,
                    'review_helpful': raw_review_helpful,
                    'review_author': raw_review_author
                }
                reviews_df = reviews_df.append(review_dict, ignore_index=True)
            p_num += 1
            if p_num > 7:
                break
    return reviews_df",0.531611681,
747,get the data,"if not params[""load_cov""]:
  data = dp.get_data(params[""data_type""], params)                                 
  params[""input_shape""] = [                                                       
    data[""train""].num_rows*data[""train""].num_cols*data[""train""].num_channels]",0.5296013355,
2180,step this is the data given as a dictionary,"#class LearningAgent(Agent):
    
def update(self, t):
    #some other code
        
    #update state
    self.state = (inputs[""light""], self.next_waypoint)  
    #done in section 2
    
    if self.state not in self.q_table:
        #initialize q_table[self.state]
        self.q_table[self.state] = \
                    {None:0, ""left"":0, ""right"":0, ""forward"":0}
            
    #some other code",0.4891721606,
2180,step this is the data given as a dictionary,"def predict(self, X):
    X_transformed = X
    for step in self.steps[:-1]:
        # iterate over all but the final step
        # transform the data
        X_transformed = step[1].transform(X_transformed)
    # predict using the last step
    return self.steps[-1][1].predict(X_transformed)",0.4888422787,
2180,step this is the data given as a dictionary,"def predict(self, X):
    X_transformed = X
    for step in self.steps[:-1]:
        # iterate over all but the final step
        # transform the data
        X_transformed = step[1].transform(X_transformed)
    # fit the last step
    return self.steps[-1][1].predict(X_transformed)",0.4888422787,
2180,step this is the data given as a dictionary,"def fit(self,X,y):
    X_transformed = X
    for name, estimator in self.steps[:-1]:
        X_transformed = estimator.fit_transform(X_transformed, y)
    self.steps[-1][1].fit(X_trnasformed, y)
    return self
def predict(self, X):
    X_transformed = X
    for steps in self.steps[:-1]:
        X_transformed = step[1].tranform(X_tranformed)
    return self.steps[-1][1].predict(X_transformed)",0.4736285806,
2180,step this is the data given as a dictionary,"def test_score(pipeline):
    X_test = pipeline.steps[0][1].transform(
        df_test['Title'] + ' ' + df_test['Review'])
    y_test = df_test['Recommended']
    print('roc_auc test score: {:.3f}'.format(roc_auc_score(
        y_test, pipeline.steps[-1][1].predict_proba(X_test)[:, 1])))",0.4721729159,
2180,step this is the data given as a dictionary,"def set_map(self, M):
    """"""Method used to set map attribute """"""
    self._reset(self)
    self.start = None
    self.goal = None
    # TODO: Set map to new value. 
    self.M = M",0.4695890844,
2180,step this is the data given as a dictionary,"def fit(self, X, y):
    X_transformed = X
    for name, estimator in self.steps[:-1]:
        # iterate over all but the final step
        # fit and transform the data
        X_transformed = estimator.fit_transform(X_transformed, y)
    # fit the last step
    self.steps[-1][1].fit(X_transformed, y)
    return self",0.4673720598,
2180,step this is the data given as a dictionary,"#class LearningAgent(Agent):
    
def update(self, t):
    #some other code
        
    #Learn and update Q-value based on state, action and reward
    new_state = (self.env.sense(self)[""light""],\
                 self.planner.next_waypoint())
        
    if new_state not in self.q_table:
        q_hat = reward + self.gamma * 0
    else:
        q_hat = reward + \
                self.gamma * max(self.q_table[new_state].values())
            
    self.q_table[self.state][action] = \
        self.alpha*q_hat + (1-self.alpha)*self.q_table[self.state][action]",0.4586417973,
2180,step this is the data given as a dictionary,"def top_features(pipe, n):
    """"""Prints features with the highest coefficient values, per class""""""
    vectorizer =  pipe.named_steps['vectorize']
    clf =  pipe.named_steps['clf']
    print(clf.coef_.shape)
    feature_names = vectorizer.get_feature_names()
    for i, class_label in enumerate(clf.classes_):
        if i >= clf.coef_.shape[0]:
            break
        top = np.argsort(clf.coef_[i])
        reversed_top = top[::-1]
        print(""%s:\n%s"" % (class_label,
              ""\n"".join(feature_names[j] for j in reversed_top[:n])))",0.4565379024,
2180,step this is the data given as a dictionary,"def action(self, action):
  if self.is_done():
    raise Exception(""Game is over"")
  self.steps_left -= 1
  return random.random()",0.4557006955,
866,if statements,"if ...:
    if ...:
        ...
    elif ...:
        ...
    else ...:
        ...
elif ...:
    if ...:
        ...
    elif ...:
        ...
    else ...:
        ...
elif ...:
    ...
else:
    ...",0.4760808051,
866,if statements,"# 2: Generate samples

while True:
    X, y_true, filenames = next(predict_generator)
    if sum([1 if x[0] == 2 else 0 for x in y_true[0]]) > 0:
        break

i = 0 # Which batch item to look at

print(""Image:"", filenames[i])
print()
print(""Ground truth boxes:\n"")
print(y_true[i])",0.4422320127,
866,if statements,"flag = True
x = 1
y = 0
if flag == True:
    print 'Hello World'# indent lines within ifs by a tab
    if x == 1 and y == 1: # a nested IF with an AND
        print 'AND'
    if x == 1 or y == 1: # a nested IF with an OR
        print 'OR'",0.4353137016,
866,if statements,"def genProcStart():
    global curlev, declarations, instructions
    declarations.append( [] )
    instructions.append( [] )
    fieldVars.append( {} )
    curlev = curlev + 1",0.4295200408,
866,if statements,"loops = []
def find_width8(op):
    """""" Find all the 'For' nodes whose extent can be divided by 8. """"""
    if isinstance(op, tvm.stmt.For):
        if isinstance(op.extent, tvm.expr.IntImm):
            if op.extent.value % 8 == 0:
                loops.append(op)",0.4247638583,
866,if statements,"#class LearningAgent(Agent):
    
def update(self, t):
    #some other code
        
    #execute action and get reward
    reward = self.env.act(self, action) 
    #this line is given in the starter code
        
    #update self.n_success, my code
    if reward > 2:
        self.n_success += 1",0.4209471941,
866,if statements,"expr  = False
expr2 = True

if expr:
    print 'statement'            # Note the indentation!
elif expr2:
    print 'statement 2'
else:
    print 'statment 3'

# The if statement ends here!
print 'New statement'            # Note the indentation!",0.4204854369,
866,if statements,"def add_statements(self, stmts):
    """"""Add INDRA Statements to the assembler's list of statements.

    Parameters
    ----------
    stmts : list[indra.statements.Statement]
        A list of :py:class:`indra.statements.Statement`
        to be added to the statement list of the assembler.
    """"""
    for stmt in stmts:
        self.statements.append(stmt)",0.4194476008,
866,if statements,"expr = True                     # expr is a boolean which is set to true. Try setting it to false!
if expr:                        # if checks if expr is true
    print 'statment'            # Note the indentation!
    
# The if statement ends here!
print 'New statement'           # Note the indentation!",0.4162917137,
866,if statements,"False      class      finally    is         return
None       continue   for        lambda     try
True       def        from       nonlocal   while
and        del        global     not        with
as         elif       if         or         yield
assert     else       import     pass
break      except     in         raise",0.4142606854,
1191,measuring the speed of light,"while True:
    d.measure()
    client.publish(""temphum"", ""{0:g}:{1:g}"".format(d.temperature(), d.humidity()))
    led.duty(0)
    time.sleep_ms(10)
    led.duty(1024)
    time.sleep(1)",0.4418331385,
1191,measuring the speed of light,"x = linspace(0, max_speed, 15)

# 1. Plot v-p points, empirical distribution
ecdf = sm.distributions.ECDF(df.speed)
y = ecdf(x)
plot(log(x), log(-log(1-y)),'o', label = 'Empirical')

# 2. Weibull distribution
weibull_params = sp.stats.exponweib.fit(df.speed, floc=0, f0=1)
y_cdf_weibull = sp.stats.exponweib.cdf(x, *weibull_params)
plot(log(x), log(-log(1-y_cdf_weibull)),'--', label = 'Weibull')

# 3. GMM distribution
y_ = [integrate.nquad(f, [[0, x_val],[0, 2*pi]]) for x_val in x]
y = array(list(zip(*y_))[0])
plot(log(x), log(-log(1-y)),'-', color='black', label = 'GMM')

plt_configure(xlabel='ln(V)',ylabel='ln(-ln(1-P))',legend={'loc':'best'})",0.4309567213,
1191,measuring the speed of light,"def clocksec():
     
    #set the turtle speed,color and pensize
    
        turtle.speed(""fastest"")
        turtle.pensize (1)  
        turtle.color(""black"")

        #insert a delay in turtle's behaviour, by default is 0, but if the number increase for example to 0.1, 
        #this will change the final result.
        time.sleep (0)

        #retrieve from the computer actual datetime object

        dt_now= datetime.datetime.now (tz=pytz.UTC)

        #convert the seconds from the actual time in a string 

        dtime = datetime.datetime.strftime (dt_now,'%S',)
        
        #convert the minutes from the actual time in a string  
        
        dminute= datetime.datetime.strftime (dt_now,'%M',)

        #convert the strings to  integers to determine the size of the circle
        #that is going to represent the seconds

        sizeline=int(dtime)
        sizeminutes=int(dminute)
        
        #set the heading angle of the turtle, to the actual second
        turtle.penup()
        turtle.setheading ((-360/60)*sizeline)
        turtle.goto(0,0)
        turtle.pendown()
       

        #make a circle with size of the ""seconds""
        turtle.color(""gray"")
        turtle.circle (sizeline*(sizeminutes/4))
        turtle.penup()
  
        #convert the datetime  object in a string that includes hour, minutes, seconds, the turtle will print this 
        #as the actual time
        dtimeprint=datetime.datetime.strftime (dt_now,'%H:%M:%S',)
        turtle.penup()
        turtle.goto(320,320)
        turtle.write(dtimeprint, True,align=""center"",font=(""arial"",15,""normal""))
        #erase the writing
        
        turtle.goto(320,320)
        turtle.setheading (0)   
        turtle.color(""white"")
        turtle.begin_fill()
        turtle.circle(120)
        turtle.end_fill()
        threading.Timer(1,clocksec).start()
      
        #start again",0.4269865155,
1191,measuring the speed of light,"def normality_test(array_like):
    jbvalue,jbp = sp.stats.jarque_bera(array_like)
    swvalue,swp = sp.stats.shapiro(array_like)
    print(""Jarque Bera statistic:"",jbvalue)
    print(""Jarque Bera p-value:"",jbp)
    print(""Shapiro statistic:"",swvalue)
    print(""Shapiro p-value:"",swp)",0.425293684,
1191,measuring the speed of light,"# Creating a function to orient the turtles starting point
def startpoint():
    for turtle in turtles:
        turtle. penup()
        turtle. speed(10)
        # Assigning a random range of values for the x and y co-ordinate of the turtle
        turtle. setposition(random.randrange(-350, 350), random.randrange(-350,350))",0.422205925,
1191,measuring the speed of light,"# Creating a function to orient the turtles starting point
def startpoint():
    for turtle in turtles:
        turtle. penup()
        turtle. speed(10)
        # Assigning a random range of values for the x and y co-ordinate of the turtle
        turtle. setposition(random.randrange(-350, 350), random.randrange(-350,350))
        difference = turtle.towards(0,0) # Finding the orientation difference to the origin
        turtle. right(360 - difference)  # Turning towards the origin by subtracting the difference from the whole angle",0.4220415354,
1191,measuring the speed of light,"def move_bike(n):
    bikeshare.olin -= n
    bikeshare.wellesley += n",0.419300735,
1191,measuring the speed of light,"def turtleClock():  
        for i in range(12):
            turtle.speed(""fastest"")
            turtle.penup()
            turtle.shape(""turtle"")
            turtle.goto(200,150)
            turtle.forward(80)
            turtle.pendown()
            turtle.forward(10)
            turtle.penup()
            turtle.forward(30)
            turtle.stamp()
            turtle.color('#%06x' % random.randint(0, 2**24 - 1))
            turtle.backward(120)
            turtle.right(30)
        time.sleep(1)",0.4185993671,
1191,measuring the speed of light,"def vel(rho):
    vel = Vmax*(1 - rho/rhomax)
    return vel",0.4152451754,
1191,measuring the speed of light,"# This function will be used by moviepy to create an output video
def process_image(img):
    # Example of how to use the Databucket() object defined above
    # to print the current x, y and yaw values 
    rover_yaw = data.yaw[data.count]
    rover_xpos = data.xpos[data.count]
    rover_ypos = data.ypos[data.count]
    #print(data.xpos[data.count], data.ypos[data.count], data.yaw[data.count])

    # TODO: 
    # 1) Define source and destination points for perspective transform
    dst_size = 5 
    # Set a bottom offset to account for the fact that the bottom of the image 
    # is not the position of the rover but a bit in front of it
    # this is just a rough guess, feel free to change it!
    bottom_offset = 6
    source = np.float32([[14, 140], [301 ,140],[200, 96], [118, 96]])
    destination = np.float32([[image.shape[1]/2 - dst_size, image.shape[0] - bottom_offset],
                  [image.shape[1]/2 + dst_size, image.shape[0] - bottom_offset],
                  [image.shape[1]/2 + dst_size, image.shape[0] - 2*dst_size - bottom_offset], 
                  [image.shape[1]/2 - dst_size, image.shape[0] - 2*dst_size - bottom_offset],
                  ])
    # 2) Apply perspective transform
    warped = perspect_transform(grid_img, source, destination)
    # 3) Apply color threshold to identify navigable terrain/obstacles/rock samples
    threshed = color_thresh(warped)
    # 4) Convert thresholded image pixel values to rover-centric coords
    xpix, ypix = rover_coords(threshed)
    dist, angles = to_polar_coords(xpix, ypix)
    mean_dir = np.mean(angles)
    # 5) Convert rover-centric pixel values to world coords
    scale = 100
    # Get navigable pixel positions in world coords
    x_world, y_world = pix_to_world(xpix, ypix, rover_xpos, 
                                rover_ypos, rover_yaw, 
                                data.worldmap.shape[0], scale)
    # Add pixel positions to worldmap
    # 6) Update worldmap (to be displayed on right side of screen)
    data.worldmap[y_world,x_world, 0] += 255

    # 7) Make a mosaic image, below is some example code
        # First create a blank image (can be whatever shape you like)
    output_image = np.zeros((img.shape[0] + data.worldmap.shape[0], img.shape[1]*2, 3))
        # Next you can populate regions of the image with various output
        # Here I'm putting the original image in the upper left hand corner
    output_image[0:img.shape[0], 0:img.shape[1]] = img

        # Let's create more images to add to the mosaic, first a warped image
    warped = perspect_transform(img, source, destination)
        # Add the warped image in the upper right hand corner
    output_image[0:img.shape[0], img.shape[1]:] = warped

        # Overlay worldmap with ground truth map
    map_add = cv2.addWeighted(data.worldmap, 1, data.ground_truth, 0.1, 0)
        # Flip map overlay so y-axis points upward and add to output_image 
    output_image[img.shape[0]:, 0:data.worldmap.shape[1]] = np.flipud(map_add)


        # Then putting some text over the image
    cv2.putText(output_image,""Populate this image with your analyses to make a video!"", (20, 20), 
                cv2.FONT_HERSHEY_COMPLEX, 0.4, (255, 255, 255), 1)
    data.count += 1 # Keep track of the index in the Databucket()
    
    return output_image",0.4145380855,
1104,load models tests,"def do_train():
    global model
    model = create_model()
    reader = create_reader(data_dir + ""/atis.train.ctf"", is_training=True)
    train(reader, model)
do_train()",0.4713255763,
1104,load models tests,"def custom_model(input_shape=None):

    if input(""If you want to load a model, enter 'yes'.\n"") == 'yes': 
        return my_func_utils.load_model()
    
    assert input_shape != None
    
    model = Sequential()

    model.add(Conv2D(filters = 32, kernel_size = (5,5), padding = 'Same', activation ='relu', input_shape = (28,28,1)))
    model.add(Conv2D(filters = 32, kernel_size = (5,5), padding = 'Same', activation ='relu'))
    model.add(MaxPool2D(pool_size=(2,2)))
    model.add(Dropout(0.25))


    model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu')) 
    model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))
    model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))
    model.add(Dropout(0.25))


    model.add(Flatten())
    model.add(Dense(256, activation = ""relu""))
    model.add(Dropout(0.5))
    
    model.add(Dense(10, activation = ""softmax""))

    
    optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-8, decay=0.0)
    
    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
    
    return model",0.4702246785,
1104,load models tests,"def load_vgg(sess, path):
    # load the model and weights
    tf.saved_model.loader.load(sess, ['vgg16'], path)
  
    # Get Tensors to be returned from graph
    graph = tf.get_default_graph()
    image_input = graph.get_tensor_by_name('image_input:0')
    keep_prob = graph.get_tensor_by_name('keep_prob:0')
    layer3 = graph.get_tensor_by_name('layer3_out:0')
    layer4 = graph.get_tensor_by_name('layer4_out:0')
    layer7 = graph.get_tensor_by_name('layer7_out:0')

    return image_input, keep_prob, layer3, layer4, layer7",0.4684761465,
1104,load models tests,"def load_vgg(model):
    #Grab layers from pretrained VGG
    tf.saved_model.loader.load(sess, [""vgg16""], ""./vgg/"")
    
    model['graph'] = tf.get_default_graph()
    
    #define key layers for us to work with, so we can take pieces of VGG16
    #for our own use
    model['input_layer'] = model['graph'].get_tensor_by_name(""image_input:0"")
    model['keep_prob'] = model['graph'].get_tensor_by_name(""keep_prob:0"") #Dropout settings
    
    #more layer grabbing
    model['layer_3'] = model['graph'].get_tensor_by_name(""layer3_out:0"")
    model['layer_4'] = model['graph'].get_tensor_by_name(""layer4_out:0"")
    model['layer_7'] = model['graph'].get_tensor_by_name(""layer7_out:0"")",0.4656842351,
1104,load models tests,"def make_xsvvnei_noh_model(name):
    """"""Create an instance of the 'xsvvnei' model which has
    no Hydrogen.""""""
    
    # Create the model instance
    ui.load_user_model(xsvvnei_no_h, name)
    
    # Get a copy of the default parameter settings for the vvnei
    # model (excluding Hydrogen).
    pars = [p for p in xspec.XSvvnei().pars if p.name != 'H']
    
    parnames = [p.name for p in pars]
    parvals = [p.val for p in pars]
    parmins = [p.min for p in pars]
    parmaxs = [p.max for p in pars]
    parunits = [p.units for p in pars]
    parf = [p.frozen for p in pars]
    
    # Set up the parameters for this instance
    ui.add_user_pars(name, parnames, parvals=parvals, parmins=parvals,
                     parmaxs=parmaxs, parunits=parunits, parfrozen=parf)
    
    # Return the model
    return ui.get_model_component(name)",0.4625197947,
1104,load models tests,"import time

def train_model(model_name):
    
    saver = tf.train.Saver()
    data_train_path = '/media/mat/ssdBackupMat/Datasets/sentiment/yelp_review_full_csv/data.train'
    start_time = time.time()
    PRINT_EVERY = 200

    sess = tf.Session()

    sess.run(init)
    coord = tf.train.Coordinator()
    tf.train.start_queue_runners(sess=sess, coord=coord)
    my_thread = queue_ctrl.start_thread(sess, coord, data_train_path) #start the publisher thread
    iteration, progress = 0, 0.
    tokens_processed = 0.
    
    print(""\nTraining model: %s"" % model_name)

    while progress < 100:
        _, loss_, n_tokens_ = sess.run([train_step, loss, n_tokens])
        tokens_processed += n_tokens_
        progress = tokens_processed / tokens_to_process *100.
        if iteration % PRINT_EVERY == 0:
            print(""Iter: %d, %.2f%% done, Minibatch loss: %.4f, Elements in queue: %d"" 
                  % (iteration, progress, loss_, sess.run(queue_ctrl.queue.size())))
        iteration += 1

    coord.request_stop()

    print(""Done. Exec time: %.2f minutes."" % ((time.time() - start_time) / 60.))
    save_path = saver.save(sess, model_name)

    coord.join(my_thread, stop_grace_period_secs=10)
    sess.close()",0.4514878392,
1104,load models tests,"def load_vgg(sess, vgg_path):
    model= tf.saved_model.loader.load(sess, ['vgg16'], vgg_path)
    
    graph =tf.get_default_graph()
    image_input=graph.get_tensor_by_name('image_input:0')
    keep_prob=graph.get_tensor_by_name('keep_prob:0')
    layer3=graph.get_tensor_by_name('layer3_out:0')
    layer4=graph.get_tensor_by_name('layer4_out:0')
    layer7=graph.get_tensor_by_name('layer7_out:0')
    return image_input, keep_prob, layer3, layer4, layer7",0.4512526691,
1104,load models tests,"def load_model(file_path):
    """"""Returns a keras model (compressed as .h5).""""""
    try:
        model = keras.models.load_model(file_path)
    except Exception as e:
        raise e
        
    return model",0.4488325119,
1104,load models tests,"def print_confusion_matrix(zero_true=False):
    cls_pred = session.run(y_pred_cls, feed_dict_test)
    cm = confusion_matrix(data.test.cls, cls_pred, labels=range(num_classes))
    
    print(cm)
    
    if zero_true:
        cm[range(num_classes), range(num_classes)] = 0
        
    plt.imshow(cm, cmap=plt.cm.Reds, interpolation='nearest')
    plt.colorbar()
    tick_marks = np.arange(num_classes)
    plt.xticks(tick_marks, rotation=45)
    plt.yticks(tick_marks, rotation=45)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.grid(False)
    plt.show()",0.4464479685,
1104,load models tests,"def get_data():
    data_files = glob(os.path.join(DATA_PATH, ""*.jpg""))
    
    rdd_train_images = sc.parallelize(data_files[:100000]) \
                              .map(lambda path: get_image(path, image_size).transpose(2, 0, 1))

    rdd_train_sample = rdd_train_images.map(lambda img: Sample.from_ndarray(img, [np.array(0.0), img]))
    return rdd_train_sample",0.4382373095,
487,displaying the data,"def display(data):
    plt.imshow(transformer.deprocess('data', data))

def get_label_name(num):
    options = labels[num].split(',')
    # remove the tag
    options[0] = ' '.join(options[0].split(' ')[1:])
    return ','.join(options[:2])
    
def predict(data, n_preds=6, display_output=True):
    net.blobs['data'].data[...] = data
    if display_output:
        display(data)
    prob = net.forward()['prob']
    probs = prob[0]
    prediction = probs.argmax()
    top_k = probs.argsort()[::-1]
    for pred in top_k[:n_preds]:
        percent = round(probs[pred] * 100, 2)
        # display it compactly if we're displaying more than the top prediction
        pred_formatted = ""%03d"" % pred
        if n_preds == 1:
            format_string = ""label: {cls} ({label})\ncertainty: {certainty}%""
        else:
            format_string = ""label: {cls} ({label}), certainty: {certainty}%""
        if display_output:
            print format_string.format(
                cls=pred_formatted, label=get_label_name(pred), certainty=percent)
    return prob",0.4816625118,
487,displaying the data,"def print_offsets(d):
    print(""offsets:"", [d.fields[name][1] for name in d.names])
    print(""itemsize:"", d.itemsize)
print_offsets(np.dtype('u1,u1,i4,u1,i8,u2'))",0.4501824081,
487,displaying the data,"def describe_data():
    df = pd.DataFrame.from_csv(""training_data/all_raw_data.csv"")
    df = df.describe()
    df.to_csv(""training_data/all_raw_data_description.csv"") 
    print(""\nRaw training data description:\n"",df)
    
    df = pd.DataFrame.from_csv(""training_data/all_processed_data.csv"")
    df = df.describe()
    df.to_csv(""training_data/all_processed_data_description.csv"") 
    print(""\nProcessed training data description:\n"",df)",0.4460044503,
487,displaying the data,"def show_join_results(df):
    print('Columns:')
    print(','.join(df.columns))
    print('\nTable:')
    df.select('House Name','price_category','prize_range','drill_cate').sort('House Name').show(10)",0.4427265823,
487,displaying the data,"def describe_columns(df):
    for i in df.columns:
        print('Column: ' + i)
        titanic.select(i).describe().show()",0.4375396073,
487,displaying the data,"def explore(dataset):
    print ""Columns: "" + "" | "".join(dataset.columns)
    print ""Summary statistics \n""
    print dataset.describe()

explore(aw_db)",0.4362560213,
487,displaying the data,"def CalMeanVar(df):
    '''
    This function calculates mean and variance of all features (excluding target and 
    dummy columns)
    Input: data frame
    Output: None
    
    '''
    
    for i in range(2, len(df.columns)):
        print(""{} | mean: {} | var: {}"".format(df.columns[i], 
                                               df.iloc[:, i].mean(), 
                                               df.iloc[:, i].var()))
    return

CalMeanVar(data_train[data_train.y == 1.0])",0.4350033402,
487,displaying the data,"def pretty_print_text_and_label(input_data, i):
    print(input_data.Label[i] + ""\t:\t"" + input_data.Text[i][:80] + ""..."")",0.4338288307,
487,displaying the data,"#Workspace
##data_url is now out of date, needs fix
def display_raw_data(data):
    '''
    Takes the data of one song file and prints or plots various information
    to gain an understanding of the object and data shapes
    '''
    print 'Data class: ', type(data) # what is the python data type?
    print 'Keys: ', data.keys()
    print 'Header: ', data['__header__']
    print 'Globals: ', data['__globals__']
    print 'Versions: ', data['__version__']
    print 'length of sng array:', len(data['sng']) # need to plot this as so long

    plt.plot(data['sng'], label='sng (audio)', linewidth=0.5)
    plt.plot(data['ref'][0], label='ref', linewidth=0.5)
    plt.plot(data['e1'][0], label='e1', linewidth=0.5)
    plt.plot(data['e2'][0], label='e2', linewidth=0.5)
    plt.plot(data['e3'][0], label='e3', linewidth=0.5)
    plt.legend()
    plt.title('Raw Data plot')
    plt.ylabel('Signal / mV')
    plt.xlabel('Data point')

display_raw_data(sp.io.loadmat(gen_path('blk161', 'sng163839_0308v3.mat'))) # load the data in the argument area.",0.4263788462,
487,displaying the data,"def MAP12_Accuracy(test_data):
    print(""Now calculating MAP@12 accuracy, should take about 5 minutes"")
    test_displays = test_data.display_id.unique()
    acc_counter = 0
    counter = 0
    for display in test_displays:
        display_df = test_data[test_data.display_id == display]
        true_ad = np.array(display_df[display_df.clicked == 1][[""ad_id""]])[0][0]
        
        # Sorting ads according to predicted click probability
        display_probs = np.array(display_df.probability_of_click)
        ads = np.array(display_df.ad_id)
        idx_sorted = display_probs.argsort()[::-1]
        ads_sorted = ads[idx_sorted]
        
        # Finding index of true ad in the array
        idx_true = np.argwhere(ads_sorted == true_ad)[0][0] + 1
        acc_counter += 1 / idx_true
        counter += 1

    accuracy_map = acc_counter / len(test_displays)
    return accuracy_map",0.4259093404,
2356,the number of trees,"# We could use some dynamic programming to be able to generate paths that yield n transitions exactly.
# INstead we randomly generate transitions on the tree until we get the desired number.
# We have two states: ancestral (0) and convergent (1).
# We count the numbers of transitions
def randomTransitions(numTransitions, tree):
    numberOfNodes = len(tree.get_tree_root().get_descendants()) + 1
    rate = float(numTransitions)/float(numberOfNodes)
    ancestralTransition=dict()
    totalNumberOfTransitions = 0
    nodesWithTransitions = list()
    for node in tree.traverse(""levelorder""):
        if node.is_root() :
            ancestralTransition[node] = False
        elif ( ancestralTransition[node.up] == True):
            ancestralTransition[node] = True
        else :
            sisterHasAlreadyTransitioned=False
            if ancestralTransition.__contains__(node.get_sisters()[0]): #Here we assume binary trees!
                sisterHasAlreadyTransitioned=True
            #randomly draw whether we do a transition or not
            transitionBool = stats.bernoulli.rvs(rate, size=1)[0] == 1
            if (transitionBool and not sisterHasAlreadyTransitioned):
                ancestralTransition[node] = True
                nodesWithTransitions.append(node)
                totalNumberOfTransitions = totalNumberOfTransitions + 1
            else:
                ancestralTransition[node] = False
    return nodesWithTransitions, totalNumberOfTransitions, ancestralTransition
        
        
        
def placeNTransitionsInTree(numTransitions, tree):
    observedNumTransitions = 2*numTransitions
    nodesWithTransitions = list()
    numTries = 0
    convergentModel = dict()
    while observedNumTransitions != numTransitions and numTries < 100:
        observedNumTransitions = 0
        nodesWithTransitions, observedNumTransitions, convergentModel = randomTransitions(numTransitions, tree)
        print (""Observed Number of Transitions: ""+ str(observedNumTransitions ) + "" compared to ""+ str(numTransitions) + "" wanted"")
        numTries = numTries + 1
    if numTries < 100:
        for n in nodesWithTransitions:
            print(n.get_ascii())
    else:
        print(""It seems like it is too difficult to place ""+ str(numTransitions) + "" events in this tree."")
    return convergentModel",0.4289913177,
2356,the number of trees,"class RTree:
    def __init__(self, tree_depth):
        self.levels = tree_depth - 1  # do not count leaves
        self.num_leaves = 2 ** self.levels
        self.num_nodes = self.num_leaves - 1
        # nodes hold splits
        self.split = [None, ]  # node indexing starts at 1
        self.leaf = []

    def train(self, db):
        # tree fitting operates only on residuals; residuals are set by the forest training
        TRIALS_PER_SPLIT = 20
        
        # initialize empty tree
        self.split = [None, ]  # node indexing starts at 1
        self.leaf = []
        for t in db:
            t.node = 1
            t.tmp_node = 0

        # go through nodes in order
        node = 0
        for level in xrange(0, self.levels):
            for k in xrange(0, 2 ** level):
                node += 1
                best = {
                    'split': None,
                    'mean': None,
                    'error': 1e20,
                }
                # select greedily the best split per node over N trials
                for n in xrange(0, TRIALS_PER_SPLIT):
                    split = DataObject.get_split()
                    mean = [DataObject.empty_shape(), DataObject.empty_shape()]
                    count = [0, 0] 
                    # compute the average residuals for left and right subtrees 
                    for t in (i for i in db if i.node == node):
                        ix = t.test_split(split, t.warping)
                        # potential next node if we choose this split:
                        t.tmp_node = 2 * node + ix
                        mean[ix] = mean[ix] + t.residual
                        count[ix] += 1
                    for ix in [0, 1]:
                        mean[ix] = mean[ix] / (count[ix] if count[ix] > 0 else 1)
                    # now compute the approximation error for this split
                    error = 0
                    for t in (i for i in db if i.node == node):
                        avg_residual = mean[t.tmp_node & 1]
                        error += np.square(t.residual - avg_residual).mean()
                    if (error < best['error']):
                        best = {'split': split, 'mean': mean, 'error': error}
                        for t in (i for i in db if i.node == node):
                            t.best_node = t.tmp_node
                # best split found :
                self.split.append(best['split'])
                for t in (i for i in db if i.node == node):
                    t.node = t.best_node
                # set leaves' values if last level
                if level == self.levels - 1:
                    self.leaf.append(best['mean'][0])
                    self.leaf.append(best['mean'][1])
        print ""*"",
    print
    
    def estimate(self, dataobject, prev_est=None):
        # use tree to estimate shape from DataObject (using optional starting point)
        if prev_est is None:
            prev_est = dataobject.mean_shape() 
        warping = dataobject.find_warping(prev_est)
        return prev_est + self.regress(dataobject, warping)
    
    def regress(self, dataobject, warping):
        # use regression tree on dataobject to find shape displacement
        ix = 1
        for k in xrange(0, self.levels):
            if dataobject.test_split(self.split[ix], warping):
                ix = 2 * ix + 1
            else:
                ix = 2 * ix
        return self.leaf[ix - self.num_nodes - 1]",0.4083686769,
2356,the number of trees,"def accom(P):
    if not P.accom:
        return 0
    
    path2 = os.getcwd() + '/AccomBoundsA' + str(P.livingArea) + '.csv'
    data2 = pd.read_csv(path2, header=0)
    
    row = 0
    if P.couple:
        if P.numKids > 0: row = 2 
        else: row = 1
    else:
        if P.numKids == 0: row = 0 
        if P.numKids == 1: row = 3 
        else: row = 4

    cutoff = deDollar(data2.iloc[row,4])
    threshold = deDollar(data2.iloc[row,3])
    maxpay = deDollar(data2.iloc[row,6])
    if P.wibt() > cutoff:
        return 0
    
    path3 = os.getcwd() + '/AccomThreshold.csv'
    data3 = pd.read_csv(path3, header=0)
 
    if P.renting: col = 1
    else: col = 2
    rentthreshold = deDollar(data3.iloc[row,col])
    sub = 0
    if P.wHousingCost > rentthreshold:
        sub = (P.wHousingCost - rentthreshold)*0.7
    if sub>maxpay: sub = maxpay
    if P.wibt()>threshold:
        sub = sub - (P.wibt()-threshold)*0.25
    if sub < 0:
        sub = 0

    return sub

# https://www.workandincome.govt.nz/map/income-support/extra-help/accommodation-supplement/income-non-beneficiaries-01.html
Person1.wHousingCost = 800
accom(Person1)",0.4047108889,
2356,the number of trees,"def getLeaves1(root):
    l,r = 0,0
    if ((not root.leftChild) and (not root.rightChild)):
        return 1
    if root.leftChild:
        l = getLeaves1(root.leftChild)
    if root.rightChild:
        r = getLeaves1(root.rightChild)
    return l+r",0.4034191966,
2356,the number of trees,"# Passion Score
def passion_score(df):
    if len(df.customer_id.unique()) == 0:
        passion_score = 0
    else:
        passion_score = round(len(df)/len(df.customer_id.unique()),2)
    return passion_score",0.4033020139,
2356,the number of trees,"def getwidth(tree):
    if tree.tchild==None and tree.fchild==None: return 1
    return getwidth(tree.tchild)+getwidth(tree.fchild)

def getdepth(tree):
    if tree.tchild==None and tree.fchild==None: return 0
    return max(getdepth(tree.tchild),getdepth(tree.fchild))+1




def drawtree(tree,jpeg='tree.jpg'):
    
    try:
        from PIL import Image,ImageDraw
    except ImportError:
        sys.exit(""""""You need PIL.
                Install it from https://pypi.python.org/pypi/PIL."""""")


    
    w=getwidth(tree)*100
    h=getdepth(tree)*100+120

    img=Image.new('RGB',(w,h),(255,255,255))
    draw=ImageDraw.Draw(img)

    drawnode(draw,tree,w/2,20)
    img.save(jpeg,'JPEG')

    
def drawnode(draw,tree,x,y):
    if tree.results==None:
        # Get the width of each branch
        w1=getwidth(tree.fchild)*100
        w2=getwidth(tree.tchild)*100

        # Determine the total space required by this node
        left=x-(w1+w2)/2
        right=x+(w1+w2)/2

        # Draw the condition string
        draw.text((x-20,y-10),str(tree.col)+':'+str(tree.value) 
                  +'? (entropy: {0:.3f})'.format(tree.entropie),(0,0,0))

        # Draw links to the branches
        draw.line((x,y,left+w1/2,y+100),fill=(255,0,0))
        draw.line((x,y,right-w2/2,y+100),fill=(255,0,0))
    
        # Draw the branch nodes
        drawnode(draw,tree.fchild,left+w1/2,y+100)
        drawnode(draw,tree.tchild,right-w2/2,y+100)
    else:
        txt=' \n'.join(['%s:%d'%v for v in tree.results.items()])
        draw.text((x-20,y),txt,(0,0,0))",0.3930556178,
2356,the number of trees,"def name_internal_nodes(T):
    i = 1
    for node in T.traverse():
        if not node.is_leaf():
            node.name = str(i) + ""_""
            i += 1
    return None

def find_selected_subclones_cached(lineage_uid, T, annotations_FayAndWusH,
                                   pvalue_cutoff=0.05):
    # Traverse tree and keep subclones with significant Fay and Wu's H
    
    hits = []
    
    def stop(node):
        if node.name == ""germline"": return False
        if node.name == ""1_"": return False
        if node.is_leaf(): return True
        if len(node) < 100:
            # print ""Stopping, branch too small"", node.name, len(node)
            return True
        elif annotations_FayAndWusH[lineage_uid].loc[node.name][""pvalue_kingman""] < pvalue_cutoff:
            print ""Hit branch"", node.name
            return True
        else:
            return False
        
    # Traverse tree and find selected nodes
    for node in T.traverse(""levelorder"", is_leaf_fn=stop):
        if node.is_leaf(): continue
        if node.name == ""1_"": continue
        if len(node) < 100: continue # distributions of H under BSC and Kingman become hard to distinguish near N = 100
        
        if annotations_FayAndWusH[lineage_uid].loc[node.name][""pvalue_kingman""] < pvalue_cutoff:
            myHit = annotations_FayAndWusH[lineage_uid].loc[node.name]
            hits.append(myHit)
    
    return hits

# Traverse tree and keep subclones with significant Fay and Wu's H
# Report number of comparisons

def find_selected_subclones_cached_Ncomparisons(lineage_uid, T, annotations_FayAndWusH, pvalue_cutoff=0.05):

    hits = []
    
    def stop(node):
        if node.name == ""germline"": return False
        if node.name == ""1_"": return False
        if node.is_leaf(): return True
        if annotations_FayAndWusH[lineage_uid].loc[node.name][""pvalue_kingman""] < pvalue_cutoff:
            # print ""Hit branch"", node.name
            return True
        elif len(node) < 100:
            # print ""Stopping branch (too small)"", node.name
            return True
        else:
            return False

    N_comparisons = 0
    # Traverse tree and find selected nodes
    for node in T.traverse(""levelorder"", is_leaf_fn=stop):
        if node.is_leaf(): continue
        if node.name == ""1_"": continue
        if len(node) < 100: continue # distributions of H under BSC and Kingman become hard to distinguish near N = 100
            
        N_comparisons += 1
        
        if annotations_FayAndWusH[lineage_uid].loc[node.name][""pvalue_kingman""] < pvalue_cutoff:
            myHit = annotations_FayAndWusH[lineage_uid].loc[node.name]
            hits.append(myHit)
    
    return hits, N_comparisons

def find_selected_subclones_cached_dynamicFDR(lineage_uid, T, annotations_FayAndWusH,
                                              total_FDR_cutoff=0.05, max_iterations=10):
    N_comparisons = 1
    N_comparisons_prev = 0
    iterations = 0
    while N_comparisons != N_comparisons_prev:
        iterations += 1
        N_comparisons_prev = N_comparisons
        my_pvalue_cutoff = total_FDR_cutoff / float(N_comparisons_prev)
        hits, N_comparisons = find_selected_subclones_cached_Ncomparisons(lineage_uid, T,
                                                                          annotations_FayAndWusH,
                                                                          pvalue_cutoff=my_pvalue_cutoff)
        if iterations >= max_iterations:
            print ""No convergence in"", max_iterations, ""iterations""
            print ""Current N_comparisons"", N_comparisons
            print ""Current len(hits)"", len(hits)
            return hits, N_comparisons
        
        if N_comparisons == 0:
            print ""No comparisons made""
            return hits, N_comparisons
        
    return hits, N_comparisons",0.3928382993,
2356,the number of trees,"def get_total_branch_length(ts):
    current = 0
    total_branch_length = np.zeros(ts.num_trees)
    for index, (_, edges_out, edges_in) in enumerate(ts.edge_diffs()):
        for edge in edges_out:
            current -= ts.node(edge.parent).time - ts.node(edge.child).time
        for edge in edges_in:
            current += ts.node(edge.parent).time - ts.node(edge.child).time
        total_branch_length[index] = current
    return total_branch_length",0.3926962614,
2356,the number of trees,"def survived_by_catagory(catagory):
    survived_by_catagory = df[df.Survived == 1].groupby(catagory).count()['Survived']
    return survived_by_catagory",0.390406847,
2356,the number of trees,"def getwidth(tree):
  if tree.tb==None and tree.fb==None: return 1
  return getwidth(tree.tb)+getwidth(tree.fb)

def getdepth(tree):
  if tree.tb==None and tree.fb==None: return 0
  return max(getdepth(tree.tb),getdepth(tree.fb))+1


from PIL import Image,ImageDraw,ImageFont

def drawtree(tree,jpeg='tree.jpg'):
  w=getwidth(tree)*100
  h=getdepth(tree)*100+120

  img=Image.new('RGB',(w,h),(255,255,255))
  draw=ImageDraw.Draw(img)

  drawnode(draw,tree,w/2,20)
  img.save(jpeg,'JPEG')
  
def drawnode(draw,tree,x,y):
  if tree.results==None:
    # Get the width of each branch
    w1=getwidth(tree.fb)*100
    w2=getwidth(tree.tb)*100

    # Determine the total space required by this node
    left=x-(w1+w2)/2
    right=x+(w1+w2)/2

    # Draw the condition string
    draw.text((x-20,y-10),str(tree.col)+':'+str(tree.value),(0,0,0))

    # Draw links to the branches
    draw.line((x,y,left+w1/2,y+100),fill=(255,0,0))
    draw.line((x,y,right-w2/2,y+100),fill=(255,0,0))
    
    # Draw the branch nodes
    drawnode(draw,tree.fb,left+w1/2,y+100)
    drawnode(draw,tree.tb,right-w2/2,y+100)
  else:
    txt=' \n'.join(['%s:%d'%v for v in tree.results.items()])
    draw.text((x-20,y),txt,(0,0,0))",0.388594836,
229,collaborators,"def observe():
    global agents;
    cla();
    white = [ag for ag in agents if ag.type==0];
    black = [ag for ag in agents if ag.type==1];
    plot([ag.x for ag in white],[ag.y for ag in white], 'wo');
    plot([ag.x for ag in black],[ag.y for ag in black], 'ko');
    axis('image');
    axis([0,1,0,1]);",0.4071747959,
229,collaborators,"def run():
    all_results = []
    for i in range(10):
        c = context(num_agents=1000)
        results = c.run(1000)
        plt.plot(range(len(results)), results)
        all_results.append(results)
    plt.show()

run()",0.3903470635,
229,collaborators,"def calculate_standard_split_test(d):
    arm = d.arm.values[0]
    if arm == 'A':
        users = A_convert(users=24*40)
    elif arm == 'B':
        users = B_convert(users=24*15)
    elif arm == 'C':
        users = C_convert(users=24*15)
    elif arm == 'D':
        users = D_convert(users=24*15)
    converted = np.sum(users)
    lost = len(users) - converted
    d['conv'] = converted
    d['lost'] = lost
    d['rate'] = float(converted)/(converted+lost)
    return d

st = []
for arm in ['A','B','C','D']:
    st_arm = pd.DataFrame()
    st_arm['day'] = np.arange(est_days_req)
    st_arm['arm'] = arm
    st.append(st_arm)
    
st = pd.concat(st, axis=0)
st = st.groupby(['arm','day']).apply(calculate_standard_split_test).reset_index(drop=True)
st.head()

print st.shape",0.3841593564,
229,collaborators,"def createStreamingContext():
    conf = SparkConf().setAppName(""Streaming Clicks"") \
                      .setMaster(""local[2]"")
    sc = SparkContext(conf=conf) 
    ssc = StreamingContext(sc, batchDuration)
    ssc.checkpoint(checkpointDirectory)   # set checkpoint directory
    return ssc",0.3807300925,
229,collaborators,"def pipe(data):
    
    global activities
    
    url = data['url']
    stopList = data['stopList']
    cat = data['categoria']
    
    # Create the activity and add to an list
    activities.append(generateCatActivity(cat))
    
    # Download the data from url
    bs = downloadData(url)

    # Store the relevant content (in html) in a list
    data = extractData(bs,stopList)

    # Extract the useful parts from the list (name, description, etc...)
    data = generateUsefulData(data)

    # Get the name of subcats ordered alphabetically
    subcats = getSubCats(data)

    # Generate the files
    genFiles(data,cat,subcats,cat)",0.379974544,
229,collaborators,"def getR_hat_mixed(params):
    global S_icm, S_icm2, S_user, S_urm, S_cbf_full, S_mf, urm, tg_playlist, evaluator
    print(params)

    S = S_icm * params[0] + S_urm * params[1] + S_user * params[2] + S_cbf_full * params[3] + S_icm2 * params[4]
    
    # maybe it's possible to avoid, who knows?
    S = top_k_filtering(S.transpose(), 300).transpose()
    
    s_norm = S.sum(axis=0)
    s_norm[s_norm==0] = 1
    # Normalize S
    S = S.multiply(csr_matrix(np.reciprocal(s_norm)))

    # Keep only the target playlists in the URM
    urm_cleaned = urm[[dataset.get_playlist_index_from_id(x)
                       for x in tg_playlist]]

    # Compute ratings
    R_hat = urm_cleaned.dot(S.tocsc()).tocsr()

    # Remove the entries in R_hat that are already present in the URM
    urm_cleaned = urm_cleaned[:, [dataset.get_track_index_from_id(x)
                                  for x in tg_tracks]]
    R_hat[urm_cleaned.nonzero()] = 0
    R_hat.eliminate_zeros()

    return R_hat",0.3788025975,
229,collaborators,"def refresh_structure_view():
    global viewer, atoms
    if hasattr(viewer, ""component_0""):
        #viewer.clear_representations()
        viewer.component_0.remove_ball_and_stick()
        viewer.component_0.remove_ball_and_stick()
        viewer.component_0.remove_ball_and_stick()
        viewer.component_0.remove_unitcell()
        cid = viewer.component_0.id
        viewer.remove_component(cid)

    viewer.add_component(nglview.ASEStructure(atoms)) # adds ball+stick
    viewer.add_unitcell()
    viewer.center()",0.3782292008,
229,collaborators,"def dealing_auto():
    '''
    Function to automatically deal first two cards to each player and dealer but leave 2nd dealer card hidden
    '''
    # each player receives his or her card
    global player_dict, d
    for i in player_dict.values():
        i.cards.clear()
        cardface, point= d.deal()
        #print(""\n{} is a dealt a(n) {}:\n"".format(i.name.capitalize(),cardface))
        i.accept_card(cardface)
        i.track_cardTotal(point)
    for i in list(player_dict.values())[:(len(player_dict)-1)]:
        cardface, point= d.deal()
        #print(""\n{} is a dealt a(n) {}:\n"".format(i.name.capitalize(),cardface))
        i.accept_card(cardface)
        i.track_cardTotal(point)
    for i in list(player_dict.values())[:-1]:
        cardface, point= d.deal()
        dealer_cardface_2 = cardface
        dealer_point_2 = point
    return dealer_cardface_2, dealer_point_2
    clear_output()
    print_tally()       

def dealing_choice():
    '''
    Function to deal cards to each player if she opts to hit. 
    '''
    # each player receives his or her card
    global player_dict, d
    clear_output()
    print_tally()
    for i in list(player_dict.values())[:(len(player_dict)-1)]:
        if i.cardTotal > 21: 
            print('\nSkipping {}. Exceeds 21.'.format(i.name.capitalize()))
            continue
        else:
            while True:
                hold_or_hit = input('{}: Do you wish to take a card? y/n '.format(i.name.capitalize()))            
                if hold_or_hit.lower().startswith('y'):
                    card,point= d.deal()
                    i.accept_card(card)
                    i.track_cardTotal(point)
                    if i.cardTotal > 21: break
                    else: 
                        clear_output()
                        print_tally()
                        continue
                elif hold_or_hit.lower().startswith('n'): break
                else: continue
            clear_output()
            print_tally()
            continue

def faceUp_dealer(card,point):
    '''
    Function to show last card of dealer and to auto-deal while dealer is below 17
    '''
    # dealer turns face up its card
    global player_dict, d
    player_dict['Dealer'].accept_card(card)
    player_dict['Dealer'].track_cardTotal(point)
    while True:
        if player_dict['Dealer'].cardTotal <17:
            card2, point2= d.deal()
            player_dict['Dealer'].accept_card(card2)
            player_dict['Dealer'].track_cardTotal(point2)
            continue
        else:
            break
    clear_output()
    print_tally()",0.378228724,
229,collaborators,"def callback(data):
    rospy.loginfo(rospy.get_caller_id() + ""I heard %s"", data.data)",0.374904871,
229,collaborators,"def run1():
    tensor_board = TensorBoard(log_dir=""../tensorboardlog/baseline"")
    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=0, mode='auto')

    model = Sequential()
    model.add(LSTM(128, return_sequences=True, activation='tanh', input_shape=(None, 2)))
    model.add(LSTM(128, return_sequences=True, activation='tanh', dropout=0.5))
    model.add(TimeDistributed(Dense(1, activation='linear')))
    model.compile(loss='mean_squared_error', optimizer='adam', metrics=[mse])

    model.summary()


    fit = model.fit(X_train_p[:,i_fit,:], y_train_p[:,i_fit,:], epochs=1000,
              validation_data=(X_train_p, y_train_p),
              verbose=2, callbacks=[early_stopping, tensor_board])
    return fit
run1()",0.3719893098,
1532,plotting query results,"Chart(engineer_totals).mark_bar().encode(
    x=X('Wages',bin=True),
    y='Prob:Q',
    title
    )",0.5255795717,
1532,plotting query results,"figsize(15,5)
df.df[df.df.hit_query=='InvG'].hit_evalue.order().plot(logy=True, kind='bar')",0.5244469047,
1532,plotting query results,"pandas.Series(.values.ravel()).hist(bins=101, figsize=(20, 6))
    gca().set_title('A distribution of edit distances.');",0.5203829408,
1532,plotting query results,"comp.plotMetricData(coaddM5_r,
                    plotFunc=plots.HealpixHistogram(),
                    paramTitles=False,
                    userPlotDict={'figsize':(8,8),'xMin':-0.65,'xMax':0.65,'zp':27.6},outDir='.',savefig=True)",0.520026505,
1532,plotting query results,"def plot_parity(single_shot_data, title=""Bell test results""):
    plt.hist([parity[(u, v)] for (u, v) in single_shot_data])
    plt.xticks(list(parity.values()), parity.keys())
    plt.title(title)
    ;",0.5157260299,
1532,plotting query results,"%%R
sumList<-list()
Hplots<-list()
outlierList<-list()

for (term in 1:length(clusterVar)){
    Hplots[[term]]<-qplot(clusterVar[[term]]$'val', xlab = term, geom=""histogram"")
    sumList[[term]]<-capture.output(summary(clusterVar[[term]]['val']))
    outlierList[[term]]<-grubbs.test(clusterVar[[term]]$'val',two.sided=TRUE)
}",0.5116595626,
1532,plotting query results,"EDA.Churn.value_counts().plot.pie(autopct='%1.1f%%')
plt.title('Churn Rate')
plt.tight_layout()",0.5105929375,
1532,plotting query results,"if gpm.overdispersion:
    plt.figure(figsize=(8, 6))

    inferred_means = gpm.nodes['overdispersion'].expected_x().ravel()
    plt.hist(inferred_means, bins=500, normed=True, alpha=0.25);
    plt.title('Overdispersion effects');
    plt.ylim(0, .026);",0.5097412467,
1532,plotting query results,"if gpm.overdispersion:
    plt.figure(figsize=(8, 6))

    inferred_means = gpm.nodes['overdispersion'].expected_x().ravel()
    plt.hist(inferred_means, bins=500, normed=True, alpha=0.25);
    plt.title('Overdispersion effects');
    plt.ylim(0, 2);",0.5097412467,
1532,plotting query results,"data.loc[data['room_type']=='Entire home/apt', 'price'].plot.hist(bins=60, logy=True, figsize=(20, 6), xlim=(0, 3000))",0.5095723867,
1944,social media,"%%html
<urth-core-channel id='itemChannel' name='initialized'>
    <urth-core-channel-item key='myvar' value='awesome'></urth-core-channel-item>
    <urth-core-channel-item key='isAwesome' value=true></urth-core-channel-item>
    <urth-core-channel-item key='otherStuff' value='{ ""json"": true, ""foo"": ""bar""}'></urth-core-channel-item>
</urth-core-channel>
<template is='urth-core-bind' channel='initialized'>
    <div>myvar is <span>[[myvar]]</span></div>
    <div>isAwesome is <span>[[isAwesome]]</span></div>
    <div>otherStuff.json is <span>[[otherStuff.json]]</span></div>
</template>",0.4093115926,
1944,social media,"[{'type': node',
  {'fuel': 
     {'GTL_diesel': 'yes',
      'biodiesel': 'no',
      'diesel': 'yes',
      'octane_100': 'yes',
      'octance_98': 'no'}
   ...]",0.4021520615,
1944,social media,"def fetch_and_classify(screen_name, classifier):
    friends = collect.get_friends(screen_name=screen_name)
    pprint(classifier.classify(friends))",0.3998913467,
1944,social media,"def event_data(performer_name):
    performer = sql(""""""
SELECT gid AS mbid,
       name
  FROM artist
 WHERE name=%(performer_name)s;
    """""", performer_name=performer_name)
    performer_mbid = performer['mbid'][0]

    df = sql(""""""
SELECT e.name            AS event_name,
       to_date(to_char(e.begin_date_year, '9999') || 
               to_char(e.begin_date_month, '99') || 
               to_char(e.begin_date_day, '99'), 'YYYY MM DD') AS start,
       to_date(to_char(e.end_date_year, '9999') || 
               to_char(e.end_date_month, '99') || 
               to_char(e.end_date_day, '99'), 'YYYY MM DD') AS end,
       e.gid             AS mbid
  FROM event          AS e
  JOIN l_artist_event AS lae ON e.id = lae.entity1
  JOIN artist         AS a   ON a.id = lae.entity0
  JOIN link           AS l   ON l.id = lae.link
 WHERE a.gid = %(performer_mbid)s
   ;"""""", performer_mbid=performer_mbid)
    return df

event = event_data('Radiohead')",0.3994689882,
1944,social media,"# Harvest the follower IDS. Note that this process takes a bit of time since
# you are limited to harvesting 75k IDs per 15 minute window.
# 
# Some example accounts that we'll look at later in this notebook:
#
# Tim O'Reilly: ~1.7M followers (~6 hours)
# Marissa Mayer: ~460k followers (~1.5 hours)
# Lady Gaga: ~40M followers (~5.5 days)
#
# All in all, it takes a little over a week to harvest all of these IDs.


# Define a simple wrapper that accepts a list of screen names for convenience
# and reusability later.

def harvest_followers_ids(screen_names=[]):
    for screen_name in screen_names:
        store_friends_followers_ids(twitter_api, screen_name=screen_name, 
                                    friends_limit=0, database=screen_name)

        
harvest_followers_ids(screen_names=[ 'timoreilly' ])

print ""Done""",0.3973874748,
1944,social media,"class Tweet:

    #Api_call
    def load_api(self):
        CONSUMER_KEY = 'ConsumerKey'
        CONSUMER_SECRET = 'Consumer secret key'
        oAuthToken = 'Authorisation token'
        OAUTH_TOKEN_SECRET= 'Authorisation token secret key'
        auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)
        auth.set_access_token(oAuthToken, OAUTH_TOKEN_SECRET)
        return tweepy.API(auth) 
    
    #creating a db in mongodb
    def create_db(self):
        self.client = MongoClient()
        self.dbNames=self.client.list_database_names()
        if ""tweet_db"" in self.dbNames:
            self.db=self.client.tweet_db
            self.tcollection=self.db.tweet_collection
            self.pointer=self.tcollection.find({})
            return self.pointer
            
        else:
            self.db=self.client.tweet_db
            self.t_collection=self.db.tweet_collection
            self.t_collection.create_index([(""id"",pymongo.ASCENDING)],unique=True)
            self.get_tweet()
        
        #query needed for the api call
    def get_tweet(self):
        self.max_count=100
        self.query=""rahul gandhi""
        self.Ap=self.load_api()
        self.results=self.Ap.search(q= self.query,count= self.max_count)
        #for r in self.results:
          #  print(r)
        self.save_tweet()
    
    #inserting required attributes in db
    def save_tweet(self):
        for r in self.results:
            individual_tweet=({""id"":r.id,""user_id"":r.user.id,""user_name"":r.user.name,""tweet_text"":r.text,""created_at"":r.created_at})
            self.t_collection.insert_one(individual_tweet)
        #pointer = self.t_collection.find({})
        #for p in pointer: 
         #   pprint(p)",0.3971559405,
1944,social media,project.social_accounts,0.3962168694,
1944,social media,"class Tweets:
    
    
    def __init__(self,term="""",corpus_size=100):
        self.tweets={}
        if term !="""":
            self.searchTwitter(term,corpus_size)
                
    def searchTwitter(self,term,corpus_size):
        searchTime=datetime.now()
        while (self.countTweets() < corpus_size):
            new_tweets = api.search(term,lang=""en"",tweet_mode='extended',count=corpus_size)
            for nt_json in new_tweets:
                nt = nt_json._json
                if self.getTweet(nt['id_str']) is None and self.countTweets() < corpus_size:
                    self.addTweet(nt,searchTime,term)
            time.sleep(30)
                
    def addTweet(self,tweet,searchTime,term="""",count=0):
        id = tweet['id_str']
        if id not in self.tweets.keys():
            self.tweets[id]={}
            self.tweets[id]['tweet']=tweet
            self.tweets[id]['count']=0
            self.tweets[id]['searchTime']=searchTime
            self.tweets[id]['searchTerm']=term
        self.tweets[id]['count'] = self.tweets[id]['count'] +1
        
    def combineTweets(self,other):
        for otherid in other.getIds():
            tweet = other.getTweet(otherid)
            searchTerm = other.getSearchTerm(otherid)
            searchTime = other.getSearchTime(otherid)
            self.addTweet(tweet,searchTime,searchTerm)
        
    def getTweet(self,id):
        if id in self.tweets:
            return self.tweets[id]['tweet']
        else:
            return None
    
    def getTweetCount(self,id):
        return self.tweets[id]['count']
    
    def countTweets(self):
        return len(self.tweets)
    
    # return a sorted list of tupes of the form (id,count), with the occurrence counts sorted in decreasing order
    def mostFrequent(self):
        ps = []
        for t,entry in self.tweets.items():
            count = entry['count']
            ps.append((t,count))  
        ps.sort(key=lambda x: x[1],reverse=True)
        return ps
    
    # reeturns tweet IDs as a set
    def getIds(self):
        return set(self.tweets.keys())
    
    # save the tweets to a file
    def saveTweets(self,filename):
        json_data =jsonpickle.encode(self.tweets)
        with open(filename,'w') as f:
            json.dump(json_data,f)
    
    # read the tweets from a file 
    def readTweets(self,filename):
        with open(filename,'r') as f:
            json_data = json.load(f)
            incontents = jsonpickle.decode(json_data)   
            self.tweets=incontents
        
    def getSearchTerm(self,id):
        return self.tweets[id]['searchTerm']
    
    def getSearchTime(self,id):
        return self.tweets[id]['searchTime']
    
    def getText(self,id):
        tweet = self.getTweet(id)
        text=tweet['full_text']
        if 'retweeted_status'in tweet:
            original = tweet['retweeted_status']
            text=original['full_text']
        return text
                
    def addCode(self,id,code):
        tweet=self.getTweet(id)
        if 'codes' not in tweet:
            tweet['codes']=set()
        tweet['codes'].add(code)
        
   
    def addCodes(self,id,codes):
        for code in codes:
            self.addCode(id,code)
        
 
    def getCodes(self,id):
        tweet=self.getTweet(id)
        if 'codes' in tweet:
            return tweet['codes']
        else:
            return None
    
    # NEW -ROUTINE TO GET PROFILE
    def getCodeProfile(self):
        summary={}
        for id in self.tweets.keys():
            tweet=self.getTweet(id)
            if 'codes' in tweet:
                for code in tweet['codes']:
                    if code not in summary:
                            summary[code] =0
                    summary[code]=summary[code]+1
        sortedsummary = sorted(summary.items(),key=operator.itemgetter(0),reverse=True)
        return sortedsummary",0.3937343061,
1944,social media,"class Tweets:
    
    
    def __init__(self,term="""",corpus_size=100):
        self.tweets={}
        if term !="""":
            self.searchTwitter(term,corpus_size)
                
    def searchTwitter(self,term,corpus_size):
        searchTime=datetime.now()
        while (self.countTweets() < corpus_size):
            new_tweets = api.search(term,lang=""en"",tweet_mode='extended',count=corpus_size)
            for nt_json in new_tweets:
                nt = nt_json._json
                if self.getTweet(nt['id_str']) is None and self.countTweets() < corpus_size:
                    self.addTweet(nt,searchTime,term)
            time.sleep(30)
                
    def addTweet(self,tweet,searchTime,term="""",count=0):
        id = tweet['id_str']
        if id not in self.tweets.keys():
            self.tweets[id]={}
            self.tweets[id]['tweet']=tweet
            self.tweets[id]['count']=0
            self.tweets[id]['searchTime']=searchTime
            self.tweets[id]['searchTerm']=term
        self.tweets[id]['count'] = self.tweets[id]['count'] +1
        
    def combineTweets(self,other):
        for otherid in other.getIds():
            tweet = other.getTweet(otherid)
            searchTerm = other.getSearchTerm(otherid)
            searchTime = other.getSearchTime(otherid)
            self.addTweet(tweet,searchTime,searchTerm)
        
    def getTweet(self,id):
        if id in self.tweets:
            return self.tweets[id]['tweet']
        else:
            return None
    
    def getTweetCount(self,id):
        return self.tweets[id]['count']
    
    def countTweets(self):
        return len(self.tweets)
    
    # return a sorted list of tupes of the form (id,count), with the occurrence counts sorted in decreasing order
    def mostFrequent(self):
        ps = []
        for t,entry in self.tweets.items():
            count = entry['count']
            ps.append((t,count))  
        ps.sort(key=lambda x: x[1],reverse=True)
        return ps
    
    # reeturns tweet IDs as a set
    def getIds(self):
        return set(self.tweets.keys())
    
    # save the tweets to a file
    def saveTweets(self,filename):
        json_data =jsonpickle.encode(self.tweets)
        with open(filename,'w') as f:
            json.dump(json_data,f)
    
    # read the tweets from a file 
    def readTweets(self,filename):
        with open(filename,'r') as f:
            json_data = json.load(f)
            incontents = jsonpickle.decode(json_data)   
            self.tweets=incontents
        
    def getSearchTerm(self,id):
        return self.tweets[id]['searchTerm']
    
    def getSearchTime(self,id):
        return self.tweets[id]['searchTime']
    
    def getText(self,id):
        tweet = self.getTweet(id)
        text=tweet['full_text']
        if 'retweeted_status'in tweet:
            original = tweet['retweeted_status']
            text=original['full_text']
        return text
                
    def addCode(self,id,code):
        tweet=self.getTweet(id)
        if 'codes' not in tweet:
            tweet['codes']=set()
        tweet['codes'].add(code)
        
   
    def addCodes(self,id,codes):
        for code in codes:
            self.addCode(id,code)
        
 
    def getCodes(self,id):
        tweet=self.getTweet(id)
        if 'codes' in tweet:
            return tweet['codes']
        else:
            return None
        
    # NEW -ROUTINE TO GET PROFILE
    def getCodeProfile(self):
        summary={}
        for id in self.tweets.keys():
            tweet=self.getTweet(id)
            if 'codes' in tweet:
                for code in tweet['codes']:
                    if code not in summary:
                            summary[code] =0
                    summary[code]=summary[code]+1
        sortedsummary = sorted(summary.items(),key=operator.itemgetter(0),reverse=True)
        return sortedsummary",0.3937343061,
1944,social media,"class Tweets:
    
    
    def __init__(self,term="""",corpus_size=100):
        self.tweets={}
        if term !="""":
            self.searchTwitter(term,corpus_size)
                
    def searchTwitter(self,term,corpus_size):
        searchTime=datetime.now()
        while (self.countTweets() < corpus_size):
            new_tweets = api.search(term,lang=""en"",tweet_mode='extended',count=corpus_size)
            for nt_json in new_tweets:
                nt = nt_json._json
                if self.getTweet(nt['id_str']) is None and self.countTweets() < corpus_size:
                    self.addTweet(nt,searchTime,term)
            time.sleep(120)
                
    def addTweet(self,tweet,searchTime,term="""",count=0):
        id = tweet['id_str']
        if id not in self.tweets.keys():
            self.tweets[id]={}
            self.tweets[id]['tweet']=tweet
            self.tweets[id]['count']=0
            self.tweets[id]['searchTime']=searchTime
            self.tweets[id]['searchTerm']=term
        self.tweets[id]['count'] = self.tweets[id]['count'] +1

    def combineTweets(self,other):
        for otherid in other.getIds():
            tweet = other.getTweet(otherid)
            searchTerm = other.getSearchTerm(otherid)
            searchTime = other.getSearchTime(otherid)
            self.addTweet(tweet,searchTime,searchTerm)
        
    def getTweet(self,id):
        if id in self.tweets:
            return self.tweets[id]['tweet']
        else:
            return None
    
    def getTweetCount(self,id):
        return self.tweets[id]['count']
    
    def countTweets(self):
        return len(self.tweets)
    
    # return a sorted list of tupes of the form (id,count), with the occurrence counts sorted in decreasing order
    def mostFrequent(self):
        ps = []
        for t,entry in self.tweets.items():
            count = entry['count']
            ps.append((t,count))  
        ps.sort(key=lambda x: x[1],reverse=True)
        return ps
    
    # reeturns tweet IDs as a set
    def getIds(self):
        return set(self.tweets.keys())
    
    # save the tweets to a file
    def saveTweets(self,filename):
        json_data =jsonpickle.encode(self.tweets)
        with open(filename,'w') as f:
            json.dump(json_data,f)
    
    # read the tweets from a file 
    def readTweets(self,filename):
        with open(filename,'r') as f:
            json_data = json.load(f)
            incontents = jsonpickle.decode(json_data)   
            self.tweets=incontents
        
    def getSearchTerm(self,id):
        return self.tweets[id]['searchTerm']
    
    def getSearchTime(self,id):
        return self.tweets[id]['searchTime']
    
    def getText(self,id):
        tweet = self.getTweet(id)
        text=tweet['full_text']
        if 'retweeted_status'in tweet:
            original = tweet['retweeted_status']
            text=original['full_text']
        return text
                
    ### NEW ROUTINE - add a code to a tweet
    def addCode(self,id,code):
        tweet=self.getTweet(id)
        if 'codes' not in tweet:
            tweet['codes']=set()
        tweet['codes'].add(code)
        
    ### NEW ROUTINE  - add multiple  codes for a tweet
    def addCodes(self,id,codes):
        for code in codes:
            self.addCode(id,code)
        
    ### NEW ROUTINE get codes for a tweet
    def getCodes(self,id):
        tweet=self.getTweet(id)
        if 'codes' in tweet:
            return tweet['codes']
        else:
            return None",0.3930366635,
628,filtering,"%%time
bf = ButterworthFilter(ts, [58., 62.], order=4, filt_type=""stop"")
ts2 = bf.filter()",0.387345612,
628,filtering,"grouped.filter(lambda x: np.all([x[col] > 1640 for col in ['High', 'Close', 'Low']]))",0.3829886317,
628,filtering,"#t2 = t1[((t1['mass'] > 250) & (t1['size'] < 3.0) & (t1['ecc'] < 0.1))]
condition = lambda x: ((x['mass'].mean() > 250) & (x['size'].mean() < 3.0) &
                       (x['ecc'].mean() < 0.1))
t2 = tp.filter(t1, condition)  # a wrapper for pandas' filter that works around a bug in v 0.12",0.3798249364,
628,filtering,"# Filter 'Units' where the sum is > 35: by_com_filt
by_com_filt = by_company.filter(lambda g:g['Units'].sum() > 35)

by_com_filt",0.3789048195,
628,filtering,"%matplotlib inline
# Take the absolute value, then low-pass filter
envelope = audio.copy()
envelope._data = np.abs(envelope._data)
envelope._data = mne.filter.filter_data(envelope._data,
                                        envelope.info['sfreq'],
                                        None,
                                        10, filter_length='1s')
envelope.resample(sfreq_new)",0.3784558773,
628,filtering,"plt.figure(figsize=(9,6))

for k, j in enumerate([0,160,1600,16000]):
    x = sm.tsa.filters.hpfilter(p_data, j)[1]
    y = sm.tsa.filters.hpfilter(u_data, j)[1]

    reg_line = sm.OLS(y, sm.add_constant(x),missing='drop').fit()
    a, b = reg_line.params
    
    plt.subplot(2, 2, k+1)
    plt.plot(x*100, (a + b*x)*100,'r',label='_nolegend_')
    plt.scatter(x*100,y*100,s=10,c='b',label='HP %i' %j)
    plt.legend(loc='best',fontsize=8)
    plt.xlabel('Inflation, percent',fontsize=8)
    plt.ylabel('Unemployment, percent',fontsize=8)
plt.tight_layout()  # creates space between subplots
plt.show()",0.3776579201,
628,filtering,"# keep attributes like name, age, etc.
attr = ['ID','NAME','RESPONSE','PREFEC','DISTRICT','PRBLOCK','INCUMB','TERM','PARTY','SEX','AGE','RESULT']

data2009_attr = data2009.filter(attr)
data2012_attr = data2012.filter(attr)
data2014_attr = data2014.filter(attr)",0.3773926198,
628,filtering,"print(""Available SEDs:\n"", simcado.source.get_SED_names())
print(""Available Filters:\n"", simcado.optics.get_filter_set())",0.3766439557,
628,filtering,"####### The delay ratio of the top 20 busiest airports ########
K = 20

# extract top_20_airports from stat_airport_traffic
top_20_airports = [item[0] for item in stat_airport_traffic.take(K)]

# select the statistic of source airports
statistic_ratio_delay_airport = (
    df_with_delay
        # select only flights that depart from one of top 20 ariports
        .filter(df_with_delay.src_airport.isin(top_20_airports))
        # group by source airport
        .groupBy('src_airport')
        #  calculate the delay ratio
        .agg((func.sum('is_delay')/func.count('*')).alias('delay_ratio'))
        # sort by name of airport
        .orderBy(['src_airport'])
    )
statistic_ratio_delay_airport.show(20)",0.3759209216,
628,filtering,"for layer in [0, 1, 2]:
    mean, cov = dn.compute_statistics(testing_sounds, layer)
    filters = dn.get_filters(layer)
    plt.figure(figsize=(14, 8))
    plt.suptitle(""Layer %d filters"" % layer, fontsize=18)
    for ii, ind in enumerate(np.argsort(mean)[-20:]):
        ax = plt.subplot(4, 5, ii + 1)
        plt.imshow(filters[ind], origin=""lower"", aspect=""auto"")
        ax.set_xticks([])
        ax.set_yticks([])",0.3751496971,
1204,model evaluation,"## TODO: Create train and evaluate function using tf.estimator
def train_and_evaluate(output_dir, num_train_steps):
  #ADD CODE HERE
  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)",0.548805356,
1204,model evaluation,"#Function

def evaluateNetwork(v_xs):
    global prediction
    y_pre = sess.run( tf.cast(prediction, tf.float32), feed_dict={xs: v_xs}) # Forward pass using v_xs as input to network
    result = np.matrix(y_pre, dtype=float)
    return result

# Calculate mean absolute error (MAE) and mean square error (MSE) for evaluation
def compute_accuracy(v_xs, v_ys):
    global prediction
    y_label = tf.reshape(v_ys, [-1, 43, 43, 1])
    y_label = tf.cast(y_label, tf.float32)
    y_pre = sess.run(prediction, feed_dict={xs: v_xs})
    y_pre = tf.cast(y_pre, tf.float32)#Force float32
    #correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1))
    diff = tf.subtract(tf.reduce_sum(y_label,[1,2]),tf.reduce_sum(y_pre,[1,2]))
    accuracyMAE = tf.cast(tf.reduce_mean(tf.abs(diff)),tf.float32)
    accuracyMSE = tf.cast(tf.sqrt(tf.reduce_mean(tf.square(diff))),tf.float32)
    MAE = sess.run(accuracyMAE, feed_dict={xs: v_xs, ys: v_ys})
    MSE = sess.run(accuracyMSE, feed_dict={xs: v_xs, ys: v_ys})
    
    return MAE,MSE

def weight_variable(shape, nameIn):
    initial = tf.truncated_normal(shape=shape, stddev=0.1)
    return tf.Variable(initial, name=nameIn)

def bias_variable(shape, nameIn):
    initial = tf.constant(0.1, shape=shape)
    return tf.Variable(initial, name=nameIn)

def conv2d(x, W):
    # stride [1, x_movement, y_movement, 1]
    # Must have strides[0] = strides[3] = 1
    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')

def max_pool_2x2(x):
    # stride [1, x_movement, y_movement, 1]
    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')

def max_pool_3x3(x):
    # stride [1, x_movement, y_movement, 1]
    return tf.nn.max_pool(x, ksize=[1,3,3,1], strides=[1,3,3,1], padding='SAME')",0.5334036946,
1204,model evaluation,"#
# Subtour Elimination Constraints: separation
# 

def SECseparation (x, G, node):
    
    #
    # Definition of the separation problem
    #
    
    SECsep = gb.Model()
    
    #
    # Variables w_ij and z_i
    #
    
    w = SECsep.addVars(G.edges(), \
                       obj=[x[u,v].x for u,v in G.edges()], \
                       ub = 1.0,\
                vtype=gb.GRB.CONTINUOUS, name='w')
        
    z= SECsep.addVars(G.nodes(), \
                      obj = -1.0, \
                      ub = 1.0, \
                  vtype=gb.GRB.CONTINUOUS, name='z')

    #
    # Objective function
    #

    SECsep.ModelSense = -1
    
    #
    # Constraints
    #

    SECsep.addConstrs((w[i,j] - z[i] <= 0 \
                       for i,j in G.edges()), name = 'WZ1')
    
    SECsep.addConstrs((w[i,j] - z[j] <= 0 \
                       for i,j in G.edges()), name = 'WZ2')
    
    SECsep.addConstr(z[node] == 1, name=""Fixing"")
    
    SECsep.update()
    
    SECsep.write('sep.lp')

    SECsep.optimize()

    if (SECsep.objVal) > -1.0:
        SEC = [i for i in G.nodes() if z[i].x > 0.1]
        return True, SEC
    else:
        return False, None",0.525149703,
1204,model evaluation,"def run_model(model,name):
    global model_set
    m = model
    m.fit(X_train, y_train)
    start = time.time()

    total_score = m.score(X_test,y_test)
    pscore = [pattern % i for i in list(metrics.precision_score(y_test, m.predict(X_test),labels=lang,average=None))]
    rscore = [pattern % i for i in list(metrics.recall_score(y_test, m.predict(X_test),labels=lang,average=None))]
    fscore = [pattern % i for i in list(metrics.f1_score(y_test, m.predict(X_test),labels=lang,average=None))]
    end = time.time()
    t= pattern % (end - start)
    cvs = cross_val_score(m, X_test,y_test)

    r = dict(zip(cols,[name,t,total_score,pscore,rscore,fscore]))
    print('1. Check for Overfitting: {}\n'.format(m.score(X_train,y_train)))
    print('2. Test Score is: {}\n'.format(total_score))
    print('3. Classification Report:')
    print(classification_report(y_test, m.predict(X_test)))
    print('')
    print('4. Cross Val Score: {} ==> Avg: {} '.format(cvs,cvs.sum()/len(cvs)))
    print('')
    
    model_set = model_set.append(r,ignore_index=True)
    return r,m",0.5249260664,
1204,model evaluation,"def compute_accuracy(v_xs, v_ys):
    global prediction
    y_pre = sess.run(prediction, feed_dict={xs: v_xs, keep_prob: 1})
    correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    result = sess.run(accuracy, feed_dict={xs: v_xs, ys: v_ys, keep_prob: 1})
    return result

def weight_variable(shape):
    initial = tf.truncated_normal(shape, stddev=0.1)
    return tf.Variable(initial)

def bias_variable(shape):
    initial = tf.constant(0.1, shape=shape)
    return tf.Variable(initial)

def conv2d(x, W):
    # stride [1, x_movement, y_movement, 1]
    # Must have strides[0] = strides[3] = 1
    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')

def max_pool_2x2(x):
    # stride [1, x_movement, y_movement, 1]
    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')",0.5247668624,
1204,model evaluation,"def compute_accuracy(v_xs,v_ys):
    global prediction
    y_pre = sess.run(prediction,feed_dict={# if it is not neccessary ???
            xs:v_xs,
            ys:v_ys,
            keep_prob:0.5
        })
    correct_prediction = tf.equal(tf.argmax(y_pre,1),tf.argmax(v_ys,1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))
    result = sess.run(accuracy,feed_dict ={
            xs:v_xs,
            ys:v_ys,
            keep_prob:0.5
        })
    return result",0.5246049166,
1204,model evaluation,"def accuracy(X, Y):
    accuracy = model.evaluate(X.reshape(shape(X)), Y)[1]
    print (""Accuracy:"", accuracy)

for i in range(len(inputs)):
    accuracy(inputs[i], targets[i])",0.5214116573,
1204,model evaluation,"import predictor.model

def build_model(moving_averages, transform_params, n_estimators, min_samples_split, min_samples_leaf, bet_threshold):
    X, y = predictor.model.build_model_inputs(historical_games_training_holdout, all_stats, moving_averages, transform_params)
    return predictor.model.build_model(X, y, n_estimators=n_estimators, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)",0.5208183527,
1204,model evaluation,"def train_conv(batch_images,batch_labels):
    _,acc_tc, loss_tc = sess.run([minimizer,accuracy,loss],feed_dict={inputs:batch_images,labels: batch_labels})
    return acc_tc,loss_tc",0.5207705498,
1204,model evaluation,"def predict_test_set(predictor, test_set):
    """"""Compute the prediction for every element of the test set.""""""
    predictions = [predictor.predict(test_set[i, :, :]) 
                   for i in xrange(len(test_set))]
    return predictions

# Choose a subset of the test set. Otherwise this will never finish.
test_set = test_images[0:100, :, :]
all_predictions = [predict_test_set(predictor, test_set) for predictor in predictors]",0.5198248029,
359,create an rdd of protein chains,"# Some prep work
from indra.sources import indra_db_rest as idbr

def display_evidence(stmt, limit=5):
    print(""\n\033[1m%s\033[0m\n"" % str(stmt))
    print('\n\n'.join(['pmid %s, from %s: %s' % (ev.pmid, ev.source_api.upper(), ev.text)
                       for ev in stmt.evidence][:limit]))
    if len(stmt.evidence) > limit:
        print('...')",0.4319869876,
359,create an rdd of protein chains,"def find_area(y,x=None,dx=1,usesimps=USESCIPY):
    """"""Use the trapezium or simps rule to find the area of the function y(x)
    
    Parameters
    ----------
    y : array
        Array of y(x) values
    x : array
        Array of x values coresponding to values the of y. If x==None 
        then dx is used instead
    dx : scalar
        Spacing between each y value. Only used if x==None.
    
    Returns
    -------
    I : scalar
        The area under the curve y(x)
    """"""
    if usesimps:
        return integ.simps(y,x,dx)
    
    if x==None:
        return dx(np.sum(y) - (y[0]+y[-1])/2.0) # (equation shown above)
    
    I = 0
    for i in range(np.size(y)-1):
        I += (x[i+1]-x[i]) * (y[i+1]+y[i]) / 2.0 # (equation shown above)
    return I",0.4281207919,
359,create an rdd of protein chains,"def main():
    pareto = tools.ParetoFront()

    pop = toolbox.population(n=MU)
    graph = []
    data = []

    # Evaluate the individuals with an invalid fitness
    invalid_ind = [ind for ind in pop if not ind.fitness.valid]
    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)
    data.append(fitnesses)
    for ind, fit in zip(invalid_ind, fitnesses):
        ind.fitness.values = fit
        graph.append(ind.fitness.values)

    # This is just to assign the crowding distance to the individuals
    # no actual selection is done
    pop = toolbox.select(pop, len(pop))

    # Begin the generational process
    for gen in range(1, NGEN):
        # Vary the population
        offspring = tools.selTournamentDCD(pop, len(pop))
        offspring = [toolbox.clone(ind) for ind in offspring]

        for ind1, ind2 in zip(offspring[::2], offspring[1::2]):
            if random.random() <= CXPB:
                toolbox.mate(ind1, ind2)
                
            toolbox.mutate(ind1)
            toolbox.mutate(ind2)
            del ind1.fitness.values, ind2.fitness.values

        # Evaluate the individuals with an invalid fitness
        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]
        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)
        data.append(fitnesses)
        
        for ind, fit in zip(invalid_ind, fitnesses):
            ind.fitness.values = fit
            graph.append(ind.fitness.values)

        # Select the next generation population
        pop = toolbox.select(pop + offspring, MU)

    pareto.update(pop)

    return pop, pareto, graph, data",0.4250941873,
359,create an rdd of protein chains,"def rnn_model(hidden_size=128, n_digits=3, abc_size=len(ABC)):
    model = Sequential()
    # encoder
    model.add(LSTM(hidden_size, input_shape=(None, abc_size)))
    # input for the decoder
    # this sets the length of the output sequence and has to
    # match the length we used when constructing the examples
    model.add(RepeatVector(2 * n_digits))

    # decoder model
    model.add((LSTM(hidden_size, return_sequences=True)))

    # For each step of the output sequence, decide which character should be chosen
    model.add(TimeDistributed(Dense(abc_size)))
    model.add(Activation('softmax'))

    model.compile(loss='categorical_crossentropy',
                  optimizer='adam',
                  metrics=['accuracy'])

    return model",0.4223147035,
359,create an rdd of protein chains,"def createGraphWithProv():
    
    graph=createWorkflowGraph()
    #Location of the remote repository for runtime updates of the lineage traces. Shared among ProvenanceRecorder subtypes

    # Ranomdly generated unique identifier for the current run
    rid='JUP_SIMPLE_'+getUniqueId()

    
    # Finally, provenance enhanced graph is prepared:
    print prov_profile

     
    #Initialise provenance storage to service:
    profile_prov_run(graph, 
                     provImpClass=(ProvenancePE,),
                     username=prov_profile['username'],
                     runId=rid,
                     description=prov_profile['description'],
                     workflowName=prov_profile['workflowName'],
                     workflowId=prov_profile['workflowId'],
                     save_mode=prov_profile['save_mode'],
                     componentsType=prov_profile['componentsType'],
                     sel_rules=prov_profile['sel_rules']
                    )
                   

    #clustersRecorders={'record0':ProvenanceRecorderToFileBulk,'record1':ProvenanceRecorderToFileBulk,'record2':ProvenanceRecorderToFileBulk,'record6':ProvenanceRecorderToFileBulk,'record3':ProvenanceRecorderToFileBulk,'record4':ProvenanceRecorderToFileBulk,'record5':ProvenanceRecorderToFileBulk}
    #Initialise provenance storage end associate a Provenance type with specific components:
    #profile_prov_run(graph,provImpClass=ProvenancePE,componentsType={'Source':(ProvenanceStock,)},username='aspinuso',runId=rid,w3c_prov=False,description=""provState"",workflowName=""test_rdwd"",workflowId=""xx"",save_mode='service')

    #
    return graph


graph=createGraphWithProv()

display(graph)",0.4221669734,
359,create an rdd of protein chains,"def test_node_insert():
    SLL = Node(10)
    SLL.insert(""a new node"")
    SLL.insert(""another node"")
    assert SLL.value == 10
    assert SLL.child.value == ""a new node""
    assert SLL.child.child.value == ""another node""
    print(""test_node_insert() passed!"")
    
test_node_insert()",0.4215856791,
359,create an rdd of protein chains,"class MLStripper(HTMLParser):
    def __init__(self):
        super().__init__()
        self.reset()
        self.fed = []
    def handle_data(self, d):
        self.fed.append(d)
    def get_data(self):
        return ''.join(self.fed)

def strip_tags(html):
    s = MLStripper()
    s.feed(html)
    return s.get_data()

def get_venue(x):
    if 'venue' not in x:
        return '-'

    v = x['venue']
    city = v['city']
    if v['country'] == 'us':
        return '{}, {}'.format(city, v['state'])
    else:
        return '{} ({})'.format(city, v['country'].upper())

def map_mus(x):
    x['description'] = strip_tags(x['description']) if 'description' in x else ''
    x['group']['url'] = ""http://meetup.com/{}"".format(x['group']['urlname'])
    x['venue_loc'] = get_venue(x)
    return x

def update_meetups_values():
    global upcoming_meetups
    global rsvp_count_history
    global total_rsvp_count

    now = datetime.now()
    cutoff = now - MU_CHANGE_CUTOFF
    
    for mu in upcoming_meetups:
        id = mu['id']
        prev_conv_count = 0 # running total for conversions
        if id not in rsvp_count_history:
            rsvp_count_history[id] = []
        else:
            # carry previous conversions count forward
            prev_conv_count = rsvp_count_history[id][-1][2]

        # store the RSVP count for each event at this time
        rsvp_count_history[id].append([now, mu['yes_rsvp_count'], prev_conv_count])
        # cleanup old rsvp history values
        rsvp_count_history[id] = list(filter(lambda x: x[0] > cutoff, rsvp_count_history[id]))

        # save change in rsvp count over time span
        mu['change_over_time'] = rsvp_count_history[id][-1][1] - rsvp_count_history[id][0][1]

    # update the total number of RSVPs for all of upcoming meetups
    total_rsvp_count.append((now,
            reduce(lambda x, y: x + y['yes_rsvp_count'], upcoming_meetups, 0),
            reduce(lambda x, y: x + y[-1][2], rsvp_count_history.values(), 0)
    ))
    # cleanup old values
    list(filter(lambda x: x[0] > cutoff, total_rsvp_count))

def refresh_meetups_list():
    '''
    Update the data for the upcoming meetup events. Once invoked, will run every
    PROD_TIMER seconds.
    
    Publishes data on the 'meetups' and 'plots' channels.
    '''
    global upcoming_meetups

    # request updated meetups data
    r = requests.get(mu_events_url, params=params)
    upcoming_meetups = list(map(map_mus, r.json()['results']))
    # update values, comparing old vs new data
    update_meetups_values()
    # reschedule
    loop.call_later(PROD_TIMER, refresh_meetups_list)

    # broadcast the updated list
    channel('meetups').set('list', upcoming_meetups)
    fig = plot_total_rsvps(total_rsvp_count)
    channel('plots').set('total_rsvp_plot', fig)",0.4207323194,
359,create an rdd of protein chains,"def optimize_model():
    if len(memory) < BATCH_SIZE:
        return
    transitions = memory.sample(BATCH_SIZE)
    # Transpose the batch (see http://stackoverflow.com/a/19343/3343043 for
    # detailed explanation).
    batch = Transition(*zip(*transitions))

    # Compute a mask of non-final states and concatenate the batch elements
    non_final_mask = torch.Tensor(tuple(map(lambda s: s is not None, batch.next_state))).type('torch.ByteTensor').cuda()
    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])
    state_batch = torch.cat(batch.state)
    action_batch = torch.cat(batch.action)
    reward_batch = torch.cat(batch.reward)

    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the
    # columns of actions taken
    state_action_values = policy_net(state_batch).gather(1, action_batch)
    # Compute V(s_{t+1}) for all next states.
    next_state_values = torch.zeros(BATCH_SIZE).cuda()
    #print(sum(non_final_mask), next_state_values[non_final_mask].shape, target_net(non_final_next_states).max(1)[0].detach().data.shape)
    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach().data
    # Compute the expected Q values
    expected_state_action_values = (next_state_values * GAMMA) + reward_batch

    # Compute Huber loss
    loss = F.smooth_l1_loss(state_action_values, Variable(expected_state_action_values.unsqueeze(1), requires_grad=False))

    # Optimize the model
    optimizer.zero_grad()
    loss.backward()
    for param in policy_net.parameters():
        if not param.grad is None:
            param.grad.data.clamp_(-1, 1)
    optimizer.step()",0.4203233719,
359,create an rdd of protein chains,"# define our function called simple-graph

def simple_graph():
    #create our empty graph
    G = nx.Graph()
    
    #add edges 
    G.add_edge('A','B')
    G.add_edge('B','C')
    G.add_edge('C','A')
    
    # draw the graph
    nx.draw(G,with_labels=True,node_color='y',node_size=800)",0.4197645187,
359,create an rdd of protein chains,"def delete_note_in_graph():
    G = nx.Graph()
    
    # add 3 egdes
    G.add_edge('A','B')
    G.add_edge('B','C')
    G.add_edge('C','A')
    
    # draw the graph
    nx.draw(G,with_labels=True,node_color='y',node_size=800)
    
    #remove note B from the graph
    G.remove_node('B')
    
    nx.draw(G,with_labels=True,node_color='y',node_size=800)
    
delete_note_in_graph()",0.4197645187,
2067,step downsample the record to a weekly frequency for each location,"def most_popular_route_by_hour(df, title_label):
    df['Count']= 1
    df.index.names = ['Date']
    df1 = df.groupby(['Route']).resample('1H', how='count')
    df1 = df1.reset_index()
    
    df1_pivoted = df1.pivot_table('Count', ['Route'], 'Date')
    df1_pivoted.columns = np.arange(24)
        
    df2 = df1_pivoted.max(axis=0, skipna=True)
    df2 = pd.concat([df1_pivoted.idxmax(axis=0, skipna=True), df1_pivoted.max(axis=0, skipna=True)], axis =1)
    df2=df2.reset_index()
    df2.columns = ['Time','Route','Count']
    df2=df2.drop(df2[df2.Count < 10].index)
    
    ##Plotting##
    c=np.random.rand(3,1)

    marker = itertools.cycle(('s', 'd', '<', 'o', '*','p','^','h')) 
    plt.figure(figsize = (15,10))
    for i, group in df2.groupby('Route'):
        x = group.Time
        y= group.Count
        plt.plot(group.Time, group.Count, marker = marker.next(),markersize= 20, linestyle = '', 
                 color  = np.random.rand(3,1), label = str(i),clip_on=False)
    plt.legend(loc='left', bbox_to_anchor=(1.05, 1.01),
              fancybox=True, shadow=True, ncol=1, fontsize = 20)

    plt.xlabel('Hour', fontsize = 30, fontweight = 'bold')
    plt.ylabel('# of Oyster trips', fontsize = 30, fontweight = 'bold')
    plt.tick_params(labelsize = 20)
    plt.xlim([0,24.5])
    plt.grid(True)
    plt.title(('Most popular routes ' + '('+title_label+')'), fontsize = 30, fontweight = 'bold',y=1.04)
    
    return df2",0.555983007,
2067,step downsample the record to a weekly frequency for each location,"def tddf(df): 
    tddf = df.groupby(pd.Grouper(freq='d')).size()
    tddf = tddf.reindex(pd.date_range('01-01-2017', '11-30-2018'),fill_value=0)
    return tddf

plot = mobi.plots.Plot()
sdf = pd.DataFrame(index=tddf(df).index.values,
                   data={'Annual':tddf(df[idx365all]).values,
                         '24h':tddf(df[idx24]).values,
                         'Monthly':tddf(df[idxmonthlyall]).values,
                         '90 day':tddf(df[idx90]),
                         'Other':tddf(df[~idx24 & ~idx365all & ~idx90 & ~idxmonthlyall])})
plot.ax.stackplot(sdf.index.values,sdf.T,labels=sdf.columns,colors=plot.colors)
plot.ax.legend(loc=2,title=""Pass Type"")
plot.ax.xaxis.set_major_formatter(mdates.DateFormatter(""%b %Y""))

plot.f.tight_layout()",0.5337304473,
2067,step downsample the record to a weekly frequency for each location,"def lot_frontage_by_ms(df, medians):
    for index, lf, ms_sub in df[['lot_frontage', 'ms_subclass']].to_records():
        if np.isnan(lf):
    #         print(ms_sub)
            df.at[index, 'lot_frontage'] = medians.at[ms_sub, 'lot_frontage']",0.5331758261,
2067,step downsample the record to a weekly frequency for each location,"def delete_columns(n_data,data):
    if 'customer_id' in n_data.columns:
        n_data.drop(['customer_id'], axis = 1, inplace = True)
    if 'model_name' in n_data.columns:
        n_data.drop(['model_name'], axis = 1, inplace = True)
    if 'cluster' in n_data.columns:
        n_data.drop(['cluster'], axis = 1, inplace = True)
    if 'customer_id' in data.columns:
        data.drop(['customer_id'], axis = 1, inplace = True)",0.5263478756,
2067,step downsample the record to a weekly frequency for each location,"# Function for training data preparation
def train_data_prep (x):
    # Sex attr. preparation
    x['Sex_flag'] = x['Sex'].map({""female"":0,""male"":1}).astype(int)
    
    # Embarked attr. preparation
    x['Embarked']=x.sort_values(['Fare','Pclass'])['Embarked'].ffill()
    x = pd.concat([x, pd.get_dummies(x['Embarked'])], axis=1)
    
    # Fare attr. preparation
    minmax_scale = preprocessing.MinMaxScaler().fit(x['Fare'].values)
    df_minmax = minmax_scale.transform(x['Fare'])
    x['fare_norm'] =pd.DataFrame(df_minmax)
   
    # Age attr. preparation
    x['Age']=x.groupby(['Sex','Pclass'])[['Age']].transform(lambda y: y.fillna(y.median()))
    bins=[0,8,16,25,40,60,100]
    x['Age_bins']=pd.cut(x.Age,bins,labels=[""Children"",""Teenagers"",""Youth"",""Adults"",""Middle_age"",""Older_People""])
    x = pd.concat([x, pd.get_dummies(x['Age_bins'])], axis=1);
    
    # Parch & SibSp attr. preparation
    x['Parch_SibSp'] =x.apply(lambda row: row['Parch']+row['SibSp'],axis=1)
    x['Parch_SibSp_bins'] =x.apply(lambda row: Parch_SibSp_bins(row),axis=1) 
    
    #Name attr. preparation
    x['Names_title_flag'] =x.apply(lambda row: Names_title(row),axis=1) 
    
    # Dropping unrequired attribiutes
    x.drop(['Fare','Age','PassengerId', 'Name', 'Ticket', 'Cabin','Age_bins','Embarked','Sex', 'SibSp','Parch','Parch_SibSp'], axis=1, inplace=True)
    
    return x",0.5256541967,
2067,step downsample the record to a weekly frequency for each location,"def uni_ratio_int_stu(df):
    #Compute ratio
    df['Ratio Int. students'] = df['# Int. students']/df['# Students']

    #Sort the rows according to ratio and locate the top 30 ones
    df_sorted_ratios = df.sort_values(by=['Ratio Int. students'], ascending=False).head(30)

    #Plot the bar chart and print the best one
    fig, ax = plt.subplots(figsize=(20,10))

    plot = sns.barplot(y=df_sorted_ratios['Ratio Int. students'], x=df_sorted_ratios.index, ax=ax)
    plot.set_xticklabels(plot.get_xticklabels(), rotation=90)

    print(""Best university according to ratio between local and international students is : "" + df_sorted_ratios.iloc[0].name)

    #Delete the ratio column of the dataframe
    del df_sorted_ratios",0.5241599083,
2067,step downsample the record to a weekly frequency for each location,"def uni_ratio_fac_stu(df):
    #Compute ratio
    df['Ratio Faculty members'] = df['# Faculty members']/df['# Students']

    #Sort the rows according to ratio and locate the top 30 ones
    df_sorted_ratios = df.sort_values(by=['Ratio Faculty members'], ascending=False).head(30)

    #Plot the bar chart and print the best one
    fig, ax = plt.subplots(figsize=(20,10))

    plot = sns.barplot(y=df_sorted_ratios['Ratio Faculty members'], x=df_sorted_ratios.index, ax=ax)
    plot.set_xticklabels(plot.get_xticklabels(), rotation=90)
    print(""Best university according to ratio between faculty members and students is : "" + df_sorted_ratios.iloc[0].name)

    #Delete the ratio column of the dataframe
    del df_sorted_ratios",0.5241599083,
2067,step downsample the record to a weekly frequency for each location,"def cumsum(data, min_period, window, column_name):
    data[column_name] = data[""taxi_call_num""].shift(1).rolling(min_periods=min_period, window=window).sum()",0.5240303278,
2067,step downsample the record to a weekly frequency for each location,"def count_flights_per_month(flights_per_day):
    flights_per_day.index = flights_per_day['FL_DATE'] # Set index to date so we can use TimeGrouper
    flights_per_month = pd.DataFrame(flights_per_day.groupby(pd.TimeGrouper(freq='M')).sum())
    
    # Plot number of arrivals at LGA per month
    ax = flights_per_month.plot(kind='bar', title='Number of LGA Arrivals per Month Since Jan 2016', legend=False, 
                                figsize=(16,8), fontsize=14)
    ax.set_xticklabels(pd.Series(flights_per_month.index).apply(dt.date).apply(lambda x: str(x)[:7])) # removes day and timestamp from datetime
    ax.set_ylabel(""Number of Arrivals"")
    ax.set_xlabel(""Month"")
    
    return flights_per_month",0.5224301219,
2067,step downsample the record to a weekly frequency for each location,"def compute_daily_stats(data):
    daily = data.resample(period=dt.timedelta(days=1),
                                upsample_method='none',
                                downsample_method={'count': agg.COUNT('latitude'),
                                                   'avg_mag': agg.MEAN('mag'),
                                                   'max_mag': agg.MAX('mag')})

    daily['count'] = daily['count'].fillna(0)
    return daily",0.5213615298,
1662,problem wrangling an astro machine learning model,from astrochairs import find_solution,0.4513232112,
1662,problem wrangling an astro machine learning model,"model.levers = [RealLever(""pollution_limit"", 0.0, 0.1, length=100)]",0.4481418729,
1662,problem wrangling an astro machine learning model,from pextant.solvers.astarMesh import astarSolver,0.4419867098,
1662,problem wrangling an astro machine learning model,from reachy import multi_robot_from_vrep,0.4390375018,
1662,problem wrangling an astro machine learning model,"# select the outermost ensemble in each sampling transition
special_ensembles = [transition.ensembles[-1] for transition in network.sampling_transitions]

alternate_shooting = strategies.OneWayShootingStrategy(
    selector=paths.UniformSelector(), # TODO: change this
    ensembles=special_ensembles
)
# note that replace=True is the default

scheme = paths.DefaultScheme(network)
scheme.movers = {} # TODO: this will be removed, and lines on either side combined, when all is integrated
scheme.append(alternate_shooting)
move_decision_tree = scheme.move_decision_tree()",0.4384503961,
1662,problem wrangling an astro machine learning model,hp.mollview(sim.driver.scheduler.survey_lists[0][1].basis_functions[2]()),0.4381291568,
1662,problem wrangling an astro machine learning model,"analysis.GraphAllPolls(polls, candidates)",0.4373734593,
1662,problem wrangling an astro machine learning model,from vae_parallel_sta663 import *,0.4356829226,
1662,problem wrangling an astro machine learning model,"objectives = []

for protocol in sweep_protocols:
    stim_start = protocol.stimuli[0].step_delay
    stim_end = stim_start + protocol.stimuli[0].step_duration
    for efel_feature_name, mean in efel_feature_means[protocol.name].iteritems():
        feature_name = '%s.%s' % (protocol.name, efel_feature_name)
        feature = ephys.efeatures.eFELFeature(
                    feature_name,
                    efel_feature_name=efel_feature_name,
                    recording_names={'': '%s.soma.v' % protocol.name},
                    stim_start=stim_start,
                    stim_end=stim_end,
                    exp_mean=mean,
                    exp_std=0.1 * mean)
	objective = ephys.objectives.SingletonObjective(feature_name, feature)
	objectives.append(objective)",0.435534656,
1662,problem wrangling an astro machine learning model,s.get_compounds_by_chemblId('CHEMBL2'),0.434396863,
2590,which country is receiving the least?,"# iterrate city_df through citipy to fill city and country
for index, row in city_df.iterrows():
    city_code = citipy.nearest_city(row[""Lat""], row[""Lng""])
    city_df.set_value(index, ""City"", city_code.city_name)
    city_df.set_value(index, ""Country"", city_code.country_code)
    
city_df.count()",0.4995600581,
2590,which country is receiving the least?,"def cap_dist(x):
    s_ee_ll= wiki_df.loc[x.Searchee_upper==wiki_df.Country,:].loc[:,['Latitude','Longitude']].iloc[0,:]
    s_er_ll= wiki_df.loc[x.Searcher_upper==wiki_df.Country,:].loc[:,['Latitude','Longitude']].iloc[0,:]
    return np.log10(vincenty(s_ee_ll, s_er_ll).miles)

search_pred_df.loc[:,'dist']=search_pred_df.apply(cap_dist,axis=1)
search_pred_df.head()",0.4961335659,
2590,which country is receiving the least?,"def comp_rel(x):
    s_ee_rel= wiki_df.loc[x.Searchee_upper==wiki_df.Country,:].loc[:,['Religion1','Religion2']].iloc[0,:]
    s_er_rel= wiki_df.loc[x.Searcher_upper==wiki_df.Country,:].loc[:,['Religion1','Religion2']].iloc[0,:]
    return len(set(s_ee_rel).intersection(set(s_er_rel)))>0

search_pred_df.loc[:,'com_rel']=search_pred_df.apply(comp_rel,axis=1)
print search_pred_df.query('com_rel==True').head()
print search_pred_df.query('com_rel==False').head()",0.4955118895,
2590,which country is receiving the least?,"def up_sample(df, under_rep_name, over_rep_name):
    
    # get the total count of underrepresented group
    sample_size = len(df[df.country_destination == over_rep_name])
#     print ""Sample size: "",sample_size
    
    under_rep_class_indices = df[df.country_destination == under_rep_name].index
    under_rep_class = df.loc[under_rep_class_indices]
    
    over_rep_class_indices = df[df.country_destination == over_rep_name].index
    over_rep_class = df.loc[over_rep_class_indices]
    
    # set random state
    rnd = np.random.RandomState(0)
    
    # use the underrepresented group count to randomly sample from overrepresented group
    random_indices = rnd.choice(under_rep_class_indices, sample_size, replace=True) 
    
    #increase the number of under_rep observations to that of the over_rep observations
    under_rep_class = df.loc[random_indices]
    
    return under_rep_class",0.4886026382,
2590,which country is receiving the least?,"def time_series(df, country, variable):
    
    # Only take data for country/variable combo 
    series = df[(df.country==country) & (df.variable==variable)]
    
    # Drop years with no data 
    series = series.dropna()[['year_measured', 'value']]
    
    # Change years to int and set as index 
    series.year_measured = series.year_measured.astype(int)
    series.set_index('year_measured', inplace=True)
    series.columns = [variable]
    return series",0.4872385263,
2590,which country is receiving the least?,"LAT_LNG_APPROX_SYDNEY_CENTER=(-33.861720, 151.012569)
def findNearSydney(merchant, suburb):
    resultJson = gmaps.places_nearby(location=LAT_LNG_APPROX_SYDNEY_CENTER,
                       rank_by='distance',
                       keyword='{}, {}'.format(merchant, suburb))
    return resultJson
                                    

def getBestMatch(merchant, jsonResult):
    candidates = {}
    rNames = []
    rName = None
    lastMatch = None
    for r in jsonResult['results']:
        rName = r['name'].encode('utf-8')
        rVicinity = r['vicinity'].encode('utf-8')
        rNameWithVicinity = ""{} {}"".format(rName, rVicinity)
        rNames.append(rNameWithVicinity)
        types = [t.encode('utf-8') for t in r['types'] if t in CATEGORY_WHITELIST][:3] + [None, None, None]
        types = types[:3]
        candidates[rNameWithVicinity] = (rName, r['place_id'].encode('utf-8'), rVicinity, types)
        lastMatch = candidates[rNameWithVicinity]

    match = lastMatch
    if len(rNames)>1:
        matches = difflib.get_close_matches(merchant, rNames)
        if matches:
            match = candidates[matches[0]]
    if match:
        return match
    else:
        return (rName, None, None, [None, None, None])

    
def getCategories(merchant, suburb):
    uncat = 'UNCATEGORIZED'
    unknown = 'UNKNOWN'
    result = findNearSydney(merchant,suburb)
    if result['status'] == 'ZERO_RESULTS':
        return unknown, unknown, unknown, [uncat, uncat, uncat]    
    bizName, placeId, addr, cats = getBestMatch(""{}, {}"".format(merchant,suburb), result)
    if not placeId:
        placeId = unknown
    if not addr:
        addr = unknown
    if not cats:
        cats = [uncat, uncat, uncat]
    elif not cats[0]:
        cats[0] = uncat
    elif not cats[1]:
        cats[1] = uncat
    elif not cats[2]:
        cats[2] = uncat
    return bizName, placeId, addr, cats",0.4869194627,
2590,which country is receiving the least?,"# Loop through coordinates in df with Citipy to return cities
#row_count = 1
for index, row in df.iterrows():
   
    city = cp.nearest_city((row[""Latitude""]), (row[""Longitude""]))
    return_city = city.city_name
    return_country = city.country_code
    
    df.set_value(index, ""City"", return_city)
    df.set_value(index, ""Country"", return_country) 
     
    # Print log to ensure loop is working correctly
    #print(""Now retrieving city # "" + str(row_count) + "" "" + return_city + "", "" + return_country)
    #row_count += 1 
    
#df.head(3)",0.4801019132,
2590,which country is receiving the least?,"for index, row in City_df.iterrows():
    city = citipy.nearest_city(row['Lat'], row['Lng']).city_name
    country = citipy.nearest_city(row['Lat'], row['Lng']).country_code
    City_df.set_value(index, ""City"", city)
    City_df.set_value(index, ""Country"", country)",0.4783889651,
2590,which country is receiving the least?,"def matching(name, df_1, df_2, index):
    #Find the matching name in df_2 based on the similarity of the name witch the same country.  
    country = df_1.country.values[index]
    matchs_1 = di.get_close_matches(name, df_2.university.values, cutoff=0.5)
    matchs_1_tmp = [removeWords(x) for x in matchs_1]
    matchs_2 = di.get_close_matches(removeWords(name), matchs_1_tmp, cutoff=0.8)
    for i in range(len(matchs_2)):
        index_1 = matchs_1_tmp.index(matchs_2[i])
        index = df_2.index[df_2['university'] == matchs_1[index_1]][0]
        if(df_2.country.values[index] == country):
            return matchs_1[index_1]
    return None",0.4777646065,
2590,which country is receiving the least?,"def stacked_bar(feature):
    ctab = pd.crosstab([df_train[feature].fillna('Unknown')], df_train.country_destination, dropna=False).apply(lambda x: x/x.sum(), axis=1)
    ctab[classes].plot(kind='bar', stacked=True, colormap='terrain', legend=False)",0.477583766,
1287,now solve the problem,"%%writefile -a RosenbrockOpt.cpp

    // solve the optimization problem
    Eigen::VectorXd xOpt = Solver->solve(x0);

    // Get the termination status
    int optStat = Solver->GetStatus();

    std::cout << ""Optimal solution = "" << xOpt.transpose() << std::endl;
    
    return 0;
} // End of ""int main()""",0.4490225315,
1287,now solve the problem,"while time < model_end_time:

    # Get solution
    solver.solve()
    
    # Calculate the RMS velocity.
    vrms = stokes.velocity_rms()

    # Record values into arrays
    if(uw.rank()==0):
        vrmsVal.append(vrms)
        timeVal.append(time)
    
    # Output to disk
    if step%outputEvery == 0:
        if(uw.rank()==0):
            print 'step = {0:6d}; time = {1:.3e}; v_rms = {2:.3e}'.format(step,time,vrms)

        filename = outputPath+""/velocityField.""+str(step)
        vFH      = velocityField.save(filename+"".h5"")
        velocityField.xdmf( filename, vFH, ""velocity"", meshFileHandle, ""Mesh"", time )
        
        filename = outputPath+""/pressureField.""+str(step)
        pFH      = pressureField.save(filename+"".h5"")
        pressureField.xdmf(filename, pFH, ""pressure"", meshFileHandle, ""Mesh"", time )
        
        outputFilename = outputPath+""image""+str(step).zfill(4)
        fig1.save_image(outputFilename)

    # We are finished with current timestep, update.
    time, step = update()
    
if(uw.rank()==0):
    print 'step = {0:6d}; time = {1:.3e}; v_rms = {2:.3e}'.format(step,time,vrms)",0.4384872913,
1287,now solve the problem,"def solve_primal_update(problem_data):
    index, problem_i, p_ang, z_i, ld_i, x = problem_data
    
    # solve problem
    problem_i.solve(verbose=False, solver=cvxpy.ECOS, abstol=1e-9)

    # get solution
    x_i = np.array(x.value).flatten()
    z_i = np.array(z_i.value).flatten()
    # calculate residual
    r_i = norm(p_ang * x_i - z_i, ord=inf)
    
    return index, x_i, r_i, prob.objective.value",0.4337999821,
1287,now solve the problem,"# Perform steps
while iloop == True:
    # Get solution for initial configuration
    stokesPIC.solve()
    # Retrieve the maximum possible timestep for the AD system.
    dt = advDiff.get_max_dt()
    if steps == 0:
        dt = 0.
    # Advect using this timestep size   
    advDiff.integrate(dt)
    # Calculate the RMS velocity and Nusselt number
    v2sum = v2sum_integral.integrate()
    volume = volume_integral.integrate()
    rms_v = math.sqrt(v2sum[0])/volume[0]
    Nu = FindNusseltNumber(temperatureField, linearMesh, Box_Length, Box_Height)
    # store v_rms and time
    vrmsvals.append(rms_v)
    timevals.append(time)
    velplotmax = max(rms_v,velplotmax)
    # check loop break condition
    if(abs((Nu - NuLast)/Nu) < epsilon):
        iloop = False
        print 'steps = {0:6d}  ;time = {1:.3e}  ;v_rms = {2:.3f}  ;Nu = {3:.3f}  ;Rel change = {4:.3e}'.format(steps,time,rms_v,Nu,abs((Nu - NuLast)/Nu))
    if steps%(steps_end/steps_output) == 0:
        print 'steps = {0:6d}  ;time = {1:.3e}  ;v_rms = {2:.3f}  ;Nu = {3:.3f}  ;Rel change = {4:.3e}'.format(steps,time,rms_v,Nu,abs((Nu - NuLast)/Nu))
    if steps>=steps_end:
        iloop = False
    # Increment time and timestep counter and save previous Nusselt number
    time += dt
    steps += 1
    NuLast = Nu",0.4310937822,
1287,now solve the problem,"def learn_and_test(solver_file):
    caffe.set_mode_cpu()
    solver = caffe.get_solver(solver_file)
    solver.solve()

    accuracy = 0
    test_iters = int(len(Xt) / solver.test_nets[0].blobs['data'].num)
    for i in range(test_iters):
        solver.test_nets[0].forward()
        accuracy += solver.test_nets[0].blobs['accuracy'].data
    accuracy /= test_iters
    return accuracy

acc = learn_and_test('hdf5_classification/solver2.prototxt')
print(""Accuracy: {:.3f}"".format(acc))",0.4299826026,
1287,now solve the problem,"def learn_and_test(solver_file):
    caffe.set_mode_cpu()
    solver = caffe.get_solver(solver_file)
    solver.solve()

    accuracy = 0
    test_iters = int(len(Xt) / solver.test_nets[0].blobs['data'].num)
    for i in range(test_iters):
        solver.test_nets[0].forward()
        accuracy += solver.test_nets[0].blobs['accuracy'].data
    accuracy /= test_iters
    return accuracy

%timeit learn_and_test('hdf5_classification/solver.prototxt')
acc = learn_and_test('hdf5_classification/solver.prototxt')
print(""Accuracy: {:.3f}"".format(acc))",0.4299826026,
1287,now solve the problem,"def solution(A):
    for i in range(len(A)):
        a = sum(A[i:])/len(A[i:])
        
        
        for j in A[i:]:",0.4268965125,
1287,now solve the problem,"for i in range(10):
    pnsolution.solve()",0.4245612025,
1287,now solve the problem,"def postSolve(self):
        '''        This method adds consumption at m=0 to the list of stable arm points,
        then constructs the consumption function as a cubic interpolation over
        those points.  Should be run after the backshooting routine is complete.

        
        Parameters
        ----------
        none
        
        Returns
        -------
        none
        '''
        # Add bottom point to the stable arm points
        self.solution[0].mNrm_list.insert(0,0.0)
        self.solution[0].cNrm_list.insert(0,0.0)
        self.solution[0].MPC_list.insert(0,self.MPCmax)
        
        # Construct an interpolation of the consumption function from the stable arm points
        self.solution[0].cFunc = CubicInterp(self.solution[0].mNrm_list,self.solution[0].cNrm_list,
                                             self.solution[0].MPC_list,self.PFMPC*(self.h-1.0),self.PFMPC)
        self.solution[0].cFunc_U = lambda m : self.PFMPC*m
        
    def update():
        '''
        This method does absolutely nothing, but should remain here for compati-
        bility with cstwMPC when doing the ""tractable"" version.
        '''
        return None
        
    def simBirth(self,which_agents):
        '''
        Makes new consumers for the given indices.  Initialized variables include aNrm, as
        well as time variables t_age and t_cycle.  Normalized assets are drawn from a lognormal
        distributions given by aLvlInitMean and aLvlInitStd.
        
        Parameters
        ----------
        which_agents : np.array(Bool)
            Boolean array of size self.AgentCount indicating which agents should be ""born"".
        
        Returns
        -------
        None
        '''
        # Get and store states for newly born agents
        N = np.sum(which_agents) # Number of new consumers to make      
        self.aLvlNow[which_agents] = drawLognormal(N,mu=self.aLvlInitMean,sigma=self.aLvlInitStd,seed=self.RNG.randint(0,2**31-1))
        self.eStateNow[which_agents] = 1.0 # Agents are born employed
        self.t_age[which_agents]   = 0 # How many periods since each agent was born
        self.t_cycle[which_agents] = 0 # Which period of the cycle each agent is currently in
        return None
        
    def simDeath(self):
        '''
        Trivial function that returns boolean array of all False, as there is no death.
        
        Parameters
        ----------
        None
        
        Returns
        -------
        which_agents : np.array(bool)
            Boolean array of size AgentCount indicating which agents die.
        '''
        # Nobody dies in this model
        which_agents = np.zeros(self.AgentCount,dtype=bool)
        return which_agents
        
    def getShocks(self):
        '''
        Determine which agents switch from employment to unemployment.  All unemployed agents remain
        unemployed until death.
        
        Parameters
        ----------
        None
        
        Returns
        -------
        None
        '''
        employed = self.eStateNow == 1.0
        N = int(np.sum(employed))
        newly_unemployed = drawBernoulli(N,p=self.UnempPrb,seed=self.RNG.randint(0,2**31-1))
        self.eStateNow[employed] = 1.0 - newly_unemployed
        
    def getStates(self):
        '''
        Calculate market resources for all agents this period.
        
        Parameters
        ----------
        None
        
        Returns
        -------
        None
        '''
        self.bLvlNow = self.Rfree*self.aLvlNow
        self.mLvlNow = self.bLvlNow + self.eStateNow
        
    def getControls(self):
        '''
        Calculate consumption for each agent this period.
        
        Parameters
        ----------
        None
        
        Returns
        -------
        None
        '''
        employed = self.eStateNow == 1.0
        unemployed = np.logical_not(employed)
        cLvlNow = np.zeros(self.AgentCount)
        cLvlNow[employed] = self.solution[0].cFunc(self.mLvlNow[employed])
        cLvlNow[unemployed] = self.solution[0].cFunc_U(self.mLvlNow[unemployed])
        self.cLvlNow = cLvlNow
        
    def getPostStates(self):
        '''
        Calculates end-of-period assets for each consumer of this type.
        
        Parameters
        ----------
        None
        
        Returns
        -------
        None
        '''
        self.aLvlNow = self.mLvlNow - self.cLvlNow
        return None",0.4242289662,
1287,now solve the problem,"stn = STN()
stn.solve()",0.4211083949,
1131,lognormal model,"def norm1(input):
  return tf.nn.lrn(input, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75)",0.534489274,
1131,lognormal model,"def norm(x):
    return tf.nn.lrn(x, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75)",0.534489274,
1131,lognormal model,"# 3 missing lines:
def missing_code():
    return
    # first pass:
    means = self.init_means_lbl_(X_label, y)
    # second pass:
    dat = np.vstack([dat, X_label[y == i]])
    # third pass:
    means[i] = np.mean(X_label[y == i], axis=0)",0.5193742514,
1131,lognormal model,"# Define temporal feature avaraging layer
def MeanOverTime():
    lam_layer = layers.Lambda(lambda x: K.mean(x, axis=1), output_shape=lambda s: (1, s[2]))
    return lam_layer

# Reshape to (batch, time steps, filters)
model.add(layers.Reshape((-1, 224)))
model.add(layers.core.Masking(mask_value = 0.0))
model.add(MeanOverTime())

# And a fully connected layer for the output
model.add(layers.Dense(4, activation='sigmoid'))

model.summary()",0.5132803917,
1131,lognormal model,"def getLogImages():
  sigmaValues = [1.0, 3.0, 5.0]
  for sigma in sigmaValues:
    logImage = imageoperations.applyLoG(image, sigmaValue=sigma)
    # Crop the filtered Image
    logImage, croppedMask, bb = imageoperations.cropToTumorMask(logImage, mask)
    yield sigma, logImage, croppedMask",0.5122018456,
1131,lognormal model,"#This function gives back the Lasso regression coefficient after the model is fitted
def lasso_fit(alpha, predictor, Target):
    clf = linear_model.Lasso(alpha=alpha)
    clf.fit(predictor,Target)
    coefficient = clf.coef_
    return coefficient",0.5110451579,
1131,lognormal model,"def analyze_moon_study():
    
    model = sklearn.linear_model.LinearRegression(fit_intercept=True)
    getX = lambda x: np.vstack((np.exp(-x),1/x, 1/x**2, 1/x**3)).transpose()
    model.fit(getX(moon_V), factor)
    print model.intercept_, list(model.coef_)

    x_model = np.linspace(16.5, 24, 50)
    y_model = model.predict(getX(x_model))
    
    plt.plot(moon_V, factor, '+')
    plt.plot(x_model, y_model, '-')
    plt.xlabel('Scattered moon V-band magnitude')
    plt.ylabel('Exposure time correction')
    plt.xlim(x_model[0], x_model[-1])
    plt.ylim(0.95, 2.25)
    
analyze_moon_study()",0.5096123219,
1131,lognormal model,"def q2B():
    def x(t, a=1):
        return a * np.exp(10 * t)

    t0 = 0.0
    tEnd = 1.0
    nt = 100
    t = np.linspace(t0, tEnd, nt, endpoint=True)

    plt.figure(figsize=(20,10))
    plt.plot(t, x(t, a=1), 'r-',
             t, x(t, a=2), 'k-', linewidth=2.5)
    plt.legend(['A=1', 'A=2'])
    plt.xlabel('$\mathbf{t}$')
    plt.ylabel('$\mathbf{X(t)}$')
    plt.title('$x(t) = Ae^{10t}$')
    plt.show()
    plt.close()

q2B()",0.5091436505,
1131,lognormal model,"def lip_linreg(A_lin):
    """"""Lipschitz constant for linear squares loss""""""    
    return np.linalg.norm(A_lin.T.dot(A_lin))/n
    
def lip_logreg(A_log):
    """"""Lipschitz constant for logistic loss""""""    
    return np.linalg.norm(A_log.T.dot(A_lin))/(4*n)
    
def ista(x0, f, grad_f, g, prox_g, step, s=0., n_iter=50, x_true=coefs, verbose=True, store=True):
    """"""Proximal gradient descent algorithm
    """"""
    
    list_x_ista = []
    
    x = x0.copy()
    x_new = x0.copy()
    n_samples, n_features = A_lin.shape

    # estimation error history
    errors = []
    # objective history
    objectives = []
    # Current estimation error
    err = np.linalg.norm(x - x_true) / np.linalg.norm(x_true)
    errors.append(err)
    # Current objective
    obj = f(x) + g(x, s)
    objectives.append(obj)
    if verbose:
        print ('Lauching ISTA solver...')
        print (' | '.join([name.center(8) for name in [""it"", ""obj"", ""err""]]))
        
    for k in range(n_iter + 1):

        x_new[:] = prox_g(x - step * grad_f(x), s*step)
        x[:] = x_new
                
        list_x_ista.append(x.copy())
                                
        obj = f(x) + g(x, s)
        err = np.linalg.norm(x - x_true, ord = 2) / np.linalg.norm(x_true, ord = 2)
        errors.append(err)
        objectives.append(obj)
        if k % 10 == 0 and verbose:
            print (' | '.join([(""%d"" % k).rjust(8), 
                              (""%.2e"" % obj).rjust(8), 
                              (""%.2e"" % err).rjust(8)]))
    return list_x_ista, x, objectives, errors",0.5080987811,
1131,lognormal model,"def lip_linreg(A):
    """"""Lipschitz constant for linear squares loss""""""    
    return np.linalg.norm(A.T.dot(A), 2) / A.shape[0]
    
def lip_logreg(A):
    """"""Lipschitz constant for logistic loss""""""    
    return lip_linreg(A) / 4
    
def ista(x0, f, grad_f, g, prox_g, step, s=0., n_iter=50,
         x_true=coefs, verbose=True):
    """"""Proximal gradient descent algorithm
    """"""
    from scipy.optimize import check_grad
    x = x0.copy()
    x_new = x0.copy()
    n_samples, n_features = A.shape
    
    # estimation error history
    errors = []
    # objective history
    objectives = []
    # Current estimation error
    err = np.linalg.norm(x - x_true) / np.linalg.norm(x_true)
    errors.append(err)
    # Current objective
    obj = f(x) + g(x, s)
    objectives.append(obj)
    if verbose:
        print (""Lauching ISTA solver..."")
        print (' | '.join([name.center(8) for name in [""it"", ""obj"", ""err""]]))
    for k in range(n_iter + 1):

        x = prox_g(x - step*grad_f(x))
        
        obj = f(x) + g(x, s)
        err = np.linalg.norm(x - x_true) / np.linalg.norm(x_true)
        errors.append(err)
        objectives.append(obj)
        if k % 10 == 0 and verbose:
            print (' | '.join([(""%d"" % k).rjust(8), 
                              (""%.2e"" % obj).rjust(8), 
                              (""%.2e"" % err).rjust(8)]))
    return x, objectives, errors",0.5078924298,
1306,observed variables and probabilistic queries,"# n * ln(SSE/n) - k * ln(n)
aic = lambda g: g.nobs * np.log((g.resid**2).sum()/g.nobs) + 2*len(g.beta)",0.381506145,
1306,observed variables and probabilistic queries,"#Execute the quantum algorithm
result = my_algorithm.Q_program.execute(my_algorithm.circ_name, backend=my_algorithm.backend, shots= my_algorithm.shots)

#Show the results obtained from the quantum algorithm 

counts = result.get_counts(my_algorithm.circ_name) ## you can declare/initiate q_name first at the time you initialize circuit,

print('\nThe measured outcomes of the circuits are:',counts)",0.3663195372,
1306,observed variables and probabilistic queries,"print(""AUC:"",roc_auc_score(y, best_model.oob_prediction_))",0.3656709194,
1306,observed variables and probabilistic queries,"def bayesian_predictions(data_eval, data, list_hypothesis, log_prior):
    # INPUT
    #  data_eval : [length ne python list] of new numbers we want to check the probability of membership for
    #     each number in data_eval is to be evaluated independently -- it's a separate 'y' in equation above
    #  data : [python list] observed numbers (X) 
    #  list_hypothesis : python list of hypotheses, each is a binary numpy array 
    #  log_prior : numpy vector [length nh] which is the log prior value for each hypothesis
    # 
    # RETURN
    #  pp : numpy vector [size ne] of predicted probabilities of new numbers in data_eval (NOTE: NOT IN LOG SPACE)
    lpost = log_posterior(data,list_hypothesis,log_prior)
    post = np.exp(lpost) # posterior probabilities
    h_mat = np.array(list_hypothesis) # create a [nh by x_max] numpy matrix, showing numbers in each hypothesis
    ne = len(data_eval) # how many numbers to evaluate
    pp = np.zeros(ne) # predicted probability of each number
    for idx,de in enumerate(data_eval):
        #TODO : Add your code here to compute predicted probabilities. Can be a single line with form ""pp[idx] = ""..
    return pp

# TODO : Check your answers for problem 2 using the bayesian_prediction function",0.3652628362,
1306,observed variables and probabilistic queries,"class ObserverModel(pm.Model):
    """"""Stores observed variables until the model is created.""""""
    def __init__(self, observed):
        self.observed = observed
        super(ObserverModel, self).__init__()

    def Var(self, name, dist, data=None, total_size=None):
        return super(ObserverModel, self).Var(name, dist,
                                              data=self.observed.get(name, data),
                                              total_size=total_size)


def sampled(f):
    """"""Decorator to delay initializing pymc3 model until data is passed in.""""""
    def wrapped_f(**observed):
        try:
            with ObserverModel(observed) as model:
                f(**observed)
        except TypeError:
            with ObserverModel(observed) as model:
                f()
        return model
    return wrapped_f",0.3652377129,
1306,observed variables and probabilistic queries,"pybnl.bn.relative_mutual_information(latent, net_ldmarks.imputed_['LAT'])",0.3643453121,
1306,observed variables and probabilistic queries,"from gym.core import ObservationWrapper
class Binarizer(ObservationWrapper):
    
    def _observation(self,state):    
        
        #state = <round state to some amount digits.>
        #hint: you can do that with round(x,n_digits)
        #you will need to pick a different n_digits for each dimension
        for index in range(len(state)):
            scalted_state = state[index] / range_width[index] / scaling_parameters[index]
            state[index] = np.round(scalted_state, 2) * range_width[index] * scaling_parameters[index]
        return tuple(state)",0.3633909225,
1306,observed variables and probabilistic queries,"from gym.core import ObservationWrapper
class Binarizer(ObservationWrapper):
    
    def _observation(self, state):    
        
        #state = <round state to some amount digits.>
        #hint: you can do that with round(x,n_digits)
        #you will need to pick a different n_digits for each dimension
        state = [round(v,dig) for v,dig in zip(state,[1,1,2,0])]
        return tuple(state)",0.3625325561,
1306,observed variables and probabilistic queries,"def service_times(arrv):
    events = arrv.events_of_qid(1)
    return [ evt.s for evt in events if (not evt.obs_a or not evt.obs_d)]",0.3612810373,
1306,observed variables and probabilistic queries,"# Rename df for convenience
X = df

# Get top overdispersed genes
myDispersion = sct.dispersion(X)
myDispersion.calc_dispersion() # calculate overdispersion
genes_overdispersed = myDispersion.get_hits(N=500)
Y = X.loc[genes_overdispersed.index]",0.3610959947,
1160,manually find the control gains k matrix,"def find_varsplit(self):
    for i in range(self.c): self.find_better_split(i)
    if self.is_leaf: return
    x = self.split_col
    lhs = np.nonzero(x<=self.split)[0]
    rhs = np.nonzero(x>self.split)[0]
    self.lhs = DecisionTree(self.x, self.y, self.idxs[lhs])
    self.rhs = DecisionTree(self.x, self.y, self.idxs[rhs])",0.4359682798,
1160,manually find the control gains k matrix,"def find_varsplit(self):
    
    # is there something better
    for i in range(self.c): self.find_better_split(i)
    if self.is_leaf: return
    x = self.split_col
    
    # gets the boolean array, but gets the indexes
    lhs = np.nonzero(x<=self.split)[0]
    rhs = np.nonzero(x>self.split)[0]
    
    # now that we have the indexes we can create a decision tree
    self.lhs = DecisionTree(self.x, self.y, self.idxs[lhs])
    self.rhs = DecisionTree(self.x, self.y, self.idxs[rhs])",0.4359682798,
1160,manually find the control gains k matrix,"def get_all_number(parameters):   #Get all numbers generated by the LCG
    generated_numbers = []
    x_n = parameters.seed
    for _ in range(0, parameters.m):
        x_n_plus_one = (parameters.a * x_n + parameters.b) % parameters.m
        generated_numbers.append(float(x_n_plus_one) / parameters.m)
        x_n = x_n_plus_one
    return generated_numbers

# Get random numbers between 0 and the upper bound
def rand(parameters, upper_bound, nb_numbers=1): 
    random_numbers = []
    for i in range(0, nb_numbers):
        new_seed = (parameters.a * parameters.seed + parameters.b) % parameters.m
        random_number = new_seed % upper_bound
        random_numbers.append(int(random_number))
        parameters.seed = new_seed
    return random_numbers

# Get random numbers between lower bound and the upper bound 
def rand_between_2_numbers(parameters, upper_bound, lower_bound, nb_numbers=1): 
    random_numbers = []
    for i in range(0, nb_numbers):
        new_seed = (parameters.a * parameters.seed + parameters.b) % parameters.m
        random_number = new_seed % (upper_bound - lower_bound)
        random_numbers.append(int(random_number) + lower_bound)
        parameters.seed = new_seed
    return random_numbers",0.4350729585,
1160,manually find the control gains k matrix,"def theoretical_density(df):
    df['td'] = 3.1 + -1.121*df.A*(df.B-1.136)**(-0.852)",0.4305269718,
1160,manually find the control gains k matrix,"def dataframe(city_data):
    housing_prices = city_data.target
    housing_features = city_data.data
    X,y = housing_features, housing_prices
    df_data = pd.DataFrame(housing_features, columns = boston.feature_names)
    df_target = pd.DataFrame(housing_prices, columns =['MEDV'])
    df_boston = pd.concat([df_data, df_target,], axis = 1)
    df = df_boston.corr()
    corr_target = df.ix[-1][:-1]
    predict = corr_target.sort(ascending=False)
    df_sort = corr_target.sort_values(ascending=False)
    print(df_sort)
    print(df)
    return df_boston",0.4291303754,
1160,manually find the control gains k matrix,"def solve_random(self):
    open_idx = list(range(self.F))
    random.shuffle(open_idx)
    open_idx = open_idx[:self.F * 4 // 5] # open 80% of facilities
    
    y = np.zeros(self.F)
    y[open_idx] = 1
    
    assignments = np.empty(problem.C)
    for cli, fac in zip(range(problem.C), cycle(open_idx)):
        assignments[cli] = fac
    
    return y, assignments

Problem.solve_random = solve_random",0.4288293123,
1160,manually find the control gains k matrix,"def solve_greedy(self):
    y = np.zeros(self.F, dtype=np.int32)
    assignments = np.ones(self.C, dtype=np.int32)*(-1)
    assigned = np.zeros(self.C, dtype=np.bool) #to check if client already assigned
    n_assigned = 0
    total_cost = 0
    while n_assigned < self.C:
        min_cost = np.max(np.max(self.assignment_costs)) + np.max(self.opening_costs) + 1
        #here we could start from any pair (c,f) but random pair must be already taken (assigned[c] is True)
        #so let's start with min_cost > any of cost(c,f)
        min_c, min_f = 0, 0
        for c in range(self.C):
            for f in range(self.F):
                if not assigned[c] and y[f] < self.capacities[f]:
                    if self.assignment_costs[f][c] + (0 if y[f] > 0 else self.opening_costs[f]) < min_cost:
                        min_cost = self.assignment_costs[f][c] + (0 if y[f] > 0 else self.opening_costs[f])
                        min_c, min_f = c, f
        total_cost += min_cost
        assigned[min_c] = True
        assignments[min_c] = min_f
        y[min_f] += 1
        n_assigned += 1
    y = y > 0
    return y, assignments

Problem.solve_greedy = solve_greedy",0.4283910394,
1160,manually find the control gains k matrix,"def get_simple_statistics(msm):

    # we use 15 PCCA+ clusters as in Analysis_Inactive_LL.ipynb
    pcca=pyemma.msm.PCCA(MSM.P,15)

    ## find which macrostate contains more microstates, we assume this to be the bulk
    #pop_macro=np.array([len(MSM.active_set[micro]) for i,micro in enumerate(pcca.metastable_sets)])
    pop_macro2=np.array([len(micro) for i,micro in enumerate(pcca.metastable_sets)])
    bulk_macro_ID = np.argmax(pop_macro2)
    
    ## be very careful with the active set indices here. DOUBLE CHECK!!
    bulk_microstates = pcca.metastable_sets[bulk_macro_ID]
    # nonbulk is all the rest (I hate python)
    nonbulk_microstates = np.concatenate(pcca.metastable_sets[:bulk_macro_ID] + pcca.metastable_sets[(bulk_macro_ID+1):])
    # start-end order
    #bind_toanything_steps = MSM.mfpt(bulk_microstates, nonbulk_microstates)
    bind_toanything_steps = MSM.mfpt(nonbulk_microstates, bulk_microstates)
    unbind_fromanything_steps = MSM.mfpt(bulk_microstates,nonbulk_microstates)
    
    return bind_toanything_steps, unbind_fromanything_steps",0.4279386401,
1160,manually find the control gains k matrix,"def Inspectors(model):
    
    step = 1. / model.lipschitz_constant()
    
    # Conjugate gradient
    cg_inspector = inspector(model.loss, x_min, verbose=True) 
    x_cg = fmin_cg(model.loss, x_init, model.grad, maxiter=n_iter, callback=cg_inspector)
    
    # BFGS
    bfgs_inspector = inspector(model.loss, x_min, verbose=True) 
    x_bfgs, _, _ = fmin_l_bfgs_b(model.loss, x_init, model.grad, maxiter=n_iter, callback=bfgs_inspector)
    
    # ISTA
    ista_inspector = inspector(model.loss, x_min, verbose=True) 
    x_ista = ista(x_init, model.grad, n_iter, step, callback=ista_inspector)
    
    # FISTA
    fista_inspector = inspector(model.loss, x_min, verbose=True) 
    x_fista = fista(x_init, model.grad, n_iter, step, callback=fista_inspector)
    
    # SGD
    sgd_inspector = inspector(model.loss, x_min, verbose=True) 
    x_sgd = sgd(x_init, iis, model.grad_i, n * n_iter, step=step0, callback=sgd_inspector)
    
    # SAG
    if isinstance(model, LogReg):
        step = 4.0 / (max_squared_sum + 4.0 * model.lbda / model.n)
    else:
        step = 1.0 / (max_squared_sum + model.lbda / model.n)
        
    sag_inspector = inspector(model.loss, x_min, verbose=True) 
    x_sag = sag(x_init, iis, model.grad_i, n * n_iter, step, callback=sag_inspector)
    
    # SVRG
    if isinstance(model, LogReg):
        step = 4.0 / (max_squared_sum + 4.0 * model.lbda / model.n)
    else:
        step = 1.0 / (max_squared_sum + model.lbda / model.n)
        
    svrg_inspector = inspector(model.loss, x_min, verbose=True) 
    x_svrg = svrg(x_init, iis, model.grad, model.grad_i, n_iter, step, callback=svrg_inspector)

    
    return ([ista_inspector, fista_inspector, cg_inspector, bfgs_inspector, sgd_inspector,
                                         sag_inspector, svrg_inspector])",0.4252303243,
1160,manually find the control gains k matrix,"def top_features(clf):
    lst = []
    feature_sorted = np.argsort(np.abs(clf.coef_[0]))[::-1]
    for i in feature_sorted:
        v = (feat_lst[i], round(float(clf.coef_[0][i]),3))
        lst.append(v)
    return lst",0.4246890247,
552,examining dimensions,"def statimg(f):
    import scipy.ndimage as ndimage
    im = ndimage.imread(f)
    return [ f, im.mean(), lapedges(im).mean() ]",0.4215287864,
552,examining dimensions,"def extract_country_year_weather(data):
    # data is a nested tuple, so we first need to extract the weather and the station data
    station = ...
    weather = ...
    # Now extract country from station
    country = ...
    # and the year from the weather measurement data
    year =  ...
    return ((country, year), weather)

# Perform extraction
weather_per_country_and_year = joined_weather.map(extract_country_year_weather)",0.4202075303,
552,examining dimensions,"# Function to check the range in years of the available data
def get_year_range(database):
    min_year = database['yearID'].min()
    max_year = database['yearID'].max()
    return min_year, max_year
    
print('')
# Check year range for the salary dataset
min_salary_year, max_salary_year = get_year_range(Salaries_dataset)
print('Salary information are available from %d to %d' % (min_salary_year, max_salary_year) )

# Check year range for the batting dataset
min_batting_year, max_batting_year = get_year_range(Batting_dataset)
print('Batting information are available from %d to %d' % (min_batting_year, max_batting_year) )

# Check year range for the fielding dataset
min_fielding_year, max_fielding_year = get_year_range(Fielding_dataset)
print('Fielding information are available from %d to %d' % (min_fielding_year, max_fielding_year) )

# Check year range for the pitching dataset
min_pitching_year, max_pitching_year = get_year_range(Pitching_dataset)
print('Pitching information are available from %d to %d' % (min_pitching_year, max_pitching_year) )
print('')

# Histogram of annual salary availability

# bin_size_years = 1
# num_bins = int((max_salary_year - min_salary_year) / 2) + 1
# Salaries_dataset['yearID'].hist(bins=num_bins, figsize=(20,15))
# plt.xlabel('Year',fontsize=20)
# plt.ylabel('Frequency',fontsize=20)
# plt.title('Salary information per year',fontsize=25)
# plt.show()


# Histogram of yearly availability of data
a = [Salaries_dataset['yearID'], Batting_dataset['yearID'], Fielding_dataset['yearID'], Pitching_dataset['yearID']]
categories = ['Salary','Batting','Fielding','Pitching']

# Compare frequency of data per year
plt.figure(figsize=(30,15))
plt.hist(a,bins=np.arange(min_batting_year, max_batting_year, 5),label=categories)
plt.legend(bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0.,fontsize=30)
plt.title('Frequency of available data per year',fontsize=35)
plt.xlabel('Year (bin size = 5)',fontsize=25)
plt.ylabel('Frequency',fontsize=25)
plt.tick_params(labelsize=20)
plt.show()",0.4191059172,
552,examining dimensions,"def min_and_max(input_dict) : 
    min_value = min(input_dict.items())
    max_value = max(input_dict.items())
    return(min_value, max_value)",0.411152482,
552,examining dimensions,"class Specimen:
    yield_stress = 0
    yield_strain = 0
    youngs_modulus = 0
        
    def __init__(self, csv_path, hooks_law_bounds):
        # Get data from CSVs
        measurement = read_csv(csv_path, header=0, index_col=0, nrows=2).transpose() # measurements are horizontal => transpose
        data = read_csv(csv_path, header=0, index_col=0, skiprows=4)
        
        # Initial measurements
        self.length = measurement[""Length""][0] / 1000.0 # Convert mm to m
        self.width = measurement[""Width""][0] / 1000.0
        self.thickness = float(measurement.index[0]) / 1000.0  # First column is the index in a Pandas dataframe
        
        cross_section = self.width * self.thickness # m^2
        
        # Get time series
        self.time = data.index[:] # s
        self.extension = [float(x) / 1000.0 for x in data[1:][""Extension""]] # m, from instron
        self.load = data[1:][""Load""].tolist() # N
        self.displacement = [float(x) / 1000.0 for x in data[1:][""Displacement (Strain 1)""]] # m, from extensiometer
        
        # Calculate secondary time series
        self.stress = [float(x) / (cross_section * 1e6) for x in self.load]
        self.instron_strain = [float(x) / self.length for x in self.extension]
        self.dirty_strain = [float(x) / self.length for x in self.displacement] # Doesn't account for extensiometer removal
        self.extensiometer_strain = self.max_grad_filter(self.dirty_strain)
        
        # Yield point calculation
        self.yield_stress = max(self.stress)
        yield_point_idx = self.stress.index(self.yield_stress)
        self.yield_strain = self.instron_strain[yield_point_idx]
        
        # Young's modulus calculation
        low, high = hooks_law_bounds    # Found by inspection of the graph
        num_samples = 4                 # Sample data to get overall gradient instead of noise
        t_strain = self.extensiometer_strain[low:high] # Get range
        t_stress = self.stress[low:low+len(t_strain)]
        t_strain = t_strain[::len(t_strain) / num_samples] # Sample
        t_stress = t_stress[::len(t_stress) / num_samples]
        self.youngs_modulus = mean(gradient(t_stress, t_strain))
        
        
    def plot_stress_strain(self, source, label):
        if source.lower() == ""instron"":
            strain = self.instron_strain
        elif source.lower() == ""extensiometer"":
            strain = self.extensiometer_strain
        else:
            print(""Unkown strain source"")
            return None
        
        plt.plot(strain, self.stress, label=""#{}"".format(label))
        
    
    def max_grad_filter(self, array):
        grad = abs(gradient(array))
        stop_index = argmax(grad)
        return array[:stop_index]

        
class Level:      
    def __init__(self, level_names, hooks_law_bounds=(90, 160)):
        # Type checks (aka slowing your code down)
        if type(level_names) == int:
            level_names = str(level_names)

        if type(level_names) == str:
            level_names = [level_names]

        self.level_name = level_names[0]
        
        # Make list of specimen
        self.specimen_list = []
        for level_name in level_names:
            num_specimen = len(os.listdir(level_string.format(level_name)))
            for specimen in range(1, num_specimen + 1):
                self.specimen_list.append(Specimen(specimen_string.format(level_name, specimen), hooks_law_bounds))
                
        # Make aggregate calculations
        self.youngs_modulus, self.youngs_modulus_std, self.youngs_modulus_err = self.aggregate(""youngs_modulus"")
        self.yield_stress, self.yield_stress_std, self.yield_stress_err = self.aggregate(""yield_stress"")
        self.yield_strain, self.yield_strain_std, self.yield_strain_err = self.aggregate(""yield_strain"")

        
        
    def aggregate(self, value):
        value_list = [getattr(spec, value) for spec in self.specimen_list]
        average = mean(value_list)
        deviation = std(value_list)
        error = deviation / average
        return (average, deviation, error)
                
    def __len__(self):
        return len(self.specimen_list)
    
    
    def __delitem__(self, idx):
        del self.specimen_list[idx]
        
        
    def __getitem__(self, idx):
        return self.specimen_list[idx]
    
    
    def plot(self, source=""instron""):
        plt.figure()
        spec_num = 1

        for specimen in self.specimen_list:
            specimen.plot_stress_strain(source, spec_num)

            spec_num += 1

        plt.legend(loc=9, bbox_to_anchor=(1.2, 0.95))
        plt.xlabel(""Strain (mm/mm)"")
        plt.ylabel(""Stress (MPa)"")
        plt.title(""Level {}"".format(self.level_name))
        plt.show()",0.4103446305,
552,examining dimensions,"# example for prove
print (Z_norm[:, 0].min (), Z_norm[:, 0].max ())
print (Z_norm[:, 0].mean (), Z_norm[:, 0].std ())",0.4079270363,
552,examining dimensions,"#testing perspective transform on a test image
def perspective_transform_test_img():
    test_image = mpimg.imread(""test_images/straight_lines1.jpg"")
    img_size = (test_image.shape[1], test_image.shape[0])
    undistorted_img = undistort_img(test_image, mtx, dist)
    i = draw_polygon(undistorted_img)
    warped = cv2.warpPerspective(undistorted_img, M, img_size)
#     warped = draw_dst_lines(warped)
    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))
    ax1.set_title(""Original Image"")
    ax1.imshow(i)

    ax2.set_title(""Undistorted Warped Image"")
    ax2.imshow(warped)
    
    f.savefig('output_images/perspective_transform_fig.jpg', bbox_inches='tight')
    cv2.imwrite('output_images/perspective_transform.jpg', warped)",0.4077734649,
552,examining dimensions,"def calculate_centroid(df):
    x = df['x'].mean()
    y = df['y'].mean()
    return (x,y)",0.4045807719,
552,examining dimensions,"def load():
    with open(""mnist.pkl"",'rb') as f:
        mnist = pickle.load(f)
    return mnist[""training_images""], mnist[""training_labels""], mnist[""test_images""], mnist[""test_labels""]
def plot_rotated_images(x_train,x_train_rotated,shift_angles,pair_num_to_use=1):
    counter=1
    fig = plt.figure()
    for term in range(pair_num_to_use):
        img_id_to_use=np.random.randint(low=0,high=shift_angles.shape[0])
        ax=fig.add_subplot(pair_num_to_use,2,counter)
        img_temp=np.squeeze(x_train[img_id_to_use])
        plt.imshow(img_temp,cmap='gray')
        ax.set_title(""no rotation"")
        
        counter+=1
        ax=fig.add_subplot(pair_num_to_use,2,counter)
        img_temp=np.squeeze(x_train_rotated[img_id_to_use])
        plt.imshow(img_temp,cmap='gray')
        ax.set_title(""rotation angle of {0:.2f}"".format(shift_angles[img_id_to_use]*180))
        counter+=1
    fig.tight_layout()
def rotate_images(x_train,shift_angles):
    x_train_rotated=np.zeros(x_train.shape)
    for term in range(x_train.shape[0]):
        img_temp=x_train[term]
        angle_temp=shift_angles[term]*359
        img_rotated=rotate(img_temp,angle_temp,reshape=False)
        x_train_rotated[term]=img_rotated
    return x_train_rotated
def get_train_validation(x_train,x_train_rotated,shift_angles,validation_size):
    total_sample_num=x_train_rotated.shape[0]
    train_size=total_sample_num-validation_size
    rand_vals=np.random.uniform(size=(total_sample_num))
    new_loc=np.argsort(rand_vals)
    x_train_rotated=x_train_rotated[new_loc]
    x_train=x_train[new_loc]
    shift_angles=shift_angles[new_loc]
    
    x_validation=x_train[-validation_size:]
    x_validation_rotated=x_train_rotated[-validation_size:]
    shift_angles_validation=shift_angles[-validation_size:]
   
    x_train=x_train[:train_size]
    x_train_rotated=x_train_rotated[:train_size]
    shift_angles=shift_angles[:train_size]
   
    out_dict={}
    out_dict[""x_train""]=x_train
    out_dict[""x_train_rotated""]=x_train_rotated
    out_dict[""shift_angles""]=shift_angles
   
    out_dict[""x_validation""]=x_validation
    out_dict[""x_validation_rotated""]=x_validation_rotated
    out_dict[""shift_angles_validation""]=shift_angles_validation
 
    return out_dict

def duplicate_data(x_train_original,repetition_num):
    x_train=x_train_original.copy()
    for term in range(repetition_num-1):
        x_train=np.concatenate((x_train,x_train_original),axis=0)
    return x_train
def get_stats(group):
    return {'median_err': group.min(), 'max_err': group.max()\
            , 'count': group.count(), 'mean_err': group.mean()\
            , ""std_err"": group.std()}",0.4025220275,
552,examining dimensions,"def stem_and_lemmatize(words):
    stems = stem_words(words)
    lemmas = lemmatize_verbs(words)
    return stems, lemmas

stems, lemmas = stem_and_lemmatize(words)
print('Stemmed:\n', stems)
print('\nLemmatized:\n', lemmas)",0.4025096297,
652,find_collinear,"# Plot CWA envelope
for i, geom in enumerate(boundaries):
    gbounds = Polygon(geom)
    intersection = gbounds.intersection
    geoms = (intersection(geom)
         for geom in boundaries
         if gbounds.intersects(geom))
    shape_feature = ShapelyFeature(geoms,ccrs.PlateCarree(), 
                        facecolor='none', linestyle=""-"",linewidth=3.,edgecolor='#cc5000')
    ax.add_feature(shape_feature)

fig",0.4139992595,
652,find_collinear,"unigrams_common = []

def get_common_unigrams(row):
    unigrams_common.append(set(row[""unigrams_ques1""]).intersection(set(row[""unigrams_ques2""])))
    return len( set(row[""unigrams_ques1""]).intersection(set(row[""unigrams_ques2""])) )

train_df[""unigrams_common_count""] = train_df.apply(lambda row: get_common_unigrams(row),axis=1)
train_df[""unigrams_common""] = unigrams_common

#test_df[""unigrams_common_count""] = test_df.apply(lambda row: get_common_unigrams(row),axis=1)",0.4068360925,
652,find_collinear,"def kNN(srcVec, destVecs, k=7, method=cv2.HISTCMP_CORREL, absolute=True, reverse=False):
    distances = []
    srcVec = srcVec.flatten().astype(np.float32)
    for destVec in destVecs:
        distance = cv2.compareHist(srcVec, destVec.flatten().astype(np.float32), method=method)
        distances.append(distance)
    
    if absolute:
        distances = np.absolute(distances)
        
    if reverse:
        sortedIdx = np.argsort(distances)[-k:][::-1]
    else:
        sortedIdx = np.argsort(distances)[:k]
        
    return sortedIdx, [distances[idx] for idx in sortedIdx]",0.4041714072,
652,find_collinear,"from copy import deepcopy

def joinNames(r, names):
    
    all_names_in_record = all( (n in r) for ntpl in names for n in ntpl )
    
    if all_names_in_record:
        l = deepcopy(r)
        for nametuple in names:
            for part in nametuple:
                l.remove(part)
            l.append("","".join(nametuple))
    else:
        l = r
    return l
                
        

namesToJoin = [(""L.A.F"", ""Santos Filho"")]
occs_itatiaia['recordedBy_atomized'] = occs_itatiaia['recordedBy_atomized'].apply( joinNames, names=namesToJoin)",0.4027878344,
652,find_collinear,"if False:
    for i in range(maxlen):
        for b in range(bm):
            v = bfreq[i,b]
            print(""%d %d %d"" % (i, b, v))
            if v == 0:
                continue
            for t in range(cm):
                c = cnt[i,b,t]
                print(""  %d %d %f"" % (t, c, c / v))

print(bfreq.shape)",0.4022200704,
652,find_collinear,"#Determine overlap of point with polygon boundaries
def has_overlap(x, y, poly):
    """"""Determines overlap of point with a polygon
    
    Keyword Arguments:
    x = longitude
    y = latitude
    poly = polygon
    
    Returns:
    True/False if the point intersects the polygon
    """"""
    this_point = Point(x, y)
    intersection = this_point.intersects(poly)
    
    return intersection",0.3992342353,
652,find_collinear,"def chi2(st_true,st_model,sigma=1e-6):
    data_vector = np.hstack((st_true[0].data,st_true[1].data,st_true[2].data))
    predicted_data_vector = np.hstack((st_model[0].data,st_model[1].data,st_model[2].data))
    chi2_misfit = np.linalg.norm((data_vector - predicted_data_vector)/sigma)**2
    #print 'CHI2',chi2_misfit
    return chi2_misfit",0.3963007331,
652,find_collinear,"def not_index(indices,n):
    keep=np.ones(n,bool)from astrometry.libkd.spherematch import match_radec
isim,itrac,d= match_radec(simcat.ra, simcat.dec, obitractor.ra, obitractor.dec,          
                          1./3600.0,nearest=True)
    keep[indices]=False
    return np.arange(n)[keep]

from astrometry.libkd.spherematch import match_radec
isim,itrac,d= match_radec(simcat.ra, simcat.dec, obitractor.ra, obitractor.dec,          
                          1./3600.0,nearest=True)
not_isim= not_index(isim,len(simcat))
not_itrac= not_index(itrac,len(obitractor))
sns.distplot(d*3600)",0.3954335749,
652,find_collinear,"def pick_data_ovo(x, y, pair):
    """"""Returns feature and labels of data points for only the chosen pair of classes.
    
    The response variables are also converted into +/- 1 labels.
    """"""
    # Pick the data for the two classes
    chosen = np.isin(y, pair)
    x_chosen = x[chosen]
    y_chosen = y[chosen]

    # Convert response variables to +/- 1 labels
    y_chosen = ((y_chosen == pair[1]) * 2 - 1).astype(x.dtype)
    
    return x_chosen, y_chosen",0.3948945105,
652,find_collinear,"def token_set_clustering(input_list):
    checked = []

    for name in input_list:
        cluster = []

        # Create a new list of names we haven't encountered so far + turn into a list
        unchecked_list = np.setdiff1d(input_list, checked, assume_unique=True).tolist()

        # Go through each 'new' name and check for similarity against current name
        for unchecked_name in unchecked_list:
            # Only consider if similar enough and new to this iteration
            if (fuzz.token_set_ratio(name, unchecked_name) >= 90) and (unchecked_name not in checked):
                checked.append(unchecked_name)
                cluster.append(unchecked_name)

        if len(cluster) != 0:
            cluster_list.append(cluster)",0.3947232962,
1798,"remove null values from county, category, and category name","def _data_clean_up(X):
    X['Sex'] = X['Sex'].map(lambda x: 1 if x == 'M' else (-1 if x == 'F' else 0))
    X.drop('Rings', inplace=True, axis=1)",0.4984885454,
1798,"remove null values from county, category, and category name","### Forward/Back Fill missing data
def fill_missing_data(stock_dict):
    for stock_name in stock_dict:
        stock_dict[stock_name]['original_data'].fillna(method='ffill', inplace=True)
        stock_dict[stock_name]['original_data'].fillna(method='bfill', inplace=True)

    return stock_dict

stock_dict = fill_missing_data(stock_dict)
df_main = stock_dict[symbol]['original_data']

## Display
# df_main[symbol].plot()
display(df_main.head(10))
print(df_main.isnull().sum())",0.4909996986,
1798,"remove null values from county, category, and category name","def remove_punctuations(df):
                            df['workclass'] = df['workclass'].apply(lambda x: np.nan if x == '?' else str(x))
                            df['occupation'] = df['occupation'].apply(lambda x: np.nan if x == '?' else str(x))
                            df['native-country'] = df['native-country'].apply(lambda x: np.nan if x == '?' else str(x))
                            return df",0.4905367196,
1798,"remove null values from county, category, and category name","def NoAge(data):
    NoAge = data['Age'].isnull()
    data['AgeMissing'] = NoAge
    return(data)",0.4872949123,
1798,"remove null values from county, category, and category name","def remove_highly_correlated_features(df):
    train_df = df[df['TARGET'].notnull()]
    test_df = df[df['TARGET'].isnull()]
    feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]
    
    # Threshold for removing correlated variables
    threshold = 0.9

    # Absolute value correlation matrix
    corr_matrix = train_df[feats].corr().abs()
    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))
    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]
    df_dropped = df.drop(columns = to_drop)
    return(df_dropped)",0.4812979698,
1798,"remove null values from county, category, and category name","def create_earliest_cr_line_days_feature(df):
    """"""
    Create a feature for the earliest credit line in days
    input 
        df - dataframe with missing earliest credit line feature
    """"""
    # Drop data missing this feature
    df.dropna(subset=['earliest_cr_line'],inplace=True)
    df['earliest_cr_line_days'] = df['earliest_cr_line'].apply(
    lambda x: convert_cr_line_days(x))
    df.drop(labels=['earliest_cr_line'],axis=1,inplace=True)
    return df",0.4805117846,
1798,"remove null values from county, category, and category name","def fill_linreg(df):
# neat function that splits our df into df's with nulls and without
    train = df[df['y'].notnull()]
    test = df[df['y'].isnull()]
    
# run our linear regression fit and predict
    slope, cons = linear_reg_fit(train)
    new_test = linear_reg_predict(test, slope, cons)
    
# neat function that fills null values with a list of predicted values
    df['y'].fillna(new_test['y'], inplace=True)
    return df",0.4801680446,
1798,"remove null values from county, category, and category name","def remove_nans(df):
    nan_df = df[pd.isnull(df).any(axis=1)]
    for col in nan_df.columns:
        tmp = nan_df[col][pd.isnull(nan_df[col])]
        if len(tmp) >= 1:
            #print ""These features have NULLs to signify the absence of such a feature:""
            print col, len(tmp)
            df[col] = df[col].fillna(0.)
    print np.shape(df)
    return df",0.4783629775,
1798,"remove null values from county, category, and category name","def clean_institute(df):
    df.dropna(inplace=True)
    df[df.str.contains('Harvard')]='Harvard University'
    df[df.str.contains('Stanford')]='Stanford University'
    df[df.str.contains('Princeton')]='Princeton University'
    df[df.str.contains('Yale')]='Yale University'
    df[df.str.contains('Massachusetts Institute of Technology')]='MIT'
    return df

ds_institute = ds.background.apply(lambda x: split_background(x,1))
health_institute = health.background.apply(lambda x: split_background(x,1))
# Clean up the text a little
ds_institute = clean_institute(ds_institute)
health_institute = clean_institute(health_institute)

plot_rank(ds_institute.value_counts()[:10].sort_values(),'Top 10 Universities in Data Science Program')
plot_rank(health_institute.value_counts()[:5].sort_values(),'Top 5 Universities in Health Program')",0.4781156182,
1798,"remove null values from county, category, and category name","def preprocess(_df):
    _df['text']=_df['text'].apply(lambda tweet:str(tweet) if str(tweet).count('\n')<=3 else '')
    _df['text']=_df['text'].apply(lambda tweet:tweet if tweet.count('#')<=3 else '')
    _df['text']=_df['text'].apply(lambda tweet:re.sub('[^ ]+\.[^ ]+','',tweet))
    _df['text']=_df['text'].apply(lambda tweet:re.sub('#[^ ]+','',tweet))
    _df['text']=_df['text'].apply(lambda tweet:re.sub('[^a-zA-Z0-9 ]',' ',(tweet)))
    _df['text']=_df['text'].apply(lambda tweet:' '.join([word.lower() for word in tweet.strip().split() if word.lower() not in stop_words]))
    _df['text']=_df['text'].apply(lambda tweet:stemmer.stem(tweet.strip()))
    return _df",0.4777418375,
317,core python crash course,cd python/IHEcourse2017/exercises/Apr18/,0.4434927702,
317,core python crash course,"os.execl(""/bin/ps"", ""-ef"")",0.4288259745,
317,core python crash course,"%%sql
SELECT COUNT(*) FROM sfcrime",0.4107446671,
317,core python crash course,ls data/crime_beats/,0.4104144275,
317,core python crash course,"killings = pd.read_csv('C:\Users\david.becker\Desktop\python\DS-SF-41\data\police-killings.csv', index_col=[0])
killings.head()",0.4057859182,
317,core python crash course,"stats.pointbiserialr(PTCAudit3.Ptc_Err_190_Cdb, PTCAudit3.BAD_RETURN_NUM)",0.4057201743,
317,core python crash course,"try:
    db.remove(db.arrays.IHI_PREDICTED_SLEEP)
except:
    print(""Array not found"")",0.4056446552,
317,core python crash course,"# compute the overall survival rate (what fraction of passengers survived the shipwreck)

<YOUR CODE>",0.4052380919,
317,core python crash course,"print ""This will
   break horribly""",0.4039465785,
317,core python crash course,lets roll a die what is the probability of getting 6.,0.401888907,
1462,part tf idf,"def build_model_lstm_oneHot(layers):
    """"""
    Given the layer settings, build the LSTM model
    @Parameters:
    layers: an array of the number of units on each layer
    """"""
    lstm_model = Sequential()

    lstm_model.add(LSTM(
        32,
        input_shape=(layers[1], layers[0]),
        return_sequences=True))
    lstm_model.add(Dropout(0.2))

    lstm_model.add(LSTM(
        32,
        return_sequences=False))
    lstm_model.add(Dropout(0.2))
    
    lstm_model.add(Dense(
        output_dim=layers[3]))
    lstm_model.add(Activation(""relu""))

    lstm_model.add(Dense(
    output_dim=layers[4]))
    lstm_model.add(Activation(""relu""))
    
    lstm_model.add(Dense(layers[5]))
    lstm_model.add(Activation(""softmax""))
    
    return lstm_model",0.4594666362,
1462,part tf idf,"## Build the basic model..
def build_basic_gru_model(layers):
        model = Sequential()
        
        model.add(GRU(2048, input_shape=(layers[0]+1, layers[1]-1), return_sequences=True))
        model.add(GRU(1024, return_sequences=True))
        model.add(GRU(512, return_sequences=False))       
        
        model.add(Dense(1, activation='linear'))
        
        model.compile(loss='mse',optimizer='adam', metrics=['accuracy'])
        return model",0.4528205395,
1462,part tf idf,"def create_model(shape,num_classes):
    model = Sequential()
    model.add(LSTM(32, input_shape=(None,shape), return_sequences=True))
    model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))
    model.compile(loss=""binary_crossentropy"", optimizer='adam', metrics=['mse'])
    return model",0.4457249045,
1462,part tf idf,"def set_input_layer(review):
    """""" Modify the global layer_0 to represent the vector form of review.
    The element at a given index of layer_0 should represent
    how many times the given word occurs in the review.
    Args:
        review(string) - the string of the review
    Returns:
        None
    """"""
    global input_layer
    #resetting previous state 
    input_layer *= 0
    for word in review.split("" ""):
        input_layer[0][vocabulary_lookup_table[word]] += 1",0.4440874755,
1462,part tf idf,"def transformer(enc_res):
    
    res1 = resnet_block(enc_res, num_feats=num_gen_filters*4)
    res2 = resnet_block(res1, num_feats=num_gen_filters*4)
    res3 = resnet_block(res2, num_feats=num_gen_filters*4)
    res4 = resnet_block(res3, num_feats=num_gen_filters*4)
    res5 = resnet_block(res4, num_feats=num_gen_filters*4)
    res6 = resnet_block(res5, num_feats=num_gen_filters*4)
    # res6.shape = (64,64,256)
    
    return res6",0.4428353906,
1462,part tf idf,"def tfidf_q(tqv):
    tqv_tfidf = []
    for i, t in enumerate(voc):
        # TODO compute TFIDF
        tf =
        # tqv[i] holds the raw frequency for term t
        tfidf = tqv[i]
        tqv_tfidf.append(tfidf)
    return tqv_tfidf",0.4408521056,
1462,part tf idf,"def read_one_from_TFRecords(filename, data_index):
    data_iterator = tf.python_io.tf_record_iterator(filename)
    i=0
    for our_data in data_iterator:
        example = tf.train.Example()
        example.ParseFromString(our_data)
        image_string = (example.features.feature['image']
                                      .bytes_list
                                      .value[0])
        masks_string = (example.features.feature['masks']
                                      .bytes_list
                                      .value[0])
        returned_image = np.fromstring(image_string, dtype=np.float64).reshape(512,512,1)
        returned_masks = np.fromstring(masks_string, dtype=np.float64).reshape(512,512,2)
        if i==data_index:
            break
        i = i + 1
    return returned_image, returned_masks",0.4349503517,
1462,part tf idf,"def RNN_model(input_shape, output_size):
    """"""
    RNN Model
    :param input_shape: trigram feature set
    :param output_size: Length of output sequence (21 languages)
    :return: Keras model built, but not trained
    """"""
    model = Sequential()
    model.add(Embedding(input_shape[0], 128, input_length=input_shape[1]))
    model.add(LSTM(128))
    model.add(Dropout(0.2))
    model.add(Dense(output_size[1], activation='softmax'))
    
    return model

def LR_model(input_shape, output_size):
    """"""
    Logistic Regression Model
    param input_shape: trigram feature set
    :param output_size: Length of output sequence (21 languages)
    :return: Keras model built, but not trained
    """"""
    model = Sequential()
    model.add(Dense(256, input_dim=input_shape[1]))
    model.add(Dense(128))
    model.add(Dense(output_size[1], activation='softmax')) 
    
    return model

print(""Models Loaded."")",0.4326090217,
1462,part tf idf,"def build_model(layers):
    d = 0.2
    model = Sequential()
    
    # now model.output_shape == (None, 128)
    model.add(LSTM(128, input_shape=(layers[1], layers[0]), return_sequences=True))
    model.add(Dropout(d))
    
    # for subsequent layers, no need to specify the input size:
    model.add(LSTM(64, return_sequences=False))
    model.add(Dropout(d))
    
    # fully connected layer
    model.add(Dense(16,kernel_initializer='uniform',activation='relu'))        
    model.add(Dense(1,kernel_initializer='uniform',activation='linear'))
    model.compile(loss='mse',optimizer='adam',metrics=['accuracy'])
    return model",0.4316019714,
1462,part tf idf,"def build_model(layers):
        d = 0.2
        model = Sequential()
        model.add(LSTM(128, input_shape=(layers[1], layers[0]), return_sequences=True))
        model.add(Dropout(d))
        model.add(LSTM(64, input_shape=(layers[1], layers[0]), return_sequences=False))
        model.add(Dropout(d))
        model.add(Dense(16,kernel_initializer='uniform',activation='tanh'))        
        model.add(Dense(1,kernel_initializer='uniform',activation='tanh'))
        model.compile(loss='mse',optimizer='adam',metrics=['accuracy'])
        return model",0.4316019714,
2290,task orders over time,"def on_quote(ts, q):
    if q.status == QuoteStatus.Trade:
        ts[""short_ma""].add(q.last.px)
        ts[""long_ma""].add(q.last.px)
        algo_logic(ts)
    elif q.status == QuoteStatus.Close:
        close_position(ts)
    check_pnl(ts)
    
ts = run()
ts[""account""].get_stats()",0.5384341478,
2290,task orders over time,"def handle_data(context, data):
    '''
    Called when a market event occurs for any of the algorithm's 
    securities. 

    Parameters

    data: A dictionary keyed by security id containing the current 
          state of the securities in the algo's universe.

    context: The same context object from the initialize function.
             Stores the up to date portfolio as well as any state 
             variables defined.

    Returns None
    '''
    # Allow history to accumulate 100 days of prices before trading
    # and rebalance every day thereafter.
    context.tick += 1
    if context.tick < 100:
        return
    # Get rolling window of past prices and compute returns
    
    #rebalance only every i days
    i = 60
    if (context.tick % i) != 0:
        return
    
    prices = history(100, '1d', 'price').dropna()
    returns = prices.pct_change().dropna()
    try:
        # Perform Markowitz-style portfolio optimization
        weights, _, _ = optimal_portfolio(returns.T)
        weights = np.around(weights)
        # Rebalance portfolio accordingly
        for stock, weight in zip(prices.columns, weights):
            order_target_percent(stock, weight)
    except ValueError as e:
        # Sometimes this error is thrown
        # ValueError: Rank(A) < p or Rank([P; A; G]) < n
        pass",0.5317507386,
2290,task orders over time,"def feat_engineering(df, features):
    # Remove outliers
    df = df.query('income < %f' % income_threshold)
    
    # Handle missing values
    min_last_payment, min_end_last_loan  = df.last_payment.min(), df.end_last_loan.min()
    df = df.fillna( {'last_payment':min_last_payment, 'end_last_loan':min_end_last_loan} )
    
    # Create new features
    df['delta_pay']  = ( df.last_payment.max()  - df.last_payment  ).astype('timedelta64[D]')
    df['delta_loan'] = ( df.end_last_loan.max() - df.end_last_loan ).astype('timedelta64[D]')
    
    # Return columns with good features only
    return df.loc[:, features]",0.5255282521,
2290,task orders over time,"def sanity_check(left_line, right_line):
    # Are the two polynomials an appropriate distance apart based on the known width of a highway lane?
    x_distances = abs(left_line.recent_xfitted[-1] - right_line.recent_xfitted[-1])
    x_min = np.min(x_distances)
    x_max = np.max(x_distances)
    threshold = (550, 750)
    if (x_min < threshold[0] or x_min > threshold[1] or x_max < threshold[0] or x_max > threshold[1]) :
        return False
    
    # Do the two polynomials have same or similar curvature? THIS IS NOT WORKING CORRECTLY
#     if abs(left_line.radius_of_curvature - right_line.radius_of_curvature) > 200:
#         print(""Curve diff: "" + str(abs(left_line.radius_of_curvature - right_line.radius_of_curvature)))
#         return False
  
    return True

def set_lines_detected(left_line, right_line):
    left_line.best_xfitted = left_line.recent_xfitted[-1]
    right_line.best_xfitted = right_line.recent_xfitted[-1]

    left_line.best_fit = left_line.current_fit
    right_line.best_fit = right_line.current_fit

    left_line.detected = True
    right_line.detected = True",0.5248577595,
2290,task orders over time,"for col in ['GarageType', 'GarageFinish']:
    order = list(categorical.groupby(col).SalePrice.median().sort_values(ascending=False).index.values)
    plotCategorical(col, order)
    plt.show()",0.5242738128,
2290,task orders over time,"# Limits the function ma_cross_over_handling to occur daily, 
# 1 hour after the market opens.
def initialize(context):
    context.aapl = sid(24)  # Apple share prices
    schedule_function(ma_crossover_handling, date_rules.every_day(), 
                      time_rules.market_open(hours=1)) 

# Buy/ sell when moving averages have crossed 
def ma_crossover_handling(context, data):
    hist = data.history(context.aapl, 'price', 50, '1d')
    sma_50 = hist.mean()  # 50 day moving average
    sma_20 = hist[-20:].mean()  # 20 day MA
    
    open_orders = get_open_orders()
        
    if sma_20 > sma_50:  # If the 20 day moving average has crossed over the 50 day moving average
        if context.aapl not in open_orders:
            order_target_percent(context.aapl, 1.0)  # Want 100% (1.0) of portfolio to be Apple (long order)
    elif sma_50 > sma_20:
        if context.aapl not in open_orders:
            order_target_percent(context.aapl, -1.0)  # Short order on Apple
        
    record(leverage=context.account.leverage)  # Record leverage",0.5198773146,
2290,task orders over time,"def model2(params,p0,numt):
    # ...
    while tick < numt: # loop over ticks
        while np.abs(price[i]-price[j]) < params['L']: # transaction criterion Eq.(L1)
            #...
        #...
        dbar = (1.0 - params['e0'])*dbar + np.random.choice([0.01,-0.01], size=params['N']) # update trend parameter
        #...",0.5158575177,
2290,task orders over time,"def get_topic_wc(row):
    topic_pc = row['percent'] / 100
    total_wc = int(df_raw.loc[df_raw.delivered == row['delivered'], 'words'])
    return topic_pc * total_wc",0.5139098167,
2290,task orders over time,"async def worker(out, stop, tag):
    i = 0
    while True:
        i += 1
        await asyncio.sleep(0.1)
        result, c = await ac.select(stop, (out, '%s-%s' % (tag, i)), priority=True)
        if c is stop:
            print('%s stopped' % tag)
            break
            
async def consumer(c, stop):
    while True:
        result, c = await ac.select(stop, c, priority=True)
        if c is stop:
            print('consumer stopped')
            break
        else:
            print('received', result)

async def main():
    c = ac.Chan()
    stop = ac.Chan()
    for i in range(3):
        ac.go(worker(c, stop, 'worker%s' % i))
    ac.go(consumer(c, stop))
    await asyncio.sleep(0.6)
    stop.close()
    await asyncio.sleep(0.2)

ac.run(main())",0.5138349533,
2290,task orders over time,"def add_to_cart(item):
    cart.append(item)
    total += item.price",0.5137603283,
930,inner product of c and d,"def khatri(A, B, C):
    return np.einsum('il,jl,kl->ijk',A,B,C)",0.5568515062,
930,inner product of c and d,"def rotate_z_mesh(x, y, alpha):
    return np.einsum('ji, mni -> jmn', Rot(alpha), np.dstack([x, y]))",0.5455530882,
930,inner product of c and d,"def bernsteinPoly (n,k,x):
    return sc.binom(n,k)*(1-x)**(n-k)*x**k",0.542889595,
930,inner product of c and d,"def return_args_and_accumulate_squares_and_ints(i, cur_int_sum, cur_square_sum):
    return i, cur_int_sum, cur_square_sum, i + cur_int_sum, i ** 2 + cur_square_sum",0.5390461683,
930,inner product of c and d,"def alternateList(item1, item2, repeats):
    alternate = [item1, item2]
    altRepeat = alternate * repeats
    return altRepeat",0.5368795395,
930,inner product of c and d,"def number_of_sequences(random_length, needle_length, needle_count):
    return scipy.misc.comb(random_length - needle_count * needle_length + needle_count, needle_count) \
            * 4 ** (random_length - needle_length * needle_count) \
            * (-1) ** (needle_count + 1)",0.5351086855,
930,inner product of c and d,"from numpy.random import uniform

def matprod(am, bm) :
    a1, a2 = np.shape(am)
    b1, b2 = np.shape(bm)
    
    assert a2 == b1, ""matrix size mismatch""
    
    vm = []
    for a in am :
        v = []
        for b in bm.T :
            v.append(sum(a*b))
        vm.append(v)
        
    return np.array(vm)
            
am = uniform(size=[5, 4])
bm = uniform(size=[4, 8])

# test if the matrix production is correct
print ""allclose = "", np.allclose(matprod(am, bm), am.dot(bm))

ns = [10, 20, 40, 80, 160, 320]
timing = []

for n in ns:
    am = uniform(size=[n, n])
    bm = uniform(size=[n, n])
    r1 = %timeit -n 3 -o matprod(am, bm);
    r2 = %timeit -n 3 -o am.dot(bm);
    timing.append((r1.best, r2.best))
    
timing = np.array(timing)",0.534948349,
930,inner product of c and d,"def dp(v, sigma):
    result = v[0]*sigma[0]
    for i in range(1, 3):
        result += v[i]*sigma[i]
    return result",0.5344411135,
930,inner product of c and d,"def ribbon(a,b,c):
    return min((a+a+b+b, a+a+c+c, b+b+c+c)) + a*b*c
print (ribbon(2,3,4))",0.5329360962,
930,inner product of c and d,"def alternateList(item1, item2, repeats):
    alternate = [item1, item2]
    altRepeat = alternate * repeats
    return alternate, altRepeat

pair, rpt = alternateList(77, 99, 5)

print(pair)",0.5327028036,
592,exploring the chinook database with sql,"assert len(connection.execute(q4).fetchall()) == 9
names = ['Sergey Brin', 'Danah Boyd', 'Susan Wojcicki', 'Mike Olson', 'Cheryl Sandberg', 'Bill Gates', 'Marissa Meyer', 'Hillary Mason', 'Mark Zuckerberg']
assert [name for name, grade in connection.execute(q4).fetchall()] == names
grades = [93.5714285714286, 91.4285714285714, 89.8571428571429, 89.0, 88.8571428571429, 87.1428571428571, 87.0, 86.8571428571429, 86.1428571428571]
assert np.allclose([grade for name, grade in connection.execute(q4).fetchall()], grades)",0.4149225354,
592,exploring the chinook database with sql,"import pandas as pd
import sqlite3
conn = sqlite3.connect(""chinook.db"")
cursor = conn.cursor()
q = ""SELECT * FROM sqlite_master WHERE type='table';""
cursor.execute(q).fetchall()b",0.4131957889,
592,exploring the chinook database with sql,"assert len(connection.execute(q1).fetchall()) == 9
assert connection.execute(q1).fetchall()[0] == (0, 'Sergey Brin', 2.8, 40, 'CS', 'M')",0.4124696553,
592,exploring the chinook database with sql,"con = mdb.connect('localhost', 'testuser', 'test623', 'testdb')
    
with con:    

    cur = con.cursor()
        
    #This query updates the name of an author where the row Id =4
    #The statement uses ANSI printf format to replace %s placeholders
    cur.execute(""UPDATE Writers SET Name = %s WHERE Id = %s"", 
        (""Guy de Maupasant"", ""4""))        
    
    print (""Number of rows updated:"",  cur.rowcount)",0.4122039676,
592,exploring the chinook database with sql,"pd.read_sql(""""""SELECT pt_id, 
       count(SBP) as TheCount, 
       min(SBP) as MinSBP, 
       max (temper) as HighTemp
FROM vitals
GROUP BY pt_id
    HAVING count(SBP)>4
limit 20;"""""", conn).head(20)",0.4120440483,
592,exploring the chinook database with sql,"pd.read_sql(""""""Select d.pt_id, ptname
    From demog d, vitals v
    Where d.pt_id=v.pt_id
    and temper > (Select avg(temper) from vitals);"""""", conn).head(20)",0.4120440483,
592,exploring the chinook database with sql,"pd.read_sql(""""""Select gender from demog;"""""", conn).head(20)",0.4120440483,
592,exploring the chinook database with sql,"pd.read_sql(""""""select 
    pt_id, 
    temper 
from vitals
order by cast(pt_id as integer), temper desc
limit 20;"""""", conn).head(20)",0.4120440483,
592,exploring the chinook database with sql,"pd.read_sql(""""""Select pt_id, temper from vitals
    order by pt_id, temper desc
    limit 20;"""""", conn).head(20)",0.4120440483,
592,exploring the chinook database with sql,"pd.read_sql(""""""Select count(*) from demog where gender is ''"""""", conn).head(20)",0.4120440483,
2088,step how many items were orderd in total?,"def assign_pct_of_house(df):
    return df.house_seats.div(df.house_seats.sum()).mul(100).round(2)",0.4952420294,
2088,step how many items were orderd in total?,"def utility_matrix_fullness(reviews):
    return len(reviews) / float(len(reviews.business_id.unique()) * len(reviews.user_id.unique()))",0.4924312234,
2088,step how many items were orderd in total?,"def find_missing_values_columns(df):
    nrow = df.count()
    for v in df.columns:
        summary_df = df.select(v).describe()
        v_count = int(summary_df.collect()[0][v])
        if v_count < nrow:
            missing_percentage = (1 - v_count/nrow) * 100
            print(""Total observations: "" + str(nrow) + ""\n""
                 ""Total observations of "" + v + "": "" + str(v_count) + ""\n""
                 ""Percentage of missing values: "" + str(missing_percentage) + ""%"" + ""\n""
                 ""----------------------------"")",0.4921733141,
2088,step how many items were orderd in total?,"def identify_topic(row):
    scores = row.order(ascending=False)
    if scores[0] - scores[1] >= 1.0:
        label = scores.index[0]
        # remove _score
        return label[:-6]
    else:
        return np.NaN

score_cols = ['taste_score', 'palate_score', 'aroma_score', 'appearance_score']
beer['topic'] = beer[score_cols].apply(identify_topic, axis=1)",0.4891503453,
2088,step how many items were orderd in total?,"# Percentage of re-orders

def reorders(df, name):
    reorder = df.reordered.sum() / df.shape[0]
    print (""Percentage of reorders in {} set: {}"".format(name,reorder))
    
reorders(df_order_products__prior, 'train')
reorders(df_order_products__train, 'test')",0.4847080112,
2088,step how many items were orderd in total?,"def calc_order_numbers_to_sum((last_order_n,order_number)):
    order_numbers_to_sum = range(last_order_n+1, order_number+1)
    return order_numbers_to_sum",0.481184274,
2088,step how many items were orderd in total?,"def entropy(examples):
    total_entropy = 0
    for class_label, class_count in examples.groupby('label').size().iteritems():
        if class_count > 0:
            class_prob = float(class_count) / total_count
            total_entropy += -(class_prob * math.log(class_prob, 2))
    return total_entropy

entropy(weather[:]), entropy(weather[:10])",0.4809907079,
2088,step how many items were orderd in total?,"# objective function
def Obj_fn(m):
    return sum((m.priceBuy[i]*m.posNetLoad[i]) + (m.priceSell[i]*m.negNetLoad[i]) for i in m.Time)  
m.total_cost = en.Objective(rule=Obj_fn,sense=en.minimize)",0.4800941646,
2088,step how many items were orderd in total?,"# Passion Score
def passion_score(df):
    if len(df.customer_id.unique()) == 0:
        passion_score = 0
    else:
        passion_score = round(len(df)/len(df.customer_id.unique()),2)
    return passion_score",0.4783091545,
2088,step how many items were orderd in total?,"def mae_item_based(PredMat):
    mae = 0.
    for user in PredMat.transpose().index:
        predicts = PredMat.transpose().loc[user][PredMat.transpose().loc[user].notnull()]
        df_values = df.loc[user][predicts.index]
        mae += np.mean(abs(predicts - df_values))
    return mae / len (PredMat.transpose().index)",0.4766489863,
1421,part implement the tanh activation function,"%%add_to EKF

def update(self, z):
    
    H = self.h_prime()
    
    S = np.matmul(np.matmul(H,self.sigma_bar),np.transpose(H)) + self.r_t     
    
    K = np.matmul(np.matmul(self.sigma_bar,np.transpose(H)),np.linalg.inv(S))
    
    mu = self.mu_bar + np.matmul(K,(z-self.h(self.mu_bar[:-1])))
    
    sigma = np.matmul((np.identity(7) - np.matmul(K,H)),self.sigma_bar)
    
    self.mu=mu
    self.sigma=sigma
    
    return mu, sigma",0.4634786546,
1421,part implement the tanh activation function,"def blr(fee, wf):
    coef = fee.freq_LHPFd*np.pi
    signal_out_cf = FE.signal_clean(fee, wf.q, -1)
    signal_r, _ = deconv_simple(signal_out_cf, coef)
    return Waveform(wf.t, signal_r)",0.462154597,
1421,part implement the tanh activation function,"def fixPhase(data):
    l=len(data.frequency.unique())
    ld=len(data.frequency)
    for i in xrange(ld/l):
        new = np.unwrap(data[i*l:(i+1)*l].phase)
        data[i*l:(i+1)*l].phase = new
    return data

data = fixPhase(data",0.4591362476,
1421,part implement the tanh activation function,"def GaA_C_update(C, mean, sample, pars):
    # Cf. p.10 of the MATLAB code of Mueller
    l_C, max_cond = pars.l_C, pars.max_cond
    delta = mean - sample
    C_next = (1 - l_C)*C + l_C*np.outer(delta, delta)
    if la.cond(C_next) <= max_cond:
        return C_next
    else: 
        return C",0.4550587833,
1421,part implement the tanh activation function,"def zero_outer_planes_and_equal_times(g2_tau):

    beta = g2_tau.mesh.components[0].beta
    N = len(g2_tau.mesh.components[0]) - 1
    
    for idxs, (t1, t2, t3) in enumerate_tau3(g2_tau):
        if t1.linear_index == t2.linear_index or \
           t2.linear_index == t3.linear_index or \
           t3.linear_index == t1.linear_index or \
           t1.linear_index == 0 or \
           t2.linear_index == 0 or \
           t3.linear_index == 0 or \
           t1.linear_index == N or \
           t2.linear_index == N or \
           t3.linear_index == N :
            #print idxs, t1.linear_index, t2.linear_index, t3.linear_index
            g2_tau.data[tuple(idxs)][:] = 0.0",0.4544692338,
1421,part implement the tanh activation function,"def nunlnu_error(Tkin):
    return 1+np.exp(-constants.h * freq / (constants.k_B * Tkin))",0.4525016844,
1421,part implement the tanh activation function,"def filter_spectrum(spectrum):
    spectrum.hs /= spectrum.fs
    spectrum.hs[0] = 0",0.4517674744,
1421,part implement the tanh activation function,"def noise_effect(noise):
    m.het_Gauss.variance[:1] = noise
    m.het_Gauss.variance.fix()
    m.optimize()
 
    m.plot_f() 
    pb.errorbar(X.flatten(),Y.flatten(),yerr=np.array(m.likelihood.flattened_parameters).flatten(),fmt=None,ecolor='r',zorder=1)        
    pb.plot(X[1:],Y[1:],'kx',mew=1.5)
    pb.plot(X[:1],Y[:1],'ko',mew=.5)
    pb.grid()",0.4497055411,
1421,part implement the tanh activation function,"def plot( ngen, myvar ):
    t = plot.t
    nd = plot.nd
    nh = plot.nh
    cnt = 0
    while cnt < ngen :
        cnt += 1
        D, T = myvar()   # Here we generate the random variables for an individual
        if( T==1 ):      # Check if the test result was positive 
            t += 1.0     # Conter for number of positive test results
            if( D==1 ):    # Check if the person is sick
                nd += 1.0  # Counter for the number of sick people with positive test results  
            else:          # We will do this bit if the person is healty
                nh += 1.0  # Counter for the number of healty people with positive test results
        if( t>0.0 ) :      # In your modified code this if block will ensure that you don't divide by zero
            yield cnt, nd, nh
        else :             
            yield cnt, 0, 0

dynamicplot( 1000, plot, genperson )  # Note that here you need to change the genperson to the name of your function",0.4476345181,
1421,part implement the tanh activation function,"def squared_hinge(x, y, beta, l):
    loss = 1 - y * (x.T @ beta)
    loss = np.mean(np.power(loss * (loss > 0), 2))
    penalty = l * (beta.T @ beta)
    return loss + penalty",0.4450458884,
2282,task find the angle of attack for a given lift coefficient,"def heuristic_cost_estimate(self, node):
    """""" Returns the heuristic cost estimate of a node """"""
    # TODO: Return the heuristic cost estimate of a node
    return self.distance(node, self.goal)",0.447556138,
2282,task find the angle of attack for a given lift coefficient,"def heuristic_cost_estimate(self, node):
    """""" Returns the heuristic cost estimate of a node """"""
    # TODO: Return the heuristic cost estimate of a node
    return self.distance(node,self.goal)",0.447556138,
2282,task find the angle of attack for a given lift coefficient,"def cross_entropy_error(y, z_x):
    z_x = np.clip(z_x, 1e-32, 0.9999999999999999)
    return -(np.sum(np.multiply(y, np.log(z_x)) + np.multiply((1.0 - y), np.log(1.0 - z_x)), 1))",0.4474210739,
2282,task find the angle of attack for a given lift coefficient,"def loss(weights, biases):
    error = prediction(training_inputs, weights, biases) - training_outputs
    return tf.reduce_mean(tf.square(error))",0.4421554208,
2282,task find the angle of attack for a given lift coefficient,"def GetTS(catalog_,events_, ns_seed = 0.5):
    N = float(len(events_))

    maxLLH = -sp.optimize.minimize(lambda ns: -LLH(1.0*ns,catalog_,events_),
                                                   np.array([ns_seed]),method='L-BFGS-B',
                                                   bounds=np.array([[0.,N]]), jac=False).fun
    return (maxLLH-LLH(0.,catalog_,events_))",0.4410880208,
2282,task find the angle of attack for a given lift coefficient,"def heuristic_cost_estimate(self, node):
    """""" Returns the heuristic cost estimate of a node """"""
    # TODO: Return the heuristic cost estimate of a node
    
    return distance(self,node,self.goal)",0.4404906929,
2282,task find the angle of attack for a given lift coefficient,"def softmax_mask(values, mask):
    # adds big negative to masked values
    INF = 1e30
    return -INF * (1 - tf.cast(mask, tf.float32)) + values",0.4397572577,
2282,task find the angle of attack for a given lift coefficient,"def compile_model(model, lr = 0.001):
    model.compile(optimizer = Adam(lr = lr),
                  loss      = 'categorical_crossentropy',
                  metrics   = ['accuracy'])

def finetune_model(model, batches):
    model.pop()
    for layer in model.layers:
        layer.trainable = False # We do not want to change the weights of the core of the network.
    model.add(Dense(batches.nb_class, activation = 'softmax'))
    compile_model(model)",0.4378216863,
2282,task find the angle of attack for a given lift coefficient,"def Run_compare(k):        # K = how many times to split
    
    taxi_reduced, label_reduced = shrinkDataSet (taxi_new,label, k ,0.5)    # split k times, into half
    print('Original shape: ',taxi_new.shape)
    print('After shape: ',taxi_reduced.shape,'\n')
    
    Regressions(taxi_reduced,label_reduced)",0.4366771579,
2282,task find the angle of attack for a given lift coefficient,"def my_rmsprop(parameters, gradients):
    rho = 0.999
    lr = 0.01
    # We use the following accumulator to store the moving average of every squared gradient
    accumulators = [C.constant(1e-6, shape=p.shape, dtype=p.dtype) for p in parameters]
    update_funcs = []
    for p, g, a in zip(parameters, gradients, accumulators):
        # We declare that `a` will be replaced by an exponential moving average of squared gradients
        # The return value is the expression rho * a + (1-rho) * g * g 
        accum_new = C.assign(a, rho * a + (1-rho) * g * g)
        # This is the rmsprop update. 
        # We need to use accum_new to create a dependency on the assign statement above. 
        # This way, when we run this network both assigns happen.
        update_funcs.append(C.assign(p, p - lr * g / C.sqrt(accum_new)))
    return C.combine(update_funcs)

my_learner = C.universal(my_rmsprop, z.parameters)
print(inspect_update(my_learner, 10, 2)[0][0])",0.4322742522,
481,discriminator tests,"def evaluate_algorithm3(my_dataset, features_list):
    
    print('Decision Tree:')
    clf = DecisionTreeClassifier(random_state=42)
    tester.test_classifier(clf, my_dataset, features_list)
    print('')

    
    print('Ada Boost:')
    clf=AdaBoostClassifier(random_state=42)
    tester.test_classifier(clf, my_dataset, features_list)",0.4873866439,
481,discriminator tests,"# Now test the performance of a predictor based on a subset of features
def test_model(mu, sigma, pi, features, tx, ty):
    ###
    ### Your code goes here
    ###
   
    k=3 #Labels 1,2,...,k
    score = np.zeros((len(ty),k+1))
    for i in range(0,len(ty)): #i is every test point
        for j in range(1,k+1): #j is label
            score[i,j]=np.log(pi[j]) + \
            multivariate_normal.logpdf(tx[i,features], mean=mu[j,features], cov=sigma[j,features,features])
    predictions = np.argmax(score[:,1:k+1], axis=1) + 1
    
    errors = np.sum(predictions != ty)
    print ""Test error using feature(s): "",
    for f in features:
        print ""'"" + featurenames[f] + ""'"" + "" "",
    print
    print ""Errors: "" + str(errors) + ""/"" + str(len(ty))# Now test the performance of a predictor based on a subset of features",0.4813066125,
481,discriminator tests,"# To write a function which will run tester function on all 4 algorithms of my interest, so I can check the performance.
import tester

def evaluate_algorithm3(my_dataset, features_list):
    print('Logistic Regression')
    clf = LogisticRegression(random_state=42)
    tester.test_classifier(clf, my_dataset, features_list)
    print('')
    
    print('Decision Tree:')
    clf = DecisionTreeClassifier(random_state=42)
    tester.test_classifier(clf, my_dataset, features_list)
    print('')
    
    print('Random Forest:')
    clf=RandomForestClassifier(random_state=42)
    tester.test_classifier(clf, my_dataset, features_list)
    print('')
    
    print('Ada Boost:')
    clf=AdaBoostClassifier(random_state=42)
    tester.test_classifier(clf, my_dataset, features_list)",0.4725916982,
481,discriminator tests,"## Linear Regression Function ##

def LinRegEval(X, y):
    '''Returns LinearRegression stats and scatter plot of observed vs predicted values.'''

    # Create train and test sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=43)

    lm = LinearRegression()
    # Fit the regressor to the training data
    lm.fit(X_train, y_train)

    # Predict on the train and test data
    predicted_train = lm.predict(X_train)
    predicted_test = lm.predict(X_test)
    test_score = lm.score(X_test, y_test)
    rmse = np.sqrt(mean_squared_error(y_test, predicted_test)) 
    spearman = spearmanr(y_test, predicted_test)
    pearson = pearsonr(y_test, predicted_test)

    # Compute and print R^2 and RMSE
    print(f'Test data R-2 score: {test_score:>5.3}')
    print(f'Root Mean Squared Error: {rmse:.4}')
    print(f'Estimated intercept coefficient: {lm.intercept_[0]:.4}')
    print(f'Test data Spearman correlation: {spearman[0]:.3}')
    print(f'Test data Pearson correlation: {pearson[0][0]:.3}')

    # scatter plot between predicted days_open (lr.predict(X)) and actuals
    fig, ax = plt.subplots(figsize=(6, 5))
#    sns.regplot(x=lm.predict(X), y=y, fit_reg = True, scatter_kws={'alpha':0.5, 's':60})
    sns.regplot(x=lm.predict(X), y=y['days_open'], fit_reg = True, scatter_kws={'alpha':0.5, 
                                                                                's':60})
                
    plt.xlabel(""Predicted"")
    plt.ylabel(""Observed"")
    plt.title(""Days Open"")
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    plt.show()
    
    # Compute 5-fold cross-validation scores: cv_scores
    cv_scores = -cross_val_score(lm, X, y, cv = 5, scoring='neg_mean_squared_error')

    # Print the 5-fold cross-validation scores
    print(""Cross-Validation Scores: {}"".format(cv_scores, 3))
    print(""Accuracy: %0.2f (+/- %0.2f)"" % (cv_scores.mean(), cv_scores.std() * 2))

    # Make cross validated predictions
    predictions = cross_val_predict(lm, X, y, cv=5)
    accuracy = r2_score(y, predictions)
    print ('Cross-Predicted Accuracy: %0.3f' % (accuracy))
    
    plt.scatter(predicted_train, predicted_train - y_train, c='b', s=40, alpha=.5)
    plt.scatter(predicted_test, predicted_test - y_test, c='g', s=40, alpha=.6) 
    plt.hlines(y=0, xmin=-10, xmax=50)
    
    plt.xlabel(""Residuals"")
    plt.ylabel(""Fitted Values"")
    plt.title(""Days Open"")
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    plt.show()",0.467516005,
481,discriminator tests,"def ld_analysis(data,printoutput,errorplots):
    print('MODELS USING LINEAR DISCRIMINANT ANALYSIS')
    lda = sklearn.discriminant_analysis.LinearDiscriminantAnalysis()
    for pred in list(data):
        if pred != 'Severity':
            buildmodels(lda,data,pred,printoutput,errorplots)
            
ld_analysis(data,True,False)",0.4674322307,
481,discriminator tests,"Utils functions'
def splitData(test_size,cv, numpoints):
    #This function from sklearn takes the length of the data and test size and returns bootstrapped indices 
    #depending on how many boostraps are required
    '''
    :param test_size: size of the test data required (value between 0 and 1).
    :param cv: Number of re-shuffling.
    :param numpoints: Total number of data points.
    :return: indices of the shuffled splits.
    '''
    ss = ShuffleSplit(n=numpoints, n_iter=cv, test_size=test_size, random_state=32)
    return ss

def calAccuracy(pred,ytest):
    '''
    :param pred: vector containing all the predicted classes
    :param ytest: vector containing all the true classes
    :return: accuracy of classification
    '''
    count = 0
    for i,j in izip(pred,ytest):
        if i==j:
            count +=1
    return count/(len(ytest))

def calgaussianprob(value,mean,std):
    '''
    :param value: Point for which the probability is to be found.
    :param mean: mean of distribution.
    :param std: standard deviation of distribution.
    :return: probability of the value to fall in normal distribution with given mean and std.
    '''
    return (np.exp(- np.power((value-mean),2) / (2*np.power(std,2)) )) / (np.sqrt(2*np.power(std,2)*np.pi ) )",0.4668276012,
481,discriminator tests,"def main():
    lines = load_lines(data_path)[1:]
    training_set_lines, validation_set_lines = train_test_split(lines, test_size=0.2)
    
    nb_training = len(training_set_lines)*6
    nb_validation = len(validation_set_lines)*6

    training_images, steering_angles = get_data_without_generator(data_path, lines[0:500])
    return (training_images, steering_angles)
data_path = ""data-from-udacity""
#main()",0.4667529464,
481,discriminator tests,"def classifier(clf, x_train, y_train, x_test, y_test):
    start = time.time()
    clf.fit(x_train, y_train)
    y_pred_train = clf.predict(x_train)
    y_pred_test = clf.predict(x_test)
    print'Training score : ', accuracy_score(y_train, y_pred_train)
    print'Testing score : ', accuracy_score(y_test, y_pred_test)
    print'Time in %fs'% (time.time()-start)
    
    return y_pred_train, y_pred_test",0.4652761817,
481,discriminator tests,"def models_test(clf, title, name):
    print clf
    print """"
    # make predictions
    expected = yt
    predicted = clf.predict(Xt)
    # summarize the fit of the model
    score = clf.score(Xt, yt)
    print ""Classification score using test set: {}\n"".format(str(score))
    print""=""*66
    print """"
    # summarize the fit of the model
    print(metrics.classification_report(expected, predicted))
    print""=""*66
    # print confusion matrix
    cm = metrics.confusion_matrix(expected, predicted)
    target_names = ['bad', 'good']
    fig, ax = plt.subplots()
    sns_cm = sns.heatmap(cm, annot=True, fmt='', xticklabels=target_names, yticklabels=target_names , ax=ax)
    title = ""Apple Tweets Test Dataset - "" + title
    ax.set_title(title, y=1.08, fontdict=font)
    #fig.tight_layout()
    #fig.savefig(name, bbox_inches='tight')
    fig.show()",0.4636449814,
481,discriminator tests,"def prediction(model, config, dataholder):
    """"""
    Generate some predictions using a batch from the test dataset

    :type model: LogisticRegression
    :type config: Config
    :type dataholder: DataHolder
    :rtype images: np array
    :rtype prediction: np array
    :rtype ground_truth: np array
    """"""
    num_features = config.num_features
    test_data = dataholder.test_dataset
    test_data = test_data.reshape((test_data.shape[0], num_features))
    test_labels = dataholder.test_labels
    batch_size = config.batch_size
    img_size = config.image_size
    with tf.Session(graph=model.graph) as sess:
        # We load the saved variables
        model.saver.restore(sess=sess, save_path=model.save_path)
        X_batch, Y_batch = get_batch(test_data,
                                     test_labels,
                                     batch_size)
        feed_dict = {model.X: X_batch, model.Y: Y_batch}
        acc, predictions = sess.run([model.acc,
                                  model.prediction],
                                  feed_dict=feed_dict)
        print('Test minibatch acc {:.2f}%'.format(acc*100))
    ground_truth = np.argmax(Y_batch, axis=1)
    images = X_batch.reshape((X_batch.shape[0], img_size, img_size))
    return images, predictions, ground_truth",0.462777555,
84,b regularization as we knew it from,"def neuron_x(x):
    a1 = sig(np.dot(x, W[0]))
    return np.dot(a1, W[1])",0.4715550542,
84,b regularization as we knew it from,"pia_hibo = wrl.atten.correct_attenuation_hb(
    data, 
    coefficients = dict(a=8.e-5, b=0.731, gate_length=1.0),
    mode=""warn"", 
    thrs=59.)",0.4604257643,
84,b regularization as we knew it from,"def transcoder(x):  
    output = tf.matmul(x, W1) + b1
    
    return output",0.4576371908,
84,b regularization as we knew it from,"def simple_network(x):
    y_pred = torch.matmul(x,w)+b
    return y_pred",0.4576371908,
84,b regularization as we knew it from,"def findslope(df):
    x = df['epoch']
    y = df['val']
    x = sm.add_constant(x)
    model = sm.OLS(y, x)
    results = model.fit(method='qr')
    coeffs = list(results.params)
    return coeffs[1]",0.4565612078,
84,b regularization as we knew it from,"pia_hibo = wrl.atten.correctAttenuationHB(
    data, 
    coefficients = dict(a=8.e-5, b=0.731, gate_length=1.0),
    mode=""warn"", 
    thrs=59.)",0.4547771811,
84,b regularization as we knew it from,"def deprocess_net_image(image):
    image = image.copy()              # don't modify destructively
    image = image.transpose(1, 2, 0)  # CHW -> HWC
    image += [104, 117, 123]          # (approximately) undo mean subtractio
    return image
plt.imshow(net.blobs['data'].data[0,...].transpose(1,2,0))",0.4522396922,
84,b regularization as we knew it from,"################################################
# Density matrix for a thermal state           #
################################################
def thermalState(beta):
    r = np.diag(np.ones(4))*0.25
    r = r + 0.125 * beta * (np.kron(sz, np.diag(np.ones(2))) + np.kron(np.diag(np.ones(2)), sz))
    return r",0.4522337914,
84,b regularization as we knew it from,"def memory_allocation(tm1, t):
    with tf.variable_scope(""psi""):
        t[""retention""] = tf.reduce_prod(1 - t[""free_gates""] * tm1[""read_weighting""], reduction_indices=1)

    with tf.variable_scope(""u""):
        t[""usage""] = (tm1[""usage""] + tm1[""write_weighting""] - tm1[""usage""] * tm1[""write_weighting""]) * \
            t[""retention""]
    
    with tf.variable_scope(""phi""):
        # trick to sort in ascending mode:
        # negate the usage vector and select the top k positions,
        # with k equal to all the positions,
        # and then negate back to get the original usage vector with ascending sorting
        t_sorted_usage, t_free = tf.nn.top_k(-1 * t[""usage""], k=num_words)
        t_sorted_usage *= -1

        t[""free""] = t_free
    
    with tf.variable_scope(""a""):
        ## TODO: review this part ##

        t_cumprod = tf.cumprod(t_sorted_usage, axis=0, exclusive=True)
        t_unorder = (1 - t_sorted_usage) * t_cumprod

        t_allocation_weighting = tf.zeros([num_words])

        t_identity = tf.constant(np.identity(num_words, dtype=np.float32))

        for pos, idx in enumerate(tf.unstack(t_free[0])):
            m = tf.squeeze(tf.slice(t_identity, [idx, 0], [1, -1]))
            t_allocation_weighting += m * t_unorder[0, pos]

        t_allocation_weighting = tf.reshape(t_allocation_weighting, [num_words, 1])

        ## -- ##
    
    t[""allocation_weighting""] = t_allocation_weighting",0.4521382749,
84,b regularization as we knew it from,"def guess_author_new(filename):
    nu, xu = measure(filename)
    lnprob = xu * np.log((mu_m_ml+1e-6)/(mu_h_ml+1e-6)) - nu * (mu_m_ml - mu_h_ml)
    return np.sum(lnprob)",0.4468672872,
1120,loading the dataset,"def load_dataset():
    # the data, shuffled and split between train and test sets
    (X_train, y_train), (X_test, y_test) = cifar10.load_data()
    
    # Allocate last 5000 training examples for validation.
    X_train, X_val = X_train[:-5000], X_train[-5000:]
    y_train, y_val = y_train[:-5000], y_train[-5000:]
    
    # convert class vectors to binary class matrices
    y_train = np_utils.to_categorical(y_train, nb_classes)
    y_test = np_utils.to_categorical(y_test, nb_classes)
    y_val = np_utils.to_categorical(y_val, nb_classes)
    
    # preprocess data
    X_train = X_train.astype('float32')
    X_test = X_test.astype('float32')
    X_val = X_val.astype('float32')
    X_train /= 255
    X_test /= 255
    X_val /= 255
    
    print('X_train shape:', X_train.shape)
    print(X_train.shape[0], 'train samples')
    print(X_test.shape[0], 'test samples')
    print(y_train.shape[0], 'training labels')
    print(y_test.shape[0], 'test labels')
    print(X_val.shape[0], 'validation samples')
    print(y_val.shape[0], 'validation labels')

    return X_train, y_train, X_test, y_test, X_val, y_val",0.5335904956,
1120,loading the dataset,"def load_data():
    """"""
    This function loads the MNIST digit data using Keras inbuilt function \
    and returns the train and test set
    """"""
    
    (X_train, y_train), (X_test, y_test) = mnist.load_data()
    return (X_train, y_train), (X_test, y_test)",0.5158867836,
1120,loading the dataset,"def load_mnist():
    (X_train, y_train), (X_test, y_test) = mnist.load_data()
    return X_train, X_test, y_train, y_test",0.5155639052,
1120,loading the dataset,"def PreprocessDataset():
    # Load dataset
    (x_train, y_train), (x_test, y_test) = mnist.load_data()
    # Set numeric type
    x_train = x_train.astype('float32')
    x_test = x_test.astype('float32')
    # Normalize value to [0, 1]
    x_train /= 255
    x_test /= 255
    # Transform lables to one-hot
    y_train = np_utils.to_categorical(y_train, 10)
    y_test = np_utils.to_categorical(y_test, 10)
    # Reshape: here x_train is re-shaped to [channel]  [width]  [height]
    # In other environment, the orders could be different; e.g., [height]  [width]  [channel].
    x_train = x_train.reshape(x_train.shape[0], 1, 28, 28)
    x_test = x_test.reshape(x_test.shape[0], 1, 28, 28)
    return [x_train, x_test, y_train, y_test]

x_train, x_test, y_train, y_test = PreprocessDataset()",0.5117121339,
1120,loading the dataset,"def PreprocessDataset():
    # Load dataset
    (x_train, y_train), (x_test, y_test) = mnist.load_data()
    # Set numeric type
    x_train = x_train.astype('float32')
    x_test = x_test.astype('float32')
    # Normalize value to [0, 1]
    x_train /= 255
    x_test /= 255
    # Transform lables to one-hot
    y_train = np_utils.to_categorical(y_train, 10)
    y_test = np_utils.to_categorical(y_test, 10)
    
    # Reshape: here x_train is re-shaped to [width]  [height] x [channel]
    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)
    x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)
    return [x_train, x_test, y_train, y_test]

x_train, x_test, y_train, y_test = PreprocessDataset()",0.5117121339,
1120,loading the dataset,"def prepare_data():
    (x_train, y_train), (x_test, y_test) = mnist.load_data()
    x_train = x_train.reshape(60000, 784)
    x_test = x_test.reshape(10000, 784)
    x_train = x_train.astype(np.float32)/255.0
    x_test = x_test.astype(np.float32)/255.0
    y_train = np_utils.to_categorical(y_train)
    y_test = np_utils.to_categorical(y_test)
    return x_train, y_train, x_test, y_test",0.5090149641,
1120,loading the dataset,"# Note: For whatever reason, I've experienced a bug with hyperas that
# prevents me from using any kind of comment in either the data() or
# model() function. For this reason I will attempt to describe the 
# code in both of these functions through comments and explanations
# outside of the functions themselves.
def data():
    (X_train, y_train), (X_test, y_test) = mnist.load_data()
    X_train = X_train.reshape(60000, 784)
    X_test = X_test.reshape(10000, 784)
    X_train = X_train.astype('float32')
    X_test = X_test.astype('float32')
    X_train /= 255
    X_test /= 255
    nb_classes = 10
    y_train = to_categorical(y_train, nb_classes)
    y_test = to_categorical(y_test, nb_classes)
    return X_train, y_train, X_test, y_test",0.5029697418,
1120,loading the dataset,"nb_classes = 10
 
def load_dataset():
    (X_train, y_train), (X_test, y_test) = cifar10.load_data()
    print 'X_train shape:', X_train.shape
    print X_train.shape[0], 'train samples'
    print X_test.shape[0], 'test samples'
 
    Y_train = np_utils.to_categorical(y_train, nb_classes)
    Y_test = np_utils.to_categorical(y_test, nb_classes)
 
    X_train = X_train.astype('float32')
    X_test = X_test.astype('float32')
    X_train /= 255
    X_test /= 255
 
    return X_train, Y_train, X_test, Y_test

X_train, y_train, X_test, y_test = load_dataset()",0.496084094,
1120,loading the dataset,"nb_classes = 10
 
def load_dataset():
    # the data, shuffled and split between train and test sets
    (X_train, y_train), (X_test, y_test) = cifar10.load_data()
    print 'X_train shape:', X_train.shape
    print X_train.shape[0], 'train samples'
    print X_test.shape[0], 'test samples'
 
    # convert class vectors to binary class matrices
    Y_train = np_utils.to_categorical(y_train, nb_classes)
    Y_test = np_utils.to_categorical(y_test, nb_classes)
 
    X_train = X_train.astype('float32')
    X_test = X_test.astype('float32')
    X_train /= 255
    X_test /= 255
 
    return X_train, Y_train, X_test, Y_test

X_train, y_train, X_test, y_test = load_dataset()",0.496084094,
1120,loading the dataset,"def load_data():
    (x_train, y_train), (x_test, y_test) = mnist.load_data()
    
    # plot 4 images as gray scale
    plt.subplot(221)
    plt.imshow(x_train[0], cmap=plt.get_cmap('gray'))
    plt.subplot(222)
    plt.imshow(x_train[1], cmap=plt.get_cmap('gray'))
    plt.subplot(223)
    plt.imshow(x_train[2], cmap=plt.get_cmap('gray'))
    plt.subplot(224)
    plt.imshow(x_train[3], cmap=plt.get_cmap('gray'))
    # show the plot
    plt.show()
    
    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)
    x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)
    x_train = x_train.astype('float32')
    x_test = x_test.astype('float32')
    x_train /= 255
    x_test /= 255
    y_train = keras.utils.to_categorical(y_train, 10)
    y_test = keras.utils.to_categorical(y_test, 10)
    return x_train, y_train, x_test, y_test",0.4944310188,
732,generative adversarial networks training lab,"def random_forest(X_train, y_train):
    param_grid = {
        'n_estimators': (10, 50),
        'min_samples_leaf': (2, 3, 10),
        'max_features': (0.2, 0.3),
    }
    model = RandomForestClassifier(random_state = 333, class_weight='balanced', n_jobs = -1)
    model_tuning = GridSearchCV(
        model,
        param_grid=param_grid,
        scoring=make_scorer(hamming_loss, greater_is_better=False),
        cv=3,
        n_jobs=-1,
        verbose=3,
    )
    model_tuning.fit(X_train, y_train)
    return model_tuning",0.5398691893,
732,generative adversarial networks training lab,"class NNClassifier:

    def __init__(self, n_classes, n_features, n_hidden_units=30,
                 l1=0.0, l2=0.0, epochs=500, learning_rate=0.01,
                 n_batches=1, random_seed=None):

        if random_seed:
            np.random.seed(random_seed)
        self.n_classes = n_classes
        self.n_features = n_features
        self.n_hidden_units = n_hidden_units
        self.w1, self.w2 = self._init_weights()
        self.l1 = l1
        self.l2 = l2
        self.epochs = epochs
        self.learning_rate = learning_rate
        self.n_batches = n_batches

    def _init_weights(self):
        w1 = np.random.uniform(-1.0, 1.0, 
                               size=self.n_hidden_units * (self.n_features + 1))
        w1 = w1.reshape(self.n_hidden_units, self.n_features + 1)
        w2 = np.random.uniform(-1.0, 1.0, 
                               size=self.n_classes * (self.n_hidden_units + 1))
        w2 = w2.reshape(self.n_classes, self.n_hidden_units + 1)
        return w1, w2

    def _add_bias_unit(self, X, how='column'):
        if how == 'column':
            X_new = np.ones((X.shape[0], X.shape[1] + 1))
            X_new[:, 1:] = X
        elif how == 'row':
            X_new = np.ones((X.shape[0] + 1, X.shape[1]))
            X_new[1:, :] = X
        return X_new

    def _forward(self, X):
        net_input = self._add_bias_unit(X, how='column')
        net_hidden = self.w1.dot(net_input.T)
        act_hidden = sigmoid(net_hidden)
        act_hidden = self._add_bias_unit(act_hidden, how='row')
        net_out = self.w2.dot(act_hidden)
        act_out = sigmoid(net_out)
        return net_input, net_hidden, act_hidden, net_out, act_out
    
    def _backward(self, net_input, net_hidden, act_hidden, act_out, y):
        sigma3 = act_out - y
        net_hidden = self._add_bias_unit(net_hidden, how='row')
        sigma2 = self.w2.T.dot(sigma3) * sigmoid_prime(net_hidden)
        sigma2 = sigma2[1:, :]
        grad1 = sigma2.dot(net_input)
        grad2 = sigma3.dot(act_hidden.T)
        return grad1, grad2

    def _error(self, y, output):
        L1_term = L1_reg(self.l1, self.w1, self.w2)
        L2_term = L2_reg(self.l2, self.w1, self.w2)
        error = cross_entropy(output, y) + L1_term + L2_term
        return 0.5 * np.mean(error)

    def _backprop_step(self, X, y):
        net_input, net_hidden, act_hidden, net_out, act_out = self._forward(X)
        y = y.T

        grad1, grad2 = self._backward(net_input, net_hidden, act_hidden, act_out, y)

        # regularize
        grad1[:, 1:] += (self.w1[:, 1:] * (self.l1 + self.l2))
        grad2[:, 1:] += (self.w2[:, 1:] * (self.l1 + self.l2))

        error = self._error(y, act_out)
        
        return error, grad1, grad2

    def predict(self, X):
        Xt = X.copy()
        net_input, net_hidden, act_hidden, net_out, act_out = self._forward(Xt)
        return mle(net_out.T)
    
    def predict_proba(self, X):
        Xt = X.copy()
        net_input, net_hidden, act_hidden, net_out, act_out = self._forward(Xt)
        return softmax(act_out.T)

    def fit(self, X, y):
        self.error_ = []
        X_data, y_data = X.copy(), y.copy()
        y_data_enc = one_hot(y_data, self.n_classes)
        for i in range(self.epochs):

            X_mb = np.array_split(X_data, self.n_batches)
            y_mb = np.array_split(y_data_enc, self.n_batches)
            
            epoch_errors = []

            for Xi, yi in zip(X_mb, y_mb):
                
                # update weights
                error, grad1, grad2 = self._backprop_step(Xi, yi)
                epoch_errors.append(error)
                self.w1 -= (self.learning_rate * grad1)
                self.w2 -= (self.learning_rate * grad2)
            self.error_.append(np.mean(epoch_errors))
        return self
    
    def score(self, X, y):
        y_hat = self.predict(X)
        return np.sum(y == y_hat, axis=0) / float(X.shape[0])",0.5355837345,
732,generative adversarial networks training lab,"class NNClassifier:
    
    # These params are pretty standard for a NN classifier, I think TF is very similar
    def __init__(self, n_classes, n_features, n_hidden_units=30,
                 l1=0.0, l2=0.0, epochs=500, learning_rate=0.01,
                 n_batches=1, random_seed=None):
        
        # Recall that a random seed was set at the beginning so this is an optional param
        if random_seed:
            np.random.seed(random_seed)
        self.n_classes = n_classes
        self.n_features = n_features
        self.n_hidden_units = n_hidden_units
        self.w1, self.w2 = self._init_weights()
        self.l1 = l1
        self.l2 = l2
        self.epochs = epochs
        self.learning_rate = learning_rate
        self.n_batches = n_batches
        
    def _init_weights(self):
        # w1 has weights between -1, 1 and shape [hidden_units, n_features+1]
        w1 = np.random.uniform(-1.0, 1.0, 
                               size=self.n_hidden_units * (self.n_features + 1))
        w1 = w1.reshape(self.n_hidden_units, self.n_features + 1)
        
        # w2 has weights between -1, 1 and shape [n_classes, hidden_units+1]
        w2 = np.random.uniform(-1.0, 1.0, 
                               size=self.n_classes * (self.n_hidden_units + 1))
        w2 = w2.reshape(self.n_classes, self.n_hidden_units + 1)
            
        # I believe the second dimension of each is incremented to make space for bias
        return w1, w2
        
    def _add_bias_unit(self, X, how='column'):
        # This adds a bias unit to either the first or second dimension of the input X
        if how == 'column':
            X_new = np.ones((X.shape[0], X.shape[1] + 1))
            X_new[:, 1:] = X
        elif how == 'row':
            X_new = np.ones((X.shape[0] + 1, X.shape[1]))
            X_new[1:, :] = X
        return X_new
        
    # This represents a forward pass
    def _forward(self, X):
        # Add a bias unit to X input
        net_input = self._add_bias_unit(X, how='column')
        # Initial values of hidden layer
        net_hidden = self.w1.dot(net_input.T)
        # Sigmoid is our activation function
        act_hidden = sigmoid(net_hidden)
        # Add a bias unit to the hidden layer (on a row because each row is a node in layer)
        act_hidden = self._add_bias_unit(act_hidden, how='row')
        # Output is second set of weights applied to activated hidden layer
        net_out = self.w2.dot(act_hidden)
        # Simply apply activation to output
        act_out = sigmoid(net_out)
        return net_input, net_hidden, act_hidden, net_out, act_out
    
    # This is the backward pass
    def _backward(self, net_input, net_hidden, act_hidden, act_out, y):
        # Params to this function are results of forward
        # sigma3 can be considered the error
        sigma3 = act_out - y
        # Add a bias unit to net_hidden to match the output dimensions
        net_hidden = self._add_bias_unit(net_hidden, how='row')
        # Use second set of weights first, sigmoid_prime as activation to get second layer error
        sigma2 = self.w2.T.dot(sigma3) * sigmoid_prime(net_hidden)
        # Elimiate error corresponding to bias
        sigma2 = sigma2[1:, :]
        # Apply second layer error to input
        grad1 = sigma2.dot(net_input)
        # Apply initial error to hidden layer
        grad2 = sigma3.dot(act_hidden.T)
        # Grads I assume are gradients derived from the error
        return grad1, grad2
    
    def _error(self, y, output):
        #L1-2 Regularization and cross_entropy loss
        L1_term = L1_reg(self.l1, self.w1, self.w2)
        L2_term = L2_reg(self.l2, self.w1, self.w2)
        error = cross_entropy(output, y) + L1_term + L2_term
        return 0.5 * np.mean(error)
    
    # This is the backpropagation (includes both a forward and backward pass)
    def _backprop_step(self, X, y):
        net_input, net_hidden, act_hidden, net_out, act_out = self._forward(X)
        # y is [n_instances, n_classes], backward assumes the transpose of this
        y = y.T

        grad1, grad2 = self._backward(net_input, net_hidden, act_hidden, act_out, y)

        # regularize
        grad1[:, 1:] += (self.w1[:, 1:] * (self.l1 + self.l2))
        grad2[:, 1:] += (self.w2[:, 1:] * (self.l1 + self.l2))

        error = self._error(y, act_out)
        
        return error, grad1, grad2
    
    def predict(self, X):
        Xt = X.copy()
        net_input, net_hidden, act_hidden, net_out, act_out = self._forward(Xt)
        # mle is defined by the author and does np.argmax(y, axis=1) here to predict
        return mle(net_out.T)
    
    def predict_proba(self, X):
        Xt = X.copy()
        net_input, net_hidden, act_hidden, net_out, act_out = self._forward(Xt)
        # softmax gives a probability distribution adding to one across each class
        return softmax(act_out.T)
    
    def fit(self, X, y):
        self.error_ = []
        X_data, y_data = X.copy(), y.copy()
        # one_hot defined by author, returns matrix with one-hot vectors instead of just the correct label
        y_data_enc = one_hot(y_data, self.n_classes)
        for i in range(self.epochs):

            X_mb = np.array_split(X_data, self.n_batches)
            y_mb = np.array_split(y_data_enc, self.n_batches)
            
            epoch_errors = []

            for Xi, yi in zip(X_mb, y_mb):
                
                # update weights
                error, grad1, grad2 = self._backprop_step(Xi, yi)
                epoch_errors.append(error)
                # weights updated using gradient descent
                self.w1 -= (self.learning_rate * grad1)
                self.w2 -= (self.learning_rate * grad2)
            # self.error_ holds the average error across epochs
            self.error_.append(np.mean(epoch_errors))
        return self
    
    def score(self, X, y):
        y_hat = self.predict(X)
        # Calculates the percentage correct across the input
        return np.sum(y == y_hat, axis=0) / float(X.shape[0])",0.5355837345,
732,generative adversarial networks training lab,"clf_sgd_20, Xtrain_sgd_20, ytrain_sgd_20, Xtest_sgd_20, ytest_sgd_20  = mu.do_classify(
                                                           SGDClassifier(loss='hinge'), 
                                                           {""alpha"": [0.01, 0.1, 1, 10, 100],
                                                            ""n_iter"": [100, 150, 200]}, 
                                                           df_20, pu.x_columns, 'click', 0)",0.5355137587,
732,generative adversarial networks training lab,"clf_sgd_33, Xtrain_sgd_33, ytrain_sgd_33, Xtest_sgd_33, ytest_sgd_33  = mu.do_classify(
                                                           SGDClassifier(loss='hinge'), 
                                                           {""alpha"": [0.01, 0.1, 1, 10, 100],
                                                            ""n_iter"": [100, 120, 150]}, 
                                                           df_33, pu.x_columns, 'click', 0)",0.534755826,
732,generative adversarial networks training lab,"%%time
if VALID == True:
    X_train, X_valid, y_train, y_valid = train_test_split(
        X, y, test_size=0.10, random_state=2018)
        
    # LGBM Dataset Formatting 
    lgtrain = lgb.Dataset(X_train, y_train,
                    feature_name=tfvocab,
                    categorical_feature = categorical)
    lgvalid = lgb.Dataset(X_valid, y_valid,
                    feature_name=tfvocab,
                    categorical_feature = categorical)
    del X, X_train; gc.collect()
    
    # Go Go Go
    lgb_clf = lgb.train(
        lgbm_params,
        lgtrain,
        num_boost_round=20000,
        valid_sets=[lgtrain, lgvalid],
        valid_names=['train','valid'],
        early_stopping_rounds=50,
        verbose_eval=100
    )
    print(""Model Evaluation Stage"")
    print('RMSE:', np.sqrt(metrics.mean_squared_error(y_valid, lgb_clf.predict(X_valid))))
    del X_valid ; gc.collect()

else:
    # LGBM Dataset Formatting 
    lgtrain = lgb.Dataset(X, y,
                    feature_name=tfvocab,
                    categorical_feature = categorical)
    #del X; gc.collect()
    # Go Go Go
    lgb_clf = lgb.train(
        lgbm_params,
        lgtrain,
        num_boost_round=1565,
        verbose_eval=100
    )",0.5341584682,
732,generative adversarial networks training lab,"'' Train and test'''

def train(epoch):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        # print(data.shape) [64,1,28,28]
        data, target = data.cuda(), target.cuda()
        data, target = Variable(data), Variable(target)
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()
        if batch_idx % 100 == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.data[0]))


def test(epoch):
    model.eval()
    test_loss = 0
    correct = 0
    for data, target in test_loader:
        
        data, target = data.cuda(), target.cuda()
        data, target = Variable(data, volatile=True), Variable(target)
        output = model(data)
        test_loss += F.nll_loss(output, target).data[0]
        # get the index of the max log-probability
        pred = output.data.max(1)[1]
        correct += pred.eq(target.data).cpu().sum().double()

    # loss function already averages over batch size
    test_loss /= len(test_loader)
    acccuracy = 100. * correct / len(test_loader.dataset)
    print('\nTest set: Average loss: {:.4f}, '
          'Accuracy: {}/{} ({:.1f}%)\n'.format(test_loss,
                                               correct,
                                               len(test_loader.dataset),
                                               acccuracy))
    return test_loss, acccuracy, data

def test_train(epoch):
    model.eval()
    train_loss = 0
    correct = 0.0
    for data, target in train_loader:
        
        data, target = data.cuda(), target.cuda()
        data, target = Variable(data, volatile=True), Variable(target)
        output = model(data)
        train_loss += F.nll_loss(output, target).data[0]
        # get the index of the max log-probability
        pred = output.data.max(1)[1]
        correct += pred.eq(target.data).cpu().sum().double()

    # loss function already averages over batch size
    train_loss /= len(train_loader)
    acccuracy = 100. * correct / len(train_loader.dataset)
    print('\nTrain set: Average loss: {:.4f}, '
          'Accuracy: {}/{} ({:.1f}%)\n'.format(train_loss,
                                               correct,
                                               len(train_loader.dataset),
                                               acccuracy))
    return train_loss, acccuracy, data",0.5338156223,
732,generative adversarial networks training lab,"gs2 = GridSearchCV (  
    
        MLPClassifier(activation = 'tanh', solver = 'sgd'),
                  
        param_grid={
        #'learning_rate': ['constant', 'invscaling', 'adaptive'],    #<- only when solver is 'sgd'
        'alpha': 10.0 ** -np.arange(1, 7),
        'hidden_layer_sizes': [(5,), (4,), (3,), (6,), (4,2)],
         }, 
                  
        scoring = 'neg_log_loss' )",0.5337916613,
732,generative adversarial networks training lab,"# here's a routine to check the feed-forward timing of each processor
for processor in ['cpu', 'cuda']:
    print(processor)
    
    optimizer = optim.Adam(model.classifier.parameters(), lr=hyperparameters['learnrate'])
    model.to(processor)
    
    start = time.time()
    for ii, (inputs, labels) in enumerate(train_dataloader):

        # Move input and label tensors to the GPU
        inputs, labels = inputs.to(processor), labels.to(processor)

        print(""Executing forward pass {} ..."".format(ii+1))
        outputs = model.forward(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        if ii==2:
            break

    total_time = time.time()-start
    print(f""Device = {processor}; Time per batch: {total_time/3:.3f} seconds"")",0.5335232019,
732,generative adversarial networks training lab,"clf_sgd_50, Xtrain_sgd_50, ytrain_sgd_50, Xtest_sgd_50, ytest_sgd_50  = mu.do_classify(
                                                           SGDClassifier(loss='hinge'), 
                                                           {""alpha"": [0.01, 0.1, 1, 10, 100],
                                                            ""n_iter"": [50, 80, 100, 120, 150]}, 
                                                           df_50, pu.x_columns, 'click', 0)",0.5324275494,
5,markov chains,"def problem3_2():
    mc, probs = qe.markov.core.MarkovChain(P), []
    for i in range(mc.P.shape[0]):
        probs.append((mc.simulate_indices(ts_length=100000) == i).sum() / 100000)
    plt.bar(range(0,8),np.array(probs))
    plt.title(r""Simulated stationary distribution of $P$"")
    plt.show()",0.4760345221,
5,markov chains,"def problem3_3():
    mc, probs = qe.markov.core.MarkovChain(P), []
    for i in range(10000):
        path = mc.simulate_indices(ts_length=10000)
    for i in range(mc.P.shape[0]):
        probs.append((path == i).sum() / 10000)
    plt.bar(range(0,8),np.array(probs))
    plt.title(r""Simulated stationary distribution of $P$"")
    plt.show()",0.4708178043,
5,markov chains,"import random
class markov_chain(object):
    
    def __init__(self,text_path,ngram=2):
        self.ngram = ngram
        self.markov_keys = dict()
        self.path = text_path
        self.text_as_list = None

    def preprocess(self):
        with open(self.path,'r') as f:
            raw = f.read()
        self.text_as_list = # student section here

    def markov_group_generator(self,text_as_list):
        if len(text_as_list) < self.ngram+1:
            raise(""NOT A LONG ENOUGH TEXT!"")
            return

        for i in range(self.ngram,len(text_as_list)):
            yield tuple(text_as_list[i-self.ngram:i+1])

    def create_probability_object(self):
        # student section here
        
        return None
    
    def generate_sentence(self, length=25, starting_word_id=None):
        # student section here
        
        return None",0.4537520409,
5,markov chains,"def problem3_1():
    mc = qe.markov.core.MarkovChain(P)
    stdist = qe.markov.core.mc_compute_stationary(mc.P)
    plt.bar(range(0,8),stdist[0])
    plt.ylabel(""Prob"")
    plt.title(r""Stationary distribution of $P$"")
    plt.plot()",0.4506994486,
5,markov chains,"class Markov():
    
    def __init__(self, filename, order=4, word=False, train_test_split=0.1):
        # Read the whole corpus into memory
        self.order = order
        self.word = word
        data = open(filename).read()
        
        # If this is a word model, we will operate on a list - almost all functions still work because
        # strings are just lists of characters!
        if self.word: 
            data = data.split()
        
        # Split in train and test data
        test_data_size = math.floor(len(data)*train_test_split)
        self.train_data = data[:-test_data_size]
        self.test_data = data[-test_data_size:]
        
        self._build_model()
    
    def _build_model(self):
        # Defaultdict of Counter dicts to keep track of transition probabilities
        # Counter dict to keep track of state probabilities
        self._transition_probs = defaultdict(Counter)
        self._state_probs = Counter()
        
        # Create state and transition probability matrices
        for position in range(len(self.train_data)-self.order):
            # Get current state and next state of specified order
            state, next_state = self.train_data[position:position+order], self.train_data[position+order]
            if self.word: 
                state = "" "".join(state)
                next_state = "" "".join([next_state])
            self._state_probs[state]+=1
            self._transition_probs[state][next_state]+=1
            
        # Store vocabulary
        self.vocabulary = list(self._state_probs.keys())
                
        # Normalize matrices
        self._transition_probs = {state:self._normalize(next_states) for state, next_states in self._transition_probs.items()}
        self._state_probs = self._normalize(self._state_probs)
        
        # Compute entropy: for higher orders, the entropy decreases as the size of the alphabet increases
        # See Shannon (1948), p. 14
        H = 0
        for i in self._transition_probs:
            Hij = 0
            Pi = self._state_probs[i]
            for pij in self._transition_probs[i].values():
                Hij += pij * math.log((1/pij), 2)
            H +=  Pi * Hij
        self.entropy = H
    
    # Helper function to normalize a Counter dictionary w.r.t. its total sum
    def _normalize(self, dictionary):
        sigma = float(sum(dictionary.values()))
        return {key:value/sigma for key, value in dictionary.items()}
    
    def test(self):
        score = 0
        attempts = 0
        for position in range(len(self.test_data)-self.order):
            # Get current state and next state of specified order
            state, next_state = self.test_data[position:position+order], self.test_data[position+order]
            if self.word: 
                state = "" "".join(state)
                next_state = "" "".join([next_state])
            # There might be unknown stuff in the test data
            if state in self._transition_probs:
                if next_state in self._transition_probs[state]:
                    score += self._transition_probs[state][next_state]
            attempts +=1
        return score/attempts
                
    def generate_ngram(self, history):
        state = history[-self.order:]
        if self.word: state = "" "".join(state)

    def generate_text(self, n=1000):
        # Initial state is a random pick from vocabulary
        history = random.choice(self.vocabulary)
        ngrams = []
        for position in range(n):
            
            if self.word:
                state = "" "".join(history.split()[-self.order:])
            else:
                state = history[-self.order:]
                
            p = list(self._transition_probs[state].values())
            v = list(self._transition_probs[state].keys())
            ngram = np.random.choice(v, p=p)
            
            if self.word:
                history = "" "".join(history.split()[-self.order:] + ngram.split())
            else:
                history = history[-self.order:] + ngram
            
            ngrams.append(ngram)
        if self.word: return "" "".join(ngrams)
        else: return """".join(ngrams)",0.4460142255,
5,markov chains,"import numpy as np

class markov_chain:
    
    def __init__(self, text, from_file=True, ngram=2, random_state=None):
        """"""
        Markov Chains are great for generating text based on previously seen text. 
        Here we'll either read from file or from one big string, then generate a 
        probabilistic understanding of the document by using ngrams as keys and
        storing all possible following words. We can then generate sentences
        using random dice and this object.
        ---
        Inputs
            text: either the path to a file containing the text or the text (string)
            from_file: whether the text is in a file or note (bool)
            ngram: how many words to use as a key for the text generation
            random_state: used to set the random state for reproducibility
        """"""
        self.ngram = int(ngram)
        self.markov_keys = dict()
        self._from_file = from_file
        if type(text) != type(""string""):
            raise TypeError(""'text' must be a PATH or string object"")
        if from_file:
            self.path = text
        else:
            self.raw = text
        self.text_as_list = None
        if random_state:
            np.random.seed(random_state)
        self.create_probability_object()

    def preprocess(self):
        """"""
        Opens and cleans the text to be learned. If self.from_file, it reads
        from the path provided. The cleaning is very minor, just lowercasing
        and getting rid of quotes. Creates a list of words from the text.
        """"""
        if self._from_file:
            with open(self.path,'r') as f:
                self.raw = f.read()
        self.text_as_list = self.raw.lower().replace('""','').replace(""'"","""").split()

    def markov_group_generator(self,text_as_list):
        """"""
        Generator that creates the ngram groupings to act as keys.
        Just grabs ngram number of words and puts them into a tuple
        and yields that upon iteration request.
        ---
        Inputs
            text_as_list: the text after preprocessing (list)
        Outputs
            keys: word groupings of length self.ngram (tuple)
        """"""
        if len(text_as_list) < self.ngram+1:
            raise ValueError(""NOT A LONG ENOUGH TEXT!"")
            return

        for i in range(self.ngram,len(text_as_list)):
            yield tuple(text_as_list[i-self.ngram:i+1])

    def create_probability_object(self):
        """"""
        Steps through the text, pulling keys out and keeping track
        of which words follow the keys. Duplication is allowed for 
        values for each key - but all keys are unique.
        """"""
        if self.markov_keys:
            print(""Probability Object already built!"")
            return
        if not self.text_as_list:
            self.preprocess()
        for group in self.markov_group_generator(self.text_as_list):
            word_key = tuple(group[:-1])
            if word_key in self.markov_keys:
                self.markov_keys[word_key].append(group[-1])
            else:
                self.markov_keys[word_key] = [group[-1]]
    
    def generate_sentence(self, length=25, starting_word_id=None):
        """"""
        Given a seed word, pulls the key associated with that word and 
        samples from the values available. Then moves to the newly generated 
        word and gets the key associated with it, and generates again. 
        Repeats until the sentence is 'length' words long.
        ---
        Inputs
            length: how many words to generate (int)
            starting_word_id: what word to use as seed, by location (int)
        Outputs
            gen_words: the generated sentence, including seed words (string)
        """"""
        if not self.markov_keys:
            raise ValueError(""No probability object built. Check initialization!"")
        
        if (not starting_word_id or type(starting_word_id) != type(int(1)) 
            or starting_word_id < 0 or starting_word_id > len(self.text_as_list)-self.ngram):
            starting_word_id = np.random.randint(0,len(self.text_as_list)-self.ngram)
            
        gen_words = self.text_as_list[starting_word_id:starting_word_id+self.ngram]
        
        while len(gen_words) < length:
            seed = tuple(gen_words[-self.ngram:])
            gen_words.append(np.random.choice(self.markov_keys[seed]))
        return ' '.join(gen_words)
    
    def print_key_value_pairs(self, num_keys=20):
        """"""
        Iterates through the probability object, printing key-value
        pairs. 
        ---
        Input
        num_keys: how many pairs to show (int)
        """"""
        i = 1
        for key,value in self.markov_keys.items():
            print(key,value)
            print()
            i+=1
            if i>int(num_keys):
                break",0.4385648668,
5,markov chains,"class Chain(object):
    """"""
        Markov Chain class
    """"""
    def __init__(self, probability_matrix, initial_state_distribution, number_of_iterations = 10):
        """"""
            Initializes the chain
            Args:
                probability_matrix: matrix of transition probabilities P
                initial_state_distribution: vector of the initial distribution of q
                number_of_iterations: number of transitions we want to simulate
        """"""
        assert type(probability_matrix) is matrix
        assert type(initial_state_distribution) is list
        assert type(number_of_iterations) is int
        assert sum(initial_state_distribution) == 1
        for i in range(len(probability_matrix)):
            assert sum(probability_matrix.A[0]) == 1

        self.p_m = probability_matrix
        self.q = initial_state_distribution
        self.number_of_iterations = number_of_iterations

    def launch(self):
        """"""
            Launches the simulation of transition

        """"""
        print ""Launching the chain with {states} states, and {trans} transitions"".\
              format(states=len(self.q), trans=self.number_of_iterations)

        q_0_array = list()
        q_1_array = list()
        
        # calculate the q*P^n and see if it converges to a stationary distribution
        for i in range(self.number_of_iterations):
            q = self.q * self.p_m ** i
            
            q_0_array.append(q.A[0][0])
            q_1_array.append(q.A[0][1])
            
            print ""{trans:^2}:  q0 = {0:.10f}, q1 = {1:.10f}"".format(q.A[0][0], q.A[0][1], trans=i+1)

            # print ""{trans:^2}:  q0 = {0:.10f}, q1 = {1:.10f}, q2 = {2:.10f}, q3 = {3:.10f}, q4 = {4:.10f}"".\
            #      format(q.A[0][0], q.A[0][1], q.A[0][2], q.A[0][3], q.A[0][4], trans=i + 1)

        print ""-----------------------------------------""
        print ""q0 + q1 = {0:.10f}"".format(q.A[0][0] + q.A[0][1])
        
        return q_0_array, q_1_array",0.432828933,
5,markov chains,"def newmark_beta(m, c, k, u0, v0, dt, p, gamma=0.5, beta=1.0/6.0):
    n = len(p)
    peff = np.zeros(n, dtype=float)
    u = np.zeros(n, dtype=float)
    v = np.zeros(n, dtype=float)
    a = np.zeros(n, dtype=float)
    u[0] = u0
    v[0] = v0
    a[0] = (p[0] - c * v[0] - k * u[0]) / m
    a1 = (1.0 / (beta * dt**2)) * m + (gamma / (beta * dt)) * c
    a2 = (1.0 / (beta * dt)) * m + ((gamma / beta) - 1.0) * c
    a3 = ((1.0 / (2.0 * beta)) - 1.0) * m + dt * (gamma / (2.0 * beta) - 1.0) * c
    #print a1, a2, a3
    keff = k + a1
    #print keff
    for i in range(n-1):
        i1 = i + 1
        # print p[i1], a1, u[i], a2, v[i], a3, a[i]
        peff[i1] = p[i1] + a1 * u[i]  + a2 * v[i] + a3 * a[i]
        u[i1] = peff[i1] / keff
        v[i1] = (gamma / (beta * dt)) * (u[i1] - u[i]) + \
                (1.0 - (gamma / beta)) * v[i] + dt * (1.0 - (gamma / (2.0*beta))) * a[i]
        a[i1] = (1.0 / (beta * dt**2)) * (u[i1] - u[i]) - \
                (1.0 / (beta * dt)) * v[i] - (1.0 / (2.0 * beta) - 1.0) * a[i]
    return (peff, u, v, a)

if __name__ == '__main__':
    '''
    Chopra, A.K., Dynamics of Structures, 4ed., Prentice Hall, 2012
    Example 5.1, pp. 169 - Numerical solution using linear interpolation of
                 excitation
    Example 5.4, pp. 178 - Newmark Linear Acceleration Method
    '''
    m = 0.2533  # kip-sec^2/in
    k = 10.0    # kips/in
    xi = 0.05   # xi = 0.05
    c = xi * 2.0 * np.sqrt(k * m)

    u0 = 0
    v0 = 0

    tmax = 1.0
    dt = 0.1
    n = int(tmax / dt) + 1
    n1 = (0.6 / dt) + 1
    t = np.linspace(0, tmax, n)
    p = np.zeros(n, dtype=float)
    p[:n1] = 10.0 * np.sin(np.pi * t[:n1] / 0.6)

    peff, u, v, a = newmark_beta(m, c, k, u0, v0, dt, p)
    for i in range(0, len(p), int(1.0 / dt / 10)):
        print ""%8.2f %12.4f %12.4f %12.4f %12.4f %12.4f"" % \
        (dt * i, p[i], peff[i], a[i], v[i], u[i])

    # Solve again, this time with dt = 0.02sec
    dt = 0.02
    n = int(tmax / dt) + 1
    n1 = (0.6 / dt) + 1
    t1 = np.linspace(0, tmax, n)
    p1 = np.zeros(n, dtype=float)
    p1[:n1] = 10.0 * np.sin(np.pi * t1[:n1] / 0.6)

    p1eff, u1, v1, a1 = newmark_beta(m, c, k, u0, v0, dt, p1)
    for i in range(0, len(p), int(1 / dt / 10)):
        print ""%8.2f %12.4f %12.4f %12.4f %12.4f %12.4f"" % \
        (dt * i, p[i], peff[i], a[i], v[i], u[i])

    plt.subplot(211)
    plt.plot(t, p)
    plt.plot(t1, p1)
    plt.xlabel('t (sec)')
    plt.ylabel('$f(t)$ (kips)')
    plt.title('Excitation Force')
    plt.grid()
    
    plt.subplot(212)
    plt.plot(t, u, label=""$\Delta t=0.10$ sec"")
    plt.plot(t1, u1, label=""$\Delta t=0.02$ sec"")
    plt.grid()
    plt.xlabel('t (sec)')
    plt.ylabel('u(t) (inch)')
    plt.legend(loc=8)
    plt.title('Response - Newmark Linear Acceleration Method')

    plt.tight_layout()
    plt.show()",0.4311577082,
5,markov chains,"def plot_kdes(sampler, coupled, burntime=300):
    '''
    Plots joint kde plots of parameters estimated from the MCMC run.
    Returns the list of median values of estimated parameters from their respective distributions.
    
    Arguments:
        sampler: Ensemble sampler from emcee package which underwent MCMC run
        coupled: True if we use model with diffusion. False if we use model without diffusion.
        burntime: Burn-in time of MCMC run
    '''    
    samples = sampler.chain[:,burntime:,:]
    
    if coupled:
        ndim = 4
        # reshape the samples into an array with dimension (ndim) x (# of data) 1D array 
        traces = samples.reshape(-1, ndim).T

        # Create a pandas DataFrame with labels.
        parameter_samples = pd.DataFrame({'k_div': traces[0], 'k_bg': traces[1], 
                                          'k_br': traces[2], 'k_mov': traces[3]})

        # Draw joint_kde plot (Referred to: http://seaborn.pydata.org/tutorial/distributions.html)
        g = sns.PairGrid(parameter_samples)
        g.map_diag(sns.kdeplot)
        g.map_offdiag(sns.kdeplot, cmap=""Blues_d"", n_levels=6);        

        k = [parameter_samples['k_div'].median(), parameter_samples['k_bg'].median(), \
             parameter_samples['k_br'].median(), parameter_samples['k_mov'].median()]
        print('k_div = %.5f' % (parameter_samples['k_div'].median()))
        print('k_bg = %.5f' % (parameter_samples['k_bg'].median()))
        print('k_br = %.5f' % (parameter_samples['k_br'].median()))        
        print('k_mov = %.5f' % (parameter_samples['k_mov'].median()))     
        
        return k
        
    else:
        ndim = 3
        # reshape the samples into an array with dimension (ndim) x (# of data) 1D array 
        traces = samples.reshape(-1, ndim).T

        # Create a pandas DataFrame with labels.
        parameter_samples = pd.DataFrame({'k_div': traces[0], 'k_bg': traces[1], 'k_br': traces[2]})

        # Draw joint_kde plot (Referred to: http://seaborn.pydata.org/tutorial/distributions.html)
        g = sns.PairGrid(parameter_samples)
        g.map_diag(sns.kdeplot)
        g.map_offdiag(sns.kdeplot, cmap=""Blues_d"", n_levels=6);

        k = [parameter_samples['k_div'].median(), parameter_samples['k_bg'].median(), parameter_samples['k_br'].median()]
        print('k_div = %.5f' % (parameter_samples['k_div'].median()))
        print('k_bg = %.5f' % (parameter_samples['k_bg'].median()))
        print('k_br = %.5f' % (parameter_samples['k_br'].median()))        
        
        return k",0.4310835004,
5,markov chains,"def Bij(BSMscalars=[], BSMfermions=[]):
    """"""One loop RGE coefficient differences Bij=bi-bj""""""
    b1, b2, b3 = [rge1LBSM(k, BSMscalars=BSMscalars, BSMfermions=BSMfermions) for k in [1,2,3]]
    return b1-b2, b2-b3",0.4281836152,
1152,make sure the classes are balanced,"def is_pair(r1, r2):
    for n1, n2 in zip(sorted(r1.names), sorted(r2.names)):
        if rltk.levenshtein_distance(n1, n2) > min(len(n1), len(n2)) / 3:
            return False
    return True",0.360622257,
1152,make sure the classes are balanced,"def calc_weights(end_points):
    current_cells = end_points
    for cell in current_cells:
        cell.weight = 1 + cell.extra_penalty
        
    while len(current_cells):
        next_cells = set()
        for cell in current_cells:
            for next_cell in cell.neighbors:
                if next_cell.passable == False:
                    continue
                new_weight = cell.weight + next_cell.extra_penalty + 1
                if new_weight < next_cell.weight or next_cell.weight == 0 :
                    next_cell.weight = new_weight
                    next_cells.add(next_cell)

        current_cells = next_cells",0.3587026596,
1152,make sure the classes are balanced,"def periodicspin(s):
    for n in range(0,L):
        s[n][L-1]=s[n][0]
        s[L-1][n]=s[0][n]
    return s",0.3567708433,
1152,make sure the classes are balanced,"#your code here

def make_withdraw(balance):
    def withdraw(amount):
        nonlocal balance
        if amount > balance:
            return 'Insufficient funds'
        balance = balance - amount
        return balance
    return withdraw",0.354821384,
1152,make sure the classes are balanced,"#your code here

def make_withdraw(balance): 
    def withdraw(amount):
        nonlocal balance
        if amount > balance:
            return 'Insufficient funds'
        balance = balance - amount
        return balance
    return withdraw",0.354821384,
1152,make sure the classes are balanced,"def find_missing_element(arr1, arr2):
    
    # The first array should have at least one element
    if len(arr1) < 1:
        return
    
    # Sort the input arrays
    arr1.sort()
    arr2.sort()
    
    # Make list of pairs of elements that are in the same indices in the sorted arrays
    # Find a pair that has different values
    for num1, num2 in zip(arr1, arr2):
        if num1 != num2:
            return num1
    
    # Otherwise return the last element
    return arr1[-1]",0.3494898677,
1152,make sure the classes are balanced,"def merge_lists(ll1, ll2):
    if ll2.value < ll1.value:
        ll1, ll2 = ll2, ll1
    ll1_cur = ll1
    ll2_cur = ll2
    
    while ll2_cur is not None:
        # Advance ll1 to just before ll2 becomes smaller
        while ll1_cur.next is not None and ll1_cur.next.value < ll2_cur.value:
            ll1_cur = ll1_cur.next
        
        # Add the rest of ll2 if at the end of ll1
        if ll1_cur.next is None and ll2_cur is not None:
            ll1_cur.next = ll2_cur
            break
        
        # Pop from ll2 and insert into ll1
        ll2_next = ll2_cur.next
        ll2_cur.next = ll1_cur.next
        ll1_cur.next = ll2_cur
        ll2_cur = ll2_next
            
    return ll1",0.3489665985,
1152,make sure the classes are balanced,"def balance_dataset(dataset, label, undersample=True):
    # function to downsample dataset
    # gets list of datasets split by class in label column
    data_split = [dataset[dataset[label] == l].copy() for l in list(set(dataset[label].values))]
    sizes = [f.shape[0] for f in data_split]  # list of dataset lengths
    dataset = pd.concat([f.sample(n=(min(sizes) if undersample else max(sizes)),
                               replace=(not undersample), random_state=42).copy() for f in data_split], axis=0).sample(frac=1)

    print 'Balanced dataset by undersampling dominant class, now have dataset of length: ' + \
          str(len(dataset))

    return dataset",0.3477189243,
1152,make sure the classes are balanced,"def make_withdraw(balance):
    def withdraw(amount):
        nonlocal balance
        if amount > balance:
            return 'Insufficient funds'
        balance = balance - amount
        return balance
    return withdraw
    
wd = make_withdraw(20)
wd2 = make_withdraw(7)",0.346252799,
1152,make sure the classes are balanced,"def set_weights(fcn, simple_cnn):
    for i in range(5):
        fcn.layers[i].set_weights(simple_cnn.layers[i].get_weights())
        
set_weights(fcn, simple_cnn)",0.3453016281,
584,exploratory data analysis,"# SVD does not accept missing values
try:
    PCAmodelAnalytical = nPYc.multivariate.exploratoryAnalysisPCA(tData, withExclusions=True, scaling=1.0)
    nPYc.reports.multivariateReport.multivariateQCreport(tData, PCAmodelAnalytical, reportType='analytical', withExclusions=True)
except ValueError:
    print('Multivariate analysis is not currently possible with vaues <LLOQ or >ULOQ.')",0.5062432289,
584,exploratory data analysis,"PCAmodelAnalytical = nPYc.multivariate.exploratoryAnalysisPCA(tData, withExclusions=True, scaling=1.0)

nPYc.reports.multivariateReport.multivariateQCreport(tData, reportType='analytical', withExclusions=True)",0.4798452258,
584,exploratory data analysis,"def run_analysis(project_file, annotation_file, pimp_file):
    
    # load m2lda object and do thresholding
    ms2lda = Ms2Lda.resume_from(project_file)
    ms2lda.do_thresholding(th_doc_topic=0.05, th_topic_word=0.01)

    # read the annotation file
    print
    print ""Annotation file""
    motif_annotation = {}
    motif_idx = {}
    i = 0
    for item in csv.reader(open(annotation_file), skipinitialspace=True):
        key = int(item[0])
        val = item[1]
        print str(key) + ""\t"" + val
        motif_annotation[key] = val
        motif_idx[key] = i
        i += 1

    motifs_of_interest = motif_annotation.keys()    
    norm = mpl.colors.Normalize(vmin=min(motif_idx.values()), vmax=max(motif_idx.values()))
    cmap = cm.gist_rainbow
    motif_colour = cm.ScalarMappable(norm=norm, cmap=cmap)    
    
    # get the network graph out for the motifs of interest
    G = get_network_graph(ms2lda, motifs_of_interest)
    print ""\n"" + nx.info(G)  
    print

    # print out some info
    ms1_count = 0
    nodes = G.nodes(data=True)
    for node_id, node_data in nodes:
        # 1 for doc, 2 for motif
        if node_data['group'] == 1: 
            ms1_count += 1
    print ""%d (out of %d) MS1 peaks found in the graph"" % (ms1_count, ms2lda.ms1.shape[0])    
    
    # load the pimp differential analysis file for matching
    de_peaks = []
    with open(pimp_file, ""rb"") as infile:
       reader = csv.reader(infile)
       next(reader, None)  # skip the headers
       for row in reader:
        PiMP_ID = int(row[0])
        polarity = row[1]
        mz = float(row[2])
        rt = float (row[3])
        mh_intensity = float(row[4])
        tup = (PiMP_ID, polarity, mz, rt, mh_intensity)
        de_peaks.append(tup)

    print
    print ""PiMP list: ""
    for tup in de_peaks:
        print tup
        
    # do the matching
    mass_tol = 3
    rt_tol = 12

    std = np.array(de_peaks)
    std_mz = np.array([x[2] for x in de_peaks])
    std_rt = np.array([x[3] for x in de_peaks])
    matches = {}

    ms1_label = {}
    for row in ms2lda.ms1.itertuples(index=True):
        peakid = row[1]
        mz = row[5]
        rt = row[4]

        # the following line is hacky for pos mode data
        mass_delta = mz*mass_tol*1e-6
        mass_start = mz-mass_delta
        mass_end = mz+mass_delta
        rt_start = rt-rt_tol
        rt_end = rt+rt_tol

        match_mass = (std_mz>mass_start) & (std_mz<mass_end)
        match_rt = (std_rt>rt_start) & (std_rt<rt_end)
        match = match_mass & match_rt

        res = std[match]
        if len(res) == 1:
            closest = tuple(res[0])
            matches[closest] = row
            ms1_label[row[1]] = closest[1]        
        elif len(res)>1:
            closest = None
            min_dist = sys.maxint
            for match_res in res:
                match_mz = float(match_res[2])
                match_rt = float(match_res[3])
                dist = math.sqrt((match_rt-rt)**2 + (match_mz-mz)**2)
                if dist < min_dist:
                    min_dist = dist
                    closest = match_res
            closest = tuple(closest)
            matches[closest] = row
            ms1_label[row[1]] = closest[1]

    print ""Matches found %d/%d"" % (len(matches), len(std))
    print

    ms1_list = []
    for match in matches:
        key = str(match)
        ms1_row = matches[match]
        value = str(ms1_row)
        pid = ms1_row[1]
        print ""Standard %s"" % key
        print ""MS1 %s"" % value
        print
        ms1_list.append(pid)
        
    # print the motifs and count their occurences
    m2m_list = motifs_of_interest
    word_map, motif_words = ms2lda.print_motif_features(quiet=True)

    c = Counter() # count the motif occurences
    for i in range(len(ms1_list)):

        ms1 = ms1_list[i]
        df = print_report(ms2lda, G, ms1, motif_annotation, motif_words, motif_colour, motif_idx, word_map, xlim_upper=770)
        if df is not None:

            # display(df) # show the table to see the mz, annotations, etc

            # get the motif ids in the dataframe
            fragment_motif_ids = df[['fragment_motif']].values.flatten()
            loss_motif_ids = df[['loss_motif']].values.flatten()

            # get rid of nan values
            fragment_motif_ids = fragment_motif_ids[~np.isnan(fragment_motif_ids)]
            loss_motif_ids = loss_motif_ids[~np.isnan(loss_motif_ids)]

            # store the unique counts
            combined = np.append(fragment_motif_ids, loss_motif_ids).astype(int)
            combined = set(combined.tolist())
            c.update(combined)
            
    return c",0.4773717523,
584,exploratory data analysis,"def analyze(results):
    df = pd.DataFrame(data=results)
    df.columns.name = 'Batch size'
    df.index.name='Repetition'

    # Show raw results.
    print ('Experimental results: raw')
    display(df)

    df_stats = df.describe()
    df_stats.loc['mean per image'] = df_stats.ix['mean'] / df.columns
    df_stats.loc['std per image']  =  df_stats.ix['std'] / df.columns # FIXME: div by sqrt(n)?
    
    # Show stats.
    print ('Experimental results: stats')
    display(df_stats)

    # Show two plots side-by-side: mean time per batch and mean time per image.
    fig, axs = plt.subplots(1,2)
    df_stats.ix['mean'] \
        .plot(ax=axs[0],
            yerr=df_stats.ix['std'],
            title='Mean time per batch (ms)',
            kind='bar', grid=True, rot=0, figsize=[10, 4], colormap=cm.autumn_r
        )
    df_stats.ix['mean per image'] \
        .plot(ax=axs[1],
            yerr=df_stats.ix['std per image'],
            title='Mean time per image (ms)',
            kind='bar', grid=True, rot=0, figsize=[10, 4], colormap=cm.autumn
        )
    
    # Show batch size giving minimum time per image, mean and std.
    min_time_per_image_idx = df_stats.ix['mean per image'].idxmin()
    if not math.isnan(min_time_per_image_idx):
        print (
            'Minimum time per image: batch size = %d, mean = %.2f, std = %.2f' % (
                min_time_per_image_idx, 
                df_stats.ix['mean per image'][min_time_per_image_idx],
                df_stats.ix['std per image'][min_time_per_image_idx]
            )
        )
    else:
        print ('Minimum time per image: N/A')",0.4753552973,
584,exploratory data analysis,"def run_single_analysis_plot(data, true_density, x_grid):
    """"""Compare different bandwidth estimators to match data on true_density
    
    true_density is sampled at the x_grid, which is also used for plotting.
    Additionally, optimization time of the estimators and r2 score 
    are reported.
    """"""
    plt.figure(figsize=(8,5))
    data = copy.deepcopy(data)
    np.random.seed(1)
    np.random.shuffle(data)
    # approaches from statsmodels
    # Optionally, ""normal_reference"" could be used, too.
    for method in [""cv_ml"", ""cv_ls""]:
        for efficient in [True, False]:
            default_KDE = nonparametric.EstimatorSettings(
                efficient=efficient)
            t = time.time()
            kde = KDEMultivariate(
                data=[data], var_type='c', bw=method, 
                defaults=default_KDE)
            # uses x_grid and true_density for testing
            density_estimate = kde.pdf(x_grid)
            tf = time.time()
            print '%s score:'%method, r2_score(
                true_density, density_estimate),\
                ' time:', tf-t, ' efficient:', str(efficient)
            plt.plot(x_grid, density_estimate, 
                     label=method+""_""+str(efficient))
    # approaches from scipy.stats
    for method in [""scott"", ""silverman""]:
        t = time.time()
        kde = gaussian_kde(data, method)
        # uses x_grid and true_density for testing
        density_estimate = kde(x_grid.flatten())
        tf = time.time()
        print '%s score:'%method, r2_score(
            true_density, density_estimate), ' time:', tf-t
        plt.plot(x_grid, density_estimate,label=method)
    plt.plot(x_grid, true_density, '--k', label ='true density')
    plt.legend(loc='best')
    
def sample_eval_kde(data, true_density, x_grid, n_samples):
    """"""Divide n_samples into 10 chunks to generate KDE performance curves
    
    The curves are not plotted but returned as dictionaries of lists
    with the estimation methods that shall be evaluated as keys.
    *data is used to fit the KDE and true_density for evaluating it 
    on x_grid.
    
    Code is very similar to :func:`run_single_analysis_plot`
    """"""
    density_scores = defaultdict(list)
    density_times = defaultdict(list)
    data = copy.deepcopy(data)
    np.random.seed(1)
    np.random.shuffle(data)
    for i in range(int(n_samples/10),n_samples+1,int(n_samples/10)):
        # Counter output to know at which stage the processing
        # is currently at.
        sys.stdout.write(""\r%d"" % i)
        sys.stdout.flush()
        for method in [""cv_ml"", ""cv_ls""]:
            for efficient in [True, False]:
                default_KDE = nonparametric.EstimatorSettings(
                    efficient=efficient)
                t = time.time()
                kde = KDEMultivariate(
                    data=[data[:i]], var_type='c', bw=method, 
                    defaults=default_KDE)
                density_estimate = kde.pdf(x_grid)
                tf = time.time()
                density_scores[method+""_""+str(efficient)].append(
                    r2_score(true_density, density_estimate))
                density_times[method+""_""+str(efficient)].append(tf-t)
        for method in [""scott"", ""silverman""]:
            t = time.time()
            kde = gaussian_kde(data[:i], method)
            density_estimate = kde(x_grid.flatten())
            tf = time.time()
            density_scores[method].append(
                r2_score(true_density, density_estimate))
            density_times[method].append(tf-t)
    return density_scores, density_times

def print_n_density_scores(density_scores, density_times, n_samples):
    """"""Plot results from :func:`sample_eval_kde`""""""
    plt.figure()
    for method in [ ""cv_ml"", ""cv_ls""]:
        for efficient in [True, False]:
            plt.plot(
                range(int(n_samples/10), n_samples+1, int(n_samples/10)),
                density_scores[method+""_""+str(efficient)], '-v',
                label=method+""_""+str(efficient))
    for method in [""scott"", ""silverman""]:
        plt.plot(range(int(n_samples/10), n_samples+1, int(n_samples/10)),
                 density_scores[method],'-v', label=method)
    plt.legend(loc='best')
    plt.xlabel('samples')
    plt.ylabel('scores')

    plt.figure()
    for method in [ ""cv_ml"", ""cv_ls""]:
        for efficient in [True, False]:
            plt.plot(range(int(n_samples/10),n_samples+1,int(n_samples/10)),
                     density_times[method+""_""+str(efficient)],'-v',
                     label=method+""_""+str(efficient))
    for method in [""scott"", ""silverman""]:
        plt.plot(range(int(n_samples/10),n_samples+1,int(n_samples/10)),
                 density_times[method],'-v', label=method)
    plt.legend(loc='best')
    plt.xlabel('samples')
    plt.ylabel('time (s)')",0.4658718705,
584,exploratory data analysis,"if IDifTask.config.doMeasurement:
    print ""Doing measurement of dipoles""
    if len(diaSources) < IDifTask.config.maxDiaSourcesToMeasure:
         IDifTask.dipoleMeasurement.run(subtractedExposure, diaSources)
    else:
         IDifTask.measurement.run(subtractedExposure, diaSources)",0.4521558583,
584,exploratory data analysis,"from process_essays_coref import *

def get_predicted_tags(essays):
    preds_by_sent = []
    proc_essays = processed_essays_predict_most_recent_tag(essays=essays, format_ana_tags=True)
    for e in proc_essays:
        for sent in e.pred_tagged_sentences:
            pred_tags = set()
            for tags in sent:
                for t in tags:
                    if t.startswith(""Anaphor:[""):
                        t = t.replace(""Anaphor:["","""").replace(""]"","""").strip()
                        pred_tags.add(t)
            preds_by_sent.append(pred_tags)
    return preds_by_sent",0.4512156844,
584,exploratory data analysis,"def display(image, preds=[], probs=[], prediction_count=5):
    image_deprocessed = transformer.deprocess('data', image)
    ppt.imshow(image_deprocessed)
    
    try:
        # Set image title with most likely prediction
        label = imagenet_labels[preds[0]]
        probability = probs[preds[0]]
        ppt.title( '{0}: {1}% certainty'.format(label, round(probability*100, 2)) )

        # print the remaining predictions to stdout
        for ind, p in enumerate(preds):
            if ind > prediction_count:
                break
            label = imagenet_labels[p]
            probability = probs[p]
            print( '{0}: {1}% certainty'.format(label, round(probability*100, 2)) )
    except IndexError:
        pass # there were no predictions for this image
    
    ppt.show()

def predict(image):
    net.blobs['data'].data[...] = image
    probabilities = net.forward()['prob'][0]
    predictions = probabilities.argsort()[::-1]
    
    return predictions, probabilities",0.4495422244,
584,exploratory data analysis,"responses = {}
for desc in descriptions:
    resp = indicoio.analyze_text(desc, apis=['sentiment_hq', 'political', 'text_tags', 'keywords'])
    responses[desc] = resp
    time.sleep(.5)",0.4494315982,
584,exploratory data analysis,"import readability


def get_readability_nums(textstr):
    pop_list = ['sentences_per_paragraph', 'type_token_ratio', 'sentences', 'paragraphs']
    res = dict(readability.getmeasures(text=textstr, merge=True))
    for i in pop_list:
        del res[i]
    return res

df[""readability""] = df.text.apply(get_readability_nums)
keyslist = list(df.at[0, ""readability""].keys())
for i in keyslist:
    df[i] = df.readability.apply(lambda x: x[i])
    print(df.groupby(""author"", as_index=False)[i].mean())",0.4492138624,
820,homework introduction,"# Generate all possible unique pairs and then subsample
def gen_pairs():
    # Generate all n * ( n - 1 ) / 2
    poss_pairs = [(x, y) for x in range(N) for \
                             y in range(N) if x < y]

    # Sample N * PR (items * pairs_per_item) from the total
    pairs = rnd.sample(poss_pairs, N * PR) * CPP
    pairs = np.array(pairs)

    return pairs

pairs = gen_pairs()
pairs[:5]",0.4099823534,
820,homework introduction,"# Combining bz2 files into one json
def get_reddit_archives(end_year):
    
    '''Gets Pushshift archive data between 2006 and end_year'''
    
    # Make list of year strings for our file names
    years = list(range(2006,end_year+1))
    for y in range(len(years)):
        years[y] = str(years[y])
    
    # Make list of file names with different years
    file_list = list()
    initial = 'RC_2006-01.bz2' # Access Data from 2006-01 to end_year-01
    for year in years:
        file_list.append(initial.replace('2006',year))
    
    # Read lines from the bz files directly to our output with JSON
    result = list()
    for file in file_list:
        bz_file = get_reddit_file(file)
        linelist = bz_file.readlines()
        for line in linelist:
            result.append(json.loads(line))
            
    return result",0.4084528089,
820,homework introduction,"def find_similar(ratio=25/26):
    print(f""We're looking for this ratio: {ratio}"")
    for i, name in enumerate(tqdm_notebook(inp)):
        s = SequenceMatcher(a=name, b=inp[i])

        for name2 in inp[i+1:]:
            s.set_seq2(name2)
            if s.ratio() == ratio:
                print(f""found {name}, {name2}"")
                return(name, name2)
            
n1, n2 = find_similar()",0.4079950452,
820,homework introduction,"def ret_top_model():
    """"""
    Since LDAmodel is a probabilistic model, it comes up different topics each time we run it. To control the
    quality of the topic model we produce, we can see what the interpretability of the best topic is and keep
    evaluating the topic model until this threshold is crossed. 
    
    Returns:
    -------
    lm: Final evaluated topic model
    top_topics: ranked topics in decreasing order. List of tuples
    """"""
    top_topics = [(0, 0)]
    while top_topics[0][1] < 0.97:
        lm = LdaModel(corpus=corpus, id2word=dictionary)
        coherence_values = {}
        for n, topic in lm.show_topics(num_topics=-1, formatted=False):
            topic = [word for word, _ in topic]
            cm = CoherenceModel(topics=[topic], texts=train_texts, dictionary=dictionary, window_size=10)
            coherence_values[n] = cm.get_coherence()
        top_topics = sorted(coherence_values.items(), key=operator.itemgetter(1), reverse=True)
    return lm, top_topics",0.4055398107,
820,homework introduction,"def count_lines():
    f = open(path)
    line_count = 0
    for line in enumerate(f):
        line_count += 1
    print('{:.2E}'.format(line_count))
    f.close()
    print('done')
%time count_lines()",0.405266583,
820,homework introduction,"def comp_graph():
    """"""Plot nthsum and gaussum""""""
    nthsum_lst_in_us = [x * 10**6 for x in itertime(nthsum)]
    gaussum_lst_in_us = [x * 10**6 for x in itertime(gausssum)]
    inputsize = [x for x in range(len(nthsum_lst_in_us))] # Since nthsum and gausssum lists are of equal length
    
    matplotlib.pyplot.xlabel('Input Size(10**x)')
    matplotlib.pyplot.ylabel('Time required')
    
    matplotlib.pyplot.plot(inputsize, nthsum_lst_in_us) # Plot nthsum
    matplotlib.pyplot.plot(inputsize, gaussum_lst_in_us) # Plot gausssum
    
    matplotlib.pyplot.legend(['nthsum', 'gausssum'])
    
    matplotlib.pyplot.show()",0.4046461582,
820,homework introduction,"def main():
    lines = load_lines(data_path)[1:]
    training_set_lines, validation_set_lines = train_test_split(lines, test_size=0.2)
    
    nb_training = len(training_set_lines)*6
    nb_validation = len(validation_set_lines)*6

    training_images, steering_angles = get_data_without_generator(data_path, lines[0:500])
    return (training_images, steering_angles)
data_path = ""data-from-udacity""
#main()",0.4042979181,
820,homework introduction,"# Where Teseo is located in the map
def findPosition():
    startPosition = [0,0]
    for row in range(len(Teseo)):
        for col in range(len(Teseo)):
            if Teseo[row][col] == START:
                startPosition[0] = row
                startPosition[1] = col
                return startPosition",0.403888762,
820,homework introduction,"def generate_data(df):
    d = dict()
    for z in range(len(density)):
        d[(density.iloc[z,0]),(density.iloc[z,1])] = density.iloc[z,2]
    return d",0.402482748,
820,homework introduction,"def test6 () :
    a = [2, 3, 4]
    m = map(lambda v : v * v, a)
    a += [5]
    assert a       == [2, 3,  4,  5]
    assert list(m) == [4, 9, 16, 25]
    assert list(m) == []
    a += [5]
    assert list(m) == []
test6()",0.4024465084,
692,from which port did the most passengers embark? which port with the fewest passengers?,data_wrangling_xml.highest_airport,0.4015908241,
692,from which port did the most passengers embark? which port with the fewest passengers?,"# Group by airport and find extrema in average delay
Delays_by_airport = pd.DataFrame(Flights_extrema.groupby(by='Departure_Airport',as_index=False).mean())

worst_airport, worst_airport_delay =Delays_by_airport.iloc[Delays_by_airport['Departure_delay'].idxmax()][['Departure_Airport','Departure_delay']]
best_airport, best_airport_delay =Delays_by_airport.iloc[Delays_by_airport['Departure_delay'].idxmin()][['Departure_Airport','Departure_delay']]

print 'The worst airport in the data set was %s, who experienced an average departure delay of %.2f minutes' \
    %(worst_airport, worst_airport_delay)
print 'The best airport in the data set was %s, who experienced an average departure delay of %.2f minutes' \
    %(best_airport, best_airport_delay)",0.4010264277,
692,from which port did the most passengers embark? which port with the fewest passengers?,"SELECT unique_carrier,
    COUNT(CASE WHEN arr_delay <= 0 OR arr_delay IS NULL THEN 'not_delayed' END) AS not_delayed,
    COUNT(CASE WHEN arr_delay > 0 THEN 'delayed' END) AS delayed
  FROM tutorial.es_flights
    GROUP BY unique_carrier",0.3856375217,
692,from which port did the most passengers embark? which port with the fewest passengers?,"def solve_z_update(bus, x_local):
    # Calculate the set of adjacent buses of i (including i)
    adj_buses = [br.bus_from for br in bus.branches_to] + [br.bus_to for br in bus.branches_from] + [bus]
    # exclude the slack bus
    adj_buses = list(filter(lambda bus: (not bus.is_slack()), adj_buses))
    # sort by bus index
    adj_buses = sorted(adj_buses, key=lambda b: b.index)
    
    # update the angle for the bus by taking the average of all opinions of this bus
    opinions_on_x = np.zeros((mp.timesteps, ))
    
    # current bus index
    i = bus.index
    # number of buses excluding the slack bus
    n_buses = mp.get_num_buses() - 1
    for adj_bus in adj_buses:
        # index of the adjacent bus j
        j = adj_bus.index
        # local state of the adjacent bus j
        x_j = x_local[j]        
        # projection matrix extracting the angles from the local state of bus j
        p_ang_j = sim_p_ang[j]
        # projection matrix extracting the angles bus j has knowledge about from the global opinion state z
        p_z_j = sim_p_z[j]
        
        # sum the opinions on the angle of i
        opinions_on_x += np.array(x_j * p_ang_j.T * p_z_j)[i::n_buses] 
    # devide by the number of adjacent buses to get the average
    z_bus = opinions_on_x / len(adj_buses)
    
    return z_bus",0.3822492957,
692,from which port did the most passengers embark? which port with the fewest passengers?,"fuel_econ_mean_ray_on = (drive[drive.ray_connected ==1].groupby('tripid').liter.max().sum() / 
                         drive[drive.ray_connected == 1].groupby('tripid').calc_dist.max().sum()) * 100
fuel_econ_mean_ray_off = (drive[drive.ray_connected == 0].groupby('tripid').liter.max().sum() / 
                         drive[drive.ray_connected == 0].groupby('tripid').calc_dist.max().sum()) * 100
fuel_econ_trip_mean_ray_on = summ_trip[summ_trip.ray_on == 1].fuel_econ.mean()
fuel_econ_trip_mean_ray_off = summ_trip[summ_trip.ray_on == 0].fuel_econ.mean()
print ('Overall fuel economy mean with ray on = ', round(fuel_econ_mean_ray_on,2), ' L/100km with ', 
       len(summ_trip[summ_trip.ray_on == 1].fuel_econ), 'samples.')
print ('Overall fuel economy mean with ray off = ', round(fuel_econ_mean_ray_off,2), ' L/100km with ', 
      len(summ_trip[summ_trip.ray_on == 0].fuel_econ), 'samples.')
print ('Trip fuel economy mean with ray on = ', round(fuel_econ_trip_mean_ray_on,2), ' L/100km')
print ('Trip fuel economy mean with ray off = ', round(fuel_econ_trip_mean_ray_off,2), ' L/100km')
t_test = stats.ttest_ind(summ_trip[summ_trip.ray_on == 1].fuel_econ, summ_trip[summ_trip.ray_on == 0].fuel_econ)
print (t_test)",0.3791714907,
692,from which port did the most passengers embark? which port with the fewest passengers?,"# Group by airline and find extrema in average delay
Delays_by_airline = pd.DataFrame(Flights_extrema.groupby(by='Airline',as_index=False).mean())
worst_airline, worst_delay =Delays_by_airline.iloc[Delays_by_airline['Departure_delay'].idxmax()][['Airline','Departure_delay']]
best_airline, best_delay =Delays_by_airline.iloc[Delays_by_airline['Departure_delay'].idxmin()][['Airline','Departure_delay']]

print 'The worst airline in the data set was %s, who experienced an average departure delay of %.2f minutes' \
    %(worst_airline, worst_delay)
print 'The best airline in the data set was %s, who experienced an average departure delay of %.2f minutes' \
    %(best_airline, best_delay)",0.3791375756,
692,from which port did the most passengers embark? which port with the fewest passengers?,"def lemke_howson_numpy(g, init_pivot=0, max_iter=10**6):
    """"""
    Wrap the procedure of initializing tableaux, complementary pivoting,
    and get the mixed actions.

    Parameters
    ----------
    g : NormalFormGame
        NormalFormGame instance with 2 players.

    init_pivot : scalar(int), optional(default=0)
        Initial pivot, an integer k such that 0 <= k < m+n, where
        integers 0, ..., m-1 and m, ..., m+n-1 correspond to the actions
        of players 0 and 1, respectively.

    max_iter : scalar(int), optional(default=10**6)
        Maximum number of pivoting steps.

    Returns
    -------
    NE : tuple(ndarray(float, ndim=1)) or None
        Tuple of computed Nash equilibrium mixed actions.
        If no Nash equilibrium is found, return None.

    """"""
    payoff_matrices = tuple(g.players[i].payoff_array for i in range(2))
    tableaux, bases = initialize_tableaux(payoff_matrices)
    
    converged = lemke_howson_tbl(tableaux, bases, init_pivot, max_iter)
    if converged:
        NE = get_mixed_actions(tableaux, bases)
        return NE
    else:
        print(""Not converged"")
        return None",0.3745756745,
692,from which port did the most passengers embark? which port with the fewest passengers?,"# Bus-bus switches
for _, switch in hv_bus_sw.iterrows():
    from_bus = pp.get_element_index(net, ""bus"", switch.from_bus)
    to_bus = pp.get_element_index(net, ""bus"", switch.to_bus)
    pp.create_switch(net, from_bus, to_bus, et=switch.et, closed=switch.closed, type=switch.type, name=switch.bus_name)

# Bus-line switches
hv_buses = net.bus[(net.bus.vn_kv == 380) | (net.bus.vn_kv == 110)].index
hv_ls = net.line[(net.line.from_bus.isin(hv_buses)) & (net.line.to_bus.isin(hv_buses))]
for _, line in hv_ls.iterrows():
        pp.create_switch(net, line.from_bus, line.name, et='l', closed=True, type='LBS', name='Switch %s - %s' % (net.bus.name.at[line.from_bus], line['name']))
        pp.create_switch(net, line.to_bus, line.name, et='l', closed=True, type='LBS', name='Switch %s - %s' % (net.bus.name.at[line.to_bus], line['name']))

# Trafo-line switches
pp.create_switch(net, pp.get_element_index(net, ""bus"", 'Bus DB 2'), pp.get_element_index(net, ""trafo"", 'EHV-HV-Trafo'), et='t', closed=True, type='LBS', name='Switch DB2 - EHV-HV-Trafo')
pp.create_switch(net, pp.get_element_index(net, ""bus"", 'Bus SB 1'), pp.get_element_index(net, ""trafo"", 'EHV-HV-Trafo'), et='t', closed=True, type='LBS', name='Switch SB1 - EHV-HV-Trafo')

# show switch table
net.switch",0.373062104,
692,from which port did the most passengers embark? which port with the fewest passengers?,"class Car:
    def __init__(self, distance_travelled=0, speed=0):
        self.distance_travelled = distance_travelled
        self.speed = speed
        
    def brake(self):
        self.speed = 0
    
    def accelerate(self, acceleration=1):
        self.speed += acceleration
        
    def move(self):
        self.distance_travelled += self.speed
    
my_car = Car(speed=2)
print(""Distance Travelled: %d"" % my_car.distance_travelled)
my_car.move()
print(""Distance Travelled: %d"" % my_car.distance_travelled)
my_car.move()
print(""Distance Travelled: %d"" % my_car.distance_travelled)
my_car.accelerate()
print(""Distance Travelled: %d"" % my_car.distance_travelled)
my_car.move()
print(""Distance Travelled: %d"" % my_car.distance_travelled)
my_car.brake()
print(""Distance Travelled: %d"" % my_car.distance_travelled)
my_car.move()
print(""Distance Travelled: %d"" % my_car.distance_travelled)",0.3727791309,
692,from which port did the most passengers embark? which port with the fewest passengers?,"#create a function to apply to ss to create a group column
def f(df):
    if df.start_station_trip_count < 591:
        val= 4
    elif df.start_station_trip_count < 3441:
        val = 3
    elif df.start_station_trip_count < 9628:
        val = 2
    elif df.start_station_trip_count >= 9628:
        val = 1
    return val

#create a new group column
ss['group'] = ss.apply(f, axis=1)",0.3718658686,
1029,lab logistic regression using tensorflow,"# Try Logistic Regression
from sklearn import linear_model
from sklearn.cross_validation import cross_val_score
model = linear_model.LogisticRegression()
scores = cross_val_score(model, X_trainset, Y_trainset[0], cv=5)
print(""Accuracy: %0.2f"" % (scores.mean()))",0.6403706074,
1029,lab logistic regression using tensorflow,"m.compile(
    optimizer=optimizers.rmsprop(),
    loss=losses.categorical_crossentropy,
    metrics=[metrics.categorical_accuracy]      
    )",0.6364072561,
1029,lab logistic regression using tensorflow,"dnn_keras_model.model.compile(optimizer='adam',
                             loss='sparse_categorical_crossentropy',
                             metrics=['accuracy'])",0.6360300779,
1029,lab logistic regression using tensorflow,"VGGbn.model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])",0.6299208999,
1029,lab logistic regression using tensorflow,"from sklearn.linear_model import LogisticRegression
from sklearn.cross_validation import cross_val_score

logreg = LogisticRegression(C=1)
scores = cross_val_score(logreg, rich_features_final, target, cv=5, scoring='accuracy')
print(scores.min(), scores.mean(), scores.max())",0.6298632622,
1029,lab logistic regression using tensorflow,"from sklearn.linear_model import LogisticRegression
from sklearn.cross_validation import cross_val_score

logreg = LogisticRegression(C=1)
scores = cross_val_score(logreg, X_train_array, y_train_array, cv=5, scoring='roc_auc')
scores.mean()",0.6298632622,
1029,lab logistic regression using tensorflow,"%%time

from sklearn.linear_model import LogisticRegression
from sklearn.cross_validation import cross_val_score

logreg = LogisticRegression(C=1)
scores = cross_val_score(logreg, rich_features_final, target, cv=5, scoring='accuracy')
print(scores.min(), scores.mean(), scores.max())",0.6298632622,
1029,lab logistic regression using tensorflow,"# %%time

from sklearn.linear_model import LogisticRegression
from sklearn.cross_validation import cross_val_score

logreg = LogisticRegression(C=1)
scores = cross_val_score(logreg, rich_features_final, target, cv=5, scoring='accuracy')
print(""Logistic Regression CV scores:"")
print(""  min: {:.3f}, mean: {:.3f}, max: {:.3f}"".format(
    scores.min(), scores.mean(), scores.max()))",0.6298632622,
1029,lab logistic regression using tensorflow,"%%time

from sklearn.linear_model import LogisticRegression
from sklearn.cross_validation import cross_val_score

logreg = LogisticRegression(C=1)
scores = cross_val_score(logreg, rich_features_final, target, 
                        cv=5, scoring='accuracy')
print(""Logistic Regression CV scores:"")
print(""min: {:.3}, maen: {:.3f}, max: {:.3f}"".format(
    scores.min(), scores.mean(), scores.max()))",0.6298632622,
1029,lab logistic regression using tensorflow,"log = sklearn.linear_model.LogisticRegression()
log = log.fit(X,y)
log_scores = sklearn.model_selection.cross_val_score(log, X, y, cv=10, scoring='accuracy')
train_log_acc = log.score(X,y)
cv_log_acc = log_scores.mean()",0.6284778118,
1779,regex,"import re
stemmer = SnowballStemmer(""english"")
def str_stem(s): 
    if isinstance(s, str):
        s = s.lower()
        s = re.sub(r""(\w)\.([A-Z])"", r""\1 \2"", s)

        s = re.sub(r""([0-9]+)( *)(inches|inch|in|')\.?"", r""\1in. "", s)
    
        s = re.sub(r""([0-9]+)( *)(foot|feet|ft|'')\.?"", r""\1ft. "", s)
    
        s = re.sub(r""([0-9]+)( *)(pounds|pound|lbs|lb)\.?"", r""\1lb. "", s)
    
        s = s.replace("" x "","" xby "")
        s = s.replace(""*"","" xby "")
        s = s.replace("" by "","" xby"")
        s = s.replace(""x0"","" xby 0"")
        s = s.replace(""x1"","" xby 1"")
        s = s.replace(""x2"","" xby 2"")
        s = s.replace(""x3"","" xby 3"")
        s = s.replace(""x4"","" xby 4"")
        s = s.replace(""x5"","" xby 5"")
        s = s.replace(""x6"","" xby 6"")
        s = s.replace(""x7"","" xby 7"")
        s = s.replace(""x8"","" xby 8"")
        s = s.replace(""x9"","" xby 9"")
        s = s.replace(""0x"",""0 xby "")
        s = s.replace(""1x"",""1 xby "")
        s = s.replace(""2x"",""2 xby "")
        s = s.replace(""3x"",""3 xby "")
        s = s.replace(""4x"",""4 xby "")
        s = s.replace(""5x"",""5 xby "")
        s = s.replace(""6x"",""6 xby "")
        s = s.replace(""7x"",""7 xby "")
        s = s.replace(""8x"",""8 xby "")
        s = s.replace(""9x"",""9 xby "")
        
        s = re.sub(r""([0-9]+)( *)(square|sq) ?\.?(feet|foot|ft)\.?"", r""\1sq.ft. "", s)
    
        s = re.sub(r""([0-9]+)( *)(gallons|gallon|gal)\.?"", r""\1gal. "", s)
        
        s = re.sub(r""([0-9]+)( *)(ounces|ounce|oz)\.?"", r""\1oz. "", s)
    
        s = re.sub(r""([0-9]+)( *)(centimeters|cm)\.?"", r""\1cm. "", s)
    
        s = re.sub(r""([0-9]+)( *)(milimeters|mm)\.?"", r""\1mm. "", s)
        
        s = re.sub(r""([0-9]+)( *)(degrees|degree)\.?"", r""\1deg. "", s)
    
        s = re.sub(r""([0-9]+)( *)(volts|volt)\.?"", r""\1volt. "", s)
        
        s = re.sub(r""([0-9]+)( *)(watts|watt)\.?"", r""\1watt. "", s)
    
        s = re.sub(r""([0-9]+)( *)(amperes|ampere|amps|amp)\.?"", r""\1amp. "", s)
        
        s = s.replace(""  "","" "")
        s = ("" "").join([stemmer.stem(z) for z in s.split("" "")])
        return s.lower()
    else:
        return ""null""",0.4562149644,
1779,regex,"def re_longer_than(N):
    return re.compile('^[a-z]{' + '{0},'.format(N) + '}')",0.4524542093,
1779,regex,"def get_packets_by_regex(string):
    # Perform Error-checking
    if not isinstance(string, str):
        raise TypeError('Should a string!')
    
    r = re.compile(string)
    return list(filter(r.search, packets))

# get_packets_by_regex('comcast')",0.4506164193,
1779,regex,"# Writing a function that updates an amenity type

str_capital = re.compile(r'[A-Z]')

def update_amenity_type(string):
    if str_capital.search(string):
        string = string.lower()
    return string",0.4500878155,
1779,regex,re.compile?,0.4494327903,
1779,regex,"""""""
A	pangram	is	a	sentence	that	contains	all	the	letters	of	the	English	alphabet	at	
least	once,	for	example:	The	quick	brown	fox	jumps	over	the	lazy	dog.	Your	task	
here	is	to	write	a	function	to	check	a	sentence	to	see	if	it	is	a	pangram	or	not.

The ideal that I am going to do is by two steps. First  check the input string itself whether it is lareger
than 26 which is all the alpabat, if not return false directly, then put all the alphabat which subtract all the space
and punctuation etc, then compare whether all the aphapbet being conunting once, by using for loops
then if all the aphapbet being usin once, then return ture

""""""
import re

def pangram(s):  #define function
    apphalist='abcdefghijklmnopqrstuvwxyz'   #define a list 
    count=0   #define the check number
    if len(s)<26:
        return False   #if the string length is less 26, obviously is wrong
    else:
        s = re.sub('[^a-zA-Z]','',s).lower()  #subtract all nonaphalbet element inthe string
        for i in range(0,len(apphalist)):  #check each individual aph in the string, if in the string count +1
            if apphalist[i] in s:
                count=count+1
        
        if count==26:  #if all apha in the list, the count should equal to 26
            return True
        else:
            return False",0.4486911297,
1779,regex,"import re

re.match('abc','abcdef')
word_regex = '\w+'

re.match(word_regex,'hi there')",0.4479449987,
1779,regex,"# Importing Libraries
import re

string = ""Woodchuck""
print(""Match Case String: "",re.match(r""[Ww]oodchuck"",string))

string = ""123""
print(""Match digits: "",re.search(r""[0-9]"",string))

string = ""abc""
print(""Match Lower Case: "",re.match(r""[a-z]"",string))

string = ""ABC""
print(""Match Upper Case: "",re.match(r""[A-Z]"",string))",0.4474104643,
1779,regex,"import re

# function to match ""green"" words in articles' titles or first words of untitled articles' content
def findWholeWord(w):
    return re.compile(r'\b({0})\b'.format(w), flags=re.IGNORECASE).search",0.4467085004,
1779,regex,"# https://docs.python.org/3/library/re.html#re.regex.match

# how do i print a single back slash

# to match a literal backslash, one might have to write '\\\\' 
# as the PATTERN string, and each backslash 
# must be expressed as \\ inside a regular Python STRING literal.
re.search('\\\\', 'p\\')

# Using raw string
re.search(r'\\', 'p\\', re.I)",0.4457728565,
2568,what is the difference between these two syntaxes?,"from proveit.logic.equality._axioms_ import substitution
from proveit._common_ import a, b, c, d, x, y, z, fx # we'll use these later
substitution",0.4017958641,
2568,what is the difference between these two syntaxes?,"print(read_expr(r'\x.\y.(dog(x) & own(y, x)) (cyril)').simplify())",0.4008224905,
2568,what is the difference between these two syntaxes?,"print(read_expr(r'\x.\y.(dog(x) & own(y, x)) (cyril, angus)').simplify())",0.4008224905,
2568,what is the difference between these two syntaxes?,"# In this example, multiple variables are assigned then switched
# Run the code

a, b = 1, ""2""
a, b = b, a

print(a)
print(b)",0.398234725,
2568,what is the difference between these two syntaxes?,S.decision_obj.simulStart.value,0.392074883,
2568,what is the difference between these two syntaxes?,"print(oceanwaves.units.simplify('m^2 / m'))
print(oceanwaves.units.simplify('m ^ 2 / m'))                               # test spaces
print(oceanwaves.units.simplify('m2 / m'))                                  # test missing exponents
print(oceanwaves.units.simplify('m^3 / (m * m)'))                           # test groups
print(oceanwaves.units.simplify('m^4 / (m * (m / m^-1))'))                  # test nested groups
print(oceanwaves.units.simplify('m^5 / (m * m)^2'))                         # test group exponents
print(oceanwaves.units.simplify('m^5 / (m * m)^2 + NAP'))                   # test terms
print(oceanwaves.units.simplify('(kg * m^2 * s^2)^2 / (kg^2 * m^3 * s^4)')) # multiple units
print(oceanwaves.units.simplify('(m^2 / Hz) * m^-2'))                       # hertz to seconds conversion",0.3918018341,
2568,what is the difference between these two syntaxes?,"a = 1
b = 2
a, b = b, a     # Swap variables
print(f""a={a}"")
print(f""b={b}"")",0.3916527629,
2568,what is the difference between these two syntaxes?,"#can also be used to swap variables
a,b=1,2
print(""a:"",a)
print(""b:"",b)
a,b=b,a
print(""a:"",a)
print(""b:"",b)",0.388916254,
2568,what is the difference between these two syntaxes?,(2).__pow__(3),0.3886810541,
2568,what is the difference between these two syntaxes?,"tupx = (1, 2)
tupy = (1, 2)
print(tupx == tupy)
print(tupx is tupy) # they are not the same tuple by construction
print(tupx.__hash__() == tupy.__hash__()) # they can be used to index, since they have a same hashable id",0.3873071074,
385,creating our own nearestneighborsclassifier class,"# (a) Write your own kNN classifier

class Knn:
# k-Nearest Neighbor class object for classification training and testing
    def __init__(self):
        
    def fit(self, x, y):
        # Save the training data to properties of this class
        
    def predict(self, x, k):
        y_hat = [] # Variable to store the estimated class label for 
        # Calculate the distance from each vector in x to the training data
        
        # Return the estimated targets
        return y_hat

# Metric of overall classification accuracy
#  (a more general function, sklearn.metrics.accuracy_score, is also available)
def accuracy(y,y_hat):
    nvalues = len(y)
    accuracy = sum(y == y_hat) / nvalues
    return accuracy",0.4585675299,
385,creating our own nearestneighborsclassifier class,"import scipy.spatial.distance as ssd

def knn(k, X_train, X_test, q_train):
    """""" k-nearest neighbors """"""
    # initialize list to store predicted class
    pred_class = []
    # for each instance in data testing,
    # calculate distance in respect to data training
    for ii, di in enumerate(X_test):
        distances = []  # initialize list to store distance
        for ij, dj in enumerate(X_train):
            # calculate distances
            distances.append((calc_dist(di,dj), ij))
        # k-neighbors
        k_nn = sorted(distances)[:k]
        # predict the class for the instance
        pred_class.append(classify(k_nn, q_train))
    # return prediction class
    return pred_class
 
def calc_dist(di,dj):
    return ssd.euclidean(di,dj) # built-in Euclidean fn
 
def evaluate(result):
    # create eval result array to store evaluation result
    eval_result = np.zeros(2,int)
    for x in result:
        # increment the correct prediction by 1
        if x == 0:
            eval_result[0] += 1
        # increment the wrong prediction by 1
        else:
            eval_result[1] += 1
    # return evaluation result
    return eval_result

def classify(k_nn, q_train):
    qlabel = []
    for dist, idx in k_nn:
        # retrieve label class and store into qlabel
        qlabel.append(q_train[idx])
    # return prediction class
    return np.argmax(np.bincount(qlabel))
 
def main():
    N = data.shape[0] #get tupple (numRows, numCols)
    np.random.shuffle(data)
  
    Nfolds = 10
    sizes = np.tile(np.floor(N/10),(1,Nfolds))
    sizes[-1] = sizes[-1] + N - sizes.sum()
    c_sizes = np.hstack((0,np.cumsum(sizes)))
    X = np.copy(data[:,:11])# change to data here if you dont want to run the cv on independent test data
    t = np.copy(data[:,11])
    
    # initialize K
    K = [1,3,7,11,19]
    cv_loss = np.zeros((Nfolds, len(K)))
    print ""Started Cross Validation for 10 folds. This may take a few minutes...""
    for i in range(len(K)):
        for fold in range(Nfolds):
            X_fold = X[c_sizes[fold]:c_sizes[fold+1],:]
            X_train = np.delete(X,np.arange(c_sizes[fold],c_sizes[fold+1],1),0)

            t_fold = t[c_sizes[fold]:c_sizes[fold+1]]
            t_train = np.delete(t,np.arange(c_sizes[fold],c_sizes[fold+1],1),0)

            # predict the data test into class
            pred_class = knn(K[i], X_train, X_fold, t_train)
            # evaluate the predicted result
            eval_result = evaluate(pred_class-t_fold)
            # Calculate accuracy
            cv_loss[fold,i] = float(eval_result[1])/float(eval_result[0]+eval_result[1])
        print ""Processing... K,"",K[i]
    
    plt.plot(K,cv_loss.mean(axis=0),'r-',label=""CV results"")
    plt.legend()
    plt.xlabel('K')
    plt.ylabel('Accuracy')
    print ""Optimal K value found"", K[np.argmin(cv_loss.mean(axis=0))]
    return K[np.argmin(cv_loss.mean(axis=0))]
    
def confusionMatrix(optimalK):
    N = data.shape[0] #get tupple (numRows, numCols)
    np.random.shuffle(data)
    
    train = data[:int(N*0.7)]
    test = data[int(N*0.7):]
    X_train = train[:,:11]
    q_train = train[:,11]
    
    X_test = test[:,:11]
    q_test = test[:,11]
    
    confusion_matrix = np.zeros((6,6)) # map class 3 to 0, 4 to 1, 5 to 2, 6 to 3, 7 to 4, 8 to 5
    m = {3:0, 4:1, 5:2, 6:3, 7:4, 8:5}

    # predict the data test into class
    pred_class = knn(optimalK, X_train, X_test, q_train)
    # build the confusion matrix
    for p in range(len(pred_class)):
        confusion_matrix[m[pred_class[p]]][m[q_test[p]]] += 1.0
    
    print ""Confusion Matrix. X-axis is the true class, Y-axis is the predicted class""
    print
    print "" "",3,"" "",4,"" "",5,"" "",6,"" "",7,"" "",8
    for i in range(6):
        line = str(i+3) + "" ""
        for j in range(6):
            line += str(confusion_matrix[i][j]) + "" ""
        print line    
print ""KNN started...""  
optimalK = main()",0.4540906549,
385,creating our own nearestneighborsclassifier class,"from sklearn.neighbors import NearestNeighbors

def find_nearest(group, match, groupname):
    nbrs = NearestNeighbors(1).fit(match['timestamp'].values[:, None])
    dist, ind = nbrs.kneighbors(group['timestamp'].values[:, None])

    group['nearesttime'] = match['timestamp'].values[ind.ravel()]
    return group",0.4539687037,
385,creating our own nearestneighborsclassifier class,"n_neighbors = 15
h = 0.01
for weights in ['uniform', 'distance']:
    # we create an instance of Neighbours Classifier and fit the data.
    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights, algorithm='brute')
    clf.fit(X, y)

    # Plot the decision boundary. For that, we will assign a color to each
    # point in the mesh [x_min, m_max]x[y_min, y_max].
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    y_min = 1.0
    y_max = 8.0
    x_min= 1.0
    x_max= 8.0
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

    # Put the result into a color plot
    Z = Z.reshape(xx.shape)
    plt.figure()
    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)

    # Plot also the training points
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)
    #plt.xlim(xx.min(), xx.max())
    #plt.ylim(yy.min(), yy.max())
    plt.xlim(1.0, 8.0)
    plt.ylim(1.0, 8.0)
    plt.title(""3-Class classification (k = %i, weights = '%s')""
              % (n_neighbors, weights))

plt.show()",0.4536933303,
385,creating our own nearestneighborsclassifier class,"k_range = list(range(1, 20))
k_scores = []
for k in k_range:
    knn = KNeighborsClassifier(n_neighbors=k)
    fit = knn.fit(X_train, y_train)
    y_pred = knn.predict(X_test)
    scores = metrics.accuracy_score(y_test, y_pred)
    k_scores.append(scores)
print(k_scores)
print
print ""The highest accuracy score is with k-value of "" + str(k_scores.index(max(k_scores)) + 1)",0.4530893564,
385,creating our own nearestneighborsclassifier class,"for n_neighbors in [i*50 for i in range(1, 5)]:
    
    title = ""Learning Curves (KNN, $\k={}$)"".format(n_neighbors)

    knn_estimator = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors)

    plt, knn_time = plot_learning_curve(knn_estimator, title, 
                                        X_scaled, y, ylim=(0.6, 1.01), 
                                        cv=cv, n_jobs=n_jobs)
    plt.show()",0.4520959258,
385,creating our own nearestneighborsclassifier class,"for n_neighbors in [i*50 for i in range(1, 5)]:
    
    title = ""Learning Curves (KNN, $\k={}$)"".format(n_neighbors)

    knn_estimator = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors)

    plt, knn_time = plot_learning_curve(knn_estimator, title, 
                                        X_scaled, y, ylim=(0.6, 1.01), 
                                        cv=cv, n_jobs=n_jobs)
    plt.savefig('voice_images/KNN/k_{}_uniform.png'.format(n_neighbors))
    plt.show()",0.4520959258,
385,creating our own nearestneighborsclassifier class,"from sklearn.neighbors import KNeighborsClassifier
def classifyKNNClassifier(XTrain, XTest, YTrain, YTest, params, ruled):
    neighbours = params['neighbours']
    neigh = KNeighborsClassifier(n_neighbors=neighbours)
    YPred = neigh.fit(XTrain, YTrain).predict(XTest)
    if ruled:
        folder = testFolder
        fileString = 'TEST_'
        XTestSubjects = readSubjects(XTest, folder, fileString)
        XTestBody = readEmailBody(XTest, folder, fileString)
        YPred = ApplyStaticRule1(XTestSubjects, YPred)
        YPred = ApplyStaticRule2(XTestBody, YPred)
    diff = YPred - YTest
    score = diff[diff == 0].size
    return (100.0 * score)/(YPred.size)",0.4510723948,
385,creating our own nearestneighborsclassifier class,"def runKNN_class(input_df, target,numNeigh, no_folds):
    start_time=time.time()
    knn_w = neighbors.KNeighborsClassifier(n_neighbors=numNeigh, weights='distance')
    # cross validation
    cvs = cross_val_score(knn_w, input_df, target, cv=no_folds)   
    print('Time taken: {} seconds.'.format('%.3f' % (time.time() - start_time)))
    print('Average accuracy KNN with weights: {}'.format('%.3f' % cvs.mean()))
    print('Standard deviation of accuracy: {}'.format('%.3f' % np.std(cvs, ddof=1)))
    return(cvs)",0.4503551722,
385,creating our own nearestneighborsclassifier class,"for i in range(1,21):
    knnclf = neighbors.KNeighborsClassifier(i,weights='distance')
    knnclf.fit(bank_train_norm,bank_target_train)
    knnpred_test = knnclf.predict(bank_test_norm)
    score = knnclf.score(bank_test_norm,bank_target_test)
    print ""K=%s, Score = %s""%(i,score)",0.4495921433,
1737,random forests,"def test_fraction_scan(degree=2, seed=123):
    gen = np.random.RandomState(seed=seed)
    test_fractions = np.arange(0.05, 0.6, 0.025)
    R2 = np.empty((2, len(test_fractions)))
    for i, test_fraction in enumerate(test_fractions):
        for j, (X, y) in enumerate(((Xa, ya), (Xb, yb))):
            X_train, X_test, y_train, y_test = model_selection.train_test_split(
                X, y, test_size=test_fraction, random_state=gen)
            fit = poly_fit(X_train, y_train, degree)
            R2[j, i] = fit.score(X_test.reshape(-1, 1), y_test)
    plt.plot(test_fractions, R2[0], 'o:', label='$N = {}$'.format(len(ya)))
    plt.plot(test_fractions, R2[1], 'o:', label='$N = {}$'.format(len(yb)))
    plt.xlabel('Test fraction')
    plt.ylabel('Test score $R^2$')
    plt.ylim(-2, 1)
    plt.legend()
    
test_fraction_scan()",0.5443151593,
1737,random forests,"def _kmeans_step(frame=0, n_clusters=n_clusters):
    rng = np.random.RandomState(random_state)
    labels = np.zeros(X.shape[0])
    centers = rng.randn(n_clusters, 2)

    nsteps = frame // 3

    for i in range(nsteps + 1):
        old_centers = centers
        if i < nsteps or frame % 3 > 0:
            dist = euclidean_distances(X, centers)
            labels = dist.argmin(1)

        if i < nsteps or frame % 3 > 1:
            centers = np.array([X[labels == j].mean(0)
                                for j in range(n_clusters)])
            nans = np.isnan(centers)
            centers[nans] = old_centers[nans]


    # plot the data and cluster centers
    plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='rainbow',
                vmin=0, vmax=n_clusters - 1);
    plt.scatter(old_centers[:, 0], old_centers[:, 1], marker='o',
                c=np.arange(n_clusters),
                s=200, cmap='rainbow')
    plt.scatter(old_centers[:, 0], old_centers[:, 1], marker='o',
                c='black', s=50)

    # plot new centers if third frame
    if frame % 3 == 2:
        for i in range(n_clusters):
            plt.annotate('', centers[i], old_centers[i], 
                         arrowprops=dict(arrowstyle='->', linewidth=1))
        plt.scatter(centers[:, 0], centers[:, 1], marker='o',
                    c=np.arange(n_clusters),
                    s=200, cmap='rainbow')
        plt.scatter(centers[:, 0], centers[:, 1], marker='o',
                    c='black', s=50)

    plt.xlim(-4, 5)
    plt.ylim(-2, 2)
    plt.ylabel('PC 2')
    plt.xlabel('PC 1')

    if frame % 3 == 1:
        plt.text(4.5, 1.7, ""1. Reassign points to nearest centroid"",
                 ha='right', va='top', size=8)
    elif frame % 3 == 2:
        plt.text(4.5, 1.7, ""2. Update centroids to cluster means"",
                 ha='right', va='top', size=8)",0.5440157056,
1737,random forests,"class RandomForest:
    def __init__(self, size=40, max_depth = 30, trees=[]):
        self.size = size
        self.max_depth = max_depth
        self.trees = []
    
    def train(self, data, labels):
        trees = []
        for i in range(self.size):
            sub_S = []
            sub_L = []
            for i in np.random.choice(len(labels), len(labels)):
                sub_S.append(data[i])
                sub_L.append(labels[i])
            sub_S = np.array(sub_S)
            sub_L = np.array(sub_L)
            clf = DecisionTree(max_depth=self.max_depth, from_tree = True)
            clf.train(sub_S, sub_L)
            self.trees.append(clf)
    def predict(self, X):
        p = []
        for t in self.trees:
            p.append(t.predict(X))
        p = np.array(p)
        if p.ndim == 1:
            p = p.reshape((len(p), 1))
        best = []
        for i in range(p.shape[1]):
            best.append(stats.mode(p[:,i])[0][0])
        return best",0.5433580875,
1737,random forests,"def plot_proposals(n=50, seed=123):
    gen = np.random.RandomState(seed=seed)
    mu = np.zeros(2)
    C = np.diag((.3, .1))
    for xy0 in np.array([-3, -1]), np.array([3, 1]):
        xy = np.array([sample_proposal(xy0, gen, mu, C, 'random_walk') for i in range(n)])
        plt.scatter(*xy.T, label='random_walk')
        xy = np.array([sample_proposal(xy0, gen, mu, C, 'independent') for i in range(n)])
        plt.scatter(*xy.T, label='independent')
        plt.scatter(*xy0, marker='+', s=300, c='k')
    
plot_proposals()",0.5422437191,
1737,random forests,"def make_dataframes(count=COUNT, n=220, seed=None):
    these_yodas = _generate_random_yodas_string(count=count, n=n, seed=seed)
    
    feature_A = _query_tables(these_yodas, 'A')
    feature_B = _query_tables(these_yodas, 'B')
    feature_C = _query_tables(these_yodas, 'C')
    feature_D = _query_tables(these_yodas, 'D')
    target = _query_target_table(these_yodas)

    feature = (feature_A.merge(feature_B, on='yoda')
                        .merge(feature_C, on='yoda')
                        .merge(feature_D, on='yoda'))
    
    target.set_index('yoda', inplace=True)
    feature.set_index('yoda', inplace=True)
    return feature, target",0.541605711,
1737,random forests,"import random, pylab

def run():
    xVals = []
    yVals = []
    wVals = []
    for i in range(1000):
        xVals.append(random.random())
        yVals.append(random.random())
        wVals.append(random.random())
    xVals = pylab.array(xVals)
    yVals = pylab.array(yVals)
    wVals = pylab.array(wVals)
    xVals = xVals + xVals
    zVals = xVals + yVals
    tVals = xVals + yVals + wVals
    return xVals, yVals, wVals, zVals, tVals

xVals, yVals, wVals, zVals, tVals = run()",0.5407778621,
1737,random forests,"def experiment8():
    Ecv_1_list= []
    for i in range(100):
        Ecv_1_list.append(Ecv_calc(1, 5, y_training, x_training, c_value=0.0001,q_value=2))
    Ecv_1 = np.mean(Ecv_1_list)
    return np.abs(Ecv_1)",0.5396164656,
1737,random forests,"def CNN_model():
    model = Sequential()
    model.add(Lambda(lambda x: x/127.5 - 1., input_shape=input_shape))
    model.add(Convolution2D(3,1,1,  border_mode='valid', name='conv0', init='he_normal'))
    model.add(Convolution2D(32,3,3, border_mode='valid', name='conv1', init='he_normal'))
    model.add(ELU())
    model.add(Convolution2D(32,3,3, border_mode='valid', name='conv2', init='he_normal'))
    model.add(ELU())
    model.add(MaxPooling2D(pool_size=(2,2)))
    model.add(Dropout(0.5))
    
    model.add(Convolution2D(64,3,3, border_mode='valid', name='conv3', init='he_normal'))
    model.add(ELU())
    model.add(Convolution2D(64,3,3, border_mode='valid', name='conv4', init='he_normal'))
    model.add(ELU())
    model.add(MaxPooling2D(pool_size=(2,2)))
    model.add(Dropout(0.5))
    
    model.add(Convolution2D(128,3,3, border_mode='valid', name='conv5', init='he_normal'))
    model.add(ELU())
    model.add(Convolution2D(128,3,3, border_mode='valid', name='conv6', init='he_normal'))
    model.add(ELU())
    model.add(MaxPooling2D(pool_size=(2,2)))
    model.add(Dropout(0.5))
    
    model.add(Flatten())
    model.add(Dense(512,name='hidden1', init='he_normal'))
    model.add(ELU())
    model.add(Dropout(0.5))
    model.add(Dense(64,name='hidden2', init='he_normal'))
    model.add(ELU())
    model.add(Dropout(0.5))
    model.add(Dense(16,name='hidden3',init='he_normal'))
    model.add(ELU())
    model.add(Dropout(0.5))
    model.add(Dense(1, name='output', init='he_normal'))
    model.compile(optimizer=""adam"", loss=""mse"")
    return model",0.5375649929,
1737,random forests,"def DrawFromRandoms(mSamples=1, nPoints=5000, Expon=3.5, methRand=np.random.power):
    
    """"""Drawn mSamples samples from distributions of interest.
    
    Example call: Unif, Rand = DrawFromRandoms(5,1000,2.5)""""""
    
    # First set up the holding-arrays:
    AvgUnif = np.zeros(nPoints)
    AvgPowr = np.zeros(nPoints)
    
    # Accumulate the samples
    for iSample in range(mSamples):
        AvgUnif += np.random.uniform(size=nPoints)
        try:
            AvgPowr += methRand(Expon, size=nPoints)
        except:
            AvgPowr += methRand(nPoints)  # for one-parameter methods
        
    # return the averages
    return AvgUnif / np.float(mSamples), AvgPowr/np.float(mSamples)",0.5366305709,
1737,random forests,"def fit_randomized_tree(random_state=0):
    X, y = train
    clf = DecisionTreeClassifier(max_depth=15)
    
    rng = np.random.RandomState(random_state)
    i = np.arange(len(y))
    rng.shuffle(i)
    visualize_tree(clf, X[i[:250]], y[i[:250]], boundaries=False,
                   xlim=(X[:, 0].min(), X[:, 0].max()),
                   ylim=(X[:, 1].min(), X[:, 1].max()))
    
from IPython.html.widgets import interact
interact(fit_randomized_tree, random_state=[0, 100]);",0.5366084576,
2004,step assign it to a variable called,"def save_checkpoint(saver, sess, step): saver.save(sess, TRAIN_DIR, global_step=step)",0.4628934562,
2004,step assign it to a variable called,"def predict(self, X):
    X_transformed = X
    for step in self.steps[:-1]:
        # iterate over all but the final step
        # transform the data
        X_transformed = step[1].transform(X_transformed)
    # predict using the last step
    return self.steps[-1][1].predict(X_transformed)",0.4567223787,
2004,step assign it to a variable called,"def predict(self, X):
    X_transformed = X
    for step in self.steps[:-1]:
        # iterate over all but the final step
        # transform the data
        X_transformed = step[1].transform(X_transformed)
    # fit the last step
    return self.steps[-1][1].predict(X_transformed)",0.4567223787,
2004,step assign it to a variable called,"for i in range(opt_settings.number_of_ascending_steps):
    proxy.param = proxy_for_ode_parameters(proxy.state,locally_linear_ODEs,dC_times_inv_C,symbols.param,simulation.ode_param)
    proxy.state = proxy_for_ind_states(proxy.state,proxy.param,locally_linear_ODEs,dC_times_inv_C,symbols.state,simulation.observed_states,state_couplings)",0.4405198097,
2004,step assign it to a variable called,"def fit(self, X, y):
    X_transformed = X
    for name, estimator in self.steps[:-1]:
        # iterate over all but the final step
        # fit and transform the data
        X_transformed = estimator.fit_transform(X_transformed, y)
    # fit the last step
    self.steps[-1][1].fit(X_transformed, y)
    return self",0.4394158721,
2004,step assign it to a variable called,"calc_flow.steps[""multiply""] = step",0.4384716749,
2004,step assign it to a variable called,"def top_features(pipe, n):
    """"""Prints features with the highest coefficient values, per class""""""
    vectorizer =  pipe.named_steps['vectorize']
    clf =  pipe.named_steps['clf']
    print(clf.coef_.shape)
    feature_names = vectorizer.get_feature_names()
    for i, class_label in enumerate(clf.classes_):
        if i >= clf.coef_.shape[0]:
            break
        top = np.argsort(clf.coef_[i])
        reversed_top = top[::-1]
        print(""%s:\n%s"" % (class_label,
              ""\n"".join(feature_names[j] for j in reversed_top[:n])))",0.438326776,
2004,step assign it to a variable called,"def log_at_step(log_drive, step):
    idx_map = build_idx_map(log_drive)
    
    log_at_step = log_drive[log_drive.step==step]
    print('At step {} (s, d) = ({}, {}) => (x, y) = ({}, {})'.format(
        step,
        log_at_step.iloc[idx_map['next_s_vals']].vals[-1],
        log_at_step.iloc[idx_map['next_d_vals']].vals[-1],
        log_at_step.iloc[idx_map['next_x_vals']].vals[-1],
        log_at_step.iloc[idx_map['next_y_vals']].vals[-1]
    ))
   
    
log_at_step(drive2_df, 692)
log_at_step(drive2_df, 693)",0.436155647,
2004,step assign it to a variable called,"iset = None
for iv2, step_set in exons[iv].steps():
    if iset is None: iset = step_set.copy()
    else: iset.intersection_update(step_set)
            
print(iset)",0.4354816675,
2004,step assign it to a variable called,"def pAssign(df, colName, colVal):
    kwargs = {colName: colVal}
    return df.assign(**kwargs)

def fillNaWithAvg(df, columnName):
    newColumn = df[columnName].fillna(df[columnName].mean())
    return pAssign(df, columnName, newColumn)

def fillNaWithMode(df, columnName):
    newColumn = df[columnName].fillna(df[columnName].mode().iloc[0])
    return pAssign(df, columnName, newColumn)

imputatedData = fillNaWithMode(
    fillNaWithMode(
        fillNaWithAvg(
            fillNaWithMode(
                numberedData,
                'Embarked'
            ),
            'Age'
        ),
        'CabinLetters'
    ),
    'Cabin'
)",0.4313840866,
1755,read numbers until and print their mean and standard deviation without using a list,"print ""Sentiment based classification scheme accuracy: "" + str(round((cm_sentiment_1[0][0]+cm_sentiment_1[1][1])/float((sum(cm_sentiment_1[0])+sum(cm_sentiment_1[1]))), 3))",0.5767229795,
1755,read numbers until and print their mean and standard deviation without using a list,"op_psr,mp_psr,spec_psr,specR_psr = ave_sim(10,[19000,1.8,2e34,2e39,1e-9,
                                               E3fgl,spec_3fgl_psr,spec_diff_lb_3fgl,bins_3fgl,'LorimerC'])
op_snr,mp_snr,spec_snr,specR_snr = ave_sim(10,[10000,1.8,2e34,1e39,1e-9,
                                               E3fgl,spec_3fgl_snr,spec_diff_lb_3fgl,bins_3fgl,'SNRGreen'])",0.5753648877,
1755,read numbers until and print their mean and standard deviation without using a list,"for s in sample_ids:
    x = np.array([np.mean(np.load(stage1_features + s + "".npy""), axis=0)])
    print('Patient {}: array shape: {}'.format(s, x.shape))",0.5718021393,
1755,read numbers until and print their mean and standard deviation without using a list,"arr = []
for arr_i in xrange(6):
    arr_temp = map(int,raw_input().strip().split(' '))
    arr.append(arr_temp)
    
""""""
1 1 1 0 0 0
0 1 0 0 0 0
1 1 1 0 0 0
0 0 2 4 4 0
0 0 0 2 0 0
0 0 1 2 4 0
""""""

maxValue = -999999
for r in range(1,5):
    for c in range(1,5):
        value = sum([arr[r-1][c-1],
                     arr[r-1][c],
                     arr[r-1][c+1],
                     arr[r][c],
                     arr[r+1][c-1],
                     arr[r+1][c],
                     arr[r+1][c+1]])
        if value > maxValue:
            maxValue = value

print maxValue",0.5717350841,
1755,read numbers until and print their mean and standard deviation without using a list,"op1,mp_1,spec1,spec1_R = ave_sim(10,[29000,1.8,2e34,2e39,1e-9,E3fgl,spec_3fgl,spec_diff_lb_3fgl,bins_3fgl])
op2,mp_2,spec2,spec2_R = ave_sim(10,[3000,1.5,4e34,2e39,1e-9,E3fgl,spec_3fgl,spec_diff_lb_3fgl,bins_3fgl])
op3,mp_3,spec3,spec3_R = ave_sim(10,[210000,2.1,1e34,2e39,1e-9,E3fgl,spec_3fgl,spec_diff_lb_3fgl,bins_3fgl])",0.5683574677,
1755,read numbers until and print their mean and standard deviation without using a list,"range_ = (1, 200)
mini_batch_size = 5
for epoch in range(7501):
    for i in range(int(len(seqs_t)/mini_batch_size)):
        loss = 0
        s, e = range_
        for seq2 in random.sample(seqs_t, mini_batch_size):
            dcnet.hidden = dcnet.init_hidden()
            dcnet.zero_grad()
            seq2 = seq2[s-1:e]
            seq_ = seq2.view(-1,4)
            out = dcnet(seq_)
            loss += loss_function(out[:-1], seq_[1:])
        loss.backward()
        optimizer.step()
    if epoch % 250==0:
        print(""epoch:"", epoch, ""loss:"", loss.cpu().data[0]/mini_batch_size, ""learning rate:"", lr)
        lr *= 0.95
        optimizer = optim.SGD(dcnet.parameters(), lr=lr)",0.5680035949,
1755,read numbers until and print their mean and standard deviation without using a list,"print(""Number of Features: {}\nNumber of Organized features: {}""
      .format(len(features),\
              len(cat_features) 
              + len(personality_scores) 
              + len(academic_scores) 
              + len(std_test_scores) 
              + len(domain_scores)))",0.5678747296,
1755,read numbers until and print their mean and standard deviation without using a list,"print(""CIFAR Test -> [Min: %d, Max: %d, Mean: %d]""%
      (min(cifar_h_t[0]), max(cifar_h_t[0]), np.mean(cifar_h_t[0])))
print(""CIFAR Training -> [Min: %d, Max: %d, Mean: %d]""%
      (min(cifar_h_tr[0]), max(cifar_h_tr[0]), np.mean(cifar_h_tr[0])))",0.5678002834,
1755,read numbers until and print their mean and standard deviation without using a list,"print(""MNIST Test -> [Min: %d, Max: %d, Mean: %d]""%
      (min(mnist_h_t[0]), max(mnist_h_t[0]), np.mean(mnist_h_t[0])))
print(""MNIST Training -> [Min: %d, Max: %d, Mean: %d]""%
      (min(mnist_h_tr[0]),max(mnist_h_tr[0]),np.mean(mnist_h_tr[0])))",0.5678002834,
1755,read numbers until and print their mean and standard deviation without using a list,"print(
    'There are {} devices detected which are broken down into: \n \
    - {} shoppers \n \
    - {} non-shoppers \n \
    - {} unclassified'.format(len(mac_address_df), len(shopper_macs), len(non_shopper_macs), len(unclassified_macs))
)",0.5676035881,
360,create arrays,"#we use vector find the different point between 2 picturs that we seperates above
def find_different_point(first_pic,second_pic):
    new_image = np.empty(first_pic)
    keep = []
    data = np.zeros((1,2)) #to make an array
    keep_value_image = np.empty(first_pic)
    for i in range(first_blurred_half.shape[0]): #first dimension of pic
        for j in range(first_blurred_half.shape[1]): #second dimension of pic
            rl = first_blurred_half[i,j,0] #red,left picture
            gl = first_blurred_half[i,j,1] #greed
            bl = first_blurred_half[i,j,2] #blue
            rr = second_blurred_half[i,j,0]
            gr = second_blurred_half[i,j,1]
            br = second_blurred_half[i,j,2]
            distance = ((rl-rr)**2 + (bl-br)**2 + (gl-gr)**2)**0.5 #to find distance to compare between 2 vectors
            if distance > 0.45: # here, if the distance different too much, so it is not the same
                #In this case, we have to find the best value to determine whether the color on that point are the same or not,
                #in this pic we choose 0.45(it depends on picture)
                new = np.array((j,i))
                data = np.vstack([data,new])#we keep the different points
    return data",0.4713198543,
360,create arrays,"def make_data(n_points, n_clusters=2, dim=2, sigma=1):
    x = [[] for i in range(dim)]
    for i in range(n_clusters):
        for d in range(dim):
            x[d].extend([random.gauss(i*3,sigma) for j in range(n_points)])
    return x",0.4645915926,
360,create arrays,"K = 5

def cheby_basis(K, x):
    """"""Return the Chebyshev basis of order K (composed of the
    first K polynomials) evaluated at x. Polynomials are generated
    by their recursive formulation.""""""
    T = np.empty((x.size, K))
    T[:,0] = np.ones(x.size)
    if K >= 2:
        T[:,1] = x
    for k in range(2, K):
        T[:,k] = 2 * x * T[:,k-1] - T[:,k-2]
#    T /= np.linalg.norm(T, axis=0)  # Scaling to unit norm.
    return T

fig = plt.figure(figsize=(15,5))
ax = fig.add_subplot(1,1,1)
x = np.linspace(-1,1,100)
T = cheby_basis(K, x)
for k in range(K):
    ax.plot(x, T[:,k], label='T_{}, E={:.1f}'.format(k, np.linalg.norm(T[:,k])**2))
ax.set_title('Chebyshev polynomials of the first kind')
ax.set_xlabel('x')
ax.set_ylabel('T_n(x)')
ax.set_xlim(-1, 1)
ax.set_ylim(-1, 1.1)
ax.legend(loc='best')
plt.show()",0.4637018442,
360,create arrays,"l = []
    # initialize the list container with 0 - 100
    l[:] = range(100)
    # slice it
    for n in l[20:40]:
        print(""{} "".format(n), end="""")
    print()
    # slice it with a step over
    for n in l[20:40:3]:
        print(""{} "".format(n), end="""")

    # assigning to the slices
    l[20:40:3] = 99,99,99,99,99,99,99
    for n in l:
        print(""{} "".format(n), end="""")",0.4630664587,
360,create arrays,"def reshpaeImage(vect, dim):
    #r = np.reshape(vect[0:1024], (dim, dim))
    #g = np.reshape(vect[1024:2048], (dim, dim))
    #b = np.reshape(vect[2048:3076], (dim, dim))
    #img = np.dstack((r,g,b))
    img = np.dstack((np.reshape(vect[0:1024], (dim,dim)),np.reshape(vect[1024:2048], (dim,dim)), np.reshape(vect[2048:3076], (dim,dim))))
    return img",0.4607591033,
360,create arrays,"a = np.zeros((2,2))  # Create an array of all zeros
print a              # Prints ""[[ 0.  0.]
                     #          [ 0.  0.]]""
    
b = np.ones((1,2))   # Create an array of all ones
print b              # Prints ""[[ 1.  1.]]""

c = np.full((2,2), 7) # Create a constant array
print c               # Prints ""[[ 7.  7.]
                      #          [ 7.  7.]]""

d = np.eye(2)        # Create a 2x2 identity matrix
print d              # Prints ""[[ 1.  0.]
                     #          [ 0.  1.]]"" 

e = np.random.random((2,2)) # Create an array filled with random values
print e 

y = np.array([4,5,6]) 
f = np.diag(y)
print f",0.4599978924,
360,create arrays,"def calibrate_camera(images, grid_size, image_size):
    objpoints = []
    imgpoints = []

    objp = np.zeros((grid_size[0] * grid_size[1], 3), np.float32)
    objp[:,:2] = np.mgrid[0:grid_size[0], 0:grid_size[1]].T.reshape(-1, 2)

    for filename in images:
        img = mpimg.imread(filename)
        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)

        ret, corners = cv2.findChessboardCorners(gray, grid_size, None)
        if ret:
            imgpoints.append(corners)
            objpoints.append(objp)
        else:
            print(""Unable to find appropriate number of corners on "" + filename)

    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, image_size, None, None)

    return mtx, dist",0.4597392678,
360,create arrays,"# Define a function for bootstrapping
def bootstrap(cat, num):
    '''Returns [num] bootstrap means of specified category'''
    # Create [num] instances of empty arrays for low_train, high_train to contain the bootstrap samples
    low_train_bootstrap = np.empty(num)
    high_train_bootstrap = np.empty(num)

    for i in range(num):
        # Define the bootstrap samples and store them in the empty arrays 
        low_train_bootstrap[i] = np.mean(np.random.choice(low_train[cat], len(low_train)))
        high_train_bootstrap[i] = np.mean(np.random.choice(high_train[cat], len(high_train)))
        
    return low_train_bootstrap, high_train_bootstrap",0.4592866898,
360,create arrays,"def add_cumulative(df):
    
    df['Cumulative Oil Production Volumes']   = np.zeros(len(df))
    df['Cumulative Gas Production Volumes']   = np.zeros(len(df))
    df['Cumulative Water Production Volumes'] = np.zeros(len(df))
    
    for field in df.FIELDNAME.unique():
        print('Working with {}...'.format(field.encode('utf-8')))
        df_field = df[df['FIELDNAME']==field]

        df_field = df_field.drop(labels='FIELDNAME', axis=1)        
        df_field.PERIODDATE = pd.to_datetime(df_field.PERIODDATE)

        df_field_cumulative = df_field.sort_values(by='PERIODDATE',
                                                   axis=0, ascending=True)[['Oil Production Volumes',
                                                                            'Gas Production Volumes',
                                                                            'Water Production Volumes']].cumsum(axis=0)
        #display(df_field_cumulative.head(10))

        df_field_cumulative.rename(columns={'Oil Production Volumes': 'Cumulative Oil Production Volumes', 
                                            'Gas Production Volumes': 'Cumulative Gas Production Volumes',
                                            'Water Production Volumes': 'Cumulative Water Production Volumes'}, inplace=True)
        #display(df_field_cumulative.head(10))

        df.loc[df['FIELDNAME']==field,'Cumulative Oil Production Volumes'] = \
            df_field_cumulative['Cumulative Oil Production Volumes']

        df.loc[df['FIELDNAME']==field,'Cumulative Gas Production Volumes'] = \
            df_field_cumulative['Cumulative Gas Production Volumes']

        df.loc[df['FIELDNAME']==field,'Cumulative Water Production Volumes'] = \
            df_field_cumulative['Cumulative Water Production Volumes']
    
    return(df)",0.45801422,
360,create arrays,"def createSample(size):
    input = np.zeros(shape=(size))
    output = np.zeros(shape=(size))
    state = 0
    rand = np.array([random() for _ in range(size)])
    for i in range(size):
        if rand[i]>0.95:
            input[i] = 1
            output[i] = state
            state = (state + 1) % 2
        else:
            input[i] = 0
            output[i] = state
    sample = pd.DataFrame({'input': input, 'output': output})
    return sample",0.4539582729,
2580,what s the probablity of weight loss with diet b? what s the odds?,"optimizer = tf.train.AdamOptimizer()

# We are masking the loss calculated for padding
def loss_function(real, pred):
    mask = 1 - np.equal(real, 0)
    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask
    return tf.reduce_mean(loss_)",0.5292160511,
2580,what s the probablity of weight loss with diet b? what s the odds?,"import tensorflow as tf
def dice_score(y_true, y_pred, eps=1e-7):
    """"""
    y_true: Tensor, representing ground truth 
    y_pred: Tensor, same shape as y_true, representing predicted values
    
    Finds dice score for multi-dimensional tensors
    """"""
    inter = tf.reduce_sum(tf.multiply(y_true, y_pred))
    denom = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred)
    return (2*inter + eps)/(denom + eps)

def dice_loss(y_true, y_pred, eps=1e-7):
    return -dice_score(y_true, y_pred, eps)",0.5171989799,
2580,what s the probablity of weight loss with diet b? what s the odds?,"loss = gloss.SoftmaxCrossEntropyLoss()

def evaluate_loss(data_iter, net, ctx):
    l_sum, n = 0.0, 0
    for X, y in data_iter:
        y = y.as_in_context(ctx)
        output_features = net.features(X.as_in_context(ctx))
        outputs = net.output_new(output_features)
        l_sum += loss(outputs, y).sum().asscalar()
        n += y.size
    return l_sum / n",0.5145871639,
2580,what s the probablity of weight loss with diet b? what s the odds?,"def accuracy(predictions, labels):
    return 1 - tf.reduce_mean(tf.square(predictions - labels))",0.5117017031,
2580,what s the probablity of weight loss with diet b? what s the odds?,"def feature_reconstruction_loss(content_img_features, output_img_features):
    """"""Takes a tensor representing a layer of VGG features from the content image
    and a tensor representing a layer of VGG features from the current output image and returns a loss value.
    """"""
    #double check this, kinda sus
    return tf.reduce_sum(tf.square(content_img_features - output_img_features))",0.5112632513,
2580,what s the probablity of weight loss with diet b? what s the odds?,"# Computes the content cost
def compute_content_cost(A_content_in, A_generated_in):
    return 0.5 * tf.reduce_mean(tf.square(A_content_in - A_generated_in))",0.5112395883,
2580,what s the probablity of weight loss with diet b? what s the odds?,"y_ground_truth = np.array([3, 5, 2, 1])
y_predict = np.array([3, 5, 3, 1])

err_mce = sklearn.metrics.zero_one_loss(y_ground_truth, y_predict)

print(""MCE = %2.3f"" % err_mce)",0.50866431,
2580,what s the probablity of weight loss with diet b? what s the odds?,"def define_cost_function(y, y_tensor, batch_size):
    cost = -tf.reduce_sum(y_tensor * tf.log(y), name='cross_entropy') / batch_size
    return cost",0.5082287788,
2580,what s the probablity of weight loss with diet b? what s the odds?,"def compute_cost(pred, Y):
    """"""
    
    Arguments:
    - pred: Y hat that computed using softmax classifier, size of C * m
    - Y: the true labels, size of C * m
    
    notice: 
    the number of classes C: 10
    when calculate the loss, sum over the 10 classes hence along the column
    """"""
   
    cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(pred), reduction_indices = 0))
    
    return cost",0.508169055,
2580,what s the probablity of weight loss with diet b? what s the odds?,"def loss_fn(preds, r): 
    # pred is output from neural network, a is action index
    # r is return (sum of rewards to end of episode), d is discount factor
    return -torch.sum(r * torch.log(preds)) # element-wise multipliy, then sum",0.5074841976,
1359,parameterizing lines,"#fix the lr
#print dir(net_param.layer[0])
for ith_layer in net_param.layer:
    if ith_layer.type == 'InnerProduct' :
        break
    else :
        for params in ith_layer.param:
            if params:
                params.lr_mult = 0

#set the data layer 
net_param.layer[0].data_param.source = '/ssd/zhangdanfeng/char_data/train_bank_card_chars/step2/img_train_lmdb'
net_param.layer[0].transform_param.mean_file = '/ssd/zhangdanfeng/char_data/train_bank_card_chars/step2/mean.binaryproto'
net_param.layer[1].data_param.source = '/ssd/zhangdanfeng/char_data/test_real_bankcard/img_test_lmdb'
net_param.layer[1].transform_param.mean_file = '/ssd/zhangdanfeng/char_data/test_real_bankcard/mean.binaryproto'

#renamne the innerproduct layer
net_param.layer[-3].name = net_param.layer[-3].name + '/new'
net_param.layer[-6].name = net_param.layer[-6].name + '/new'
net_param.layer[-7].name = net_param.layer[-7].name + '/new'
#eval exec

#write protobuf to file
with tempfile.NamedTemporaryFile(delete=False) as net_f:
    net_f.write(str(net_param))
print net_f.name",0.4355233312,
1359,parameterizing lines,"for text_line in letters[:25]:
    stripped_line = text_line.strip() # saving to a variable!
    is_box = stripped_line.startswith(""Box "")
    if is_box == True:
        print(stripped_line)
    elif text_line == '\n': #remember that stripped lines will remove \n
        print(""blank line"")
    else:
        print(""---"", stripped_line)",0.4302765727,
1359,parameterizing lines,"for text_line in letters[:25]:
    stripped_line = text_line.strip() # saving to a variable!
    is_box = stripped_line.startswith(""Box "")
    if is_box == True:
        print(stripped_line)
    else:
        print(""---"")",0.4302765727,
1359,parameterizing lines,"for text_line in letters[:25]:
    stripped_line = text_line.strip() # saving to a variable!
    is_box = stripped_line.startswith(""Box "")
    if is_box == True:
        print(stripped_line)
    elif text_line == '\n': #remember that stripped lines will remove \n
        continue
    else:
        print(""---"", stripped_line)",0.4302765727,
1359,parameterizing lines,"for line in lines:
    # separate into words
    line_list = line.split()
    
    if line_list[0] == ""toggle"":
        # assign range from line to x1,y1,x2,y2
        x1 = int(line_list[1].split(',')[0])
        y1 = int(line_list[1].split(',')[1])
        x2 = int(line_list[3].split(',')[0])
        y2 = int(line_list[3].split(',')[1])
        
        # change values for keys where x1 <= x <= x2 AND y1 <= y <= y2
        for item in grid:
                if item[0] <= x2 and item[0] >= x1 and item[1] <= y2 and item[1] >= y1:
                    if grid[item] == True:
                        grid[item] = False
                    else:
                        grid[item] = True
        
    else:
        # assign range from line to x1,y1,x2,y2
        x1 = int(line_list[2].split(',')[0])
        y1 = int(line_list[2].split(',')[1])
        x2 = int(line_list[4].split(',')[0])
        y2 = int(line_list[4].split(',')[1])
    
        if line_list[1] == ""on"":
            # value == True for keys where x1 <= x <= x2 AND y1 <= y <= y2
            for item in grid:
                if item[0] <= x2 and item[0] >= x1 and item[1] <= y2 and item[1] >= y1:
                    grid[item] = True
        
        elif line_list[1] == ""off"":
            # value == False for keys where x1 <= x <= x2 AND y1 <= y <= y2
            for item in grid:
                if item[0] <= x2 and item[0] >= x1 and item[1] <= y2 and item[1] >= y1:
                    grid[item] = False",0.4300015271,
1359,parameterizing lines,"class Line():
    def __init__(self):
        # Was the line detected in the last iteration?
        self.detected = False

        # Number of iterations to smooth over
        self.n = 3

        # Most recent coefficients
        self.current = [0, 0, 0]

        # Difference in coefficients between last and new fit
        self.diffs = [0, 0, 0]

        # Polynomial coefficients with length n
        self.A = []
        self.B = []
        self.C = []

        # Average coefficients from last n iterations
        self.avg_A = 0
        self.avg_B = 0
        self.avg_C = 0

        # Radius of curvature
        self.rad_of_curve = None

    def add_avg_n_coefficients(self, A, B, C, radius):
        # Most recent coefficients
        self.current = [A, B, C]

        # Difference in coefficients between last n and new
        self.diffs = [A - self.avg_A, B - self.avg_B, C - self.avg_C]

        # Add new coeffiecients to array
        self.A.append(A)
        self.B.append(B)
        self.C.append(C)

        # Remove oldest coefficient if number of stored coefficients is greater than n
        if len(self.A) > self.n:
            self.A.pop(0)
            self.B.pop(0)
            self.C.pop(0)

        # Update average of coefficients
        self.avg_A = np.mean(self.A)
        self.avg_B = np.mean(self.B)
        self.avg_C = np.mean(self.C)

        # Radius of curvature
        self.rad_of_curve = radius
        return(self.avg_A, self.avg_B, self.avg_C)

    def get_coefficients(self):
        return(self.avg_A, self.avg_B, self.avg_C, self.diffs, self.rad_of_curve)",0.4260956943,
1359,parameterizing lines,"def feature_handling_and_value_cleanup(df, intruction_list):
    
    for i, instruction in enumerate(intruction_list):
     
        if instruction[1] == 'None':
            
            pass
        
        elif instruction[1] == 'Mean':
            
            mean = df[instruction[0]].mean()
            
            df[instruction[0]] = df[instruction[0]].fillna(mean)
                                                         
        elif instruction[1] == 'Median':
            
            median = df[instruction[0]].median()
            
            df[instruction[0]] = df[instruction[0]].fillna(median)
        
        elif instruction[1] == 'Drop':
            
            df = df.drop([feature], axis = 1)
            
        elif instruction[1] == 'conv_to_categorical':
        
            merged_data[feature] = merged_data[feature].astype(str)
            
        else:
    
            df[instruction[0]] = df[instruction[0]].fillna(instruction[1])
    
    return df

intruction_list = [('Alley','NA'),
                    ('BldgType','None'),
                    ('BsmtCond','NA'),
                    ('BsmtExposure','NA'),
                    ('BsmtFinType1','NA'),
                    ('BsmtFinType2','NA'),
                    ('BsmtQual','NA'),
                    ('CentralAir','None'),
                    ('Condition1','None'),
                    ('Condition2','None'),
                    ('Electrical','SBrkr'),
                    ('ExterCond','None'),
                    ('ExterQual','None'),
                    ('Exterior1st','VinylSd'),
                    ('Exterior2nd','VinylSd'),
                    ('Fence','NA'),
                    ('FireplaceQu','NA'),
                    ('Foundation','None'),
                    ('Functional','Typ'),
                    ('GarageCond','NA'),
                    ('GarageFinish','NA'),
                    ('GarageQual','NA'),
                    ('GarageType','NA'),
                    ('Heating','None'),
                    ('HeatingQC','None'),
                    ('HouseStyle','None'),
                    ('KitchenQual','TA'),
                    ('LandContour','None'),
                    ('LandSlope','None'),
                    ('LotConfig','None'),
                    ('LotShape','None'),
                    ('MSZoning','RL'),
                    ('MasVnrType','NA'),
                    ('MiscFeature','NA'),
                    ('Neighborhood','None'),
                    ('PavedDrive','NA'),
                    ('PoolQC','NA'),
                    ('RoofMatl','None'),
                    ('RoofStyle','None'),
                    ('SaleCondition','None'),
                    ('SaleType','WD'),
                    ('Street','None'),
                    ('Utilities','AllPub')]

merged_data = feature_handling_and_value_cleanup(merged_data, intruction_list)",0.4247963428,
1359,parameterizing lines,"# add new column that represents the year of birth
for b in legislators:
    try:
        birth_year = int(b[2].split('-')[0])
    except Exception:
        birth_year = 0
    b.append(birth_year)
print(legislators)",0.4228480756,
1359,parameterizing lines,"def find_max(l):
    if len(l) == 1:
        return l[0][0]
    percentages = [float(subl[1].strip('%')) for subl in l]
    i = percentages.index(max(percentages))
    return l[i][0]",0.4221519828,
1359,parameterizing lines,"class MLP:
    """"""
    Multi-layer perceptron class.
    """"""
    def __init__(self, desc):
        self.layers = desc
        self.reshaped = False
        
        # Here we aggregate all tunable parameters into a single list.
        self.params = []
        for l in self.layers:
            try:
                for v in l.params.values():
                    self.params.append(v)
            except Exception, e:
                pass
    
    def reshape(self, input_shape):
        """"""
        Invokes reshape methods of all layers.
        """"""
        batch_size = input_shape[0]
        self.data = {
            'data': np.zeros(input_shape), 
            'diff': np.zeros(input_shape)
        }
        self.labels = {
            'data': np.zeros((batch_size, 1), dtype=np.int32)
        }
        
        self.blobs = [] # Holds all internal blobs created by reshape methods.
        
        # Your code goes here. ################################################ 
        self.blobs.append(self.data)
        for layer in desc:
            new_bottom = [{'data': None, 'diff': None}]
            layer.reshape(self.blobs[-1:], new_bottom)
            self.blobs.append(new_bottom[0])
        # Your code goes here. ################################################ 
        
        self.reshaped = True
    
    def set_input(self, data, labels):
        if not self.reshaped:
            self.reshape(data.shape)
            
        # Populate self.data and self.labels.
        # Your code goes here. ################################################
        self.data = {'data':data, 'diff':np.zeros(data.shape)}
        self.labels = {'data':labels}
        # Your code goes here. ################################################
        
    def fprop(self):
        """"""
        Conducts forward-propagation through the network.
        
        (i.e. fills self.blobs[:]['data'])
        """"""
        # Your code goes here. ################################################ 
        for i in xrange(len(self.layers) - 1):
            self.layers[i].fprop(self.blobs[i:i+1], self.blobs[i+1:i+2])
        self.layers[-1].fprop([self.blobs[len(self.layers) - 1], self.labels], \
                               self.blobs[len(self.layers):])
        # Your code goes here. ################################################ 
        # NOTE: Keep in mind that the last layer should receive ground-truth 
        #       labels as well as blob from the lower layer.
        
    def bprop(self):
        """"""
        Conducts backward-propagation through the network.
        
        (i.e. fills self.blobs[:]['diff'] and updates 'diff's of the internal 
        weight blobs)
        """"""
        # Your code goes here. ################################################ 
        self.layers[-1].bprop(self.blobs[len(self.layers):], [self.blobs[len(self.layers)-1], self.labels])
        for i in xrange(1, len(self.layers)):
            self.layers[len(self.layers) - 1 - i].bprop(\
                                    self.blobs[len(self.blobs) - 1 - i:len(self.blobs) - i], \
                                    self.blobs[len(self.blobs) - 2 - i:len(self.blobs) - 1 - i])
        for i in xrange(len(self.params)):
            self.params[i]['data'] -= self.params[i]['diff']
        # Your code goes here. ################################################ 
        
    def get_loss(self):
        """""" Return the value of the objective function """"""
        return self.blobs[-1]['data'].mean()
    
    def test(self, data, labels):
        """"""
        Helper function for evaluating the performance of the network on a test
        set (which can be larger than the batch size).
        
        Returns accuracy.
        """"""
        
        batch_size = self.data['data'].shape[0]
        preds = []
        for start in xrange(0, data.shape[0], batch_size):
            self.set_input(data[start : start + batch_size, :], labels[start : start + batch_size])
            self.fprop()
            preds += [self.layers[-1].probs.argmax(axis=1)]
        preds = np.hstack(preds)
        return np.mean(preds == labels)",0.4219776094,
479,dimensionality reduction with pca,"from bbflow.reduction import reduce
projection, lengths = reduce(case, ensemble, decomp, threshold=1e-6)
projection.shape",0.5391228199,
479,dimensionality reduction with pca,"from gmmlib import dimReducePCA

# reduce the dimension of data to 2 using PCA
mnistData2D, eigenVectors, eigenValues = dimReducePCA(mnistData, 2)",0.5211576223,
479,dimensionality reduction with pca,pca = decomposition.PCA(n_components=3),0.5182390213,
479,dimensionality reduction with pca,"make_mask(p=(1,0.5,0.2,1), shape=(2,5,10,1))",0.5161438584,
479,dimensionality reduction with pca,d_pca = decomposition.PCA(n_components=2) # reduce 4 dimension to 2 dimension,0.5157512426,
479,dimensionality reduction with pca,pca = decomposition.PCA(n_components=2),0.5157512426,
479,dimensionality reduction with pca,"from gmmlib import dimReducePCA

# reduce the dimension of data to 2 using PCA
mnistData2D, eigenVectors, eigenValues = dimReducePCA(mnistData, 2)
showImageSamples(eigenVectors.T, 9, random=False)",0.5155387521,
479,dimensionality reduction with pca,"# Building the 3D model
pc.log_.level = 3
m3d = pc.C3D(Models, dims = dim, angles = [45,0,0], plan_sym = True)",0.5127607584,
479,dimensionality reduction with pca,"def pca(X,numDims):
    # YOUR CODE GOES HERE",0.5117509365,
479,dimensionality reduction with pca,"f2reproj, f2footprint = reproject.reproject_exact((f2d, f2wcs), f1wcs, shape_out=f1d.shape, parallel=True)",0.5109402537,
277,concatenate the three images into one point,"def flatten(arr):
    return reduce(lambda x, y: np.concatenate((x, y)), arr)",0.458424896,
277,concatenate the three images into one point,"# Answer
def max_projection(image, filename):
    projection = np.max(image[0, 0, :, :, :,0], axis = 0)
    im = Image.fromarray(projection)
    im.save(f'{filename}_MIP.tif')",0.4543667436,
277,concatenate the three images into one point,"# Answer
def max_project(image):
    projection = np.max(image[0, 0, :, :, :, 0], axis = 0)
    im = Image.fromarray(projection)
    im.save('MIP.tif')
    return projection
max_project(my_image)",0.4502451122,
277,concatenate the three images into one point,"# Since openCV loads the data in BGR instead of RGB, we convert them to RGB
def convert_to_rgb(image):
    b,g,r = cv2.split(image)
    return cv2.merge([r,g,b])

# We also resize the images to 32x32 images
def resize_to_32(image):
    return cv2.resize(image, (32, 32))
    
new_images = [resize_to_32(convert_to_rgb(image)) for image in new_images]
new_labels = [20, 14, 14, 8,20]",0.4501636028,
277,concatenate the three images into one point,"def append_h_flipped(images):
    flipped = np.copy(images)[:, :, ::-1, :]
    return np.concatenate((images, flipped), axis=0)",0.4469529688,
277,concatenate the three images into one point,"def f(v, t0, k):
    # v has four components: v=[u, u'].
    u, udot = v[:2], v[2:]
    # We compute the second derivative u'' of u.
    udotdot = -k/m * udot
    udotdot[1] -= g
    # We return v'=[u', u''].
    return np.r_[udot, udotdot]",0.4457704127,
277,concatenate the three images into one point,"def gray_scale(image):
    # Get RGB individual values
    R, G, B = image[...,0], image[...,1], image[...,2]

    # Get gray scale from RGB colors: PIX = 0.299 R + 0.587 G + 0.114 B 
    pixels = np.array(0.299*R + 0.587*G + 0.114*B, dtype=np.int)

    # Replace each channel by this gray scale
    im_gs = np.stack([pixels, pixels, pixels], axis=2)
    
    return im_gs
    
# Plot the result
plot_image(gray_scale(image))",0.4423363805,
277,concatenate the three images into one point,"def CNNcodes(folder,filename):
    path = folder + filename;
    img_path = path
    img = image.load_img(img_path, target_size=(224, 224))
    x = image.img_to_array(img)
    x = np.expand_dims(x, axis=0)
    x = preprocess_input(x)
    z=model_vgg16_conv.predict(x)
    return(z.flatten())",0.4420622587,
277,concatenate the three images into one point,"def save_new_image(out_grid, filename):
    flat_pixels = sum(out_grid, [])  # flatten the grid by adding all rows together
    new_image = my_image.copy()      # create a new Image
    new_image.putdata(flat_pixels)   # save the pixel list in the new Image
    new_image.save(filename)         # save the Image to file
    return DisplayImage(filename)    # return a version to display in Jupyter",0.4416792989,
277,concatenate the three images into one point,"def bgr2rgb(img):
    b,g,r = cv.split(img)
    return cv.merge([r,g,b])",0.4414414763,
1990,standardize year column,"def extract_date_variables(input_data, date_in_index=False):

    if date_in_index:
        
        #input_data['weeknr'] = map(lambda x: str(x), input_data.index.week)
        input_data['year'] = map(lambda x: str(x), input_data.index.year)
        input_data['month'] = map(lambda x: str(x), input_data.index.month)

        #input_data['week_year'] = input_data['weeknr'] + '_' + input_data['year']
        input_data['month_year'] = input_data['month'] + '_' + input_data['year']

        return input_data
    
    else:
        output_data = input_data.to_frame(name='timestamp')
        
        #output_data['weeknr'] = map(lambda x: str(x.week), output_data['timestamp'])
        output_data['year'] = map(lambda x: str(x.year), output_data['timestamp'])
        output_data['month'] = map(lambda x: str(x.month), output_data['timestamp'])

        #output_data['week_year'] = output_data['weeknr'] + '_' + output_data['year']
        output_data['month_year'] = output_data['month'] + '_' + output_data['year']

        return output_data",0.5689327121,
1990,standardize year column,"#Now we're going to process our dates
def process_dates():

    #Create column with years in integer before dropping the string version
    data['intYear'] = data.Year.astype(int)
    predictions['intYear'] = predictions.Year.astype(int)

    #Now we drop all the variables that we don't need
    data.drop('Year', axis = 1, inplace = True)
    data.drop('Month', axis = 1, inplace = True)
    
    predictions.drop('Year', axis = 1, inplace = True)
    predictions.drop('Month', axis = 1, inplace = True)
    
    status('Dates')

process_dates()",0.5409179926,
1990,standardize year column,"def handleOutlierAge(df):
    df['age']=df['age'].apply(lambda x: datetime.now().year-x if x>1900 else x)
    
    #Valid age range between 14 to 90 as per data, otherwise check if its outlier or not
    df['age']=df['age'].apply(lambda x: x if 14<=x<=90 else np.nan)     
    mean = df['age'].mean()
    mean = int(mean)
    df['age']=df['age'].apply(lambda x: mean if np.isnan(x) else x) 
    return df",0.5388456583,
1990,standardize year column,"def recode_dfp(df):
    df['Collection Year'] = pd.DatetimeIndex(df.Date).year
    df = df.rename(columns={""Village"": ""Location Code"", ""Fly_Number"": ""Fly Number""})
    return df[[""Location Code"",""Collection Year"",""Fly Number"",""Sex""]]",0.5367069244,
1990,standardize year column,"def preprocess(df):
    df['sale_year'] = pd.to_datetime(df['saledate']).apply(lambda x: x.year)
    df['age'] = df['sale_year'] - df['YearMade']

preprocess(df_train)
preprocess(df_test)",0.5314382315,
1990,standardize year column,"def cleanup_country(data, region):
    if 'Country' in data.columns.values:
        data.loc[pd.isnull(data['Country']), 'Country'] = 'United Arab Emirates' # replace blank countries with UAE
        data.loc[data['Country']=='', 'Country'] = 'United Arab Emirates' # replace blank countries with UAE
        data.loc[data['Country']==' ', 'Country'] = 'United Arab Emirates' # replace blank countries with UAE
        
        data = data.merge(region, left_on = 'Country', right_on = 'country', how = 'left')
        no_region = data.loc[pd.isnull(data['region_2'])]
        print('\n Region Summary\n')
        print(data['region_2'].value_counts())
        print('\nNumber of No Region -- '+ str(len(no_region)))
        return data, no_region
    else:
        print('There is no Country column')",0.5271151066,
1990,standardize year column,"dateCol = 'project_submitted_datetime'
def getTimeFeatures(T):
    T['year'] = T[dateCol].apply(lambda x: x.year)
    T['month'] = T[dateCol].apply(lambda x: x.month)
    T['day'] = T[dateCol].apply(lambda x: x.day)
    T['dow'] = T[dateCol].apply(lambda x: x.dayofweek)
    T['hour'] = T[dateCol].apply(lambda x: x.hour)
    T['days'] = (T[dateCol]-T[dateCol].min()).apply(lambda x: x.days)
    return T

T[dateCol] = pd.to_datetime(T[dateCol])
T = getTimeFeatures(T)

P_tar = T[T.tr==1][target].mean()
timeFeatures = ['year', 'month', 'day', 'dow', 'hour', 'days']
for col in timeFeatures:
    Stat = T[['id', col]].groupby(col).agg('count').rename(columns={'id':col+'_stat'})
    Stat /= Stat.sum()
    T = T.join(Stat, on=col)
    statFeatures.append(col+'_stat')

numFeatures += timeFeatures
numFeatures += statFeatures",0.5256652832,
1990,standardize year column,"def process_build_year(data):
    
    data['build_year']=data.build_year.apply(lambda x: -10000 if x<1800 or np.isnan(x) else x)
    return data
temp_merged=process_build_year(temp_merged)",0.5253385305,
1990,standardize year column,"def prepareDesignMatrix(x):
    x['M'] = x.index.month
    for elem in x['M'].unique():
        x['M'+str(elem)] = (x['M'] == elem).astype(int)
    x['c'] = 1.
    return x.drop('M', axis=1)
cohort_over_time = cohort_over_time[cohort_over_time.index >= dt.datetime(2010,2,1)]
cohort_over_time = prepareDesignMatrix(cohort_over_time)
res = OLS(cohort_over_time['Customer ID'], cohort_over_time[['M1','M2','M3','M4','M5','M6','M7','M8','M9','M10','M11','M12','c']]).fit()
print (res.summary())
cohort_over_time['predicted'] = res.predict()
f_cohort = pd.DataFrame(None, index= pd.date_range(cohort_over_time.index.max(), periods=5*12, freq='M'))
f_cohort = prepareDesignMatrix(f_cohort)
f_cohort['forecast'] = res.predict(f_cohort)
cohort_over_time['predicted'].plot(style='--')
cohort_over_time['Customer ID'].plot(legend=None)
ax = f_cohort['forecast'].plot(style='--', figsize=(12,4))
ax.set_xlabel('Date'); ax.set_ylabel('New Customer Cohort Size')",0.5237262249,
1990,standardize year column,"def basic_analysis(df,year):
    
    if year == 2015:
        df_year = df.loc[:,['name','2015_discharges','2015_ranking']]
    else:
        df_year = df.loc[:,['name','2014_discharges','2014_ranking']]

   # print(df_year.shape)

    df_year = df_year.sort_values(['2015_discharges'], ascending=[False])

  #  print(df_year.iloc[:10,:])
    
    print(""{0:50}"".format(df_year.columns[0]),'   ',\
          ""{0:7}"".format(df_year.columns[1]),'   ',""{0:7}"".format(df_year.columns[2]))


    for k in range(30):
        print(""{0:3}"".format(k+1),'   ',""{0:50}"".format(df_year.iloc[k,0]),'   ',\
          ""{0:7}"".format(df_year.iloc[k,1]),'   ',""{0:12}"".format(df_year.iloc[k,2]))

    return


df_cases = pd.read_csv('101 cleaned with names.csv')
df_cases.columns
#print(df_cases.shape)

basic_analysis(df_cases,2015)",0.5232091546,
1954,some notes on syntax,"def name(args,...):
    instruction 1
    instruction 2
    ...
    instruction n
    optional return address",0.4195318818,
1954,some notes on syntax,"class FiveCardHand(Enum):
    """"""Types of five-card poker hands ordered ascending by hand strength.""""""
    NOTHING = 0
    ONE_PAIR = 1
    TWO_PAIR = 2
    THREE_KIND = 3
    STRAIGHT = 4
    FLUSH = 5
    FULL_HOUSE = 6
    FOUR_KIND = 7
    STRAIGHT_FLUSH = 8
    ROYAL_FLUSH = 9",0.4125510752,
1954,some notes on syntax,"class AddressTypeParser(object):
    
    ALLEY = 0
    AVENUE = 1
    HIGHWAY = 2
    LANE = 3
    PLAZA = 4
    STAIRWAY = 5
    STREET = 6
    WALK = 7
     
    address_types_prefixes = {
        'I-'   : HIGHWAY,
    }
    
    address_types_suffixes = {
        'AL'   : ALLEY,
        'AV'   : AVENUE,
        'BL'   : AVENUE,
        'WAY'  : AVENUE,
        'WY'   : AVENUE,
        'EX'   : HIGHWAY,
        'HWY'  : HIGHWAY,
        'HY'   : HIGHWAY,
        'LN'   : LANE,
        'PZ'   : PLAZA,
        'STWY' : STAIRWAY,
        'CR'   : STREET,
        'CT'   : STREET,
        'DR'   : STREET,
        'PARK' : STREET,
        'PL'   : STREET,
        'RD'   : STREET,
        'RW'   : STREET,
        'ST'   : STREET,
        'TER'  : STREET,
        'TR'   : STREET,
        'WK'   : WALK
    }
    
    def __init__(self):
        pass
    
    def flag_address_types_vector_with_street_type(self, address_types_vector, street):
        address_type_found = False
        for address_type in self.address_types_suffixes:
            address_type_found = (street.endswith(address_type))
            if (address_type_found):
                address_types_vector[self.address_types_suffixes[address_type]] = 1
                break
        if (not address_type_found):
            for address_type in self.address_types_prefixes:
                address_type_found = (street.startswith(address_type))
                if (address_type_found):
                    address_types_vector[self.address_types_prefixes[address_type]] = 1
                    break
        
    
    def get_distinct_binary_address_types_vector(self, input_address):
        address_types_vector = [0] * 8
        intersection_types_vector = [0] * 8
        
        if (' / ' in input_address):
            intersections = input_address.split(' / ')
            street_1 = intersections[0]
            street_2 = intersections[1]
            self.flag_address_types_vector_with_street_type(address_types_vector, street_1)
            self.flag_address_types_vector_with_street_type(intersection_types_vector, street_2)
        else:
            self.flag_address_types_vector_with_street_type(address_types_vector, input_address)
        
        return address_types_vector + intersection_types_vector
    
    def get_binary_address_types_vector(self, input_address):
        address_types_vector = [0] * 8
        intersection_types_vector = [0] * 8
        
        if (' / ' in input_address):
            intersections = input_address.split(' / ')
            street_1 = intersections[0]
            street_2 = intersections[1]
            self.flag_address_types_vector_with_street_type(address_types_vector, street_1)
            self.flag_address_types_vector_with_street_type(intersection_types_vector, street_2)
        else:
            self.flag_address_types_vector_with_street_type(address_types_vector, input_address)
            self.flag_address_types_vector_with_street_type(intersection_types_vector, input_address)
        
        return address_types_vector + intersection_types_vector
    
    def get_intersection(self, input_address):
        if ' / ' in input_address:
            return 1
        else:
            return 0",0.4066842794,
1954,some notes on syntax,"# test eval_list()
def test_eval_list():
    print('expect 1: ', eval_list(1))
    print('expect 2: ', eval_list([2]))
    print('expect 3: ', eval_list([[3]]))
    
    print('expect 4: ', eval_list(['+', 4]))
    print('expect -5: ', eval_list(['-', 5]))
    print('expect 6: ', eval_list(['-', ['-', 6]]))
    
    print('expect 2: ', eval_list([1, '+', 1]))
    print('expect 6: ', eval_list([[1, '+', 2], '+', 3]))
    print('expect -6: ', eval_list(['-', [[1, '+', 2], '+', 3]]))
    
    # error test
    #print(eval_list(['*', 3]))
    #print(eval_list([1, '$', 1]))
    #print(eval_list([1, '+', 1, '+', 1]))
    #print(eval_list([]))
    
# test_eval_list()",0.4049096107,
1954,some notes on syntax,"def num_passes(B, N, P):
    # YOUR CODE HERE",0.402978301,
1954,some notes on syntax,"def print5():
    print(1)
    print(2)
    print(3)
    print(4)
    print(5)",0.4007587433,
1954,some notes on syntax,"@nandpp
def ANDPP():
    Y[0] = IF(Yvalid[0],Y[0],one(X[0]))
    Yvalid[0] = one(X[0])
    ignore = OR(Visited[i],NOT(Xvalid[i]))
    Y[0] = AND(OR(ignore,X[i]),Y[0])
    Visited[i] = one(X[0])
    loop = COPY(Xvalid[i])",0.4005927742,
1954,some notes on syntax,"# Here is the import statement for the math module. Once done all cells below will
# have super math powers!

import math

# A couple of examples:

print ""Pi is equal to"", math.pi             # This 'comma trick' for printing is useful in examples
print ""Cosine of 2 is"", math.cos(2.0)

# To get a list of tools from the math module:

print dir(math)",0.4004074037,
1954,some notes on syntax,"# If you're not familiar with syntax highlighting...
# note the red color indicates something is wrong.  
a = [2, 1, 3.1)",0.4001737237,
1954,some notes on syntax,"def gen123():
    print(""Hi"")
    yield 1
    print(""Little"")
    yield 2
    print(""Baby"")
    yield 3",0.3990637958,
2005,step assign it to a variable called baby_names,"def CleanFemPreg(df):
    df.agepreg /= 100.0 #agepreg has motheres age in centiyears
    #dividing by 100.0 gives a floating point age in years
    
    #birthwgt_lb and birthwgt_oz contain the weight of the baby
    #it includes the following special codes;
    #97 NOT ASCERTAINED
    #98 REFUSED
    #99 DONT KNOW
    #we dont want specical codes as numbers, it can lead to use thinking
    #there's a 99 pound baby
    na_vals = [97, 98, 99] #here they are
    #below we look for the value in np_vals
    #replace it with numpy's not a number value
    #we do it in place, so the dataframe is updated
    df.birthwgt_lb.replace(na_vals, np.nan, inplace=True)
    df.birthwgt_oz.replace(na_vals, np.nan, inplace=True)
    
    #finally we add a column that is the baby's weight in pounts(float)
    df[totalwgt_lb] = df.birthwgt_lb + df.birthwgt_oz / 16.0",0.4106159806,
2005,step assign it to a variable called baby_names,"def mother_tense_calc(cl):
    
    mother_tense = ''
    cl_atoms = L.d(cl, 'clause_atom')
    moth = E.mother.f(cl_atoms[0])
    if len(moth) == 0:
        mother_tense += 'no_mother'
    else: 
        if F.otype.v(moth[0]) in {'word', 'phrase'}:
            mother_tense += F.otype.v(moth[0])
        else:
            cl = L.u(moth[0], 'clause')[0]
            if F.kind.v(cl) == 'NC':
                mother_tense += 'nominal'
            elif F.kind.v(cl) == 'WP':
                mother_tense += 'no_pred'
                        
            else:
                phrases = L.d(cl, 'phrase')
                pred = False
                prec = False
                for phr in phrases:
                    if F.function.v(phr) in {'Pred', 'PreS', 'PreO'}:
                        pred = True
                        pred_phr = phr
                    elif F.function.v(phr) in {'PreC', 'PtcO'}:
                        prec = True
                        prec_phr = phr
                if pred == True:
                    words = L.d(pred_phr, 'word')
                    for word in words:
                        if F.sp.v(word) == 'verb':
                            mother_tense += F.vt.v(word)
                elif prec == True:
                    words = L.d(prec_phr, 'word')
                    for word in words:
                        if F.sp.v(word) == 'verb':
                            mother_tense += F.vt.v(word)
                            
    return(mother_tense)",0.4091060758,
2005,step assign it to a variable called baby_names,"import pyspark

def gender_ratio(array):
    num_males = 0
    for item in array:
        if item.gender == 'M':
            num_males += 1
            
    return float(num_males) / len(array)

sparkSql.udf.register('gender_ratio',
                      gender_ratio,
                      pyspark.sql.types.FloatType())",0.4080421925,
2005,step assign it to a variable called baby_names,"def save_checkpoint(saver, sess, step): saver.save(sess, TRAIN_DIR, global_step=step)",0.406637907,
2005,step assign it to a variable called baby_names,"def get_var_trace(mcmc_model, var_name):
    """"""
    Exract parameter trace from PyMC MCMC object.
    
    Parameters
    ----------
    mcmc_model: pymc.MCMC.MCMC
        PyMC MCMC object
    var_name: str
        The name of the parameter you wish to extract
    
    Returns
    -------
    If the variable has been found:
    trace: numpy.ndarray
        the trace of the parameter of interest.
    """"""
    found = False
    for stoch in mcmc_model.stochastics:
        if stoch.__name__ == 'DeltaG':
            found = True
            trace = stoch.trace._trace[0]
    if found:
        return trace
    else:
        print('Variable {0} not present in MCMC object'.format(var_name))",0.4057256877,
2005,step assign it to a variable called baby_names,"class generation :
    def __init__(self,previous_gen):
        self.previous_gen = previous_gen
        self.nb_fathers = 1
        if previous_gen != 0 :
               self.nb_fathers = previous_gen.nb_indiv
        self.law=Probability_law
        self.list_indiv=self.law(self.nb_fathers)
        self.nb_indiv=sum(self.list_indiv)

Jerry_sons = generation(0)
Jerry_sons",0.403609097,
2005,step assign it to a variable called baby_names,"#
# children school
#
if do_load:    
    child_school = mg.load(dmg.FNAME_CHILD_SCHOOL)
else:
    child_school = dmg.get_child_school()",0.3999378085,
2005,step assign it to a variable called baby_names,"# After groupby, a group is a sub-division that's created.
# Each group can be taken, modified, and then re-joined.

def add_prop(group):
    #Integer division floors
    births = group.births.astype(float)
    
    group['prop'] = births / births.sum()
    return group


bk_names = bk_names.groupby(['year', 'sex']).apply(add_prop)
bk_names.head()",0.3994099498,
2005,step assign it to a variable called baby_names,"def add_prop(group):
    #Integer division floors
    births = group.births.astype(float)
    
    group['prop'] = births / births.sum()
    
    return group

all_names = all_names.groupby(['year', 'sex']).apply(add_prop)
all_names.head()",0.3994099498,
2005,step assign it to a variable called baby_names,"def add_prop(group):
    births = group.births.astype(float)
    group['prop'] = births / births.sum()
    return group

names = names.groupby(['year', 'sex']).apply(add_prop)
names.head()",0.3994099498,
311,convert the categorical variables into unordered integral values,"def adf_test(df):
    X = df.values
    X = X.flatten()
    result = adfuller(X)
    print('ADF Statistic: %f' % result[0])
    print('p-value: %f' % result[1])
    print('Critical Values:')
    for key, value in result[4].items():
        print('\t%s: %.3f' % (key, value))",0.5287299752,
311,convert the categorical variables into unordered integral values,"'' For the following functions, ""df"" is a pandas data frame.
    Function ""return_data_mat"" turns the data frame into numpy array with player name column removed.
    Function ""return_names"" returns the names column of the players.
'''

def return_data_mat(df):
    vals = df.values
    trimvals = vals[:,1:]
    data_mat = trimvals.astype(float)
    return data_mat

def return_names(df):
    vals = df.values
    names = (vals[:,0]).astype('str')
    return names",0.5287072062,
311,convert the categorical variables into unordered integral values,"def trans_data(data):
    new_data=data.values
    features=new_data[:,0:-1]
    response=new_data[:,-1]
    features=np.insert(features,0,1,axis=1)
    return features,response
#transform data into matrix form to implement vectorization later",0.5281332135,
311,convert the categorical variables into unordered integral values,"def le_helper(obj):
    le = LabelEncoder()
    encoded_obj = le.fit_transform(obj.values.ravel()).reshape(-1, 1)
    return le, encoded_obj",0.5243923068,
311,convert the categorical variables into unordered integral values,"import numpy as np

def d2s(df,key='Latitude'):
    '''convert from decimal degrees to sessagesimal degrees
    take as input a pandas dataframe and a column name'''
    g = df[key].values.astype(int)
    p = ((( df[key].values) - df[key].values.astype(int))*60).astype(int)
    s = (((df[key].values - df[key].values.astype(int)) * 60. ) - p ) * 60.
    param = key
    ses = pd.DataFrame(np.array([g,p,s]).T, columns=['Degree','Minute','Second'] , dtype=float)
    return ses

def s2d(df,key=['Degree','Minute','Second']):
    '''convert from sessagesimal degrees to decimal degrees
    take as input a pandas dataframe and a list of column names'''
    deg=df[key[0]].values+(df[key[1]].values/60.+df[key[2]].values/3600.)
    return deg",0.5230811238,
311,convert the categorical variables into unordered integral values,"def sheetToDF(sheet):
    data = sheet.values
    cols = next(data)[0:]
    data = list(data)
    df = pd.DataFrame(data, columns=cols)
    return df",0.5211979151,
311,convert the categorical variables into unordered integral values,"def aggregate_score(Q_i, w_i=None):
    Q_i = np.array(Q_i)
    
    if w_i is None:
        w_i = np.ones(Q_i.shape)
    else:
        w_i = np.array(w_i)
    
    return np.sum(w_i * Q_i) / np.sum(w_i)

Q_i = [1.0, 1.0, 1.0, 0.0]
w_i = [1.0, 1.0, 1.0, 1.0]

print aggregate_score(Q_i)  # simple average (0.75)
print aggregate_score(Q_i, w_i) # explicit simple average (0.75)

Q_i = [1.0, 1.0, 1.0, 1.0]
w_i = [3.0, 1.0, 1.0, 1.0]
print aggregate_score(Q_i, w_i)  # weighted average (1.0)

Q_i = [1.0, 0.0, 0.0, 0.0]
w_i = [3.0, 1.0, 1.0, 1.0]
print aggregate_score(Q_i, w_i)  # weighted average (0.5)

Q_i = [0.0, 1.0, 0.0, 0.0]
w_i = [3.0, 1.0, 1.0, 1.0]
print aggregate_score(Q_i, w_i)  # weighted average (1/6 or 0.1666)",0.5205596089,
311,convert the categorical variables into unordered integral values,"def groupfre(dataFrame):
    arr1 = dataFrame.values
    arr2 = dataFrame.sum(axis=1) # sum across columns
    arr2 = [arr2]*3
    arr2 = np.transpose(arr2)   
    arr1.astype(float)
    arr2.astype(float)
    perct = arr1/arr2
    return perct",0.5190057755,
311,convert the categorical variables into unordered integral values,"def getTrainTest(df):    
    '''
    Input:
        df: A dataframe
    Output:
        train_X, train_y, test_X, test_y: Returns train and test splits
    '''
    # split into train and test sets
    values = df.values
    train = values[:int(values.shape[0]*0.8), :] #80% of the data
    test = values[int(values.shape[0]*0.8):, :] #Remaining 20% of the data
    # split into input and outputs
    train_X, train_y = train[:, :-1], train[:, -1]
    test_X, test_y = test[:, :-1], test[:, -1]
    return train_X, train_y, test_X, test_y",0.5183935165,
311,convert the categorical variables into unordered integral values,"def emp_samp(ser, nsamp):
    '''
    ser is a pandas series
    nsamp is the number of samples from the empirical distribution to be returned
    '''
    cumsums = ser.value_counts(normalize=True).cumsum()
    result = np.empty(nsamp)
    for i in range(nsamp):
        uni = np.random.uniform()
        a = cumsums-uni
        m = min(i for i in a if i > 0)
        result[i] = cumsums[cumsums == m+uni].index[0]
        
    return result",0.5168670416,
2311,testing a correlation,"import numpy
def piersonCorrelation(x,y):
    print(""The Pierson Correlation Coefficient is: "")
    print numpy.corrcoef(x, y)[0, 1]",0.4834308624,
2311,testing a correlation,"chi_test=stats.chi2_contingency(observed= observed_count)
print ""Chi squared value""
print ""{:f}"".format(chi_test[0])
#print(""Critical value"")
print""critical_value_Pclass_survival""
print ""{:f}"".format(critical_value_Pclass_survival)
#print(chi_test[])
print(""P value"")
print(chi_test[1])",0.4814822078,
2311,testing a correlation,core_instr.sim_frequencies,0.4808882475,
2311,testing a correlation,from mgcpy.independence_tests.rv_corr import RVCorr,0.480161041,
2311,testing a correlation,"np.testing.assert_array_almost_equal(X_sc.corr(), X.corr())",0.4773760736,
2311,testing a correlation,from mgcpy.independence_tests.dcorr import DCorr,0.4749377966,
2311,testing a correlation,"causal_estimate_reg = model.estimate_effect(identified_estimand,
        method_name=""backdoor.linear_regression"",
        test_significance=True)
print(causal_estimate_reg)
print(""Causal Estimate is "" + str(causal_estimate_reg.value))",0.4733644128,
2311,testing a correlation,"estimate = model.estimate_effect(identified_estimand,
        method_name=""backdoor.linear_regression"", 
        test_significance=True
        )         
print(estimate)
print(""Causal Estimate is "" + str(estimate.value))",0.4733644128,
2311,testing a correlation,"ds.correlation([[""E"", ""Lz""], [""E"", ""L""]])",0.4729536176,
2311,testing a correlation,"guru.check_correlation(X, [3, 4, 5])",0.4725323319,
2371,threshold the image point,"def binarize(image, threshold=30):
    _, binarized = cv2.threshold(image.copy(), threshold, 255, cv2.THRESH_BINARY)
    scaled = binarized
    return scaled",0.5423868299,
2371,threshold the image point,"#def get_lane_pixels(img):
#moved to part 3

def fit_line(img, thresholded_img_class):
    thresholded_warpped = thresholded_img_class.get_thresh_warp_img(img)
    leftx, lefty, rightx, righty = get_lane_pixels(thresholded_warpped)
    leftx, lefty, rightx, righty = ti.append_prev_edge_points(leftx, lefty, rightx, righty )
    # Fit a second order polynomial to each fake lane line
    yvals = range(720)
    try:
        left_fit = np.polyfit(lefty, leftx, 2)
        left_fit = ti.set_leftfit(left_fit)
        left_fitx = np.multiply(left_fit[0],np.power(yvals,2)) + np.multiply(left_fit[1],yvals) + left_fit[2]
    except:
        left_fit = ti.set_leftfit(None)
        left_fitx = np.multiply(left_fit[0],np.power(yvals,2)) + np.multiply(left_fit[1],yvals) + left_fit[2]
    
    try:
        right_fit = np.polyfit(righty, rightx, 2)
        right_fit = ti.set_rightfit(right_fit)
        right_fitx = np.multiply(right_fit[0],np.power(yvals,2)) + np.multiply(right_fit[1],yvals) + right_fit[2]
    except:
        right_fit = ti.set_rightfit(None)
        right_fitx = np.multiply(right_fit[0],np.power(yvals,2)) + np.multiply(right_fit[1],yvals) + right_fit[2]
    
    return left_fit, right_fit, left_fitx, right_fitx, leftx, lefty, rightx, righty, yvals
    
    #plt.plot(leftx, lefty, 'o', color='red')
    #plt.plot(rightx, righty, 'o', color='blue')
    #plt.xlim(0, 1280)
    #plt.ylim(0, 720)
    #plt.plot(left_fitx, lefty, color='green', linewidth=3)
    #plt.plot(right_fitx, righty, color='green', linewidth=3)
    #plt.gca().invert_yaxis() # to visualize as we do the images",0.5351412296,
2371,threshold the image point,"def threshold_image(image, th):
    """""" Function that thresholds an image with an input threshold level.
    Input:   image: numpy array with the image data.
    =======     th: thershold value
    
    Output: img_th: numpy array with the thresholded image.
    =======
    """"""
    _, img_th = cv2.threshold(image, th, 255, 0)
    return img_th

fig = plt.figure();
for i, img in enumerate(images):
    th_image = threshold_image(img, 100) # Call the function to threshold 'img' with a th level of 100
    plt.subplot(3,3,1+i); plt.imshow(th_image, cmap='gray'); # Subplot the thresholded image
fig.tight_layout()
plt.show()",0.5306540132,
2371,threshold the image point,"def threshold(val):
    global threshold_value
    threshold_value = val
    
    fig = plt.figure(figsize=(9, 9))
    ax = fig.add_subplot(1,1,1)
    ax.set_title(""Image after Thresholding"")
    ax.imshow(image < val, cmap=plt.cm.gray, interpolation='nearest')
    
    #plt.imshow(image > val, cmap=plt.cm.gray, interpolation='nearest')
    
    
    
    #plt.show()
    #plt.tight_layout()

threshold_slider = widgets.IntSlider(
    value=0,
    min=0,
    max=255,
    step=1,
    description='Threshold:',
    disabled=False,
    continuous_update=False,
    orientation='horizontal',
    readout=True,
    readout_format='d'
)

interact(threshold, val=threshold_slider)",0.5294981003,
2371,threshold the image point,"def morphology2(image,sizes,thresh):
    ret,thre = cv2.threshold(image,thresh,255,cv2.THRESH_BINARY)
    image  = ndimage.grey_closing(thre ,size = sizes)
    return image",0.5258091688,
2371,threshold the image point,"def read_document(path):
    return cv2.threshold(src=cv2.imread(path, cv2.IMREAD_GRAYSCALE),
                         thresh=200,
                         maxval=1,
                         type=cv2.THRESH_BINARY)[1]

TEXT = read_document(""document.jpg"")

imshow(TEXT, cmap='gray', figure=figure_image())",0.520418644,
2371,threshold the image point,"# http://scikit-image.org/docs/dev/api/skimage.exposure.html

# Performs a contrast adjustment. This function transforms
# the input image pixelwise according to the equation 
# O = 1/(1 + exp*(gain*(cutoff - I))) after scaling each
# pixel to the range 0 to 1.
def adjust_contrast(image, cutoff=.5, gain=1):
    return exposure.adjust_sigmoid(image, cutoff=cutoff, gain=gain, inv=False);",0.519536972,
2371,threshold the image point,"def thres(Value):    
    ret,thresh1 = cv2.threshold(img_gray,Value,255,cv2.THRESH_BINARY)
    
    plt.imshow(thresh1,plt.get_cmap('gray'))
    plt.title(""Interactive Threshold"")
    plt.show()
    
interact(thres,Value=(1,255,2))",0.5143421888,
2371,threshold the image point,"def rotate(data_set, i, width = 100):
    c = data_set.copy(True)
    n = i * width
    return c[0:n].append(c[n+width:]), c[n:n+width]

n0, n0t = rotate(data_set, 0)

# Rotations are of form, (training[900], test[100])[10]
rotations = [rotate(data_set, k) for k in range(10)]",0.5129072666,
2371,threshold the image point,"def threshold(img):
    ret,thresh = cv2.threshold(img,150,255,cv2.THRESH_BINARY)
    plt.subplot(1,2,1), plt.imshow(img, cmap='gray')
    plt.subplot(1,2,2), plt.imshow(thresh, cmap='gray')",0.5110756159,
2601,word border,"def random_shift_scale_rotate(image, angle, scale, aspect, shift_dx, shift_dy,
                              borderMode=cv2.BORDER_CONSTANT, u=0.5):
    if np.random.random() < u:
        if len(image.shape) == 3:  # Img or mask
            height, width, channels = image.shape
        else:
            height, width = image.shape

        sx = scale * aspect / (aspect ** 0.5)
        sy = scale / (aspect ** 0.5)
        dx = round(shift_dx * width)
        dy = round(shift_dy * height)

        cc = np.math.cos(angle / 180 * np.math.pi) * sx
        ss = np.math.sin(angle / 180 * np.math.pi) * sy
        rotate_matrix = np.array([[cc, -ss], [ss, cc]])

        box0 = np.array([[0, 0], [width, 0], [width, height], [0, height], ])
        box1 = box0 - np.array([width / 2, height / 2])
        box1 = np.dot(box1, rotate_matrix.T) + np.array([width / 2 + dx, height / 2 + dy])

        box0 = box0.astype(np.float32)
        box1 = box1.astype(np.float32)
        mat = cv2.getPerspectiveTransform(box0, box1)

        image = cv2.warpPerspective(image, mat, (width, height), flags=cv2.INTER_LINEAR,
                                    borderMode=borderMode, borderValue=(0, 0, 0, 0))
    return image


def random_horizontal_flip(image, mask, u=0.5):
    if np.random.random() < u:
        image = cv2.flip(image, 1)
        mask = cv2.flip(mask, 1)

    return image, mask


def augment_img(img, mask):
    rotate_limit = (-45, 45)
    aspect_limit = (0, 0)
    scale_limit = (-0.1, 0.1)
    shift_limit = (-0.0625, 0.0625)
    shift_dx = np.random.uniform(shift_limit[0], shift_limit[1])
    shift_dy = np.random.uniform(shift_limit[0], shift_limit[1])
    angle = np.random.uniform(rotate_limit[0], rotate_limit[1])  # degree
    scale = np.random.uniform(1 + scale_limit[0], 1 + scale_limit[1])
    aspect = np.random.uniform(1 + aspect_limit[0], 1 + aspect_limit[1])

    img = random_shift_scale_rotate(img, angle, scale, aspect, shift_dx, shift_dy)
    mask = random_shift_scale_rotate(mask, angle, scale, aspect, shift_dx, shift_dy)

    img, mask = random_horizontal_flip(img, mask)
    return img, mask",0.484352082,
2601,word border,"def conv(x, nf, sz, wd, p):
    x = Convolution2D(nf, sz, sz, init='he_uniform', border_mode='same', 
                          W_regularizer=l2(wd))(x)
    return dropout(x,p)",0.4799273014,
2601,word border,"def show_Eres(P, x, y):
    print(P)
    P_o = np.array([100,0.3])
    print(""relative error: %.1f %%""%(100*sum(abs(P-P_o)/P_o)/len(P_o)))
    plt.figure(figsize=(6,8))
    plt.subplot(211)
    plt.plot(x, y,'x')
    plt.plot(x, Exponen(x, *P), 'r' )

    plt.subplot(212)
    plt.plot(x, Eres(P, x, y) )
show_Eres(P_fit, x, y_meas)",0.4786929786,
2601,word border,"### Draw text in an image
def draw_text_in_image(image, text, ypos):
    cv2.putText(image, text, (20, 80*ypos), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), thickness=3)
    
    return image",0.4718683362,
2601,word border,"# WATCH OUT!: The priors have to be multiplied by ""-0.5""!
def lnPosteriorWithPriors(theta, z, mu, muError):
    ""ln(Posterior) including Gaussian priors""
    OmL, OmM, a = theta
    
    '''
    if 0.<w<10000 and 0.<OmL<1 and 0.<OmNu<0.02:
        lnPostWithPriorsInt = lnLikelihood_SNe(theta, z, mu, muError)
        return lnPostWithPriorsInt
    return -np.inf
    '''
     
    lnPostWithPriorsInt = lnLikelihood_SNe(theta, z, mu, muError)
    return lnPostWithPriorsInt",0.4710090756,
2601,word border,"# Function to rank XY-points into quadrants 0-3
def xyrank(a, b, c, d, e, f):
    ''' 
    Assign qudrant value 1,2,3,4.
    :type inputs: floats and str
    :param inputs: X, Y, Q(1), Q(2), Q(3), Q(4)  
    :rtype: str
    :return: quadrant number as str    
    '''
    if a > 0 and b > 0:  # [Q1]
        return c
    elif a > 0 > b:  # [Q2]
        return d
    elif a < 0 and b < 0:  # [Q3]
        return e
    else:  # [Q4] 1 a>0, b<0
        return f
        
        
# Function to calculate slope
def slope(a, b):
    if a != 0:
        slope = (1.0 * b / a)  # expects float
        return slope
    else:
        slope = 0  # for a = zero, set slope arbitrarily to 0
        return slope",0.4704017341,
2601,word border,"def REuler(l, alpha, beta, gamma):
    """"""Return the rotation matrix for a single degree `l`.""""""
    res = zeros(2 * l + 1, 2 * l + 1)
    if l == 0:
        res[0, 0] = 1
        return res
    foo = re(U(l).inv() * D(l, alpha, beta, gamma) * U(l))
    for m in range(2 * l + 1):
        for n in range(2 * l + 1):
            if abs(foo[m, n]) < 1e-15:
                res[m, n] = 0
            else:
                res[m, n] = foo[m, n]
    return res",0.4699784815,
2601,word border,"def print_policy(P,grid):
    for i in range(grid.width):
        print(""---------------------------"")
        for j in range(grid.height):
            a = P.get((i,j),' ')
            if isinstance(a,dict):
                a = list(a)[0]
            print(""  %s  |"" % a, end="""")
        print("""")",0.4664911628,
2601,word border,"def print_values(V,grid):
    for i in range(grid.width):
        print(""--------------------------"")
        for j in range(grid.height):
            v = V.get((i,j),0)
            if v >= 0:
                print("" %.2f|"" % v, end="""")
            else:
                print(""%.2f|"" % v, end="""")
        print("""")",0.4664911628,
2601,word border,"def plot_alias_check(t1,y1,Fs, t2,y2,Fs_d):
    subplot2grid((1,5), (0,0), colspan=3)
    axhline(0, color=""grey"", alpha=0.5)
    axis(""off"")
    
    plot(t1,y1)
    plot(t2,y2, 'o')
    title(""Signal, with downsampled points highlighted"")
    ylim(-1.1,1.1)
    xlim(0, t2[20])
    
    subplot2grid((1,5), (0,3), colspan=2)

    psd(y1, Fs=Fs)
    psd(y2, Fs=Fs_d)
    xlim(0, 2*Fs_d)
    #ylim(-190,0)
    ylabel("""")",0.4660902619,
1009,knn classification map for iris,"# Store all classifiers in a dictionary
Classifiers1 = dict(LDA = LDA(),
                    NB= GaussianNB(), 
                    KNN1=KNeighborsClassifier(algorithm='auto',n_neighbors=3), 
                    SVCL0=SVC(kernel='linear'), 
                    SVCR0=SVC(C=1.0, cache_size=400, class_weight=None, coef0=0.0, degree=3,
                              gamma=10.0, kernel='rbf', max_iter=-1, probability=False, shrinking=True,
                              tol=0.0001, verbose=False),
                    CART0=DecisionTreeClassifier(max_depth=10))

DataSet = {}
Markers = {}


def selFeatsFilter(A,b,top=5):
    selector = SelectPercentile(f_classif, percentile=10)
    selector.fit(A, b)
    scores = -np.log10(selector.pvalues_)
    scores[np.isnan(scores)]=0
    np.argsort(scores)
    if top>len(scores): top=len(scores)
    return np.argsort(scores)[-top:]

def myScore(Y_test,Y_pred):
    X = dict(TP=np.sum(np.logical_and(Y_pred==1, Y_test==1)),
             TN=np.sum(np.logical_and(Y_pred==0, Y_test==0)),
             FP=np.sum(np.logical_and(Y_pred==1, Y_test==0)),
             FN=np.sum(np.logical_and(Y_pred==0, Y_test==1)))
    for i,v in X.iteritems(): v *= 1.0
    TP,TN,FP,FN = X['TP'],X['TN'],X['FP'],X['FN']
    return dict(cm=X,acc=1.0*(TP+TN)/(TP+TN+FP+FN),sens=1.0*TP/(TP+FN),spec=1.0*TN/(TN+FP))

def countPosNeg(X):
    n0=np.sum(X==0)
    n1=np.sum(X==1)
    if n1>n0: 
        return 1
    else:
        return 0

def trainAndTestClassifiers(X,Y,I_test,I_train,n_desc,dbg=False,
                            TOPD=Markers,CLSF=Classifiers1):
    X_test = X.ix[I_test,:]
    X_train = X.ix[I_train,:]
    Y_test = Y.ix[I_test]
    Y_train = Y.ix[I_train]
    F_best = selFeatsFilter(X_train,Y_train,n_desc)
    Perf_i   =[]
    Vote   =pd.DataFrame(np.zeros((len(I_test),len(CLSF))),columns=CLSF.keys())
    
    if dbg: print ""Train"",X_train.shape,"" Test"",X_test.shape,"" n_desc"",len(F_best)
    for nm,clf in CLSF.iteritems():
        if dbg: print nm
        try:
            if dbg: print "" Training""
            clf.fit(X_train.ix[:,F_best], Y_train)
            if dbg: print "" Testing""
            Y_pred = clf.predict(X_test.ix[:,F_best])
            Vote[nm]=Y_pred
            if dbg: print "" Calc score""
            P_i = myScore(Y_test,Y_pred)
            if dbg: print "" Make results""
            P = dict(cl=nm,sens=P_i['sens'],spec=P_i['spec'],
                     bacc=0.5*(P_i['sens']+P_i['spec']),
                     prec=precision_score(Y_test,Y_pred),
                     n_pos=np.sum(Y_train==1),n_neg=np.sum(Y_train==0),
                     n_desc=n_desc,
                     acc=accuracy_score(Y_test,Y_pred))
                
            Perf_i.append(P)
                             
        except:
            print ""   %s Failed!"" % nm
        
    # Now get the majority vote for the class
    Y_pred=Vote.apply(countPosNeg,axis=1)
    P_i = myScore(Y_test,Y_pred)
    P = dict(cl='ENSMB',sens=P_i['sens'],spec=P_i['spec'],
             bacc=0.5*(P_i['sens']+P_i['spec']),
             prec=precision_score(Y_test,Y_pred),
             n_pos=np.sum(Y_train==1),n_neg=np.sum(Y_train==0),
             n_desc=n_desc,
             acc=accuracy_score(Y_test,Y_pred))
    Perf_i.append(P)

    return pd.DataFrame(Perf_i)


def resampleBalCVLearn(n_pos,n_neg,trial=1,
                       n_desc_min=5,n_desc_max=70,n_desc_step=5,
                       K=10,DS=DataSet,TOPD=Markers,
                       CLSF=Classifiers1,dbg=False):
    """"""
    Creates a balanced undersampled subset of the data for K-fold CV testing n_reps times 
    Requires: DS (which is pushed by dview)

    """"""
    Data = DS['Data']
    Y_pos = Data[DS['pos_cls']]
    X_pos = Data[DS['Desc']].ix[Y_pos==1,:]
    N_pos = np.sum(Y_pos[Y_pos==1])
    
    Y_neg = Data[DS['neg_cls']]
    X_neg = Data[DS['Desc']].ix[Y_neg==1,:]
    N_neg = np.sum(Y_neg[Y_neg==1])

    I_pos = np.random.randint(0,N_pos,n_pos)
    X_pi = X_pos.ix[I_pos,:]
    X_pi['out']=1

    I_neg = np.random.randint(0,N_neg,n_neg)
    X_ni = X_neg.ix[I_neg,:]
    X_ni['out']=0

    X = pd.concat((X_pi,X_ni))
    Y = X['out']
    X = X.drop('out',axis=1)
    
    Perf_cv = []
    
    #pid=os.getpid()

    SKF = StratifiedKFold(Y,n_folds=K)
    i_step = 0
    for n_desc in range(n_desc_min,n_desc_max,n_desc_step):
        for I_train,I_test in SKF:
            i_step += 1
            if dbg: print 'Data',X.shape
            P_df=trainAndTestClassifiers(X,Y,I_test,I_train,n_desc,
                                         TOPD=Markers,CLSF=Classifiers1,dbg=dbg)
            P_df['n_obs']=n_pos+n_neg
            P_df['n_pos']=n_pos
            P_df['n_neg']=n_neg
            P_df['dtp'] = DS['dtp']
            P_df['tox'] = DS['tox']
            P_df['cvk'] = K
            P_df['itrl']= ""%d-%d"" % (trial,i_step)
            Perf_cv += P_df.T.to_dict().values()
        
    return Perf_cv

# The rest is just to make a summary table of performance results from all the replicates

C1=pd.MultiIndex.from_tuples(
[('n_desc', 'bio'),
 ('n_desc', 'chm'),
 ('n_desc', 'bc'),
 ('bacc', 'bio'),
 ('bacc', 'chm'),
 ('bacc', 'bc'),
 ('sens', 'bio'),
 ('sens', 'chm'),
 ('sens', 'bc'),
 ('spec', 'bio'),
 ('spec', 'chm'),
 ('spec', 'bc')])
ii_tox=0
ii_dtp=1
ii_cl =2
ii_cvk=3
ii_nds=4
ii_nob=5


def buildMSTable1(P_st,cols=C1):
    X= P_st.groupby(level=['tox','dtp','cl']).max()
    Tox=set([i[0] for i in X.index])
    Dtp=set([i[1] for i in X.index])
    CL =set([i[2] for i in X.index])
    Res=[]
    for tox in Tox:
        for dtp in Dtp:
            for cl in CL:
                Bacc=X.xs((tox,dtp,cl))['bacc']
                Y = P_st.xs((tox,dtp,cl)).reset_index()[['n_desc','bacc']]
                Y1 = Y[(Y['bacc']==Bacc)].n_desc
                #print tox,dtp,cl,Bacc,int(Y1)
                Res.append(dict(tox=tox,dtp=dtp,cl=cl,n_desc=int(list(Y1)[0])))
    Res_df = pd.DataFrame(Res).set_index(['tox','dtp','cl'])
    X = pd.merge(X,Res_df,left_index=True,right_index=True)
    #X['bacc2'] = X.apply(lambda x: ""%3.2f(%d)""%(x[1],x[-1]),axis=1)

    T1 = np.round(pd.pivot_table(X.reset_index(),index=['tox','cl'],columns=['dtp'],values=['bacc','n_desc','sens','spec']),decimals=2)
    return T1[cols]

def buildMSTable2(P_mn,P_sd,cols=C1):
    IMB_mn = buildMSTable1(P_mn)
    IMB_sd = buildMSTable1(P_sd)

    if len(cols)>0: 
        C1 = cols 
    else:
        C1 = IMB_mn.columns
        
    R1=IMB_mn.index
    IMB_res = pd.DataFrame(columns=C1,index=R1)

    for i in [x for x in C1 if x[0]=='n_desc']:
        IMB_res[i] = IMB_mn[i]

    for r in R1:
        for c in [c for c in C1 if c[0]!='n_desc']:
            IMB_res.ix[r,c] = ""%3.2f (%3.2f)"" % (IMB_mn.ix[r,c],IMB_sd.ix[r,c]) 

    return IMB_res

def buildMSTable11(P_st,cols=C1):
    X= P_st.groupby(level=['tox','dtp']).max()
    Tox=set([i[0] for i in X.index])
    Dtp=set([i[1] for i in X.index])
    Res=[]
    for tox in Tox:
        for dtp in Dtp:
            Bacc=X.xs((tox,dtp))['bacc']
            Y = P_st.xs((tox,dtp)).reset_index()[['n_desc','bacc']]
            Y1 = Y[(Y['bacc']==Bacc)].n_desc
            #print tox,dtp,cl,Bacc,int(Y1)
            Res.append(dict(tox=tox,dtp=dtp,n_desc=int(list(Y1)[0])))
    Res_df = pd.DataFrame(Res).set_index(['tox','dtp'])
    X = pd.merge(X,Res_df,left_index=True,right_index=True)
    #X['bacc2'] = X.apply(lambda x: ""%3.2f(%d)""%(x[1],x[-1]),axis=1)

    T1 = np.round(pd.pivot_table(X.reset_index(),index=['tox'],columns=['dtp'],values=['bacc','n_desc','sens','spec']),decimals=2)
    return T1[cols]

def buildMSTable21(P_mn,P_sd,cols=C1):
    IMB_mn = buildMSTable11(P_mn)
    IMB_sd = buildMSTable11(P_sd)

    if len(cols)>0: 
        C1 = cols 
    else:
        C1 = IMB_mn.columns
    
    R1=IMB_mn.index
    IMB_res = pd.DataFrame(columns=C1,index=R1)

    for i in [x for x in C1 if x[0]=='n_desc']:
        IMB_res[i] = IMB_mn[i]

    for r in R1:
        for c in [c for c in C1 if c[0]!='n_desc']:
            IMB_res.ix[r,c] = ""%3.2f (%3.2f)"" % (IMB_mn.ix[r,c],IMB_sd.ix[r,c]) 

    return IMB_res",0.471575737,
1009,knn classification map for iris,"# build a classifier
knn = KNeighborsClassifier()

# grid over parameters
param_grid = {""metric"": [""minkowski"", ""manhattan"", ""chebyshev""], 
              ""n_neighbors"": list(range(1,41)),
              ""weights"": [""uniform"", ""distance""]}

# run grid search
grid_search = GridSearchCV(knn, param_grid=param_grid, cv = 178)
start = time()
grid_search.fit(train_data[all_predictors], train_data['Survived'])

print(""GridSearchCV took %.2f seconds for %d candidate parameter settings.""
      % (time() - start, len(grid_search.grid_scores_)))
report(grid_search.grid_scores_)",0.4599689841,
1009,knn classification map for iris,"# Do not worry if those lines confuse you. 
# They are only needed to get some data to work with and you do not need to understand them.
# If you are curious take a look here:
# https://stackoverflow.com/questions/38105539/how-to-convert-a-scikit-learn-dataset-to-a-pandas-dataset
import numpy as np
from sklearn import datasets

def load_iris():
    """"""Loads the iris dataset and returns it as a dataframe""""""
    iris = datasets.load_iris()
    iris_df = pd.DataFrame(data= np.c_[iris['data'], iris['target']],
                         columns= iris['feature_names'] + ['target'])
    return iris_df

iris_df = load_iris()
iris_df.head()",0.4599195123,
1009,knn classification map for iris,"def map_values():
    
    full_data[""oMSZoning""] = full_data.MSZoning.map({'C (all)':1, 'RH':2, 'RM':2, 'RL':3, 'FV':4})

    full_data[""oNeighborhood""] = full_data.Neighborhood.map({'MeadowV':1,
                                               'IDOTRR':2, 'BrDale':2,
                                               'OldTown':3, 'Edwards':3, 'BrkSide':3,
                                               'Sawyer':4, 'Blueste':4, 'SWISU':4, 'NAmes':4,
                                               'NPkVill':5, 'Mitchel':5,
                                               'SawyerW':6, 'Gilbert':6, 'NWAmes':6,
                                               'Blmngtn':7, 'CollgCr':7, 'ClearCr':7, 'Crawfor':7,
                                               'Veenker':8, 'Somerst':8, 'Timber':8,
                                               'StoneBr':9,
                                               'NoRidge':10, 'NridgHt':10})
    
    full_data[""oCondition1""] = full_data.Condition1.map({'Artery':1,
                                           'Feedr':2, 'RRAe':2,
                                           'Norm':3, 'RRAn':3,
                                           'PosN':4, 'RRNe':4,
                                           'PosA':5 ,'RRNn':5})
    
    full_data[""oBldgType""] = full_data.BldgType.map({'2fmCon':1, 'Duplex':1, 'Twnhs':1, '1Fam':2, 'TwnhsE':2})
    
    full_data[""oHouseStyle""] = full_data.HouseStyle.map({'1.5Unf':1, 
                                           '1.5Fin':2, '2.5Unf':2, 'SFoyer':2, 
                                           '1Story':3, 'SLvl':3,
                                           '2Story':4, '2.5Fin':4})
    
    full_data[""oExterior1st""] = full_data.Exterior1st.map({'BrkComm':1,
                                             'AsphShn':2, 'CBlock':2, 'AsbShng':2,
                                             'WdShing':3, 'Wd Sdng':3, 'MetalSd':3, 'Stucco':3, 'HdBoard':3,
                                             'BrkFace':4, 'Plywood':4,
                                             'VinylSd':5,
                                             'CemntBd':6,
                                             'Stone':7, 'ImStucc':7})
    
    full_data[""oMasVnrType""] = full_data.MasVnrType.map({'BrkCmn':1, 'None':1, 'BrkFace':2, 'Stone':3})
    
    full_data[""oExterQual""] = full_data.ExterQual.map({'Fa':1, 'TA':2, 'Gd':3, 'Ex':4})
    
    full_data[""oFoundation""] = full_data.Foundation.map({'Slab':1, 
                                           'BrkTil':2, 'CBlock':2, 'Stone':2,
                                           'Wood':3, 'PConc':4})
    
    full_data[""oBsmtQual""] = full_data.BsmtQual.map({'Fa':2, 'None':1, 'TA':3, 'Gd':4, 'Ex':5})
    
    full_data[""oBsmtExposure""] = full_data.BsmtExposure.map({'None':1, 'No':2, 'Av':3, 'Mn':3, 'Gd':4})
    
    full_data[""oHeating""] = full_data.Heating.map({'Floor':1, 'Grav':1, 'Wall':2, 'OthW':3, 'GasW':4, 'GasA':5})
    
    full_dataset[""oHeatingQC""] = full_data.HeatingQC.map({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})
    
    full_dataset[""oKitchenQual""] = full_data.KitchenQual.map({'Fa':1, 'TA':2, 'Gd':3, 'Ex':4})
    
    full_dataset[""oFunctional""] = full_data.Functional.map({'Maj2':1, 'Maj1':2, 'Min1':2, 'Min2':2, 'Mod':2, 'Sev':2, 'Typ':3})
    
    full_dataset[""oFireplaceQu""] = full_data.FireplaceQu.map({'None':1, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})
    
    full_dataset[""oGarageType""] = full_data.GarageType.map({'CarPort':1, 'None':1,
                                           'Detchd':2,
                                           '2Types':3, 'Basment':3,
                                           'Attchd':4, 'BuiltIn':5})
    
    full_data[""oGarageFinish""] = full_data.GarageFinish.map({'None':1, 'Unf':2, 'RFn':3, 'Fin':4})
    
    full_data[""oPavedDrive""] = full_data.PavedDrive.map({'N':1, 'P':2, 'Y':3})
    
    full_data[""oSaleType""] = full_data.SaleType.map({'COD':1, 'ConLD':1, 'ConLI':1, 'ConLw':1, 'Oth':1, 'WD':1,
                                       'CWD':2, 'Con':3, 'New':3})
    
    full_data[""oSaleCondition""] = full_data.SaleCondition.map({'AdjLand':1, 'Abnorml':2, 'Alloca':2, 'Family':2, 'Normal':3, 'Partial':4})            

map_values()",0.4566189051,
1009,knn classification map for iris,"def getData():
    from sklearn import datasets
    data = datasets.load_iris() 
    #data = datasets.load_boston() 
    #data = datasets.load_diabetes()
    
    X = data['data']
    Y = data['target']
    Y = Y.reshape(len(data['target']), 1)
    dataset = np.hstack((X, Y))
    return dataset",0.4564990401,
1009,knn classification map for iris,"clfs = {'LogisticRegression':LogisticRegression(),'SVC': SVC(), 
        'KNeighborsClassifier': KNeighborsClassifier(n_neighbors = 3),
        'GaussianNB': GaussianNB(),
        'Perceptron': Perceptron(), 'LinearSVC': LinearSVC(), 
        'SGDClassifier': SGDClassifier(), 
        'DecisionTreeClassifier': DecisionTreeClassifier(),
        'RandomForestClassifier': RandomForestClassifier(n_estimators=100)}

for name, clf in clfs.items():
    clf.fit(X_train,y_train)
    ypred = clf.predict(X_test)
    precision_ = precision_score(ypred, y_test)
    recall_ = recall_score(ypred, y_test)
    accuracy_ = accuracy_score(ypred,y_test)
    f1_ = f1_score(ypred,y_test)
    print('%s classifier: precision = %.4f, recall = %.4f, accuracy = %.4f, f1 score = %.4f' 
          %(name, precision_, recall_, accuracy_, f1_))",0.4519273639,
1009,knn classification map for iris,"def random_forest(trainingData,testData):
    from pyspark.ml.classification import RandomForestClassifier
    print(""Random Forest Classifier"")
    rf = RandomForestClassifier(labelCol=""label"", featuresCol=""features"")
    model = rf.fit(trainingData)
    predictions = model.transform(testData)
    # Select example rows to display.
    predictions.select(""value"",""Category"",""probability"",""label"",""prediction"") \
        .orderBy(""probability"", ascending=False) \
        .show(n = 30, truncate = 30)
    evaluator = MulticlassClassificationEvaluator(predictionCol=""prediction"")
    print(""Accuracy :- "" + str(100*evaluator.evaluate(predictions)) +"" %""+""\n"")",0.4505610168,
1009,knn classification map for iris,"def load_dataset():
    iris = datasets.load_iris()
    X = iris.data
    y = np_utils.to_categorical(iris.target) #i transformed the target (different integers for each flower class) into a one-hot vector 


    xy = np.hstack((X,y)) #stacking the features and classes vectors horizontally, so during the shuffle phase i'll not get lost
    np.random.shuffle(xy) #shuffling the lines

    #there are four iris features and 150 samples. I'm spliting the now suffled vector into 4. Training vectors with 105 samples and 45 samples to model validation.
    X_train, X_test = xy[:105,:4], xy[105:,:4] 
    y_train, y_test = xy[:105,4:], xy[105:,4:]

    return { 'data':{'train': X_train, 'test': X_test},
             'classes':{'train': y_train, 'test': y_test} }",0.4498289526,
1009,knn classification map for iris,"class IrisClassifier(TraceableModel):
    """"""
    A simple NN with one hidden layer to classify the iris dataset,
    reprsented as a TraceableModel subclass.  We will use it to save
    and visualize the progress of training this model.
    """"""
    def __init__(self):
        super().__init__(name='IRIS_CLASSIFIER')
        self.add(Dense(8, input_shape=(4,)))
        self.add(Activation('sigmoid'))
        self.add(Dense(3))
        self.add(Activation('softmax'))
        self.compile(optimizer='adam', loss='categorical_crossentropy',
                     metrics=['accuracy'])

def classify_iris():
    """"""
    This function instantiates an IrisClassifier, trains it on the iris data,
    and saves the training metrics, model, and weights to file.  The process
    is repeated twice... loss and accuracy continue to improve after first
    training.
    """"""
    iris = sns.load_dataset('iris')
    X = iris.values[:, :4] # First 4 columns are features
    y = iris.values[:, 4] # Last (5th) column are the labels (iris type)
    ic = IrisClassifier()

    # Directory to save model, weights, and results
    dir_results ='/Users/bryanfry/repos/scr' 

    res1 = ic.train(X, y, batch_size=2, epochs=16, perform_one_hot_encoding=True,
             record_X_train=True, record_y_train=True, record_X_test=True,
             record_y_test=True, record_y_pred=True, random_state=42)

    # Save model and weights after first training
    ic.save_model_and_weights(dir_results)  
    
    # Save results of first training (Accuracy, less, timing, train/test split data)
    res1.save(dir_results, fn=None)  

    res2 = ic.train(X, y, batch_size=2, epochs=16, perform_one_hot_encoding=True,
             record_X_train=True, record_y_train=True, record_X_test=True,
             record_y_test=True, record_y_pred=True, random_state=43)

    # Save model and weights after first training
    ic.save_model_and_weights(dir_results) #.... as above, second training
    
    # Save results of first training (Accuracy, less, timing, train/test split data)
    res2.save(dir_results, fn=None)
    return (res1, res2)",0.4491224885,
1009,knn classification map for iris,"def que2():
    iris = datasets.load_iris() 
    X = iris.data
    target = iris.target 
    names = iris.target_names
    acc_rf = cross_val(RandomForestClassifier(n_estimators = 20), X, target, 5)
    acc_svm = cross_val(SVC(), X, target, 5)
    # skLearn function from cross validation accuracy score
    sk_learn_cross_val_rf = cross_val_score(RandomForestClassifier(n_estimators = 20), X, target, cv = 10, scoring = 'accuracy')
    sk_learn_cross_val_svc = cross_val_score(SVC(), X, target, cv = 10, scoring = 'accuracy')
    # Answer 2-b -START
    print(""Non aggregated accuracy of user defined cross validation function v/s sk learn cross valtion function"")
    print(""Accuraccy score of user defined cross val function"")
    print(acc_rf)
    
    print(""Accuraccy score of sk learn cross val function"")
    print(sk_learn_cross_val_rf)
    # Answer 2-b -FINISH
    
    # Answer 2-c - START
    print(""Comparing Accuracy score between Random Forest and SVM Classification :"")
    print(""Random Forest accuracy (Mean) :"")
    print(np.mean(sk_learn_cross_val_rf))
    print(""SVM Classification accuracy (Mean) :"")
    print(np.mean(sk_learn_cross_val_svc))
    # Answer 2-c FINISH

que2()",0.4472416043,
114,binary data,"def read_var_int(stream):
    i = read_int(stream, 1)
    if i == 0xff:
        return read_int(stream, 8)
    elif i == 0xfe:
        return ""FIXME""
    elif ""FIXME"":
        return ""FIXME""
    else:
        ""FIXME""",0.4969916046,
114,binary data,np.unpackbits(np.array(29).astype('uint8')),0.4872981906,
114,binary data,np.unpackbits(np.array(28).astype('uint8')),0.4845222235,
114,binary data,"def get_AgeGroupId(age):
    if age < 21:
        return 1
    if age < 41:
        return 2
    if age < 61:
        return 3
    if age < 81:
        return 4
    else:
        return 5

def get_AgeGroup(age):
    if age < 21:
        return '1: 0-20'
    if age < 41:
        return '2: 21-40'
    if age < 61:
        return '3: 41-60'
    if age < 81:
        return '4: 61-80'
    else:
        return '5: 80+'

if False:
    print(get_AgeGroupId(0)) 
    print(get_AgeGroupId(55)) 
    print(get_AgeGroupId(100))
    print(get_AgeGroup(0)) 
    print(get_AgeGroup(55)) 
    print(get_AgeGroup(100))",0.4792015851,
114,binary data,"def decode_pixel(encoded_pixel):
    # extract 4 least significant bits from ecoded_pixel --> 1
    ls_bits = encoded_pixel & 0x000f
    # left shift 1 by 4 --> 2
    pixel_val = ls_bits << 4
    # zero out the 4 ls bits of 2
    pixel_val = pixel_val & 0xfff0
    return pixel_val",0.478952229,
114,binary data,"def DOY (timestamp): #Stand for ""Day of Year""
    if (type(timestamp) is str):
        if len(timestamp) == 19:
            Date = datetime.datetime.strptime(timestamp,""%Y-%m-%d %H:%M:%S"").date()
        if len(timestamp) == 10:
            Date = datetime.datetime.strptime(timestamp,""%Y-%m-%d"").date()
    elif (type(timestamp) is datetime.date):
        Date = timestamp
    elif (type(timestamp) is pd.tslib.Timestamp):
        Date= timestamp.date()
    else:
        print 'not valid time object'
    y = str(Date.year)
    NewYear = datetime.datetime.strptime(y+'-01-01',""%Y-%m-%d"").date()
    d = (Date - NewYear).days
    if (int(y)%4==0)&(Date > datetime.datetime.strptime(y+'-02-28',""%Y-%m-%d"").date()): #leap year correction
        d = d-1
    return d;

#one can compute DBW (""Day Before Winter Solstice"") by substracting DOY with 355. as  DOY (""2014-12-22 00:00:00"") = 355.


#one can compute DBW (""Day Before Winter Solstice"") by substracting DOY with 355. as  DOY (""2014-12-22 00:00:00"") = 355.",0.4768653512,
114,binary data,"def DOY (timestamp): #Stand for ""Day of Year""
    if (type(timestamp) is str):
        if len(timestamp) == 19:
            Date = datetime.datetime.strptime(timestamp,""%Y-%m-%d %H:%M:%S"").date()
        if len(timestamp) == 10:
            Date = datetime.datetime.strptime(timestamp,""%Y-%m-%d"").date()
    elif (type(timestamp) is datetime.date):
        Date = timestamp
    elif (type(timestamp) is datetime.datetime):
        Date= timestamp.date()
    else:
        print 'not valid time object'
    y = str(Date.year)
    NewYear = datetime.datetime.strptime(y+'-01-01',""%Y-%m-%d"").date()
    d = (Date - NewYear).days
    if (int(y)%4==0)&(Date > datetime.datetime.strptime(y+'-02-28',""%Y-%m-%d"").date()): #leap year correction
        d = d-1
    return d;

#one can compute DBW (""Day Before Winter Solstice"") by substracting DOY with 355. as  DOY (""2014-12-22 00:00:00"") = 355.",0.4768653512,
114,binary data,np.unpackbits(np.array(13).astype('uint8')),0.4767840505,
114,binary data,np.unpackbits(np.array(12).astype('uint8')),0.4765877724,
114,binary data,"def get_face_points(points, method='average', top='eyebrow'):
    width_left, width_right = points[0], points[16]
    
    if top == 'eyebrow':
        top_left = points[18]
        top_right = points[25]
        
    elif top == 'eyelid':
        top_left = points[37]
        top_right = points[43] 
        
    else:
        raise ValueError('Invalid top point, use either ""eyebrow"" or ""eyelid""')
        
    bottom_left, bottom_right = points[50], points[52]
    
    if method == 'left':
        coords = (width_left[0], width_right[0], top_left[1], bottom_left[1])
        
    elif method == 'right':
        coords = (width_left[0], width_right[0], top_right[1], bottom_right[1])
        
    else:
        top_average = int((top_left[1] + top_right[1]) / 2)
        bottom_average = int((bottom_left[1] + bottom_right[1]) / 2)
        coords = (width_left[0], width_right[0], top_average, bottom_average)
        
    ## Move the line just a little above the top of the eye to the eyelid    
    if top == 'eyelid':
        coords = (coords[0], coords[1], coords[2] - 4, coords[3])
        
    return {'top_left' : (coords[0], coords[2]),
            'bottom_left' : (coords[0], coords[3]),
            'top_right' : (coords[1], coords[2]),
            'bottom_right' : (coords[1], coords[3])
           }",0.4763939381,
1758,read table,"def tweets(filename):
    df = pd.read_table(""%s"" %filename, header = None) #reads text file
    df.columns = [""tweets""] #labels the columns as tweets
    return df #returns the data as a pandas table",0.5611979961,
1758,read table,"def create_review_df_from_txt(file_path):
    df = pd.read_table(file_path, header=None, quoting=csv.QUOTE_NONE)
    df.columns = ['review', 'sentiment']
    df['sentiment'] = df.sentiment.map({0: 'negative', 1: 'positive'}) 
    df['review'] = df['review'].str.strip()
    return df",0.552167356,
1758,read table,"def parse_protein_table(fname):
    df = pd.read_table('proteins/{}'.format(fname))
    df['file_name'] = fname
    return df",0.5486530662,
1758,read table,"def read_table(*args, dtype=None, **kwargs):
    try:
        df = pd.read_table(*args, **kwargs, dtype=dtype)
    except:
        df = pd.read_table(*args, **kwargs)
        
        for col, value in dtype.items():
            df = dtype_safe_column(df, column=col, types=value).reset_index(drop=True)
            
    return df",0.5432930589,
1758,read table,"def make_wormbase_and_format(original_gff, editing_annotation_file):
    """"""
    pulls just the wormbase annotations
    need to reformat the chromosome names from 'I, II' to 'chrI and chrII etc.'
    """"""
    original_gff_df = pd.read_table(original_gff,names=gffhead, comment='#')
    print(""size of the original gff: {}"".format(original_gff_df.shape[0]))  # wow... lots of entries
    wormbase = original_gff_df[original_gff_df['source']=='WormBase']
    wormbase['seqname'] = 'chr'+wormbase['seqname'].replace('MtDNA','M')
    wormbase[['start','end']] = wormbase[['start','end']].astype(int)  # change positions to INT from FLOAT
    wormbase.sort_values(by=['seqname','start','end'], inplace=True)
    wormbase.to_csv(editing_annotation_file, sep='\t', index=None, header=None)

    print(""size of the wormbase gff: {}"".format(wormbase.shape[0]))  # better...
    
if not os.path.exists(editing_annotation_file):
    make_wormbase_and_format(original_gff, editing_annotation_file)",0.542576313,
1758,read table,"# Import the fuel map keeping the coordinates as strings so as not to lose precision with float conversion
def ImportPreserve(FileName,labelName):
    DF=pd.read_table(FileName, header=None)
    DF.columns=['Row']
    DF[""Eastern""]=DF[""Row""].apply(lambda x: x.split("" "")[0])
    DF[""Northern""]=DF[""Row""].apply(lambda x: x.split("" "")[1])
    DF[labelName]=DF[""Row""].apply(lambda x: x.split("" "")[2])
    del DF[""Row""]
    return DF",0.5410671234,
1758,read table,"def ImportXYZ(FileName,labelName):
    DF=pd.read_table(FileName, header=None)
    DF.columns=['Row']
    DF[""Eastern""]=DF[""Row""].apply(lambda x: float(x.split("" "")[0]))
    DF[""Northern""]=DF[""Row""].apply(lambda x: float(x.split("" "")[1]))
    DF[labelName]=DF[""Row""].apply(lambda x: x.split("" "")[2])
    del DF[""Row""]
    return DF",0.5410671234,
1758,read table,"def create_saf_file(original_gff, saf_file):
    original_gff_df = pd.read_table(original_gff,names=gffhead, comment='#')
    print(""size of the original gff: {}"".format(original_gff_df.shape[0]))  # wow... lots of entries
    wormbase = original_gff_df[(original_gff_df['source']=='WormBase') & (original_gff_df['feature']=='gene')]
    wormbase['seqname'] = 'chr'+wormbase['seqname'].replace('MtDNA','M')
    wormbase[['start','end']] = wormbase[['start','end']].astype(int)  # change positions to INT from FLOAT
    wormbase['GeneID'] = wormbase['attribute'].str.extract("":([\w\d\.]+)"")
    wormbase = wormbase[['GeneID','seqname','start','end','strand']]
    wormbase.to_csv(saf_file, sep='\t', header=False, index=False)

if not os.path.exists(for_differential_expression):
    create_saf_file(original_gff, for_differential_expression)",0.5355345011,
1758,read table,"def loadData(path):
    column_names = [""ownerId"", ""contentId"", ""rating"", ""timestamp""]
    opened = pd.read_table(path, encoding=""latin1"", sep="" "", names=column_names, engine=""python"")

    opened.timestamp = pd.to_datetime(opened.timestamp, unit='s')
    
    opened = opened.sort_values('timestamp')

    opened.ownerId = opened.ownerId.apply(str)
    opened.contentId = opened.contentId.apply(int)
    opened.contentId = opened.contentId.apply(str)

    return opened",0.5349448919,
1758,read table,"def load_data(filen = FILE_DIR + 'good_data.fits'):
    """"""
    INPUT: files created in bright_moon_cont.py
    The following files can be loaded:
    bright_data.fits: when sky > 2.5*dark sky (defined as 2.78x10^17erg/s/cm2/A)
    dark_data.fits: Sun Alt < -18, Moon Alt < -18
    gray_data.fits: Moon Alt > 0, sky < 2.5*dark sky
    good_data.fits: All sky measurements on BOSS that are 'good'
    
    The files include required meta data and the measured continuum between 460-480nm
    
    OUTPUT: fits table with sky from 460-480nm from DESI sky model included
    """"""
    data = astropy.table.Table.read(filen)
    moon_zenith = 90-data['MOON_ALT']
    data['MOON_ZENITH'] = astropy.table.Column(moon_zenith.astype(np.float32), unit='deg')
    # Compute the pointing zenith angle in degrees.
    obs_zenith = 90 - data['ALT']
    data['OBS_ZENITH'] = astropy.table.Column(obs_zenith.astype(np.float32), unit='deg')

    mphase = np.arccos(2 * data['MOON_ILL'] - 1) / np.pi
    data['MPHASE'] = astropy.table.Column(mphase.astype(np.float32))
    moon_sep = data['MOON_SEP']
    
    cont_level = [get_continuum(get_sky_model(line)) for line in data]
    
    data['MODEL_SKY'] = astropy.table.Column(np.array(cont_level).astype(np.float32), unit=sky_unit)
    
    return data

data = load_data()",0.5329850912,
2445,unit vectors,"def turn(state):
    units = sorted(state.units)
    
    for unit_position in units:
        # Check unit wasn't destroyed in a previous attack
        if unit_position in state.units:
            new_position = next_position(state, unit_position)
            state = move_unit(state, unit_position, new_position)
            state = attack(state, new_position)
            
    return state",0.485057056,
2445,unit vectors,"def sent_vec(sent):
    wv_res = np.zeros(glove_model.vector_size)
    ctr = 1
    for w in sent:
        if w in glove_model:
            ctr += 1
            wv_res += glove_model[w]
    wv_res = wv_res/ctr
    #return (wv_res, ctr)
    return wv_res",0.4614808559,
2445,unit vectors,"def sent_vec_w2v(sent):
    wv_res = np.zeros(w2v.vector_size)
    ctr = 1
    for w in sent:
        if w in w2v:
            ctr += 1
            wv_res += w2v[w]
    wv_res = wv_res/ctr
    return wv_res",0.4614808559,
2445,unit vectors,"# Convert RGB to BGR. We need exchange R and B leaving G as it is.
def convertImage(BGR_array):
    imageNp = np.zeros_like(BGR_array)
    imageNp[:,:, 0] = BGR_array[:,:, 2]
    imageNp[:,:, 1] = BGR_array[:,:, 1]
    imageNp[:,:, 2] = BGR_array[:,:, 0]
    return imageNp

plt.imshow(convertImage(image))",0.459852159,
2445,unit vectors,"def BGR2RGB(Input):
   output = np.zeros(Input.shape);
   output[:,:,0] = Input[:,:,2] 
   output[:,:,1] = Input[:,:,1] 
   output[:,:,2] = Input[:,:,0]
   output = output.astype('uint8')
   return output
pic_ori = cv2.imread('./images/origin/Office.png')
pic_bil = cv2.imread('./Bi_Office.png')
pic_pro = cv2.imread('./Pr_Office.png')
# plot the image out
plt.figure(figsize=(50,32))
plt.subplot(311),plt.imshow(BGR2RGB(pic_ori))
plt.title('Original Image'), plt.xticks([]), plt.yticks([])
plt.subplot(312),plt.imshow(BGR2RGB(pic_bil))
plt.title('Bilinear Image'), plt.xticks([]), plt.yticks([])
plt.subplot(313),plt.imshow(BGR2RGB(pic_pro))
plt.title('Proposed Image'), plt.xticks([]), plt.yticks([])",0.4596341252,
2445,unit vectors,"def wrapper_func(theta):
    return(CostFunction(df.X, df.Y, theta[0], theta[1]))",0.4592118859,
2445,unit vectors,"def BGR2RGB(Input):
   output = np.zeros(Input.shape);
   output[:,:,0] = Input[:,:,2] 
   output[:,:,1] = Input[:,:,1] 
   output[:,:,2] = Input[:,:,0]
   output = output.astype('uint8')
   return output",0.4584608674,
2445,unit vectors,"def rgb2cmy(image):
    normalized = np.zeros(image.shape, dtype='float')

    normalized[:,:,0] = image[:,:,0] / np.max(image[:,:,0])
    normalized[:,:,1] = image[:,:,1] / np.max(image[:,:,1])
    normalized[:,:,2] = image[:,:,2] / np.max(image[:,:,2])
    
    return np.ones(image.shape, dtype='float') - normalized",0.4563351274,
2445,unit vectors,"def with_bias(df):
    X = df.as_matrix()
    return np.hstack([np.ones((X.shape[0], 1)), X])",0.4512088895,
2445,unit vectors,"def f(x):
    if (x.data > 0).all():
        return torch.sin(x)
    else:
        return torch.cos(x)",0.450285852,
2405,training data,"def plot_lat_lon():
    global training_data
    data = training_data[(training_data.longitude < 0) & (training_data.latitude > 0)]
    plt.figure(figsize=(7,6))
    for klass in ['low', 'medium', 'high']:
        subdata = data[data.interest_level == klass]
        plt.scatter(subdata['longitude'], subdata['latitude'], alpha=0.4)
    plt.legend(['low', 'medium', 'high'])
    plt.show()
plot_lat_lon()",0.5628694892,
2405,training data,"def training_routine(args, path, train_datalist, val_datalist):
    
    #open files and load content
    
    # Create the network
    if args.load_disk_model==True:
        
        
        load_model = torch.load(args.load_checkpoint_filename, map_location=lambda storage, loc: storage)
        #load_model = torch.load('ChexNet_model2.pytorch')
        model = DenseNet121(args)
        new_state_dict = OrderedDict()
        
        tmp = list(model.state_dict().keys())
            
        for i,(k, v)  in enumerate(load_model.items()):
            #print (k)
            name = k[7:] # remove `module.`
            new_state_dict[tmp[i]] = v
        
        # load params
        model.load_state_dict(new_state_dict)
    elif args.load_disk_model == False:
        model = DenseNet121(args)  
    
    #Initialize weitgths
    #my_model.initialize_weigths()
    
    
    #Choose the loss function / optimizer
    loss = nn.BCELoss(size_average = True)
    
    #choose optimizer
    optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),
                            lr=args.learn_rate,
                            weight_decay=args.l2)
#     optim = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()),
#                              lr=args.learn_rate,
#                              weight_decay=args.l2,
#                              momentum = 0)
    scheduler = ReduceLROnPlateau(optim, factor = 0.1, patience = 5, mode = 'min')
                         
    print (""Created Neural Network arquitecture"")
    
    if torch.cuda.is_available():
        # Move the network and the optimizer to the GPU
        print (""Moving to GPU"")
        model = model.cuda()
        model = torch.nn.DataParallel(model).cuda()
        loss = loss.cuda()
    
    dataset = ChestXray(args, train_datalist, path, is_val=False)
    
    
    data_loader = torch.utils.data.DataLoader(
                    dataset, batch_size=args.batch_size, shuffle=True,
                    num_workers=args.num_workers, pin_memory=args.pin_memory)
    
    print (""Created data objects"")
    val_losses = []
    losses= []
    for epoch in range(args.epochs): 
        model.train()
        t0 = datetime.datetime.now()
        losses = []
        for i,(input_val,labels) in enumerate(data_loader): 

            #if transform_1 using, we got a vector of 5 DIM
            #and we only need a 4 DIM
            if args.transform == 'transform_1':  
                bs, n_crops, c, h, w = input_val.size() #e.g(4,10,3,224,244)
                input_val = input_val.view(-1, c, h, w) #e.g(40,3,224,224)
                bs, n_classes = labels.size() #e.g(4,14)
                labels = labels.unsqueeze(2).repeat(1, n_crops, 1 )
                labels = labels.contiguous().view(bs*n_crops, n_classes)
                
            prediction = model(to_variable(input_val))

            #print(""Finished forward pass"")
            #print (prediction.shape)
            
            train_loss = loss(prediction, to_variable(labels))
            optim.zero_grad()# Reset the gradients NEVER FORGET THIS
            train_loss.backward()
            optim.step() # Update the network
            
            losses.append(train_loss.data.cpu().numpy())

            
            if i % 400 == 0:
                print('Minibatch ',i,train_loss.data.cpu().numpy())
            #if i % 1200 == 0:
                #val_loss = validate_routine(model, args, val_datalist, path)
                #scheduler.step(np.asscalar(np.mean(val_loss)))
                #model.train()
            
        val_loss = np.asscalar(np.mean(validate_routine(model, args, val_datalist, path)))   
        
        val_losses.append(val_loss)
        print ('EPOCH', end='\t')
        print (""Epoch {} Train Loss: {:.4f}"".format(epoch, np.asscalar(np.mean(losses))), end='\t')
        print (""Epoch {} Validation Loss: {:.4f}"".format(epoch, val_loss), end='\t')
        print (""Epoch Time: {}"".format(datetime.datetime.now()-t0))
        if min(val_losses) == val_loss:
            print (""Saving model on Disk"")
            torch.save(model.state_dict(), args.save_checkpoint_filename)
        torch.save(model.state_dict(), args.save_checkpoint_filename2)
    return model,losses",0.5456756949,
2405,training data,"def run_epoch_confidences(iterator: Iterator):
        is_train = iterator.train
        batch_accuracies = []
        batch_losses = []
        epoch_start = time.time()
        for batch in iterator:
            input_dict = {}
            lengths_dict = {}
            if hasattr(batch, 'text'):
                text, lengths = batch.text
                
                #IMPORTANT.  ADJUSTS BETWEEN DATASETS
#                 text = text.data.cpu().numpy().tolist()
#                 for i, group in enumerate(text):
#                     for j, item in enumerate(group):
#                         text[i][j]=asr_to_clean_vocab[item]
#                 text = Variable(torch.from_numpy(np.asarray(text))).cuda()
                
                input_dict['text'] = text
                lengths_dict['text'] = lengths

            page = batch.page
            
            #IMPORTANT.  ADJUSTS BETWEEN DATASETS
#             page = page.data.cpu().numpy()
#             page = [asr_to_clean[i] for i in page]
#             page = Variable(torch.from_numpy(np.asarray(page))).cuda()
            
            
            qnums = batch.qnum.cuda()
            #if hasattr(batch, 'confidence'):
            confidences = batch.confidence
            #else:
            #    confidences = torch.FloatTensor(batch.text[0].shape).fill_(1)
                #print (""Missing confidences"")
                #raise
            
            if is_train:
                model.zero_grad()

            out = model(input_dict, lengths_dict, qnums, confidences,)
            _, preds = torch.max(out, 1)
            accuracy = torch.mean(torch.eq(preds, page).float()).data[0]
            batch_loss = loss_function(out, page)
            if is_train:
                batch_loss.backward()
                torch.nn.utils.clip_grad_norm(model.parameters(), .25)
                optimizer.step()

            batch_accuracies.append(accuracy)
            batch_losses.append(batch_loss.data[0])

        epoch_end = time.time()

        return np.mean(batch_accuracies), np.mean(batch_losses), epoch_end - epoch_start",0.5427991152,
2405,training data,"def run_epoch(iterator: Iterator):
        is_train = iterator.train
        batch_accuracies = []
        batch_losses = []
        epoch_start = time.time()
        for batch in iterator:
            input_dict = {}
            lengths_dict = {}
            if hasattr(batch, 'text'):
                text, lengths = batch.text
                
                #IMPORTANT.  ADJUSTS BETWEEN DATASETS
                text = text.data.cpu().numpy().tolist()
                for i, group in enumerate(text):
                    #print(group)
                    for j, item in enumerate(group):
                        text[i][j]=asr_to_clean_vocab[item]
                text = Variable(torch.from_numpy(np.asarray(text))).cuda()
  
                input_dict['text'] = text
                lengths_dict['text'] = lengths
            page = batch.page
            qnums = batch.qnum.cuda()

            if is_train:
                model.zero_grad()

            out = model(input_dict, lengths_dict, qnums)
            _, preds = torch.max(out, 1)
            
           #IMPORTANT.  ADJUSTS BETWEEN DATASETS
            page = page.data.cpu().numpy()
            page = [asr_to_clean[i] for i in page]
            page = Variable(torch.from_numpy(np.asarray(page))).cuda()
            
            
            accuracy = torch.mean(torch.eq(preds, page).float()).data[0]
            batch_loss = loss_function(out, page)
            if is_train:
                batch_loss.backward()
                torch.nn.utils.clip_grad_norm(model.parameters(), .25)
                optimizer.step()

            batch_accuracies.append(accuracy)
            batch_losses.append(batch_loss.data[0])

        epoch_end = time.time()

        return np.mean(batch_accuracies), np.mean(batch_losses), epoch_end - epoch_start",0.5399870276,
2405,training data,"def training_routine(args, path, train_datalist, val_datalist):
    
    #open files and load content
    
    # Create the network
    if args.load_disk_model==True:
        load_model = torch.load(args.load_checkpoint_filename, map_location=lambda storage, loc: storage)
        #load_model = torch.load('ChexNet_model2.pytorch')
        model = DenseNet121(args)
        new_state_dict = OrderedDict()
        for k, v in load_model.items():
            #print (k)
            name = k[7:] # remove `module.`
            new_state_dict[name] = v
        # load params
        model.load_state_dict(new_state_dict)
    elif args.load_dis_model == False:
        model = DenseNet121(args)  
    
    #Initialize weitgths
    #my_model.initialize_weigths()
    
    
    #Choose the loss function / optimizer
    loss = nn.BCELoss(size_average = True)
    
    #choose optimizer
    optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),
                             lr=args.learn_rate,
                             weight_decay=args.l2)
    scheduler = ReduceLROnPlateau(optim, factor = 0.1, patience = 10, mode = 'min')
                         
    print (""Created Neural Network arquitecture"")
    
    if torch.cuda.is_available():
        # Move the network and the optimizer to the GPU
        print (""Moving to GPU"")
        model = model.cuda()
        model = torch.nn.DataParallel(model).cuda()
        loss = loss.cuda()
    
    dataset = ChestXray(args, train_datalist, path, is_val=False)
    
    
    data_loader = torch.utils.data.DataLoader(
                    dataset, batch_size=args.batch_size, shuffle=True,
                    num_workers=args.num_workers, pin_memory=args.pin_memory)
    
    print (""Created data objects"")
    
    losses= []
    for epoch in range(args.epochs): 
        model.train()
        t0 = datetime.datetime.now()
        losses = []
        for i,(input_val,labels) in enumerate(data_loader): 

            #if transform_1 using, we got a vector of 5 DIM
            #and we only need a 4 DIM
            if args.transform == 'transform_1':  
                bs, n_crops, c, h, w = input_val.size() #e.g(4,10,3,224,244)
                input_val = input_val.view(-1, c, h, w) #e.g(40,3,224,224)
                bs, n_classes = labels.size() #e.g(4,14)
                labels = labels.unsqueeze(2).repeat(1, n_crops, 1 )
                labels = labels.contiguous().view(bs*n_crops, n_classes)
                
            prediction = model(to_variable(input_val))

            #print(""Finished forward pass"")
            #print (prediction.shape)
            
            train_loss = loss(prediction, to_variable(labels))
            optim.zero_grad()# Reset the gradients NEVER FORGET THIS
            train_loss.backward()
            optim.step() # Update the network
            
            losses.append(train_loss.data.cpu().numpy())

            
            if i % 200 == 0:
                print('Minibatch ',i,train_loss.data.cpu().numpy())
            if i % 800 == 0:
                val_loss = validate_routine(model, args, val_datalist, path)
                scheduler.step(np.asscalar(np.mean(val_loss)))
                model.train()
            
            
        print ('EPOCH', end='\t')
        print (""Epoch {} Train Loss: {:.4f}"".format(epoch, np.asscalar(np.mean(losses))), end='\t')
        print (""Epoch {} Validation Loss: {:.4f}"".format(epoch, np.asscalar(np.mean(val_loss))), end='\t')
        print (""Epoch Time: {}"".format(datetime.datetime.now()-t0))
        
        torch.save(model.state_dict(), args.save_checkpoint_filename)
        
    return model,losses",0.5387916565,
2405,training data,"def train_net(n_epochs):
    print_every = 40
    save_every = 100
    steps = 0
    running_loss = 0
    # prepare the net for training
    net.train()

    for epoch in range(n_epochs):  # loop over the dataset multiple times
        
        start_t = time.time()

        # train on batches of data, assumes you already have train_loader
        for batch_i, data in enumerate(train_loader):
            steps += 1

            # get the input images and their corresponding labels
            images = data['image']
            key_pts = data['keypoints']

            # flatten pts
            key_pts = key_pts.view(key_pts.size(0), -1)

            # convert variables to floats for regression loss
            key_pts = key_pts.type(torch.FloatTensor)
            images = images.type(torch.FloatTensor)

            # forward pass to get outputs
            #output_pts = net(images)
            output_pts,_,_,_,_ = net.forward(images.to(device))#.to(device) is for using GPU

            # calculate the loss between predicted and target keypoints
            #loss = criterion(output_pts, key_pts)
            loss = criterion(output_pts, key_pts.to(device))#.to(device) is for using GPU

            # zero the parameter (weight) gradients
            optimizer.zero_grad()
            
            # backward pass to calculate the weight gradients
            loss.backward()

            # update the weights
            optimizer.step()

            # print loss statistics
            # to convert loss into a scalar and add it to the running_loss, use .item()
            running_loss += loss.item()
            
            if steps % print_every == 0:
                # Model in inference mode, dropout is off
                net.eval()

                accuracy = 0
                test_loss = 0
                #for ii, (images, labels) in enumerate(testloader):
                for batch_it, datat in enumerate(test_loader):
                    # get sample data: images and ground truth keypoints
                    images = datat['image']
                    key_pts = datat['keypoints']

                    # convert images to FloatTensors
                    images = images.type(torch.FloatTensor)
                    #images = images.resize_(1, 224,224)
                    #images = images.to(device)

                    key_pts = key_pts.type(torch.FloatTensor)

                    # forward pass to get net output
                    output_pts,_,_,_,_ = net.forward(images.to(device))
                    #output_pts = net.forward(images.to(device))

                    # reshape to batch_size x 68 x 2 pts
                    output_pts = output_pts.view(output_pts.size()[0], 68, -1)

                    loss = criterion(output_pts, key_pts.to(device))#.to(device) is for using GPU 
                    # to convert loss into a scalar and add it to the running_loss, use .item()
                    test_loss += loss.item()
                    #print('testing Avg. Loss: {:f}'.format(running_loss/1000))
                    #running_loss = 0.0

                train_avg_loss=running_loss/print_every
                test_avg_loss = test_loss/len(test_loader)
                print(""Epoch: {}/{}.. "".format(epoch+1, n_epochs),
                      ""Training Loss: {:.5f}.. "".format(train_avg_loss),
                      ""Test Loss: {:.5f}.. "".format(test_avg_loss),
                      #""Test Accuracy: {:.3f}"".format(accuracy/len(testloader))
                         )
                train_avgloss_list.append(train_avg_loss)
                test_avgloss_list.append(test_avg_loss)

                running_loss = 0

                # Make sure dropout is on for training
                net.train()
                
        time_cost = time.time() - start_t
        print(""1 Epoch time cost {:.3f}"".format(time_cost))
        
        if (epoch+1) % save_every == 0:
            save_time = datetime.datetime.now().strftime(""%Y-%m-%d-%H-%M-%S"")
            #print(save_time)
            ## TODO: change the name to something uniqe for each new model
            model_dir = 'saved_models/'
            model_name = save_time+'_keypoints_model_'+'e'+str(epoch)+'totale'+str(n_epochs)+'_'+str(train_batch_size)+'_l_'+str(learning_rate)+'.pt'
            print(model_name)
            # after training, save your model parameters in the dir 'saved_models'
            torch.save(net.state_dict(), model_dir+model_name)

    print('Finished Training')",0.5380803347,
2405,training data,"def train(epoch):
    losses = []
    network.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = Variable(data), Variable(target)
        optimizer.zero_grad()
        output = network(data)
        loss = F.cross_entropy(output, target)
        losses.append(loss.data[0])
        loss.backward()
        optimizer.step()
        if batch_idx % 100 == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.data[0]))
    
    return losses

def test():
    network.eval()
    test_loss = 0
    correct = 0
    for data, target in test_loader:
        data, target = Variable(data, volatile=True), Variable(target)
        output = network(data)
        test_loss += F.cross_entropy(output, target, size_average=False).data[0] # sum up batch loss
        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability
        correct += pred.eq(target.data.view_as(pred)).cpu().sum()

    test_loss /= len(test_loader.dataset)
    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))
    
    incorrect_perc = (len(test_loader.dataset) - correct) / len(test_loader.dataset)
    
    return incorrect_perc",0.5376576185,
2405,training data,"def predict(train):
    tr_train,tr_test = load_ml100k.get_train_test(train, random_state=34)
    tr_predicted0 = regression.predict(tr_train)
    tr_predicted1 = regression.predict(tr_train.T).T
    tr_predicted2 = corrneighbours.predict(tr_train)
    tr_predicted3 = corrneighbours.predict(tr_train.T).T
    tr_predicted4 = norm.predict(tr_train)
    tr_predicted5 = norm.predict(tr_train.T).T
    stack_tr = np.array([
        tr_predicted0[tr_test > 0],
        tr_predicted1[tr_test > 0],
        tr_predicted2[tr_test > 0],
        tr_predicted3[tr_test > 0],
        tr_predicted4[tr_test > 0],
        tr_predicted5[tr_test > 0],
        ]).T

    lr = linear_model.LinearRegression()
    lr.fit(stack_tr, tr_test[tr_test > 0])

    stack_te = np.array([
        tr_predicted0.ravel(),
        tr_predicted1.ravel(),
        tr_predicted2.ravel(),
        tr_predicted3.ravel(),
        tr_predicted4.ravel(),
        tr_predicted5.ravel(),
        ]).T

    return lr.predict(stack_te).reshape(train.shape)",0.5309401155,
2405,training data,"def train_net(n_epochs):

    loss_over_time = [] # to track the loss as the network trains
#     train_accuracy = []

    # prepare the net for training
    net.train()

    for epoch in range(n_epochs):  # loop over the dataset multiple times
        
        running_loss = 0.0

        # train on batches of data, assumes you already have train_loader
        for batch_i, data in enumerate(train_loader):
            # get the input images and their corresponding labels
            images = data['image']
            key_pts = data['keypoints']

            # flatten pts
            key_pts = key_pts.view(key_pts.size(0), -1)

            # convert variables to floats for regression loss
            # key_pts = key_pts.type(torch.cuda.FloatTensor)
            key_pts = key_pts.type(torch.FloatTensor).to(device)
            #images = images.type(torch.cuda.FloatTensor)            
            images = images.type(torch.FloatTensor).to(device)

            # forward pass to get outputs
            output_pts = net(images)

            # calculate the loss between predicted and target keypoints
            loss = criterion(output_pts, key_pts)

            # zero the parameter (weight) gradients
            optimizer.zero_grad()
            
            # backward pass to calculate the weight gradients
            loss.backward()

            # update the weights
            optimizer.step()

            # print loss statistics
            # to convert loss into a scalar and add it to the running_loss, use .item()
            running_loss += loss.item()
            if batch_i % 10 == 9:    # print every 10 batches
                avg_loss = running_loss/10
                # record and print the avg loss over the 10 batches
                loss_over_time.append(avg_loss)                
                print('Epoch: {}, Batch: {}, Avg. Loss: {}'.format(epoch + 1, batch_i+1, avg_loss))
                running_loss = 0.0

    print('Finished Training')
    return loss_over_time",0.5307400823,
2405,training data,"def test_model_on_breast_cancer_data(model):
    X, y = get_breast_cancer_data()
    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=0)
    for C in [0.1, 1, 10, 100, 250]:
        clf = model(C=C).fit(X_train, y_train)
        print(f'Breast cancer Logistic regresion with C = {C}')
        print(""Train Accurary: {}"".format(clf.score(X_train, y_train)))
        print(""Test Accurary: {}"".format(clf.score(X_test, y_test)))

test_model_on_breast_cancer_data(sklearn.linear_model.LogisticRegression)",0.5292785168,
822,homework setup and re introduction to python,"import unittest

def even_numbers(n):
    """"""
    Return a list of the first n even numbers 
    
    Zero is considered to be the first even number.

    >>> even_numbers(5)
    [0,2,4,6,8]
    """"""    
    r = [2 * x for x in range(n)]
    r[n // 2] = 3   # <-- evil bug, puts number '3' in the middle
    return r

class MyTest(unittest.TestCase):

    def test_long_list(self):
        self.assertEqual(even_numbers(5),[0,2,4,6,8])",0.458344996,
822,homework setup and re introduction to python,"for i in range(20):
    succ, tot, _ = successRate(5*i, 5*i+4, dfPenaltyShots)
    print(5*i + 2.5, binom.cdf(succ, tot, 1756/2307))",0.452976048,
822,homework setup and re introduction to python,"star_size = 2 * size - 1
def fn3():
    format_string = '{:^'+str(star_size)+'}'
    for i in range(1,size+1):
        print(format_string.format('*'*(2*i-1)))",0.4510479569,
822,homework setup and re introduction to python,"star_size = 2 * size - 1
def fn4():
    format_string = '{:^'+str(star_size)+'}'
    for i in range(1,size+1):
        print(format_string.format('*'*(star_size - 2*(i-1))))",0.4510479569,
822,homework setup and re introduction to python,"Nz, Ny, Nx = SHP = len(z)-1, len(y)-1, len(x)-1
IBOUND = np.ones(SHP)
IBOUND[0, :, :] = -1 # fixed head maintained in top layer (above confining layer)
IBOUND[-1, :, 0] = -1 # fixed head in aquifer at x=0

HI = np.zeros(SHP)
HI[-1, :, 0] = hw # fixed level of IJssel Lake
HI[ 0, :, :] = hp # fixed polder level

FQ = np.zeros(SHP) # no fixed flows

kAquif = k;
kConf  = d/c;
kTop   = 100;  #immaterial

K = np.array([kTop, kConf, kAquif]).reshape(Nz, 1, 1) * np.ones(SHP)",0.4495251179,
822,homework setup and re introduction to python,"print (9 + sum(9 - int(10**(1-1.0/n)) for n in range(2, 22)))",0.448161304,
822,homework setup and re introduction to python,"M = {""0""*m + ""1""*n for m in range(3) for n in range(4) if m < n }
print('M = ', M)
print('lexp(M,2) = ')
lexp(M,2)",0.4449566901,
822,homework setup and re introduction to python,"for epoch in range(10):
    total_cost = 0
    
    for i in range(total_batch):
        batch_xs, batch_ys = Xtr[i*batch_size:(i+1)*batch_size], Ytr_onehot[i*batch_size:(i+1)*batch_size]
        _, curr_loss = sess.run([optimizer, cost],
                               feed_dict={X: batch_xs,
                                          Y: batch_ys,
                                          dropout_prob: 0.7})
        total_cost += curr_loss
    epoch = epoch + 1
    Avg_cost = total_cost/total_batch
    print('Epoch:', '%04d' % (epoch),'Avg. cost = ', '{:,.3f}'.format(Avg_cost))
    correctness = tf.equal(tf.argmax(model, 1), tf.argmax(Y,1))
    accuracy = tf.reduce_mean(tf.cast(correctness, tf.float32))
    Test_Accuracy = sess.run(accuracy, feed_dict={X: Xte,Y: Yte_onehot,dropout_prob: 1})
    print('Test Accuracy', Test_Accuracy)
    Epochs.append(epoch)
    Avg_costs.append(Avg_cost)
    Test_Accuracies.append(Test_Accuracy)",0.4446741939,
822,homework setup and re introduction to python,"rare_diseases = []
for num in range(27):
    if num == 26:
        sym = ""0-9""
    else:
        sym = chr(ord('A')+num)
    sys.stdout.write(""Processing: %s\r"" % (sym))
    url = baseUrl + sym
    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    lists = soup.find('ul', class_=""listing-diseases"")
    for entry in lists.findAll(""li""):
        if not ""\n -"" in entry.text:
            rare_diseases.append(entry.text.strip())
    time.sleep(1)",0.444464922,
822,homework setup and re introduction to python,"params = {'n_estimators':[10*i for i in range(1, 4)],
         'learning_rate': [i/20 for i in range(4, 9)],
         'max_depth':[i for i in range(2, 5)]}
params",0.4436771274,
1439,part outliers,"def detect_outliers(vector):
    Q1 = np.percentile(vector, q=25)
    Q3 = np.percentile(vector, q=75)
    IQR = Q3 - Q1
    mask = (vector < (Q1 - 1.5*IQR)) | (vector > (Q3 + 1.5*IQR))
    return vector[mask]

print detect_outliers(np.random.normal(size=100)) # 100 random normal values
print detect_outliers(np.random.normal(size=500)) # 500 random normal values
print detect_outliers(np.random.normal(size=2000)) # 2000 random normal values",0.5106543303,
1439,part outliers,"def find_outliers(col, lo_q, hi_q):
    """"""
    Return all values in a column lower than a low quantile and higher than a high one
    :param col: the column to retrieve outliers from
    :type col: Series
    :param lo_q: low quantile value
    :param hi_q: high quantile value
    :type lo_q, hi_q: float
    :return: (Series, Series)
    """"""
    lo_quant, hi_quant = col.quantile([lo_q, hi_q])
    return col[col < (lo_quant)], col[col > (hi_quant)]


def print_outliers(col, lo_q, hi_q):
    """"""
    Print outliers nicely.
    :param col: the column to retrieve outliers from
    :type col: Series
    :param lo_q: low quantile value
    :param hi_q: high quantile value
    :type lo_q, hi_q: float
    :return: None
    """"""
    fmt_str = (""Low {c} outliers (below {mn} quantile):\n{lo}""
               ""\nHigh {c} outliers (above {mx} quantile):\n{hi}""
               ""\nThere are {lnlo} low {c} outliers and {lnhi} ""
               ""high {c} outliers"")
    lows, highs = find_outliers(col, lo_q, hi_q)
    print fmt_str.format(c=col.name.upper(), mn=lo_q, mx=hi_q, lo=lows,
                         hi=highs, lnlo=len(lows), lnhi=len(highs))

print_outliers(gre, 0.1, 0.9)
print_outliers(gre, 0.05, 0.95)
print_outliers(gpa, 0., 0.9)
print_outliers(gpa, 0.05, 0.95)",0.4865608811,
1439,part outliers,"def splitTrainingData(df, train_data_prcnt=80):
    msk = np.random.rand(len(df)) < train_data_prcnt/100
    train = df[msk]
    test = df[~msk]
    return train, test",0.4802069664,
1439,part outliers,"# Defining crieteria for outliers: points having error values more than 2 standard deviation.

def is_outliers(data, m=2):
    return data[abs(data - numpy.mean(data)) > m * numpy.std(data)]",0.477897495,
1439,part outliers,"def apply_threshold(proba_array, threshold=0.5):
    ar = proba_array.copy()
    ar[ar>=threshold]=1
    ar[ar<threshold]=0
    return ar",0.4732831419,
1439,part outliers,"def apply_threshold(heatmap, threshold):
    heated=np.copy(heatmap)
    # Zero out pixels below the threshold
    heated[heated <= threshold] = 0
    # Return thresholded map
    return heated
def draw_labeled_bboxes(img, labels):
    # Iterate through all detected cars
    for car_number in range(1, labels[1]+1):
        # Find pixels with each car_number label value
        nonzero = (labels[0] == car_number).nonzero()
        # Identify x and y values of those pixels
        nonzeroy = np.array(nonzero[0])
        nonzerox = np.array(nonzero[1])
        # Define a bounding box based on min/max x and y
        bbox = ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy)))
        #car.heatmap.append(bbox)
        # Draw the box on the image
        cv2.rectangle(img, bbox[0], bbox[1], (255,0,0), 6)
    # Return the image
    return img",0.4732066691,
1439,part outliers,"def reject_outliers(data, m=2):
    return data[abs(data - np.mean(data)) < m * np.std(data)]

def reject_xyz_outliers(x,y,z):
    return reject_outliers(x), reject_outliers(y), reject_outliers(z)

#Basic outlier reject and mean shift
def perform_preprocessing(x, y, z):
    x = reject_outliers(x)
    y = reject_outliers(y)
    z = reject_outliers(z)
    
    #zero mean 
    x = x - np.mean(x)
    y = y - np.mean(y)
    z = z - np.mean(z)
    
    return x,y,z

#Savitzky-Golay curve smoothing
def smooth_curve(x,y,z, window_size=51, polynomial=3):
    x_hat = scipy.signal.savgol_filter(x, window_size, polynomial)
    y_hat = scipy.signal.savgol_filter(y, window_size, polynomial)
    z_hat = scipy.signal.savgol_filter(z, window_size, polynomial)
    
    return x_hat, y_hat, z_hat",0.472446084,
1439,part outliers,"def reject_outliers(data, m=2):
    return data[abs(data - np.mean(data)) < m * np.std(data)]",0.4721872211,
1439,part outliers,"# Function to remove outlier data
def reject_outliers(data, m=2):
    return data[abs(data - np.mean(data)) < m * np.std(data)]",0.4721872211,
1439,part outliers,"#method to remove outliers
def reject_outliers(data, m=2):
    return data[abs(data - np.mean(data)) < m * np.std(data)]

prices_without_outliers = reject_outliers(prices)

sns.distplot(prices_without_outliers, bins=50, kde=True, color='r').set(xlim=(0, max(prices_without_outliers)))
plt.show()",0.4709081054,
142,building a model,"#Create the model
def create_model():
    model = Sequential()
    model.add(Conv2D(32, (5, 5), input_shape=(28, 28, 1), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.2))
    model.add(Flatten())
    model.add(Dense(128,  activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

# Creating and training the model with different Batch Sizes
def train_model_batch(model, batchSize):
    print(y_train.shape)
    history = model.fit(X_train, y_train, 
                        validation_data=(X_test, y_test),
                        epochs=10, batch_size=batchSize, verbose=0)
    return history

# Creating and training the model with different Weights Initializations
#  Weights initialization: https://keras.io/initializers/
def train_model_weights(weightsInit):
    if('xavier' in initType['Conv2D']):
        Conv = glorot_normal()
    elif('random' in initType['Conv2D']):
        Conv = RandomNormal()
    elif('zeros' in initType['Conv2D']):
        Conv = Zeros()
    else:
        Conv = he_normal()
    if('xavier' in initType['Dense']):
        Denses = glorot_normal()
    elif('random' in initType['Dense']):
        Denses = RandomNormal()
    elif('zeros' in initType['Dense']):
        Denses = Zeros()
    else:
        Denses = he_normal()
    
    model = Sequential()
    model.add(Conv2D(32, (5, 5),  kernel_initializer=Conv, input_shape=(28, 28, 1), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.2))
    model.add(Flatten())
    model.add(Dense(128, kernel_initializer=Denses,  activation='relu'))
    model.add(Dense(num_classes, kernel_initializer=Denses, activation='softmax'))
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=30, batch_size=batch, verbose=)
    return history",0.4853454828,
142,building a model,"def eval_hyperparam(learning_rate, n_epochs): 
    model = Sequential()
    model.add(Conv2D(32, (5, 5), input_shape=(28, 28, 1), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.2))
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))
    sgd = optimizers.SGD(lr=learning_rate) #using SGD as an optimizer
    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=n_epochs, batch_size=200, verbose=0)
    scores = model.evaluate(X_test, y_test, verbose=0)
    print(""Baseline Error: %.2f%%"" % (100-scores[1]*100))
    return history",0.4769075513,
142,building a model,"def run_model(model,name):
    global model_set
    m = model
    m.fit(X_train, y_train)
    start = time.time()

    total_score = m.score(X_test,y_test)
    pscore = [pattern % i for i in list(metrics.precision_score(y_test, m.predict(X_test),labels=lang,average=None))]
    rscore = [pattern % i for i in list(metrics.recall_score(y_test, m.predict(X_test),labels=lang,average=None))]
    fscore = [pattern % i for i in list(metrics.f1_score(y_test, m.predict(X_test),labels=lang,average=None))]
    end = time.time()
    t= pattern % (end - start)
    cvs = cross_val_score(m, X_test,y_test)

    r = dict(zip(cols,[name,t,total_score,pscore,rscore,fscore]))
    print('1. Check for Overfitting: {}\n'.format(m.score(X_train,y_train)))
    print('2. Test Score is: {}\n'.format(total_score))
    print('3. Classification Report:')
    print(classification_report(y_test, m.predict(X_test)))
    print('')
    print('4. Cross Val Score: {} ==> Avg: {} '.format(cvs,cvs.sum()/len(cvs)))
    print('')
    
    model_set = model_set.append(r,ignore_index=True)
    return r,m",0.4759817421,
142,building a model,"def cnn_model(img_rows, img_cols, img_channels):
    model = Sequential()
    model.add(Conv2D(16, (3, 3),activation='linear',kernel_initializer='he_uniform',
                     input_shape=(img_rows, img_cols, img_channels)))
    model.add(LeakyReLU(alpha=.01))   # add an advanced activation
    model.add(Conv2D(16, (3, 3),activation='linear',kernel_initializer='he_uniform'))
    model.add(LeakyReLU(alpha=.01))   # add an advanced activation
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.2)) # Avoid over-fitting 
    model.add(Conv2D(32, (3, 3),activation='linear',kernel_initializer='he_uniform'))
    model.add(LeakyReLU(alpha=.01))   # add an advanced activation
    model.add(Conv2D(32, (3, 3),activation='linear',kernel_initializer='he_uniform'))
    model.add(LeakyReLU(alpha=.01))   # add an advanced activation
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.2)) # Avoid over-fitting 
    model.add(Conv2D(64, (3, 3),activation='linear',kernel_initializer='he_uniform'))
    model.add(LeakyReLU(alpha=.01))   # add an advanced activation
    model.add(Conv2D(64, (3, 3),activation='linear',kernel_initializer='he_uniform'))
    model.add(LeakyReLU(alpha=.01))   # add an advanced activation
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Conv2D(128, (3, 3),activation='linear',kernel_initializer='he_uniform'))
    model.add(LeakyReLU(alpha=.01))   # add an advanced activation
    model.add(Conv2D(128, (3, 3),activation='linear',kernel_initializer='he_uniform'))
    model.add(LeakyReLU(alpha=.01))   # add an advanced activation
    model.add(MaxPooling2D(pool_size=(5, 5)))
    model.add(keras.layers.noise.GaussianNoise(0.6))
    model.add(Flatten())
    model.add(Dense(1024))
    model.add(LeakyReLU(alpha=.01))   # add an advanced activation
    model.add(Dropout(0.7)) # Avoid over-fitting  
    model.add(Dense(2))
    model.add(Activation('softmax'))

    return model


def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.gray,savepdf='output.pdf'):
    """"""
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """"""
    f=plt.figure()
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    
#   plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print(""Normalized confusion matrix"")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, ('%.2f' %cm[i, j]),
                 horizontalalignment=""center"",
                 color=""red"" if cm[i, j] > thresh else ""white"")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.show()
    f.savefig(savepdf)
   
    # This function will set up the weights for problems with unbalanced data. 
def get_class_weights(y):
    counter = Counter(y)
    majority = max(counter.values())
    return  {cls: float(majority/count) for cls, count in counter.items()}",0.4744935036,
142,building a model,"class Environment:
    def __init__(self, array, gamma=0.9):
        self.board = Board(array)
        self.top=np.shape(array)[0]
        self.right=np.shape(array)[1]
        self.gamma = gamma
        self.vf_matrix=Board(np.zeros(np.shape(array)))
    
    def show(self, state):
        import seaborn as sns

        fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12, 4))        
        
        # The board itself as a heat map
        axes = sns.heatmap(self.board.array, annot=True, 
                           cmap='bwr_r', ax=ax1, cbar=False)
        axes.set_xticks([], [])
        axes.set_yticks([], [])
        
        # The actor's position
        axes.plot([state.pos[0]+0.5], [self.top-state.pos[1]-0.5], 
                  'go', markersize=30)
        axes.set_title(""World of rewards"")
        
        # the actor's direction
        l = np.max([np.sqrt(state.d[0]**2 + state.d[1]**2), 1])
        d = 0.4/l
        axes.plot([state.pos[0]+0.5+d*state.d[0]], 
                  [self.top-state.pos[1]-0.5-d*state.d[1]], 
                  'go', markersize=15)

        # the value function's heat map
        axes = sns.heatmap(self.vf_matrix.array,
                           annot=True, cmap='bwr_r', ax=ax2)
        axes.set_xticks([], [])
        axes.set_yticks([], [])
        axes.set_title('Bellmann values')

        return """"

  
    def bellmann(self, pos):
        """"""
        The new value of a state according to the bellmann equation
        """"""

        r = self.reward_at(np.array(pos))
        if r == 1 or r == -1:
            return 0
        best_neighbour = np.max([
            self.reward_at(np.array(pos)+Move.actions()[a]) + 
            self.gamma * self.vf_matrix.get(*(np.array(pos) + Move.actions()[a]))
            for a in State(env, pos).options()])
        return best_neighbour

    def reward_at(self, pos):
        if pos[0] < 0 or pos[0] >= self.right or pos[1] < 0 or pos[1] >= self.top:
            return np.nan
        if np.isnan(self.board.get(*pos)):
            return np.nan
        else:
            return self.board.get(*pos)
    

    def success(self, pos):
        return self.reward_at(pos) == 1
       
    def failure(self, pos):
        return self.reward_at(pos) == -1
  
    def is_done(self, pos):
        return self.failure(pos) or self.success(pos)

    def clear_vf(self):
        self.vf_matrix=Board(np.zeros((self.top, self.right)))",0.4731906056,
142,building a model,"class ensemble():
    def __init__(self):
        self.models = []
        self.weights = []
        self.size = 0
        self.inputPipe = None
        
    def _loadModel(self, cycle, location='train_weights/train_'): #Function to load a specified classifier
        cycle = int(cycle)
        model = load_model(location + str(cycle) + '.h5')
        return model
    
    def _getWeights(self, value, mode='rec'): #How the weight is calculated.
        if mode == 'rec':
            return 1/value #Reciprocal of metric is a simple way of assigning larger weights to better metrics
        elif mode == 'uni':
            return 1 #Uniform weighting
        else:
            print(""Weight mode not recognised/supported"")
            return 0

    def buildEnsemble(self, results, size, weighting='rec', verbose=True):
        self.models = []
        weights = []

        dtype = [('cycle', int), ('result', float)]
        values = np.sort(np.array([(i, result['loss']) for i, result in enumerate(results)], 
                                  dtype=dtype), order=['result'])

        for i in range(min([size, len(results)])):
            self.models.append(self._loadModel(values[i]['cycle']))
            weights.append(self._getWeights(values[i]['result'], mode=weighting))
            if verbose: print(""Model {} is {} with loss = {}"". format(i, values[i]['cycle'], values[i]['result']))

        weights = np.array(weights)
        self.weights = weights/weights.sum() #normalise weights
        self.size = len(self.models)
    
    def addInputPipe(self, pipe):
        self.inputPipe = pipe

    def predict(self, X, n=-1): #Loop though each classifier and predict data class
        if not isinstance(self.inputPipe, types.NoneType): #Preprocess if necessary
            X = self.inputPipe.transform(X)
        
        pred = np.zeros((len(X), 1))
        
        if n == -1:
            n = len(self.models)+1
            
        models = self.models[0:n] #Use only specified number of classifiers
        weights = self.weights[0:n]/self.weights[0:n].sum() #Renormalise weights

        for i, model in enumerate(models): #Weight each model prediction
            pred += weights[i]*model.predict(X, verbose=0)
        
        return pred
    
    def save(self, filename, overwrite=False):
        if (len(glob.glob(filename + ""*.json"")) or len(glob.glob(filename + ""*.h5"")) or len(glob.glob(filename + ""*.pkl""))) and not overwrite:
            print(""Ensemble already exists with that name, call with overwrite=True to force save"")
        
        else:
            os.system(""rm "" + filename + ""*.json"")
            os.system(""rm "" + filename + ""*.h5"")
            os.system(""rm "" + filename + ""*.pkl"")
            
            for i, model in enumerate(self.models):
                model.save(filename + '_' + str(i) + '.h5')
                
            with open(filename + '_weights.pkl', 'w') as fout:
                pickle.dump(self.weights, fout)
                
            if not isinstance(self.inputPipe, types.NoneType):
                with open(filename + '_inputPipe.pkl', 'w') as fout:
                    pickle.dump(inputPipe, fout)
                    
    def load(self, filename):
                
        self.models = []
        
        for model in glob.glob(filename + '_*.h5'):
            self.models.append(load_model(model))
        
        with open(filename + '_weights.pkl', 'r') as fin:
            self.weights = weights = pickle.load(fin)
            
        try:
            with open(filename + '_inputPipe.pkl', 'r') as fin:
                self.inputPipe = pickle.load(fin)
        except IOError:
            self.inputPipe = None",0.4703811705,
142,building a model,"class model:
    
    def __init__(self, model):
        self.model = model
        self.x_train = None
        self.y_train = None
        self.x_test = None
        self.y_test = None
        self.y_pred_train = None
        self.y_pred_test = None
        self.train_score = None
        self.test_score = None
        self.train_score_log = None
        self.test_score_log = None
    
    def data_split(self, x, y, test_size):
        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(x, y, test_size=test_size)
    
    def score_reg(self):
        return self.train_score, self.test_score
    
    def score_log(self):
        self.train_score_log = metrics.r2_score(np.exp(self.y_train), np.exp(self.y_pred_train))
        self.test_score_log = metrics.r2_score(np.exp(self.y_test), np.exp(self.y_pred_test))
        return self.train_score_log, self.test_score_log
    
    def data_frame_convert(self):
        df_train = pd.DataFrame({'y_pred': self.y_pred_train, 'y_real': self.y_train})
        df_test = pd.DataFrame({'y_pred_test': self.y_pred_test, 'y_real_test': self.y_test})
        return self.train_score, self.test_score, df_train, df_test

    def data_frame_convert_log(self):
        df_train = pd.DataFrame({'y_pred': np.exp(self.y_pred_train), 'y_real': np.exp(self.y_train)})
        df_test = pd.DataFrame({'y_pred_test': np.exp(self.y_pred_test), 'y_real_test': np.exp(self.y_test)})
        return self.train_score_log, self.test_score_log, df_train, df_test
    
    def fit_model(self, x, y, test_size):
        self.data_split(x, y, test_size)
        self.model = self.model.fit(self.x_train, self.y_train)
        self.train_score = self.model.score(self.x_train, self.y_train)
        self.test_score = self.model.score(self.x_test, self.y_test)
        self.y_pred_train = self.model.predict(self.x_train)
        self.y_pred_test = self.model.predict(self.x_test)
    
def model_iterations(n, x, y, model_arg, log_bool=False):
    training_scores = [None]*n
    testing_scores = [None]*n

    for i in range(n):
        new_model = model(model_arg)
        new_model.fit_model(x, y, 0.3)
        training_scores[i], testing_scores[i] = new_model.score_reg() if not log_bool else new_model.score_log()

    print 'Mean Train Score:', np.mean(training_scores)
    print 'Mean Test Score:', np.mean(testing_scores)
    return new_model",0.4701396823,
142,building a model,"class SVMp():
    def train(self, prob : svm_problem):
        self.prob = prob
        self.C = self.prob.C

        self.L = self.prob.num

        self.x = self.prob.X
        self.xStar = self.prob.Xstar
        self.y = self.prob.Y

        self.gamma = self.prob.gamma

        P1 = (self.prob.xi_xj * self.prob.yi_yj) + self.gamma*(self.prob.xstari_xstarj)
        P2 = self.gamma*(self.prob.xstari_xstarj)
        P11 = np.hstack((P1, P2))
        P22 = np.hstack((P2, P2))
        P = np.vstack((P11, P22))

        q = np.hstack((np.repeat(-1, self.L),np.zeros(self.L)))

        negativeEye = -np.eye(self.L, dtype='d')
        zeros = np.zeros((self.L, self.L))
        g1 = np.hstack((negativeEye, zeros))
        g2 = np.hstack((zeros, negativeEye))

        G = np.vstack((g1,g2))

        h1 = np.zeros(((self.L),1))
        h2 = np.repeat(self.C, (self.L)).reshape(-1,1)
        h = np.vstack((h1, h2))

        A1 = np.repeat(1, 2*self.L)
        A2 = np.hstack((self.y, np.zeros(self.L)))
        A = np.vstack((A1, A2))

        b = np.zeros(2)
        b = b.reshape(-1,1)

        P = matrix(P, tc='d')
        q = matrix(q, tc='d')
        G = matrix(G, tc='d')
        h = matrix(h, tc='d')
        A = matrix(A, tc='d')
        b = matrix(b, tc='d')

        solvers.options['show_progress'] = False
        sol = solvers.qp(P, q, G, h, A, b)
        alphasAndDeltas = np.array(sol['x'])
        self.alphas = alphasAndDeltas[:self.L]
        self.deltas = alphasAndDeltas[self.L:]

        # Not really writing about it, w as before        
        w = np.sum(self.alphas * self.y[:, None] * self.x, axis = 0)
        bias = self.getB()
        
        clf = classifier()
        clf.w = w
        clf.b = bias
        clf.alphas = self.alphas
        clf.support_vectors = self.x[(self.alphas > 1e-5).flatten()]
        return clf
    
    def F(self, i):
        runningTotal = 0
        for j in range(self.L):
            runningTotal += self.alphas[j] * self.y[j] * self.prob.kernel(self.x[i], self.x[j])
        return runningTotal[0]
    
    def f(self, i):
        runningTotal = 0
        for j in range(self.L):
            runningTotal += (self.alphas[j] + self.deltas[j]) * self.prob.kernel(self.xStar[i], self.xStar[j])
        return runningTotal[0]
    
    def sPos(self):
        runningTotal = 0
        for i in range(self.L):
            if self.alphas[i] > 1e-5 and self.y[i] == 1:
                runningTotal += 1-(self.f(i)/self.gamma)-self.F(i)
        return runningTotal
    
    def sNeg(self):
        runningTotal = 0
        for i in range(self.L):
            if self.alphas[i] > 1e-5 and self.y[i] == -1:
                runningTotal += 1-(self.f(i)/self.gamma)-self.F(i)
        return runningTotal
    
    def nPos(self):
        runningTotal = 0
        for i in range(self.L):
            if self.alphas[i] > 1e-5 and self.y[i] == 1:
                runningTotal += 1
        return runningTotal
    
    def nNeg(self):
        runningTotal = 0
        for i in range(self.L):
            if self.alphas[i] > 1e-5 and self.y[i] == -1:
                runningTotal += 1
        return runningTotal
    
    def getB(self):
        return ((self.sPos()/self.nPos())+(self.sNeg()/self.nNeg()))/2",0.4685274065,
142,building a model,"# Function that builds the rest of the net
def generator(images, train_phase):
    
    # Ge the model data and set up
    print(""setting up vgg initialized conv layers ..."")
    model_data = utils.get_model_data(model_dir, model_url)
    weights = np.squeeze(model_data['layers'])

    # Build the remaining ""decoder"" that will colorize the image
    with tf.variable_scope(""generator"") as scope:
        
        # First Layer: 3x3 2dConv with bias follower by RELU
        #              Need this layer because the input is only 1 channel
        W0 = utils.weight_variable([3, 3, 1, 64], name=""W0"")
        b0 = utils.bias_variable([64], name=""b0"")
        conv0 = utils.conv2d_basic(images, W0, b0)
        hrelu0 = tf.nn.relu(conv0, name=""relu"")

        # Add in the VGG network 
        image_net = vgg_net(weights, hrelu0)
        vgg_final_layer = image_net[""relu5_3""]
        pool5 = utils.max_pool_2x2(vgg_final_layer)
        
        # Decoder Level 1: begin to upscale the image and decrease the number of filters 
        #                  Use conv2d_transpose_strided() with 4x4 filter
        deconv_shape1 = image_net[""pool4""].get_shape()
        W_t1 = utils.weight_variable([4, 4, deconv_shape1[3].value, pool5.get_shape()[3].value], name=""W_t1"")
        b_t1 = utils.bias_variable([deconv_shape1[3].value], name=""b_t1"")
        conv_t1 = utils.conv2d_transpose_strided(pool5, W_t1, b_t1, output_shape=tf.shape(image_net[""pool4""]))
        fuse_1 = tf.add(conv_t1, image_net[""pool4""], name=""fuse_1"")

        # Decoder Level 2: continue to upscale the image and decrease the number of filters 
        deconv_shape2 = image_net[""pool3""].get_shape()
        print(deconv_shape2)
        W_t2 = utils.weight_variable([4, 4, deconv_shape2[3].value, deconv_shape1[3].value], name=""W_t2"")
        b_t2 = utils.bias_variable([deconv_shape2[3].value], name=""b_t2"")
        conv_t2 = utils.conv2d_transpose_strided(fuse_1, W_t2, b_t2, output_shape=tf.shape(image_net[""pool3""]))
        fuse_2 = tf.add(conv_t2, image_net[""pool3""], name=""fuse_2"")
        
        # Decoder Level 3: continue to upscale the image and decrease the number of filters 
        shape = tf.shape(images)
        deconv_shape3 = tf.stack([shape[0], shape[1], shape[2], 2])
        W_t3 = utils.weight_variable([16, 16, 2, deconv_shape2[3].value], name=""W_t3"")
        b_t3 = utils.bias_variable([2], name=""b_t3"")
        pred = utils.conv2d_transpose_strided(fuse_2, W_t3, b_t3, output_shape=deconv_shape3, stride=8)

    # return the concatenation of the input with the output to make it the full image
    return tf.concat(axis=3, values=[images, pred], name=""pred_image"")",0.4680398405,
142,building a model,"class MLP:
    
    def __init__(self,*neurons):
        self.layers = len(neurons)
        self.neuPerL = [n for n in neurons]
    
    def setWeights(self,init):
        if init == 'SND':
            self.weights = [float(np.random.randn(1)) for l in range(self.layers-1)]
        elif init == 'Uniform':
            self.weights = [float(np.random.rand(1)) for l in range(self.layers-1)]
        elif init == 'LeCun':
            self.weights = [float(np.random.normal(0,1/np.sqrt(self.neuPerL[l]),1)) for l in range(self.layers-1)]
        self.W_Hist = [self.weights]
        self.LastChange = [0,0]
  
              
    def tanh(self,x):
        #return 1/(1+np.exp(-x))
        return 1.7159 * np.tanh(2/3 * x)
            
    def der(self,x):
        #return np.exp(x)/(np.exp(x)+1)**2
        return 1.14393 * 1/np.cosh(2*x/3)**2
    
    def forward(self,inp):
        self.activation = [inp]
        for l in range(self.layers-1):
            self.activation.append(self.tanh(np.dot(self.activation[-1],self.weights[l])))       
             
    def sgd(self,LR,epochs,mom,data,targ):
        # Implements Stochastic Gradient Descent
        self.LR = LR
        self.mom = mom
        p = np.zeros([epochs+1])
        p[0] = self.test(data,targ)
        for epoch in range(epochs):
            for sample,target in zip(data,targ):
                self.forward(sample)   
                # Compute errors
                self.deltas = [(self.activation[-1] - target) * self.der(np.dot(self.activation[-2],self.weights[-1]))]
                for l in range(len(self.weights)-1):
                    self.deltas.append(self.der(np.dot(self.activation[-3-l],self.weights[-2-l])) * self.weights[-1-l]*self.deltas[-1-l])
                self.deltas = list(reversed(self.deltas))
                
                # Adapt weights
                self.adaptWeights()   
            self.LR = 0.99 * self.LR # Decaying LR
            p[epoch+1] = self.test(data,targ)
        return p
            
    def batch_gd(self,LR,epochs,mom,data,targets):
        # Implements Batch Gradient Descent
        # DOESNT WORK PROPERLY FOR SOEM REASON!!
        p = np.zeros([epochs+1])
        self.mom = mom
        self.LR = LR
        for epoch in range(epochs):
            activity = np.zeros([len(data),self.layers])
            for ind,sample in enumerate(data):
                self.forward(sample) 
                activity[ind,:] = self.activation
            loss = np.mean(activity[:,-1] - targets) 
            self.deltas = [loss * self.der(np.mean(activity[:,-2])*self.weights[-1])]
            for l in range(len(self.weights)-1):
                self.deltas.append(self.der(np.mean(activity[:,-3-l])*self.weights[-2-l]) * self.weights[-1-l]*self.deltas[-1-l])
            self.deltas = list(reversed(self.deltas))
            self.adaptWeights()
            p[epoch+1] = self.test(data,targets)
            self.LR = 0.99 * self.LR
        return p
    
    def adaptWeights(self):
        #print('w_vorrher', self.weights)
        for l in range(len(self.weights)):
            #print('d',self.deltas[l], 'mom',self.mom * self.W_Hist[-1][l],'a', self.activation[l],'adapt',self.LR * self.deltas[l] * self.activation[l] + self.mom * self.W_Hist[-1][l])
            tmp = self.LR * self.deltas[l] * self.activation[l] + self.mom * self.LastChange[l]
            self.weights[l] = self.weights[l] - tmp
            self.LastChange[l] = tmp
        self.W_Hist.append(self.weights)
        #print('w_nach',self.weights)
        
    def test(self,data,targ):
        correct = 0
        for sample,target in zip(data,targ):
            self.forward(sample)
            correct += 1 if round(self.activation[-1]) == target else 0
        return 100*correct/len(data)
    

    def getMSE(self, data, label):
        errors = []
        for ind,sample in enumerate(data):
            self.forward(sample)
            errors.append((self.activation[-1]-label[ind])**2)
        return np.mean(errors)",0.467941463,
792,handling time differences,"def correct_dates(departure, arrival):
    if arrival.hour < departure.hour:
        try:
            # Add another day to arrival
            arrival.replace(day = day + 1)
        except ValueError:
            # Error if day went into next month
            if arrival.month == 12:
                # New day goes into next year
                arrival.replace(month = 1, day = 1)
            else:
                # Any other month
                arrival.replace(month = month + 1, day = 1)
    # Return arrival whether altered or not
    return arrival",0.4285440147,
792,handling time differences,"while sdd.get_current_time() < sdd.get_end_time():
    seb.update()
    sdd.update()
    time.append(sdd.get_current_time())
    seb_snow_depth.append(seb.get_value('snowpack__depth').max())
    sdd_snow_depth.append(sdd.get_value('snowpack__depth').max())",0.425214529,
792,handling time differences,"def sanity_check(left_line, right_line):
    # Are the two polynomials an appropriate distance apart based on the known width of a highway lane?
    x_distances = abs(left_line.recent_xfitted[-1] - right_line.recent_xfitted[-1])
    x_min = np.min(x_distances)
    x_max = np.max(x_distances)
    threshold = (550, 750)
    if (x_min < threshold[0] or x_min > threshold[1] or x_max < threshold[0] or x_max > threshold[1]) :
        return False
    
    # Do the two polynomials have same or similar curvature? THIS IS NOT WORKING CORRECTLY
#     if abs(left_line.radius_of_curvature - right_line.radius_of_curvature) > 200:
#         print(""Curve diff: "" + str(abs(left_line.radius_of_curvature - right_line.radius_of_curvature)))
#         return False
  
    return True

def set_lines_detected(left_line, right_line):
    left_line.best_xfitted = left_line.recent_xfitted[-1]
    right_line.best_xfitted = right_line.recent_xfitted[-1]

    left_line.best_fit = left_line.current_fit
    right_line.best_fit = right_line.current_fit

    left_line.detected = True
    right_line.detected = True",0.4217346907,
792,handling time differences,"def emtr(P):
    P.hours = P.hours - 10
    if P.hours < 0: P.hours = 0
    temp = takeHome(P)
    P.hours += 10
    diff = (takeHome(P) - temp)/10
    return 100*(P.hwage-diff)/(P.hwage)
emtr(Person1)",0.4095606208,
792,handling time differences,"def handle_data(context, data):
    '''
    Called when a market event occurs for any of the algorithm's 
    securities. 

    Parameters

    data: A dictionary keyed by security id containing the current 
          state of the securities in the algo's universe.

    context: The same context object from the initialize function.
             Stores the up to date portfolio as well as any state 
             variables defined.

    Returns None
    '''
    # Allow history to accumulate 100 days of prices before trading
    # and rebalance every day thereafter.
    context.tick += 1
    if context.tick < 100:
        return
    # Get rolling window of past prices and compute returns
    
    #rebalance only every i days
    i = 60
    if (context.tick % i) != 0:
        return
    
    prices = history(100, '1d', 'price').dropna()
    returns = prices.pct_change().dropna()
    try:
        # Perform Markowitz-style portfolio optimization
        weights, _, _ = optimal_portfolio(returns.T)
        weights = np.around(weights)
        # Rebalance portfolio accordingly
        for stock, weight in zip(prices.columns, weights):
            order_target_percent(stock, weight)
    except ValueError as e:
        # Sometimes this error is thrown
        # ValueError: Rank(A) < p or Rank([P; A; G]) < n
        pass",0.4056820869,
792,handling time differences,"rate_limit = 0.04

def limit(x):
    if abs(x - limit.last) > rate_limit:
        y = limit.last + math.copysign(rate_limit, x - limit.last)
    else:
        y = x
    limit.last = y
    return y

limit.last = -1.0

def plot_limit(df):
    ax = df.plot()
    pct_per_sec = rate_limit / (20/1000) * 100
    plt.title(""Rate Limiting at {}% Full Travel per sec"".format(pct_per_sec))
    plt.xlabel(""Time"")
    plt.ylabel(""Output"")


            
step['output'] = step['input'].apply(limit)",0.401779294,
792,handling time differences,"def createtimevector(timearray, time, row_num):
    col_num1 = time.hour*4+time.minute//15
    col_num2 = time.dayofweek+95
    timearray[row_num, col_num1] = 1
    timearray[row_num, col_num2]
    return timearray",0.4015396237,
792,handling time differences,"def young_stars(pfilter, data):
    age = data.ds.current_time - data[pfilter.filtered_type, ""creation_time""]
    filter = np.logical_and(age.in_units('Myr') <= 5, age >= 0)
    return filter

def old_stars(pfilter, data):
    age = data.ds.current_time - data[pfilter.filtered_type, ""creation_time""]
    filter = np.logical_or(age.in_units('Myr') >= 5, age < 0)
    return filter",0.3984858394,
792,handling time differences,"def print_time(start_time, message):
    end_cv_time = time.time()
    total_mins = (end_cv_time - start_time) /(60.0 )
    total_hours = (end_cv_time - start_time) /(60.0 * 60.0 )
    print (message)
    print(""\tTotal time taken hours = {:.3f}"".format(total_hours))
    print(""\tTotal time taken mins = {:.3f}"".format(total_mins))",0.397180438,
792,handling time differences,"model.update_until(200., units='year')
model.get_current_time()",0.3953797221,
401,data analysis,"def analyze(results):
    df = pd.DataFrame(data=results)
    df.columns.name = 'Batch size'
    df.index.name='Repetition'

    # Show raw results.
    print ('Experimental results: raw')
    display(df)

    df_stats = df.describe()
    df_stats.loc['mean per image'] = df_stats.ix['mean'] / df.columns
    df_stats.loc['std per image']  =  df_stats.ix['std'] / df.columns # FIXME: div by sqrt(n)?
    
    # Show stats.
    print ('Experimental results: stats')
    display(df_stats)

    # Show two plots side-by-side: mean time per batch and mean time per image.
    fig, axs = plt.subplots(1,2)
    df_stats.ix['mean'] \
        .plot(ax=axs[0],
            yerr=df_stats.ix['std'],
            title='Mean time per batch (ms)',
            kind='bar', grid=True, rot=0, figsize=[10, 4], colormap=cm.autumn_r
        )
    df_stats.ix['mean per image'] \
        .plot(ax=axs[1],
            yerr=df_stats.ix['std per image'],
            title='Mean time per image (ms)',
            kind='bar', grid=True, rot=0, figsize=[10, 4], colormap=cm.autumn
        )
    
    # Show batch size giving minimum time per image, mean and std.
    min_time_per_image_idx = df_stats.ix['mean per image'].idxmin()
    if not math.isnan(min_time_per_image_idx):
        print (
            'Minimum time per image: batch size = %d, mean = %.2f, std = %.2f' % (
                min_time_per_image_idx, 
                df_stats.ix['mean per image'][min_time_per_image_idx],
                df_stats.ix['std per image'][min_time_per_image_idx]
            )
        )
    else:
        print ('Minimum time per image: N/A')",0.5668770075,
401,data analysis,"def run_analysis(project_file, annotation_file, pimp_file):
    
    # load m2lda object and do thresholding
    ms2lda = Ms2Lda.resume_from(project_file)
    ms2lda.do_thresholding(th_doc_topic=0.05, th_topic_word=0.01)

    # read the annotation file
    print
    print ""Annotation file""
    motif_annotation = {}
    motif_idx = {}
    i = 0
    for item in csv.reader(open(annotation_file), skipinitialspace=True):
        key = int(item[0])
        val = item[1]
        print str(key) + ""\t"" + val
        motif_annotation[key] = val
        motif_idx[key] = i
        i += 1

    motifs_of_interest = motif_annotation.keys()    
    norm = mpl.colors.Normalize(vmin=min(motif_idx.values()), vmax=max(motif_idx.values()))
    cmap = cm.gist_rainbow
    motif_colour = cm.ScalarMappable(norm=norm, cmap=cmap)    
    
    # get the network graph out for the motifs of interest
    G = get_network_graph(ms2lda, motifs_of_interest)
    print ""\n"" + nx.info(G)  
    print

    # print out some info
    ms1_count = 0
    nodes = G.nodes(data=True)
    for node_id, node_data in nodes:
        # 1 for doc, 2 for motif
        if node_data['group'] == 1: 
            ms1_count += 1
    print ""%d (out of %d) MS1 peaks found in the graph"" % (ms1_count, ms2lda.ms1.shape[0])    
    
    # load the pimp differential analysis file for matching
    de_peaks = []
    with open(pimp_file, ""rb"") as infile:
       reader = csv.reader(infile)
       next(reader, None)  # skip the headers
       for row in reader:
        PiMP_ID = int(row[0])
        polarity = row[1]
        mz = float(row[2])
        rt = float (row[3])
        mh_intensity = float(row[4])
        tup = (PiMP_ID, polarity, mz, rt, mh_intensity)
        de_peaks.append(tup)

    print
    print ""PiMP list: ""
    for tup in de_peaks:
        print tup
        
    # do the matching
    mass_tol = 3
    rt_tol = 12

    std = np.array(de_peaks)
    std_mz = np.array([x[2] for x in de_peaks])
    std_rt = np.array([x[3] for x in de_peaks])
    matches = {}

    ms1_label = {}
    for row in ms2lda.ms1.itertuples(index=True):
        peakid = row[1]
        mz = row[5]
        rt = row[4]

        # the following line is hacky for pos mode data
        mass_delta = mz*mass_tol*1e-6
        mass_start = mz-mass_delta
        mass_end = mz+mass_delta
        rt_start = rt-rt_tol
        rt_end = rt+rt_tol

        match_mass = (std_mz>mass_start) & (std_mz<mass_end)
        match_rt = (std_rt>rt_start) & (std_rt<rt_end)
        match = match_mass & match_rt

        res = std[match]
        if len(res) == 1:
            closest = tuple(res[0])
            matches[closest] = row
            ms1_label[row[1]] = closest[1]        
        elif len(res)>1:
            closest = None
            min_dist = sys.maxint
            for match_res in res:
                match_mz = float(match_res[2])
                match_rt = float(match_res[3])
                dist = math.sqrt((match_rt-rt)**2 + (match_mz-mz)**2)
                if dist < min_dist:
                    min_dist = dist
                    closest = match_res
            closest = tuple(closest)
            matches[closest] = row
            ms1_label[row[1]] = closest[1]

    print ""Matches found %d/%d"" % (len(matches), len(std))
    print

    ms1_list = []
    for match in matches:
        key = str(match)
        ms1_row = matches[match]
        value = str(ms1_row)
        pid = ms1_row[1]
        print ""Standard %s"" % key
        print ""MS1 %s"" % value
        print
        ms1_list.append(pid)
        
    # print the motifs and count their occurences
    m2m_list = motifs_of_interest
    word_map, motif_words = ms2lda.print_motif_features(quiet=True)

    c = Counter() # count the motif occurences
    for i in range(len(ms1_list)):

        ms1 = ms1_list[i]
        df = print_report(ms2lda, G, ms1, motif_annotation, motif_words, motif_colour, motif_idx, word_map, xlim_upper=770)
        if df is not None:

            # display(df) # show the table to see the mz, annotations, etc

            # get the motif ids in the dataframe
            fragment_motif_ids = df[['fragment_motif']].values.flatten()
            loss_motif_ids = df[['loss_motif']].values.flatten()

            # get rid of nan values
            fragment_motif_ids = fragment_motif_ids[~np.isnan(fragment_motif_ids)]
            loss_motif_ids = loss_motif_ids[~np.isnan(loss_motif_ids)]

            # store the unique counts
            combined = np.append(fragment_motif_ids, loss_motif_ids).astype(int)
            combined = set(combined.tolist())
            c.update(combined)
            
    return c",0.5554660559,
401,data analysis,"def get_blob_coord(train_img):
    t = time.time()
    print ('Getting Color Blobs...')
    colors = ['red','magenta','brown','blue','green','error']
    df_blob = pd.DataFrame(index=df.index, columns=colors)

    count = 0
    R, G, B, color = [], [], [], []
    for fname in range(train_img[0],train_img[1]):
        count += 1
        if fname in list(df_skip['train_id']): continue
            
        img_1 = mpimg.imread('./input/Train/' + str(fname) + '.jpg')
        img_2 = mpimg.imread('./input/TrainDotted/' + str(fname) + '.jpg')
        img_3 = cv2.absdiff(img_2,img_1)
    
        mask_2 = cv2.cvtColor(img_2, cv2.COLOR_RGB2GRAY)
        mask_2[mask_2 < 20] = 0
        mask_2[mask_2 > 0] = 255

        img_4 = cv2.bitwise_or(img_3, img_3, mask=mask_2)
        img_5 = cv2.cvtColor(img_4, cv2.COLOR_RGB2GRAY)

        blobs = blob_log(img_5, min_sigma=3, max_sigma=4, num_sigma=1, threshold=0.02)
        red, magenta, green, blue, brown, error = [], [], [], [], [], []
        
        #cut = np.copy(img_2)
        for blob in blobs:
            y, x, s = blob
            r,g,b = img_2[int(y)][int(x)][:]
        
            pred = predict_color(r,g,b)
            
            if pred[0] == 'red':
                red.append((int(y),int(x)))
                color.append('red')
            elif pred[0] == 'magenta':
                magenta.append((int(y),int(x)))
                color.append('magenta')
            elif pred[0] == 'brown':
                brown.append((int(y),int(x)))
                color.append('brown')
            elif pred[0] == 'blue':
                blue.append((int(y),int(x)))
                color.append('blue')
            elif pred[0] == 'green':
                green.append((int(y),int(x)))
                color.append('green')
            else:
                error.append((int(y),int(x)))
                color.append('error')
                
            R.append(int(r))
            G.append(int(g))
            B.append(int(b))
            #cv2.rectangle(cut, (int(x)-32,int(y)-32),(int(x)+32,int(y)+32), 0,-1)
        
        if count % 25 == 0:
            print ('File progress:',fname,len(red),len(magenta),len(brown),len(blue),len(green),len(error))
            
        df_blob.loc[fname,'red'] = red
        df_blob.loc[fname,'magenta'] = magenta
        df_blob.loc[fname,'brown'] = brown
        df_blob.loc[fname,'blue'] = blue
        df_blob.loc[fname,'green'] = green
        df_blob.loc[fname,'error'] = error
    
    df_color = pd.DataFrame({'R':R, 'G':G, 'B':B,'color':color})
    hf.check_colors(df,df_color,train_img)
    print ('\nTotal Training Images:',train_img[1]-train_img[0])
    print ('Time for getting Sea Lion coordinates:', np.round(time.time() - t, 4),'\n')
    return R,G,B,color,df_color,df_blob",0.5450665951,
401,data analysis,"def test(isTrue): #Runs CNN on test images
    pred=fitAndPredict()
    print(""creating test file"")
    df = pd.DataFrame(pred, columns=['Type_1','Type_2','Type_3']) #Instantiates dataframe
    df['image_name'] = test_id #image_name holds the .jpg file name
    if (isTrue): #if(True), it will create a .csv file with the dataframe
        df.to_csv('test.csv', index=False)
        print(""Test file created in users/keerat/..."")
    else: #if(False), it will just show the dataframe
        print(df.to_string())",0.5437158346,
401,data analysis,"# read training and testing data into matrices
def read_data(self):
    column_headers = ['user_id', 'item_id', 'rating', 'timestamp']

    print 'Reading training data from ', self.cfg.training_file, '...'
    training_data = ps.read_csv(self.cfg.training_file, sep='\t', names=column_headers)

    print 'Reading testing data from ', self.cfg.testing_file, '...'
    testing_data = ps.read_csv(self.cfg.testing_file, sep='\t', names=column_headers)

    num_users = max(training_data.user_id.unique())
    num_items = max(training_data.item_id.unique())

    self.ratings = np.zeros((num_users, num_items))
    self.test_ratings = np.zeros((num_users, num_items))

    for row in training_data.itertuples(index=False):
        self.ratings[row.user_id - 1, row.item_id - 1] = row.rating

    for row in testing_data.itertuples(index=False):
        self.test_ratings[row.user_id - 1, row.item_id - 1] = row.rating
        
GroupRec.read_data = read_data",0.5398346782,
401,data analysis,"def run_single_analysis_plot(data, true_density, x_grid):
    """"""Compare different bandwidth estimators to match data on true_density
    
    true_density is sampled at the x_grid, which is also used for plotting.
    Additionally, optimization time of the estimators and r2 score 
    are reported.
    """"""
    plt.figure(figsize=(8,5))
    data = copy.deepcopy(data)
    np.random.seed(1)
    np.random.shuffle(data)
    # approaches from statsmodels
    # Optionally, ""normal_reference"" could be used, too.
    for method in [""cv_ml"", ""cv_ls""]:
        for efficient in [True, False]:
            default_KDE = nonparametric.EstimatorSettings(
                efficient=efficient)
            t = time.time()
            kde = KDEMultivariate(
                data=[data], var_type='c', bw=method, 
                defaults=default_KDE)
            # uses x_grid and true_density for testing
            density_estimate = kde.pdf(x_grid)
            tf = time.time()
            print '%s score:'%method, r2_score(
                true_density, density_estimate),\
                ' time:', tf-t, ' efficient:', str(efficient)
            plt.plot(x_grid, density_estimate, 
                     label=method+""_""+str(efficient))
    # approaches from scipy.stats
    for method in [""scott"", ""silverman""]:
        t = time.time()
        kde = gaussian_kde(data, method)
        # uses x_grid and true_density for testing
        density_estimate = kde(x_grid.flatten())
        tf = time.time()
        print '%s score:'%method, r2_score(
            true_density, density_estimate), ' time:', tf-t
        plt.plot(x_grid, density_estimate,label=method)
    plt.plot(x_grid, true_density, '--k', label ='true density')
    plt.legend(loc='best')
    
def sample_eval_kde(data, true_density, x_grid, n_samples):
    """"""Divide n_samples into 10 chunks to generate KDE performance curves
    
    The curves are not plotted but returned as dictionaries of lists
    with the estimation methods that shall be evaluated as keys.
    *data is used to fit the KDE and true_density for evaluating it 
    on x_grid.
    
    Code is very similar to :func:`run_single_analysis_plot`
    """"""
    density_scores = defaultdict(list)
    density_times = defaultdict(list)
    data = copy.deepcopy(data)
    np.random.seed(1)
    np.random.shuffle(data)
    for i in range(int(n_samples/10),n_samples+1,int(n_samples/10)):
        # Counter output to know at which stage the processing
        # is currently at.
        sys.stdout.write(""\r%d"" % i)
        sys.stdout.flush()
        for method in [""cv_ml"", ""cv_ls""]:
            for efficient in [True, False]:
                default_KDE = nonparametric.EstimatorSettings(
                    efficient=efficient)
                t = time.time()
                kde = KDEMultivariate(
                    data=[data[:i]], var_type='c', bw=method, 
                    defaults=default_KDE)
                density_estimate = kde.pdf(x_grid)
                tf = time.time()
                density_scores[method+""_""+str(efficient)].append(
                    r2_score(true_density, density_estimate))
                density_times[method+""_""+str(efficient)].append(tf-t)
        for method in [""scott"", ""silverman""]:
            t = time.time()
            kde = gaussian_kde(data[:i], method)
            density_estimate = kde(x_grid.flatten())
            tf = time.time()
            density_scores[method].append(
                r2_score(true_density, density_estimate))
            density_times[method].append(tf-t)
    return density_scores, density_times

def print_n_density_scores(density_scores, density_times, n_samples):
    """"""Plot results from :func:`sample_eval_kde`""""""
    plt.figure()
    for method in [ ""cv_ml"", ""cv_ls""]:
        for efficient in [True, False]:
            plt.plot(
                range(int(n_samples/10), n_samples+1, int(n_samples/10)),
                density_scores[method+""_""+str(efficient)], '-v',
                label=method+""_""+str(efficient))
    for method in [""scott"", ""silverman""]:
        plt.plot(range(int(n_samples/10), n_samples+1, int(n_samples/10)),
                 density_scores[method],'-v', label=method)
    plt.legend(loc='best')
    plt.xlabel('samples')
    plt.ylabel('scores')

    plt.figure()
    for method in [ ""cv_ml"", ""cv_ls""]:
        for efficient in [True, False]:
            plt.plot(range(int(n_samples/10),n_samples+1,int(n_samples/10)),
                     density_times[method+""_""+str(efficient)],'-v',
                     label=method+""_""+str(efficient))
    for method in [""scott"", ""silverman""]:
        plt.plot(range(int(n_samples/10),n_samples+1,int(n_samples/10)),
                 density_times[method],'-v', label=method)
    plt.legend(loc='best')
    plt.xlabel('samples')
    plt.ylabel('time (s)')",0.5371446609,
401,data analysis,"def classify(corp):
    print (""Classifying"", corp.corpusName)
    #load the vocabulary/word/term list for the entire corpus from file
    corpusTerms = pd.read_csv(""Features/""+corp.corpusName+""/""+corp.corpusName+""termMI.csv"", index_col = 0)
    totalLegitTerms = corpusTerms['LegitCount'].sum(axis=0)
    totalSpamTerms = corpusTerms['SpamCount'].sum(axis=0)
    totalTerms = totalLegitTerms + totalSpamTerms
    
    filepathList, actualClass = combineFiles(""bare"")
    for num in numFeatures:  
        terms = pd.read_csv(""MI/""+corp.corpusName+""/""+str(num)+""terms.csv"", index_col = 0)
        print (""Features: "", num) 
        
        spamCount = terms['SpamCount'].tolist()
        legitCount = terms['LegitCount'].tolist()
        
        for t in threshold:
            print (""Threshold: "", t)
            predClass = classifyDocs(corp, terms, spamCount, legitCount, totalTerms,
                                 filepathList, t, num)
            writeResults(actualClass,predClass, corp, num, t)",0.5364351869,
401,data analysis,"def fill_lane(image,objpoints, imgpoints):
    
    combined_binary = apply_thresholds(image,objpoints, imgpoints, show=False)
    
    rightx = []
    righty = []
    leftx = []
    lefty = []
    
    x, y = np.nonzero(np.transpose(combined_binary))
    i = 720
    j = 630
    while j >= 0:
        histogram = np.sum(combined_binary[j:i,:], axis=0)
        left_peak = np.argmax(histogram[:640])
        x_idx = np.where((((left_peak - 25) < x)&(x < (left_peak + 25))&((y > j) & (y < i))))
        x_window, y_window = x[x_idx], y[x_idx]
        if np.sum(x_window) != 0:
            leftx.extend(x_window.tolist())
            lefty.extend(y_window.tolist())

        right_peak = np.argmax(histogram[640:]) + 640
        x_idx = np.where((((right_peak - 25) < x)&(x < (right_peak + 25))&((y > j) & (y < i))))
        x_window, y_window = x[x_idx], y[x_idx]
        if np.sum(x_window) != 0:
            rightx.extend(x_window.tolist())
            righty.extend(y_window.tolist())
        i -= 90
        j -= 90

    lefty = np.array(lefty).astype(np.float32)
    leftx = np.array(leftx).astype(np.float32)
    righty = np.array(righty).astype(np.float32)
    rightx = np.array(rightx).astype(np.float32)
    left_fit = np.polyfit(lefty, leftx, 2)
    left_fitx = left_fit[0]*lefty**2 + left_fit[1]*lefty + left_fit[2]
    right_fit = np.polyfit(righty, rightx, 2)
    right_fitx = right_fit[0]*righty**2 + right_fit[1]*righty + right_fit[2]
    rightx_int = right_fit[0]*720**2 + right_fit[1]*720 + right_fit[2]
    rightx = np.append(rightx,rightx_int)
    righty = np.append(righty, 720)
    rightx = np.append(rightx,right_fit[0]*0**2 + right_fit[1]*0 + right_fit[2])
    righty = np.append(righty, 0)
    leftx_int = left_fit[0]*720**2 + left_fit[1]*720 + left_fit[2]
    leftx = np.append(leftx, leftx_int)
    lefty = np.append(lefty, 720)
    leftx = np.append(leftx,left_fit[0]*0**2 + left_fit[1]*0 + left_fit[2])
    lefty = np.append(lefty, 0)
    lsort = np.argsort(lefty)
    rsort = np.argsort(righty)
    lefty = lefty[lsort]
    leftx = leftx[lsort]
    righty = righty[rsort]
    rightx = rightx[rsort]
    left_fit = np.polyfit(lefty, leftx, 2)
    left_fitx = left_fit[0]*lefty**2 + left_fit[1]*lefty + left_fit[2]
    right_fit = np.polyfit(righty, rightx, 2)
    right_fitx = right_fit[0]*righty**2 + right_fit[1]*righty + right_fit[2]
    
    # Measure Radius of Curvature for each lane line
    ym_per_pix = 30./720 # meters per pixel in y dimension
    xm_per_pix = 3.7/700 # meteres per pixel in x dimension
    left_fit_cr = np.polyfit(lefty*ym_per_pix, leftx*xm_per_pix, 2)
    right_fit_cr = np.polyfit(righty*ym_per_pix, rightx*xm_per_pix, 2)
    left_curverad = ((1 + (2*left_fit_cr[0]*np.max(lefty) + left_fit_cr[1])**2)**1.5) \
                                 /np.absolute(2*left_fit_cr[0])
    right_curverad = ((1 + (2*right_fit_cr[0]*np.max(lefty) + right_fit_cr[1])**2)**1.5) \
                                    /np.absolute(2*right_fit_cr[0])

    curvature = (left_curverad + right_curverad) / 2
    
    min_curvature = min(left_curverad, right_curverad)
    
    # Calculate the position of the vehicle
    center = abs(640 - ((rightx_int+leftx_int)/2))
    vehicle_position = center / 12800 
    offset = 0 
    img_size = (img.shape[1], img.shape[0])
    src = np.float32([[490, 482],[810, 482],
                      [1250, 720],[40, 720]])
    dst = np.float32([[0, 0], [1280, 0], 
                     [1250, 720],[40, 720]])
    Minv = cv2.getPerspectiveTransform(dst, src)
    
    warp_zero = np.zeros_like(combined_binary).astype(np.uint8)
    color_warp = np.dstack((warp_zero, warp_zero, warp_zero))
    pts_left = np.array([np.flipud(np.transpose(np.vstack([left_fitx, lefty])))])
    pts_right = np.array([np.transpose(np.vstack([right_fitx, righty]))])
    pts = np.hstack((pts_left, pts_right))
    cv2.polylines(color_warp, np.int_([pts]), isClosed=False, color=(0,0,255), thickness = 40)
    cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))
    newwarp = cv2.warpPerspective(color_warp, Minv, (combined_binary.shape[1], combined_binary.shape[0]))
    result = cv2.addWeighted(mpimg.imread(image), 1, newwarp, 0.5, 0)
    
    f, (ax1, ax2) = plt.subplots(1,2, figsize=(9, 6))
    f.tight_layout()
    ax1.imshow(cv2.cvtColor((birds_eye(image,objpoints, imgpoints, display=False)[0]), cv2.COLOR_BGR2RGB))
    ax1.set_xlim(0, 1280)
    ax1.set_ylim(0, 720)
    ax1.plot(left_fitx, lefty, color='green', linewidth=3)
    ax1.plot(right_fitx, righty, color='green', linewidth=3)
    ax1.set_title('Fit Polynomial to Lane Lines', fontsize=16)
    ax1.invert_yaxis() # to visualize as we do the images
    ax2.imshow(result)
    ax2.set_title('Fill Lane Between Polynomials', fontsize=16)

    
    if center < 640:
        ax2.text(200, 100, 'Vehicle is {:.2f}m left of center'.format(center*3.7/700),
                 style='italic', color='white', fontsize=10)
    else:
        ax2.text(200, 100, 'Vehicle is {:.2f}m right of center'.format(center*3.7/700),
                 style='italic', color='white', fontsize=10)
    ax2.text(200, 175, 'Radius of curvature is {}m'.format(int((left_curverad + right_curverad)/2)),
             style='italic', color='white', fontsize=10)",0.5313766003,
401,data analysis,"def returnPrecision(y_truth, y_pred, classifier):
    """"""This function takes in an array of true observed and an array of predicted observed and outputs three 
    different measures for precision""""""
    precision = {}
    precision[""p_macro""] = precision_score(y_truth, y_pred, average='macro')
    precision[""p_micro""] = precision_score(y_truth, y_pred, average='micro')
    precision[""p_weight""] = precision_score(y_truth, y_pred, average='weighted')
    precision[""p""] = precision_score(y_truth, y_pred, pos_label=True, average='binary')
    return pd.DataFrame(precision, index=[classifier])

def returnRecall(y_truth, y_pred, classifier):
    """"""This function takes in an array of true observed and an array of predicted observed and outputs three 
    different measures for precision""""""
    recall = {}
    recall[""r_macro""] = recall_score(y_truth, y_pred, average='macro')
    recall[""r_micro""] = recall_score(y_truth, y_pred, average='micro')
    recall[""r_weight""] = recall_score(y_truth, y_pred, average='weighted')
    recall[""r""] = recall_score(y_truth, y_pred, pos_label=True, average=""binary"")
    return pd.DataFrame(recall, index=[classifier])

def returnF1(y_truth, y_pred, classifier):
    """"""This function takes in an array of true observed and an array of predicted observed and outputs three 
    different measures for F1""""""
    F1 = {}
    F1[""f1_macro""] = f1_score(y_truth, y_pred, average='macro')
    F1[""f1_micro""] = f1_score(y_truth, y_pred, average='micro')
    F1[""f1_weight""] = f1_score(y_truth, y_pred, average='weighted')
    F1[""f1""] = f1_score(y_truth, y_pred, pos_label=True, average='binary')
    return pd.DataFrame(F1, index=[classifier])

def returnAccuracy(y_truth, y_pred, classifier):
    """"""This function takes in an array of true observed and an array of predicted observed and outputs accuracy""""""
    accuracy = {}
    accuracy[""accu""] = accuracy_score(y_truth, y_pred)
    return pd.DataFrame(accuracy, index=[classifier])

def returnMetrics(y_truth, y_pred, classifier):
    """"""Returns recall, precision, and accuracy as a pandas dataframe for each classifier""""""
    rec = returnRecall(y_truth, y_pred, classifier)
    prec = returnPrecision(y_truth, y_pred, classifier)
    accu = returnAccuracy(y_truth, y_pred, classifier)
    f1 = returnF1(y_truth, y_pred, classifier)
    df = pd.concat([rec, prec, f1, accu], axis=1)
    return(df)

def runModels(X, y, kf, names, classifiers):
    """"""Function that runs over the classifiers and does k-fold cross-validation on the training set - returns metrics as dataframe""""""
    df = pd.DataFrame([])
    for name, clf in zip(names, classifiers):
        for train_index, test_index in kf.split(X):
            clf.fit(X[train_index], y[train_index])
            predicted = clf.predict(X[test_index])
            data = returnMetrics(y[test_index], predicted, name)
            myscore = clf.score(X[test_index], y[test_index])
            score = pd.DataFrame({""score"": myscore}, index = [name])
            ccat = pd.concat([data, score], axis = 1)
            df = df.append(ccat)
    return df

def runTruth(X, y, names, classifiers):
    """"""This function will run the ML classifiers on the testing set  - returns metrics as dataframe""""""
    df = pd.DataFrame([])
    falses = []
    for name, clf in zip(names, classifiers):
        clf.fit(X, y)
        predicted = clf.predict(X)
        data = returnMetrics(y, predicted, name)
        df = df.append(data)
        for i in range(len(predicted)):
            if (predicted[i] != y[i]):
                outtuple = (i,predicted[i],y[i], name)
                falses.append(outtuple) 
    return (df, falses)
    

def MLmodels(X, Y, nsplits,crossv = True):
    """"""This function will run classifiers and export accuracy, precision, and recall for each classifier.
    N-fold cross validation has been automated here, where user determines number of folds""""""
    ###########################################
    #Define classifiers
    ###########################################
    names = [""Decision Tree"", ""Logistic"", ""SVM"", ""Random Forest"", ""ExtraTrees""]
    classifiers = [ LogisticRegression(), tree.DecisionTreeClassifier(), svm.SVC(),
                   RandomForestClassifier(), ExtraTreesClassifier()]
    
    ###########################################
    #Create Train/Test Split
    ###########################################
    kf = KFold(n_splits = nsplits)
    
    ###########################################
    #Run Models
    ###########################################
    if(crossv == True): 
        return(runModels(X, y, kf, names, classifiers))
    else:
        return(runTruth(X, y, names, classifiers))",0.5312572718,
401,data analysis,"def earnings_expirations(ticker,start_date, end_date):
    '''
        The function obtains earning date, front month, second month and January leap
        expiration for the selected ticker and date range. Expirations if 
        recorded as saturdays on IVY will converted to previous trading day expirations.
        (ATM query within synthetic_ATM() function works with these adjusted dates for
        options expirations)
    '''
    df_out = pd.DataFrame(columns=['EarningDate','First','Second','JanLeap'])
    # Obtaining earning dates from the web
    df_out['EarningDate'] = earnings_releases(ticker,start_date,end_date).EarningDate
    # Initializing the dataframe
    df_out[['First','Second','JanLeap']] = datetime.datetime.now()
    # Creating a SQL table Earnings
    data_frame_to_sql(df_out,'Earnings')
    sql_raw = open('Exp.sql', 'r').read()
    sql_format = sql_raw.format(
                    ticker = ticker,
                    date_start = start_date,
                    date_end = end_date)
    res = query_dataframe(sql_format)
    df_out['First'] = res.First.where(res.First.dt.dayofweek!=5, 
                                      res.First[res.First.dt.dayofweek==5]-TradingDay(1)).values
    df_out['Second'] = res.Second.where(res.Second.dt.dayofweek!=5,
                                      res.Second[res.Second.dt.dayofweek==5]-TradingDay(1)).values
    df_out['JanLeap'] = res.JanLeap.where(res.JanLeap.dt.dayofweek!=5,
                                      res.JanLeap[res.JanLeap.dt.dayofweek==5]-TradingDay(1)).values
    return df_out.reset_index(drop=True)",0.5308578014,
286,configure spark,"# Set Spark Session as entry point
spark = SparkSession.builder\
                    .appName(""Simple recommendation engine using Spark MLlib"")\
                    .config(""spark.some.config.option"", ""config-value"")\
                    .getOrCreate()\",0.5321336985,
286,configure spark,"#Init the spark session
spark = SparkSession.builder\
               .appName(""SparkSession"")\
               .enableHiveSupport()\
               .getOrCreate()\",0.5106436014,
286,configure spark,"if __name__ == '__main__':
    conf = (SparkConf().setAppName(""TwitterSA"").set(""spark.executor.memory"", ""6g""))
    #sc = SparkContext(conf=conf)
    sc = pyspark.SparkContext('local[*]')
    sqlContext = SQLContext(sc)
    input_train = ""train.csv""
    input_test = ""test.csv""

    raw_train = sc.textFile(input_train)
    print('## Parsing Train Data...')
    train_data = raw_train.map(parse_line)
    print('## Tokenizing Train Data...')
    train_data = train_data.map(lambda x: (tokenizer(x[0]),x[1]))
    train_data.cache()
    # print(train_data.take(3))

    raw_test = sc.textFile(input_test)
    print('## Parsing Test Data...')
    test_data = raw_test.map(parse_line)
    print('## Tokenizing Test Data...')
    test_data = test_data.map(lambda x: (tokenizer(x[0]),x[1]))
    test_data.cache()
    # print(test_data.take(3))",0.4960677028,
286,configure spark,"if 'pyspark' not in vars():   # set up Apache Spark environment if not yet done so
    
    # set environment variables for Spark
    os.environ['SPARK_HOME'] = SPARK_HOME
    os.environ['SPARK_HIVE'] = 'true'
    
    # enable importing of PySpark through FindSpark package
    import findspark
    findspark.init()
    
    # import PySpark and set up SparkContext (""sc"") & HiveContext (""hc"")
    import pyspark
    
    sc = pyspark.SparkContext(
        conf=pyspark.SparkConf()
            .setMaster(SPARK_MODE)
            .setAppName('BostonHousing')
            .set('spark.driver.memory', SPARK_DRIVER_MEMORY)
            .set('spark.executor.memory', SPARK_EXECUTOR_MEMORY)
            .set('spark.driver.maxResultSize', SPARK_DRIVER_MAX_RESULT_SIZE))
    
    hc = pyspark.sql.HiveContext(sc)
    
print('SparkContext:', sc)
print('HiveContext:', hc)",0.4920502603,
286,configure spark,"#""""""
#Load packages and create context objects...
#""""""
import os
import platform
import sys
import pandas

if not 'sc' in vars():
    print """"""
***********************************************
*** Warning: Spark needs to be initialized ****
***********************************************
    """"""
else:
    print(""""""Already running
          ____              __
         / __/__  ___ _____/ /__
        _\ \/ _ \/ _ `/ __/  '_/
       /__ / .__/\_,_/_/ /_/\_\   version %s
          /_/
    """""" % sc.version)

if not 'sqlCtx' in vars():
    sqlCtx = SQLContext(sc)
print 'Spark Context available as `sc`'
print 'Spark SQL Context (%s) available as `sqlCtx`'%str(type(sqlCtx))
### The spark context doesn't really know the URL to the job manager... this would be different on every system
print ""Monitor this application at http://data-science-02.atl.primedia.com:8088/proxy/""+sc.applicationId",0.4912407696,
286,configure spark,"def real_main():
    sc = pyspark.SparkContext()
    company = _init_list(sc)
    dataRDD1 = sc.textFile(""gs://group688/nytimes"",5)
    dataRDD1 = dataRDD1.mapPartitions(lambda x:_data_filter(x,company,""nytimes""))
    dataRDD2 = sc.textFile(""gs://group688/wsj"",10)
    dataRDD2 = dataRDD2.mapPartitions(lambda x:_data_filter(x,company,""wsj""))
    dataRDD3 = sc.textFile(""gs://group688/reuters.dat"",10)
    dataRDD3 = dataRDD3.mapPartitions(lambda x:_data_filter(x,company,""reuters""))
    dataRDD  = dataRDD3.union(dataRDD2).union(dataRDD1)
    dataRDD.sortByKey().map(lambda x:x[1]).saveAsTextFile(""gs://group688/688v1"")

real_main()",0.4908556342,
286,configure spark,"def email_data(data_list):
    from sparkpost import SparkPost

#     print('//AA\n{}\n//ZZ'.format('\n'.join(data_list)))

    # Send email using the SparkPost api
    sp = SparkPost() # uses environment variable named SPARKPOST_API_KEY

    response = sp.transmission.send(
            recipients=['practicedata@globe.gov'],
            bcc=['aroller@ucar.edu'],
            text='//AA\n{}\n//ZZ'.format('\n'.join(data_list)),
            from_email='roller@rollercomsolutions.com',
            subject='DATA'
    )

    print(response)",0.488982141,
286,configure spark,"def startSparkContext(max_cores=16):
    from setupSpark import initSpark
    
    executor_cores = 8 # the number of cores to be used on each worker
    executor_memory = '25G' # the amount of memory to be used on each worker
    max_cores = max_cores # the max. number of cores Spark is allowed to use overall

    # returns the SparkContext object 'sc' which tells Spark how to access the cluster
    sc = initSpark(nbBackend, executor_cores=executor_cores, \
                   max_cores=max_cores, executor_memory=executor_memory)
    
    # provide OpenStack credentials to the Spark Hadoop configuration
    sc._jsc.hadoopConfiguration().set('fs.swift.service.SparkTest.username', os_username)
    sc._jsc.hadoopConfiguration().set('fs.swift.service.SparkTest.tenant', os_tenant_name)
    sc._jsc.hadoopConfiguration().set('fs.swift.service.SparkTest.password', os_password)
    
    # add Python files in 'utils' folder to the SparkContext 
    # this is required so that all files are available on all the cluster workers
    for filename in os.listdir(utils_dir):
        if filename.endswith('.py'):
            sc.addPyFile(os.path.join(utils_dir, filename))
            
    return sc",0.4870375991,
286,configure spark,"from pyspark.sql import SparkSession

# @hidden_cell
# This function is used to setup the access of Spark to your Object Storage. The definition contains your credentials.
# You might want to remove those credentials before you share your notebook.
def set_hadoop_config_with_credentials_efcfdf24587941948e423d7c9fdd8ff6(name):
    """"""This function sets the Hadoop configuration so it is possible to
    access data from Bluemix Object Storage using Spark""""""

    prefix = 'fs.swift.service.' + name
    hconf = sc._jsc.hadoopConfiguration()
    hconf.set(prefix + '.auth.url', 'https://identity.open.softlayer.com'+'/v3/auth/tokens')
    hconf.set(prefix + '.auth.endpoint.prefix', 'endpoints')
    hconf.set(prefix + '.tenant', '694a746300614629be7bdcfa60142623')
    hconf.set(prefix + '.username', '9e80f89e74fe4f1fbe962bb1cac03ab9')
    hconf.set(prefix + '.password', 'OqOVHe-.S,r7UX3~')
    hconf.setInt(prefix + '.http.port', 8080)
    hconf.set(prefix + '.region', 'dallas')
    hconf.setBoolean(prefix + '.public', False)

# you can choose any name
name = 'keystone'
set_hadoop_config_with_credentials_efcfdf24587941948e423d7c9fdd8ff6(name)

spark = SparkSession.builder.getOrCreate()

departments_df = spark.read\
  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\
  .option('header', 'true')\
  .load('swift://InstaCart.' + name + '/departments.csv')
departments_df.take(5)",0.4845682085,
286,configure spark,"from pyspark.sql import SparkSession

# @hidden_cell
# This function is used to setup the access of Spark to your Object Storage. The definition contains your credentials.
# You might want to remove those credentials before you share your notebook.
def set_hadoop_config_with_credentials_f63cbb38899d47179c49ed4a7cf03ccf(name):
    """"""This function sets the Hadoop configuration so it is possible to
    access data from Bluemix Object Storage using Spark""""""

    prefix = 'fs.swift.service.' + name
    hconf = sc._jsc.hadoopConfiguration()
    hconf.set(prefix + '.auth.url', 'https://identity.open.softlayer.com'+'/v3/auth/tokens')
    hconf.set(prefix + '.auth.endpoint.prefix', 'endpoints')
    hconf.set(prefix + '.tenant', '265dd6d8a99a4549a24ac9574846808d')
    hconf.set(prefix + '.username', '886a93bbc2564a539f02a62ed61e1a61')
    hconf.set(prefix + '.password', 'h8bjj]J[1DME7LnC')
    hconf.setInt(prefix + '.http.port', 8080)
    hconf.set(prefix + '.region', 'dallas')
    hconf.setBoolean(prefix + '.public', False)

# you can choose any name
name = 'keystone'
set_hadoop_config_with_credentials_f63cbb38899d47179c49ed4a7cf03ccf(name)

spark = SparkSession.builder.getOrCreate()

df = spark.read\
  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\
  .option('header', 'true')\
  .option(""inferSchema"", ""true"")\
  .load('swift://ITBSProjectFraudDetection.' + name + '/frauddetectionsmall.csv')
df.take(5)
df.dtypes",0.4845682085,
1097,load dataset,"def prepare_data():
    (x_train, y_train), (x_test, y_test) = mnist.load_data()
    x_train = x_train.reshape(60000, 784)
    x_test = x_test.reshape(10000, 784)
    x_train = x_train.astype(np.float32)/255.0
    x_test = x_test.astype(np.float32)/255.0
    y_train = np_utils.to_categorical(y_train)
    y_test = np_utils.to_categorical(y_test)
    return x_train, y_train, x_test, y_test",0.5426396728,
1097,load dataset,"def load_mnist():
    (X_train, y_train), (X_test, y_test) = mnist.load_data()
    return X_train, X_test, y_train, y_test",0.5419590473,
1097,load dataset,"def load_data():
    """"""
    This function loads the MNIST digit data using Keras inbuilt function \
    and returns the train and test set
    """"""
    
    (X_train, y_train), (X_test, y_test) = mnist.load_data()
    return (X_train, y_train), (X_test, y_test)",0.5417009592,
1097,load dataset,"def PreprocessDataset():
    # Load dataset
    (x_train, y_train), (x_test, y_test) = mnist.load_data()
    # Set numeric type
    x_train = x_train.astype('float32')
    x_test = x_test.astype('float32')
    # Normalize value to [0, 1]
    x_train /= 255
    x_test /= 255
    # Transform lables to one-hot
    y_train = np_utils.to_categorical(y_train, 10)
    y_test = np_utils.to_categorical(y_test, 10)
    
    # Reshape: here x_train is re-shaped to [width]  [height] x [channel]
    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)
    x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)
    return [x_train, x_test, y_train, y_test]

x_train, x_test, y_train, y_test = PreprocessDataset()",0.5366559029,
1097,load dataset,"def PreprocessDataset():
    # Load dataset
    (x_train, y_train), (x_test, y_test) = mnist.load_data()
    # Set numeric type
    x_train = x_train.astype('float32')
    x_test = x_test.astype('float32')
    # Normalize value to [0, 1]
    x_train /= 255
    x_test /= 255
    # Transform lables to one-hot
    y_train = np_utils.to_categorical(y_train, 10)
    y_test = np_utils.to_categorical(y_test, 10)
    # Reshape: here x_train is re-shaped to [channel]  [width]  [height]
    # In other environment, the orders could be different; e.g., [height]  [width]  [channel].
    x_train = x_train.reshape(x_train.shape[0], 1, 28, 28)
    x_test = x_test.reshape(x_test.shape[0], 1, 28, 28)
    return [x_train, x_test, y_train, y_test]

x_train, x_test, y_train, y_test = PreprocessDataset()",0.5366559029,
1097,load dataset,"def load_dataset():
    # the data, shuffled and split between train and test sets
    (X_train, y_train), (X_test, y_test) = cifar10.load_data()
    
    # Allocate last 5000 training examples for validation.
    X_train, X_val = X_train[:-5000], X_train[-5000:]
    y_train, y_val = y_train[:-5000], y_train[-5000:]
    
    # convert class vectors to binary class matrices
    y_train = np_utils.to_categorical(y_train, nb_classes)
    y_test = np_utils.to_categorical(y_test, nb_classes)
    y_val = np_utils.to_categorical(y_val, nb_classes)
    
    # preprocess data
    X_train = X_train.astype('float32')
    X_test = X_test.astype('float32')
    X_val = X_val.astype('float32')
    X_train /= 255
    X_test /= 255
    X_val /= 255
    
    print('X_train shape:', X_train.shape)
    print(X_train.shape[0], 'train samples')
    print(X_test.shape[0], 'test samples')
    print(y_train.shape[0], 'training labels')
    print(y_test.shape[0], 'test labels')
    print(X_val.shape[0], 'validation samples')
    print(y_val.shape[0], 'validation labels')

    return X_train, y_train, X_test, y_test, X_val, y_val",0.536444664,
1097,load dataset,"# Note: For whatever reason, I've experienced a bug with hyperas that
# prevents me from using any kind of comment in either the data() or
# model() function. For this reason I will attempt to describe the 
# code in both of these functions through comments and explanations
# outside of the functions themselves.
def data():
    (X_train, y_train), (X_test, y_test) = mnist.load_data()
    X_train = X_train.reshape(60000, 784)
    X_test = X_test.reshape(10000, 784)
    X_train = X_train.astype('float32')
    X_test = X_test.astype('float32')
    X_train /= 255
    X_test /= 255
    nb_classes = 10
    y_train = to_categorical(y_train, nb_classes)
    y_test = to_categorical(y_test, nb_classes)
    return X_train, y_train, X_test, y_test",0.5345757008,
1097,load dataset,"def data_vectors():

    X_test = np.loadtxt(util.dat_dir()+""X_test_A.dat"")
    X_train = np.loadtxt(util.dat_dir()+""X_train_A.dat"")
   
#     Xerr_test = np.loadtxt(util.dat_dir()+""Xerr_test_A.dat"")
#     Xerr_train = np.loadtxt(util.dat_dir()+""Xerr_train_A.dat"")
    
    Y_test = np.loadtxt(util.dat_dir()+""Y_test_A.dat"")
    Y_train = np.loadtxt(util.dat_dir()+""Y_train_A.dat"")
    
    return X_test, X_train, Y_test, Y_train",0.5321788788,
1097,load dataset,"train_size = 8

def load_data(size):
    (X_train, y_train), (_, _) = mnist.load_data()
    X_train = X_train[:size]
    X_train = (X_train.astype(np.float32) - 127.5) / 127.5
    X_train = X_train.reshape((X_train.shape[0], 1) + X_train.shape[1:])
    return X_train

X_train = load_data(train_size)",0.5280761719,
1097,load dataset,"nb_classes = 10
 
def load_dataset():
    (X_train, y_train), (X_test, y_test) = cifar10.load_data()
    print 'X_train shape:', X_train.shape
    print X_train.shape[0], 'train samples'
    print X_test.shape[0], 'test samples'
 
    Y_train = np_utils.to_categorical(y_train, nb_classes)
    Y_test = np_utils.to_categorical(y_test, nb_classes)
 
    X_train = X_train.astype('float32')
    X_test = X_test.astype('float32')
    X_train /= 255
    X_test /= 255
 
    return X_train, Y_train, X_test, Y_test

X_train, y_train, X_test, y_test = load_dataset()",0.5251182318,
733,generative model,"def build_model(l2_reg=0, dropout_rate=0):
    model = Sequential()
    model.add(Dense(20, input_dim=2, activation='relu'))
    if dropout_rate>0:
        model.add(Dropout(dropout_rate))
    model.add(Dense(40, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)))
    if dropout_rate>0:
        model.add(Dropout(dropout_rate))
    model.add(Dense(40, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)))
    model.add(Dense(1, activation='sigmoid'))
    # Compile model
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model",0.5550451279,
733,generative model,"def create_model(): 
    model = Sequential()
    model.add(LSTM(10,input_shape=(1,5),return_sequences=True))
    model.add(Dense(5,activation='relu'))
    model.add(Dense(1,activation='sigmoid'))
    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
    return model",0.5469132066,
733,generative model,"def create_baseline():
    model = Sequential()
    model.add(Dense(65, input_dim=65, init='normal', activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(32, input_dim=65, init='normal', activation='relu'))
    model.add(Dense(16, init= 'normal', activation = 'relu'))
    model.add(Dense(1, init='normal', activation='sigmoid'))
    #sgd = SGD(lr = 0.1, decay = 1e-6, momentum = 0.9, nesterov = True)
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model",0.5385064483,
733,generative model,"def build_generator():
    model = Sequential()
    model.add(Dense(256,input_dim=latent_dim))
    model.add(LeakyReLU(alpha=0.2))
    model.add(BatchNormalization(momentum=0.8))
    model.add(Dense(512))
    model.add()

    Dense: units=512
    LeakyReLU: alpha=0.2
    BatchNormalization: momentum=0.8
    Dense: units=1024
    LeakyReLU: alpha=0.2
    BatchNormalization: momentum=0.8
    Dense: units=number of pixel in each image in MNIST, actiovation = 'tanh'
    Reshape: shape = img_shape
    ####### CODE #########
    ####### CODE #########
    ####### CODE #########
    ####### CODE #########
    ####### CODE #########
    ####### CODE #########

    model.summary()
    noise = Input(shape=(latent_dim,))
    img = model(noise)

    return Model(noise, img)",0.5383685827,
733,generative model,"def new_model(input_dim=784, hidden_dim=512, dropout_rate=0.2):
    
    # Initialize model
    model = Sequential()
    
    # Layer 1 - INPUT
    model.add(Dense(hidden_dim, activation='relu', input_shape=(input_dim,)))
    model.add(Dropout(dropout_rate))
    
    # Layer 2 - HIDDEN
    model.add(Dense(hidden_dim, activation='relu'))
    model.add(Dropout(dropout_rate))
    
    # Layer 3 - OUTPUT
    model.add(Dense(10, activation='softmax'))
    
    # Compile model
    model.compile(loss='categorical_crossentropy',
                  optimizer=RMSprop(),
                  metrics=['accuracy'])
    
    print(model.summary())
    
    return model",0.5379365683,
733,generative model,"def baseline_model():
    model = Sequential()
    model.add(Dense(1024, input_dim=178, activation='relu'))
    model.add(Dropout(0.25))
    model.add(Dense(1024, activation='sigmoid'))
    model.add(Dropout(0.4))
    model.add(Dense(5, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer=Adagrad(), metrics=['accuracy'])
    return model",0.5378446579,
733,generative model,"def create_model():   
    model = Sequential()
    model.add(Convolution2D(32, 3, 3, activation='relu', input_shape=(25,25,192)))
    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(32, 3, 3, activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(32, 3, 3, activation='relu'))
    model.add(MaxPooling2D((2,2), strides=(2,2)))
    model.add(Flatten())
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(32, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(2, activation='softmax'))
    sgd = SGD(lr=0.0005, decay=1e-6, momentum=0.9, nesterov=True)
    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])
    return model",0.5375663638,
733,generative model,"# build the classifier
def build_classifier():
    model_k = Sequential()

    # first hidden layer
    model_k.add(Dense(10, activation='relu', input_shape=(11,)))
    model.add(Dropout(0.25))
    
    # second hidden layer
    model_k.add(Dense(10, activation='relu'))
    model.add(Dropout(0.25))
    # output layer
    model_k.add(Dense(1, activation='sigmoid'))
    
    # compile model
    
    model_k.compile(loss='binary_crossentropy', optimizer=optimizers.adam(), metrics=['accuracy'])

    return model_k",0.5369831324,
733,generative model,"def get_new_model():
    model = Sequential()
    model.add(Dense(25,input_shape=(n_cols,),activation='relu'))
    model.add(Dense(50, activation = 'relu'))
    model.add(Dense(50, activation = 'relu'))
    model.add(Dense(1, activation = 'sigmoid'))
    return model",0.5347138643,
733,generative model,"# define base mode
def baseline_model():
	# create model
	model = Sequential()
	model.add(Dense(30, input_dim=5, init='normal', activation='relu'))
	model.add(Dense(1, init='normal'))
	# Compile model
	model.compile(loss='mean_squared_error', optimizer='adam')
	return model",0.5344309807,
1135,look at how pandas seamlessly handles the time axis when you have a range of only year below,"if 2014 in org.index.levels[1]:
    wrong = org[(mb.MH_E - org.MH_E) != 0]
    right =  mb[(mb.MH_E - org.MH_E) != 0]

    for index in right.index:
        org.loc[index] = wrong[wrong.MH_E == right.loc[index].MH_E].values

    del wrong
    del right
    del index",0.5152480602,
1135,look at how pandas seamlessly handles the time axis when you have a range of only year below,"#Create  the synthetic labels. 
synthetic_labels = numpy.random.rand(employee_data.shape[0]) < p * employee_data.YearsAtCompany
#Plot the data with the synthetic labels
sns.swarmplot(y='years', x='quit', data=pd.DataFrame({""quit"":synthetic_labels, 'years':employee_data.YearsAtCompany}));",0.5129044056,
1135,look at how pandas seamlessly handles the time axis when you have a range of only year below,"# predict the missing past 3 years data
pred_steps = 90
predday = ""2020-01-01""
idxN = pd.date_range(Y.index[0], predday, freq=""D"")
idxO = pd.date_range(Y.index[0], Y.index[(len(Y)-1)], freq=""D"")
#futurem = len(idxN)-len(idxO)+1

mp_res = mf.predict(len(Y_log),len(Y_log)+pred_steps,dynamic = True)
mf_res = mf.forecast(steps=pred_steps+1)
mf_err = pd.DataFrame(mf_res[2])",0.5108030438,
1135,look at how pandas seamlessly handles the time axis when you have a range of only year below,"# Interpolate to get on-the-hour values
newidx = pd.date_range(start=ot.index[0].round('d')+pd.Timedelta('0h'),
                       end=ot.index[-1].round('d')-pd.Timedelta('1s'),
                       freq='1h', tz='UTC')

if True:
    # Simple linear interpolation
    at_interp_func = scipy.interpolate.interp1d(ot.index.astype('int64').values, 
                                           ot['AT'].values, 
                                           kind='linear', 
                                           fill_value=np.nan, #(0,1) 
                                           bounds_error=False)
else:
    # Should be better method, but has some screwy thing using updated data
    at_interp_func = scipy.interpolate.InterpolatedUnivariateSpline(
        ot.index.astype('int64').values, 
        ot['AT'].values, 
        k=BASE_INTERPOLATION_K,
        ext='const')

nt = pd.DataFrame({'AT':at_interp_func(newidx.astype('int64').values)},
                   index=newidx)",0.5076072812,
1135,look at how pandas seamlessly handles the time axis when you have a range of only year below,"# Resample the dataframe by month, and return the min value
df.resample('1M').min().head(5)",0.5050404072,
1135,look at how pandas seamlessly handles the time axis when you have a range of only year below,"if process_region2:
    # create date range dataframe
    date_missing_start = '2017-07-04'
    date_missing_end = '2018-01-03'

    date_range = pd.date_range(date_missing_start, date_missing_end)

    # create df and set dtypes
    df_region2 = pd.DataFrame(data={'Date': date_range, 'Region': '2'})

    df_region2.Date = pd.to_datetime(df_region2.Date)
    df_region2.Region = df_region2.Region.astype('category')

    print(df_region2.shape)
    df_region2.head()",0.5049543977,
1135,look at how pandas seamlessly handles the time axis when you have a range of only year below,"def reindex_by_date(df):
    dates = pd.date_range(start='2010-02-05',end='2012-10-26', freq='W-FRI')
    return df.reindex(dates).ffill()

store1_dept1=train[(train.Store==1) & (train.Dept==1)]
store1_dept1.groupby('id').apply(reindex_by_date).reset_index(0, drop=True)",0.5049427748,
1135,look at how pandas seamlessly handles the time axis when you have a range of only year below,"xticks = pd.date_range(start='2014-12-16 18:00:00', end=test.index[-1], freq='2D')",0.5043520927,
1135,look at how pandas seamlessly handles the time axis when you have a range of only year below,"#df.protection.head()
if protection_from_flopros:
#protection from FLOPROS
    minrp = 1/2 #assumes nobody is flooded more than twice a year
    df[""protection""]= pd.read_csv(""inputs/protection_national_from_flopros.csv"", index_col=""country"", squeeze=True 
                                 ).clip(lower=minrp)
else:
    #assumed a function of the income group
    protection_assumptions = pd.read_csv(""inputs/protection_level_assumptions.csv"", index_col=""Income group"")
    df[""protection""]=groups[""Income group""].replace(protection_assumptions[""protection""])",0.5040331483,
1135,look at how pandas seamlessly handles the time axis when you have a range of only year below,"rng = pd.date_range(px.index[0], periods=len(px) + N, freq='B')",0.5039322376,
851,how much test error do you get based on the optimizer you found above?,"NUM_EPOCHS = 10000
BATCH_SIZE = 100
DECAY_TIMING = 100
DECAY_RATE = 0.97

def decay_learning_rate(model, epoch):
    # Step decay
    opt = model.optimizer
    if (epoch + 1) % DECAY_TIMING == 0:
        K.set_value(opt.lr, DECAY_RATE * K.get_value(opt.lr))",0.4951388836,
851,how much test error do you get based on the optimizer you found above?,"class Optimizer(object):
    '''This is a basic class. 
    All other optimizers will inherit it
    '''
    def __init__(self, model, lr=0.01, weight_decay=0.0):
        self.model = model
        self.lr = lr
        self.weight_decay = weight_decay
        
    def update_params(self):
        pass


class SGD(Optimizer):
    '''Stochastic gradient descent optimizer
    https://en.wikipedia.org/wiki/Stochastic_gradient_descent
    '''
        
    def update_params(self):
        weights = self.model.get_params()
        grads = self.model.get_params_gradients()
        for w, dw in zip(weights, grads):
            update = self.lr * dw + self.weight_decay * w
            # it writes the result to the previous variable instead of copying
            np.subtract(w, update, out=w)",0.4665687978,
851,how much test error do you get based on the optimizer you found above?,"mapping = sdr.optimize(static, moving, static_affine, moving_affine, pre_align)",0.4617476761,
851,how much test error do you get based on the optimizer you found above?,"warp = sdr.optimize(static, moving, static_grid2world, moving_grid2world, pre_align)",0.4617476761,
851,how much test error do you get based on the optimizer you found above?,"# Defining NN for the action value function approximation.
class NN_Q_approx:
    def __init__(self, learning_rate, hidden_size, state_space, action_space, name):
        self.learning_rate = learning_rate
        self.state_space = state_space
        self.action_space = action_space
        self.name = name
        with tf.variable_scope(self.name):
            self.inputs = tf.placeholder(dtype=tf.float32, shape=[None, self.state_space], name='state_input')
            
            # Placeholder to choose the action value for the action done.
            self.actions = tf.placeholder(dtype=tf.int32, shape=[None], name='action_output')
            actions_oh = tf.one_hot(self.actions, self.action_space)
            
            self.target_Q = tf.placeholder(dtype=tf.float32, shape= [None], name='target_Q')
            
            self.fc1 = tf.contrib.layers.fully_connected(self.inputs, hidden_size, scope='fc1')
            self.fc2 = tf.contrib.layers.fully_connected(self.fc1, hidden_size, scope='fc2')
#             self.fc3 = tf.contrib.layers.fully_connected(self.fc2, hidden_size, scope='fc3')
            self.output = tf.contrib.layers.fully_connected(self.fc2, self.action_space, activation_fn=None, scope='output')
            # Until here for action value prediction.
            
            # Q size [m, action_space] -> sum, size [m, 1]
            self.Q = tf.reduce_sum(tf.multiply(self.output, actions_oh), axis=1)
            
            self.loss = tf.reduce_mean(tf.square(self.Q - self.target_Q))
            self.opt = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)",0.4614932537,
851,how much test error do you get based on the optimizer you found above?,"# Import Optimize API module
import quantopian.optimize as opt


def rebalance(context, data):
    # Create MaximizeAlpha objective using
    # sentiment_score data from pipeline output
    objective = opt.MaximizeAlpha(
        context.output.sentiment_score
    )",0.4604116678,
851,how much test error do you get based on the optimizer you found above?,"## solve model
res = scipy.optimize.minimize(objective, x0, method = 'BFGS', tol = 1e-6)
print(res.message)
print(res.success)",0.4603227377,
851,how much test error do you get based on the optimizer you found above?,"transform = TranslationTransform3D()
params0 = None
translation = affreg.optimize(target_data, source_data, transform, params0,
                              target_affine, source_affine)",0.4600922465,
851,how much test error do you get based on the optimizer you found above?,"class AdamOptimizer:
    def __init__(self, weights, alpha=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):
        self.alpha = alpha
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
        self.m = 0
        self.v = 0
        self.t = 0
        self.theta = weights
        
    def backward_pass(self, gradient):
        self.t = self.t + 1
        self.m = self.beta1*self.m + (1 - self.beta1)*gradient
        self.v = self.beta2*self.v + (1 - self.beta2)*(gradient**2)
        m_hat = self.m/(1 - self.beta1**self.t)
        v_hat = self.v/(1 - self.beta2**self.t)
        self.theta = self.theta - self.alpha*(m_hat/(np.sqrt(v_hat) - self.epsilon))
        return self.theta",0.4598098993,
851,how much test error do you get based on the optimizer you found above?,"model.objective = 'EX_his__L_e'
model.reactions.EX_glc__D_e.lower_bound = -10
model.reactions.EX_o2_e.lower_bound = -1000
max_flux = model.optimize().f",0.4579140842,
2458,use sklearn kfold,"from sklearn.cross_validation import KFold


def run_cv(X,y,criterion):
    # Construct a kfolds object
    kf = KFold(len(y),n_folds=5,shuffle=True)
    y_pred = y.copy()
    dc=DecisionTreeClassifier(criterion=criterion)
    for train_index, test_index in kf:
        X_train,X_test=X[train_index],X[test_index]
        y_train=y[train_index]
#         print(X_train.shape,y_train.shape)
        dc.fit(X_train.T,y_train)
        y_pred[test_index]=dc.predict(X_test.T)
        
#         print(X_train.shape,X_test.shape)
    return y_pred

y_pred_entropy=run_cv(df.T,target,'entropy')
y_pred_gini=run_cv(df.T,target,'gini')",0.6212351918,
2458,use sklearn kfold,"from sklearn.model_selection import KFold

def poisson_pseudoR2(y, yhat, ynull):
    """""" Compute the pseudo R2 score
        y : real firing rate
        yhat : predicted firing rate
        ynull : mean of the real firing rate
    """"""    
    yhat = yhat.reshape(y.shape)
    eps = np.spacing(1)
    L1 = np.sum(y*np.log(eps+yhat) - yhat)
    L1_v = y*np.log(eps+yhat) - yhat
    L0 = np.sum(y*np.log(eps+ynull) - ynull)
    LS = np.sum(y*np.log(eps+y) - y)
    R2 = 1-(LS-L1)/(LS-L0)
    return R2

def fit_cv(X, Y, algorithm, n_cv=10):
    """"""Performs cross-validated fitting. Returns (Y_hat, pR2_cv); a vector of predictions Y_hat with the
    same dimensions as Y, and a list of pR2 scores on each fold pR2_cv.
    X  = input data
    Y = spiking data
    algorithm = a function of (Xr, Yr, Xt) {training data Xr and response Yr and testing features Xt}
                and returns the predicted response Yt
    n_cv = number of cross-validations folds
    """"""
    if np.ndim(X)==1:
        X = np.transpose(np.atleast_2d(X))
    cv_kf = KFold(n_splits=n_cv, shuffle=True, random_state=42)
    skf  = cv_kf.split(X)
    Y_hat=np.zeros(len(Y))
    pR2_cv = list()
    for idx_r, idx_t in skf:
        Xr = X[idx_r, :]
        Yr = Y[idx_r]
        Xt = X[idx_t, :]
        Yt = Y[idx_t]           
        Yt_hat = eval(algorithm)(Xr, Yr, Xt)        
        Y_hat[idx_t] = Yt_hat
        pR2 = poisson_pseudoR2(Yt, Yt_hat, np.mean(Yr))
        pR2_cv.append(pR2)

    return Y_hat, pR2_cv",0.6126961708,
2458,use sklearn kfold,"from sklearn.model_selection import KFold


def cv_train_nn(x_train, y_train, n_splits):
    """""" Create and Train models for cross validation. Return best model """"""

    skf = KFold(n_splits=n_splits, shuffle=True)

    score = []

    best_model = None
    best_loss = float('inf')

    print('Training {} models for Cross Validation ...'.format(n_splits))

    for train, val in skf.split(x_train[:, 0], y_train[:, 0]):
        model = None
        model = helper.build_nn_reg(
            x_train.shape[1],
            y_train.shape[1],
            hidden_layers=1,
            input_nodes=x_train.shape[1] // 2,
            dropout=0.2,
            kernel_initializer=weights,
            bias_initializer=weights,
            optimizer=opt,
            summary=False)

        history = helper.train_nn(
            model,
            x_train[train],
            y_train[train],
            show=False,
            validation_data=(x_train[val], y_train[val]),
            epochs=500,
            batch_size=16,
            callbacks=callbacks)

        val_loss = history.history['val_loss'][-1]

        score.append(val_loss)

        if val_loss < best_loss:  # save best model (fold) for evaluation and predictions
            best_model = model
            best_loss = val_loss

    print('\nCross Validation loss: {:.3f}'.format(np.mean(score)))
    return best_model


model = cv_train_nn(x_train, y_train, 10)",0.6124923229,
2458,use sklearn kfold,"from sklearn.model_selection import KFold

def multi_reg_mean_encoding(train, cols, splits =5):
    kf = KFold(n_splits = splits)
    global_mean = train.any_spot.mean()
    
    for col in cols:
        means = train.groupby(col).any_spot.mean()
        train[col+'_mean_enc'] = train[col].map(means)        
        for tr_ind,val_ind in kf.split(train):
            tr,val = train.iloc[tr_ind],train.iloc[val_ind]
            foldmeans = tr.groupby(col).any_spot.mean()
            train.loc[val_ind,col+""_mean_enc""] = train.loc[val_ind,col].map(foldmeans)    
        train[col+""_mean_enc""].fillna(global_mean,inplace=True)   
        
#mean encoding for validation and test data
def multi_test_mean_encoding(test, train, cols):
    for col in cols:
        global_mean = train.any_spot.mean()
        means = train.groupby(col).any_spot.mean()
        test[col+""_mean_enc""] = test[col].map(means)
        test[col+""_mean_enc""].fillna(global_mean, inplace=True)",0.6124901175,
2458,use sklearn kfold,"from sklearn.base import clone
from sklearn.cross_validation import KFold

def kfold_stack(estimators, X, y=None, predict_method=""predict"",
                n_folds=3, shuffle=False, random_state=None,
                return_map=False):
    """"""Splits the dataset into `n_folds` (K) consecutive folds (without shuffling
    by default). Predictions are made on each fold while the K - 1 remaining folds
    form the training set for the predictor.

    Parameters
    ----------
    estimators : list of estimators
        The dictionary of estimators used to construct meta-features on
        the dataset (X, y). A cloned copy of each estimator is fitted on
        remainind data of each fold.

    X : {array-like, sparse matrix}, shape = [n_samples, n_features]
        Training vectors, where n_samples is the number of samples and
        n_features is the number of features.

    y : array-like, shape = [n_samples], optional
        Target values.
        
    predict_method : string, default=""predict""
        The method of each estimator, to be used for predictiong the
        meta features.

    n_folds : int, default=3
        Number of folds. Must be at least 2.

    shuffle : boolean, optional
        Whether to shuffle the data before splitting into batches.

    random_state : None, int or RandomState
        When shuffle=True, pseudo-random number generator state used for
        shuffling. If None, use default numpy RNG for shuffling.

    Returns
    ----------
    meta : array-like, shape = [n_samples, ...]
        Computed meta-features of each estimator.
        
    map : array-like
        The map, identifying which estimator each column of `meta`
        came from.
        
    """"""
    stacked_, index_ = list(), list()
    folds_ = KFold(X.shape[0], n_folds=n_folds,
                   shuffle=shuffle, random_state=random_state)
    for rest_, fold_ in folds_:
        fitted_ = [clone(est_).fit(X[rest_], y[rest_])
                   for est_ in estimators]

        predicted_ = [getattr(fit_, predict_method)(X[fold_])
                      for fit_ in fitted_]
        stacked_.append(np.stack(predicted_, axis=1))
        index_.append(fold_)

    stacked_ = np.concatenate(stacked_, axis=0)
    meta_ = stacked_[np.concatenate(index_, axis=0)]
    if not return_map:
        return meta_

    map_ = np.repeat(np.arange(len(estimators)),
                 [pred_.shape[1] for pred_ in predicted_])
    return meta_, map_",0.6100007892,
2458,use sklearn kfold,"from sklearn.cross_validation import KFold, train_test_split

def run_cv_coeffs(X,y,clf_class,**kwargs):
    # Construct a kfolds object
    kf = KFold(len(y),n_folds=5,shuffle=False)
    y_pred = y.copy()
    coeffs=[]
    scores=[]
    # Iterate through folds
    for train_index, test_index in kf:        
        X_train, X_test = X[train_index], X[test_index]

        y_train = y[train_index]
        y_test = y[test_index]
        # Initialize a classifier with key word arguments
        clf = clf_class(**kwargs)
        clf.fit(X_train,y_train)
        y_pred[test_index] = clf.predict(X_test)
        #print clf.coef_[0]
        coeffs.append(clf.coef_)
        scores.append(clf.score(X_test,y_test))
    return coeffs,scores


def get_coeffs(scoeffs,ncoeffs=None):
    if ncoeffs is None:
        ncoeffs = scoeffs.shape[1]
    coeffs_avgd = [(scoeffs[0][i] + scoeffs[1][i] + scoeffs[2][i] + scoeffs[3][i] + scoeffs[4][i])/5 for i in range(0,ncoeffs)]
    coeffs_std = [np.std([scoeffs[0][i],scoeffs[1][i],scoeffs[2][i],scoeffs[3][i],scoeffs[4][i]]) for i in range(0,ncoeffs)]
    return coeffs_avgd, coeffs_std",0.6091544628,
2458,use sklearn kfold,"from sklearn.grid_search import GridSearchCV
from sklearn.cross_validation import KFold

def gridsearch(model, params):
    
    """"""
    This function simplifies the process of a gridsearch by taking parameters and a model. It searches over a 5 cross\
    validations and prints out the best parameters that it's found as well as the best score

    """"""
    
    # Initiates a 5-fold gridsearch with specified params
    gs = GridSearchCV(model, params, cv=5)
    
    # Fits the gridsearch to the standardized predictors and target
    gs.fit(Xs,y)
    
    # Displays the best parameters it has found and its corresponding accuracy
    print (""------------------------------------------------------------------------------------"")
    print (""Best parameters: {}"".format(gs.best_params_))
    print (""Best accuracy score before splitting and training dataset: {:.2f}%\n"".format(gs.best_score_*100))
    
    return gs",0.6085378528,
2458,use sklearn kfold,"from sklearn.model_selection import KFold
def kfold_by_date(data,k=5):
    kf = KFold(n_splits=k,random_state=0,shuffle=True)
    dates = np.array(data['date'].value_counts().index.tolist())
    for train_index,test_index in kf.split(dates):
        train_data = data[data['date'].isin(dates[train_index])]
        valid_data = data[data['date'].isin(dates[test_index])]
        yield train_data,valid_data",0.6083773971,
2458,use sklearn kfold,"from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import KFold

# 50% Train / test validation
def train_nn(neuron_arch, train_features, train_labels):
    mlp = MLPClassifier(hidden_layer_sizes=neuron_arch)
    mlp.fit(train_features, train_labels)
    return mlp

def test(model, test_features, test_labels):
    predictions = model.predict(test_features)
    train_test_df = pd.DataFrame()
    train_test_df['correct_label'] = test_labels
    train_test_df['predicted_label'] = predictions
    overall_accuracy = sum(train_test_df[""predicted_label""] == train_test_df[""correct_label""])/len(train_test_df)    
    return overall_accuracy

def cross_validate(neuron_arch):
    fold_accuracies = []
    kf = KFold(n_splits = 4, random_state=2)
    for train_index, test_index in kf.split(data):
        train_features, test_features = data.loc[train_index], data.loc[test_index]
        train_labels, test_labels = labels.loc[train_index], labels.loc[test_index]
       
        model = train_nn(neuron_arch, train_features, train_labels)
        overall_accuracy = test(model, test_features, test_labels)
        fold_accuracies.append(overall_accuracy)
    return fold_accuracies",0.6073497534,
2458,use sklearn kfold,"from sklearn.datasets import load_iris
from sklearn.cross_validation import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics

# read in the iris data
iris = load_iris()
features = iris.data
labels = iris.target",0.6062803268,
175,central finite differences,"def getCoefficientsFFT(arr):
    fft1=np.fft.fft(arr)
    fft1=fft1[:fft1.shape[0]/2]
    ret=np.sqrt(np.power(np.real(fft1),2)+np.power(np.imag(fft1),2))
    return ret",0.391002357,
175,central finite differences,"def KdV_WA(delta_t, t_init, t_final, grid_points, U0, L):
    """""" Solves the KdV equation numerically using the Walkley
    finite difference scheme with periodic boundary conditions.
    
    Args: 
        delta_t      (float):   time step 
        t_init       (float):   initial time
        t_final      (float):   final time 
        grid_points  (int):     number of gripoints including boundaries
        U0           (ndarray): initial condition
        L            (float):   length of numerical domain 
    
    Returns:
        U_xt         (ndarray): value of numerical solution to Kdv equation
                                at each time step.""""""
    
    # Calculate the grid subdivision width and number of timesteps.
    delta_x = L / (grid_points - 1)
    num_tsteps = int((t_final - t_init) / delta_t)
    
    U_xt = numpy.empty([U0.shape[0], num_tsteps])
    U_xt[:,0] = U0
    
    print(""WALKLEY simulation parameters:\n"")
    print(""Delta x: %s\nDelta t: %s\nGridpoints: %s\nTimesteps: %s\n""
           %(delta_x, delta_t, grid_points, num_tsteps)) 

    t = t_init
    i = 1
    
    # For each timestep we compute the finite difference approximation 
    # U_xt to the solution of the KdV equation.
    while t < t_final and i < num_tsteps:
    
        Ui = U_xt[:,i-1]
        
        # Impose periodic boundary conditions by 'wrapping' 
        # the solution vector around the length of the domain.
        Ui_p1 = numpy.roll(U_xt[:,i-1], -1)
        Ui_p2 = numpy.roll(U_xt[:,i-1], -2)
        Ui_m1 = numpy.roll(U_xt[:,i-1], 1)
        Ui_m2 = numpy.roll(U_xt[:,i-1], 2)

        U_xt[:,i] = Ui - 0.25 * (delta_t / delta_x) * (Ui_m2**2 - 8 * Ui_m1**2 \
                     + 8 * Ui_p1**2 - Ui_p2**2) - delta_t / (2 * delta_x**3) \
                      * (Ui_p2 - Ui_m2 - 2.0 * (Ui_p1 - Ui_m1))

        t += delta_t
        i += 1
        
    return U_xt",0.3888126612,
175,central finite differences,"# Fetch results
def results(ucp):
    N, T, p, u, v = ucp.N, ucp.T, ucp.p, ucp.u, ucp.v
    # Check if problem is feasible
    if ucp.M.getProblemStatus(SolutionType.Default) in [ProblemStatus.PrimalFeasible]:
        # For time-constrained optimization it may be wise to accept any feasible solution
        ucp.M.acceptedSolutionStatus(AccSolutionStatus.Feasible)
        
        # Some statistics:
        print('Solution status: {0}'.format(ucp.M.getPrimalSolutionStatus()))
        print('Relative optimiality gap: {:.2f}%'.format(100*ucp.M.getSolverDoubleInfo(""mioObjRelGap"")))
        print('Total solution time: {:.2f}s'.format(ucp.M.getSolverDoubleInfo(""optimizerTime"")))

        return p.level().reshape([N,T+1]), u.level().reshape([N,T+1])
    else:
        raise ValueError(""No solution"")

pVal, uVal = results(ucp)",0.3866015375,
175,central finite differences,"def signum(u):
    return Vec(u.D, {k: 1 if v >= 0 else -1 for k, v in u.f.items()})",0.3853398561,
175,central finite differences,"def FT2D(im):
    
    ft2d = np.fft.fft2(im)
    # Create k (""frequency"") values
    kx = np.fft.fftfreq(ft2d.shape[1])
    ky = np.fft.fftfreq(ft2d.shape[0])

    # Sort kx, ky and Fourier components
    xidx = np.argsort(kx) #argsort returns sorted indices
    yidx = np.argsort(ky)

    # Note how the sorting is done for 2D Fourier map
    ft2d = ft2d[:, xidx]
    ft2d = ft2d[yidx, :]

    kx = kx[xidx]
    ky = kx[yidx]
    
    return ft2d, kx, ky

ft2d, kx, ky = FT2D(im)

plt.figure()
plt.title(""After sorting"")
plt.imshow(np.sqrt(np.abs(ft2d)), cmap = plt.cm.gray)
plt.colorbar()
plt.show()",0.3832933307,
175,central finite differences,"def spectrum_getter(X):
    Spectrum = scipy.fft(X, n=X.size)
    return np.abs(Spectrum)",0.3826188445,
175,central finite differences,"def fourier(X):
    Y = scipy.fft(X, n=X.size)
    return np.abs(Y)",0.3826188445,
175,central finite differences,"def Laplacian(f):
    df = np.zeros_like(f)
    df[1:-1, 1:-1] += np.diff(f, n=2, axis=0)[:, 1:-1]
    df[1:-1, 1:-1] += np.diff(f, n=2, axis=1)[1:-1, :]
        
    return df",0.3811653852,
175,central finite differences,"def discretization_errors():
    from numpy import log, abs
    I = 1
    a = 1
    T = 4
    t = linspace(0, T, 101)
    schemes = {'FE' : 0, 'BE' : 1, 'CN' : 0.5} # theta to scheme name
    dt_values = [0.8, 0.4, 0.1, 0.01]
    
    for dt in dt_values:
        figure()
        legends = []
        for scheme in schemes:
            theta = schemes[scheme]
            u, t = solver(I, a, T, dt, theta)
            u_e = model(t, I, a)
            error = u_e - u
            print '%s: dt=%.2f, %d steps, max error: %.2E'%\
                (scheme, dt, len(u)-1, abs(error). max())
            # Plot log(error), but exclude error[0] since it is 0
            plot(t[1:], log(abs(error[1:])))
            legends. append(scheme)
        xlabel('t' ); ylabel('log(abs(numerical error))' )
        legend(legends, loc='upper right' )
        title(r'$\Delta t=%g$'%dt)
        savefig('tmp_dt%g.png'%dt); savefig('tmp_dt%g.pdf'%dt)",0.3806293905,
175,central finite differences,"def NormalizeData(X):
    mu = [0] * len(X.T)
    for i in range(len(X)):
        mu += X[i]  
    mu = mu/len(X)
    for i in range(len(X)):
        X[i] = X[i] - mu
    
    
    
    ro2 = 0
    #print(X[0])
    for i in range(len(X)):
        for j in range(len(X[i].A1)):
            #print(X[i].A1[j])
            temp =X[i].A1[j]
            ro2 +=  temp*temp 
    
    ro2 = ro2 / (len(X))
    
    ro = ro2**(1/2)
    
    X = X/ro
    
    return X",0.3761706352,
1202,mnist data,"import os
import struct
import numpy as np

def load_mnist(path, kind='train'):
    """"""Load MNIST data from `path`""""""
    labels_path = os.path.join(path, 
                               '%s-labels.idx1-ubyte' 
                                % kind)
    images_path = os.path.join(path, 
                               '%s-images.idx3-ubyte' 
                               % kind)
        
    with open(labels_path, 'rb') as lbpath:
        magic, n = struct.unpack('>II', 
                                 lbpath.read(8))
        labels = np.fromfile(lbpath, 
                             dtype=np.uint8)

    with open(images_path, 'rb') as imgpath:
        magic, num, rows, cols = struct.unpack("">IIII"", 
                                               imgpath.read(16))
        images = np.fromfile(imgpath, 
                    dtype=np.uint8).reshape(len(labels), 784)
 
    return images, labels",0.4073011279,
1202,mnist data,"import os
import struct
import numpy as np
 
def load_mnist(path, kind='train'):
    """"""Load MNIST data from `path`""""""
    labels_path = os.path.join(path, 
                               '%s-labels-idx1-ubyte' 
                                % kind)
    images_path = os.path.join(path, 
                               '%s-images-idx3-ubyte' 
                               % kind)
        
    with open(labels_path, 'rb') as lbpath:
        magic, n = struct.unpack('>II', 
                                 lbpath.read(8))
        labels = np.fromfile(lbpath, 
                             dtype=np.uint8)

    with open(images_path, 'rb') as imgpath:
        magic, num, rows, cols = struct.unpack("">IIII"", 
                                               imgpath.read(16))
        images = np.fromfile(imgpath, 
                             dtype=np.uint8).reshape(len(labels), 784)
 
    return images, labels",0.4071238041,
1202,mnist data,"def get_dataflow(batch_size, is_train='train'): # str type
    # bool_is_train = is_train == 'train' :  T/F    . 
    df = dataset.Mnist(is_train, shuffle=True) # return type: dataflow(df)
    # dataset  Mnist data  . FashionMnist Data . 
    # ----- Image Augmentation Options -------- #
    if is_train is 'train':
        augs = [
            #   imgaug.CenterCrop(256, 256),
            #   imgaug.Resize((225, 225)),
            #   imgaug.Grayscale(keepdims=True),
#                imgaug.Flip(horiz=True, vert=False, prob=0.5),
        ]
    else:
        augs = [
            #   imgaug.CenterCrop(256, 256),
            #   imgaug.Resize((225, 225)),
        ]
    df = AugmentImageComponent(df, augs)
    # group data into batches of size 128
    df = BatchData(df, batch_size)
    # start 3 processes to run the dataflow in parallel
    # df = PrefetchDataZMQ(df, 10, multiprocessing.cpu_count())
    # df = PrefetchDataZMQ(df, 64) # : batch size = 64
    return df",0.4031618536,
1202,mnist data,"path = ""./data""

def load_mnist(path, kind='train'):
    import os
    import gzip
    import numpy as np

    """"""Load MNIST data from `path`""""""
    labels_path = os.path.join(path,
                               '%s-labels-idx1-ubyte.gz'
                               % kind)
    images_path = os.path.join(path,
                               '%s-images-idx3-ubyte.gz'
                               % kind)

    with gzip.open(labels_path, 'rb') as lbpath:
        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,
                               offset=8)

    with gzip.open(images_path, 'rb') as imgpath:
        images = np.frombuffer(imgpath.read(), dtype=np.uint8,
                               offset=16).reshape(len(labels), 784)

    return images, labels",0.4024435878,
1202,mnist data,"def load_mnist():
    mnist = input_data.read_data_sets('../data/', one_hot=True)
    trainimg,trainlabel = mnist.train.images,mnist.train.labels
    testimg,testlabel = mnist.test.images,mnist.test.labels
    valimg,vallabel = mnist.validation.images,mnist.validation.labels
    return trainimg,trainlabel,testimg,testlabel,valimg,vallabel
# Demo usage of mnist loader
if __name__=='__main__':
    trainimg,trainlabel,testimg,testlabel,valimg,vallabel = load_mnist()
    print (""We have [%d] train, [%d] test, and [%d] validation images.""
           %(trainimg.shape[0],testimg.shape[0],valimg.shape[0]))",0.3964308202,
1202,mnist data,"# Fashion MNIST
def load_mnist(path, kind='train'):
    import os
    import struct
    import gzip
    import numpy as np

    """"""Load MNIST data from `path`""""""
    labels_path = os.path.join(path,
                               '%s-labels-idx1-ubyte.gz'
                               % kind)
    images_path = os.path.join(path,
                               '%s-images-idx3-ubyte.gz'
                               % kind)

    with gzip.open(labels_path, 'rb') as lbpath:
        struct.unpack('>II', lbpath.read(8))
        labels = np.frombuffer(lbpath.read(), dtype=np.uint8)

    with gzip.open(images_path, 'rb') as imgpath:
        struct.unpack("">IIII"", imgpath.read(16))
        images = np.frombuffer(imgpath.read(), dtype=np.uint8).reshape(len(labels), 784)

    return images, labels",0.3796773255,
1202,mnist data,"def load_mnist(path, kind='train'):

    labels_path = os.path.join(path,
                               '%s-labels-idx1-ubyte.gz'
                               % kind)
    images_path = os.path.join(path,
                               '%s-images-idx3-ubyte.gz'
                               % kind)

    with gzip.open(labels_path, 'rb') as lbpath:
        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,
                               offset=8)

    with gzip.open(images_path, 'rb') as imgpath:
        images = np.frombuffer(imgpath.read(), dtype=np.uint8,
                               offset=16).reshape(len(labels), 784)

    return images, labels",0.3751273751,
1202,mnist data,"class Cleaned_Lemmatizer(TransformerMixin, BaseEstimator):
    """"""
    Takes a pandas Series of strings
    cleans the html tags, removes punctuation,
    spaces, non-aplhabetic characters, words less
    than 3 characters, pronouns
    
    returns a panda Series with a string of 
    clean and lemmatized sentence.
    """"""
    
    def __init__(self):
        # disable parser and ner as we will not be using and it
        # will unnecessary slow down the process
        self.nlp = spacy.load('en', disable=['parser','ner'])
        self.count = 0
        
        
    def fit(self, X, y=None):
        return self
    
    
    def transform(self, X):
        return X.apply(self.clean_sent)
 
  
    def clean_sent(self, review):
        """"""
        Takes a sentence string clean html, punctuation,
        whitespaces,
        Tokenizes and lemmatizes.
        Returns a list of unigrams,
        (for compatibility with gensim).
        """"""
        
        self.count += 1
        if self.count % 10000 == 0:
            print('Count: ', self.count)
        
        review = self.cleanhtml(review)

        # apply spaCy nlp pipeline to review
        doc = self.nlp(review)

        return ' '.join([token.lemma_ for token in doc 
                         if self.word_filter(token)])

    
    def cleanhtml(self, sent):
        """"""
        Clean words of any html tags
        """"""
        cleanr = re.compile('<.*?>')
        cleantext = re.sub(cleanr, ' ', sent)
        return cleantext
    

    def word_filter(self, token):
        """"""
        Helper to remove unwanted tokens
        token - spacy token object
        """"""
        # filter punc and spaces
        if token.is_punct or token.is_space:
            return False


        # When I did not filter no-alpha characters
        # I get a lot of measurement information in 
        # bi-grams and tri-grams like 1/2_cup_tea, 1/4_tea_spoon,
        # etc.
        if len(token) < 3 or not token.is_alpha:
            return False

        # filter pronouns as they are replaced by '-PRON-'
        # by spaCy lematizer and also pronouns are
        # (most likely) not important for our objective.
        if token.lemma_ == '-PRON-':
            return False

        # We need to filter conjunction and determiner
        # because while generating bi-grams using Phrase model 
        # the most frequent bi-grams were in format det-conj, 
        # det-noun, conj-det, conj-verb, etc 
        # and they are (most likely) useless in our context.
        if token.pos_ == 'CCONJ' or token.pos_ == 'DET':
            return False

        return True",0.3747823834,
1202,mnist data,"def get_dataflow(batch_size, is_train='train'):
    df = dataset.Mnist(is_train, shuffle=True)
    istrain = is_train == 'train'
    
    # ----- Image Augmentation Options -------- #
    if istrain:
        augs = [
            #   imgaug.CenterCrop(256, 256),
            #   imgaug.Resize((225, 225)),
            #   imgaug.Grayscale(keepdims=True),
            #   imgaug.Flip(horiz=True, vert=False, prob=0.5),
        ]
    else:
        augs = [
            #   imgaug.CenterCrop(256, 256),
            #   imgaug.Resize((225, 225)),
        ]
    df = AugmentImageComponent(df, augs)
    # group data into batches of size 128
    df = BatchData(df, batch_size)
    # start 3 processes to run the dataflow in parallel
    # df = PrefetchDataZMQ(df, 10, multiprocessing.cpu_count())
    return df",0.3739720583,
1202,mnist data,"if ""CIFAR10_EXP_1"" not in os.environ:
    sys.stderr.write(""loading mnist...\n"")
    train_data, valid_data, _ = hp.load_mnist(""../../data/mnist.pkl.gz"")
    X_train, y_train = train_data
    X_valid, y_valid = valid_data
    # minimal
    X_train_minimal = X_train[0:200]
    y_train_minimal = y_train[0:200]
    # ---
    X_train = theano.shared(np.asarray(X_train, dtype=theano.config.floatX), borrow=True)
    y_train = theano.shared(np.asarray(y_train, dtype=theano.config.floatX), borrow=True)
    X_valid = theano.shared(np.asarray(X_valid, dtype=theano.config.floatX), borrow=True)
    y_valid = theano.shared(np.asarray(y_valid, dtype=theano.config.floatX), borrow=True)
    # minimal
    X_train_minimal = theano.shared(np.asarray(X_train_minimal, dtype=theano.config.floatX), borrow=True)
    y_train_minimal = theano.shared(np.asarray(y_train_minimal, dtype=theano.config.floatX), borrow=True)
    # ---
    #y_train = T.cast(y_train, ""int32"")
    #y_valid = T.cast(y_valid, ""int32"")
    # minimal
    #y_train_minimal = T.cast(y_train_minimal, ""int32"")
    # ---
else:
    sys.stderr.write(""loading cifar10...\n"")
    dat = deep_residual_learning_CIFAR10.load_data()
    X_train_and_valid = dat[""X_train""]
    y_train_and_valid = dat[""Y_train""]
    
    X_train_minimal = theano.shared(X_train_and_valid[0:100].astype(theano.config.floatX), borrow=True)
    y_train_minimal = theano.shared(y_train_and_valid[0:100].astype(theano.config.floatX), borrow=True)
    
    X_test = theano.shared(dat[""X_test""].astype(theano.config.floatX), borrow=True)
    y_test = theano.shared(dat[""Y_test""].astype(theano.config.floatX), borrow=True)
    n = X_train_and_valid.shape[0]
    X_train = theano.shared(X_train_and_valid[0 : 0.85*n].astype(theano.config.floatX), borrow=True)
    y_train = theano.shared(y_train_and_valid[0 : 0.85*n].astype(theano.config.floatX), borrow=True)
    X_valid = theano.shared(X_train_and_valid[0.85*n :: ].astype(theano.config.floatX), borrow=True)
    y_valid = theano.shared(y_train_and_valid[0.85*n :: ].astype(theano.config.floatX), borrow=True)",0.3737593293,
197,checkpoint,"def print_midi_message(port, message):
    if message.bytes()[0] == 176:
        print(""[{}] @{:0.6f} {}"".format(port.name, message.time, message.bytes()))
read_midi_messages(device_name)",0.4103427529,
197,checkpoint,"def set_middle_pixel_for_p_shades(pixel, p):
    R = pixel[0]
    
    if p < 1 or p >= 256:
        print('p value not valid')
    
    lst = [0]
    diff = 255 / p
    running_total = 0 + diff
    
    for i in range(1, p):
        lst.append(running_total)
        
        if lst[i-1] <= R < lst[i]:
            # print(lst)
            # print(diff)
            return (lst[i-1] + lst[i]) / 2
        
        running_total += diff",0.4085541368,
197,checkpoint,"def pretty_print(count, permute):
    print('A={} B={} C={} D={} E={} F={} G={} H={} I={}'.format(permute[0], permute[1],
                                                            permute[2], permute[3],
                                                            permute[4], permute[5], 
                                                            permute[6], permute[7],
                                                            permute[8]))
    print('\nSearches:', count)",0.4039559364,
197,checkpoint,"#####look at gene EGFR
os.system('mkdir sufam')
###
def cmd_extract_reads(loc, filename):
    folder = filename[24:26]
    cmd = 'samtools view -bh ../B_G_'+folder+'/'+filename+' '+loc+' > EGFR.'+filename
    return(os.system(cmd))
def cmd_index(filename):
    cmd = 'samtools index '+filename
    return(os.system(cmd))
def cmd_tofasta(filename):
    cmd = 'samtools fasta '+filename+' > '+filename+'.fa'
    return(os.system(cmd))
def cmd_sufam(filename):
    cmd = 'sufam /avicenna/genomes/hg38_2/hg38.fa mutations.vcf '+filename+' 2> sufam/'+filename+'.log > sufam/'+filename+'.tsv'
    return(os.system(cmd))
expr = pd.read_csv(""scater/filtered_exprs_edited.txt"", sep=',',index_col = 0)
####
cluster3_index = ['4','5','9','10','38']
gene = 'ENSG00000146648'
####
print_cluster3_genevals(gene,cluster3_index)

####pull out EGFR exon 21 reads from bam files
loc = ""chr7:55191822""
###
files = pd.read_csv(""scater/filtered_file.txt"", sep= '\t', index_col = 0)
files = list(files['x'])
###
# for name in files:
#     cmd_extract_reads(loc,name)
#     cmd_index('EGFR.'+name)
#     cmd_tofasta('EGFR.'+name)
#     cmd_sufam('EGFR.'+name)",0.4030137956,
197,checkpoint,"def myfunction(pack):
    a, b, c, d = pack
    print a+b+c+d[0]-d[1]
    
t = 1, 2, 3, (8, 9)  # Pack arguments to pass it to a function

myfunction(t)",0.4010262191,
197,checkpoint,"# called when all files finish uploading
def done_cb(w, name):
    # Do something with the files here...
    # We just print their names
    print(""%s uploaded"" % name)
    
    # reset clears and re-enables the widget for more uploads
    # You may want to do this somewhere else, depending on your GUI
    w.reset()",0.3973141909,
197,checkpoint,"def DeconvSimple(signal,signal_inv):
	""""""
	Deconvolution of the fine-grained fee signal (no DAQ)
	no noise
	using true start and end of signals
	""""""

	coef = signal_inv[100]
	
	print ""coef = %7.4g""%coef

	acum = np.zeros(len(signal))

	acum[0]=coef*signal[0]
	for n in xrange(1,len(signal)):
		acum[n] = acum[n-1] + signal[n]

	signal_r = signal + coef*acum

	return signal_r",0.3962554336,
197,checkpoint,"def chained_nested_conditions(a, b):
    if a + b == 42:
        print(""a + b can't be 42"")
    # elif is how you test another condition
    # you can call function and use their return value
    elif is_even(a):
        # this condition will be tested only if a is even
        if is_even(b):
            print(""a and b are even"")
        else:
            print(""a is even, not b"")
    # if you don't match any of the previous conditions you will finish in this one
    else:
        if is_even(b):
            print(""a uneven, b even"")
        else:
            print(""a and b uneven"")

chained_nested_conditions(40, 2)
print(""____"")
chained_nested_conditions(2, 2)",0.3961757123,
197,checkpoint,"def check_len(master):
    check = len(master) - 141
    if check == 0:
        print ""Hurray! We didn't lose any rows.""
    if check < 0:
        print ""Uh oh. We lost {} rows!"".format(str(abs(check)))
    if check > 0:
        print ""we somehow...gained {} row? Well that's not right."".format(str(abs(check)))
check_len(master)",0.3961526155,
197,checkpoint,"def print_midi_message(port, message):
    if message.bytes()[0] >= 128 and message.bytes()[0] < 160:
        print(""[{}] @{:0.6f} {}"".format(port.name, message.time, message.bytes()))
read_midi_messages(device_name)",0.3960216045,
1558,predictions and evaluation of decision tree,"class Decision_Tree:

    """"""creates a decision tree """"""

    def __init__(self, training_data, T_index, max_depth=10**9, num_Features=None):
        
        self.training_data = training_data
        self.T_index       = T_index
        self.max_depth     = max_depth
        self.num_Features  = num_Features
        
        # build the decision tree
        self.tree = self.build_tree(self.training_data)
        
        # training statistics (these become initialized after calling train)
        self.training_classifications = None
        self.training_booleans = None
        self.training_per = None
        
        #testing statistics (these become initialized after using the ""use"" method)
        self.testing_data = None
        self.testing_classifications = None
        self.testing_booleans = None
        self.testing_per = None

    def class_counts(self, rows):
        """"""Counts the number of each type of example in a dataset.""""""

        counts = {}  # a dictionary of label -> count.
        for row in rows:
            label = row[self.T_index]
            if label not in counts:
                counts[label] = 0
            counts[label] += 1
        return counts

    def is_numeric(self,value):
        """"""Test if a value is numeric.""""""
        return isinstance(value, int) or isinstance(value, float)
    
    class Question:
        """"""A Question is used to partition a dataset.

        This class just records a 'column number' (e.g., 0 for Color) and a
        'column value' (e.g., Green). The 'match' method is used to compare
        the feature value in an example to the feature value stored in the
        question. See the demo below.
        """"""

        def __init__(self, column, value):
            self.column = column
            self.value = value

        def match(self, example):
            # Compare the feature value in an example to the
            # feature value in this question.
            val = example[self.column]
            if is_numeric(val):
                return val >= self.value
            else:
                return val == self.value

        def __repr__(self):
            # This is just a helper method to print
            # the question in a readable format.
            condition = ""==""
            if is_numeric(self.value):
                condition = "">=""
            return ""Is %s %s %s?"" % (
                labels[self.column], condition, str(self.value))

    def partition(self, rows, question):
        """"""Partitions a dataset.

        For each row in the dataset, check if it matches the question. If
        so, add it to 'true rows', otherwise, add it to 'false rows'.
        """"""
        true_rows, false_rows = [], []
        for row in rows:
            if question.match(row):
                true_rows.append(row)
            else:
                false_rows.append(row)
        return true_rows, false_rows

    def gini(self, rows):
        """"""Calculate the Gini Impurity for a list of rows.

        There are a few different ways to do this, I thought this one was
        the most concise. See:
        https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity
        """"""
        counts = class_counts(rows,self.T_index)
        impurity = 1
        for lbl in counts:
            prob_of_lbl = counts[lbl] / float(len(rows))
            impurity -= prob_of_lbl**2
        return impurity   
    
    def info_gain(self, left, right, current_uncertainty):
        """"""Information Gain.

        The uncertainty of the starting node, minus the weighted impurity of
        two child nodes.
        """"""
        p = float(len(left)) / (len(left) + len(right))
        return current_uncertainty - p * self.gini(left) - (1 - p) * self.gini(right)
    
    def find_best_split(self, rows):
        """"""Find the best question to ask by iterating over every feature / value
        and calculating the information gain.""""""

        best_gain = 0  # keep track of the best information gain
        best_question = None  # keep train of the feature / value that produced it
        current_uncertainty = self.gini(rows)

        
        range_indices = list(range(0, len(rows[0])))
        range_indices.remove(self.T_index)
        indices = range_indices
                
        if indices is None: print(""Indices are None"")
        
        # picks random features to analyze.
        # excludes the feature that is being predicted on
        if self.num_Features is not None:
            indices = random.sample(indices, self.num_Features)
        
        for col in indices:  # for each feature

            values = set([row[col] for row in rows])  # unique values in the column

            for val in values:  # for each unique value

                question = Question(col, val)

                # try splitting the dataset
                true_rows, false_rows = self.partition(rows, question)

                # Skip this split if it doesn't divide the dataset.
                if len(true_rows) == 0 or len(false_rows) == 0:
                    continue

                # Calculate the information gain from this split
                gain = self.info_gain(true_rows, false_rows, current_uncertainty)

                # You actually can use '>' instead of '>=' here
                # but I wanted the tree to look a certain way for our
                # toy dataset.
                if gain >= best_gain:
                    best_gain, best_question = gain, question

        return best_gain, best_question
    
    class Leaf:
        """"""A Leaf node classifies data.

        This holds a dictionary of class (e.g., ""Apple"") -> number of times
        it appears in the rows from the training data that reach this leaf.
        """"""

        def __init__(self, rows):

            self.predictions = class_counts(rows,T_index)        # all the prediction info
            self.class_prediction = self._get_class_prediction() # leaf prediction

        def format_predictions(self):
            '''this is used in the nodes'''
            formatted = ""\n""
            for key in self.predictions:
                formatted += str(key) + "" in node="" + str(self.predictions[key])+""\n""
            return formatted

        def _get_class_prediction(self):
            # invert dictionary
            inverted = dict((self.predictions[keys], keys) for keys in self.predictions)

            # find max value
            max_val = max(inverted.keys())

            #guess the class with the most votes
            return inverted[max_val]


        def __repr__(self):
            return str(self.predictions)

    class Decision_Node:
        """"""A Decision Node asks a question.

        This holds a reference: 
        * The question
        * Two child nodes
        * Information Gain 
        """"""

        def __init__(self,
                     question,
                     gain,
                     gi,
                     true_branch,
                     false_branch):

            self.question = question
            self.gain     = gain
            self.gi       = gi

            self.true_branch  = true_branch
            self.false_branch = false_branch
         
    def build_tree(self, rows, current_depth=0):
        """"""Builds the tree.

        Rules of recursion: 1) Believe that it works. 2) Start by checking
        for the base case (no further information gain). 3) Prepare for
        giant stack traces.
        """"""
        
        #update my current depth
        current_depth += 1
        
        if current_depth == self.max_depth:
            return Leaf(rows)
        
        # Try partitioing the dataset on each of the unique attribute,
        # calculate the information gain,
        # and return the question that produces the highest gain.
        gain, question = self.find_best_split(rows)

        # Base case: no further info gain
        # Since we can ask no further questions,
        # we'll return a leaf.
        if gain == 0:
            return Leaf(rows)

        # If we reach here, we have found a useful feature / value
        # to partition on.
        true_rows, false_rows = self.partition(rows, question)

        # Recursively build the true branch.
        true_branch = self.build_tree(true_rows, current_depth)

        # Recursively build the false branch.
        false_branch = self.build_tree(false_rows, current_depth)

        # Return a Question node.
        # This records the best feature / value to ask at this point,
        # as well as the branches to follow
        # dependingo on the answer.
        return Decision_Node(question, gain, gini(rows,self.T_index), true_branch, false_branch)

    def print_tree(self,node=None): 
        '''
        creates a visual representation of the decision tree graph, starting from whatever node is passed in.
        * It is important to note that some characters, such as "":"" wont display properly in the nodes.
        '''
        if node is None:
            node = self.tree
        
        import pydot
        from IPython.display import Image, display

        #create a graph with a root node:
        G = pydot.Dot(graph_type=""digraph"")
        label = str(node.question)+\
                ""\ninfo gain=""+str(round(node.gain,2))+\
                ""\ngini=""+str(round(node.gi,2))
        root_node = pydot.Node(label,style=""filled"", fillcolor=""red"")
        G.add_node(root_node)

        #call print function on leaf nodes

        leaf_count = [1] # I use a list so that the leaf_count is passed by refference

        self.print_node(node.true_branch,  G, root_node, leaf_count, truth=""True"")
        self.print_node(node.false_branch, G, root_node, leaf_count)

        im = Image(G.create_png())
        display(im)

        print(""KEY:"")
        print(""* Root Node:      Red"")
        print(""* Decision Nodes: Green"")
        print(""* Leaf Nodes:     Yellow"")

    def print_node(self, node, graph, graph_node, leaf_count,truth=""False""):
        '''
        This is a resursive function that gets called by print_tree to display a decision tree
        '''
        # Base case: we've reached a leaf
        if isinstance(node, Leaf):

            label = ""Leaf-""+str(leaf_count[0])+\
                    ""\nprediction=""+str(node.class_prediction)+\
                    node.format_predictions()

            leaf_node = pydot.Node(label,style=""filled"", fillcolor=""yellow"")
            leaf_count[0] = leaf_count[0]+1
            graph.add_node(leaf_node)
            edge = pydot.Edge(graph_node,leaf_node,label=truth)
            graph.add_edge(edge)
            return

        # Print Node Information
        label = str(node.question)+\
                ""\ninfo gain=""+str(round(node.gain,2))+\
                ""\ngini=""+str(round(node.gi,2))
        decision_node = pydot.Node(label,style=""filled"", fillcolor=""green"")
        graph.add_node(decision_node)
        edge = pydot.Edge(graph_node,decision_node, label=truth)
        graph.add_edge(edge)

        # Call this function recursively on the true branch
        #print(spacing + '--> True:')
        print_node(node.true_branch,  graph, decision_node, leaf_count, truth=""True"")

        # Call this function recursively on the false branch
        #print (spacing + '--> False:')
        print_node(node.false_branch, graph, decision_node, leaf_count)
        
    def get_leaf(self, row, node=None):
        """"""Gets the leaf that corresponds to a particular row""""""

        if node is None:
            node = self.tree
            
        # Base case: we've reached a leaf
        if isinstance(node, Leaf):
            return node

        # Decide whether to follow the true-branch or the false-branch.
        # Compare the feature / value stored in the node,
        # to the example we're considering.
        if node.question.match(row):
            return self.get_leaf(row, node.true_branch)
        else:
            return self.get_leaf(row, node.false_branch)
        
    def classify(self,row,node=None):
        '''classifies a particular row, starting from a particular node'''
        if node is None:
            node = self.tree
            
        return self.get_leaf(row,node).class_prediction
    
    def classify_all(self, data, node=None):
        '''Returns a list that describes that category that reach row in the data was classified as'''
        if node is None:
            node = self.tree
            
        return [self.classify(row,node) for row in data] 
    
    def classification_results(self, data, classifications, node=None):
        '''returns a boolean list describing whether or not the classifcations were done correctly'''
        
        if node is None:
            node = self.tree
            
        results = []
        for i in range(len(data)):
            results.append(data[i][self.T_index] == classifications[i])

        return results
    
    def train(self):
        # training statistics
        self.training_classifications = self.classify_all(self.training_data)
        self.training_booleans = self.classification_results(self.training_data, self.training_classifications)
        self.training_per = sum(self.training_booleans)/len(self.training_booleans)
    
    def use(self,testing_data):
        
        self.testing_data = testing_data
        self.testing_classifications = self.classify_all(self.testing_data)
        self.testing_booleans = self.classification_results(self.testing_data, self.testing_classifications)
        self.testing_per = sum(self.testing_booleans)/len(self.testing_booleans)
    
    def print_stats(self):
        print(""\nTree Stats:"")
        print(str(round(100*self.training_per,2))   +""% of the training data was classified correctly"")
        if self.testing_data is not None:
            print(str(round(100*self.testing_per,2))+""% of the testing  data was classified correctly"")",0.6170179844,
1558,predictions and evaluation of decision tree,"class decision_tree:
    def __init__(self, data, labels, eita=5, pi=0.9, show_score=False):
        self.eita = eita
        self.pi = pi
        num_of_classes = len(set(labels))
        self.root = self.build_decision_tree(data, labels, num_of_classes, eita, pi, show_score)
    
    def classify(self, data_point):
        return self.recursive_classify(self.root, data_point)
    
    def recursive_classify(self, root, data_point):
        if isinstance(root, leaf_node):
            return root.label
        if root.classify(data_point):
            return self.recursive_classify(root.left, data_point)
        return self.recursive_classify(root.right, data_point)
        
    def build_decision_tree(self, data, labels, num_of_classes, eita, pi, show_score):
        assert len(data) == len(labels)
        n = len(data)

        class_data = list_to_dict(labels)
        for i in range(len(data)):
            label = labels[i]
            class_data[label].append(data[i])

        # Convert class-specific subsets to np matrix
        for label in class_data:
            temp = class_data[label]
            class_data[label] = np.array(temp).reshape(len(temp), len(temp[0]))

        # find class with max purity
        purity = 0
        purest_class = -1
        for label in class_data:
            if len(class_data[label]) > purity:
                purity = len(class_data[label])
                purest_class = label
        if n == 0: # shouldn't happen unless a split point didn't split anything
            return leaf_node(-1, 0)
        purity = purity / n

        # stopping condition / base case
        if eita >= n or purity > pi:
            leaf = leaf_node(purest_class, purity)
            return leaf

        split_attr, split_point = -1,-1
        best_score = -1

        # for each attribute
        for attr in range(len(data[0])):
            if type(data[0][attr]) is 'bool':
                continue
            else:
                split, score = evaluate_numeric_attr(data, labels, num_of_classes, attr)
                if show_score:
                    print (""attr: {}, score: {}"".format(attr, score))
                if score > best_score:
                    split_attr, split_point = attr, split
                    best_score = score

        data_y = data[data[:,split_attr] < split_point]
        data_n = data[data[:,split_attr] >= split_point]

        label_y = labels[data[:,split_attr] < split_point]
        label_n = labels[data[:,split_attr] >= split_point]

        int_node = internal_node(purity)
        int_node.set_splitter(split_attr, split_point)
        int_node.set_left_child(self.build_decision_tree(data_y, label_y, num_of_classes, eita, pi, show_score))
        int_node.set_right_child(self.build_decision_tree(data_n, label_n, num_of_classes, eita, pi, show_score))

        return int_node",0.6037424803,
1558,predictions and evaluation of decision tree,"class DecisionTree:
    def __init__(self, height, random = False, m = 25):
        self.height = height
        self.random = random
        self.m = m
        self.root = None
        
    def impurity(self, left_label_hist, right_label_hist):
        # weighted entropy    
        left_size = 0
        for key, val in left_label_hist.items():
            left_size+=val
        right_size = 0
        for key, val in right_label_hist.items():
            right_size+=val
        def calc_entropy(index_set):
            total = 0
            for key, val in index_set.items():
                total+=val
            if total == 0:
                # return big number
                return 9999999999
            probs = {}
            for key, val in index_set.items():
                probs[key] = val/total
            entropy_sum = 0
            for key in index_set:
                if probs[key] == 0:
                    # Case where all the samples go to one class
                    return 0
                entropy_sum += -probs[key] * np.log(probs[key])
            return entropy_sum
        left_entropy = calc_entropy(left_label_hist)
        right_entropy = calc_entropy(right_label_hist)
        weighted_entropy = (left_size * left_entropy + right_size * right_entropy)/(left_size + right_size)
        return weighted_entropy
    
    def segmenter(self, splits, data, labels):
        # for each feature
        feature_entropies = []
        best_rules = []
        rand_features = []
        if self.random:
            index = 0
            feature_map = {}
            while len(rand_features) < self.m:
                i = np.random.randint(len(data.T))
                if not i in rand_features:
                    rand_features.append(i)
                    feature_map[index] = i
                    index+=1
        else:
            feature_map = {i:i for i in range(len(data.T))}
            rand_features = range(len(data.T))
        for feature in rand_features:
            curr_list = []
            counts_dict_0 = {}
            counts_dict_1 = {}
            for i in splits:
                feature_val = data[i][feature]
                label = labels[i]
                curr_list.append(feature_val)
                if label == 0:
                    if feature_val in counts_dict_0:
                        counts_dict_0[feature_val] += 1
                    else:
                        counts_dict_0[feature_val] = 1
                    if not feature_val in counts_dict_1:
                        counts_dict_1[feature_val] = 0
                else:
                    if feature_val in counts_dict_1:
                        counts_dict_1[feature_val] += 1
                    else:
                        counts_dict_1[feature_val] = 1
                    if not feature_val in counts_dict_0:
                        counts_dict_0[feature_val] = 0
            unique_list = list(set(curr_list))
            unique_list.sort()
            
            left_label_hist = {0:0, 1:0}
            right_label_hist = {0:0, 1:0}
            left_label_hist[0]+= counts_dict_0[unique_list[0]]
            left_label_hist[1]+= counts_dict_1[unique_list[0]]
            for i in range(1, len(unique_list)):
                right_label_hist[0]+= counts_dict_0[unique_list[i]]
                right_label_hist[1]+= counts_dict_1[unique_list[i]]

            lowest_entropy = self.impurity(left_label_hist, right_label_hist)
            best_rule = unique_list[0]
            # Testing each split
            for i in range(1, len(unique_list)-1):
                left_label_hist[0]+= counts_dict_0[unique_list[i]]
                left_label_hist[1]+= counts_dict_1[unique_list[i]]
                right_label_hist[0]-= counts_dict_0[unique_list[i]]
                right_label_hist[1]-= counts_dict_1[unique_list[i]]
                curr_entropy = self.impurity(left_label_hist, right_label_hist)
                if curr_entropy < lowest_entropy:
                    lowest_entropy = curr_entropy
                    
                    best_rule = unique_list[i]
            feature_entropies.append(lowest_entropy)
            best_rules.append(best_rule)
            
        # MAKE SURE FEATURE MAPPING IS CORRECT
        best_index = np.argmin(feature_entropies)
        rule = best_rules[best_index]
        best_feature = feature_map[best_index]
        return (best_feature, rule)
    
    def train(self, train_data, train_labels):
        S = []
        for i in range(train_data.shape[0]):
            S.append(i)
        def grow_tree(S, train_data, train_labels, height):

            def get_majority(labels):
                count0 = 0
                count1 = 0
                for i in range(len(new_labels)):
                    if new_labels[i][0] == 0:
                        count0+=1
                    else:
                        count1+=1
                if count0 > count1:
                    return 0
                return 1

            same = True
            val = train_labels[S[0]]
            new_labels = train_labels[S]
            for i in S:
                if val != train_labels[i]:                    
                    same = False
                    break
            if height == 0:
                return Node(None, None, None, get_majority(new_labels))
            if same:
                return Node(None, None, None, int(val[0]))
            else:
                feature, split = self.segmenter(S, train_data, train_labels)
                S_l = []
                S_r = []
                for i in S:
                    if train_data[i][feature] <= split:
                        S_l.append(i)
                    else:
                        S_r.append(i)
                
                if len(S_l) == 0 or len(S_r) == 0:
                    return Node(None, None, None, get_majority(new_labels))
                return Node((feature, split), grow_tree(S_l, train_data, train_labels, height-1), grow_tree(S_r, train_data, train_labels, height-1))
        self.root = grow_tree(S, train_data, train_labels, self.height)
    
    
    def predict(self, test_data):
        predicted_labels = np.zeros((len(test_data), 1))
        for i in range(len(test_data)):
            curr_node = self.root
            while curr_node.label==None:
                feature, rule = curr_node.split_rule
                if test_data[i][feature] <= rule:
                    curr_node = curr_node.left
                else:
                    curr_node = curr_node.right
            predicted_labels[i] = curr_node.label
        return predicted_labels

class Node:
    def __init__(self, split_rule, left, right, label = None):
        self.split_rule = split_rule
        self.left = left
        self.right = right
        self.label = label 
    
    def __str__(self):
        return ""[Split rule: "" + str(self.split_rule) + "" "" + str(self.label) + "" ]""
    
def benchmark(pred_labels, true_labels):
    errors = pred_labels != true_labels
    err_rate = sum(errors) / float(len(true_labels))
    indices = errors.nonzero()
    return err_rate, indices",0.5990525484,
1558,predictions and evaluation of decision tree,"train_scores, test_scores = validation_curve(tree.DecisionTreeClassifier(criterion=""entropy""), features.values,
                                            labels.values, param_name = ""max_depth"", param_range  = np.linspace(1, 50, num = 50),
                                            scoring = ""accuracy"", n_jobs = -1, cv = 5)
# get the validation results for the decision tree across various depths",0.5947524309,
1558,predictions and evaluation of decision tree,"# For every artist return a list of labels
def predict_cluster(test_data, test_np_matrix, kmeans_model):
    
    predicted_labels = kmeans_model.predict(test_np_matrix)
    
    output_data = {}
    
    for index, artist_id in test_data['ArtistID'].iteritems():
        output_data[artist_id] = predicted_labels[index]
    
    return output_data


# Call the function for every model from before
kmeans_5_predicted = predict_cluster(user_art_mat_test, user_np_matrix_test, kmeans_5)
kmeans_25_predicted = predict_cluster(user_art_mat_test, user_np_matrix_test, kmeans_25)
kmeans_50_predicted = predict_cluster(user_art_mat_test, user_np_matrix_test, kmeans_50)",0.5943951011,
1558,predictions and evaluation of decision tree,"# draw the roc curves.

fprs, tprs = [], []

for clf in clfs:
    y_score = clf.decision_function(X_test)
    fpr, tpr, _ = metrics.roc_curve(y_test, y_score, pos_label=1)
    fprs.append(fpr)
    tprs.append(tpr)
    
plot_roc_curves(fprs, tprs)",0.5943185687,
1558,predictions and evaluation of decision tree,"def benchmark_model(model, X_train, y_train, X_test, y_test):
    model.fit = timing(model.fit)
    model.predict_proba = timing(model.predict_proba)
    
    #fit
    model.fit(X_train, y_train)
    
    #prediction
    y_pred_probs = model.predict_proba(X_test)
    y_pred = np.argmax(y_pred_probs, axis=1)
    
    #scores
    roc = roc_auc_score(y_test, y_pred_probs[:, 1])
    prec = precision_score(y_test, y_pred)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    
    print(""ROC-AUC: {}\nPrecision: {}\nRecall: {}\nF1: {}\n"".format(roc, prec, rec, f1))
    
    print(""Confusion matrix\nRows  true labels\nColumns - predicted labels"")
    conf_matrix = confusion_matrix(y_test, y_pred)
    cm_df = pd.DataFrame(conf_matrix)
    display(cm_df)",0.5928148031,
1558,predictions and evaluation of decision tree,"rf = sess.decisionTree.forestTrain(
  table={
    ""name"":""hr_part"",
    ""where"":""strip(put(_partind_, best.))='1'""
  },
  inputs=all_inputs,
  nominals=class_vars,
  target=""left"",
  nTree=20,
  nBins=20,
  leafSize=5,
  maxLevel=21,
  crit=""GAIN"",
  varImp=True,
  missing=""USEINSEARCH"",
  vote=""PROB"",
  OOB=True,
  casOut={""name"":""forest_model"", ""replace"":True}
)

# Output model statistics
render_html(rf)

# Score 
sess.decisionTree.forestScore(
  table={""name"":""hr_part""},
  modelTable={""name"":""forest_model""},
  casOut={""name"":""_scored_rf"", ""replace"":True},
  copyVars={""left"", ""_partind_""},
  vote=""PROB""
)

# Create p_b_tgt0 and p_b_tgt1 as _rf_predp_ is the probability of event in _rf_predname_
sess.dataStep.runCode(
  code=""""""data _scored_rf; set _scored_rf; if _rf_predname_=1 then do; p_left1=_rf_predp_; 
    p_left0=1-p_left1; end; if _rf_predname_=0 then do; p_left0=_rf_predp_; p_left1=1-p_left0; end; run;""""""
)",0.5911251307,
1558,predictions and evaluation of decision tree,"def f1_precision_recall(clf, le, test):
    y_pred = clf.predict(unlabel(test))
    y_true = le.transform(get_labels(test))
    stats_per_label = {}
    for label in set(y_true):
        # Fucking NumPy, so good...
        tp = ((y_pred == label) & (y_true == label)).sum()
        tn = ((y_pred != label) & (y_true != label)).sum()
        fp = ((y_pred == label) & (y_true != label)).sum()
        fn = ((y_pred != label) & (y_true == label)).sum()
        precision = tp / (tp + fp)
        recall = tp / (tp + fn)
        f1_score = 2 * precision * recall / (precision + recall)
        stats_per_label[label] = {'Precision': precision, 'Recall': recall, 'F1 Score': f1_score}
    df = pd.DataFrame(stats_per_label).sort_values('F1 Score', axis=1).T
    df.index = le.classes_
    return df[['Precision', 'Recall','F1 Score']]


def confusion_matrix(clf, le, test):
    y_pred = clf.predict(unlabel(test))
    y_true = le.transform(get_labels(test))

    labels = np.array(list(sorted(set(y_true) | set(y_pred))))

    sample_weight = np.ones(y_true.shape[0], dtype=np.int)

    n_labels = labels.size
    label_to_ind = dict((y, x) for x, y in enumerate(labels))
    y_pred = np.array([label_to_ind.get(x, n_labels + 1) for x in y_pred])
    y_true = np.array([label_to_ind.get(x, n_labels + 1) for x in y_true])
    ind = np.logical_and(y_pred < n_labels, y_true < n_labels)
    y_pred = y_pred[ind]
    y_true = y_true[ind]
    sample_weight = sample_weight[ind]

    CM = coo_matrix((sample_weight, (y_true, y_pred)), shape=(n_labels, n_labels)).toarray()

    return CM, labels


def plot_confusion_matrix(cm, classes,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment=""center"",
                 color=""white"" if cm[i, j] > thresh else ""black"")
    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')",0.5882096887,
1558,predictions and evaluation of decision tree,"finalvote = VotingClassifier(estimators=[('knn', pipe_final_knn), 
                            ('lr', pipe_final_lr), ('gp', pipe_final_gp), ('tree', pipe_final_tree),
                            ('forest', pipe_final_forest), ('gb', pipe_final_gb), ('ada', pipe_final_ada)],
                            voting = 'soft', weights=[1,1,3,1,2,3,1])
finalvote.fit(X_train, y_train)
print('Training score: {}'.format(score_ensemble(finalvote, X_train, y_train)))
print('Cross-validation score: {}'.format(score_ensemble(finalvote, X_cv, y_cv)))",0.5879349709,
1514,plot the data,"sv_fig = plt.figure()
def fit_changed(bunch):
    sv_fig.clear()
    fits[bunch['new']].plot(fig=sv_fig,
                            data_kws=dict(color='black', marker='o', markersize=1, markerfacecolor='none'),
                            fit_kws=dict(color='red', linewidth=4))

# Widget for the spectrum index
sv_label = widgets.Label('spectrum index', layout=widgets.Layout(width='10%'))
sv_int_text = widgets.BoundedIntText(value=0, min=0, max=(n_spectra - 1),
                                     layout=widgets.Layout(width='20%'))
sv_int_text.observe(fit_changed, 'value')
sv_hbox_l = [widgets.HBox([sv_label, sv_int_text])]

sv_vertical_layout = widgets.VBox(sv_hbox_l)
display(sv_vertical_layout)",0.6732477546,
1514,plot the data,"def plot_raw_data(age):
    lt.plot_data(age)
    plt.xlabel('Year', fontsize=12)
    plt.ylabel('Survivors per 100,000', fontsize=12)
    plt.show()

interact(plot_raw_data,age=(1,110))",0.6549853086,
1514,plot the data,"def plot_data(time_ind):
    """"""
    Given a time index, plot_data will plot the skytem data across 
    the survey area at that time
    """"""
    fig, ax = plt.subplots(1,1, figsize = (8,8))
    
    # grid the data
#     nskip = 
    out = Utils.plot2Ddata(np.vstack([easting, northing]).T, data[:, time_ind], ncontour=100, ax=ax)
    plt.colorbar(out[0], ax=ax, label=""db/dt"")
    
    # add the river path 
    ax.plot(river_path[:, 0], river_path[:, 1], 'k', lw=0.5)
    
    # labels
    ax.set_xlabel('easting (m)')
    ax.set_ylabel('northing (m)')
    
    # get title    
    ax.set_title(f""time channel {time_channels[time_ind, 0]}, t = {time_channels[time_ind, 1]}s"")
    return ax",0.6547731161,
1514,plot the data,"def onclick(event):
    plt.cla()
    x = event.ydata
    graph(x, df, yerr)",0.6527084112,
1514,plot the data,"def plot_episode_results(plot_data):
    import matplotlib.pyplot as plt
    import seaborn as sns
    %matplotlib inline
    for n,(k,v) in enumerate(plot_data.items()):
        if k == 'episode':
            # don't plot episode against itself
            continue
            
        plt.figure(n)
            
        plt.plot(plot_data['episode'], plot_data[k], label=k.upper())
        plt.ylabel(k.upper())
        plt.xlabel(""Episodes Survived"")
        
        plt.legend()
        plt.show()
    
    
def display_permutation_info(permutation):
    print(""Permutation Info:"")
    print(""==============================="")
    print("""")
    for k,v in permutation.items():
        print(k.upper())
        print(""-----------------"")
        print(v)
        print("""")
    print("""")
    print("""")


def try_a_permutation(permutation, sess, agent=None, num_episodes=10):

    from agents.agent import DeepLearnAgent
    from task import Task, Target, Hover
    import tensorflow as tf
    
    class Perm(object):
        def __init__(self, permutation):
            self.__dict__ = permutation
    
    p = Perm(permutation)
    
    if agent is None:
        
        task = Hover(action_size=p.action_size, hover_height=10 + np.random.randint(-2, 2))
        agent = DeepLearnAgent(
            task, 
            verbose=False,
            **permutation,
            ) 
        
    agent.sess = sess

    labels = ['episode', 'avg_reward', 'best_reward', 'num_timesteps', 'avg_speed', 'avg_height', 'avg_height_error']
#     file_output = 'learning_data.txt'                         # file name for saved results

    plot_data = {x : [] for x in labels}

#     # Run the simulation, and save the results.
#     with open(file_output, 'w') as csvfile:
#         writer = csv.writer(csvfile)
#         writer.writerow(labels)


    for episode in range(num_episodes):
        
        task = Hover(action_size=p.action_size, hover_height=10)
        agent.task = task
        state = agent.reset_episode()
        try:
            results = agent.act_many_times(task, state, n=50)
        except ValueError as ex:
            if ""dead"" in str(ex):
                break
            else:
                raise

        for n in range(2):
            # print(""Trying to learn from {} experiences"".format(len(agent.memory)))
            agent.learn_from_experiences()

        if True: # try getting more Learning from each round, hopefully improve?
            agent.do_periodic_update(
                critic_update_rate=p.critic_update_rate, 
                actor_update_rate=p.actor_update_rate)
        row = dict(
            episode=episode,
            avg_reward=agent.total_reward/agent.count,
            best_reward=agent.best_reward, 
            num_timesteps=agent.count,
            avg_speed=agent.total_speed/agent.count,
            avg_height=agent.total_height/agent.count,
            avg_height_error=agent.total_height_error/agent.count)

        print(""Episode #{}: Avg reward: {}, Num_Timesteps {}, Avg_speed: {}, Avg_Height: {}, Avg_Height_Error: {}"".format(
            episode, 
            np.round(row[""avg_reward""],2),
            np.round(row[""num_timesteps""],2), 
            np.round(row[""avg_speed""],2), 
            np.round(row[""avg_height""],2), 
            np.round(row[""avg_height_error""],2)))
        if row['avg_speed'] == 0:
            break

        for k,v in plot_data.items():
            v.append([row[k]])
            
    print(""I Survived {} episodes"".format(episode))
    display_permutation_info(permutation)
    plot_episode_results(plot_data)
    return agent, plot_data, episode",0.6493194699,
1514,plot the data,"def plot_dataframe(dataframe):
    fig = plt.figure(figsize = (25,10))
    ax = fig.gca()
    pd.options.display.mpl_style = 'default'
    dataframe.boxplot(rot=45)
    
plot_dataframe(df[financial_features])",0.6485649347,
1514,plot the data,"def plot_multiple_boxplots():
    
    for key in mapping:
        if key[0] != 'Y':
            dataset[[mapping[key]]].plot.box(figsize = (5,5))
            
# plot_multiple_boxplots()",0.6471460462,
1514,plot the data,"def plot_wigner_2d_3d(psi):
    #fig, axes = plt.subplots(1, 2, subplot_kw={'projection': '3d'}, figsize=(12, 6))
    fig = plt.figure(figsize=(17, 8))
    
    ax = fig.add_subplot(1, 2, 1)
    plot_wigner(psi, fig=fig, ax=ax, alpha_max=6);

    ax = fig.add_subplot(1, 2, 2, projection='3d')
    plot_wigner(psi, fig=fig, ax=ax, projection='3d', alpha_max=6);
    
    plt.close(fig)
    return fig",0.6459696889,
1514,plot the data,"def plot_responses(responses):
    fig1, ax = plt.subplots(len(responses))
    ax.plot(responses['step1.soma.v']['time'], responses['step1.soma.v']['voltage'], label='step1.soma.v')
    ax.plot(steps_dict['ShortStepPos'][0][:, 0], steps_dict['ShortStepPos'][0][:, 1])
plot_responses(responses)",0.6455112696,
1514,plot the data,"def plot_POA(poa):
    # Plot all POAs in a single plot; the y-axis on the left is for ROW,
    # the one on the right is for all other segments.
    fig, ax1 = plt.subplots()
    ax2 = ax1.twinx()
    ax1.plot(poa[:, 0], color='k')
    ax2.plot(poa[:, 1:])

    ax1.set_xlabel('i_search_task')
    ax1.set_xlim([0, n_search_tasks])
    ax1.set_ylabel('ROW')
    ax1.set_ylim([0, 1])
    ax2.set_xlim([0, n_search_tasks])
    ax2.set_ylabel('segments')
    ax2.set_ylim([0, 1.05 * np.amax(p_Bayes[1:,1:])])

    plt.show()",0.6443936825,
1645,remove unnecessary columns point,"def add_binned_ratings(df, old_col, new_col):
    '''
    Add column for binned ratings.
    
    INPUT:
    - df (full dataframe)
    - old_col (str): column name of average ratings
    - new_col (str): new column name for binned average ratings
    OUTPUT:
    - new dataframe
    '''
    df[new_col] = pd.cut(df[old_col].copy(), bins=[0., 3.99, 4.99, 5],
                            include_lowest=True, right=True)
    df[new_col].cat.add_categories('Missing', inplace=True)
    df[new_col].fillna('Missing', inplace=True)                        
    return df

df = add_binned_ratings(df, 'avg_rating_by_driver', 'bin_avg_rating_by_driver')
df = add_binned_ratings(df, 'avg_rating_of_driver', 'bin_avg_rating_of_driver')",0.4970100522,
1645,remove unnecessary columns point,"def createBOW(df,token_column):
    dictionary = Dictionary(df.loc[:,token_column])
    dictionary.filter_extremes(no_below = 150, no_above = 0.6)
    df['bow'] = [dictionary.doc2bow(doc) for doc in df[token_column]]
    return df, dictionary",0.4964949489,
1645,remove unnecessary columns point,"def min_score_batsman_match(match_id):
    x = deliveries.groupby(['match_id', 'batsman'])['batsman_runs'].sum()
    name = x[match_id].idxmin()
    runs = x[match_id].min()
    return [name, runs]",0.4918045402,
1645,remove unnecessary columns point,"def add_binned_ratings(df, old_col, new_col):
    '''
    Add column for binned ratings.
    
    INPUT:
    - df (full dataframe)
    - old_col (str): column name of average ratings
    - new_col (str): new column name for binned average ratings
    OUTPUT:
    - new dataframe
    '''
    df[new_col] = pd.cut(df[old_col].copy(), bins=[0., 3.99, 4.99, 5],
                            include_lowest=True, right=True)
    df[new_col].cat.add_categories('Missing', inplace=True)
    df[new_col].fillna('Missing', inplace=True)                        
    return df

data = add_binned_ratings(data, 'avg_rating_by_driver', 'bin_avg_rating_by_driver')
data = add_binned_ratings(data, 'avg_rating_of_driver', 'bin_avg_rating_of_driver')",0.4906321168,
1645,remove unnecessary columns point,"def halo_effect(speed_df):
        
    speed_df['attr_q5'] = pd.qcut(speed_df['attr'],q=10,duplicates='drop')
    halo_tab = speed_df.groupby('attr_q5')['sinc','intel','fun','amb','shar'].agg('mean').T
    
    plt.figure(figsize=(10,6))
    sns.heatmap(halo_tab, cmap='RdBu', linewidths=1, annot=True)
    plt.title('Halo effect: how other attributes than attractiveness are\n scored for each attractiveness group', fontsize =14)
    plt.xlabel('Attractiveness score')
    return plt.show()",0.4905243814,
1645,remove unnecessary columns point,"def add_age_class(df):
    df['AgeClass'] = pd.cut(df['Age'], 8, labels=[1,2,3,4,5,6,7,8]).astype('int64')
    return df",0.4895730615,
1645,remove unnecessary columns point,"def nanrate(train, valname):
    ## train, the dataset name
    ## valname, the feature name to study
    #print(train[valname].describe())
    train[valname+""_na""] = pd.isnull(train[valname])
    book_rate=[]
    click_rate=[]
    c_summary=[]
    b_summary=[]
    cond = []
    for i, gb in train.groupby(valname+""_na""):
        if i:
            cond.append(1)
            print('non_NaN value click summary:')
            print(gb[""click_bool""].describe())
            print('non_NaN value booking summary:')
            print(gb[""booking_bool""].describe())
        else:
            cond.append(0)
            print('NaN value click summary:')
            print(gb[""click_bool""].describe())
            print('NaN value booking summary:')
            print(gb[""booking_bool""].describe())
        book_rate.append(gb[""booking_bool""].mean())
        click_rate.append(gb[""click_bool""].mean())
        c_summary.append(gb[""click_bool""].describe())
        b_summary.append(gb[""booking_bool""].describe())
    df = pd.DataFrame(np.array([cond, click_rate, book_rate]), index=[""Condition"",""Click Rate"", ""Book Rate""])
    df = df.transpose()
    #print(df)
    df.plot( x=""Condition"",kind=""bar"")
    locs, labels = plt.xticks()
    plt.xticks(locs, [""Not NULL"", ""NULL""], size='small', rotation='horizontal')
    plt.title(""NaN rate wrt: ""+valname+"" feature."")
    plt.show()",0.4895567894,
1645,remove unnecessary columns point,"def remove_nan_rows(dataframe):
    for column in dataframe.columns:
        if dataframe[column].isnull().sum() > 10000:
            del dataframe[column]
    return dataframe",0.4850489795,
1645,remove unnecessary columns point,"def fit_orbit(df_obs):
    df_obs = df_obs.ix[['#' not in row['date'] for ind, row in df_obs.iterrows()]]   # filter comment lines
    nobs = len(df_obs)
    ralist = [ephem.hours(r) for r in df_obs['ra'].values]
    declist = [ephem.degrees(d) for d in df_obs['dec'].values]
    datelist = [ephem.date(d) for d in df_obs['date'].values]
    orbit = Orbit(dates=datelist, ra=ralist, dec=declist, obscode=np.ones(nobs, dtype=int)*807, err=0.15)
    return orbit",0.4848427773,
1645,remove unnecessary columns point,"def preprocess(df):

    # We'll set PARID (Parcel Identifier) to the index
    df = df.set_index(['PARID'])   
    
    # Remove so called ""skeleton"" listings. These have a USECODE of 001 & 002.
    df = df[df['USECODE'] > 2]

    # We're going to eliminate known outliers or special cases so we can focus on predicting fairmarket value 
    # for residential properties
    df = df[df['SALEPRICE'] > 1000] # Play around with this number. You'll notice the perceptron struggles under a $100k
    df = df[df['CARDNUMBER'] == 1] # Some parcels have more than one building, we'll restrict this to single-building properties
    df = df[df['SALEDESC'] == 'VALID SALE'] # We only want records that county think were at fair market value.
    df = df[df['TAXCODE'] == 'T']
    df = df[df['OWNERCODE'] == 10] #Regular, residental owners only
    
    # We'll also remove any special use properties
    df = df[pd.isnull(df['TAXSUBCODE'])]
    df = df[pd.isnull(df['ABATEMENTFLAG'])]
    df = df[pd.isnull(df['FARMSTEADFLAG'])]
    df = df[pd.isnull(df['HOMESTEADFLAG'])]
    df = df[pd.isnull(df['CLEANGREEN'])]

    # Remove the now homogeneous columns
    extracol = ['OWNERCODE', 'OWNERDESC','CARDNUMBER', 'SALEDESC','TAXCODE','TAXSUBCODE','USECODE', 'ABATEMENTFLAG', 'FARMSTEADFLAG', 'HOMESTEADFLAG', 'CLEANGREEN']
    df = df.drop(extracol, axis=1)

    # Correct the format of specific rows:
    df['SALEDATE'] = pd.DatetimeIndex(df['SALEDATE'], errors='coerce') #convert to datetime format
    df['LOTAREA'] = df['LOTAREA'].astype(float)
    df['PROPERTYZIP'] = df['PROPERTYZIP'].astype(str)
    df['PROPERTYHOUSENUM'] = df['PROPERTYHOUSENUM'].astype(int)
    
    #Fill in missing values
    df['NEIGHDESC'] = df['NEIGHDESC'].fillna("""")
    df['PROPERTYFRACTION'] = df['PROPERTYFRACTION'].fillna("""")
    df['ROOFDESC'] = df['ROOFDESC'].fillna("""")
    
    # Remove ""skeleton"" listings that aren't properly labelled
    df = df.dropna(subset=['BASEMENT','BEDROOMS','BSMTGARAGE','CONDITION','STORIES','TOTALROOMS','YEARBLT'],axis=0)

    # Drop the columns that are redundant, we have no interest in, that would lead our algorithm astray, 
    # or are just filler
    extracol = ['STYLE','NEIGHCODE','SALECODE','_id','CLASS','TAXDESC','MUNICODE','SCHOOLCODE','LEGAL1','LEGAL2','LEGAL3','ALT_ID','ASOFDATE','RECORDDATE','DEEDBOOK','DEEDPAGE','TAXSUBCODE_DESC','LOCALTOTAL','FAIRMARKETBUILDING','FAIRMARKETLAND','FAIRMARKETTOTAL','EXTERIORFINISH','ROOF','BASEMENTDESC','GRADEDESC','CONDITIONDESC','CDUDESC','PREVSALEDATE','PREVSALEPRICE','PREVSALEDATE2','PREVSALEPRICE2','CHANGENOTICEADDRESS1','CHANGENOTICEADDRESS2','CHANGENOTICEADDRESS3','CHANGENOTICEADDRESS4','COUNTYBUILDING','COUNTYLAND','COUNTYTOTAL','COUNTYEXEMPTBLDG','LOCALBUILDING','LOCALLAND','CLASSDESC','HEATINGCOOLING','TAXYEAR']
    df = df.drop(extracol, axis=1)
    
    #Finally, we're going to cycle through and ensure that we only have no ""NaN"" in our numeric features
    for col in df.columns:
        if (df[col].dtypes == float) | (df[col].dtypes == int):
            df = df[pd.notnull(df[col])]
    
    return df

prop_assess = preprocess(results)",0.4831463993,
112,biasing and unbiasing probability nass functions (pmfs),"def naive_bayes_classifier(train_x, train_y):  
    from sklearn.naive_bayes import MultinomialNB  
    model = MultinomialNB(alpha=0.01)  
    model.fit(train_x, train_y)  
    return model",0.547936976,
112,biasing and unbiasing probability nass functions (pmfs),"def predict_proba(states):
    """""" 
    Predict action probabilities given states.
    :param states: numpy array of shape [batch, state_shape]
    :returns: numpy array of shape [batch, n_actions]
    """"""
    # convert states, compute logits, use softmax to get probability
    <your code here>
    return < your code >",0.5354670286,
112,biasing and unbiasing probability nass functions (pmfs),"def log_likelihood(n, lam):
    assert n >= 1
    return scipy.stats.poisson.logpmf(n, lam) - np.log(1 - np.exp(-lam))",0.5294279456,
112,biasing and unbiasing probability nass functions (pmfs),"def trainRawNB(X_train, y_train):
    from sklearn.naive_bayes import BernoulliNB
    clf = BernoulliNB(alpha=1.0e-10)
    clf.fit(X_train, y_train)
    return(clf)",0.5285922289,
112,biasing and unbiasing probability nass functions (pmfs),"def prob_below(y, n, k):
    return 1 - scipy.stats.binom.cdf(k=2*k-y*n-1, n=n, p=k/n)
[(p,prob_below(p,10,9)) for p in np.linspace(0,1,11)]",0.5277969241,
112,biasing and unbiasing probability nass functions (pmfs),"def print_material():
    rho = rundata.probdata.rho
    K = rundata.probdata.K
    Z = sqrt(K*rho)
    c = sqrt(K/rho)
    print(""The density rho = %g and bulk modulus %g give"" % (rho,K))
    print(""  speed of sound c = %g"" % c)
    print(""  impedance Z = %g"" % Z)
print_material()",0.526263237,
112,biasing and unbiasing probability nass functions (pmfs),"sigma_p = getting_sigma_bs_theory( cosmo2, b = bbin, cov = covp_model, datavs = datav_p, 
                                     mockdatavs = p_mock, p=True, kmin=kmin, kmax=kmax )
sigma_x = getting_sigma_bs_theory( cosmo2, b = bbin, cov = covxi_model, datavs = datav_xi, 
                                     mockdatavs = xi_mock, rmin = rmin, rmax=rmax )
sigma_d = getting_sigma_bs_diff_theory( cosmo2, b = bbin, covp = covp_model, covxi = covxi_model, 
                          datavsp = datav_p, datavsxi = datav_xi, 
                          mockdatavsp = p_mock, mockdatavsxi = xi_mock, kmin=kmin, kmax=kmax, rmin=rmin, rmax=rmax )
sigma_c = getting_sigma_bs_com_theory( cosmo2, b = bbin, cov = covtot_model, 
                                     datavs = datav_com, mockdatavs = mocks,
                                    kmin = kmin, kmax = kmax, rmin=rmin, rmax=rmax)

# print out results-------------------------------
print '---------------------------------\
\n Error on bias (Theory) \n sigma_p      : {} \n sigma_xi     : {} \n sigma_combin : {} \n sigma_diff   : {} \
\n---------------------------------'.format(sigma_p, sigma_x, sigma_c, sigma_d)",0.5245010853,
112,biasing and unbiasing probability nass functions (pmfs),"def randprob(deltas):
    return( np.random.beta(deltas[:,0], deltas[:,1]) )",0.5227169394,
112,biasing and unbiasing probability nass functions (pmfs),"def loss(x, y, A):
    return 0.5 * x.T.dot(A.T).dot(A).dot(x) -  y.T.dot(A).dot(x)",0.5222672224,
112,biasing and unbiasing probability nass functions (pmfs),"def tf_graph_fullyconnected_softmax():
    global h_state_outputs_flat, y_preds_logit, y_preds_prob
    
    # flatting h_state_outputs
    h_state_outputs_flat = tf.reshape(h_state_outputs, [-1, hStateSize])

    y_preds_logit = layers.fully_connected(
        h_state_outputs_flat,
        alphaSize,
        activation_fn=tf.nn.relu # the default
        # activation_fn=tf.nn.softmax => WARNING: This op expects unscaled logits,
        # since it performs a softmax on logits internally for efficiency.
        # https://www.tensorflow.org/versions/master/api_docs/python/tf/nn/softmax_cross_entropy_with_logits_v2
    )

    y_preds_prob = tf.nn.softmax(y_preds_logit, name=""y_preds_prob"")  # (BS*MSL, AS)",0.5188529491,
2418,transforming the data,"def wrap_parameters(parameters, scaler=None):
    if scaler:
        parameters = scaler.transform(parameters)
    return parameters


def unwrap_parameters(parameters, scaler=None):
    if scaler:
        parameters = scaler.inverse_transform(parameters)
    p = [int(parameters[0]), parameters[1], int(parameters[2]),
         max(0, min(parameters[3], 1))]
    return p


def model_error_cv(parameters, X, y, scaler=None):
    errors = []
    for p in parameters:
        p = unwrap_parameters(p, scaler)
        model = xgboost.XGBRegressor(max_depth=p[0],
                                     learning_rate=p[1],
                                     n_estimators=p[2],
                                     subsample=p[3],
                                     )

        score = cross_val_score(model, X, y, cv=3).mean()
        errors.append(score)
    return np.array(errors).reshape(-1, 1)",0.4281536937,
2418,transforming the data,"X_test = numpy.array(map(le1.transform, X_test))
Y_test = le2.transform(Y_test)",0.4212359786,
2418,transforming the data,"tag_vector_map = [(tag, model.transform(tag)) for tag in tag_vector_list]
tag_vector_map[0]",0.4193072915,
2418,transforming the data,"# Make reflections 

def change_sign_x(m):
    return np.array([[-a, b] for [a,b] in m])

def change_sign_y(m):
    return np.array([[a, -b] for [a,b] in m])",0.4192985892,
2418,transforming the data,"project = partial(pyproj.transform, pr1, pr2)
project",0.4185678959,
2418,transforming the data,"def ratio_tosses(list_of_tuples):  
     
    return [(x / (x + y)) for x, y in list_of_tuples]",0.417743057,
2418,transforming the data,"# To complete
def applyNormalization(data, mean, std):
    """"""
        Given a mean and std, normalize the data
    """"""
    normalizedData = 
    return normalizedData

def normalize(data):
    """"""
        Normalize the given data
    """"""
    mean =
    std = 
    normalizedData = applyNormalization(data, mean, std)
    return normalizedData, mean, std",0.4174858928,
2418,transforming the data,"PATH = {0:path/comp/TEST_DATA_FILE, 1:path_to_test_data_file, 2:alt_path_to_test_data_file}",0.4152703881,
2418,transforming the data,"def update(pmf):
    post = pmf.Copy()
    post[p1] *= (a + c) / q
    post[p2] *= (e + g) / (1-q)
    post.Normalize()
    return post",0.4147239327,
2418,transforming the data,"# Reproject to LatLng
import pyproj
from functools import partial
from shapely.ops import transform

def reproject_geom(g, proj1, proj2):
    """"""
    Reprojects a shapely geometry
    """"""

    project = partial(
        pyproj.transform,
        proj1, # source coordinate system
        proj2) # destination coordinate system

    return transform(project, g)  # apply projection",0.4136688113,
1365,parsing lambda phage fastq,"!cd $workDir; \
    seqDB_tools accession-GI2fasta < M.barkeri_refseq.txt > M.barkeri.fna",0.5245847702,
1365,parsing lambda phage fastq,"!cd $workDir; \
    seqDB_tools accession-GI2fasta < M.extorquens_AM1_refseq.txt > M.extorquens_AM1.fna",0.5113703012,
1365,parsing lambda phage fastq,zz=skin.filter_fasta('./ag-skin-saliva-cluster.fa'),0.5101422071,
1365,parsing lambda phage fastq,skinbact=skin.filter_fasta('./seqs-skin-skin.fa'),0.5101422071,
1365,parsing lambda phage fastq,st=skin.filter_fasta('./saliva-seqs-new.fa'),0.5101422071,
1365,parsing lambda phage fastq,"# Parse the stderr files containing the number of read aligned with kallisto
sname_list=[]
tae=[] # total all RNA ERCC
mae=[] # mapped all RNA ERCC
tle=[] # total lncRNA ERCC
mle=[] # mapped lncRNA ERCC
ma=[] # mapped all RNA no ERCC
ml=[] # mapped lncRNA no ERCC
flen_list=[]

# results obtained with basic alignment using Kallisto quant
for file in glob(""./Localisation_Original_Datasets/Djebali-ENCODE/kallisto/allRNA_ERCC/*/stderr.txt""):
    sname_list.append(file.split(""/"")[-2])   
    lines = !cat {file}
    flen_list.append(float(lines[12].split()[-1].replace("","","""")))
    tae.append(int(lines[11].split()[2].replace("","","""")))
    mae.append(int(lines[11].split()[4].replace("","","""")))

# results obtained with stranded alignment using Kallisto quant
for file in glob(""./Localisation_Original_Datasets/Djebali-ENCODE/kallisto/lncRNA_ERCC/*/stderr.txt""):
    lines = !cat {file}
    tle.append(int(lines[11].split()[2].replace("","","""")))
    mle.append(int(lines[11].split()[4].replace("","","""")))

# results obtained with stranded alignment using Kallisto quant
for file in glob(""./Localisation_Original_Datasets/Djebali-ENCODE/kallisto/allRNA/*/stderr.txt""):
    lines = !cat {file}
    ma.append(int(lines[11].split()[4].replace("","","""")))
    
# results obtained with stranded alignment using Kallisto quant
for file in glob(""./Localisation_Original_Datasets/Djebali-ENCODE/kallisto/lncRNA/*/stderr.txt""):
    lines = !cat {file}
    ml.append(int(lines[11].split()[4].replace("","","""")))",0.5093665123,
1365,parsing lambda phage fastq,"MISC.fasta_maka(set(EcoRI_marker_names_all),""/home/djeffrie/Data/RADseq/R_temp_fams/EcoRI_map/Stacks/batch_1.catalog.tags.tsv.gz"" )",0.5078958273,
1365,parsing lambda phage fastq,"TAIR10 = 'reference/TAIR10-masked.fa'

chromosomes  = dict()  # chromosome sequences
chrSize      = dict()  # chromosome sizes, for convenience
maskedRanges = dict()  # masked ranges in each chromosome

for chromosome in SeqIO.parse(TAIR10, 'fasta', generic_dna) :
    name, seq  = chromosome.name, chromosome.seq
    
    # ignore chloroplast and mitochondria
    if name == 'chloroplast' or name == 'mitochondria' : continue
    
  #  chromosomes[name]  = seq  # we don't really need the actual sequence beyond this loop, save some memory
    chrSize[name]      = len(seq) 
    maskedRanges[name] = get_masked_ranges(seq)
    
    # get total size of masked regions
    maskedSize = sum(map(lambda x : x[1]-x[0], maskedRanges[name]))
    
    print '{} is of length {}, covered {:.1f}% by masked regions'.format\
    (name, chrSize[name], maskedSize*100.0/chrSize[name])",0.5076642036,
1365,parsing lambda phage fastq,"with open('test/test.fasta') as file:
    entries = misctools.iterfasta(file)
    first_entry = next(entries)
    next_ten_entries = [next(entries) for i in range(10)]
    rest_of_entries = list(entries)
    
first_entry",0.5075904727,
1365,parsing lambda phage fastq,"## Make a fasta file of the tag sequences

MISC.fasta_maka(whitelist, ""/home/djeffrie/Data/RADseq/R_temp_fams/batch_1.catalog.tags.tsv.gz"")",0.5066044927,
1026,lab intro to visualizations,"def loadTDMSImages(file):
    global dimx, dimy, binning, frames, exposure
    tdms_file = TdmsFile(file)
    p = tdms_file.object().properties # get the properties
    dimx = int(p['dimx'])
    dimy = int(p['dimy'])
    binning = int(p['binning'])
    frames = int(p['dimz'])
    exposure = float(p['exposure'])
    images = tdms_file.channel_data('Image','Image')
    return images.reshape(frames,dimx,dimy)",0.500636816,
1026,lab intro to visualizations,"class FunctionAnimatedImage():
    
    def func(self):
        return np.sin(self.x) + np.cos(self.y)
    
    def __init__(self):
        self.x = np.linspace(0, 2 * np.pi, 120)
        self.y = np.linspace(0, 2 * np.pi, 120).reshape(-1, 1)

        self.im = plt.imshow(self.func(), animated=True)
        
    def next_frame(self, i, *args):
        
        self.x += np.pi / 5.
        self.y += np.pi / 20.
        self.im.set_array(self.func())
        return self.im,

fig = plt.figure()
anim_img = FunctionAnimatedImage()",0.4755024314,
1026,lab intro to visualizations,"# Instantiate
sciI = scienceimage.ScienceImage(file_list=sci_image_files, datasec_img=datasec_img,
                                 bpm=msbpm, det=det, setup=setup, settings=sci_settings,
                                 maskslits=maskslits, pixlocn=pixlocn, tslits_dict=tslits_dict,
                                 tilts=mstilts, fitstbl=fitstbl, scidx=scidx)",0.4753430486,
1026,lab intro to visualizations,"def getTextCandidates(image):
    #binarize image
    gray, thresh = binarize(denoise(img))
    #get contours od the binary image
    contours, hierarchy = cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_TC89_KCOS)
    #sort contours by size - largest to smallest
    contours = sorted(contours, key=cv2.contourArea,reverse=True)
    #Calculate bounding boxes for contours
    bounding_boxes = []
    for c in contours:
        (x, y, w, h) = cv2.boundingRect(c)
        bounding_boxes.append([x, y, w, h])
    #Return bounding boxes as a list of candidates
    return bounding_boxes",0.4751829207,
1026,lab intro to visualizations,"datasets = ['ld9m10ujq', 'ld9m10uyq']
visit = hst_observation.Visit(datasets, instrument='cos', data_folder='../data/')
# There is also the 'stis' option for instrument",0.4747686386,
1026,lab intro to visualizations,"# Load the data
def load():
    mat = scipy.io.loadmat('myData.mat')
    XTrain = mat.get(""XTrain"")
    yTrain = mat.get(""yTrain"")

    XTest = mat.get(""XTest"")
    yTest = mat.get(""yTest"")
    return (XTrain, yTrain, XTest, yTest)",0.4726957679,
1026,lab intro to visualizations,"# Realize the galaxy by itself
galaxy_alone = helper.realize_all_visits_single_object(target_object_id=sample_id, is_galaxy=True, 
                                                       fov=fov, deblending_scale=deblending_scale,
                                                       galaxies_df=galaxies, points_df=point_neighbors, obs_history=obs_history)
galaxy_alone['ccdVisitId'] = galaxy_alone['ccdVisitId'].astype(int)
galaxy_alone = pd.merge(galaxy_alone, obs_history_filter, on='ccdVisitId', how='left')

# Realize the star by itself
star_alone = helper.realize_all_visits_single_object(target_object_id=sample_id, is_galaxy=False, 
                                              fov=fov, deblending_scale=deblending_scale,
                                              galaxies_df=galaxies, points_df=point_neighbors, obs_history=obs_history)
star_alone['ccdVisitId'] = star_alone['ccdVisitId'].astype(int)
star_alone = pd.merge(star_alone, obs_history_filter, on='ccdVisitId', how='left')",0.4707394838,
1026,lab intro to visualizations,"def led_brightness(intensity):
    leds.setIntensity(""RightFaceLedsRed"", intensity)
    leds.setIntensity(""LeftFaceLedsRed"", intensity)",0.4691502452,
1026,lab intro to visualizations,"def Lines_Pipeline(PassedImage):
    
    cal_coefficients = pickle.load(open(""calibration_coefficients.p"", 'rb'))
    
    # source needed for the presepective transform
    src = np.float32([(575,460), (720,460), (100,720), (1200,720)])

    # destination needed for the prespective transform
    dest = np.float32([(250,0), 
                       (1050,0), 
                       (250,700), 
                       (1050,700)
                      ])
    
    # undistort the selected image.
    undist_image = undistort_image(PassedImage, cal_coefficients[""mtx""], cal_coefficients[""dist""])

    # Apply different thresholds in order to detect the line perfectly.
    combined_image = combined_thresholds(undist_image)
    
    # warp the selected image.
    warped_image, Matx, Matx_inv = warper(combined_image, src, dest)

    # Find Lines
    image_lines, LeftFitGlb, RightFitGlb, LeftX, LeftY, RightX, RightY, LeftFitX, RightFitX = Visualize_Lines(warped_image)
    
    # Get lines Radius
    Radius = Get_Curvature(LeftX, LeftY, RightX, RightY)
    
    # Draw lines on the original image
    output_image = draw_lines (PassedImage, warped_image, LeftFitX, RightFitX, Matx_inv_list[index])
    
    Final_image = plot_data (output_image, Radius, LeftFitX, RightFitX)
    
    return Final_image",0.4685003161,
1026,lab intro to visualizations,"def perspective_adjust():
    fname = '../test_images/straight_lines2.jpg'
    img = cv2.imread(fname)

    src = np.float32((
        [570,460],
        [710,460],
        [1110,700],
        [220,700]
        ))
    img_w = img.shape[1]
    img_h = img.shape[0]
    img_size = (img_w,img_h)
    offset = 250
    dst = np.float32((
        [offset,0],
        [img_w-offset, 0],
        [img_w-offset,img_h],
        [offset,img_h]
        ))
    M = cv2.getPerspectiveTransform(src, dst)
    Minv = cv2.getPerspectiveTransform(dst, src)

    M_values = {
        ""M"" : M,
        ""Minv"" : Minv
    }

    pickle.dump(M_values, open(""perspective_values.p"",""wb""))
    
    print('Perspective Matrices Computed')",0.4683337212,
496,doing the fit,"def plot_image(some_image):
    
    some_digit_image = some_image.values.reshape(28,28)

    plt.imshow(some_digit_image, cmap = matplotlib.cm.binary, interpolation = ""nearest"")
    plt.axis(""off"")
    plt.show()",0.4617125094,
496,doing the fit,"# Define a class to receive the characteristics of each line detection
class Line():
    def __init__(self):
        # x values of the last n fits of the line
        self.recent_xfitted = [] 
        #average x values of the fitted line over the last n iterations
        self.bestx = None     
        #polynomial coefficients for the previous fit
        self.previous_fit = np.array([0,0,0], dtype='float') 
        #polynomial coefficients for the most recent fit
        self.current_fit = np.array([0,0,0], dtype='float') 
        #radius of curvature of the line in some units
        self.radius_of_curvature = None 
        #x values for detected line pixels
        self.allx = None  
        #y values for detected line pixels
        self.ally = None",0.4598815441,
496,doing the fit,"def cleanup_boolean_features(df):
    df.public_meeting = df.public_meeting.fillna(df.public_meeting.median()).astype(int)
    df.permit = df.permit.fillna(df.permit.median()).astype(int)
    return df",0.4569737911,
496,doing the fit,"def pca_conf_matrix(algorithm):
    
    y_pred_test = algorithm.predict(X_pca2_test)

    mat = confusion_matrix(y_test, y_pred_test)
    plt.figure(figsize=(8,8))
    sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=True, cbar_kws={""shrink"": .82})
    plt.title(""CONFUSION MATRIX"", fontsize=15)
    plt.xlabel('true label', fontsize=12)
    plt.ylabel('predicted label', fontsize=12);",0.4547566175,
496,doing the fit,"def conf_matrix(algorithm):
    
    y_pred_test = algorithm.predict(X_test)

    mat = confusion_matrix(y_test, y_pred_test)
    plt.figure(figsize=(8,8))
    sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=True, cbar_kws={""shrink"": .82})
    plt.title(""CONFUSION MATRIX"", fontsize=15)
    plt.xlabel('true label', fontsize=12)
    plt.ylabel('predicted label', fontsize=12);",0.4547566175,
496,doing the fit,"def conf_matrix(algorithm):
    
    y1_pred_test = algorithm.predict(X1_test)

    mat = confusion_matrix(y1_test, y1_pred_test)
    plt.figure(figsize=(5,5))
    sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)
    plt.title(""CONFUSION MATRIX"", fontsize=15)
    plt.xlabel('true label', fontsize=12)
    plt.ylabel('predicted label', fontsize=12);",0.4547566175,
496,doing the fit,"def conf_matrix(algorithm):
    
    y_pred_test = algorithm.predict(X_test)

    mat = confusion_matrix(y_test, y_pred_test)
    plt.figure(figsize=(7,7))
    sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)
    plt.title(""CONFUSION MATRIX"", fontsize=15)
    plt.xlabel('true label', fontsize=12)
    plt.ylabel('predicted label', fontsize=12);",0.4547566175,
496,doing the fit,"def fit_param_filter(line, alpha):
    '''
    First order filter
    '''
    line.best_fit = alpha * line.current_fit + (1. - alpha) * line.best_fit",0.4527929425,
496,doing the fit,"from collections import deque

-

    def update_current_fit(self, fit):
        self.current_fit = fit
    
    def update_current_xval(self, img):
        yvals = np.linspace(0, img.shape[0]-1, img.shape[0])
        self.current_xval = self.current_fit[0]*yvals**2 + self.current_fit[1]*yvals + self.current_fit[2]
    
    def update_curvature(self, img):
        self.radius_of_curvature = cal_curvature(img, self.current_fit)
        
    def update_offset(self, img):
        self.line_base_pos = cal_offset_pixel(img, self.current_fit)            
    
    def update_diffs(self):
        if self.n_buffered > 0:
            self.diffs = self.current_fit - self.best_fit
        else:
            self.diffs = np.array([0,0,0], dtype='float') 
            
    def update_best(self, img):
        if len(self.recent_xfitted) > 0:
            self.best_xval = np.mean(self.recent_xfitted, axis=0)
            yvals = np.linspace(0, img.shape[0]-1, img.shape[0])
            xvals = self.best_xval
            self.best_fit = np.polyfit(yvals, xvals, 2)
            
    def add_data(self):
        self.recent_xfitted.appendleft(self.current_xval)
        self.recent_fit.appendleft(self.current_fit)
        self.n_buffered = len(self.recent_xfitted)
        
    def pop_data(self):        
        if self.n_buffered > 0:
            self.recent_xfitted.pop()
            self.recent_fit.pop()
            self.n_buffered = len(self.recent_xfitted)

    def update(self, img, fit):
        self.update_current_fit(fit)
        self.update_current_xval(img)
        self.update_diffs()
        self.update_curvature(img)
        self.update_offset(img)
                
    def set_detected(self, img, inp=True):
        self.detected = inp
        if self.detected:
            self.add_data()
            self.update_best(img)
        else:
            if self.n_buffered > 0:
                self.pop_data()
                self.update_best(img)",0.4488587976,
496,doing the fit,"def plot_intercept_distribution(ens):
    pylab.subplot(1,2,1)
    intercepts = ens.intercepts.sample(ens.n_neurons)
    seaborn.distplot(intercepts, bins=20)
    pylab.xlabel('intercept')

    pylab.subplot(1,2,2)
    pts = ens.eval_points.sample(n=1000, d=ens.dimensions)
    model = nengo.Network()
    model.ensembles.append(ens)
    sim = nengo.Simulator(model)
    _, activity = nengo.utils.ensemble.tuning_curves(ens, sim, inputs=pts)
    p = np.mean(activity>0, axis=0)
    seaborn.distplot(p, bins=20)
    pylab.xlabel('proportion of pts neuron is active for')",0.4460148513,
1409,part explore the dataset,"def onclick_polygon_interrogation(event):
    global pixelx, pixely, AlldataMasked, FieldMean, Clickedpolygon
    pixelx, pixely = int(event.xdata), int(event.ydata)
    # Generate a point from the location
    Clickpoint = shapely.geometry.Point(pixelx, pixely)
    IrrigatedShapes = IrrigatedPolygons.shapes()
    # Find the polygon that contains the selected point
    Clickedpolygon = []
    for ix, shapes in enumerate(IrrigatedPolygons.shapes()):
        if shapely.geometry.shape(shapes).contains(Clickpoint) == True:
            Clickedpolygon.append(ix)
    # Colour the chosen polygon on the figure
    x = [i[0] for i in IrrigatedShapes[Clickedpolygon[0]].points[:]]
    y = [i[1] for i in IrrigatedShapes[Clickedpolygon[0]].points[:]]
    
    plt.figure(fig1.number)
    plt.plot(x, y, 'r')
    
    # Grab the geometry from the polygon we want to interrogate
    with fiona.open(shape_file) as shapes:
        crs = geometry.CRS(shapes.crs_wkt)
        first_geometry = shapes[Clickedpolygon[0]]['geometry']
        geom = geometry.Geometry(first_geometry, crs=crs)

    querys2 = {'output_crs': 'EPSG:3577',
               'resolution': (-10, 10),
               'geopolygon': geom,
               'time':(start_date, end_date)
              }
    queryls = {'geopolygon': geom,
               'time':(start_date, end_date)
              }

    # dc.load the data using the polygon as the bounds
    Alldata = dict()
    for Sensor in AllSensors:
        if Sensor[0] == 'l':
            try:
                Alldata[Sensor], LScrs, LSaffine = DEADataHandling.load_nbarx(dc, Sensor, queryls, product = 'nbart')
            except TypeError:
                print('No data available for {}'.format(Sensor))
        if Sensor[0] == 's':
            prodname = '{0}_ard_granule'.format(Sensor)
            try:
                Alldata[Sensor], S2crs, S2affine = DEADataHandling.load_sentinel(dc, prodname, querys2)
            except TypeError:
                print('No data available for {}'.format(Sensor))

    # Tidy up our dict to remove any empty keys
    for Sensor in AllSensors:
        try:
            Alldata[Sensor]
            if Alldata[Sensor] is None:
                del Alldata[Sensor]
            else:
                try:
                    Alldata[Sensor].time
                except AttributeError:
                    del Alldata[Sensor]
        except KeyError:
                pass

    # Mask the returned data with the polygon to remove any extra data
    AlldataMasked = Alldata.copy()
    for Sensor in Alldata.keys():
        mask = rasterio.features.geometry_mask([geom.to_crs(Alldata[Sensor].geobox.crs) for geoms in [geom]],
                                               out_shape=Alldata[Sensor].geobox.shape,
                                               transform=Alldata[Sensor].geobox.affine,
                                               all_touched=False,
                                               invert=True)
        AlldataMasked[Sensor] = Alldata[Sensor].where(mask)

    # Calculate the NDVI for each sensor
    for Sensor in AllSensors:
        try:
            AlldataMasked[Sensor]['NDVI'] = BandIndices.calculate_indices(AlldataMasked[Sensor], 'NDVI')
        except KeyError:
            pass

    # Concatenate all out 
    Allvalues = xr.concat([masked.NDVI for masked in AlldataMasked.values()], dim='time')
    Allvalues = Allvalues.sortby('time')
    Allvalues.values[Allvalues.values == -999] = np.nan
    FieldMean = Allvalues.mean(dim = ('x', 'y')).dropna(dim = 'time')",0.5283901691,
1409,part explore the dataset,"def onclick(event):
    '''
    This function performs all of the work in this notebook. We have put all of the
    processing within this function so that all the calculations are done following
    a 'click' and therefore do not need to be in separate cells. This makes the notebook
    smoother and minimises the cells that need to be manually run by the user once a 
    location has been selected. 

    This particular widget function uses the selected to location to plot two images:
    - spectra for the pixel closest to the chosen location
    - spectra for the pixel/s closest to the chosen location using the 60m pixel as
    a bounding box for the higher resolution pixels.
    These two images are run in the subsequent cells.
    '''
    global pixelx, pixely, spectra, spectramin, spectramax, spectramean
    pixelx, pixely = int(event.xdata), int(event.ydata)
    w.value = 'pixelx : {}, pixely : {}'.format(pixelx, pixely)
    plt.plot(pixelx, pixely, 'ro', markersize=5)

    # Find the pixel closest to the chosen pixel at each resolution
    Pixel10m = Resolution10m.sel(y=pixely, x=pixelx, method='nearest')
    Pixel20m = Resolution20m.sel(y=pixely, x=pixelx, method='nearest')
    Pixel60m = Resolution60m.sel(y=pixely, x=pixelx, method='nearest')

    # Grab the pixel spectral values for each band
    spectra = [Pixel60m.nbar_coastal_aerosol.isel(time=mytime).values,
               Pixel10m.nbar_blue.isel(time=mytime).values,
               Pixel10m.nbar_green.isel(time=mytime).values,
               Pixel10m.nbar_red.isel(time=mytime).values,
               Pixel20m.nbar_red_edge_1.isel(time=mytime).values,
               Pixel20m.nbar_red_edge_2.isel(time=mytime).values,
               Pixel20m.nbar_red_edge_3.isel(time=mytime).values,
               Pixel10m.nbar_nir_1.isel(time=mytime).values,
               Pixel20m.nbar_nir_2.isel(time=mytime).values,
               Pixel20m.nbar_swir_2.isel(time=mytime).values,
               Pixel20m.nbar_swir_3.isel(time=mytime).values,
               ]

    # Get the location of the selected pixel at the coursest resolution
    Pixel60m = Resolution60m.sel(y=pixely, x=pixelx, method='nearest')

    # Find the index locations of the lat/lon of that pixel
    xindex = Resolution60m.indexes['x'].get_loc(
        Pixel60m.x.values.item(), method='nearest')
    yindex = Resolution60m.indexes['y'].get_loc(
        Pixel60m.y.values.item(), method='nearest')

    # Get the index for the pixels next to the chosen pixel
    xmax = Resolution60m.x.isel(x=xindex+1)
    xmin = Resolution60m.x.isel(x=xindex-1)
    ymax = Resolution60m.y.isel(y=yindex-1)
    ymin = Resolution60m.y.isel(y=yindex+1)

    # Now work out what the lat/lon is for halfway between pixel +-1 (to keep our resolution at 60 x 60 m)
    latmin = mean((xmin, pixelx))
    latmax = mean((pixelx, xmax))
    lonmin = mean((ymin, pixely))
    lonmax = mean((pixely, ymax))

    # Grab all of the pixels that fall within the 60m pixel bounds
    bluepixels = Resolution10m.nbar_blue.sel(
        x=slice(latmin, latmax), y=slice(lonmax, lonmin))
    greenpixels = Resolution10m.nbar_green.sel(
        x=slice(latmin, latmax), y=slice(lonmax, lonmin))
    redpixels = Resolution10m.nbar_red.sel(
        x=slice(latmin, latmax), y=slice(lonmax, lonmin))
    rededge1pixels = Resolution20m.nbar_red_edge_1.sel(
        x=slice(latmin, latmax), y=slice(lonmax, lonmin))
    rededge2pixels = Resolution20m.nbar_red_edge_2.sel(
        x=slice(latmin, latmax), y=slice(lonmax, lonmin))
    rededge3pixels = Resolution20m.nbar_red_edge_3.sel(
        x=slice(latmin, latmax), y=slice(lonmax, lonmin))
    nir1pixels = Resolution10m.nbar_nir_1.sel(
        x=slice(latmin, latmax), y=slice(lonmax, lonmin))
    nir2pixels = Resolution20m.nbar_nir_2.sel(
        x=slice(latmin, latmax), y=slice(lonmax, lonmin))
    swir2pixels = Resolution20m.nbar_swir_2.sel(
        x=slice(latmin, latmax), y=slice(lonmax, lonmin))
    swir3pixels = Resolution20m.nbar_swir_3.sel(
        x=slice(latmin, latmax), y=slice(lonmax, lonmin))

    # Grab the min, max and mean of the pixels within the 60m bounding box
    spectramin = [Pixel60m.nbar_coastal_aerosol.isel(time=mytime).min().item(),
                  bluepixels.isel(time=mytime).min().item(),
                  greenpixels.isel(time=mytime).min().item(),
                  redpixels.isel(time=mytime).min().item(),
                  rededge1pixels.isel(time=mytime).min().item(),
                  rededge2pixels.isel(time=mytime).min().item(),
                  rededge3pixels.isel(time=mytime).min().item(),
                  nir1pixels.isel(time=mytime).min().item(),
                  nir2pixels.isel(time=mytime).min().item(),
                  swir2pixels.isel(time=mytime).min().item(),
                  swir3pixels.isel(time=mytime).min().item(),
                  ]

    spectramax = [Pixel60m.nbar_coastal_aerosol.isel(time=mytime).max().item(),
                  bluepixels.isel(time=mytime).max().item(),
                  greenpixels.isel(time=mytime).max().item(),
                  redpixels.isel(time=mytime).max().item(),
                  rededge1pixels.isel(time=mytime).max().item(),
                  rededge2pixels.isel(time=mytime).max().item(),
                  rededge3pixels.isel(time=mytime).max().item(),
                  nir1pixels.isel(time=mytime).max().item(),
                  nir2pixels.isel(time=mytime).max().item(),
                  swir2pixels.isel(time=mytime).max().item(),
                  swir3pixels.isel(time=mytime).max().item(),
                  ]

    spectramean = [Pixel60m.nbar_coastal_aerosol.isel(time=mytime).mean().item(),
                   bluepixels.isel(time=mytime).mean().item(),
                   greenpixels.isel(time=mytime).mean().item(),
                   redpixels.isel(time=mytime).mean().item(),
                   rededge1pixels.isel(time=mytime).mean().item(),
                   rededge2pixels.isel(time=mytime).mean().item(),
                   rededge3pixels.isel(time=mytime).mean().item(),
                   nir1pixels.isel(time=mytime).mean().item(),
                   nir2pixels.isel(time=mytime).mean().item(),
                   swir2pixels.isel(time=mytime).mean().item(),
                   swir3pixels.isel(time=mytime).mean().item(),
                   ]",0.5096834898,
1409,part explore the dataset,"def set_test_datafolder(learner, test_folder):
    learner.data = (src.add_test_folder(test_folder)
                    .transform(tfms, size=256)
                    .databunch().normalize(imagenet_stats))",0.4964271486,
1409,part explore the dataset,"def plot_intercept_distribution(ens):
    pylab.subplot(1,2,1)
    intercepts = ens.intercepts.sample(ens.n_neurons)
    seaborn.distplot(intercepts, bins=20)
    pylab.xlabel('intercept')

    pylab.subplot(1,2,2)
    pts = ens.eval_points.sample(n=1000, d=ens.dimensions)
    model = nengo.Network()
    model.ensembles.append(ens)
    sim = nengo.Simulator(model)
    _, activity = nengo.utils.ensemble.tuning_curves(ens, sim, inputs=pts)
    p = np.mean(activity>0, axis=0)
    seaborn.distplot(p, bins=20)
    pylab.xlabel('proportion of pts neuron is active for')",0.4958819747,
1409,part explore the dataset,"def grepColumns(gen):
    columnRe = re.compile(columnDetect, re.X)
    correctRe = re.compile(columnCorrectPat, re.X)
    faceRe = re.compile(faceDetect, re.X)

    columns = []
    curFace = NOFACE
    prevTablet = None
    ncTablets = set()
    for (period, tablet, ln, line, skip) in gen:
        if skip:
            ncTablets.add(tablet)
            continue
        if tablet != prevTablet:
            curFace = NOFACE
        prevTablet = tablet

        match = faceRe.match(line)

        if match:
            face = match.group(1)
            if face in FACES:
                curFace = face

        if columnRe.match(line):
            if not line.startswith('@column '):
                match = correctRe.match(line)
                if match:
                    colSpec = match.group(1)
                    sep = match.group(2)
                    colNum = match.group(3)
                    line = f'@column {colNum}'
                    print(f'GREP: corrected ""{colSpec}{sep}{colNum}"" => ""{line}""')
                else:
                    print(f'GREP: found ""{line}""')
                
            columns.append((period, tablet, ln, curFace, line.strip()))
    if ncTablets:
        NCmsg(len(ncTablets))
    return columns",0.4928503335,
1409,part explore the dataset,"def eda_feature(df):
    y = np.asarray(df.eda)
    present = np.mean(df.present)
    yn = (y - y.mean()) / y.std()
    Fs = 4.
    [r, p, t, l, d, e, obj] = cvxEDA.cvxEDA(yn, 1./Fs)
    tm = pl.arange(1., len(y)+1.) / Fs
    
    y_mean = np.mean(yn)
    r_mean = np.mean(r)
    p_mean = np.mean(p)
    t_mean = np.mean(t)

    y_stdev = np.std(yn)
    r_stdev = np.std(r)
    p_stdev = np.std(p)
    t_stdev = np.std(t)

    y_max = np.amax(yn)
    r_max = np.amax(r)
    p_max = np.amax(p)
    t_max = np.amax(t)

    feature_list=[]
    feature_list.extend((y_mean,r_mean,p_mean,t_mean,
                    y_stdev,r_stdev,p_stdev,t_stdev,
                    y_max,r_max,p_max,t_max,
                    present))
    return feature_list",0.4915599823,
1409,part explore the dataset,"def vect_test(vect):
    # transform the unstructured data using the vectorizer
    X_train_dtm = vect.fit_transform(X_train)
    X_test_dtm = vect.transform(X_test)

    # print the number of features generated as a result of the transformation
    print 'Features: ', X_train_dtm.shape[1]

    # train Logistic Regression model and measure accuracy
    lr = LogisticRegression()
    lr.fit(X_train_dtm, y_train)
    y_pred_class_lr = lr.predict(X_test_dtm)
    print 'Logistic Regression Accuracy: ', metrics.accuracy_score(y_test, y_pred_class_lr)

    # train Naive Bayes model and measure accuracy
    nb = MultinomialNB()
    nb.fit(X_train_dtm, y_train)
    y_pred_class_nb = nb.predict(X_test_dtm)
    print 'Naive Bayes Accuracy: ', metrics.accuracy_score(y_test, y_pred_class_nb)
    
    # train Random Forest model and measure accuracy
    rf = RandomForestClassifier()
    rf.fit(X_train_dtm, y_train)
    y_pred_class_rf = rf.predict(X_test_dtm)
    print 'Random Forest Accuracy: ', metrics.accuracy_score(y_test, y_pred_class_rf)",0.4900667667,
1409,part explore the dataset,"def calc_pdm(x,u):
    global pdm
    global centers
    new_pdm = []
    for c in range(groups):
        u_scalar = 0.0
        u_scalar_2 = 0.0
        for i in range(len(data_set)):
            u_scalar = u_scalar + ( u[i][c]**m )*( mahalanobis_distance(x, centers[c]))**2
            u_scalar_2 = u_scalar_2 + ( u[i][c]**m )
        new_pdm.append(np.divide(u_scalar, u_scalar_2))
    pdm = new_pdm
    return pdm",0.4894155264,
1409,part explore the dataset,"def onclick(event):
    global pixelx, pixely
    x, y = int(event.xdata), int(event.ydata)
    image_coords = data.affine * (x, y)
    pixelx = int(image_coords[0])
    pixely = int(image_coords[1])
    w.value = 'pixelx : {}, pixely : {}'.format(pixelx, pixely)",0.4893978834,
1409,part explore the dataset,"def clean_text(data):
    for i, target in zip(range(len(data.data)), data.target):
        #------------TO LOWER CASE-------------
        file = data.data[i].decode(""utf-8"").lower()
        #------------TOKENIZE-------------
        word_tokens = word_tokenize(file)
        #------------REMOVE STOP WORDS-------------
        filtered_sentence = [w for w in word_tokens if not w in stop_words]
        filtered_sentence = []
        for w in word_tokens:
            if w not in stop_words:
                    filtered_sentence.append(w)
        #------------STEMMING WITH PORTER STEMMER-------------
        ps = PorterStemmer()
        stemmedFile = []
        for word in filtered_sentence:
            for w in word.split("" ""):
                stem = ps.stem(w)
                stemmedFile.append(stem)
                #COUNT THE TERMS PER CATEGORY
                term_per_category[train_set.target_names[target]][word] += 1
        #------------PUT FILE BACK-------------
        data.data[i] = ' '.join(stemmedFile)",0.4871175885,
2399,training a decision tree model,"import math
import numpy as np

class decision_tree_regressor:
    
    def __init__(self, max_depth = None, criteria='std'):
        """"""
        Builds a decision tree to regress on the target data. The
        decision tree is built by trying to minimize the requested
        criteria at each possible split. Starting with the whole data
        the tree will try every possible split (column and value pair)
        and choose the split which results in the greatest reduction 
        of the criteria. Then for each sub-group of that split, the 
        process is repeated recursively until no more splits are possible
        or no splits cause a reductuion of the criteria.
        ---
        KWargs:
        max_depth: how many splits to allow in the tree (depth not breadth)
        criteria: what metric to use as a measure of split strength 
        ('std'= reduction of standard deviation in the data, 'mae'=
        minimize the mean error size (abs value))
        """"""
        self.tree = self.tree_split()
        self.data_cols = None
        self.max_depth = max_depth
        self.current_depth = 0
        self.criteria = criteria
    
    # Sub class for handling recursive nodes (only makes sense in the scope of a tree)
    class tree_split:
        """"""
        A sub class for handling recursive nodes. Each node will contain the value and column
        for the current split, as well as links to the resulting nodes from the split. The 
        results attribute remains empty unless the current node is a leaf. 
        """"""
        def __init__(self,col=-1,value=None,results=None,label=None,tb=None,fb=None):
            self.col=col # column index of criteria being tested
            self.value=value # vlaue necessary to get a true result
            self.results=results # dict of results for a branch, None for everything except endpoints
            self.tb=tb # true decision nodes 
            self.fb=fb # false decision nodes
    
    def split_data(self, X, y, colnum, value):
        """"""
        Returns: Two sets of data from the initial data. Set 1 contains those that passed
        the condition of data[colnum] >= value
        ----------
        Input: The dataset, the column to split on, the value on which to split
        """"""
        splitter = None
        if isinstance(value, int) or isinstance(value,float):
            splitter = lambda x: x[colnum] >= value
        else:
            splitter = lambda x: x[colnum] == value
        split1 = [i for i,row in enumerate(X) if splitter(row)]
        split2 = [i for i,row in enumerate(X) if not splitter(row)]
        set1X = X[split1]
        set1Y = y[split1]
        set2X = X[split2]
        set2Y = y[split2]
        return set1X, set1Y, set2X, set2Y

    def get_mean_target_value(self, data):
        """"""
        Returns: A dictionary of target variable counts in the data
        """"""
        return np.mean(data)

    def split_criteria(self, y):
        """"""
        Returns the criteria we're trying to minimize by splitting.
        Current options are target Mean Absolute Error (from the target 
        mean) or Standard deviation of the target.
        ---
        Input: targets in the split
        Output: Criteria
        """"""
        if self.criteria == 'mae':
            mu = np.mean(y)
            return np.mean(np.abs(y-mu))
        else:
            return np.std(y)
    
    def pandas_to_numpy(self, x):
        """"""
        Checks if the input is a Dataframe or series, converts to numpy matrix for
        calculation purposes.
        ---
        Input: X (array, dataframe, or series)
        Output: X (array)
        """"""
        if type(x) == type(pd.DataFrame()) or type(x) == type(pd.Series()):
            return x.as_matrix()
        if type(x) == type(np.array([1,2])):
            return x
        return np.array(x) 
        
    def handle_1d_data(self,x):
        """"""
        Converts 1 dimensional data into a series of rows with 1 columns
        instead of 1 row with many columns.
        """"""
        if x.ndim == 1:
            x = x.reshape(-1,1)
        return x
    
    def convert_to_array(self, x):
        """"""
        Takes in an input and converts it to a numpy array
        and then checks if it needs to be reshaped for us
        to use it properly
        """"""
        x = self.pandas_to_numpy(x)
        x = self.handle_1d_data(x)
        return x
    
    def fit(self, X, y):
        """"""
        Helper function to wrap the fit method. This makes sure the full nested, 
        recursively built tree gets assigned to the correct variable name and 
        persists after training.
        """"""
        self.tree = self._fit(X,y)
    
    def _fit(self, X, y, depth=0):
        """"""
        Builds the decision tree via a greedy approach, checking every possible
        branch for the best current decision. Decision strength is measured by
        information gain/score reduction. If no information gain is possible,
        sets a leaf node. Recursive calls to this method allow the nesting. If
        max_depth is met, all further nodes become leaves as well.
        ---
        Input: X (feature matrix), y (labels)
        Output: A nested tree built upon the node class.""""""
        X = self.convert_to_array(X)
        y = self.convert_to_array(y)
       
        if len(X) == 0: return tree_split()
        current_score = self.split_criteria(y)

        best_gain = 0.0
        best_criteria = None
        best_sets = None
        
        self.data_cols = X.shape[1]
        
        
        # Here we go through column by column and try every possible split, measuring the
        # information gain. We keep track of the best split then use that to send the split
        # data sets into the next phase of splitting.
        
        for col in range(self.data_cols):
            column_values = set(X.T[col])
            for value in column_values:
                set1, set1_y, set2, set2_y = self.split_data(X, y, col, value)
                p = float(len(set1)) / len(y)
                gain = current_score - p*self.split_criteria(set1_y) - (1-p)*self.split_criteria(set2_y)
                if gain > best_gain and len(set1_y) and len(set2_y):
                    best_gain = gain
                    best_criteria = (col, value)
                    best_sets = (np.array(set1), np.array(set1_y), np.array(set2), np.array(set2_y))
        
        # Now decide whether it's an endpoint or we need to split again.
        if (self.max_depth and depth < self.max_depth) or not self.max_depth:
            if best_gain > 0:
                self.current_depth += 1
                true_branch = self._fit(best_sets[0], best_sets[1], depth=depth+1)
                false_branch = self._fit(best_sets[2], best_sets[3], depth=depth+1)
                return self.tree_split(col=best_criteria[0], value=best_criteria[1],
                        tb=true_branch, fb=false_branch)
            else:
                return self.tree_split(results=self.get_mean_target_value(y))
        else:
            return self.tree_split(results=self.get_mean_target_value(y))

    def print_tree(self, indent=""---""):
        """"""
        Helper function to make sure the correct tree gets printed.
        ---
        In: indent (how to show splits between nodes)
        """"""
        self.__original_indent = indent
        self._print_tree_(self.tree, indent)
    
    def _print_tree_(self, tree, indent):
        """"""
        Goes through node by node and reports the column and value used to split
        at that node. All sub-nodes are drawn in sequence below the node.
        """"""
        if tree.results: # if this is a end node
            print(str(tree.results))
        else:
            print('Column ' + str(tree.col)+' : '+str(tree.value)+'? ')
            # Print the branches
            print(indent+' True: ', end=' ')
            next_indent = indent+self.__original_indent
            self._print_tree_(tree.tb,indent=next_indent)
            print(indent+' False: ', end=' ')
            self._print_tree_(tree.fb,indent=next_indent)

    def predict(self, newdata):
        """"""
        Helper function to make sure the correct tree is used to
        make predictions. Also manages multiple rows of input data
        since the tree must predict one at a time.
        ---
        In: new data point of the same structure as the training X.
        Out: numpy array of the resulting predictions
        """"""
        results = []
        newdata = self.convert_to_array(newdata)
        for x in newdata:
            results.append(self._predict(x,self.tree))
        return np.array(results)
            
    def _predict(self, newdata, tree):
        """"""
        Uses the reusive structure of the tree to follow each split for
        a new data point. If the node is an endpoint, the available classes
        are sorted by ""most common"" and then the top choice is returned.
        """"""
        if tree.results: # if this is a end node
            return tree.results

        if isinstance(newdata[tree.col], int) or isinstance(newdata[tree.col],float):
            if newdata[tree.col] >= tree.value:
                return self._predict(newdata, tree.tb)

            else:
                return self._predict(newdata, tree.fb)
        else:
            if newdata[tree.col] == tree.value:
                return self._predict(newdata, tree.tb)
            else:
                return self._predict(newdata, tree.fb) 

    def score(self, X, y):
        """"""
        Uses the predict method to measure the accuracy of the model.
        ---
        In: X (list or array), feature matrix; y (list or array) labels
        Out: accuracy (float)
        """"""
        pred = self.predict(X)
        return np.mean((pred-y)**2)",0.5303142667,
2399,training a decision tree model,"import math
import numpy as np

class decision_tree_classifier:
    
    def __init__(self, max_depth = None):
        """"""
        Builds a decision tree to classify target data. The
        decision tree is built by trying to minimize the requested
        criteria at each possible split. Starting with the whole data
        the tree will try every possible split (column and value pair)
        and choose the split which results in the greatest reduction 
        of the criteria. Then for each sub-group of that split, the 
        process is repeated recursively until no more splits are possible
        or no splits cause a reduction of the criteria. In this class
        entropy is used as the criteria.
        ---
        KWargs:
        max_depth: how many splits to allow in the tree (depth not breadth)
        """"""
        self.tree = self.tree_split()
        self.max_depth = max_depth
    
    # Sub class for handling recursive nodes (only makes sense in the scope of a tree)
    class tree_split:
        """"""
        A sub class for handling recursive nodes. Each node will contain the value and column
        for the current split, as well as links to the resulting nodes from the split. The 
        results attribute remains empty unless the current node is a leaf. 
        """"""
        def __init__(self,col=-1,value=None,results=None,label=None,tb=None,fb=None):
            self.col=col # column index of criteria being tested
            self.value=value # vlaue necessary to get a true result
            self.results=results # dict of results for a branch, None for everything except endpoints
            self.tb=tb # true decision nodes 
            self.fb=fb # false decision nodes
    
    def split_data(self, X, y, colnum, value):
        """"""
        Returns: Two sets of data from the initial data. Set 1 contains those that passed
        the condition of data[colnum] >= value
        ----------
        Input: The dataset, the column to split on, the value on which to split
        """"""
        splitter = None
        if isinstance(value, int) or isinstance(value,float):
            splitter = lambda x: x[colnum] >= value
        else:
            splitter = lambda x: x[colnum] == value
        split1 = [i for i,row in enumerate(X) if splitter(row)]
        split2 = [i for i,row in enumerate(X) if not splitter(row)]
        set1X = X[split1]
        set1Y = y[split1]
        set2X = X[split2]
        set2Y = y[split2]
        return set1X, set1Y, set2X, set2Y

    def count_target_values(self, data):
        """"""
        Returns: A dictionary of target variable counts in the data
        """"""
        results = {}
        counts = np.unique(data, return_counts=True)
        for i,j in zip(*counts):
            results[i] = j
        return results

    def entropy(self, y):
        """"""
        Returns: Entropy of the data set, based on target values. 
        ent = Sum(-p_i Log(p_i), i in unique targets) where p is the percentage of the
        data with the ith label.
        Sidenote: We're using entropy as our measure of good splits. It corresponds to 
        information gained by making this split. If the split results in only one target type
        then the entropy new sets entropy is 0. If it results in a ton of different targets, the
        entropy will be high. 
        """"""
        results = self.count_target_values(y)
        log_base = len(results.keys())
        if log_base < 2:
            log_base = 2
        logb=lambda x:math.log(x)/math.log(log_base)
        ent=0.
        for r in results.keys():
            p=float(results[r])/len(y) 
            ent-=p*logb(p)
        return ent  
    
    def pandas_to_numpy(self, x):
        """"""
        Checks if the input is a Dataframe or series, converts to numpy matrix for
        calculation purposes.
        ---
        Input: X (array, dataframe, or series)
        Output: X (array)
        """"""
        if type(x) == type(pd.DataFrame()) or type(x) == type(pd.Series()):
            return x.as_matrix()
        if type(x) == type(np.array([1,2])):
            return x
        return np.array(x) 
    
    def handle_1d_data(self,x):
        """"""
        Converts 1 dimensional data into a series of rows with 1 columns
        instead of 1 row with many columns.
        """"""
        if x.ndim == 1:
            x = x.reshape(-1,1)
        return x
    
    def convert_to_array(self, x):
        """"""
        Takes in an input and converts it to a numpy array
        and then checks if it needs to be reshaped for us
        to use it properly
        """"""
        x = self.pandas_to_numpy(x)
        x = self.handle_1d_data(x)
        return x
    
    def fit(self, X, y):
        """"""
        Helper function to wrap the fit method. This makes sure the full nested, 
        recursively built tree gets assigned to the correct variable name and 
        persists after training.
        """"""
        self.tree = self._fit(X,y)
    
    def _fit(self, X, y, depth=0):
        """"""
        Builds the decision tree via a greedy approach, checking every possible
        branch for the best current decision. Decision strength is measured by
        information gain/entropy reduction. If no information gain is possible,
        sets a leaf node. Recursive calls to this method allow the nesting. If
        max_depth is met, all further nodes become leaves as well.
        ---
        Input: X (feature matrix), y (labels)
        Output: A nested tree built upon the node class.""""""
        X = self.convert_to_array(X)
        y = self.convert_to_array(y)

        if len(X) == 0: return tree_split()
        current_score = self.entropy(y)

        best_gain = 0.0
        best_criteria = None
        best_sets = None
        
        self.data_cols = X.shape[1]
        
        
        # Here we go through column by column and try every possible split, measuring the
        # information gain. We keep track of the best split then use that to send the split
        # data sets into the next phase of splitting.
        
        for col in range(self.data_cols):
            
            # find different values in this column
            column_values = set(X.T[col])
            # for each possible value, try to divide on that value
            for value in column_values:
                set1, set1_y, set2, set2_y = self.split_data(X, y, col, value)

                # Information gain
                p = float(len(set1)) / len(y)
                gain = current_score - p*self.entropy(set1_y) - (1-p)*self.entropy(set2_y)
                if gain > best_gain and len(set1_y) and len(set2_y):
                    best_gain = gain
                    best_criteria = (col, value)
                    best_sets = (np.array(set1), np.array(set1_y), np.array(set2), np.array(set2_y))
        
        
        # Now decide whether it's an endpoint or we need to split again.
        if (self.max_depth and depth < self.max_depth) or not self.max_depth:
            if best_gain > 0:
                true_branch = self._fit(best_sets[0], best_sets[1], depth=depth+1)
                false_branch = self._fit(best_sets[2], best_sets[3], depth=depth+1)
                return self.tree_split(col=best_criteria[0], value=best_criteria[1],
                        tb=true_branch, fb=false_branch)
            else:
                return self.tree_split(results=self.count_target_values(y))
        else:
            return self.tree_split(results=self.count_target_values(y))

    def print_tree(self, indent=""---""):
        """"""
        Helper function to make sure the correct tree gets printed.
        ---
        In: indent (how to show splits between nodes)
        """"""
        self.__original_indent = indent
        self._print_tree_(self.tree, indent)
    
    def _print_tree_(self, tree, indent):
        """"""
        Goes through node by node and reports the column and value used to split
        at that node. All sub-nodes are drawn in sequence below the node.
        """"""
        if tree.results: # if this is a end node
            print(str(tree.results))
        else:
            print('Column ' + str(tree.col)+' : '+str(tree.value)+'? ')
            # Print the branches
            print(indent+' True: ', end=' ')
            next_indent = indent+self.__original_indent
            self._print_tree_(tree.tb,indent=next_indent)
            print(indent+' False: ', end=' ')
            self._print_tree_(tree.fb,indent=next_indent)

    def predict(self, newdata):
        """"""
        Helper function to make sure the correct tree is used to
        make predictions. Also manages multiple rows of input data
        since the tree must predict one at a time.
        ---
        In: new data point of the same structure as the training X.
        Out: numpy array of the resulting predictions
        """"""
        results = []
        for x in newdata:
            results.append(self._predict(x,self.tree))
        return np.array(results)
            
    def _predict(self, newdata, tree):
        """"""
        Uses the reusive structure of the tree to follow each split for
        a new data point. If the node is an endpoint, the available classes
        are sorted by ""most common"" and then the top choice is returned.
        """"""
        newdata = self.pandas_to_numpy(newdata)
        if tree.results: # if this is a end node
            return sorted(list(tree.results.items()), key=lambda x: x[1],reverse=True)[0][0]

        if isinstance(newdata[tree.col], int) or isinstance(newdata[tree.col],float):
            if newdata[tree.col] >= tree.value:
                return self._predict(newdata, tree.tb)

            else:
                return self._predict(newdata, tree.fb)
        else:
            if newdata[tree.col] == tree.value:
                return self._predict(newdata, tree.tb)
            else:
                return self._predict(newdata, tree.fb) 

    def score(self, X, y):
        """"""
        Uses the predict method to measure the accuracy of the model.
        ---
        In: X (list or array), feature matrix; y (list or array) labels
        Out: accuracy (float)
        """"""
        pred = self.predict(X)
        correct = 0
        for i,j in zip(y,pred):
            if i == j:
                correct+=1
        return float(correct)/float(len(y))",0.526358366,
2399,training a decision tree model,"# Now plot the trees individually
irf_jupyter_utils.draw_tree(decision_tree = all_rf_tree_data['rf_obj'].estimators_[0])",0.5242379308,
2399,training a decision tree model,"iris_dt2 = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth=2)
iris_dt2.fit(iris.data, iris.target)
disp_iris_tree('iris_dt2',iris_dt2)",0.5229946375,
2399,training a decision tree model,"dt_model_1 = tree.DecisionTreeClassifier(criterion='entropy',max_depth=1,random_state=123)",0.5217779875,
2399,training a decision tree model,"class Decision_Tree:

    """"""creates a decision tree """"""

    def __init__(self, training_data, T_index, max_depth=10**9, num_Features=None):
        
        self.training_data = training_data
        self.T_index       = T_index
        self.max_depth     = max_depth
        self.num_Features  = num_Features
        
        # build the decision tree
        self.tree = self.build_tree(self.training_data)
        
        # training statistics (these become initialized after calling train)
        self.training_classifications = None
        self.training_booleans = None
        self.training_per = None
        
        #testing statistics (these become initialized after using the ""use"" method)
        self.testing_data = None
        self.testing_classifications = None
        self.testing_booleans = None
        self.testing_per = None

    def class_counts(self, rows):
        """"""Counts the number of each type of example in a dataset.""""""

        counts = {}  # a dictionary of label -> count.
        for row in rows:
            label = row[self.T_index]
            if label not in counts:
                counts[label] = 0
            counts[label] += 1
        return counts

    def is_numeric(self,value):
        """"""Test if a value is numeric.""""""
        return isinstance(value, int) or isinstance(value, float)
    
    class Question:
        """"""A Question is used to partition a dataset.

        This class just records a 'column number' (e.g., 0 for Color) and a
        'column value' (e.g., Green). The 'match' method is used to compare
        the feature value in an example to the feature value stored in the
        question. See the demo below.
        """"""

        def __init__(self, column, value):
            self.column = column
            self.value = value

        def match(self, example):
            # Compare the feature value in an example to the
            # feature value in this question.
            val = example[self.column]
            if is_numeric(val):
                return val >= self.value
            else:
                return val == self.value

        def __repr__(self):
            # This is just a helper method to print
            # the question in a readable format.
            condition = ""==""
            if is_numeric(self.value):
                condition = "">=""
            return ""Is %s %s %s?"" % (
                labels[self.column], condition, str(self.value))

    def partition(self, rows, question):
        """"""Partitions a dataset.

        For each row in the dataset, check if it matches the question. If
        so, add it to 'true rows', otherwise, add it to 'false rows'.
        """"""
        true_rows, false_rows = [], []
        for row in rows:
            if question.match(row):
                true_rows.append(row)
            else:
                false_rows.append(row)
        return true_rows, false_rows

    def gini(self, rows):
        """"""Calculate the Gini Impurity for a list of rows.

        There are a few different ways to do this, I thought this one was
        the most concise. See:
        https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity
        """"""
        counts = class_counts(rows,self.T_index)
        impurity = 1
        for lbl in counts:
            prob_of_lbl = counts[lbl] / float(len(rows))
            impurity -= prob_of_lbl**2
        return impurity   
    
    def info_gain(self, left, right, current_uncertainty):
        """"""Information Gain.

        The uncertainty of the starting node, minus the weighted impurity of
        two child nodes.
        """"""
        p = float(len(left)) / (len(left) + len(right))
        return current_uncertainty - p * self.gini(left) - (1 - p) * self.gini(right)
    
    def find_best_split(self, rows):
        """"""Find the best question to ask by iterating over every feature / value
        and calculating the information gain.""""""

        best_gain = 0  # keep track of the best information gain
        best_question = None  # keep train of the feature / value that produced it
        current_uncertainty = self.gini(rows)

        
        range_indices = list(range(0, len(rows[0])))
        range_indices.remove(self.T_index)
        indices = range_indices
                
        if indices is None: print(""Indices are None"")
        
        # picks random features to analyze.
        # excludes the feature that is being predicted on
        if self.num_Features is not None:
            indices = random.sample(indices, self.num_Features)
        
        for col in indices:  # for each feature

            values = set([row[col] for row in rows])  # unique values in the column

            for val in values:  # for each unique value

                question = Question(col, val)

                # try splitting the dataset
                true_rows, false_rows = self.partition(rows, question)

                # Skip this split if it doesn't divide the dataset.
                if len(true_rows) == 0 or len(false_rows) == 0:
                    continue

                # Calculate the information gain from this split
                gain = self.info_gain(true_rows, false_rows, current_uncertainty)

                # You actually can use '>' instead of '>=' here
                # but I wanted the tree to look a certain way for our
                # toy dataset.
                if gain >= best_gain:
                    best_gain, best_question = gain, question

        return best_gain, best_question
    
    class Leaf:
        """"""A Leaf node classifies data.

        This holds a dictionary of class (e.g., ""Apple"") -> number of times
        it appears in the rows from the training data that reach this leaf.
        """"""

        def __init__(self, rows):

            self.predictions = class_counts(rows,T_index)        # all the prediction info
            self.class_prediction = self._get_class_prediction() # leaf prediction

        def format_predictions(self):
            '''this is used in the nodes'''
            formatted = ""\n""
            for key in self.predictions:
                formatted += str(key) + "" in node="" + str(self.predictions[key])+""\n""
            return formatted

        def _get_class_prediction(self):
            # invert dictionary
            inverted = dict((self.predictions[keys], keys) for keys in self.predictions)

            # find max value
            max_val = max(inverted.keys())

            #guess the class with the most votes
            return inverted[max_val]


        def __repr__(self):
            return str(self.predictions)

    class Decision_Node:
        """"""A Decision Node asks a question.

        This holds a reference: 
        * The question
        * Two child nodes
        * Information Gain 
        """"""

        def __init__(self,
                     question,
                     gain,
                     gi,
                     true_branch,
                     false_branch):

            self.question = question
            self.gain     = gain
            self.gi       = gi

            self.true_branch  = true_branch
            self.false_branch = false_branch
         
    def build_tree(self, rows, current_depth=0):
        """"""Builds the tree.

        Rules of recursion: 1) Believe that it works. 2) Start by checking
        for the base case (no further information gain). 3) Prepare for
        giant stack traces.
        """"""
        
        #update my current depth
        current_depth += 1
        
        if current_depth == self.max_depth:
            return Leaf(rows)
        
        # Try partitioing the dataset on each of the unique attribute,
        # calculate the information gain,
        # and return the question that produces the highest gain.
        gain, question = self.find_best_split(rows)

        # Base case: no further info gain
        # Since we can ask no further questions,
        # we'll return a leaf.
        if gain == 0:
            return Leaf(rows)

        # If we reach here, we have found a useful feature / value
        # to partition on.
        true_rows, false_rows = self.partition(rows, question)

        # Recursively build the true branch.
        true_branch = self.build_tree(true_rows, current_depth)

        # Recursively build the false branch.
        false_branch = self.build_tree(false_rows, current_depth)

        # Return a Question node.
        # This records the best feature / value to ask at this point,
        # as well as the branches to follow
        # dependingo on the answer.
        return Decision_Node(question, gain, gini(rows,self.T_index), true_branch, false_branch)

    def print_tree(self,node=None): 
        '''
        creates a visual representation of the decision tree graph, starting from whatever node is passed in.
        * It is important to note that some characters, such as "":"" wont display properly in the nodes.
        '''
        if node is None:
            node = self.tree
        
        import pydot
        from IPython.display import Image, display

        #create a graph with a root node:
        G = pydot.Dot(graph_type=""digraph"")
        label = str(node.question)+\
                ""\ninfo gain=""+str(round(node.gain,2))+\
                ""\ngini=""+str(round(node.gi,2))
        root_node = pydot.Node(label,style=""filled"", fillcolor=""red"")
        G.add_node(root_node)

        #call print function on leaf nodes

        leaf_count = [1] # I use a list so that the leaf_count is passed by refference

        self.print_node(node.true_branch,  G, root_node, leaf_count, truth=""True"")
        self.print_node(node.false_branch, G, root_node, leaf_count)

        im = Image(G.create_png())
        display(im)

        print(""KEY:"")
        print(""* Root Node:      Red"")
        print(""* Decision Nodes: Green"")
        print(""* Leaf Nodes:     Yellow"")

    def print_node(self, node, graph, graph_node, leaf_count,truth=""False""):
        '''
        This is a resursive function that gets called by print_tree to display a decision tree
        '''
        # Base case: we've reached a leaf
        if isinstance(node, Leaf):

            label = ""Leaf-""+str(leaf_count[0])+\
                    ""\nprediction=""+str(node.class_prediction)+\
                    node.format_predictions()

            leaf_node = pydot.Node(label,style=""filled"", fillcolor=""yellow"")
            leaf_count[0] = leaf_count[0]+1
            graph.add_node(leaf_node)
            edge = pydot.Edge(graph_node,leaf_node,label=truth)
            graph.add_edge(edge)
            return

        # Print Node Information
        label = str(node.question)+\
                ""\ninfo gain=""+str(round(node.gain,2))+\
                ""\ngini=""+str(round(node.gi,2))
        decision_node = pydot.Node(label,style=""filled"", fillcolor=""green"")
        graph.add_node(decision_node)
        edge = pydot.Edge(graph_node,decision_node, label=truth)
        graph.add_edge(edge)

        # Call this function recursively on the true branch
        #print(spacing + '--> True:')
        print_node(node.true_branch,  graph, decision_node, leaf_count, truth=""True"")

        # Call this function recursively on the false branch
        #print (spacing + '--> False:')
        print_node(node.false_branch, graph, decision_node, leaf_count)
        
    def get_leaf(self, row, node=None):
        """"""Gets the leaf that corresponds to a particular row""""""

        if node is None:
            node = self.tree
            
        # Base case: we've reached a leaf
        if isinstance(node, Leaf):
            return node

        # Decide whether to follow the true-branch or the false-branch.
        # Compare the feature / value stored in the node,
        # to the example we're considering.
        if node.question.match(row):
            return self.get_leaf(row, node.true_branch)
        else:
            return self.get_leaf(row, node.false_branch)
        
    def classify(self,row,node=None):
        '''classifies a particular row, starting from a particular node'''
        if node is None:
            node = self.tree
            
        return self.get_leaf(row,node).class_prediction
    
    def classify_all(self, data, node=None):
        '''Returns a list that describes that category that reach row in the data was classified as'''
        if node is None:
            node = self.tree
            
        return [self.classify(row,node) for row in data] 
    
    def classification_results(self, data, classifications, node=None):
        '''returns a boolean list describing whether or not the classifcations were done correctly'''
        
        if node is None:
            node = self.tree
            
        results = []
        for i in range(len(data)):
            results.append(data[i][self.T_index] == classifications[i])

        return results
    
    def train(self):
        # training statistics
        self.training_classifications = self.classify_all(self.training_data)
        self.training_booleans = self.classification_results(self.training_data, self.training_classifications)
        self.training_per = sum(self.training_booleans)/len(self.training_booleans)
    
    def use(self,testing_data):
        
        self.testing_data = testing_data
        self.testing_classifications = self.classify_all(self.testing_data)
        self.testing_booleans = self.classification_results(self.testing_data, self.testing_classifications)
        self.testing_per = sum(self.testing_booleans)/len(self.testing_booleans)
    
    def print_stats(self):
        print(""\nTree Stats:"")
        print(str(round(100*self.training_per,2))   +""% of the training data was classified correctly"")
        if self.testing_data is not None:
            print(str(round(100*self.testing_per,2))+""% of the testing  data was classified correctly"")",0.5213522911,
2399,training a decision tree model,"from sklearn.tree import DecisionTreeRegressor

# Fit decision tree regressor to get max depth and max number features 
dr = DecisionTreeRegressor().fit(X_train, y_train)

# Create a range of depth and number of features for parameter grid to test 
param_grid = {'max_depth': range(1, dr.tree_.max_depth + 1, 2), 
              'max_features': range(1, len(dr.feature_importances_) + 1)}

# Tune depth and feature number hyperparameters using GridSearchCV
GR_sugar = GridSearchCV(DecisionTreeRegressor(random_state = 42), 
                        param_grid = param_grid, 
                        scoring = 'neg_mean_squared_error', 
                        n_jobs = -1) 

GR_sugar = GR_sugar.fit(X_train, y_train)",0.5213060975,
2399,training a decision tree model,"from sklearn.tree import DecisionTreeRegressor

dr = DecisionTreeRegressor().fit(X_train, y_train)

param_grid = {'max_depth':range(1, dr.tree_.max_depth+1, 2),
              'max_features': range(1, len(dr.feature_importances_)+1)}

GR_sugar = GridSearchCV(DecisionTreeRegressor(random_state=42),
                     param_grid=param_grid,
                     scoring='neg_mean_squared_error',
                      n_jobs=-1)

GR_sugar = GR_sugar.fit(X_train, y_train)",0.5213060975,
2399,training a decision tree model,"# Try Decision tree classifier on the above data

dtree = tree.DecisionTreeClassifier(random_state=20160121, criterion='entropy')",0.5203764439,
2399,training a decision tree model,"tree_pruned = tree.DecisionTreeClassifier(criterion=""gini"",max_leaf_nodes=5)
tree_pruned = tree_pruned.fit(X_train, y_train)

tree_pruned.tree_.node_count",0.5197076201,
2097,step how much was the revenue for the period in the dataset?,"# Redistribute pool of investment money towards long and short orders 
# to get (near) zero beta
def rebalance(context, data):
    long_secs = context.output[context.output['longs']].index  # Get ticker
    long_weight = 0.5 / len(long_secs)
    
    short_secs = context.output[context.output['shorts']].index
    short_weight = -0.5 / len(short_secs)
    
    for security in long_secs:  # Long security
        if data.can_trade(security):  # If we can trade that security
            order_target_percent(security, long_weight)
            
    for security in short_secs:  # Short security
        if data.can_trade(security):  # If we can trade that security
            order_target_percent(security, short_weight)
    
    # Iterate over positions that we have, check if the ticker exists, whether we 
    # can trade it, and if it does not exist in our long or short securities list.
    for security in context.portfolio.positions:
        if data.can_trade(security) and security not in long_secs and security not in short_secs:
            order_target_percent(security,0)",0.6108421087,
2097,step how much was the revenue for the period in the dataset?,"def compare(group, data):
    return data.groupby([group])['Survived'].sum()*100/data.groupby([group])['Survived'].count()",0.6048532724,
2097,step how much was the revenue for the period in the dataset?,"def f(df,fieldame):
    """"""calculate quota for middle age group based on other data""""""
    pop_tot = df['tot unemp'] / df['tot unemp quote'] * 100 # total population
    pop_cat1 = df['youth unemp'] / df['youth unemp quote'] * 100 # total population in age cat 1 (15-24y)
    pop_cat3 = df['50+ unemp'] / df['50+ unemp quote'] * 100 # total population in age cat 3 (50+y)

    quote_cat2 = (df['25-49y unemp'] / (pop_tot - pop_cat1 - pop_cat3))*100

    pop_cat2 = np.sum(df['25-49y unemp'] / quote_cat2 * 100)
    return quote_cat2

# we first have to sort 
df_mw.sort_values(by=['canton','nationality'],inplace=True)
pct_cat2 = pd.DataFrame(df_mw.groupby('canton').apply(f,'cat')).values

df_mw['25-49y unemp quote'] = pct_cat2",0.5980656147,
2097,step how much was the revenue for the period in the dataset?,"def dist(zipcentroid, precinct_df, N):
    '''
This function is used to calculate the distance between one zipcode centroid
and precinct centroids and select the top N closest precincts. It returns a dataframe
of the top N precincts and their distance to the zipcode centroid.

zipcentroid: the centroid of the zipcode;
precinct_df: dataframe of the precincts;
N: how many precincts we want to return;
'''
    x = precinct_df
    x['distance'] = x['geometry'].apply(lambda j: 
                                        j.centroid.distance(zipcentroid))
    y = x.sort_values(by = 'distance', 
                      ascending = True).reset_index().iloc[:N,:]
    return y",0.597525537,
2097,step how much was the revenue for the period in the dataset?,"def com_baseline(user, item, df):
    all_mean = df['stars'].mean()
    user_mean = df[df['user_id'] == user]['user_avg'].mean()
    item_mean = df[df['business_id'] == item]['business_avg'].mean()
    return all_mean +(user_mean - all_mean) + (item_mean - all_mean)",0.5955117345,
2097,step how much was the revenue for the period in the dataset?,"def get_ratio_currbalance_creditlimit(account_data):
    ratio_currbalance_creditlimit = account_data.groupby('customer_no')['cur_balance_amt', 'creditlimit'].agg(
        'sum').reset_index()
    ratio_currbalance_creditlimit['ratio_currbalance_creditlimit'] = ratio_currbalance_creditlimit['cur_balance_amt'] / \
                                                                     ratio_currbalance_creditlimit['creditlimit']
    ratio_currbalance_creditlimit = ratio_currbalance_creditlimit.drop(['cur_balance_amt', 'creditlimit'], axis=1)
    return ratio_currbalance_creditlimit
    
ratio_currbalance_creditlimit = get_ratio_currbalance_creditlimit(account_train_data)
ratio_currbalance_creditlimit.head(10)",0.5951941013,
2097,step how much was the revenue for the period in the dataset?,"def calculate_dmag_stats(decaps_lsst,visit,bins):
    mask_distance = decaps_lsst['d2darcsec'].data < 0.5
    x = decaps_lsst[mask_distance]['decaps_mag']
    y = decaps_lsst[mask_distance]['lsst_mag']
    err_decaps = decaps_lsst[mask_distance]['decaps_magerr']
    err_lsst = decaps_lsst[mask_distance]['lsst_magerr']
    dm = y-x 
    
    # calculate the median delta mag per bin 
    median = bs(x,dm, bins=bins,statistic='median')
    med_dmag = median.statistic
    
    # calculate the spread of delta mag  : sigmaG 
    spread_sig = bs(x,dm, bins=bins,statistic=sigmaG)
    sig_dmag = spread_sig.statistic
    
    # calculate the median error 
    median_err_decaps = bs(x,err_decaps, bins=bins, statistic='median')
    median_err_lsst = bs(x,err_lsst, bins=bins, statistic='median')
    
    dic = {'median_dmag':med_dmag, 'sigmaG_dmag':sig_dmag,
        'med_err_decaps':median_err_decaps.statistic, 
        'med_err_lsst':median_err_lsst.statistic}
    return dic",0.5946845412,
2097,step how much was the revenue for the period in the dataset?,"# Cohen's D
def CohensD(DF, variable):
    cohensd = (DF[variable][DF['Group']=='Monolingual'].mean() - DF[variable][DF['Group']=='Bilingual'].mean()) / (sqrt((DF[variable][DF['Group']=='Monolingual'].std() ** 2 + DF[variable][DF['Group']=='Bilingual'].std() ** 2) / 2))
    return cohensd

# T-test
def QuickTtest(DF, variable):
    return stats.ttest_ind(DF[variable][DF['Group']=='Monolingual'], DF[variable][DF['Group']=='Bilingual'])

# T-test with D
def TtestD(DF, variable):
    output = np.array(QuickTtest(DF, variable))
    output = np.append(output, CohensD(DF, variable))
    output = [round(output[0], 2), round(output[1], 5), round(output[2], 2)]
    return output",0.5940625072,
2097,step how much was the revenue for the period in the dataset?,"def separate_rt(dataframef, dataframeh): 
    
    standard_rt_f = dataframef[dataframef[""policy_id""] == 280].mean()
    executive_rt_f = dataframef[dataframef[""policy_id""] == 314].mean()
    
    standard_rt_h = dataframeh[dataframeh[""policy_id""] == 280].mean()
    executive_rt_h = dataframeh[dataframeh[""policy_id""] == 314].mean()
    
    return standard_rt_f, executive_rt_f, standard_rt_h, executive_rt_h

def separate_ct(dataframef, dataframeh): 
    
    standard_ct_f = dataframef[dataframef[""policy_id""] == 345].mean()
    executive_ct_f = dataframef[dataframef[""policy_id""] == 346].mean()
    
    standard_ct_h = dataframeh[dataframeh[""policy_id""] == 345].mean()
    executive_ct_h = dataframeh[dataframeh[""policy_id""] == 346].mean()
    
    return standard_ct_f, executive_ct_f, standard_ct_h, executive_ct_h",0.5934021473,
2097,step how much was the revenue for the period in the dataset?,"def disparity(gr):
    # Compute the spread of gr['gdp']: s
    s = gr['gdp'].max() - gr['gdp'].min()
    # Compute the z-score of gr['gdp'] as (gr['gdp']-gr['gdp'].mean())/gr['gdp'].std(): z
    z = (gr['gdp'] - gr['gdp'].mean())/gr['gdp'].std()
    # Return a DataFrame with the inputs {'z(gdp)':z, 'regional spread(gdp)':s}
    return pd.DataFrame({'z(gdp)':z , 'regional spread(gdp)':s})

gapminder = pd.read_csv('gapminder.csv')
gapminder_2010 = gapminder[gapminder['Year'] == 2010]
gapminder_2010 = gapminder_2010.set_index('Country')

# Group gapminder_2010 by 'region': regional
regional = gapminder_2010.groupby('region')

# Apply the disparity function on regional: reg_disp
reg_disp = regional.apply(disparity)

# Print the disparity of 'United States', 'United Kingdom', and 'China'
print(reg_disp.loc[['United States','United Kingdom','China']])",0.59220469,
2610,working with doi lists and article level metrics,"for stock in stocks:
    new_stock = quandl.get('WIKI/'+stock,
    start_date=""2012-01-01"")['Adj. Close']
    stock_prices[stock] = new_stock",0.4570485353,
2610,working with doi lists and article level metrics,FC.metrics['good_sol'],0.4537169933,
2610,working with doi lists and article level metrics,FC.metrics['rot_ants'],0.4537169933,
2610,working with doi lists and article level metrics,FC.metrics['bad_ants'],0.4537169933,
2610,working with doi lists and article level metrics,train.metrics['roc_auc'],0.4537169933,
2610,working with doi lists and article level metrics,train_custom.metrics['roc_auc'],0.4537169933,
2610,working with doi lists and article level metrics,"print('{0:.2f} % of the data is ""protocol"" text. Recall that protocol text is virtually identical and always begins with ""Congress has the power to....""'.format(100*len(df[df.protocol==True]['protocol'])/len(df)))",0.4525398612,
2610,working with doi lists and article level metrics,"sess.cardinality.summarize(
  table={""name"":indata}, 
  cardinality={""name"":""data_card"", ""replace"":True}
)

tbl_data_card = sess.CASTable('data_card').query('_NMISS_ > 0')

print(""Data Summary"".center(80, '-')) # print title

df_data_card = tbl_data_card.to_frame(fetchvars=['_VARNAME_', '_NMISS_', '_NOBS_'])
df_data_card['PERCENT_MISSING'] = (df_data_card['_NMISS_'] / df_data_card['_NOBS_']) * 100

tbl_forplot = pd.Series(list(df_data_card['PERCENT_MISSING']), index=list(df_data_card['_VARNAME_']))
ax = tbl_forplot.plot(
  kind='bar', 
  title='Percentage of Missing Values',
  figsize=(11,5)
)
ax.set_ylabel('Percent Missing')
ax.set_xlabel('Variable Names');",0.4508523643,
2610,working with doi lists and article level metrics,"print codonprop.get(""ACT"", 1.0/64)",0.4472221434,
2610,working with doi lists and article level metrics,"print(""%s: %.2f"" % (model.metrics_names[1], scores[1]*100))",0.4470040798,
1138,loss function,"def regularizer(W_e, W_d):
    with tf.variable_scope(""regularizer""):
        regularizer = tf.nn.l2_loss(W_e) + tf.nn.l2_loss(W_d)
    return regularizer",0.498783201,
1138,loss function,"def get_train_op(loss):
    initial_learning_rate = 0.05
    train_op = tf.contrib.layers.optimize_loss(
        loss, tf.contrib.framework.get_global_step(),
        optimizer = tf.train.AdamOptimizer,
        learning_rate = initial_learning_rate)
    return train_op",0.498270154,
1138,loss function,"def get_train_op_fn(loss, params):
    return tf.contrib.layers.optimize_loss(
        loss=loss,
        global_step=tf.contrib.framework.get_global_step(),
        optimizer=tf.train.AdamOptimizer,
        learning_rate=params.learning_rate
)",0.4955507517,
1138,loss function,"def get_train_op_fn(loss, params):
    """"""Get the training Op.

    Args:
         loss (Tensor): Scalar Tensor that represents the loss function.
         params (HParams): Hyperparameters (needs to have `learning_rate`)

    Returns:
        Training Op
    """"""
    return tf.contrib.layers.optimize_loss(
        loss=loss,
        global_step=tf.contrib.framework.get_global_step(),
        optimizer=tf.train.AdamOptimizer,
        learning_rate=params.learning_rate
    )",0.4878758192,
1138,loss function,"def l2_loss(logits, labels):
    """"""Euclidean distance or L2-norm: sqrt(sum((logits - labels)^2))""""""
    labels = tf.to_float(labels)
    return tf.nn.l2_loss(logits - labels, name='l2_loss')

with tf.name_scope('error'):
    error = l2_loss(output, y_placeholder)  # predicted vs. true labels
    tf.summary.scalar(error.op.name, error)  # write error (loss) to log",0.48030442,
1138,loss function,"def EvalLogLoss(predicted, actual):
    return sklearn.metrics.log_loss(actual, predicted, eps=1e-15)",0.4770131111,
1138,loss function,"def define_cost(y_true, y_pred, weights=1.):
    return tf.losses.log_loss(y_true, y_pred, weights)",0.4756718874,
1138,loss function,"def compute_loss(y,pred,parameters):
    """"""
    softmax cross entropy loss with l2 regularization 
    params: placeholder: y (labels) and pred (predicition) and Variable: weights. 
    returns: loss tensor object
    """"""
    beta = 0.000005
    regularizer = (beta * tf.nn.l2_loss(parameters[""w1""]) + 
                   beta * tf.nn.l2_loss(parameters[""w2""]) +
                   beta * tf.nn.l2_loss(parameters[""w3""]) +
                   beta * tf.nn.l2_loss(parameters[""w4""]))
    
    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=pred)) + regularizer  # softmax ce loss",0.4750372171,
1138,loss function,"def perform_optimization(self):
    '''
    Specifies train_op that optimizes self.loss
    '''
    
    self.train_op = tf.contrib.layers.optimize_loss(
        loss = self.loss,
        global_step = tf.train.get_global_step(),
        learning_rate = self.learning_rate_ph,
        optimizer = 'Adam',
        clip_gradients= 1.0,
        )",0.4714061022,
1138,loss function,"def regularizer(wd):
    fc1_w = tf.trainable_variables(scope='fc1/Weights')[0]
    wd_fc1 = tf.multiply(tf.nn.l2_loss(fc1_w), wd, name='fc1/weight_loss')
    tf.summary.scalar(""fc1_decay"",wd_fc1)

    fc2_w = tf.trainable_variables(scope='fc2/Weights')[0]
    wd_fc2 = tf.multiply(tf.nn.l2_loss(fc2_w), wd, name='fc2/weight_loss')
    tf.summary.scalar(""fc2_decay"",wd_fc2)
    
    return wd_fc1+wd_fc2",0.4672166407,
1155,makes a list of even numbers from to using a python list comprehension,"# Transforming/filtering a list is so common, that it has its own syntax
[x**2 + 2 for x in inputs if not x % 2]",0.5829483271,
1155,makes a list of even numbers from to using a python list comprehension,[x**2 for x in l if x%2==0],0.5781835914,
1155,makes a list of even numbers from to using a python list comprehension,[i**2 for i in a if i % 2 == 0],0.5781835914,
1155,makes a list of even numbers from to using a python list comprehension,"# List comprehensions:
[i**2 for i in l if i % 2 == 0]",0.5781835914,
1155,makes a list of even numbers from to using a python list comprehension,[a*a for a in x if a%2 == 0],0.5739545226,
1155,makes a list of even numbers from to using a python list comprehension,"[x * (x+1)//2 for x in range(1,10)]",0.572265625,
1155,makes a list of even numbers from to using a python list comprehension,"#find odd numbers using modulo
[i*2 for i in example if i%2]",0.568251133,
1155,makes a list of even numbers from to using a python list comprehension,"[[o,o*o] for o in my_list if o%2==1]",0.5653401613,
1155,makes a list of even numbers from to using a python list comprehension,"[10*i+j for i in [1,2,3] if i%2==1 for j in [4,5,7] if j >= i+4] # keep odd i and  j larger than i+4 only",0.5651368499,
1155,makes a list of even numbers from to using a python list comprehension,"[[x, x//2, x*2] for x in range(-6,7,2) if x > 0]  # range from -6 upto but not including 7, by two steps,
                                                 # leave the first alone, divide the second by 2, and square the third
                                                 # and only show numbers that are positive (more than 0)",0.5626819134,
164,calculating gamma and nu for oneclass svm,"hw2_module.gamma = 0.0568
hw2_module.p = 2.9025
hw2_module.beta_1 = -12.6051
hw2_module.beta_2 = -2.0992
hw2_module.beta = np.array([hw2_module.beta_1, hw2_module.beta_2])",0.4988019466,
164,calculating gamma and nu for oneclass svm,"from scipy.special import gamma, factorial",0.4984120727,
164,calculating gamma and nu for oneclass svm,"hw2_module.gamma = 0.0048375
hw2_module.p = 1.31377
hw2_module.beta_1 = 69.75265
hw2_module.beta_2 = -0.4082
hw2_module.beta = np.array([hw2_module.beta_1, hw2_module.beta_2])",0.4982978106,
164,calculating gamma and nu for oneclass svm,"# Fig 7
reload(sf)

# These are simple coordinates where the points meet
# The values were taken from the paper and are stored in oscFit
# All are evaluated at logE equal:
x1e = sf.x1e
x2e = sf.x2e

# For NuMu/NuE
e1max_mu_e = sf.e1max_mu_e
e2max_mu_e = sf.e2max_mu_e

# For NuMu/NuMuBar
e1max_mu = sf.e1max_mu
e2max_mu = sf.e2max_mu

# For NuE/NuEBar
e1max_e = sf.e1max_e + sf.e1max_mu
e2max_e = sf.e2max_e + sf.e2max_mu


# NuMu uncertainty
plt.figure()
plt.plot(np.log10(numue_ratio[::2]), numue_ratio[1::2], 'xg',
         label='NuMu/NuE')
plt.plot(np.log10(numu_eratio[::2]), numu_eratio[1::2], 'xk',
         label='NuMu/NuMuBar')
plt.plot(np.log10(nue_eratio[::2]), nue_eratio[1::2], 'xr',
         label='NuE/NuEBar')

x = np.log10(numue_ratio[::2])
y = np.log10(numue_ratio[1::2])
new_x = np.linspace(0, 3, 1000)
# Using the LogLogParam fcn in oscFit

plt.plot(new_x, 100*sf.LogLogParam(10**new_x, e1max_mu_e, 
                                   e2max_mu_e, x1e, x2e), '--g')
plt.plot(new_x, 100*sf.LogLogParam(10**new_x, e1max_mu, 
                                   e2max_mu, x1e, x2e), '--k')
plt.plot(new_x, 100*sf.LogLogParam(10**new_x, e1max_e, 
                                   e2max_e, x1e, x2e), '--r')


plt.xlabel('log10(Energy/GeV)')
plt.ylabel('Uncertainty (%)')
plt.yscale('log')
plt.xlim([0.2,3])
plt.ylim([.3, 100])

plt.legend(loc=0)

plt.text(1.2, 0.6, 'Points from paper, lines from oscFit fcns')",0.4976062179,
164,calculating gamma and nu for oneclass svm,"hh = alos2.hh_gamma0
hv = alos2.hv_gamma0
vh = s1a.vv_gamma0
vv = s1a.vv_gamma0",0.494479239,
164,calculating gamma and nu for oneclass svm,"import numpy as np
import numba
from scipy.special import factorial, comb, gamma",0.4919452071,
164,calculating gamma and nu for oneclass svm,from scipy.stats import gamma as sp_gamma,0.490432322,
164,calculating gamma and nu for oneclass svm,"# Pohlhausen Boundary Layer class
class Pohlhausen:
    def __init__(self,panels,nu):
        self.u_e = [abs(p.gamma) for p in panels]   # tangential velocity
        self.s = numpy.empty_like(self.u_e)         # initialize distance array
        self.s[0] = panels[0].S
        for i in range(len(self.s)-1):              # fill distance array
            self.s[i+1] = self.s[i]+panels[i].S+panels[i+1].S           
        ds = numpy.gradient(self.s)     
        self.du_e = numpy.gradient(self.u_e,ds)     # compute velocity gradient

        self.nu = nu                                # kinematic viscosity
        self.xc = [p.xc for p in panels]            # x and ...
        self.yc = [p.yc for p in panels]            # y locations
        
    def march(self):
        # march down the boundary layer until separation
        from BoundaryLayer import march
        self.delta,self.lam,self.iSep = march(self.s,self.u_e,self.du_e,self.nu)

        # interpolate values at the separation point
        def sep_interp(y): return numpy.interp(    # interpolate function
            12,-self.lam[self.iSep:self.iSep+2],y[self.iSep:self.iSep+2])
        self.s_sep = sep_interp(self.s)
        self.u_e_sep = sep_interp(self.u_e)
        self.x_sep = sep_interp(self.xc)
        self.y_sep = sep_interp(self.yc)
        self.delta_sep = sep_interp(self.delta)",0.4894189239,
164,calculating gamma and nu for oneclass svm,"from scipy.special import gamma as gammafn
def gamma(x, alpha, beta):
    return ((beta**alpha)/gammafn(alpha)) * x**(alpha-1) * np.exp(-beta*x)",0.4890105724,
164,calculating gamma and nu for oneclass svm,"s = model2_cov.beam_model.sigma[0]
nu1 = model2_cov.nu[0]
nu2= model2_cov.nu[1]
u = model2_cov.u

frontbit = (2*np.pi)**(2-kappa) * model2_cov.source_counts.total_flux_density[0]**2 * nu1**(-kappa) * s**4 / (nu1*nu2)**(2+model2_cov.source_counts.spectral_index)
otherfront = 2*np.pi * np.exp(-4*np.pi**2 * s**2*u**2)

k = np.arange(20)
y = np.atleast_2d(4*np.pi**2*s**2*u*(1+nu2/nu1))

sumbit = y.T**(2*k) * (2*np.pi**2*s**2*(1+nu2**2/nu1**2))**(-(2*k+2-kappa)/2) * gamma((2*k+2-kappa)/2)/(4**k * factorial(k)**2 * 2)

res = frontbit*otherfront*np.sum(sumbit,axis=1)",0.4888246655,
903,imports and data,import data,0.4211772382,
903,imports and data,import datasets,0.418789953,
903,imports and data,"import copy

def has_passed(special_add, tests):
    for test in tests:
        current_func = copy.deepcopy(test.function)
        current_func.func_globals['special_add'] = special_add
        try:
            current_func()
        except Exception as e:
            return False
    return True",0.4151695371,
903,imports and data,"import sys 
sys.path.append('./src')
import DataLoading

features, trip_sources, trip_dests = DataLoading.load_ride_data()",0.4114255905,
903,imports and data,from datasets import *,0.4112272263,
903,imports and data,"def load_lda_models(models, model_names):
    for i in model_names:
        models.append(gensim.models.ldamodel.LdaModel.load(model_path + i))",0.4107144475,
903,imports and data,"class AnalyzeData:
    def __init__(self, fname):
        self.fname = fname
        self.import_data()
        self.analyze_data()
        
    def import_data(self):
        valid_extensions = {'.csv': self._import_csv,
                            '.tab': self._import_tab,
                            '.dat': self._import_dat}
        file_extension = os.path.splitext(self.fname)[-1]
        importer_function = valid_extensions[file_extension]
        importer_function()

    def _import_csv(self):
        print(""Import comma-separated data"")
        # many lines of code, perhaps with function calls
        
    def _import_tab(self):
        print(""Import tab-separated data"")
        # many lines of code, perhaps with function calls
        
    def _import_dat(self):
        print(""Import data with | delimiters (old-school)"")
        # many lines of code, perhaps with function calls
        
    def analyze_data(self):
        """"""Do some super-awesome data analysis!""""""

        
a = AnalyzeData('data.tab')
# a = AnalyzeData('data.xls')   # unknown file type, throws exception!",0.4095421433,
903,imports and data,"import types
def imports():
    for name, val in globals().items():
        if isinstance(val, types.ModuleType):
            yield val.__name__
            
import pdb",0.4087939858,
903,imports and data,"# Import model files
import model
import datasets",0.408375442,
903,imports and data,from data import *,0.4061383009,
2120,step ops it seems it is going only until index is it true?,"def invL(F):
    return sympy.inverse_laplace_transform(F, s, t)",0.4371586442,
2120,step ops it seems it is going only until index is it true?,"loop_remain = iter_number - 1  # remove the first time
index = 2 # Start from the second frame

while loop_remain != 0 :
    beginning = calculate_active(current_frame=beginning, index_next_frame=index)
    loop_remain -= 1
    index += 1",0.4263776243,
2120,step ops it seems it is going only until index is it true?,"j = 1
for i in range(time_steps):
    # CALL THE forwardcn FUNCTION TO PROPAGATE 1 TIME STEP, ASSIGN THE RESULT TO PsiaR (1 LINE)
    PsiaR = 
    if i%niter == 0:
        # SAVE THE NEW PsiaR VALUE TO THE j-th ELEMENT OF THE psitaR MATRIX (1 LINE)
        psitaR[j, :] = 
        # COMPUTE THE AUTOCORRELATION FUNCTION FOR THE j-th TIME STEP (1 LINE)  
        Caa[j] = 
        j+=1",0.4251080751,
2120,step ops it seems it is going only until index is it true?,"for i in range(n_turns):
    for m in machine.transverse_map:
        x_rec[i, :] = bunch.x
        y_rec[i, :] = bunch.y
        m.track(bunch)",0.4244825244,
2120,step ops it seems it is going only until index is it true?,"# Remember: 
# dp.add_transition(stage, from_state, decision, to_state, probability, contribution)

# Transition probabilities from (A, 6, d) up to stage T - 2
# Decision: 2-point conversion
for t in range(T - 1):
    for d in range(-20, 21):

        if d - 4 <= -20:
            dp.add_transition(t, ('A', 6, d), 2, ('B', 6, -20), (1 - qA) * tB + qA * tB, 0)
        else:
            dp.add_transition(t, ('A', 6, d), 2, ('B', 6, max(d - 6, -20)), (1 - qA) * tB, 0)
            dp.add_transition(t, ('A', 6, d), 2, ('B', 6, max(d - 4, -20)), qA * tB, 0)
        
        if d - 1 <= -20:
            dp.add_transition(t, ('A', 6, d), 2, ('B', 3, -20), (1 - qA) * gB + qA * gB, 0)
        else:
            dp.add_transition(t, ('A', 6, d), 2, ('B', 3, max(d - 3, -20)), (1 - qA) * gB, 0)
            dp.add_transition(t, ('A', 6, d), 2, ('B', 3, max(d - 1, -20)), qA * gB, 0)
            
        if d >= 20:
            dp.add_transition(t, ('A', 6, d), 2, ('B', 0, 20),  (1 - qA) * zB + qA * zB, 0)
        else:
            dp.add_transition(t, ('A', 6, d), 2, ('B', 0, min(d, 20)), (1 - qA) * zB, 0)
            dp.add_transition(t, ('A', 6, d), 2, ('B', 0, min(d + 2, 20)), qA * zB, 0)",0.4243071079,
2120,step ops it seems it is going only until index is it true?,"# loop 1
while our_variable < 5: 
    print (""our_variable has the value=>"",our_variable) # we are in loop 1
    our_variable = our_variable + 1                     # we are in loop 1

# we are now outside of loop 1

print (""\n we are now assigning new value of 6 to our_varible"")
our_variable = 6
# loop 2
while our_variable < 5:
    print (our_variable) # we are now in loop 2
    
print (""\n wonder why it didn't print our new value?"")",0.42394346,
2120,step ops it seems it is going only until index is it true?,"# Remember: 
# dp.add_transition(stage, from_state, decision, to_state, probability, contribution)

# Transition probabilities from (A, 6, d) up to stage T - 2
# Decision: 1-point conversion
for t in range(T - 1):
    for d in range(-20, 21):

        if d - 5 <= -20:
            dp.add_transition(t, ('A', 6, d), 1, ('B', 6, -20), (1 - pA) * tB + pA * tB, 0)
        else:
            dp.add_transition(t, ('A', 6, d), 1, ('B', 6, max(d - 6, -20)), (1 - pA) * tB, 0)
            dp.add_transition(t, ('A', 6, d), 1, ('B', 6, max(d - 5, -20)), pA * tB, 0)
        
        if d - 2 <= -20:
            dp.add_transition(t, ('A', 6, d), 1, ('B', 3, -20), (1 - pA) * gB + pA * gB, 0)
        else:
            dp.add_transition(t, ('A', 6, d), 1, ('B', 3, max(d - 3, -20)), (1 - pA) * gB, 0)
            dp.add_transition(t, ('A', 6, d), 1, ('B', 3, max(d - 2, -20)), pA * gB, 0)
        
        if d >= 20:
            dp.add_transition(t, ('A', 6, d), 1, ('B', 0, 20), (1 - pA) * zB + pA * zB, 0)
        else:
            dp.add_transition(t, ('A', 6, d), 1, ('B', 0, min(d, 20)), (1 - pA) * zB, 0)
            dp.add_transition(t, ('A', 6, d), 1, ('B', 0, min(d + 1, 20)), pA * zB, 0)",0.4238891602,
2120,step ops it seems it is going only until index is it true?,"# Transition probabilities from (B, 6, d) up to stage T - 2
for t in range(T - 1):
    for d in range(-20, 21):
        if d + 4 >= 20:
            dp.add_transition(t, ('B', 6, d), 'none', ('A', 6, 20), (1 - b1*pB - b2*qB) * tA + b1*pB*tA + b2*qB*tA, 0)
        elif d + 5 >= 20:
            dp.add_transition(t, ('B', 6, d), 'none', ('A', 6, 20), (1 - b1*pB - b2*qB) * tA + b1*pB*tA, 0)    
            dp.add_transition(t, ('B', 6, d), 'none', ('A', 6, min(d + 4, 20)), b2*qB*tA, 0)
        else:
            dp.add_transition(t, ('B', 6, d), 'none', ('A', 6, min(d + 6, 20)), (1 - b1*pB - b2*qB) * tA, 0)
            dp.add_transition(t, ('B', 6, d), 'none', ('A', 6, min(d + 5, 20)), b1*pB*tA, 0)
            dp.add_transition(t, ('B', 6, d), 'none', ('A', 6, min(d + 4, 20)), b2*qB*tA, 0)

        if d + 1 >= 20:
            dp.add_transition(t, ('B', 6, d), 'none', ('A', 3, 20), (1 - b1*pB - b2*qB) * gA + b1*pB*gA + b2*qB*gA, 0)
        elif d + 2 >= 20:
            dp.add_transition(t, ('B', 6, d), 'none', ('A', 3, 20), (1 - b1*pB - b2*qB) * gA + b1*pB*gA, 0)    
            dp.add_transition(t, ('B', 6, d), 'none', ('A', 3, min(d + 1, 20)), b2*qB*gA, 0)
        else:
            dp.add_transition(t, ('B', 6, d), 'none', ('A', 3, min(d + 3, 20)), (1 - b1*pB - b2*qB) * gA, 0)
            dp.add_transition(t, ('B', 6, d), 'none', ('A', 3, min(d + 2, 20)), b1*pB*gA, 0)
            dp.add_transition(t, ('B', 6, d), 'none', ('A', 3, min(d + 1, 20)), b2*qB*gA, 0)
            
        if d <= -20:
            dp.add_transition(t, ('B', 6, d), 'none', ('A', 0, -20), (1 - b1*pB - b2*qB) * zA + b1*pB*zA + b2*qB*zA, 0)
        elif d - 1 <= -20:
            dp.add_transition(t, ('B', 6, d), 'none', ('A', 0, d), (1 - b1*pB - b2*qB) * zA, 0)
            dp.add_transition(t, ('B', 6, d), 'none', ('A', 0, -20), b1*pB*zA + b2*qB*zA, 0)    
        else:
            dp.add_transition(t, ('B', 6, d), 'none', ('A', 0, d), (1 - b1*pB - b2*qB) * zA, 0)
            dp.add_transition(t, ('B', 6, d), 'none', ('A', 0, max(d - 1, -20)), b1*pB*zA, 0)
            dp.add_transition(t, ('B', 6, d), 'none', ('A', 0, max(d - 2, - 20)), b2*qB*zA, 0)",0.422172904,
2120,step ops it seems it is going only until index is it true?,"for row in cursor:
    
    # store weather data
    w.get_conditions(row[0],row[1],row[2],row[3])

    # time stamp the zinc plate
    zp.time_domain.append(row[0])

    # zinc plate corrodes from exposure to humidity
    zp.corrode(row[2])

    # zinc plate experiences a runoff event from precipitation
    zp.runoff(row[3])

# set the time stamp for the initial conditions
dt_obj = zp.time_domain[0] - datetime.timedelta(hours=1)
zp.time_domain.insert(0,dt_obj)",0.4202474356,
2120,step ops it seems it is going only until index is it true?,"# Adjust the lower bounds of uptake secretion reactions
# for things that are not in the media
for u in uptake_secretion_reactions:
    is_media_component = False
    for c in uptake_secretion_reactions[u].all_compounds():
        if c in media:
            is_media_component = True
    if not is_media_component:
        reactions[u].lower_bound = 0.0
        uptake_secretion_reactions[u].lower_bound = 0.0",0.4131640196,
861,hypothesis testing with slopes,"class SlopeTest(thinkstats2.HypothesisTest):

    def TestStatistic(self, data):
        x, y = data
        _, slope = thinkstats2.LeastSquares(x, y)
        return slope

    def MakeModel(self):
        _, y = self.data
        self.ybar = y.mean()
        self.res = y - self.ybar

    def RunModel(self):
        x, _ = self.data
        y = self.ybar + np.random.permutation(self.res)
        return x, y
    
ht = SlopeTest((_df['TempAvgF'], _df['duration_minutes']))
pvalue = ht.PValue()
pvalue",0.610440135,
861,hypothesis testing with slopes,"class SlopeTest(ts2.HypothesisTest):
    def TestStatistic(self, data):
        rain, pmt = data
        _, slope = ts2.LeastSquares(rain, pmt)
        return slope
    
    def MakeModel(self):
        _, pmt = self.data
        self.ybar = pmt.mean()
        self.res = pmt - self.ybar
        
    def RunModel(self):
        rain, _ = self.data
        pmt = self.ybar + np.random.permutation(self.res)
        return rain, pmt
    
    def PValue(self, iters=1000):
        """"""Computes the distribution of the test statistic and p-value.

        iters: number of iterations

        returns: float p-value
        """"""
        self.test_stats = [self.TestStatistic(self.RunModel()) 
                           for _ in range(iters)]
        self.test_cdf = ts2.Cdf(self.test_stats)

        count = sum(1 for x in self.test_stats if x <= self.actual)
        return count / iters",0.6083554029,
861,hypothesis testing with slopes,"class SlopeTest(thinkstats2.HypothesisTest):
    
    def TestStatistic(self, data):
        ages, weights = data
        _, slope = thinkstats2.LeastSquares(ages, weights)
        return slope

    def MakeModel(self):
        _, weights = self.data
        self.ybar = weights.mean()
        self.res = weights - self.ybar
    def RunModel(self):
        ages, _ = self.data
        weights = self.ybar + np.random.permutation(self.res)
        return ages, weights",0.6048633456,
861,hypothesis testing with slopes,"class SlopeTest(thinkstats2.HypothesisTest):

    def TestStatistic(self, data):
        ages, weights = data
        _, slope = thinkstats2.LeastSquares(ages, weights)
        return slope

    def MakeModel(self):
        _, weights = self.data
        self.ybar = weights.mean()
        self.res = weights - self.ybar

    def RunModel(self):
        ages, _ = self.data
        weights = self.ybar + np.random.permutation(self.res)
        return ages, weights",0.6048633456,
861,hypothesis testing with slopes,"import emceemr
from emceemr import priors",0.5745210052,
861,hypothesis testing with slopes,"from Fred2.EpitopePrediction import APSSMEpitopePrediction
from Fred2.Core import EpitopePredictionResult
import random
import pandas

class RandomEpitopePrediction(APSSMEpitopePrediction):
    __alleles = [""A*02:01""]
    __supported_length = [9]
    __name = ""random""
    __version= ""1.0""
    
    #the interface defines three class properties
    @property
    def name(self):
        #retunrs the name of the predictor
        return self.__name
    
    @property
    def supportedAlleles(self):
        #returns the supported alleles as strings (without the HLA prefix)
        return self.__alleles
    
    @property
    def supportedLength(self):
        #returns the supported epitope lengths as iterable
        return self.__supported_length
    
    @property
    def version(self):
        #returns the version of the predictor
        return self.__version
    
    #the interface defines a function converting Fred2's HLA allele presentation
    #into an internal presentation used by different methods.
    #for this predictor we won't need it but still have to provide it!
    #the function consumes a list of alleles and converts them into the internally used presentation
    def convert_alleles(self, alleles):
        #we just use the identity function
        return alleles
    
    #additionally the interface defines a function `predict` 
    #that consumes a list of peptides or a single peptide and optionally a list 
    #of allele objects
    #
    #this method implements the complete prediction routine
    def predict(self, peptides, alleles=None):
        
        #test whether one peptide or a list
        if isinstance(peptides, basestring):
            peptides = list(peptides)
        
        #if no alleles are specified do predictions for all supported alleles
        if alleles is None:
            alleles = self.supportedAlleles
        else:
            #filter for supported alleles
            alleles = filter(lambda a: a.name in self.supportedAlleles, alleles) 
        
        result = {}
        #now predict binding/non-binding for each peptide at random
        for a in alleles:
            result[a] = {}
            for p in peptides:
                if random.random() >= 0.5:
                    result[a][p] = 1.0
                else:
                    result[a][p] = 0.0
        
        #create EpitopePredictionResult object. This is a multi-indexed DataFrame 
        #with Peptide and Method as multi-index and alleles as columns
        df_result = EpitopePredictionResult.from_dict(result)
        df_result.index = pandas.MultiIndex.from_tuples([tuple((i,self.name)) for i in df_result.index],
                                                        names=['Seq','Method'])
        return df_result",0.5687538385,
861,hypothesis testing with slopes,temp_hypo = ts.hypothesisTest_oneSample(temp.temperature),0.5601108074,
861,hypothesis testing with slopes,gh.plot_hypothesis2(),0.5573790073,
861,hypothesis testing with slopes,gh_internal.plot_hypothesis2(),0.5573790073,
861,hypothesis testing with slopes,book_plots.plot_hypothesis2(),0.5573790073,
1474,parts to be implemented,"def learning_curve(df):
    """"""
    Plot the learning curve using the log-loss error on the training and validation set
    """"""
    
    features = df.copy()
    for item in cat_features:
        le_feature = LabelEncoder()
        features[item] = le_feature.fit_transform(df[item])
    features = features[col_features]
    le_target = LabelEncoder()
    target = le_target.fit_transform(df[""Tag""])
    
    train_sizes=np.linspace(0.1, 0.9, 5)
    train_logloss = []
    val_logloss = []
    for train_size in train_sizes:
        X_train, X_val, y_train, y_val = train_test_split(features, target, test_size=1-train_size,
                                                          stratify=target, random_state=42)
        
        eval_set = [(X_train, y_train), (X_val, y_val)]
        clf = XGBClassifier(max_depth=4, learning_rate=0.2, objective='multi:softprob', seed=42)
    
        clf.fit(X_train, y_train, eval_set=eval_set, eval_metric=""mlogloss"", verbose=False)
        
        results = clf.evals_result()
        train_logloss.append(results[""validation_0""][""mlogloss""][-1])
        val_logloss.append(results[""validation_1""][""mlogloss""][-1])
    plt.subplots(figsize=(16,6))
    plt.plot(train_sizes, train_logloss, 'r', label='training logloss')
    plt.plot(train_sizes, val_logloss, 'g', label='validation logloss')
    plt.xlabel('training size')
    plt.ylabel('log loss error')
    plt.title('Learning Curve')
    plt.legend()
    return",0.2843106687,
1474,parts to be implemented,"#sol.1 dfs recursive
class Solution(object):
    def permute(self, nums):
        """"""
        :type nums: List[int]
        :rtype: List[List[int]]
        """"""
        res=[]
        self.getpermute(nums,[],res)
        return res
    
    def getpermute(self,nums,path,res):
        if not nums:
            res.append(path)
            return
        for i in xrange(len(nums)):
            self.getpermute(nums[:i]+nums[i+1:],path+[nums[i]],res)
            
#sol.2 iterative
def permute(self, nums):
    perms = [[]]   
    for n in nums:
        new_perms = []
        for perm in perms:
            for i in xrange(len(perm)+1):   
                new_perms.append(perm[:i] + [n] + perm[i:])   ###insert n
        perms = new_perms
    return perms",0.2837131619,
1474,parts to be implemented,"parameters = [
    {
        'clf': (Lasso(),),
        'clf__alpha': (1e-4, 1e-3, 1e-1)
    }, {
        'clf': (LinearSVC(),),
        'clf__C': (0.01, 0.5, 1.0)
    }, {
        'scaler__with_mean': (False,),
        'scaler__with_std': (False,),
        'clf': (RandomForestClassifier(100),),
        'clf__min_samples_split': (2, 25, 100)
    }
]
param_descriptions = [""Lasso alpha=1e-4"", 
                      ""Lasso alpha=1e-3"",
                      ""Lasso alpha=1e-1"",
                      ""LinearSVC C=0.01"",
                      ""LinearSVC C=0.5"",
                      ""LinearSVC C=1.0"", 
                      ""RandomForest min_samples=2"",
                      ""RandomForest min_samples=25"",
                      ""RandomForest min_samples=100""
                    ]",0.2778124213,
1474,parts to be implemented,"def model_test(test_image):
    
    def add_heat(heatmap, bbox_list):
        # Iterate through list of bboxes
        for box in bbox_list:
            # Add += 1 for all pixels inside each bbox
            # Assuming each ""box"" takes the form ((x1, y1), (x2, y2))
            heatmap[box[0][1]:box[1][1], box[0][0]:box[1][0]] += 1
    
        # Return updated heatmap
        return heatmap
    
    def apply_threshold(heatmap, threshold):
        # Zero out pixels below the threshold
        heatmap[heatmap <= threshold] = 0
        # Return thresholded map
        return heatmap
    
    def draw_labeled_bboxes(img, labels):
        # Iterate through all detected cars
        for car_number in range(1, labels[1]+1):
            # Find pixels with each car_number label value
            nonzero = (labels[0] == car_number).nonzero()
            # Identify x and y values of those pixels
            nonzeroy = np.array(nonzero[0])
            nonzerox = np.array(nonzero[1])
            # Define a bounding box based on min/max x and y
            bbox = ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy)))
            # Draw the box on the image
            cv2.rectangle(img, bbox[0], bbox[1], (255,0,0), 6)
        # Return the image
        return img 
        
    
    test_image = np.array(test_image)      
    with open(save_model_path + 'scale_features_X.pkl', 'rb') as inputs:
            X_scaler = pickle.load(inputs)          
           
    if model_name == 'SVM':
        with open(save_model_path + 'Best SVC_Classifier' + '.pkl', 'rb') as inputs:
            clf = pickle.load(inputs)
            car_detected_boxes, draw_image = model_predict(test_image, model_name, X_scaler, scales, y_start_stop_list, 
                                                           classfier = clf)
    elif model_name == 'SGD':
        with open(save_model_path + 'SGD_Classifier' + '.pkl', 'rb') as inputs:
            clf = pickle.load(inputs)      
            car_detected_boxes, draw_image = model_predict(test_image, model_name, X_scaler, scales, y_start_stop_list, 
                                                           classfier = clf)
        
    elif model_name == 'Neural_Network':
        loaded_graph = tf.Graph()        
        with tf.Session(graph=loaded_graph) as sess:            
            # Load model
            
            #t = time.time()
            loader = tf.train.import_meta_graph(tf_save_model_path + '.meta')
            loader.restore(sess, tf_save_model_path)

            # Get Tensors from loaded model
            loaded_x = loaded_graph.get_tensor_by_name('x:0')            
            loaded_logits = loaded_graph.get_tensor_by_name('logits:0')
            loaded_keep_probability = loaded_graph.get_tensor_by_name('kp:0') 
            car_detected_boxes, draw_image = model_predict(test_image, model_name, X_scaler, scales, y_start_stop_list, 
                                                           classfier = None, sess = sess,
                                                           loaded_x = loaded_x, loaded_logits = loaded_logits, 
                                                           loaded_keep_probability = loaded_keep_probability)
        
        
    heat  = np.zeros_like(test_image[:,:,0]).astype(np.float)
    heatmap = add_heat(heat, car_detected_boxes)
    heatmap = apply_threshold(heatmap, 2)
    heatmap = np.clip(heat, 0, 255)
    labels = label(heatmap)
    if plot_detections:
        print(labels[1], 'cars found')
    final_image = draw_labeled_bboxes(np.copy(test_image), labels)   
    
    if plot_detections:        
        plt.figure(figsize=(15,15))
        plt.subplot(131)
        plt.imshow(draw_image)
        plt.title('All Detections')
        plt.subplot(132)
        plt.imshow(heatmap, cmap='hot')
        plt.title('All Detections heat')
        plt.subplot(133)
        plt.imshow(final_image)
        plt.title('Final Detection')
    
    
    return final_image",0.268422842,
1474,parts to be implemented,"def PrintSymSuite(suite):
    for hypo, prob in suite.Items():
        print(hypo, prob.simplify())

single, pair = scenario_b(p, s, pmf_t)
PrintSymSuite(single)
PrintSymSuite(pair)",0.2680539489,
1474,parts to be implemented,"class Plane:
    planes = []
    def __init__(self):
        self._in_air = False
        #self.planes.append(self)
        type(self).planes.append(self)
    
    def take_off(self):
        self._in_air = True
    
    def land(self):
        self._in_air = False
    
    @classmethod
    def num_planes(cls):
        return len(cls.planes)
    
    @classmethod
    def num_planes_in_air(cls):
        return len([plane for plane in cls.planes if plane._in_air])

p1 = Plane()
p2 = Plane()
p3 = Plane()
p1.take_off()
p2.take_off()
p1.land()
Plane.num_planes(), Plane.num_planes_in_air()",0.2678146958,
1474,parts to be implemented,"class maps:

	def __init__(self, centerLat, centerLng, zoom ):
		self.center = (float(centerLat),float(centerLng))
		self.zoom = int(zoom)
		self.grids = None
		self.paths = []
		self.points = []
		self.radpoints = []
		self.gridsetting = None
		self.coloricon = 'http://chart.apis.google.com/chart?cht=mm&chs=12x16&chco=FFFFFF,XXXXXX,000000&ext=.png'

	def setgrids(self,slat,elat,latin,slng,elng,lngin):
		self.gridsetting = [slat,elat,latin,slng,elng,lngin]

	def addpoint(self, lat, lng, color = '#FF0000'):
		self.points.append((lat,lng,color[1:]))

	#def addpointcoord(self, coord):
	#	self.points.append((coord[0],coord[1]))

	def addradpoint(self, lat,lng,rad,color = '#0000FF'):
		self.radpoints.append((lat,lng,rad,color))

	def addpath(self,path,color = '#FF0000'):
		path.append(color)
		self.paths.append(path)
	
	#create the html file which inlcude one google map and all points and paths
	def draw(self, htmlfile):
		f = open(htmlfile,'w')
		f.write('<html>\n')
		f.write('<head>\n')
		f.write('<meta name=""viewport"" content=""initial-scale=1.0, user-scalable=no"" />\n')
		f.write('<meta http-equiv=""content-type"" content=""text/html; charset=UTF-8""/>\n')
		f.write('<title>Google Maps - pygmaps </title>\n')
		f.write('<script type=""text/javascript"" src=""http://maps.google.com/maps/api/js?sensor=false""></script>\n')
		f.write('<script type=""text/javascript"">\n')
		f.write('\tfunction initialize() {\n')
		self.drawmap(f)
		self.drawgrids(f)
		self.drawpoints(f)
		self.drawradpoints(f)
		self.drawpaths(f,self.paths)
		f.write('\t}\n')
		f.write('</script>\n')
		f.write('</head>\n')
		f.write('<body style=""margin:0px; padding:0px;"" onload=""initialize()"">\n')
		f.write('\t<div id=""map_canvas"" style=""width: 100%; height: 100%;""></div>\n')
		f.write('</body>\n')
		f.write('</html>\n')
		f.close()

	def drawgrids(self, f):
		if self.gridsetting == None:
			return
		slat = self.gridsetting[0]
		elat = self.gridsetting[1]
		latin = self.gridsetting[2]
		slng = self.gridsetting[3]
		elng = self.gridsetting[4]
		lngin = self.gridsetting[5]
		self.grids = []

		r = [slat+float(x)*latin for x in range(0, int((elat-slat)/latin))]
		for lat in r:
			self.grids.append([(lat+latin/2.0,slng+lngin/2.0),(lat+latin/2.0,elng+lngin/2.0)])

		r = [slng+float(x)*lngin for x in range(0, int((elng-slng)/lngin))]
		for lng in r:
			self.grids.append([(slat+latin/2.0,lng+lngin/2.0),(elat+latin/2.0,lng+lngin/2.0)])
		
		for line in self.grids:
			self.drawPolyline(f,line,strokeColor = ""#000000"")
	def drawpoints(self,f):
		for point in  self.points:
			self.drawpoint(f,point[0],point[1],point[2])

	def drawradpoints(self, f):
		for rpoint in self.radpoints:
			path = self.getcycle(rpoint[0:3])
			self.drawPolygon(f,path,strokeColor = rpoint[3])

	def getcycle(self,rpoint):
		cycle = []
		lat = rpoint[0]
		lng = rpoint[1]
		rad = rpoint[2] #unit: meter
		d = (rad/1000.0)/6378.8;
		lat1 = (math.pi/180.0)* lat
		lng1 = (math.pi/180.0)* lng

		r = [x*30 for x in range(12)]
		for a in r:
			tc = (math.pi/180.0)*a;
			y = math.asin(math.sin(lat1)*math.cos(d)+math.cos(lat1)*math.sin(d)*math.cos(tc))
			dlng = math.atan2(math.sin(tc)*math.sin(d)*math.cos(lat1),math.cos(d)-math.sin(lat1)*math.sin(y))
			x = ((lng1-dlng+math.pi) % (2.0*math.pi)) - math.pi 
			cycle.append( ( float(y*(180.0/math.pi)),float(x*(180.0/math.pi)) ) )
		return cycle

	def drawpaths(self, f, paths):
		for path in paths:
			#print path
			self.drawPolyline(f,path[:-1], strokeColor = path[-1])

	#############################################
	# # # # # # Low level Map Drawing # # # # # # 
	#############################################
	def drawmap(self, f):
		f.write('\t\tvar centerlatlng = new google.maps.LatLng(%f, %f);\n' % (self.center[0],self.center[1]))
		f.write('\t\tvar myOptions = {\n')
		f.write('\t\t\tzoom: %d,\n' % (self.zoom))
		f.write('\t\t\tcenter: centerlatlng,\n')
		f.write('\t\t\tmapTypeId: google.maps.MapTypeId.ROADMAP\n')
		f.write('\t\t};\n')
		f.write('\t\tvar map = new google.maps.Map(document.getElementById(""map_canvas""), myOptions);\n')
		f.write('\n')



	def drawpoint(self,f,lat,lon,color):
		f.write('\t\tvar latlng = new google.maps.LatLng(%f, %f);\n'%(lat,lon))
		f.write('\t\tvar img = new google.maps.MarkerImage(\'%s\');\n' % (self.coloricon.replace('XXXXXX',color)))
		f.write('\t\tvar marker = new google.maps.Marker({\n')
		f.write('\t\ttitle: ""no implimentation"",\n')
		f.write('\t\ticon: img,\n')
		f.write('\t\tposition: latlng\n')
		f.write('\t\t});\n')
		f.write('\t\tmarker.setMap(map);\n')
		f.write('\n')
		
	def drawPolyline(self,f,path,\
			clickable = False, \
			geodesic = True,\
			strokeColor = ""#FF0000"",\
			strokeOpacity = 1.0,\
			strokeWeight = 2
			):
		f.write('var PolylineCoordinates = [\n')
		for coordinate in path:
			f.write('new google.maps.LatLng(%f, %f),\n' % (coordinate[0],coordinate[1]))
		f.write('];\n')
		f.write('\n')

		f.write('var Path = new google.maps.Polyline({\n')
		f.write('clickable: %s,\n' % (str(clickable).lower()))
		f.write('geodesic: %s,\n' % (str(geodesic).lower()))
		f.write('path: PolylineCoordinates,\n')
		f.write('strokeColor: ""%s"",\n' %(strokeColor))
		f.write('strokeOpacity: %f,\n' % (strokeOpacity))
		f.write('strokeWeight: %d\n' % (strokeWeight))
		f.write('});\n')
		f.write('\n')
		f.write('Path.setMap(map);\n')
		f.write('\n\n')

	def drawPolygon(self,f,path,\
			clickable = False, \
			geodesic = True,\
			fillColor = ""#000000"",\
			fillOpacity = 0.0,\
			strokeColor = ""#FF0000"",\
			strokeOpacity = 1.0,\
			strokeWeight = 1
			):
		f.write('var coords = [\n')
		for coordinate in path:
			f.write('new google.maps.LatLng(%f, %f),\n' % (coordinate[0],coordinate[1]))
		f.write('];\n')
		f.write('\n')

		f.write('var polygon = new google.maps.Polygon({\n')
		f.write('clickable: %s,\n' % (str(clickable).lower()))
		f.write('geodesic: %s,\n' % (str(geodesic).lower()))
		f.write('fillColor: ""%s"",\n' %(fillColor))
		f.write('fillOpacity: %f,\n' % (fillOpacity))
		f.write('paths: coords,\n')
		f.write('strokeColor: ""%s"",\n' %(strokeColor))
		f.write('strokeOpacity: %f,\n' % (strokeOpacity))
		f.write('strokeWeight: %d\n' % (strokeWeight))
		f.write('});\n')
		f.write('\n')
		f.write('polygon.setMap(map);\n')
		f.write('\n\n')",0.2671029568,
1474,parts to be implemented,"class Solution(object):
    def largestDivisibleSubset(self, nums):
        """"""
        :type nums: List[int]
        :rtype: List[int]
        """"""
        from copy import copy
        nums.sort()
        n = len(nums)
        if n == 0: return []
        dp = [0] * n
        dp[0] = [nums[0]]
        #print(dp)
        for i in xrange(1, n):
            curNum = nums[i]
            maxSet = []
            for j in xrange(i):
                if curNum % nums[j] == 0:
                    localSet = copy(dp[j])
                    if len(localSet) >= len(maxSet):
                        maxSet = localSet
            
            maxSet.append(nums[i])
            dp[i] = maxSet
            #print(dp)
        
        #print(dp)
        res = []
        for localSet in dp:
            if len(localSet) > len(res):
                res = localSet
        return res",0.2663202286,
1474,parts to be implemented,"class Particle(object):

	def __init__(self,r,v):
		""""""Defines initial parameters (position, velocity)""""""
		self.R = r
		self.V = v

	def initialstep(self,h,galaxy_M,galaxy_r,RK4step):
		""""""Initial step taking to obtain r_(1) from initial values""""""
	
		if not RK4step:
			# Using Taylor expansion about t = 0 to 2nd order
			self.Rnew = self.R + h*self.V + 0.5*atest(self.R,galaxy_r,galaxy_M)*h**2
		else:
			self.Rnew = RKstep(h,self.V,self.R,galaxy_r,galaxy_M,atest)

		self.Rold = self.R
		self.R = self.Rnew

	def verletstep(self,h,galaxy_M,galaxy_r):
		""""""Verlet Integration algorithm""""""

		# Note: Must be performed after a single call of the initialstep function

		self.Rnew = 2*self.R-self.Rold+(h**2)*atest(self.R,galaxy_r,galaxy_M)

		self.Rold = self.R
		self.R = self.Rnew

		# Do not need velocity in simulation so comment out to increase speed

		# self.V = (self.Rnew-self.Rold)/(2*h)

	def returnvalues(self):
		""""""Returns position vector for plotting purposes""""""
		return self.R",0.2661890686,
1474,parts to be implemented,"face_connections = {'face':
                    {0: {'X':  ((12, 'Y', False), (3, 'X', False)),
                         'Y':  (None,             (1, 'Y', False))},
                     1: {'X':  ((11, 'Y', False), (4, 'X', False)),
                         'Y':  ((0, 'Y', False),  (2, 'Y', False))},
                     2: {'X':  ((10, 'Y', False), (5, 'X', False)),
                         'Y':  ((1, 'Y', False),  (6, 'X', False))},
                     3: {'X':  ((0, 'X', False),  (9, 'Y', False)),
                         'Y':  (None,             (4, 'Y', False))},
                     4: {'X':  ((1, 'X', False),  (8, 'Y', False)),
                         'Y':  ((3, 'Y', False),  (5, 'Y', False))},
                     5: {'X':  ((2, 'X', False),  (7, 'Y', False)),
                         'Y':  ((4, 'Y', False),  (6, 'Y', False))},
                     6: {'X':  ((2, 'Y', False),  (7, 'X', False)),
                         'Y':  ((5, 'Y', False),  (10, 'X', False))},
                     7: {'X':  ((6, 'X', False),  (8, 'X', False)),
                         'Y':  ((5, 'X', False),  (10, 'Y', False))},
                     8: {'X':  ((7, 'X', False),  (9, 'X', False)),
                         'Y':  ((4, 'X', False),  (11, 'Y', False))},
                     9: {'X':  ((8, 'X', False),  None),
                         'Y':  ((3, 'X', False),  (12, 'Y', False))},
                     10: {'X': ((6, 'Y', False),  (11, 'X', False)),
                          'Y': ((7, 'Y', False),  (2, 'X', False))},
                     11: {'X': ((10, 'X', False), (12, 'X', False)),
                          'Y': ((8, 'Y', False),  (1, 'X', False))},
                     12: {'X': ((11, 'X', False), None),
                          'Y': ((9, 'Y', False),  (0, 'X', False))}}}",0.2660579681,
1171,"master yoda given a sentence, return a sentence with the words reversed","#Create a helper function that generates stems for each document
def getStems(doc):
    """"""
    getStems: get the stem from a document.
    INPUT: doc(str): An article from the Aggie.
    OUTPUT: stems(set): the set of stem words from the article.""""""
    stems=set([st.stem(words) for words in nltk.tokenize.word_tokenize(doc)])
    return(stems)",0.4496936202,
1171,"master yoda given a sentence, return a sentence with the words reversed","def clean_word(stemmer, word):
    return stemmer.stem(word.replace('#', ''))",0.4452885091,
1171,"master yoda given a sentence, return a sentence with the words reversed","def preprocess_query(query_string):
    query = re.sub(punctStr, '', ' '.join( [ snowball_stemmer.stem(x) for x in wordpunct_tokenize(query_string)]))
    return query",0.4432088137,
1171,"master yoda given a sentence, return a sentence with the words reversed","def get_lemmatized_text(corpus):
    
    from nltk.stem import WordNetLemmatizer
    lemmatizer = WordNetLemmatizer()
    return [' '.join([lemmatizer.lemmatize(word) for word in review.split()]) for review in corpus]

lemmatized_reviews_train = get_lemmatized_text(reviews_train_clean)
lemmatized_reviews_test = get_lemmatized_text(reviews_test_clean)

cv = CountVectorizer(binary=True)
cv.fit(lemmatized_reviews_train)
X = cv.transform(lemmatized_reviews_train)
X_test = cv.transform(lemmatized_reviews_test)

X_train, X_val, y_train, y_val = train_test_split(
    X, target, train_size = 0.75
)

for c in [0.01, 0.05, 0.25, 0.5, 1]:
    
    lr = LogisticRegression(C=c)
    lr.fit(X_train, y_train)
    print (""Accuracy for C=%s: %s"" 
           % (c, accuracy_score(y_val, lr.predict(X_val))))
    
final_lemmatized = LogisticRegression(C=0.25)
final_lemmatized.fit(X, target)
print (""Final Accuracy: %s"" 
       % accuracy_score(target, final_lemmatized.predict(X_test)))",0.4427908659,
1171,"master yoda given a sentence, return a sentence with the words reversed","def string_stemmer(string):
    return ' '.join([stemmer.stem(word) for word in string.lower().split()])",0.4409322739,
1171,"master yoda given a sentence, return a sentence with the words reversed","# Defining a function to preprocess and clean the text data similar to Text Mining package in R.

def preprocess(x):
    x = re.sub('[^a-z\s]',' ', x.lower())                      # Removing noise from Description
    x = [stemmer.stem(w) for w in x.split() if stemmer.stem(w) not in set(stopwords)]   # Removing words not in list of stopwords
    x= [y for y in x if len(y)>2]                           # Removing words with length less than 2 characters.
    x= list(set(x))
    return ' '.join(x)",0.4359142184,
1171,"master yoda given a sentence, return a sentence with the words reversed","# funtion to remove stemmer using porterstem
def stem(text):
    stemmer = nltk.stem.porter.PorterStemmer()
    # stem each word individually, and concatenate
    text = ' '.join([stemmer.stem(word) for word in text.split(None)])
    return text",0.4333654642,
1171,"master yoda given a sentence, return a sentence with the words reversed","def tokenize_with_preprocess(text):
    """"""
    Tokenize with preprocessing. Remove all punctuations and apply stemming on the tokens
    :param text: String
    :return: list of tokens
    """"""
    return map(__stemmer.stem, filter(lambda w: w not in stop,
                                        nltk.word_tokenize(re.sub(_punc_pattern, '', text.lower()))))",0.4331122637,
1171,"master yoda given a sentence, return a sentence with the words reversed","def clean_text(text):
    stopword = nltk.corpus.stopwords.words('english')
    text = """".join([char for char in text if char not in string.punctuation])
    tokens = re.split('\W+', text)
    text = [ps.stem(word) for word in tokens if word not in stopword]
    return text",0.4329381585,
1171,"master yoda given a sentence, return a sentence with the words reversed","import re
from stemming.porter2 import stem

# define the fucntion to clean text data
def clean(x):
    x = re.sub('[^A-Za-z]+', "" "",  x.lower())
    x=x.replace('.',' ')
    stopwords=['dummi','acv','discount','volume','season','new year','christma','easter','month','thanksgiv',
               'price','promo','coupon','print','media','social','twitter','facebook','instagram','radio','competit','campaign',
               'magazin','mobil','intercept','constant','trade','display','impress','paid','search','click','dummi','base','rollback',
               'pr','memorial','digital','market','july','cost','fsi','can','bag','total','merch','ani','featur','brand',
              'general','sale','categori','actual','predict','valentin','lag','trp','tv','distribut','weight']
    querywords = x.split()
    querywords = [stem(querywords[i]) for i in np.arange(len(querywords))]
    resultwords  = [word for word in querywords if word in stopwords]
    result = ' '.join(resultwords)
    return(result) # The text to search",0.4311924577,
627,filter the dataframe to store sales and aggregate over departments to compute the total sales per store,"def get_delays_for_grade(stations):
    delays = ttc[ttc['Station'].isin(stations['Station'])]
    return delays.groupby('Code Description').size().sort_values(ascending = False)",0.5101581216,
627,filter the dataframe to store sales and aggregate over departments to compute the total sales per store,"def normalize_pop():
    global off
    off = off.sort_values(by = ['Pases\Industrias'] , ascending = True )

    pop_sub = pop[pop['Country Name'].isin(off[off.columns[0]])]
    pop_sub = pop_sub.sort_values(by = ['Country Name'] , ascending = True )
    s_pop_l = pop[year].sum()

    for i in off.columns[1:]:
        W_i = pop_sub[year]*(off[i].sum()/s_pop_l)
        off[i] = off[i]/list(W_i)",0.5099639893,
627,filter the dataframe to store sales and aggregate over departments to compute the total sales per store,"new_genre = df_video_games[(df_video_games['Genre'] == 'Shooter') | (df_video_games['Genre'] == 'Sports') | (df_video_games['Genre'] == 'Action')]
G_video = df_video_games.groupby(['Year_of_Release', 'Genre']).Global_Sales.sum()
G_video2 = new_genre.groupby(['Year_of_Release', 'Genre']).Global_Sales.sum()
G_video2.unstack().plot(kind = 'bar', colormap = 'Greens', stacked = True)
plt.ylabel('Global sales')
plt.title( 'Yearly sales of Action, Shooter, Sports games sold globaly')
plt.show()",0.5006756783,
627,filter the dataframe to store sales and aggregate over departments to compute the total sales per store,"def top_dist(Cod):
    ProdDistrib = df.groupby('Cod_Prod').ID_Customer.nunique()
    top5 = ProdDistrib.sort_values(ascending=False).head(5).index
    n=0
    f, axarr = plt.subplots(5, sharex=True)
    for product in top5:
        serie = df[df[""Cod_Prod""] == [product]].groupby(Cod).Cod_Month.count()
        axarr[n].bar(serie.index,serie.values)
        axarr[n].set_title(product)
        n=n+1
    plt.rcParams[""figure.figsize""] = (10,20)
    plt.show()",0.4992471933,
627,filter the dataframe to store sales and aggregate over departments to compute the total sales per store,"if process_region2:
    # HasPromotions (replacement with store 1006 since all are equal)
    df_region2['HasPromotions'] = all_data[all_data.StoreID == 1006]\
    .loc[(all_data.Date >= date_missing_start) & (all_data.Date <= date_missing_end)]\
    .HasPromotions.reset_index().drop('index', axis=1)",0.4971192479,
627,filter the dataframe to store sales and aggregate over departments to compute the total sales per store,"# to Group in Bunch of Orders placed by CustomerID and adding up Revenue generated per Order for each CustomerID
group_revenue = test_b.groupby(['CustomerID',test_b.CustomerID.diff().ne(0).cumsum()],sort=False)['Revenue'].sum()

#to check the last 5 values
group_revenue.tail()",0.4945896268,
627,filter the dataframe to store sales and aggregate over departments to compute the total sales per store,"def fix_multiple_events(df):
    """"""
    Remove the ""RECOVR AUD"" and other types of records as they seem to conflict with the ""REGULAR"" ones
    This may have the consequence of losing the first couple hours of readings after a turnstile is reset
    However, my subsequent algorithm will be able to backfill the ""loss"" using estimates of entry/exit of the same period from different weeks
    """"""
    
    df = df.ix[df.DESC == ""REGULAR""]

    # Sort into groups
    g_dup = df.groupby(by=[""ID"", ""DATETIME""])

    # Keep the max values of the duplicate entries
    idx = g_dup['ENTRIES'].transform(max) == df['ENTRIES']
    df = df.ix[idx]
    idx = g_dup['EXITS'].transform(max) == df['EXITS']
    df = df.ix[idx]
    
    return df
    
df = fix_multiple_events(df)",0.4944217801,
627,filter the dataframe to store sales and aggregate over departments to compute the total sales per store,"vg_df.groupby(['Publisher']).filter(lambda x: len(x) > 5).groupby(['Publisher', 'Year_of_Release'], as_index=False).mean().groupby('Publisher').mean().Global_Sales.sort_values(ascending=False).head(10)",0.4938784838,
627,filter the dataframe to store sales and aggregate over departments to compute the total sales per store,"def make_sex_cols(df):
    df['Female'] = df.Sex.map(lambda x: 1 if x == 'female' else 0)
    return df",0.4922478199,
627,filter the dataframe to store sales and aggregate over departments to compute the total sales per store,"def make_sex_cols(df):
    # male is reference class
    df['Female'] = df.Sex.map(lambda x: 1 if x == 'female' else 0)
    return df",0.4922478199,
644,find the mean number of full time and part time workers across majors,"def answer_three():
    Top15 = answer_one()   
    return  Top15.iloc[:,-10:].mean(axis=1).sort_values(ascending=False).rename('avgGFP')
answer_one().iloc[3]",0.4790453315,
644,find the mean number of full time and part time workers across majors,"#training here is computing several conditional probs
def train():
    
    global pSpam
    global pHam
    
    #initialize
    total = 0
    spamCount = 0
    
    for idx,row in df_train.iterrows():
        
        if row.label == 1:
            spamCount +=1
            
        total += 1
        
        processComment(row.comment,row.label)
        
    #compute prior probabilities P(spam), P(ham)
    pSpam = spamCount/float(total)
    pHam = (total - spamCount)/float(total)
    
    print('Training complete')",0.4724082649,
644,find the mean number of full time and part time workers across majors,"def movies_by_mean_ratings(movies_and_ratings_data):
    movies_mean_ratings = movies_and_ratings_data[['movie_id','movie title','rating']].groupby(['movie_id','movie title']).mean()
    movies_mean_ratings = movies_mean_ratings.sort_values(['rating'],ascending=False)
    return movies_mean_ratings",0.4559274614,
644,find the mean number of full time and part time workers across majors,"def answer_three():
    Top15 = answer_one()
    years = ['2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015']
    return (Top15[years].mean(axis=1)).sort_values(ascending=False).rename('avgGDP')

answer_three()",0.4557581842,
644,find the mean number of full time and part time workers across majors,"def three_most_populous_counties():
    censu_copy = census_df.copy()
    censu_copy = censu_copy.set_index(['STNAME', 'CTYNAME'])
    
    return list(censu_copy.groupby(level='STNAME')['POPESTIMATE2015'].nlargest(3).sum(level=0).sort_values(ascending=False).index[:3])


three_most_populous_counties()",0.4521505833,
644,find the mean number of full time and part time workers across majors,"def by_county(county='Albany'):
    status_counts = df.groupby(['Hospital County', 'Type of Admission']).size().ix[county]

    cmap = plt.cm.summer
    colors = cmap(np.linspace(0., 1., len(status_counts)))

    fig, ax = plt.subplots(figsize=(10.0, 10.0))

    ax.pie(status_counts, autopct=lambda p: '{0:.1f}% ({1:})'. format(p, int(p * sum(status_counts) / 100)), 
               labels=status_counts.index, colors=colors)
    fig.suptitle(""{}"".format(county))
    plt.show()",0.4500867128,
644,find the mean number of full time and part time workers across majors,"def prepare_data_fitting(data):
    df = (data.resample('D').sum().reset_index())[['ide_dhEmi_$date', 'prod_vProd']].fillna(0)
    df.columns = ['ds', 'y']
    return df",0.4492833018,
644,find the mean number of full time and part time workers across majors,"def quiz_pivot(df, percent=True):
    tmp = df.pivot_table(index=['week_number','step_number','question_number'],
                         columns='response',
                         values='correct',
                         aggfunc='count')
    if percent:
        return tmp.apply(lambda x : 100*(x / x.sum()), axis=1)
    return tmp

qnresp_pivot=quiz_pivot(qnresp)
qnresp_pivot",0.4485703111,
644,find the mean number of full time and part time workers across majors,"#crimes vs mean max temperature per month
    dfCrMoUnformat = df['Month'].value_counts()

    dfCrMo = pd.concat([dfCrMoUnformat,dfCrMoUnformat.index.to_series()], axis=1).reset_index(drop=True)
    dfCrMo.columns = ['CrimeCount', 'Month']


    dfGraph = pd.merge(ad, dfCrMo, on='Month')


    fit = np.polyfit(dfGraph['Temperature'],dfGraph['CrimeCount'],1)
    fit_fn = np.poly1d(fit)



    fig, ax = plt.subplots()
    ax.scatter(dfGraph['Temperature'], dfGraph['CrimeCount'])
    plt.plot(dfGraph['Temperature'], fit_fn(dfGraph['Temperature']), '--k')
    plt.xlabel('Maximum Monthly Average Temperature in 2014 [F]', fontsize=12)
    plt.ylabel('Monthly Number of Offenses in Seattle 2014', fontsize=12)

    for i, txt in enumerate(['Dec','Nov','Oct','Sep','Agu','Jul','Jun','May','Apr','Mar','Feb','Jan']):
        ax.annotate(txt, (dfGraph['Temperature'][i]+0.5,dfGraph['CrimeCount'][i]))

    plt.show()",0.4466188252,
644,find the mean number of full time and part time workers across majors,"def answer_seven():
    Top15 = answer_one()
    Top15['Ratio'] = Top15['Self-citations'] / Top15['Citations']
    Top15 = Top15.sort_values(['Ratio'], ascending=[0])
    topCountry = Top15.iloc[0]
    return (topCountry.name, topCountry['Ratio'])

answer_seven()",0.4460253119,
2208,step what was the quantity of the most expensive item ordered?,"# functions for summarizing word use by a trait
def times_diff2(row, group, ref):
    if row[ref] > row[group]:
        return -1 * (row[ref] / row[group])
    else:
        return row[group] / row[ref]

def count(data, per_person):
    #count the people in each category
    l = len(data)

    #apply the right aggregation function, depending whether we want 
    #most common words, or words used by most people
    if per_person:
        data = chain.from_iterable([set(x) for x in profiles.stems])
    else:
        data = chain.from_iterable(data)
            
    c = Counter(data)
    
    return c, l

def word_use(df, att, ref=None, per_person=False, undostems=False):
    #list all of the categories in this column
    types = list(df[att].value_counts().index.values)
    #variables that will store our results
    data = {}
    lens = {}
    
    print(""Counting the words used by each group..."")
    for t in types:
        #get the stems for each category
        tmp = df[df[att] == t].stems
        #count how often each is used
        data[t], lens[t] = count(tmp, per_person)
        
        #also compute the inverse of each category
        tmp = df[df[att] != t].stems
        data['not_'+str(t)], lens['not_'+str(t)] = count(tmp, per_person)        
        
    #convert those results to a pandas data frame for easy handling
    popular_words = pd.DataFrame(data)
    
    print('Calculating percentages...')
    # convert the counts in each column to percents
    for t in popular_words.columns:
        n = lens[t] #if we want percent of people
        
        if not per_person: #if we want percent of total words 
            n = popular_words[t].sum()
        
        popular_words[t] = (popular_words[t] / n) * 100
    
    print('Selecting the most popular words...')
    #find overall most popular words
    popular_words['max'] = popular_words.max(axis=1)
    #sort the words and select the top 1000 most popular
    popular_words = popular_words.sort_values(by='max', ascending=False)
    popular_words = popular_words.head(1000)

    print('Calculating most distinctive words...')
    #calculate the rate each type of person uses these words relative to others
    for t in types:
        r = ref
        
        if ref == None: #if we do not have a reference category, use the inverse
            r = 'not_'+str(t)
            
        if t != ref: #don't compare a trait to itself
            #apply our times_diff2 function
            popular_words['times_diff_'+str(t)] = popular_words.apply(times_diff2, 
                                                                 group=t, 
                                                                 ref=r, 
                                                                 axis=1)

    #remove the inverse columns we created
    popular_words = popular_words.drop(popular_words.filter(regex='not_'), axis=1)
    
    if undostems:
        print('Cleaning up word stems for readability...')
        popular_words = clean_index(popular_words, df)
    
    print('Done!')
    return popular_words",0.4667799473,
2208,step what was the quantity of the most expensive item ordered?,"class Tank(Suite):
    # hypo is the number of tanks
    # data is an observed serial number
    def Likelihood(self, data, hypo):
        if data > hypo:
            return 0
        else:
            return 1 / hypo",0.4587750733,
2208,step what was the quantity of the most expensive item ordered?,"def earlyStopping(history):
    hist = history.history
    idx = np.argmin(np.array(hist[""val_YP_loss""]) + np.array(hist[""val_YT_loss""]))
    return {""val_YP_loss"": hist[""val_YP_loss""][idx], 
           ""val_YT_loss"": hist[""val_YT_loss""][idx],
           ""val_YP_acc"": hist[""val_YP_acc""][idx],
           ""val_YT_acc"": hist[""val_YT_acc""][idx]}",0.4557572901,
2208,step what was the quantity of the most expensive item ordered?,"def _fitness(perm, inst):
    # note negative indexing trick to include final step (return to origin)
    return -sum([inst[perm[i-1]][perm[i]] for i in range(0,len(perm))])
fitness = lambda perm: _fitness(perm, instance)",0.4531016946,
2208,step what was the quantity of the most expensive item ordered?,"def fitness(perm, inst):
    # note negative indexing trick to include final step (return to origin)
    return -sum([inst[perm[i-1]][perm[i]] for i in range(0,len(perm))])",0.4518377781,
2208,step what was the quantity of the most expensive item ordered?,"# objective function
def Obj_fn(m):
    return sum((m.priceBuy[i]*m.posNetLoad[i]) + (m.priceSell[i]*m.negNetLoad[i]) for i in m.Time)  
m.total_cost = en.Objective(rule=Obj_fn,sense=en.minimize)",0.449880302,
2208,step what was the quantity of the most expensive item ordered?,"def times_diff(row):
    #calculate how many times more men use a word than women
    #or vice versa if women use the word more
    if row.men > row.women:
        return row.men / row.women
    else:
        return -1 * (row.women / row.men)
    
popular_words['times_diff'] = popular_words.apply(times_diff, axis=1)
popular_words = popular_words.sort_values(by='max', ascending=False)

print('Most popular words:')
popular_words.head(10).round(3)",0.449664861,
2208,step what was the quantity of the most expensive item ordered?,"def energy_comparison(wf_i, wf_r):
    energy_r = np.sum(wf_r.q[1000:11000])
    energy_i = np.sum(wf_i.q)
    return 100 * abs(energy_i - energy_r) / energy_i",0.4487953782,
2208,step what was the quantity of the most expensive item ordered?,"def com_sim(reviews_user0, reviews_user1):
    sim = pearsonr(reviews_user0['stars']-reviews_user0['user_avg'], reviews_user1['stars']-reviews_user1['user_avg'])[0] 
    if np.isnan(sim):
        return 0
    else:
        return sim",0.4460566044,
2208,step what was the quantity of the most expensive item ordered?,"def covWN(x1, x2, cov_params):
    if x1 == x2:
        return cov_params[0]
    else:
        return 0",0.4454376698,
352,create an array of fives,"class population:
    def __init__(self, plist, N):
        self.plist = np.array(plist)
        self.N = N
        
        self.Rb = 1
        self.k = 20000
        self.rd = 10**(-3)
    
    def update(self):

        birth = int(self.N*self.Rb*(1 - self.N/self.k))

        if birth == 0: Birthplist = np.zeros(len(self.plist)); print('no birth')
        else:Birthplist = np.array([np.random.choice([1,0], birth,p=[pi, 1-pi]).sum()/birth for pi in self.plist])

        death = int(self.rd*self.N**2)
        survived = int(self.N - death)
        if survived == 0: Survivingplist = np.zeros(len(self.plist));print('ish')
        Survivingplist = np.array([np.random.choice([1,0], survived ,p=[pi, 1-pi]).sum()/survived for pi in self.plist])

        self.N += birth - death
        if self.N <= 0: print(""dead pop"")

        birthNSurvivedMatrix = np.array([Birthplist, Survivingplist ]) #self.plist
        self.plist = np.average(birthNSurvivedMatrix, axis = 0, weights=[birth/self.N,survived/self.N])
        

        
        
    def Hlist(self): return np.array([2*p*(1-p) for p in self.plist])
    def averageH(self): return self.Hlist().mean()",0.4836117029,
352,create an array of fives,"def createSignal(T,samples,A,freq):
    temp =[]
    for i in range(samples):
        temp.append(A * sin(2 * pi * freq * i * T))
    return temp",0.4831131101,
352,create an array of fives,"def rot(A, n, x1, y1): #this is the function which rotates a given block
    temple = []
    for i in range(n):
        temple.append([])
        for j in range(n):
            temple[i].append(arr[x1+i, y1+j])
    for i in range(n):
        for j in range(n):
            arr[x1+i,y1+j] = temple[n-1-i][n-1-j]",0.4831131101,
352,create an array of fives,"@njit
def create_n_random_particles(n, m, domain=1):
    '''
    Creates `n` particles with mass `m` with random coordinates
    between 0 and `domain`
    '''
    parts = np.zeros((n), dtype=particle_dtype)
    #attribute access only in @jitted function
    for p in parts:
        p.x = np.random.random() * domain
        p.y = np.random.random() * domain
        p.z = np.random.random() * domain
        p.m = m
        p.phi = 0
    return parts",0.4796765447,
352,create an array of fives,"class PriorArrays():
    def __init__(self, ppg_prior_param, swing_prior_param, minutes_param):
        self.ppg_values = np.empty(shape = (0,ppg_prior_param)) #<------------------ placeholder empty array.
        self.swing_values = np.empty(shape = (0,swing_prior_param, 4))
        self.ppg_param = ppg_prior_param
        self.swing_param = swing_prior_param
        self.minutes_param = minutes_param
    
    def ppg_new_entry(self,price, ppg = None): 
        to_add = np.full((1,self.ppg_param), self.ppg_prior_function(price, ppg))
        self.ppg_values = np.vstack([self.ppg_values, to_add])
        
    def update_round_ppg(self, row, value): #row = player_id
        push(self.ppg_values[row], value)",0.4793987274,
352,create an array of fives,"@njit
def wealth_time_series(w_0, z, h):
    """"""
    Generate a single time series for wealth given aggregate shock 
    sequence z.
    
    The returned time series w satisfies len(w) = len(z).
    """"""
    n = len(z)
    w = np.empty(n)
    w[0] = w_0
    for t in range(n-1):
        w[t+1] = h(w[t], z[t+1])
    return w",0.4785809517,
352,create an array of fives,"def population(n_points, pop_size, fig_height, fig_width):
    # Generate an array of Voronoi objects
    pop = []
    for i in range(pop_size):
        points = np.array([[random.randint(0, fig_width), random.randint(0, fig_height)] for i in range(n_points)])
        pop.append(Voronoi(points))
    return pop",0.4762479067,
352,create an array of fives,"def simulateTrialReturns_Student(numTrials, factorMeans, factorCov, weights):
    trialReturns = []
    weights = np.array(weights)
    for i in range(0, numTrials):
        trialFactorReturns = multivariatet(factorMeans, factorCov,10,1)[0]
        trialFeatures = featurize(trialFactorReturns)
        trialFeatures.insert(0,1)
        tmp = np.sum(weights.dot(trialFeatures))

        trialReturns.append(tmp)
    return trialReturns",0.4741390347,
352,create an array of fives,"def simulateTrialReturns(numTrials, factorMeans, factorCov, weights):
    trialReturns = []
    weights = np.array(weights)
    for i in range(0, numTrials):
        trialFactorReturns = np.random.multivariate_normal(factorMeans, factorCov)
        trialFeatures = featurize(trialFactorReturns)
        trialFeatures.insert(0,1)
        tmp = np.sum(weights.dot(trialFeatures))

        trialReturns.append(tmp)
    return trialReturns",0.4741390347,
352,create an array of fives,"#Take bootstrap replicates of the temperature data
#number of replicates
def get_replicates(samp,n_of_rep,size):
    samp_replicates=np.empty(n_of_rep)
    for i in range(n_of_rep):
                samp_replicates[i]= np.mean(np.random.choice(samp, size))
            
    return samp_replicates",0.4740401506,
329,covariance,"def loss(X, Y, w, B, lam):
    err = Y - X.T.dot(w) - B
    err = np.square(err)
    loss_val = sum(err) + lam * sum(abs(w))
    return loss_val",0.4037697911,
329,covariance,"def ssq(data, grand_mean):
    #SUM of SQUARES FOR EACH FACTOR (Independent variables: factor_1 AND factor_2): 
    ssq_factor_1 = sum( [(data[data.factor_1 == f1_category].dependent_variable.mean() - grand_mean)**2 for f1_category in data.factor_1] )
    ssq_factor_2 = sum( [(data[data.factor_2 == f2_category].dependent_variable.mean() - grand_mean)**2 for f2_category in data.factor_2] )
    #TOTAL SUM OF SQUARES:
    ssq_t = sum((data.dependent_variable - grand_mean)**2)

    f1_dict = {f1_lvl: data[data.factor_1 == f1_lvl] for f1_lvl in data.factor_1.unique()}
    
    #For within group variation I need first to estimate means in each treatment group (treatment = each combination of the two factors, f1xf2_dict)
    f1xf2_dict = { k: [v[v.factor_2 == f2_c].dependent_variable.mean() for f2_c in v.factor_2] for k, v in f1_dict.iteritems()}

    #And GET SSQ_w
    ssq_w = 0
    for k in f1_dict.keys():
        ssq_w += sum((f1_dict[k].dependent_variable - f1xf2_dict[k])**2)

    #SUM OF SQUARES OF THE INTERACTION 
    ssq_factor_1_x_factor_2 = ssq_t-ssq_factor_1-ssq_factor_2-ssq_w
    return(ssq_factor_1, ssq_factor_2, ssq_t, ssq_w, ssq_factor_1_x_factor_2)",0.4037358463,
329,covariance,"# Fetch results
def results(ucp):
    N, T, p, u, v = ucp.N, ucp.T, ucp.p, ucp.u, ucp.v
    # Check if problem is feasible
    if ucp.M.getProblemStatus(SolutionType.Default) in [ProblemStatus.PrimalFeasible]:
        # For time-constrained optimization it may be wise to accept any feasible solution
        ucp.M.acceptedSolutionStatus(AccSolutionStatus.Feasible)
        
        # Some statistics:
        print('Solution status: {0}'.format(ucp.M.getPrimalSolutionStatus()))
        print('Relative optimiality gap: {:.2f}%'.format(100*ucp.M.getSolverDoubleInfo(""mioObjRelGap"")))
        print('Total solution time: {:.2f}s'.format(ucp.M.getSolverDoubleInfo(""optimizerTime"")))

        return p.level().reshape([N,T+1]), u.level().reshape([N,T+1])
    else:
        raise ValueError(""No solution"")

pVal, uVal = results(ucp)",0.401545167,
329,covariance,"def my3dplot(model):
    xs = model.X.mean.values[:,0]
    ys = model.X.mean.values[:,1]
    zs = model.X.mean.values[:,2]

    fig = plt.figure(figsize=(14, 14))
    ax = fig.add_subplot(121, projection='3d')
    xLabel = ax.set_xlabel('Latent Dimension 1', linespacing=3.2)
    yLabel = ax.set_ylabel('Latent Dimension 2', linespacing=3.2)
    zLabel = ax.set_zlabel('Latent Dimension 3', linespacing=3.2)
    ax.scatter3D(xs , ys , zs)",0.3997229338,
329,covariance,"def heat_balance_main_rule(model):
    return model.F*model.H_F + sum(model.L[s]*model.H_L_[s] + model.V[s]*model.H_V_[s] for s in model.inlet) \
            + model.Q_main - sum(model.L[s]*model.H_L + model.V[s]*model.H_V for s in model.outlet) == 0
model.heat_balance_main_con = pe.Constraint(rule=heat_balance_main_rule)",0.3933272958,
329,covariance,"def heat_balance_main_rule(model):
    return model.F*model.H_F + sum(model.L[s]*model.H_L_[s] + model.V[s]*model.H_V_[s] for s in model.inlet) \
            + model.Q_main - sum(model.L[s]*model.H_L + model.V[s]*model.H_V for s in model.outlet) - model.W*model.energy_block.dH_L['H2O'] == 0
model.heat_balance_main_con = pe.Constraint(rule=heat_balance_main_rule)",0.3933272958,
329,covariance,"def graph_feature_import(clf, features, Xtest, y_col, prop_feats_to_show):
    
    coefs = np.std(Xtest, 0)*clf.coef_[0,0:]

    df_graph = pd.DataFrame({'Feature': features,
                             'Coefficient': coefs,
                             'Coeff_AV': abs(coefs)})

    # sort dataframe by absolute value of coefficients
    df_graph2 = df_graph.sort_values(by = 'Coeff_AV', ascending = False)

    print('\t\nStandardized coefficients for ' + str(y_col) + '\n')
    print(df_graph2)
    
    feats_to_show = int(df_graph2.shape[0]*prop_feats_to_show)

    #x = range(df_graph2.shape[0])
    x = range(feats_to_show)
    y = df_graph2['Coeff_AV'][0:feats_to_show]
    
    plt.figure(figsize=(20,5))
    plt.bar(x, y, align='center')
    plt.xticks(np.arange(min(x), max(x)+1, 1))    
    plt.xticks(x, df_graph2['Feature'], rotation='vertical')
    plt.xlabel('Feature')
    plt.ylabel('Absolute Value of Standardized Coefficient')
    title = 'Absolute Value of Standardized Coefficients for Top {} features'.format(feats_to_show)
    plt.title(title)

    plt.show()
    
    return df_graph2",0.3910014629,
329,covariance,"def get_predictions(mu, U, V, Ub, Vb):
    ratings = mu + np.dot(U.T, V) + Ub.reshape(-1, 1) + Vb.reshape(-1, 1).T
    return ratings, np.argsort(-ratings)",0.3876137137,
329,covariance,"def z_score_norm(images):
    return ((images.T - images.mean(axis=-1)) / images.std(axis=-1)).T",0.3872377574,
329,covariance,"def plot_results(results):
    xs = results.V_out.index
    ys = results.V_out.values

    t_end = get_last_label(results)
    if t_end < 10:
        xs *= 1000
        xlabel = 'Time (ms)'
    else:
        xlabel = 'Time (s)'
        
    plot(xs, ys)
    decorate(xlabel=xlabel,
             ylabel='$V_{out}$ (volt)',
             legend=False)
    
plot_results(results)",0.386631757,
2633,write a python program to create a tuple with different data types,"from collections import namedtuple
Point = namedtuple('Point', ['x', 'y'])
p = Point(4,6)
p[0] + p[1]",0.4954977036,
2633,write a python program to create a tuple with different data types,"import ipytest, pytest
import test_data as td

enumerated = (
    (td.eight_byte_int, td.eight_byte_var_int),
    (td.four_byte_int, td.four_byte_var_int),
    (td.two_byte_int, td.two_byte_var_int),
    (td.one_byte_int, td.one_byte_var_int),
)

def test_read_var_int():
    for correct_int, var_int in enumerated:
        stream = td.make_stream(var_int)
        calculated_int = read_var_int(stream)
        assert correct_int == calculated_int

ipytest.run_tests(doctest=True)
ipytest.clean_tests(""test_read_var_int*"")",0.4949619472,
2633,write a python program to create a tuple with different data types,"Point(1,1) + Point(2,2)",0.4947885275,
2633,write a python program to create a tuple with different data types,mytuple.,0.4945717454,
2633,write a python program to create a tuple with different data types,?tuple(),0.4944650531,
2633,write a python program to create a tuple with different data types,"tuple1 = ('Python', 1)
print 'Contents of tuple1: %s'%(tuple1)  # Will throw TypeError",0.4936107993,
2633,write a python program to create a tuple with different data types,"(1, 2) + (3,) * 5",0.4913726151,
2633,write a python program to create a tuple with different data types,"(1,2) + (2,4)",0.4906930029,
2633,write a python program to create a tuple with different data types,"1, 2",0.4876765311,
2633,write a python program to create a tuple with different data types,"(1,2)",0.4876765311,
562,example survey responses,"import random

def generate_new_lyrics(chain):
    """"""
    Args:
      - chain: a dict representing the Markov chain,
               such as one generated by generate_new_lyrics()
    
    Returns:
      A string representing the randomly generated song.
    """"""
    
    # a list for storing the generated words
    words = []
    # generate the first word
    words.append(random.choice(chain[""<START>""]))
    
    # YOUR CODE HERE
    while words[-1] != '<END>':
        words.append(random.choice(chain[words[-1]]))
    
    # join the words together into a string with line breaks
    lyrics = "" "".join(words[:-1])
    return ""\n"".join(lyrics.split(""<N>""))",0.5110269785,
562,example survey responses,"# The main purpose of the caller_func() is to integrate the previous utility functions.

def caller_func():
    '''This caller function will pick a random point from the dataframe and treat it as a test point.
    It'll get the similar users, degree of similarity and then predict rating '''
    test_point = data.sample(n=1)
    test_uid = int(test_point['userId']); test_mid = int(test_point['movieId'])

    similar_users = get_similar_user(test_uid,test_mid,index)

    distances = get_similarity_degree(test_uid,test_mid,similar_users,index,data_dict)

    #sns.distplot(distances)

    observed = float(test_point['rating'])
    predicted = predict_rating(distances,similar_users,test_mid,data_dict, param2) # param2 is the t-th percentile for choosing most-similar users
    
    if (predicted == ""ERROR""):
        return (""ERROR"", ""ERROR"")
    
    return (observed,predicted)",0.5027784109,
562,example survey responses,"def getOneOfEach():
    # List of all the image indeces
    selected = list()
    # list to keep track of labels selected
    slected_labels = list()
    
    # loop until one of each label is found
    while len(selected) < 43:
        index = np.random.randint(len(y_train))
            
        if y_train[index] not in slected_labels:
            selected.append(index)
            slected_labels.append(y_train[index])

    return list(selected)",0.4962028563,
562,example survey responses,"def getPredictedSeries(result):
    import random
    json_result = json.loads(result)
    y_data      = json_result['predictions'][0]
    y_mean      = y_data['mean']
    y_q1        = y_data['quantiles'][q1]
    y_q2        = y_data['quantiles'][q2]
    y_sample    = y_data['samples'][random.randint(0, num_samples)]

    #print(""Mean: %s\n"" % y_mean)
    #print(""Quartile %s: %s\n"" % (q1, y_q1))
    #print(""Quartile %s: %s\n"" % (q2, y_q2))
    return y_mean, y_q1, y_q2, y_sample",0.4916131496,
562,example survey responses,"def game():
    
    record = df5.sample()
    andrew = record['andrew_id_hash'].to_string(header=False, index=False)
    ht = int(record['HT'].to_string(header=False, index=False))
    new_id = record['new_id'].to_string(header=False, index=False)
    
    print(""Helping "" + andrew + "" with "" + new_id + ""!"")
    
    assignment_stats = df5['HT'].where(df5['new_id'] == new_id).dropna()
    plt.hist(assignment_stats, bins=20, range=(0,1800))
    
    person_stats = df5['HT'].where(df5['andrew_id_hash'] == andrew).dropna()
    plt.hist(person_stats, bins=20)
    
    plt.show()
    
    prediction = int(input(""Predict the Help time!: ""))
    
    print(""Your prediction: "" + str(prediction))
    print(""Actual help time: "" + str(ht))
    print(""Error is: "" + str(abs(prediction-ht)/ht * 100))",0.4905181527,
562,example survey responses,"def manually_validate_mnist_dataset(dataset):
    """"""Manually check the dataset by random visualize some of them""""""
    random_train = np.random.randint(1, len(dataset['X_train']))-1
    random_dev = np.random.randint(1, len(dataset['X_dev']))-1
    random_test = np.random.randint(1, len(dataset['X_test']))-1
    print(dataset['Y_train'][random_train], dataset['Y_dev'][random_dev], dataset['Y_test'][random_test])
    fig, (ax1, ax2, ax3) = plt.subplots(1,3, sharey=True, figsize=[10,3])
    ax1.imshow(dataset['X_train'][random_train], cmap='gray')
    ax2.imshow(dataset['X_dev'][random_dev], cmap='gray')
    mappable = ax3.imshow(dataset['X_test'][random_test], cmap='gray')
    fig.colorbar(mappable)
    plt.show()",0.4883044362,
562,example survey responses,"def grid_search(essays, expected_tags):

    rows_ana = []
    proc_essays = processed_essays_use_predicted_tag(essays=essays)

    metrics = get_metrics_raw(proc_essays, expected_tags=expected_tags,  micro_only=True)
    row = metrics[""MICRO_F1""]
    rows_ana.append(row)

    df_results = pd.DataFrame(rows_ana)
    return df_results",0.4878154397,
562,example survey responses,"def new_actions(test):
    ''' samples 5 possible actions from the given test set'''
    C = test[test.position_sign == 'C'].sample(1)
    G = test[test.position_sign == 'G'].sample(2)
    F = test[test.position_sign == 'F'].sample(2)
    return pd.concat([C,G,F])",0.48506248,
562,example survey responses,"import random

def generate_new_lyrics(chain):
    """"""
    Args:
      - chain: a dict representing the Markov chain,
               such as one generated by generate_new_lyrics()
    
    Returns:
      A string representing the randomly generated song.
    """"""
    
    # a list for storing the generated words
    words = []
    # generate the first word
    words.append(random.choice(chain[(None, ""<START>"")]))
    
    # YOUR CODE HERE
    words.append(random.choice(chain[(""<START>""), words[-1]]))
    while words[-1] != ""<END>"":
        words.append(random.choice(chain[(words[-2], words[-1])]))
    
    # join the words together into a string with line breaks
    lyrics = "" "".join(words[:-1])
    return ""\n"".join(lyrics.split(""<N>""))",0.4841758907,
562,example survey responses,"jsample = jeopardy.sample(n=1000)

def overlap_q(row):

    #row = jeopardy.loc[4]
    listofintersects = []
    q_set = row[""questions_sets""]
    q_set_len = len(row[""questions_sets""])
    for i, r in jsample.iterrows():
        new_set_len = 0
        if i == row.name:
            listofintersects.append(0)
        else:
            new_set_len = len(q_set.intersection(r[""questions_sets""]))
            try:
                listofintersects.append(new_set_len/q_set_len)
            except:
                listofintersects.append(0)

    return (np.mean(listofintersects))

jsample[""question_overlap2""] = jsample.apply(overlap_q, axis=1)
mean_answer2 = jsample[""question_overlap2""].mean()
mean_answer2",0.4834729135,
2534,warping,"def fast_warp(image_data, transf, mode = 'edge'):
    """"""Proposed by Florian muellerklein""""""
    out_shape = image_data.shape
    return transform._warps_cy._warp_fast(image_data,  transf.params, output_shape = out_shape, mode = mode)

def batch_augmentation(X, y, batch_size, augmentation = True):
    # Settings
    n = X.shape[0]
    x_translation_bounds = np.array([-2, 2])
    y_translation_bounds = np.array([-2, 2])
    rotation_bounds      = np.array([-5, 5])
    scale_bounds         = np.array([1, 1.1])
    shear_bounds         = np.array([-1, 1])    
    
    # Shuffling
    X, y = shuffle(X, y)
    output_x = []
    output_y = []
    
    for offset in range(0, n, batch_size):
        
        # Slicing
        batch_x, batch_y = X[offset:offset+batch_size], y[offset:offset+batch_size]
        
        if augmentation:
            l = batch_x.shape[0]

            # Translation
            x_translation = np.random.randint(x_translation_bounds[0], x_translation_bounds[1], l) 
            y_translation = np.random.randint(y_translation_bounds[0], y_translation_bounds[1], l)

            # Rotation
            rotation = np.random.uniform(rotation_bounds[0], rotation_bounds[1], l)

            # Scaling
            scale = np.random.uniform(scale_bounds[0], scale_bounds[1], l)

            # Shearing
            shear = np.random.uniform(shear_bounds[0], shear_bounds[1], l)

            # Transformation
            mid_point = np.array(batch_x.shape[1:3])/2.0
            transf_center = transform.SimilarityTransform(translation = -mid_point)
            transf_uncenter = transform.SimilarityTransform(translation = mid_point)

            # Apply transformation
            for i in range(batch_x.shape[0]):
                # Augmentation transformation is unique for each image
                transf_aug = transform.AffineTransform(translation = (x_translation[i], y_translation[i]),
                                                       rotation = np.deg2rad(rotation[i]),
                                                       scale = (1/scale[i], 1/scale[i]),
                                                       shear = np.deg2rad(shear[i]))
                transf = transf_center + transf_aug + transf_uncenter
                batch_x[i] = fast_warp(batch_x[i].reshape((32,32)), transf)
        
        # Storage
        output_x.append(batch_x)
        output_y.append(batch_y)
        
    # Return list of batches
    return output_x, output_y",0.4440976977,
2534,warping,"from mpmath import *
# The mpmath package allows us to compute with arbitrary precision!
# It has specialized functions for log, sin, exp, etc.., with arbitrary precision.
# It is probably installed with your version of Python.

def prob_prime(N, witnesses):
    '''
    Conservatively estimates the probability of primality, given a positive test result.
    N is an approximation of the size of the tested number.
    witnesses is the number of witnesses.
    '''
    mp.dps = witnesses # mp.dps is the number of digits of precision.  We adapt this as needed for input.
    prob_prime = 1 - (log(N) - 1) / (4**witnesses)
    print(str(100*prob_prime)+""% chance of primality"") # Use str to convert mpmath float to string for printing.",0.4271063209,
2534,warping,"def split_data(city_data):
    # Get the features and labels from the Boston housing data
    X, y = city_data.data, boston.target
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30,random_state=None)
    print ""X_training:"", X_train.shape
    print ""X_test:"", X_test.shape 
    return X_train, y_train, X_test, y_test",0.4266751409,
2534,warping,"from utils import warper, src_points, dst_points

def view_perspective_trans(rgb_in):
    img = np.copy(rgb_in)
    warped = warper(img)        
    # draw line
    for i in range(3):
        cv2.line(img,    tuple(src_points[i]), tuple(src_points[i+1]), color=[255, 0, 0], thickness=4)
        cv2.line(warped, tuple(dst_points[i]), tuple(dst_points[i+1]), color=[255, 0, 0], thickness=4)
    
    view_imgs([img, warped], ['input', 'bird eye view'], figsize=(14,14), multi_col=False)
    
view_perspective_trans(undist_img)",0.4257022142,
2534,warping,"def warp(img,M):
    img_size = img_size = (img.shape[1],img.shape[0])
    return cv2.warpPerspective(img,M,img_size,flags=cv2.INTER_LINEAR)",0.4255061746,
2534,warping,"def fast_warp(img, tf, output_shape, mode='wrap'):
    """"""
    Faster warping with skimage transform. 
    Credit: http://florianmuellerklein.github.io/cnn_streetview/
    """"""
    return transform._warps_cy._warp_fast(img, tf.params,
                                          output_shape=output_shape, mode=mode)",0.4253695011,
2534,warping,"def warp(img, M):
    '''
    Apply perspective transform to image `img` and return the transformed or warped image
    '''
    img_size = (img.shape[1], img.shape[0])
    return cv2.warpPerspective(img, M, img_size, flags=cv2.INTER_LINEAR)",0.4233329892,
2534,warping,"def split_data():
    # Get the features and labels from the Boston housing data
    X, y = boston.data, boston.target
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30,random_state=None)
    #print ""X_training:"", X_train.shape
    #print ""X_test:"", X_test.shape 
    return X_train, y_train, X_test, y_test

X_train, y_train, X_test, y_test = split_data()",0.4190584421,
2534,warping,"def loadValidationData():
    validation_x = mnist23.data[training_samples:]
    validation_y = np.array([mnist23.target[training_samples:]]) 
    return validation_x,validation_y",0.4158796668,
2534,warping,"#def get_lane_pixels(img):
#moved to part 3

def fit_line(img, thresholded_img_class):
    thresholded_warpped = thresholded_img_class.get_thresh_warp_img(img)
    leftx, lefty, rightx, righty = get_lane_pixels(thresholded_warpped)
    leftx, lefty, rightx, righty = ti.append_prev_edge_points(leftx, lefty, rightx, righty )
    # Fit a second order polynomial to each fake lane line
    yvals = range(720)
    try:
        left_fit = np.polyfit(lefty, leftx, 2)
        left_fit = ti.set_leftfit(left_fit)
        left_fitx = np.multiply(left_fit[0],np.power(yvals,2)) + np.multiply(left_fit[1],yvals) + left_fit[2]
    except:
        left_fit = ti.set_leftfit(None)
        left_fitx = np.multiply(left_fit[0],np.power(yvals,2)) + np.multiply(left_fit[1],yvals) + left_fit[2]
    
    try:
        right_fit = np.polyfit(righty, rightx, 2)
        right_fit = ti.set_rightfit(right_fit)
        right_fitx = np.multiply(right_fit[0],np.power(yvals,2)) + np.multiply(right_fit[1],yvals) + right_fit[2]
    except:
        right_fit = ti.set_rightfit(None)
        right_fitx = np.multiply(right_fit[0],np.power(yvals,2)) + np.multiply(right_fit[1],yvals) + right_fit[2]
    
    return left_fit, right_fit, left_fitx, right_fitx, leftx, lefty, rightx, righty, yvals
    
    #plt.plot(leftx, lefty, 'o', color='red')
    #plt.plot(rightx, righty, 'o', color='blue')
    #plt.xlim(0, 1280)
    #plt.ylim(0, 720)
    #plt.plot(left_fitx, lefty, color='green', linewidth=3)
    #plt.plot(right_fitx, righty, color='green', linewidth=3)
    #plt.gca().invert_yaxis() # to visualize as we do the images",0.4133265018,
915,independent component analysis,"def pca_sklearn():
    pca = PCA()
    pca.fit(X)

    V = pca.components_
    S = pca.explained_variance_
    X_mean = pca.mean_

    plt.scatter(*X.T, marker='o', edgecolors='b', c='none', s=15);
    plt.plot(*array([X_mean, X_mean + S[0] * V[:,0]]).T, 'r-')
    plt.plot(*array([X_mean, X_mean + S[1] * V[:,1]]).T, 'r-')
    plt.xlim(0,8)
    plt.ylim(2,8)
    
    pca.set_params(n_components=1)
    pca.fit(X)
    X_redux = pca.transform(X)

    plt.scatter(*(pca.inverse_transform(X_redux)).T, marker='o', edgecolors='g', c='none', s=15);
    plt.axis('scaled');
    return S, V

pca_sklearn()",0.4789917171,
915,independent component analysis,"# Complete code below this comment for FA
# ----------------------------------
def FA_explained_variance_ratio(fa):
    fa.explained_variance_ = np.flip(np.sort(np.sum(fa.components_**2, axis=1)), axis=0)
    total_variance = np.sum(fa.explained_variance_) + np.sum(fa.noise_variance_)
    fa.explained_variance_ratio_ = fa.explained_variance_ / total_variance

FA_explained_variance_ratio(fa)
print('FA', fa.explained_variance_ratio_)",0.4629699588,
915,independent component analysis,"MAX = 5000000
def measure(n):
    """"""Measurement model, return two coupled measurements.""""""
    PcmX, PcmY , PcmZ , Pmiss = [], [] , [] , []
    
    for i in range(MAX):        
        x = np.random.uniform(-0.8,0.8)
        y = np.random.uniform(-0.8,0.8)
        z = np.random.uniform(-0.3,1.4)
        pm = np.random.uniform(-0.3,1.)
    
        probability = Gaussian3DwithOffset( (x , y, z , pm) , amplitude=1,
                                       sigma_t=0.155, a1=0.143 , a2=0.159 , b1=0.562 , b2=0.159 )
        uniform = np.random.uniform(0,1.5)
        if uniform<probability:        
            PcmX.append(x)
            PcmY.append(y) 
            PcmZ.append(z) 
            Pmiss.append(pm)
        if (len(Pmiss)%(n/5)==0): print len(Pmiss),'samples so far...'
        if len(Pmiss)>n: break
    return np.array(PcmX), np.array(PcmY) , np.array(PcmZ) , np.array(Pmiss)",0.4315628409,
915,independent component analysis,"def post(self):
    self.spike = (self.v1 < self.v2) * (self.v < self.v2) * (self.v > -30.)
    self.v1 = self.v2
    self.v2 = self.v
    self.count += self.spike

HodgkinHuxley.post = post
HodgkinHuxley.Default_Inters.update({'v1': -65, 'v2': -65, 'count': 0.})",0.4284149706,
915,independent component analysis,"def megaplot(the_data, features):
    """"""
    Produces an N x N scatter plot that visualizes all the data, coloring 
    points by whether or not an employee is a person of interest

    Parameters
    -----------
    the_data : a pandas dataframe with characteristics in features_list. These
               characteristics should be numerical for the output to make
               any sense.
    
    features : the list of features to be plotted
    """"""
    indx = 1
    poi = the_data.poi == True

    fig = plt.figure(figsize=(18,18))

    for ii in range(len(features)):
        for jj in range(len(features)):
            if ii != jj:
                ax = plt.subplot(len(features), len(features) - 1, indx)
                x = the_data[features[jj]]
                y = the_data[features[ii]]
                    
                ax.scatter(list(x[~poi]), 
                           list(y[~poi]), 
                           c=""b"", s=10, edgecolor=""None"")
                
                ax.scatter(list(x[poi]), 
                           list(y[poi]), 
                           c=""r"", s=10, edgecolor=""None"")
                
                ax.set_xticklabels([])
#                 ax.set_ylim([min(y), max(y)])
#                 ax.set_yticklabels([])
                if (jj == 0) or ((jj == 1) & (ii == 0)):
                    ax.set_ylabel(features[ii])
                    
                indx += 1
    plt.show()",0.4276636243,
915,independent component analysis,"def out_impedance(num_component, components, freq, Z_load):
    Z = np.zeros((num_component+1,),dtype = complex)
    Z_out = np.zeros((num_component+1,),dtype = complex)
    # calculate impedance of each component
    Z[0] = Z_load
    print (Z[0],end = '   ')
    for i in range(num_component):
        if components[i][0] == 'C':
            Z[i+1] = 1/(1j*2*np.pi*freq*components[i][2])
        elif components[i][0] == 'L':
            Z[i+1] = 1j*2*np.pi*freq*components[i][2]
        else:
            Z[i+1] = components[i][2]
    # calculate result
    Z_out[0] = Z_load
    for i in range(num_component):
        if components[i][1] == 'ser':
            Z_out[i+1] = series(Z_out[i], Z[i+1])
        else:
            Z_out[i+1] = parallel(Z_out[i], Z[i+1])
        print ('\n Zout', i+1,' :', Z_out[i+1])
    return Z_out[num_component]",0.4251519442,
915,independent component analysis,"def experiment(pca_comp, gmm_comp, cov_type):
    pca = PCA(n_components=pca_comp)
    pca.fit(df_ptrainf) 
    dim_reduc = pca.transform(df_ptrainf) 
    gmm = []
    lp_dev = []
    lp_test = []
    label_set = np.unique(df_ptrain['Activity'])
    for i in range(0,len(label_set)):
        one_label = dim_reduc[df_ptrain['Activity'] == label_set[i]]
        gmm.extend([GaussianMixture(n_components=gmm_comp, covariance_type=cov_type)])
        gmm[i].fit(one_label)
        lp_dev.extend([gmm[i].score_samples(pca.transform(df_pdevf))]) # weighted log probabilities
        lp_test.extend([gmm[i].score_samples(pca.transform(df_ptestf))]) 
    pred_dev = []
    pred_test = []
    for j in range(0,lp_dev[0].shape[0]):
        col = [row[j] for row in lp_dev]
        pred_dev.extend([label_set[np.argmax(col)]])
    for j in range(0,lp_test[0].shape[0]):
        col = [row[j] for row in lp_test]
        pred_test.extend([label_set[np.argmax(col)]]) 
    means = gmm[0].means_.size
    covs = gmm[0].covariances_.size
    ppc = means + covs # parameters per class
    dev_acc = round(metrics.accuracy_score(df_pdev['Activity'],pred_dev)*100,1)
    test_acc = round(metrics.accuracy_score(df_ptest['Activity'],pred_test)*100,1)
    tab_cells.append([means, covs, ppc, dev_acc, test_acc])
    tab_rows.extend([""%d-component PCA with %d-component %s covariance GMM"" 
                     % (pca_comp, gmm_comp, cov_type)])

tab_cols = ['means', 'covariances', 'params per class', 'dev accuracy', 'test accuracy']
tab_rows = []
tab_cells = []
ppc_lim = 50 # arbitrary limit on parameters per class
covs = ['spherical', 'diag', 'tied', 'full'] 

gmm_comp = np.unique(df_ptrain['Activity']).shape[0] # setting to class count for simplicity
for cov in covs:
    if cov == 'spherical':
        pca_comp = 5 # each component has its own single variance
    elif cov == 'diag':
        pca_comp = 3 # each component has its own diagonal covariance matrix
    elif cov == 'tied':
        pca_comp = 4 # all components share the same general covariance matrix
    elif cov == 'full':
        pca_comp = 2 # each component has its own general covariance matrix
    else:
        print(""error: bad cov"")
    if gmm_comp > 0:
        experiment(pca_comp, gmm_comp, cov)
            
tab_cells = np.array(tab_cells)
print(""Accuracy varies somewhat between runs due to random initilization."")
print(""Reasonably close agreement in accuracy against dev and test suggests overfitting isn't excessive."")

plt.figure(figsize=(10, 1)) 
tab = plt.table(cellText=tab_cells,rowLabels=tab_rows, colLabels=tab_cols, 
                cellLoc='center', loc='upper left')
for key, cell in tab.get_celld().items():
    row, col = key
plt.axis('off')
tab.scale(xscale=1, yscale=1.5) # for row spacing relative to text
plt.show()",0.4204011559,
915,independent component analysis,"num_samples = 100000
lower_q, upper_q = [.025, .975]

def plot_ci(p, post, num_samples, lower_q, upper_q):
    ## This function computes a credible interval using an assumption
    ## of symetry in the bulk of the distribution to keep the 
    ## calculation simple. 
    ## Compute a large sample by resampling with replacement
    samples = numpy.random.choice(p, size=num_samples, replace=True, p=post)
    ci = scipy.percentile(samples, [lower_q*100, upper_q*100]) # compute the quantiles
    
    interval = upper_q - lower_q
    pyplot.title('Posterior density with %.3f credible interval' % interval)
    pyplot.plot(p, post, color='blue')
    pyplot.xlabel('Parameter value')
    pyplot.ylabel('Density')
    pyplot.axvline(x=ci[0], color='red')
    pyplot.axvline(x=ci[1], color='red')
    print('The %.3f credible interval is %.3f to %.3f' 
          % (interval, lower_q, upper_q))
    
plot_ci(p, post, num_samples, lower_q, upper_q)",0.4197406173,
915,independent component analysis,"# CAN JUST COPY THIS CELL
def chi2(data, model, errors = None):
    '''Calculates the chi sq given data, model and errors
    Arguments:
    data: series of datapoints (endogenous variable)
    model: series of predicted values corresponding to the observed data
    errors: serie of errors (optional). 
    If errors are not passes all errors are set to 1
    '''
    if errors is None:
        errors = np.ones_like(data)
    if data.shape == model.shape and data.shape == errors.shape:
        return (((data - model)**2) / errors**2).sum()
    else: 
        print ('''ERROR:
must pass arrays of identical dimension for data, model and (optional) error)''')
    return -1",0.4156711102,
915,independent component analysis,"def fit_predict_model(city_data, print_output=True):
    """"""Find and tune the optimal model. Make a prediction on housing data.""""""

    # Get the features and labels from the Boston housing data
    X, y = city_data.data, city_data.target

    # Setup a Decision Tree Regressor
    regressor = DecisionTreeRegressor()

    parameters = {'max_depth':(1,2,3,4,5,6,7,8,9,10)}

    ###################################
    ### Step 4. YOUR CODE GOES HERE ###
    ###################################

    # 1. Find an appropriate performance metric. This should be the same as the
    # one used in your performance_metric procedure above:
    # http://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html
    scorer = make_scorer(mean_squared_error, greater_is_better=False)
    
    # 2. We will use grid search to fine tune the Decision Tree Regressor and
    # obtain the parameters that generate the best training performance. Set up
    # the grid search object here.
    # http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html#sklearn.grid_search.GridSearchCV
    reg = grid_search.GridSearchCV(regressor, parameters, scoring=scorer)
                             

    # Fit the learner to the training data to obtain the best parameter set
    reg = reg.fit(X, y)
    best_model = reg.best_estimator_
    if print_output:
        print ""Final Model: ""
        print best_model

        print ""All scores: "" + str(reg.grid_scores_)
    
    # I have added additional print statements to help undertand output of the Grid Search.     
    optim_max_depth = reg.best_params_['max_depth']
    score = reg.best_score_
    
    if print_output:
        print ""The optimal max_depth parameter found by Grid Search: "" + str(optim_max_depth)
        print ""The score given by Grid Search for the optimal model: "" + str(score)
    
    # Use the model to predict the output of a particular sample
    x = [11.95, 0.00, 18.100, 0, 0.6590, 5.6090, 90.00, 1.385, 24, 680.0, 20.20, 332.09, 12.13]
    y = best_model.predict([x])
    
    if print_output:
        print ""House: "" + str(x)
        print ""Prediction: "" + str(y)
    
    # By returning this tuple of max_depth, model score and predictin, it is possible to analyse a sample of results.    
    return (optim_max_depth, score, y[0])",0.4136795402,
506,drop out,"def dropout(encoder_op, keep_prob):
    with tf.variable_scope(""dropped""):
        dropped = tf.nn.dropout(encoder_op, keep_prob)
    return dropped",0.455388695,
506,drop out,"def drop_out(x_tensor, keep_prob):
    drop = tf.nn.dropout(x_tensor, keep_prob)
    return drop",0.4527363181,
506,drop out,"def apply_threshold(heatmap, threshold):
    heatmap_copy = np.copy(heatmap)
    # Zero out pixels below the threshold
    heatmap_copy[heatmap_copy <= threshold] = 0
    # Return thresholded map
    return heatmap_copy",0.4250958562,
506,drop out,"def apply_heatmap_threshold(heatmap, threshold):
    heatmap_threshold = np.copy(heatmap)
    heatmap_threshold[ heatmap_threshold <= threshold ] = 0
    return heatmap_threshold",0.4250958562,
506,drop out,"def thresholder(matched_img, val):
    threshold_img = np.copy(matched_img)
    threshold_img[matched_img < val] = 0
    return threshold_img",0.4193287492,
506,drop out,"def build_cell(lstm_size, keep_prob):
    lstm = BasicLSTMCell(lstm_size)
    # adding dropout to cell 
    lstm = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)
    return lstm",0.4165289104,
506,drop out,"def heatmap_thresh(heatmap, threshold):
    heatmap_thresh = np.copy(heatmap)
    heatmap_thresh[heatmap_thresh <= threshold] = 0
    return heatmap_thresh

heatmap_t = heatmap_thresh(heatmap, 4)
plot_images([heatmap_t],['Threshold Heatmap'], cmap='hot', fname='heatmap_threshold.jpg')",0.4155070186,
506,drop out,"def dropout_layer(input_layer, keep_prob):
    return tf.nn.dropout(input_layer, keep_prob=keep_prob)",0.4149658978,
506,drop out,"def add_binned_ratings(df, old_col, new_col):
    '''
    Add column for binned ratings.
    
    INPUT:
    - df (full dataframe)
    - old_col (str): column name of average ratings
    - new_col (str): new column name for binned average ratings
    OUTPUT:
    - new dataframe
    '''
    df[new_col] = pd.cut(df[old_col].copy(), bins=[0., 3.99, 4.99, 5],
                            include_lowest=True, right=True)
    df[new_col].cat.add_categories('Missing', inplace=True)
    df[new_col].fillna('Missing', inplace=True)                        
    return df

df = add_binned_ratings(df, 'avg_rating_by_driver', 'bin_avg_rating_by_driver')
df = add_binned_ratings(df, 'avg_rating_of_driver', 'bin_avg_rating_of_driver')",0.4085080624,
506,drop out,"def apply_threshold(heatmap, threshold):
    heated=np.copy(heatmap)
    # Zero out pixels below the threshold
    heated[heated <= threshold] = 0
    # Return thresholded map
    return heated
def draw_labeled_bboxes(img, labels):
    # Iterate through all detected cars
    for car_number in range(1, labels[1]+1):
        # Find pixels with each car_number label value
        nonzero = (labels[0] == car_number).nonzero()
        # Identify x and y values of those pixels
        nonzeroy = np.array(nonzero[0])
        nonzerox = np.array(nonzero[1])
        # Define a bounding box based on min/max x and y
        bbox = ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy)))
        #car.heatmap.append(bbox)
        # Draw the box on the image
        cv2.rectangle(img, bbox[0], bbox[1], (255,0,0), 6)
    # Return the image
    return img",0.4075188041,
2243,summarize the data and engage in elementary data exploration,"def compute_means_and_cov(self):
        self.assets_means = []
        rets = []
        
        #print('asset0', self.assets[0])
        #print('asset1', self.assets[1])

        self.assets_rets = np.array(self.assets_rets)
        print(self.assets_rets.shape)
        #print(self.assets_rets)
        min_size = 9999999999
        print('Computing means and cov for', self.r.shape[1], 'assets')
        print(self.r.shape)
        for asset in range(self.assets_rets.shape[0]):
            print(self.assets_rets[asset].shape)
            print('ass', asset)
            returns = [self.assets_rets[asset][i]['return'] for i in range(self.assets_rets[asset].shape[0])]
            min_size = min(min_size, len(returns))
            rets.append(np.array(returns))
            mean_ret = np.mean(returns)
            self.assets_means.append(mean_ret)
            print('mean', mean_ret)
            #print('rets', returns)

        rets = np.array(rets)

        #self.assets_means = np.array(self.assets_means)

        print(min_size)

        self.rets_norm = []
        for asset in range(self.assets_rets.shape[0]):
            self.rets_norm.append(rets[asset][:min_size])

        self.rets_norm = np.array(self.rets_norm)
        self.cov = np.cov(self.rets_norm)
        
        #print(self.cov.shape)
        
        return (np.array(self.assets_means), self.cov)
    
    portfolio_manager.compute_means_and_cov = compute_means_and_cov",0.5361078978,
2243,summarize the data and engage in elementary data exploration,"def clean_text(data):
    for i, target in zip(range(len(data.data)), data.target):
        #------------TO LOWER CASE-------------
        file = data.data[i].decode(""utf-8"").lower()
        #------------TOKENIZE-------------
        word_tokens = word_tokenize(file)
        #------------REMOVE STOP WORDS-------------
        filtered_sentence = [w for w in word_tokens if not w in stop_words]
        filtered_sentence = []
        for w in word_tokens:
            if w not in stop_words:
                    filtered_sentence.append(w)
        #------------STEMMING WITH PORTER STEMMER-------------
        ps = PorterStemmer()
        stemmedFile = []
        for word in filtered_sentence:
            for w in word.split("" ""):
                stem = ps.stem(w)
                stemmedFile.append(stem)
                #COUNT THE TERMS PER CATEGORY
                term_per_category[train_set.target_names[target]][word] += 1
        #------------PUT FILE BACK-------------
        data.data[i] = ' '.join(stemmedFile)",0.5086866617,
2243,summarize the data and engage in elementary data exploration,"#replacing values first_mean

def value_first_mean ():
    global first_mean
    numerators = [0.0]*len(matrix)
    denominators = [0.0]*len(matrix)
    for i in range(len(matrix)):
        numerators [i] = (1. - responsibility[i]) * matrix[i]
        denominators [i] = (1. - responsibility[i])
    numerator_value = sum(numerators)
    denominator_value = sum(denominators)
    
    first_mean = numerator_value/denominator_value
    return first_mean

def value_second_mean ():
    global second_mean
    numerators = [0.0]*len(matrix)
    denominators = [0.0]*len(matrix)
    for i in range(len(matrix)):
        numerators [i] = responsibility[i] * matrix[i]
        denominators [i] = responsibility[i]
    numerator_value = sum(numerators)
    denominator_value = sum(denominators)
    second_mean = numerator_value/denominator_value


def value_first_cov ():
    global first_cov
    numerators = [0.0]*len(matrix)
    denominators = [0.0]*len(matrix)
    for i in range(len(matrix)):
        numerators [i] = (1 - responsibility[i]) * (matrix[i] - first_mean)**2
        denominators [i] = (1 - responsibility[i])
    numerator_value = sum(numerators)
    denominator_value = sum(denominators)
    first_cov = numerator_value/denominator_value
    first_cov = np.mean(first_cov)*np.identity(2)
    

def value_second_cov ():
    global second_cov
    numerators = [0.0]*len(matrix)
    denominators = [0.0]*len(matrix)
    for i in range(len(matrix)):
        numerators [i] = responsibility[i] * (matrix[i] - second_mean)**2
        denominators [i] = responsibility[i]
    numerator_value = sum(numerators)
    denominator_value = sum(denominators)
    second_cov = numerator_value/denominator_value
    second_cov = np.mean(second_cov)*np.identity(2)


def maximize_step():
    value_first_mean ()
    value_second_mean ()
    value_first_cov ()
    value_second_cov ()
    
def value_phi ():
    phi = sum(responsibility)/len(matrix)",0.5019984245,
2243,summarize the data and engage in elementary data exploration,"def onclick_polygon_interrogation(event):
    global pixelx, pixely, AlldataMasked, FieldMean, Clickedpolygon
    pixelx, pixely = int(event.xdata), int(event.ydata)
    # Generate a point from the location
    Clickpoint = shapely.geometry.Point(pixelx, pixely)
    IrrigatedShapes = IrrigatedPolygons.shapes()
    # Find the polygon that contains the selected point
    Clickedpolygon = []
    for ix, shapes in enumerate(IrrigatedPolygons.shapes()):
        if shapely.geometry.shape(shapes).contains(Clickpoint) == True:
            Clickedpolygon.append(ix)
    # Colour the chosen polygon on the figure
    x = [i[0] for i in IrrigatedShapes[Clickedpolygon[0]].points[:]]
    y = [i[1] for i in IrrigatedShapes[Clickedpolygon[0]].points[:]]
    
    plt.figure(fig1.number)
    plt.plot(x, y, 'r')
    
    # Grab the geometry from the polygon we want to interrogate
    with fiona.open(shape_file) as shapes:
        crs = geometry.CRS(shapes.crs_wkt)
        first_geometry = shapes[Clickedpolygon[0]]['geometry']
        geom = geometry.Geometry(first_geometry, crs=crs)

    querys2 = {'output_crs': 'EPSG:3577',
               'resolution': (-10, 10),
               'geopolygon': geom,
               'time':(start_date, end_date)
              }
    queryls = {'geopolygon': geom,
               'time':(start_date, end_date)
              }

    # dc.load the data using the polygon as the bounds
    Alldata = dict()
    for Sensor in AllSensors:
        if Sensor[0] == 'l':
            try:
                Alldata[Sensor], LScrs, LSaffine = DEADataHandling.load_nbarx(dc, Sensor, queryls, product = 'nbart')
            except TypeError:
                print('No data available for {}'.format(Sensor))
        if Sensor[0] == 's':
            prodname = '{0}_ard_granule'.format(Sensor)
            try:
                Alldata[Sensor], S2crs, S2affine = DEADataHandling.load_sentinel(dc, prodname, querys2)
            except TypeError:
                print('No data available for {}'.format(Sensor))

    # Tidy up our dict to remove any empty keys
    for Sensor in AllSensors:
        try:
            Alldata[Sensor]
            if Alldata[Sensor] is None:
                del Alldata[Sensor]
            else:
                try:
                    Alldata[Sensor].time
                except AttributeError:
                    del Alldata[Sensor]
        except KeyError:
                pass

    # Mask the returned data with the polygon to remove any extra data
    AlldataMasked = Alldata.copy()
    for Sensor in Alldata.keys():
        mask = rasterio.features.geometry_mask([geom.to_crs(Alldata[Sensor].geobox.crs) for geoms in [geom]],
                                               out_shape=Alldata[Sensor].geobox.shape,
                                               transform=Alldata[Sensor].geobox.affine,
                                               all_touched=False,
                                               invert=True)
        AlldataMasked[Sensor] = Alldata[Sensor].where(mask)

    # Calculate the NDVI for each sensor
    for Sensor in AllSensors:
        try:
            AlldataMasked[Sensor]['NDVI'] = BandIndices.calculate_indices(AlldataMasked[Sensor], 'NDVI')
        except KeyError:
            pass

    # Concatenate all out 
    Allvalues = xr.concat([masked.NDVI for masked in AlldataMasked.values()], dim='time')
    Allvalues = Allvalues.sortby('time')
    Allvalues.values[Allvalues.values == -999] = np.nan
    FieldMean = Allvalues.mean(dim = ('x', 'y')).dropna(dim = 'time')",0.4965378642,
2243,summarize the data and engage in elementary data exploration,"%%time

def batch_gen():
    while True:        
        batch_features, batch_labels = [], []
        
        MAX_SIZE = 1500  # pad rest with 0
        
        batch_features = np.zeros((BATCH_SIZE, MAX_SIZE, MAX_SIZE, 3))
        batch_labels = np.zeros((BATCH_SIZE, MAX_SIZE, MAX_SIZE, 1))
        
        idx = 0
        max_r = 0
        max_c = 0
        for _, c_row in train_img_df.sample(BATCH_SIZE).iterrows():
            x, y = 0, 0
            block_f = c_row['images']
            block_l = c_row['masks']
            assert (block_f.shape[:2] == block_l.shape)
            size_r = block_f.shape[0]
            size_c = block_f.shape[1]
            batch_features[idx, x: x + size_r, y: y + size_c] = block_f
            batch_labels[idx, x: x + size_r, y: y + size_c] = np.expand_dims(block_l, -1)
            max_r = max(max_r, size_r)
            max_c = max(max_c, size_c)
            idx += 1
        
        batch_features = batch_features[:, 0: max_r, 0: max_c, :]
        batch_labels = batch_labels[:, 0: max_r, 0: max_c, :]
        yield (batch_features, batch_labels)

# for _ in tqdm(range(100)):
#     nxt = next(batch_gen())
#     len(nxt)
#     nxt[0][0].shape
    
nxt = next(batch_gen())
print('Elements in each generated object', len(nxt))
print('Feature instances {} labels {}'.format(len(nxt[0]), len(nxt[1])))
print('Shape of instance 0 data, label: ', nxt[0][0].shape, nxt[1][0].shape)
# print('Shape of instance 1 data, label: ', nxt[0][1].shape, nxt[1][1].shape)",0.4933168292,
2243,summarize the data and engage in elementary data exploration,"def train_model(model, train_set, num_epochs):
    optimizer = torch.optim.Adam(lr=0.001, params=model.parameters())
    for epoch in range(num_epochs):
        epoch_accuracy, epoch_loss = 0, 0
        train_set_size = 0
        for images, labels in train_set:
            batch_size = images.size(0)
            images_var, labels_var = Variable(images), Variable(labels)
            
            log_probs = model(images_var)
            _, preds = torch.max(log_probs, dim=-1)
            
            loss = nn.NLLLoss()(log_probs, labels_var)
            epoch_loss += loss.data.numpy()[0] * batch_size
            
            accuracy = preds.eq(labels_var).float().mean().data.numpy()[0] * 100.0
            epoch_accuracy += accuracy * batch_size
            train_set_size += batch_size
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        epoch_accuracy = epoch_accuracy / train_set_size
        epoch_loss = epoch_loss / train_set_size
        print(""epoch {}: loss= {:.3}, accuracy= {:.4}"".format(epoch + 1, epoch_loss, epoch_accuracy))
        
    return model",0.4931617379,
2243,summarize the data and engage in elementary data exploration,"def test(loss_vector, accuracy_vector):
    model.eval()
    val_loss, correct = 0, 0
    for data, target in test_loader:
        if cuda:
            data, target = data.cuda(), target.cuda()
        data, target = Variable(data), Variable(target)
        output = model(data)
        val_loss += loss_func(output, target).data[0]
        pred = output.data.max(1)[1] # get the index of the max log-probability
        correct += pred.eq(target.data).cpu().sum()

    val_loss /= len(test_loader)
    loss_vector.append(val_loss)

    accuracy = 100. * correct / len(test_loader.dataset)
    accuracy_vector.append(accuracy)
    
    print('\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        val_loss, correct, len(test_loader.dataset), accuracy))",0.4924066663,
2243,summarize the data and engage in elementary data exploration,"def test(loss_vector, accuracy_vector):
    model.eval()
    test_loss, correct = 0, 0
    for data, target in test_loader:
        if cuda:
            data, target = data.cuda(), target.cuda()
        data, target = Variable(data, volatile=True), Variable(target)
        output = model(data)
        test_loss += F.nll_loss(output, target).data[0]
        pred = output.data.max(1)[1] # get the index of the max log-probability
        correct += pred.eq(target.data).cpu().sum()

    test_loss /= len(test_loader)
    loss_vector.append(test_loss)

    accuracy = 100. * correct / len(test_loader.dataset)
    accuracy_vector.append(accuracy)
    
    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset), accuracy))",0.4924066663,
2243,summarize the data and engage in elementary data exploration,"def validate(loss_vector, accuracy_vector):
    model.eval()
    val_loss, correct = 0, 0
    for data, target in validation_loader:
        if cuda:
            data, target = data.cuda(), target.cuda()
        data, target = Variable(data, volatile=True), Variable(target)
        output = model(data)
        val_loss += F.nll_loss(output, target).data[0]
        pred = output.data.max(1)[1] # get the index of the max log-probability
        correct += pred.eq(target.data).cpu().sum()

    val_loss /= len(validation_loader)
    loss_vector.append(val_loss)

    accuracy = 100. * correct / len(validation_loader.dataset)
    accuracy_vector.append(accuracy)
    
    print('\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        val_loss, correct, len(validation_loader.dataset), accuracy))",0.4924066663,
2243,summarize the data and engage in elementary data exploration,"def fit_predict_model(city_data, print_output=True):
    """"""Find and tune the optimal model. Make a prediction on housing data.""""""

    # Get the features and labels from the Boston housing data
    X, y = city_data.data, city_data.target

    # Setup a Decision Tree Regressor
    regressor = DecisionTreeRegressor()

    parameters = {'max_depth':(1,2,3,4,5,6,7,8,9,10)}

    ###################################
    ### Step 4. YOUR CODE GOES HERE ###
    ###################################

    # 1. Find an appropriate performance metric. This should be the same as the
    # one used in your performance_metric procedure above:
    # http://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html
    scorer = make_scorer(mean_squared_error, greater_is_better=False)
    
    # 2. We will use grid search to fine tune the Decision Tree Regressor and
    # obtain the parameters that generate the best training performance. Set up
    # the grid search object here.
    # http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html#sklearn.grid_search.GridSearchCV
    reg = grid_search.GridSearchCV(regressor, parameters, scoring=scorer)
                             

    # Fit the learner to the training data to obtain the best parameter set
    reg = reg.fit(X, y)
    best_model = reg.best_estimator_
    if print_output:
        print ""Final Model: ""
        print best_model

        print ""All scores: "" + str(reg.grid_scores_)
    
    # I have added additional print statements to help undertand output of the Grid Search.     
    optim_max_depth = reg.best_params_['max_depth']
    score = reg.best_score_
    
    if print_output:
        print ""The optimal max_depth parameter found by Grid Search: "" + str(optim_max_depth)
        print ""The score given by Grid Search for the optimal model: "" + str(score)
    
    # Use the model to predict the output of a particular sample
    x = [11.95, 0.00, 18.100, 0, 0.6590, 5.6090, 90.00, 1.385, 24, 680.0, 20.20, 332.09, 12.13]
    y = best_model.predict([x])
    
    if print_output:
        print ""House: "" + str(x)
        print ""Prediction: "" + str(y)
    
    # By returning this tuple of max_depth, model score and predictin, it is possible to analyse a sample of results.    
    return (optim_max_depth, score, y[0])",0.4918077588,
1229,mpg cars,"def optimal_car(single_cluster):
    '''
    check the output of the function with
        if type(x) == str:
    if it's True, then there is no car available for this cluster
    '''
    global cars_left
    
    unique_capacity, unique_cost, _ = total_cars()
    
    copy_cars_left = cars_left
    cars_possible = []
    cars_available = []
    
    for value in range(len(unique_capacity)):
        if total_demand_satisfied(single_cluster) < unique_capacity[value]:
            cars_possible.append(unique_capacity[value])
    
    cars_possible = sorted(cars_possible)
    cars_available = cars_possible[:]
    
    for car in cars_possible:
        test_value = copy_cars_left[car] - 1
        if test_value < 0:
            cars_available.remove(car)
            
    try:
        optimal_car = min(cars_available)
        return optimal_car
        
    except ValueError:
        return ""no car available""",0.4718484879,
1229,mpg cars,"def run():

    # creates model
    ndim = int(2)
    model = Hive.BeeHive(lower = [-5.12]*ndim  ,
                         upper = [ 5.12]*ndim  ,
                         fun       = evaluator ,
                         numb_bees =  50       ,
                         max_itrs  =  100       ,)

    # runs model
    cost = model.run()

    # plots convergence
    Utilities.ConvergencePlot(cost)

    # prints out best solution
    print(""Fitness Value ABC: {0}"".format(model.best))
    
    return model

sol=run()
rec=np.array(sol.record)
vals=sol.nectars
bests=sol.bests
n=len(vals[0])",0.4587309062,
1229,mpg cars,"def merge(initial_clusters):
    
    '''
    returns the merged clusters 
    '''
    
    global cars_used
    
    initial_cost_ratio = []
    
    for cluster in initial_clusters:
        initial_cost_ratio.append(demand_cost_ratio(cluster, optimal_car(cluster)))
        
    solution = [list(value) for value in initial_clusters]
    
    #sort clusters according to demand/ cost ratio
    cluster_and_cost_dict = dict( zip(initial_cost_ratio, solution))
    sorted_cluster_and_cost_dict = sorted(cluster_and_cost_dict.items(), key = lambda s: s[0])
    
    #first approach was to only consider the 50% worst initial clusters
    #but yields worse results; if you want to try this also uncomment the join with the other 50% before the return
    #this variable mainly tells how around many 100 capacity cars are kept(i.e not selected for the merge in this iteration)
    #split_point = math.floor(len(sorted_cluster_and_cost_dict)/2)
    
    # consindering all initial clusters   
    clusters_to_be_merged = []
    for i in range(len(solution)):
        clusters_to_be_merged.append(sorted_cluster_and_cost_dict[i][1])
    
    
    copy_clusters_to_be_merged = clusters_to_be_merged[:]
    #print(copy_clusters_to_be_merged)
    start_cluster = []
    next_cluster = []
    new_cluster = []
    merged_clusters = []
    new_solution = []

    i = 0
    j = 0
        
    len_initial_cluster = len(clusters_to_be_merged)
    
    while j < len_initial_cluster:
        start_cluster = clusters_to_be_merged[j]
        i = j
    
        while i < len_initial_cluster-1:
            next_cluster = clusters_to_be_merged[i+1]
            merged_clusters = start_cluster[:-1] + next_cluster[1:]

            if new_solution == []: 
                new_solution.append(start_cluster)
            if all([next_cluster not in new_solution]): 
                new_solution.append(next_cluster)
            
            # from the global variable cars_used
            start_car = cars_used.get(str(start_cluster))
            next_car = cars_used.get(str(next_cluster))
            
            # checks if the optimal car for this cluster is available (if not, then merging is not possible)
            if optimal_car(merged_clusters) == ""no car available"":
                break
            else:
                merged_car = optimal_car(merged_clusters)

            # that was a different approach, substituting the demand_cost_ratio and using only cost() in the next if-statement
            #cost_factor_ratio = ((start_car + next_car)/merged_car)
            
            #print(""start_car:"", start_car)
            #print(""next_car:"", next_car)
            #print(""merged_cluster"", total_demand_satisfied(merged_clusters))
            #print(""merged_car"", merged_car)           

            #if the merged_cluster has a better demand_cost_ratio, merge
            if (demand_cost_ratio(start_cluster, start_car)) <= (demand_cost_ratio(merged_clusters, merged_car)):
            
            #different if statement that considers only cost not; unfortunately: IT'S NOT MERGING
            #if (cost(start_cluster)) >= (cost(merged_clusters)):
                
                index = new_solution.index(start_cluster)
                new_solution[index] = merged_clusters[:]
                new_solution.remove(next_cluster)

                del cars_used[str(start_cluster)]
                start_cluster = merged_clusters[:]

                len_initial_cluster = len_initial_cluster - 1
                
                # This is an annoying case. Happens when new_solution's last element is the merged cluster(else statement are the usual steps while if and elif take care of the boundary cases)   
                if len(new_solution) == 1:
                    clusters_to_be_merged = new_solution[:] + clusters_to_be_merged[1:]
                elif new_solution[-1] not in clusters_to_be_merged:
                    index_end = clusters_to_be_merged.index(new_solution[-2])
                    clusters_to_be_merged = new_solution[:] + clusters_to_be_merged[index_end+2:]
                else:
                    index_end = clusters_to_be_merged.index(new_solution[-1])
                    clusters_to_be_merged = new_solution[:] + clusters_to_be_merged[index_end+1:]


                if next_cluster in clusters_to_be_merged:
                    clusters_to_be_merged.remove(next_cluster)

                # update global variable cars_left used for function optimal_car()
                update_cars_left(""+"", start_car)
                update_cars_left(""+"", next_car)
                update_cars_left(""-"", merged_car)

                del cars_used[str(next_cluster)]
                cars_used[str(merged_clusters)] = merged_car

                # update while loops
                i = clusters_to_be_merged.index(start_cluster) - 1
                len_initial_cluster = len(clusters_to_be_merged)

             
            i = i + 1
        j = j + 1
        
    better_solution = clusters_to_be_merged[:]
    
    #only when taking the worst clusters
    #for i in range(len(solution) - split_point):
        #better_solution.append(sorted_cluster_and_cost_dict[i+split_point][1])
    
    return better_solution",0.4550842047,
1229,mpg cars,"def evaluate_solution(list_of_clusters):
    
    global cars_used
    total_cost = 0
   
    unique_capacity, unique_cost, _ = total_cars()
    
    cost_dict = dict(zip(unique_capacity, unique_cost))
    sum_distance = 0
    
    for cluster in list_of_clusters:
        capacity = cars_used.get(str(cluster))
        cost = cost_dict.get(capacity)
        sum_distance = average_dist(cluster)
        
#         for node in range(len(cluster)-1):
#             sum_distance += distance_matrix[node][node+1]
        
        total_cost += (cost*sum_distance)
        
    return total_cost",0.4549185634,
1229,mpg cars,"def make_powell_goggles(bush_img):
    # bush with shades
    shades = bush_img - 100 * raybans_mask.flatten()
    
    # find the correction which triggers mis-identification
    res = minimize(lambda(v): lnprob(shades, v), np.random.normal(size=150))
    
    # add the correction:
    corr = res.x[:,np.newaxis] * eigenshades
    corr = np.sum(corr, axis=0)
    new  = shades + corr
    
    # plot each of them
    fig, axs = plt.subplots(1, 4, figsize=(12,12),
                            subplot_kw={'xticks':[], 'yticks':[]},
                            gridspec_kw=dict(hspace=0.1, wspace=0.1))

    axs[0].imshow(bush_img.reshape(*image_shape), cmap='gray', vmin=0, vmax=256);
    axs[1].imshow(shades.reshape(*image_shape),   cmap='gray', vmin=0, vmax=256);
    axs[2].imshow(corr.reshape(*image_shape),     cmap='RdBu_r');
    axs[3].imshow(new.reshape(*image_shape),      cmap='gray', vmin=0, vmax=256);

    pbush = int(np.round(np.exp(lnprob_bush(bush_img))))
    axs[0].set_title('{0:d}:1 odds for Bush'.format(pbush));
    
    pbush = int(np.round(np.exp(lnprob_bush(shades))))
    axs[1].set_title('{0:d}:1 odds for Bush'.format(pbush));
    
    axs[2].set_title('Printed Pattern')

    ppowell = int(np.round(np.exp(-lnprob_bush(new))))
    axs[3].set_title('{0:d}:1 odds for Powell'.format(ppowell));",0.4522952139,
1229,mpg cars,"def run_simulation(car_density, num_lanes, num_sims):
    '''
    Run the simulation. 
    
    Arguments:
        car_density (float) The fraction of cells that have a car.
        
        num_lanes (int) The number of lanes. Default: 1 or 2. 
        
        num_sims (int) The number of times to run the simulation. 
        
    Outputs:
        Average traffic flow (float) The traffic flow (number of cars passing 
          the periodic boundary condition) averaged for a specific set of paramters. 
    '''
    sim = TrafficSimulation(car_density = car_density, num_lanes = num_lanes)
    sim.display()
    for x in range(num_sims):
        sim.update()
        sim.display()
    print('Traffic density:', sim.car_density)
    print('Average traffic flow:', sim.traffic_flow / sim.time_step)",0.4511016607,
1229,mpg cars,"def cluster_initialization():

    global cars_left
    global cars_used
    
    list_of_clusters = []
    customers = list(range(len(demand_matrix))[1:-1])
    visited_customers = []
    remaining_distance_matrix = copy.deepcopy(distance_matrix[0])

    while len(visited_customers) < len(customers)-1: 
        cluster = []
        demand = 0

        #add customers with smallest demand/distance ratio to cluster as long as overall demand of cluster is <= 100
        while total_demand_satisfied(cluster) < min(capacity_matrix):
            best_demand_distance_ratio = 0
            
            if cluster == []:
                #add depot, because that's where we are starting
                cluster.append(0)
                #choose customer furthest away from depot to create cluster
                while(cluster == [0]):
                    if visited_customers == []:
                        pass
                    else:
                        for values in visited_customers:
                            remaining_distance_matrix[values] = 0
                    first_customer = np.argmax(remaining_distance_matrix)
                    #add the best fitting customer to the cluster 
                    if first_customer not in visited_customers:
                        visited_customers.append(first_customer)
                        cluster.append(first_customer)

            else:
                #go through all customers
                for i in range(len(customers)):

                    #if customer is not added yet
                    if customers[i] not in visited_customers:
                        
                        #create a hypothetical cluster with the regarded customer and check the resulting demand-distance-ratio
                        cluster_copy = cluster[:]
                        cluster_copy.append(customers[i])
                        cluster_copy.append(0)

                        #calculate the demand_distance_ratio for comparison
                        demand_distance_ratio = total_demand_satisfied(cluster_copy) / average_dist(cluster_copy)

                        #find best customer
                        if demand_distance_ratio > best_demand_distance_ratio and total_demand_satisfied(cluster_copy) <= min(capacity_matrix):
                            best_demand_distance_ratio = demand_distance_ratio
                            best_customer = customers[i]

                if best_customer in visited_customers:
                    break
                #add the best fitting customer to the cluster
                else:
                    cluster.append(best_customer)
                    visited_customers.append(best_customer)
        cluster.append(0)
        update_cars_left(""-"", min(capacity_matrix))
        
        cars_used[str(cluster)] = min(capacity_matrix)
        
        list_of_clusters.append(cluster)
        
    return np.asarray([np.array(clusters) for clusters in list_of_clusters])",0.4489611089,
1229,mpg cars,"def make_data(df):
    ##add points for away and home team : win 3 points, draw 1 point, loss 0 point
    df['HP']=np.select([df['FTR']=='H',df['FTR']=='D',df['FTR']=='A'],[3,1,0])
    df['AP']=np.select([df['FTR']=='H',df['FTR']=='D',df['FTR']=='A'],[0,1,3])
    ## add difference in goals for home and away team
    df['HDG']=df['FTHG']-df['FTAG']
    df['ADG']=-df['FTHG']+df['FTAG']
    ##add momentum to data 
    cols=['Team','Points','Goal','Shoot','TargetShoot','DiffG']
    df1=df[['HomeTeam','AwayTeam','HP','AP','FTHG','FTAG','HS','AS','HST','AST','HDG','ADG']]
    df1.columns=[np.repeat(cols,2),['Home','Away']*len(cols)]
    d1=df1.stack()
    ##find momentum of previous five games for each team
    mom5 = d1.groupby('Team').apply(lambda x: x.shift().rolling(5, 4).mean())
    mom=d1.groupby('Team').apply(lambda x: pd.expanding_mean(x.shift()))
    ##add the found momentum to the dataframe
    df2=d1.assign(MP=mom5['Points'],MG=mom5['Goal'],MS=mom5['Shoot'],MST=mom5['TargetShoot'],MDG=mom5['DiffG'],AP=mom['Points'],AG=mom['Goal'],AS=mom['Shoot'],AST=mom['TargetShoot'],ADG=mom['DiffG']).unstack()
    df2=df2.drop(['Points','Goal','Shoot','TargetShoot','DiffG'],axis=1)
    df_final=pd.merge(df[['HomeTeam','AwayTeam','FTR','B365H','B365D','B365A','Ade','Aatt','Apo','Atot','Hde','Hatt','Hpo','Htot']],df2,left_on=['HomeTeam','AwayTeam'],right_on=[df2['Team']['Home'],df2['Team']['Away']])
    df_final=df_final.dropna(axis=0,how='any')
    ##Full time results ('FTR') : Home=0,Draw=1,Away=2
    Y_all=df_final['FTR']
    ##Full time results ('FTR') : Home=0,Draw=1,Away=2
    ##Prediction of betting company (bet365)=Y_Bet
    Y_Bet=df_final[['B365H','B365D','B365A']].apply(lambda x:1/x)
    ## winner based on bet365 data
    Y_Bet_FTR=np.select([Y_Bet.idxmax(axis=1)=='B365H',Y_Bet.idxmax(axis=1)=='B365D',Y_Bet.idxmax(axis=1)=='B365A'],['H','D','A'])
    ##scale data
    df_X=df_final.drop([('Team', 'Home'),('Team', 'Away'),'FTR','HomeTeam','AwayTeam','B365H','B365D','B365A'],axis=1)
    return df_X, Y_all,Y_Bet,Y_Bet_FTR
df_X, Y_all,Y_Bet,Y_Bet_FTR=make_data(df)",0.4472166598,
1229,mpg cars,"def make_powell_goggles(bush_img):
    # bush with shades
    shades = bush_img - 100 * raybans_mask.flatten()
    
    # find the best correction which triggers misidentification
    res = minimize(lambda(v): lnprob(shades, v), np.random.normal(size=150))
    
    return (res.success, res.x)",0.4462483823,
1229,mpg cars,"def process_image(img):

    global heat, carslist
    
    frame_heat = np.zeros_like(img[:,:,0]).astype(np.float32)
    
    img = img.astype(np.float32)/np.amax(img)
    img_out = np.copy(img*255).astype(np.uint8)
    
    for y, w in zip(y_start_stops, w_sizes):
        windows = slide_window(img, x_start_stop=[None, None], y_start_stop=y, 
                            xy_window=w, xy_overlap=(0.5, 0.5))
        hot_windows = search_windows(img, windows, clf, X_scaler, color_space=color_space, 
                                spatial_size=spatial_size, hist_bins=hist_bins, 
                                orient=orient, pix_per_cell=pix_per_cell, 
                                cell_per_block=cell_per_block, 
                                hog_channel=hog_channel, spatial_feat=spatial_feat, 
                                hist_feat=hist_feat, hog_feat=hog_feat)
    
        # Add heat to window with a detection
        frame_heat = add_heat(frame_heat,hot_windows)
    
    if len(heat) >= 6:
        del heat[0]
    
    heat.append(frame_heat)
    
    # Find final boxes from heatmap using label function
    labels = label(apply_threshold(np.sum(heat,0), HEAT_THRESH))
    img_out, boxes = draw_labeled_bboxes(img_out, labels)
    
    for b in boxes:
        continue
        
    
    return img_out",0.4400786161,
2421,trend lines in pyplot,"palantir.plot.highlight_cells_on_tsne(tsne, cells)",0.6372579336,
2421,trend lines in pyplot,"spectrum_augite1.make_baseline(linetype='spline')
fig, ax = spectrum_augite1.plot_showbaseline()",0.6314835548,
2421,trend lines in pyplot,"from lint.models import Offset

import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
%matplotlib inline

def plot_baseline_series(year1=y1, year2=y2):
    
    series = Offset.baseline_series(year1, year2)
    
    plt.title('All tokens')
    plt.xlabel('Position')
    plt.ylabel('Count')
    plt.plot(*zip(*series.items()))
    plt.show()",0.6252402067,
2421,trend lines in pyplot,"dirname = 'landau-kd035'

osiris.plot_tk_arb(dirname,'Ex',klim=0.5,tlim=50)
#rundir, field, title='potential', klim=5,tlim=100",0.6214398742,
2421,trend lines in pyplot,"# each spine and line is actually a 3D point

from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt

segmentID = [] # set segmentID to empty and we can get objects from all segments

# grab all lines
xyzLine = m.stacks[stackIdx].line.getLine(segmentID) #this returns a 2d numpy array with columns of (x,y,z)

# grab all x, y, and z of spines
x = m.stacks[stackIdx].getStackValues2('x', segmentID=segmentID)
y = m.stacks[stackIdx].getStackValues2('y', segmentID=segmentID)
z = m.stacks[stackIdx].getStackValues2('z', segmentID=segmentID) # assuming z-step is 1 um this is equivalent to um (not always the case!)

# plot with matplotlib
fig = plt.figure(figsize=(10, 10))
ax = fig.add_subplot(111, projection='3d')

ax.scatter(xyzLine[:,0], xyzLine[:,1], xyzLine[:,2], marker='.', c='g') # plot lines
ax.scatter(x, y, z, marker='.', c='y') # plot spines

ax.set_xlabel('x (um)')
ax.set_ylabel('y (um)')
ax.set_zlabel('z (slices)')

numSlices = m.stacks[stackIdx].numSlices # FIXED: this is not valid until image is loaded -->> BAD BAD BAD
ax.set_zlim(numSlices, 0)

plt.show()",0.6201584935,
2421,trend lines in pyplot,"der.compute_marker_trends(data_part, None, False, n_jobs=1)",0.6195046902,
2421,trend lines in pyplot,"plt.plot(event_range, [l[0] for l in abcd_at_events_005])
plt.ylim(0,900)",0.6183465719,
2421,trend lines in pyplot,"fig,ax=plt.subplots() 
print(type(ax))
indx=[0,1,2,3,4,5,6,7,8,9]
ax.stem(indx,Xabs)",0.617592454,
2421,trend lines in pyplot,"plot_trend=x.plot_trend(pol_degree=-1)  
plot_trend.show()",0.617262125,
2421,trend lines in pyplot,"from utils import plot_straight_tracks

plot_straight_tracks(event, labels=None)
plt.xlim(-0.5, 9.5);",0.6172276735,
940,installing pytorch,"""""""
    FineTuneModel: https://gist.github.com/panovr/2977d9f26866b05583b0c40d88a315bf
    
    ResNet: 
        https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py
        
        Layer Size:
            0: (0:4) 1/4
            1: (4:5) 1/4
            2: (5:6) 1/8
            3: (6:7) 1/16
            4: (7:8) 1/32
            5: (8:9) 1/32


""""""

class CommonModel(nn.Module):
    def __init__(self, original_model, arch):
        super(CommonModel, self).__init__()
        if arch.startswith('resnet') :
            self.unary_2M = nn.Sequential(*list(original_model.children())[0:7])
            self.unary_1M = nn.Sequential(*list(original_model.children())[7:8])
    def forward(self, x):
        _2M = self.unary_2M(x) 
        _1M = self.unary_1M(_2M)
        return _2M, _1M

class PotentialModel(nn.Module):
    def __init__(self, arch, output_channels):
        super(PotentialModel, self).__init__()
        
        # Scale 1
        self.unary_2M_to_8M = nn.ConvTranspose2d(kernel_size=8, stride=4, padding=2, in_channels=256, out_channels=256, groups=256, bias=False)
        self.unary_1M_to_8M = nn.ConvTranspose2d(kernel_size=16, stride=8, padding=4, in_channels=512, out_channels=512, groups=512, bias=False)

        tmp_model = models.__dict__[args.arch](pretrained=True)
        
        # Scale 2
        tmp_model.inplanes = 3
        self.unary_layer_raw = tmp_model._make_layer(models.resnet.BasicBlock, 128, 2, stride=4)
        
        # Merged
        tmp_model.inplanes = 896
        self.unary_layer1 = tmp_model._make_layer(models.resnet.BasicBlock, 256, 2, stride=1)
        tmp_model.inplanes = 256
        self.unary_layer2 = tmp_model._make_layer(models.resnet.BasicBlock, 128, 2, stride=1)

        self.unary_deconv = nn.ConvTranspose2d(kernel_size=8, stride=4, padding=2, in_channels=128, out_channels=output_channels, bias=True)
        
        
    def forward(self, x, _2M, _1M):
        raw = self.unary_layer_raw(x)
        col1 = self.unary_2M_to_8M(_2M)
        col2 = self.unary_1M_to_8M(_1M)
        cat = torch.cat([raw, col1, col2], 1)
        logcat = torch.log1p(cat)
        layer1 = self.unary_layer1(logcat)
        layer2 = self.unary_layer2(layer1)
        output = self.unary_deconv(layer2)
#         output = deconv.view(deconv.nelement(), -1)
        return output

class CrfLossModel(nn.Module):
    def __init__(self):
        pass
    
    def forward(self, unary, pairwise, gt):
        E_unary          = (unary-gt)**2
        E_pairwise_up    = pairwise[:,0,:,:].repeat(gt.size()) * ((generateShiftGd(gt, 0) - gt) ** 2)
        E_pairwise_right = pairwise[:,1,:,:].repeat(gt.size()) * ((generateShiftGd(gt, 1) - gt) ** 2)
        E_pairwise_down  = pairwise[:,2,:,:].repeat(gt.size()) * ((generateShiftGd(gt, 2) - gt) ** 2)       
        E_pairwise_left  = pairwise[:,3,:,:].repeat(gt.size()) * ((generateShiftGd(gt, 3) - gt) ** 2)
        E = E_unary + E_pairwise_up + E_pairwise_right + E_pairwise_down + E_pairwise_left
        Z = torch.sum(E)
        return E/Z
    
    def generateShiftGd(gt, dir_):
        if dir_ == 0:
            new = torch.zeros(gt.size())
            new[:,:,0,:] = gt[:,:,0,:]
            new[:,:,1:,:] = gt[:,:,:-1,:]
            return new
        elif dir_ == 1:
            new = torch.zeros(gt.size())
            new[:,:,:,-1] = gt[:,:,:,-1]
            new[:,:,:,:-1] = gt[:,:,:,1:]
            return new
        elif dir_ == 2:
            new = torch.zeros(gt.size())
            new[:,:,-1,:] = gt[:,:,-1,:]
            new[:,:,:-1,:] = gt[:,:,1:,:]
            return new
        elif dir_ == 3:
            new = torch.zeros(gt.size())
            new[:,:,:,0] = gt[:,:,:,0]
            new[:,:,:,1:] = gt[:,:,:,:-1]
            return new
        
class Net(nn.Module):
    def __init__(self, original_model, CommonModel, PotentialModel, CrfLossModel=None):
        super(Net, self).__init__()
        self.common_net = CommonModel(original_model, args.arch)
        self.unary_net = PotentialModel(args.arch, output_channels=3)
        self.pairwise_net = PotentialModel(args.arch, output_channels=4)
#         self.crfloss_net = CrfLossModel() 
        
    def forward(self,x, gt):
        _2M, _1M = self.common_net(x)
        unary = self.unary_net(x, _2M, _1M)
        pairwise = self.pairwise_net(x, _2M, _1M)
#         crfloss = self.crfloss_net(unary, pairwise, gt)
#         return crfloss, unary, pairwise
        return unary, pairwise

def cat(a, b, axis=0):
    if a is None:
        a = b
    else:
        a = np.concatenate((a,b), axis=axis)
    return a

def generateR(pairwise_):
    
    n,_,h,w = pairwise_.numpy().shape
    c = 3
    
    nele = n*c*h*w 
    planesize = h*w
    imgsize = c*h*w
    row = np.linspace(0, planesize-1, planesize, dtype=np.uint32)
    col_up = (row.copy() - w).clip(0,planesize-1)
    col_right = (row.copy() + 1).clip(0,planesize-1)
    col_down = (row.copy() + w).clip(0,planesize-1)
    col_left = (row.copy() -1).clip(0,planesize-1)

    row = row.reshape((1,-1))
    col_up = col_up.reshape((1,-1))
    col_right = col_right.reshape((1,-1))
    col_down = col_down.reshape((1,-1))
    col_left = col_left.reshape((1,-1))

    
    tmp_row = None
    tmp_col = None
    
    tmp_col_up = None
    tmp_col_right = None
    tmp_col_down = None
    tmp_col_left = None
    
    
    for i in range(0, 4):
        
        for j in range(0,4):
            tmp_row = cat(tmp_row, row + planesize*i)
        tmp_col_up = cat(tmp_col_up, col_up + planesize*i)
        tmp_col_right = cat(tmp_col_right, col_right + planesize*i)
        tmp_col_down = cat(tmp_col_down, col_down + planesize*i)
        tmp_col_left = cat(tmp_col_left, col_left + planesize*i)

        tmp_col = cat(tmp_col, tmp_col_up)
        tmp_col = cat(tmp_col, tmp_col_right)
        tmp_col = cat(tmp_col, tmp_col_down)
        tmp_col = cat(tmp_col, tmp_col_left)
        
        
    Rdata = pairwise[tmp_col,0]
    
    ind = np.zeros((2, tmp_row.size))
    ind[0,:] = tmp_row
    ind[1,:] = tmp_col
    return Rdata, ind
    
def getBestPredictY(unary_, pairwise_):
    Rdata_np, ind_np = generateR(pairwise_)
    R = torch.sparse(torch.from_numpy(), )
    
class BestPredictFunc(torch.autograd.Function):
    def forward(self, unary_, pairwise_, gt_):
        n,c,h,w = gt_.numpy().shape
        
        self.nele = n*c*h*w 
        self.planesize = h*w
        self.imgsize = c*h*w
        self.row = np.linspace(0, self.planesize-1, self.planesize, dtype=np.uint32)
        tmp = np.linspace(0, self.planesize-1, self.planesize, dtype=np.uint32)
        self.col_up = (tmp.copy() - w).clip(0,self.planesize-1)
        self.col_right = (tmp.copy() + 1).clip(0,self.planesize-1)
        self.col_down = (tmp.copy() + w).clip(0,self.planesize-1)
        self.col_left = (tmp.copy() -1).clip(0,self.planesize-1)
    
        self.row = self.row.reshape((1,-1))
        self.col_up = self.col_up.reshape((1,-1))
        self.col_right = self.col_right.reshape((1,-1))
        self.col_down = self.col_down.reshape((1,-1))
        self.col_left = self.col_left.reshape((1,-1))
        
        self.unary    = unary_.clone().view(-1,1).numpy().astype(np.float32)
        self.pairwise = pairwise_.clone().view(-1,1).numpy().astype(np.float32)
        self.gt = gt_.clone().view(-1,1).numpy().astype(np.float32)
        self.R = lil_matrix((self.imgsize, self.imgsize))
        for i in range(0, 4):
            tmp_row = np.concatenate(tmp_row,torch.from_numpy(self.row + self.planesize).type(torch.LongTensor)
            tmp_col_up = torch.from_numpy(self.col_up + self.planesize).type(torch.LongTensor)
            tmp_col_right = torch.from_numpy(self.col_right + self.planesize).type(torch.LongTensor)
            tmp_col_down = torch.from_numpy(self.col_down + self.planesize).type(torch.LongTensor)
            tmp_col_left = torch.from_numpy(self.col_left + self.planesize).type(torch.LongTensor)
            tmp_pw_up = torch.from_numpy(self.pairwise[self.col_up + self.planesize,0]).type(torch.FloatTensor)
            tmp_pw_right = torch.from_numpy(self.pairwise[self.col_right + self.planesize,0]).type(torch.FloatTensor)
            tmp_pw_down = torch.from_numpy(self.pairwise[self.col_down + self.planesize,0]).type(torch.FloatTensor)
            tmp_pw_left = torch.from_numpy(self.pairwise[self.col_left + self.planesize,0]).type(torch.FloatTensor)
            
        self.I = lil_matrix((self.imgsize,self.imgsize))
        self.I.setdiag(1)
        self.D = lil_matrix((self.imgsize,self.imgsize))
        self.D.setdiag(self.R.sum(axis=1))
        self.A = self.I + self.D - self.R
        predict_albedo = spsolve(self.A.tocsr(), self.unary)   
        tw = torch.LongTensor(w)
        th = torch.LongTensor(h)
        self.save_for_backward(th, tw)
        return torch.from_numpy(predict_albedo).type(torch.FloatTensor)
    
    def backward(self, grad_output):
        th, tw = self.saved_tensors
        return torch.zeros(1,3,th[0],tw[0]), torch.zeros(1,4,th[0],tw[0]), torch.zeros(1,3,th[0],tw[0])
    
class BestPredictModule(nn.Module):
    def __init__(self, n,c,h,w):
        super(BestPredictModule, self).__init__()
        self.nele = n*c*h*w 
        self.planesize = h*w
        self.imgsize = c*h*w
        print ""self.imgsize = "", self.imgsize
        self.row = np.linspace(0, self.planesize-1, self.planesize, dtype=np.uint32)
        tmp = np.linspace(0, self.planesize-1, self.planesize, dtype=np.uint32)
        self.col_up = (tmp.copy() - w).clip(0,self.planesize-1)
        self.col_right = (tmp.copy() + 1).clip(0,self.planesize-1)
        self.col_down = (tmp.copy() + w).clip(0,self.planesize-1)
        self.col_left = (tmp.copy() -1).clip(0,self.planesize-1)
    
        self.row = self.row.reshape((1,-1))
        self.col_up = self.col_up.reshape((1,-1))
        self.col_right = self.col_right.reshape((1,-1))
        self.col_down = self.col_down.reshape((1,-1))
        self.col_left = self.col_left.reshape((1,-1))
        
    def forward(self, unary_, pairwise_, gt_):
        return BestPredictFunc()(unary_, pairwise_, gt_)",0.4989683032,
940,installing pytorch,"import syft as sy
import torch
# this is our hook
hook = sy.TorchHook()",0.494484961,
940,installing pytorch,"MODEL_VERSION='v1'
RUNTIME_VERSION='1.10'
MODEL_CLASS='model.PyTorchIrisClassifier'

!gcloud alpha ml-engine versions create {MODEL_VERSION} --model={MODEL_NAME} \
            --origin=gs://{BUCKET}/{GCS_MODEL_DIR} \
            --runtime-version={RUNTIME_VERSION} \
            --framework='SCIKIT_LEARN' \
            --python-version=2.7 \
            --package-uris=gs://{BUCKET}/{GCS_PACKAGE_URI}\
            --model-class={MODEL_CLASS}",0.490285337,
940,installing pytorch,"import os

!pip install yacs opencv-python json_tricks pycocotools

if not os.path.isfile('pose_hrnet_w48_384x288.pth'):
    !curl -O https://eurmqg.ch.files.1drv.com/y4mNy2IRQtZ8_2fiDe8-Usmo4MGjRnWRgb2y3jGIx8wq5PQ4ABVUqSZqaumGtuUfhjQpNN8OlpOQZB3Q5gZ11lif97gLj8zaKgPgyYzFdgd8yIiq8rFWsh5oTIxCh9UTyah8gxiqS9ldmIp24C_zKrNOXrMwttYDiMZdA8XaY-g5t3bt2G37fjfWk6qnkyb7lHUFaW-zUhddzpClyRgYCNZYA/pose_hrnet_w48_384x288.pth

if not os.path.isdir('deep-high-resolution-net.pytorch'):
    !git clone https://github.com/leoxiaobin/deep-high-resolution-net.pytorch.git
    !cd deep-high-resolution-net.pytorch/lib/nms/ && python setup_linux.py build && python setup_linux.py install",0.4755084515,
940,installing pytorch,??torch.unbind,0.4747571945,
940,installing pytorch,"from google.colab import drive
drive.mount('/content/gdrive')

# http://pytorch.org/
from os.path import exists
from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag
platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())
cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\.\([0-9]*\)\.\([0-9]*\)$/cu\1\2/'
accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'
!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision",0.47179842,
940,installing pytorch,"from syft.core.hooks import TorchHook
import torch

# this is our hook
hook = TorchHook()",0.4698714018,
940,installing pytorch,import connector_kit,0.4682862759,
940,installing pytorch,"from clipper_admin.deployers import pytorch as pytorch_deployer
pytorch_deployer.deploy_pytorch_model(
    clipper_conn,
    name=""pytorch-model"",
    version=1, 
    input_type=""bytes"", 
    func=predict_torch_model,
    pytorch_model=model,
)",0.4679653049,
940,installing pytorch,"!cd my-model && \
    ks registry add seldon-core ../../../seldon-core && \
    ks pkg install seldon-core/seldon-core@master",0.4671500027,
195,check the performance of our kmeans test,"def print_bench_k_means(estimator, name, data, labels):
    t0 = time()
    estimator.fit(data)
    print('%-9s\t%.2fs\t%i\t%.3f\t%.3f\t%.3f\t%.3f\t%.3f\t%.3f\t%.3f'
          % (name, (time() - t0), estimator.inertia_,
             metrics.homogeneity_score(labels, estimator.labels_),
             metrics.completeness_score(labels, estimator.labels_),
             metrics.v_measure_score(labels, estimator.labels_),
             metrics.adjusted_rand_score(labels, estimator.labels_),
             metrics.adjusted_mutual_info_score(labels, estimator.labels_),
             metrics.calinski_harabaz_score(data, estimator.labels_) ,
             metrics.silhouette_score(data, estimator.labels_, metric='euclidean', sample_size=1000))
         )",0.5612770319,
195,check the performance of our kmeans test,"from time import time

sample_size = 300

def bench_k_means(estimator, name, data):
    ""Calculate various metrics for comparing system clustering to a gold partition""
    t0 = time()
    estimator.fit(data)
    print('% 9s   %.2fs    %i   %.3f   %.3f   %.3f   %.3f   %.3f    %.3f'
          % (name, (time() - t0), estimator.inertia_,
             metrics.homogeneity_score(labels, estimator.labels_),
             metrics.completeness_score(labels, estimator.labels_),
             metrics.v_measure_score(labels, estimator.labels_),
             metrics.adjusted_rand_score(labels, estimator.labels_),
             metrics.adjusted_mutual_info_score(labels,  estimator.labels_),
             metrics.silhouette_score(data, estimator.labels_,
                                      metric='euclidean',
                                      sample_size=sample_size)))

# print table header
print(75 * '_')
print('init         time  inertia    homo   compl  v-meas     ARI     AMI silhouet')
print(75 * '_')

# benchmark k-means++ initialisation
bench_k_means(KMeans(init='k-means++', n_clusters=n_digits, n_init=10),
              name=""k-means++"", data=data)

# benchmark random initialisation
bench_k_means(KMeans(init='random', n_clusters=n_digits, n_init=10),
              name=""random"", data=data)

# benchmark PCA initalisation
pca = PCA(n_components=n_digits).fit(data)
bench_k_means(KMeans(init=pca.components_, n_clusters=n_digits, n_init=1),
              name=""PCA-based"",
              data=data)

print(75 * '_')",0.5561261773,
195,check the performance of our kmeans test,"def bench_k_means(estimator, name, data):
    t0 = time()
    
    # Run the k-means algoritmh
    estimator.fit(data) 
    
    # Print metrics
    print('%-9s\t%.2fs\t%i\t%.3f\t%.3f\t%.3f\t%.3f\t%.3f\t%.3f'
          % (name, (time() - t0), estimator.inertia_, # Sum of squared distances of samples to their closest cluster center.
             metrics.homogeneity_score(labels, estimator.labels_),
             metrics.completeness_score(labels, estimator.labels_),
             metrics.v_measure_score(labels, estimator.labels_),
             metrics.adjusted_rand_score(labels, estimator.labels_),
             metrics.adjusted_mutual_info_score(labels,  estimator.labels_),
             metrics.silhouette_score(data, estimator.labels_,
                                      metric='euclidean',
                                      sample_size=300)))",0.5455234051,
195,check the performance of our kmeans test,"sample_size = 300

def bench_k_means(estimator, name, data, labels):
    t0 = time()
    estimator.fit(data)
    print('%-9s\t%.2fs\t%i\t%.3f\t%.3f\t%.3f\t%.3f\t%.3f\t%.3f'
          % (name, (time() - t0), estimator.inertia_,
             metrics.homogeneity_score(labels, estimator.labels_),
             metrics.completeness_score(labels, estimator.labels_),
             metrics.v_measure_score(labels, estimator.labels_),
             metrics.adjusted_rand_score(labels, estimator.labels_),
             metrics.adjusted_mutual_info_score(labels,  estimator.labels_),
             metrics.silhouette_score(data, estimator.labels_,
                                      metric='euclidean',
                                      sample_size=sample_size)))

labels = feats[:,-1]
print(82 * '_')
print('init\t\ttime\tinertia\thomo\tcompl\tv-meas\tARI\tAMI\tsilhouette')

# INSTRUCTIONS: Use the bench_k_means method to compare your clustering results
#",0.5358036757,
195,check the performance of our kmeans test,"def bench_k_means(estimator, name, data):
    results = {'time':0, 'inertia':0, 'homo':0, 'compl':0, 'v-meas':0, 'ARI':0, 'silhouette':0}
    print(79 * '_')
    print('% 9s' % 'init''         time   inertia   homo   compl   v-meas   ARI   silhouette')
    t0 = time()
    estimator.fit(data)
    results['time'] = (time() - t0)
    results['inertia'] =  estimator.inertia_
    results['homo'] = metrics.homogeneity_score(labels, estimator.labels_)
    results['compl'] = metrics.completeness_score(labels, estimator.labels_)
    results['v-meas'] = metrics.v_measure_score(labels, estimator.labels_)
    results['ARI'] = metrics.adjusted_rand_score(labels, estimator.labels_)
    results['silhouette'] = metrics.silhouette_score(data, estimator.labels_, metric='euclidean', sample_size=sample_size)
    print('%s % 7.2fs % 5i % 10.3f % 5.3f % 7.3f % 7.3f % 7.3f'
      % (name, results['time'], results['inertia'], results['homo'], results['compl'], results['v-meas'],
         results['ARI'], results['silhouette']))
    print(79 * '_')
    return results",0.5292756557,
195,check the performance of our kmeans test,"# print classification results

def test_results(y_test, y_predicted):

    print '\nThe accuracy is: {0:4.2} ' \
    .format(accuracy_score(y_test, y_predicted))

    print '\nThe confusion matrix: '
    cm = confusion_matrix(y_test, y_predicted)
    print cm

    print '\nThe True Positive rate is: {0:4.2}' \
    .format(float(cm[1][1])/np.sum(cm[1]))

    print '\nThe Matthews correlation coefficient: {0:4.2f} \n' \
    .format(matthews_corrcoef(y_test, y_predicted))

    print(classification_report(y_test, y_predicted))",0.5284128189,
195,check the performance of our kmeans test,"## k-fold cross validation designed to be agnostic to training data and training function
def k_fold_validation(k,data,train_func,parser):
    
    b_length = len(data)/k; # the testing bucket length
    sum_acc = 0; # Use this for average accuracy
    print k, ""- fold cross validation""
    print ""-----------------------------""
    for i in range(0,k):       
        s_i = i*b_length; # testing interval start index
        f_i = s_i + b_length; # testing interval end index
        test_data = data[s_i:f_i]
        train_data = data[0:s_i]+data[f_i:len(data)];
        metrics = train_func(train_data,test_data);
        sum_acc = parser(metrics,sum_acc);

    print '\nAverage Accuracy:', sum_acc / k;",0.5256271362,
195,check the performance of our kmeans test,"from sklearn import cross_validation
def cross_val(clf):
    scores = cross_validation.cross_val_score(clf, df_std, labels, cv=7)
    print(""Standardization Accuracy: %0.2f (+/- %0.2f)"" % (scores.mean(), scores.std() * 2))
    scores2 = cross_validation.cross_val_score(clf, df_minmax, labels, cv=7)
    print(""Min-Max Scalin Accuracy: %0.2f (+/- %0.2f)"" % (scores2.mean(), scores2.std() * 2))",0.5202475786,
195,check the performance of our kmeans test,"def full_eval(network, baseline_mean, baseline_std):
    # Check performance at the point in training with the best dev performance
    network.restore(network.best_dev_tup)
    print 'At best Dev performance...'
    print '\nTrain accuracy (appoximate): '
    network.check_accuracy(data_dict['train'], 2000);
    print '\nDev accuracy: '
    network.check_accuracy(data_dict['dev']);
    print '\nTest accuracy: '
    network.check_accuracy(data_dict['test']);
    
    # Check the performance at the termination of training
    network.restore(network.end_of_train_tup)
    print '\n\nAt end of training...'
    print '\nTrain accuracy (appoximate): '
    network.check_accuracy(data_dict['train'], 2000);
    print '\nDev accuracy: '
    network.check_accuracy(data_dict['dev']);
    print '\nTest accuracy: '
    a = 100.*network.check_accuracy(data_dict['test']);

    # Compare to the goal
    print '\n\nProbability this score beats Sheng Tai baseline:'
    print '{}%'.format(
        100.*np.round(1000.*np.mean(np.random.normal(baseline_mean, baseline_std, 10000) < a))/1000.)",0.5182221532,
195,check the performance of our kmeans test,"trained = True
if not trained:
    print(""--Training--"")
    clf = MultinomialNB().fit(preprocessing.normalize(df_features_train.values), train[""label""])
    sgd = SGDClassifier().fit(preprocessing.normalize(df_features_train.values), train[""label""])
    rf = RandomForestClassifier(n_estimators=200).fit(df_features_train, train[""label""])
    ada = AdaBoostClassifier(n_estimators=200).fit(df_features_train, train[""label""])
    pickle.dump([clf, sgd, rf, ada], open(""models_trained.pkl"", 'wb'))
else:
    models = pickle.load(open(""models_trained.pkl"", 'rb'))
    clf = models[0]
    sgd = models[1]
    rf = models[2]
    ada = models[3]
    for model in models:
        print(model)",0.5176843405,
2467,users,"# Given a set of user ids, this function calls get_screen_names and plots a table of the first (max_ids) ID's and screen names.
def draw_pretty_table(ids):
    users = api.lookup_users(user_ids=ids)
    pretty_table = PrettyTable(['ID','Screen Name'])
    [pretty_table.add_row ([row.id,row.screen_name]) for row in users]
    pretty_table.align = 'l'
    print(pretty_table)",0.5137650967,
2467,users,"def barchart(uid):
    
    user = sasby[sasby.user_id==int(uid)]
    current_user = user.groupby('user_id').sum().reset_index()
    
    attempted = float(current_user.n_attempted)

    incorrect = float(current_user.n_attempted) - float(current_user.n_partial)
    partially_correct = float(current_user.n_partial) - float(current_user.n_perfect)
    correct = float(current_user.n_perfect)
    
    sa_incorrect = float(current_user.n_show_answer_attempted) - float(current_user.n_show_answer_partial)
    sa_partially_correct = float(current_user.n_show_answer_partial) - float(current_user.n_show_answer_perfect)
    sa_correct = float(current_user.n_show_answer_perfect)
    
    if partially_correct > 0:
        pct_sa_partially_correct = int((sa_partially_correct / partially_correct) * 100)
    else:
        pct_sa_partially_correct = 0
    if correct > 0:
        pct_sa_correct = int((sa_correct / correct) * 100)
    else:
        pct_sa_correct = 0
    if incorrect > 0:
        pct_sa_incorrect = int((sa_incorrect / incorrect) * 100)
    else:
        pct_sa_incorrect = 0
    
    avg_user = sasby.mean()
    
    avg_attempted = float(avg_user.n_attempted)
        
    avg_incorrect = float(avg_user.n_attempted) - float(avg_user.n_partial)
    avg_partially_correct = float(avg_user.n_partial) - float(avg_user.n_perfect)
    avg_correct = float(avg_user.n_perfect)
    
    avg_sa_incorrect = float(avg_user.n_show_answer_attempted) - float(avg_user.n_show_answer_partial)
    avg_sa_partially_correct = float(avg_user.n_show_answer_partial) - float(avg_user.n_show_answer_perfect)
    avg_sa_correct = float(avg_user.n_show_answer_perfect)
    
    avg_pct_sa_partially_correct = int((avg_sa_partially_correct / avg_partially_correct) * 100)
    avg_pct_sa_correct = int((avg_sa_correct / avg_correct) * 100)
    avg_pct_sa_incorrect = int((avg_sa_incorrect / avg_incorrect) * 100)

    
 
    xlabels = ['Correct Problems', 'Partially Correct Problems', 'Incorrect Problems']
    
    trace1 = go.Bar(
    x=xlabels,
    y=[pct_sa_correct, pct_sa_partially_correct, pct_sa_incorrect],
    marker=dict(
        color=['rgba(87, 88, 187, 1)','rgba(18, 137, 167, 1)','rgba(163, 203, 56, 1)']),
    name='User {}'.format(uid))
    
    trace2 = go.Bar(
    x=xlabels,
    y=[avg_pct_sa_correct, avg_pct_sa_partially_correct, avg_pct_sa_incorrect],
        marker=dict(
        color=['rgba(87, 88, 187, 0.5)','rgba(18, 137, 167, 0.5)','rgba(163, 203, 56, 0.5)']),
    name='Average Student')
    
    data = [trace1, trace2]
    layout = go.Layout(
    yaxis=dict(
        range=[0,100]),
    xaxis=dict(
        tickfont=dict(
            size=10)),
    barmode='group',
    title='Percentage of Show Answer',
    showlegend=False)
       
    return dict(data = data, layout = layout)",0.4873308539,
2467,users,"def update(uid):
    user = sasby[sasby.user_id==int(uid)]
    current_user = user.groupby('user_id').sum().reset_index()

    
    trace0 = go.Scatter(x=current_user.n_problems_seen,
                   y=current_user.n_show_answer_problem_seen,
                   text=current_user.user_id,
                   mode = 'markers',
                   marker=dict(
                       size=current_user.n_problems_seen,
                       sizeref = 2,
                       sizemode = 'diameter'),
                   name='problems_seen')
    
    trace1 = go.Scatter(x=current_user.n_attempted,
                   y=current_user.n_show_answer_attempted,
                   text=current_user.user_id,
                   mode = 'markers',
                   marker=dict(
                       size=current_user.n_attempted,
                       sizeref = 2,
                       sizemode = 'diameter'),
                   name='problems_attempted')
    
    trace2 = go.Scatter(x=current_user.n_not_attempted,
                   y=current_user.n_show_answer_not_attempted,
                   text=current_user.user_id,
                   mode = 'markers',
                   marker=dict(
                       size=current_user.n_not_attempted,
                       sizeref = 2,
                       sizemode = 'diameter'),
                   name='problems_not_attempted')
    
    trace3 = go.Scatter(x=current_user.n_perfect,
                   y=current_user.n_show_answer_perfect,
                   text=current_user.user_id,
                   mode = 'markers',
                   marker=dict(
                       size=current_user.n_perfect,
                       sizeref = 2,
                       sizemode = 'diameter'),
                   name='problems_perfect')
    
    trace4 = go.Scatter(x=current_user.n_partial,
                   y=current_user.n_show_answer_partial,
                   text=current_user.user_id,
                   mode = 'markers',
                   marker=dict(
                       size=current_user.n_partial,
                       sizeref = 2,
                       sizemode = 'diameter'),
                   name='problems_partial')
    
    data = [trace0,trace1,trace2,trace3,trace4]

    layout = dict(
        title='nshow_answer stats for: {}'.format(uid),
        xaxis=dict(title='problems',type='linear'),
        yaxis=dict(title='show_answer'),
                   hovermode = 'closest')

    fig = go.Figure(data=data, layout=layout)
    iplot(fig)

    
    

uid = widgets.Text(
    value='534715',
    placeholder='',
    description='User ID:',
    disabled=False
)
    

widgets.interactive(update,uid=uid)",0.4870371521,
2467,users,"def simple_donut(uid):
    user = sasby[sasby.user_id==int(uid)]
    current_user = user.groupby('user_id').sum().reset_index()

    attempted = int(current_user.n_attempted[0])
    
    hoverinfo = 'label+percent+name'
    hole = 0.4
        
    incorrect = int(current_user.n_attempted[0]) - int(current_user.n_partial[0])
    partially_correct = int(current_user.n_partial) - int(current_user.n_perfect)
    correct = int(current_user.n_perfect)
    
    labels1 = ['Incorrect','Partially Correct','Correct']
    values1 = [incorrect, partially_correct, correct]

    marker1 = dict(colors=['rgba(163, 203, 56, 1)','rgba(18, 137, 167, 1)','rgba(87, 88, 187, 1)'])
    
    name1 = 'User {}'.format(uid)
    
    domain1 = dict(x=[0,0.45],
                   y=[0,1])
    
    trace1 = go.Pie(labels=labels1,
                    values=values1,
                    hoverinfo=hoverinfo,
                    hole=hole,
                    domain=domain1,
                    marker=marker1,

                    name=name1)
    
    avg_user = sasby.mean()
    
    avg_attempted = int(avg_user.n_attempted)
        
    avg_incorrect = int(avg_user.n_attempted) - int(avg_user.n_partial)
    avg_partially_correct = int(avg_user.n_partial) - int(avg_user.n_perfect)
    avg_correct = int(avg_user.n_perfect)
    
    labels2 = ['Incorrect','Partially Correct','Correct']
    values2 = [avg_incorrect, avg_partially_correct, avg_correct]
    
    marker2 = dict(colors=['rgba(163, 203, 56, 0.7)','rgba(18, 137, 167, 0.7)','rgba(87, 88, 187, 0.7)'])
    
    name2 = 'Average Student'
    
    domain2 = dict(x=[0.55,1],
                   y=[0,1])
    
    trace2 = go.Pie(labels=labels2,
                    values=values2,
                    hoverinfo=hoverinfo,
                    hole=hole,
                    marker=marker2,
                    domain=domain2,
                    name=name2)
    
    layout = go.Layout(title=""Problem Distribution"",
                       showlegend=False,
                       hovermode = 'closest',
                       )
    
    data = [trace1,trace2]
    
    return dict(data = data, layout = layout)",0.4852702022,
2467,users,"def get_twitter_data():
    res = api.search(""Apple OR iphone OR iPhone"")
    for i in res:
        record = ''
        record += str(i.user.id_str)
        record += ';'
        record += str(normalize_timestamp(str(i.created_at)))
        record += ';'
        record += str(i.user.followers_count)
        record += ';'
        record += str(i.user.location)
        record += ';'
        record += str(i.favorite_count)
        record += ';'
        record += str(i.retweet_count)
        record += ';'
        producer.send(topic_name, str.encode(record))",0.4828958809,
2467,users,"def parse_user(usr):
    user = {}  
    user[""created_at""] = usr['created_at']
    user[""description""] = usr['description']
    user[""favourites_count""] = usr['favourites_count']
    user[""followers_count""] = usr['followers_count']
    user[""friends_count""] = usr['friends_count']
    user[""geo_enabled""] = usr['geo_enabled']
    user[""id""] = usr['id']
    user[""id_str""] =usr['id_str']
    user[""name""] = usr['name']
    user[""screen_name""] = usr['screen_name']
    user[""statuses_count""] = usr['statuses_count']
    user[""profile_image_url""] = usr['profile_image_url']
    if usr['time_zone'] <> None:
        user[""time_zone""] = usr['time_zone']
        
    if user[""profile_image_url""] <> None:
        save_profile_image_url(user[""id_str""], user[""profile_image_url""])
    
    return user",0.4783928394,
2467,users,"def grader_name(grader_id):
    grader = db.user.find_one({'_id': grader_id})
    return grader['name']",0.4740138948,
2467,users,"def grader_id(grader_name):
    grader = db.user.find_one({'name': grader_name})
    return grader['_id']",0.4740138948,
2467,users,"def pull_messages(client, friend_name):
	friendList = client.getUsers(friend_name)
	friend = friendList[0]

	last_messages = client.getThreadInfo(friend.uid, 10000) # 10000 should cover all messages?
	last_messages.reverse()  # messages come in reversed order

	result = []

	for message in last_messages:
		if message.author.split("":"")[1] == str(os.environ['ID']):
			name = ""You""
		else: 
			name = friend_name
		if hasattr(message, 'body') and message.body:
			result.append(name + "": "" + message.body)

	return result

client = fbchat.Client(os.environ['ID'], os.environ['PASSWORD'])
friend_name = input(""Friend's name: "")

f = pull_messages(client, friend_name)",0.4733785987,
2467,users,"def student_id(user_id):
    student = db.user.find_one({'_id': user_id})
    if '@student.dtu.dk' in student['email']:
        return student['email'][:7]
    else:
        return student['email']
    
def get_TA_scores(assignment_id, calc_mean=True):
    TA1_Scores = dict()
    TA2_Scores = dict()
    if assignment_id != ObjectId(""562d0edb79c852000f99c6dd""):
        Ass1B = defaultdict(list, json.load(open('SocialGraph/Ass1Bjarke.json')) )
        Ass1S = defaultdict(list, json.load(open('SocialGraph/Ass1Snorre.json')) )    

    
    questions_norm = []
    for section_id in db.assignment.find_one({'_id': assignment_id})['sections']:
        for question_id in db.question_section.find_one({'_id': section_id})['questions']:
            q = db.question.find_one({'_id': question_id})
            if q['question_type'] == 'numerical' or q['question_type'] == 'boolean':
                norm = 3.0 if q['question_type'] == 'numerical' else 1.0
                questions_norm.append(norm)
                
    # Find the TA scores for the handins
    for handin in db.handin.find({'assignment': assignment_id}):
        studentId = student_id(handin['submitter'])
        # Hot fix
        handin_scores_ta1 = list()
        handin_scores_ta2 = list()
        
        # Fetch relevant data
        if assignment_id == ObjectId(""562d0edb79c852000f99c6dd""):
            if have_graded_handin(handin['_id'],ObjectId(""56151d31ed0827000f7ad204"")):
                handin_scores_ta1 = answer_handin_grader(handin['_id'],ObjectId(""56151d31ed0827000f7ad204""))
            if have_graded_handin(handin['_id'],ObjectId(""56151d6bed0827000f7ad205"")):
                handin_scores_ta2 = answer_handin_grader(handin['_id'],ObjectId(""56151d6bed0827000f7ad205""))
        else:
            for index, question_norm in enumerate(questions_norm):
                if studentId in Ass1B:
                    handin_scores_ta1.append(Ass1B[studentId][index] / question_norm)
                if studentId in Ass1S:
                    handin_scores_ta2.append(Ass1S[studentId][index] / question_norm)

        if len(handin_scores_ta1) != 0:
            if calc_mean:
                TA1_Scores[str(handin['_id'])] = np.mean(handin_scores_ta1)
            else:
                TA1_Scores[str(handin['_id'])] = handin_scores_ta1
        if len(handin_scores_ta2) != 0:
            if calc_mean:
                TA2_Scores[str(handin['_id'])] = np.mean(handin_scores_ta2)
            else:
                TA2_Scores[str(handin['_id'])] = handin_scores_ta2

    return TA1_Scores, TA2_Scores",0.4729629159,
2441,unique favourite ice cream,"def connect_players_by_combinations():
    connections, non_connections = set(), set()
    for player in dpc:
        for other in dpc:
            if player!=other:
                duo = frozenset([player, other])
                if duo not in connections and duo not in non_connections:
                    if teammates(duo):
                        dpc[player]['Count'] += 1
                        dpc[other]['Count'] += 1
                        connections.add(duo)
                    else: non_connections.add(duo)
    return connections",0.3512011766,
2441,unique favourite ice cream,"class FlowerBasket(object):
    
    df = pd.read_csv(""iris.csv"")
    revenue = 0
    
    def __init__(self,owner, price):
        self.owner = owner
        self.price = price
        
    def sellVirginica(cls, quantity, pricePerUnit):
        if (quantity > len(cls.df[cls.df['species'] == 'virginica'])):
            return False
        else:
            #randomly picks a quantity of virginica rows to discard
            sold = cls.df[cls.df['species'] == 'virginica'].sample(quantity)
            cls.df.drop(cls.df.index[sold.index.tolist()], inplace=True)
            cls.revenue += pricePerUnit * quantity
            print(""Sale of {0} virginica flowers executed!"".format(quantity))
            return True",0.3486910164,
2441,unique favourite ice cream,"cafe_dict = {
    ""CBD"" : [""Baristacats"", ""Craven A's"", ""La Noisette""]
    ""Mt Eden"" : [""Fraser's"",""Olaf's"",""RAD""],
    ""Kingsland"" : [""Fridge""],
    ""Newmarket"" : [""Little & Friday's"",""Zarbo's""],
    ""Parnell"" : [""Cafe Rhythm"", ""Mink Cafe""]
}",0.3480638266,
2441,unique favourite ice cream,"def figure_out_neurons(df, max_animals=df.animal.max() + 1):
    # figure out subset of neurons with replicates:
    # Initialize list to save neurons labeled in all samples
    neurons_with_replicates = []
    # Loop through data frames grouped by column neuron
    for neuron, group in df.groupby('neuron'):
        # List the index of unique animals
        animals = group.animal.unique()
        if len(animals) == max_animals:
            neurons_with_replicates += [neuron]
    neurons_with_replicates = np.sort(neurons_with_replicates)
    return neurons_with_replicates

neurons_with_replicates = figure_out_neurons(df)
print(neurons_with_replicates)",0.3432582021,
2441,unique favourite ice cream,"def import_wine_dataset():
    data = datasets.load_wine()
    
    target = data.target
    data = data.data
    
    return data, target",0.3428438902,
2441,unique favourite ice cream,"def load_mnist():
    train, test = dict(), dict()
    
    for i, filename in enumerate(FILENAMES):
        path = os.path.join(ABSPATH_MNIST, filename)
        
        # Using python's gzip library to unpack files
        with gzip.open(path, mode = 'rb') as f:
            if 'labels' in filename:
                data = f.read(8)  # fetch the first 8 bytes and move file pointer pointing to the 9th byte
                magic, size = struct.unpack('>II', data)
                
                # Check if the file signature is correct
                if magic != 2049:
                    raise ValueError('Expected magic number 2049, got {magic} instead.'.format(magic = magic))
                    
                buffrr = f.read()
                # Convert bytes to array of desired type
                labels = np.frombuffer(buffrr, dtype = np.uint8)
                
                if 'train' in filename:
                    train['labels'] = labels
                else:
                     test['labels'] = labels
                
            if 'images' in filename:
                data = f.read(16) # fetch the first 16 bytes and move file pointer pointing to the 17th byte
                magic, size, rows, cols = struct.unpack('>IIII', data)
                
                # Check if the file signature is correct
                if magic != 2051:
                    raise ValueError('Expected magic number 2051, got {magic} instead.'.format(magic = magic))
                    
                buffrr = f.read()
                images = np.reshape(np.frombuffer(buffrr, dtype = np.uint8), (size, rows * cols))
                
                if 'train' in filename:
                    train['images'] = images
                else:
                     test['images'] = images
                        
    # wrap processed data into a Pandas DataFrame                 
    train = pd.DataFrame(np.concatenate((train['images'], np.atleast_2d(train['labels']).T), axis = 1))
    test  = pd.DataFrame(np.concatenate(( test['images'], np.atleast_2d( test['labels']).T), axis = 1))
    
    return train, test",0.3367236853,
2441,unique favourite ice cream,"def allPairsInP3(p):
    return { (x, y) for (x, y) in P3 
                    if x * y == p
           }",0.3360245824,
2441,unique favourite ice cream,"# DO NOT RUN
def do_not_run():
    elves = [elf for elf in range(1, input_data + 1)]
    while len(elves) > 1:
        elves.pop(len(elves)//2)
        elves = elves[1:] + elves[:1]
    print(elves[0])",0.335711211,
2441,unique favourite ice cream,"def calc_distance(dists, beer1, beer2, weights):
    mask = (dists.beer1==beer1) & (dists.beer2==beer2)
    row = dists[mask]
    row = row[['overall_dist', 'aroma_dist', 'palate_dist', 'taste_dist']]
    dist = weights * row
    return dist.sum(axis=1).tolist()[0]

weights = [2, 1, 1, 1] #Give your ratings here
print (calc_distance(simple_distances, ""Fat Tire Amber Ale"", ""Dale's Pale Ale"", weights))
print (calc_distance(simple_distances, ""Fat Tire Amber Ale"", ""Michelob Ultra"", weights))",0.331512332,
2441,unique favourite ice cream,"def calc_distance(dists, beer1, beer2, weights):
    mask = (dists.beer1==beer1) & (dists.beer2==beer2)
    row = dists[mask]
    row = row[['overall_dist', 'aroma_dist', 'palate_dist', 'taste_dist']]
    dist = weights * row
    return dist.sum(axis=1).tolist()[0]

weights = [2, 1, 1, 1]
print calc_distance(simple_distances, ""Fat Tire Amber Ale"", ""Dale's Pale Ale"", weights)
print calc_distance(simple_distances, ""Fat Tire Amber Ale"", ""Michelob Ultra"", weights)",0.331512332,
571,exercise,"env = gym.make('CartPole-v0')
    policy = Policy()
    bestparams = None
    bestreward = 0
    for _ in xrange(10000):
        policy.reset_parameters()
        reward = run_episode(env, policy)
        if reward > bestreward:
            bestreward = reward
            bestparams = policy.get_parameters()
            # considered solved if the agent lasts 200 timesteps
            if reward == 200:
                print('found best {}'.format(bestreward))
                break

    policy.set_parameters(bestparams)
    reward = run_episode(env, policy, True)",0.4639431238,
571,exercise,"def grid_search(essays, expected_tags):

    rows_ana = []
    proc_essays = processed_essays_use_predicted_tag(essays=essays)

    metrics = get_metrics_raw(proc_essays, expected_tags=expected_tags,  micro_only=True)
    row = metrics[""MICRO_F1""]
    rows_ana.append(row)

    df_results = pd.DataFrame(rows_ana)
    return df_results",0.4449121952,
571,exercise,"class trainer():
    def __init__(self,model,optimizer,loss,trainloader):
        self.model=model
        self.optimizer=optimizer
        self.loss=loss
        if torch.cuda.is_available():
            self.loss=self.loss.cuda()
        self.trainloader=trainloader
        
    def stopcon(self):
        def deriv(ns):
            return [ns[i+1]-ns[i] for i in range(len(ns)-1)]
        return sum(deriv(self.errors['val_error'][-10:]))>0
    
    def save_model(self,path):
        torch.save(self.model.state_dict(),path)
        
    def train(self,epoch,val_loader,val_size,train_size):
        print(""Start training..."")
        print(self.optimizer,self.loss)
        print(len(self.trainloader), self.trainloader)
        self.errors={'val_error':[],'loss':[],'train_error':[]}
        schedule=StepLR(self.optimizer,step_size=1,gamma=0.5)
        for i in range(epoch):
            if self.stopcon():
                return
            correct = 0
            epoch_loss = 0
            t1=time.time()
            schedule.step()
            for index,(data,label) in enumerate(self.trainloader):
                self.optimizer.zero_grad()
                inputs=to_variable(data.view(-1,inputsize))
                outputs=self.model.forward(inputs)
                labels=to_variable(label)
                loss=self.loss(outputs,labels)
                loss.backward()
                self.optimizer.step()
                pred = outputs.data.max(1, keepdim=True)[1]
                predicted = pred.eq(labels.data.view_as(pred))
                correct += predicted.sum()
                epoch_loss+=loss.data[0]
            t2=time.time()
            total_loss=epoch_loss/train_size
            train_error=1-correct/train_size
            val_accu=inference(self.model,val_loader,val_size)
            print('epoch:{0},loss:{1:.8f},validate accuracy:{2:.8f},train error:{3:.8f},time:{4:.2f}'.format(i+1,total_loss,val_accu,train_error,t2-t1))
            self.errors['val_error'].append(1-val_accu)
            self.errors['train_error'].append(train_error)
            self.errors['loss'].append(total_loss)
        self.model.eval()",0.4402658939,
571,exercise,"## TODO: Create train and evaluate function using tf.estimator
def train_and_evaluate(output_dir, num_train_steps):
  #ADD CODE HERE
  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)",0.4394865632,
571,exercise,"from lessonFunctions import *
def process_image(image, params):
    config, clf, all_windows = params['clf_config'], params['clf'], params['windows']

    if params['cache_enabled']:
        cache = process_image.cache
        if cache['heatmaps'] is None:
            cache['heatmaps'] = collections.deque(maxlen=params['heatmap_cache_length'])
        
        if 'tracker' not in cache:
            cache['tracker'] = VehicleTracker(image.shape)
        frame_ctr = cache['frame_ctr']
        tracker = cache['tracker']
        cache['frame_ctr'] += 1

#         windows = all_windows[frame_ctr % len(all_windows)]# + extra
        windows = itertools.chain(*all_windows)
    else:
        windows = itertools.chain(*all_windows)

    measurements = car_detect(image, clf, config, windows)
    current_heatmap = update_heatmap(measurements, image.shape)
    if not params['cache_enabled']:
        thresh_heatmap = current_heatmap
    else:
        cache['heatmaps'].append(current_heatmap)
        thresh_heatmap = sum(cache['heatmaps'])
    
    thresh_heatmap[thresh_heatmap < params['heatmap_threshold']] = 0
    cv2.GaussianBlur(thresh_heatmap, (31,31), 0, dst=thresh_heatmap)

    labels = label(thresh_heatmap)
    im2 = draw_labeled_bboxes(np.copy(image), labels)

#     if not params['cache_enabled']:
#         im2 = draw_labeled_bboxes(np.copy(image), labels)
#     else:
#         Z = []
#         for car_number in range(1, labels[1]+1):
#             nonzero = (labels[0] == car_number).nonzero()
#             nonzeroy = np.array(nonzero[0])
#             nonzerox = np.array(nonzero[1])
#             Z.append((np.min(nonzerox), np.min(nonzeroy), np.max(nonzerox), np.max(nonzeroy)))
#         tracker.detect(Z)
#         im2 = tracker.draw_bboxes(np.copy(image))
        
    return im2

def clear_cache():
    process_image.cache = {
        'meas': None,
        'heatmaps': None,
        'frame_ctr': 0
    }",0.4386885464,
571,exercise,"def generate_random_samples(k, v):
    # generate data from k gaussians each having v features(variables)
    weights, means, covariances = initialize_gmm_parameters(v, k) # randomly initialize gmm parameters
    print('\n\nActual Weights: \n %s' % weights)
    print('Actual Means: \n %s' % means)
    print('Actual Covariances: \n %s' % covariances)
    n = 100 # no of samples
    # generate n random samples from these parameters 
    xs = np.concatenate([np.random.multivariate_normal(m, s, int(w*n))
                        for w, m, s in zip(weights, means, covariances)])
    return xs",0.4359610081,
571,exercise,"class Trainer:
    def __init__(self, data, seq_length):
        self.optimizer = AdamOptimizer()
        self.step, self.pos, self.h = 0, 0, None
        self.seq_length = seq_length
        self.data = data

    def train(self, params):
        self.step += 1
        if self.pos + self.seq_length + 1 >= len(self.data):
            # reset data position and hidden state
            self.pos, self.h = 0, None
        x = self.data[self.pos : self.pos + self.seq_length]
        y = self.data[self.pos + 1 : self.pos + self.seq_length + 1]
        
        loss, gradients, self.h = back_propagation(params, x, y, self.h)
        s = self.optimizer.send(gradients)
        for par,  in zip(params, s):
            par += 
        self.pos += self.seq_length
        return loss",0.4350316525,
571,exercise,"import random
def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):
    """"""
    Apply convolution then max pooling to x_tensor
    :param x_tensor: TensorFlow Tensor 
    :param conv_num_outputs: Number of outputs for the convolutional layer
    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer
    :param conv_strides: Stride 2-D Tuple for convolution
    :param pool_ksize: kernal size 2-D Tuple for pool
    :param pool_strides: Stride 2-D Tuple for pool
    : return: A tensor that represents convolution and max pooling of x_tensor
    """"""
    print(""x_tensor"", x_tensor)
    print(""conv_num_outputs"", conv_num_outputs)
    print(""conv_ksize"", conv_ksize)
    print(""conv_strides"", conv_strides)
    print(""pool_ksize"", pool_ksize)
    print(""pool_strides"", pool_strides)
    
    
    #for this function, our input image is is x_tensor
    # input = tf.placeholder(tf.float32, shape=[None, image_height, image_width, color_channels]
    
    #BEGINNING OF CONVOLUTION LAYER
    
    #create weights
    input_depth = x_tensor.get_shape().as_list()[-1]
    
    # outputs a ""Dimension"" object, not an int, which is why the .as_list() method needs to be added
    # >>> type(x_tensor.get_shape()[-1])
    # <class 'tensorflow.python.framework.tensor_shape.Dimension'>
    
    #weight = tf.Variable(tf.truncated_normal([filter_size_height,filter_size_width, color_channels, k_output]))
    weights = tf.Variable(tf.truncated_normal((conv_ksize[0],conv_ksize[1],input_depth,conv_num_outputs),stddev=0.05))
    
    #assign bias
    biases = tf.Variable(tf.zeros(conv_num_outputs))

    # Apply a convolution to x_tensor using weight and conv_strides. tf.nn.conv2D() parameters below for reference
    # tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)

    conv_layer = tf.nn.conv2d(x_tensor, weights, 
                strides=[1, conv_strides[0], conv_strides[1], 1], padding='SAME')
    conv_layer = tf.nn.bias_add(conv_layer, biases)
    
    # Non-linear activation function for conv_layer
    conv_layer = tf.nn.relu(conv_layer)
    
    
    #END OF CONVOLUTION LAYER
    
    #BEGINNING OF POOLING LAYER: Apply Max Pooling using pool_ksize and pool_strides.
    
    # output shape of pooling layer should be [None, 4, 4, 10]
    # tf.nn.max_pool(value, ksize, strides, padding, data_format='NHWC', name=None)
    pool_ksize = [1, pool_ksize[0], pool_ksize[1], 1]
    pool_strides = [1, pool_strides[0], pool_strides[1], 1]
    
    maxpool_layer = tf.nn.max_pool(value=conv_layer, ksize=pool_ksize, strides=pool_strides, padding='SAME')
    #END OF POOLING LAYER
    
    return maxpool_layer

""""""
DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE
""""""
tests.test_con_pool(conv2d_maxpool)",0.434854269,
571,exercise,"def mask_row(row,iii):
    # loads nifti volumes
    volume = nib.load(row['working_vol']).get_data() 
    mask = nib.load(row['brain_mask']).get_data()
    # reshape to VOXELSxTIME_POINTS
    volume = volume.reshape((prod(volume.shape[:3]),-1))
    mask = mask.reshape((prod(mask.shape[:3]),-1))
    
    # take only time points where mask is 1
    masked_vol = np.array([volume[i] for i in xrange(len(volume)) if mask[i] == 1])
    print 'volume {} : {} voxels selected on mask'.format(iii,masked_vol.shape)
    sf = '{}{}_{}_masked'.format(base_dir,row['subject'],str(row['runID']))
    # save masked_vols to disk
    np.save(sf,masked_vol)
    return sf + '.npy'",0.4331635237,
571,exercise,"#delete rows from table adultdb which have education as ""Some-college"" - Delete Data with SQLAlchemy ORM
#delete(synchronize_session='fetch')
#'fetch' - performs a select query before the delete to find objects that are matched by the delete query and 
#need to be removed from the session. Matched objects are removed from the session
if __name__ == ""__main__"":
    session = loadSession()
    session.query(Adultdb).filter_by(education=""Some-college"").delete(synchronize_session='fetch')
    session.commit()
    rows = session.query(Adultdb).filter_by(education=""Some-college"").all()
    print(""Count of rows after Delete : "",len(rows))",0.4326021075,
1286,now plot your predictions for every x,"def plot_prediction(model, sample_idx=0, classes=range(10)):
    fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))

    ax0.imshow(scaler.inverse_transform(X_test[sample_idx]).reshape(8, 8), cmap=plt.cm.gray_r,
               interpolation='nearest')
    ax0.set_title(""True image label: %d"" % y_test[sample_idx]);


    ax1.bar(classes, one_hot(len(classes), y_test[sample_idx]), label='true')
    ax1.bar(classes, model.forward(X_test[sample_idx]), label='prediction', color=""red"")
    ax1.set_xticks(classes)
    prediction = model.predict(X_test[sample_idx])
    ax1.set_title('Output probabilities (prediction: %d)'
                  % prediction)
    ax1.set_xlabel('Digit class')
    ax1.legend()
    
plot_prediction(lr, sample_idx=0)",0.571442008,
1286,now plot your predictions for every x,"def plot_predict(model, dates_series, num_observations):
    fig,ax = plt.subplots(figsize = (12,8))
    model.plot_predict(
        start = dates_series[len(dates_series)-num_observations], 
        end = dates_series[len(dates_series)-1],
        ax = ax
    )
    plt.show()
plot_predict(arima_model, train_set['DEPARTURE_TIME'], 100)",0.5691697598,
1286,now plot your predictions for every x,"def time_slicer(df, timeframes):
    """"""
    Function to count observation occurrence through different lenses of time.
    """"""
    f, ax = plt.subplots(len(timeframes), figsize = [12,7])
    for i,x in enumerate(timeframes):
        df.loc[:,[x,""Employer""]].groupby([x]).count().plot(ax=ax[i])
        ax[i].set_ylabel(""Incident Count"")
        ax[i].set_title(""Incident Count by {}"".format(x))
        ax[i].set_xlabel("""")
    ax[len(timeframes)-1].set_xlabel(""Time Frame"")
    plt.tight_layout(pad=0)
    
# Time Frames of Interest
df[""Date of Year""] = df['EventDate'].dt.dayofyear # Day of Year
df[""Weekday""] = df['EventDate'].dt.weekday 
# Plot
time_slicer(df=df,timeframes=[""EventDate"",""Date of Year"",""Weekday""])",0.565161109,
1286,now plot your predictions for every x,"def cool_plot(df):
    #snsplot = sns.stripplot(y=df.SalePrice, x=df.GrLivArea,
    #                        data=df,
    #                        jitter=True,
    #                        marker='o',
    #                        alpha=0.7,
    #                        hue=""YrSold"")
    snsplot = sns.pairplot(df_train[['SalePrice', 'GrLivArea','OverallCond']], hue='OverallCond', palette='husl', size=5);

    #snsplot.axes.set_title(
    #    'Distributions of SalesPrices  Living Area', fontsize=16)
    #snsplot.set_xlabel('Living Area', fontsize=14)
    #snsplot.set_ylabel('Sale Price', fontsize=14)
    #snsplot.tick_params(labelsize=16)
    #fig = snsplot.get_figure()
    
    fig.set_size_inches(8, 10)
    
    plt.show()

cool_plot(df_train)",0.5649980307,
1286,now plot your predictions for every x,"def plot_predict(model, dates_series, num_observations):
    fig = plt.figure(figsize = (12,5))
    model.plot_predict(
        start = str(dates_series[len(dates_series)-num_observations]), 
        end = str(dates_series[len(dates_series)-1])
    )
    plt.show()
    
plot_predict(arima_model, train_set['Date'], 100)
plot_predict(arima_model, train_set['Date'], 10)",0.5646398664,
1286,now plot your predictions for every x,"def plot_matrix(clf, X_test, y_test):
    plt.clf()
    plt.imshow(confusion_matrix(clf.predict(X_test), y_test),
               interpolation='nearest', cmap=plt.cm.Blues)
    plt.colorbar()
    plt.xlabel(""true label"")
    plt.ylabel(""predicted label"")
    plt.show()",0.564419508,
1286,now plot your predictions for every x,"def draw_figure(train_data, column_name):
    fig, axes = plt.subplots(1, 1, figsize=(16, 9))

    (train_data[column_name]).plot(ax=axes)
    axes.set(xlabel='Index of Entries', ylabel=column_name)
    axes.set_title(column_name)",0.5623465776,
1286,now plot your predictions for every x,"def plot_bin_features(count_df, count_df_norm):
    # Two subplots, unpack the axes array immediately
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=((15,5)))
    count_df.plot.bar(ax = ax1,
                      legend=True,
                      title= ""Distribution for binary features"")


    colors = sns.color_palette(""Paired"", n_colors=5)


    count_df_norm[['control_0','control_1']].plot.bar(ax=ax2,
                                                      legend=True,
                                                      title=""Normalized distribution for binary features"",
                                                      label = 'control',
                                                      stacked=True,
                                                      color = colors,
                                                      yerr= count_df_norm['error_control'],
                                                      position=0.25)

    count_df_norm[['treatment_0','treatment_1']].plot.bar(ax=ax2,
                                                          legend=True, 
                                                          title=""Normalized distribution for binary features"",
                                                          label='treatment',
                                                          stacked=True,
                                                          color = colors[2:],
                                                          yerr= count_df_norm['error_treatment'], 
                                                          position=0.75)

    # Change the width of the bars
    h,l = ax2.get_legend_handles_labels() 
    width=0.3
    for i in range(0, 4, 2): 
        for container in h[i:i+2]:
            for bar in container.patches: 
                bar.set_width(width)


    plt.suptitle(""comparison between normalized distribution and global distribution for binary features"",
                 fontsize=20,
                 y = -0.1,
                )


    plt.show()
plot_bin_features(count_df, count_df_norm)",0.5616042614,
1286,now plot your predictions for every x,"def print_kmeans(db, X):
    import matplotlib.pyplot as plt
    labels = db.predict(X)
    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
    unique_labels = set(labels)
    n_clusters_ = len(set(labels))
    city_lon_border = (-74.05, -73.75)
    city_lat_border = (40.60, 41)
    
    plt.figsize=(5,5)
    fig, ax = plt.subplots(ncols=1, nrows=1,figsize=(15,15))    

    city_lon_border = (-74.05, -73.75)
    city_lat_border = (40.60, 41)
    colors = [plt.cm.Spectral(each)
                  for each in np.linspace(0, 1, len(unique_labels))]

    for k, col in zip(unique_labels, colors):

        class_member_mask = (labels == k)

        xy = np.array(X.loc[class_member_mask,:])
        ax.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),
                 markeredgecolor='none', markersize=2)


    ax.set_ylim(city_lon_border)
    ax.set_xlim(city_lat_border)
    ax.set_title('Estimated number of centroids: %d' % n_clusters_)
    pass",0.5602101088,
1286,now plot your predictions for every x,"# Define the scatterplot function
def plot_scat(frame, X, Y):
    fig, axes = plt.subplots(1, len(X), figsize=(14,3))
    for var in X:
        idx = X.index(var)
        sns.regplot(y=frame[Y], x=frame[var], ax=axes[idx])",0.5598500967,
1940,small example,"def plot_performance():
    image_idx = np.random.randint(0, data.test.num_examples, 5)
    original_images = data.test.images[image_idx]
    generated_images = session.run(y_pred, {x: data.test.labels[image_idx], y_true: data.test.images[image_idx]})
    plot_images(original_images, generated_images)",0.4944025874,
1940,small example,"def show_random_mnist_train_example(mnist):
    """"""Draws a random training image from MNIST dataset and displays it.
    
    Args:
        mnist: MNIST dataset.
    """"""
    random_idx = random.randint(0, mnist.train.num_examples)
    image = mnist.train.images[random_idx].reshape(28, 28)
    imgplot = plt.imshow(image, cmap='Greys')
    ### [TASK] Get a correct label for the image 
    label = None
    ###
    print('Correct label for image #{0}: {1}'.format(random_idx, label))

show_random_mnist_train_example(mnist)",0.4860063791,
1940,small example,"def linear_function():
    """"""
    Implements a linear function: 
            Initializes W to be a random tensor of shape (4,3)
            Initializes X to be a random tensor of shape (3,1)
            Initializes b to be a random tensor of shape (4,1)
    Returns: 
    result -- runs the session for Y = WX + b 
    """"""
    
    np.random.seed(1)
    X = np.random.randn(3, 1)
    W = np.random.randn(4, 3)
    b = np.random.randn(4, 1)
    Y = tf.add(tf.matmul(W, X), b)
    
    # Create the session using tf.Session() and run it with sess.run(...) on the variable you want to calculate
    sess = tf.Session()
    result = sess.run(Y) 
    # close the session 
    sess.close()

    return result",0.4817201495,
1940,small example,"# example stolen from scikit-learn docs
def plot_gmm():
    n_samples = 300

    # generate random sample, two components
    np.random.seed(0)

    # generate spherical data centered on (20, 20)
    shifted_gaussian = np.random.randn(n_samples, 2) + np.array([20, 20])

    # generate zero centered stretched Gaussian data
    C = np.array([[0., -0.7], [3.5, .7]])
    stretched_gaussian = np.dot(np.random.randn(n_samples, 2), C)

    # concatenate the two datasets into the final training set
    X_train = np.vstack([shifted_gaussian, stretched_gaussian])

    # fit a Gaussian Mixture Model with two components
    clf = skl.mixture.GMM(n_components=2, covariance_type='full')
    clf.fit(X_train)

    # display predicted scores by the model as a contour plot
    x = np.linspace(-20.0, 30.0)
    y = np.linspace(-20.0, 40.0)
    X, Y = np.meshgrid(x, y)
    XX = np.array([X.ravel(), Y.ravel()]).T
    Z = -clf.score_samples(XX)[0]
    Z = Z.reshape(X.shape)
    
    plt.figure(figsize=(10,8))
    CS = plt.contour(X, Y, Z, norm=mpl.colors.LogNorm(vmin=1.0, vmax=1000.0),
                     levels=np.logspace(0, 3, 10))
    CB = plt.colorbar(CS, shrink=0.8, extend='both')
    plt.scatter(X_train[:, 0], X_train[:, 1], .8)

    plt.title('Negative log-likelihood predicted by a GMM')
    plt.axis('tight')",0.4807475209,
1940,small example,"def classify():
  figsize(13, 3.0)
  for i in range(10):
    filenumber = np.random.randint(5)
    offset = np.random.randint(10000 - 32)
    orig_im, l = get_mini_batch(random_batch(1), 1)
    im = distort(orig_im[0])
    im = tf.expand_dims(im, 0)
    x = pool2(norm2(conv2(norm1(pool1(conv1(im))))))
    x = tf.reshape(x, [1, 6*6*64])
    x = tf.nn.softmax(softmax_linear(local4(local3(x))))
    set_printoptions(precision=3)
    prediction = label_names[np.argmax(x.eval()[0])]
    ground_truth = label_names[l[0].eval()[0]]
    subplot(2, 5, i+1)
    myshow(orig_im[0].eval(), 'P:'+prediction +', GT:'+ground_truth)",0.4764305353,
1940,small example,"from random import randint
    
def test_model():
    num = randint(0, mnist.test.images.shape[0])
    img = mnist.test.images[num]

    classification = sess.run(tf.argmax(y, 1), feed_dict={x: [img]})
    plt.imshow(img.reshape(28, 28), cmap=plt.cm.binary)
    plt.show()
    print ('NN predicted', classification[0])
test_model()",0.4719107449,
1940,small example,"def linear_function():
    
    W = np.random.randn(4,3)
    X = np.random.randn(3,1)
    b = np.random.randn(4,1)
    Y = tf.add(tf.matmul(W,X),b)
    
    sess = tf.Session()
    result = sess.run(Y)
    
    sess.close()
    
    return result",0.4698506296,
1940,small example,"def test_entropy():
    builder = Data1030TreeBuilder()
    y = np.random.randint(2, size=10)*10
    print('y = ', y)
    X = np.column_stack((np.random.randint(2, size=10)*10, y)) # Second column of X contains y
    print('X = ', X)
    cost_split_1 = builder.entropy(X, y, 0, 5) # Try splitting using X[:,0] < 5
    cost_split_2 = builder.entropy(X, y, 1, 5) # Try splitting using X[:,1] < 5
    print('cost_split_1 =', cost_split_1)
    print('cost_split_2 =', cost_split_2)
    assert cost_split_1 >= 0 # cost of random split  
    assert cost_split_2 == 0 # cost of a perfect split is zero

    
test_entropy()",0.4696769714,
1940,small example,"async def main():
    print('map', await ac.from_range(10).map(lambda x: x*2).collect())
    print('filter', await ac.from_range(10).filter(lambda x: x % 2 == 0).collect())
    print('take', await ac.from_range(10).take(5).collect())
    print('drop', await ac.from_range(10).drop(5).collect())
    print('take_while', await ac.from_range(10).take_while(lambda x: x < 5).collect())
    print('drop_while', await ac.from_range(10).drop_while(lambda x: x < 5).collect())
    
ac.run(main())",0.4696218073,
1940,small example,"def view_colors(summary):
    vals,labs = load_floss_colors(example=False)
    fig = plt.figure(1,figsize=(15,15))
    plt.axes()

    count = 0
    #Plot original and matched colors
    for (skeins, floss, name, oldcolor, matchedcolor) in summary:
        circle = plt.Circle((count, 26), radius=0.5, fc=(oldcolor[0]/255.,oldcolor[1]/255.,oldcolor[2]/255.));
        plt.gca().add_patch(circle)
        plt.plot([-2,27],[25.5,25.5],""k"")
        circle2 = plt.Circle((count, 25), radius=0.5, fc=(matchedcolor[0]/255.,matchedcolor[1]/255.,matchedcolor[2]/255.));
        plt.gca().add_patch(circle2)
        plt.text(count,24,floss,va='center', ha='center',fontsize=10)
        count += 1

    plt.text(17,25.75,""Original Colors"",fontsize=15)
    plt.text(17,24.75,""Matched Floss"",fontsize=15)
    plt.text(-6,22,""DMC Flosses:"",fontsize=15)
    
    #Plot available floss colors
    for i in range(len(vals)):
        row=i/20
        col=i%20
        circle = plt.Circle((col, row), radius=0.5, fc=(vals[i,1]/255.,vals[i,2]/255.,vals[i,3]/255.));
        plt.gca().add_patch(circle)
        plt.text(col,row,vals[i,0],va='center', ha='center',fontsize=10)
        plt.axis('scaled')
    plt.xlim(-2,22)
    plt.ylim(-2,27)
    plt.show()",0.4689733386,
1744,ratings,"def plot_feature_class_hists(df, fsize):
    buys = df[df.rating=='BUY']
    holds = df[df.rating=='HOLD']
    sells = df[df.rating=='SELL']
    fig, ax = plt.subplots(df.shape[1]-1,1,figsize=fsize)
    for i, feature in enumerate(df.loc[:,df.columns != 'rating']):
        ax[i].set_title(f""{feature} BHS (green-yellow-red)"");
        ax[i].hist(sells[feature], bins=30, color='red', alpha=0.3);
        ax[i].hist(buys[feature], bins=30, color='green', alpha=0.4, );
        ax[i].hist(holds[feature], bins=30, color='yellow', alpha=0.5);
    plt.show()",0.4332527518,
1744,ratings,"def add_ratings(self, ratings):
    """"""Add additional movie ratings in the format (user_id, movie_id, rating)
    """"""
    # Convert ratings to an RDD
    new_ratings_RDD = self.sc.parallelize(ratings)
    # Add new ratings to the existing ones
    self.ratings_RDD = self.ratings_RDD.union(new_ratings_RDD)
    # Re-compute movie ratings count
    self.__count_and_average_ratings()
    # Re-train the ALS model with the new ratings
    self.__train_model()

    return ratings

# Attach the function to a class method
RecommendationEngine.add_ratings = add_ratings",0.4313351512,
1744,ratings,"def compute_recommendations(algo):
    global df_ratings
    df_ratings=df_ratings[['user_id','item_id','rating']]
    df_ratings = df_ratings.dropna()
    df_ratings = df_ratings.drop_duplicates()

    #formatting the dataset using the surprise library
    reader = Reader(line_format='user item rating', sep=',', rating_scale=(1, 5))
    data = Dataset.load_from_df(df_ratings, reader=reader)
    training_set = data.build_full_trainset()

    algo = algo # use the singular value decomposition
#     cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)

#     algorithm.fit(training_set)# fit the data to the model
#     testing_set = training_set.build_anti_testset()
#     predictions = algorithm.test(testing_set)# make prediction



    param_grid = {'n_epochs': [5, 10], 'lr_all': [0.002, 0.005],
                  'reg_all': [0.4, 0.6]}
    
    print(""\nGrid Search time"")
    start_time = time.time()
    gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3)
    gs.fit(data)
    print(time.time() - start_time, ""seconds"")
    
    # best RMSE score
    print(gs.best_score['rmse'])

    # combination of parameters that gave the best RMSE score
    print(gs.best_params['rmse'])
    
    
    algo = gs.best_estimator['rmse']
    
    print(""\nCross Validation Score"")
    cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)

    
    print(""\nAlgorithm Train Time"")
    start_time = time.time()
    algo.fit(training_set)# fit the data to the model
    print(time.time() - start_time, ""seconds"")
    
    
    print(""\nAlgorithm BUILD Anti-Testset Time"")
    start_time = time.time()
    algo.fit(training_set)# fit the data to the model
    print(time.time() - start_time, ""seconds"")
    
    
    print(""\nAlgorithm BUILD Anti-Testset Time"")
    start_time = time.time()
    testing_set = training_set.build_anti_testset()
    print(time.time() - start_time, ""seconds"")
    
    
    
    print(""\nAlgorithm Testing Anti_testset Time"")
    start_time = time.time()
    predictions = algo.test(testing_set)# make prediction
    print(time.time() - start_time, ""seconds"")
    
#     print(""\nMean Absolute Error"", float(accuracy.mae(predictions)))
#     print(""\nMean Square Error"", float(accuracy.rmse(predictions)))

    global mae
    global rmse
    mae = accuracy.mae(predictions)
    rmse = accuracy.rmse(predictions)
    mae = float(mae)
    rmse = float(rmse)
    print(""\nMean Absolute Error"", mae)
    print(""\nMean Square Error"", rmse)",0.4200853407,
1744,ratings,"def denormalize(coeffs, X, y):
    
      
    if len(coeffs) == 2:
        
        new_coeffs = [(y.std()*coeffs[0] + y.mean()) - (coeffs[1]*X.mean())*(y.std()/X.std()), 
                      coeffs[1]*y.std()/X.std()]

    else:
        xStds = [X[col].std() for col in X]
        yStds = y.std()
        avgs = [X[col].mean() for col in X]
        
        const = []
        new_coeffs = []
        for i in range(len(xStds)):
            new_coeffs.append(coeffs[i+1]*(yStds/xStds[i]))
            const.append(coeffs[i+1]*avgs[i] * yStds/xStds[i])
        
        new_coeffs.insert(0,yStds*coeffs[0]+y.mean()-sum(const))
        
        
    return new_coeffs",0.4191791415,
1744,ratings,"# compare the distributions with chisquare
def compare_ratings(values_a, values_b):
    distributions = []
    for values in [values_a, values_b]:
        counts = values.rating.astype(int).value_counts().sort_index() 
        distributions.append(counts / sum(counts) * 100)
    print(stats.chisquare(distributions[1], distributions[0]))
    print(stats.chisquare(distributions[0], distributions[1]))",0.4191142917,
1744,ratings,"def players_involved_oh(X_train, X_test):
    # Players involved in the game using a one-hot encoding
    pitcher_onehot = CountVectorizer(binary=True).fit(X_train.pitchers.values)
    pitcher_train = pitcher_onehot.transform(X_train.pitchers.values).todense()
    pitcher_test = pitcher_onehot.transform(X_test.pitchers.values).todense()

    batter_onehot = CountVectorizer(binary=True).fit(X_train.batters.values)
    batter_train = batter_onehot.transform(X_train.batters.values).todense()
    batter_test = batter_onehot.transform(X_test.batters.values).todense()
    
    return pitcher_train, pitcher_test, batter_train, batter_test",0.4186628461,
1744,ratings,"def sgd_factorize(self):
    #solve for these for matrix ratings        
    ratings_row, ratings_col = self.ratings.nonzero()
    num_ratings = len(ratings_row)
    learning_rate = self.cfg.learning_rate_mf
    regularization = self.cfg.lambda_mf

    self.ratings_global_mean = np.mean(self.ratings[np.where(self.ratings != 0)])

    print 'Doing matrix factorization...'
    try:
        for iter in range(self.cfg.max_iterations_mf):
            print 'Iteration: ', iter
            rating_indices = np.arange(num_ratings)
            np.random.shuffle(rating_indices)

            for idx in rating_indices:
                user = ratings_row[idx]
                item = ratings_col[idx]

                pred = self.predict_user_rating(user, item)
                error = self.ratings[user][item] - pred

                self.user_factors[user] += learning_rate \
                                            * ((error * self.item_factors[item]) - (regularization * self.user_factors[user]))
                self.item_factors[item] += learning_rate \
                                            * ((error * self.user_factors[user]) - (regularization * self.item_factors[item]))

                self.user_biases[user] += learning_rate * (error - regularization * self.user_biases[user])
                self.item_biases[item] += learning_rate * (error - regularization * self.item_biases[item])

            self.sgd_mse()

    except FloatingPointError:
        print 'Floating point Error: '
GroupRec.sgd_factorize = sgd_factorize


def sgd_mse(self):
    self.predict_all_ratings()
    predicted_training_ratings = self.predictions[self.ratings.nonzero()].flatten()
    actual_training_ratings = self.ratings[self.ratings.nonzero()].flatten()

    predicted_test_ratings = self.predictions[self.test_ratings.nonzero()].flatten()
    actual_test_ratings = self.test_ratings[self.test_ratings.nonzero()].flatten()

    training_mse = mean_squared_error(predicted_training_ratings, actual_training_ratings)
    print 'training mse: ', training_mse
    test_mse = mean_squared_error(predicted_test_ratings, actual_test_ratings)
    print 'test mse: ', test_mse
GroupRec.sgd_mse = sgd_mse


def predict_user_rating(self, user, item):
    prediction = self.ratings_global_mean + self.user_biases[user] + self.item_biases[item]
    prediction += self.user_factors[user, :].dot(self.item_factors[item, :].T)
    return prediction
GroupRec.predict_user_rating = predict_user_rating

def predict_group_rating(self, group, item, method):
    if (method == 'af'):
        factors = group.grp_factors_af; bias_group = group.bias_af
    elif (method == 'bf'):
        factors = group.grp_factors_bf; bias_group = group.bias_bf
    elif (method == 'wbf'):
        factors = group.grp_factors_wbf; bias_group = group.bias_wbf

    return self.ratings_global_mean + bias_group + self.item_biases[item] \
                                    + np.dot(factors.T, self.item_factors[item])
GroupRec.predict_group_rating = predict_group_rating

def predict_all_ratings(self):
    for user in range(self.num_users):
        for item in range(self.num_items):
            self.predictions[user, item] = self.predict_user_rating(user, item)
GroupRec.predict_all_ratings = predict_all_ratings",0.4180263281,
1744,ratings,"def train_and_eval(model):
    model.fit(train_data[:,0:-1], train_data[:,-1])
    return model.score(test_data[:,:-1], test_data[:,-1])",0.4173049331,
1744,ratings,"def answer_eight():
    X_train, X_test, y_train, y_test = answer_four()
    knn = answer_five()
    
    # Your code here
    return knn.score(X_test, y_test)",0.4170301855,
1744,ratings,"# Function to check model accuracy
def DT(data):
    clf = DecisionTreeClassifier()
    clf.fit(data[0], data[2])
    return (clf.score(data[1], data[3])*100)",0.4117171168,
1119,loading the data and setting the datetimeindex,"def datetime_process(df):
    '''
    convert datetime in string format to datetime object
    '''
    df['pickup_datetime'] = pd.to_datetime(df.pickup_datetime)
    # get date
    df.loc[:, 'pickup_date'] = df['pickup_datetime'].dt.date
    df.loc[:, 'pickup_month'] = df['pickup_datetime'].dt.month
    df.loc[:, 'pickup_weekday'] = df['pickup_datetime'].dt.weekday
    df.loc[:, 'pickup_weekofyear'] = df['pickup_datetime'].dt.weekofyear
    # hour of the day
    df.loc[:, 'pickup_hour'] = df['pickup_datetime'].dt.hour
    # minute 
#     df.loc[:, 'pickup_minute'] = df['pickup_datetime'].dt.minute
    # 'pickup_dt' - duration to the minimum pickup_datetime in train
#     df.loc[:, 'pickup_dt'] = (df['pickup_datetime'] - df['pickup_datetime'].min()).dt.total_seconds()
    # hour from the begining of a week
#     df.loc[:, 'pickup_week_hour'] = df['pickup_weekday'] * 24 + df['pickup_hour']
    return df",0.4960606992,
1119,loading the data and setting the datetimeindex,"# Mean and Median trip distance grouped by hour of the day
def pickup_dropTime(pdt):
    pdt.loc[:,'lpep_pickup_datetime'] = pd.to_datetime(pdt['lpep_pickup_datetime'])
    pdt.loc[:,'Lpep_dropoff_datetime'] = pd.to_datetime(pdt['Lpep_dropoff_datetime'])
    pdt.loc[:,'trav_mins'] = (pdt.Lpep_dropoff_datetime-pdt.lpep_pickup_datetime).astype('timedelta64[m]')
    pdt.loc[:,'hour'] = pdt.lpep_pickup_datetime.map(lambda x: x.hour)
    pdt.loc[:,'weekday'] = pdt.lpep_pickup_datetime.map(lambda x: x.weekday())
    pdt.loc[:,'weekday'] = pdt.weekday.map({0:0,1:0,2:0,3:0,4:0,5:1,6:1})
    return pdt",0.4927093089,
1119,loading the data and setting the datetimeindex,"def convert_to_NYC(df):
    df.timestamp = pd.to_datetime(df.timestamp, utc=True)
    df.set_index('timestamp', drop=True, inplace=True)
    df.index = df.index.tz_convert('America/New_York')
    return df",0.4870942235,
1119,loading the data and setting the datetimeindex,"def clean_datetime (df):
    #clean up datetime
    df.loc[:,'datetime_request'] = pd.to_datetime(df.unix_timestamp_of_request.map(lambda x: format(x, 'f')), unit = 's')
    df.loc[:,'datetime_request_utc'] = pd.to_datetime(df.unix_timestamp_of_request_utc.map(lambda x: format(x, 'f')), unit = 's')
    #build local month, week, day, hour
    df.loc[:,'month_request'] = df['datetime_request'].dt.month
    df.loc[:,'week_request'] = df['datetime_request'].dt.week
    df.loc[:,'weekday_request'] = df['datetime_request'].dt.weekday
    df.loc[:,'hour_request'] = df['datetime_request'].dt.hour
    
clean_datetime(train_data_df)
clean_datetime(dev_data_df)",0.4865990281,
1119,loading the data and setting the datetimeindex,"def process_bike_count_data(df):
    """"""
    Process the provided dataframe: parse datetimes and rename columns.
    
    """"""
    df.index = pd.to_datetime(df['dag'] + ' ' + df['tijdstip'], format=""%d.%m.%y %H:%M:%S"")
    df = df.drop(['dag', 'tijdstip'], axis=1)
    df = df.rename(columns={'noord': 'north', 'zuid':'south', 'actief': 'active'})
    return df",0.4767262936,
1119,loading the data and setting the datetimeindex,"def prepare_data():
    csv_raw = pd.read_csv(""fb_ads_raw.csv"")
    csv_raw.date = pd.to_datetime(csv_raw['date'])

    #get detailed season and day_of_week from the date function
    month = csv_raw.date.dt.month
    day = csv_raw.date.dt.dayofweek

    #change months into season format
    conditions = [ (month >= 9), (month >= 6), (month >= 3)]
    choices = [3, 2, 1]
    season = np.select(conditions, choices, default=0)

    #insert the day and the season columns
    csv_raw.insert(1,'day', day)
    csv_raw.insert(0,'season', season)
    grouped = csv_raw.groupby('date').mean()
    return grouped",0.472874105,
1119,loading the data and setting the datetimeindex,"def massage_data(run):
    # Filter some nonsense values
    run.linuxtime = run.linuxtime.apply(datetime.datetime.fromtimestamp)
    run.rpm = run.rpm.apply(lambda x: x if x < 4000 and x >= 1 else np.nan)
    run.ds_volt = run.ds_volt.apply(lambda x: x if x < 400 and x > 100 else np.nan)
    run.ds_cur_48v = run.ds_cur_48v.apply(lambda x: x if x > 0 else np.nan)
    
    # Fill forward
    run.fillna(method='pad', inplace=True)
    
    run.set_index(run.linuxtime, drop=True, inplace=True)
    del run[""linuxtime""]
    del run[""time""]
#     del run[""soc""]
    for col in run:
        run[col].index = run.index

    return run",0.4713687301,
1119,loading the data and setting the datetimeindex,"## Adjusting datetime data

def clean_datetime(df):
    df['pickup_datetime'] = pd.to_datetime(df.pickup_datetime)
    df.loc[:, 'pickup_week'] = df.pickup_datetime.apply(lambda x: x.isocalendar()[1])
    df.loc[:, 'pickup_weekday'] = df.pickup_datetime.apply(lambda x: x.weekday())
    df.loc[:, 'pickup_hour'] = df.pickup_datetime.apply(lambda x: x.hour)
    return df

train = clean_datetime(train)
test = clean_datetime(test)",0.4709992409,
1119,loading the data and setting the datetimeindex,"def compute_tender_duration(df):
    df['FechasFechaPublicacion'] = pd.to_datetime(df.loc[:,'FechasFechaPublicacion'])
    df['FechasFechaCierre'] = pd.to_datetime(df.loc[:,'FechasFechaCierre'])
    df['tender_duration'] = df['FechasFechaCierre'] - df['FechasFechaPublicacion']",0.4703938067,
1119,loading the data and setting the datetimeindex,"#Read data from file
def read():
    filename='dmdata.csv'
    global df
    df=pd.read_csv(filename)
    #df=df.drop(columns=['Unnamed: 0'])
    df['dt']=pd.to_datetime(df['dt'])
    df['ds']=pd.to_datetime(df['ds'])
    df.index=df.dt
    df=df.drop(columns=['dt'])
read()",0.4678466916,
1958,spam classification,"def doPCA():
    from sklearn.decomposition import PCA
    pca = PCA(n_components=2)
    pca.fit(data)
    return pca",0.5134782791,
1958,spam classification,"def makeDataset(n_samples=500, n_features=4, n_classes=2):
    # wraper to generate synthetic classification datasets
    X,y = datasets.make_classification(n_samples=n_samples, n_features=n_features, 
                                      n_classes=n_classes, n_clusters_per_class=1,
                                      n_informative=n_features, n_redundant=0, n_repeated=0,
                                      weights=None, shuffle=True, random_state=0)

    predictors = ['Feat%1d' %(i+1) for i in range(n_features)]
    data = pd.DataFrame(X, columns=predictors)
    data['class'] = pd.Series(y)

    response = 'class'
    return (data, predictors, response, n_classes)",0.5120461583,
1958,spam classification,"def make_fooling_image_high(X, target_y, model):

    X_fooling = X.copy()
    learning_rate = 1

    objective = model.classifier[0, target_y]
    grad = tf.gradients(objective, [model.image])[0]
    probs = tf.nn.softmax(model.classifier)
    
    for i in xrange(200):
        g, scores = sess.run([grad, probs[0]], feed_dict={model.image: X_fooling})
        print(""Iteration %d: max label %d, max label prob %f, taget label prob %f"" % \
              (i, np.argmax(scores), np.max(scores), scores[target_y]))
        
        # only stop when ouput probability is higher than 99%
        if scores[target_y] >= 0.99:
            print(""Complete!"")
            break
        else:
            X_fooling += learning_rate * g #/ np.sqrt(np.sum(g ** 2))

    return X_fooling",0.5074990988,
1958,spam classification,"from sklearn import preprocessing

def clahe_transform(X_batch):
    output_c = np.empty(X_batch.shape)
    #https://stackoverflow.com/questions/24341114/simple-illumination-correction-in-images-opencv-c/24341809#24341809

    clahe = cv2.createCLAHE(clipLimit=4.0, tileGridSize=(8,8))
    
    if len(X_batch.shape) == 3:
        image = X_batch
        image = image.astype(np.uint8)
        lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)
        l, a, b = cv2.split(lab)
        transform = clahe.apply(l)
        temp = cv2.merge((transform,a,b))
        output_c = cv2.cvtColor(temp, cv2.COLOR_LAB2RGB)
    else:
        for index,image in enumerate(X_batch):
            image = image.astype(np.uint8)
            lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)
            l, a, b = cv2.split(lab)
            transform = clahe.apply(l)
            temp = cv2.merge((transform,a,b))
            output_c[index] = cv2.cvtColor(temp, cv2.COLOR_LAB2RGB)

    return output_c",0.5074928403,
1958,spam classification,"def doPCA():
    from sklearn.decomposition import PCA
    pca = PCA(n_components=5)
    pca.fit(train_tiles)
    return pca",0.5045194626,
1958,spam classification,"def getTestTrainSplits(X, y):
    return train_test_split(X, y, test_size=0.3, train_size=0.7)
xTrain, xTest, yTrain, yTest = getTestTrainSplits(X, y)",0.5039333105,
1958,spam classification,"def select_features(X,y):
    selector = RandomizedLogisticRegression(n_resampling=500, 
                                            random_state=101, 
                                            selection_threshold=0.001)
    selector.fit(X,y)
    return(selector.transform(X))",0.5032095909,
1958,spam classification,"def predict_age_LightGBM():

    X = pre_age_train_test_drop
    y = cats
    
    model_age_lgb = lgb.LGBMClassifier(nthread=3)
    model_age_lgb.fit(X,y)

    print(classification_report(y, model_age_lgb.predict(pre_age_train_test_drop)))
    model_age_lgb = model_age_lgb.predict(pre_age_mission_test_drop)
    model_age_lgb = pd.DataFrame(model_age_lgb)
    
    return model_age_lgb

model_age_lgb = predict_age_LightGBM()",0.5021846294,
1958,spam classification,"def P10(num_examples=20):

### STUDENT START ###
    num_digits = 10 # number of digits to show

    # Fit BernoulliNB model with alpha=0.01 (as per P8's response)
    bnb = BernoulliNB(alpha=0.01, binarize=.5, fit_prior=False)
    bnb.fit( mini_train_data , mini_train_labels) 
    
    ## Generate the figure
    fig, ax = plt.subplots(num_digits, num_examples, figsize=(16,8))

    for dig in xrange(num_digits): # for each digit
        for j in xrange(num_examples): # for each plot within each digit
            
            # Generate example by generating random numbers and using the model's probabilities to binarize
            _example = (np.random.rand(784) < np.exp(bnb.feature_log_prob_[dig])).reshape(28,28)

            # Plot with no axes
            ax[dig,j].imshow(_example, cmap=plt.cm.gray)
            ax[dig,j].axis('off')

    # Minor figure adjustments
    fig.suptitle('Showing {} randomly generated examples for each digit'.format(num_examples), size=18)
    fig.subplots_adjust(wspace=.02, hspace=.02)


### STUDENT END ###

P10(20)",0.5016934872,
1958,spam classification,"def random_forest(X_train, y_train):
    param_grid = {
        'n_estimators': (10, 50),
        'min_samples_leaf': (2, 3, 10),
        'max_features': (0.2, 0.3),
    }
    model = RandomForestClassifier(random_state = 333, class_weight='balanced', n_jobs = -1)
    model_tuning = GridSearchCV(
        model,
        param_grid=param_grid,
        scoring=make_scorer(hamming_loss, greater_is_better=False),
        cv=3,
        n_jobs=-1,
        verbose=3,
    )
    model_tuning.fit(X_train, y_train)
    return model_tuning",0.5010307431,
445,declaring the dataset,"# Initialize placholder for biases
# Define a bias initialization function
def init_biases(shape):
#     biases = tf.Variable(tf.random_normal(shape))
    biases = tf.Variable(tf.zeros(shape))
    return biases",0.415951252,
445,declaring the dataset,"def scale_parametrized_guide(guess):
    a = pyro.param(""a"", Variable(torch.randn(1) + guess.data.clone(), requires_grad=True))
    b = pyro.param(""b"", Variable(torch.randn(1), requires_grad=True))
    return pyro.sample(""weight"", dist.normal, a, torch.abs(b))",0.4108654857,
445,declaring the dataset,"def scale_parametrized_guide(guess):
    a = pyro.param(""a"", Variable(torch.randn(1) + guess.data.clone(), requires_grad=True))
    b = pyro.param(""b"", Variable(torch.randn(1), requires_grad=True))
    return pyro.sample(""weight"", dist.normal, a, torch.abs(b))
# IMPORTANT: The parameters are the ones that will get updated by optimization
# (eg: by SGD). When setting requires_grad = True, it signals that (unsure?)",0.4108654857,
445,declaring the dataset,"def input_fn(data):
    features = {feature_name: tf.constant(data.data)}
    label = tf.constant(data.target)
    return features, label",0.4084785581,
445,declaring the dataset,"class DT(object):
    def __init__(self):
        self.data = None  # training data set (loaded into memory)
        self.model = None  # decision tree model
        self.default_class = None  # default target class

    def __load_data(self):
        with open(INFILE) as csvfile:
            self.data = []
            csvreader = csv.reader(csvfile, delimiter=',')
            for row in csvreader:
                rec = []
                for i in range(len(ATTR)):
                    val = row[i].strip()
                    # convert numerical attributes
                    if ATTR[i].type == AttrType.num:  # Note that this will break for ""?"" (missing attribute)
                        val = float(val)
                    rec.append(val)
                self.data.append(rec)
                # self.data.append([element.strip() for element in row])  # strip spaces

    def __entropy(self, records):
        """"""
        Calculates entropy for a selection of records.

        :param records: Data records (given by indices)
        """"""
        # TODO
        return 0

    def __find_best_attr(self, attrs, records):
        """"""
        Finds the attribute with the largest gain.

        :param attrs: Set of attributes
        :param records: Training set (list of record ids)
        :return:
        """"""
        entropy_p = self.__entropy(records)  # parent's entropy
        splittings = []  # holds the splitting information for each attribute

        for a in attrs:
            assert ATTR[a].type in AttrType
            splits = {}  # record IDs corresponding to each split
            # splitting condition depends on the attribute type
            if ATTR[a].type == AttrType.target:  # skip target attribute
                continue
            elif ATTR[a].type == AttrType.cat:  # categorical attribute
                # multi-way split on each possible value
                split_mode = SplitType.multi
                # each possible attr value corresponds to a split (indexed with categorical labels)
                # Note: it's important to consider attr values from the entire training set
                split_cond = set([self.data[idx][a] for idx in range(len(self.data))])
                
                # TODO collect training records for each split 
                # `splits[val]` holds a list of records for a given split,
                # where `val` is an element of `split_cond`
            elif ATTR[a].type == AttrType.num:  # numerical attribute => binary split on median value
                split_mode = SplitType.bin
                split_cond = self.__median(a)  # (i.e., if less or equal than this value)
                # TODO collect training records for each split (in `splits`)

            # TODO compute gain for attribute a
            infogain = 0

            splitting = Splitting(a, infogain, split_mode, split_cond, splits)
            splittings.append(splitting)

        # find best splitting
        best_splitting = sorted(splittings, key=lambda x: x.infogain, reverse=True)[0]
        return best_splitting

    def __add_node(self, parent_id, node_type=NodeType.internal, edge_value=None, val=None, split_type=None,
                   split_cond=None):
        """"""
        Adds a node to the decision tree.

        :param parent_id:
        :param node_type:
        :param edge_value:
        :param val:
        :param split_type:
        :param split_cond:
        :return:
        """"""
        node_id = len(self.model)  # id of the newly assigned node
        if not self.model:  # the tree is empty
            node_type = NodeType.root

        node = Node(node_id, node_type, parent_id, children=[], edge_value=edge_value, val=val, split_type=split_type,
                    split_cond=split_cond)
        self.model.append(node)

        # also add it as a child of the parent node
        if parent_id is not None:
            self.model[parent_id].append_child(node_id)

        return node_id

    def __id3(self, attrs, records, parent_id=None, value=None):
        """"""
        Function ID3 that returns a decision tree.

        :param attrs: Set of attributes
        :param records: Training set (list of record ids)
        :param parent_id: ID of parent node
        :param value: Value corresponding to the parent attribute, i.e., label of the edge on which we arrived to this node
        :return:
        """"""
        # empty training set or empty set of attributes => create leaf node with default class
        if not records or not attrs:
            self.__add_node(parent_id, node_type=NodeType.leaf, edge_value=value, val=self.default_class)
            return

        # if all records have the same target value => create leaf node with that target value
        same = all(self.data[idx][IDX_TARGET] == self.data[records[0]][IDX_TARGET] for idx in records)
        if same:
            target = self.data[records[0]][IDX_TARGET]
            self.__add_node(parent_id, node_type=NodeType.leaf, edge_value=value, val=target)
            return

        # find the attribute with the largest gain
        splitting = self.__find_best_attr(attrs, records)
        # add node
        node_id = self.__add_node(parent_id, edge_value=value, val=splitting.attr, split_type=splitting.split_type,
                                  split_cond=splitting.cond)
        # TODO call tree construction recursively for each split

    def print_model(self, node_id=0, level=0):
        node = self.model[node_id]
        indent = ""  "" * level
        if node.type == NodeType.leaf:
            print(indent + str(node.edge_value) + "" [Leaf node] class="" + node.val)
        else:
            cond = "" <= "" + str(node.split_cond) if ATTR[node.val].type == AttrType.num else "" == ? ""
            if node.type == NodeType.root:
                print(""[Root node] '"" + ATTR[node.val].label + ""'"" + cond)
            else:
                print(indent + str(node.edge_value) + "" [Internal node] '"" + ATTR[node.val].label + ""'"" + cond)
            # print tree for child notes recursively
            for n_id in node.children:
                self.print_model(n_id, level + 1)

    def build_model(self):
        self.__load_data()
        self.model = []  # holds the decision tree model, represented as a list of nodes
        # Get majority class
        #   Note: Counter returns a dictionary, most_common(x) returns a list with the x most common elements as
        #         (key, count) tuples; we need to take the first element of the list and the first element of the tuple
        self.default_class = Counter([x[IDX_TARGET] for x in self.data]).most_common(1)[0][0]
        self.__id3(set(range(len(ATTR) - 1)), list(range(len(self.data))))

    def apply_model(self, record):
        node = self.model[0]
        while node.type != NodeType.leaf:
        	# TODO based on the value of the record's attribute that is tested in `node`,
        	# set `node` to one of its child nodes until a leaf node is reached
        return node.val",0.4077078104,
445,declaring the dataset,"def get_train_inputs():
    x = tf.constant(training_set.data)
    y = tf.constant(training_set.target)
    #  y = tf.summary.scalar(, training_set.target)
    return x, y",0.4049842656,
445,declaring the dataset,"def get_train_inputs():
    x = tf.constant(training_set.data)
    y = tf.constant(training_set.target)
    return x, y",0.4049842656,
445,declaring the dataset,"def training_inputs():
    x = tf.constant(training_set.data)
    y = tf.constant(training_set.target)
    
    return x,y",0.4049842656,
445,declaring the dataset,"# Define test inputs
def test_inputs():
    x = tf.constant(test_set.data)
    y = tf.constant(test_set.target)
    
    return x, y",0.4049842656,
445,declaring the dataset,"def get_test_inputs():
    x = tf.constant(test_set.data)
    y = tf.constant(test_set.target)
    return x, y",0.4049842656,
2363,the scientific python ecosystem,from synthanalysis import benchio,0.4790279269,
2363,the scientific python ecosystem,def blue_frog_prob():,0.4663023055,
2363,the scientific python ecosystem,beatbox.You.calculate_sdv_Cyy_inverse(),0.4634310305,
2363,the scientific python ecosystem,import peslearn,0.4621460438,
2363,the scientific python ecosystem,result.poles,0.4601308405,
2363,the scientific python ecosystem,"sol.solve(sys)
semilogy(sol.getSp().Funer())",0.4591273069,
2363,the scientific python ecosystem,"### REPLACE BY YOUR CODE
import solutions
solutions.s1c()
###",0.4587479234,
2363,the scientific python ecosystem,from poliastro.neos import neows,0.4580100477,
2363,the scientific python ecosystem,from equilibrium.efit import EfitEquilibrium,0.4576363564,
2363,the scientific python ecosystem,lrisb.show_science(),0.4567244947,
855,how well does kmeans perform?,import kmeans,0.5447570086,
855,how well does kmeans perform?,"#sklearn.metrics .mean_squared_log_error seems to exist but I cannot load it..
from sklearn.metrics import mean_squared_error
def mean_squared_log_error(y_pred, y_true, **dict):
    '''Assume y_true starts earlier than y_pred, y_true is NaN free, and NaN in y_pred are only in the beginning'''
    indafterNaN = y_pred.first_valid_index()
    if (y_true.index[0] > y_pred.index[0]): return ""Check indices of prediction and true value""
    ind1stcommon = y_true.index[y_true.index==y_pred.index[0]]
    indstart = max(indafterNaN, ind1stcommon)
    indend = y_true.index[-1]
    return mean_squared_error(np.log(y_true[indstart:indend]+1), 
                              np.log(y_pred[indstart:indend]+1) )**0.5

def plotSARIMAX(labels, pred):
    fig = plt.figure(figsize=(12, 8))
    layout = (2, 2)
    ax1 = plt.subplot2grid(layout, (0, 0), colspan=2)
    ax3 = plt.subplot2grid(layout, (1, 0))
    ax4 = plt.subplot2grid(layout, (1, 1))
    labels.plot(ax=ax1);
    pred.plot(ax=ax1, title='MSE: %.4f'% mean_squared_log_error(pred, labels))
    ax3 = sm.graphics.tsa.plot_acf(results.resid, lags=40, alpha=.05, ax=ax3, title=""ACF of residuals"")
    ax4 = sm.graphics.tsa.plot_pacf(results.resid, lags=40, alpha=.05, ax=ax4, title=""PACF of residuals"")
    plt.tight_layout()
    print(""ACF and PACF of residuals"")",0.5432511568,
855,how well does kmeans perform?,"# Step 5: try some ML models
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import cross_val_score

# function to display results from cross validation
def display_scores(scores):
    print(""Scores:"", scores)
    print(""Mean:"", scores.mean())
    print(""Standard deviation:"", scores.std())
    
# prepare training set by calling full pipeline
dataset_train_prepared_Xattributes_numpy = full_pipeline.fit_transform(dataset_train_Xattributes)

from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(dataset_train_prepared_Xattributes_numpy, dataset_train_Ylabel)

# rmse on redicted values
dataset_predictions = lin_reg.predict(dataset_train_prepared_Xattributes_numpy)
lin_mse = mean_squared_error(dataset_train_Ylabel, dataset_predictions)
lin_rmse = np.sqrt(lin_mse)
print(""RMSE:"", lin_rmse)

# cross validation on the training dataset
scores = cross_val_score(lin_reg,dataset_train_prepared_Xattributes_numpy,dataset_train_Ylabel,scoring=""neg_mean_squared_error"", cv=10)
lin_rmse_scores = np.sqrt(-scores)
display_scores(lin_rmse_scores)",0.5421372652,
855,how well does kmeans perform?,"def find_centroids(clustered_data, num_of_clusters):
    """""" Use scipy.cluster.vq.kmeans to determine centroids of clusters
    Parameters:
        clustered_data: the data projected onto the new basis
        num_of_clusters: the expected number of clusters in the data
    Returns: 
        The centroids of the clusters
    Hint 1: make sure to first 'whiten' the data (refer to docs)
    """"""
    whitened_data = scipy.cluster.vq.whiten(clustered_data)
    return scipy.cluster.vq.kmeans(whitened_data, num_of_clusters)[0]",0.5411944389,
855,how well does kmeans perform?,"m, c = spectral.kmeans(img, 4, 5)",0.5383319855,
855,how well does kmeans perform?,"from sklearn.metrics import mean_squared_error as mse
import matplotlib.pyplot as plt

def get_slope(input_data, target, weights):
    preds = (input_data * weights).sum()
    error = preds - target
    return 2 * input_data * error


def get_mse(input_data, target, weights):
    preds = (input_data * weights).sum()
    return mse(target, preds)


n_updates = 20
mse_hist = []

for i in range(n_updates):
    slope = get_slope(input_data, target, weights)
    
    weights = weights - 0.01 * slope
    
    mse = get_mse(input_data, target, weights)
    
    mse_hist.append(mse)
    

# Plot the mse History
plt.plot(mse_hist)
plt.xlabel('Iterations')
ply.ylabel('Mean Squared Error')
plt.show()",0.5381851792,
855,how well does kmeans perform?,"# Calculating the cluster centroids and variance # from kmeans
centroids, variance = vq.kmeans(data, 3)",0.536062479,
855,how well does kmeans perform?,"# Machine Learning in Python is a oneliner!
centroids, variance = vq.kmeans(data, 3)
#... after preparing the data and importing the scikit-learn package",0.536062479,
855,how well does kmeans perform?,"# calculate the centroid coordinates of each cluster 
# and the variance of all the clusters
centroids, variance  = vq.kmeans(battles, 3)",0.536062479,
855,how well does kmeans perform?,from kmeans import *,0.5357906222,
163,calculate transcripts per million from the reads counted by featurecounts,"#Counting the frequency of each words

def common_words(manifesto, n):
    '''returns the n most common words
    assumes the manifesto is tokenized by words'''
    freq = nltk.FreqDist(manifesto)
    result = freq.most_common(n)
    return(result)


most_common_renua = common_words(words_renua, 20)
most_common_ddi = common_words(words_ddi, 20)


#Printing the 20 most common words

print(""Renua's manifesto most common words are:"")
for i,j in enumerate(most_common_renua):
    print(""%s. \""%s\"" appears %s times"" % (i + 1, j[0], j[1]))

print(""\n\nDirect Democracy Ireland's manifesto most common words are:"")
for i,j, in enumerate(most_common_ddi):
    print(""%s. \""%s\"" appears %s times"" % (i + 1, j[0], j[1]))",0.4738522172,
163,calculate transcripts per million from the reads counted by featurecounts,"def qualStats(sourceDir, fileName):
    outFile = fileName + '_qualStats'
    !cd $sourceDir; \
        fastx_quality_stats -i $fileName -o $outFile -Q 33
    return outFile
    
qualStatsRes = qualStats(seqDir, 'pear_merged-2017-04-18.assembled.fastq')",0.4729509354,
163,calculate transcripts per million from the reads counted by featurecounts,"def calcMortalityRateGroupedByFeature(data, feature):
    count = data.groupby(feature).count()['HADM_ID']
    expires = data.groupby(feature).sum()['HOSPITAL_EXPIRE_FLAG']
    
    print
    print ""Mortality rates by {}: {} distinct items"".format(feature, len(count))
    for key in count.keys():
        print key, float(expires[key]) / count[key], count[key]
    
#calcMortalityRateGroupedByFeature(er_demographic_data, 'AGE')
#calcMortalityRateGroupedByFeature(er_demographic_data, 'GENDER')
#calcMortalityRateGroupedByFeature(er_demographic_data, 'INSURANCE')
#calcMortalityRateGroupedByFeature(er_demographic_data, 'MARITAL_STATUS')
#calcMortalityRateGroupedByFeature(er_demographic_data, 'ETHNICITY')
#calcMortalityRateGroupedByFeature(er_demographic_data, 'RELIGION')",0.4712943137,
163,calculate transcripts per million from the reads counted by featurecounts,"def answer_eight():
    pos = [y for x, y in nltk.pos_tag(text1)]
    df = pd.value_counts(pos)
    ans = [(x, y) for x, y in zip(df.index[:5], df[:5])]
    return ans

answer_eight()",0.4680119753,
163,calculate transcripts per million from the reads counted by featurecounts,"def load_pathway_definitions_file(pathways_file, shorten_pathway_name):
    pathway_definitions_df = pd.read_table(
        pathways_file, header=None, names=[""pw"", ""size"", ""genes""])
    pathway_definitions_df[""genes""] = pathway_definitions_df[""genes""].map(
        lambda x: x.split("";""))
    pathway_definitions_df.set_index(""pw"", inplace=True)

    pathway_definitions_map = {}
    for index, row in pathway_definitions_df.iterrows():
        pathway = shorten_pathway_name(index)
        pathway_definitions_map[pathway] = set(row[""genes""])
    return pathway_definitions_map",0.4657963216,
163,calculate transcripts per million from the reads counted by featurecounts,"def load_tennis_data(partition='train', data_dir='data/'):
    df_mens = pd.read_csv(os.path.join(data_dir, 'mens_%s_file.csv' % partition))
    df_womens = pd.read_csv(os.path.join(data_dir, 'womens_%s_file.csv' % partition))
    df = df_mens.append(df_womens)
    df.columns = [col.replace('.', '_') for col in df.columns]
    df = df.drop(['train', 'id'], axis=1)
    return df",0.4653238654,
163,calculate transcripts per million from the reads counted by featurecounts,"def load_vcf(path, include_filter=False):
    # Load vcf
    callset = allel.read_vcf(path)

    mapping = {
        ""key"": list(map(lambda x: ""{}_{}"".format(*x),
                   zip(callset['variants/CHROM'], callset['variants/POS']))),
        ""CHROM"": callset['variants/CHROM'],
        ""POS"": callset['variants/POS'],
        ""REF"": callset['variants/REF'],
        ""ALT"": list(map(lambda x: """".join(x), callset['variants/ALT'])),
    }
    if include_filter:
        mapping[""FILTER_PASS""] = callset['variants/FILTER_PASS']

    print(mapping.keys())
    snp = pandas.DataFrame(mapping)
    snp = snp.set_index('key')
    return snp, callset",0.4637320042,
163,calculate transcripts per million from the reads counted by featurecounts,"def do_seqlogo(input_fasta, output_seqlogo):
    fin = open(input_fasta)
    seqs = weblogolib.read_seq_data(fin) 
    data = weblogolib.LogoData.from_seqs(seqs)
    options = weblogolib.LogoOptions()
    options.yaxis_scale = 0.2
    options.yaxis_tic_interval  = 0.2
    format_logo = weblogolib.LogoFormat(data, options)
    fout = weblogolib.eps_formatter(data, format_logo)
    with open(output_seqlogo, 'wb') as fo:
        fo.write(fout)",0.4616024494,
163,calculate transcripts per million from the reads counted by featurecounts,"def percentages_per_rating(reviews_df, reviews_count):
    rating_counts = (reviews_df
         .groupBy('overall')
         .count()
         .rdd
         .map(lambda row: row.asDict().values())
         .collect())
    
    return [ (str(int(rating)), rating_count / float(reviews_count))
        for rating_count, rating
        in rating_counts ]",0.4614298642,
163,calculate transcripts per million from the reads counted by featurecounts,"def answer_four():
    
    dist = nltk.FreqDist(text1)
    return sorted([w for w in dist.keys() if len(w) > 5 and dist[w] > 150])

answer_four()",0.4613800645,
2496,using word vectors,"from utils import word_to_vector, text_to_vectors
# show word vector for existing word
w = ""word""
print(w, utils.word_to_vector(w, word_index, embedding_matrix))
# test word that probably doesn't exist
w = ""linus""
print(w, utils.word_to_vector(w, word_index, embedding_matrix))",0.5837945938,
2496,using word vectors,"word2vec.word2vec('./data/text8-phrases', './data/text8.bin', size=100, verbose=False)",0.5768297911,
2496,using word vectors,"word2vec.word2vec('/Users/drodriguez/Downloads/text8-phrases', '/Users/drodriguez/Downloads/text8.bin', size=100, verbose=True)",0.5760335922,
2496,using word vectors,"word2vec.word2vec('result.txt', 'ress.bin', size=100, verbose=True)",0.5760335922,
2496,using word vectors,"get_cross_val_score(pipeline,word2vec=True)",0.5738028884,
2496,using word vectors,wv_model = word2vec.Word2Vec([tokens]),0.5709826946,
2496,using word vectors,"# Print normed word vector for a word
iWord = ""edmond""
print(""Normed word vector for word ""+ iWord + "" :"")
print(ft.word_to_vec(iWord, model, iNormed=True))
print("""")

# Use ft utils to get vectors for a word list :
iSentence = ""edmond dantes est monte-cristo""
print(""Normed word vector for sentence '""+ iSentence + ""' :"")
print(ft.wordlist_to_vec(iSentence.split("" ""), model, iNormed=True))
print("""")",0.5679342747,
2496,using word vectors,"s = ""Perhaps the best way to describe us is as a Center for the Recently Possible""
vecs = np.array([t.get_item_vector(lookup[w.lower()]) for w in s.split()])
print(s)
op_vecs = np.array([t.get_item_vector(lookup[""error""])]*len(vecs))
for res in progress(vecs, op_vecs, n=25):
    print("" "".join([words[nnslookup(t, nlp, words, i)[0]] for i in res]))",0.5672277808,
2496,using word vectors,"s = ""I am sitting in a room different from the one you are in now""
vecs = np.array([t.get_item_vector(lookup[w.lower()]) for w in s.split()])
print(s)
op_vecs = np.array([t.get_item_vector(lookup[""humming""])]*len(vecs))
for res in progress(vecs, op_vecs, n=25):
    print("" "".join([words[nnslookup(t, nlp, words, i)[0]] for i in res]))",0.5672277808,
2496,using word vectors,"words = np.array(model.get_words())
word_vectors = np.array([model.get_word_vector(w) for w in words])",0.5659576058,
1620,problem functional programming,"def fn(input):
    if <base case>:
        return ...
    else:
        return <combine input and fn(input_reduced)>",0.4476051033,
1620,problem functional programming,"class Solution(object):
    def generateTrees(self, n):
        """"""
        :type n: int
        :rtype: List[TreeNode]
        """"""
        
        t = TreeNode(1)
        if n == 1:
            return [t]
        if n == 0:
            return []
        
        record = [[None],[t]]
        result = []
        
        def fill(n):
            tmp = []
            for i in range(n):
                for j in record[i]:
                    for k in record[n - 1 - i]:
                        t = TreeNode(0)
                        t.left = j
                        t.right = k
                        tmp.append(t)
            record.append(tmp[:])
        
        def FillTree(root,p,nr):
            if root is None:
                return
            else:
                if root.left is not None:
                    nr.left = TreeNode(0)
                    FillTree(root.left,p,nr.left)
                nr.val = p[0] + 1
                p[0] += 1
                if root.right is not None:
                    nr.right = TreeNode(0)
                    FillTree(root.right,p,nr.right)
        
        for i in range(2,n+1):
            fill(i)

        for r in record[n]:
            nr = TreeNode(0)
            p = [0]
            FillTree(r,p,nr)
            result.append(nr)
        return result",0.4322795272,
1620,problem functional programming,"sum(4*l**2 - 6*(l-1) for l in xrange(3, 1002, 2)) + 1",0.4317663312,
1620,problem functional programming,"[i for i in range(5, 56, 5)]",0.4304785728,
1620,problem functional programming,"class Solution(object):
    def bulbSwitch(self, n):
        """"""
        :type n: int
        :rtype: int
        """"""
        bulbs = [1]*n
        for i in range(2,n+1):
            bulbs[i-1::i] = map(lambda x: -x, bulbs[i-1::i])
        return sum(map(lambda x:x==1, bulbs))",0.4301699698,
1620,problem functional programming,"class Solution(object):
    def addTwoNumbers(self, l1, l2):
        
        tmp = ListNode(0) 
        l_result = tmp
        dummy = tmp

 
        
#       dummy = l_result = ListNode(0)   # ??? what does dummy mean  # python's Chained Assignment 
        carry_bit = 0
        while l1 or l2 or carry_bit:
 
            if l1:
                carry_bit += l1.val
                l1 = l1.next
            if l2:
                carry_bit += l2.val
                l2 = l2.next
 
            # get the remainder, and carry bit if necessary
            l_result.next = ListNode(carry_bit%10)
            
            print(showChainList(tmp),'tmp_pre')
            print(showChainList(l_result),'l_result_pre')   
            print(showChainList(dummy),'dummy_pre')
            print('------------')            
            l_result = l_result.next
            carry_bit = carry_bit // 10
            
            print(showChainList(tmp),'tmp')
            print(showChainList(l_result),'l_result')
            print(showChainList(dummy),'dummy')
            print('------------')
            
        return dummy.next
                
 

    # from leetcode 
    def addTwoNumbers2(self, l1, l2):
        """"""
        :type l1: ListNode
        :type l2: ListNode
        :rtype: ListNode
        """"""
        dummy = ListNode(0)
        dummy.next = l1

        curr1 = l1
        curr2 = l2
        while curr1 is not None and curr2 is not None:
            tmp_sum = curr1.val + curr2.val
            if tmp_sum < 10:
                curr1.val = tmp_sum
            else:
                curr1.val = tmp_sum % 10
                if curr1.next is not None:
                    curr1.next.val += tmp_sum // 10
                else:
                    curr1.next = ListNode(tmp_sum // 10)

            prev_curr1 = curr1
            curr1 = curr1.next
            curr2 = curr2.next

        if curr1 is not None and curr2 is None:
            while curr1 is not None and curr1.val >= 10:
                if curr1.next is not None:
                    curr1.next.val += curr1.val // 10
                else:
                    curr1.next = ListNode(curr1.val // 10)
                    
                curr1.val = curr1.val % 10
                curr1 = curr1.next
        if curr1 is None and curr2 is not None:
            prev_curr1.next = curr2

        return dummy.next",0.429880023,
1620,problem functional programming,"class Solution(object):
    def searchRange(self, nums, target):
        def search(n):
            lo, hi = 0, len(nums)
            while lo < hi:
                mid = (lo + hi) / 2
                if nums[mid] >= n:
                    hi = mid
                else:
                    lo = mid + 1
            return lo
        lo = search(target)
        return [lo, search(target+1)-1] if target in nums[lo:lo+1] else [-1, -1]",0.4287160039,
1620,problem functional programming,"(sp.simplify(pfd.args[0]),sp.simplify(pfd.args[1]),sp.simplify(pfd.args[2]))",0.4273666739,
1620,problem functional programming,"class Solution(object):
    def climbStairs(self, n):
        a = b = 1
        for _ in range(n):
            a, b = b, a + b
        return a",0.4273366928,
1620,problem functional programming,"class Solution(object):
    def canPlaceFlowers(self, flowerbed, n):
        """"""
        :type flowerbed: List[int]
        :type n: int
        :rtype: bool
        """"""
        i, count = 0, 0
        while(i < len(flowerbed)):
            if(flowerbed[i] == 0 and (i == 0 or flowerbed[i - 1] == 0) and (i == len(flowerbed)-1 or flowerbed[i+1] == 0)):
                flowerbed[i] = 1
                count += 1
            i += 1
        return count >= n",0.4259246588,
688,fraud detection algorithm for banks,"for x in fraud_splits:
    sample_X, sample_y = split_var_tar(sample_dataset(fraud_data=fraud_w_elim, normal_data=normal_w_elim, pct_fraud=x))
    X_train, X_test, y_train, y_test = train_test_split(sample_X, sample_y, test_size=0.3, random_state=1)
    print('percent fraud: {}%'.format(x))
    model_scores(DecisionTreeClassifier(), X_train, y_train, X_test, y_test, 5)",0.468124181,
688,fraud detection algorithm for banks,"sample_X, sample_y = split_var_tar(sample_dataset(fraud_data=fraud_w_elim, normal_data=normal_w_elim, pct_fraud=50))",0.4414719641,
688,fraud detection algorithm for banks,"# create sample dataset with 50/50 balance between fraud and normal transactions
sample_X, sample_y = split_var_tar(sample_dataset(fraud_data=fraud, normal_data=normal, pct_fraud=50))",0.4414719641,
688,fraud detection algorithm for banks,"The airports seem most secured place as there are hardly any attacks happened.We can see most of the attacks are to the 
government personnel,buildings,political party meetings etc which states they are at the top of the hater list.Though 
many attacks also done to the education institutes but all the attacks are intended to either to buldings or instructors,
but not to the students.In transportation trains are the common victims and so the constructions in Business.",0.4333898723,
688,fraud detection algorithm for banks,data[data.BankcardUtilization.isna()].sample(5),0.4280068278,
688,fraud detection algorithm for banks,"if(False):
    #Build data (blobs for now)
    counter = 0
    hdbscan_min_samples =15 
    hdbscan_min_cluster_size = 10

    for rep in replications:
        for size in sizes:
            for dataset_dimension in dimensions:
                for dataset_n_clusters in clusters:
                    process_timer = timer()
                    data, labels = sklearn.datasets.make_blobs(n_samples=size, 
                                                               n_features=dataset_dimension, 
                                                               centers=dataset_n_clusters)
                    #Time hdbscan model
                    start = timer()
                    labels = hdbscan.HDBSCAN(min_samples=hdbscan_min_samples, 
                                                    min_cluster_size=hdbscan_min_cluster_size,
                                                    match_reference_implementation=False).fit_predict(data)
                    hdbscan_time = timer() - start

                    #Find best matching dbscan model
                    search = best_dbscan(data, labels)
                    params = search.x

                    #Time the training of the dbscan model
                    start = timer()
                    dbscan_labels = sklearn.cluster.DBSCAN(eps=params[0], min_samples= params[1]).fit_predict(data)
                    dbscan_time = timer() - start
                    results.ix[counter] = [size,dataset_dimension, dataset_n_clusters, rep, hdbscan_time, dbscan_time, search.fun,
                                           hdbscan_min_samples, hdbscan_min_cluster_size,
                                           params[0], params[1]]
                    print('size:', size, 'dim:', dataset_dimension, 'clust:', dataset_n_clusters, 'rep: ',rep,
                          'hdbTime:',round(hdbscan_time,3), 'dbTime:', round(dbscan_time,2), 'eps:', round(params[0],2), 
                          'min_samples:', params[1], 'rand:', round(-1*search.fun,2), round(timer() - process_timer, 2), 'sec')
                    counter += 1
                    results.to_csv(""optimizationResults.csv"")",0.4276294708,
688,fraud detection algorithm for banks,"plot_stats('mean', 'sub_grade', (random.randint, 1, 5), num_trials=20, use_cover=True, vancouver_steps=20)",0.4269914627,
688,fraud detection algorithm for banks,"ppv.visu_mean_behaviour(prediction_results, score_folders, day_indexes, first_snapshot, last_snapshot, ""mixed"", metric_id.upper(), img_dir, custom_palette)",0.4255408645,
688,fraud detection algorithm for banks,UCR.ViolentCrimesPerPop == 1,0.4254746437,
688,fraud detection algorithm for banks,S1.filter_bank(),0.4254102111,
328,counting words,"print 'shared words %d\neng1000 only %d\ncorpus only %d' % (len(set(eng1000.vocab).intersection(set(corpus_vocab))),
                                                            len(set(corpus_vocab) - set(eng1000.vocab)),
                                                            len(set(eng1000.vocab) - set(corpus_vocab)))",0.4650916755,
328,counting words,"selected_words = ['awesome', 'great', 'fantastic', 'amazing', 'love', 'horrible', 'bad', 'terrible', 'awful', 'wow', 'hate']

def wordCount(key, dictionary):
    if not key in dictionary:
        return 0;
    else:
        return dictionary[key];

for word in selected_words:
    products[word] = products['word_count'].apply(lambda x : wordCount(word, x));
    
for word in selected_words:
    print ""The sum of word %s is %s"" % (word, products[word].sum());",0.4646860361,
328,counting words,"a = 'Calvin called Suzy a Boogerbrain' # Create and instance of string class
print(a.upper())       #Method to convert the string to uppercase
print(a.count('in'))   #Method to count the number times a substring appears in a string
print(a.split())       #Method to split a string in a list of words",0.4601774514,
328,counting words,"#keep only legal words, and non stop-words (i.e. ""."" or ""&"")
#get counter of legal English
legalWordCounter = co.Counter(nltk.corpus.words.words())
stopWordsCounter = co.Counter(nltk.corpus.stopwords.words())
#filter narrativeWordList
filterSummaryWordList = [i for i in summaryWordList
                           if i in legalWordCounter and
                              i not in stopWordsCounter]
#counter for the legal words in our filtered list
filteredWordCounter = co.Counter(filterSummaryWordList)",0.4564747512,
328,counting words,"words = co.Counter(nltk.corpus.words.words())
stopWords =co.Counter( nltk.corpus.stopwords.words() )
x=[i for i in x if i in words and i not in stopWords]
string="" "".join(x)
c = co.Counter(x)",0.4552384019,
328,counting words,"# Next only want valid strings
words = co.Counter(nltk.corpus.words.words())
stopWords =co.Counter( nltk.corpus.stopwords.words() )
k=[i for i in k if i in words and i not in stopWords]
c = co.Counter(k)
printmd(""We see that we $"" + str(len(k)) + ""$ legal word tokens in our corpus. There are $"" + str(
        len(list(c.most_common())))
       + ""$ legal non-stopword types in our corpus."")",0.4548823833,
328,counting words,"def count_price(inp):
    return s.lower().count('price')",0.4520105124,
328,counting words,"def countTokens(tokenizedRDD):
    return tokenizedRDD.flatMap(lambda t:t[1]).distinct().count()

print ""ther are {} distinct tokens in the abt data set, and {} distinct tokens in the buy dataset. total: {}"".format(
    countTokens(abtTokenized), countTokens(buyTokenized), countTokens(abtTokenized)+countTokens(buyTokenized))",0.4515827894,
328,counting words,"# TODO: Replace <FILL IN> with appropriate code
def wordCount(wordListDF):
    """"""Creates a DataFrame with word counts.

    Args:
        wordListDF (DataFrame of str): A DataFrame consisting of one string column called 'word'.

    Returns:
        DataFrame of (str, int): A DataFrame containing 'word' and 'count' columns.
    """"""
    return wordListDF.groupBy(wordListDF.word).count()

wordCount(wordsDF).show()",0.4492772222,
328,counting words,"iterations = 0
sum = 0
while sum < 1000000:
    sum += vowel_count(sentence)
    iterations += 1

print iterations;",0.4471179545,
189,character classes can also accept certain ranges,"melody_instruments = range(88) + range(104, 112)",0.3466601968,
189,character classes can also accept certain ranges,"def categorize_age(age):
    """"""
    categorize age data into different categories, and each category as one life stages.
    Age ranges of life stages are based on the research of American Institute For Learning and Human Development.
    """"""
    life_stage = {
        (0, 11): ""Childhood"",
        (11, 20): ""Adolescence"",
        (20, 35): ""Early Adulthood"",
        (35, 51): ""Midlife"",
        (51, 81): ""Mature Adulthood"",
        (81, 100): ""Late Adulthood""
    }
    
    return [life_stage[i] for i in life_stage.keys() if (age >= i[0]) & (age < i[1])].pop()",0.3429500461,
189,character classes can also accept certain ranges,"def find_palindromes(seq, n):
    palindromes = []
    
    for _________________:
        
        
        if _________________:
            palindromes.append(___________)
            
            
            
            
    return palindromes

DNA_seq = 'GGAGCTCCCAAAGCCATCAATATTCATCAAAACGAATTCAACGGAGCTCGATATCGCATCGCAAAAGACACC'
palindromic_sequences = find_palindromes(DNA_seq,6)
assert palindromic_sequences == ['GAGCTC', 'AATATT', 'GAATTC', 'GAGCTC', 'GATATC']",0.3399085999,
189,character classes can also accept certain ranges,"def class_id_to_name(ids):
    mapping = {
        0: 'safe',
        1: 'texting right',
        2: 'phone right',
        3: 'texting left',
        4: 'phone left',
        5: 'radio',
        6: 'drinking',
        7: 'reach behind',
        8: 'hair/makeup',
        9: 'talk passenger'
    }
    return [mapping[id] for id in ids]",0.3395193815,
189,character classes can also accept certain ranges,"def cartesian(G):
    return [{'cartesianLayout': [
        {'node': n, 'x': float(G.pos[n][0]), 'y': float(G.pos[n][1])}
        for n in G.pos
        ]}]

def apply_spring_layout(network):
    my_networkx = network.to_networkx()
    my_networkx.pos = nx.drawing.spring_layout(my_networkx)
    #my_networkx.pos = nx.drawing.circular_layout(my_networkx)
    cartesian_aspect = cartesian(my_networkx)
    network.set_opaque_aspect(""cartesianCoordinates"", cartesian_aspect)",0.3370267749,
189,character classes can also accept certain ranges,"# Interaction Network Fidelity
> Interaction Network Fidelity is defined by ***RNA. 2009 Oct; 15(10): 18751885.***
* It checks the percentage of interactions that are found in the predicted structure. 
* The interaction types are assigned according to [MC-annotate](http://www-lbit.iro.umontreal.ca/mcannotate-simple/). 
* The interaction types can be sorted to: All interactions (INF_ALL), Watson-Crick interactin (INF_WC), non-Watson-Crick interaction 
   (INF_NWC) and Stacking (INF_STACK). 
* Deformation Index is defined as 
* This function returns a tuple of the metrices [RMSD, Deformation Index, INF_ALL, INF_WC, INF_NWC,INF_STACK]",0.3364146948,
189,character classes can also accept certain ranges,"def annotate_example(doc):
    return (doc.text, {'entities': [(e.start_char, e.end_char, e.label_, e.text) for e in doc.ents]})

unclean_train = [annotate_example(nlp(unicode(open('/home/datascience/stones/examples/'+f).read()))) for f in ['sun_one.txt', 'oracle_one.txt', 'oracle2_one.txt', 'oracle3_one.txt']]
unclean_train",0.3350564837,
189,character classes can also accept certain ranges,"def cartesian(G):
    return [
        {'node': n, 'x': float(G.pos[n][0]), 'y': float(G.pos[n][1])}
        for n in G.pos
        ]

cartesian_aspect_elements = cartesian(my_networkx)
alk1_signaling_events.set_opaque_aspect(""cartesianLayout"", cartesian_aspect_elements)
print(json.dumps(alk1_signaling_events.get_opaque_aspect(""cartesianLayout""), indent=4))",0.3315652013,
189,character classes can also accept certain ranges,"# counts
def gene_count(df):
    x, y = df.Gene, df.Text
    return len(re.findall(x,y))

def var_count(df):
    x, y = df.Variation, df.Text
    return len(re.findall(x,y)) 

def gene_count_mu(df):
    x, y = df.Gene, df.Text
    x = x + "" mutation""
    return len(re.findall(x, y))

# new text bag
def gene_context(df): # extract context starts with Gene
    #re_word = "".{5}"" + ""(?i)"" + word + "" .{100}""   # extract by length
    re_word = "".{5}"" + ""(?i)"" + df.Gene + "".*?[\.;!?] "" # extract by the 
    context = re.findall(re_word, df.Text,  re.MULTILINE | re.DOTALL )
    return '|'.join(context)

def var_context(df): # extract context starts with Variations
    re_word = "".{5}"" + ""(?i)"" + df.Variation + "".*?[\.;!?] "" # extract by the 
    context = re.findall(re_word, df.Text,  re.MULTILINE | re.DOTALL )
    return '|'.join(context)

def word_context(word, df): # input any word interested to extract context
    re_word = "".{50}"" + ""(?i)"" + word + "".*?[\.;!?] "" # extract by the 
    context = re.findall(re_word, df.Text,  re.MULTILINE | re.DOTALL )
    return '|'.join(context)",0.3315418363,
189,character classes can also accept certain ranges,"from textblob import TextBlob

def lemmatize(message):
    message = unicode(message, 'utf8').lower()
    return [word.lemma for word in TextBlob(message).words]",0.3245758414,
1960,spark dataframes,"training = spark.sparkContext.parallelize([[""hello world""], [""this is some more text""], [""and here another sentence""]]).toDF().toDF(""text"")
training.show()",0.5735881925,
1960,spark dataframes,"dummy_data = spark.sparkContext.parallelize([[""I am happy and like this spark NLP""], [""Have to say something bad now""]]).toDF().toDF(""text"")
dummy_data.show()",0.5712897778,
1960,spark dataframes,"def real_main():
    sc = pyspark.SparkContext()
    company = _init_list(sc)
    dataRDD1 = sc.textFile(""gs://group688/nytimes"",5)
    dataRDD1 = dataRDD1.mapPartitions(lambda x:_data_filter(x,company,""nytimes""))
    dataRDD2 = sc.textFile(""gs://group688/wsj"",10)
    dataRDD2 = dataRDD2.mapPartitions(lambda x:_data_filter(x,company,""wsj""))
    dataRDD3 = sc.textFile(""gs://group688/reuters.dat"",10)
    dataRDD3 = dataRDD3.mapPartitions(lambda x:_data_filter(x,company,""reuters""))
    dataRDD  = dataRDD3.union(dataRDD2).union(dataRDD1)
    dataRDD.sortByKey().map(lambda x:x[1]).saveAsTextFile(""gs://group688/688v1"")

real_main()",0.5679394603,
1960,spark dataframes,"# Initial imports

import pyspark
from pyspark.sql import SparkSession
from pyrasterframes import *
from pyrasterframes.rasterfunctions import *

# Add other configuration options as needed

spark = SparkSession.builder. \
    master(""local[*]""). \
    appName(""RasterFrames""). \
    config(""spark.ui.enabled"", ""false""). \
    getOrCreate(). \
    withRasterFrames()",0.5657897592,
1960,spark dataframes,"import pyspark
from pyspark.sql import SparkSession
from pyrasterframes import *
from pyrasterframes.rasterfunctions import *

# Add other configuration options as needed

spark = SparkSession.builder. \
    master(""local[*]""). \
    appName(""RasterFrames""). \
    config(""spark.ui.enabled"", ""false""). \
    getOrCreate().\
    withRasterFrames()",0.5657897592,
1960,spark dataframes,"# Initial imports

import pyspark
from pyspark.sql import SparkSession
from pyrasterframes import *
from pyrasterframes.rasterfunctions import *

import matplotlib
import matplotlib.pyplot as plt
import numpy as np

# Add other configuration options as needed

spark = SparkSession.builder. \
    master(""local[*]""). \
    appName(""RasterFrames""). \
    config(""spark.ui.enabled"", ""false""). \
    getOrCreate(). \
    withRasterFrames()
    
bandnums = list(range(1,5))",0.5656034946,
1960,spark dataframes,"from pyrasterframes import *
from pyrasterframes.rasterfunctions import *
import pyspark
from pyspark.sql import SparkSession
from pathlib import Path


spark = SparkSession.builder. \
    master(""local[*]""). \
    appName(""RasterFrames""). \
    config(""spark.ui.enabled"", ""false""). \
    getOrCreate(). \
    withRasterFrames()",0.5613589883,
1960,spark dataframes,"from pyspark.sql import SparkSession
from pyspark.ml import feature, regression, evaluation, Pipeline
from pyspark.sql import functions as fn, Row
import matplotlib.pyplot as plt
spark = SparkSession.builder.getOrCreate()
sc = spark.sparkContext",0.5584433079,
1960,spark dataframes,"from pyrasterframes import *
from pyrasterframes.rasterfunctions import *
import pyspark
from pyspark.sql import SparkSession
from pathlib import Path

spark = SparkSession.builder. \
    master(""local[*]""). \
    appName(""RasterFrames""). \
    config(""spark.ui.enabled"", ""false""). \
    getOrCreate(). \
    withRasterFrames()

sc = spark.sparkContext

resource_dir = Path('../samples').resolve()
filenamePattern = ""L8-B{}-Elkton-VA.tiff""
def readTiff(name):
    return spark.read.geotiff(resource_dir.joinpath(filenamePattern.format(name)).as_uri())",0.5573180318,
1960,spark dataframes,"from pyrasterframes import *
from pyrasterframes.rasterfunctions import *
import pyspark
from pyspark.sql import SparkSession
from pathlib import Path

spark = SparkSession.builder. \
    master(""local[*]""). \
    appName(""RasterFrames""). \
    config(""spark.ui.enabled"", ""false""). \
    getOrCreate(). \
    withRasterFrames()
    
resource_dir = Path('./samples').resolve()

filenamePattern = ""L8-B{}-Elkton-VA.tiff""

def readTiff(name):
    return spark.read.geotiff(resource_dir.joinpath(filenamePattern.format(name)).as_uri())

bandNumbers = range(1, 5)
bandColNames = ['band_{}'.format(n) for n in bandNumbers]",0.5564510822,
548,ex groupby,"def check_age(column,column2,column3):
    result = training[[column, column2,column3, 'Age']].groupby([column,column2,column3], as_index=False).agg({'Age':['mean','median','count']})\
    .sort_values(by=[column], ascending=False)
    return result
check_age('Title','Pclass','Sex')",0.46887514,
548,ex groupby,"def un_one_hot(data, regex):
    data_filter = data.filter(regex=regex, axis=1)
    return data_filter.idxmax(1).rename(regex)

cov_reconstructed = pd.concat([df_cov.drop(df_cov.filter(regex='Wilderness_Area|Soil_Type'), axis=1), un_one_hot(df_cov, 'Soil_Type*').rename('Soil_Type'), un_one_hot(df_cov, 'Wilderness_Area*').rename('Wilderness_Area')], axis=1)

cov_reconstructed.info()",0.4679205418,
548,ex groupby,"# group by the PIN key, which would be duplicated for each station and take the minimum
def get_min_rows(df, grpby, aggcol):
    min_values = df.groupby(grpby)[aggcol].transform(min)
    return df[df[aggcol] == min_values] 

min_parcels = get_min_rows(result, 'PIN', 'dist_to_station')
min_parcels.shape",0.461289525,
548,ex groupby,"def plot_bar_chart(df,groupby_var,group,title,axis,xorder=None):
    train_group = df.groupby(groupby_var).get_group(group)
    total_group_applicants = train_group.shape[0]
    if xorder == None:
        plot = (train_group[""Loan_Status""].value_counts()*100/total_group_applicants).plot(kind=""bar"",ax=axis)
    else:
        plot = ((train_group[""Loan_Status""].value_counts()*100/total_group_applicants)).ix[xorder].plot(kind=""bar"",ax=axis,x=xorder)
    
    plot.set_xlabel(""Loan_Status"")
    plot.set_ylabel(""Percentage"")
    plot.set_ylim([0,80])
    plot.set_title(title)",0.4573974609,
548,ex groupby,"def summarize_by_dept(df,col_of_interest,dept_col):
    """"""
    Args:    Dataframe to summarize, name of column that is of interest (e.g., 'Salary' or 'Years Served'),
             and name of department column to group by
    Returns: Table with summary statistics by department (count, median, and standard deviation)
    """"""
    cat_group = salary[[col_of_interest,dept_col]].groupby(dept_col)
    cat_med = cat_group.median().round(1)
    cat_cnt = cat_group.count()
    cat_sd = cat_group.std().round(1)
    cat_sum = pd.concat([cat_cnt,cat_med,cat_sd],axis=1)
    cat_sum.columns = ['Count','Median','Standard Deviation']
    cat_sum = cat_sum.sort_values(['Median'],ascending=False).transpose()
    return cat_sum",0.4560731053,
548,ex groupby,"def max_score_batsman_match_inTeam(match_id, team):
    x = deliveries.groupby(['match_id','batting_team', 'batsman'])['batsman_runs'].sum()
    name = x[match_id][team].idxmax()
    runs = x[match_id][team].max()
    return [name, runs]",0.4511607885,
548,ex groupby,"def max_score_batsman_match_inTeam(match_id, team):
    x = deliveries.groupby(['match_id','batting_team', 'batsman'])['batsman_runs'].sum()
    name = x[match_id][team].idxmax()
    runs = x[match_id][team].max()
    return name, team, runs, match_id",0.4511607885,
548,ex groupby,"## HELPER FUNCITON TO RETURN RELEVANT COORDINATES TO PLOT AND A HISTOGRAM TO DECIDE IF
# THRESHOLF USED IS RELEVANT
def get_pre_process_locs_toplot(month_df,criteria_list,thresh,count_tag):
    all_coords_df = month_df[criteria_list]
    
    grp_coords = all_coords_df.groupby(criteria_list)

    grp_coords = grp_coords.aggregate(len)
    uniq_coords = grp_coords.reset_index().rename(columns={0: count_tag})
    uniq_coords = uniq_coords[uniq_coords[count_tag] > thresh]
   # hist_plot = uniq_coords[count_tag].hist()
    uniq_coords_lst = uniq_coords.values.tolist()
    #return uniq_coords_lst,hist_plot
    return uniq_coords_lst",0.4511042833,
548,ex groupby,"def runs_batsman_match(batsman_name, match_id):
    x = deliveries.groupby(['match_id', 'batsman'])['batsman_runs'].sum()
    runs = x[match_id][batsman_name]
    return batsman_name, runs, match_id",0.4498216808,
548,ex groupby,"def runs_batsman_match(batsman_name, match_id):
    x = deliveries.groupby(['match_id', 'batsman'])['batsman_runs'].sum()
    runs = x[match_id][batsman_name]
    return runs",0.4498216808,
1754,read numbers until and collect them in a list print the elements backwards zero not included,"j = 0
for i in result:
    print( i )
    j = j + 1
    if ( j == 10 ):
        break",0.4974632263,
1754,read numbers until and collect them in a list print the elements backwards zero not included,"%%julia
begin
    result = 0
    for i = 3:9
        result += i
    end
    println(""Julia says the result is $result !"")
end",0.4954984188,
1754,read numbers until and collect them in a list print the elements backwards zero not included,"min_length = 2

while True:
    name = input('Please enter your name:')
    if len(name) >= min_length  and name.isprintable() and name.isalpha():
        break

print('Hello, {0}'.format(name))",0.4943700433,
1754,read numbers until and collect them in a list print the elements backwards zero not included,"for i in range(len(heros)):
    if i+1 >= len(heros):
        break
    print(heros[i], ' vs ', heros[i+1])",0.4914771318,
1754,read numbers until and collect them in a list print the elements backwards zero not included,"for i in range(0, len(classifiers)):
    clas = classifiers[i]
    fignumber = i + 1
    plot_svm_kernels(fignumber, clas[2], ""rbf"", clas[3], clas[4], clas[0], clas[1])
plt.show()",0.4906194806,
1754,read numbers until and collect them in a list print the elements backwards zero not included,"for i, variable in enumerate(variable_list):
    X_train = training_set[variable_list[:i+1]]
    Y_train = training_set['CO(GT)'].as_matrix().ravel()
    X_test = testing_set[variable_list[:i+1]]
    Y_test = testing_set['CO(GT)'].as_matrix().ravel()
    clf = Ridge(alpha=1)
    clf.fit(X_train, Y_train)
    print('train MAE\t', mean_absolute_error(Y_train, clf.predict(X_train)), '\t; test MAE\t', mean_absolute_error(Y_test, clf.predict(X_test)))",0.489503324,
1754,read numbers until and collect them in a list print the elements backwards zero not included,"a, b, n = 0, 1, 0
while n < 10:
    print(a, end = "" "")
    a, b = b, a + b
    n += 1",0.4889627695,
1754,read numbers until and collect them in a list print the elements backwards zero not included,"for i, c in enumerate(com_list):
    if len(c) < 10:
        continue
    filename = './data_files/community_updated' + str((i+1)) +'.txt'
    with(open(filename, 'w')) as f:
        for screen_name in c:
            f.write(screen_name +'\n')
        f.close()",0.488391012,
1754,read numbers until and collect them in a list print the elements backwards zero not included,"#print first ten lines in the bed file to see what the data looks like:
i = 0
for line in open('test.bed', 'r'):
    print(line[:-1])
    i += 1
    if i == 10:
        break",0.4882081747,
1754,read numbers until and collect them in a list print the elements backwards zero not included,"nhead = 0
for line in open(filename):
    if '#CHROM'==line[:6]:
        break
    nhead += 1",0.4874458909,
2334,the data bike sharing,"def load_boston():
    scaler = StandardScaler()
    boston = datasets.load_boston()
    X=boston.data
    y=boston.target
    X = scaler.fit_transform(X)
    return train_test_split(X,y)",0.4578280449,
2334,the data bike sharing,"def load_boston():
    scaler = StandardScaler()
    boston = datasets.load_boston()
    X=boston.data
    y=boston.target
    X = scaler.fit_transform(X) # This transforms the data about zero (by subtracting the mean) and scales by the std.dev.
    return train_test_split(X,y) #Splits into 3:1 training:test sets",0.4578280449,
2334,the data bike sharing,"def load_boston():
    scaler = StandardScaler()
    boston = datasets.load_boston()
    X=boston.data
    y=boston.target
    X = scaler.fit_transform(X)
    return train_test_split(X,y)#,random_state=42)",0.4578280449,
2334,the data bike sharing,"#imports cali housing cost data sets
def data_retrieve():
    if not os.path.isdir(HOUSING_PATH):
        os.makedirs(HOUSING_PATH)
    tgz_path = os.path.join(HOUSING_PATH, ""housing.tgz"")
    request.urlretrieve(HOUSING_URL, tgz_path)
    housing_tgz = tarfile.open(tgz_path)
    housing_tgz.extractall(path=HOUSING_PATH)
    housing_tgz.close()

data_retrieve()
    
#initializes master dataframe
housing_df = pd.read_csv(os.path.join(HOUSING_PATH, ""housing.csv""))",0.455078125,
2334,the data bike sharing,"def get_replay_sites():
    """"""Loads a list of session replay providers from the Princeton WebTAP project,
    which listed sites within the Alexa top 10,000 that show signs of session replay scripts.
    """"""
    sr_csv_raw = requests.get(""https://raw.githubusercontent.com/mozilla/UCOSP-winter-2018_TrackingTechnologies/master/data/sr_site_list.csv"")
    sr_csv = pd.read_csv(StringIO(sr_csv_raw.text))
    return list(sr_csv.third_party.unique())",0.4522116184,
2334,the data bike sharing,"def sp500_data():
    df = pd.DataFrame()
    df = quandl.get('YAHOO/INDEX_GSPC', trim_start='1975-01-01', authtoken=api_key, collapse='monthly')
    df['Adjusted Close'] = (df['Adjusted Close'] - df['Adjusted Close'][0]) / df['Adjusted Close'][0] * 100.00
    df.rename(columns={'Adjusted Close':'sp500'}, inplace=True)
    df = df['sp500']
    return df",0.4497467577,
2334,the data bike sharing,"#Loading pickle
def load_pickle():
    dist_pickle = pickle.load( open('data/calibration_pickle.p', 'rb'))
    mtx = dist_pickle['mtx']
    dist = dist_pickle['dist']
    return mtx, dist",0.4473881125,
2334,the data bike sharing,"def tweet_response():
    weather = {}
    weather_response = requests.get('https://twitter.com/marswxreport?lang=en')
    soup3 = bs(weather_response.text, 'html.parser')

    weather_result = soup3.find(class_=""content"")

    weather['today'] = weather_result.find('p', class_=""TweetTextSize"").text

    return(weather)

tweet_response()",0.4472392797,
2334,the data bike sharing,"def slow_function():
    d = cis.read_data(""../resources/WorkshopData2016/od550aer.nc"",
                      ""od550aer"")
    global_means = []
    for t in d.slices_over('time'):
        global_means.append(t.data.mean())
    return global_means",0.4463953972,
2334,the data bike sharing,"def add_features_to_side():
    FEATURES = pd.DataFrame(temp)
    merge = pd.read_csv(PATH + ""Expanding and Rolling Result\\Correlated_merged_full.csv"" , skipinitialspace = True, encoding = ""ISO-8859-1"")
    merge = pd.concat([FEATURES,merge], axis =1)
    return merge",0.44420892,
1327,optional exercises missing pois,"import pandas
'''accepts a PANDAS DataFrame, creates a new column EXITSn_hourly assinging a value of 
the difference between the current and previous row,  #or 1 for any value = NaN, then returns a DataFrame'''

def get_hourly_exits(df): 
    '''
    The data in the MTA Subway Turnstile data reports on the cumulative
    number of entries and exits per row.  Assume that you have a dataframe
    called df that contains only the rows for a particular turnstile machine
    (i.e., unique SCP, C/A, and UNIT).  This function should change
    these cumulative exit numbers to a count of exits since the last reading
    (i.e., exits since the last row in the dataframe).
    
    More specifically, you want to do two things:
       1) Create a new column called EXITSn_hourly
       2) Assign to the column the difference between EXITSn of the current row 
          and the previous row. If there is any NaN, fill/replace it with 0.
    
    You may find the pandas functions shift() and fillna() to be helpful in this exercise.
    
    Example dataframe below:

          Unnamed: 0   C/A  UNIT       SCP     DATEn     TIMEn    DESCn  ENTRIESn    EXITSn  ENTRIESn_hourly  EXITSn_hourly
    0              0  A002  R051  02-00-00  05-01-11  00:00:00  REGULAR   3144312   1088151                0              0
    1              1  A002  R051  02-00-00  05-01-11  04:00:00  REGULAR   3144335   1088159               23              8
    2              2  A002  R051  02-00-00  05-01-11  08:00:00  REGULAR   3144353   1088177               18             18
    3              3  A002  R051  02-00-00  05-01-11  12:00:00  REGULAR   3144424   1088231               71             54
    4              4  A002  R051  02-00-00  05-01-11  16:00:00  REGULAR   3144594   1088275              170             44
    5              5  A002  R051  02-00-00  05-01-11  20:00:00  REGULAR   3144808   1088317              214             42
    6              6  A002  R051  02-00-00  05-02-11  00:00:00  REGULAR   3144895   1088328               87             11
    7              7  A002  R051  02-00-00  05-02-11  04:00:00  REGULAR   3144905   1088331               10              3
    8              8  A002  R051  02-00-00  05-02-11  08:00:00  REGULAR   3144941   1088420               36             89
    9              9  A002  R051  02-00-00  05-02-11  12:00:00  REGULAR   3145094   1088753              153            333
    '''
    
    df['EXITSn_hourly'] = df.EXITSn.diff(1)
    df.EXITSn_hourly.fillna(0,inplace = True)
    return df",0.3868287802,
1327,optional exercises missing pois,"def get_ratios(schedule):
    women_work = [a.work for a in schedule if a.gender == ""female""]
    men_work = [a.work for a in schedule if a.gender == ""male""]
    ratio_of_women_in_STEM = len([woman for woman in women_work if woman > 0])/len(women_work)
    ratio_of_men_in_STEM = len([men for men in men_work if men > 0])/len(men_work)
    return ratio_of_women_in_STEM, ratio_of_men_in_STEM",0.3857828975,
1327,optional exercises missing pois,"def recording_list(work_mbid):
    recordings = sql(""""""
SELECT a.name AS artist,
       to_date(to_char(l.begin_date_year, '9999') ||
               to_char(COALESCE(l.end_date_month, 1), '99') ||
               to_char(COALESCE(l.end_date_day, 1), '99'), 'YYYY MM DD') AS start,
       to_date(to_char(l.end_date_year, '9999') ||
               to_char(COALESCE(l.end_date_month, 1), '99') ||
               to_char(COALESCE(l.end_date_day, 1), '99'), 'YYYY MM DD') AS end,
       r.gid AS mbid,
       r.length * interval '1ms' AS duration,
       p.name AS place,
       p.coordinates
  FROM work               AS w
  JOIN l_recording_work   AS lrw ON w.id = lrw.entity1
  JOIN recording          AS r   ON r.id = lrw.entity0
LEFT OUTER JOIN l_place_recording  AS lpr ON r.id = lpr.entity1
LEFT OUTER JOIN place              AS p   ON p.id = lpr.entity0
LEFT OUTER JOIN l_artist_recording AS lar ON r.id = lar.entity1
LEFT OUTER JOIN artist             AS a   ON a.id = lar.entity0
LEFT OUTER JOIN link               AS l   ON l.id = lar.link
 WHERE w.gid = %(work_mbid)s
ORDER BY start;
"""""", work_mbid=work_mbid)

    recordings.duration = recordings.duration.fillna(0).apply(lambda t: t.round('s'))
    recordings['seconds'] = recordings.duration.apply(lambda s: s.total_seconds())
    recordings.duration = recordings.duration.apply(lambda s: str(s).replace('0 days 00:', ''))
    recordings['url'] = recordings.mbid.apply(mb_recording_link)
    recordings.drop('mbid', axis=1, inplace=True)
    return recordings
    
recordings = recording_list(work_mbid)
print('Number of recordings: %d' % len(recordings))
print('First recordings (chronological order): ')

iplot(ff.create_table(recordings[['start', 'artist', 'duration', 'url']].head(10)))",0.3847661018,
1327,optional exercises missing pois,"def plot_obs_att_errs(obsid):
    manvr = events.manvrs.filter(obsid=obsid)[0]
    dwell = events.dwells.filter(obsid=obsid)[0]
    obs_att_data = fetch.Msidset(att_msids, manvr.kalman_start, dwell.stop)
    figure(figsize=(15, 3))
    for i, msid, label, msidrange in izip(count(1), att_msids, axis_label, plotrange):
        subplot(1, 3, i)
        plot_cxctime(obs_att_data[msid].times, 3600 * np.degrees(obs_att_data[msid].vals), 'b,')
        title(""obsid {} {} ({})"".format(obsid, msid, label))
        ylabel(""arcsec"")
        ylim(msidrange)
        margins(x=.1)
        if ((3600 * np.degrees(np.min(obs_att_data[msid].vals)) < msidrange[0])
            or (3600 * np.degrees(np.max(obs_att_data[msid].vals)) > msidrange[1])):
            raise ValueError(""Range does not contain all data"")

def plot_obs_err_hist(obsid):
    manvr = events.manvrs.filter(obsid=obsid)[0]
    dwell = events.dwells.filter(obsid=obsid)[0]
    obs_att_data = fetch.Msidset(att_msids,
                                 DateTime(manvr.kalman_start).secs + 60,
                                 DateTime(dwell.stop).secs)
    figure(figsize=(15, 3))
    for i, msid, label, msidbin in izip(count(1), att_msids, axis_label, bins):
        subplot(1, 3, i)
        hist(3600 * np.degrees(obs_att_data[msid].vals), bins=msidbin, log=True)
        title(""obsid {} {} ({})"".format(obsid, msid, label))
        xlabel(""arcsec"")",0.3842440248,
1327,optional exercises missing pois,"def evaluate_season(model, season):
    """"""
    Return score and model predictions for given season
    """"""
    
    yi, Xs, y = udata.get_seasons_data(ad, components, [season], TARGET, REGION)
    predictions = np.zeros_like(Xs[0])

    for i in range(len(yi)):
        # HACK: Check if this is an oracle
        # This should ideally go in as a flag in the model
        if ""truth"" in inspect.signature(model.predict).parameters:
            # This is an oracle
            predictions[i, :] = model.predict(yi.iloc[i], [X[i] for X in Xs], y[i])
        else:
            predictions[i, :] = model.predict(yi.iloc[i], [X[i] for X in Xs]) 
        # Pass in feedback if model accepts it
        try:
            model.feedback(y[i])
        except NotImplementedError:
            pass

    score = np.log(udists.prediction_probabilities([predictions], y, TARGET)).mean()
    return score, predictions

def cross_validate(model, post_training_hook=None):
    """"""
    Run cross validation and return mean score
    """"""
    
    cv_scores = []
    train_scores = []
    for season in tqdm(TRAINING_SEASONS):
        train_seasons = [s for s in TRAINING_SEASONS if s != season]
        yi, Xs, y = udata.get_seasons_data(ad, components, train_seasons, TARGET, REGION)
        model.fit(yi, Xs, y)
            
        for ts in train_seasons:
            # Need to reset the model before every evaluation
            # TODO: This should be done even when not using a hook
            if post_training_hook:
                model = post_training_hook(model)
            score, _= evaluate_season(model, ts)
            train_scores.append(score)
        
        if post_training_hook:
            model = post_training_hook(model)
        score, _ = evaluate_season(model, season)
        cv_scores.append(score)

    return np.mean(train_scores), np.mean(cv_scores)

def train_and_save(model):
    """"""
    Train the model on all data and save 
    """"""
    
    yi, Xs, y = udata.get_seasons_data(ad, components, TRAINING_SEASONS, TARGET, REGION)
    model.fit(yi, Xs, y)
    model_file_name = f""{stringcase.spinalcase(type(model).__name__)}.json""
    model.save(path.join(OUTPUT_DIR, TARGET, model_file_name))",0.3830803037,
1327,optional exercises missing pois,"def replace_precip_nan(df):
    """"""Replace all NaN values in the PRECIP_MIN and PRECIP_MAX columns with zeroes.""""""
    df['PRECIP_MIN'] = df['PRECIP_MIN'].fillna(0.0)
    df['PRECIP_MAX'] = df['PRECIP_MAX'].fillna(0.0)",0.3813413382,
1327,optional exercises missing pois,"def plot_retweets_per_tweet_by_day():
    q = tweets_clean.retweets.quantile([0.05, 0.95])

    g2_subset = tweets_clean
    g2_subset = g2_subset.drop(tweets_clean[tweets_clean['retweets'] < q[0.05]].index)
    g2_subset = g2_subset.drop(tweets_clean[tweets_clean['retweets'] > q[0.95]].index)

    fig, ax = plt.subplots(1, 1, figsize=(16, 9))

    ax.spines['top'].set_visible(False)
    ax.spines['bottom'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_visible(False)

    x = g2_subset['timestamp'].dt.dayofyear + \
        (g2_subset['timestamp'].dt.year-2015)*365-319
    y = g2_subset['retweets']

    plt.scatter(x, y);

    z = np.polyfit(x, y, 1)
    p = np.poly1d(z)
    plt.plot(x,p(x),""r--"")

    label = ""retweets = %.0f * days + %.0f""%(z[0],z[1])

    plt.text(630, 5220, label, fontsize=18, color='red')
    plt.xticks(fontsize=18)
    plt.yticks(fontsize=18)
    plt.xlabel('Days Since First Tweet', fontsize=18)
    plt.ylabel('Retweets per Tweet', fontsize=18)
    # fig.suptitle('Retweets by Days Since First Tweet', fontsize=24, ha='center');",0.3805683553,
1327,optional exercises missing pois,"my_layer.update(data=data, purge_all=True, fly_to=[48.,-12.,6.0e11,0.,0.])",0.3776115775,
1327,optional exercises missing pois,"# Write a function that will categorize project Goals in to quartiles

def get_goal_quartile(i):
    g0 = kick['usd_goal_real'].quantile(q=.0)
    g25 = kick['usd_goal_real'].quantile(q=.25)
    g50 = kick['usd_goal_real'].quantile(q=.50)
    g75 = kick['usd_goal_real'].quantile(q=.75)
    g100 = kick['usd_goal_real'].quantile(q=1)
    
    gq=[]
    for gval in i:
        if gval >= g0 and gval < g25:
            gq.append('0-25th percentile')
        elif gval >= g25 and gval < g50:
            gq.append('25-50th percentile')
        elif gval >= g50 and gval < g75:
            gq.append('50-75th percentile')
        elif gval >= g75 and gval < g100:
            gq.append('75-99th percentile')
        else:
            gq.append('PROBLEM')
    return gq
    
    
kick['goal_quartile'] = get_goal_quartile(kick['usd_goal_real'])",0.3773860335,
1327,optional exercises missing pois,"# Write a function that will categorize project Backers in to quartiles

def get_backers_quartile(i):
    b0 = kick['backers'].quantile(q=.0)
    b25 = kick['backers'].quantile(q=.25)
    b50 = kick['backers'].quantile(q=.50)
    b75 = kick['backers'].quantile(q=.75)
    b100 = kick['backers'].quantile(q=1)
    
    bq=[]
    for val in i:
        if bval >= b0 and pval < b25:
           bq.append('0-25th percentile')
        elif bval >= b25 and pval < b50:
            bq.append('25-50th percentile')
        elif bval >= b50 and pval < b75:
            bq.append('50-75th percentile')
        elif bval >= b75 and pval < b100:
            bq.append('75-99th percentile')
        else:
            bq.append('PROBLEM')
    return bq
    
    
kick['backers_quartile'] = get_goal_quartile(kick['backers'])",0.3773860335,
1517,plot the error and execution time,"plt.semilogy(range(len(train_err)), train_err, label='train')
    plt.semilogy(range(len(val_err)), val_err, label='val')
    plt.title('error')
    plt.legend(loc=3)
    plt.xlabel('epoch')
    plt.ylabel('Mean squared error')
    plt.ylim(1e-4, 1)
    plt.xlim(0, 700)
    print",0.6080192327,
1517,plot the error and execution time,"def plot_larger_error(model):
    plt.plot(range(len(model.error_)), model.error_)
    plt.ylim([0, 1000])
    plt.ylabel('Error')
    plt.xlabel('Epochs')
    plt.show()
    
plot_larger_error(nn)",0.6026996374,
1517,plot the error and execution time,"# Define a plotting function to allow easy tight plot
def pd_errorbar(ax, df, name_params,fontsize=8):
    # Plot the information related to the training score
    ax.errorbar(df.loc[:, 'subsampling'], df.loc[:, 'avg_score_training'],
                yerr=df.loc[:, 'std_score_training'], label='training', marker='o')
    ax.errorbar(df.loc[:, 'subsampling'], df.loc[:, 'avg_score_testing'],
                yerr=df.loc[:, 'std_score_testing'], label='testing', marker='o')
    #df.plot.barh(ax=ax, stacked=True, legend=False)
    ax.set_xscale('log')
    ax.set_xlabel('Fitting time', fontsize=fontsize)
    ax.set_ylabel('Score', fontsize=fontsize)
    ax.set_title('{:.2E} S - {} E - {} D'.format(*name_params),
                fontsize=fontsize)
    ax.tick_params(axis='x', labelsize=fontsize)
    ax.tick_params(axis='y', labelsize=fontsize)
    #ax.xaxis.get_major_formatter().set_powerlimits((0, 1))
    ax.legend(loc=2)
    patches, labels = ax.get_legend_handles_labels()
    return patches, labels",0.5896093249,
1517,plot the error and execution time,"# def plot_stuff_nodf(model):
#     # diagnostic plots
#     pm.traceplot(model.trace, varnames=model.df_params);
#     pm.autocorrplot(model.trace, varnames=model.df_params);

#     # pairplot
#     trace_df = pm.trace_to_dataframe(model.trace, varnames=model.df_params)
#     sns.pairplot(trace_df);

def inspect_model(model):
    
    print(""median log loss: "", np.median(model.metrics['log_loss']))

    # diagnostic plots
    pm.traceplot(model.trace, varnames=model.df_params);
    pm.autocorrplot(model.trace, varnames=model.df_params);

    # pairplot
    trace_df = pm.trace_to_dataframe(model.trace, varnames=model.df_params)
    sns.pairplot(trace_df);",0.587479949,
1517,plot the error and execution time,"def plot_stuff(model):
    
    print(""median log loss: "", np.median(model.metrics['log_loss']))

    # diagnostic plots
    pm.traceplot(model.trace, varnames=model.df_params);
    pm.autocorrplot(model.trace, varnames=model.df_params);

    # plot discount function + data
    model.plot(data)

    # pairplot
    trace_df = pm.trace_to_dataframe(model.trace, varnames=model.df_params)
    sns.pairplot(trace_df);",0.5866547227,
1517,plot the error and execution time,"if (make_model):
    helper.plot_errors({""training"": training_errors, ""test"": test_errors}, title=""Simulation Learning Curve"")
                
    # clean-up some variables we no longer need to reduce our memory footprint...
    # otherwise our Azure Notebook might run out of memory
    del validation_input_images
    del validation_input_image_files
    del validation_target_mask_files
    del validation_target_masks
    del trainer
    del training_errors
    del test_errors
    print(""Garbage collection reclaimed {} objects"".format(gc.collect()))
    plt.close()",0.5842314363,
1517,plot the error and execution time,"def plot_example_errors(cls_pred, correct):
    ut.plot_example_errors(cls_pred, correct, images_test, cls_test, plot_images, images)",0.5827257633,
1517,plot the error and execution time,"#used for plotting radvel plots
def plot_results(like):
    fig = pl.figure(figsize=(12,4))
    fig = pl.gcf()
    pl.errorbar(
        like.x, like.model(t)+like.residuals(), 
        yerr=like.yerr, fmt='o'
        )
    pl.plot(ti, like.model(ti))
    fig.set_tight_layout(True)
    pl.xlabel('Time')
    pl.ylabel('RV')
    pl.draw()",0.5776899457,
1517,plot the error and execution time,"def bla():
    for q in per_ch_means:
        plt.plot(ts, q, alpha=0.1, c='k', linestyle='steps-mid')
    plt.errorbar(ts, weighted_mean, 
                 yerr=err_on_w_mean,
                 linestyle='steps-mid', label='Weighted mean', linewidth=1, c='r')
    plt.plot(ts, mean_template, linestyle='steps-mid', label='Unweighted mean', linewidth=1)
    
inset_plot(bla)
finish('reweighting_comparison')",0.5776705742,
1517,plot the error and execution time,"def plot_stop(pps):
    n_misc = pps.checkMisclassified(X, y)
    # print info.
    print '---- Ran over '+str(pps.stop_)+' data, stop at id '+str(pps.stop_-1)+' ----'
    print 'weight :'+str(pps.w_)
    print 'misclass (online): '+str(pps.error_)
    print 'misclass (test)  : '+str(n_misc)
    # Plotting
    plt.figure(figsize=(8, 6))
    plot_decision_regions( X, y, classifier=pps )
    plt.xlabel('sepal length [cm]', fontsize=18)
    plt.ylabel('petal length [cm]', fontsize=18)
    plt.legend(loc='upper left', title='Ran # of data: '+str(pps.stop_+1), fontsize=18)
    # Emphasize the stop point
    plt.scatter( X[pps.stop_-1][0], X[pps.stop_-1][1], marker='o', c='none', edgecolors='g', s=100, linewidth=2)
    plt.show()",0.574929595,
1008,key note,"def handle_keydown(self, evt):
    if evt.key == pygame.K_LEFT:
        self.current_anim=""left""
    elif evt.key == pygame.K_RIGHT:
        self.current_anim=""right""
    if evt.key == pygame.K_UP:
        self.current_anim=""up""
    if evt.key == pygame.K_DOWN:
        self.current_anim=""down""
    
Character.handle_keydown = handle_keydown",0.4417964816,
1008,key note,"def area_of_triangle(triangle):
    """""" Computes the area of a triangle given the coordinates of the vertices
        
        Arguments:
            triangle = list of triangle vertices (v0, v1, v2) composed of tuples (x0, y0), (x1, y1) and (x2, y2)
            note on coordinates: v[0] = x, v[1] = y
            
        Returns:
            Float value representing area of triangle defined by vertices v0, v1 and v2
    """"""
    # Implement tests on user input
    if len(triangle) != 3:
        print(""Triangle must have 3 vertices."")
        return
    for vertex in triangle:
        if len(vertex) != 2:
            print(""Each triangle vertex must have both 'x' and 'y' coordinates."")
            return
    
    # Assign each vertex to a variable and calculate area
    v0 = triangle[0]
    v1 = triangle[1]
    v2 = triangle[2]
    numerator = (v0[0] * (v1[1] - v2[1])) + (v1[0] * (v2[1] - v0[1])) + (v2[0] * (v0[1] - v1[1]))
    return abs(numerator / 2)",0.4252101183,
1008,key note,"def sort(L):
    A = L[:]  # A is a copy of L
    mergeSort(L, 0, len(L), A)",0.4218091667,
1008,key note,"def request(driver, value):
    elem = driver.find_element_by_id(""search_bas"")
    elem.clear()
    elem.send_keys(value)
    elem.send_keys(Keys.RETURN)
    return driver.page_source",0.4213042259,
1008,key note,"# Defining function translate that takes an input string and returns the string in rvarsprket.
def translate(a):
    """"""
    This function takes 1 parameter of type string. The function will return a translation of 
    the string in rvarsprket aka robber's language.
    """"""
    # Checking to see if parameter is a string.
    if not isinstance(a, str):
        # Parameter is not a string. # Print error. Return nothing. 
        print(""Invalid input. Input parameter must be of type string"")
        return
    
    k = """" # Initializing an empty string to hold translation.
    
    # Iterating for each letter in the string.
    for i in a:
        # Checking to see if letter is a lower case consonant.
        if i in [""b"",""c"",""d"",""f"",""g"",""h"",""j"",""k"",""l"",""m"",""n"",""p"",""q"",""r"",""s"",""t"",""v"",""w"",""x"",""y"",""z""]:
            # If letter is a consonant, replace the letter with double the letter and ""o"" in between.
            l = i + ""o"" + i
        # Checking to see if letter is an upper case consonant.
        elif i in [""B"",""C"",""D"",""F"",""G"",""H"",""J"",""K"",""L"",""M"",""N"",""P"",""Q"",""R"",""S"",""T"",""V"",""W"",""X"",""Y"",""Z""]:
            # If letter is a consonant, replace the letter with double the letter and ""O"" in between.
            l = i + ""O"" + i
        else:
            l = i
        k = k + l
    return k # Returning the final translated string. 

translate(""This is fun"")

# SW: good, one thought on implementation: is it easier to keep track of all the consonants 
# and check against them, or keep track of the vowels (a much smaller set) and check
# against them using appropriate logic?",0.4193966985,
1008,key note,"# Defining function translate that takes an input string and returns the string in rvarsprket.
def translate(a):
    """"""
    This function takes 1 parameter of type string. The function will return a translation of 
    the string in rvarsprket aka robber's language.
    """"""
    # Checking to see if parameter is a string.
    if not isinstance(a, str):
        # Parameter is not a string. # Print error. Return nothing. 
        print(""Invalid input. Input parameter must be of type string"")
        return
    
    k = """" # Initializing an empty string to hold translation.
    
    # Iterating for each letter in the string.
    for i in a:
        # Checking to see if letter is a lower case consonant.
        if i in [""b"",""c"",""d"",""f"",""g"",""h"",""j"",""k"",""l"",""m"",""n"",""p"",""q"",""r"",""s"",""t"",""v"",""w"",""x"",""y"",""z""]:
            # If letter is a consonant, replace the letter with double the letter and ""o"" in between.
            l = i + ""o"" + i
        # Checking to see if letter is an upper case consonant.
        elif i in [""B"",""C"",""D"",""F"",""G"",""H"",""J"",""K"",""L"",""M"",""N"",""P"",""Q"",""R"",""S"",""T"",""V"",""W"",""X"",""Y"",""Z""]:
            # If letter is a consonant, replace the letter with double the letter and ""O"" in between.
            l = i + ""O"" + i
        else:
            l = i
        k = k + l
    return k # Returning the final translated string. 

translate(""This is fun"")",0.4193966985,
1008,key note,"def serial_sorting(dataset, buffer_size):
    """"""
    Perform a serial external sorting method based on sort-merge
    The buffer size determines the size of eac sub-record set

    Arguments:
    dataset -- the entire record set to be sorted
    buffer_size -- the buffer size determining the size of each sub-record set

    Return:
    result -- the sorted record set
    """"""
    
    if (buffer_size <= 2):
        print(""Error: buffer size should be greater than 2"")
        return
    
    result = []

    ### START CODE HERE ### 
    
    # --- Sort Phase ---
    sorted_set = []
    
    # Read buffer_size pages at a time into memory and
    # sort them, and write out a sub-record set (i.e. variable: subset)
    start_pos = 0
    N = len(dataset)
    while True:
        if ((N - start_pos) > buffer_size):
            # read B-records from the input, where B = buffer_size
            subset = dataset[start_pos:start_pos + buffer_size] 
            # sort the subset (using qucksort defined above)
            sorted_subset = mergesort(subset) 
            sorted_set.append(sorted_subset)
            start_pos += buffer_size
        else:
            # read the last B-records from the input, where B is less than buffer_size
            subset = dataset[start_pos:] 
            # sort the subset (using qucksort defined above)
            sorted_subset = mergesort(subset) 
            sorted_set.append(sorted_subset)
            break
    
    # --- Merge Phase ---
    merge_buffer_size = buffer_size - 1
    dataset = sorted_set
    while True:
        merged_set = []

        N = len(dataset)
        start_pos = 0
        while True:
            if ((N - start_pos) > merge_buffer_size): 
                # read C-record sets from the merged record sets, where C = merge_buffer_size
                subset = dataset[start_pos:start_pos + merge_buffer_size]
                merged_set.append(k_way_merge(subset)) # merge lists in subset
                start_pos += merge_buffer_size
            else:
                # read C-record sets from the merged sets, where C is less than merge_buffer_size
                subset = dataset[start_pos:]
                merged_set.append(k_way_merge(subset)) # merge lists in subset
                break

        dataset = merged_set
        if (len(dataset) <= 1): # if the size of merged record set is 1, then stop 
            result = merged_set
            break
    ### END CODE HERE ###
    
    return result",0.41632846,
1008,key note,"# Defining function isVowel that returns True if input is a vowel. Input must be one character string.
def isVowel(a):
    """"""
    This function takes 1 parameter of type string and length 1. The function will return true if the input is a
    vowel, otherwise, it will return false.
    """"""
    if not isinstance(a, str):
        # Parameter is not a string. # Print error. Return nothing. 
        print(""Invalid input. Input parameter must be of type string and length 1"")
        return
    elif len(a) != 1:
        # Parameter is not of length 1. # Print error. Return nothing.
        print(""Invalid input. Input parameter must be of type string and length 1."")
        return
    else:
        # Parameter is of type string and length one; a valid input. 
        
        # Checking to see if input is a vowel. 
        if a in [""a"",""e"",""i"",""o"",""u""]:
            # Input is a vowel. Returning true.
            return True
        else:
            # Input is not a vowel. Returning false.
            return False

# SW: lost points: what about upper case, i.e., {A,E,I,O,U}?
isVowel('A')",0.4131401777,
1008,key note,"def report (wages):
    students = wages.keys()
    students.sort()
    for student in students :
        print ""%-20s %12.2f"" % (student, wages[student])",0.4115627408,
1008,key note,"def write_output(data):
    keys = data.keys()
    keys.sort()
    
    for k in keys:
        print ""%s %d"" % (k, data[ k][ ""net""])",0.4115627408,
783,groupby,"def plot_bar_chart(df,groupby_var,group,title,axis,xorder=None):
    train_group = df.groupby(groupby_var).get_group(group)
    total_group_applicants = train_group.shape[0]
    if xorder == None:
        plot = (train_group[""Loan_Status""].value_counts()*100/total_group_applicants).plot(kind=""bar"",ax=axis)
    else:
        plot = ((train_group[""Loan_Status""].value_counts()*100/total_group_applicants)).ix[xorder].plot(kind=""bar"",ax=axis,x=xorder)
    
    plot.set_xlabel(""Loan_Status"")
    plot.set_ylabel(""Percentage"")
    plot.set_ylim([0,80])
    plot.set_title(title)",0.4785104692,
783,groupby,"def feature_weight(data, feature):
    return data[[feature, 'Survived']].groupby(feature, as_index=False).mean()",0.4642153978,
783,groupby,"def display_dfs(data, cat, col):
    avg_df = data.groupby(cat, as_index=False)[col].mean().sort_values(by = col, ascending = False)
    avg_df = avg_df.dropna(axis = 0) # control for NaN
    # top 5
    df1 = avg_df.head()
    # bottom 5
    df2 = avg_df.tail()
    display_side_by_side(df1,df2)",0.4621896744,
783,groupby,"for year in data_annual.groups.keys():
    data_year = data_annual.get_group(year)
    month_mean = data_year.groupby('Month')['wlev'].apply(np.mean)
    month_mean.plot(label=year)
plt.legend()",0.4613136649,
783,groupby,"def get_group(g, key):
    if key in g.groups: return g.get_group(key)
    return pd.DataFrame(list(key).append(np.nan))",0.4579720497,
783,groupby,"def get_all_stats(df,column1,column2):
    temp = df.groupby([column1])[[column2]].agg(['size','mean','median','var','std']).reset_index()
    temp = temp.round(3)
    temp.columns = [column1,'size','mean','median','var','std']
    temp = temp.set_index(column1)
    return temp",0.4577082396,
783,groupby,"def group_summary_statistics(df, group_field, field):
    """"""
    Computes summary statistics such as the mean, median, std, min, and  max
    Input: Dataframe, groupby field, and variable of interest
    Ouput: a tuple of mean, median, std, minimum, maximum
    """"""
    return (df.groupby([group_field])[field].mean(), df.groupby([group_field])[field].median(), 
            df.groupby([group_field])[field].std(), df.groupby([group_field])[field].min(), 
            df.groupby([group_field])[field].max())",0.4556366205,
783,groupby,"#AF method
def af_runner(self, groups = None, aggregator = Aggregators.average):
    #if groups is not passed, use self.groups
    if (groups is None):
        groups = self.groups

    #calculate factors
    for group in groups:
        member_factors = self.user_factors[group.members, :]
        member_biases = self.user_biases[group.members]

        #aggregate the factors
        if (aggregator == Aggregators.average):
            group.grp_factors_af = aggregator(member_factors)
            group.bias_af = aggregator(member_biases)
        elif (aggregator == Aggregators.weighted_average):
            group.grp_factors_af = aggregator(member_factors, weights = group.ratings_per_member)
            group.bias_af = aggregator(member_biases, weights = group.ratings_per_member)

        #predict ratings for all candidate items
        group_candidate_ratings = {}
        for idx, item in enumerate(group.candidate_items):
            cur_rating = self.predict_group_rating(group, item, 'af')

            if (cur_rating > self.cfg.rating_threshold_af):
                group_candidate_ratings[item] = cur_rating

        #sort and filter to keep top 'num_recos_af' recommendations
        group_candidate_ratings = sorted(group_candidate_ratings.items(), key=lambda x: x[1], reverse=True)[:self.cfg.num_recos_af]

        group.reco_list_af = np.array([rating_tuple[0] for rating_tuple in group_candidate_ratings])

GroupRec.af_runner = af_runner",0.4545223713,
783,groupby,"grouped = df.groupby('MSSubClass')

# Define the aggregation
def f(x): 
    if x.dtype in ['int','float']:
        return np.mean(x)
    else :
        if len(x) == 1: return x.values[0]
        m = x.mode()
        return m[0] if len(m) > 0 else np.nan

# Apply the aggregation function then check for an null values
g = grouped.aggregate(f)
for col in g:
    if g[col].isnull().any(): print g[col]
        
# Look at the result
g",0.4525788426,
783,groupby,"# Create a pandas Series for the yearly overview of a particular item in the column
def createYearOverviewItem(df,columnName,item):
    
    item_record = df.groupby([columnName]).get_group(item)
    year_groupby = item_record.groupby(['Year'])
    year_totals = year_groupby.size()

    return year_totals

# Create a DataFrame with the yearly overview of all the unique items in a particular column
def createYearlyOverview(df,columnName):
    
    #list of all unique items in the column
    columnList = df[columnName].unique()
    
    #Ouput frame
    yearlyOverviewFrame = pd.DataFrame()
    
    # For each item create the yearly overview and add these to the Overview frame
    for item in columnList:
        pandasSeries = createYearOverviewItem(df,columnName,item)
        yearlyOverviewFrame[item] = pandasSeries
    
    return yearlyOverviewFrame

# Make an overview of percentages
def createYearlyOverviewPercentages(dfYearOverviewIn):

    dfYearOverview = dfYearOverviewIn.copy()
    
    # Adding a total to the year overview DataFrame
    dfYearOverview['Total'] = dfYearOverview.sum(axis=1)
    
    # Normalizing
    for colm in dfYearOverview.columns.values:
        dfYearOverview[colm] = dfYearOverview[colm]/dfYearOverview['Total']

    dfYearOverview = dfYearOverview.drop(['Total'],axis=1)
    
    return dfYearOverview",0.4523646832,
483,discussion date ranges and frequencies,"def proc_issue_d():
    df['issue_month'], df['issue_year'] = zip(*df.issue_d.str.split('-'))
    df.drop(['issue_d'], 1, inplace=True)",0.4342900515,
483,discussion date ranges and frequencies,"class Day(Enum):
    SUN = ""Sunday""
    MON = ""Monday""
    TUE = ""Tuesday""
    WED = ""Wednesday""
    THU = ""Thursday""
    FRI = ""Friday""
    SAT = ""Saturday""
    
class Week(Enum):
    FULL = [Day.SUN, Day.MON, Day.TUE, Day.WED, Day.THU, Day.FRI, Day.SAT]
    WORK = [Day.MON, Day.TUE, Day.WED, Day.THU, Day.FRI]

    def days(self):
        return self.value
    
    def day_names(self):
        return list(map(lambda day: day.value, self.value))

class Project(Enum):
    UPLINK = ""Uplink""
    SKINIO = ""SkinIO""
    TRUMPET = ""Trumpet""
    ALEXANDRIA = ""Alexandria""
    QUEST = ""Quest""
    SCINTILLOMETER = ""Scintillometer""
    WEDDING_WEBSITE = ""Wedding Website""
    OTHER = ""Other""
    
    def numerical_value(self):
        return {
            Project.SKINIO: 1,
            Project.UPLINK: 2,
            Project.TRUMPET: 3,
            Project.QUEST: 4,
            Project.WEDDING_WEBSITE: 5,
            Project.ALEXANDRIA: 6,
            Project.SCINTILLOMETER: 6, 
            Project.OTHER: 6
        }[self]
    
    def is_job(self):
        return self == Project.SKINIO

    def is_side_project(self):
        return self in [
            Project.UPLINK,
            Project.ALEXANDRIA,
            Project.QUEST,
            Project.SCINTILLOMETER,
            Project.WEDDING_WEBSITE
        ]
    
    def is_work(self):
        return self.is_job() or self.is_side_project()
    
    @staticmethod
    def get_from(project, task):
        clean_project = project.strip()
        clean_task = task.strip()
        
        if (clean_project == ""Uplink""):
            return Project.UPLINK
        elif (clean_task == ""SkinIO""):
            return Project.SKINIO
        elif (clean_project == ""Trumpet""):
            return Project.TRUMPET
        elif (clean_project == ""Alexandria""):
            return Project.ALEXANDRIA
        elif (clean_project == ""Quest""):
            return Project.QUEST
        elif (clean_project == ""Scintillometer""):
            return Project.SCINTILLOMETER
        elif (clean_project == ""Wedding Website""):
            return Project.WEDDING_WEBSITE
        else:
            return Project.OTHER
    
    @staticmethod
    def jobs():
        return list(filter(lambda project: project.is_job(), Project.all()))

    @staticmethod
    def side_projects():
        return list(filter(lambda project: project.is_side_project(), Project.all()))
    
    def work():
        return list(filter(lambda project: project.is_work(), Project.all()))
    
    @staticmethod
    def all():
        return [
            Project.UPLINK,
            Project.SKINIO,
            Project.TRUMPET,
            Project.ALEXANDRIA,
            Project.QUEST,
            Project.SCINTILLOMETER,
            Project.WEDDING_WEBSITE,
            Project.OTHER
        ]

class Key(Enum):
    DATE = ""date""
    DAY = ""day""
    PROJECT = ""project""
    START = ""start""
    END = ""end""
    AMOUNT = ""amount""
    
HIST_COLOR = (42/255.0, 111/255.0, 177/255.0, 1)",0.4332720637,
483,discussion date ranges and frequencies,"def print_topic_temporal_dists_per_periodical(model):
    for name, documents_for_periodical in periodicals:
        T_periodical = extract_topics.gen_topic_temporal_dists(documents_for_periodical, model, tf_vectorizer, dtm_tf)
        print_topic_temporal_dists(T_periodical, documents=documents_for_periodical, title_suffix="" for %s"" % name)",0.4254770875,
483,discussion date ranges and frequencies,"def add_feature_visitors_stats(df):
    """""" 
    Receives a Dataframe and computes daily visitors' statistics, such as mean/median and also the number of observations (per day of the week).
    """""" 
    df.visit_date = pd.to_datetime(df.visit_date)
    df['dow'] = df['visit_date'].apply(lambda x: x.weekday())  # Monday=0, Sunday=6

    # Calculate mean() median() and size() for every restaurant per day of the week
    tmp = df.groupby(['air_store_id','dow']).agg({'visitors' : [np.mean,np.median,np.size]}).reset_index()
    tmp.columns = ['air_store_id', 'dow', 'mean_visitors', 'median_visitors','count_observations']
    return pd.merge(df, tmp, how='left', on=['air_store_id','dow'])",0.4250783026,
483,discussion date ranges and frequencies,"def test_run():
    # Read data
    dates = pd.date_range('2012-07-01', '2012-07-31')  # one month only
    symbols = ['SPY','XOM']
    df = get_data(symbols, dates)
    plot_data(df)

    # Compute daily returns
    daily_returns = compute_daily_returns(df)
    plot_data(daily_returns, title=""Daily returns"", ylabel=""Daily returns"")

test_run()",0.4248128533,
483,discussion date ranges and frequencies,"def test_run():
    # Read data
    dates = pd.date_range('2012-07-01', '2012-07-31')  # one month only
    symbols = ['SPY','XOM']
    df = get_data(symbols, dates)
    plot_data(df)

    # Compute daily returns
    df_day = daily_returns(df)
    plot_data(df_day, title=""Daily returns"", ylabel=""Daily returns"")
    
    df_cum = cumulative_returns(df)
    plot_data(df_cum, title=""Cumulative returns"", ylabel=""Cumulative returns"")
    
if __name__ == ""__main__"":
    test_run()",0.4245475531,
483,discussion date ranges and frequencies,"def bio_names(raw_names):
    """""" 
       Function that returns list and dictionary with content of celebritie's 
       biography from Wikipedia
    """"""
    bio_list = []
    bio_dict = {}
    category_list = []
    category_dict = {}
    # Set counter to zero
    k = 0
    # For each name in the list of names...
    for name in raw_names:
        # We increase counter
        k +=1        
        # print number and name stripped from ""chomp""
        aux_name = name.split('\t')[0]
        aux_category = name.split('\t')[len(name.split('\t'))-1] 
        print k , aux_name #.strip('\n')
        # Load wikipedia information from the person in turn...
        p = wikipedia.page(aux_name)
        # ... get the content...
        wiki_content = p.content
        # We manage content extracted from Wikipedia by eliminating the ""chomp"" 
        # followed by regular expressions.
        wiki_content = wiki_content.replace('\n',' ')
        wiki_content = re.sub(r'[^a-z0-9\s]', '', wiki_content.strip().lower())    
        # ... and append it to the corresponding lists and dictionaries
        bio_list.append(wiki_content)
        bio_dict[aux_name]= wiki_content 
        category_list.append(aux_category.strip('\n'))
        category_dict[aux_name]= aux_category.strip('\n')
    # Display message to indicate we are finished
    print 'Done'
    return bio_list, bio_dict, category_list, category_dict",0.4244684577,
483,discussion date ranges and frequencies,"def subDataFrame(record):
    dfArray = []
    for time in range(int(record.startDate), int(record.endDate) + 1, 60 * 60 * 24):
        apiKey = os.environ['FORECAST_IO_API_KEY']
        URL = ""https://api.darksky.net/forecast/""
        URL += apiKey + ""/"" + str(record.lat) + "","" + str(record.lng) + "","" + str(time) + ""?""
        URL += ""exclude=currently,minutely,hourly,alerts,flags""
        try:
            r = requests.get(URL)
            r.raise_for_status()
        except requests.exceptions.HTTPError as err:
            print(err)
            sys.exit(1)
        rJSON = r.text
        weather = json.loads(rJSON)
        daily = weather['daily']['data'][0]

        # add days and locations to daily weather information
        daily['days'] = datetime.datetime.fromtimestamp(time).strftime('%Y-%m-%d')
        daily['locations'] = record.locId

        for key in daily:
            daily[key] = [daily[key]]

        df = pd.DataFrame(daily)
        dfArray.append(df)

    subDataFrame = pd.concat(dfArray)
    return(subDataFrame)",0.4240508974,
483,discussion date ranges and frequencies,"def test_run():
    
    # Read in data
    dates = pd.date_range('2010-01-01', '2012-12-31')
    symbols = ['SPY']
    df = get_data(symbols, dates)
    
    # Compute Bollinger Bands
    rm_spy = get_rolling_mean(df['SPY'], window=20)
    rstd_spy = get_rolling_std(df['SPY'], window=20)
    upper_band, lower_band = get_bollinger_bands(rm_spy, rstd_spy)
    
    # Plot the values
    ax = df['SPY'].plot(title='Bollinger Bands', label='SPY')
    rm_spy.plot(label='Rolling Mean', ax=ax)
    upper_band.plot(label='Upper Band', ax=ax)
    lower_band.plot(label='Lower Band', ax=ax)

test_run()",0.4230523705,
483,discussion date ranges and frequencies,"totals=analysis_utils.get_freq_totals(system_articles[systems[0]],
                                      set(),
                                      ambiguous_only=False
                                     )",0.4215489626,
140,building a web scraper,"class Scraper:
    """"""Class that scrapes 311.boston.gov""""""
    
    def __init__(self, driver):
        self.report_urls = []
        self.driver = driver
        self.driver.get(BOSTON_URL)
        self.services = []
        self.get_services()
        
    def get_services(self):
        """"""Get service categories and service category count""""""
        service_elems = self.driver.find_elements_by_xpath(SERVICE_ELEMS_XPATH)
        
        service_category_elems = [elem.find_element_by_xpath(""./a"") for elem in service_elems]
        service_count_elems = [elem.find_element_by_xpath(""./span"") for elem in service_elems]
        
        service_categories = [elem.text for elem in service_category_elems]
        service_counts = [int(''.join(re.findall('\d+', elem.text))) for elem in service_count_elems]
        
        self.services = list(zip(service_categories, service_counts))
        
    def get_reports_on_page(self):
        """"""Append report urls from a page onto self.report_urls""""""
        report_ids = [elem.get_attribute(""onclick"").split(""location.href='"")[1][:-2]
                        for elem in self.driver.find_elements_by_class_name('report')]
        
        self.report_urls += [BOSTON_URL + report_id
                            for report_id in report_ids
                            if BOSTON_URL + report_id not in self.report_urls]
        
    def next_page(self):
        """"""Go to next page""""""
        next_page_button = self.driver.find_element_by_xpath(""//li/a[contains(text(),'Next')]"")
        next_page_button.click()
        
    def search_for(self, search_text):
        """"""Search for inputed text in search field and go to the results""""""
        search_box = self.driver.find_element_by_class_name('search-query')
        search_box.send_keys(search_text)
        search_box.send_keys(Keys.ENTER)",0.5227661133,
140,building a web scraper,"class Scraper:
    """"""Scrapes the Boston's 311 main page with BeautifulSoup""""""
    
    def __init__(self):
        self.current_page = 1
        self.report_urls = []
        
        # Get main page soup
        res = requests.get(BOSTON_URL)
        self.soup = BeautifulSoup(res.text, ""html5lib"")
        
    def get_next_page_soup(self):
        """"""Adds soup from next page to soup""""""
        if not self.current_page == 20:
            time.sleep(1)
            self.current_page += 1
            
            res = requests.get(BOSTON_URL + ""/?page="" + str(self.current_page))
            new_soup = BeautifulSoup(res.text, ""html5lib"")
            
            self.soup.append(new_soup.body)
            
    def get_report_urls(self):
        """"""Gets all '.report' class 'onclick' attributes, and gets the appropriate URLs""""""
        onclick_attrs = [elem['onclick'] for elem in self.soup.select("".report"")]
        self.report_urls = [BOSTON_URL + onclick_text.split(""'"")[1] for onclick_text in onclick_attrs]",0.5156935453,
140,building a web scraper,"class Inquiry:
    
    def __init__(self):
        self.url = url
        self.page_response = self.pull_site()
        self.pull_time = time.ctime(time.time())
        self.titles = self.parse_titles()
    
    def __str__(self):
        ob_str = f'Queried {self.url} on {self.pull_time}. {str(len(self.titles))} titles found.'
        return ob_str
        
    def pull_site(self):
        site_response = requests.get(self.url, timeout=5)
        site_response.raise_for_status()
        return site_response
    
    def parse_titles(self):
        titles_list = []
        
        #raw page held in 'content' attribute of response obj
        soup = bs4.BeautifulSoup(self.page_response.content, 'html.parser')
        select_list = soup.select('.grid-view-item__title')
        
        for titles in select_list:
            titles_list.append(titles.getText())
        return titles_list

class InquiryRecord(Inquiry):
    
    def __init__(self, url, ptime, titles):
        self.url = url
        self.pull_time = ptime
        self.titles = titles",0.5099977255,
140,building a web scraper,"class PASEWitnesses:
    """"""Gets basic information about people on the charter from a PASE witnesses page""""""
    
    def __init__(self, url):
        self._url = url
        self._soup = soup_page(self._url)
        # get only portion of page with relevant data
        try:
            self.data = self._soup.find('div', class_='rec').find('ul').find_all('li')
        # sometimes notes preceed data, in which case get second div with class rec
        except:
            self.data = self._soup.find_all('div', class_='rec')[1].find('ul').find_all('li')
    @property
    def witnesses(self):
        witness_list = []
        for witness_entry in self.data:
            witness_link_element = witness_entry.find('a')
            try:
                witness_role = witness_entry.find('strong').get_text()
            except:
                witness_role = 'Witness'
            witness_list.append({
                    'role': witness_role,
                    'name': witness_link_element.get_text().lstrip(),
                    'link': witness_link_element['href'].replace('../', 'http://www.pase.ac.uk/jsp/'),
                    'description': witness_entry.find('em').get_text()
                })
            # look for nested witnesses, sometimes buried in recursive em tags
            nested_witnesses_element = None
            try:
                nested_witnesses_element = self._soup.find('div', class_='rec').find('ul').find('em')
            except:
                nested_witnesses_element = self._soup.find_all('div', class_='rec')[1].find('ul').find('em')
            while nested_witnesses_element is not None:
                nested_witnesses = nested_witnesses_element.find_all('li')
                for nested_witness_entry in nested_witnesses:
                    nested_witness_link_element = witness_entry.find('a')
                    try:
                        nested_witness_role = nested_witness_entry.find('strong').get_text()
                    except:
                        nested_witness_role = 'Witness'
                    witness_list.append({
                        'role': nested_witness_role,
                        'name': nested_witness_link_element.get_text(),
                        'link': nested_witness_link_element['href'].replace('../', 'http://www.pase.ac.uk/jsp/'),
                        'description': nested_witness_entry.find('em').get_text()
                    })
                nested_witnesses_element = nested_witnesses_element.find('em')
        return witness_list
    

print('PASE Witnesses Scraper Defined')",0.5095677376,
140,building a web scraper,"import scrapy


class FestivalSpider(scrapy.Spider):
    name = 'music_festivals'

    custom_settings = {
        ""DOWNLOAD_DELAY"": 3,
        ""CONCURRENT_REQUESTS_PER_DOMAIN"": 3,
        ""HTTPCACHE_ENABLED"": True
    }

    start_urls = [
        'https://www.musicfestivalwizard.com/festival-guide/us-festivals/',
        'https://www.musicfestivalwizard.com/festival-guide/canada-festivals/'
    ]

    def parse(self, response):
        # Extract the links to the individual festival pages


        # Follow pagination links and repeat",0.5049147606,
140,building a web scraper,"class Website():
    def __init__(self, index_site):
        self.index_site = index_site
        self.index_htmlContent = ''
        self.num_page = 0
        self.num_article = 0
        self.page_df = ''
        self.page_info_categories = []
        self.page_info_title = []
        self.page_info_date = []
        self.page_info_author = []
        self.page_info_url = []
        
    def get_index_htmlContents(self):
        # request to open the website via urlopen
        req = Request(self.index_site, headers={'User-Agent': 'Mozilla/5.0'})
        webpage = urlopen(req).read()
        # html parser via beautifulsoup
        soupIndex = BeautifulSoup(webpage, ""html.parser"")
        self.index_htmlContent = soupIndex
        return self.index_htmlContent
    
    def get_num_page(self):
        # transfer to variable
        soupIndex = self.index_htmlContent
        pageNum = soupIndex.select('.page-numbers')
        num_pageOnSite = len(pageNum)
        self.num_page = int(pageNum[num_pageOnSite-2].text)
        return self.num_page
    
    def get_info_ofEach_page(self, testPrint):
        # transfer to variable
        soupIndex = self.index_htmlContent
        # choose the news part
        full_latest_post = soupIndex.select('#latest-posts')[0]
        # count the number of article
        num_article = len(full_latest_post.select('article'))
        # declare the list for storing the article meta-data
        info_categories = []
        info_title = []
        info_date = []
        info_author = []
        info_url = []
        # parsing the article meta-data to list
        for i in range(num_article):
            # get article
            article_contents = full_latest_post.select('article')[i]
            # parsing and storing
            info_categories.append(article_contents.select('.categories')[0].text[14:-1])
            info_title.append(article_contents.select('.entry-title')[0].text[1:-1])
            info_date.append(article_contents.select('.post-info')[0].text[1:-1])
            info_author.append(article_contents.select('.post-info')[1].text[4:-1])
            info_url.append(article_contents.a['href'])
            
        # save to single dataframe
        threatPost_com = pd.DataFrame(np.column_stack([info_categories, info_title, info_date, info_author, info_url])
                                     , columns=['categories', 'title', 'date', 'author', 'url'])
        # print the head of dataframe
        if testPrint==1:
            threatPost_com.head()
        # save to class attributes
        self.page_df = threatPost_com
        self.page_info_categories = info_categories
        self.page_info_title = info_title
        self.page_info_date = info_date
        self.page_info_author = info_author
        self.page_info_url = info_url
        
        return threatPost_com, info_categories, info_title, info_date, info_author, info_url",0.4798848033,
140,building a web scraper,"class ScrapeData :
    docs=[]
    links=[]
    titles=[]
    
    # For scraping data from technologyreview.com website
    def downloadData(self,link) :
        response=requests.get(link)
        soup = BeautifulSoup(response.content, ""lxml"")
        print(soup.title.string)
        self.titles.append(soup.title.string)
        article_body=soup.find(attrs={""class"": ""article-body__content""})
        pTags=article_body.findChildren('p')
        p=''
        for pTag in pTags :
            #print(pTag.get_text())
            p+=pTag.get_text()
            #print('\n') 
        self.docs.append(p)
        self.links.append(link)
        
    def processTechnologyReview(self,baseUrl,categoryUrl) :

        response=requests.get(baseUrl+categoryUrl)

        soup = BeautifulSoup(response.content, ""lxml"")

        liTags=soup.find('li',attrs={""class"": ""tech""})

        articleTag=soup.find(attrs={""class"": ""article""})

        mainTag=soup.find('main')

        if liTags :

            while liTags :
                link =baseUrl+liTags.findChild('a').get('href')
                try:
                    self.downloadData(link)
                except requests.exceptions.MissingSchema:
                    print(""Invalid Url .."")
                    print(""Let me sleep for 5 seconds"")
                    print(""ZZzzzz..."")
                    sleep(5)
                    print(""Was a nice sleep, now let me continue..."")
                    continue
                liTags=liTags.findNextSibling()

        elif articleTag:

            h3Tags=articleTag.findAll('h3')

            for h3Tag in h3Tags :
                if h3Tag.find('a') :
                    link =h3Tag.find('a').get('href')
                    try:
                        self.downloadData(link)
                    except requests.exceptions.MissingSchema:
                        print(""Invalid Url .."")
                        print(""Let me sleep for 5 seconds"")
                        print(""ZZzzzz..."")
                        sleep(5)
                        print(""Was a nice sleep, now let me continue..."")
                        continue
        elif mainTag:

            liClass=mainTag.find('li',attrs={'class':'nav-li nav-li--with-big-dropdown'})
            ulClass=liClass.findChild('ul')
            anchorTags=ulClass.findChild('a')
            while anchorTags :
                link =baseUrl+anchorTags.get('href')
                print(link)
                try:
                    self.downloadData(link)
                except requests.exceptions.MissingSchema:
                    print(""Invalid Url .."")
                    print(""Let me sleep for 5 seconds"")
                    print(""ZZzzzz..."")
                    sleep(5)
                    print(""Was a nice sleep, now let me continue..."")
                    continue
                anchorTags=anchorTags.findNextSibling()",0.4798279405,
140,building a web scraper,"from scrapely import Scraper
s = Scraper()",0.4790976048,
140,building a web scraper,"import scrapy

class DmozSpider(scrapy.Spider):
    name = ""FOXNEWS""
    allowed_domains = [""Foxnews.com""]
    start_urls = [""http://www.foxnews.com/""]

    def parse(self, response):
                  f.write(response.body)",0.471680671,
140,building a web scraper,"class Fetcher:
    
    def __init__(self, url, host):
        self.url = url
        self.host = host
        self.response = b''  # Empty array of bytes.

        
    def fetch(self):
        global stopped
        sock = socket.socket()

        sock.setblocking(False)
        try:
            sock.connect((self.host, 80))
        except BlockingIOError:
            pass

        f = MyFuture()

        #resolves the future by setting a result on it
        def on_connected():
            print('on connected cb ran', flush=True)
            f.set_result(None)
        
        
        
        selector.register(sock.fileno(),
                          EVENT_WRITE,
                          on_connected)
        print(""about to yield connection future"", flush=True)
        yield f#this makes it look like fetch has returned the ""future""
        #bit we have not lost the state (or have to have carried it in obj)
        #a send in will continue us here
        print('we were connected! now back in gen', flush=True)
        selector.unregister(sock.fileno())
        request = 'GET {} HTTP/1.0\r\nHost: {}\r\n\r\n'.format(self.url, self.host)
        sock.send(request.encode('ascii'))
        while True:
            print(""in loop"")
            #now create a new future for the data-recieving call
            f = MyFuture()
            def on_response():
                chunky = sock.recv(4096)  # 4k chunk size.
                f.set_result(chunky)
            selector.register(sock.fileno(),
                              EVENT_READ,
                              on_response)
            #now to restart the gen, we will from the main
            #throw the data right back in
            chunk = yield f
            selector.unregister(sock.fileno())
            if chunk:
                print(""len(chunk)"",len(chunk))
                self.response += chunk
            else:
                print(""all read"")
                stopped= True
                break",0.4693425298,
2583,what you should already know,"def cnn_hyParam_training_sweep(n_iterations, simple_cnn, microscope, *swept_params):
    '''
    [int], [keras Sequential Model], [string], *[strings]  -->  [pd.DataFrame], [keras Sequential Model]
    
    Function takes in specified neural network, trains it ""n_iterations"" times, selecting different hyperparameters
    randomly selecting each argument of ""*swept_params"" each pass, and outputs a DataFrame storing the hyperparameters
    and corresponding Training and Dev Accuracies for each pass
    
    Inputs:
    1) number of training passes desired for the hyperparameter sweep. Set to 1 if just want to train the model once
    2) the CNN to be trained. Set to either ""simple_cnn_fluorescent"" or ""simple_cnn_brightfield""
    3) string specifying microscope type of cnn to be trained. EITHER ""fluorescent"" OR ""brightfield""
    4) strings corresponding to keys of the hyperparameter dictionary for variables desired to be swept
    
    Outputs:
    1) DataFrame containing hyperparameters and corresponding Train and Dev set accuracies for each pass in rows
    2) Trained cnn
    
    '''
    
    
    ## Restore BSF hyperparameter values (in case this function has been run more than once)
    hyperparam_BSF_vals = hyperparam_BSF_dict()
    
    ## Initialise sweep DataFrame which stores hyperparameter values and corresponding Train and Dev Accuracies 
    ## for each iteration
    
    # Row index and column labels for DataFrame
    columns = list(hyperparam_BSF_vals.keys())
    columns.extend(('Train_Accuracy', 'Dev_Accuracy'))
    index = range(n_iterations)    
    # Initialise empty DataFrame
    df_hyParam_training_sweep = pd.DataFrame(index=index, columns=columns)
    df_hyParam_training_sweep = df_hyParam_training_sweep.fillna(0) # with 0s rather than NaNs
    
    
    ## Initialise counter for While Loop
    count = 0
    
    
    while count < n_iterations:
        
        # Restore initial (Batch Normalised) weights of model, otherwise succeeding iterations of this loop
        # will train models with weights from earlier iterations rather than being independent
        simple_cnn.load_weights('model.h5')        
        print('Iteration number {} \n'.format(count + 1))
        

        ## Randomly sample hyperparameters for given swept_params, altering the ""hyperparam_BSF_vals"" dictionary in place
        swept_params = hyperparam_sweep(hyperparam_BSF_vals, *swept_params)
        for item in swept_params:
            print('{} = {} \n'.format(item, hyperparam_BSF_vals[item]))
        # Based on ""microscope"" input string, select hyperparams for either fluorescent or brightfield CNN
        if microscope == 'fluorescent':
            learning_rate = hyperparam_BSF_vals['LEARNING_RATE_FLUOR']
            minibatch = hyperparam_BSF_vals['MINIBATCH_FLUOR']
            epochs = hyperparam_BSF_vals['EPOCH_FLUOR']
            train_image_Series = train_dev_fluor_df['images']
            train_mask_Series = train_dev_fluor_df['masks']
        elif microscope == 'brightfield':
            learning_rate = hyperparam_BSF_vals['LEARNING_RATE_BRIGHT']
            minibatch = hyperparam_BSF_vals['MINIBATCH_BRIGHT']
            epochs = hyperparam_BSF_vals['EPOCH_BRIGHT'] 
            train_image_Series = train_dev_bright_df['images']
            train_mask_Series = train_dev_bright_df['masks']
        else:
            print('Didn\'t recognise \""microscope\"" input string: training fluorescent network')
            learning_rate = hyperparam_BSF_vals['LEARNING_RATE_FLUOR']
            minibatch = hyperparam_BSF_vals['MINIBATCH_FLUOR']
            epochs = hyperparam_BSF_vals['EPOCH_FLUOR']
            train_image_Series = train_dev_fluor_df['images']
            train_mask_Series = train_dev_fluor_df['masks']
            
        ## Optimiser
        adam_optimiser = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)


        ## Compile Model
        simple_cnn.compile(optimizer= adam_optimiser, loss= dice_coef_loss, metrics= [dice_coef, 'acc', 'mse'])    


        ## Train Model
        # pd.DataFrame to NumPy array for convenience
        X_train = imageSeries_to_ndArray(train_image_Series)
        y_train = maskSeries_to_ndArray(train_mask_Series)
        # Train CNN
        history = simple_cnn.fit(X_train, y_train, validation_split= hyperparam_BSF_vals['DEV_SPLIT'],
                       batch_size= minibatch, epochs= epochs)
        train_accuracy = history.history['acc']   # This will be a list, with an ""acc"" value for each epoch
        
        ## Evaluate accuracy for Train Set
#         train_accuracy = simple_cnn.evaluate(X_train, y_train)[2]
        print('Train set accuracy for {} Dataset is {}\n'.format(microscope,
            train_accuracy))

        ## Apply Model to Dev Set and calculate accuracy
        # Fluorescent Dev Set Data
#         X_dev = imageSeries_to_ndArray(dev_image_Series)
#         y_dev = maskSeries_to_ndArray(dev_mask_Series)

        ### CLEAN DATA

        ## Evaluate accuracy for Dev Set
        dev_accuracy = history.history['val_acc']   # This will be a list, with an ""acc"" value for each epoch
#         dev_accuracy = simple_cnn.evaluate(X_dev, y_dev)[2]
        print('Dev set accuracy for {} Dataset is {}\n'.format(microscope,
            dev_accuracy))
        
        iteration_dict = hyperparam_BSF_vals
        iteration_dict['Train_Accuracy'] = train_accuracy[-1]   # ""-1"" indexes last epoch accuracy from ""history""
        iteration_dict['Dev_Accuracy'] = dev_accuracy[-1]   # ""-1"" indexes last epoch accuracy from ""history""
        df_hyParam_training_sweep.iloc[count] = pd.Series(iteration_dict)
        
        # Update iterator for While Loop
        count += 1
    
    return df_hyParam_training_sweep, simple_cnn, history",0.2376748323,
2583,what you should already know,"def try_garden(garden, hostname):
    # hostname from ~/.ssh/config
    launchdirs_exist_file = os.path.join(LOCAL_GARDEN, 'launchdirs_exist.txt')
    if os.path.exists(launchdirs_exist_file):
        copyfile(launchdirs_exist_file, LAUNCHDIRS)
    with open(LAUNCHDIRS, 'a') as f1:
        with open(os.path.join(LOCAL_GARDEN, 'launchdirs_not_exist.txt'), 'r') as f2:
            for line in f2.readlines():
                dest, block = get_dest_blocks(line)
                remote_dir = os.path.join(garden, block)
                print(remote_dir, file=f1)
    launchdirs_exist(hostname)
    #!head -1 {LOCAL_GARDEN}launchdirs_not_exist.txt",0.2353418022,
2583,what you should already know,"def delete_old_point(x, y, ac_to_shunt):
    coord_points[str(x) + ""_"" + str(y)].remove(ac_to_shunt)
    x_coords_points[x].remove(ac_to_shunt)
    y_coords_points[y].remove(ac_to_shunt)
    return",0.2307494879,
2583,what you should already know,"self.tts.say(""Grab the red ball and hide it from me."" + 
                     "" I will tell you once I see the ball."")
        
        self.lastSeen = 0
        self.lastNotSeen = 0
        
        START = time.time()

        while time.time() - START < 10:

            cover_eyes(self)

            if time.time() - self.lastNotSeen > 5:
                self.tts.say(""Where's the ball? I don't see the ball"")
                self.lastNotSeen = time.time()    
        
        uncover_eyes(self)",0.2296327353,
2583,what you should already know,"%%sql
    SELECT *
    FROM hospital h1, hospital h2 
    WHERE h1.provider = h2.provider 
          AND (h1.hospital <> h2.hospital 
               OR h1.address <> h2.address 
               OR h1.city <> h2.city
               OR h1.state <> h2.state
               OR h1.zip <> h2.zip 
               OR h1.county <> h2.county 
               OR h1.phone_number <> h2.phone_number 
               OR h1.hospital_type <> h2.hospital_type 
               OR h1.hospital_owner <> h2.hospital_owner 
               OR h1.emergency_service <> h2.emergency_service
               OR h1.condition <> h2.condition 
               OR h1.measure_code <> h2.measure_code)
    LIMIT 1;
# NO",0.2269345224,
2583,what you should already know,"def generate_text(synthesizer_model, corpus, text_len):    
    start_index = random.randint(0, len(corpus) - seq_len - 1)

    sentence_english = corpus[start_index: start_index + seq_len]
    word_vec_sequence = map(lambda word: word_to_vec[word], sentence_english)

    prog = Progbar(text_len)
    for _ in range(text_len):
        x = np.zeros((1, seq_len, embed_size))
        for i, word_vec in enumerate(word_vec_sequence):
            x[0, i] = word_vec

        y = synthesizer_model.predict(x, verbose=0)        
                
        word_vec_sequence = word_vec_sequence[1:] + [y]
        sentence_english.append(closest_word(y))
        prog.add(1)

    return ' '.join(sentence_english)",0.2254880667,
2583,what you should already know,"def callback(data):
    rospy.loginfo(rospy.get_caller_id() + ""I heard %s"", data.data)",0.2249613404,
2583,what you should already know,"def sgd(theta, x, y):
    """"""
    continue taking grad steps until conevrgence, ie. until grad == 0
    """"""
    step = tz.curry(sgd_step)(x)(y)
    converge = lambda x: abs(sum(map(sub, x[0], x[1]))) > 1e-5
    return until_convergence(tz.iterate(step, theta), converge)",0.2233785689,
2583,what you should already know,"def fill_assets(self):
        ##### FILL IN WITH YOUR ASSET DATA

        # for FX basket
        full_bkt    = ['EURUSD', 'USDJPY', 'GBPUSD', 'AUDUSD', 'USDCAD',
                       'NZDUSD', 'USDCHF', 'USDNOK', 'USDSEK']

        basket_dict = {}

        for i in range(0, len(full_bkt)):
            basket_dict[full_bkt[i]] = [full_bkt[i]]

        basket_dict['Thalesians FX CTA'] = full_bkt

        br = self.fill_backtest_request()

        self.logger.info(""Loading asset data..."")

        vendor_tickers = ['FRED/DEXUSEU', 'FRED/DEXJPUS', 'FRED/DEXUSUK', 'FRED/DEXUSAL', 'FRED/DEXCAUS',
                          'FRED/DEXUSNZ', 'FRED/DEXSZUS', 'FRED/DEXNOUS', 'FRED/DEXSDUS']

        time_series_request = TimeSeriesRequest(
                    start_date = br.start_date,                     # start date
                    finish_date = br.finish_date,                   # finish date
                    freq = 'daily',                                 # daily data
                    data_source = 'quandl',                         # use Quandl as data source
                    tickers = full_bkt,                             # ticker (Thalesians)
                    fields = ['close'],                                 # which fields to download
                    vendor_tickers = vendor_tickers,                    # ticker (Quandl)
                    vendor_fields = ['close'],                          # which Bloomberg fields to download
                    cache_algo = 'internet_load_return')                # how to return data

        asset_df = self.tsfactory.harvest_time_series(time_series_request)

        # signalling variables
        spot_df = asset_df
        spot_df2 = None

        return asset_df, spot_df, spot_df2, basket_dict
    
    StrategyFXCTA_Example.fill_assets = fill_assets",0.2230340838,
2583,what you should already know,"def lotka_voltera_rhs(X, t, *p):
    '''
    Set up the right hand side (rhs) function for the system 
    (necessary step to feed sympy's odeint function).
    '''
    rho = p
    evald_dt=dX_dt_numeric(X[0],X[1], rho)
    
    
    return np.array([evald_dt[0], 
                    evald_dt[1]])",0.2224977016,
1560,predictive distribution,"def log_likelihood(model, text):
    probs = model.predict(parse_text(text, vocab_size, padding=True)).squeeze()
    return sum([np.log(probs[i, char_indices[c]]) 
                 for i,c in enumerate(text[1:]) ])",0.5951036215,
1560,predictive distribution,"def predict(model,xtest):
    predicted = model.predict(xtest)
    return predicted",0.5921465158,
1560,predictive distribution,"def predict_tags_clf1(clf1, X):
    Y_pred = clf1.predict(X)
    return Y_pred",0.5921465158,
1560,predictive distribution,"def model_prediction(model,x_test=x_test):
    prediction = model.predict(x_test)
    return prediction.reshape(prediction.shape[0],)",0.5902569294,
1560,predictive distribution,"def predict_point_by_point(model, data):
    predicted = model.predict(data, batch_size=1)
    predicted = np.reshape(predicted, (predicted.size,))
    return predicted",0.5881693363,
1560,predictive distribution,"def forecast_lstm(model, batch_size, X):
    X = X.reshape(1, 1, len(X))
    yhat = model.predict(X, batch_size=batch_size)
    return yhat[0,0]",0.5826977491,
1560,predictive distribution,"# note about resize/reshaping contraints

def forecast_lstm(model, batch_size, X):
    X = X.reshape(1, 1, len(X))
    yhat = model.predict(X, batch_size=batch_size)
    return yhat[0,0]",0.5826977491,
1560,predictive distribution,"def covnet_transform(covnet_model, raw_images):

    # Pass our training data through the network
    pred = covnet_model.predict(raw_images)

    # Flatten the array
    flat = pred.reshape(raw_images.shape[0], -1)
    
    return flat",0.5818475485,
1560,predictive distribution,"def predict_test_set(predictor, test_set):
    """"""Compute the prediction for every element of the test set.""""""
    predictions = [predictor.predict(test_set[i, :, :]) 
                   for i in xrange(len(test_set))]
    return predictions

# Choose a subset of the test set. Otherwise this will never finish.
test_set = test_images[0:100, :, :]
all_predictions = [predict_test_set(predictor, test_set) for predictor in predictors]",0.5816879272,
1560,predictive distribution,"#This function takes the dataset and the trained classifier to return the prediction obtained.

def predict(data,clf):

    data_x = data[:,:-1]
    data_y = data[:,-1]
    predicted_y = clf.predict(data_x)
    
    return predicted_y",0.5803723335,
346,create a spark context and import dependencies,"if 'pyspark' not in vars():   # set up Apache Spark environment if not yet done so
    
    # set environment variables for Spark
    os.environ['SPARK_HOME'] = SPARK_HOME
    os.environ['SPARK_HIVE'] = 'true'
    
    # enable importing of PySpark through FindSpark package
    import findspark
    findspark.init()
    
    # import PySpark and set up SparkContext (""sc"") & HiveContext (""hc"")
    import pyspark
    
    sc = pyspark.SparkContext(
        conf=pyspark.SparkConf()
            .setMaster(SPARK_MODE)
            .setAppName('BostonHousing')
            .set('spark.driver.memory', SPARK_DRIVER_MEMORY)
            .set('spark.executor.memory', SPARK_EXECUTOR_MEMORY)
            .set('spark.driver.maxResultSize', SPARK_DRIVER_MAX_RESULT_SIZE))
    
    hc = pyspark.sql.HiveContext(sc)
    
print('SparkContext:', sc)
print('HiveContext:', hc)",0.5054114461,
346,create a spark context and import dependencies,"if __name__ == '__main__':
    conf = (SparkConf().setAppName(""TwitterSA"").set(""spark.executor.memory"", ""6g""))
    #sc = SparkContext(conf=conf)
    sc = pyspark.SparkContext('local[*]')
    sqlContext = SQLContext(sc)
    input_train = ""train.csv""
    input_test = ""test.csv""

    raw_train = sc.textFile(input_train)
    print('## Parsing Train Data...')
    train_data = raw_train.map(parse_line)
    print('## Tokenizing Train Data...')
    train_data = train_data.map(lambda x: (tokenizer(x[0]),x[1]))
    train_data.cache()
    # print(train_data.take(3))

    raw_test = sc.textFile(input_test)
    print('## Parsing Test Data...')
    test_data = raw_test.map(parse_line)
    print('## Tokenizing Test Data...')
    test_data = test_data.map(lambda x: (tokenizer(x[0]),x[1]))
    test_data.cache()
    # print(test_data.take(3))",0.4860141873,
346,create a spark context and import dependencies,"def real_main():
    sc = pyspark.SparkContext()
    company = _init_list(sc)
    dataRDD1 = sc.textFile(""gs://group688/nytimes"",5)
    dataRDD1 = dataRDD1.mapPartitions(lambda x:_data_filter(x,company,""nytimes""))
    dataRDD2 = sc.textFile(""gs://group688/wsj"",10)
    dataRDD2 = dataRDD2.mapPartitions(lambda x:_data_filter(x,company,""wsj""))
    dataRDD3 = sc.textFile(""gs://group688/reuters.dat"",10)
    dataRDD3 = dataRDD3.mapPartitions(lambda x:_data_filter(x,company,""reuters""))
    dataRDD  = dataRDD3.union(dataRDD2).union(dataRDD1)
    dataRDD.sortByKey().map(lambda x:x[1]).saveAsTextFile(""gs://group688/688v1"")

real_main()",0.4808449745,
346,create a spark context and import dependencies,"# Set Spark Session as entry point
spark = SparkSession.builder\
                    .appName(""Simple recommendation engine using Spark MLlib"")\
                    .config(""spark.some.config.option"", ""config-value"")\
                    .getOrCreate()\",0.4626272619,
346,create a spark context and import dependencies,"#""""""
#Load packages and create context objects...
#""""""
import os
import platform
import sys
import pandas

if not 'sc' in vars():
    print """"""
***********************************************
*** Warning: Spark needs to be initialized ****
***********************************************
    """"""
else:
    print(""""""Already running
          ____              __
         / __/__  ___ _____/ /__
        _\ \/ _ \/ _ `/ __/  '_/
       /__ / .__/\_,_/_/ /_/\_\   version %s
          /_/
    """""" % sc.version)

if not 'sqlCtx' in vars():
    sqlCtx = SQLContext(sc)
print 'Spark Context available as `sc`'
print 'Spark SQL Context (%s) available as `sqlCtx`'%str(type(sqlCtx))
### The spark context doesn't really know the URL to the job manager... this would be different on every system
print ""Monitor this application at http://data-science-02.atl.primedia.com:8088/proxy/""+sc.applicationId",0.446407795,
346,create a spark context and import dependencies,"#Init the spark session
spark = SparkSession.builder\
               .appName(""SparkSession"")\
               .enableHiveSupport()\
               .getOrCreate()\",0.4460714757,
346,create a spark context and import dependencies,"# Import PySpark stuff
    from pyspark.sql import *
    from pyspark.sql.functions import udf, asc, desc
    from pyspark import SparkContext, SparkConf
    from pyspark.sql.types import IntegerType",0.4377424717,
346,create a spark context and import dependencies,"def startSparkContext(max_cores=16):
    from setupSpark import initSpark
    
    executor_cores = 8 # the number of cores to be used on each worker
    executor_memory = '25G' # the amount of memory to be used on each worker
    max_cores = max_cores # the max. number of cores Spark is allowed to use overall

    # returns the SparkContext object 'sc' which tells Spark how to access the cluster
    sc = initSpark(nbBackend, executor_cores=executor_cores, \
                   max_cores=max_cores, executor_memory=executor_memory)
    
    # provide OpenStack credentials to the Spark Hadoop configuration
    sc._jsc.hadoopConfiguration().set('fs.swift.service.SparkTest.username', os_username)
    sc._jsc.hadoopConfiguration().set('fs.swift.service.SparkTest.tenant', os_tenant_name)
    sc._jsc.hadoopConfiguration().set('fs.swift.service.SparkTest.password', os_password)
    
    # add Python files in 'utils' folder to the SparkContext 
    # this is required so that all files are available on all the cluster workers
    for filename in os.listdir(utils_dir):
        if filename.endswith('.py'):
            sc.addPyFile(os.path.join(utils_dir, filename))
            
    return sc",0.4304051399,
346,create a spark context and import dependencies,"def mars_news():
    browser = init_browser()
    news_url = 'https://mars.nasa.gov/news'  
    browser.visit(news_url)
    browser.find_by_name('content_title')
    browser.click_link_by_partial_href('news/')
    html = browser.html
    soup = bs(html, 'html.parser')
    news_title = soup.select_one('h1.article_title').text
    news_p = soup.select_one('div.wysiwyg_content').text
    return news_title, news_p",0.4300259948,
346,create a spark context and import dependencies,"def email_data(data_list):
    from sparkpost import SparkPost

#     print('//AA\n{}\n//ZZ'.format('\n'.join(data_list)))

    # Send email using the SparkPost api
    sp = SparkPost() # uses environment variable named SPARKPOST_API_KEY

    response = sp.transmission.send(
            recipients=['practicedata@globe.gov'],
            bcc=['aroller@ucar.edu'],
            text='//AA\n{}\n//ZZ'.format('\n'.join(data_list)),
            from_email='roller@rollercomsolutions.com',
            subject='DATA'
    )

    print(response)",0.4294306636,
1635,problem optimizing the likelihood,"# finding best hyperparams for E Wail
    bestlearningRateE=optimize(modelRE,spr)
    bestBatchHiddenNeuronsE=optimize(modelHBE,sphb)
    
    #finding best hyperparams for S  Jean
    bestlearningRateS=optimize(modelRS,spr)
    bestBatchHiddenNeuronsS=optimize(modelHBS,sphb)
    
    
    print(""hyperparams optimization take too long "")",0.4053129554,
1635,problem optimizing the likelihood,"class QLearningDecisionPolicy(DecisionPolicy):
    def __init__(self, actions, input_dim):
        self.epsilon = 0.9
        self.gamma = 0.001
        self.actions = actions
        output_dim = len(actions)
        h1_dim = 200

        self.x = tf.placeholder(tf.float32, [None, input_dim])
        self.y = tf.placeholder(tf.float32, [output_dim])
        W1 = tf.Variable(tf.random_normal([input_dim, h1_dim]))
        b1 = tf.Variable(tf.constant(0.1, shape=[h1_dim]))
        h1 = tf.nn.relu(tf.matmul(self.x, W1) + b1)
        W2 = tf.Variable(tf.random_normal([h1_dim, output_dim]))
        b2 = tf.Variable(tf.constant(0.1, shape=[output_dim]))
        self.q = tf.nn.relu(tf.matmul(h1, W2) + b2)

        loss = tf.square(self.y - self.q)
        self.train_op = tf.train.AdagradOptimizer(0.01).minimize(loss)
        self.sess = tf.Session()
        self.sess.run(tf.global_variables_initializer())

    def select_action(self, current_state, step):
        threshold = min(self.epsilon, step / 1000.)
        if random.random() < threshold:
            # Exploit best option with probability epsilon
            action_q_vals = self.sess.run(self.q, feed_dict={self.x: current_state})
            action_idx = np.argmax(action_q_vals)  # TODO: replace w/ tensorflow's argmax
            action = self.actions[action_idx]
        else:
            # Explore random option with probability 1 - epsilon
            action = self.actions[random.randint(0, len(self.actions) - 1)]
        return action

    def update_q(self, state, action, reward, next_state):
        action_q_vals = self.sess.run(self.q, feed_dict={self.x: state})
        next_action_q_vals = self.sess.run(self.q, feed_dict={self.x: next_state})
        next_action_idx = np.argmax(next_action_q_vals)
        action_q_vals[0, next_action_idx] = reward + self.gamma * next_action_q_vals[0, next_action_idx]
        action_q_vals = np.squeeze(np.asarray(action_q_vals))
        self.sess.run(self.train_op, feed_dict={self.x: state, self.y: action_q_vals})",0.3824713826,
1635,problem optimizing the likelihood,"from scipy.optimize import minimize

def neg_log_like(params, y, gp):
    gp.set_parameter_vector(params)
    return -gp.log_likelihood(y)

initial_params = gp.get_parameter_vector()
bounds = gp.get_parameter_bounds()

r = minimize(neg_log_like, initial_params, method=""L-BFGS-B"", bounds=bounds, args=(y, gp))
gp.set_parameter_vector(r.x)
print(r)",0.3806357384,
1635,problem optimizing the likelihood,"results = []
learning_rate = None
for t in range(500):
    # Forward pass and log loss



    # Zero the gradients


    # Backward pass: compute gradient of the loss with respect to all the learnable
    # parameters of the model. 


    # Update the weights using gradient descent. Each parameter is a Variable, so
    # we can access its data and gradients like we did before.",0.3803599477,
1635,problem optimizing the likelihood,"NatGradOptimizer(gamma=1.).minimize(vgp, maxiter=1, var_list=[[vgp.q_mu, vgp.q_sqrt]])
vgp.compute_log_likelihood()",0.3776371777,
1635,problem optimizing the likelihood,"def sample_Z():
    global epsilon
    global encoder_mean, encoder_log_var
    
    return encoder_mean + tf.exp(encoder_log_var / 2) * epsilon",0.3767397106,
1635,problem optimizing the likelihood,"def nll_grad(mlp_model):
    loss = mlp_model.negative_log_likelihood()
    params = mlp_model.params
    grads = theano.grad(loss, wrt=params)
    # Return (param, grad) pairs
    return zip(params, grads)

def sgd_updates(params_and_grads, learning_rate):
    return [(param, param - learning_rate * grad)
            for param, grad in params_and_grads]

def get_simple_training_fn(mlp_model, learning_rate):
    inputs = [mlp_model.input, mlp_model.target]
    params_and_grads = nll_grad(mlp_model)
    updates = sgd_updates(params_and_grads, learning_rate=lr)
    
    return theano.function(inputs=inputs, outputs=[], updates=updates)",0.3765130639,
1635,problem optimizing the likelihood,"class Q_learning:
    '''  Q-Learning Algorithm
    
    Utilizes Q-learning (reinforcement learning) to find optimal path given a graph with reward/weighted edges.
    Can take pre-made graphs or can produce randomly-generated ones. Currently, reward matrix has following attributes:
        -1 = no edge present
        0 = edge without reward (not connected to goal node)
        100 = edge with reward (connected to goal node)
    Fit with `learn` and find optimal path with `predict_path`
    
    Parameters
    ---
    gamma : float, [0,1], (default = 0.8)
            discount factor, trades off the importance of earlier versus later rewards,
            may also be interpreted as the probability to succeed (or survive) at every step.
    
    epochs : int, (default = 1000)
            number of iterations the algorithm runs and thus learns
            
    Attributes
    ---
    q_matrix_ : array, shape = reward.shape
            ""algorithm's brain"", matrix of different (state, action) Q-scores,
            higher scores correspond to optimal path for given state
    '''
    def __init__(self, gamma=0.8, epochs=1000):
        
        self.gamma = gamma
        self.epochs = epochs
    
    
    def random_graph(self, nodes=6, edges=8):
        '''
        Create random graph with n nodes and e edges. Every node has at least one connection,
        every edge is bi-directional, the final node is always treated as the goal state.
        
        Parameters
        ---
        nodes : int, (default = 6)
                number of nodes, must be less than or equal to the number of edges
                
        edges : int, (default = 8)
                number of edges, can be equal to or at most two times the number of nodes
                
        Attributes
        ---
        graph : nx.Graph.dense_gnm_random_graph object
                Randomly generated graph
                
        Return
        ---
        matrix : numpy array, shape = [nodes, nodes]
                Matrix representation of graph, w/ pre-stated reward attributes
        '''
        while True:
            self.graph = nx.dense_gnm_random_graph(nodes, edges)
            matrix = nx.to_numpy_array(self.graph)
            if 0 not in np.sum(matrix, axis=1):
                break
        
        matrix = matrix - 1
        matrix[:, -1][matrix[:, -1] == 0] = 100
        matrix[-1,-1] = 100
        
        return matrix
    
    
    def print_graph(self, matrix=None):
        '''
        Illustrates randomly-generated graph or pre-made reward matrix.
        
        Parameters
        ---
        matrix : array, (default = None)
                pre-made reward matrix, needs to be in pre-stated format (no edge = -1, edge w/o reward = 0)
        '''
        if np.any(matrix):
            matrix[matrix == 100] = 0
            matrix = matrix + 1
            nx.draw_networkx(nx.from_numpy_array(matrix))
        else:
            nx.draw_networkx(self.graph)
      
    
    def _actions(self, reward, state):
        '''
        Finds all possible actions for given state. (Helper function for the learn method)
        
        Parameters
        ---
        reward : array
                graph matrix w/ edges and corresponding rewards/weights
        
        state : int
                current state in graph/reward matrix
        
        Return
        ---
        (list) all possible actions
        '''
        return np.argwhere(reward[state] >= 0).ravel()
    
    
    def _q_score(self, reward, state, action):
        '''
        Calculate the Q-score for the given (state, action) pair (Helper function for the learn method)
            Q(state, action) = Reward(state, action) + gamma * max[Q(next_state, all actions)],
            where next_state = current action
            
        Parameters
        ---
        reward : array
                graph matrix w/ edges and corresponding rewards/weights
        
        state : int
                current state in graph/reward matrix
        
        action : int
                selected action, which will become next state
        
        Return
        ---
        (float) Q-score
        '''
        next_actions = self._actions(reward, action)
        return reward[state, action] + self.gamma*max([self.q_matrix_[action, a] for a in next_actions])
    
    
    def learn(self, reward, goal=None):
        '''
        Q-learning algorithm. Initializes Q-score matrix and updates during each iteration.
        At end, standardizes q_matrix_ by the maximum score [0-100]
        
        Parameters
        ---
        reward : array
                either pre-made reward matrix or randomly-generated one
                
        goal : int, (default = None)
                terminal/goal node, corresponds to the `goal`-th column in reward
                either user-given or simply assigned to the last node/column if not
        '''
        self.q_matrix_ = np.zeros(reward.shape)
        self.goal_ = goal
        
        if not self.goal_:
            self.goal_ = reward.shape[0] - 1
        
        for _ in range(self.epochs):

            state = random.randint(0, self.goal_)
            next_state = None

            while next_state != self.goal_:

                next_state = random.choice(self._actions(reward, state))
                q = self._q_score(reward, state, next_state)
                
                if q > self.q_matrix_[state, next_state]:
                    self.q_matrix_[state, next_state] = q
                    
                state = next_state
                
        self.q_matrix_ = self.q_matrix_ * (100 / np.amax(self.q_matrix_))
        
        
    def predict_path(self, start_node=0):
        '''
        Find the optimal path after learning the graph.
        Given a start node, use the Q-score-matrix to get to goal node. (Prints path)
        
        Parameters
        ---
        start_node : int, (default = 0)
                starting point, can be any node within graph
                
        Return
        ---
        path : list
                optimal path
        '''
        path = [start_node]
        state = start_node
        
        while state != self.goal_:
            options = np.argwhere(self.q_matrix_[state] == np.amax(self.q_matrix_[state])).ravel()
            state = random.choice(options)
            path.append(state)
        
        for i, node in enumerate(path):
            print(node, end=' ')
            if i < len(path)-1:
                print('-->', end=' ')
                
        return path",0.3762456775,
1635,problem optimizing the likelihood,"def update():
    ""One update step of network, update outputs and weightings""
    global W_i,W_h,S_i,S_h,v_n
    o = np.tanh(np.multiply(np.concatenate((W_i,W_h),axis=1),np.concatenate((S_i,S_h),axis=1)).dot(np.concatenate((v_i,v_n),axis=0)))
    dt = o-v_n  # Change in nueron output
    W_h = W_h+np.tanh(LEARN*np.tensordot(dt,dt)) # Update weights of hidden layer 
    v_n = o",0.3757172823,
1635,problem optimizing the likelihood,"for i in range(3000): #(1000 iterations of Gradient Descent)
    pred=np.dot(w,X.T)+b #(Prediction Vector)
    J=(1/(2*m))*(np.sum(pred-y)**2) #(Cost Function)
    print (J) #printing cost function after each iteration to see whether it's decreasing or not
    dw=np.dot((pred-y.T),X) #(Result after partial differentiation W.r.t. W)
    db=np.sum(pred-y)       #(Result after partial differentiation W.r.t. W)
    w=w-alpha*(1/m)*np.dot((pred-y.T),X) #(Weight Vector updated)
    b=b-alpha*np.sum(pred-y)*(1/m)",0.3751063645,
1841,sales data,"# function to get training samples
def get_training_data():
    # extract training samples
    df_train = df_model.loc[id_train]
    
    # split SalePrice and features
    y = df_train.SalePrice
    X = df_train.drop('SalePrice',axis=1)
    
    return X, y

# extract test data (without SalePrice)
def get_test_data():
    return df_model.loc[id_test].drop('SalePrice',axis=1)",0.5468786955,
1841,sales data,"sc.textFile('sales.txt')\
    .map(lambda x: x.split())\
    .filter(lambda x: not x[0].startswith('#'))\
    .map(lambda x: (x[-3],(float(x[-1]),1)))\
    .reduceByKey(lambda (amount1,count1),(amount2,count2): \
        (amount1+amount2, count1+count2))\
    .collect()",0.5388697386,
1841,sales data,"def pull_data():
    symblist=['AAPL','GOOG','NFLX']
    price=[]
    date= datetime.datetime.utcnow()
    for sym in symblist:
    
        url = ""http://finance.yahoo.com/q;_ylt=AqAeVPItbXuQ1azBg40Rso3FgfME?uhb=uhb2&fr=uh3_finance_vert_gs_ctrl2_e&type=2button&s=""+sym
    
        htmlfile=urllib.urlopen(url)
        htmltext=htmlfile.read()
                        )    
        regex='<span id=""yfs_l84_'+sym.lower()+'"">(.+?)</span>'
        pattern=re.compile(regex)
        price.append(re.findall(pattern,htmltext))
        
    return date, price",0.5360344648,
1841,sales data,"sc.textFile('sales.txt')\
    .map(lambda x: x.split())\
    .filter(lambda x: not x[0].startswith('#'))\
    .map(lambda (id,date,store,state,product,amount): (state,float(amount)))\
    .reduceByKey(lambda amount1,amount2: amount1+amount2)\
    .sortBy(lambda (state,amount):amount,ascending=False) \
    .collect()",0.5292018056,
1841,sales data,"def get_data():
    
    data = pd.read_csv('./data/train.csv')
    
    y = data.SalePrice
    X = data[cols_to_use]
    
    my_imputer = Imputer()
    imputed_X = my_imputer.fit_transform(X)
    
    return imputed_X, y",0.5261948109,
1841,sales data,"def compile_stores(store_numbers):
    """"""Group the sales daily_sales_data of a list of stores""""""
    # Empty daily_sales_dataframe
    filtered_stores = daily_sales_data[daily_sales_data.Sales == -8384]
    for j in store_numbers:
        matches = daily_sales_data[daily_sales_data.Store == j]
        filtered_stores = filtered_stores.append(matches)
    return filtered_stores

def group_by_store_type(store_type, exclude_no_sales=False, attribute='StoreType'):
    store_numbers = list(store_data[store_data[attribute] == store_type]['Store'])
    if exclude_no_sales:
        store_numbers = store_numbers[store_numbers.Sales > 0]
    matches = compile_stores(store_numbers)
    return matches

def plot_sales_averages(type_groups, labels=('A', 'B', 'C', 'D'), sale_color='r', customer_color='y', legend=True):
    """"""Plot the daily_sales_data""""""
    indices = np.arange(len(type_groups))
    sales_means = [x.describe()['Sales']['mean'] for x in type_groups]
    customer_means = [x.describe()['Customers']['mean'] for x in type_groups]
    WIDTH = 0.35
    fig, ax = plt.subplots()
    sales_bars = ax.bar(indices, sales_means, WIDTH, color=sale_color)
    customer_bars = ax.bar(indices + WIDTH, customer_means, WIDTH, color=customer_color)
    plt.xticks(indices, labels)
    if legend:
        plt.legend((sales_bars[0], customer_bars[0]), ('Average Sales', 'Average Shoppers'))
    plt.figure()
    sales_daily_sales_data = [np.array(x['Sales']) for x in type_groups]
    plt.boxplot(sales_daily_sales_data, 0 , 'rs')
    plt.xticks(1 + indices,(labels))
    plt.show()",0.5214542747,
1841,sales data,"def Xy(df):
    X = df[ ['Size'] ] # X is a DataFrame

    y = df.SalePrice # y is a Series

    return X, y

X, y = Xy(df)

model = smf.OLS(y, X).fit()

model.summary()",0.5186806917,
1841,sales data,"def Xy(df):
    df = df.dropna(subset = ['Size', 'SalePrice'])
    X = df[ ['Size'] ] # X is a DataFrame

    y = df.SalePrice # y is a Series

    return X, y

X, y = Xy(df)

model = smf.OLS(y, X).fit()

model.summary()",0.515591681,
1841,sales data,"def _evaluate(model):
    """"""
    Return score and model predictions
    """"""
    
    yi, Xs, y = udata.get_seasons_data(ad, components, [None], TARGET, REGION)
    predictions = np.zeros_like(Xs[0])

    for i in range(len(yi)):
        # HACK: Check if this is an oracle
        # This should ideally go in as a flag in the model
        if ""truth"" in inspect.signature(model.predict).parameters:
            # This is an oracle
            predictions[i, :] = model.predict(yi.iloc[i], [X[i] for X in Xs], y[i])
        else:
            predictions[i, :] = model.predict(yi.iloc[i], [X[i] for X in Xs]) 
        # Pass in feedback if model accepts it
        try:
            model.feedback(y[i])
        except NotImplementedError:
            pass

    score = np.log(udists.prediction_probabilities([predictions], y, TARGET)).mean()
    return score, predictions

def evaluate(model, post_training_hook=None):
    """"""
    Evaluate on the testing seasons
    """"""
    
    # Need to reset the model before every evaluation
    # TODO: This should be done even when not using a hook
    if post_training_hook:
        model = post_training_hook(model)
    score, _= _evaluate(model)
    return score

def load_model(model):
    """"""
    Load weights from saved
    """"""
    
    model_file_name = f""{stringcase.spinalcase(type(model).__name__)}.json""
    model.load(path.join(INPUT_DIR, TARGET, model_file_name))
    return model",0.5136256218,
1841,sales data,"rdd = sc.textFile('sales.csv')\
    .filter(lambda line: not line.startswith('#'))\
    .map(lambda line: line.split(','))\
    .map(lambda \
      (id,date,store,state,product,amount):\
      (int(id),date,int(store),state,int(product),float(amount)))
rdd.collect()",0.5107489824,
727,generate some sample data point,"# Mutates a multiple real-value
# genome
def mutate_genome(genome):
    
    # Select one of the real-values uniform randomly
    geneToMutateIdx = random.randint(len(genome.genes))
    
    # Perturbs that value by adding random value in [-0.01, 0.01]
    genome.genes[geneToMutateIdx] += random.uniform(-0.01, 0.01)",0.513582468,
727,generate some sample data point,"# Mutates a multiple real-value
# genome
def mutate_genome(genome):
    
    # Select one of the real-values uniform randomly
    geneToMutateIdx = random.randint(len(genome.genes))
    
    # Perturbs that value by adding random value in [-10, 10]
    genome.genes[geneToMutateIdx] += random.uniform(-10, 10)",0.5052052736,
727,generate some sample data point,"# Mutation takes a genome and alters 
# its genes to create a new solution instance
def mutate_genome(genome): 
    
    # This mutatation adds a value between 
    # -0.01 and 0.01 to the genome's genes
    genome.genes += random.uniform(-0.01, 0.01)",0.4889309108,
727,generate some sample data point,"power = 0.1

# Mutates a multiple real-value
# genome
def mutate_genome(genome):
    
    # Select one of the real-values uniform randomly
    geneToMutateIdx = random.randint(len(genome.genes))
    
    # Perturbs that value by adding random value in [-power, power]
    genome.genes[geneToMutateIdx] += random.uniform(-power, power)",0.4767102599,
727,generate some sample data point,"sigma = 0.1

# Mutates a multiple real-value
# genome
def mutate_genome(genome):
    
    # Select one of the real-values uniform randomly
    geneToMutateIdx = random.randint(len(genome.genes))
    
    # Perturbs that value by adding random value in [-power, power]
    genome.genes[geneToMutateIdx] += random.normal(sigma)",0.4767102599,
727,generate some sample data point,"def step(walker):
    new_pos = walker.pos + 2.0*(random.random() - 0.5)
    while new_pos < walker._min_val or new_pos > walker._max_val:
        new_pos = walker.pos + 2.0*(random.random() - 0.5)
    walker._pos = new_pos
    return walker.pos",0.4487109184,
727,generate some sample data point,"def create_sentence(data, model, diversity = 1.2, num_of_words = 25, verbose = 1):
    start_index = random.randint(0, len(data.list_words) - maxlen - 1)
    if verbose == 1:
        print(""Create Sentence.."")
        print('Diversity:', diversity)
    generated = ''
    sentence = data.list_words[start_index: start_index + 10]
    seed = ' '.join(sentence)
    if verbose == 1:
        print('Generating with seed:')
        print(seed)       
        
    for i in range(num_of_words):
        x = np.zeros((1, maxlen, len(data.words)))
        for t, word in enumerate(sentence):
            x[0, t, data.word_indices[word]] = 1.

        preds = model.predict(x, verbose=0)[0]
        next_index = sample(preds, diversity)
        next_word = data.indices_word[next_index]
        generated += next_word
        generated += "" ""
        del sentence[0]
        sentence.append(next_word)
    if verbose == 1:
        print(""The sentence:"")
        print(generated)
    return generated",0.4452375174,
727,generate some sample data point,"def fridge0(idx,state,t_state,T,U):
    t_state+=1
    if t_state > T:
        state = not state
        t_state = 0 + np.random.randint(10)
    return state,t_state",0.4365232587,
727,generate some sample data point,"def optimize_model():
    if len(memory) < BATCH_SIZE * 12:
        return

    transitions = memory.sample(BATCH_SIZE)
    # Transpose the batch (see http://stackoverflow.com/a/19343/3343043 for
    # detailed explanation).
    batch = Transition(*zip(*transitions))

    # Compute a mask of non-final states and concatenate the batch elements
    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,
                                          batch.next_state)), device=device, dtype=torch.uint8)
    non_final_next_states = torch.cat([s.to(device=device) for s in batch.next_state
                                                if s is not None])
    state_batch = torch.cat(batch.state).to(device=device)
    action_batch = torch.cat(batch.action).to(device=device)
    reward_batch = torch.cat(batch.reward).to(device=device)

    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the
    # columns of actions taken
    state_action_values = policy_net(state_batch).gather(1, action_batch)

    # Compute V(s_{t+1}) for all next states.
    next_state_values = torch.zeros(BATCH_SIZE, device=device)
    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()
    # Compute the expected Q values
    temp = next_state_values * GAMMA
    expected_state_action_values = (temp) + reward_batch

    # Compute Huber loss
    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))

    # Optimize the model
    optimizer.zero_grad()
    loss.backward()
    for param in policy_net.parameters():
        param.grad.data.clamp_(-1, 1)
    optimizer.step()",0.435192436,
727,generate some sample data point,"def get_binary_lane_image(img, line, window_center, width=300):
    if line.detected:
        window_center=line.line_pos
    else:
        peaks, filtered = find_peaks(img,thresh=3000)
        if len(peaks)!=2:
            print('Trouble ahead! '+ str(len(peaks)) +' lanes detected!')
            plt.imsave('troublesome_image_3.jpg',img)                        
        
        if len(peaks)==0:
            print('Trouble ahead! '+ str(len(peaks)) +' lanes detected!')
            plt.imsave('troublesome_image_0.jpg',img)
            peaks, filtered = find_peaks(img,thresh=2000)
            
        peak_ind = np.argmin(abs(peaks-window_center))
        peak  = peaks[peak_ind]
        window_center = peak
    
    lane_binary = lane_from_window(img,window_center,width)
    return lane_binary",0.435039252,
976,introduction to scikit learn,"from sklearn import preprocessing

# Train, Validation, Test Split
def loadDataSplit():
    df_train = pd.read_csv(BASE_FOLDER + TRAINING_DATA_CSV)
    # TOURNAMENT_DATA_CSV has both validation and test data provided by NumerAI
    df_test_valid = pd.read_csv(BASE_FOLDER + TOURNAMENT_DATA_CSV)

    answers_1_SINGLE = df_train[TARGET_VAR]
    df_train.drop(TARGET_VAR, axis=1,inplace=True)
    df_train.drop('id', axis=1,inplace=True)
    df_train.drop('era', axis=1,inplace=True)
    df_train.drop('data_type', axis=1,inplace=True)    
    
#     df_train=oneHOT(df_train)

    df_train.to_csv(BASE_FOLDER + TRAINING_DATA_CSV + 'clean.csv', header=False,  index = False)    
    df_train= pd.read_csv(BASE_FOLDER + TRAINING_DATA_CSV + 'clean.csv', header=None, dtype=np.float32)    
    df_train = pd.concat([df_train, answers_1_SINGLE], axis=1)
    feature_cols = list(df_train.columns[:-1])
#     print (feature_cols)
    target_col = df_train.columns[-1]
    trainX, trainY = df_train[feature_cols], df_train[target_col]
    
    
    # TOURNAMENT_DATA_CSV has both validation and test data provided by NumerAI
    # Validation set
    df_validation_set=df_test_valid.loc[df_test_valid['data_type'] == 'validation'] 
    df_validation_set=df_validation_set.copy(deep=True)
    answers_1_SINGLE_validation = df_validation_set[TARGET_VAR]
    df_validation_set.drop(TARGET_VAR, axis=1,inplace=True)    
    df_validation_set.drop('id', axis=1,inplace=True)
    df_validation_set.drop('era', axis=1,inplace=True)
    df_validation_set.drop('data_type', axis=1,inplace=True)
    
#     df_validation_set=oneHOT(df_validation_set)
    
    df_validation_set.to_csv(BASE_FOLDER + TRAINING_DATA_CSV + '-validation-clean.csv', header=False,  index = False)    
    df_validation_set= pd.read_csv(BASE_FOLDER + TRAINING_DATA_CSV + '-validation-clean.csv', header=None, dtype=np.float32)    
    df_validation_set = pd.concat([df_validation_set, answers_1_SINGLE_validation], axis=1)
    feature_cols = list(df_validation_set.columns[:-1])

    target_col = df_validation_set.columns[-1]
    valX, valY = df_validation_set[feature_cols], df_validation_set[target_col]
                            
    # Test set for submission (not labeled)    
    df_test_set = pd.read_csv(BASE_FOLDER + TOURNAMENT_DATA_CSV)
#     df_test_set=df_test_set.loc[df_test_valid['data_type'] == 'live'] 
    df_test_set=df_test_set.copy(deep=True)
    df_test_set.drop(TARGET_VAR, axis=1,inplace=True)
    tid_1_SINGLE = df_test_set['id']
    df_test_set.drop('id', axis=1,inplace=True)
    df_test_set.drop('era', axis=1,inplace=True)
    df_test_set.drop('data_type', axis=1,inplace=True)   
    
#     df_test_set=oneHOT(df_validation_set)
    
    feature_cols = list(df_test_set.columns) # must be run here, we dont want the ID    
#     print (feature_cols)
    df_test_set = pd.concat([tid_1_SINGLE, df_test_set], axis=1)            
    testX = df_test_set[feature_cols].values
        
    return trainX, trainY, valX, valY, testX, df_test_set",0.500280261,
976,introduction to scikit learn,"def print_model_fit(model):
    # Print out the parameters for the best fit line
    print('Intercept: {i}  Slope: {c}'.format(i=model.intercept_, c=model.coef_))",0.4933142066,
976,introduction to scikit learn,"def print_model_fit(model):
    # Print out the parameters for the best fit line
    print('Intercept: {i}  Coefficients: {c}'.format(i=model.intercept_, c=model.coef_))",0.4933142066,
976,introduction to scikit learn,"def undersample_data(data):

    fraud_indices = np.array(data[data.Class == 1].index)
    print 'Number of frauds', len(fraud_indices)

    non_fraud = data[data.Class==0]
    fraud = data[data.Class==1]

    print 'number of non fraud: ', len(non_fraud)
    non_fraud = non_fraud.loc[np.random.choice(non_fraud.index, len(fraud_indices), replace=False)]

    undersampled_data = pd.concat([non_fraud, fraud])
    print 'non_fraud after: ', len(non_fraud)

    # Assign variables x and y corresponding to row data and it's class value
    X = undersampled_data.loc[:, data.columns != 'Class']
    y = undersampled_data.loc[:, data.columns == 'Class']
    
    return X, y",0.4879238605,
976,introduction to scikit learn,"def test(isTrue): #Runs CNN on test images
    pred=fitAndPredict()
    print(""creating test file"")
    df = pd.DataFrame(pred, columns=['Type_1','Type_2','Type_3']) #Instantiates dataframe
    df['image_name'] = test_id #image_name holds the .jpg file name
    if (isTrue): #if(True), it will create a .csv file with the dataframe
        df.to_csv('test.csv', index=False)
        print(""Test file created in users/keerat/..."")
    else: #if(False), it will just show the dataframe
        print(df.to_string())",0.485732466,
976,introduction to scikit learn,"## Defining an evaluation metric based on (http://scikit-learn.org/stable/modules/model_evaluation.html)
from sklearn.metrics import classification_report
def evaluateClassif(clf):
    classes=['Non-POI','POI']  ## Defining the classes labels
    predTrain = clf.predict(features_train)
    print('################### Training data ##################')
    print(classification_report(labels_train, predTrain, target_names=classes))
    
    predTest = clf.predict(features_test)
    print('################### Testing data ###################')
    print(classification_report(labels_test, predTest, target_names=classes))
    
    return",0.4826464653,
976,introduction to scikit learn,"#function to return all the models for a given ticker
from sklearn.preprocessing import Imputer
    
def allModelsResultForAllStocks():
    
    best_result_per_ticker = pd.DataFrame(columns=['Ticker','Model','RMSE'])
    ticker_list = np.unique(stock[""Name""])
    best_result_per_ticker = list()
    for ticker_name in ticker_list:
        result = pd.DataFrame(columns=['Ticker','Model','RMSE'])
        stock_a = stock[stock['Name'] == ticker_name]
        #Adding new features 
        #1 Price movement during day time 
        stock_a['changeduringday'] = ((stock['high'] - stock['low'] )/ stock['low'])*100

        #2 Price movement 
        stock_a['changefrompreviousday'] = (abs(stock_a['close'].shift() - stock_a['close'] )/ stock['close'])*100

        X_stock_a = stock_a.drop(['date', 'Name','close'], axis=1)
        y_stock_a = stock_a['close']

        
        imputer = Imputer(missing_values='NaN', strategy='median')
        
        imputer.fit_transform(X_stock_a)
       
        X_stock_train, X_stock_test, y_stock_train, y_stock_test = train_test_split(X_stock_a, y_stock_a, test_size=0.2, 
                                                                                random_state=42)


        Lr_pipeline_std.fit(X_stock_train, y_stock_train)
        Lr_pipeline_nor.fit(X_stock_train, y_stock_train)

        svr_pipeline_nor.fit(X_stock_train, y_stock_train)
        svr_pipeline_std.fit(X_stock_train, y_stock_train)

        svrrbf_pipeline_nor.fit(X_stock_train, y_stock_train)
        svrrbf_pipeline_std.fit(X_stock_train, y_stock_train)


        dt_pipeline_nor.fit(X_stock_train, y_stock_train)
        dt_pipeline_std.fit(X_stock_train, y_stock_train)    
   
        # Predict & Calculate RMSE for all the models 

        #Linear Regression with normalisation and standardisation
        lr_stock_predictions_nor = Lr_pipeline_nor.predict(X_stock_test)
        lr_mse_nor = mean_squared_error(y_stock_test, lr_stock_predictions_nor)
        lr_rmse_nor = np.sqrt(lr_mse_nor)
        rmse_row =   [ticker_name,'Lr RMSE with Normalization', lr_rmse_nor]

        result.loc[-1] = rmse_row  # adding a row
        result.index = result.index + 1  # shifting index
     
    
        lr_stock_predictions_std = Lr_pipeline_std.predict(X_stock_test)
        lr_mse_std = mean_squared_error(y_stock_test, lr_stock_predictions_std)
        lr_rmse_std = np.sqrt(lr_mse_std)
        rmse_row =   [ticker_name,'Lr RMSE with standardization', lr_rmse_std]
    
    

        result.loc[-1] = rmse_row  # adding a row
        result.index = result.index + 1  # shifting index

        #SVM with normalisation and standardisation
        svm_stock_predictions_nor = svr_pipeline_nor.predict(X_stock_test)
        svm_mse_nor = mean_squared_error(y_stock_test, svm_stock_predictions_nor)
        svm_rmse_nor = np.sqrt(svm_mse_nor)
        rmse_row =   [ticker_name,'SVM RMSE with Normalization', svm_rmse_nor]
        

        result.loc[-1] = rmse_row  # adding a row
        result.index = result.index + 1  # shifting index

        svm_stock_predictions_std = svr_pipeline_std.predict(X_stock_test)
        svm_mse_std = mean_squared_error(y_stock_test, svm_stock_predictions_std)
        svm_rmse_std = np.sqrt(svm_mse_std)
        rmse_row =   [ticker_name,'SVM RMSE with standardization', svm_rmse_std]
    
        result.loc[-1] = rmse_row  # adding a row
        result.index = result.index + 1  # shifting index


        #SVM with RFB Kernel with normalisation and standardisation
        svmrbf_stock_predictions_nor = svrrbf_pipeline_nor.predict(X_stock_test)
        svmrbf_mse_nor = mean_squared_error(y_stock_test, svmrbf_stock_predictions_nor)
        svmrbf_rmse_nor = np.sqrt(svmrbf_mse_nor)
        rmse_row =   [ticker_name,'SVM RBF RMSE with Normalization', svmrbf_rmse_nor]
   
        result.loc[-1] = rmse_row  # adding a row
        result.index = result.index + 1  # shifting index


        svmrbf_stock_predictions_std = svrrbf_pipeline_std.predict(X_stock_test)
        svmrbf_mse_std = mean_squared_error(y_stock_test, svmrbf_stock_predictions_std)
        svmrbf_rmse_std = np.sqrt(svmrbf_mse_std)
        rmse_row =   [ticker_name,'SVM RBF RMSE with standardization', svmrbf_rmse_std]
    
        result.loc[-1] = rmse_row  # adding a row
        result.index = result.index + 1  # shifting index

        #Decision Tree with normalisation and standardisation
        dt_stock_predictions_nor = dt_pipeline_nor.predict(X_stock_test)
        dt_mse_nor = mean_squared_error(y_stock_test, dt_stock_predictions_nor)
        dt_rmse_nor = np.sqrt(dt_mse_nor)
        rmse_row =   [ticker_name,'DecisionTree RMSE with Normalization', dt_rmse_nor]

        result.loc[-1] = rmse_row  # adding a row
        result.index = result.index + 1  # shifting index

        dt_stock_predictions_std = dt_pipeline_std.predict(X_stock_test)
        dt_mse_std = mean_squared_error(y_stock_test, dt_stock_predictions_std)
        dt_rmse_std = np.sqrt(dt_mse_std)
        rmse_row = [ticker_name,'DecisionTree RMSE with standardization', dt_rmse_std]
 
        result.loc[-1] = rmse_row  # adding a row
        result.index = result.index + 1  # shifting index
        result = result.sort_values(by = ['RMSE'])
        
       
        best_result_per_ticker.append(np.array(result.iloc[0, :]))
       


    best_result_per_ticker_df = pd.DataFrame(data=best_result_per_ticker, columns=['Ticker','Model','RMSE'])
    
    
    return best_result_per_ticker_df

best_result_per_ticker = allModelsResultForAllStocks()",0.4818835855,
976,introduction to scikit learn,"def print_coef(model, predictors):
    '''
        custom function to print coefficients of LASSO model
    '''
    coef_vec = list(model.coef_)
    print(""%-16s"" % ""Predictor"", ""| "", ""Coefficient"")
    print(""-----------------------------------------"")
    for i,p in enumerate(coef_vec):
        print(""%-16s"" % predictors[i], "": "", p)

print_coef(model_all, all_features)",0.4803377986,
976,introduction to scikit learn,"def setupMinuit(params):
    print 'Setup Minuit...'
    PRINT_LEVEL = 1  # -1 => quiet, 1 => loud
    UP = 0.5 # 1: appropriate for 68% CL using chisq (use 0.5 for log-likelihood)
    npar = len(params)
    minuit = rt.TMinuit(npar)
    minuit.SetFCN(nnl)
    minuit.SetErrorDef(UP)
    minuit.SetPrintLevel(PRINT_LEVEL)

    status = rt.Long() # needed for integers passed by refence (int& ii)
    print ""%-20s %10s %10s %10s %10s"" % \
    ('param', 'guess', 'step', 'min', 'max')
    
    for ii, t in enumerate(params):
        print ""%-20s %10.2e %10.3e %10.3e %10.3e"" % t
        name, guess, step, pmin, pmax = t
        minuit.mnparm(ii, name, guess, step, pmin, pmax, status)
        if status != 0:
            sys.exit(""** mnparm(%s) status = %d"" % (name, status))
    return minuit",0.4777681828,
976,introduction to scikit learn,"from sklearn.svm import SVC

def len_of_doc():
    length_train = []
    length_test = []
    for text in X_train:
        length_train.append(len(text))
    for text in X_test:
        length_test.append(len(text))
    return length_train, length_test
    
def task_seven():
    
    length_train, length_test = len_of_doc()
    vect = TfidfVectorizer(min_df=5).fit(X_train)
    X_train_vectorized = vect.transform(X_train)
    
    
    X_train_vectorized = add_feature(X_train_vectorized, length_train)
    clfr = SVC(C=10000).fit(X_train_vectorized, y_train)
    
    
    X_test_vectorized = add_feature(vect.transform(X_test), length_test)
    
    predicted = clfr.predict(X_test_vectorized)
    
    return roc_auc_score(y_test, predicted)",0.4769052267,
890,import numpy as np,"import sys

def sample(preds, temperature=None):
    # helper function to sample an index from a probability array
    
    # If the user specifies a temperature, we use this to and some more randomization
    if temperature:
        preds = np.asarray(preds).astype('float64')
        preds = np.log(preds) / temperature
        exp_preds = np.exp(preds)
        preds = exp_preds / np.sum(exp_preds)
        probas = np.random.multinomial(1, preds, 1)
        return np.argmax(probas)
    # if not, we just return the most likely prediction. This is likely to get trapped in loops.
    else:
        return np.argmax(preds)",0.4505912066,
890,import numpy as np,"import numpy as np
def print_mean(array):
    if np.dtype(array) == ""float"" or np.dtype(array) == ""int"":
        print('Average: ' + str(np.mean(array))) 
    else:
        print(""Warning: value is non numeric"")",0.4463754594,
890,import numpy as np,"import numpy as np;
DTYPE = 'float32'

def perfect_data():
    input_array = np.array([ image2bits( image[digit] ) for digit in range(0,10) ]);
    output_array = np.array([ onehot(digit,10) for digit in range(0,10) ]);
    return input_array.astype(DTYPE), output_array.astype(DTYPE);",0.4451336265,
890,import numpy as np,"def sharpe_ratio(returns, risk_free=None):
    """"""
    Calculates Annualised Sharpe Ratio given a returns series 
    
    Parameters
    ----------
    returns : pandas series
    risk_free : pandas series. Required that this be in daily interest rate format.
    
    If data is from FRED. It is in pandas DataFrame format. Need to use `.squeeze()`
    method to convert it into pandas Series first.
    
    FRED data: 2018-02-15 1.56. 1.56 is in percentage annualised format. To convert
    to daily format, one needs to take 1.56 divide by 100 and 365. In mathematical 
    format:
    1.56 / 365 / 100
    
    Assumed default is daily returns

    Returns
    -------
    Sharpe Ratio : scalar
    
    Example:
    sharpe_ratio(returns, rf.squeeze() / 100 / 365)
    
    """"""
    import numpy as np
    
    if risk_free is not None:
        excess_rtn = excess_returns(returns, risk_free)
    else:
        excess_rtn = returns
    return excess_rtn.mean() / excess_rtn.std() * np.sqrt(TRADING_DAYS_PER_YEAR)",0.4437019229,
890,import numpy as np,"import theano
import theano.tensor as T
import numpy as np
def floatX(x):
    return np.asarray(x, dtype='float32')",0.4417713583,
890,import numpy as np,"import numpy as np

def reshape(x):
  """"""return x_reshaped as a flattened vector of the multi-dimensional array x""""""
  x_reshaped = x.reshape(-1, 1)
  return x_reshaped",0.4414587319,
890,import numpy as np,"def np_to_torch(np_array):
    return torch.from_numpy(np_array).float()",0.44023633,
890,import numpy as np,"from numpy import *

def loadDataSet():
    postingList=[['I', 'got', 'free', 'two', 'movie', 'ticket', 'from', 'your', 'boy', 'friend'],
                 ['free', 'coupon', 'from', 'xx.com'],
                 ['watch', 'free', 'new', 'movie', 'from', 'freemovie.com'],
                 ['best', 'deal', 'promo', 'code', 'here'],
                 ['there', 'will', 'be', 'free', 'pizza', 'during', 'the', 'meeting'],
                 ['scheduled', 'meeting', 'tomorrow'],
                 ['can','we','have','lunch','today'],
                 ['I','miss','you'],
                 ['thanks','my','friend'],
                 ['it','was','good','to','see','you','today'],
                 ['free','coupon','last','deal'],
                 ['free','massage','coupon'],
                 ['I','sent','the','coupon','you','asked','it','is','not','free'],
                 ['coupon','promo','code','here']]
    classVec = [0,1,1,1,0,0,0,0,0,0,1,1,0,1]    #1 is spam, 0 not
    return postingList,classVec",0.4396052957,
890,import numpy as np,"import numpy as np
def test_sin(x):
    y, nmax = sin_series(x)
    return compare_to_sin(x, y, nmax)

def test_sin_bad(x):
    y, nmax = sin_series_bad(x)
    return compare_to_sin(x, y, nmax)


def compare_to_sin(x, y, nmax):
    y0 = np.sin(x)
    delta = y - y0
    if y0 != 0:
        relative_error = delta/y0
    else:
        relative_error = 0
    # print(x, y, y0, delta, relative_error)
    #return x, y, y0, delta, relative_error
    return x, nmax, y, relative_error, delta",0.4389548004,
890,import numpy as np,"import sys
import numpy as np

def numpyIntro(argv):
  '''
  '''

  # Create a dumy data set to run some functions on
  # arange and linspace are two very useful ways of creating arrays.

  myArray = np.arange(16)

  print(myArray)  

  myArray = myArray.reshape(4,4)

  print(myArray)

  # The linspace method creates an array from a given start value
  # to a given end value with a given number of elements in the 
  # array.  The elements are spaced evenly
  anotherArray = np.linspace(1,6,10).reshape(2,5)

  print(anotherArray)

  # Numpy provides easy to access parameters discribing the structure of an array.

  # Print the shape of the array
  print(myArray.shape)
  # Print the number of dimensions of the array.  In this case
  # a 2D array
  print(myArray.ndim)
  # Print out the type of the data.
  print(myArray.dtype)
  print(myArray.size)

  # Numpy arrays have basic property methods for finding min, max and sum
  print(myArray.min())
  print(myArray.max())
  print(myArray.sum())

  print(myArray.sum(axis=0)) # sums the colums
  print(myArray.sum(axis=1)) # sums the rows

if __name__ == ""__main__"":
  numpyIntro(sys.argv[1:]);",0.4353511333,
389,crimes data,"def fetch_data(table_name, feat, feat_total_cnt, feat_needed_cnt):
    #pdb.set_trace()
    if group_id_cnt < RAND_COUNT:
        fetch_data_query = ""select * from {0} where feat = '{1}'"" \
        .format(table_name, feat)
    else:
        fetch_data_query = ""select * from {0} join \
        (select ceil(rand()*{3}) as id from {0} where feat='{1}') as r2 using(id) limit {2}"" \
        .format(table_name, feat, feat_needed_cnt, feat_total_cnt)
        #fetch_data_query = ""select * from {0} where feat = '{1}' order by RAND() limit {2}"" \
        #.format(table_name, feat, row_limit)
    # pdb.set_trace()
    raw_df = pd.read_sql(fetch_data_query, con=engine)
    return raw_df
    
    
def prepare_data(table_name, feat, feat_total_cnt, feat_needed_cnt, include_zero=True, group_cnt=RAND_COUNT):
    raw_df = fetch_data(table_name, feat, feat_total_cnt, feat_needed_cnt)
    data = raw_df.group_norm.tolist()
    nonExisting_cnt = max(0, int(group_cnt - len(data)))
    # Appending zero for those that does not use this feature at all
    # By removing this line you can find the best 
    if include_zero:
        data = data + ([SMALL_VAL]*nonExisting_cnt)
    return data

def find_best_fit_for_all_feats(table_name, include_zero=True, useAnderson=False):
    DISTRIBUTIONS = [
        ss.norm, ss.expon, ss.lognorm
    ]
    pval_threshold = 0.05
    cntr = 1
    usr_best_fit = []
    query_time = 0
    fit_time = 0

    # Load DATA from DB
    stime = time.time()
    global group_id_cnt
    #pdb.set_trace()
    group_id_cnt = pd.read_sql('select count(distinct group_id) cnt from ' + table_name, con=engine)
    group_id_cnt = group_id_cnt.cnt[0]
    # Load distinct features
    features = pd.read_sql('select feat, count(*) cnt from ' + table_name + ' group by feat', con=engine)
    feat_cnt = len(features)
    print(str(feat_cnt) + ' unique features' )

    if include_zero:
        rand_proportion = min(RAND_COUNT/group_id_cnt, 1)
    else:
        rand_proportion = 1
    #pdb.set_trace()
    # Load data feature by feature and find the best distribution for each feature
    for index, eachfeat in features.iterrows():
        feat = eachfeat.feat
        feat = feat.replace(""'"", ""''"").replace(""%"", ""%%"").replace('\\', '\\\\')
        qs = time.time()
        feat_datapoint_cnt = int(eachfeat.cnt * rand_proportion)
        data = prepare_data(table_name, feat, eachfeat.cnt, feat_datapoint_cnt, include_zero)
        qe = time.time()
        if useAnderson:
            best_fit_name, best_sse, best_fit_paramms = best_anderson_fit_distribution(data)
        else:    
            best_fit_name, best_sse, best_fit_paramms = best_fit_distribution(data)
        usr_best_fit.append([feat, eachfeat.cnt, feat_datapoint_cnt, best_fit_name, best_sse, best_fit_paramms])
        beste = time.time()
        query_time += (qe-qs)
        fit_time += (beste-qe)
        x = str(cntr) + ' out of ' + str(feat_cnt) + ', ' + feat + '-> query exec: ' \
        + str(round(qe-qs)) + 's, fit: ' + str(round(beste-qe)) + 's'
        print '\r', x, 
        #print x
        cntr += 1
    etime = time.time()
    print
    print('overall time: ' + str(round(etime-stime)) + 's -> query exec: ' + str(round(query_time)) + \
          's , fit: ' + str(round(fit_time)) + 's') 
    df = pd.DataFrame(usr_best_fit, \
                      columns=['feat', 'total_cnt', 'nonzero_cnt', 'best_fitted',  \
                               'best_statistic', 'best_params'])
    feat_dist = df.groupby('best_fitted').feat.count()
    print(feat_dist)

    df = df.sort_values('best_statistic')
    return df",0.4308576286,
389,crimes data,"def get_data(city,start,end):
    '''Function to get data from the github repo. city refers to the city of interest, start and
    end are the years of data you want. For instance start=2006 and end=2009 means data from 2006 to 2009'''
    dd=[]
    for x in np.arange(start,end+1):
        url='https://raw.githubusercontent.com/Data4Democracy/usa-dashboard/master/'+str(city)+'/data/'+str(city)+'-'+str(x)+'-crime.csv'
        try:
            dd.append(pd.read_csv(url))
        except urllib2.HTTPError, err: # We can get this error if the city or year data is absent
            if err.code==404:
                url='https://raw.githubusercontent.com/Data4Democracy/usa-dashboard/master/'+str(city)+'/'+str(city)+'-'+str(x)+'-crime.csv'
                try:
                    dd.append(pd.read_csv(url))
                except urllib2.HTTPError, err:
                    if err.code==404:
                        url='https://raw.githubusercontent.com/devinaconley/usa-dashboard/master/'+str(city)+'/'+str(x)+'.csv'
                        try:
                            dd.append(pd.read_csv(url))
                        except urllib2.HTTPError, err:
                            if err.code==404:
                                print ""data for "" + str(city) + str(x)+ "" doesn't exist""
    data=pd.concat(dd)
    data['city']=city
    data['date']=pd.to_datetime(data['year'].astype(str)+'/'+data['month'].astype(str)+'/'+data['day'].astype(str))
    return data",0.4302677512,
389,crimes data,"def get_stock(ticker_symbol, start, end):    # a function passes 3 parameters and returns a dataframe
    df = web.DataReader(ticker_symbol, 'yahoo', start, end)
    return df",0.4271550775,
389,crimes data,"# Functions goes here
def grab_poster_tmdb(movie):
    response = search.movie(query=movie)
    id=response['results'][0]['id']
    movie = tmdb.Movies(id)
    posterp=movie.info()['poster_path']
    title=movie.info()['original_title']
    url='image.tmdb.org/t/p/original'+posterp
    title='_'.join(title.split(' '))
    strcmd='wget -O '+poster_folder+title+'.jpg '+url
    os.system(strcmd)
    #print(response['results'])
    
def get_movie_id_tmdb(movie):
    response = search.movie(query=movie)
    movie_id=response['results'][0]['id']
    return movie_id

def get_movie_info_tmdb(movie):
    response = search.movie(query=movie)
    movie_id=response['results'][0]['id']
    movie=tmdb.Movies(movie_id)
    info = movie.info()
    return info

def get_movie_genres_tmdb(movie):
    response=search.movie(query=movie)
    id=response['results'][0]['id']
    movie=tmdb.Movies(id)
    genres=movie.info()['genres']
    return genres",0.4268211424,
389,crimes data,"#Functions that take in a movie and return ID, Genre, info and Posters
def get_poster(movie):
    response = search.movie(query=movie)
    id=response['results'][0]['id']
    movie = tmdb.Movies(id)
    posterpath=movie.info()['poster_path']
    title=movie.info()['original_title']
    url='http://image.tmdb.org/t/p/original'+posterpath
    title='_'.join(title.split(' '))
    r = requests.get(url)
    completeName = os.path.join(poster_folder, title) 
    with open(completeName,'wb') as w:
        w.write(r.content)

def getmovie_id(movie):
    resp= search.movie(query=movie)
    movie_id=resp['results'][0]['id']
    return movie_id

def getmovie_info(movie):
    resp= search.movie(query=movie)
    id=resp['results'][0]['id']
    movie = tmdb.Movies(id)
    info=movie.info()
    return info

def getmovie_genre(movie):
    resp = search.movie(query=movie)
    id=resp['results'][0]['id']
    movie = tmdb.Movies(id)
    genres=movie.info()['genres']
    return genres",0.4255905151,
389,crimes data,"def get_test_data(args):
    '''
    Input: (sess,offset)
    '''
    sess,i=args
    test_fingerprints, test_ground_truth = audio_processor.get_data(
        batch_size, i, model_settings, 0.0, 0.0, 0, 'testing', sess)
    return test_fingerprints,test_ground_truth",0.4239609241,
389,crimes data,"def get_val_data(args):
    '''
    Input: (sess,offset)
    '''
    sess,i=args
    validation_fingerprints, validation_ground_truth = (
            audio_processor.get_data(batch_size, i, model_settings, 0.0,
                                     0.0, 0, 'validation', sess))
    return validation_fingerprints,validation_ground_truth",0.4239609241,
389,crimes data,"sampling_factor = 0.5

def getData(df,sampling_factor):
    srdd = df.rdd.sample(False,sampling_factor)
    etm1 = srdd.map(lambda row : (row.timestamp[:19],row.deviceId,row.data.d.motorTemp)).collect()
    etm2 = filter(lambda (ts,dev,temp) : (not temp is None) and (not ts is None),etm1) 
    return sorted(etm2, key = lambda data : data[0])",0.4217485189,
389,crimes data,"def match_data(date, team_name, player_name):
    """"""
    This function gets all the columns from the ""Match"" table in the database according to the input filters below 
    and adds some additional columns that will become handy.

    Args:
        date (str): YYYY-MM-DD format string
        team_name (str): Name or part of the name of the team of interest
        player_name (str): Name or part of name of player of interest

    Returns:
        pandas.DataFrame 
    """"""
    conn = sqlite3.connect(""database.sqlite"")
    
    # get the match details of a team starting from a given date
    df = pd.read_sql_query('''select *  from match, Team 
            where (match.home_team_api_id == Team.team_api_id or match.away_team_api_id == Team.team_api_id) 
            and date(date) > date(:date)
            and Team.team_long_name like :team_name'''
            ,conn, params = {""date"":date, ""team_name"": '%'+ team_name + '%'})
    
    # column to decide whether this row is an away or home game for the ""team_name""
    df[""ha""] = np.where(df.home_team_api_id == df.team_api_id, 'h', 'a')
    
    # decide if ""team_name"" team draw(d), won(w) or lost(l) the match
    df[""result""] = 'd'
    df[""result""] = np.where((df[""ha""] == 'h') & (df.home_team_goal > df.away_team_goal), 'w', df[""result""])
    df[""result""] = np.where((df[""ha""] == 'h') & (df.home_team_goal < df.away_team_goal), 'l', df[""result""])
    df[""result""] = np.where((df[""ha""] == 'a') & (df.home_team_goal > df.away_team_goal), 'l', df[""result""])
    df[""result""] = np.where((df[""ha""] == 'a') & (df.home_team_goal < df.away_team_goal), 'w', df[""result""])
    
    # to determine the goals of the team of interest
    df[""goals""] = np.where((df[""ha""] == 'h'), df.home_team_goal, df.away_team_goal)
    
    # check the player in the squads 
    squad = df.loc[:, 'home_player_1':'away_player_11']
    a=squad[squad == player_id(player_name)].sum(axis = 1)
    df[""played""] = np.where((a > 0), True, False)   
     
    return df",0.4213809967,
389,crimes data,"#Function to prepare data for user input playerIDspercitr01
def player_hof(pitch, IDs):
    #Pull each of those players Player IDs
    pitch_hof = pitch[pitch.playerID.isin(IDs)]

    #Count the number of seasons played
    seasons_tot = pitch_hof[pitch_hof.stint == 1]
    seasons_tot = seasons_tot[['playerID', 'yearID']].groupby(['playerID']).count()
    seasons_tot['playerID'] = seasons_tot.index
    seasons_tot = seasons_tot.rename(columns={'yearID':'season_count'})
    
    #Total the career statistics of each playyer
    pitch_career = pitch_hof.groupby('playerID').sum()
    pitch_career = pitch_career.drop(['stint'],1)
    pitch_career = pitch_career.drop(['yearID'],1)
    pitch_career['playerID'] = pitch_career.index
    
    #Add seasons to the career statistics dataframe
    pitch_career = pitch_career.merge(seasons_tot, how = 'left', on = 'playerID')
    
    #ERA
    pitch_career['ERA'] = (pitch_career.ER * 9)/(pitch_career.IPouts/3)

    #Winning Percentage
    pitch_career['Wpct'] = (pitch_career.W/pitch_career.G)

    #Stirekouts per walks
    pitch_career = pitch_career.replace(0, .0000001)
    pitch_career['S/W'] = pitch_career.SO/pitch_career.BB

    #WHIP
    pitch_career['WHIP'] = (pitch_career.BB + pitch_career.H) / (pitch_career.IPouts/3)

    #IP
    pitch_career['IP'] = pitch_career.IPouts/3

    return(pitch_career)",0.4207119346,
1868,search,"def query_solr(query, fq=None, wt='json', fl='id,title,subtitle,answer,answerScore,upModVotes', num_rows=10):
    "" Query standalone Solr ""
    params = dict(q=query, wt=wt, fl=fl, rows=num_rows)
    if fq is not None:
        params['fq'] = fq
    return solr_experiment.rr_service.select(params)

def query_retrieve_and_rank(query, fq=None, wt='json', fl='id,title,subtitle,answer,answerScore,upModVotes', num_rows=10):
    "" Query the retrieve and rank API ""
    ranker_id = rr_experiment.ranker_id
    params = dict(q=query, ranker_id=ranker_id, wt=wt, fl=fl, rows=num_rows)
    if fq is not None:
        params['fq'] = fq
    return rr_experiment.rr_service.fcselect(params)

def get_doc_by_id(doc_id, query=None):
    "" Get the solr document, if we have the id""
    query = '*:*' if query is None else query
    resp = query_solr(query='*:*', fq='id:%s' % doc_id, fl='id,title,subtitle,answer,answerScore,upModVotes')
    if resp.ok:
        docs = resp.json().get('response', {}).get('docs', [])
        if len(docs) > 0 and docs[0]['id'] == doc_id:
            return docs[0]
        elif len(docs) == 0:
            raise ValueError('No docs returned. Response json : %r' % resp.json())
        else:
            raise ValueError('ID of top document does not match. Response json : %r' % resp.json())
    else:
        raise resp.raise_for_status()",0.4901514351,
1868,search,"class MySearchEngine():
    def __init__(self):
        # Dict[str, str]: maps document id to original/raw text
        self.raw_text = {}
        
        # Dict[str, Counter]: maps document id to term vector (counts of terms in document)
        self.term_vectors = {}
        
        # Counter: maps term to count of how many documents contain term
        self.doc_freq = Counter()
        
        # Dict[str, set]: maps term to set of ids of documents that contain term
        self.inverted_index = defaultdict(set)
    
    # ------------------------------------------------------------------------
    #  indexing
    # ------------------------------------------------------------------------

    def tokenize(self, text):
    
        """""" Converts text into tokens (also called ""terms"" or ""words"").
        
            This function should also handle normalization, e.g., lowercasing and 
            removing punctuation.
        
            For example, ""The cat in the hat."" --> [""the"", ""cat"", ""in"", ""the"", ""hat""]
        
            Parameters
            ----------
            text: str
                The string to separate into tokens.
        
            Returns
            -------
            list(str)
                A list where each element is a token.
        
        """"""
        # Hint: use NLTK's recommended word_tokenize() then filter out punctuation
        # It uses Punkt for sentence splitting and then tokenizes each sentence.
        # You'll notice that it's able to differentiate between an end-of-sentence period 
        # versus a period that's part of an abbreviation (like ""U.S."").
        
        # tokenize

        ### YOUR CODE HERE ###
        

        #tokenizer = RegexpTokenizer(r'\w+')
        #return tokenizer.tokenize(text)
        almost = nltk.word_tokenize(text.lower())
        newlyst = []
        for s in almost:
            if s not in string.punctuation:
                newlyst.append(s)    
        return newlyst
        # lowercase and filter out punctuation (using string.punctuation)

        ### YOUR CODE HERE ###

    def add(self, id, text):
        """""" Adds document to index.
        
            Parameters
            ----------
            id: str
                A unique identifier for the document to add, e.g., the URL of a webpage.
            text: str
                The text of the document to be indexed.
        """"""
        # check if document already in collection and throw exception if so
        if id in self.raw_text:
            raise ValueError('this document already exists')
        ### YOUR CODE HERE ###
        
        # store raw text for this doc id

        ### YOUR CODE HERE ###
        self.raw_text[id] = text
        
        # tokenize
        ### YOUR CODE HERE ###
        tokenized = self.tokenize(text)
        
        # create term vector for document (a Counter over tokens)
        terms = Counter(tokenized)
        self.term_vectors[id] = terms
        
        ### YOUR CODE HERE ###
        
        # store term vector for this doc id

        ### YOUR CODE HERE ###
        
        # update inverted index by adding doc id to each term's set of ids

        for atoken in tokenized:
            self.inverted_index[atoken].add(id)


        ### YOUR CODE HERE ###
        
        # update document frequencies for terms found in this doc
        # i.e., counts should increase by 1 for each (unique) term in term vector
        for aterm in terms:
            self.doc_freq[aterm] +=1
        ### YOUR CODE HERE ###

    def remove(self, id):
        """""" Removes document from index.
        
            Parameters
            ----------
            id: str
                The identifier of the document to remove from the index.
        """"""
        # check if document exists and throw exception if so
        if id not in self.raw_text:
            raise ValueError(""doesnt exist"")
        ### YOUR CODE HERE ###

        # remove raw text for this document
        ### YOUR CODE HERE ###
        
        terms = Counter(self.tokenize(self.raw_text[id]))
        for aterm in terms:
            self.doc_freq[aterm] -=1
            self.inverted_index[aterm].remove(id)
            
        del self.raw_text[id]
        # update document frequencies for terms found in this doc
        # i.e., counts should decrease by 1 for each (unique) term in term vector
         
        ### YOUR CODE HERE ###
        del self.term_vectors[id]
        # remove term vector for this doc
        
        ### YOUR CODE HERE ###

    def get(self, id):
        """""" Returns the original (raw) text of a document.
        
            Parameters
            ----------
            id: str
                The identifier of the document to return.
        """"""
        # check if document exists and throw exception if so
        if id not in self.raw_text:
            raise ValueError('dont have it')
        ### YOUR CODE HERE ###

        # return raw text
        return self.raw_text[id]
        ### YOUR CODE HERE ###
    
    def num_docs(self):
        """""" Returns the current number of documents in index. 
        """"""
        return(len(self.raw_text))

        ### YOUR CODE HERE ###

    # ------------------------------------------------------------------------
    #  matching
    # ------------------------------------------------------------------------

    def get_matches_term(self, term):
        """""" Returns ids of documents that contain term.
        
            Parameters
            ----------
            term: str
                A single token, e.g., ""cat"" to match on.
            
            Returns
            -------
            set(str)
                A set of ids of documents that contain term.
        """"""
        # NOTE: term needs to be lowercased so can match output of tokenizer
        # look up term in inverted index
        term = term.lower()
        return self.inverted_index[term]
        ### YOUR CODE HERE ###

    def get_matches_OR(self, terms):
        """""" Returns set of documents that contain at least one of the specified terms.
        
            Parameters
            ----------
            terms: iterable(str)
                An iterable of terms to match on, e.g., [""cat"", ""hat""].
            
            Returns
            -------
            set(str)
                A set of ids of documents that contain at least one of the term.
        """"""
        # initialize set of ids to empty set
        tempids = set()
        
        for aterm in terms: #cat, dog
            aterm = aterm.lower()
            aterm = self.inverted_index[aterm]
            tempids.update(tempids.union(aterm))
            
        return tempids
        ### YOUR CODE HERE ###
        
        # union ids with sets of ids matching any of the terms

        ### YOUR CODE HERE ###
    
    def get_matches_AND(self, terms):
        """""" Returns set of documents that contain all of the specified terms.
        
            Parameters
            ----------
            terms: iterable(str)
                An iterable of terms to match on, e.g., [""cat"", ""hat""].
            
            Returns
            -------
            set(str)
                A set of ids of documents that contain each term.
        """""" 
        tempids = set()
        
        for aterm in terms: #cat, dog
            aterm = aterm.lower()
            aterm = self.inverted_index[aterm]
            tempids.update(tempids.union(aterm))
            
        # initialize set of ids to those that match first term

        ### YOUR CODE HERE ###
        
        # intersect with sets of ids matching rest of terms

        ### YOUR CODE HERE ###
    
    def get_matches_NOT(self, terms):
        """""" Returns set of documents that don't contain any of the specified terms.
        
            Parameters
            ----------
            terms: iterable(str)
                An iterable of terms to avoid, e.g., [""cat"", ""hat""].
            
            Returns
            -------
            set(str)
                A set of ids of documents that don't contain any of the terms.
        """"""
        # initialize set of ids to all ids
        tempids = set([key for key in self.raw_text])
        print(tempids)

        for aterm in terms:
            tempids = tempids.difference(self.inverted_index[aterm])
        return tempids
        ### YOUR CODE HERE ###
        
        # subtract ids of docs that match any of the terms

        ### YOUR CODE HERE ###

    # ------------------------------------------------------------------------
    #  scoring
    # ------------------------------------------------------------------------
        
    def idf(self, term):
        return np.log10(self.num_docs()/self.doc_freq[term])
        """""" Returns current inverse document frequency weight for a specified term.
        
            Parameters
            ----------
            term: str
                A term.
            
            Returns
            -------
            float
                The value idf(t, D) as defined above.
        """""" 

        ### YOUR CODE HERE ###
    
    def dot_product(self, tv1, tv2):
        """""" Returns dot product between two term vectors (including idf weighting).
            
            Parameters
            ----------
            tv1: Counter
                A Counter that contains term frequencies for terms in document 1.
            tv2: Counter
                A Counter that contains term frequencies for terms in document 2.
            
            Returns
            -------
            float
                The dot product of documents 1 and 2 as defined above.
        """"""
        # iterate over terms of one document
        total = 0
        for subtv1 in tv1:
            if subtv1 in tv2:
                
        # if term is also in other document, 
        # then add their product (tfidf(t,d1) * tfidf(t,d2)) to a running total

        ### YOUR CODE HERE ###
    
    def length(self, tv):
        
        """""" Returns the length of a document (including idf weighting).
        
            Parameters
            ----------
            tv: Counter
                A Counter that contains term frequencies for terms in the document.
            
            Returns
            -------
            float
                The length of the document as defined above.
        """"""

        ### YOUR CODE HERE ###
    
    def cosine_similarity(self, tv1, tv2):
        """""" Returns the cosine similarity (including idf weighting).

            Parameters
            ----------
            tv1: Counter
                A Counter that contains term frequencies for terms in document 1.
            tv2: Counter
                A Counter that contains term frequencies for terms in document 2.
            
            Returns
            -------
            float
                The cosine similarity of documents 1 and 2 as defined above.
        """"""

        ### YOUR CODE HERE ###

    # ------------------------------------------------------------------------
    #  querying
    # ------------------------------------------------------------------------

    def query(self, q, k=10):
        """""" Returns up to top k documents matching at least one term in query q, sorted by relevance.
        
            Parameters
            ----------
            q: str
                A string containing words to match on, e.g., ""cat hat"".
        
            Returns
            -------
            List(tuple(str, float))
                A list of (document, score) pairs sorted in descending order.
                
        """"""
        # tokenize query
        # note: it's very important to tokenize the same way the documents were so that matching will work
        tokenized = self.tokenize(q)
        ### YOUR CODE HERE ###
        
        # get matches (just support OR style queries for now...)
        matches = self.get_matches_OR(tokenized)
        ### YOUR CODE HERE ###
                
        # convert query to a term vector (Counter over tokens)
        tv = Counter(tokenized)
        ### YOUR CODE HERE ###
        
        # score each match by computing cosine similarity between query and document

        ### YOUR CODE HERE ###

        # sort results and return top k

        ### YOUR CODE HERE ###",0.4804334342,
1868,search,"# you need to install Biopython:
# pip install biopython

# Full discussion:
# https://marcobonzanini.wordpress.com/2015/01/12/searching-pubmed-with-python/

def search(query,db='pubmed', sort='relevance', retmax='20',retmode='xml'):
    Entrez.email = 'j.davidgriffiths@gmail.com',
    handle = Entrez.esearch(db=db,sort=sort,retmax=retmax,retmode=retmode,term=query)
    results = Entrez.read(handle)
    return results

def fetch_details(id_list, db='pubmed', retmode='xml'):
    ids = ','.join(id_list)
    Entrez.email = 'j.davidgriffiths@gmail.com',
    handle = Entrez.efetch(db=db,retmode=retmode,id=ids)
    results = Entrez.read(handle)
    return results",0.4757769108,
1868,search,"GET /_search
{
    ""from"" : 0, 
    ""size"" : 10,
    ""track_scores"": true,
    ""query"" : {
        ....
    },
    ""indices_boost"" : [
            { ""en_gb"" : 2 },
            { ""en_us"" : 1 }
        ]
}",0.4708090127,
1868,search,"def search_tweets(q, count, extra_iters):
    search_results = twitter_api.search(q=q, count=count)
    tweets = search_results
    print ""Total tweets retrieved:"", len(tweets)

    # Iterate through 5 more batches of results by following the cursor
    for _ in range(extra_iters):
        next_results = search_results.next_results
        # No more results when next_results is None
        # If next_results exists, search again
        # next_results has the following form:
        # ?max_id=313519052523986943&q=NCAA&include_entities=1
        # Unpacking the values in a dictionary into keyword arguments
        # for the next search
        if next_results:
            kwargs = dict([ kw.split('=') for kw in next_results[1:].split(""&"") ])
            search_results = twitter_api.search(**kwargs)
            tweets += search_results
            print ""Total tweets retrieved:"", len(tweets)
    return tweets",0.465041995,
1868,search,"from searchengine import *
    
    # tarfile.open(""wiki.tar.gz"", 'r:gz').extractall(""."")
    crawlerObj = crawler('searchindex.db')
    # crawlerObj.createindextables()
    # pages = \
    #    ['file:///'+os.path.dirname(os.path.realpath(__file__)) + '\\wiki\\Categorical_list_of_programming_languages.html']
    # crawlerObj.crawl(pages)
    # print([row for row in crawlerObj.con.execute('select rowid from wordlocation where wordid=22')])
    # crawlerObj.calculatepagerank()",0.4632210135,
1868,search,"def searchSelect(b):
    if Graph.w.searchSelect.value:   #<-- when activated
        Graph.w.searchSelect.icon = 'eye-slash'
        display(Javascript(""""""
        cy.io.to_hide = cy.elements().difference( cy.io.selection )
        cy.io.to_hide.style(""visibility"", ""hidden"")
        """"""))
    else: #<-- when not clicked
        Graph.w.searchSelect.icon = 'eye'
        display(Javascript(""""""cy.io.to_hide.style(""visibility"", ""visible"")""""""))
Graph.w.searchSelect.observe( searchSelect, names='value' )",0.4616272151,
1868,search,"def get_spotify_artist_obj(sp, name):
    '''
    Get the artist ID for a spotify artist. Returns the first match.
    '''
    print('Searching Spotify for: \'{}\''.format(name))
    results = sp.search(q=name, type='artist')
    if 'artists' not in results or results['artists']['total'] < 1 or len(results['artists']['items']) < 1:
        print('No results found\n')
        return None
#         raise RuntimeError('No artists found\n')
    first_match = results['artists']['items'][0]
    print('           Found artist \'{}\''.format(first_match['name']))
    print('                Genres: \'{}\'\n'.format(first_match['genres']))
    return first_match",0.456335187,
1868,search,"def search(es_object, index_name, search ):
    res = es_object.search(index = index_name, body = search)
    pprint(res)",0.4549025297,
1868,search,"def sentiment_lexicon_example(
        mat=None, 
        rownames=None, 
        hidden_dim=100, 
        maxiter=1000, 
        output_filename=None, 
        display_progress=False):
    # Get the lexicon:
    lex = read_valence_arousal_dominance_lexicon()
    # Build the training data:
    sentidata, sentivocab = build_supervised_dataset(mat=mat, rownames=rownames, lex=lex)
    # Set up the network:
    sentinet = ShallowNeuralNetwork(
        input_dim=len(sentidata[0][0]), 
        hidden_dim=hidden_dim, 
        output_dim=len(sentidata[0][1]))
    # Train the network:
    sentinet.train(copy.deepcopy(sentidata), maxiter=maxiter, display_progress=display_progress)
    # Build the new matrix of hidden representations:
    inputs, labels = zip(*sentidata)
    sentihidden = np.array([sentinet.hidden_representation(x) for x in inputs])
    # Visualize the results with t-SNE:
    def colormap(vals):
        """"""Simple way to distinguish the 2x2x2 possible labels -- could be done much better!""""""
        signs = ['CC' if x < 0.0 else '00' for _, x in sorted(vals.items())]
        return ""#"" + """".join(signs)    
    colors = [colormap(lex[word]) for word in sentivocab]
    tsne_viz(mat=sentihidden, rownames=sentivocab, colors=colors, display_progress=display_progress, output_filename=output_filename)",0.4541725516,
169,ab testing,"from jupiter.simulator import jupiter
%config InlineBackend.figure_format = 'retina'
jupiter.test(is_notebook=True)",0.3431263566,
169,ab testing,"%%unittest_run

from hypothesis import given

class Tests(unittest.TestCase):
    @given(st.uuids(), st.fractions(), 
           st.binary(min_size=2, max_size=10))
    def test_primitive_strategies(self, uuid, fraction, binary):
        print(uuid, fraction, binary)
        self.assertTrue(False)",0.3308733106,
169,ab testing,"better_inst = MoreSpecificClass(""Alice"", 2)
better_inst.say_name()
print(better_inst.test_exponent(5))
print(better_inst.tally)",0.3294324875,
169,ab testing,"inst = ExampleClass('Bob', 3)
inst.say_name()
print(inst.test_exponent(3))
print(inst.tally)",0.3285655379,
169,ab testing,cd python/IHEcourse2017/exercises/Apr18/,0.3274552822,
169,ab testing,"# TEST Counting 404 (4a)
Test.assertEquals(badRecords.count(), 6185, 'incorrect badRecords.count()')
Test.assertTrue(badRecords.is_cached, 'incorrect badRecords.is_cached')",0.327316165,
169,ab testing,A.display_test(),0.3269171119,
169,ab testing,echo_icustay_vent.on_ventilator_during_echo.sum(),0.3261637688,
169,ab testing,"if notebook.nbextensions.check_nbextension('theme', user=False):
    E = notebook.nbextensions.EnableNBExtensionApp()
    E.enable_nbextension('theme/theme_selector')
else:
    print (""Extension not found"")",0.3220505118,
169,ab testing,"if notebook.nbextensions.check_nbextension('showSASLog', user=False):
    E = notebook.nbextensions.EnableNBExtensionApp()
    E.enable_nbextension('showSASLog/main')
else:
    print (""Extension not found"")",0.3220505118,
109,reverse digits in a number,"x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

def reverse(x):
    '''return a 1D vector with the elements reversed
Input x: 1D array
Output res: 1D array, with the elements of `x` reversed'''
    return x[::-1]
x.reverse()  #the numpy method to reverse an array
x",0.5456931591,
109,reverse digits in a number,reversed(,0.5446910858,
109,reverse digits in a number,"def reverse(l):
    """"""Receives a list and returns the list in reverse order""""""
    return l[::-1]

l = [1, 2, 3]
d = reverse(l)
print(d)
print(reverse.__doc__)",0.5334767103,
109,reverse digits in a number,"def digitize(n):
    return map(int, str(n)[::-1])",0.5308512449,
109,reverse digits in a number,"colors = ['red', 'blue', 'green', 'black', 'white']

rcolors= colors[::-1]

print(rcolors)

rcolors.reverse() # in-place.

print(rcolors)",0.5298884511,
109,reverse digits in a number,"fruits = ['strawberry', 'fig', 'apple', 'cherry', 'rasberry', 'banna']

def reverse(word):
    return word[::-1]
print(reverse('1234'))

s1 = sorted(fruits, key=reverse)
s2 = sorted(fruits, key=lambda word: word[::-1])
print(s1)
print(s2)",0.52778548,
109,reverse digits in a number,"str = ""hello world""
def reversedString(arq):
    return arq[::-1]

print(reversedString(str))",0.5247721672,
109,reverse digits in a number,"aList = [123, 'ram', 'sham', 'abc', 'ram'];
aList.reverse();
print (""List : "", aList)",0.5228159428,
109,reverse digits in a number,"# iterate through the list in reverse, printing each element multiplied by 2
for number in numbers[::-1]:
    print(number * 2)",0.5226492882,
109,reverse digits in a number,"str1 = ""Mr. Owl Ate My Metal Worm""


def reverse_string(string):
    return string[::-1]


print(reverse_string(str1))",0.5221868753,
1317,"one hot encode the passenger class column, and return a dataframe with cols for first and second","def transform_features(data,column):
    tmp = pd.get_dummies(data[column])
    tmp.columns = [column+ '_' + x for x in tmp]
    return tmp",0.4775378108,
1317,"one hot encode the passenger class column, and return a dataframe with cols for first and second","# function to show feature importance
def show_feature_importance(model, df):
    results = pd.DataFrame({'feature': df.columns.values, 'importance': model.feature_importances_})
    results = results.sort_values('importance', ascending=False)
    plt.figure(figsize=(16,8))
    sns.barplot(x='importance', y='feature', data=results, orient='h')
    plt.title('Feature Importance')",0.4751828611,
1317,"one hot encode the passenger class column, and return a dataframe with cols for first and second","# This helper function is for zipping columns together
def zip_importances(X, model):
    return pd.DataFrame(list(zip(X.columns, model.feature_importances_))).sort_values(1, ascending=True)",0.472761035,
1317,"one hot encode the passenger class column, and return a dataframe with cols for first and second","def rf_feat_importance(m, df):
    return pd.DataFrame({'imps': m.feature_importances_, 'cols': df.columns}
                       ).sort_values('imps', ascending=False)",0.4694437981,
1317,"one hot encode the passenger class column, and return a dataframe with cols for first and second","def dum_feat():
    # Making dummies.
    # Also creating a data frame to hold our features.
    features = pd.get_dummies(heart['num'])
    features['num'] = heart['num']
    features = features.drop(features.columns[0:5], axis=1)
    features['age'] = heart['age']
    features['exists'] = np.where(heart['num'] >= 1, 1, 0)
    features['gender'] = heart['sex']
    features['adult'] = np.where(heart['sex'] >= 18, 1, 0)

    features['chol'] = heart['chol']
    features['cholwarn'] = np.where(heart['chol'] >= 240, 1, 0)
    features['bp'] = heart['trestbps']
    features['bpwarn'] = np.where(heart['chol'] >= 140, 1, 0)
    loc = ['cleveland','va','hungarian']
    features['location'] = pd.Categorical(heart.location).codes
    return features

features = dum_feat()",0.4682831168,
1317,"one hot encode the passenger class column, and return a dataframe with cols for first and second","# define a function to replace the categorical variable with dummy variables (one-hot encoding)
# e.g column 'gender' = female --> column 'female' = 1 
def Category2Dummy(df, feature):
    dummy = pd.get_dummies(df[feature])
    dummy.columns = [feature + '_' +str(x) for x in dummy.columns]
    dummy.drop(dummy.columns[len(dummy.columns)-1], axis=1, inplace=True)
    df = pd.concat([df, dummy], axis=1)
    return df

def Category2Numeric(x, Map):
    if x in Map:
        return Map[x]
    
def Numeric2Category(x,Map):
    for i, item in Map.items():
        if x==item:
            return(i)

# impute the missing values: (dataframe, model, columns for imputation, target column with missing values)
def Impute(df, model, column, target):
    for i, row in df[df[target].isnull()].iterrows():
        df.ix[i, target] = model.predict(row[column].reshape(1, -1))
    return df",0.4678864479,
1317,"one hot encode the passenger class column, and return a dataframe with cols for first and second","#Display feature importance
def feature_importance(model, trainData, display_n_rows):
    """"""Display feature importance & weighting for tree based model""""""
    fi = model.feature_importances_*100
    feat_imp = pd.DataFrame(list(zip(fi,trainData.columns.values)))
    feat_imp = feat_imp.sort_values(by=0, axis=0, ascending=False)
    feat_imp.columns = ['importance %', 'feature']
    print(feat_imp[:display_n_rows])",0.4646963477,
1317,"one hot encode the passenger class column, and return a dataframe with cols for first and second","def join_data():
    import pandas as pd
    
    # Join the three datasets: GDP, Energy, and ScimEn into a new dataset (using the intersection of country names). 
    # Use only the last 10 years (2006-2015) of GDP data and only the top 15 countries by Scimagojr 
    #'Rank' (Rank 1 through 15).

    rank_energy = pd.merge(scima_country_rank,energy, how='inner', left_on='Country', right_on='Country')
    #The index of this DataFrame should be the name of the country.
    rank_energy_gdp = pd.merge(rank_energy, gdp_current_years, how='inner', 
                               left_on='Country', right_on='Country').set_index(['Country'])

    return rank_energy_gdp[0:15]

rank_energy_gdp=join_data()

rank_energy_gdp",0.4623237848,
1317,"one hot encode the passenger class column, and return a dataframe with cols for first and second","def label_categories(df, categorical_cols):
    return pd.get_dummies(df, columns=categorical_cols, drop_first=True)",0.4615398347,
1317,"one hot encode the passenger class column, and return a dataframe with cols for first and second","def process_pclass():
    
    global combined
    # encoding into 3 categories:
    pclass_dummies = pd.get_dummies(combined['Pclass'], prefix=""Pclass"")
    
    # adding dummy variables
    combined = pd.concat([combined,pclass_dummies],axis=1)
    
    # removing ""Pclass""
    
    combined.drop('Pclass',axis=1,inplace=True)

process_pclass()",0.4591503739,
1464,part the sum product algorithm,"def sample_loss(params):
    
    C = params[0]
    
    gamma = params[1]
    
    # Sample C and gamma on the log-uniform scale
    model = SVC(C=10 ** C, gamma=10 ** gamma, random_state=12345)

    # Sample parameters on a log scale
    return cross_val_score(model,
                         X=X_train,
                         y=y_train,
                         #scoring='roc_auc',
                         cv=3).mean()",0.5156072974,
1464,part the sum product algorithm,"def applyaugmentation(F, PW, printflag):
        """"""Augmentation: if X --> Y, then XZ --> YZ
        PW is powerset of the schema
        """"""
        N = {FD(x.lhs.union(y), x.rhs.union(y)) for x in F for y in PW}
        for fd in N - F:
                if printflag: print ""   Adding "" + str(fd) + "" by Augmenting "" + str(x) + "" using "" + """".join(y)
        return F.union(N)",0.5150684118,
1464,part the sum product algorithm,"# (pose[4], pose[5], pose[6]) is the unit vector for the direction

def cal_wc(pose):
    px = pose[0]
    py = pose[1]
    pz = pose[2]
    (roll, pitch, yaw) = transformations.euler_from_quaternion([pose[3],pose[4], pose[5], pose[6]], 'sxyz')
    #print(roll, pitch, yaw)
    L = 0.303
    
    Rrpy_urdf = transformations.euler_matrix(roll,pitch,yaw,'sxyz')
    
    R_z = Matrix([[           cos(np.pi),         -sin(np.pi),            0,              0],
                           [          sin(np.pi),          cos(np.pi),            0,              0],
                           [                   0,                   0,            1,              0],
                           [                   0,                   0,            0,              1]])
    # 90 degrees on the Y axis
    R_y = Matrix([[        cos(-np.pi/2),                   0,sin(-np.pi/2),              0],
                           [                   0,                   1,            0,              0],
                           [      -sin(-np.pi/2),                   0,cos(-np.pi/2),              0],
                           [                   0,                   0,            0,              1]])
    R_correction = R_z * R_y

    Rrpy_4 = N(Rrpy_urdf * R_correction)
    #print(t06r)

    Rrpy = Rrpy_4[:3,:3]
    #print(t06r_3)
    
    EE = Matrix([[px],[py],[pz]])
    
    #print(r06corr_3*Matrix([[1],[0],[0]]))
    
    WC = EE - L * (Rrpy*Matrix([[0],[0],[1]]))
    
    #print(""WCx = "", WC[0])
    
    return WC, Rrpy
    
#pose = (2.153, 0, 1.947, 0, 0, 0, 1)
#pose =  (1.851, 0, 1.643, 0, 0.706, 0, 0.708)
#pose = (1.851, 0, 2.249, 0, -0.706, 0.025, 0.708)
#pose = (2.137, 0, 1.850, 0, 0.161, 0.0, 0.987)
#pose = (0.904, 1.744, 0.487, -0.281, 0.462, 0.437, 0.719)
#pose = (1.728, 1.772, 1.157, -0.076, 0.181, 0.381, 0.903)
pose = (1.425, -0.611, 2.551, -0.040, -0.193, -0.197, 0.960)
pose = (1.499, 0.0, 3.39, 0.0, -0.439, 0, 0.898)

cal_wc(pose)",0.5085585117,
1464,part the sum product algorithm,"def factorial(vector):
    start = vector[0]
    end = vector[1]
    result = 1
    for i in range(start, end+1):
        result = result * i
    return result",0.5067383647,
1464,part the sum product algorithm,"def lift_and_drag(panels):
    lift,drag=0,0
    for panel in panels:
        lift -= panel.cp*np.sin(panel.beta)*panel.length
        drag += panel.cp*np.cos(panel.beta)*panel.length
        
    return lift, drag",0.5064743161,
1464,part the sum product algorithm,"def getCountsAndAverages(IDandRatingsTuple): # Get num ratings and average rating
    MovieID = IDandRatingsTuple[0]
    numRatings = len(IDandRatingsTuple[1])
    averageRating = sum(IDandRatingsTuple[1])/float(numRatings)
    return (MovieID, (numRatings, averageRating))",0.5051007867,
1464,part the sum product algorithm,"# explicit recursion

def recursive_total_cart(items):
    head, *tail = items
    if len(tail) == 0:
        return head.cost
    else:
        return head.cost + recursive_total_cart(tail)

recursive_total_cart([Item(10), Item(5)])",0.5035562515,
1464,part the sum product algorithm,"def function_to_minize(variables):
    beta = variables[0]
    N = variables[1]

    final_size = compute_final_size(beta=beta, N=N)
    peak = compute_peak(beta=beta, N=N)
    return (final_size-outbreak_final_size)**2 + 20*(peak-epidemic_peak)**2

minimize(function_to_minize, [2.3, 440], bounds=[(0, 10), (outbreak_final_size, population)])",0.5034351945,
1464,part the sum product algorithm,"def function_to_minize(variables):
    beta = variables[0]
    N = variables[1]

    final_size = compute_final_size(beta=beta, N=N)
    peak = compute_peak(beta=beta, N=N)
    return (final_size-outbreak_final_size)**2 + 20*(peak-epidemic_peak)**2

minimize(function_to_minize, [2.3, 440])",0.5034351945,
1464,part the sum product algorithm,"def function_to_minize(variables):
    beta = variables[0]
    N = variables[1]

    final_size = compute_final_size(beta=beta, N=N)
    peak = compute_peak(beta=beta, N=N)
    return (final_size-outbreak_final_size)**2 + (peak-epidemic_peak)**2

minimize(function_to_minize, [2.3, 440])",0.5034351945,
2022,step calculate absolute time of the first arrivals at the station,"def get_jsqNO(data, w2):
    df_jsqNO = pd.DataFrame()

    df_jsqNO['small_arrival'] = data[data.dispatcher == 'jsqNO'].small_arrival
    df_jsqNO['avg_resp_time'] = data[data.dispatcher == 'jsqNO'].avg_resp_time
    df_jsqNO['avg_power'] = data[data.dispatcher == 'jsqNO'].avg_power
    df_jsqNO['wgtd_sum_cost'] = data[data.dispatcher == 'jsqNO'].avg_num_jobs + w2 * 1000 * data[data.dispatcher == 'jsqNO'].avg_power
    
    df_jsqNO = df_jsqNO.groupby('small_arrival',as_index=False).agg({'avg_resp_time':np.mean, 'avg_power':np.mean, 'wgtd_sum_cost':[np.mean]})

    return df_jsqNO",0.5193976164,
2022,step calculate absolute time of the first arrivals at the station,"#class LearningAgent(Agent):
    
def update(self, t):
    #some other code
        
    #Learn and update Q-value based on state, action and reward
    new_state = (self.env.sense(self)[""light""],\
                 self.planner.next_waypoint())
        
    if new_state not in self.q_table:
        q_hat = reward + self.gamma * 0
    else:
        q_hat = reward + \
                self.gamma * max(self.q_table[new_state].values())
            
    self.q_table[self.state][action] = \
        self.alpha*q_hat + (1-self.alpha)*self.q_table[self.state][action]",0.5142290592,
2022,step calculate absolute time of the first arrivals at the station,"def update_func1(state, t, system):
    if t <= system.transition_year:
        births = system.birth_rate1 * state.young
    else:    
        births = system.birth_rate2 * state.young
        
    maturings = system.mature_rate * state.young
    deaths = system.death_rate * state.old
    
    young = state.young + births - maturings
    old = state.old + maturings - deaths
    
    return State(young=young, old=old)",0.5133852363,
2022,step calculate absolute time of the first arrivals at the station,"def plt_inv_target(station):
    # Set up axis
    x_axis = inv_target_hourly_df.time
    y_axis = inv_target_hourly_df.loc[inv_target_hourly_df.station == station].inventory_target
    y_axis_supply = inv_target_hourly_df.loc[inv_target_hourly_df.station == station].supply_count
    y_axis_demand = inv_target_hourly_df.loc[inv_target_hourly_df.station == station].demand_count
 
    #Plot
    fig, ax = plt.subplots(1, 1, figsize=(30, 10))
    fig.suptitle(""Bike Share Hourly Inventory Target @ ""+station, fontsize=24, fontweight=""bold"")
    colors = {'Inv Target': 'darkblue', 'Supply':'lightblue', 'Demand':'lightcoral'}
    
    ax.bar(x_axis[inv_target_hourly_df.station == station], y_axis[inv_target_hourly_df.station == station], color=""darkblue"", width=1.0, alpha=0.75, label = 'Inventory Target')
    ax.plot(x_axis[inv_target_hourly_df.station == station], y_axis_demand[inv_target_hourly_df.station == station], marker=""o"", c=""lightcoral"", linewidth=7.0, alpha=0.75, label = ""Demand"")
    ax.plot(x_axis[inv_target_hourly_df.station == station], y_axis_supply[inv_target_hourly_df.station == station], marker=""o"", c=""lightblue"", linewidth=7.0, alpha=0.75, label = ""Supply"")
    
    ax.set_ylabel('Number of Bikes')
    ax.set_xlabel('Time')
    plt.legend(loc=""upper left"", fancybox=True)

    # Set Style
    jtplot.style(theme='onedork')
    plt.show()",0.5095084906,
2022,step calculate absolute time of the first arrivals at the station,"def criteria(p):
    if p.visits[0].age < 19: # the patient is younger than 19 years during first visit recorded for the patient 
        for v in p.visits: # iterate over all visits
            for pr in v.prs: # iterate over all procedures
                if pr.pcode == code and v.year == 2006: # if there is a procedure during 2006 return True
                    return True
    return False
P022_in_2006 = filter(criteria,patients) # filter list of patients using above criteria
print ""Number of patients who underwent {} during 2006, at age < 19 or younger is {}"".format(code,len(P022_in_2006))",0.5052358508,
2022,step calculate absolute time of the first arrivals at the station,"def get_jsqNO(data, w2):
    df_jsqNO = pd.DataFrame()

    df_jsqNO['small_arrival'] = data[data.dispatcher == 'jsqNO'].small_arrival
    df_jsqNO['avg_resp_time'] = data[data.dispatcher == 'jsqNO'].avg_resp_time
    df_jsqNO['avg_power'] = data[data.dispatcher == 'jsqNO'].avg_power
    df_jsqNO['wgtd_sum_cost'] = data[data.dispatcher == 'jsqNO'].avg_num_jobs + w2 * 1000 * data[data.dispatcher == 'jsqNO'].avg_power
    
    df_jsqNO = df_jsqNO.groupby('small_arrival',as_index=False).agg({'avg_resp_time':np.mean, 'avg_power':np.mean, 'wgtd_sum_cost':[np.mean]})

    return df_jsqNO

def plot(df, orig_data, w2, stp_time):
    
    df_jsqNO = get_jsqNO(orig_data, w2)
    
    fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(18,4))
    
    style = {
        'rnd':'--b',
        'jsq':'-r',
        'jsqNO':'--*r',
        'fpi': '-og'
    }
    
    axes  = {
        'ax1':(ax1, 'avg_resp_time'),
        'ax2':(ax2, 'avg_power'),
        'ax3':(ax3, 'wgtd_sum_cost')
    }
    
    readable = {
        'avg_resp_time': 'Mean resp. time (sec)',
        'avg_power': 'Mean power (KW)',
        'wgtd_sum_cost': 'Weighted sum cost'
    }
    
    
    for axis in ['ax1', 'ax2', 'ax3']:
        ax = axes[axis][0]
        for dispatcher in ['rnd', 'fpi', 'jsq']:
            ax.plot(
                    df[df.dispatcher == dispatcher].small_arrival, 
                    df[df.dispatcher == dispatcher][axes[axis][1]],

                    # np.array(df[df.dispatcher == dispatcher][axes[axis][1]].tolist())/np.array(df_jsqNO[(axes[axis][1], 'mean')].tolist()),
                    style[dispatcher],
                    label=dispatcher.upper(),
                    linewidth = 1
                )
            
        
        ax.plot(
                df_jsqNO.small_arrival,
                df_jsqNO[(axes[axis][1],'mean')],
                style['jsqNO'],
                label = 'JSQ-NO'
            )
            
        [item.set_fontsize(14) for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] + ax.get_xticklabels() + ax.get_yticklabels())]
         
        ax.set_xticks(range(1,21,2))
        ax.legend(loc=0)
        ax.set_xlabel('Arrival rate at small cells, $k\in\{4,..,7\}$')
        ax.set_ylabel(readable[axes[axis][1]])

        
    plt.savefig('ED-{}-w2-{}-hetrogen.pdf'.format(stp_time, w2))
            
            

def plot2(df, orig_data, w2, stp_time,metric, rel_cost=False): 
    df_jsqNO = get_jsqNO(orig_data, w2)
    fig, ax = plt.subplots(1,1,figsize=(6,4))
    
    style = {
        'rnd':'--b',
        'jsq':'-r',
        'jsqNO':'--*r',
        'fpi': '-og'
    }
    
    readable = {
        'avg_resp_time': 'Mean resp. time (sec)',
        'avg_power': 'Mean power (KW)',
        'wgtd_sum_cost': 'Weighted sum cost'
    }
    
    if rel_cost:
        for dispatcher in ['fpi', 'jsq']:
            ax.plot(
                df[df.dispatcher == dispatcher].small_arrival,
                np.round(np.array(df[df.dispatcher == dispatcher][metric].tolist())/np.array(df[df.dispatcher == 'rnd'][metric].tolist()),2),
                style[dispatcher],
                label=dispatcher.upper(),
                linewidth = 1
            )

            
            [item.set_fontsize(14) for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] + ax.get_xticklabels() + ax.get_yticklabels())]
        
            ax.set_ylabel(readable[metric] + ' (relative)')
            
        ax.plot(
            df_jsqNO.small_arrival,
            np.round(np.array(df_jsqNO[(metric,'mean')].tolist())/np.array(df[df.dispatcher == 'rnd'][metric].tolist()),2),
            style['jsqNO'],
            label     = 'JSQ-NO',
            linewidth = 1
        )
        
        ax.plot(
            df_jsqNO.small_arrival,
            [1 for _ in df_jsqNO.small_arrival.tolist()],
            '--k',
            linewidth = 0.8,
            alpha=0.6
        )
        
        ax.set_xticks(range(1,19,2))
        ax.legend(loc=0)
        ax.set_xlabel('Arrival rate at small cell')
        
        plt.savefig('ED-{}-w2-{}-{}-relative-hetrogen.pdf'.format(stp_time, w2, metric))
        
        return
        

    for dispatcher in ['rnd', 'fpi', 'jsq']:
        ax.plot(
            df[df.dispatcher == dispatcher].small_arrival,
            df[df.dispatcher == dispatcher][metric],
            style[dispatcher],
            label=dispatcher.upper(),
            linewidth = 1
            )

            
        [item.set_fontsize(14) for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] + ax.get_xticklabels() + ax.get_yticklabels())]
        
        ax.set_ylabel(readable[metric])
        
        
    ax.plot(
        df_jsqNO.small_arrival,
        df_jsqNO[(metric,'mean')],
        style['jsqNO'],
        label     = 'JSQ-NO',
        linewidth = 1
    )
    
    ax.set_xticks(range(1,19,2))
    ax.legend(loc=0)
    ax.set_xlabel('Arrival rate at small cell')

    plt.savefig('ED-{}-w2-{}-{}-hetrogen.pdf'.format(stp_time, w2, metric))",0.5033783317,
2022,step calculate absolute time of the first arrivals at the station,"def model_results(model):
    start_time= time.clock()
    model.fit(X, Y)
    end_time = time.clock()
    score = cross_val_score(model, X, Y, cv=5)
    print(""Time to fit: "", end_time - start_time)
    print(""\nCross Validation Scores:\n"", score)
    print(""\nCross Validation range: %0.2f to %0.2f"" % (score.min(), score.max()))
    print(""\nCross Validation mean of: %0.2f (+/- %0.2f)"" % (score.mean(), score.std() * 2))",0.5008864999,
2022,step calculate absolute time of the first arrivals at the station,"def build_dataset(ar,time_index):
    #get current time
    cur_time = ar.loops[0].time[time_index]
    #flatten density,temperature arrays
    density_flat = np.array([n for l in ar.loops for n in l.density.value[time_index,:]])
    temperature_flat = np.array([T for l in ar.loops for T in l.temperature.value[time_index,:]])
    #calculate emission
    aia94_emiss = (density_flat**2)*f_aia94(temperature_flat)
    aia131_emiss = (density_flat**2)*f_aia171(temperature_flat)
    aia171_emiss = (density_flat**2)*f_aia131(temperature_flat)
    aia193_emiss = (density_flat**2)*f_aia193(temperature_flat)
    aia211_emiss = (density_flat**2)*f_aia211(temperature_flat)
    aia335_emiss = (density_flat**2)*f_aia335(temperature_flat)
    #load dataset
    data = dict(
        sdoaia94=(np.histogramdd(coords_flat,bins=bins,weights=aia94_emiss)[0],""1/cm/s""),
        sdoaia131=(np.histogramdd(coords_flat,bins=bins,weights=aia131_emiss)[0],""1/cm/s""),
        sdoaia171=(np.histogramdd(coords_flat,bins=bins,weights=aia171_emiss)[0],""1/cm/s""),
        sdoaia193=(np.histogramdd(coords_flat,bins=bins,weights=aia193_emiss)[0],""1/cm/s""),
        sdoaia211=(np.histogramdd(coords_flat,bins=bins,weights=aia211_emiss)[0],""1/cm/s""),
        sdoaia335=(np.histogramdd(coords_flat,bins=bins,weights=aia335_emiss)[0],""1/cm/s""),
    )
    bbox = np.array([
            np.array([bin_x_edges[0],bin_x_edges[-1]]),
            np.array([bin_y_edges[0],bin_y_edges[-1]]),
            np.array([bin_z_edges[0],bin_z_edges[-1]]),
        ])
    ds = yt.load_uniform_grid(data,data['sdoaia94'][0].shape,bbox=bbox,
                              length_unit='cm',geometry=('cartesian',('x','y','z')))
    ds.current_time = cur_time*yt.units.second
    return ds",0.4995799065,
2022,step calculate absolute time of the first arrivals at the station,"def get_delay(app):
    return app.stages[0].start - app.start",0.4955080152,
2022,step calculate absolute time of the first arrivals at the station,"def plot_training(history):
    epochs = np.asarray(history.epoch) + 1
    acc = history.history['acc']
    val_acc = history.history['val_acc']
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    plt.plot(epochs, loss, 'bo', label='loss')
    plt.plot(epochs, val_loss, 'ro', label='val_loss')
    plt.grid('on')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend(loc='best')
    plt.xlim(0, len(epochs)+1)
    plt.ylim(0.0, 0.5)",0.4945164323,
1745,ratios,"def simple_lnprior(pars):
    if -10 < pars[0] < 10 and -10 < pars[1] < 10 and -10 < pars[2] < 10:
        return 0.
    else:
        return -np.inf",0.3825080395,
1745,ratios,"def lnprior(pars):
    if -10 < pars[0] < 10 and -10 < pars[1] < 10 and -10 < pars[2] < 10:
        return 0.
    else:
        return -np.inf",0.3825080395,
1745,ratios,"def get_zodiac(row):
    day = row[1]*100 + row[2]
    for z in zodiacs:
        if day <= z[0]:
            return z[1]",0.3699709773,
1745,ratios,"def vertical_downward_mobilisation(soil, D):
    
    if ""sand"" in soil:
        return 0.1 * D

    elif ""clay"" in soil:
        return 0.2 * D

    else:
        raise ValueError(""Unknown soil type."")

delta_qd = vertical_downward_mobilisation(soil, D)
delta_qd",0.3695080876,
1745,ratios,"# Apply costfun to array (row = [amount, label, predicted_label, overhead])
def costArrFun(row):
    if row[1]==0:
        # True negative
        if row[2]==0:
            return 0.0
        # False positive
        else:
            if row[0] > row[3]:
                return row[3]
    else:
        # False negative
        if row[2] == 0:
            return row[0]
        # True positive
        else:
            return (row[0] if (row[0] <= row[3]) else row[3])


# # Old cost function (first table above)

# amounts should be a column of values of each transaction
def costFun(amounts,labels,predicted_labels,overhead,scale=10000.0):    
    # Scale is factor by which to divide quantities to prevent overflow
    cost = 0.0
    scaled_overhead = overhead / scale
    for k in xrange(len(amounts)):
        if labels[k]==0:
            # True negative
            if predicted_labels[k]==0:
                continue
            # False positive
            else:
                if amounts[k] > overhead:
                    cost += scaled_overhead
        else:
            # False negative
            if predicted_labels[k] == 0:
                cost += (amounts[k] / scale)
            # True positive
            else:
                cost += (amounts[k]/scale if (amounts[k] <= overhead) else scaled_overhead)
    
#     Vectorized implementation (apply function that accomplishes the above to each row)

#     ***This is way slower***
#     cost = np.sum(np.apply_along_axis(costArrFun,1,np.stack((amounts,labels,predicted_labels,overhead*np.ones_like(labels)),axis=1)))

    return cost * scale



# New cost function (second table above)

# def costFun(amounts,labels,predicted_labels,overhead,scale=10000.0):    
#     # Scale is factor by which to divide quantities to prevent overflow
#     cost = 0.0
#     scaled_overhead = overhead / scale
#     for k in xrange(len(amounts)):
#         # Predicted fraud (True positive or false positive)
#         if predicted_labels[k]==1:
#             cost += scaled_overhead
#         # False negative
#         elif labels[k]==1:
#             cost += (amounts[k] / scale)
    
#     return cost",0.3686082065,
1745,ratios,"def rule_identification_post(x):
    if np.isnan(x.tablet_or_mobile_after_reinf) or np.isnan(x.device_pixel_ratio_after_reinf) or np.isnan(x.device_screen_height_after_reinf) or np.isnan(x.device_screen_width_after_reinf):
        return 9
    elif x.os_after_reinf == ""Mac OS"":
        return 5
    elif x.os_after_reinf != ""iOS"" and x.os_after_reinf != ""Android"" and x.os_after_reinf != ""Mac OS"":
        return 4
    elif x.os_after_reinf == ""Android"" and x.tablet_or_mobile_after_reinf == False:
        return 8
    elif x.os_after_reinf == ""Android"" and x.device_pixel_ratio_after_reinf == 2:
        return 1
    elif x.os_after_reinf == ""Android"" and (x.device_pixel_ratio_after_reinf == 3 or x.device_pixel_ratio_after_reinf == 4):
        return 2
    elif x.os_after_reinf == ""iOS"" and x.device_pixel_ratio_after_reinf == 3:
        return 3
    elif x.os_after_reinf == ""Android"" and x.device_pixel_ratio_after_reinf > 1000:
        return 6
    elif x.os_after_reinf == ""iOS"" and x.device_screen_height_after_reinf == 1024 and x.device_screen_width_after_reinf == 768 and (x.device_pixel_ratio_after_reinf == 1 or x.device_pixel_ratio_after_reinf == 2):
        return 7
    else:
        return 9",0.3653943539,
1745,ratios,"def get_GAP(row):
    if (row[0] == 0) or (row[0] == 1 and row[1] >= 5.05):
        return 0
    else:
        return (5.05 - row[1]) / row[1]
def year_to_binary(x):
    if x == 1:
        return 0
    else:
        return 1
    
min_wage['GAP'] = min_wage[['state', 'wage_st']].apply(get_GAP, axis = 1)
min_wage['year'] = min_wage.year.apply(year_to_binary)
min_wage['price_full_meal'] = min_wage.psoda + min_wage.pfry + min_wage.pentree
min_wage['state_year'] = min_wage.state * min_wage.year",0.3636148572,
1745,ratios,"def function_to_evaluate(ind):  
    '''Function that we want to optimize -> to calculate fitness'''
    return -0.5*ind[0] + 2*ind[1] + 3*ind[2] + 0*ind[3] + sum(ind[4:]),",0.3615909815,
1745,ratios,"def freq(note):
    # general purpose function to convert a note  in standard notation 
    #  to corresponding frequency
    if len(note) < 2 or len(note) > 3 or \
        note[0] < 'A' or note[0] > 'G':
        return 0
    if len(note) == 3:
        if note[1] == 'b':
            acc = -1
        elif note[1] == '#':
            acc = 1
        else:
            return 0
        octave = int(note[2])
    else:
        acc = 0
        octave = int(note[1])
    SEMITONES = {'A': 0, 'B': 2, 'C': -9, 'D': -7, 'E': -5, 'F': -4, 'G': -2}
    n = 12 * (octave - 4) + SEMITONES[note[0]] + acc
    f = 440 * (2 ** (float(n) / 12.0))
    #print note, f
    return f


def ks_chord(chord, N, alpha):
    y = np.zeros(N)
    # the chord is a dictionary: pitch => gain
    for note, gain in chord.iteritems():
        # create an initial random-filled KS buffer the note
        x = np.random.randn(int(np.round(float(Fs) / freq(note))))
        y = y + gain * KS(x, N, alpha)
    return y",0.3607486784,
1745,ratios,"def normalise3DVectorToZEquals1(vec):
    
    normalised = [x/vec[2] for x in vec]
    
    return normalised",0.3599694371,
2160,step select the third cell down in the column named deaths,"def binary_combinations(data, fourier_shape):
    for pair in itertools.combinations(data.columns[fourier_shape:].values, 2):
        data[pair[0] + ""_"" + pair[1]] = data[pair[0]] * data[pair[1]]",0.5111435652,
2160,step select the third cell down in the column named deaths,"def CleanFemPreg(df):
    df.agepreg /= 100.0 #agepreg has motheres age in centiyears
    #dividing by 100.0 gives a floating point age in years
    
    #birthwgt_lb and birthwgt_oz contain the weight of the baby
    #it includes the following special codes;
    #97 NOT ASCERTAINED
    #98 REFUSED
    #99 DONT KNOW
    #we dont want specical codes as numbers, it can lead to use thinking
    #there's a 99 pound baby
    na_vals = [97, 98, 99] #here they are
    #below we look for the value in np_vals
    #replace it with numpy's not a number value
    #we do it in place, so the dataframe is updated
    df.birthwgt_lb.replace(na_vals, np.nan, inplace=True)
    df.birthwgt_oz.replace(na_vals, np.nan, inplace=True)
    
    #finally we add a column that is the baby's weight in pounts(float)
    df[totalwgt_lb] = df.birthwgt_lb + df.birthwgt_oz / 16.0",0.4871576428,
2160,step select the third cell down in the column named deaths,"def convert_str2int(d, idx_numeric):
    d.iloc[::,idx_numeric:] = d.iloc[::,idx_numeric:].applymap(lambda x: pd.to_numeric(x.replace(',', '') if type(x)!=int and type(x)!=float else x))",0.480402112,
2160,step select the third cell down in the column named deaths,"def get_winner(p):
    max_fitness = -9999999999
    best_genome = None
    for v in p.population:
        genome = p.population[v]
        if genome.fitness > max_fitness:
            max_fitness = genome.fitness
            winner = genome
    print(max_fitness)
    return(winner)",0.4802026153,
2160,step select the third cell down in the column named deaths,"def nanrate(train, valname):
    ## train, the dataset name
    ## valname, the feature name to study
    #print(train[valname].describe())
    train[valname+""_na""] = pd.isnull(train[valname])
    book_rate=[]
    click_rate=[]
    c_summary=[]
    b_summary=[]
    cond = []
    for i, gb in train.groupby(valname+""_na""):
        if i:
            cond.append(1)
            print('non_NaN value click summary:')
            print(gb[""click_bool""].describe())
            print('non_NaN value booking summary:')
            print(gb[""booking_bool""].describe())
        else:
            cond.append(0)
            print('NaN value click summary:')
            print(gb[""click_bool""].describe())
            print('NaN value booking summary:')
            print(gb[""booking_bool""].describe())
        book_rate.append(gb[""booking_bool""].mean())
        click_rate.append(gb[""click_bool""].mean())
        c_summary.append(gb[""click_bool""].describe())
        b_summary.append(gb[""booking_bool""].describe())
    df = pd.DataFrame(np.array([cond, click_rate, book_rate]), index=[""Condition"",""Click Rate"", ""Book Rate""])
    df = df.transpose()
    #print(df)
    df.plot( x=""Condition"",kind=""bar"")
    locs, labels = plt.xticks()
    plt.xticks(locs, [""Not NULL"", ""NULL""], size='small', rotation='horizontal')
    plt.title(""NaN rate wrt: ""+valname+"" feature."")
    plt.show()",0.4782792926,
2160,step select the third cell down in the column named deaths,"def trim_counts(x, n):
    x_ = x.copy()
    other = x_.iloc[n:].sum()
    x_ = x_.head(n)
    x_['other'] = other
    x_ = x_.reset_index()
    return x_

top_n = 8
num_bag = trim_counts(
    bag_of_words(num), top_n)
denom_bag = trim_counts(
    bag_of_words(denom), top_n)",0.4761370122,
2160,step select the third cell down in the column named deaths,"def extract_traj(tid, traj_all):
    traj = traj_all[traj_all['trajID'] == tid].copy()
    traj.sort_values(by=['startTime'], ascending=True, inplace=True)
    return traj['poiID'].tolist()",0.4743660092,
2160,step select the third cell down in the column named deaths,"def create_history_encoded_single_exp(orig_df, history_length):
    hist_df = orig_df.copy(deep=True) # later operations are ""in place"" so we need to avoid changing original dataframe
    columns_to_shift = hist_df.columns[:-1] # omit the action column, we don't want to duplicate it
    for i in range(1,history_length + 1):
        shift_df = orig_df.shift(i)
        for col_name in columns_to_shift:
            new_col_name = ""prev_{0}_"".format(i) + col_name
            hist_df[new_col_name] = shift_df[col_name] # add shifted column, aka history, as a column to orignal dataframe
            
    hist_df = hist_df[history_length:] # we don't return the first ""history_length"" sample - they have missing history data
    return hist_df",0.472700119,
2160,step select the third cell down in the column named deaths,"def evaluate_group_5_and_6(som, complet_data, test_dataframe):
    new_complet_data = complet_data.copy()
    new_complet_data = new_complet_data.drop(new_complet_data[new_complet_data['Age'] == 1].index)
    new_complet_data = new_complet_data.drop(new_complet_data[new_complet_data['Age'] == 2].index)
    new_complet_data = new_complet_data.drop(new_complet_data[new_complet_data['Age'] == 3].index)
    new_complet_data = new_complet_data.drop(new_complet_data[new_complet_data['Age'] == 0].index)

    new_test_dataframe = test_dataframe.copy()
    new_test_dataframe = new_test_dataframe.drop(new_test_dataframe[new_test_dataframe['Age'] == 1].index)
    new_test_dataframe = new_test_dataframe.drop(new_test_dataframe[new_test_dataframe['Age'] == 2].index)
    new_test_dataframe = new_test_dataframe.drop(new_test_dataframe[new_test_dataframe['Age'] == 3].index)
    new_test_dataframe = new_test_dataframe.drop(new_test_dataframe[new_test_dataframe['Age'] == 0].index)

    # Replaces age groups number by 0 and 1
    new_complet_data.loc[new_complet_data['Age'] == 5, 'Age'] = 1
    new_complet_data.loc[new_complet_data['Age'] == 4, 'Age'] = 0

    new_test_dataframe.loc[new_test_dataframe['Age'] == 5, 'Age'] = 1
    new_test_dataframe.loc[new_test_dataframe['Age'] == 4, 'Age'] = 0

    evaluate_som(group_5_and_6, new_complet_data, classes_name=['40~49', '>50'])
    print('-------------------TEST DATASET EVALUATION----------------------')
    evaluate_som(group_5_and_6_test, new_test_dataframe, classes_name=['40~49', '>50'])
    
    print('-------------------------------------------------------------------------------------------------')
    
    analyse_cluster_per_group(new_complet_data)

    new_complet_data = complet_data.copy()
    new_complet_data = new_complet_data.drop(new_complet_data[new_complet_data['Age'] == 1].index)
    new_complet_data = new_complet_data.drop(new_complet_data[new_complet_data['Age'] == 2].index)
    new_complet_data = new_complet_data.drop(new_complet_data[new_complet_data['Age'] == 3].index)
    new_complet_data = new_complet_data.drop(new_complet_data[new_complet_data['Age'] == 0].index)

    new_test_dataframe = test_dataframe.copy()
    new_test_dataframe = new_test_dataframe.drop(new_test_dataframe[new_test_dataframe['Age'] == 1].index)
    new_test_dataframe = new_test_dataframe.drop(new_test_dataframe[new_test_dataframe['Age'] == 2].index)
    new_test_dataframe = new_test_dataframe.drop(new_test_dataframe[new_test_dataframe['Age'] == 3].index)
    new_test_dataframe = new_test_dataframe.drop(new_test_dataframe[new_test_dataframe['Age'] == 0].index)

    # Replaces age groups number by 0 and 1
    new_complet_data.loc[new_complet_data['Age'] == 5, 'Age'] = 0
    new_complet_data.loc[new_complet_data['Age'] == 4, 'Age'] = 1

    new_test_dataframe.loc[new_test_dataframe['Age'] == 5, 'Age'] = 0
    new_test_dataframe.loc[new_test_dataframe['Age'] == 4, 'Age'] = 1

    evaluate_som(group_5_and_6, new_complet_data, classes_name=['40~49', '>50'])

    print('-------------------TEST DATASET EVALUATION----------------------')
    evaluate_som(group_5_and_6_test, new_test_dataframe, classes_name=['40~49', '>50'])",0.4726616442,
2160,step select the third cell down in the column named deaths,"def evaluate_group_1_and_3(som, complet_data, test_dataframe):
    new_complet_data = complet_data.copy()
    new_complet_data = new_complet_data.drop(new_complet_data[new_complet_data['Age'] == 1].index)
    new_complet_data = new_complet_data.drop(new_complet_data[new_complet_data['Age'] == 3].index)
    new_complet_data = new_complet_data.drop(new_complet_data[new_complet_data['Age'] == 4].index)
    new_complet_data = new_complet_data.drop(new_complet_data[new_complet_data['Age'] == 5].index)


    new_test_dataframe = test_dataframe.copy()
    new_test_dataframe = new_test_dataframe.drop(new_test_dataframe[new_test_dataframe['Age'] == 1].index)
    new_test_dataframe = new_test_dataframe.drop(new_test_dataframe[new_test_dataframe['Age'] == 3].index)
    new_test_dataframe = new_test_dataframe.drop(new_test_dataframe[new_test_dataframe['Age'] == 4].index)
    new_test_dataframe = new_test_dataframe.drop(new_test_dataframe[new_test_dataframe['Age'] == 5].index)

    # Replaces age groups number by 0 and 1
    new_complet_data.loc[new_complet_data['Age'] == 0, 'Age'] = 1
    new_complet_data.loc[new_complet_data['Age'] == 2, 'Age'] = 0

    new_test_dataframe.loc[new_test_dataframe['Age'] == 0, 'Age'] = 1
    new_test_dataframe.loc[new_test_dataframe['Age'] == 2, 'Age'] = 0

    evaluate_som(group_1_and_3, new_complet_data, classes_name=['0~10', '20~29'])
    
    print('-------------------TEST DATASET EVALUATION----------------------')
    evaluate_som(group_1_and_3_test, new_test_dataframe, classes_name=['0~10', '20~29'])
    
    print('-------------------------------------------------------------------------------------------------')
    
    analyse_cluster_per_group(new_complet_data)

    new_complet_data = complet_data.copy()
    new_complet_data = new_complet_data.drop(new_complet_data[new_complet_data['Age'] == 1].index)
    new_complet_data = new_complet_data.drop(new_complet_data[new_complet_data['Age'] == 3].index)
    new_complet_data = new_complet_data.drop(new_complet_data[new_complet_data['Age'] == 4].index)
    new_complet_data = new_complet_data.drop(new_complet_data[new_complet_data['Age'] == 5].index)


    new_test_dataframe = test_dataframe.copy()
    new_test_dataframe = new_test_dataframe.drop(new_test_dataframe[new_test_dataframe['Age'] == 1].index)
    new_test_dataframe = new_test_dataframe.drop(new_test_dataframe[new_test_dataframe['Age'] == 3].index)
    new_test_dataframe = new_test_dataframe.drop(new_test_dataframe[new_test_dataframe['Age'] == 4].index)
    new_test_dataframe = new_test_dataframe.drop(new_test_dataframe[new_test_dataframe['Age'] == 5].index)

    # Replaces age groups number by 0 and 1
    new_complet_data.loc[new_complet_data['Age'] == 0, 'Age'] = 0
    new_complet_data.loc[new_complet_data['Age'] == 2, 'Age'] = 1

    new_test_dataframe.loc[new_test_dataframe['Age'] == 0, 'Age'] = 0
    new_test_dataframe.loc[new_test_dataframe['Age'] == 2, 'Age'] = 1

    evaluate_som(group_1_and_3, new_complet_data, classes_name=['0~10', '20~29'])

    print('-------------------TEST DATASET EVALUATION----------------------')
    evaluate_som(group_1_and_3_test, new_test_dataframe, classes_name=['0~10', '20~29'])",0.4726616442,
232,collocations,"def createLabeledPointFromMatchup(m):
    global players_dict_broadcast
    global num_players_broadcast
    home = m['home']
    away = m['away']
    home_unit = m[home]['on']
    away_unit = m[away]['on']
    home_poss = m[home]['stats']['poss']
    away_poss = m[away]['stats']['poss']
    avg_poss = (home_poss+away_poss)/2.
    if avg_poss <= 0:
        avg_poss = 1
    players_dict = {players_dict_broadcast.value[player]:avg_poss for player in home_unit}
    players_dict.update({players_dict_broadcast.value[player]:-avg_poss for player in away_unit})
    home_pts = m[home]['stats']['pts']
    away_pts = m[away]['stats']['pts']
    return LabeledPoint(100*(home_pts-away_pts)/avg_poss, SparseVector(num_players_broadcast.value, players_dict))",0.4033204317,
232,collocations,"def my_classifier(row):
    if row['total_UPDRS'] > 50:
        return 0
    elif row['total_UPDRS'] > 40: 
        return 1
    elif row['total_UPDRS'] > 30:
        return 2
    elif row['total_UPDRS'] > 20:
        return 3
    elif row['total_UPDRS'] > 10:
        return 4
    else:
        return 5
    
quantum = df1.apply(my_classifier, axis=1)

df1['updrs']= quantum",0.3922375441,
232,collocations,"def coord2city(coords):
    # assume coords is a 2 item list of form: [lat,long]
    from citipy import citipy
    # extracts city name
    city = citipy.nearest_city(coords[0],coords[1]).city_name
    # extracts country code
    country = citipy.nearest_city(coords[0],coords[1]).country_code
    out = (city, country)
    return out",0.3867481649,
232,collocations,"def getBigrams(tokens):
    bigram_measures=nltk.collocations.BigramAssocMeasures();
    finder = BigramCollocationFinder.from_words(tokens)
    sorted_bgs = (sorted(list(finder.ngram_fd.items()), key=lambda item: item[-1], reverse=True))
    
    return sorted_bgs",0.3855357766,
232,collocations,"#map each battle to pokemon data
def stat_connect(data):
    data['First_pokemon_stats'] = data.First_pokemon.map(attr_dict)
    data['Second_pokemon_stats'] = data.Second_pokemon.map(attr_dict)
    
    return data",0.3811979294,
232,collocations,"def klabel_connect(data):
    data['P1_klabel'] = data.First_pokemon.map(klabel_dict)
    data['P2_klabel'] = data.Second_pokemon.map(klabel_dict)   
    return data",0.3811979294,
232,collocations,"#map each battle to pokemon data
def type_connect(data):
    data['First_pokemon'] = data.First_pokemon.map(type_dict)
    data['Second_pokemon'] = data.Second_pokemon.map(type_dict)
    return data",0.3811979294,
232,collocations,"def accom(P):
    if not P.accom:
        return 0
    
    path2 = os.getcwd() + '/AccomBoundsA' + str(P.livingArea) + '.csv'
    data2 = pd.read_csv(path2, header=0)
    
    row = 0
    if P.couple:
        if P.numKids > 0: row = 2 
        else: row = 1
    else:
        if P.numKids == 0: row = 0 
        if P.numKids == 1: row = 3 
        else: row = 4

    cutoff = deDollar(data2.iloc[row,4])
    threshold = deDollar(data2.iloc[row,3])
    maxpay = deDollar(data2.iloc[row,6])
    if P.wibt() > cutoff:
        return 0
    
    path3 = os.getcwd() + '/AccomThreshold.csv'
    data3 = pd.read_csv(path3, header=0)
 
    if P.renting: col = 1
    else: col = 2
    rentthreshold = deDollar(data3.iloc[row,col])
    sub = 0
    if P.wHousingCost > rentthreshold:
        sub = (P.wHousingCost - rentthreshold)*0.7
    if sub>maxpay: sub = maxpay
    if P.wibt()>threshold:
        sub = sub - (P.wibt()-threshold)*0.25
    if sub < 0:
        sub = 0

    return sub

# https://www.workandincome.govt.nz/map/income-support/extra-help/accommodation-supplement/income-non-beneficiaries-01.html
Person1.wHousingCost = 800
accom(Person1)",0.3801578879,
232,collocations,"def getTrigrams(tokens):
    trigram_measures =nltk.collocations.TrigramAssocMeasures();
    finder = TrigramCollocationFinder.from_words(tokens)
    sorted_tgs = (sorted(list(finder.ngram_fd.items()), key=lambda item: item[-1], reverse=True))

    return sorted_tgs",0.3793122172,
232,collocations,"# {x:Nat, y:Nat, z:Nat} -> {x:Nat, y:Nat, z:Nat}
def dragon(p):
    return {'x': p['x']+10, 'y': p['y']+50, 'z': p['z']+1}",0.3722989261,
2257,svc pipeline and stop words,"# Inspired by this script: https://gist.github.com/yanofsky/5436496.
def get_tweets_by_screen_name(screen_name=""MIT""):
    user_tweets = []
    new_tweets = api.user_timeline(screen_name, count=200)
    user_tweets.extend(new_tweets)
    #-------------------------
    last_tweet=user_tweets[-1]
    oldest = last_tweet.id - 1
    while len(new_tweets) > 0:
        print ""getting tweets before %s"" % (oldest)
        new_tweets = api.user_timeline(screen_name, count=200, max_id=oldest)
        user_tweets.extend(new_tweets)
        # Update the id of last tweet
        last_tweet=user_tweets[-1]
        oldest = last_tweet.id - 1
    #---------------------------
    all_tweets= [tweet.text.encode(""utf-8"") for tweet in user_tweets]
    out_tweets= [[tweet.id_str, tweet.created_at, tweet.text.encode(""utf-8"")] for tweet in user_tweets]
    
    return out_tweets

out_tweets= get_tweets_by_screen_name(screen_name=""UBC"")
for i in out_tweets[:10]:
    print(i)",0.4703096151,
2257,svc pipeline and stop words,"def pricer(result_XGBoost,do_dichotomy):
    start_time = time.time()
    
    if(do_dichotomy):
        result_XGBoost = test.dichotomie_coupon(result_XGBoost,coupons,prix_min,prix_max,max_iter,bst)
    else:
        result_XGBoost['Coupon']=float(coupon.result.split(':')[1][1:])
    
    dtest = xgb.DMatrix(result_XGBoost.values, missing=-999.0) 
    prediction_price = bst.predict(dtest)

    print(""Temps de calcul : ""+str(time.time()-start_time))
    
    if(do_dichotomy):
        print('Coupon slectionn : {}. Prix estim : {}'.format(result_XGBoost['Coupon'][0],prediction_price[0]))
    else:
        print('Prix estim : {}'.format(prediction_price[0]))",0.4604876041,
2257,svc pipeline and stop words,"def predict_pipeline1(data, count_letter=True):
    start_time = time.time()
    count = 0
    for bad_cut in data:
        flag = False
        count += 1
        bad_cuts = np.asarray([bad_cut])
        if count_letter:
            print (""Predictions for the supposed letter number "" + str(count))
        for char in ALPHABET_ALL:
            predictions = single_letter_models[char].predict([bad_cuts] * number_of_models)
            if (predictions[0][1] > predictions[0][0]):
                print (""Cut "" + str(count) + "" has been classified as good corresponding to char '"" +\
                       char + ""' with a confidence of "" + str(predictions[0][1] * 100) + ""%"")
                flag = True
        if not flag:
            print (""Bad cut"")
        print (""---"")
    elapsed_time = time.time() - start_time
    print(""Elapsed time:"", elapsed_time)
        
def predict_pipeline2(data, count_letter=True):
    start_time = time.time()
    count = 0
    for bad_cut in data:
        count += 1
        bad_cuts = np.asarray([bad_cut])
        if count_letter:
            print (""Predictions for the supposed letter number "" + str(count))
        predictions = segmentator_model.predict([bad_cuts] * number_of_models)
        if (predictions[0][1] > predictions[0][0]):
            predictions = ocr_model.predict([bad_cuts] * number_of_models)
            ind = (-predictions[0]).argsort()[:3]
            for i in ind:
                print (""Good cut corresponding to letter '"" + ALPHABET_ALL[i] + \
                   ""' with a confidence of "" + str(predictions[0][i] * 100) + ""%"")
        else: 
            print (""Bad cut with a confidence of "" + str(predictions[0][0] * 100) + ""%"")
        print (""---"")
    elapsed_time = time.time() - start_time
    print(""Elapsed time:"", elapsed_time)
        
def predict_pipeline3(data, count_letter=True):
    start_time = time.time()
    count = 0
    for bad_cut in data:
        flag = False
        count += 1
        bad_cuts = np.asarray([bad_cut])
        if count_letter:
            print (""Predictions for the supposed letter number "" + str(count))
        for char in ALPHABET_ALL:
            predictions = single_letter_models[char].predict([bad_cuts] * number_of_models)
            if (predictions[0][1] > predictions[0][0]):
                print (""Good cut with a confidence of "" + str(predictions[0][1] * 100) + ""% by letter '"" + char + ""'"")
                flag = True
        if flag:
            predictions = ocr_model.predict([bad_cuts] * number_of_models)
            ind = (-predictions[0]).argsort()[:3]
            for i in ind:
                print (""Good cut corresponding to letter '"" + ALPHABET_ALL[i] + \
                   ""' with a confidence of "" + str(predictions[0][i] * 100) + ""%"")
        else: 
            print (""Bad cut"")
        print (""---"")
    elapsed_time = time.time() - start_time
    print(""Elapsed time:"", elapsed_time)

def predict_pipeline4(data, count_letter=True):
    start_time = time.time()
    count = 0
    for bad_cut in data:
        count += 1
        bad_cuts = np.asarray([bad_cut])
        if count_letter:
            print (""Predictions for the supposed letter number "" + str(count))
        predictions = segmentator_model.predict([bad_cuts] * number_of_models)
        if (predictions[0][1] > predictions[0][0]):
            for char in ALPHABET_ALL:
                predictions = single_letter_models[char].predict([bad_cuts] * number_of_models)
                if (predictions[0][1] > predictions[0][0]):
                    print (""Good cut with a confidence of "" + str(predictions[0][1] * 100) + ""% by letter '"" + char + ""'"")
        else: 
            print (""Bad cut with a confidence of "" + str(predictions[0][0] * 100) + ""%"")
        print (""---"")
    elapsed_time = time.time() - start_time
    print(""Elapsed time:"", elapsed_time)",0.4596424997,
2257,svc pipeline and stop words,"def get_stopwords(stop_tweeters=True, extras=[]):
    # start with a standard set of stopwords:
    stopwords = set(open(""stopwords.txt"", 'r').readline().split(','))

    if stop_tweeters:
        # remove all the twitter handles from the users
        twitter_handle_names = list(tweets[""user.screen_name""].values)
    else:
        twitter_handle_names = []
        
    manual_stopwords = [""https"",""co"",""rt"",""don't"",""i'm"",""you're"",""i've"", ""it's"",
              ""nUaDgbMNgU"",""kuLfgZoEL0"",""kdfGPTtRow"",""PQQfWNyNGh"",""pe54vvn"", ""zSvLp3M55T"",
              ""7YNot1GvdT"",""stLeWaRLlF"", ""Shsapk86wb"", ""BX9Nz83yFh"",
              ""mlanet18"",         
                       ]
    for w in twitter_handle_names + manual_stopwords + extras:
        stopwords.add(w)
    return stopwords",0.450766921,
2257,svc pipeline and stop words,"def simulate_it(hist=True, typ='straddle', fig_name='fig_2'):
    """""" generic simulation procedure """"""
    # read the S&P 500 data
    snp = ocl.SnP500_data()

    # last date values as a start for forecasting
    snp500 = snp.last_value()

    # use mean annual volatility for 5 last years as a model volatility parameter
    vol = snp.annual_vols().tail().mean()[0]

    # spread for butterfly strikes
    spr = snp500 * vol

    # multiple for spread
    mult = 0.5

    # strikes and dates
    K = snp500
    wid = K * 0.15
    Kl, Ks = K - wid, K + wid
    exp_dat = '2017-12-01'
    cur_dat = '2017-04-01'

    # calls straddle portfolio
    if typ == 'straddle':
        buc = ocl.straddle(K, expir=exp_dat)
    elif typ == 'bull':
        buc = ocl.bull_spread(Kl, Ks, expir=exp_dat)
    else:
        buc = ocl.bull_spread(Kl, Ks, expir=exp_dat, vlm=-1)

    # calculation dates
    calc_dats = ['2017-09-01','2017-10-01','2017-11-01','2017-12-01']

    # output dataframe templates
    dfo = pd.DataFrame()
    n_paths = 5000
    dfe = pd.DataFrame(index=range(n_paths))

    # two plots in one figure
    figsize(12,4)
    
    # suffix in file names
    suff = 'hist' if hist else 'MC'

    # first plot
    subplot(1,2,1)
    for calc_dat in calc_dats:
        # dates difference in calendar and business days
        tot_days = (pd.to_datetime(calc_dat) - pd.to_datetime(cur_dat)).days
        bus_days = int(tot_days / 365 * 250)
    
        if hist:
            # simulate S&P500 values using historic method
            s = snp500 * snp.make_hist_returns(n_paths=n_paths, n_days=bus_days)[-1, :]
        else:
            s = snp500 * snp.make_mc_returns(n_paths=n_paths, n_days=bus_days)[-1, :]
    
        # portfolio payoff for all simulates snp500 values
        po = buc.payoff(snp500, s, vol, rf, cur_dat, calc_dat)
        po.name = ocl.port_payoff_lab
    
        # keep descriptive stats for simulated payoffs
        dfo[calc_dat] = po.describe([0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95])
    
        # plot empiric CDF for simulated payoffs
        poe = ocl.ecdf(po)
        poe.plot(label=calc_dat)
        dfe[calc_dat] = np.array(poe)
        #poe.round(3).to_csv('csv/{0}_a.csv'.format(fig_name), header=True)
    ti = title(ocl.port_payoff_tit)
    le = legend()
    #dfe.index.name = ''
    dfe.round(3).to_csv('csv/{0}_a.csv'.format(fig_name), index=None)

    # now plot the percentiles of payoffs vs the calc date
    dfd = dfo.T.round(3)
    dfd.index.name = 'date'
    prc = ['50%','75%','90%','95%']
    subplot(1,2,2)
    for pr in prc:
        dfd[pr].plot(label=pr)
    ti = title(ocl.port_perc_tit)
    le = legend()
    savefig('images/{0}.png'.format(fig_name))
    dfd.round(3).to_csv('csv/{0}_b.csv'.format(fig_name))",0.4505556822,
2257,svc pipeline and stop words,"def pre_processing(X, y = None, vocab_len = 26, vec_len = 400):
    X = [i[:-1] for i in X] # Remove last character from each entry 
    alphabet = []
    for letter in range(97,123):
        alphabet.append(chr(letter)) # Create a list of alphabets (lower case)
    t = Tokenizer(char_level = True)
    t.fit_on_texts(alphabet) # Tokenize the list of alphabets
    seq = pad_sequences(t.texts_to_sequences(X), maxlen=vec_len, padding='post', truncating='post', value=0)
    # Create a sequence from each data point with maximum length of 400
    # If the entry is less than 400 character length, pad it with 0s
    X = np.array(seq)
    if y is None:
        return X
    else:
        y = [i[:-1] for i in y]
        y = to_categorical(y) # Binarize the entries
        return X, y",0.4483181834,
2257,svc pipeline and stop words,"# Warning:  only run this block once or you may end up with duplicate records in the database
# TODO: check if the database is already there and populated and if so, skip the work...

def load_shapes_into_db(state_name, prepare_only=False):
    command = 'shp2pgsql'
    if prepare_only:
        command += ' -p'   # prepare the tables only, don't load
        sys.stdout.write('Creating table tl_2000_tabblock schema using state %s\n' % state_name)
    else:
        command += ' -a'   # append to tables
        sys.stdout.write('Appending state %s to tl_2000_tabblock\n' % state_name)
    command += ' -s 4326' # Shapefiles are in EPSG:4326 -- WGS84 lat,lon
    command += ' capture/tabblock_2000/%s_block_2000_4326.shp tl_2000_tabblock' % (state_name.upper())
    command += ' | psql -q -d timelapse'
    print command
    out = subprocess.check_output(command, shell=True)
    sys.stdout.write('psql output: %s\n' % out.strip())
    
# Drop and recreate tl_2000_tabblock    
psql('DROP TABLE tl_2000_tabblock;')
load_shapes_into_db(state_names[0], prepare_only=True)

# Loading all states in parallel worked OK on 32-core 64GB earthdev2.  psql was i/o bound, but no paging occurred
# On a lesser machine it might be important to do only a few in parallel at a time
threads = []

for state_name in state_names:
    threads.append(threading.Thread(target=load_shapes_into_db, args=(state_name, False)))
    threads[-1].start()

for t in threads:
    t.join()",0.4472881556,
2257,svc pipeline and stop words,"def get_data(input_folder, verbose=False): 
    """""" load the files train and test and return two variables with its content""""""
    train = pd.read_csv(path.join(input_folder,""train.csv""))
    test = pd.read_csv(path.join(input_folder,""test.csv""))
    if verbose: 
        print (""train.shape ="", train.shape, ""; test.shape  ="", test.shape)
    return train, test
def combine_data(train, test):
    """""" combining the data makes easier the cleaning (cutting a numerical
    feature in bins using the same distribution for train and test) 
    and imputation (using the same mean for inputing the train and the set)""""""
    # missing target values get NaN
    combined       = pd.concat([train,test])
    return combined
def split_data(combined, target_name=TARGET): 
    """""" after we work with the data cleaning and imputation we split in two sets""""""
    n_train = combined[target_name].notnull().sum()
        
    train, test = np.split(combined, [n_train], axis=0)
    
    return train, test",0.4468024969,
2257,svc pipeline and stop words,"def pre_processing(X, y = None, vocab_len = 26, vec_len = 400):
    X = [i[:-1] for i in X] # Remove last character from each entry 
    alphabet = [] 
    for letter in range(97,123):
        alphabet.append(chr(letter)) # Create a list of alphabets (lower case)
    t = Tokenizer(char_level = True)
    t.fit_on_texts(alphabet) # Tokenize the list of alphabets
    seq = pad_sequences(t.texts_to_sequences(X), maxlen=vec_len, padding='post', truncating='post', value=0)
    # Create a sequence from each data point with maximum length of 400
    # If the entry is less than 400 character length, pad it with 0s
    binarizer = LabelBinarizer() 
    binarizer.fit(range(26))
    X = np.array([binarizer.transform(x) for x in seq]) # Binarize the entries
    if y is None:
        return X
    else:
        y = [i[:-1] for i in y]
        y = to_categorical(y) # Convert dependent variable to one-hot encoding
        return X, y",0.4462137818,
2257,svc pipeline and stop words,"def pre_processing(X, y = None, vocab_len = 26, vec_len = 400):
    X = [i[:-1] for i in X] # Remove last character from each entry 
    alphabet = [] 
    for letter in range(97,123):
        alphabet.append(chr(letter)) # Create a list of alphabets (lower case)
    t = Tokenizer(char_level = True)
    t.fit_on_texts(alphabet) # Tokenize the list of alphabets
    seq = pad_sequences(t.texts_to_sequences(X), maxlen=vec_len, padding='post', truncating='post', value=0)
    # Create a sequence from each data point with maximum length of 400
    # If the entry is less than 400 character length, pad it with 0s
    binarizer = LabelBinarizer()
    binarizer.fit(range(26)) 
    X = np.array([binarizer.transform(x) for x in seq]) # Binarize the entries
    if y is None:
        return X
    else:
        y = [i[:-1] for i in y]
        y = to_categorical(y) # Convert dependent variable to one-hot encoding
        return X, y",0.4462137818,
865,identify misclassifications,"class Line():
    def __init__(self):
        # was the line detected in the last iteration?
        self.detected = False  
        #polynomial coefficients averaged over the last n iterations
        self.best_fit = None  
        #polynomial coefficients for the most recent fit
        self.current_fit = [np.array([False])]  
        #x values for detected line pixels
        self.allx = None  
        #y values for detected line pixels
        self.ally = None",0.5179642439,
865,identify misclassifications,"# Define a class to receive the characteristics of each line detection
class Line():
    def __init__(self):
        # was the line detected in the last iteration?
        self.detected = False  
        #polynomial coefficients averaged over the last n iterations
        self.best_fit = None  
        #polynomial coefficients for the most recent fit
        self.current_fit = [np.array([False])]
        #polynomial coefficients for the last iteration
        self.last_iter_fit = [np.array([False])]        
        #polynomial coefficients for the most recent fit in meters
        self.current_fit_m = [np.array([False])]
        #radius of curvature of the line in some units
        self.radius_of_curvature = None 
        #distance in meters of vehicle center from the line
        self.line_base_pos = None 
        #difference in fit coefficients between last and new fits
        self.diffs = np.array([0,0,0], dtype='float') 
        #x values for detected line pixels
        self.allx = None  
        #y values for detected line pixels
        self.ally = None
        #x values for windows
        self.win_x = None  
        #y values for windows
        self.win_y = None
        #x values for windows on last_iteration
        self.last_iter_win_x = None  
        #y values for windows
        self.last_iter_win_y = None


line = {'left':Line(), 'right':Line()}",0.5117773414,
865,identify misclassifications,"# Class definition
class line():
    def __init__(self):
        # was the line detected in the last iteration
        self.detected = False
        # polynominal coeeficients averaged over the last n iterations
        self.best_fit = None
        # polynominal coefficients for the most recent fit
        self.current_fit = []
        # difference in fit coefficients between last and new fits
        self.diffs = np.array([0,0,0],dtype='float')
    # Add a fit
    def add_fit(self,fit):
        if fit is not None:
            if self.best_fit is not None:
                #if a best_fit exists, there will be a comparison between the current and the best fit
                # If the difference between them is large, we will discard the current fit
                self.diffs = abs(fit - self.best_fit)
            if (self.diffs[0] > 0.001 or self.diffs[1] > 1.0 or self.diffs[2] > 150.) and (len(self.current_fit) > 0):
                self.detected = False
            else:
                self.detected = True
                self.current_fit.append(fit)
                # Only keep 5 newest fits
                if len(self.current_fit) > 5:
                    self.current_fit = self.current_fit[len(self.current_fit)-5:]
                # best fit is the average of the newest current fits
                self.best_fit = np.average(self.current_fit, axis = 0)
                
        else:
        # If the new fit is None, the current fits should discard the oldest one
            self.detected = False
            if len(self.current_fit) > 0:
                self.current_fit = self.current_fit[1 : ]
            if len(self.current_fit) > 0:
                self.best_fit = np.average(self.current_fit, axis = 0)",0.5115461946,
865,identify misclassifications,"class Line():
    def __init__(self):
        # was the line detected in the last iteration?
        self.detected = False  
        # x values of the last n fits of the line
        self.recent_xfitted = [] 
        #average x values of the fitted line over the last n iterations
#         self.bestx = None     
        #polynomial coefficients averaged over the last n iterations
#         self.best_fit = None  
        #polynomial coefficients for the most recent fit
        self.current_fit = [np.array([False])]  
        #radius of curvature of the line in meters
        self.radius_of_curvature = None 
        #distance in meters of vehicle center from the line
        self.line_base_pos = None 
        #difference in fit coefficients between last and new fits
#         self.diffs = np.array([0,0,0], dtype='float') 
        #x values for detected line pixels
        self.allx = None  
        #y values for detected line pixels
        self.ally = None",0.5111222267,
865,identify misclassifications,"# Define a class to receive the characteristics of each line detection
class Line:
    def __init__(self):
        # was the line detected in the last iteration?
        self.detected = False  
    
        #polynomial coefficients averaged over the last n iterations
        self.best_fit = None  
        
        #polynomial coefficients for the most recent fit
        self.current_fit = []  
        
        #difference in fit coefficients between last and new fits
        self.diffs = np.array([0,0,0], dtype='float') 

    
    def add_fit(self, fit):
        # add a found fit to the line, up to n
        if fit is not None:
            if self.best_fit is not None:
                self.diffs = abs(fit-self.best_fit)
            # If diff is not in the limitation, detected failed.
            if (self.diffs[0] > 0.001 or self.diffs[1] > 1.0 or self.diffs[2] > 100.) and \
               len(self.current_fit) > 0:
                self.detected = False
            else:
                self.detected = True
                self.current_fit.append(fit)
                
                # keep newest 5 fits
                if len(self.current_fit) > 5:
                    self.current_fit = self.current_fit[len(self.current_fit)-5:]
                 
                # smoothing
                self.best_fit = np.average(self.current_fit, axis=0)
        else:
            self.detected = False
            # throw out oldest fit
            if len(self.current_fit) > 0:
                self.current_fit = self.current_fit[:len(self.current_fit)-1]
            # taking the average    
            if len(self.current_fit) > 0:
                self.best_fit = np.average(self.current_fit, axis=0)",0.5063719749,
865,identify misclassifications,"class Lane():
    def __init__(self):
        self.detected = False
        self.recent_xfitted = []
        self.bestx = None
        self.best_fit = None
        self.current_fit = [np.array([False])]
        self.radius_of_curvature = None
        self.line_base_pos = None
        self.diffs = np.array([0,0,0], dtype='float')
        self.allx = None
        self.ally = None",0.5062156916,
865,identify misclassifications,"#Line class to keep track of various features of Lane lines
class Line():
    def __init__(self):
        # was the line detected in the last iteration?
        self.detected = False  
        # recent polynomial coefficients
        self.recent_fit = []
        # polynomial coefficients averaged over the last n iterations
        self.best_fit = None  
        # polynomial coefficients for the most recent fit
        self.current_fit = [np.array([False])]  
        # difference in fit coefficients between last and new fits
        self.diffs = np.array([0,0,0], dtype='float') 
        # x values for detected line pixels
        self.all_x = None  
        # y values for detected line pixels
        self.all_y = None
        # counter to reset after 5 iterations if issues arise
        self.counter = 0
        #radius of curvature of the line in some units
        self.radius_of_curvature = None 
        #distance in meters of vehicle center from the line
        self.line_base_pos = None",0.5027544498,
865,identify misclassifications,"# Define a class to receive the characteristics of each line detection
class Line():
    def __init__(self):
        # was the line detected in the last iteration?
        self.detected = False  
        # recent polynomial coefficients
        self.recent_fit = []
        # polynomial coefficients averaged over the last n iterations
        self.best_fit = None  
        # polynomial coefficients for the most recent fit
        self.current_fit = [np.array([False])]  
        # difference in fit coefficients between last and new fits
        self.diffs = np.array([0,0,0], dtype='float') 
        # x values for detected line pixels
        self.allx = None  
        # y values for detected line pixels
        self.ally = None
        # counter to reset after 5 iterations if issues arise
        self.counter = 0",0.5027544498,
865,identify misclassifications,"class Line():
    def __init__(self):
        # Was the line detected in the last iteration?
        self.detected = False   
        # Polynomial coefficients averaged over the last n iterations
        self.best_fit = None  
        # Polynomial coefficients for the most recent fit
        self.current_fit = []  
        # Difference in fit coefficients between last and new fits
        self.diffs = np.array([0,0,0], dtype='float') 
        # Max number of fit's to keep
        self.keep_n_fits = 5
    def sanity_check(self, fit):
        if (self.best_fit is not None) and fit is not None:
            self.diffs = abs(fit - self.best_fit)
        # The threshold values were determined by looking at the order of magnitude by the fit's coefficients
        # Typically : [a * e-05  b * e-02   c * e+02] 
        if (self.diffs[0] > 100 * (1e-05) or self.diffs[1] > 100*(1e-02) or self.diffs[2] > (1e+02)) and len(self.current_fit) > 0:
            self.detected = False
            # Sanity check failed
        else:
            self.detected = True
            # Sanity check passed
    def add_fit(self, fit):
        if fit is not None: 
            if self.detected == True:
                self.current_fit.append(fit)
                if len(self.current_fit) > self.keep_n_fits:
                    self.current_fit = self.current_fit[len(self.current_fit) - self.keep_n_fits:]
        else:
            if len(self.current_fit) > 0:
                self.current_fit = self.current_fit[:len(self.current_fit)-1]
    def smooth(self):
        if len(self.current_fit) > 0:
            self.best_fit = np.average(self.current_fit, axis=0)",0.5019258261,
865,identify misclassifications,"# Define a class to receive the characteristics of each line detection
class Line():
    def __init__(self, n=10):
        # was the line detected in the last iteration?
        self.detected = False  
        # x values of the last n fits of the line
        self.recent_xfitted = [] 
        #average x values of the fitted line over the last n iterations
        self.bestx = None     
        #polynomial coefficients averaged over the last n iterations
        self.best_fit = None  
        #polynomial coefficients for the most recent fit
        self.current_fit = [False]  
        #radius of curvature of the line in some units
        self.radius_of_curvature = None 
        #distance in meters of vehicle center from the line
        self.line_base_pos = None 
        #difference in fit coefficients between last and new fits
        self.diffs = np.array([0,0,0], dtype='float') 
        #x values for detected line pixels
        self.allx = None  
        #y values for detected line pixels
        self.ally = None
        
        self.n = n
        self.n_ = 0
                
    def update_params(self, fit_line, fit_pts, curvature, offset):
        
        # Information taken from the line fitting
        self.current_fit = fit_line
        self.allx = fit_pts[0]; 
        self.ally = fit_pts[1]
        self.radius_of_curvature = curvature
        self.line_base_pos = offset
        
    def choose_fitline(self, fit_line):
    
        # Fit lines
        if not self.n_:
            self.best_fit = fit_line
        else:
            self.best_fit = self.best_fit*self.n_/(self.n_+1) + fit_line/self.n_ 
        
        if not self.n_ == self.n:
            self.n_ += 1",0.5003020167,
2453,use list comprehensions to split the text into words,"word.lemmatize() for word in word in article_body
words=[.words
words.append(word.lemmatize() for word in word in article_body)
def split_into_lemmas(text):
    text = text.lower()
    words = TextBlob(text).words
    return [word.lemmatize() for word in words]
       
# vect = TfidfVectorizer(analyzer=split_into_lemmas, decode_error='replace')
# tokenize_test(vect)",0.4171960354,
2453,use list comprehensions to split the text into words,"corpus_vecs_clean=[]
corpus_clean=[]
for document in corpus:
    #doc = re.sub(';?:!""', '.', document) #replace these with period, they will be splits for sentences
    doc= document.translate(None, string.punctuation)    
    doc = doc.translate(None, digits) #remove digits
    doc = doc.lower()  #make all letters lowercase
    doc = re.sub(r'\s+', ' ',   doc) #remove any extra white spaces, from https://lzone.de/examples/Python%20re.sub
    doc= doc.split( ' ') #split document into sentences
    #doc2 = [j.strip().split(' ') for j in doc] #split up each sentence into tokens and strip any extra whitespace if remaining
    for i in doc:
        filter(None, i) #get rid of empty tokens, '', some stubborn ones still here, but they will be eliminated later. 

    word_vecs_doc=[] #will populate this with the word-vectors for each word
    for i in doc:
        try:
            word_vecs_doc.append(currentmodel.wv[i])
        except KeyError, e: #looked up key errors in python to account for the vocab words that didn't occur often enough to learn embeddings for, https://stackoverflow.com/questions/16154032/catch-keyerror-in-python
            continue
    
    corpus_clean.append(doc) #attach sentences to corpus_clean, note this might now include a few ecctra word not in W2V vocab
    corpus_vecs_clean.append(word_vecs_doc)
    
del(corpus) #save up some space
len(corpus_clean)==150 #make sure still 150!",0.4152605236,
2453,use list comprehensions to split the text into words,"# Do not add to list if:
def lemmatizer(listOfWords):
    
    
def removeStopWords(listOfWords):
    
    
    stopWords = []
    stopWords = stopwords.words('English')
        with open(r""C:\nlp\extra_stopwords.txt"", 'r', encoding = 'UTF-8') as f:
            extra_stopWords = f.read()
            extra_stopWords = extra_stopWords.split(""\n"")
            stopWords.append(extra_stopWords)
            
    if (lemmatize == True):
        wordNet = WordNetLemmatizer()

            
            if (get_wordnet_pos(pos) == -1): # kept POS: ADJ, VERB, NOUN, ADV 
                continue
            if (stopWords_filter == True) & (word in stopWords): 
                continue
            
            if (lemmatize == True):
                word = wordNet.lemmatize(word)",0.4105309844,
2453,use list comprehensions to split the text into words,"stops = set(stopwords.words(""english""))
def remove_stopwords(post):
    post = post.lower().split()
    p_post = []
    for word in post:
        if word not in stops:
            p_post.append(word)
    final_post = "" "".join(p_post)
    return final_post",0.4054479599,
2453,use list comprehensions to split the text into words,"def retrievePOS(descriptions):
    tags = []
    for description in tqdm(descriptions):
        text = nltk.word_tokenize(description)
        pos_tags = nltk.pos_tag(text)
        pos_dict = {}
        for (word,pos) in pos_tags:
            pos_dict[word.lower()] = pos
            tags.append(pos_dict)
    return tags",0.4050295949,
2453,use list comprehensions to split the text into words,"def tokenization(text):
    # tokenize
    tokens = nltk.RegexpTokenizer(pattern=r""(?u)\b\w\w+\b"").tokenize(text)
    # remove stop words
    stopwords = nltk.corpus.stopwords.words('english')
    return [t for t in tokens if t.lower() not in stopwords]

def tokenization_and_stemming(text):
    # tokenize and remove stopwords
    tokens = tokenization(text)
    # stem
    porter = nltk.PorterStemmer()
    stems = [porter.stem(t) for t in tokens]
    return stems

tokenization_and_stemming(all_docs[1][:188])",0.4032875299,
2453,use list comprehensions to split the text into words,"def create_features(processed_tweets, rare_words):
    """""" creates the feature matrix using the processed tweet text
    Inputs:
        tweets: pd.DataFrame: tweets read from train/test csv file, containing the column 'text'
        rare_words: list(str): one of the outputs of get_feature_and_rare_words() function
    Outputs:
        sklearn.feature_extraction.text.TfidfVectorizer: the TfidfVectorizer object used
                                                we need this to tranform test tweets in the same way as train tweets
        scipy.sparse.csr.csr_matrix: sparse bag-of-words TF-IDF feature matrix
    """"""
    docs = []
    stopwords = set(rare_words + nltk.corpus.stopwords.words('english'))
    for terms in processed_tweets[""text""]:
        docs.append("" "".join(terms))
    model = sklearn.feature_extraction.text.TfidfVectorizer(stop_words=stopwords)
    return model, model.fit_transform(docs)

# AUTOLAB_IGNORE_START
(tfidf, X) = create_features(processed_tweets, rare_words)
print type(tfidf)
print type(X)
# AUTOLAB_IGNORE_STOP",0.401720345,
2453,use list comprehensions to split the text into words,"# actual search for reciprocity - seems to be coming up on error analysis
def reciprocity_search(df):
    reciprocate = []
    all_text = df.all_text.values
    comps = [re.compile(r'return.+favor'), re.compile(r'pay.+forward'), re.compile(r'pay.+back'),
             re.compile(r'give.+back'), re.compile(r'repay'), re.compile(r'reciprocate'), re.compile(r'participate')]

    for entry in range(len(all_text)):
        contains_words = 0
        for comp in comps:
            if len(re.findall(comp, all_text[entry].lower())) > 0:
                contains_words = 1
        reciprocate.append(contains_words)
    return reciprocate",0.4010314941,
2453,use list comprehensions to split the text into words,"stoplist = [word for word in stopwords.words('english')]
texts = [[word for word in doc.lower().split() if word not in stoplist] for doc in docs]",0.4003499448,
2453,use list comprehensions to split the text into words,"wid=nltk.corpus.words.fileids()
wlist=[]
for id in wid:
    words=nltk.corpus.words.words(id)
    for word in words:
        wlist.append(word.lower())
words_set=set(wlist)
print(len(words_set))",0.3996216059,
1630,problem light curve data,"# Fit the function to the data
def calculate_fit(y, x, avg_e):
    popt, pcov = curve_fit(line, x, y, sigma=avg_e)
    print ""NE2001 popt "", popt  # put this in main
    return popt, pcov",0.5065072179,
1630,problem light curve data,"#Learning Curve Graphs
    max_depths = [1,2,3,4,5,6,7,8,9,10]
    for max_depth in max_depths:
        learning_curve(max_depth, X_train, y_train, X_test, y_test)

        pl.figure()
        pl.title('Decision Trees: Performance vs Training Size')
        pl.plot(sizes, test_err, lw=2, label = 'test error')
        pl.plot(sizes, train_err, lw=2, label = 'training error')
        pl.legend()
        pl.xlabel('Training Size')
        pl.ylabel('Error')
        pl.show()",0.4940199554,
1630,problem light curve data,"def score(series):
    
    fit = lin_reg(series)
    
    if fit.pvalues[1] < 1e-20:
        return fit.params[1]

result = ratios.query_series(score)

print_ratios(list(result.keys())[:200])",0.4925085902,
1630,problem light curve data,"def score(series):
    
    fit = lin_reg(series)
    
    if fit.pvalues[1] < 1e-20:
        return -fit.params[1]
    
result = ratios.query_series(score)

print_ratios(list(result.keys())[:200])",0.4918588102,
1630,problem light curve data,"def calculate_paths(method = ""euler""):

    theta1 = theta1_0
    theta2 = theta2_0
    p1, p2 = 0.0, 0

    paths = []
    
    if method == ""euler"":
        print ""Running EULER method""
        for i in range(nsteps):
            if (i % 500==0): print ""Step = %d"" % i
            theta1, theta2, p1, p2 = euler(theta1, theta2, p1, p2)
            r1 = array([l1*sin(theta1), -l1*cos(theta1)])
            r2 = r1 + array([l2*sin(theta2), -l2*cos(theta2)])
            paths.append([r1, r2])


    elif method == ""scipy"":
        print ""Running SCIPY method""
        yint = [theta1, theta2, p1, p2]
        # r = ode(deriv).set_integrator('zvode', method='bdf')
        r = ode(deriv).set_integrator('vode', method='bdf')
        r.set_initial_value(yint, 0)
        
        paths = []
        while r.successful() and r.t < max_t:
            r.integrate(r.t+dt)
            theta1, theta2 = r.y[0], r.y[1]
            r1 = array([l1*sin(theta1), -l1*cos(theta1)])
            r2 = r1 + array([l2*sin(theta2), -l2*cos(theta2)])
            paths.append([r1, r2])

    return array(paths)",0.488266021,
1630,problem light curve data,"%time evolveInTime(phase1, conc1, Cs1, Cl1, viewer1, \
             phaseEq1, concEq1, solver1, dt1, dV1, \
             solidHistory1, massHistory1, chemHistory1, cflHistory1, \
             totalTime1, currentStep1, totalSteps1, \
             'data/FiPy/1D')",0.4879029095,
1630,problem light curve data,"def get_color_clf():
    df_color = pickle.load(open('df_color.p','rb'))
    X_train, y_train = hf.drive_learning_curve(df_color)
    clf = GaussianNB()
    clf.fit(X_train,y_train.flatten())
    return clf
    
def predict_color(r,g,b):
    pred = clf.predict([np.array([r,g,b])])
    return pred
    
if (1):
    # getting locations of sea lions (color dots) in TrainDotted/*.jpg
    # coordinates will be used to extract sea lion pics from Train/*.jpg
    # using more accurate GaussianNB over if-then for color values
    clf = get_color_clf()
    train_img = (41,51) # (0,947) 
    R,G,B,color,df_color,df_blob = get_blob_coord(train_img)
    pickle.dump(df_color,open('df_color2.p','wb'))
    pickle.dump(df_blob,open('df_blob2.p','wb'))

df_color = pickle.load(open('df_color2.p','rb'))
df_blob = pickle.load(open('df_blob2.p','rb')) 
df_blob[43:47]",0.48758322,
1630,problem light curve data,"def create_python_array(n=100):
    x = []
    for k in range(n):
        x.append(math.sin(math.pi * k / n))
    return x",0.4851242602,
1630,problem light curve data,"for n in xrange(-3, 5):
    alpha = 10 ** n
    print ""alpha %10.4f  OLS %.4f Lasso %.4f Ridge %.4f"" % \
        (alpha, 
         -cross_val_score(LinearRegression(), X, y, cv=10, scoring='mean_absolute_error').mean(),
         -cross_val_score(Lasso(alpha), X, y, cv=10, scoring='mean_absolute_error').mean(), 
         -cross_val_score(Ridge(alpha), X, y, cv=10, scoring='mean_absolute_error').mean())",0.4798999131,
1630,problem light curve data,"def MyCust(x):
    if len(x) > 2:
        return x[1] * 1.234
    return pd.NaT",0.4798767567,
2361,the python numerical stack,"s = Stack()

s.print_stack()

for i in range(0,10):
    s.push(i)

s.print_stack()

s.pop()

s.peek()

s.print_stack()

s.size()

s.peek()",0.459982872,
2361,the python numerical stack,"class Stack(object):
    def __init__(self):
        self.stack = []
        
    def is_empty(self):
        return self.stack == []
    
    def push(self, data ):
        self.stack.append(data)
        
    def pop(self):
        if self.is_empty():
            raise Exception(""Stack is empty.."")
        data = self.stack[-1]
        del self.stack[-1]
        return data
    
    def peek(self):
        if self.is_empty():
            raise Exception(""Stack is empty"")
        return self.stack[-1]
    
    def size_stack(self):
        return len(self.stack)
    
    
##main
stack = Stack()
stack.push(1)
stack.push(2)
stack.push(3)
print(stack.size_stack())
print(""Popped: "", stack.pop())

print(""Popped: "", stack.pop())
print(stack.size_stack())
print(""Peek:"", stack.peek())
print(stack.size_stack())",0.4479969144,
2361,the python numerical stack,"x = iter([1,4,9,11,16,18])
print(x.__next__())
print(x.__next__())
print(x.__next__())
print(x.__next__())",0.4460631311,
2361,the python numerical stack,"class Stack(object):
    """"""
        Implements a basic stack which supports max operation.
    """"""
    def __init__(self):
        self.max_ = None
        self.data = []
    
    def pop(self):
        if self.empty():
            raise ValueError(""Can't pop off an empty stack"")
        #get cached max
        self.max_ = self.data[-1][1]
        return self.data.pop()
    
    def push(self, val):
        if self.empty():
            self.max_ = val
        self.data.append(val, self.max_)
        self.max_ = max(self.max_, val)
    
    def __len__(self):
        return len(self.data)
    
    def max_(self):
        self.max_ = self.data[0]
        for i in range(1: len(self.data)):
            if self.data[i] > self.max_:
                self.max_ = self.data[i]
        return self.max_
            
    def empty(self):
        return len(self.data) == 0
    
    def top(self):
        return self.data[-1]",0.4413142204,
2361,the python numerical stack,"def DoubleMedWall():
    return STRUCT([CurvedMedWall(),S(1)(-1)(CurvedMedWall())])",0.4397442341,
2361,the python numerical stack,"def DoubleExtWall():
    return STRUCT([CurvedExtWall(),S(1)(-1),
                   CurvedExtWall()])",0.4372484684,
2361,the python numerical stack,"def DoubleWall():
    return STRUCT([SingleWall(),S(1)(-1),SingleWall()])",0.4372484684,
2361,the python numerical stack,"class Stack:
    def __init__(self):
        self.top = None
        self.size = 0
        
    def push(self, data):
        node = Node(data)
        if self.top is None:
            self.top = node
        else:
            node.next = self.top
            self.top = node
            
    def pop(self):
        if self.top is None:
            return None
        node = self.top
        self.top = self.top.next
        return node.data
    
    def peek(self):
        return self.top.data if self.top is not None else None
            
    def __iter__(self):
        current = self.top
        while current is not None:
            yield current.data
            current = current.next",0.4340775013,
2361,the python numerical stack,"x=iter([1,2,3])
print x.next(), x.next(),x.next()",0.4318743646,
2361,the python numerical stack,"large_np = %timeit -o for i in in_: fwd.forward(i, nlevels=nlevels)",0.4294362962,
2018,step bayes theorem implementation from scratch,"# define the objective loss function that needs to be minimized
KL = 0.5*mx.symbol.sum(1+logvar-pow( mu,2)-mx.symbol.exp(logvar),axis=1)
loss = -mx.symbol.sum(mx.symbol.broadcast_mul(loss_label,mx.symbol.log(y)) 
                      + mx.symbol.broadcast_mul(1-loss_label,mx.symbol.log(1-y)),axis=1)-KL
output = mx.symbol.MakeLoss(sum(loss),name='loss')",0.4686012268,
2018,step bayes theorem implementation from scratch,"# define the objective loss function that needs to be minimized
KL = 0.5*mx.symbol.sum(1+logvar-pow( mu,2)-mx.symbol.exp(logvar),axis=1)
loss = -mx.symbol.sum(mx.symbol.broadcast_mul(loss_label,mx.symbol.log(y)) + mx.symbol.broadcast_mul(1-loss_label,mx.symbol.log(1-y)),axis=1)-KL
output = mx.symbol.MakeLoss(sum(loss),name='loss')",0.4686012268,
2018,step bayes theorem implementation from scratch,"laplace_polar = FinDiff(0, dr, 2) + Coefficient(1/R) * FinDiff(0, dr) + Coefficient(1/R**2) * FinDiff(1, dphi, 2)
result = laplace_polar(f_polar)",0.4637383223,
2018,step bayes theorem implementation from scratch,"def orthocost(a):
    return ((torch.mm(a, a.transpose(1, 0))-torch.eye(a.size(0)).cuda())**2).sum()

def l1cost(y):
    return (y.abs().sum(1)-1).abs().sum()",0.4623910785,
2018,step bayes theorem implementation from scratch,"fit(model, data, 4, opt, F.mse_loss)",0.46162799,
2018,step bayes theorem implementation from scratch,"# 8

doubleInt3(2*x*y,[x,y],[0,1/2],[0,sqrt(1-x)])",0.4613884687,
2018,step bayes theorem implementation from scratch,"# start training loop
fit(model, data, 3, opt, F.mse_loss)",0.4603719711,
2018,step bayes theorem implementation from scratch,"fit(model, data, 3, opt, F.mse_loss)",0.4603719711,
2018,step bayes theorem implementation from scratch,"# Looks like it *could* approach a value. But I already know it's a divergent series.

n, N = sy.symbols('n N')
fn = sy.summation(1/(2*n+1), (n, 0, N))
f = sy.lambdify(N, fn, 'numpy')

# Note: it appears that sympy doesn't vectorize sums along the index arguments (e.g. N)
x = [10, 20, 40, 80, 160, 320, 640]
y = [f(n) for n in x]

plt.figure(figsize=(4,4))
plt.plot(x, y)
plt.show()",0.4592860341,
2018,step bayes theorem implementation from scratch,"plot(
    exp(x),
    (
        1 + 
        x + 
        Rational(1, factorial(2)) * (x ** 2) + 
        Rational(1, factorial(3)) * (x ** 3) + 
        Rational(1, factorial(4)) * (x ** 4) +
        Rational(1, factorial(5)) * (x ** 5) +
        Rational(1, factorial(6)) * (x ** 6)
    ), 
    xlim=(-5, 5), 
    ylim=(-5, 5),
)",0.4573640227,
1585,print the unique districts and how many there are,"# Below we create a district function based on the types of destricts in a city
def types_districts(dataset,per):
    
    # Group by crime type and district 
    hoods_per_type = dataset.groupby('Description').District.value_counts(sort=True)
    t = hoods_per_type.unstack().fillna(0)
    
    # Sort by hood sum
    hood_sum = t.sum(axis=0)
    hood_sum.sort(ascending=False)
    t = t[hood_sum.index]
    
    # Filter by crime per district
    crime_sum = t.sum(axis=1)
    crime_sum.sort()
    
    # Large number, so let's slice the data.
    p = np.percentile(crime_sum,per)
    ix = crime_sum[crime_sum > p]
    t = t.loc[ix.index]
    return t
    
t=types_districts(dataset,96)",0.5017706752,
1585,print the unique districts and how many there are,"# A helper method to explore some of the outputs. Keeping it simple for now.
def quick_tear(symbol, fully_shifted):
    print(""{}:"".format(symbol))
    # convert bools to ints for % on mean
    print(fully_shifted[['above', 'below', 'between']].astype(int).describe().loc[['mean']])
    # plots the last 60 days for a visual
    fully_shifted[['high','mid', 'low', 'zz_pred_y']][-60:].plot(figsize=(18,8));

quick_tear('SPY',fully_shifted)",0.4772568345,
1585,print the unique districts and how many there are,"#Function to compute survival rate
def survival_stats(data):
    survived = data.groupby('Survived').get_group(1)['PassengerId'].count()
    total = data['PassengerId'].count()
    print (""Total Passengers: {}"".format(total))
    print (""Surviving Passengers: {}"".format(survived))
    print (""Percentage of passengers who survived: {:.2f}%"".format(survived*100.00/total))",0.4706047177,
1585,print the unique districts and how many there are,"for state in states_fips:
    print (state, fips[state], int(dataset[state]['POP010210']),\
    sum([int(dataset[cf]['POP010210']) \
               for cf in county_fips_for_state(state)]) == int(dataset[state]['POP010210']))",0.4660781622,
1585,print the unique districts and how many there are,"def col_percent(dframe):
    '''A funciton to loop through columns and return the percentage of populated items.'''
    for item in dframe:
        print('{} = {:.1f}%'.format(item, 100 *
                                    dframe[item].count() / len(dframe)))",0.4622072577,
1585,print the unique districts and how many there are,"for (cont, country), group in df3.groupby(['continent', 'country']):
    print (cont, country, sep="": "")
    print('-----')
    print (group)
    print()
    print()",0.4610059261,
1585,print the unique districts and how many there are,"# Solution: This cell should contain a worked solution,
# which may be both used by an autograder and/or may be 
# shown to the learner. Comments on what is happening
# throughout are welcomed!


def solution9(df):

    group_by_region = df.groupby(['Region'])
    group_by_region.size()
    #grouped = df.groupby(lambda dframe: dframe['Region'])
    df_stats = df['2015'].groupby(df['Region']).describe()
    df_stats = df_stats.astype(int)
    df_stats
    #which regions that have at least 4 countries in them
    # has the highest median (50%) per capita spend?

    # find the index of those regions with >=4 countries
    idx = df_stats[ (df_stats['count']>=4)].index.tolist()

 # now find the region that has that spend
    max_median_spend_region = np.argmax(df_stats.loc[idx,'50%'])

    max_median_spend = np.max(df_stats.loc[idx,'50%'])

    region_max_median_spend = [max_median_spend_region, max_median_spend]

 

    return region_max_median_spend

a = solution9(df)
a",0.4555903673,
1585,print the unique districts and how many there are,"def observe_matching(matching, verbose):
    sum_differences = 0
    for (a, b) in matching.items():
        propensity_score_a = data_propensity.loc[a][""propensity""]
        propensity_score_b = data_propensity.loc[b][""propensity""]
        diff = abs(propensity_score_a - propensity_score_b)
        if verbose:
            print(""Difference between {} and {} is {}"".format(a, b, diff))
        sum_differences += diff
    print(""Mean difference in prop score is {}"".format(sum_differences / len(matching)))
    
observe_matching(matching_propensity, False)",0.4554390907,
1585,print the unique districts and how many there are,"def popular_stations(city_file):
    '''This function calculates the popular start station and the end station
    It takes city dataframe as the argument'''
    start_station=city_file.groupby([""Start Station""]).size()
    end_station=city_file.groupby([""End Station""]).size()
    print (""The most comon start staion is {} and its count is{} "".format(start_station.idxmax(),start_station.max()))
    print (""The most common end station is {} and its count is {}"".format(end_station.idxmin(),end_station.max()))",0.4552053213,
1585,print the unique districts and how many there are,"#Function to compute survival rate by gender and travel class
def statistics_by_gender_and_class(sex,Pclass):
    print (sex + ""s in class "" + str(Pclass))
    titanic_data_by_class_and_gender = titanic_data.groupby(['Sex','Pclass']).get_group((sex,Pclass))
    survival_stats(titanic_data_by_class_and_gender)",0.4549424052,
2366,the stream object,"class StreamProcessor:
    
    def __init__(self, stream):
        self.stream = stream
        self.pointer = 0
        self.groupValue = 0
        self.groupValues = []
        self.garbageMode = False
        self.garbageCount = 0
    
    def processStream(self):
        while self.pointer < len(self.stream):
            currChar = self.stream[self.pointer]
            if self.garbageMode:
                if currChar == ""!"":
                    self.pointer += 2 #Skip the next character
                    continue
                elif currChar == "">"":
                    self.garbageMode = False
                else:
                    self.garbageCount += 1
                self.pointer += 1
            else:
                if currChar == ""{"":
                    self.groupValue += 1
                    self.groupValues.append(self.groupValue)
                elif currChar == ""}"":
                    self.groupValue -= 1
                elif currChar == ""<"":
                    self.garbageMode = True
                elif currChar == "","":
                    pass
                else:
                    raise Exception(""Invalid character - {}"".format(currChar))
                self.pointer += 1
        return self.computeGroupTotal()
            
    def computeGroupTotal(self):
        return sum(self.groupValues)
    
    def computeGoodCharCount(self):
        return len(self.stream) - self.garbageCount
    
assert StreamProcessor(""{}"").processStream() == 1
assert StreamProcessor(""{{{}}}"").processStream() == 6
assert StreamProcessor(""{{},{}}"").processStream() == 5
assert StreamProcessor(""{{{},{},{{}}}}"").processStream() == 16
assert StreamProcessor(""{<a>,<a>,<a>,<a>}"").processStream() == 1
assert StreamProcessor(""{{<ab>},{<ab>},{<ab>},{<ab>}}"").processStream() == 9
assert StreamProcessor(""{{<!!>},{<!!>},{<!!>},{<!!>}}"").processStream() == 9
assert StreamProcessor(""{{<a!>},{<a!>},{<a!>},{<ab>}}"").processStream() == 3

day_9_parta=""""""{{{{{{<!o!>!!!!!!"">},{<e!>!!>}},{<},!,!!!>}e!!!>{!>{,i!!!>,<}>},{<a!!!>e'!!!>,<!>!<'o'!!{au!{!>},<}!<>}},{{{<eu<!!!>!!!>!>!>,<!,oo>}}},{{{},{<ai!>!>,<e!>},<}!!!>e>,{{<!u!>,<,a<ae!>,<'!o'!>!u{!!!>,<,!!!>>}}},{{{<!!!oe}e},}!>}{e!!o!!,o'{<a>},<ei,{!!!>{!!{a!!!><<!{>},{{<>}},{{{{<{!><ei>},{{}}},{{},{<,!u!<o!!!>e!>},<!>!!!>,<ia!>!!!>!}}""{>}}},{}}}}}},{{{<!>},<,!!}<i<!><!a!o!!<>},{<'""!,!!!>},<!!!>,<i!>oo"">,<>},{{{<!>},<''!>},<!>,<!!<}!ea!<!>},<o>},<!i!oo>},{{},{<a!>,<!e"",'}>}},{{<!!!}iu!>!!,!!iau!>""""!>},<!>e{i}>,<""!!>},{{<<}""!ou!!o,>}}}}},{{{{{<!!!>'!!{!>,<>},<!>},<!'!>},<u!>""<,!!!><i!!!!,!>u'!>},<!>a>},{{{<"">},<!!iu!!!>'e!>,<<{,!!a{!>},<u>}}},{{{}},{{{<!!!>},<!!!>,<u'ae""!>>}},{{<""!""'!>,<<!!!!!>!>},<>}}}},{{<!}!<>},<e!!!>!!i,!!ao!>,<!>},<>}},{{{{<i!>},<u<!""!>e"",!i!>i,}!>!!!!!>!!!!!>>},{<!!!>!!'!!!!!!!!!!!>,'i!!aau,!>!!>}}},{{},{<,!!!!,o!!!>},<!>,<o!>},<"",!!!>u!!'eou!>>,{{},{<!!!!!>!""{<e!>},<}}>}}}}},{{{{{<>,<e<!!!>},<!""!!!}!i}!>},<e<,>},{{{<,!o!}>}},{{<u,'!!,o!!!>!!!>ui!>},<!'""o!>,<',!o>},{<!>},<!!i!i!>uiui""}<!!!>!!}!!!>a}>}}},{{<!!!!!!a"",""io<,ea!!!>},<!!!!!>o!>>},<!!a!>,<!>!u>}}},{{{<!>,<e!!!>,<<!>,<!e!!!>'e""{io!>!>ie>,<!u!>},<!>!!e!>!>,<!},!!!>>},{{<!>,<o!!'!!!>},<!><!>,<>},{}},{<>,<a}oouu!>,<e}!>e{{}uu>}},{{{<a!!!!!>,<'!>,<!e!>},<!>e!!}!!!>}>},{<aa{!>,<!!'e,<o,!!!>o<!!!>o>}},{{{}},{<!>!!!>'>,<!{!!iu>}},{{{<""!!!'!!,!>,<,!""!>},<{!!,}{!!}!>o!!u>},{{<!<o!u'!>,<!!!>},<,!!e<!!>}}},<>}},{{{{{{},{}},{<""!>,<!!}!>,<!>,<{},}<!!!}>,{}},{{<<!>},<i!!!>},<ii!!}!!!>},<!!""a"",>}}}},{{{{{{{<a}>}},{}},{{},{{<i!!!>a!!i}}o!!!>!>,<<}a'!!!!!>},<!>},<o!!!!o>},{<a{!>!>,<!<}!!!>,<!>},<<"">}}}},{{{}},{<>,{}}}},{{<!!!>'o<ei>},{{}}},{{<<!!!""e{,!>!!,,!!!!!>{""u!>}!a>}}},{{{<!!!>},<'!>""}!>,<!!!>>,{{},<ua!!!>i<u""!!!!>}}},{{},{{<a""a!>},<!>},<i!!!!a!>},<<'!!i!!!!!o!!!>i<a>},<!!!""!>},<a>}}},{{<!i'o!!e}<!>i{>,<{u,!!e!!{""'o!!!>,<!>},<!>,<>},{<i'!!u!>,<""""<!!!>!!!>!!!>!!!!e'!!!>!>},<}>,{{},{{<'e!!!}!!i}{a!!!>{!}!!!!!a>}}}},{<!>,<o}ui!<e!>,<{>}}},{{<>},{{{<""!!!!,}}!!!{!>},<i"",""!!!>,<e>}}}}},{{<!!<iiu!!!!u!!!o>,{<!!!>},<""'>}},{{}},{{{{<!!eu}!>a!!a!!!>>,<u{{i!!'>},<!'!!!!!>i,}!!""""!o!!!>>},<'!!}!>,<<u""!>,<!>i!{i!>,<!<<!!!!,!>,<"">}}},{{{{{<!>},<!!<u!,e<o>}}}},{<!>,<!!!>!!""<<>,<""!>},<!'!>,<!!!!!>},<e!!iu>},{{{{}},<o!!e!!!!!>ei!!'!>,<<!!<}o!'i!>>},{<!{uui!,>}}}},{{{<!>,<{!><!!!!>},{}},{{{<'i>},<!'!>!!o!}!!<e!>!>,<,a>}},{{},{{<{iu}""!>,<e>}}}}},{{{<!!!>!!!>!!!><u!>,<!,!!!!!>,<!>,<""!!>},{}}}},{{{}},{{{},{}},{<'u!>},<!!!>},<!>,<!!}!i,'!!!>!!e>},{{{}},{<!>,<<!o!!,!!!>""u!}a<}>,{{<!!!>,<eo!>{!!{!>,<<!!,{!>},<a!!!!e!u!>>},<i""}!!""o<oi>}}}},{{<'!>},<ie>},{<!i!>,<!>,<!}}!>}""{!!}>}}},{{{{{<}!u>},{<!!!>oa{!!!!!>o{!!a!!!>!>,<i!>},<',!>{>}}}},{{{{{<!!!>},<'""!!{,e>}},<<,!!!,!}i!!<',u>},{{{}},{<!!''u!e!!!>!>},<i!>,<!!<!'o'!>!!!>!!!!!>!!!u!!>}},{{<ee!>!>,,!!!>,<!!!>e!><}!<>}}},{{<!!!>},<!!}!!!!{i""}!!"">,{}}}}},{{<!!<!e{'>,{<ee>}},{{<u!!!!{<u!><iooa'e<uo>},{<!>uou""""!>,<!>},<!!!>},<u""u>}}}}}},{{{{{<!>,<!>!>},<!>a!>},<""{eeu!a{!>,<>,<!!!>>}}},{{{{<uea}""!e!>>},<<!!'!!!>!,{'o!>},<!>!!!>,<e{o,'}>},{{<o}e!!!>!>},<!>},<}!!,!!a{ie{<u}>},{{{}},{{<>}}}}},{{<}i!!!>!>},<}""e!>,<!e<>,{<'!>!!!>a!!{'oe}!!oo>}},{{<i}i""<!>,<u!>u!!!>!!!>},<e>}},{<,}!!<a!>o!>,<!""<>,<!!!>!>},<a!!e'!'u!>{,{!!o}>}},{{{<oa!!!>!!!>!>a,,,!!oe,o!>!!'!!!>>,{<}""!!<""i!>,<!>,<aa}<{u"">}},{<!!a!!<!!!>},<'>},{{<"",!!!>!>,<,a!}<""}!>!!!>!>},<!>,<>},<!>,<!!<,>}},{{{{},{{{{},<!!!>},<,<u}<oe!""<!>o!!!>""!!!>!i>},{<<}oa!ae<e!!ooo'>,<!!,!!u!>},<u!!!!""<!>u!>!e""!!!!!>{u<!>},<!>,<>},{{},<>}},{{<!!!>u},{!!!>,<{!>,<a!>},<<!>u>,{}},<>},{{{{},{}},{<!>!!!!!a!!!>!e,""!!!>>,{<!>,<!>},<ieo!e>}},{{<e!>,<''!>,<<ia!i''>,{<{!>,<<u}!!!>u,e,!>!!!>},<>}}}},{},{{<!}!!u!!!>!!!>i<o!,'!!!>{!!o!!iu,!!!>>},{{<o'u!>,<>}}}}},{{<!>},<!!u>},<oi""""uo!>'!!i""uuu!>},<""{>}}},{{},{}},{{{}},{}},{{{{<!>,<>}},{{{{{}},<>}}},{{<u<<!!o!>,<u<!>ai!!!>}>},<!!""!>},<'}e""<""''!>>}},{{{<!!}!{eao!!!>},<!>,<i>}},{{{{{<ou!!<!>},<!!!>>}},<o!!!!u!!!!!>!'!>o!io>}}},{{{<iu,}}!>!!!!ei}!>,<'!>,<,!>},<{!>!>},<{>}},{<{!>,<!!!!}!!!!!>!>,<>,<!!{!><a!>,<i!!!>},<<!!!>!>!>,<!!!>,!!u!io>}}}}}}},{{<>},{{{<>}},{{<!>,<,iea{>},{}}}}},{{<}<<eo!u""!>,<!>},<u!!!ee""!!!>},<>,<}ii!!!>},<<a!!<ua>}},{{{<!!!>!>,<,'!!<>,{{},<<oee!>},<'a!!!>a!!>}},{<!>,<!>},<,a!!!!!>aau!>},<!>},<!!>}},{{},<!!{<<""""!>i!!!>!!!>!!a!>,<}!!!>!!'!!>}},{{{{<,,oa!!!!!>u!!'ee!!!!!>""""a{>},{<o!""'u!!i>}},{{<e>,{<!>},<!!{o!>,<!,!>,<!!!>u""ou,{,>}}}}}},{{{<o>,<!>},<e!ui!!!>e!>},<!!e}!!!>'o!!!>!!{a!!!,>}},{{{{<!!ae<oa!!!!!!!>>}},{{},{{{<!>,<!ei!>!!!!!>'i!!!!{,""""!,iui!!!>>}},<!!!!!>!!!>},<iu!>},<""!>},<ee!!u'o!!{}{!>,<!!!>>}},{{{<<}u!{e>,{<!!!>>}}}}},{},{}},{{<!>>,{}}},{{<}>,{<<!,!!uu}'!!e,!e""<e!>},<!!o!!""o>}},{{},{<u'<!!!>!e!>},<u}<''""!>,<!!!>},<!>},<ae!!!>},<>}},{<!i!!!>!>,<<!!!>a!'!!o{!!,!>},<u!>},<o}o>}}}},{{{{{<i!!!!!>!>,<u""e!!!!!>!><<""!u""!!!!!!{!!>},{<e>}},{},{{<o!!!>o!!o"",>,{<e,i!>!!!>},<!>},<!'!!!>{!>,<',!!!>ei!!!!>,{<!!!>,<!!'!<e>,<!>,<,i!!ei!>{!!!><!,euo!!i""e"">}}}}},{{<!ee"",>},{{},<>},{{<}!o<a!>!!!>u<!>,<!!ua""'u""!>u!>,<!>,<>}}},{{{{{{{<!'i{!!o,{""eo!>{i>}},{<!>{!>},<<e!{""{!>,<i''>}},{<>,<!!!>},<{>}},{{{{<""e!<uu!>,<,!>},<!u,>},{}},{{{{<!{!ae!>},<'!>,<!,>,{<{!><}a'!>,<u!>,<,!!!!,e!!i{<'!>,<i>,{<e!!{ia!!o!!!>!>,<>}}},{<!<'{<!>,<}!!<""<!!'<<'ae}}!>,<a>}}},<,{o!>!i!!oo""!>},<!<>},{{{<u!i}u{>},<!!!>'{}!!!>,<{!!u,>},<eea!!'e!>,<!>},<{!>,<{!!i!>e>}},{{},{<e!!!}aoa!>!e!>,<ue}o!>},<>}},{{},{<{!"""">}}},{{{{<!au!!o!""!>},<oo}>},{}},{{{<>}},<""""a""!>,<i<!!!!a!>!!}!>,<u'!!!>>},{<aaaaa<'uai{io>,{<o>}}}},{{<a,!!e,{{o'!>""e,<>}}},{}},{{{<""!>,<!!!u<>},{<u!!>}},{{{<"">,{<>}},{<a""oe!!!>!!<>}}},{<!>!!!!!>ooa!!}!!!>,<!!!>!>,<aa!>}'!>},<>,<i!>,<!>,<e""<u!>u>}}}},{{{{<o!>},<,'i!>oe{!!<!!!>!!}>},{{<i!!i<<!!!!''!!!>,<'!!!>!>},<!>!!!>>,{<u{!!!>!>!>},<>}}}}}},{{{<!i<{,uea!>},<,{'o!!o!>>},{<u,!!!!}!!!>},<>,{{{{{<!!,!!!!}'!>!!!>,<!>},<}!>,<!>!!<!>,<'o!>},<e}!>,<i>}},{}}},{}}},{{{},<,<'u>}}},{{{},{<!>},<}!!iau!!a}<!>o!!!>,<e!!u!!u'!!!>>},{{},<!!'<!!!>'}!!{!!}i""'!a,!>},<<e>}},{{},{{<uu,'!!!>e>},{}}}},{{{<!a'!!}!!{<{}oe!!!!!o}!>},<>,<!>},<e!!!>'o!{>},{{<!>e!>,<,!!!>i!!!>{!{u'!>,<,""!>,<eu<{>},{{},{<,e!!{{!!u!>}!e!!i{ie}!>},<a'ou>}}},{{{},{<e!>},<'oe""o!>,<>}}}},{{},{}},{<!!!>i!!{<u!!!>!>,<!>,<!>,<""{'<e},>,{}}},{{<!u!!,o,}!>,<ie!>,<!o!>},<u!,!>,<!""o>,<!>},<a!!!!{!!{!>},<a!>!>},<!>,!!!>>},{<!>},<!u!>,<!>o!!!!!>o<!>},<e<!}}!}e>}}},{{{{<}!>,<}oi>},<!!!!}!>},<!e!!e>},{{{<!!!>>}}}},{{{<!>},<i!!!!!>},<>,<!>!!}!!!!!!{i!!a>},{}},{{},{{{<!i!>,<{}}a!!!!!>iu!>!!ie,!!!>i!>,<{>}},{{{<u!}i!>},<>},{<!>,<o!<!!!!!>!!!>!!e}""!!>}}}},{{},{{<!>o''""!>!ue}!>!!<""}!>!!!>!!{>}}}},{{{<""e>},{<'!i!>,<u>,{<!!u!>},<e!!!>>}}},{<""<!!!><""}u,!>e!!u'{!>},<ui!!!!,>},{}}},{{<!!o""uu!<!>},<>},{}},{{{<u!>>}},{<!!}>},{<i!!!>}i!>>,{<!!oa!>!!!!eo}o,!}oia!!!>!!!>,<o}>}}}}},{{{{{<>},{}},{{{<a<','!!!!a""{!>,<!!{!!,}'!>!!<!>},<>,<>},{{<}a!!!>"",!>,<u!>!>,<!!"",!!!>},<!>},<oioo>}},{<u!>},,>,{<o{!>},<!!""'""u!>,<oiu!>,<!!o>}}}},{{{{<!!,{,!!,!!i!!!>,<!,i""!>},<!!>,{<!{<!>,<u>}}}},{<a!!{}!!!!<>,{<eee'>}}},{{},{{<}uo!!{<aae""!>,'>},{<!!!>,<!}!<>}}}},{{{<}<{>}},{{{<<!!!!<!>},<e}'}ea!!!>!!u!>>},<!>},<{""!>,<!>},<!!!>!>},<!>,{e!>,<'oa!!!>u>},{{{<a'{!u!!<{!>},<a!>'ea'i>},<!!!>ii!><u}!>},<!io!>,<i""!>},<!>,<!>},<!>>}},{{},{<!>o'!>!>""""!i!!!!e!!!>!!!}i>,<<!>},<!!oai}o!'!>,<o!>,<!>!!i{!}a!!!!!!!>>}}}}},{{{<i}!!uee'""iu'>}},{{{}}},{{{<!>},<<{e{i!>,<o!>,<,!u>},{{<!!<""{<!a>}}},{{},{{{{<!>,<!{!!a!!!>!!!>!!{!<!>},<,<"",""!!o>},{<i,},!!!>>}}},{{{<<!!!>!>,<!!!>},<{<u>},<<!!!!i>},{}}},{<!!!!e{!>},<ao!!,!{i{i!""}u!!!>,e'>,<""!!!>,<u!!!>,<!><!>>}}},{{<""}!>!>},<!!!>!>,!!'a!!!!!>""!!!!o!!{""!>},<u,!>>,{<!!,""""!>!u!!!>!!{!>!aaa,>}},{<,ee!!i>},{{{}},{{<!!}{i!e}{!!e!!}}!>},<{!!}!!!><!!>,<!>,<!>,<!>}!>,<o!>},<{,i""<e!!}!!!>,<>}},{{<!>,<!!!!!>},<<!!!>!>},<e'}!>,<!!!!!}{o!>!>,<>}}}}},{{{},{{{<'o""<',,e>}}},{{{<!!!>""o!>},<u""u<,>}},{{<ei}<u<}!!>},<e""!>u,!}!>,<>}}}}},{{{{<i!!<"">,<!!!!{a,ae!<!>,<>}},{{<!!i>,{}},{<,!!e!>},<i!>>}},{{{<!!!>},<}>},<{a!>!ao!!!>},<e}{o>},{{<u!>,!>""'e{!!!>,<,!}au!>},<!'o!!<>},{{{<{',{}o!e!>,<>}}}}},{{{{<!!""<e!""'""a>}}},{{{{<!!!!i!>!<}<!!!>>},{<eo!>,<!>!><,!>!!!>!!!>},<>}}},{{<{>},{<!>,<u!!,}!'!>},<!>'!>!!{!!!>}!!>}},{{<<!>!',}e!""!!!>!!!>a!ie!!!!!!i>,{<o!""!!!>},<!!!!!>>,{}}},<!>},<,!>""'i>}},{{},{{<oia>},<""'aueu!!u>}}}},{{{{<!o<<!>!{!>u'e'!!e,iu>},{<','!>,<!!!!{!>},<}u""ue>}}},{{<!!<<uo!!!>},<!>,<!>,<i!>!""i!!o!!{!>,<u""e}!{>,<}iu!{ou!u'""!!}!!<,!o""{>},{<!>,<}!>a!!!>!>,<!!,!>,<ao!>},<>,<!!!>,'!!!a""'!!<!!!>,<<!>{!>,<o'<!>,<a}>},{{{{<!!""!'!!!>>}},{{<i>},<}{}<!!!!}e""{!!!>!!!>!!!!!>},<!aee!>e>},{<}u'<<"",!e!>},<!!!!a!>,<i!!e>}},{{{},{}},{}}}},{{{<!>,<""!!!>ea!>!!}!>oa!!!>,<!>},<i}""io>},<a'!!i}!>},<}!uii!!i!>},<!>,<!>,<u!a!!>},{},{<!>},<'u{u!!<'!!!>e<a}e!!>}}},{{{{{{<u,ia!}{}u{!!}>}},<!!""!!}!!a!!o}""!>{!!<!>},<'!!o>},{},{<}!!o!!!!u!!!""o{!!}!!{!>,<ui""!!!>,<'>}},{{<>},{{<u}u{'e<}!'!!,!a""!>>},{<!>},<>,{}},{{<o!!!>u!>},<!>,<!!!!!'e!>},<!>i!ou!>},<>},<e!!!>a!!!>}!!!!!!>}}},{}},{{<'u!oi}!!!>!>},<!>},<>,<ie!!!>},<e!>,<u>},{{<e{a<a!!}'ia{i!!!>},<!!i}{>},<a>}},{{<!!!>!!o""!!'i!>,<!i!>e!>,<{!!!>""'o>,{}},{{},{<{}io}}!o!!!>},<e!!!>>}}},{{{<""!>{!!{!!!>!!!>{oa}'u!>},<u>},<'!u!>!>u""'!e}!!!>'!!!!ee!!!!!!!!!!<>},{{<!>,<u!>,<'<""o!!!>i!>!""!!i>,{<o{>}},{<!>},<!>},<a!!<ia!!!>!!!>i!""u>}}}},{{{},{{<u!!!!""i!!!>!>,<{!!!>}>},{<!!!>'!>,<!!!>!!i!!!>,<!>,<""<i!!!!!>!>a!>},<"">}},{{<ou!!!>u!!}>}}}}},{{{{{<<!!e}!i!!'io<{!>!!!!>},{<>,<ii{e>},{{<>}}}},{{{}},{<a<!>},<!>,<!!!!<!>,<o!""e{!,e>,{<!!u}!>,<u}!>oa!>>}},{{<!!!>!!!!{'<<i<u!>},<a!>e>}}}},{{{{{{<!>},<e!>},<o'e}ie'i!>},<!!!>>},{{<ei""!!!>!>aa,<<}i!!e{a>,{<!>,<<!>},<iu!o!>,<}a!>e!!!>!>,<o!>,<>}}},{{<'i{ea<<{>},{<{!!!>,>}}}},{{{{<{!>},<o!""e!}!>,<<>},{<!>,<!>},<!>!!<'!!oae<'>,{{<!!!,io!>,<"">}}},{<<!>},<>}}}}},{{{{{<!!!><u!!""a!!!!!>!>,<!{!>},<}<!>},<!!!!a!!!""!>!>,<<>},{<!!!>!!<!>},<!>,<e{!>!!{""!!!!""eo>}},{{{{<!>,<oi!!u<u!!!>!!""{{u>}},{{<>,<!!!>>}}}}},{{{<o!!!>a!>},<e!!u!>!>,<{{!>},<!!!au""e!>{>},{{{{<!>,<i!>},<>},<>},{{<!{!>!>!!i!>!>,<>},{<'<<e>}}}}}},{{<!ia>},{{<<ao>}},{<,"">}},{{{<!!u{i,!>!!!>{,!{,!!>},{{{{<!i}!!i>},<!!!>e!>""}>},<<!!{!>,<!!""""<!!!!!>""!!{eu!!,>}},{}},{},{{{{{},<!"",uu,u}!>>},<{{!>!!!""i!!e'!>},<!><i!>!!!>!!>},{{<u,o!>,<<u!!!>ii!!u!>,<!!<!!!>!>},<!!a!>},<i>},{<{a!>},<>}},{{{<!!'o!!!>!<!!!!'>,<,!""e}}u>},{{{{<,>},{}},{{{{},<'>},<!!!>,<i!!!!!>!>,<!>,<,<,!!o!>}!!,!>},<!>>}}}}},{{<u!!{!>!o!>},<ao'i!!!>'u>,<!>},<ae""i!!u!>,<!<e,'!!""!>'>},{{{{{{},<<""!>e!>,<,}u""!u{!>>},{<>}},{<'!>},<{""""'{u,>}},{<ea!!!!!>o!!!>,,""'!!!>},<i}{!!""!>},<!!i}<>,<}<>},{{{{{<o!>,<!e!!!e!e<!>,<i!!!!a>}},{<!>,<!!!>u!!a>,<!ii}'!>,<!{{!!!>!>!!>}},{{}},{}},{{{<!>},<<i!a!!!!e!>,<o'!>},<,!!!>!!<>},{<!!!>o""!>},<}!!!>!i""{<!>!>},<,,!>!>,<>}}},{{<!!!>'!!!>},<i!!<!>ui{""{,a!!!,i!!u'!!>,{<!>auo!!!>!>},<!a}'!>,<!!!>!>,<'<uo!>,<,o>}},{{{{<e!>,<!!'{!!!o!<u!>!!!>a>}},<},!!!>!!ae!!!e""!!}!!!>!!{!>,<>},{}},{{},<!,>}}}},{{},{{<!!<'!!!!!!!!!>!!!>!>},<i'!>},<e,!!,>}}}}}}},{{{{{<}>,{<{!>!>,<!!!>>}}},{{<!',!oe}!>,<<!>,>,{}}},{}},{{{<'!>},<!>,<ee!!!>!!{!>!>!>},<!!!>u!{!>},<{<>}},{{<u!!!!!!!!,uu!>io,>},<oe>}},{{<!u!!!>!>o""!!""i<>},<a!>},<!>,<!>,<>}},{{{{}},<!!!{<'i!>},<<i!>,<!!!!!!!><<>},{{}},{{<e>}}}}}}},{{<""!a!>},<!>},<!>}a<!><}uuo>},{{}},{{}}},{{<e'aoi!>'e'u!>!>},<!>,<}!!!{!>,<>},{<""e'"",e!e<!!!>!o!!{>,{<!!ea'oa!>,<a!!'>}}},{{<!!!!!>'u!!<a>,<!!!>u>},{<!!}!!!>!!}!!{!u}!>,<!>},<>,{<,!!o{""!>,<}""a>}},{{<!>""e!>{},i,}ue!!!>{{}!>>},<}i!>>}}},{}},{{{},{}},{{{<!!!>!>,<!>},<!>},<!,!!!!ia!>,<e}!>},<!>,<<a<>}},{{}}},{{<!!!>,<i!!""!!!>!aia!>},<{>,{<<,o!!}ao!>,<!!!>,<ae>}},{<,!!!>!>},<u!>,<oe!>},<!!,!>>},{<!>,<!!!>>,{<{!!""!<!!a""i!!u<!e!!!>i!e!!!u'!!e!>,<>}}},{{{{{<!}!>,<{!!i!!o!a""ueo!}""!>a<!>,<!!e>}},{<o>,{}}},{{<!!!>!!!!!>aa!>},<>,{<>}}}},{<!!!!'!!!!!>o!>,<!!i!>},<a"">,{{<i,!>},<}}{!o!e!!!<!>},<!>!>,<>},<!'!>!!!!!>o}!>e!!>}},{<!!oi<i!!!!!!!>},<{a{>}}},{{{<!>,<i!!!!e'u!>},<!!!>!>!>},<i!>,<!!!!!>>},{<{i,u!!!>,>,{}},{<ioa!!!>o!>},<!>},<au>}},{{},{<<e!!!!e'!!{}>},{{{<<a!>,<!!!!!>!}{!>,<}!!<!!'e!!!!""!!,!>},<'>}},{{},<!!!>!>,<!e}""!>,<'!!!>,<,!>},<'{!!{}'>}}},{{{{{<!!u>,<!>,<!>>}},{<!>!>},<!!e<i''!>>,{<e!>},<u,!'!>},<!!!!a<,!!>}}}},{}}},{{{<}'!!!!>,<<}'!!'{''!!>},{{{<auio!!'!>},<!!u{!ao!>!!!!!>u!!,e!""i!!!>>}},<!,ao!o!>,<!!!>!>},<u!!i!>},<!!!>{!>,<>},{<!!i!>!!{>,<""!!!><e>}},{{{{}},{{},{<!'!>},<!!!ao{!>>}},{{<!!,{!!a>},{<!a<!!!>!>!>},<}u!>},<>,{{<}'{!>,<a<>}}}}}},{{{{}}},{{{},{<>}},{},{{}}},{}}}},{{{{{<!><!>},<!>,<!!}>},{{<{!>,<<!!{e>},{{{{},{}}},{<!!!>i!!a,!>,<!>},<o!e'""ea!e}!!!!!>a<{>,{}},{{{<>},<!!!>,>},{<,o!!{<!""!>,<}""u>}}},{{}}}},{{<>,<""',""<,!>},<i!!!>!!!>!!e}!!!>>},{<u!>!>},<""u!!!>>,<!i""<!>,<!>>},{{<""e{!e!>!>,<!>!>},<,!>""'!!!!i!>!!'>}}},{{{{{{},{{<{e!!!>!>>}}},{{}}},{{<eeo"">},{<>}},{{<i!>},<'a!!!}{!!!>o,!ui!<!>},<!>,<'!>,<"">},{<'}!>},<<>}}},{<""!>u!u<a<,!""{u{>},{{<ai!!!>},<oi!!u!>},<!!!!!>eu!',!!aui!!""<>},{{<!>,<!>,<!><!!,!}o!!!>,<!!ie,!!u!!'!!!>!!""{o>},{<""!>,<a,}<o!!e!{{!>,<}u}!>},<>}}}},{{{<{{""!!!>!!!>!>,<i>},<'uu!>!!!}{u,a!>,<!>},<<o!!!>>},{{{},{{},{{<o!>!a'{u!!!!!u!!,!>,<e!e<!!i{>}}}}}},{{<<!!!>},<!>,<oue!!!>!!!!!>!>},<a!!!>o!!!>!>},<''e!!<>}},{{<!!!>!>},<o'o!}u!!,!"">},{{{<<u!!!>{},!>},<!u,!>},<""!!!!'ou!!!>eu>}},{<uo!!!>!>},<'!>},<!>,<!!!>{i>,{<{{{'!>ai'!!!>iie!<}!!!>!!!!!>},<}!!>}}},{{{{},<!>},<oou!!!>""i!>!!!!,!>,<!,""!!!>,u,,>},{<oa!>}o{o!''o!!!>>}},{{<!o,<o!>!>>},<uoea!o!ui!!,!>}!!!>}<u>},{}}}},{{{<!>,<'u!i>}},{<>,<,}!>,<<!!!!,!!!>a''e!u}e!>},<!!!>>},{}}}}},{{{{{},<o!>!!}u!>,<{,!!!>e!!<oa!>},<!>},<!"">},{{<'!!a!!,o'o!>,<!e{<o>}},{<!!!><>,{{<!>!>,<ou,!>e!!!>!!}i!!!{!!""!<i!>,<!>,<>},{<>,{{}}}}}},{{<u}<u{!!!!!>i!!'>},{},{<a!>!}}!>a!a!>},<{!!!!!!,u!u>,{{<}'!!u!!!>',,!>,<a!!ai'}{!!e!a>},<<e!>,<,io>}}},{{{{<!!!>,<u{,e!!!>!>,<!!!>!!}!>,<!>,<!!!!!>},<a,{>}},<!!!>!ue}>},{<,!>},<!!""<u,{ai!'e!>,<o{{!>,<!>,<'>}}},{{{{<!>,<ue!>,<iu!>},<{'a!!}!>,<!}!>},<<<!!!>>}}}},{{{{{{<""!!!>!!ue'""<'!>!>},<!!""o'""!>,<!>,<}>},<!!<e>},<!!"",!>,<ou'<""e!!e>}},{{<}oa>,<<e,o>}},{{{{<""!!!!!>e,!!""!!!{!<i!>},<a!oa!>,<{!!{>},{<i<u,',u!uuu!<>}}}},{}},{{{{{<,o!!!>!u!!!!iei!>},<!!}'<a>}},{<'{ai}ui!!u!>},<,<!!i>}},{{{<!>,<iu""<!!!!i{""e}<!>},<!!i!o!>>},<!!!>,<!>,<a!>}!!!>{!>!>,<o}""o!!!>{!>,<'e}>}}}},{{{{{},{{<!!,'o!!!>,!>,<!>,<!}!!!>},<<""!!!!!!!!!!!>{<>}}}},{{<!!<""o!!!>u!!i""!!u!!!>{a}oa{!a!>},<>}},{{{{<!!!>},<""!!!>},<""""a'!!o""ioa""}'>}},{}},{{}},{{<!>},<!""!i,}>},<!!>}}}},{{{{<!!!!!>,<>},{{},{{<>}}}},{{}},{{<a!!}u>},{{},{{<!!!!!!!>!'!!!>}{!>},<!!<ao!,!!>}}}}},{{{<!>},<!!!!}!>,<!>!!a!!!o!>!>,<e}'!>},<!>},<e>,<!!!>>},{{<,}!,e!>,<!!!>,u<""!""!!>}}},{<i!!!>,<!!!>}!!ea!,!>,!>,<o!>},<!>},<!!!>!>,<!<!>,<>,<>},{{<e<!i<}ii!!!!}>,<uu!!!""!!,u!>!""e'<>},{},{<""{{iu,!>e!!!!,!>},<!>!!!!!>,<!e!!!>>}}},{{{}}},{{<>},{{}}}},{{{<o>,<!>,<ia!>!!}}'"",i!>{<!>,<>},{<!!}'!!{!!!!!!!>!!ae,"",<!!o}!>},<>}}}},{{{{},{<!>},<}o""!!!!!o!>,<,iu!!!!""!!'<!!!!i>}}},{{{{}},{<<""}!!!>,<!!u!!}!'!!!>},<!!!>>}},{{{<i}!!ae!e!>},<!>},<iui!>!<!>!>!i!!'>},{<!>},<}ee!uuu!!,!>},<!!o,a{,e>}}},{<!""!>,<>,{<!!!!!>!!!>,<!!!>!>,<,,!!!>'!o""!<>}}},{{{}}}},{{{{{{<'o!!!>},<>,{}}},{},{{<o!<,,,!>},<i>}}}},{{{<}!o!!{""!'!!!>},<!!!>aa""!!!>ieao>}}}},{{{{<!!!>},<!!!>uia!!!>!>,<!>,<ia'e!>u''io{>,{}},{<u<!>,<!>},<!!!>o}!!iaa!>},<>}},{{<a'""!!!!a{,!>,<!!<e!>},<{ea!>!!!!!>>}},{{{},{<o!!!>ee!!i!!!>i!!!>,<!!<u,!'!>,<oi!>!>,<>},{{<}u}',iai!>,<',!!au!>>},<!!!!!!u!!!>!>},<!!!>!>,<a>}},{{<>,<!>},<,'!>"",{o!!!>,<!!u!!<}>}},{<u!""i'!!!!!>!>,<"",oi}!!<!!!>,<'>,{<a!{""!>,<!>},<{i!!,""!!e}!>!!!aao>,<!!{u'!!!><}!!'>}}},{{<o!!<u'!>!i!e},o,!>,<o!>,<,!i!>""<>,<""u}>},{{{{<!!!>u!!!>e!!!<u!!!{>,{{{}},{<!>},<'{!a!><!!u!>ue!!!>!>,<>}}}},<!!e{!>,<eau<!,<'!!!>}!!!!!>,<u'eua>}},{{}}}},{{{<""u}!!!>!!!>o{!!!!{io!!!>e!<!!!',>,{<!!{!>,<!>},<}""!!<""''e!!ao>}},<iao!!!>!o!i!>,<!>e!o>},{<!!!>e!'o'<>,{<e,!!!>iui!>!!o!>,<!>},<>}},{{<!!!>,<o!!!>},!!!!!>},<e}oo>},<'!>,<!!!!{e!>,<!>{!e',!"",>}},{{{{<!!e!>>,<!!a!>,<i!>},<i!!e{!>},<!!>},<u,!!!>},<i!!''{i{!>},<uo!!!>{o!!!!!!o!!e>}},{{<>}}},{{{},<a!!<!>},<""!o!!!>i!!!>!!e!!!>,!!!!!>ia<<'>}}},{},{{{},{{<!!!>,<!>!>,<<o!""!>,<!!'!!!!,>}}},{{{{}},{}}}},{{{{{<'!>a!!!!!>""!!!!!>},<!>!!!>>}},{}},{<i!!!><!e!!!>,<,!>,<""!!!>u!!!!!>!!!>!>!<{>,<""!!!>,<i""!!i!>,<}'!!e!!'!!!!!>u!>""u>},{{},{<!>},<e!>},<u!>!!!>},<>}}},{{{{{}},<!o!>!!>},{{<,,o!!'!>},<ai!><>,{<!!,}!!!>e>}}}},{<!!!>,!>,<>,<!!""uuue>}},{{<!>,<!!<ue!!!>a!>,<oo<!>,<!!o!,u>,<ei!!!>a'!>,<!!!}<!>,<>}},{{{<u!>,<i""i!>,<!<'ee,<!>},<>,<""i}{!!!o'{,i!!,!>""u>}},{{<!>,<!>!!!>ea<oa""<,{<!!}e!!'>},<!!>}}}}}},{{{{<!,ei{ua!!!!}!!>}},{{{{}},{{{{<!!<{!i,<!!""!>,<eu>},<'>}},<}o<i!!{,>}},{<!a!>},<!!ioau}>,{<!!,>}}},{{},{{{<!>,<e!!!>'a!!!><!a!!!>'!!!!!>!>},<>}},<""e!>,<!!!>},<!!!'!!!>o!>},<o}!!,u!!!>,>},{{<eu!>,<u!o!>},<!!!><'<!>},<!>},<!'iaii!!!!u!o>,{<ia>}},{<a!>,<!!{{!>!>},<>}}}},{{{{<}!>},<!>!!>}},{{<!!""oeo>},<u}}!>,<!!!!u,!""!!'{!>a!!>}},{{{<!!!>i}'eeu!!!>!>'>}},{<!>,<""!!!>,<!!}!,!!,!>,<eo!!!,!>},<!!i}{>,{<{!i{e!>,<oi""!a!!!!!>ue!>},<{!>},<,!>,<i>,{<,>}}},{{<!!!>,<'!>},<}>}}},{{{{{<<a!!!i}!>!!!>a!!ao!!!>,<>}}},{{<!>,<>}},{{<!!u!!'""!>},<!!}!,'""a!,,e>},<!>},<e'!>,<!>!>},<ea!>!!'!>},<!!e!!{<!>,<!!iu>}}}},{{{{},<u,""a,a>},{{<!>},<>},<!!!!!>'{,!>,<<>}},{{{}},{{<!!!!e<{!!!oi,,u!>>}},{{<,i'o!!>},{<u'eo!>,<!!i>}}},{{<!>i>},{<!>>,{{<!!!>u!!{!!{!>,<!>!!>},{<a,e<!!!!o!>},<!}>}}},{{{{}},{{<!>e!>},<oi<!!o,o>}},{{<!""!!<a>}}},{},{{<!!!>!!a!>,<e!!!>,<{!!!>!!,!>',>}}}}}}}""""""
sp = StreamProcessor(day_9_parta)
sp.processStream()",0.3758812547,
2366,the stream object,"def count_links(record):
    if ""html"" not in record.http_headers.get(""content-type""):
        return
    doc = BeautifulSoup(record.raw_stream, ""lxml"")
    for link in doc.select(""a""):
        url = urlparse(link[""href""])
        counts[url.hostname] = counts.get(url.hostname, 0) + 1",0.3676646948,
2366,the stream object,"def start_parquet_streaming_query(csv_sdf):
    """"""
    Create an run a streaming query from a Structured DataFrame 
    outputing the results into a parquet database
    """"""
    streaming_query = csv_sdf \
      .writeStream \
      .format(""parquet"") \
      .option(""path"", os.path.join(root_dir, ""output_parquet"")) \
      .trigger(processingTime=""2 seconds"") \
      .option(""checkpointLocation"", os.path.join(root_dir, ""output_chkpt"")) \
      .start()
    return streaming_query",0.3544118404,
2366,the stream object,"def get_comment():
    comment_body = None

    for comment in subreddit.stream.comments():
        try:
            parent_id = str(comment.parent())
            submission = reddit.comment(parent_id)
            comment_body = comment.body
            print(comment_body)
        except praw.exceptions.PRAWException as e:
            pass
        return comment_body",0.3479163647,
2366,the stream object,"# Write a function to determine the file's status:
def get_status( f ):
    if ( f.closed != False ) :
        return ""Closed""
    else :
        return ""Open""",0.344289124,
2366,the stream object,"def flat_sharp_first_display():
    if SFButtons.value == '#':
        return SharpsDropList
    else:
        return FlatsDropList",0.330732584,
2366,the stream object,"# Returns the next test_x, test_y pair
# This is just a slightly modified version of the function in the source code

def getnext(data_provider):
    data, label = data_provider._next_data()

    train_data = data_provider._process_data(data)
    labels = data_provider._process_labels(label)

    train_data, labels = data_provider._post_process(train_data, labels)

    nx = data.shape[1]
    ny = data.shape[0]

    return train_data.reshape(1, ny, nx, data_provider.channels), labels.reshape(1, ny, nx, data_provider.n_class)",0.3261774182,
2366,the stream object,"def start():
    '''
    Start a long-poll to the Gitter stream API for chat messages
    in the preconfigured channel.
    '''
    global stream
    http_client = AsyncHTTPClient(force_instance=True)
    req = HTTPRequest('https://stream.gitter.im/v1/rooms/{}/chatMessages'.format(CHANNEL_ID), 
                      headers=HEADERS,
                      streaming_callback=on_messages,
                      connect_timeout=2592000,
                      request_timeout=2592000)
    stream = http_client.fetch(req, callback=on_complete)",0.3255538344,
2366,the stream object,"# make a function to go through a score, identify the parts,
# and recurse a function on the measures of each part

def recurse_measure_level_function_on_score(score, function):
    """"""takes a function taht works on a MEASURE and makes it work on
    an overall stream/SCORE""""""
    for score_element in score.elements:
        # do this on score elements that are PARTS
        if type(score_element) == m21.stream.Part:
            print 'yup this is part', score_element.partName

            # within a part, do this on part elements that are MEASURES
            for part_element in score_element.elements:
                if type(part_element) == m21.stream.Measure:
#                     print 'yup this is measure', part_element.measureNumber
                    function(part_element)",0.3247131109,
2366,the stream object,"def get(self):
    with self.lock:
        return self.items.popleft()",0.3243672848,
1211,modifying the target column,"iris.remove_examples(""virginica"")
print(iris.values[iris.target])",0.4147363901,
1211,modifying the target column,"iris2 = DataSet(name=""iris"")

iris2.remove_examples(""virginica"")
print(iris2.values[iris2.target])",0.3992521167,
1211,modifying the target column,viz.check_data_balance(target_column = -1),0.3965282142,
1211,modifying the target column,ones = digits.data[digits.target == 1],0.3962658048,
1211,modifying the target column,"#mr eleve a tfidf-el transzformlt anyagot tltjk le
data2 = fetch_20newsgroups_vectorized(subset='test', remove=('headers', 'footers', 'quotes'))
X2, y2 = data2.data, data2.target",0.3925571442,
1211,modifying the target column,"df[""target""] = np.where(df.survived == 0, ""N"", ""Y"") 
df[""target_num""] = df.target.map({""N"":0, ""Y"":1})",0.3918876052,
1211,modifying the target column,dataset.target_col,0.3914874196,
1211,modifying the target column,"ids_vs_keywords.purify_column(target_column_name='authorKeywords')
ids_vs_keywords.dataframe",0.3868291974,
1211,modifying the target column,"logisticregressionmodel = LogisticRegression()
y_true, y_preds, model = predict_val(converted_train.drop(['enrollee_id','target'],1),
                                    converted_train.target, logisticregressionmodel)",0.3815560639,
1211,modifying the target column,len(concat[concat.Target.isnull()]),0.3795760572,
1075,linear regression review lab,"#classifier algorithm
if model is 'SVR':
   # clf = LinearCSVMC(tube_epsilon=0.01)
    from sklearn.svm import LinearSVR
    clf = SKLLearnerAdapter(LinearSVR())
elif model is 'SVRrbf':
    from sklearn.svm import SVR
    clf = SKLLearnerAdapter(SVR(kernel='rbf',C=1e03))
elif model is 'lasso':
    from sklearn.linear_model import Lasso
    clf = SKLLearnerAdapter(Lasso())
elif model is 'decisionTree':
    from sklearn.tree import DecisionTreeRegressor
    clf = SKLearnerAdapter(DecisionTreeRegressor())
elif model is 'ridge':
    from sklearn.linear_model import RidgeCV
    clf = SKLLearnerAdapter(RidgeCV())
elif model is 'randForest':
    from sklearn.ensemble import RandomForestRegressor
    clf = SKLLearnerAdapter(RandomForestRegressor(n_estimators=10,max_features='sqrt'))
elif model is 'gpr':
    from sklearn.gaussian_process import GaussianProcessRegressor
    clf = SKLLearnerAdapter(GaussianProcessRegressor(normalize_y=True))
elif model is 'kernelRidge':
    from sklearn.kernel_ridge import KernelRidge
    clf = SKLLearnerAdapter(KernelRidge(kernel='linear'))
        
#cross-validation algorithm
if cv_type is 'LOSO': #leave-one-subject out 
    cv = CrossValidation(clf,
                         NFoldPartitioner(attr='subject'),
                         errorfx=None,
                         pass_attr='subject')
elif cv_type is 'nfld':
    cv = CrossValidation(clf,
                         NFoldPartitioner(cvtype=0.2,count=5,
                                         selection_strategy='random', attr='subject'),
                         errorfx=None,
                         enable_ca=['stats'],
                         pass_attr='subject')

    
#run classification for all sample sizes
# test = random.sample(range(50),10)
# fds[test]
cv_out = []
for n in nsubs[5:6]:
    rndsubs = random.sample(range(1000),n)
    tmp = cv(fds[rndsubs])
    cv_out.append(tmp)
    verbose(2, ""sample size of %i subjects analyzed"" % n)",0.5314443111,
1075,linear regression review lab,"def ridge_regression():
    from sklearn.linear_model import RidgeCV
    model = RidgeCV(alphas = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60])
    model.fit(X,y)
    alpha = model.alpha_
    model = RidgeCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, 
                          alpha * .9, alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15,
                          alpha * 1.25, alpha * 1.3, alpha * 1.35, alpha * 1.4], 
                cv = 10)
    model.fit(X,y)
    alpha = model.alpha_
    y_pred = model.predict(X)
    print(""RMSE for Ridge Regression with alpha {}: {:0.4f} (+/- {:0.4f})"".format(alpha, rmse_cv(model).mean(), rmse_cv(model).std() * 2))
    return model",0.5193101168,
1075,linear regression review lab,"def ridge_regression5():
    from sklearn.linear_model import RidgeCV
    model = RidgeCV(alphas = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60])
    model.fit(X,y)
    alpha = model.alpha_
    model = RidgeCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, 
                          alpha * .9, alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15,
                          alpha * 1.25, alpha * 1.3, alpha * 1.35, alpha * 1.4], 
                cv = 10)
    model.fit(X,y)
    alpha = model.alpha_
    y_pred = model.predict(X)
    print(""RMSE for Ridge Regression with alpha {}: {:0.4f} (+/- {:0.4f})"".format(alpha, rmse_cv(model).mean(), rmse_cv(model).std() * 2))
   
    out = expanded_scores(model)
    return out",0.5189611316,
1075,linear regression review lab,"best_score, param_val, param_name = \
    plot_validation_curve(clf,
                          # X_train, Y_train,
                          X, Y,
                          ""n_estimators"", [5, 7, 10, 11, 13, 15, 17, 19, 23, 25, 29],
                          ""n_estimators Validation Curve"",
                          cv=cv, scoring=SCORING, n_jobs=JOBS, scale='linear')",0.5173670053,
1075,linear regression review lab,"test_map(c_long, c_lat, 5, -125., 32., -114., 42.5)",0.5163635612,
1075,linear regression review lab,"#full run
if False:
    scores, bid_profiles = evaluate_slices(df,
                                       bidders=['last_click',
                                                'first_click',
                                                'AA'],
                                       utilities=['last_click',
                                                  'first_click',
                                                  'AA_normed',
                                                  'AA_not_normed'],
                                       test_days=range(22,29),
                                       learning_duration=21,
                                       hash_space = 2**18,
                                       AA_bidder_label='all_clicks')",0.5141284466,
1075,linear regression review lab,"parameters, df = regression_model(X,Y, [32,20,20,10,5,1], 0.0075, 0, 50000, yscaler)",0.5137747526,
1075,linear regression review lab,"closedFormAlgo(XTrain,yTrain,0.01,8,2)",0.5135817528,
1075,linear regression review lab,"from sklearn import linear_model
def compare(X_Train,Y_Train,X_Test,Y_Test):
    """""" Uses Scikit Learn's LinearRegression Function  """"""
    regr = linear_model.LinearRegression()

    # Train the model using the training sets
    regr.fit(X_Train.reshape(-1, 1) , Y_Train)
    y_cap = regr.predict(X_Test.reshape(-1, 1))
    print ""By Scikit Learn Mean Squared Error: "",mean_squared_error(Y_Test,y_cap)
    
start_time = timeit.default_timer()   
compare(X_Train,Y_Train,X_Test,Y_Test)
end_time = timeit.default_timer()
print ""Time Taken by Scikit Learn"", end_time - start_time

print ""By implemented Model Mean Square Error"",mean_squared_error(Y_Test,Y_Cap) 
print ""Time Taken by Implemented model"", single_time",0.5121830106,
1075,linear regression review lab,"from sklearn import svm
svr_model = svm.SVR()
svr_model.fit(list(data['Fingerprints']), data['-log(nm)'])",0.5113109946,
588,exploring dataframes,"print(str(iris.data), str(iris.target), str(iris.target_names), str(iris.feature_names), str(iris.DESCR))",0.4098265767,
588,exploring dataframes,"# Statistics of X and Y


stats={'Stats':['Min','Mean','Max','GMean','STD'],
    'X':[Data.X.min(),Data.X.mean(),Data.X.max(),gmean(Data.X),Data.X.std()],
    'Y':[Data.Y.min(),Data.Y.mean(),Data.Y.max(),gmean(Data.Y),Data.Y.std()]}
stats=pd.DataFrame(stats)
print(stats)",0.3981158733,
588,exploring dataframes,"outliers = (((df.DepDelay - df.DepDelay.mean()).abs() > df.DepDelay.std()*3) | 
            ((df.ArrDelay - df.ArrDelay.mean()).abs() > df.ArrDelay.std()*3))",0.3947065175,
588,exploring dataframes,"df_CPI = df[df.Data == 'Consumer Price Index']
df_UL = df[df.Data == 'Unemployment Level']
df_CLF = df[df.Data == 'Civilian Labor Force']",0.3938554823,
588,exploring dataframes,"total_frequency = np.zeros((len(grid_cpol.x['data']), 
                           len(grid_cpol.y['data'])))
convective_frequency = np.zeros((len(grid_cpol.x['data']), 
                                 len(grid_cpol.y['data'])))
total_changes = np.zeros((len(grid_cpol.x['data']), 
                          len(grid_cpol.y['data'])))
intermittency = np.zeros((len(grid_cpol.x['data']), 
                          len(grid_cpol.y['data'])))
modes = np.zeros((len(grid_cpol.x['data']), 
                  len(grid_cpol.y['data'])))

for j in range(0, len(grid_cpol.x['data'])):
    for k in range(0, len(grid_cpol.y['data'])):
        ptypes = np.squeeze(precip_types[:,j,k])
        total_frequency[j,k] = sum(np.where(ptypes > 0, 1, 0))
        if(total_frequency[j,k] > 0):
            convective_frequency[j,k] = float(sum(np.where(ptypes == 2, 1, 0))) / float(total_frequency[j,k])
        changes = np.zeros(ptypes.shape)
        changes[1:] = np.diff(ptypes)
        changes[ptypes == 0] = 0
        total_changes[j,k] = sum(changes[ptypes > 0])
        intermittency[j,k] = float(total_changes[j,k])/float(opportunities)
        if(convective_frequency[j,k] < 0.33):
            modes[j,k] = 1
        elif(convective_frequency[j,k] > 0.33 and
             convective_frequency[j,k] < 0.66 and
             intermittency[j,j] > 0.33):
            modes[j,k] = 2
        elif(convective_frequency[j,k] > 0.66):
            modes[j,k] = 3
        else:
            modes[j,k] = 4
        
        # Little precipitation periods
        if(float(total_frequency[j,k]/float(opportunities) < 0.3)):
            modes[j,k] = 0",0.39316535,
588,exploring dataframes,"news_train_data.data[2], news_train_data.target_names[news_train_data.target[2]]",0.3930376768,
588,exploring dataframes,"# classify features: allNaN, numeric, object
fnames_allNaN, fnames_numeric, fnames_obj = [],[],[]
for col in df.columns.values:
    if df[col].isnull().mean()==1:
        fnames_allNaN.append(col)
    else:
        ctype = df[col].dtype
        if ctype == 'object':
            fnames_obj.append(col)
        else:
            fnames_numeric.append(col)

print('numbers of numeric, object, allNaN features are %d, %d, %d' % (len(fnames_numeric),len(fnames_obj),len(fnames_allNaN)))",0.3927876353,
588,exploring dataframes,"# separate data by type
fresh = all_data[all_data.Type == ""Fresh""]
juice = all_data[all_data.Type == ""Juice""]
frozenJuice = all_data[all_data.Type == ""Frozen juice""]
canned = all_data[all_data.Type == ""Canned""]
dried = all_data[all_data.Type == ""Dried""]
frozenFruit = all_data[all_data.Type == ""Frozen fruit""]

# find min/max of each type
print( 'Detailed Description: ')
pdFresh = minMaxPrice(fresh)
pdJuice = minMaxPrice(juice)
pdFrozenjuice = minMaxPrice(frozenJuice)
pdCanned = minMaxPrice(canned)
pdDried = minMaxPrice(dried)
pdFrozenfruit = minMaxPrice(frozenFruit)

priceByType = pd.DataFrame(columns = ['Type', 'Most expensive', 'Least expensive'])
priceByType['Type'] = all_data.Type.unique()
priceByType['Most expensive'] = [pdFresh[0].values[0],
                                 pdJuice[0].values[0], 
                                 pdFrozenjuice[0].values[0], 
                                 pdCanned[0].values[0],
                                 pdDried[0].values[0],
                                 pdFrozenfruit[0].values[0]]

priceByType['Least expensive'] = [pdFresh[1].values[0],
                                 pdJuice[1].values[0], 
                                 pdFrozenjuice[1].values[0], 
                                 pdCanned[1].values[0],
                                 pdDried[1].values[0],
                                 pdFrozenfruit[1].values[0]]

priceByType = priceByType.set_index(['Type'])
priceByType",0.3924286962,
588,exploring dataframes,"ab_break = ab[(ab.LOSS_DESC == ""BREAK AND ENTRY"") | (ab.LOSS_DESC == ""ARMED ROBBERY"")]",0.3919469118,
588,exploring dataframes,"bush   = faces.data[faces.target_names[faces.target] == 'George W Bush']
powell = faces.data[faces.target_names[faces.target] == 'Colin Powell']",0.3905305862,
497,donuts,"def experiment():
    
    # Configure governor
    target.cpufreq.set_all_governors('sched')

    # Get workload
    wload = Workload(te).getInstance(te, 'YouTube')
    
    # Run Youtube workload
    wload.run(te.res_dir, 'https://youtu.be/XSGBVzeBUbk?t=45s',
              video_duration_s=5, collect='ftrace')

    # Dump platform descriptor
    te.platform_dump(te.res_dir)",0.3958412111,
497,donuts,"def experiment():
    # Configure governor
    target.cpufreq.set_all_governors('sched')
    
    # Get workload
    wload = Workload.getInstance(te, 'GMaps')
        
    # Run GMaps
    wload.run(out_dir=te.res_dir,
          collect=""ftrace"",
          location_search=""London British Museum"",
          swipe_count=10)
        
    # Dump platform descriptor
    te.platform_dump(te.res_dir)",0.3918691278,
497,donuts,"def experiment():
    
    # Configure governor
    target.cpufreq.set_all_governors('sched')
    
    # Get workload
    wload = Workload.getInstance(te, 'Geekbench')
    
    # Run Geekbench workload
    wload.run(te.res_dir, test_name='CPU', collect='ftrace')
        
    # Dump platform descriptor
    te.platform_dump(te.res_dir)",0.3901031613,
497,donuts,"def doNothing(N):
    X = range(N)
    for n in X:
        #do nothing
        ;
    print(""done"")
    
%timeit doNothing(100000000)",0.3863971233,
497,donuts,"# sample action function: takes in the ""Player""
def action(self):
    # inputs:
    # resources - an array of resources
    # costs - an array of costs, 0 - settlement, 1 - card, 2 - city
    # basic strategy: Once we get 4 of one resource, we make a trade. 
    # Then we try to buy development cards
    if self.board.settlements == []:
        (x,y) = self.preComp #use the optimal settlement location  
        self.buy(""settlement"", x, y) # we determined previously
    elif self.if_can_buy(""card""):
        self.buy(""card"")
    elif self.resources[np.argmax(self.resources)] >= 4:
        rmax, rmin = np.argmax(self.resources), np.argmin(self.resources)
        self.trade(rmax,rmin)
    return

def planBoard(baseBoard):
    x = np.random.randint(0, baseBoard.width+1)
    y = np.random.randint(0, baseBoard.height+1)
    optSettlementLoc = (x,y)
    return optSettlementLoc
    
num_trials = 100

width, height = 4, 4
dice = get_random_dice_arrangement(width, height)
resources = np.random.randint(0, 3, (height, width))
board = Catan(dice, resources)
print(simulate_game(action, planBoard, board, num_trials))",0.3845118284,
497,donuts,"def test_is_won():
    for state in REFERENCE_MATCH[:-1]:
        assert False == is_won(visible_pieces(state.board))

    assert True == is_won(visible_pieces(REFERENCE_MATCH[-1].board))

test_is_won()",0.3780498505,
497,donuts,"def first_deal():
    global Card_deck_position
    Card_deck_position = 0
    import random
    random.shuffle(deck)
    D.blackjackstatus = False
    #players get cards
    playerNameIndex = 0
    
    #resets cards and stats
    for playing_person in list_of_players_as_classes:
        list_of_players_as_classes[playerNameIndex].reset_CARDamount()
        list_of_players_as_classes[playerNameIndex].reset_CardsHeld()
        list_of_players_as_classes[playerNameIndex].blackjack_1st_status = False
        list_of_players_as_classes[playerNameIndex].blackjack_2nd_status = False
        playerNameIndex += 1 
        continue
        
    #resets count through list and deals out cards
    playerNameIndex = 0
    for playing_person in list_of_players_as_classes:
        #bankruptcy coplaying_preconditional check
        if playing_person.bankrupt_status == False:
            
            
            #put player dealing in here
            #each player starts with 2 cards
            
            list_of_players_as_classes[playerNameIndex].card_list.append(deck[Card_deck_position])
            Card_deck_position += 1
            list_of_players_as_classes[playerNameIndex].card_list.append(deck[Card_deck_position])
            Card_deck_position += 1
            playerNameIndex += 1
            
        else:
            #print list_of_player_names[playerNameIndex] + "", you are bankrupt.  You are not playing anymore for this game."" 
            playerNameIndex += 1
    
    #Dealer cards are reset
    D.reset_CARDamount()
    D.reset_CardsHeld()
    
    #Dealer gets cards
    D.card_list.append(deck[Card_deck_position])
    Card_deck_position += 1
    D.card_list.append(deck[Card_deck_position])
    Card_deck_position += 1",0.3776330948,
497,donuts,"def stand():
    global playing, chip_pool, deck, player_hand, dealer_hand, result, bet
    '''This function will now play the dealers hand, since stand was chosen.'''
    
    if playing == False:
        if player_hand.calc_val() > 0:
            result = ""Sorry, you can't stand!""
            
    # Now go through all the other possible options
    else:
        
        # Soft 17 rule
        while dealer_hand.calc_val() < 17:
            dealer_hand.card_add(deck.deal())
            
        # Dealer Busts    
        if dealer_hand.calc_val() > 21:
            result = 'Dealer busts! You win!' + restart_phrase
            chip_pool += bet
            playing = False
            
        #Player has better hand than dealer
        elif dealer_hand.calc_val() < player_hand.calc_val():
            result = 'You beat the dealer, you win!' + restart_phrase
            chip_pool += bet
            playing = False
        
        # Push
        elif dealer_hand.calc_val() == player_hand.calc_val():
            result = 'Tied up, push!' + restart_phrase
            playing = False
        
        # Dealer beats player
        else:
            result = 'Dealer Wins!' + restart_phrase
            chip_pool -= bet
            playing = False
    game_step()",0.3765324354,
497,donuts,"def stand():
    global playing,chip_pool,deck,player_hand,dealer_hand,result,bet
    ''' This function will now play the dealers hand, since stand was chosen '''
    
    if playing == False:
        if player_hand.calc_val() > 0:
            result = ""Sorry, you can't stand!""
            
    # Now go through all the other possible options
    else:
        
        # Soft 17 rule
        while dealer_hand.calc_val() < 17:
            dealer_hand.card_add(deck.deal())
            
        # Dealer Busts    
        if dealer_hand.calc_val() > 21:
            result = 'Dealer busts! You win!' + restart_phrase
            chip_pool += bet
            playing = False
            
        #Player has better hand than dealer
        elif dealer_hand.calc_val() < player_hand.calc_val():
            result = 'You beat the dealer, you win!' + restart_phrase
            chip_pool += bet
            playing = False
        
        # Push
        elif dealer_hand.calc_val() == player_hand.calc_val():
            result = 'Tied up, push!' + restart_phrase
            playing = False
        
        # Dealer beats player
        else:
            result = 'Dealer Wins!' + restart_phrase
            chip_pool -= bet
            playing = False
    game_step()",0.3765324354,
497,donuts,"def drift_pop(num_msats, pop_size):
    m = SinglePop(gens=100)
    m.num_msats = num_msats
    m.pop_size = pop_size
    BasicView(m, [ExpHe()], ['mean'], min_y=[0], max_y=[1])
    m.run()
    
interact(drift_pop, num_msats=(2, 20, 1), pop_size=(30, 530, 50))",0.3746549487,
2329,text to vector function,"def sentvec(s):
    sent = nlp(s)
    return meanv([w.vector for w in sent])",0.4953596592,
2329,text to vector function,"def sent2vec(s):
    sent = nlp(s)
    return meanv([w.vector for w in sent])",0.4953596592,
2329,text to vector function,"def date_rounder(freq):
    """"""
    Returns function for rounding dates/times with constant frequencies (eg Days, Hours, 15 Minutes)

    This function does not apply to more complicated rounding schemes like business days (see anchored_date_rounder).

    :param freq: pd.tseries.offset instance or frequency string
    :return: Date rounding function
    """"""
    if isinstance(freq, str):
        freq = to_offset(freq)
    return lambda x: pd.Timestamp((x.value // freq.delta.value) * freq.delta.value)",0.4866321087,
2329,text to vector function,"def meanvector(text):
    s = nlp(text)
    vecs = [word.vector for word in s \
            if word.pos_ in ('NOUN', 'VERB', 'ADJ', 'ADV', 'PROPN', 'ADP') \
            and np.any(word.vector)] # skip all-zero vectors
    if len(vecs) == 0:
        raise IndexError
    else:
        return np.array(vecs).mean(axis=0)
meanvector(""this is a test"").shape",0.4852587581,
2329,text to vector function,"def convert_text_to_index_array(text):
    words = kpt.text_to_word_sequence(text)
    wordIndices = []
    for word in words:
        if word in dictionary:
            wordIndices.append(dictionary[word])
    return wordIndices",0.483394295,
2329,text to vector function,"def word_count(path):
    """""" Count the word in a text file using spark, using flatMap, map and reduceByKey function
    Args: 
        path (string): path to the file
    Returns:
        dict: a dict that word is the key and the count is the value
    """"""
    text_file = sc.textFile(path)
    words = text_file.flatMap(split_line)
    pairs = words.map(word_pair)
    counts = pairs.reduceByKey(count)
    count_dict = dict(counts.collect())
    return count_dict",0.4813822508,
2329,text to vector function,"def sent_vec_w2v(sent):
    wv_res = np.zeros(w2v.vector_size)
    ctr = 1
    for w in sent:
        if w in w2v:
            ctr += 1
            wv_res += w2v[w]
    wv_res = wv_res/ctr
    return wv_res",0.4803115427,
2329,text to vector function,"def sent_vec(sent):
    wv_res = np.zeros(glove_model.vector_size)
    ctr = 1
    for w in sent:
        if w in glove_model:
            ctr += 1
            wv_res += glove_model[w]
    wv_res = wv_res/ctr
    #return (wv_res, ctr)
    return wv_res",0.4803115427,
2329,text to vector function,"def to_review_vector(review):
    words = clean_text(review, remove_stopwords=True)
    array = np.array([model[w] for w in words if w in model])
    return pd.Series(array.mean(axis=0))",0.477265656,
2329,text to vector function,"def word_count(path):
    """""" Count the word in a text file using spark
    Args: 
        path (string): path to the file
        
    Returns:
        dict: a dict that word is the key and the count is the value
    """"""
    text_file = sc.textFile(path)
    count = text_file.flatMap(lambda line: line.split(' '))\
                    .map(lambda word: (word, 1))\
                    .reduceByKey(lambda a, b: a+b)
    count_dict = dict(count.collect())
    return count_dict",0.4746999443,
1273,normalizing vectors,"def normalize_array(mat):
    for idx in range(mat.shape[0]):
        mat[idx] /= np.linalg.norm(mat[idx])
        
def cosine_sim(mat, vec):
    """"""Assumes normalized arrays""""""
    cosine_sims = np.inner(vec, mat)
    return np.argsort(cosine_sims)[::-1], cosine_sims",0.5345311761,
1273,normalizing vectors,"measured[:,1] = measured[:,1] / np.linalg.norm(measured[:,1])
true[:,1] = true[:,1] / np.linalg.norm(true[:,1])",0.5275211334,
1273,normalizing vectors,"# To complete
def applyNormalization(data, mean, std):
    """"""
        Given a mean and std, normalize the data
    """"""
    normalizedData = 
    return normalizedData

def normalize(data):
    """"""
        Normalize the given data
    """"""
    mean =
    std = 
    normalizedData = applyNormalization(data, mean, std)
    return normalizedData, mean, std",0.5197859406,
1273,normalizing vectors,"emb = emb / np.linalg.norm(emb, axis=1, keepdims=True)",0.5168756247,
1273,normalizing vectors,"for i in [0,1,2,3]:
    X[i,:] = X[i,:] / la.norm(X[i,:])

print(X)",0.5126532912,
1273,normalizing vectors,"en_wv = lwvlib.load(""GoogleNews-vectors-negative300.100K.bin"")
en_wv.vectors = sklearn.preprocessing.normalize(en_wv.vectors, norm='l2') #Normalize",0.5119062066,
1273,normalizing vectors,"X = [[ 1., -1.,  2.],
     [ 2.,  0.,  0.],
     [ 0.,  1., -1.]]
X_normalized = preprocessing.normalize(X, norm='l2')
X_normalized",0.5092776418,
1273,normalizing vectors,"# Sample data
X = [[1., -1.,  2.],
     [2.,  0.,  0.],
     [0.,  1., -1.]]

# Lets normalize the data using norms l1 or l2
X_normalized = preprocessing.normalize(X, norm='l1')

X_normalized",0.5092776418,
1273,normalizing vectors,"X = [[ 1., -1., 2.],
     [ 2.,  0., 0.],
     [ 0.,  1.,-1.]]
X_normalized = preprocessing.normalize(X,norm='l2')
X_normalized",0.5092776418,
1273,normalizing vectors,"X = [[ 1., -1.,  2.],
     [ 2.,  0.,  0.],
     [ 0.,  1., -1.]]
X_normalized = preprocessing.normalize(X, norm='l2')

X_normalized",0.5092776418,
563,examples,"def _convert_to_example(image_data, label):
	#print 'shape: {}, height:{}, width:{}'.format(shape,shape[0],shape[1])
	example = tf.train.Example(features=tf.train.Features(feature={
			'image/encoded': bytes_feature(image_data),
			'label/encoded': bytes_feature(label)
			}))
	return example
	
    
def _processing_image(seq, label, depth):
	seqs = []
	labs = []
	for d in [depth-2, depth-1, depth]:
		label_data = label[d]
		labs.append(np.array(label_data))
		mod = []
		for im in seq:
			image_data = im[d]
			#image_data = scipy.ndimage.interpolation.zoom(image_data, 2, order=1, mode='nearest')
			# upsample 
			mod.append(image_data)
		seqs.append(np.array(mod))
	seqs = np.array(seqs)
	labs = np.array(labs)
	return seqs.tobytes(), labs.tobytes()

def norm_image_by_patient(imname):
	im = sitk.GetArrayFromImage(sitk.ReadImage(imname)).astype(np.float32)
	return (im - im.mean()) / im.std()
	roi_index = im > 0
	mean = im[roi_index].mean()
	std = im[roi_index].std()
	im[roi_index] -= mean
	im[roi_index] /= std
	return im

def count_class_freq(label_batch):
    hist = np.zeros(5)
    imagesPresent = [0,0,0,0,0]
    for i in range(len(label_batch)):
        new_hist = np.bincount(label_batch[i], minlength=5)
        hist += new_hist
        for ii in range(5):
            if (new_hist[ii] != 0):
                imagesPresent[ii] += 1
    print(hist)
    freqs = [hist[v]/float((imagesPresent[v]+1e-5)*240*240) for v in range(5)]
    median = np.median(freqs)
    o = []
    for i in range(5):
        if (freqs[i] <= 1e-5):
            o.append(0.0)
        else:
            o.append(float(median)/(freqs[i]))
    print(o)
    return o

def checkLabel(label, d):
	if np.count_nonzero(label[d]) > 0:
		return True, 1
	else:
		return False, 0
    
def count_freq(labels):
	freq = np.array([0.0,0.0,0.0,0.0,0.0])
	for la in labels:
		freq += np.bincount(la, minlength=5).astype(np.float32)
	print(freq)
	print(freq/freq.sum())
	count_class_freq(labels)",0.5017137527,
563,examples,"def evaluate_single_image(index):
    getlabel = y;
    prediction = sess.run(getlabel, feed_dict={x: [mnist.test.images[index]], y_: [mnist.test.labels[index]]});
    showimg(mnist.test.images[index]);
    showprediction(prediction);
    truth_label = getdigit(mnist.test.labels[index]);
    print(""Original label"", truth_label);",0.4998182356,
563,examples,"def create_example(data, label, filename):
  """"""
  Creates a training example for the Ubuntu Dialog Corpus dataset.
  Returns the a tensorflow.Example Protocol Buffer object.
  """"""
  # New Example
  example = tf.train.Example()
  example.features.feature[""size""].int64_list.value.extend([data.shape[0]])
  example.features.feature[""embedding""].float_list.value.extend(data.tolist())
  example.features.feature[""label""].int64_list.value.extend([label])

  return example

def create_file_list(dir, classes_map):
    classes_list = sorted(os.listdir(dir))
    lines = []
    for cl in classes_list:
        class_dir = os.path.join(dir, cl)
        samples_list = sorted(os.listdir(class_dir))
        sample_class = classes_map[cl]
        for sample in samples_list:
            if sample:
                sample_path = os.path.join(class_dir, sample)
                record = (sample_path, sample_class)
                lines.append(record)
    random.shuffle(lines)
    return lines


def process_data(input_files, output_filename, labels_map):
    writer = tf.python_io.TFRecordWriter(output_filename)
    for file_name, label_name in tqdm(input_files):
    
        try:
            examples_batch = wavfile_to_examples(file_name)
        except Exception:
            print(file_name)
            print()
            continue
            
        if examples_batch.shape[0]:
            label = labels_map[label_name]
#             print(""examples_batch shape:{0}"".format(examples_batch.shape))
            examples_batch = examples_batch.flatten()
#             print(""examples_batch flattened shape:{0}"".format(examples_batch.shape))
            example = create_example(examples_batch, label, file_name)
#             print(""example type:{0}"".format(type(example)))
            writer.write(example.SerializeToString())
    writer.close()",0.499153614,
563,examples,"def create_example(data, label, filename):
  """"""
  Creates a training example for the Ubuntu Dialog Corpus dataset.
  Returns the a tensorflow.Example Protocol Buffer object.
  """"""
  # New Example
  example = tf.train.Example()
  example.features.feature[""size""].int64_list.value.extend([data.shape[0]])
  example.features.feature[""embedding""].float_list.value.extend(data.tolist())
  example.features.feature[""label""].int64_list.value.extend([label])

  return example

def create_file_list(dir, classes_map):
    classes_list = sorted(os.listdir(dir))
    lines = []
    for cl in classes_list:
        class_dir = os.path.join(dir, cl)
        samples_list = sorted(os.listdir(class_dir))
        sample_class = classes_map[cl]
        for sample in samples_list:
            if sample:
                sample_path = os.path.join(class_dir, sample)
                record = (sample_path, sample_class)
                lines.append(record)
    random.shuffle(lines)
    return lines


def process_data(input_files, output_filename, labels_map):
    writer = tf.python_io.TFRecordWriter(output_filename)
    for file_name, label_name in tqdm(input_files):
    
        try:
            examples_batch = wavfile_to_examples(file_name)
        except Exception:
            print(file_name)
            print()
            continue
            
        if examples_batch.shape[0]:
            label = labels_map[label_name]
            print(""examples_batch shape:{0}"".format(examples_batch.shape))
            examples_batch = examples_batch.flatten()
            print(""examples_batch flattened shape:{0}"".format(examples_batch.shape))
            example = create_example(examples_batch, label, file_name)
            print(""example type:{0}"".format(type(example)))
            writer.write(example.SerializeToString())
    writer.close()",0.499153614,
563,examples,"def demo(net, im_name):
    """"""Detect object classes in an image using pre-computed object proposals""""""
    
    # Load the test image
    im_file = os.path.join(IMG_DIR, im_name)
    im = cv2.imread(im_file)
    
    # Detect all object classes and regress object bounds
    timer = Timer()
    timer.tic()
    scores, boxes = im_detect(net, im)
    timer.toc()
    print ('Detection took {:3f}s for '
           '{:d} object proposals\n').format(timer.total_time, boxes.shape[0])
    
    # Visualize detections for each class
    CONF_THRESH = 0.7
    NMS_THRESH = 0.3
    vis_detections(im, scores, boxes, im_name, CONF_THRESH, NMS_THRESH)",0.491347611,
563,examples,"def insert_data(jsonfile):
    
    print jsonfile

    client = MongoClient(""mongodb://localhost:27017"")
    db = client.examples

    with open(jsonfile) as f:
        data = json.loads(f.read())
        db.dublin.insert_many(data)",0.4904491305,
563,examples,"def plot_activations(layer,img):
    units = layer.eval(session=sess,feed_dict={x:np.reshape(img,[1,784],order='F'),keep_prob:1.0})
    filters = units.shape[3]
    plt.figure(1, figsize=(20,20))
    for i in xrange(0,filters):
        plt.subplot(7,6,i+1)
        plt.title('Filter ' + str(i))
        plt.imshow(units[0,:,:,i], interpolation=""nearest"", cmap=""gray"")",0.4874651432,
563,examples,"def show_random_mnist_train_example(mnist):
    """"""Draws a random training image from MNIST dataset and displays it.
    
    Args:
        mnist: MNIST dataset.
    """"""
    random_idx = random.randint(0, mnist.train.num_examples)
    image = mnist.train.images[random_idx].reshape(28, 28)
    imgplot = plt.imshow(image, cmap='Greys')
    ### [TASK] Get a correct label for the image 
    label = None
    ###
    print('Correct label for image #{0}: {1}'.format(random_idx, label))

show_random_mnist_train_example(mnist)",0.4850688577,
563,examples,"def make_print_confusion_matrix(clf, clf_name):
    x_train, x_test, y_train, y_test = train_test_split(aggr_rf_input_data, player_colors_2, test_size=0.25)
    clf.fit(x_train, y_train)
    prediction = clf.predict(x_test)
    accuracy = np.mean(cross_val_score(clf, aggr_rf_input_data, player_colors_2, cv=5, n_jobs=3, pre_dispatch='n_jobs+1', verbose=1))
    print(clf_name + ' Accuracy: ',accuracy)
    cm = confusion_matrix(y_test, prediction)
    class_names = ['WWW', 'BBB']
    plot_confusion_matrix(cm, classes=class_names, title='Confusion matrix of '+clf_name)
    plt.show()",0.4844035506,
563,examples,"def explain(idx_lst):
    for idx in idx_lst:
        print('Prediction: ', explainerAnchor.class_names[predict_fn(dataset.test[idx].reshape(1, -1))[0]])
        print(""Instance being explained: \n"")
        print(pd.Series(dataset.test[idx], index=dataset.feature_names ))
        print('\n')
        # LIME Explanation for the instance prediction
        wrong_exp_LIME = explainer.explain_instance(data_row = dataset.test[idx], 
                                               predict_fn = predict_prob_fn)

        # Plot LIME prediction explaination
        wrong_exp_LIME.as_pyplot_figure()
        plt.title('LIME Explanation', size = 18)
        plt.xlabel('Effect on Prediction', size = 14)
        plt.show()
        # Anchor explanation
        # We get an anchor for prediction number idx. 
        # Remember: An anchor is a sufficient condition - that is, when the anchor holds, 
        # the prediction should be the same as the prediction for this instance.
        print('Anchor for the Instance: \n')
        np.random.seed(1)
        desired_label = predict_fn(dataset.test[idx].reshape(1, -1))[0]
        #print('Prediction: ', explainerAnchor.class_names[predict_fn(dataset.test[idx].reshape(1, -1))[0]])

        exp = explainerAnchor.explain_instance(dataset.test[idx], rtf.predict, threshold=0.90)
        print('%s' % (' AND '.join(exp.names())))
        print('Precision: %.2f' % exp.precision())
        print('Coverage: %.2f \n' % exp.coverage())

        # Get test examples where the anchor applies
        print('Test examples where the anchor applies:')
        fit_anchor = np.where(np.all(dataset.test[:, exp.features()] == dataset.test[idx][exp.features()], axis=1))[0]
        print('Anchor test precision: %.2f' % (np.mean(predict_fn(dataset.test[fit_anchor]) == predict_fn(dataset.test[idx].reshape(1, -1)))))
        print('Anchor test coverage: %.2f' % (fit_anchor.shape[0] / float(dataset.test.shape[0])))

        # See a visualization of the anchor with examples
        exp.show_in_notebook()
        print('\n\n ----------------------------------------------- \n\n')",0.483997941,
2111,step join the two dataframes along rows and assign all_data,"# This function normalize the dataset 
def normalize_all_features():
    
    global data_processed 
    
    features = list(data_processed.columns)
    data_processed[features] = data_processed[features].apply(lambda x: x/x.max(), axis=0)
    
    print ('Features normalized successfully !')
    
normalize_all_features()",0.5263060331,
2111,step join the two dataframes along rows and assign all_data,"# divide into train and test sets
def split_data(city_data):
    # Get the features and labels from the Boston housing data
    X, y = city_data.data, city_data.target

    X_train, X_test, y_train, y_test = cross_validation.train_test_split(\
        X, y, test_size=0.3, random_state=0)

    return X_train, y_train, X_test, y_test

X_train, y_train, X_test, y_test = split_data(boston)",0.5226292014,
2111,step join the two dataframes along rows and assign all_data,"def loadValidationData():
    validation_x = mnist23.data[training_samples:]
    validation_y = np.array([mnist23.target[training_samples:]]) 
    return validation_x,validation_y",0.5196440816,
2111,step join the two dataframes along rows and assign all_data,"def fit_predict_model(city_data):

    # Get the features and labels from the Boston housing data
    
    X, y = city_data.data, city_data.target

    # Setup a Decision Tree Regressor
    
    regressor = DecisionTreeRegressor()
    
    parameters = {'max_depth':(1,2,3,4,5,6,7,8,9,10)}
    
    mse_scoring = make_scorer(mean_squared_error, greater_is_better=False)
    
    #using grid search to fine tune the Decision Tree Regressor and
    #obtain the parameters that generate the best training performance. 

    reg = GridSearchCV(regressor, parameters, scoring = mse_scoring)
    reg.fit(X,y)
    
    # Fit the learner to the training data to obtain the best parameter set
    print ""Final Model: ""
    print (reg.fit(X, y))    

    # Using the model to predict the output of a particular sample
    x = [11.95, 0.00, 18.100, 0, 0.6590, 5.6090, 90.00, 1.385, 24, 680.0, 20.20, 332.09, 12.13]
    x = np.array(x)
    x = x.reshape(1, -1)
    y = reg.predict(x)
    
    print ""Best Parameters: "", reg.best_params_
    print ""Best Estimator:"", reg.best_estimator_
    print ""Grid Score:"", reg.grid_scores_

    print ""House: "" + str(x)
    print ""Predicted: "" + str(y)
    
    #DataFrame of Client_Features
    #x = [11.95, 0.00, 18.100, 0, 0.6590, 5.6090, 90.00, 1.385, 24, 680.0, 20.20, 332.09, 12.13]
    #pd.DataFrame(zip(boston.feature_names, x), columns = ['Features', 'Client_Features'])",0.519557178,
2111,step join the two dataframes along rows and assign all_data,"def fit_predict_model(city_data):
    # Get the features and labels from the Boston housing data
    X, y = city_data.data, city_data.target

    # Setup a Decision Tree Regressor
    regressor = DecisionTreeRegressor()

    parameters = {'max_depth':(1,2,3,4,5,6,7,8,9,10)}

    # 1. Find the best performance metric
    # should be the same as your performance_metric procedure
    # http://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html
    mse_scorer = metrics.make_scorer(metrics.mean_squared_error, greater_is_better=False)

    # 2. Use gridearch to fine tune the Decision Tree Regressor and find the best model
    # http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html#sklearn.grid_search.GridSearchCV
    reg = grid_search.GridSearchCV(regressor, parameters, scoring=mse_scorer, cv=None)

    # Fit the learner to the training data
    print ""Final Model: ""
    print reg.fit(X, y)
    print reg.grid_scores_
    print reg.best_estimator_
    print reg.best_score_
    print reg.best_params_

    # Use the model to predict the output of a particular sample
    # Call predict on estimator with best found parameters
    # http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html#sklearn.grid_search.GridSearchCV.predict
    x = [11.95, 0.00, 18.100, 0, 0.6590, 5.6090, 90.00, 1.385, 24, 680.0, 20.20, 332.09, 12.13]
    y = reg.predict(x)
    print ""House: "" + str(x)
    print ""Prediction: "" + str(y)",0.519557178,
2111,step join the two dataframes along rows and assign all_data,"def scores_generator(df_train, df_test, pair_fn):
    
    df_test_temp = df_test.copy()
    df_test_temp = df_test_temp.reset_index()

    df_scores = pd.DataFrame(index=df_train.index, columns=['score'])
    df_scores_test = pd.DataFrame(index=df_test_temp.index, columns=['score'])

    for i in range(0, df_train.shape[0]):
        df_scores.loc[i,'string'] = df_train.loc[i,'string']
        df_scores.loc[i,'score'] = score_sentence_sg(pair_fn, model, re.findall(string_pattern, df_train.loc[i,'string']))
        
    X_train = df_scores.score
    y_train = df_train.incoherence
    
    for i in range(0, df_test_temp.shape[0]):
        df_scores_test.loc[i,'string'] = df_test_temp.loc[i,'string']
        df_scores_test.loc[i,'score'] = score_sentence_sg(pair_fn, model, re.findall(string_pattern, df_test_temp.loc[i,'string']))
        
    X_test = df_scores_test.score
    y_test = df_test_temp.incoherence
    
    return df_scores, X_train, y_train, df_scores_test, X_test, y_test",0.5183540583,
2111,step join the two dataframes along rows and assign all_data,"def process_embarked():
    global combined
    # two missing embarked values - filling them with the most frequent one in the train set(S)
    combined.Embarked.fillna('S', inplace=True)
    # dummy encoding
    embarked_dummies = pd.get_dummies(combined['Embarked'], prefix='Embarked')
    combined = pd.concat([combined, embarked_dummies], axis=1)
    combined.drop('Embarked', axis=1, inplace=True)
    status('embarked')
    return combined",0.5178658962,
2111,step join the two dataframes along rows and assign all_data,"def process_embarked():
    global combined
    # two missing embarked values - filling them with the most frequent one in the train  set(S)
    combined.Embarked.fillna('S', inplace=True)
    # dummy encoding 
    embarked_dummies = pd.get_dummies(combined['Embarked'], prefix='Embarked')
    combined = pd.concat([combined, embarked_dummies], axis=1)
    combined.drop('Embarked', axis=1, inplace=True)
    status('embarked')
    return combined",0.5178658962,
2111,step join the two dataframes along rows and assign all_data,"def process_embarked():
    
    global combined
    # There are only two missing embarked values - let's fill them with the most frequent one (S)
    combined.Embarked.fillna('S',inplace=True)
    
    # dummy encoding 
    embarked_dummies = pd.get_dummies(combined['Embarked'],prefix='Embarked')
    combined = pd.concat([combined,embarked_dummies],axis=1)
    combined.drop('Embarked',axis=1,inplace=True)
    
    status('embarked')",0.5178658962,
2111,step join the two dataframes along rows and assign all_data,"def process_embarked():
    global combined
    
    # 2 missing values in embarked column
    combined.Embarked.fillna('S',inplace=True)
    
    # encoding in dummy variables
    embarked_dummies=pd.get_dummies(combined['Embarked'],prefix='Embarked')
    combined=pd.concat([combined, embarked_dummies],axis=1)
    
    # removing embarked variable
    combined.drop('Embarked',axis=1,inplace=True)
    
    status('Embarked')",0.5178658962,
960,intro to dataframes,"# Create a DataFrame that maps product matrix row index to movie
movie_to_product_matrix = pd.DataFrame(
    list(model.trainset._raw2inner_id_items.items()
), columns=['movie_id', 'vT_index'], dtype=int).set_index('movie_id', drop=False)

# Join the newly created dataframe with the movie dataset
mapping_matrix_with_title = movie_to_product_matrix.join(movie_data['title'])

# Create a dataframe containing latent features, and join it to the remaining dataset
latent_features = pd.DataFrame(model.qi, columns=[f""Latent Feature {k}"" for k in range(1, 101)])
mapping_matrix_with_title_and_features = mapping_matrix_with_title.set_index('vT_index').join(latent_features)

mapping_matrix_with_title_and_features.head(10)",0.4339597821,
960,intro to dataframes,"# Fast b/c fetches data for all neurons at once
nl = pymaid.find_neurons(annotations='uPN right')    
nl.get_skeletons()
print(nl.cable_length)",0.4264383316,
960,intro to dataframes,"# Import data from FITS file
irc9 = hp.read_map('/work1/users/aaronb/Databrary/HEALPix/AKARI_HEALPix_orig/4096_nside/akari_9_4096_unseen.fits', 
                   nest=False)

# Replace UNSEEN with NaNs:

irc9[irc9==hp.UNSEEN] = np.nan

# Do the smoothing
irc9_smooth = hp.sphtfunc.smoothing(irc9, 
                                    fwhm = 0.00285, 
                                    iter=1)
hp.mollview(irc9_smooth,
           norm='hist',
           nest=False)",0.422082454,
960,intro to dataframes,"iris = datasets.load_iris()
X, y = iris.data[:, 1:3], iris.target

clf1 = KNeighborsClassifier(n_neighbors=1)
clf2 = RandomForestClassifier(random_state=1)
clf3 = GaussianNB()
lr = LogisticRegression()
sclf = StackingClassifier(classifiers=[clf1, clf2, clf3], 
                          meta_classifier=lr)",0.4156297445,
960,intro to dataframes,"Plot.surface3D(v1.df_pivot*10000*unit, z_label='volNormal', z_round=2)",0.4152678847,
960,intro to dataframes,"with tf.Session() as sess:
    probs = sess.run(potential_energy(tf.to_float(points)))

plt.tricontourf(points[:, 0], points[:, 1], np.exp(-probs), 100)
plt.axis('equal') 
plt.xlim([-5, 5])
_ = plt.ylim([-5, 5])",0.414832294,
960,intro to dataframes,"# add save=True, save_folder='??', name='??' to save the plot on disk
Plot.surface3D(f1.df_pivot*10000, z_label='forwardRate', z_round=2)",0.4132862091,
960,intro to dataframes,"catchment = eroCatch.catchmentErosion(folder='output',timestep=13)
catchment.regridTINdataSet()",0.411501348,
960,intro to dataframes,"(pd.read_feather('cps1990.ft')
   .query('MONTH == 2 and LFS == ""Employed"" and OCC80M in [6, 7, 8]')
   .BASICWGT.sum()) * (1 / 0.913) # BLS historical comparatability",0.4113789201,
960,intro to dataframes,"# baseline2018a is a v4 database, so we're cool to use the defaults.
slicer2 = slicers.HealpixSlicer(lonCol='randomDitherFieldPerVisitRa', 
                                latCol='randomDitherFieldPerVisitDec', 
                                latLonDeg=degrees, nside=nside)
myBundles['random dither'] = metricBundles.MetricBundle(
    metric, slicer2, sqlconstraint, runName=runName, metadata='random dither')",0.4098893106,
1801,removing outliers,"# Function to remove outlier data
def reject_outliers(data, m=2):
    return data[abs(data - np.mean(data)) < m * np.std(data)]",0.4788469374,
1801,removing outliers,"def reject_outliers(data, m=2):
    return data[abs(data - np.mean(data)) < m * np.std(data)]",0.4788469374,
1801,removing outliers,"def reject_outliers(data, m=1):
    return data[abs(data - np.mean(data)) < m * np.std(data)]",0.4788123965,
1801,removing outliers,"#clean the cata, eliminate null value and outliers
def reject_outliers(data, m=3):
    return data[abs(data - np.mean(data)) < m * np.std(data)]
    
def missingToMean(data):
    return data.fillna(np.mean(data))",0.4783806205,
1801,removing outliers,"# Defining crieteria for outliers: points having error values more than 2 standard deviation.

def is_outliers(data, m=2):
    return data[abs(data - numpy.mean(data)) > m * numpy.std(data)]",0.4737942815,
1801,removing outliers,"def find_outliers(col, lo_q, hi_q):
    """"""
    Return all values in a column lower than a low quantile and higher than a high one
    :param col: the column to retrieve outliers from
    :type col: Series
    :param lo_q: low quantile value
    :param hi_q: high quantile value
    :type lo_q, hi_q: float
    :return: (Series, Series)
    """"""
    lo_quant, hi_quant = col.quantile([lo_q, hi_q])
    return col[col < (lo_quant)], col[col > (hi_quant)]


def print_outliers(col, lo_q, hi_q):
    """"""
    Print outliers nicely.
    :param col: the column to retrieve outliers from
    :type col: Series
    :param lo_q: low quantile value
    :param hi_q: high quantile value
    :type lo_q, hi_q: float
    :return: None
    """"""
    fmt_str = (""Low {c} outliers (below {mn} quantile):\n{lo}""
               ""\nHigh {c} outliers (above {mx} quantile):\n{hi}""
               ""\nThere are {lnlo} low {c} outliers and {lnhi} ""
               ""high {c} outliers"")
    lows, highs = find_outliers(col, lo_q, hi_q)
    print fmt_str.format(c=col.name.upper(), mn=lo_q, mx=hi_q, lo=lows,
                         hi=highs, lnlo=len(lows), lnhi=len(highs))

print_outliers(gre, 0.1, 0.9)
print_outliers(gre, 0.05, 0.95)
print_outliers(gpa, 0., 0.9)
print_outliers(gpa, 0.05, 0.95)",0.4709972143,
1801,removing outliers,"#method to remove outliers
def reject_outliers(data, m=2):
    return data[abs(data - np.mean(data)) < m * np.std(data)]

prices_without_outliers = reject_outliers(prices)

sns.distplot(prices_without_outliers, bins=50, kde=True, color='r').set(xlim=(0, max(prices_without_outliers)))
plt.show()",0.4672130644,
1801,removing outliers,"def detect_outliers(vector):
    Q1 = np.percentile(vector, q=25)
    Q3 = np.percentile(vector, q=75)
    IQR = Q3 - Q1
    mask = (vector < (Q1 - 1.5*IQR)) | (vector > (Q3 + 1.5*IQR))
    return vector[mask]

print detect_outliers(np.random.normal(size=100)) # 100 random normal values
print detect_outliers(np.random.normal(size=500)) # 500 random normal values
print detect_outliers(np.random.normal(size=2000)) # 2000 random normal values",0.4591640234,
1801,removing outliers,"def reject_outliers(data, m=2):
    return data[abs(data - np.mean(data)) < m * np.std(data)]

def reject_xyz_outliers(x,y,z):
    return reject_outliers(x), reject_outliers(y), reject_outliers(z)

#Basic outlier reject and mean shift
def perform_preprocessing(x, y, z):
    x = reject_outliers(x)
    y = reject_outliers(y)
    z = reject_outliers(z)
    
    #zero mean 
    x = x - np.mean(x)
    y = y - np.mean(y)
    z = z - np.mean(z)
    
    return x,y,z

#Savitzky-Golay curve smoothing
def smooth_curve(x,y,z, window_size=51, polynomial=3):
    x_hat = scipy.signal.savgol_filter(x, window_size, polynomial)
    y_hat = scipy.signal.savgol_filter(y, window_size, polynomial)
    z_hat = scipy.signal.savgol_filter(z, window_size, polynomial)
    
    return x_hat, y_hat, z_hat",0.4580235779,
1801,removing outliers,"# use (data - mean) < 3 * std to determine whether to drop
def drop_outliers(df, col):
    return df[((df[col] - df[col].mean()) / df[col].std()).abs() < 3]
features = ['debt_ratio']
old_len = len(df)
for f in features:
    df = drop_outliers(df, f)

print('drop out {} outliers'.format(old_len - len(df)))",0.4420579672,
1084,list of recommendations,"def test_recommenders(client):
    return tuple([recommender.can_recommend(client) for recommender in recommenders.values()])",0.4217510819,
1084,list of recommendations,"class TopMovies(RecommenderModel):
    def build(self):
        self._recommendations = None # this is required line in order to ensure consitency in experiments
        itemid = self.data.fields.itemid # get the name of the column, that corresponds to movieid
        
        # calculate popularity of the movies based on the number of ratings
        item_scores = self.data.training[itemid].value_counts().sort_index().values
        
        # store it for later use in some attribute
        self.item_scores = item_scores
        
        
    def get_recommendations(self):
        userid = self.data.fields.userid #get the name of the column, that corresponds to userid
        
        # get the number of test users
        # we expect that userid doesn't have gaps in numbering (as it might be in original dataset,
        # RecommenderData class takes care of that)
        num_users = self.data.test.testset[userid].max() + 1
        
        # repeat computed popularity scores in accordance with the number of test users
        scores = np.repeat(self.item_scores[None, :], num_users, axis=0)
        
        # we got the scores, but what we actually need is items (their id)
        # we also need only top-k items, not all of them (for top-k recommendation task)
        # here's how to get it:
        top_recs = self.get_topk_items(scores)
        # here leftmost items are those with the highest scores
        
        return top_recs",0.4177908003,
1084,list of recommendations,"# Creating a new data frame
recommendations <- debate_tools

# Grouping by language_preference and then LanguageRecommendationSelect
recommendations <- recommendations  %>% 
    group_by(language_preference, LanguageRecommendationSelect)  %>% 
    summarise(count=n()) %>%

# Removing empty responses and include the top recommendations
    filter(is.na(LanguageRecommendationSelect)==F) %>%
    arrange(language_preference, desc(count)) %>%
    mutate(row_number = row_number()) %>%
    filter(row_number <= 4)

recommendations",0.4032126367,
1084,list of recommendations,"def get_recommendations(user_id,n,U,V_df,all_movies_genre):
        #Get all the movies rated by user ID 
        user_rated = Utility.loc[user_id][Utility.loc[user_id].notnull()]    
        #Get all the ratings of the user having more than 4    
        user_high_rated_df = all_movies_genre[all_movies_genre[""item_id""].isin( \
        user_rated[user_rated.values >= 4].index)]
        user_V_df = V_df[V_df[""item_id""].isin(user_rated.index)]
        
        user_watched= pd.merge(user_V_df,user_high_rated_df,on=""item_id"")
        user_watched[""watched""] = ""Yes""

        
        user_UV = UV.loc[user_id].copy()
        user_UV[user_rated.index] = -1
        recommended_movie_ids =  user_UV.sort_values(ascending=False).index[0:n]
        recommended_movies_df = V_df[V_df[""item_id""].isin(recommended_movie_ids)]

        recommended_movies_df=pd.merge(recommended_movies_df,
         all_movies_genre[all_movies_genre[""item_id""].isin(recommended_movie_ids)],
         on=""item_id"")
        recommended_movies_df[""watched""] = ""No""

        print ""The following movies were liked by user ID:{}, since s/he gave a rating of 4 or more\
         for these movies:"".format(user_id)
        display(user_watched)
        print ""The following movies are recommended to user ID:{}"".format(user_id)
        display(recommended_movies_df)
        plot_df = pd.concat([user_watched,recommended_movies_df])
        #display(plot_df)
        #fig, ax = plt.subplots()
        #for dd,daata in plot_df.groupby('watched'):
        #     ax.plot(daata['Factor-1']*100,daata['Factor-2']*100,'o',alpha=0.3)
        import seaborn as sns
        sns.set(style=""ticks"", context=""talk"")
        # Make a custom sequential palette using the cubehelix system
        #pal = sns.cubehelix_palette(20, 1, 1, light=.4, dark=.6)
        pal= sns.color_palette(""husl"", 2)
         
        sns.lmplot(x=""Factor-1"",y=""Factor-2"",fit_reg=False,hue=""watched"",
           palette=pal,data=plot_df,size=10,scatter_kws={'alpha':0.3})
         
        plt.show()
        return plot_df",0.3963856697,
1084,list of recommendations,"class RecommendationType(Enum):
    STABLE = auto()
    TESTING = auto()
    LATEST = auto()",0.3937881291,
1084,list of recommendations,"# This function calculates the catelog coverage 
# The inputs are: the model we trained, the test dataset and number of recommendations we make for each user
# The output is a result of catelog coverage 
def cal_coverage(model,test,num_k):
    recs= float(model.recommend(users=test['user_id'],k=num_k,items=test[""item_id""],verbose=False).to_dataframe().item_id.nunique())
    all = float(test.to_dataframe().item_id.nunique())
    return recs/all",0.3923383951,
1084,list of recommendations,"def gen_recs(model, data):
    recs = model.recommend(data['user_id'])
    return recs",0.3913006186,
1084,list of recommendations,"def evaluate_mrr(model):
    '''Function to calculate MRR score.'''
    is_holdout = model.recommendations==0 # holdout items are always in the first column before sorting
    pos = np.where(is_holdout)[1] + 1.0 # position of holdout items (indexing starts from 0, so adding 1) 
    mrr = np.reciprocal(pos).mean() # mean reciprocal rank
    return mrr",0.3896771073,
1084,list of recommendations,"def calc_avg_rating(m_list):
    '''
        Given a list of movies, finds the average movie rating. 
    '''
    
    rating = []

    # get rating for each movie
    for m in m_list:
        rating.append(m.imdb_rating)

    return np.mean(rating)

def find_avg_director_rating(tup):
    '''
        Given a director:movie_list object, finds the average rating for the 
        director. 
    '''

    # break apart tuple
    director, m_list = tup
    
    return calc_avg_rating(m_list)",0.3857802749,
1084,list of recommendations,"class Recommender(object):
    '''
    Main recommender engine, that decides how to proceed with a particular customer ID.
    '''
    def __init__(self, data, mba_analyzer):
        self.cust_id = None
        self.shop_id = None
        self.cust_type = None
        self.suggested_items = None
        self.basket = []
        self.basket_analyzer = mba_analyzer
        print(""Initialized Recommender.."")
        
    def add_to_basket(self, barcode):
        self.basket.append(barcode)
        
    def remove_from_basket(self, barcode):
        self.basket.remove(barcode)

    def clear_data(self):
        self.cust_id = None
        self.shop_id = None
        self.basket.clear() # or del self.basket[:]
        self.cust_type = None
        print()
        
    def calc_retention(self, _from, _until):
        # train_[train_['shopId'] == self.shop_id]['orderProcessingTime'].min()
        df_earlier = train_[train_['orderProcessingTime'] < _from]
        df_during = train_[(train_['orderProcessingTime'] >= _from) & (train_['orderProcessingTime'] <= _until)]
        S = df_earlier['customerId']
        E = train_[train_['orderProcessingTime'] <= _until]['customerId']
        N = set(E) - set(S)
        CR = set(df_during['customerId']) - N
#         print(CR)
        
#         print(S.unique().tolist())
#         _from = _from.strftime(""%d, %b %Y"")
#         _until = _until.strftime(""%d, %b %Y"")
        _from = custom_strftime('%B {S}, %Y', _from)
        _until = custom_strftime('%B {S}, %Y', _until)

        
        print(""\nFor the period:- from %s, until %s :-"" % (_from, _until))
        print(""\tCustomer Retention Rate = (CR/S)*100 \nCRR = %s"" 
              % (len(CR)/len(set(S))*100))        
        
        print(""""""\t# of customer at the end [{}] (E): {}
        # of customers before [{}] (S): {}\n\
        # of new customers acquired (N): {}\n\
        # of customers retained [from before {}] (CR): {}""""""\
              .format(_until, len(E.unique()),
                      _from, len(S.unique()), 
                      len(N),
                      _from, len(CR)))
    
#     def load_basket_pairs(self):
#         self.basket_analyzer.load()
    def details(self):
        print(""--------------------------------------"")
        print(""Customer ID: "", self.cust_id)
        print(""Shop ID: "", self.shop_id)
        basket = df_products[df_products['barcodeId'].isin(self.basket)].productName.tolist()
        print(""Basket: "", basket)
#         if not self.suggested_items.empty:
#             print(""======================================"")
#             print(""\nTop Recommendations: \n"", self.suggested_items)
#             print(""======================================"")
#         print(""--------------------------------------"")
        print()
        
    def market_basket_analyzer(self, basket=[]):
        """"""
        @cust_type: kind of customer (if retained or not). 
               Values stand for:
                - 0: Retained (and went to same store)
                - 1: Retained (but goes to a new store this time)
                - 2: Acquired (doesn't exist in the database yet)        
        """"""
        # TODO: add cohort analysis and make basket analysis a conditional on whether customer data exists
        if bool(basket):
            if self.cust_type==2:
                self.suggested_items = self.basket_analyzer.get_closest_pairs(self.shop_id, basket)
            elif self.cust_type==1:
                # TODO: add cohort analysis
                self.suggested_items = self.basket_analyzer.get_closest_pairs(self.shop_id, basket)            
            else:
                # TODO: add cohort analysis
                pair_1 = self.basket_analyzer.get_closest_pairs(self.shop_id, basket)
                if pair_1.empty:
                    pair_1 = self.basket_analyzer.top_items(self.shop_id)
                pair_2 = self.basket_analyzer.analyze_historical_data(self.shop_id, basket)
                self.suggested_items = set(pair_1) & pair_2
        else:
            self.suggested_items = self.basket_analyzer.top_items(self.shop_id)
           
        if self.suggested_items.empty:
#         if not bool(self.suggested_items):
            self.suggested_items = self.basket_analyzer.top_items(self.shop_id)['productName'].values   
        print(""Top Recommendations: "")
        print(self.suggested_items['productName'].values)
        
    def populate(self, cust_id=None, shop_id=None, cust_type=None, basket=[]):
        """"""
        @cust_id: Customer's ID (if new, should be serialized in sync with the Database)
        
        @shopId: Current shop in which the customer is looking for recommendations
                
        @basket: barcoded list of items that are already present in this customer's cart
        """"""
        if not cust_id:
            print(""ERROR: need a customer ID to suggest items with.."")
            self.invalid_data = 1
            return
        
        self.cust_id = cust_id
        self.shop_id = shop_id
        if basket:
            self.basket = basket   
        if shop_id and (not any(train_.shopId == shop_id)):
            print(""ERROR: Customer ID %s supplied with Invalid Shop ID %s. "" 
                  % (cust_id, shop_id))
            self.invalid_data = 1
            return
                
        print(""INFO: Customer ID %s with shop # %s -- "" 
              % (cust_id, shop_id), end="""")
        self.invalid_data = 0
        
    def suggest(self):
        """"""
        @cust_type: kind of customer (if retained or not). 
               Values stand for:
                - 0: Retained (and went to same store)
                - 1: Retained (but goes to a new store this time)
                - 2: Acquired (doesn't exist in the database yet)        
        """"""
        if self.invalid_data == 1:
            return
        
        if not self.cust_type:
            if ((train_['customerId'] == self.cust_id) & (train_['shopId'] == self.shop_id)).any():
                print(""[Retained][went to same store]"")
                self.cust_type=0
            elif ((train_['customerId'] == self.cust_id) & (train_['shopId'] != self.shop_id)).any():
                print(""[Retained][goes to a new store]"")
                self.cust_type=1
            else:
                # not any(train_.customerId == cust_id)
                print(""[Acquired]"")
                self.cust_type=2
                
        self.market_basket_analyzer()
            
#         return self.suggested_items",0.3845792711,
1817,review of linear algebra,"fp11.wright_fisher.evolve(rng,pop_anc,
                          params_const,recorder_anc) # constant pop  + outcrossing

fp11.wright_fisher.evolve(rng,pop_bottle_outcross,
                          params_bottle,recorder_bottle_outcross) # bottleneck  + outcrossing

clonal.evolve(rng,pop_const_clonal,
              params_const,recorder_const_clonal) # constant pop  + clonal

clonal.evolve(rng,pop_bottle_clonal,
              params_bottle,recorder_bottle_clonal)# constant pop  + clonal",0.5085457563,
1817,review of linear algebra,"# TODO
lr = LogisticRegression(solver='liblinear')
compute_score(lr, X, y)",0.4983897805,
1817,review of linear algebra,"mesh.left_boundary = ""zero""
mesh.right_boundary = ""zero""

solver = diffusion.Solver(materials, mesh)
solver.solve()

eigs = solver.extract_eigenvalues()
slab_x, fluxes = solver.extract_eigenvectors()

fig, ax = plt.subplots(figsize=(10, 7.5))

ax.plot(slab_x, fluxes[0,:,0], label=""Fast flux"")
ax.plot(slab_x, fluxes[1,:,0], label=""Thermal flux"")
ax.set_xlabel(""Slab position [cm]"", fontsize=14)
ax.set_ylabel(""Flux [-]"", fontsize=14)
ax.legend(fontsize=14)",0.4964576364,
1817,review of linear algebra,"# non-BPR states from already needed SU(5) reps that can possibly be low
vbl = [# From vectorlike fermion 5
       Weyl(3,1,-Rational(2,3), 6),
       # from 10
       ComplexScalar(3,1,-Rational(4,3)),
       ComplexScalar(3,2,Rational(1,3)),
       # from 24
       RealScalar(8,1,0),
       # from 45
       ComplexScalar(1,2,1),
       ComplexScalar(3,2,-Rational(7,3)),
       ComplexScalar(6,1,-Rational(2,3)),
       ComplexScalar(8,2,1)]",0.494927913,
1817,review of linear algebra,"# States from already needed SU(5) reps that can possibly be low
vbl70 = [
       # From vectorlike fermion 5
       Weyl(3,1,-Rational(2,3), 6),
       # from 10
       ComplexScalar(3,1,-Rational(4,3)),
       ComplexScalar(3,2,Rational(1,3)),
       # from 24
       RealScalar(8,1,0),
       # from 70
       ComplexScalar(1,2,1),
       ComplexScalar(1,4,1),
       ComplexScalar(3,3, Rational(8,3)),
       ComplexScalar(6,2,-Rational(7,3)),
       ComplexScalar(8,2,1),
       # non-pert?? ComplexScalar(15,1,-Rational(2,3)),
]",0.494927913,
1817,review of linear algebra,"class IncompressibleNavierStokesSolver:
    
    def __init__(self, 
            mesh, boundary_condition_values, parameters):
        
        element = fenics.MixedElement([
            fenics.VectorElement('P', mesh.ufl_cell(), 2),
            fenics.FiniteElement('P', mesh.ufl_cell(), 1)])
        
        function_space = fenics.FunctionSpace(mesh, element)
        
        solution = fenics.Function(function_space)
        
        
        self.function_space = function_space
        
        self.solution = solution
        
        self.parameters = parameters
        
        
        class Boundaries(fenics.SubDomain):

            def inside(self, x, on_boundary):

                return on_boundary
                
        boundaries = Boundaries()
        
        boundary_conditions = [
            fenics.DirichletBC(
                function_space.sub(i),
                boundary_condition_values[i],
                boundaries)
            for i, g in enumerate(boundary_condition_values)]
        
        
        F = self.variational_form()
        
        problem = fenics.NonlinearVariationalProblem(
            F = F,
            u = solution,
            bcs = boundary_conditions,
            J = fenics.derivative(F, solution))
        
        fenics_solver = fenics.NonlinearVariationalSolver(problem)
        
        
        self.element = element
        
        self.fenics_solver = fenics_solver
        
        
        """""" `self.fenics_solver` breaks 
        (presumably due to some kind of scoping issue)
        if we don't make `boundary_conditions` an attribute. """"""
        self.boundary_conditions = boundary_conditions
        
    def variational_form(self):
        
        mu = fenics.Constant(self.parameters[""dynamic_viscosity""])
        
        inner, dot, grad, div, sym = \
            fenics.inner, fenics.dot, fenics.grad, fenics.div, fenics.sym
        
        u, p = fenics.split(self.solution)
        
        psi_u, psi_p = fenics.TestFunctions(self.function_space)
        
        variational_form = (dot(psi_u, dot(grad(u), u))
            - div(psi_u)*p + 2.*mu*inner(sym(grad(psi_u)), sym(grad(u)))
            + psi_p*(div(u)))*fenics.dx
        
        return variational_form
    
    def solve(self):
        
        self.fenics_solver.solve()",0.4918513298,
1817,review of linear algebra,"var('a b c d e f')
solve(
[
2*a-b-d, # equation for p_a
3*b-a-c-e, # equation for p_b
2*c-b-1,# ...
3*d-a-e-f,
3*e-b-d,
f-d    
],
[a,b,c,d,e,f]  # unknowns
)",0.4915543199,
1817,review of linear algebra,"likelihood_H0 = (3*(-1+1*log(1)-log(1)))+(3*(-1+0*log(1)-log(1)))+(-1+4*log(1)-log(24))+(2*(-1+2*log(1)-log(2)))+(-1+3*log(1)-log(6))
lML = sum([1,1,0,4,2,1,3,0,0,2])/10
likelihood_ML = (3*(-lML+1*log(lML)-log(1)))+(3*(-lML+0*log(lML)-log(1)))+(-lML+4*log(lML)-log(24))+(2*(-lML+2*log(lML)-log(2)))+(-lML+3*log(lML)-log(6))
2*(likelihood_ML-likelihood_H0)",0.4888572693,
1817,review of linear algebra,"# States from already needed SU(5) reps that can possibly be low
mnm = [# From vectorlike fermion 5
       Weyl(1,2,-1, 6), # E_L,R
       Weyl(3,1,-Rational(2,3), 6),
       # from 10
       ComplexScalar(1,1,2),   # h
       ComplexScalar(3,1,-Rational(4,3)),
       ComplexScalar(3,2,Rational(1,3)),
       # from 24
       RealScalar(1,3,0),   # Delta
       RealScalar(8,1,0)]",0.4859764576,
1817,review of linear algebra,"integrate((4 - 4*z)*(4 - (2*y - 2*y*z)**2), (x,0,1), (y,0,1), (z,0,1))",0.4850293994,
1335,our first metacharacters and,"def open_bounds():
    for met in mcaps.metabolites:
        if met.id not in ['na1_c', 'ca2_c', 'h_c', 'cl_c']:
            rxn_id = ""TEST_"" + met.id
            rxn = mcaps.add_boundary(met, reaction_id=rxn_id, type=""custom"", ub=1000, lb=0)",0.3895004988,
1335,our first metacharacters and,"@arts_agenda
def inversion_iterate_agenda(ws):
    ws.x2artsStandard()  
    ws.atmfields_checkedCalc()
    ws.atmgeom_checkedCalc()
    ws.yCalc()
    ws.Print(ws.y)
    ws.Print(ws.jacobian)
    ws.VectorAddVector( ws.yf, ws.y, ws.y_baseline )
    ws.IndexAdd(ws.inversion_iteration_counter, ws.inversion_iteration_counter, 1)
    
ws.Copy(ws.inversion_iterate_agenda, inversion_iterate_agenda)",0.3860869706,
1335,our first metacharacters and,"class Character:
    def __init__(self, x, y):
        self.img=pygame.image.load(""art/LPC/walk.png"")
        self.frames = list()
        self.cycle_index = 0
        self.cycle_tick = 0
        self.cycle_tick_per_frame = 100
        self.cycle_length = 7
        for i in range(self.cycle_length):
            self.frames.append(self.img.subsurface((64+i*64,0,64,64)))
        self.pos = (x,y)
        
    def update(self, dt):
        self.cycle_tick = (self.cycle_tick + dt) % (self.cycle_length*self.cycle_tick_per_frame)
        self.cycle_index = int(self.cycle_tick/self.cycle_tick_per_frame)
        pass
        
    def render(self, display):
        display.blit(self.frames[self.cycle_index], (self.pos[0]+200, self.pos[1]))",0.3732410073,
1335,our first metacharacters and,"def find_palindromes(seq, n):
    palindromes = []
    
    for _________________:
        
        
        if _________________:
            palindromes.append(___________)
            
            
            
            
    return palindromes

DNA_seq = 'GGAGCTCCCAAAGCCATCAATATTCATCAAAACGAATTCAACGGAGCTCGATATCGCATCGCAAAAGACACC'
palindromic_sequences = find_palindromes(DNA_seq,6)
assert palindromic_sequences == ['GAGCTC', 'AATATT', 'GAATTC', 'GAGCTC', 'GATATC']",0.3701088428,
1335,our first metacharacters and,"class Character:
    def __init__(self, x, y):    
        self.img=pygame.image.load(""art/LPC/walk.png"")
        # Each animation is stored in the anim dict as a list with the following 
        # format: (tick_per_frame, frame1, frame2, ...)
        
        self.anim = dict()
        self.cycle_index = 0
        self.cycle_tick = 0
        seq = list()
        seq.append(80) # ticks per frame
        for i in range(8):
            seq.append(self.img.subsurface((64+i*64,0,64,64)))
        self.anim[""up""] = seq
        
        seq = list()
        seq.append(80) # ticks per frame
        for i in range(8):
            seq.append(self.img.subsurface((64+i*64,128,64,64)))
        self.anim[""down""] = seq

        seq = list()
        seq.append(80) # ticks per frame
        for i in range(8):
            seq.append(self.img.subsurface((64+i*64,64,64,64)))
        self.anim[""left""] = seq

        seq = list()
        seq.append(80) # ticks per frame
        for i in range(8):
            seq.append(self.img.subsurface((64+i*64,192,64,64)))
        self.anim[""right""] = seq

        self.current_anim = ""up""
        self.current_frames = self.anim[self.current_anim]
        self.pos = [x,y]

    def update(self, dt):
        ca = self.anim[self.current_anim]
        self.cycle_tick = (self.cycle_tick + dt) % ((len(ca)-1)*ca[0])
        self.cycle_index = int(self.cycle_tick/ca[0])
        pass
        
    def render(self, display):
        ca = self.anim[self.current_anim]
        display.blit(ca[1+self.cycle_index], (self.pos))",0.3692531586,
1335,our first metacharacters and,"class CARE:

	def __init__(self, filename):
		self.patients, self.diseases, self.disease_codes, self.dic = self.setupCARE(filename)

	def getPatients(self):
		return self.patients

	def getDiseases(self):
		return self.diseases

	def getDiseaseCodes(self):
		return self.disease_codes

	def getDic(self):
		return self.dic


	##### FUNCTION TO SET UP DATA FOR ANALYSIS #####
	def setupCARE(self, filename):

		def cleanData(filename):
			
			def calculate_gender(gender):
				if gender == 'M':
					return 0
				elif gender == 'F':
					return 1
				else:
					return 2
			def calculate_age(born):
				today = date.today()
				b_date = datetime.strptime(born, '%m/%d/%Y')
				return today.year - b_date.year - ((today.month, today.day) < (b_date.month, b_date.day))


			relevant_columns = ['Member System ID', 'Adjudication Date', 'Patient Birth Date', \
								'Patient Gender Code', 'Diagnosis One Code', \
								'Diagnosis Two Code', 'Diagnosis Three Code', \
								'Diagnosis Four Code', 'Diagnosis Five Code']

			df = pd.read_csv(filename, usecols=relevant_columns, dtype=np.str)\
					.drop_duplicates()\
					.reset_index().drop('index', axis=1).fillna(0)
			df = df[relevant_columns]
			df['Adjudication Date'] = pd.to_datetime(df['Adjudication Date'], format='%m/%d/%Y')
			df['Patient Birth Date'] = df['Patient Birth Date'].apply(calculate_age)
			df['Patient Gender Code'] = df['Patient Gender Code'].apply(calculate_gender)

			return df

		def parseCSV(categoryfile='$dxref 2015.csv'):
			""""""
			USAGE
			categoryfile - The file provided by HCUP. Should be called '$dxref 2015.csv'

			RETURNS
			dictionary - Dictionary mapping from {icd9 codes : icd9 description }
			""""""

			dictionary = {}

			# parse the diagnosis codes file
			count = 0
			with open(categoryfile, 'rb') as csvfile:
				datareader = csv.reader(csvfile)

				for row in datareader:
					if count >= 3:
						row[0] = row[0].replace(""'"","""").strip()
						dictionary[row[0]] = row[3]
					count+=1

			csvfile.close()

			return dictionary

		def createPatients(df, disease_codes):

			def parse_diags(diag_list, disease_codes):

				def check_valid(code, disease_codes):

					if code == 0 or code == '-------':
						return 0

					if code in disease_codes:
						return code
					else:
						new_code = '0' + code
						if new_code in disease_codes:
							return new_code
						else:
							new_code2 = '0' + new_code
							if new_code2 in disease_codes:
								return new_code2
							else:
								return 0


				new_list = []
				for diag in diag_list:
					new_diag = check_valid(diag, disease_codes)
					(new_list.append(new_diag) if new_diag is not 0 else 0)
				return new_list

			patients, diseases = {}, {}

			for row in df.itertuples():
				mem_id = row[1]
				adj_date = row[2]
				age = row[3]
				gender = row[4]
				visit = parse_diags(row[5:], disease_codes)
				
				for item in visit:
					if item not in diseases:
					    diseases[item] = set()
					diseases[item].add(mem_id)

				if mem_id not in patients:
					p = Patient(mem_id, gender, age, visit, adj_date)
					patients[mem_id] = p
				else:
					patients[mem_id].addVisit(adj_date, visit)

			return patients, diseases

		#categoryfile = '$dxref 2015.csv'
		df = cleanData(filename)
		dic = parseCSV()
		disease_codes = set(dic.keys())
		patients, diseases = createPatients(df, disease_codes)
		return patients, diseases, disease_codes, dic


	##### FUNCTION TO FILTER DATA FOR TRAINING SETS #####
	def train(self, target):
		patient_train = {}
		disease_train = {}
		target_diseases = target.getUnique()

		for patient in self.patients.values():
			combined = target_diseases & patient.getUnique()
			if len(combined) >= 2:
				patient_train[patient.getMemID()] = patient
				for disease in patient.getUnique():
					if disease not in disease_train:
						disease_train[disease] = set()
					disease_train[disease].add(patient.getMemID())
			

		return patient_train, disease_train



	##### COLLABORATIVE FILTERING ALGORITHMS #####

	def evaluate(self, a, patient_set, disease_set, mode):

		def w(a, i):
			
			def f(self, j):
				""""""Returns: log(# of patients in database / # of patients with disease j)""""""
				if j not in disease_set:
					return 1.0                
				return np.log( (1.0)*len(patient_set) / len(disease_set[j]) )

			total_sum = 0
			combined = a.getUnique() & i.getUnique()
			for disease in combined:
				first_half = f(self, disease) / math.sqrt(sum(f(self, k)**2 for k in a.getUnique()))
				second_half = f(self, disease) / math.sqrt(sum(f(self, k)**2 for k in i.getUnique()))
				total_sum += first_half * second_half
			return total_sum

		def K(self, a):
			return 1.0 / (sum(w(a, i) for i in patient_set.values()))

		def V(self, j):
			return (1.0) * len(disease_set[j]) / len(patient_set)

		def V_C(self, j, c):
			return (1.0) * len(disease_set[j] & disease_set[c]) / len(patient_set)

		def z(j, c):
			
			def S(self, p):
				n1 = len(self.diseases[c])
				n2 = len(self.patients)
				return math.sqrt( (p * (1.0 - p) / n1) + (p * (1.0 - p) / n2) )

			p1 = V_C(self, j, c)
			p2 = V(self, j)
			weighted_avg = (p1 + p2) / 2
			score = (p1 - p2) / S(self, weighted_avg)
			return score

		def p(self, j):
				return V(self, j) + K(self, a) * (1.0 - V(self, j)) * (sum(w(a, patient_set[i]) for i in disease_set[j]))

		def getCARE(self):
			disease_score = []
			for disease in disease_set.keys():
				score = p(self, disease)
				disease_score.append([score, disease])
			return disease_score

		def getICARE(self):
			disease_score = []
			norm_constant = K(self, a)
			for j in disease_set.keys():
				max_score = 0
				for c in a.getUnique():
					if j == c:
						continue
					if z(j, c) >= 1.96 or z(j, c) <= -1.96:
						combined = disease_set[c] & disease_set[j]
						current_score = V_C(self, j, c) + norm_constant * (1.0 - V_C(self, j, c)) * (sum(w(a, patient_set[i]) for i in combined))
						if current_score > max_score:
							max_score = current_score
				disease_score.append([max_score, j])

			return disease_score

		if mode == 'CARE':
			disease_score = getCARE(self)
		
		elif mode == 'ICARE':
			disease_score = getICARE(self)

		return disease_score

	##############################################
	

	def predict(self, target, mode):

		# Filter the data first
		if mode == 'CARE':
			patient_train, disease_train = self.train(target)
		else:
			patient_train = self.patients
			disease_train = self.diseases

		disease_score = self.evaluate(target, patient_train, disease_train, mode)
		disease_score.sort(key = lambda x: x[0], reverse=True)

		#self.printPatient(target, self.dic)
		self.printDiseases(target, disease_score[:20], self.dic)


	##############################################

	##### PRINT FUNCTIONS #####

	def printPatient(self, patient, dic):
		count = 1
		print('The patient has the following diseases:')
		for disease in patient.getUnique():
			print('\t%d. ' %count + dic[disease] + ' (' + disease + ')')
			count+=1
		print('\n')


	def printDiseases(self, patient, predDisease, dic):
		count = 1
		print('The patient has a possibility of getting the following diseases:')
		for disease in predDisease:
			if disease[1] in patient.getUnique():
				continue
			if disease[0] <= 0.0:
				break
			print('\t%d. ' %count + dic[disease[1]] + ' (' + disease[1] + \
				  ') -- ' + '{0:.2f}'.format(disease[0]))
			count+=1
			if count == 11:
				return

	###########################",0.368209362,
1335,our first metacharacters and,"def loadMovieNames():
    movieNames = {}
    with open(""ml-100k/u.ITEM"") as f:
        for line in f:
            fields = line.split('|')
            movieNames[int(fields[0])] = fields[1]
    return movieNames # {id:name}

## broadcast the dictionary to every node in cluster, efficient! 
nameDict = sc.broadcast(loadMovieNames())

result = (moviesrdd
          .map(lambda x:x.split('\t')[1])
          .map(lambda x:(int(x),1))
          .reduceByKey(lambda x,y:x+y) #counting
          .sortBy(lambda (id,count):count, ascending = False)
          .map(lambda (id,count):(nameDict.value[id],count))   ## retrieve dictionary value using .value
          .take(10)
          )
result
#name, count",0.366481483,
1335,our first metacharacters and,"class Character():
    def __init__(self): # Creating a dunder method, that will intialize everytime an object is created. (Constructor Function)
        self.name = """" # Creates empty string
        self.sex = """"
        self.level = 0 
        self.max_health = int() # Sets self.max_health to an 'empty'(0) int.
        self.current_health = int()
        self.max_speed = 100 # Just for sawg.
        self.current_speed = int()
    
    #These are methods that describe behaviours of the characters. 
    def sprint(self):
        self.current_speed += 25
        
    def is_hurt(self):
        self.current_health -= 20
    
    def levels_up(self):
        self.max_health += 50
        self.level += 1

        
# The above class provides a template where we can create multiple objects or characters in our case.
# Now we can create objects from the class. (Create a painting from the template we made above.)
print(""State"", ""Name"", ""Level"", ""Max_Health"", ""Current_Health"", ""Current_Speed"") #Ignore. 


# Creating a character called Dora.
dora = Character()   #This line creates the object.

dora.name = ""Dora""   
dora.sex = ""Male""
dora.level = 6
dora.max_health = 100
dora.current_health = 75
dora.current_speed = 50

print(""Before:"", dora.name, dora.level, dora.max_health, dora.current_health, dora.current_speed)  #etc.


#Creating a character called Swiper. 
swiper = Character()

swiper.name = ""Swiper""
swiper.sex = ""He's a cartoon monkey, IDK""
swiper.level = 3
swiper.max_health = 150
swiper.current_health = 100
swiper.current_speed = 50

print(""Before:"", swiper.name, swiper.level, swiper.max_health, swiper.current_health, swiper.current_speed) #etc.


# Activites (these would be triggred by events in the game.)
dora.sprint()
swiper.is_hurt()
dora.is_hurt()
swiper.levels_up()
print(""After Activity:"", dora.name, dora.level, dora.max_health, dora.current_health, dora.current_speed)
print(""After Activity:"", swiper.name, swiper.level, swiper.max_health, swiper.current_health, swiper.current_speed)",0.3649973273,
1335,our first metacharacters and,"for current_montage in get_builtin_montages():

    montage = mne.channels.read_montage(current_montage,
                                        unit='auto',
                                        transform=False)

    info = mne.create_info(ch_names=montage.ch_names,
                           sfreq=1,
                           ch_types='eeg',
                           montage=montage)

    fig = plot_alignment(info, trans=None,
                         subject='fsaverage',
                         subjects_dir=subjects_dir,
                         eeg=['projected'],
                         )
    mlab.view(135, 80)
    mlab.title(montage.kind, figure=fig)",0.3627815545,
1335,our first metacharacters and,"def klabel_connect(data):
    data['P1_klabel'] = data.First_pokemon.map(klabel_dict)
    data['P2_klabel'] = data.Second_pokemon.map(klabel_dict)   
    return data",0.3567141294,
2391,train it,"def main():
    args = docopt(__doc__)
    try:
        model = DenseGGNNChemModel(args)
        model.train()
    except:
        typ, value, tb = sys.exc_info()
        traceback.print_exc()
        pdb.post_mortem(tb)


if __name__ == ""__main__"":
    main()",0.5342186689,
2391,train it,"def train_input_fn():
    ds_tr = dataset.training_dataset(hparams.data_dir, DATA_SET)
    ds_tr_tr, _ = split_datasource(ds_tr, 60000, 0.95)
    ds1 = ds_tr_tr.cache().shuffle(buffer_size=57000).\
        repeat(hparams.train_epochs).\
        batch(hparams.batch_size)
    return ds1

def eval_input_fn():
    ds_tr = dataset.training_dataset(hparams.data_dir, DATA_SET)
    _, ds_tr_ev = split_datasource(ds_tr, 60000, 0.95)
    ds2 = ds_tr_ev.batch(hparams.batch_size)
    return ds2",0.5329604149,
2391,train it,"def main():
    lines = load_lines(data_path)[1:]
    training_set_lines, validation_set_lines = train_test_split(lines, test_size=0.2)
    
    nb_training = len(training_set_lines)*6
    nb_validation = len(validation_set_lines)*6

    training_images, steering_angles = get_data_without_generator(data_path, lines[0:500])
    return (training_images, steering_angles)
data_path = ""data-from-udacity""
#main()",0.5327712297,
2391,train it,"def train(model, data):
    """"""
    This is the standard training function
    
    Arguments:
        - model: This is the resnet model to be
                trained
        - data: This is the dataloader for the
                training data
    """"""
    
    model.train()
    
    epoch_len = len(data.dataset)
    
    with tqdm(range(num_epochs)) as bar:
        for _ in bar:
            running_loss = 0.0
            running_corrects = 0.0
            
            for inputs, labels in data:
                inputs = inputs.to(device)
                labels = labels.to(device)
                
                optimizer.zero_grad()
                
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                
                loss.backward()
                optimizer.step()
                
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(
                    torch.argmax(outputs, 1) == labels.data
                )
                
            bar.set_postfix({
                ""loss"": ""%.4f"" % (running_loss / epoch_len),
                ""acc"": ""%.4f"" % (running_corrects.double() / epoch_len)
            })",0.5289000273,
2391,train it,"def do_train():
    global model
    model = create_model()
    reader = create_reader(data_dir + ""/atis.train.ctf"", is_training=True)
    train(reader, model)
do_train()",0.5271136761,
2391,train it,"def validate_model(model, loader):
    """"""
    This function parses a given loader and returns the avergae (per image) loss (as defined by ""my_loss"" 
    of the entire dataset associated with the given loader.
    
    Args:
        model  - neural network to examine
        loader - where input data comes from (train, validation, or test)
        
    returns:
        average loss per image in variable named ""avg_loss""
    """"""

    model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)
                  # (dropout is set to zero)

    #----------------------------------------------
    # implementation needed here 
    #----------------------------------------------
    
   




    model.train()  #back to default
    return avg_loss",0.5253720284,
2391,train it,"def run_epoch_confidences(iterator: Iterator):
        is_train = iterator.train
        batch_accuracies = []
        batch_losses = []
        epoch_start = time.time()
        for batch in iterator:
            input_dict = {}
            lengths_dict = {}
            if hasattr(batch, 'text'):
                text, lengths = batch.text
                
                #IMPORTANT.  ADJUSTS BETWEEN DATASETS
#                 text = text.data.cpu().numpy().tolist()
#                 for i, group in enumerate(text):
#                     for j, item in enumerate(group):
#                         text[i][j]=asr_to_clean_vocab[item]
#                 text = Variable(torch.from_numpy(np.asarray(text))).cuda()
                
                input_dict['text'] = text
                lengths_dict['text'] = lengths

            page = batch.page
            
            #IMPORTANT.  ADJUSTS BETWEEN DATASETS
#             page = page.data.cpu().numpy()
#             page = [asr_to_clean[i] for i in page]
#             page = Variable(torch.from_numpy(np.asarray(page))).cuda()
            
            
            qnums = batch.qnum.cuda()
            #if hasattr(batch, 'confidence'):
            confidences = batch.confidence
            #else:
            #    confidences = torch.FloatTensor(batch.text[0].shape).fill_(1)
                #print (""Missing confidences"")
                #raise
            
            if is_train:
                model.zero_grad()

            out = model(input_dict, lengths_dict, qnums, confidences,)
            _, preds = torch.max(out, 1)
            accuracy = torch.mean(torch.eq(preds, page).float()).data[0]
            batch_loss = loss_function(out, page)
            if is_train:
                batch_loss.backward()
                torch.nn.utils.clip_grad_norm(model.parameters(), .25)
                optimizer.step()

            batch_accuracies.append(accuracy)
            batch_losses.append(batch_loss.data[0])

        epoch_end = time.time()

        return np.mean(batch_accuracies), np.mean(batch_losses), epoch_end - epoch_start",0.5252164602,
2391,train it,"import bleu
def eval(gen_config):
    vocab, rev_vocab, dev_set, train_set = data_utils.prepare_data(gen_config)
    for b_set in dev_set:
        print(""b_set: "", len(b_set))

    with tf.Session() as sess:
        model = Seq2SeqModel(gen_config, name_scope=""Basic_Seq2seq"", forward_only=True,
                                            dtype=tf.float32)
        gen_ckpt_dir = os.path.abspath(os.path.join(gen_config.save_dir, ""checkpoints""))
        ckpt = tf.train.get_checkpoint_state(gen_ckpt_dir)
        if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):
            #print(""Reading Gen model parameters from %s"" % ckpt.model_checkpoint_path)
            model.saver.restore(sess, ckpt.model_checkpoint_path)
        else: 
            raise ValueError(""Please run the training first"")
        # Run evals on development set and print their perplexity.
        for bucket_id in xrange(len(gen_config.buckets)):
            encoder_inputs, decoder_inputs, target_weights, inputs_len, target_len = \
                    data_utils.get_batch(gen_config, dev_set, bucket_id)
            _, eval_loss, sample_ids = model.step(sess, encoder_inputs, decoder_inputs, 
                                        target_weights, inputs_len, target_len, bucket_id, True)
            eval_ppx = math.exp(eval_loss) if eval_loss < 300 else float('inf')
            print(""eval: bucket %d loss %.4f perplexity %.2e"" % (bucket_id, eval_loss, eval_ppx))
            # process the output for better demonstration.
            queries = data_utils.clean(encoder_inputs, data_utils.PAD_ID)
            answers = data_utils.clean(decoder_inputs[1:], data_utils.EOS_ID)
            gens = data_utils.clean(sample_ids, data_utils.EOS_ID)
            references = [[gen] for gen in gens]
            # compute four BLEU score
            for i in range(4):
                bleu_score, _, _, _, _, _ = bleu.compute_bleu(references, answers, max_order = i+1)
                print(""BLEU %d sorces: %.4f""%(i+1, 100 * bleu_score))
            for i in range(3):
                print(""Q:"", "" "".join([tf.compat.as_str(rev_vocab[j]) for j in queries[i]]))
                print(""A:"", "" "".join([tf.compat.as_str(rev_vocab[j]) for j in answers[i]]))
                print(""G:"", "" "".join([tf.compat.as_str(rev_vocab[j]) for j in gens[i]]))
                bleu_score, _, _, _, _, _ = bleu.compute_bleu([[gens[i]]], [answers[i]], max_order = 1)
                print(""BLEU sorces: %.4f""%(100 * bleu_score))
                print()",0.5250617266,
2391,train it,"import time

def train_model(model_name):
    
    saver = tf.train.Saver()
    data_train_path = '/media/mat/ssdBackupMat/Datasets/sentiment/yelp_review_full_csv/data.train'
    start_time = time.time()
    PRINT_EVERY = 200

    sess = tf.Session()

    sess.run(init)
    coord = tf.train.Coordinator()
    tf.train.start_queue_runners(sess=sess, coord=coord)
    my_thread = queue_ctrl.start_thread(sess, coord, data_train_path) #start the publisher thread
    iteration, progress = 0, 0.
    tokens_processed = 0.
    
    print(""\nTraining model: %s"" % model_name)

    while progress < 100:
        _, loss_, n_tokens_ = sess.run([train_step, loss, n_tokens])
        tokens_processed += n_tokens_
        progress = tokens_processed / tokens_to_process *100.
        if iteration % PRINT_EVERY == 0:
            print(""Iter: %d, %.2f%% done, Minibatch loss: %.4f, Elements in queue: %d"" 
                  % (iteration, progress, loss_, sess.run(queue_ctrl.queue.size())))
        iteration += 1

    coord.request_stop()

    print(""Done. Exec time: %.2f minutes."" % ((time.time() - start_time) / 60.))
    save_path = saver.save(sess, model_name)

    coord.join(my_thread, stop_grace_period_secs=10)
    sess.close()",0.5245773196,
2391,train it,"def run_epoch(iterator: Iterator):
        is_train = iterator.train
        batch_accuracies = []
        batch_losses = []
        epoch_start = time.time()
        for batch in iterator:
            input_dict = {}
            lengths_dict = {}
            if hasattr(batch, 'text'):
                text, lengths = batch.text
                
                #IMPORTANT.  ADJUSTS BETWEEN DATASETS
                text = text.data.cpu().numpy().tolist()
                for i, group in enumerate(text):
                    #print(group)
                    for j, item in enumerate(group):
                        text[i][j]=asr_to_clean_vocab[item]
                text = Variable(torch.from_numpy(np.asarray(text))).cuda()
  
                input_dict['text'] = text
                lengths_dict['text'] = lengths
            page = batch.page
            qnums = batch.qnum.cuda()

            if is_train:
                model.zero_grad()

            out = model(input_dict, lengths_dict, qnums)
            _, preds = torch.max(out, 1)
            
           #IMPORTANT.  ADJUSTS BETWEEN DATASETS
            page = page.data.cpu().numpy()
            page = [asr_to_clean[i] for i in page]
            page = Variable(torch.from_numpy(np.asarray(page))).cuda()
            
            
            accuracy = torch.mean(torch.eq(preds, page).float()).data[0]
            batch_loss = loss_function(out, page)
            if is_train:
                batch_loss.backward()
                torch.nn.utils.clip_grad_norm(model.parameters(), .25)
                optimizer.step()

            batch_accuracies.append(accuracy)
            batch_losses.append(batch_loss.data[0])

        epoch_end = time.time()

        return np.mean(batch_accuracies), np.mean(batch_losses), epoch_end - epoch_start",0.5245577097,
258,compute covariance matrix,"def forward(self, X):
    #forward propagation through our network
    self.z = np.dot(X, self.W1) # dot product of X (input) and first set of 3x2 weights
    self.z2 = self.sigmoid(self.z) # activation function
    self.z3 = np.dot(self.z2, self.W2) # dot product of hidden layer (z2) and second set of 3x1 weights
    o = self.sigmoid(self.z3) # final activation function
    return o",0.4931244254,
258,compute covariance matrix,"# Complete code below this comment for FA
# ----------------------------------
def FA_explained_variance_ratio(fa):
    fa.explained_variance_ = np.flip(np.sort(np.sum(fa.components_**2, axis=1)), axis=0)
    total_variance = np.sum(fa.explained_variance_) + np.sum(fa.noise_variance_)
    fa.explained_variance_ratio_ = fa.explained_variance_ / total_variance

FA_explained_variance_ratio(fa)
print('FA', fa.explained_variance_ratio_)",0.4818788469,
258,compute covariance matrix,"# calculate covariance matrix using NumPy.cov()
def calculate_covariance_matrix(X_standardized):
    covariance_matrix = np.cov(X_standardized.T)
    return covariance_matrix",0.4812172949,
258,compute covariance matrix,"def advance_state(self, dt):
    # TODO:
    # Calculate state vector after dt time 
    
    X_dot =np.array([self.X[2], self.X[3], self.z_dot_dot, self.psi_dot_dot])

    # Change in state will be 
    self.X = self.X + X_dot * dt
    
    return self.X",0.4781802297,
258,compute covariance matrix,"%%add_to CoaxialCopter

def advance_state(self, dt):
    # TODO:
    # Calculate state vector after dt time 
    
    X_dot =np.array([self.X[2], self.X[3], self.z_dot_dot, self.psi_dot_dot])

    # Change in state will be 
    self.X = self.X + X_dot * dt
    
    return self.X",0.4781802297,
258,compute covariance matrix,"def PCA_manual(df):    
    c = df.cov()     
    val, vec = np.linalg.eig(c)
    decor = np.argsort(val)[::-1]
    val, vec = val[decor], vec[:, decor]
    tr = sum(val)
    var_ratio_custom = np.cumsum(np.round(val/tr, decimals=4))
    return val, vec, var_ratio_custom",0.4649482667,
258,compute covariance matrix,"def symmetrize(a):
    return a + a.T - np.diag(a.diagonal())",0.4640823305,
258,compute covariance matrix,"def multivariate_gaussian(X, mu, sigma2):
    if len(sigma2) == 1:
        sigma2 = np.diag(sigma2)
    return multivariate_normal(mean=mu, cov=sigma2).pdf(X)",0.4632593989,
258,compute covariance matrix,"def Vee(self,R):
    #compute coulumb potential
    #remembering to add the epsilon
    # for attractive potentials.
    # self.C accesses the C",0.4629070163,
258,compute covariance matrix,"def heat_balance_main_rule(model):
    return model.F*model.H_F + sum(model.L[s]*model.H_L_[s] + model.V[s]*model.H_V_[s] for s in model.inlet) \
            + model.Q_main - sum(model.L[s]*model.H_L + model.V[s]*model.H_V for s in model.outlet) == 0
model.heat_balance_main_con = pe.Constraint(rule=heat_balance_main_rule)",0.4621124268,
2158,step select the rows to and the columns to,"def drawSpiral(myTurtle, lineLen):
    if lineLen > 0:
        myTurtle.forward(lineLen)
        myTurtle.right(90)
        drawSpiral(myTurtle,lineLen-5)
        return None

def spir_draw():
    myTurtle = turtle.Turtle()
    myWin = turtle.Screen()
    myPoints = [[-100,-50],[0,100],[100,-50]]
    drawSpiral(myTurtle,200)
    myWin.exitonclick()
    return None
spir_draw()",0.4930146337,
2158,step select the rows to and the columns to,"# get center position
def get_center_position(line,img):
    xm_per_pix = 3.7/700 # meteres per pixel in x dimension
    line.center_position = (((line.left_fitx[-1]+line.right_fitx[-1]) / 2.) 
                            - img.shape[1] / 2) * xm_per_pix",0.4863263667,
2158,step select the rows to and the columns to,"def nudge_subplot(sp, dy):
    """"""A helper function to move subplots.""""""
    sp_ax = sp.get_position()
    sp.set_position([sp_ax.x0, sp_ax.y0+dy,
        sp_ax.x1-sp_ax.x0, sp_ax.y1-sp_ax.y0])",0.4773433506,
2158,step select the rows to and the columns to,"def set_legend(ax, labels):
    # Shrink current axis's height by 10% on the bottom
    box = ax.get_position()
    ax.set_position([box.x0, box.y0 + box.height * 0.1,
                 box.width, box.height * 0.9])

    # Put a legend below current axis
    ax.legend(labels, loc='upper center', bbox_to_anchor=(0.5, -0.05),
          fancybox=True, shadow=True, ncol=len(labels))",0.475681901,
2158,step select the rows to and the columns to,"def change_wall_prop(ax, coordinates, depths, angles):
    ax.w_xaxis.set_pane_color((1.0, 1.0, 1.0, 1.0))
    ax.w_yaxis.set_pane_color((1.0, 1.0, 1.0, 1.0))
    ax.w_zaxis.set_pane_color((1.0, 1.0, 1.0, 1.0))
    ax.w_xaxis.gridlines.set_linestyles(':')
    ax.w_yaxis.gridlines.set_linestyles(':')
    ax.w_zaxis.gridlines.set_linestyles(':')
    ax.view_init(angles[0], angles[1])
    ax.set_xlim(coordinates[0], coordinates[1])
    ax.set_ylim(coordinates[2], coordinates[3])
    ax.set_zlim(depths[0], depths[1])
    ax.set_zlabel('\nDepth (m)')

    ax.set_zticks(np.arange(depths[0], depths[1] + 10, depths[2]))
    ax.set_zticklabels(range(int(-depths[0]), -int(depths[1]) - 10, -int(depths[2])))",0.4717829525,
2158,step select the rows to and the columns to,"#class LearningAgent(Agent):
    
def update(self, t):
    #some other code
        
    #Learn and update Q-value based on state, action and reward
    new_state = (self.env.sense(self)[""light""],\
                 self.planner.next_waypoint())
        
    if new_state not in self.q_table:
        q_hat = reward + self.gamma * 0
    else:
        q_hat = reward + \
                self.gamma * max(self.q_table[new_state].values())
            
    self.q_table[self.state][action] = \
        self.alpha*q_hat + (1-self.alpha)*self.q_table[self.state][action]",0.4680711925,
2158,step select the rows to and the columns to,"def drawSquare(myTurtle,maxSide):
		for sideLength in range(1,maxSide+1):
			myTurtle.forward(100)
			myTurtle.right(170)",0.4657858014,
2158,step select the rows to and the columns to,"def apply_boundary(T,nx,ny):
    T[:,0] = 100 # Left boundary
    T[:,-1] = 100*np.sin(np.arange(0,nx)/nx) # Right boundary
    T[0,:] = 0 # Top boundary
    T[-1,:] = 0 # Bottom boundary
    T[10,10]=100
plot_and_solve()",0.4655251503,
2158,step select the rows to and the columns to,"L,W=100,12

def Field(site1,site2,phi):
    x1,y1=site1.pos
    x2,y2=site2.pos
    return -np.exp(-0.5j * phi * (x1 - x2) * (y1 + y2))

H[lat.neighbors()] = Field",0.4651713967,
2158,step select the rows to and the columns to,"def initialState(count,count1,goal):
    global position 
    if count == 0 and count1 ==0:
        position = np.random.randint(0,9)
    if count == 1 and count1 ==0:
        position = goal
    #print(position)
    return np.array([10*np.random.random_sample(), 3*(0.5-np.random.random_sample()),position ])

def nextState(s,a):
    s = copy.copy(s)   # s[0] is position, s[1] is velocity. a is -1, 0 or 1
    deltaT = 0.1                           # Euler integration time step
    s[0] += deltaT * s[1]                  # Update position
    s[1] += deltaT * (2 * a - 0.2 * s[1])  # Update velocity. Includes friction
    if s[0] < 0:        # Bound next position. If at limits, set velocity to 0.
        s = np.array([0,0,s[2]])
    elif s[0] > 10:
        s = np.array([10,0,s[2]])
    return s

def reinforcement(s):  # s is new state
    goal = s[2]
    return 0 if abs(s[0]-goal) < 1 else -0.1

def policy(qnet, state, epsilon):
    if np.random.rand(1) < epsilon:
        actioni = np.random.randint(validActions.shape[0])
    else:
        inputs = np.hstack(( np.tile(state, (validActions.shape[0], 1)), validActions.reshape((-1,1))))
        qs = qnet.use(inputs)
        actioni = np.argmax(qs)
    return validActions[actioni]",0.4650137722,
1757,read pdb structures,"# Type of the data to read (""stride"", ""dssp"")
    data_type = ""stride""
    # File name
    file_name = ""../data/"" + data_type + ""_info.txt""
    # Read the data
    input_data = preprocess_input(file_name, aminoacid_codes)

    # Append the PDB chain code to the PDB code, to obtain unique protein identifier.
    input_data.PDB_code = input_data.PDB_code + ""_"" + input_data.PDB_chain_code
    
    prediction_list = None
    # Make the predictions.
    
    
    # # UNCOMMENT IF NECESSARY
    #prediction_list = predict_sec_structures(input_data, data_type, 2, l_1_out=True)
    
    if prediction_list is not None:
    
        # Overall Q3
        q3_tot = np.mean([x.q3 for x in prediction_list])
        print(""\nOVERALL Q3:"", q3_tot)

        # Load CATH file
        cath = pd.read_csv(""../data/cath_info.txt"", header=None, sep=""\t"",
                             names=[""PDB_code"", ""PDB_chain"", ""overall_structure""])

        # Add a composite key to the cath data
        cath[""PDB_code_and_chain""] = cath.PDB_code + ""_"" + cath.PDB_chain

        # Save predictions in a file.
    
        result_frame = pd.DataFrame(index=range(len(prediction_list)), columns=[""PDB_code_and_chain"",
                                                                                ""length"",
                                                                                ""prediction"",
                                                                                ""real_sec_structure"",
                                                                                ""num_predicted_h"",
                                                                                ""num_predicted_b"",
                                                                                ""num_predicted_c"",
                                                                                ""num_real_h"",
                                                                                ""num_real_b"",
                                                                                ""num_real_c"",
                                                                                ""num_am_a"",
                                                                                ""num_am_r"",
                                                                                ""num_am_n"",
                                                                                ""num_am_d"",
                                                                                ""num_am_c"",
                                                                                ""num_am_q"",
                                                                                ""num_am_e"",
                                                                                ""num_am_g"",
                                                                                ""num_am_h"",
                                                                                ""num_am_i"",
                                                                                ""num_am_l"",
                                                                                ""num_am_k"",
                                                                                ""num_am_m"",
                                                                                ""num_am_f"",
                                                                                ""num_am_p"",
                                                                                ""num_am_s"",
                                                                                ""num_am_t"",
                                                                                ""num_am_w"",
                                                                                ""num_am_y"",
                                                                                ""num_am_v"",
                                                                                ""protein"",
                                                                                ""q3"",
                                                                                ""q3_h"",
                                                                                ""q3_b"",
                                                                                ""q3_c"",
                                                                                ""mcc_h"",
                                                                                ""mcc_b"",
                                                                                ""mcc_c"",
                                                                                ""mcc"",
                                                                                ""overall_pred"",
                                                                                ""overall_structure_real""])
        for p_i, p in enumerate(prediction_list):
            result_frame.iloc[p_i, 0] = p.pdb_code
            result_frame.iloc[p_i, 1] = len(p.prediction)
            result_frame.iloc[p_i, 2] = p.prediction
            result_frame.iloc[p_i, 3] = p.real_sec_structure
            result_frame.iloc[p_i, 4] = p.prediction.count(""h"")
            result_frame.iloc[p_i, 5] = p.prediction.count(""b"")
            result_frame.iloc[p_i, 6] = p.prediction.count(""c"")
            result_frame.iloc[p_i, 7] = p.real_sec_structure.count(""h"")
            result_frame.iloc[p_i, 8] = p.real_sec_structure.count(""b"")
            result_frame.iloc[p_i, 9] = p.real_sec_structure.count(""c"")
            result_frame.iloc[p_i, 10] = p.protein.count(""a"")
            result_frame.iloc[p_i, 11] = p.protein.count(""r"")
            result_frame.iloc[p_i, 12] = p.protein.count(""n"")
            result_frame.iloc[p_i, 13] = p.protein.count(""d"")
            result_frame.iloc[p_i, 14] = p.protein.count(""c"")
            result_frame.iloc[p_i, 15] = p.protein.count(""q"")
            result_frame.iloc[p_i, 16] = p.protein.count(""e"")
            result_frame.iloc[p_i, 17] = p.protein.count(""g"")
            result_frame.iloc[p_i, 18] = p.protein.count(""h"")
            result_frame.iloc[p_i, 19] = p.protein.count(""i"")
            result_frame.iloc[p_i, 20] = p.protein.count(""l"")
            result_frame.iloc[p_i, 21] = p.protein.count(""k"")
            result_frame.iloc[p_i, 22] = p.protein.count(""m"")
            result_frame.iloc[p_i, 23] = p.protein.count(""f"")
            result_frame.iloc[p_i, 24] = p.protein.count(""p"")
            result_frame.iloc[p_i, 25] = p.protein.count(""s"")
            result_frame.iloc[p_i, 26] = p.protein.count(""t"")
            result_frame.iloc[p_i, 27] = p.protein.count(""w"")
            result_frame.iloc[p_i, 28] = p.protein.count(""y"")
            result_frame.iloc[p_i, 29] = p.protein.count(""v"")
            result_frame.iloc[p_i, 30] = p.protein
            result_frame.iloc[p_i, 31] = p.q3
            result_frame.iloc[p_i, 32] = p.q3_h
            result_frame.iloc[p_i, 33] = p.q3_b
            result_frame.iloc[p_i, 34] = p.q3_c
            result_frame.iloc[p_i, 35] = p.mcc_h
            result_frame.iloc[p_i, 36] = p.mcc_b
            result_frame.iloc[p_i, 37] = p.mcc_c
            result_frame.iloc[p_i, 38] = p.mcc
            result_frame.iloc[p_i, 39] = p.overall_prediction
            result_frame.iloc[p_i, 40] = cath.loc[cath['PDB_code_and_chain'] == p.pdb_code].overall_structure.iloc[0]

    # SAVE THE RESULTS TO CSV
    # UNCOMMENT IF NECESSARY
    # result_frame.to_csv(""../data/pred_result_"" + data_type + "".csv"", index=False)",0.4799692035,
1757,read pdb structures,"# Open the proteins used for testing.  
    test_proteins_input = pd.read_csv(""../data/test_proteins.csv"", header=None, sep="","",
                         names=[""PDB_code_and_chain"",""protein"",""sec_structure"",""overall_structure""])
    
    # Replace ""E"" with ""B""
    test_proteins_input.sec_structure = test_proteins_input.sec_structure.str.replace(""E"", ""B"")
    
    # Build a frame with the appropriate structure
    test_proteins = pd.DataFrame(columns=[""PDB_code"",""a"",""s""])
    
    for p_i in range(len(test_proteins_input.index)):
        p = test_proteins_input.iloc[p_i,:]
        temp_frame = pd.DataFrame(index=np.arange(len(p.protein)), columns=[""PDB_code"",""a"",""s""])
        temp_frame[""PDB_code""] = p.PDB_code_and_chain
        temp_frame[""a""] = list(p.protein)
        temp_frame[""s""] = list(p.sec_structure)
        test_proteins = test_proteins.append(temp_frame, ignore_index=True)
     
    # Put to lowercase
    test_proteins.a = test_proteins.a.str.lower()
    test_proteins.s = test_proteins.s.str.lower()",0.4636129141,
1757,read pdb structures,"def tweets(filename):
    df = pd.read_table(""%s"" %filename, header = None) #reads text file
    df.columns = [""tweets""] #labels the columns as tweets
    return df #returns the data as a pandas table",0.4509378076,
1757,read pdb structures,"import pdb
def check_distribution(labels):
  infos = []
  for index, label in enumerate('a b c d e f g h i j'.split()):
    infos.append({
        'label': label,
        'total': labels[labels == index].shape[0]
    })
  
  for info in infos:
    percentage = round(float(info['total']) / float(labels.shape[0]) * 100, 2)
    print(""class: {}, total: {}({}% of all data)"".format(info['label'], info['total'], percentage))
    
print(""Distribution of train dataset"")
check_distribution(train_labels)
print(""------------------------------"")
print(""Distribution of test dataset"")
check_distribution(test_labels)
print(""------------------------------"")
print(""Distribution of validation dataset"")
check_distribution(valid_labels)",0.4477741122,
1757,read pdb structures,"def parse_protein_table(fname):
    df = pd.read_table('proteins/{}'.format(fname))
    df['file_name'] = fname
    return df",0.4451113939,
1757,read pdb structures,"import requests
from ssbio.protein.structure.properties import opm
def run_and_download_opm(orig_file, outdir_html, outdir_opm):
    """"""Manually run OPM, save the HTML and PDB result""""""
#     try:
#     print('{}: running OPM...'.format(orig_file))
    infodict = opm.run_ppm_server(orig_file, outfile=op.join(outdir_html, op.basename(orig_file) + '.opm'))
#     except:
#         print('{}: OPM ERROR'.format(orig_file))
#         return None, None
    newname = op.join(outdir_opm, op.basename(orig_file) + '.opm.pdb')
    
    if not op.exists(newname):
        r = requests.get(infodict['Output file download link'])
        with open(newname, 'wb') as f:
            f.write(r.content)
    
    return newname, infodict",0.4436401129,
1757,read pdb structures,"# Type of the data to read (""stride"", ""dssp"")
    data_type = ""stride""
    # File name
    file_name = ""../data/"" + data_type + ""_info.txt""
    # Read the data
    input_data = preprocess_input(file_name, aminoacid_codes)

    # Look at the aminoacid values
    aminoacids = set(input_data.residue_name)
    print(aminoacids)
    # Same stuff with the secondary structures
    secondary_structures = set(input_data.secondary_structure)
    print(secondary_structures)",0.4431717694,
1757,read pdb structures,"def parse_pubmed_file(file, pubmed_output_file, pmid2mesh_output_file):

    print('Start parsing %s' % file)
    sys.stdout.flush()

    t1 = time.time()

    f = open(file, 'r')

    tree = etree.parse(f)
    articles = itertools.chain(tree.findall('PubmedArticle'), tree.findall('BookDocument'))
    count = 0

    noabs = 0
    
    for article in articles:

        count += 1
        result = {}
        pmid2mesh = {}

        # PMID - Exactly One Occurrance
        result['PMID'] = get_text(article, './/PMID')
        pmid2mesh['PMID'] = get_text(article, './/PMID')

        # # Article title - Zero or One Occurrences
        # result['ArticleTitle'] = get_text(article, './/ArticleTitle')

        # Abstract - Zero or One Occurrences
        abstractList = article.find('.//Abstract')
        if abstractList != None:
            try:
                abstract = '\n'.join([line.text for line in abstractList.\
                                      findall('AbstractText')])
                result['Abstract'] = abstract
            except:
                result['Abstract'] = ''
                noabs += 1
        else:
            result['Abstract'] = ''
            noabs += 1
            
            

            
            
        # # Author List - Zero or More Occurrences
        # authors = article.findall('.//Author')
        # result['AuthorList'] = parse_author(authors)
        
        # # Journal - Exactly One Occurrance
        # journal = article.find('.//Journal')
        # result['Journal'] = get_text(journal, 'Title')
        
        
        result['PubDate'] = {}
        result['PubDate']['Year'] = get_text(journal, 'JournalIssue/PubDate/Year')
        
        # result['PubDate']['Month'] = get_text(journal, 'JournalIssue/PubDate/Month')
        # result['PubDate']['Day'] = get_text(journal, 'JournalIssue/PubDate/Day')
        # result['PubDate']['Season'] = get_text(journal, 'JournalIssue/PubDate/Season')
        # result['PubDate']['MedlineDate'] = get_text(journal,\
        #                                   'JournalIssue/PubDate/MedlineDate')
        
        
        
        
        

        # MeshHeading - Zero or More Occurrences
        headings = article.findall('.//MeshHeading')
        
        result['MeshHeadingList'] = []
        pmid2mesh['MeshHeadingList'] = []
        if headings:
            for heading in headings:
                descriptor_names = heading.findall('DescriptorName')
                qualifier_names = heading.findall('QualifierName')
                if descriptor_names:
                    for descriptor_name in descriptor_names:
                        result['MeshHeadingList'].append(descriptor_name.text)
                        pmid2mesh['MeshHeadingList'].append(descriptor_name.text)
                if qualifier_names:
                    for qualifier_name in qualifier_names:
                        result['MeshHeadingList'].append(qualifier_name.text)
                        pmid2mesh['MeshHeadingList'].append(qualifier_name.text)
                        
                        
        
        
        mesh_count = len(result['MeshHeadingList'])
        
        if mesh_count in mesh_statistics:
            mesh_statistics[mesh_count] += 1
        else:
            mesh_statistics[mesh_count] = 1

        # Dump to pubmed json file <----------------------------
        json.dump(result, pubmed_output_file)
        pubmed_output_file.write('\n')
        
        # Dump to pmid2mesh json file <-------------------------
        json.dump(pmid2mesh, pmid2mesh_output_file)
        pmid2mesh_output_file.write('\n')


    print('Finish parsing %s, totally %d articles parsed. Total time: %fs'\
                             % (file, count, time.time() - t1))
    print('%d acticles no abstracts' % (noabs))
    sys.stdout.flush()
    f.close()",0.4407936335,
1757,read pdb structures,"def plot_log(log_file, title=None):
    #!$parse_script_path $logpath $logfolder
    train_log_file = log_file + '.train'
    test_log_file = log_file + '.test'
    df_train = pd.read_table(train_log_file, sep = ',')
    df_test = pd.read_table(test_log_file, sep = ',')
    
    print 'Total Time:', df_train.Seconds.iloc[-1], 'seconds'
    print 'CV loss:', df_test.loss.iloc[-1], 'CV accuracy:', df_test.accuracy.iloc[-1]
    
    plt.figure()
    plt.plot(df_train['NumIters'],df_train['loss'] , label = 'Train Loss')
    plt.plot(df_test['NumIters'],df_test['loss'] , label = 'CV Loss')
    plt.plot(df_test['NumIters'],df_test['accuracy'] , label = 'CV Accuracy')
    plt.grid()
    plt.legend()
    plt.title(log_file)
    plt.xlabel('num_iters')
    plt.ylabel('loss')
    plt.show()",0.4390715957,
1757,read pdb structures,"def plot_log_vggnet(log_file, title=None): 
    train_log_file = log_file + '.train'
    test_log_file = log_file + '.test'
    df_train = pd.read_table(train_log_file, sep = ',')
    df_test = pd.read_table(test_log_file, sep = ',')

    #print df_train.head()
    #print df_test.head()
    print 'Total Time:', df_train.Seconds.iloc[-1], 'seconds'
    print 'CV loss:', df_test['loss/loss'].iloc[-1], 'CV accuracy:', df_test['accuracy@1'].iloc[-1]

    plt.figure()
    plt.plot(df_train['NumIters'],df_train['loss/loss'] , label = 'Train Loss')
    plt.plot(df_test['NumIters'],df_test['loss/loss'] , label = 'CV Loss')
    plt.plot(df_test['NumIters'],df_test['accuracy@1'] , label = 'CV Accuracy - top1')
    plt.plot(df_test['NumIters'],df_test['accuracy@5'] , label = 'CV Accuracy - top5')
    plt.grid()
    plt.legend(loc=3, bbox_to_anchor=(1, 0.5))
    plt.title(log_file)
    plt.xlabel('num_iters')
    plt.ylabel('loss')
    plt.show()",0.4390715957,
1049,lesson develop a predictive theory,visualizers_1.visualize_jurisdiction_countly(total_jurisdiction_count.copy()),0.4576996863,
1049,lesson develop a predictive theory,visualizers_1.visualize_jurisdiction_countly(jurisdiction_count.copy()),0.4576996863,
1049,lesson develop a predictive theory,"exp.explain_instance(X_test[1], logistic.predict_proba).show_in_notebook()",0.436748445,
1049,lesson develop a predictive theory,"exp.explain_instance(X_test[0], logistic.predict_proba).show_in_notebook()",0.4365289509,
1049,lesson develop a predictive theory,"eng5.backend.get_probability('1', qubit0)",0.4365202785,
1049,lesson develop a predictive theory,"mothersumi.evaluate_prediction(start_date = '2014-06-22', end_date = '2015-03-22', nshares=300)",0.43565166,
1049,lesson develop a predictive theory,building.utility.electric = building.utility.electric.sum_split_supplies(),0.435606271,
1049,lesson develop a predictive theory,"# train a bigram detector (ignore all words and bigrams with total collected count lower than 20)
bigram = models.Phrases(sentences_no_stops, min_count = 20)",0.4355862737,
1049,lesson develop a predictive theory,ticker.evaluate_prediction(nshares=1000),0.4354530275,
1049,lesson develop a predictive theory,boeing.evaluate_prediction(nshares=1000),0.4354530275,
47,alternate pca or other techniques,"def get_Kmeans_error(Xtrain, ytrain, Xtest=Xtest, ytest=ytest, K=9, gamma=1.5):
    Ntrain = len(Xtrain)
    phi_train = np.zeros((Ntrain, K))
    
    Ntest = len(Xtest)
    phi_test = np.zeros((Ntest, K))
    
    uks = kmeans(Xtrain, K)

    for j in range(K):
        for i in range(Ntrain):
            phi_train[i, j] = np.exp(-gamma * np.linalg.norm(Xtrain[i] - uks[j]) ** 2)
            
    for j in range(K):
        for i in range(Ntest):
            phi_test[i, j] = np.exp(-gamma * np.linalg.norm(Xtest[i] - uks[j]) ** 2)

 
    w = np.dot(np.dot(np.linalg.inv(np.dot(phi_train.T, phi_train)), phi_train.T), ytrain)

    E_in  = np.mean(np.where(np.dot(phi_train, w) > 0, 1, -1) != ytrain)
    E_out = np.mean(np.where(np.dot(phi_test, w) > 0, 1, -1) != ytest)
    return E_in, E_out",0.4109836817,
47,alternate pca or other techniques,"def plot_heatmap_count(data_df,feature1, feature2, color, title):
    matrix = data_df.pivot(feature1, feature2, 'Teams')
    fig, (ax1) = plt.subplots(ncols=1, figsize=(16,6))
    sns.heatmap(matrix, 
        xticklabels=matrix.columns,
        yticklabels=matrix.index,ax=ax1,linewidths=.1,linecolor='darkblue',annot=True,cmap=color)
    plt.title(title, fontsize=14)
    plt.show()",0.4109566212,
47,alternate pca or other techniques,"# TO-DO: incorporate the mean score func internall below. Add flag for ""sum_score"" function as well (incorporate too)

def render_lap2_mean_grad(t_obj, img0=img_noise, t_score2=0, op='add', wght1=0.5, wght2=0.5, visfunc=visstd,
                   iter_n=10, step=1.0, octave_n=3, octave_scale=1.4, lap_n=4):
    t_score = tf.reduce_mean(t_obj) # defining the optimization objective
    
    if op=='add':
        t_grad = tf.gradients(wght1*t_score+wght2*t_score2, t_input)[0] # behold the power of automatic differentiation!
    if op=='subtract':
        t_grad = tf.gradients(wght1*t_score-wght2*t_score2, t_input)[0] # behold the power of automatic differentiation!
    # build the laplacian normalization graph
    lap_norm_func = tffunc(np.float32)(partial(lap_normalize, scale_n=lap_n))

    img = img0.copy()
    for octave in range(octave_n):
        if octave>0:
            hw = np.float32(img.shape[:2])*octave_scale
            img = resize(img, np.int32(hw))
        for i in range(iter_n):
            g = calc_grad_tiled(img, t_grad)
            g = lap_norm_func(g)
            img += g*step
            print('.', end = ' ')
        clear_output()
        showarray(visfunc(img))",0.407407999,
47,alternate pca or other techniques,"def plotVectorSectionsOctree(mesh, m, normal='X', ind=0, vmin=None, vmax=None,
                      subFact=2, scale=1., xlim=None, ylim=None, vec='k',
                      title=None, axs=None, actvMap=None, contours=None, fill=True,
                      orientation='vertical', cmap='pink_r'):

    """"""
    Plot section through a 3D tensor model
    """"""
    # plot recovered model
    normalInd = {'X': 0, 'Y': 1, 'Z': 2}[normal]
    antiNormalInd = {'X': [1, 2], 'Y': [0, 2], 'Z': [0, 1]}[normal]

    h2d = (mesh.h[antiNormalInd[0]], mesh.h[antiNormalInd[1]])
    x2d = (mesh.x0[antiNormalInd[0]], mesh.x0[antiNormalInd[1]])

    #: Size of the sliced dimension
    szSliceDim = len(mesh.h[normalInd])
    if ind is None:
        ind = int(szSliceDim//2)

    cc_tensor = [None, None, None]
    for i in range(3):
        cc_tensor[i] = np.cumsum(np.r_[mesh.x0[i], mesh.h[i]])
        cc_tensor[i] = (cc_tensor[i][1:] + cc_tensor[i][:-1])*0.5
    slice_loc = cc_tensor[normalInd][ind]

#     if type(ind) not in integer_types:
#         raise ValueError('ind must be an integer')

    #create a temporary TreeMesh with the slice through
    temp_mesh = Mesh.TreeMesh(h2d, x2d)
    level_diff = mesh.max_level - temp_mesh.max_level

    XS = [None, None, None]
    XS[antiNormalInd[0]], XS[antiNormalInd[1]] = np.meshgrid(cc_tensor[antiNormalInd[0]],
                                                             cc_tensor[antiNormalInd[1]])
    XS[normalInd] = np.ones_like(XS[antiNormalInd[0]])*slice_loc
    loc_grid = np.c_[XS[0].reshape(-1), XS[1].reshape(-1), XS[2].reshape(-1)]
    inds = np.unique(mesh._get_containing_cell_indexes(loc_grid))

    grid2d = mesh.gridCC[inds][:, antiNormalInd]
    levels = mesh._cell_levels_by_indexes(inds) - level_diff
    temp_mesh.insert_cells(grid2d, levels)
    tm_gridboost = np.empty((temp_mesh.nC, 3))
    tm_gridboost[:, antiNormalInd] = temp_mesh.gridCC
    tm_gridboost[:, normalInd] = slice_loc

    #interpolate values to mesh.gridCC if not 'CC'
#     if vType in ['F', 'E']:
#     aveOp = 'aveF2CC'
#     Av = getattr(mesh, aveOp)
#     print(Av.shape,actvMap.P.shape, m.shape)
    mx = (actvMap*m[:,0])
    my = (actvMap*m[:,1])
    mz = (actvMap*m[:,2])
    
    m = np.c_[mx, my, mz]
#     m = Av*m

    #interpolate values from mesh.gridCC to grid2d
    ind_3d_to_2d = mesh._get_containing_cell_indexes(tm_gridboost)
    v2d = m[ind_3d_to_2d, :]
    amp = np.sum(v2d**2., axis=1)**0.5
    
    if axs is None:
        fig = plt.figure()
        axs = plt.subplot(111)
    else:
        assert isinstance(ax, matplotlib.axes.Axes), ""ax must be an matplotlib.axes.Axes""
        fig = ax.figure

#     out = temp_mesh.plotImage(v2d, ax=ax, grid=grid, showIt=showIt, clim=clim)

    if fill:
#         im2 = axs.contourf(xx, yy, model,
#                            30, vmin=vmin, vmax=vmax,
#                            cmap=cmap)

        im2 = temp_mesh.plotImage(amp, ax=axs, clim=[vmin, vmax], grid=True)
 
    axs.quiver(temp_mesh.gridCC[:,0],
               temp_mesh.gridCC[:,1],
               v2d[:,antiNormalInd[0]],
               v2d[:,antiNormalInd[1]],
               pivot='mid',
               scale_units=""inches"", scale=scale, linewidths=(1,),
               edgecolors=(vec),
               headaxislength=0.1, headwidth=10, headlength=30)",0.4047019184,
47,alternate pca or other techniques,"def test(d_bios,
       signal='cos',
       es=0,
       cs=1,
       ss=2,
       n=100,
       tau=0.1,
       freq=5,
       t=1,
       n_syns=[1],
       neuron_type=nengo.AdaptiveLIF(tau_n=0.1, inc_n=0.01),
       h_out=nengo.Lowpass(0.1),
       secs=['apical'],  # list of sections
       verbose=False):
    
    # first find the norm of the filtered signal
    with nengo.Network() as model:
        if signal == 'cos':
            stim = nengo.Node(output=lambda t: np.cos(freq*t))
        elif signal == 'white_noise':
            stim = nengo.Node(nengo.processes.WhiteSignal(
                period=100,
                high=freq,
                rms=0.5,
                seed=ss))
        p_stim = nengo.Probe(stim, synapse=None)
        p_integral = nengo.Probe(stim, synapse=1/s)
        
    with nengo.Simulator(model, progress_bar=verbose) as sim:
        sim.run(t, progress_bar=verbose)
        
    stimulus = sim.data[p_stim]
    target = sim.data[p_integral]
    target_f = h_out.filt(sim.data[p_integral])
    norm_s = np.max(np.abs(stimulus))
    norm = np.max(np.abs(target))
    norm_f = np.max(np.abs(target_f))
    
    # now run the full simulations with the appropriate normalizing transforms
    with nengo.Network() as model:
        if signal == 'cos':
            stim = nengo.Node(output=lambda t: np.cos(freq*t))
        elif signal == 'white_noise':
            stim = nengo.Node(nengo.processes.WhiteSignal(
                period=100,
                high=freq,
                rms=0.5,
                seed=ss))
        pre = nengo.Ensemble(n, 1, radius=norm_s, neuron_type=nengo.LIF(), seed=es)
        bio = nengo.Ensemble(n, 1, neuron_type=neuron_type, seed=es)
        tar = nengo.Ensemble(1, 1, neuron_type=nengo.Direct(), seed=es)

        p_bios = []
        nengo.Connection(stim, pre, synapse=None)
        for j, sec in enumerate(secs):
            nengo.Connection(pre, bio,
                             sec=sec,
                             n_syn=n_syns[j],
                             syn_type='ExpSyn',
                             tau_list=[tau],
                             synapse=tau,
                             transform=tau/norm_f,
                             seed=cs)
            nengo.Connection(bio, bio,
                             sec=sec,
                             n_syn=n_syns[j],
                             syn_type='ExpSyn',
                             tau_list=[tau],
                             synapse=tau,
                             solver=nengo.solvers.NoSolver(d_bios[j]),
                             seed=cs)
            p_bios.append(nengo.Probe(bio, synapse=h_out, solver=nengo.solvers.NoSolver(d_bios[j])))
        p_a = nengo.Probe(bio.neurons, synapse=None)
        nengo.Connection(stim, tar, synapse=1/s, transform=1.0/norm_f)

#         p_act = nengo.Probe(bio.neurons, synapse=None)
        p_target = nengo.Probe(tar, synapse=h_out)

    with nengo.Simulator(model, progress_bar=verbose) as sim:
        sim.run(t, progress_bar=verbose)

    target = sim.data[p_target]
    xhat_bios = []
    e_bios = []
    for i in range(len(secs)):
        xhat_bios.append(sim.data[p_bios[i]])
        e_bios.append(nengolib.signal.nrmse(xhat_bios[-1], target=target))
    
    plt.figure()
    plt.plot(sim.trange(), target, label=r""$x$"", linestyle='--')
    for i in range(len(secs)):
        plt.plot(sim.trange(), xhat_bios[i], label=r""$\hat{x}$, NRMSE=%.3f"" %e_bios[i])
    plt.legend()
    plt.show()
    
    fig, ax = plt.subplots(1, 1, figsize=(6, 8))
    rasterplot(sim.trange(), sim.data[p_a], ax=ax)
    plt.show()",0.4037430882,
47,alternate pca or other techniques,"def PlotMap2(df, fig, show=0, zips = geo_NY, key = 'log(tweets)', layout = 111, loc_ix = 0, alpha = 0.7,
            density = False, title = 'Community Map'):
       
    ax = fig.add_subplot(layout + loc_ix)
    # if we are plotting partition
    levels = 10
    p = zips.merge(df, on='postalCode', how='outer')
    m = len(p)
    p['part'] = (p[key] - np.log(p.Shape_Area)) if density else p[key]
    p['Rank'] = p.part.rank(method = 'min') * levels/m - 1.0/m
    p.Rank = p.Rank.fillna(-1).astype(int)

    for s in xrange(m):
        poly = Polygon(p.geometry[s])
        c = 'b' if p.Rank[s] == show else 'w'
        ax.add_patch(PolygonPatch(poly, fc = c, ec = 'k', alpha = alpha if c == 'w' else 1, zorder = 2))
      
    plt.title(title, fontweight = 'bold', size = 18)
    ax.axis('scaled')",0.4029012918,
47,alternate pca or other techniques,"# select best categorical columns (after transformation in dummy variables) 
# by choosing columns with lowest p-value from ANOVA test

class anova_best_p_selector(BaseEstimator, TransformerMixin):
    def __init__(self, k_best=None, p_limit=10**(-5)):
        self.p_limit = p_limit
        self.k_best = k_best
        
    def fit(self, X, y):
        # select dummy columns
        X_cat = X.filter(regex='_')
        # append target value to X
        df = X_cat.join(y)
        # select column name for y value
        self.target_col = df.columns[-1]
        
        # create a dataframe to write F-scores and p-values
        self.ANOVA_cat_cols = pd.DataFrame(columns=['F', 'p'])
        
        for col in X_cat.columns:
            pivot_df = df.pivot(columns=col, values=self.target_col)

            group_cols = pivot_df.columns
            groups = []
            for group_col in group_cols:
                groups.append(pivot_df[group_col].dropna())

            F, p = stats.f_oneway(*groups)
            self.ANOVA_cat_cols.loc[col, 'F'] = F
            self.ANOVA_cat_cols.loc[col, 'p'] = p
        
        if self.k_best == None:
            # select worst columns of p-value above p_limit
            ANOVA_cat_cols_worst = self.ANOVA_cat_cols[ANOVA_cat_cols.p > self.p_limit]
            self.worst_cols = ANOVA_cat_cols_worst.index
        
        else:
            # select k_best worst columns of highest p-value
            ANOVA_cat_cols_sorted = self.ANOVA_cat_cols.sort_values(by='p')
            self.worst_cols = ANOVA_cat_cols_sorted.index[self.k_best:]
        
        return self
    
    def transform(self, X):
        # drop the worst columns
        X_transformed = X.drop(self.worst_cols, axis=1)

        return X_transformed",0.4008618295,
47,alternate pca or other techniques,"def map_all_possible_edges(g, g2, df, data_dict = data_dict, basemap='Mapbox', save_to_html = False):
    centerLat = np.average([data_dict[i]['geometry']['coordinates'][1] for i in df['Station']])
    centerLon = np.average([data_dict[i]['geometry']['coordinates'][0] for i in df['Station']])
    new_edges = get_new_edges(g, g2)
    
    # Choose basemap (currently either Mapbox or OSM)
    if basemap == 'Mapbox':
        baseurl = 'http://{s}.tiles.mapbox.com/v4/mapbox.light/{z}/{x}/{y}.png'
        mapbox_api_key = 'pk.eyJ1Ijoid2lsbGdlYXJ5IiwiYSI6ImNpdW9wcmkxNjAxbDUydXQ0MzFwdmdvOWkifQ.9YIdQhYTOpRpocyFK-tBNA'
        token = '?access_token={}'.format(mapbox_api_key)
        m = folium.Map(location=[centerLat, centerLon], zoom_start=13, tiles=baseurl+token, attr='Mapbox')

    elif basemap == 'OSM':
        m = folium.Map(location=[centerLat, centerLon], zoom_start=13)
        
    # Draw existing edges (Black)
    for i in g.edges():
        source = i[0]
        target = i[1]
        source_name = data_dict[source]['name']
        target_name = data_dict[target]['name']
        source_lat = data_dict[source]['geometry']['coordinates'][1]
        source_lon = data_dict[source]['geometry']['coordinates'][0]
        target_lat = data_dict[target]['geometry']['coordinates'][1]
        target_lon = data_dict[target]['geometry']['coordinates'][0]
        edge_type = ""Existing edge""
        
        popup_html = folium.Html('<b>Station Name: </b> {} <br> <b>Stop Name: </b> {} <br> <b>Station ID: </b> {} <br> <b>Stop ID: </b> {} <br> <b>Type: </b> {}'.format(source_name, target_name, source, target, edge_type), script=True)
        popup = folium.Popup(popup_html, max_width=2650)
        folium.PolyLine(popup=popup, locations=[(source_lat, source_lon), (target_lat, target_lon)], color='Black', weight=4, opacity=1).add_to(m)
    
    # Draw new edges (Purple)
    for i in new_edges:
        source = i[0]
        target = i[1]
        source_name = data_dict[source]['name']
        target_name = data_dict[target]['name']
        source_lat = data_dict[source]['geometry']['coordinates'][1]
        source_lon = data_dict[source]['geometry']['coordinates'][0]
        target_lat = data_dict[target]['geometry']['coordinates'][1]
        target_lon = data_dict[target]['geometry']['coordinates'][0]
        edge_type = ""Potential edge""
        
        popup_html = folium.Html('<b>Station Name: </b> {} <br> <b>Stop Name: </b> {} <br> <b>Station ID: </b> {} <br> <b>Stop ID: </b> {} <br> <b>Type: </b> {}'.format(source_name, target_name, source, target, edge_type), script=True)
        popup = folium.Popup(popup_html, max_width=2650)
        folium.PolyLine(popup=popup, locations=[(source_lat, source_lon), (target_lat, target_lon)], color='Purple', weight=1, opacity=0.8).add_to(m)
    
    
    # Define point color scheme
    colors = {'Simple Stop': '#3186cc', 
              'Station': '#ef3b2c',
              'Stop Platform': '#32CD32', 
              'Line': '#000000'}
    
    # Draw nodes  
    for i in g2.nodes():
        
        # Get text for popup
        name = data_dict[i]['name']
        try: 
            operators = operators = data_dict[i]['operators_serving_stop_and_platforms'][0]['operator_name']
        except:
            operators = "" ""
        stop_station_type = data_dict[i]['stop_station_type']
        try:
            vehicles = ','.join(data_dict[i]['vehicle_types_serving_stop_and_platforms'])
        except:
            vehicles = "" ""
        location = str(data_dict[i]['geometry']['coordinates'])
        onestop_id = data_dict[i]['onestop_id']
                
        popup_html = folium.Html(""<b>Name: </b> {} <br> <b>Operators: </b> {} <br> <b>Type:</b> {} <br> <b>Vehicles:</b> {} <br> <b>Location:</b> {} <br> <b>Onestop-id:</b> {}"".format(name, operators, stop_station_type, vehicles, location, onestop_id), script=True)
        popup = folium.Popup(popup_html, max_width=2650)
        folium.CircleMarker(location=[data_dict[i]['geometry']['coordinates'][1], data_dict[i]['geometry']['coordinates'][0]], radius=4,
                        popup=popup, color=colors[data_dict[i]['stop_station_type']], fill_opacity=0.8,
                        fill_color=colors[data_dict[i]['stop_station_type']]).add_to(m)
    
    # Optional save map to html
    if save_to_html:
        m.save(save_to_html)
    else:
        return m",0.3951087892,
47,alternate pca or other techniques,"def precision_recall_plot(y_true, y_proba, label=' ', l='-', lw=1.0):
    from sklearn.metrics import precision_recall_curve, average_precision_score
    precision, recall, _ = precision_recall_curve(y_true,
                                                  y_proba[:,1])
    average_precision = average_precision_score(y_true, y_proba[:,1],
                                                     average=""micro"")
    ax.plot(recall, precision, label='%s (average=%.3f)'%(label,average_precision),
            linestyle=l, linewidth=lw)",0.3939989209,
47,alternate pca or other techniques,"def train_filter(spikes,
                 target,
                 method='hyperopt',
                 n_zeros=0,
                 n_poles=2,
                 z_min=0,
                 z_max=1,
                 p_min=-1e2,
                 p_max=-1e1,
                 reg_min=1e-1,
                 reg_max=1e-0,
                 max_evals=10,
                 hs=3,  # hyperopt seed
                 ):
    
    if method == 'hyperopt':
#         print 'Running hyperopt ...'
        hyperparams = {}
        for z in range(n_zeros):
            hyperparams['zeros_%s'%z] = hp.uniform('zeros_%s'%z, z_min, z_max)
        for p in range(n_poles):
            hyperparams['poles_%s'%p] = hp.uniform('poles_%s'%p, p_min, p_max)
        hyperparams['reg'] = hp.uniform('reg', reg_min, reg_max)
        hyperparams['n_zeros'] = n_zeros
        hyperparams['n_poles'] = n_poles
        hyperparams['spikes'] = spikes
        hyperparams['target'] = target

        def objective(hyperparams):
            zeros = np.array([hyperparams['zeros_%s'%z] for z in range(hyperparams['n_zeros'])])
            poles = np.array([hyperparams['poles_%s'%p] for p in range(hyperparams['n_poles'])])
            reg = hyperparams['reg']
            
            h = nengolib.signal.LinearSystem((zeros, poles, 1.0))
            h /= h.dcgain
            
            act = h.filt(np.load(hyperparams['spikes'])['spikes'])
            target = np.load(hyperparams['target'])['target']
            d = nengo.solvers.LstsqL2(reg=reg)(act, target)[0]
            xhat = np.dot(act, d)
            nrmse = nengolib.signal.nrmse(xhat, target=target)
            
            return {'loss': nrmse,
                    'zeros': zeros,
                    'poles': poles,
                    'reg': reg,
                    'status': STATUS_OK }
        
        trials = Trials()

        best = fmin(objective,
                    rstate=np.random.RandomState(seed=hs),
                    space=hyperparams,
                    algo=tpe.suggest,
                    max_evals=max_evals,
                    trials=trials)
        
        best_zeros = []
        best_poles = []
        for z in range(n_zeros):
            best_zeros.append(best['zeros_%s'%z])
        for p in range(n_poles):
            best_poles.append(best['poles_%s'%p])
        best_reg = best['reg']
        
        return best_zeros, best_poles, best_reg",0.3933161497,
482,discriminator training tests,"def evaluate_algorithm3(my_dataset, features_list):
    
    print('Decision Tree:')
    clf = DecisionTreeClassifier(random_state=42)
    tester.test_classifier(clf, my_dataset, features_list)
    print('')

    
    print('Ada Boost:')
    clf=AdaBoostClassifier(random_state=42)
    tester.test_classifier(clf, my_dataset, features_list)",0.4898322821,
482,discriminator training tests,"# To write a function which will run tester function on all 4 algorithms of my interest, so I can check the performance.
import tester

def evaluate_algorithm3(my_dataset, features_list):
    print('Logistic Regression')
    clf = LogisticRegression(random_state=42)
    tester.test_classifier(clf, my_dataset, features_list)
    print('')
    
    print('Decision Tree:')
    clf = DecisionTreeClassifier(random_state=42)
    tester.test_classifier(clf, my_dataset, features_list)
    print('')
    
    print('Random Forest:')
    clf=RandomForestClassifier(random_state=42)
    tester.test_classifier(clf, my_dataset, features_list)
    print('')
    
    print('Ada Boost:')
    clf=AdaBoostClassifier(random_state=42)
    tester.test_classifier(clf, my_dataset, features_list)",0.482312113,
482,discriminator training tests,"def main():
    lines = load_lines(data_path)[1:]
    training_set_lines, validation_set_lines = train_test_split(lines, test_size=0.2)
    
    nb_training = len(training_set_lines)*6
    nb_validation = len(validation_set_lines)*6

    training_images, steering_angles = get_data_without_generator(data_path, lines[0:500])
    return (training_images, steering_angles)
data_path = ""data-from-udacity""
#main()",0.4745939672,
482,discriminator training tests,"log_losses = []
accuracies = []

def train_model():
    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.2)
    
    # Create XGBClassifier using the parameters found from GridSearch
    xgboost_model = XGBClassifier(n_estimators=200, max_depth=5, learning_rate=0.2)
    xgboost_model.fit(X_train, y_train)

    # Predict model and calculate accurracy
    y_pred = xgboost_model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    accuracies.append(accuracy)
    print(""Accuracy: {:.2f}%"".format(accuracy * 100.0))

    # Predict model probabilities for each class and calculate log loss
    y_pred_prob = xgboost_model.predict_proba(X_test)
    loss = log_loss(y_test, y_pred_prob)
    log_losses.append(loss)
    print(""Log Loss: {:.4f}"".format(loss))
    
for i in range(5):
    train_model()",0.4600978494,
482,discriminator training tests,"def prediction(model, config, dataholder):
    """"""
    Generate some predictions using a batch from the test dataset

    :type model: LogisticRegression
    :type config: Config
    :type dataholder: DataHolder
    :rtype images: np array
    :rtype prediction: np array
    :rtype ground_truth: np array
    """"""
    num_features = config.num_features
    test_data = dataholder.test_dataset
    test_data = test_data.reshape((test_data.shape[0], num_features))
    test_labels = dataholder.test_labels
    batch_size = config.batch_size
    img_size = config.image_size
    with tf.Session(graph=model.graph) as sess:
        # We load the saved variables
        model.saver.restore(sess=sess, save_path=model.save_path)
        X_batch, Y_batch = get_batch(test_data,
                                     test_labels,
                                     batch_size)
        feed_dict = {model.X: X_batch, model.Y: Y_batch}
        acc, predictions = sess.run([model.acc,
                                  model.prediction],
                                  feed_dict=feed_dict)
        print('Test minibatch acc {:.2f}%'.format(acc*100))
    ground_truth = np.argmax(Y_batch, axis=1)
    images = X_batch.reshape((X_batch.shape[0], img_size, img_size))
    return images, predictions, ground_truth",0.4600554705,
482,discriminator training tests,"Utils functions'
def splitData(test_size,cv, numpoints):
    #This function from sklearn takes the length of the data and test size and returns bootstrapped indices 
    #depending on how many boostraps are required
    '''
    :param test_size: size of the test data required (value between 0 and 1).
    :param cv: Number of re-shuffling.
    :param numpoints: Total number of data points.
    :return: indices of the shuffled splits.
    '''
    ss = ShuffleSplit(n=numpoints, n_iter=cv, test_size=test_size, random_state=32)
    return ss

def calAccuracy(pred,ytest):
    '''
    :param pred: vector containing all the predicted classes
    :param ytest: vector containing all the true classes
    :return: accuracy of classification
    '''
    count = 0
    for i,j in izip(pred,ytest):
        if i==j:
            count +=1
    return count/(len(ytest))

def calgaussianprob(value,mean,std):
    '''
    :param value: Point for which the probability is to be found.
    :param mean: mean of distribution.
    :param std: standard deviation of distribution.
    :return: probability of the value to fall in normal distribution with given mean and std.
    '''
    return (np.exp(- np.power((value-mean),2) / (2*np.power(std,2)) )) / (np.sqrt(2*np.power(std,2)*np.pi ) )",0.4589065909,
482,discriminator training tests,"def Random_Sampling_lwR():
    RS_Lw_R=[]
    X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.99, random_state = seed)
    for i in range(0,Budget):
        print(i)
        nb.fit(X_train, Y_train)
        predicted = nb.predict(X_test)
        RS_Lw_R.append(nb.score(X_test,Y_test))
        random_list = X_test.iloc[0:5].index
        print(random_list)
        findind_rationale(random_list)
        X_train = X_train.append(X_test.iloc[0:5])
        X_test = X_test.drop(X_test.iloc[0:5].index)
        Y_train = Y_train.append(Y_test.iloc[0:5])
        Y_test = Y_test.drop(Y_test.iloc[0:5].index)

Random_Sampling_lwR()
print(""Accuracy: "",RS_Lw_R)",0.458742559,
482,discriminator training tests,"def Random_Sampling_lwoR():
    RS_Lwo_R=[]
    X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.99, random_state = seed)
    for i in range(0,Budget): 
        length_trainset.append(len(X_train))
        nb.fit(X_train, Y_train)
        predicted = nb.predict(X_test)
        RS_Lwo_R.append(nb.score(X_test,Y_test))
        X_train = X_train.append(X_test.iloc[0:5])
        X_test = X_test.drop(X_test.iloc[0:5].index)
        Y_train = Y_train.append(Y_test.iloc[0:5])
        Y_test = Y_test.drop(Y_test.iloc[0:5].index)
Random_Sampling_lwoR()
print(""Accuracy: "",RS_Lwo_R)",0.458742559,
482,discriminator training tests,"def Active_Learning_lwR():
    AL_Lw_R=[]
    X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.99, random_state = seed)
    for i in range(0,Budget):
        print(i)
        nb.fit(X_train, Y_train)
        predicted = nb.predict(X_test)
        AL_Lw_R.append(nb.score(X_test,Y_test))
        uncertain_list = finding_uncertain(X_test)
        findind_rationale(uncertain_list)
        for i, value in enumerate(uncertain_list): 
            X_train = X_train.append(X_test.loc[value])
            Y_train = Y_train.append(Y_test.loc[value])
            X_test = X_test.drop(value)
            Y_test = Y_test.drop(value)
Active_Learning_lwR()
print(""Accuracy: "",AL_Lw_R)",0.458742559,
482,discriminator training tests,"def Active_Learning_lwoR():
    AL_Lwo_R=[]
    X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.99, random_state = seed)
    for i in range(0,Budget): 
        print(i)
        nb.fit(X_train, Y_train)
        predicted = nb.predict(X_test)
        AL_Lwo_R.append(nb.score(X_test,Y_test))
        uncertain_list = finding_uncertain(X_test)
        for i, value in enumerate(uncertain_list):
            X_train = X_train.append(X_test.loc[value])
            Y_train = Y_train.append(Y_test.loc[value])
            X_test = X_test.drop(value)
            Y_test = Y_test.drop(value)
Active_Learning_lwoR()
print(""Accuracy: "",AL_Lwo_R)",0.458742559,
1180,matrices,"def read_counts():
    adata_counts = sc.read('./data/CountsNorm.csv', cache=True).T
    # this is not yet in sparse format, as the data was in a dense csv file
    from scipy.sparse import csr_matrix
    adata_counts.X = csr_matrix(adata_counts.X)
    adata_counts.write('./write/zebrafish_sparse_counts.h5ad')
# read_counts()",0.4141644835,
1180,matrices,"# Function to return dummy variables for certain columns

# Company name was not included

def get_features(dataframe):
    
   features = patsy.dmatrix('~ C(State) + C(City)', dataframe)
   X_df = pd.DataFrame(features, columns=features.design_info.column_names)
   X_df.drop('Intercept', axis=1, inplace=True)
   return X_df",0.4105123281,
1180,matrices,"def tolst(term):
    return [list(term.mat.Xs),list(term.mat.Zs),term.val]
random.seed(3)
system = SBRG(TFIsing(8,J=1.,K=1.,h=1.,alpha=0.2))
RGdat = []
while system.phybits:
    (H0, Rs, offdiag) = system.nextstep()
    RGdat.append([tolst(H0),[tolst(R) for R in Rs],[tolst(S) for S in offdiag]])
Hbdy = [tolst(term) for term in system.Hbdy]
Heff = [tolst(term) for term in system.Heff]
export('WFO_J1K1h1_a02_8',{'bits':system.size,'RGdat': RGdat,'Hbdy':Hbdy,'Heff':Heff})",0.3992372453,
1180,matrices,"%%add_to Network
def initialize_weights(self):
    # YOUR CODE HERE
    self.weights = np.random.randn(self.sizes[2], self.sizes[1])",0.3989248276,
1180,matrices,"A, B, C, D = [sympy.Matrix(m) for m in [G2ss.A, G2ss.B, G2ss.C, G2ss.D]]",0.3974797726,
1180,matrices,"# Write a function 'kalman_filter' that implements a multi-
# dimensional Kalman Filter for the example given

from math import *


class matrix:
    
    # implements basic operations of a matrix class
    
    def __init__(self, value):
        self.value = value
        self.dimx = len(value)
        self.dimy = len(value[0])
        if value == [[]]:
            self.dimx = 0
    
    def zero(self, dimx, dimy):
        # check if valid dimensions
        if dimx < 1 or dimy < 1:
            raise (ValueError, ""Invalid size of matrix"")
        else:
            self.dimx = dimx
            self.dimy = dimy
            self.value = [[0 for row in range(dimy)] for col in range(dimx)]
    
    def identity(self, dim):
        # check if valid dimension
        if dim < 1:
            raise (ValueError, ""Invalid size of matrix"")
        else:
            self.dimx = dim
            self.dimy = dim
            self.value = [[0 for row in range(dim)] for col in range(dim)]
            for i in range(dim):
                self.value[i][i] = 1
    
    def show(self):
        for i in range(self.dimx):
            print(self.value[i])
        print(' ')
    
    def __add__(self, other):
        # check if correct dimensions
        if self.dimx != other.dimx or self.dimy != other.dimy:
            raise (ValueError, ""Matrices must be of equal dimensions to add"")
        else:
            # add if correct dimensions
            res = matrix([[]])
            res.zero(self.dimx, self.dimy)
            for i in range(self.dimx):
                for j in range(self.dimy):
                    res.value[i][j] = self.value[i][j] + other.value[i][j]
            return res
    
    def __sub__(self, other):
        # check if correct dimensions
        if self.dimx != other.dimx or self.dimy != other.dimy:
            raise (ValueError, ""Matrices must be of equal dimensions to subtract"")
        else:
            # subtract if correct dimensions
            res = matrix([[]])
            res.zero(self.dimx, self.dimy)
            for i in range(self.dimx):
                for j in range(self.dimy):
                    res.value[i][j] = self.value[i][j] - other.value[i][j]
            return res
    
    def __mul__(self, other):
        # check if correct dimensions
        if self.dimy != other.dimx:
            raise (ValueError, ""Matrices must be m*n and n*p to multiply"")
        else:
            # multiply if correct dimensions
            res = matrix([[]])
            res.zero(self.dimx, other.dimy)
            for i in range(self.dimx):
                for j in range(other.dimy):
                    for k in range(self.dimy):
                        res.value[i][j] += self.value[i][k] * other.value[k][j]
            return res
    
    def transpose(self):
        # compute transpose
        res = matrix([[]])
        res.zero(self.dimy, self.dimx)
        for i in range(self.dimx):
            for j in range(self.dimy):
                res.value[j][i] = self.value[i][j]
        return res
    
    # Thanks to Ernesto P. Adorio for use of Cholesky and CholeskyInverse functions
    
    def Cholesky(self, ztol=1.0e-5):
        # Computes the upper triangular Cholesky factorization of
        # a positive definite matrix.
        res = matrix([[]])
        res.zero(self.dimx, self.dimx)
        
        for i in range(self.dimx):
            S = sum([(res.value[k][i])**2 for k in range(i)])
            d = self.value[i][i] - S
            if abs(d) < ztol:
                res.value[i][i] = 0.0
            else:
                if d < 0.0:
                    raise (ValueError, ""Matrix not positive-definite"")
                res.value[i][i] = sqrt(d)
            for j in range(i+1, self.dimx):
                S = sum([res.value[k][i] * res.value[k][j] for k in range(self.dimx)])
                if abs(S) < ztol:
                    S = 0.0
                res.value[i][j] = (self.value[i][j] - S)/res.value[i][i]
        return res
    
    def CholeskyInverse(self):
        # Computes inverse of matrix given its Cholesky upper Triangular
        # decomposition of matrix.
        res = matrix([[]])
        res.zero(self.dimx, self.dimx)
        
        # Backward step for inverse.
        for j in reversed(range(self.dimx)):
            tjj = self.value[j][j]
            S = sum([self.value[j][k]*res.value[j][k] for k in range(j+1, self.dimx)])
            res.value[j][j] = 1.0/tjj**2 - S/tjj
            for i in reversed(range(j)):
                res.value[j][i] = res.value[i][j] = -sum([self.value[i][k]*res.value[k][j] for k in range(i+1, self.dimx)])/self.value[i][i]
        return res
    
    def inverse(self):
        aux = self.Cholesky()
        res = aux.CholeskyInverse()
        return res
    
    def __repr__(self):
        return repr(self.value)


########################################

# Implement the filter function below

def kalman_filter(x, P):
    for n in range(len(measurements)):
        
        # measurement update
        Z = matrix([[measurements[n]]])
        y = Z - (H * x)
        S = H * P * H.transpose() + R
        K = P * H.transpose() * S.inverse()
        x = x + (K * y)
        
        P = (I - (K * H)) * P

        # prediction
        x = (F * x) + u
        P = F * P * F.transpose()
        
    return x,P

############################################
### use the code below to test your filter!
############################################

measurements = [1, 2, 3]

x = matrix([[0.], [0.]]) # initial state (location and velocity)
P = matrix([[1000., 0.], [0., 1000.]]) # initial uncertainty
u = matrix([[0.], [0.]]) # external motion
F = matrix([[1., 1.], [0, 1.]]) # next state function
H = matrix([[1., 0.]]) # measurement function
R = matrix([[1.]]) # measurement uncertainty
I = matrix([[1., 0.], [0., 1.]]) # identity matrix

print(kalman_filter(x, P))
# output should be:
# x: [[3.9996664447958645], [0.9999998335552873]]
# P: [[2.3318904241194827, 0.9991676099921091], [0.9991676099921067, 0.49950058263974184]]",0.3972291946,
1180,matrices,"from math import *

class matrix:
    def __init__(self, value):
        self.value = value
        self.dimx = len(value)
        self.dimy = len(value[0])
        if value == [[]]:
            self.dimx = 0
    
    def zero(self, dimx, dimy):
        # check if valid dimensions
        if dimx < 1 or dimy < 1:
            raise ValueError(""Invalid size of matrix"")
        else:
            self.dimx = dimx
            self.dimy = dimy
            self.value = [[0 for row in range(dimy)] for col in range(dimx)]
    
    def identity(self, dim):
        # check if valid dimension
        if dim < 1:
            raise ValueError(""Invalid size of matrix"")
        else:
            self.dimx = dim
            self.dimy = dim
            self.value = [[0 for row in range(dim)] for col in range(dim)]
            for i in range(dim):
                self.value[i][i] = 1
    
    def show(self):
        for i in range(self.dimx):
            print(self.value[i])
        print (' ')
    
    def __add__(self, other):
        # check if correct dimensions
        if self.dimx != other.dimx or self.dimy != other.dimy:
            raise ValueError(""Matrices must be of equal dimensions to add"")
        else:
            # add if correct dimensions
            res = matrix([[]])
            res.zero(self.dimx, self.dimy)
            for i in range(self.dimx):
                for j in range(self.dimy):
                    res.value[i][j] = self.value[i][j] + other.value[i][j]
            return res
    
    def __sub__(self, other):
        # check if correct dimensions
        if self.dimx != other.dimx or self.dimy != other.dimy:
            raise ValueError(""Matrices must be of equal dimensions to subtract"")
        else:
            # subtract if correct dimensions
            res = matrix([[]])
            res.zero(self.dimx, self.dimy)
            for i in range(self.dimx):
                for j in range(self.dimy):
                    res.value[i][j] = self.value[i][j] - other.value[i][j]
            return res
    
    def __mul__(self, other):
        # check if correct dimensions
        if self.dimy != other.dimx:
            raise ValueError(""Matrices must be m*n and n*p to multiply"")
        else:
            # subtract if correct dimensions
            res = matrix([[]])
            res.zero(self.dimx, other.dimy)
            for i in range(self.dimx):
                for j in range(other.dimy):
                    for k in range(self.dimy):
                        res.value[i][j] += self.value[i][k] * other.value[k][j]
            return res
    
    def transpose(self):
        # compute transpose
        res = matrix([[]])
        res.zero(self.dimy, self.dimx)
        for i in range(self.dimx):
            for j in range(self.dimy):
                res.value[j][i] = self.value[i][j]
        return res
    
    # Thanks to Ernesto P. Adorio for use of Cholesky and CholeskyInverse functions
    
    def Cholesky(self, ztol=1.0e-5):
        # Computes the upper triangular Cholesky factorization of
        # a positive definite matrix.
        res = matrix([[]])
        res.zero(self.dimx, self.dimx)
        
        for i in range(self.dimx):
            S = sum([(res.value[k][i])**2 for k in range(i)])
            d = self.value[i][i] - S
            if abs(d) < ztol:
                res.value[i][i] = 0.0
            else:
                if d < 0.0:
                    raise ValueError(""Matrix not positive-definite"")
                res.value[i][i] = sqrt(d)
            for j in range(i+1, self.dimx):
                S = sum([res.value[k][i] * res.value[k][j] for k in range(self.dimx)])
                if abs(S) < ztol:
                    S = 0.0
                res.value[i][j] = (self.value[i][j] - S)/res.value[i][i]
        return res
    
    def CholeskyInverse(self):
        # Computes inverse of matrix given its Cholesky upper Triangular
        # decomposition of matrix.
        res = matrix([[]])
        res.zero(self.dimx, self.dimx)
        
        # Backward step for inverse.
        for j in reversed(range(self.dimx)):
            tjj = self.value[j][j]
            S = sum([self.value[j][k]*res.value[j][k] for k in range(j+1, self.dimx)])
            res.value[j][j] = 1.0/tjj**2 - S/tjj
            for i in reversed(range(j)):
                res.value[j][i] = res.value[i][j] = -sum([self.value[i][k]*res.value[k][j] for k in range(i+1, self.dimx)])/self.value[i][i]
        return res
    
    def inverse(self):
        aux = self.Cholesky()
        res = aux.CholeskyInverse()
        return res
    
    def __repr__(self):
        return repr(self.value)",0.3963378668,
1180,matrices,"class matrix_obj(object):
    def __init__(self, matrix):
        self.matrix = matrix
    def Distance(self, from_node, to_node):
        return self.matrix[from_node][to_node]
    
class demand_obj(object):
    def __init__(self, demands):
        self.demands = demands
    def Demand(self, from_node, to_node):
        return self.demands[from_node]
    
# distance callback
dist_matrix_obj = matrix_obj(dist_matrix)
dist_callback = dist_matrix_obj.Distance
routing.SetArcCostEvaluatorOfAllVehicles(dist_callback)

# demand callback
demands_obj = demand_obj(demands)
demands_callback = demands_obj.Demand",0.3926136792,
1180,matrices,"def normalize(dataset):
    for i in range(0,dataset.shape[0]):
        dataset[i,:,:] = (dataset[i,:,:] - dataset[i,:,:].mean()) / dataset[i,:,:].std()
    return dataset

train_dataset = normalize(train_dataset)
valid_dataset = normalize(valid_dataset)
test_dataset = normalize(test_dataset)",0.3922253251,
1180,matrices,"# INPUT: V - np.array(N_films, k), test_data - scipy.sparse.csr_matrix (N_train x N_films)
# OTPUT: total_score - integer
def total_score_folding(V, test_data): # 8 pts
    # enter you code here
    total_score = 0
    test_data[:,-3:]=scipy.sparse.csr_matrix(np.zeros((test_data.shape[0],3)))
    for user in test_data:
        rec = V@user.T
        rec = scipy.sparse.csr_matrix(V.T@rec)
        total_score+=np.intersect1d(top_n(user,3), top_n(rec,3)).shape[0]
    return total_score",0.3912163675,
25,adding a special state,"def save_checkpoint(epoch, filename):
    ''' Save the training state. '''
    state = {
        'epoch': epoch,
        'state_dict': tbd_net.state_dict(),
        'optimizer': optimizer.state_dict()
        }
    torch.save(state, filename)",0.4241232276,
25,adding a special state,"%%add_to DroneIn3D

def advance_state(self, dt):
    
    # TODO replace this with your own implementation
    # 
    #   make sure this function returns the new state! 
    #   Some of the code that calls this function will expect
    #   it to return the state, so simply updating self.X 
    #   is not enough (though you should do that in this
    #   method too.)
    
    return super(DroneIn3D, self).advance_state(dt)",0.4149196744,
25,adding a special state,"def select_action_using_simulator(selected_action, other_actions):
    simulator_chance = (random.randint(1, 100))

    if 71 <= simulator_chance <= 80 and len(other_actions) >= 1:
        selected_action = other_actions[0]

    elif 81 <= simulator_chance <= 90 and len(other_actions) >= 2:
        selected_action = other_actions[1]

    elif 91 <= simulator_chance <= 100 and len(other_actions) >= 3:
        selected_action = other_actions[2]

    return selected_action",0.4088844061,
25,adding a special state,"def callback(node, obj):
    if node.data in ('e', 'f'):
        print 'Found node of interest', node
        obj.append(node)
    else:
        print 'Ignoring node', node

result = a.depth_first_traversal(callback=callback, direction=FORWARD, obj=[])
result",0.4077341855,
25,adding a special state,"def connect(a, b, weight):
    conn = Connection(weight)
    a.outputs.append((b, conn))
    b.inputs.append((a, conn))

def connect_layer(a, b, weights):
    for j in range(len(b)):
        connect(Neuron(1.0), b[j], weights[j][0])
        for i in range(len(a)):
            connect(a[i], b[j], weights[j][i + 1])",0.4058183134,
25,adding a special state,"from Picker import Picker # An home-made module

def dataplot(ax, ImgNum, args):
    """"""
    A function that plot the data and the pickable markers
    and return the plot of the pickable markers
    The signature of this function is non-negotiable (forced by Picker)
    """"""
    data, markers = args
    ax.plot(*data[ImgNum], '-k')
    ax.grid('on')
    plot, = ax.plot(*markers[ImgNum], 'ro', picker=50.)
    return plot

    
#For jupyter, keep this object in memory
picker = Picker()

# let the user pick one point per image (the first dim of data)
result = picker.pick((data, markers), ndata=na, plotfx=dataplot)",0.4057900608,
25,adding a special state,"def mcmc(problem, n_iter):
    # want to keep growing these lists to track the parameter estimates and log-posterior score
    alpha = [problem.current.state[0]]
    beta = [problem.current.state[1]]
    lpost = [problem.current.value]
    
    # n_accept will keep tracking the number of times where we accept the proposed new move
    n_accept = 0
    
    np.seterr(over=""ignore"")
    for t in range(n_iter):
        # Here we will propose a new move
        nextMove, nextValue = problem.random_move()
        
        # calculate the acceptance probability and call it delta_obj
        delta_obj = np.exp(nextValue - lpost[-1])
        
        # the proposed move is an improvement, so accept w.p. 1
        if delta_obj > 1:
            n_accept += 1
            
            # keep tracking our states
            alpha.append(nextMove[0])
            beta.append(nextMove[1])
            lpost.append(nextValue)
            
            # the state in iteration t+1 will be proposed new move
            problem.current.state = nextMove
            problem.current.value = nextValue
            
        # the proposed move is worse; accept with probability we mentioned above
        else:
            p_accept = delta_obj
            accept = np.random.choice([True, False], p=[p_accept, 1 - p_accept])
            if accept:
                n_accept += 1
                
                # keep tracking our states
                alpha.append(nextMove[0])
                beta.append(nextMove[1])
                lpost.append(nextValue)
                
                # since we accept the new move, the state in iteration t+1 will be proposed new move
                problem.current.state = nextMove
                problem.current.value = nextValue
            else:
                
                # since we didn't accept the new move, the state in iteration t+1 will be the same as the state in iteration t
                # so we don't need to change the current state
                alpha.append(alpha[-1])
                beta.append(beta[-1])
                lpost.append(lpost[-1])

    return (alpha, beta, lpost, n_accept / n_iter)",0.4054614604,
25,adding a special state,"def mcmc(problem, n_iter):
    # define the parameter lists
    alpha = [problem.current.state[0]] 
    beta = [problem.current.state[1]]
    lpost = [problem.current.value]
    
    n_accept, iter_ = 0, 0
    
    np.seterr(over='ignore')
    
    with Pool(4) as p:
        for t in range(n_iter):
            iter_ += 1
            nextMove, nextValue = problem.random_move()
            delta_obj = np.exp(nextValue - lpost[-1])
            if delta_obj > 1: # if the proposed move is an improvement, so accept w/ prob. 1
                n_accept += 1 # increase the number of acceptances by 1
                alpha.append(nextMove[0])
                beta.append(nextMove[1])
                lpost.append(nextValue)
                problem.current.state = nextMove
                problem.current.value = nextValue
            else: # if the proposed move is worse; accept with probability exp[delta_obj/T]
                p_accept = delta_obj
                accept = np.random.choice([True, False], p=[p_accept, 1-p_accept])
                if accept:
                    n_accept += 1
                    alpha.append(nextMove[0])
                    beta.append(nextMove[1])
                    lpost.append(nextValue)
                    problem.current.state = nextMove
                    problem.current.value = nextValue
                else:
                    alpha.append(alpha[-1])
                    beta.append(beta[-1])
                    lpost.append(lpost[-1])

    return (alpha, beta, lpost, n_accept/iter_)",0.4054614604,
25,adding a special state,"def mcmc(problem, n_iter):
    parameters = [problem.current.state]
    posterior = [problem.current.value]
    
    for t in range(n_iter):
        #propose a new state
        (new_state, new_post) = problem.random_move()
        #calculate the acceptance probability of our new state
        p_accept = (new_post)/(posterior[t])
        
        if p_accept >= 1:
            
            #add the posterior score of our new proposed state
            posterior.append(new_post)
            #add our proposed state to the list of parameters that we have built
            parameters.append(new_state)
            problem.current.state = new_state
            problem.current.value = new_post
            
        if p_accept < 1:
            #check to see if we accept our supposed parameter
            selection = np.random.choice([True, False], p = [p_accept, 1-p_accept])
            
            if selection == True:
                #add the posterior score of our new proposed state
                posterior.append(new_post)
                #add our proposed state to the list of parameters that we have built
                parameters.append(new_state)
                problem.current.state = new_state
                problem.current.value = new_post
                
            if selection == False:
                #otherwise, we reject the new parameter and instead
                #add another copy of the current parameter and its
                #corresponding posterior score
                parameters.append(parameters[t])
                posterior.append(posterior[t])

    return parameters, posterior",0.4043707252,
25,adding a special state,"def update_target(current_model, target_model):
    target_model.load_state_dict(current_model.state_dict())",0.4041692913,
2300,tasks,"def latex_table_per_task(task_name):
    task_S = per_measurement[per_measurement.task == task_name]
    del task_S['task']
    task_S = task_S.set_index(['submission', 'domain']).unstack('domain')
    task_S.columns = task_S.columns.levels[1]
    task_S = task_S[['Reviews', 'TED', 'Twitter', 'Micro-Avg.']]
    print(task_S.to_latex())
    
latex_table_per_task('M')
latex_table_per_task('S')
latex_table_per_task('C')",0.4605081975,
2300,tasks,"def request_description(data):
    regex = compile(""\w+"")
    if data[1]:
        soup = parse_page(data[1])
        data = (data[0], reduce(lambda a, v: a + str(v), soup.find_all([""p"", ""h2""], string=regex), """"))
    return data",0.4542946219,
2300,tasks,"def execute(workers):
    threads = [Thread(target=w.map) for w in workers]
    for thread in threads: thread.start()
    for thread in threads: thread.join()
        
    first, rest = workers[0], workers[1:]
    for worker in rest:
        first.reduce(worker)
    return first.result",0.4496192634,
2300,tasks,"# repetitive function for slicing to compar top three teams.  Only used in the excercise for class, more useful code in
#actual analysis

def repetitive_slicing(plpos):
    df1=roster_df.loc[(roster_df.team_num==3)&(roster_df.pos==plpos)&(roster_df.Status==""Active""),[""week"",""team_num"",""Team"",""pos"",""FPTs""]]
    position_group = df1.groupby('week')
    position_totals = position_group.sum()

    df2=roster_df.loc[(roster_df.team_num==11)&(roster_df.pos==plpos)&(roster_df.Status==""Active""),[""week"",""team_num"",""Team"",""pos"",""FPTs""]]
    SAM_group = df2.groupby('week')
    SAM_totals = SAM_group.sum()


    df3=roster_df.loc[(roster_df.team_num==6)&(roster_df.pos==plpos)&(roster_df.Status==""Active""),[""week"",""team_num"",""Team"",""pos"",""FPTs""]]
    B2B_group = df3.groupby('week')
    B2B_totals = B2B_group.sum()


    df4=roster_df.loc[(roster_df.pos==plpos)&(roster_df.Status==""Active""),[""week"",""team_num"",""Team"",""pos"",""FPTs""]]
    ALL_group = df4.groupby('week')
    ALL_totals = ALL_group.sum()
    median_points=[ALL_totals.FPTs.median()/12 for x in ALL_totals.FPTs]
    
    fig = Figure()

    x = position_totals.index.values
    y1= list(position_totals.FPTs.values)
    y2=list(SAM_totals.FPTs.values)
    y3=list(B2B_totals.FPTs.values)

    ax = plt.subplot(111)
    ax.bar(x-0.25, y1,width=0.25,color='#016c59',align='center',label=""Pete's"")
    ax.bar(x, y2,width=0.25,color='#bdc9e1',align='center')
    ax.bar(x+.25, y3,width=0.25,color='r',align='center',label='B2B')

    ax.plot(x,y2,'o', lw=2, color='k', label='SAM')
    ax.plot(x,median_points,'x-',color='#fe9212', label=""League Avg."")

    ax.legend(loc='best')

    title=plpos+""s points by week""
    plt.title(title)
    return",0.4458469748,
2300,tasks,"def createLabeledPointFromMatchup(m):
    global players_dict_broadcast
    global num_players_broadcast
    home = m['home']
    away = m['away']
    home_unit = m[home]['on']
    away_unit = m[away]['on']
    home_poss = m[home]['stats']['poss']
    away_poss = m[away]['stats']['poss']
    avg_poss = (home_poss+away_poss)/2.
    if avg_poss <= 0:
        avg_poss = 1
    players_dict = {players_dict_broadcast.value[player]:avg_poss for player in home_unit}
    players_dict.update({players_dict_broadcast.value[player]:-avg_poss for player in away_unit})
    home_pts = m[home]['stats']['pts']
    away_pts = m[away]['stats']['pts']
    return LabeledPoint(100*(home_pts-away_pts)/avg_poss, SparseVector(num_players_broadcast.value, players_dict))",0.4399574995,
2300,tasks,"def pipe(data):
    
    global activities
    
    url = data['url']
    stopList = data['stopList']
    cat = data['categoria']
    
    # Create the activity and add to an list
    activities.append(generateCatActivity(cat))
    
    # Download the data from url
    bs = downloadData(url)

    # Store the relevant content (in html) in a list
    data = extractData(bs,stopList)

    # Extract the useful parts from the list (name, description, etc...)
    data = generateUsefulData(data)

    # Get the name of subcats ordered alphabetically
    subcats = getSubCats(data)

    # Generate the files
    genFiles(data,cat,subcats,cat)",0.4384664595,
2300,tasks,"def clean_data(tempdf):
    newdf = tempdf
    newdf = newdf.assign(team = newdf.apply(lambda x: 0 if x.player_id < 5 else 1,axis=1))
    newdf[['hero_1','hero_2','hero_3','hero_4','hero_5','hero_6','hero_7','hero_8','hero_9','hero_10','hero_id']] = newdf[['hero_1','hero_2','hero_3','hero_4','hero_5','hero_6','hero_7','hero_8','hero_9','hero_10','hero_id']].astype(str)
    #on this line we can check to make sure every game has a full roster of players (having 10 players).
    #print ""We want to make sure each id occurs the same number of times""
    #print df['player_id'].value_counts()
    #When we run the above line we see that there is at least 1 game without a full roster!
    C = Counter(newdf['match_id'])
    less = [x for x in C.keys() if C[x] < 10]
    #print ""So it turns out we only have %d ID that doesn't show up 10 times"" % (len(less))
    #print ""Previously our dataframe had %d lines"" % len(newdf)
    newdf = newdf[newdf['match_id'] != less[0]]
    #Once we emilinate these rows we have only full game data!
    #print ""now we have %d lines"" % len(newdf)
    
    #Now we want to change all values of 0 after the match ends into the last non-zero value
    newdf = newdf.apply(lambda x: set_zero(x),axis=1)
    
    #We want to count the number of games each team wins
    counts = newdf['first_5_won'].value_counts()
    
    newdf.columns = newdf.columns.str.replace('g_5','g_05')
    newdf.columns = newdf.columns.str.replace('e_5','e_05')
    
    # df.rename(index=str, columns={""g_5"": ""a"", ""B"": ""c""})
    
    newdf.to_csv(""output.csv"")
    return newdf,counts

#There has to be a better way to do this but I have not thought it up
def set_zero(row):
    value = 5
    while value <= 40:
        e1 = 'e_' + str(value+5)
        e2 = 'e_' + str(value)
        g1 = 'g_' + str(value+5)
        g2 = 'g_' + str(value)
        if row[e1] == 0:
            row[e1] = row[e2]
            row[g1] = row[g2]
        value += 5
    return row

overview,detail = load_data(""MatchOverview.csv"",""MatchDetail.csv"")
df = overview.merge(detail,on=""match_id"")
df,counts = clean_data(df)",0.4379699826,
2300,tasks,"def clean_data(tempdf):
    newdf = tempdf
    newdf = newdf.assign(team = newdf.apply(lambda x: 0 if x.player_id < 5 else 1,axis=1))
    newdf[['hero_1','hero_2','hero_3','hero_4','hero_5','hero_6','hero_7','hero_8','hero_9','hero_10','hero_id']] = newdf[['hero_1','hero_2','hero_3','hero_4','hero_5','hero_6','hero_7','hero_8','hero_9','hero_10','hero_id']].astype(str)
    #on this line we can check to make sure every game has a full roster of players (having 10 players).
    #When we run the above line we see that there is at least 1 game without a full roster!
    C = Counter(newdf['match_id'])
    less = [x for x in C.keys() if C[x] < 10]

    #We run in to an issue where each player_id does not occur the same amount of times. 
    newdf = newdf[newdf['match_id'] != less[0]]
    #Once we emilinate these rows we have only full game data
    
    #Now we want to change all values of 0 after the match ends into the last non-zero value
    newdf = newdf.apply(lambda x: set_zero(x),axis=1)
    
    #We want to count the number of games each team wins
    counts = newdf['first_5_won'].value_counts()
    
    newdf.columns = newdf.columns.str.replace('g_5','g_05')
    newdf.columns = newdf.columns.str.replace('e_5','e_05')
    newdf = newdf.assign(won = newdf.apply(lambda x: 1 if (x.first_5_won and x.team == 0) or (x.first_5_won == False and x.team == 1) else 0, axis=1))
    newdf.drop('first_5_won', axis=1, inplace=True)
    newdf.to_csv(""output.csv"")
    return newdf,counts

#This is the function that changes all of the 0's at the end of every match to the last value that was non-zero for gold/experience
def set_zero(row):
    value = 5
    while value <= 40:
        e1 = 'e_' + str(value+5)
        e2 = 'e_' + str(value)
        g1 = 'g_' + str(value+5)
        g2 = 'g_' + str(value)
        if row[e1] == 0:
            row[e1] = row[e2]
            row[g1] = row[g2]
        value += 5
    return row

overview,detail = load_data(""MatchOverview.csv"",""MatchDetail.csv"")
df = overview.merge(detail,on=""match_id"")
df,counts = clean_data(df) 
print df.dtypes",0.4378501475,
2300,tasks,"# Code for generating environments and training data

def simulate(x):
    s = Simulation(np.zeros(8),objs=x[1])
    a = s.simulateActions(x[0])
    s.stop()
    return a

def getSimsStates(BS=10):
    olist = []
    states = []
    for i in range(BS):
        nobj = np.random.randint(6)
        objlist = []
        for j in range(nobj):            
            if np.random.randint(2)==0:
                otype=""cube.urdf""
            else:
                otype=""sphere.urdf""
            
            pos = [np.random.rand()*10-5, np.random.rand()*10-5, np.random.rand()*5+2]
            
            while (pos[0]**2+pos[1]**2 < 4.5*4.5):
                pos = [np.random.rand()*10-5, np.random.rand()*10-5, np.random.rand()*5+2]
                
            objlist.append([otype, pos])
        olist.append(objlist)
        s = Simulation(np.zeros(8),objs=objlist)
        states.append(s.getInState())
        s.stop()
    
    states = np.array(states)
    
    return states, olist

def getData(N):
    MESH = 7
    PROPOSALS = MESH*MESH
    states, olist = getSimsStates(BS=N)
    acts = []
    ends = []
    
    a,p = propose.generate(tovar(states), predict)    
    a = a.cpu().data.numpy()
    
    for i in range(N):
        acts.append(np.clip(1.0*(np.random.rand(ACTUATORS)*2-1),-1,1))
    ends = []
    
    pool = Pool(4)
    ends = pool.map(simulate, zip(acts,olist))
    pool.close()
    pool.join()
    
    ends = np.array(ends)
    
    return states, np.array(acts), ends

def getProposeData(N):
    MESH = 7
    PROPOSALS = MESH*MESH
    states, olist = getSimsStates(BS=N)
    acts = []
    ends = []
    
    a,p = propose.generate(tovar(states), predict)    
    a = a.cpu().data.numpy()
    
    for i in range(N):
        acts.append(np.clip(a[i,:,i%PROPOSALS] + 0.1*(np.random.rand(ACTUATORS)*2-1),-1,1))
    ends = []
    
    pool = Pool(4)
    ends = pool.map(simulate, zip(acts,olist))
    pool.close()
    pool.join()
    
    ends = np.array(ends)
    
    return states, np.array(acts), ends",0.4377070367,
2300,tasks,"def producer (args):
    prefix, maxN = args
    global dataQueue    
    print (""Started producer ---->"", os.getpid())
    for i in range(maxN):
        data = ""%s-%d"" % (prefix, i)        
        dataQueue.put(data, block=True)    
        time.sleep(random.random() * 0.1)",0.4367878735,
350,create a visualization,"def process_frame(frame):
    """"""Process a single frame""""""
    
    # Run detection
    output_dict = run_inference_for_single_image(frame, detection_graph)

    # Visualise the results
    vis_util.visualize_boxes_and_labels_on_image_array(
        frame,
        output_dict['detection_boxes'],
        output_dict['detection_classes'],
        output_dict['detection_scores'],
        category_index,
        instance_masks=output_dict.get('detection_masks'),
        use_normalized_coordinates=True,
        line_thickness=8)
    
    return frame",0.4697383642,
350,create a visualization,"def visualize_tree(tree, feature_names):
    """"""Create tree png using graphviz.

    Args
    ----
    tree -- scikit-learn DecsisionTree.
    feature_names -- list of feature names.
    """"""
    with open(""dt.dot"", 'w') as f:
        export_graphviz(tree, out_file=f,
                        feature_names=feature_names)

    command = [""dot"", ""-Tpng"", ""dt.dot"", ""-o"", ""dt.png""]
    try:
        subprocess.check_call(command)
    except:
        print (""Could not run dot, ie graphviz, to ""
             ""produce visualization"")",0.4664325714,
350,create a visualization,"from wordcloud import WordCloud
def visualCloud(string):
    wordcloud = WordCloud().generate(text)
    # Display the generated image:
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis(""off"")
    plt.show()",0.4608536065,
350,create a visualization,"def word_cloud(text):
    
    wordcloud = WordCloud().generate(text)
    plt.imshow(wordcloud)
    plt.axis(""off"")
    plt.show()",0.4603263736,
350,create a visualization,"def get_conv_feat(f, model):
    '''
    For any given image (file or url),
    convert to NumPy Array, resize to img_width x img_height,
    preprocess the values for ResNet, get the convolutional features
    from ResNet, and flatten the output.
    '''
    img = read_img(f)
    np_img = resize_img_to_array(img, img_shape=(img_width, img_height))
    X = preprocess_resnet(np.expand_dims(np_img, axis=0).astype(np.float))
    X_conv = model.predict(X)
    X_conv_2d = X_conv[0].flatten()
        
    return X_conv_2d",0.4587128758,
350,create a visualization,"def binary_encoding(df,columns):
    """"""Binary encoding""""""
    print('*'*5,'Binary encoding','*'*5)
    lb = LabelBinarizer()
    print('Original shape:',df.shape)
    original_col = df.columns
    #columns = [i for i in columns if df[columns].nunique()>2]
    for i in columns:
        if df[i].nunique() >2:
            result = lb.fit_transform(df[i].fillna(df[i].mode()[0],axis=0))
            col = ['BIN_'+ str(i)+'_'+str(c) for c in lb.classes_]
            result1 = pd.DataFrame(result, columns=col)
            df = df.join(result1)
    print('After:',df.shape)
    new_col = [c for c in df.columns if c not in original_col]
    return df, new_col",0.457002461,
350,create a visualization,"# Support Vector Machine from Scratch (w/ a lot of comments :) )
class SVM():
    
    def __init__(self, visualization=True):
        
        # set visualization as a default
        self.visualization = visualization
        self.colors = {1: 'r', -1: 'b'}
        if self.visualization:
            self.fig = plt.figure()
            self.ax = self.fig.add_subplot(111)
    # training method
    def fit(self, data):
        
        self.data = data
        
        # { ||w||: [w,b] }
        opt_dict = {}
        
        # used to create every combination of data for input
        transforms = [[1,1],
                      [-1,1],
                      [-1,-1],
                      [1,-1]]
        
        # parse out each datapoint
        all_data = []    
    
        for yi in self.data:
            for featureset in self.data[yi]:
                for feature in featureset:
                    all_data.append(feature)
        
        # of those datapoints, find the maximum and minimum
        # this will be used as a starting point for finding global minimum
        self.max_feature_value = max(all_data)
        self.min_feature_value = min(all_data)
        
        # how about we save some memory
        all_data = None
        
        # list of different step sizes (based max feature value)
        # at the thousandth, things begin to slow down. limit training
        # at the ten-thousandth until after other paramaters are optimized.
        step_sizes = [self.max_feature_value * 0.1,
                      self.max_feature_value * 0.01,
                      self.max_feature_value * 0.001,
                      # train at this step after param optimization
                      #self.max_feature_value * 0.0001
                     ]

        # similar to finding the max value, this will be used as the starting
        # point for finding optimal 'b' value. the larger this is, the more
        # expensive it will be. this is a useful paramater to optimize.
        b_range_multiple = 5
        
        # we don't need to take as small of a step with 'b' as we do 'w', so this
        # multiplies the step size. this is another easy win in terms of optimization.
        b_multiple = 5
        
        # sets the latest optimum as something very high - this is arbitrary and
        # will be set as a 'true latest optimium' after one step size iteration.
        latest_optimum = self.max_feature_value * 10
        
        # now we'll iterate through each step size, trying to find minimum
        for step in step_sizes:
            
            # initialize w
            w = np.array([latest_optimum, latest_optimum])
            
            # loop until we find the minimum for this step size
            optimized = False
            while not optimized:
                
                # there may be a better way to do this, but first we'll loop through
                # each possible 'b' as set by the max feature value and the multiple
                for b in np.arange(-1 * (self.max_feature_value * b_range_multiple),
                                   self.max_feature_value * b_range_multiple,
                                   step * b_multiple):
                    
                    # and now, we'll loop through each transformation, to create
                    # every possible combination for 'w' (positive and negative)
                    for transformation in transforms:
                        w_t = w * transformation
                        
                        # True until otherwise proven False ;)
                        found_option = True
                        
                        # here we'll loop through each datapoint and check whether
                        # or not yi(xi.w+b) >= 1 is satisfied. if even one combo
                        # does not satisfy it, it is all thrown out (as declared by
                        # found_option = False)
                        for i in self.data:
                            for xi in self.data[i]:
                                yi=i
                                if not yi * (np.dot(w_t, xi) + b) >= 1:
                                    found_option = False
                        
                        # if all datapoints satisfy yi(xi.w+b) >= 1, then we save
                        # it in the ""option dictionary""
                        if found_option:
                            # use np.linalg.norm() to get magnitude of w (w_t)
                            opt_dict[np.linalg.norm(w_t)] = [w_t, b]
                
                # after we loop through every step in 'w', then we break the while
                # loop and begin on the next step (or escape altogether if there
                # are no more steps)
                if w[0] < 0:
                    optimized = True
                    print('Optimized a step.')
                else:
                    w = w - step
            
            # sort options to find the one with the lowest magnitude
            norms = sorted([n for n in opt_dict])
            opt_choice = opt_dict[norms[0]]
            
            # using the optimal option, set 'w' and 'b'
            self.w = opt_choice[0]
            self.b = opt_choice[1]
            
            # and finallyyyyy set new optimal 'w' (with some padding :) )
            latest_optimum = opt_choice[0][0] + step * 2
            

    def predict(self, features):
        # using the model we just created using the 'fit' method, we'll classify
        # any new datapoints by finding their sign: sign ( x.w + b )
        classification = np.sign(np.dot(np.array(features), self.w) + self.b)
        
        # if we are visualizing, then add classified datapoints to scatter
        # plot using 'x' marker
        if classification !=0 and self.visualization:
            self.ax.scatter(features[0], features[1], s=200, marker='x', c=self.colors[classification])
        
        return classification
    
    def visualize(self):
        
        # plot classified datapoints
        [[self.ax.scatter(x[0], x[1], s = 100, color = self.colors[i]) for x in data[i]] for i in data]

        # create helper for hyperplane: = x.w + b
        def hyperplane(x, w, b, v):
            return (-w[0] * x - b + v) / w[1]
        
        # set (and add some padding) to the min and max of hyperplane
        # (this basically changes the length of the support vectors)
        datarange = (self.min_feature_value * 1.1, self.max_feature_value * 1.1)
        hyp_x_min = datarange[0]
        hyp_x_max = datarange[1]

        # plot positive support vector hyperplane: (w.x + b) = 1
        psv1 = hyperplane(hyp_x_min, self.w, self.b, 1)
        psv2 = hyperplane(hyp_x_max, self.w, self.b, 1)
        self.ax.plot([hyp_x_min,hyp_x_max],[psv1,psv2], 'k', linewidth = 2)

        # (w.x+b) = -1
        # plot negative support vector hyperplane: : (w.x + b) = -1
        nsv1 = hyperplane(hyp_x_min, self.w, self.b, -1)
        nsv2 = hyperplane(hyp_x_max, self.w, self.b, -1)
        self.ax.plot([hyp_x_min,hyp_x_max],[nsv1,nsv2], 'k', linewidth = 2)

        # (w.x+b) = 0
        # plot decision barrier: (w.x + b) = 0
        db1 = hyperplane(hyp_x_min, self.w, self.b, 0)
        db2 = hyperplane(hyp_x_max, self.w, self.b, 0)
        self.ax.plot([hyp_x_min,hyp_x_max],[db1,db2], 'y--', linewidth = 2)

        plt.show()",0.4557856023,
350,create a visualization,"def plot_glass_brain_4D_timepoint(filename_4D, t=0, **kwargs):
    iterator = image.iter_img(filename_4D)
    nip.plot_glass_brain(next(islice(iterator, t, t+1)), **kwargs)",0.4557219744,
350,create a visualization,"from wordcloud import WordCloud

def plot_word_cloud(text):
    wordcloud = WordCloud().generate(text)
    plt.imshow(wordcloud, interpolation = 'bilinear')
    plt.axis(""off"")
    plt.show()",0.4536380768,
350,create a visualization,"def perform_evaluate(fun, *args):
    #fit Model
    print(""start..perform.."")
    Y_predict = fun(*args)
    
    # Binarize the output
    y_bin = label_binarize(Y_test, classes=[1, 2, 3, 4])

    #Calculate AUC
    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    for i in range(4):
        fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], Y_predict[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])
        score = sum(roc_auc.values())/len(roc_auc)

    print(roc_auc)
    print(""average score : "",score)
    return roc_auc",0.453111887,
2619,write a list comprehension,"for i in range(0,10):
    f.write(""2 times %d = %d\n"" % (i, 2*i))    # C printf-like formatting",0.411256969,
2619,write a list comprehension,"with open(""languages_count.csv"",""w"") as languages_file:
    for language in languages:
        languages_file.write(""{}\n"".format("";"".join(str(i) for i in language)))",0.4108268023,
2619,write a list comprehension,"for i in range(0,5):
    fout.write( ""Current value of i is %d\n"" % i )  # note, we add a newline with \n at the end of each line",0.4107108116,
2619,write a list comprehension,"fout = open(""simple.csv"",""w"")
# create the records
for row in range(5):
    # start the record with an identifier
    fout.write(""record_%d"" % (row+1) )
    # create the fields for each record
    for col in range(4):
        value = (row+1)*(col+1)     # just create some dummy values
        fout.write("",%d"" % value )  # notice the comma separator
    # move on to a new line in the file
    fout.write(""\n"")
# finished, so close the file
fout.close()",0.4032764435,
2619,write a list comprehension,"# Step 2:
for i in range(10, 15):
     f.write(""This is line {}\n"".format(i+1))",0.3934481442,
2619,write a list comprehension,"with open('kira.out', 'w') as outfile:
    outfile.write("""".join([str(story) for story in storylist]))",0.3924970627,
2619,write a list comprehension,"# Step 2:
for i in range(10):
    f.write(""This is line {}\n"".format(i+1))",0.3921444416,
2619,write a list comprehension,"fn = 'coeffs.txt'
with open(fn, 'w') as f:
    f.write('\n'.join([str(c) for c in h]))",0.3914011717,
2619,write a list comprehension,"writefile=open('Fibre_LV.cmap',""w"")
writeFile.write(""%d\n"" % len(LineTable))
for Line in LineTable:
    writefile.write(""%s \n"" % Line[6])
writeFile.close()",0.3910860419,
2619,write a list comprehension,"L = [str(i) + ' is my favorite number' for i in range(3)]
L",0.3910022974,
942,instructions for the assignment,"# Do the lifting
def lift(poly_map, vars):  
  m = polypy.Assignment()
  lift_first_var(poly_map, vars, m)",0.4259106517,
942,instructions for the assignment,"#function for goal against, if goal is conceded,, then 0 else 1
def goalagainst(df):
    for i in range(len(df)):
        pd.options.mode.chained_assignment = None
        if df['GoalkeeperGoalsAgainst'][i]<1:
            df['goal_against_label'][i]=0
        else:
            df['goal_against_label'][i]=1

#function for goal against, if goal is conceded then 0,1,2 depending on how many, else 3 
def goalagainstunique(df):
    for i in range(len(df)):
        pd.options.mode.chained_assignment = None
        if df['GoalkeeperGoalsAgainst'][i]<3:
            df['GA_unique_label'][i]=df['GoalkeeperGoalsAgainst'][i]
        else:
            df['GA_unique_label'][i]=3          
#function for goal scored, if goal is scored then 0,1,2 depending on how many, else 3             
def goals(df):    
    for i in range(len(df)):
        pd.options.mode.chained_assignment = None
        if df['Goals'][i]<3:
            df['goal_label'][i]=df['Goals'][i]
        else:
            df['goal_label'][i]=3",0.4108560085,
942,instructions for the assignment,"#Defining Functions that will be used to treat the data

def create_boolean_column (df_name, new_col_name, rule):
    pd.options.mode.chained_assignment = None
    df_name[new_col_name] = np.where(rule, 1, 0)
    return df_name

def standardize_values(values):
    return (values - values.mean()) / values.std()",0.4086825252,
942,instructions for the assignment,"def result(df):
    for i in range(len(df)):
        pd.options.mode.chained_assignment = None
        if ((df['Goals'].iloc[i])<(df['GoalkeeperGoalsAgainst'].iloc[i])):
            df['Result'].iloc[i]=0
        else:
            df['Result'].iloc[i]=1",0.4082896411,
942,instructions for the assignment,"def func_treatment(data, head, year):

    pd.options.mode.chained_assignment = None                       # permit to avoid error in copying 
    
    cols = [head[1], head[2],head[3],head[4]]
    for c in cols:
        data[c] = pd.to_numeric(data[c], errors='coerce')           # cast the type str to numeric
        
    # Select a part of the dataframe where the year is > at year passed in the function 
    data = data[(data[head[3]].astype(int)<=datetime.now().year) & (data[head[3]].astype(int)>=year) ]
    data = data[data[head[4]]>0]
    # ---- Date creation --------------------------------------------
    #a = data[head[4]].apply(lambda x : x if x>0 else 1)             # Month can be = 0 when not found in the crawl 
    #data[head[4]]=a.values                                          # so, it's better to fix them at 1 
    a = data[head[4]].apply(lambda x : str(x) if x>=10 else '0'+str(x))# string format and add a 0 when the month is 
    data[head[4]]=a.values                                          # < 10 (01, 02, 03...)
    
    date = data[head[3]].astype(str)+'-'+data[head[4]]              # concat the year-month to generate the Date
    data['Date'] = date.values                                      # .astype() permit to cast a type to another     
                                                                    # passed in the () 
    data_month = [i.split('/') for i in data.data_month]
    data_month = [i[2]+'-'+i[1] for i in data_month]
    data.data_month=data_month
    #print(data.data_month[:5])
    #print(data.data_month.iat[0],'%Y-%m')
    size = len(data)
    date_delta=[datetime.strptime(data.data_month.iat[i],'%Y-%m')-datetime.strptime(data.Date.iat[i],'%Y-%m') \
              for i in range(size)]
    #print(date_delta[0].days/30)
    date_age=[round(i.days/30) for i in date_delta] #round
    data['Age_months']=date_age
    data = data[data.Age_months>=0]
    # ---- Data cleaning --------------------------------------------
    a = data[head[0]].fillna(' ')
    data[head[0]] = a.values
    a = data[head[1]].apply(lambda x : x if x>0 else 0 )
    data[head[1]] = a.values
    
    return data",0.3942468762,
942,instructions for the assignment,"def func_treatment(data, head, year):

    pd.options.mode.chained_assignment = None                       # permit to avoid error in copying 
    
    cols = [head[1], head[2],head[3],head[4]]
    for c in cols:
        data[c] = pd.to_numeric(data[c], errors='coerce')           # cast the type str to numeric
        
    # Select a part of the dataframe where the year is > at year passed in the function 
    data = data[(data[head[3]].astype(int)<=datetime.now().year) & (data[head[3]].astype(int)>=year) ]
    data = data[data[head[4]]>0]
    # ---- Date creation --------------------------------------------
    #a = data[head[4]].apply(lambda x : x if x>0 else 1)        # Month can be = 0 when not found in the crawl 
    #data[head[4]]=a.values                                          # so, it's better to fix them at 1 
    a = data[head[4]].apply(lambda x : str(x) if x>=10 else '0'+str(x))# string format and add a 0 when the month is 
    data[head[4]]=a.values                                          # < 10 (01, 02, 03...)
    
    date = data[head[3]].astype(str)+'-'+data[head[4]]  # concat the year-month to generate the Date
    data['Date'] = date.values                                      # .astype() permit to cast a type to another     
                                                                    # passed in the () 
    # ---- Create the age to compare --------------------------------    
    
    data_month = [i.split('/') for i in data.data_month]
    data_month = [i[2]+'-'+i[1] for i in data_month]
    data.data_month=data_month
    size = len(data)
    date_delta=[datetime.strptime(data.data_month.iat[i],'%Y-%m')-datetime.strptime(data.Date.iat[i],'%Y-%m') \
              for i in range(size)]
    #print(date_delta[0].days/30)
    date_age=[round(i.days/30) for i in date_delta] #round
    data['Age_months']=date_age
    data = data[data.Age_months>=0]
    # ---- Data cleaning --------------------------------------------
    a = data[head[0]].fillna(' ')
    data[head[0]] = a.values
    a = data[head[1]].apply(lambda x : x if x>0 else 0 )
    data[head[1]] = a.values
    
    return data",0.3942468762,
942,instructions for the assignment,"assignCount = 0

def genAssign(x, y):
    """"""Assume x is Var, generate x := y""""""
    global assignCount, regs
    if type(y) == Cond:
        #putB(condOp(negate(y.cond)), y.left, y.right, y.labA[0])
        put('cmp', y.left, y.right)
        putInstr(condOp(negate(y.cond)) + "" "" + y.labA[0])
        releaseReg(y.left); releaseReg(y.right); r = obtainReg()
        putLab(y.labB); put('mov', r, '#1') # load true
        lab = '.A' + str(assignCount); assignCount += 1
        putInstr('B ' + lab)
        putLab(y.labA); put('mov', r, '#0') # load false 
        putLab(lab)
    elif type(y) != Reg: 
        y = loadItem(y); r = y.reg
    else: r = y.reg
    # Need to load x to reg because of label
    if x.reg == ZR:
        # TODO: Int's here mean stack var
        #print(x.adr)
        s = loadAddressOfLabel(x.adr)
    else:
        s = x.reg
    #print(x.reg)
    #putInstr('str ' + r + ', [x' + s[1:] + ']')
    if x.reg == FP:
        putM('str', r, s, x.adr)
    else:
        putM('str', r, s)
    releaseReg(s) 
    releaseReg(r)",0.3876352906,
942,instructions for the assignment,"# change the bel
def get_colours(color_id):
    global assignation
    colors = ['red','blue','green','orange','black','yellow']
    #red = X_norm[np.where(np.array(assignation)[:,0]>0.5)]
    pass",0.3859205842,
942,instructions for the assignment,"def reject_bad_assignments(rejected_assignment_ids, rejected_worker_ids):
    for assignment in rejected_assignment_ids:
        mturk.reject_assignment(assignment)
        
        rejection_message_subject = ""One of your HITs was rejected""
        
        rejection_message = """"""
        Your HIT was rejected because it was either incomplete or largely incorrect.
        """"""
        
        mturk.notify_workers(rejected_worker_ids, rejection_message_subject, rejection_message)",0.3843300939,
942,instructions for the assignment,"def genProcStart():
    global curlev, declarations, instructions
    declarations.append( [] )
    instructions.append( [] )
    fieldVars.append( {} )
    curlev = curlev + 1",0.3798495233,
295,constants_sequences_and_random_values,"import string
import random 

# Pro tip: check contents of the string class and random module capabilities

def generate_passwd():

    NUM_LOWER_LETTERS = 4
    NUMBERS = 3
    SPECIALS = 2

    # Select random capital letter
    capital_letter = random.choice(string.ascii_uppercase)
    
    # Select random lower-case letters
    lower_letters = random.choices(string.ascii_lowercase, k=NUM_LOWER_LETTERS)
    
    # Select random numbers and turn them into ints
    numbers = [str(n) for n in random.sample(range(0, 10), NUMBERS)]
    
    # Select random special characters
    specials = random.choices(string.punctuation, k=SPECIALS)

    # Join all elements into one sequence
    all_elems = [capital_letter] + lower_letters + numbers + specials

    # Change order of elements
    random.shuffle(all_elems)

    # Join them all and return as string
    return ''.join(all_elems)


generate_passwd()",0.4546310902,
295,constants_sequences_and_random_values,"def test_n_n_times():
    assert n_n_times(5) == ""1223334444""
    assert n_n_times(2) == ""1""
    assert n_n_times(3) == ""122""",0.454122901,
295,constants_sequences_and_random_values,"%%time
def problem80():
    getcontext().prec = 102
    s = 0
    for i in range(1, 101):
        if i not in [y**2 for y in range(1, 11)]:
            s += sum([int(x) for x in str(Decimal(i).sqrt())[:101]
                      if x != '.'])
    return s

print(problem80())",0.4540955424,
295,constants_sequences_and_random_values,"def test_every_other_num():
    assert every_other_num(1, 10000) == [x for x in range(1, 10001, 2)]
    assert every_other_num(4, 15) == [x for x in range(4, 16, 2)]
    assert every_other_num(1, 8) == [x for x in range(1, 9, 2)]",0.4473067224,
295,constants_sequences_and_random_values,"# Mutation takes a genome and alters 
# its genes to create a new solution instance
def mutate_genome(genome): 
    
    # This mutatation adds a value between 
    # -0.01 and 0.01 to the genome's genes
    genome.genes += random.uniform(-0.01, 0.01)",0.4430379272,
295,constants_sequences_and_random_values,"def test_factorial():
    assert factorial(0) == 1
    assert factorial(5) == 120
    assert factorial(3) == 6",0.4418054223,
295,constants_sequences_and_random_values,"def random_individual():
    total_weight = 0
    
    #This is a list of the indexes from the item list
    #the individual has in its knapsack
    org_genotype = []
    
    while(total_weight < max_weight):
        random_item_index = random.randint(0, num_items-1)
        
        #Make sure this item isn't already in our knapsack.
        if random_item_index not in org_genotype:
            
            #But if this item would break our knapsack, 
            #lets just go ahead and call this organism done.
            if total_weight + items[random_item_index].weight > max_weight:
                return org_genotype
            
            #Otherwise we can add this item and keep going!
            else:
                org_genotype.append(random_item_index)
                total_weight += items[random_item_index].weight
    
    return org_genotype",0.441752851,
295,constants_sequences_and_random_values,"def add_two_and_five():
    two = 2
    five = 5
    return two + five",0.4404038191,
295,constants_sequences_and_random_values,"# An example of a generator function
def gen_func():
    print('First item')
    yield 10
    print('Second item')
    yield 20
    print('Third item')
    yield 30
    print('No more items left!')",0.4381877184,
295,constants_sequences_and_random_values,"def gen_chars():
    for i in range(97, 123):
        yield chr(i)
        
    for i in range(65, 91):
        yield chr(i)",0.435241282,
946,integer division,"#subtract function 
def subtract (number1, number2):
    answer = number1-number2
    return  answer
#floorDivide function
def floorDivide (number1, number2):
    answer = number1//number2
    return  answer
#divide function
def divideAnswer = number1/number2
    return  answer

# multiply function
return  answer
def multiply (number1, number2):
    answer = number1*number2
    
# getRemainder function
def getRemainder (number1, number2):
    answer = number1%number2
    return  answer

#power function
def power (number1, number2):
    answer = number1**number2
    return  answer",0.4300370216,
946,integer division,"def fibR(n):
 if n is 1 or n is 2:
  return 1
 return fib(n-1)+fib(n-2)
print(fibR(5))",0.4273553491,
946,integer division,10 * (1/0),0.426184237,
946,integer division,"# Or this one
10 * (1/0)",0.426184237,
946,integer division,5 == 10/2,0.4250037968,
946,integer division,3 * (1 / 3),0.4218785167,
946,integer division,"def square_root(x):
    return x**(1/2.)

print (square_root(2))
print (square_root(4))
print (square_root(16))",0.4204449058,
946,integer division,10 + 4 / 2,0.4201104045,
946,integer division,"# we can extract the mobile name from the device name using the missing data 

617/10000",0.419383347,
946,integer division,"# Using recursion
def fib(n):
    if n == 1 or n == 2:
        return 1
    return fib(n-1)+fib(n-2)


timer(lambda: fib(35))",0.4192201495,
2565,what is einstein notation?,"# Based on https://github.com/SeanNaren/deepspeech.pytorch/blob/master/decoder.py.
import Levenshtein  # https://github.com/ztane/python-Levenshtein/

def phoneme_error_rate(p_seq1, p_seq2):
    p_vocab = set(p_seq1 + p_seq2)
    p2c = dict(zip(p_vocab, range(len(p_vocab))))
    c_seq1 = [chr(p2c[p]) for p in p_seq1]
    c_seq2 = [chr(p2c[p]) for p in p_seq2]
    return Levenshtein.distance(''.join(c_seq1),
                                ''.join(c_seq2)) / len(c_seq2)",0.4675857425,
2565,what is einstein notation?,import Levenshtein,0.4668524265,
2565,what is einstein notation?,import Levenshtein as L,0.4636352658,
2565,what is einstein notation?,"from algebraixlib.mathobjects import Atom
peanut_butter = Atom(""peanut butter"")
jelly = Atom(""jelly"")",0.4627629519,
2565,what is einstein notation?,"import sympy as S
from sympy.stats import E, Bernoulli
xdata =[Bernoulli(i,p) for i in S.symbols('x:10')]
ph = sum(xdata)/float(len(xdata))
g = ph*(1-ph)",0.4587260485,
2565,what is einstein notation?,"from sympy import Function,solve,Derivative
y=Function(""y"")
y=y(x)",0.4549037218,
2565,what is einstein notation?,"import Levenshtein as l

l.distance('SMYTHE', 'SMITH')",0.452020973,
2565,what is einstein notation?,"#use jellyfish package is better
import Levenshtein as L

L.distance('Hello, World!', 'Hallo, World!')",0.4509356022,
2565,what is einstein notation?,"import Levenshtein as L

L.distance('Hello, World!', 'Hallo, World!')",0.4509356022,
2565,what is einstein notation?,from Levenshtein import distance,0.4505890012,
1041,law of averages,"def edge_length(fraction_of_observations, n_dimensions):
    return np.power(fraction_of_observations, 1/n_dimensions)",0.5636617541,
1041,law of averages,"def pi(k):
    return 1/np.power(k, alpha3)",0.5466979742,
1041,law of averages,"def fiterror(beta):
    return sum((y - f2(x, *beta))**2)",0.5465592146,
1041,law of averages,"def evalFunction(a,x):
    if x <= 0.0 :
        return 0.0
    else:
        return np.power(x,a)

#returns n nodes that form uniform intervals between the bounds specified (n-1 intervals)
def getUniformNodes(a,b,n):
    nodes = np.linspace(a,b,n)
    return nodes

#gets the approximated value of the function according to steps given by nodes
def getMidStepApprox(a,x,nodes):
    #Assume x in [a,b] of nodes
    
    nodeIndex = 0;
    while x > nodes[nodeIndex+1] :
        nodeIndex +=1
    
    end = -1
    if(nodeIndex >= len(nodes)-1):
        end=len(nodes)-1
    else:
        end=nodeIndex+1
        
    mid = (nodes[nodeIndex]+nodes[end]) / 2.0
    
    val = evalFunction(a,mid)
    return val",0.5405187607,
1041,law of averages,"def zc_bond_v0(face_v,r,t):
    return(face_v/np.power(1+r,t))",0.5394847989,
1041,law of averages,"def calc(n):
    return 1 - np.power((1-1/n),n)",0.5371125937,
1041,law of averages,"def fun1(a,x):
    if x <= 0:
        return 0
    else:
        return np.power(x,a)",0.5369257331,
1041,law of averages,"def ADF(r,n): 
    return ( ( 1 - np.power(1 + r ,-n)) / r )",0.536590457,
1041,law of averages,"def func_z_n(c, z_n):
    #return z_n*z_n +c
    return numpy.power(z_n,2) + c",0.5353614092,
1041,law of averages,"def ACF(r,n):
    return ( ( np.power(1 + r ,n) - 1) / r )",0.5331903696,
1332,orthogonal vectors,"def sent_vec_w2v(sent):
    wv_res = np.zeros(w2v.vector_size)
    ctr = 1
    for w in sent:
        if w in w2v:
            ctr += 1
            wv_res += w2v[w]
    wv_res = wv_res/ctr
    return wv_res",0.4329092801,
1332,orthogonal vectors,"def sent_vec(sent):
    wv_res = np.zeros(glove_model.vector_size)
    ctr = 1
    for w in sent:
        if w in glove_model:
            ctr += 1
            wv_res += glove_model[w]
    wv_res = wv_res/ctr
    #return (wv_res, ctr)
    return wv_res",0.4329092801,
1332,orthogonal vectors,"def upper_triangle_to_full_matrix(A):
    out = A.T + A
    np.fill_diagonal(out, np.diag(A))
    return out",0.4327687621,
1332,orthogonal vectors,"# Convert RGB to BGR. We need exchange R and B leaving G as it is.
def convertImage(BGR_array):
    imageNp = np.zeros_like(BGR_array)
    imageNp[:,:, 0] = BGR_array[:,:, 2]
    imageNp[:,:, 1] = BGR_array[:,:, 1]
    imageNp[:,:, 2] = BGR_array[:,:, 0]
    return imageNp

plt.imshow(convertImage(image))",0.430960238,
1332,orthogonal vectors,"def reconstruct(U, S, V, rank):
    return U[:,0:rank] * np.diag(S[:rank]) * V[:rank]",0.4295527339,
1332,orthogonal vectors,"def reconstruct(U, S, V, rank):
    return U[:,0:rank] * np.diag(S[:rank]) * V[:rank]

r = len(S) 
reconstruct(U, S, V, r)",0.427282691,
1332,orthogonal vectors,"def cosine_similarity(model):
    sim = model.item_vecs.dot(model.item_vecs.T)
    norms = np.array([np.sqrt(np.diagonal(sim))])
    return sim / norms / norms.T

als_sim = cosine_similarity(best_als_model)
sgd_sim = cosine_similarity(best_sgd_model)",0.4260490835,
1332,orthogonal vectors,"def symmetrize(a):
    return a + a.T - np.diag(a.diagonal())",0.4255290329,
1332,orthogonal vectors,"def BGR2RGB(Input):
   output = np.zeros(Input.shape);
   output[:,:,0] = Input[:,:,2] 
   output[:,:,1] = Input[:,:,1] 
   output[:,:,2] = Input[:,:,0]
   output = output.astype('uint8')
   return output
pic_ori = cv2.imread('./images/origin/Office.png')
pic_bil = cv2.imread('./Bi_Office.png')
pic_pro = cv2.imread('./Pr_Office.png')
# plot the image out
plt.figure(figsize=(50,32))
plt.subplot(311),plt.imshow(BGR2RGB(pic_ori))
plt.title('Original Image'), plt.xticks([]), plt.yticks([])
plt.subplot(312),plt.imshow(BGR2RGB(pic_bil))
plt.title('Bilinear Image'), plt.xticks([]), plt.yticks([])
plt.subplot(313),plt.imshow(BGR2RGB(pic_pro))
plt.title('Proposed Image'), plt.xticks([]), plt.yticks([])",0.4233611822,
1332,orthogonal vectors,"def BGR2RGB(Input):
   output = np.zeros(Input.shape);
   output[:,:,0] = Input[:,:,2] 
   output[:,:,1] = Input[:,:,1] 
   output[:,:,2] = Input[:,:,0]
   output = output.astype('uint8')
   return output",0.4198396206,
2218,stream exercises,"def run_one_episode(env):
    obs = env.reset()
    done = False
    episode_id = client.start_episode() # We start by requesting a new episode id from the server
    total_reward = 0
    while not done:
        action = ??? # TODO call get_action to get the action for the current observation
        obs, rew, done, info = env.step(action)
        ??? # TODO tell the server about the recent returns of the action
        if done:
            ??? # TODO tell the server the episode ended
        total_reward += rew
    print(""Episode reward"", total_reward)

run_one_episode(env)",0.4840175807,
2218,stream exercises,"while episode_number < 50000 and agent.steps < 1980000:
    episode_number += 1
    reward = run_episode(agent, render=False)
    reward_sum += reward
    
    if agent.steps >= last_batch_steps + BATCH_SIZE:
        reward_avg = reward_sum / (episode_number - last_batch_episode)
        last_batch_episode = episode_number
        last_batch_steps = int(agent.steps / BATCH_SIZE) * BATCH_SIZE
        episodes.append((episode_number, agent.steps, reward_avg))

        print('Episode: %d, steps: %d, epsilon: %f, average reward: %f.' \
              % (episode_number, agent.steps, agent.epsilon, reward_avg))
        
        if reward_avg > reward_best:
            reward_best = reward_avg
            agent.brain.model.save_model('best.mod')

        reward_sum = 0
       
agent.brain.model.save_model('last.mod')
print('Done.')",0.4675148129,
2218,stream exercises,"while episode_number < 30000 and agent.steps < 2980000:
    episode_number += 1
    reward = run_episode(agent, render=False)
    reward_sum += reward
    
    if agent.steps >= last_batch_steps + BATCH_SIZE:
        reward_avg = reward_sum / (episode_number - last_batch_episode)
        last_batch_episode = episode_number
        last_batch_steps = int(agent.steps / BATCH_SIZE) * BATCH_SIZE
        episodes.append((episode_number, agent.steps, reward_avg))

        print('Episode: %d, steps: %d, epsilon: %f, average reward: %f.' \
              % (episode_number, agent.steps, agent.epsilon, reward_avg))
        
        if reward_avg > reward_best:
            reward_best = reward_avg
            agent.brain.model.save_model('best.mod', False)

        reward_sum = 0
       
agent.brain.model.save_model('last.mod', False)
print('Done.')",0.4664639235,
2218,stream exercises,"def play_one(env, pmodel, gamma):
    observation = env.reset()
    done = False
    totalreward = 0
    iters = 0

    while not done and iters < 2000:
        # if we reach 2000, just quit, don't want this going forever
        # the 200 limit seems a bit early
        action = pmodel.sample_action(observation)
        # oddly, the mountain car environment requires the action to be in
        # an object where the actual action is stored in object[0]
        observation, reward, done, info = env.step([action])

        totalreward += reward
        iters += 1
    return totalreward


def play_multiple_episodes(env, T, pmodel, gamma, print_iters=False, status=False):
    totalrewards = np.empty(T)
    r = range(T)
    if status:
        r = tqdm_notebook(range(T), desc='Episodes'):
    for i in range(T):
        totalrewards[i] = play_one(env, pmodel, gamma)
        if print_iters:
            print(i, ""avg so far:"", totalrewards[:(i+1)].mean())

    avg_totalrewards = totalrewards.mean()
    print(""avg totalrewards:"", avg_totalrewards)
    return avg_totalrewards",0.4657971561,
2218,stream exercises,"def grab_data(server, port, scnl,T1,T2,fill_value=0):
	from obspy import Stream
	from obspy.clients.earthworm import Client

	st=Stream()
	client = Client(server, port)
	for sta in scnl:
		station = sta.split('.')[0]
		channel = sta.split('.')[1]
		network = sta.split('.')[2]
		location = sta.split('.')[3]
		print(station, channel, network, location, T1, T2)
		try:
			tr=client.get_waveforms(network, station, location, channel, T1, T2)
			if len(tr)==0:
				tr=create_trace(sta, T1, T2)
			else:
				if len(tr)>1:
					if fill_value==0 or fill_value==None:
						tr.detrend('demean')
						tr.taper(max_percentage=0.01)
					tr.merge(fill_value=fill_value)
				tr.trim(T1,T2,pad=0)
				tr.detrend('demean')
		except Exception as err:
			print(err)
			print(""No data found for ""+sta)
			tr=create_trace(sta, T1, T2)
		st+=tr
	print(st)
	return st

def create_trace(sta, T1, T2):
	from obspy import Trace
	from numpy import zeros
	tr=Trace()
	tr.stats['station']=sta.split('.')[0]
	tr.stats['channel']=sta.split('.')[1]
	tr.stats['network']=sta.split('.')[2]
	tr.stats['location']=sta.split('.')[3]
	tr.stats['sampling_rate']=100
	tr.stats['starttime']=T1
	tr.data=zeros(int((T2-T1)*tr.stats['sampling_rate']))
	return tr",0.4613743424,
2218,stream exercises,"while 1:
    play = input(""Play teleport (T), play zap (Z) or break (B) "")
    if play == ""T"" or play == ""t"":
        teleport.play()
    elif play == ""Z"" or play == ""z"":
        zap.play()
    elif play == ""B"" or play == ""b"":
        print(""break selected, breaking out..."")
        break
    else:
        print(""invalid selection, breaking out..."")
        break",0.4554483891,
2218,stream exercises,"for i in range(no_of_episodes):     
    
    # for each episode start the game
    game.new_episode()
    
    # loop until the episode is over
    while not game.is_episode_finished():
        
        # get the game state
        state = game.get_state()
        img = state.screen_buffer
        
        # get the game variables
        misc = state.game_variables
        
        # perform some action randomly and receuve reward
        reward = game.make_action(random.choice(actions))
        
        print(reward)
    
    # we will set some time before starting the next epiosde
    time.sleep(2)",0.4454951882,
2218,stream exercises,"while train_iter.epoch < max_epoch:

    train_batch = train_iter.next()
    image_train, target_train = concat_examples(train_batch) 
    #batch conversion function to change batch into image and target arrays suitable for the network

    prediction_train = model(image_train)

    loss = F.softmax_cross_entropy(prediction_train, target_train)
    train_losses.append(loss.data)

    model.cleargrads()
    loss.backward()

    optimizer.update()
    if train_iter.is_new_epoch:

        test_losses = []
        test_accuracies = []
        while True:
            test_batch = test_iter.next()
            image_test, target_test = concat_examples(test_batch) 

            prediction_test = model(image_test)

            loss_test = F.softmax_cross_entropy(prediction_test, target_test)
            test_losses.append(to_cpu(loss_test.data))

            accuracy = F.accuracy(prediction_test, target_test)
            accuracy.to_cpu()
            test_accuracies.append(accuracy.data)

            if test_iter.is_new_epoch:
                test_iter.epoch = 0
                test_iter.current_position = 0
                test_iter.is_new_epoch = False
                test_iter._pushed_position = None
                break
                #get out of test loop if test epoch completed

        mean_acc.append(np.mean(test_accuracies))
        mean_test_loss.append(np.mean(test_losses))
        mean_train_loss.append(np.mean(train_losses))
        #track mean losses for visualization later
        train_losses = []",0.4444012642,
2218,stream exercises,"def generateData(nameGenerator, numTokens, input_token_index,sequenceSize =64, batchSize = 2048,returnStrings=False,factor =0.05):
    while True:
        X = []
        Y = []
        for _ in range(batchSize):
            y = next(nameGenerator)
            
            X.append(pad(perturbeSentence(y,factor =factor)))
            Y.append(pad(y)[::-1])
            
        encoder_input_data = np.zeros(
            (len(X), sequenceSize, numTokens),
            dtype='float32')

        decoder_target_data = np.zeros(
            (len(Y), sequenceSize, numTokens),
            dtype='float32')
    
        for i, (input_text, target_text) in enumerate(zip(X, Y)):
            input_text = input_text[0:64]
            for t, char in enumerate(input_text):
                try:
                    encoder_input_data[i, t, input_token_index[char]] = 1.
                except KeyError:
                    pass # Padding
            for t, char in enumerate(target_text):
                # decoder_target_data is ahead of decoder_input_data by one timestep
                try:
                    decoder_target_data[i, t, input_token_index[char]] = 1.
                except KeyError:
                    pass # Padding
        if returnStrings:
            yield X,Y,encoder_input_data,decoder_target_data
        else:
            yield encoder_input_data,decoder_target_data",0.4441692233,
2218,stream exercises,"# crawling function
# @pg_limit:  max number of pages to crawl
# @max_qsize: max number of pages stored in the url_queue
# @headers:   the http GET request headers
# @params:    the http GET request params
# @interval:  interval between two consecutive requests
def crawl(self, pg_limit=100, 
    headers={}, params={},
    interval=0.1):
    #only download pg_limit number of web pages
    while(self.pg_cng < pg_limit):
        time.sleep(interval)
        if self.url_queue.qsize()>0:
            #get the first url in the queue
            current_url = self.url_queue.get() 
            try:
                #request web pages  
                response = requests.get(current_url, headers=headers, params=params)
                html = response.text
                self.process_page(html,current_url)
            except Exception as e:
                print(e)
            pass
        else:
            break",0.4417487681,
1640,problem prepare the data for modelling indicator variables point,"def grid_tune(param_test,scores):
    
    for score in scores:
        print(""# Tuning hyper-parameters for %s"" % score)
        print

        clf = GridSearchCV(LogisticRegression(penalty = 'l1', C=1,random_state = 2), param_test, cv=5,
                           scoring=score)
        smt=SMOTE(ratio=0.1, random_state=2, k=None, k_neighbors=5, 
          m=None, m_neighbors=10, out_step=0.5, kind='regular', svm_estimator=None, n_jobs=2)
        X_smt, y_smt=smt.fit_sample(train[predictors],train[response])
        X_smt=pd.DataFrame(X_smt)
        columns=train[predictors].columns
        X_smt.columns=columns
        
        clf.fit(X_smt[predictors], y_smt)

        print(""Best parameters set found on development set:"")
        print
        print(clf.best_params_,clf.best_score_)
        print
        print(""Grid scores on development set:"")
        print
        means = clf.cv_results_['mean_test_score']
        stds = clf.cv_results_['std_test_score']
        for mean, std, params in zip(means, stds, clf.cv_results_['params']):
            print(""%0.3f (+/-%0.03f) for %r""
                  % (mean, std * 2, params))",0.5130521059,
1640,problem prepare the data for modelling indicator variables point,"def parameter_sweep(tickers,p,params,N,progress = False):
    pnls = []
    sharpes = []
    ddwns = []
    new_params = []
    for i in range(N):
        if progress: print i
        a = min(params[i])
        b = max(params[i])
        if a==b: continue
        try:
            sig = calc_signals(tickers,p,a,b)
            pnl = calc_pnl(sig,p)
            pnls.append(pnl[-1])
            sharpes.append(calc_sharpe(pnl))
            ddwns.append(calc_ddwn(pnl))
            new_params.append([a,b])

        except:
            pnls.append(np.nan)
            sharpes.append(np.nan)
            ddwns.append(np.nan)
            new_params.append([a,b])

    return pnls,sharpes,ddwns,new_params",0.5012652874,
1640,problem prepare the data for modelling indicator variables point,"def parameter_sweep(tickers,p,params,N,progress = False):
    pnls = []
    sharpes = []
    ddwns = []
    new_params = []
    for i in range(N):
        if progress: print i
        a = params[i][0]
        b = params[i][1]
        if a==b: continue
        try:
            sig = calc_signals(tickers,p,a,b)
            pnl = calc_pnl(sig,p)
            pnls.append(pnl[-1])
            sharpes.append(calc_sharpe(pnl))
            ddwns.append(calc_ddwn(pnl))
            new_params.append([a,b])

        except:
            pnls.append(np.nan)
            sharpes.append(np.nan)
            ddwns.append(np.nan)
            new_params.append([a,b])

    return pnls,sharpes,ddwns,new_params",0.4997779131,
1640,problem prepare the data for modelling indicator variables point,"def steering_flip(df):
    df.loc[:, 'flipped'] = False
    new_df = df[df.steering != 0].sample(frac=0.99) 
    new_df.loc[:,'flipped'] = True
    new_df.loc[:, 'steering'] = new_df.steering * -1
    return pd.concat([df, new_df])

steering_combined = steering_flip(shift_df)
# reset index
steering_combined = steering_combined.reset_index(drop=True)

plt.figure(figsize=(18, 6))
steering_combined.steering.hist(bins=200)
plt.xlabel('Steering Angle')
plt.ylabel('Counts')
plt.show()
print(""Total Data"", len(steering_combined.steering))
print(""Total Zeros:"",len(steering_combined[steering_combined.steering == 0]))
display(steering_combined.head(3))
display(steering_combined.tail(3))",0.495095253,
1640,problem prepare the data for modelling indicator variables point,"def retrain_model(new_embs):
    """"""
    Repeat the steps above with a new set of word embeddings.
    """"""
    global model, embeddings, name_sentiments
    embeddings = new_embs
    pos_vectors = embeddings.loc[pos_words].dropna()
    neg_vectors = embeddings.loc[neg_words].dropna()
    vectors = pd.concat([pos_vectors, neg_vectors])
    targets = np.array([1 for entry in pos_vectors.index] + [-1 for entry in neg_vectors.index])
    labels = list(pos_vectors.index) + list(neg_vectors.index)

    train_vectors, test_vectors, train_targets, test_targets, train_labels, test_labels = \
        train_test_split(vectors, targets, labels, test_size=0.1, random_state=0)
        
    model = SGDClassifier(loss='log', random_state=0, n_iter=100)
    model.fit(train_vectors, train_targets)
    
    accuracy = accuracy_score(model.predict(test_vectors), test_targets)
    print(""Accuracy of sentiment: {:.2%}"".format(accuracy))
    
    name_sentiments = name_sentiment_table()
    ols_model = statsmodels.formula.api.ols('sentiment ~ group', data=name_sentiments).fit()
    print(""F-value of bias: {:.3f}"".format(ols_model.fvalue))
    print(""Probability given null hypothesis: {:.3}"".format(ols_model.f_pvalue))
    
    # Show the results on a swarm plot, with a consistent Y-axis
    plot = seaborn.swarmplot(x='group', y='sentiment', data=name_sentiments)
    plot.set_ylim([-10, 10])",0.4922671914,
1640,problem prepare the data for modelling indicator variables point,"def models(clf, title, name):
    start = time.time()
    print clf
    print """"
    # fit the model
    clf.fit(X_train, y_train)
    # make predictions
    clf.predict(X_test)
    # summarize the fit of the model
    score = clf.score(X_train, y_train)
    print ""Classification score using train set: {}\n"".format(str(score))
    score = clf.score(X_test, y_test)
    print ""Classification score using test set: {}\n"".format(str(score))
    print""=""*66
    print """"
    expected = y
    predicted = clf.predict(X)
    # print confusion matrix
    cm = metrics.confusion_matrix(expected, predicted)
    target_names = ['bad', 'good']
    fig, ax = plt.subplots()
    sns_cm = sns.heatmap(cm, annot=True, fmt='', xticklabels=target_names, yticklabels=target_names , ax=ax)
    title = ""Apple Tweets Training Dataset - "" + title
    ax.set_title(title, y=1.08, fontdict=font)
    #fig.tight_layout()
    #fig.savefig(name, bbox_inches='tight')
    fig.show()
    # train the model with whole training dataset now
    clf.fit(X, y)
    end = time.time()
    total = end-start
    minutes = total//60.0
    seconds = total%60.0
    print ""Total running time for this model = {} seconds ({} minutes {} seconds)"".format(total, minutes, seconds)
    print """"
    return",0.4892439246,
1640,problem prepare the data for modelling indicator variables point,"def model_param_search(df, features, target, classifier, param_grid):
    """"""Test our various models""""""
    
    # split our data into train and test split, set random_state=1 so we can repeat analysis
    train, test = train_test_split(df, test_size=0.3) 
           
    # assign our features and target variables
    X_test = test[features]
    y_test = test[target]    
    X_train = train[features]
    y_train = train[target]
    
    #scale the features
    min_max_scaler = MinMaxScaler()
    X_train = min_max_scaler.fit_transform(X_train) 
    X_test = min_max_scaler.transform(X_test)

    # Set the parameters by cross-validation
    param_grid = param_grid
    
    scores = ['precision', 'recall']

    for score in scores:
        print ""# Tuning hyper-parameters for %s"" % score
        
        clf = GridSearchCV(classifier, param_grid, cv=15,
                       scoring='%s_macro' % score)
        clf.fit(X_train, y_train)

        print ""Best Accuracy: %.2f%%"" % clf.best_score_
        print ""Best parameters set found on development set:""
        print  
        print clf.best_params_
     
        y_true, y_pred = y_test, clf.predict(X_test)
        print ""Detailed classification report:""
        print """"
        print ""The model is trained on the full development set.""
        print ""The scores are computed on the full evaluation set.""
        print """"
        print classification_report(y_true, y_pred), ""\n\n""",0.4865329862,
1640,problem prepare the data for modelling indicator variables point,"def q_minus_one(row):
    r""""""Subtract 1 from q.""""""
    sol = row.data[row.index[0]]
    xc = sol.state.grid.p_centers[0] + 0
    q = sol.state.q[0,:] - 1
    return xc, q",0.4863955081,
1640,problem prepare the data for modelling indicator variables point,"def get_column_info(df,column):
    # print general info about column
    print df[column].describe()
    
    # show histogram amd box plot
    plt.figure()
    df[df.TARGET==0][column].plot(kind=""hist"", label=""satisfied customers"")
    df[df.TARGET==1][column].plot(kind=""hist"", label=""unsatisfied customers"")
    plt.legend();
    
    plt.figure()
    df[df.TARGET==0][column].plot(kind=""kde"", label=""satisfied customers"")
    df[df.TARGET==1][column].plot(kind=""kde"", label=""unsatisfied customers"")
    plt.legend();
    
    plt.figure()
    df[column].plot(kind='box')",0.4838326871,
1640,problem prepare the data for modelling indicator variables point,"# This is a modified version of function created by Mez Gebre
# https://mez.github.io/deep%20learning/2017/02/14/mainsqueeze-the-52-parameter-model-that-drives-in-the-udacity-simulator/
def shift_img_augmentation(df):
    df.loc[:,'random_shift'] = 0
    new_df = df[df.steering != 0].copy()
    df.loc[:,'is_shift'] = False
    new_df.loc[:,'is_shift'] = True
    
    
    max_shift = 30
    max_ang = 0.12
    
    def row_shift_update(row):
        random_shift = np.random.randint(-max_shift, max_shift + 1)
        row.random_shift = random_shift
        updated_steer = row.steering + (random_shift / max_shift) * max_ang
        if abs(updated_steer) > 1:
            updated_steer = -1 if (updated_steer < 0) else 1

        row.steering = updated_steer
        return row

    new_df = new_df.apply(row_shift_update, axis=1)
    return pd.concat([df, new_df])

# Test it
new_drive_log = shift_img_augmentation(new_drive_log)
plotSteering(new_drive_log, figsize=[14,5])
new_drive_log.tail()",0.4825683236,
705,function get_version,"def get_user_age():
    ""Function that asks user for their age. If input is invalid, user is prompted to try again""
    try:
        # Get age from user input - if conversion to int fails Python raises an exception
        age = int(input('How old are you? '))

        # Conversion to int has been successful, but we need to check that age is positive. Raise
        # exception if age is less than 0
        if age < 0:
            raise ValueError(""Age must be a positive integer"")

        return age
    except:
        # Getting age from user input unsuccessful, so print message
        print(""Invalid age entered. Please try again"")

        # Prompt user again to input age
        return get_user_age()

    
# Uncomment the below lines to test
age = get_user_age()
print(age)",0.4784979522,
705,function get_version,"def main():
  # If the training and test sets aren't stored locally, download them.
  if not os.path.exists(IRIS_TRAINING):
    raw = urlopen(IRIS_TRAINING_URL).read()
    with open(IRIS_TRAINING, ""wb"") as f:
      f.write(raw)

  if not os.path.exists(IRIS_TEST):
    raw = urlopen(IRIS_TEST_URL).read()
    with open(IRIS_TEST, ""wb"") as f:
      f.write(raw)

main()",0.4779914021,
705,function get_version,"def main():
  # If the training and test sets aren't stored locally, download them.
  if not os.path.exists(IRIS_TRAINING):
    raw = urlopen(IRIS_TRAINING_URL).read()
    with open(IRIS_TRAINING, ""wb"") as f:
      f.write(raw)

  if not os.path.exists(IRIS_TEST):
    raw = urlopen(IRIS_TEST_URL).read()
    with open(IRIS_TEST, ""wb"") as f:
      f.write(raw)",0.4779914021,
705,function get_version,"def generate_noise():
    """"""Adds noise to a copy of the training data set.""""""

    NOISE_FILE = os.path.join(PREPROCESSED_DIR, ""noise.p"")
    NOISE_LEVEL = 30

    try:
        with open(NOISE_FILE, mode='rb') as f:
            X_train_noise = pickle.load(f)
    except:
        print(""Generating noise..."")
        X_train_noise = np.float32(X_train_src.copy())
        noise = np.random.uniform(low=-NOISE_LEVEL, high=NOISE_LEVEL, size=X_train_noise.shape)

        X_train_noise = X_train_noise + noise
        X_train_noise = np.minimum(np.maximum(0, X_train_noise), 255)
        X_train_noise = X_train_noise.astype(np.uint8)

        with open(NOISE_FILE, mode='wb') as f:
            pickle.dump(X_train_noise, f)

    return X_train_noise, y_train_src",0.4779116213,
705,function get_version,"def get_image_array(patientId=None):
    image_file = os.path.join(train_image_dir, patientId + '.dcm')
    img_info = pydicom.read_file(image_file)
    img = img_info.pixel_array
    img_arr = np.stack([img] * 3, axis=2)
    return img_arr",0.4775822759,
705,function get_version,"def MDR():
    num=input(""enter a number:"")
    
    while(len(num)>1):
        num=str(prodDigits(num))
    
    return int(num)


def  MPersistence():
    
    num=input(""enter a number:"")
    count=0
    while(len(num)>1):
        num=str(prodDigits(num))
        count+=1
    
    return count",0.476277113,
705,function get_version,"def get_user_age():
    ""Function that asks user for their age. If input is invalid, user is prompted to try again""
    try:
        # Get age from user input - if conversion to int fails Python raises an exception
        age = int(input('How old are you? '))

        # Conversion to int has been successful, but we need to check that age is positive. Raise
        # exception if age is less than 0
        if age < 0:
            raise ValueError(""Age must be a positive integer"")

        return age
    except:
        # Getting age from user input unsuccessful, so print message
        print(""Invalid age entered. Please try again"")

        # Prompt user again to input age
        return get_user_age()

    
# Uncomment the below lines to test
# age = get_user_age()
# print(age)",0.4762588739,
705,function get_version,"def getRiskFreeRate():
    '''getRiskFreeRate function.
    description: This fuction returns the risk-free interest rate (cetes) for 28,91,182 days.
    
    ---- inputs
     
    
    ---- outputs
    
    
    '''
    
    # import libraries
    import urllib.request
    
    # open url to get source
    with urllib.request.urlopen('http://www.banxico.org.mx/SieInternet/'+
                                'consultarDirectorioInternetAction.do?a'+
                                'ccion=consultarCuadro&idCuadro=CF107&s'+
                                'ector=22&locale=es') as response:
        
        # read source and save as string 
        html_source = response.read().decode('latin-1')
    
    
    # identify target
    def getTarget(source):
        '''getTarget function
        description: function adapted to retrieve the value of cetes interest rate. 
        
        ---- inputs
        source: 
        
        ---- outputs
        position_index: 
        value: 
        
        '''
        
        tasa_de_rendimiento = source.find('Tasa de rendimiento')
        visibility_hidden   = 0
        
        for i in range(3):
            visibility_hidden += source[tasa_de_rendimiento:][visibility_hidden:].find('<span style=""visibility:hidden"">')+34
            
        position_index = tasa_de_rendimiento + visibility_hidden - 10 - 34
        value          = float(source[position_index:position_index+10].strip(' '))
        return position_index, value

    
    # get key,values and save in dictionary 
    cetes_dictionary = {}
    reference_index  = 0
    
    for i in [28, 91, 182]:
        html_source            = html_source[reference_index:]
        reference_index, value = getTarget(html_source)
        cetes_dictionary[i]    = value
    
    
    return cetes_dictionary",0.4759548903,
705,function get_version,"def downloadGTZAN():
    filename = 'genres.tar.gz'
    if not os.path.exists(filename):
        filename, _ = urlretrieve('http://opihi.cs.uvic.ca/sound/genres.tar.gz', filename)
    else:
        print('File ' + filename + ' exists')
    
    return filename
filename = downloadGTZAN()",0.4745544791,
705,function get_version,"def fahrenheit_to_celsius3():
    """""" MORE IMPROVED. Does even more checking of input before using it. 
    Input from keyboard, which is always a string and must often be
    converted to an int or float. 
    Converts Fahrenheit temp to Celsius.
    Uses if to check whether input is a number and then uses .isdigit() method 
    of strings to check whether input is made of of digits. 
    """"""
        
    temp_str = input(""Enter a Fahrentheit temperature: "")
    if temp_str:
        if temp_str.isdigit():  
            temp = int(temp_str)
            newTemp = 5*(temp-32)/9
            print(""The Fahrenheit temperature"",temp,""is equivalent to "",end='')
            print(newTemp,""degrees Celsius"")
        else:
            print(""You must enter a number. Bye"")",0.4732806683,
2253,survey,"class Student():
    def __init__(self, courses, age, sex):
        """"""
        What the class should do when it 
        is used to create an instance
        """"""
        self.courses = courses
        self.age = age
        self.sex = sex

    def have_a_birthday(self):
        self.age += 1",0.4923900068,
2253,survey,"def processSurvey(svyID, tblPatterns, tblVars, allTblIdCols, masterTable, outFile, verbose = True ):
    # this creates the new temporary db
    db = sqlite3.connect(':memory:')
    cursor = db.cursor()
    srcTableInfos = {}
    
    for tblName, tblCols in tblVars.iteritems():
        tblIdCols = allTblIdCols[tblName]
        # Load one table of this survey into the database
        # Find the individual file required
        for tblPattern in tblPatterns:
            tblFiles = glob.glob(tblPattern.format(svyID, tblName))
            if len(tblFiles) > 0:
                break
        if len(tblFiles) != 1:
            print (""Survey ""+str(svyID)+"" table ""+tblName+"" does not exist or is not well specified!"")
            continue
        print tblName +""... "",
        tblFile = tblFiles[0]
        with open(tblFile) as tbl:
            reader = csv.DictReader(tbl)
            
            # Create a tableinfo object which will handle building the sql necessary
            # for interacting with this table in the database
            srcTable = TableInfo(tblName, tblIdCols, tblCols)
            if (skipDB):
                # For debugging of TableInfo
                continue
                
            # Get the sql to create the table in the database
            createSql = srcTable.GetCreateTableSQL()
            orderedCols = srcTable.AllColumns()
            
            # Get the ""insert into xx(yy,bb) VALUES..."" part of the SQL to populate the
            # data into the DB from the CSV reader
            insertSql = srcTable.GetInsertSQLTemplate()
            
            # Read the data from the CSV file into a list of lists, each correctly 
            # ordered for the columns that are in the DB table insert statement.
            # Use value ""N/A"" for any columns that are not present in this survey
            data = [([row.get(i, 'N/A') for i in orderedCols]) for row in reader ]
            
            # if an incoming CSV table has none of the columns we asked for except IDs
            # (e.g. we only wanted some non-standard survey specific columns and this survey 
            # doesn't have them)
            # then don't just include its ID columns, just skip dealing with it altogether
            gotData = False
            for i in data:
                if i.count('N/A') < (len(i) - len(tblIdCols)):
                    gotData = True
                    break
            if skipBlanks and not gotData:
                print ""Skipping table {0} as none of the required cols are present"".format(
                    tblName)
                continue
            
            # otherwise save the tableinfo
            srcTableInfos[tblName] = srcTable
            # and create and populate the table into the db
            cursor.execute(createSql)
            cursor.executemany(insertSql, data)
            # and create indexes in the DB on the relevant join columns
            idxSql = srcTable.GetCreateIndexSQL()
            cursor.executescript(idxSql)
        db.commit()
    
    # Now the in-memory database is populated for this survey we can continue.
    # Get a list of all the table names, but with the master table"" 
    # - i.e. the left one on the left outer join - at the start of 
    # the list as required by MultiTableJoiner and the rest sorted after
    tblNames = [i for i in sorted(srcTableInfos) if i != masterTable]
    if masterTable in srcTableInfos:
        tblNames[0] = masterTable
    else:
        print ""Warning: requested master table {0} isn't present! Join may fail!"".format(masterTable)
    
    if (len(tblNames)) == 0:
        print ""Nothing for survey "" + str(svyID)
        return
    
    # Note that we also don't actually check here if the join is appropriate. 
    # For example from a Child master table we can join to its parents table and the household 
    # table. But we shouldn't do the reverse as for each household there are many children.
    # If we tried, we'd get repeated rows (probably) on the left join.
    # If there was more than one such table then we would get an exploding number of rows.
    # The table joiner code makes some basic effort to check this based on the number and length
    # of join columns.
    
    # Instantiate the object to write the join SQL
    multi = MultiTableJoiner(""outputTbl"", [srcTableInfos[n] for n in tblNames] )
    # and get the SQL. This is just returning the SQL string, not running it.
    # Use GetCreateIntoSQL(QualifyFieldNames=True) to name output fields like 
    # RECH2_HV270 rather than just HV270
    joinEmAllSQL = multi.GetCreateIntoSQL(QualifyFieldNames=True)
   
    # One-off bodge for Donal's data where we want to join the household schedule table 
    # to the child table. This uses a different join column in the child tables, i.e. normally 
    # child tables are joined to mother tables based on BIDX, but the child tables also contain a column 
    # for household-schedule joining called B16. Thanks to that unhelpful name (REC21.B16) there is 
    # no way that I can see of automatically inferring this from the .DCF specification files.
    #joinEmAllSQL = joinEmAllSQL.replace (
    #    'LEFT JOIN RECH1 ON substr(REC21.CASEID, 1, length(REC21.CASEID)-3) = RECH1.HHID and REC21.BIDX = RECH1.HVIDX',
    #    'LEFT JOIN RECH1 ON substr(REC21.CASEID, 1, length(REC21.CASEID)-3) = RECH1.HHID and REC21.B16 = RECH1.HVIDX'
    #)
    
    if isDebug:
        print joinEmAllSQL
        return
    
    # now execute the SQL, thus creating outputTbl in the in-memory database
    cursor.execute(joinEmAllSQL)
    # and dumpy the results out to CSV
    cursor.execute(""select * from outputTbl"")
    colNames = [description[0] for description in cursor.description]
    
    # TODO a given column should always appear in the same table but occasionally 
    # this is not the case. So we have to specify in the input file all the places it 
    # could come from, which will generate multiple columns in the output.
    # e.g. some surveys have HV270 in RECH3 rather than RECH2 and so we need to specify 
    # both if we are running for all surveys.
    # Ideally we would check here for these duplicates and write out only the one which 
    # doesn't have ""N/A"" in the values. But that would need to inspect each row, and thus 
    # would be much slower. So for now it's best to just use the QualifyFieldNames option 
    # above to ensure they all get written somehow.
    with open(outFile, ""wb"") as f:
        writer = UnicodeWriter(f)
        writer.writerow(colNames)
    #print """"
        writer.writerows(cursor)
    db.close()",0.4915262759,
2253,survey,"def star_wars_rankings(col_name, new_col_name):
    global star_wars_survey
    star_wars_survey[col_name] = star_wars_survey[col_name].astype(float)
    star_wars_survey = star_wars_survey.rename(columns={col_name:new_col_name})
    return

for i in np.arange(9,15): 
    star_wars_rankings(star_wars_survey.columns[i], ""ranking_""+str(i-8))

star_wars_survey.iloc[:,9:15].head()",0.4892735481,
2253,survey,"def grid_search(essays, expected_tags):

    rows_ana = []
    proc_essays = processed_essays_use_predicted_tag(essays=essays)

    metrics = get_metrics_raw(proc_essays, expected_tags=expected_tags,  micro_only=True)
    row = metrics[""MICRO_F1""]
    rows_ana.append(row)

    df_results = pd.DataFrame(rows_ana)
    return df_results",0.4839758873,
2253,survey,"def xgb_draw_importance_features(model, df, importance_type='gain'):

    fscore = model.get_booster().get_score(importance_type=importance_type) #cover, gain, weight
    maps_name = dict([ (""f{0}"".format(i), col) for i, col in enumerate(df.columns)])

    impdf = pd.DataFrame([ {'feature': maps_name[k], 'importance': fscore[k]} for k in fscore ])
    impdf = impdf.sort_values(by='importance', ascending=False).reset_index(drop=True)
    impdf['importance'] /= impdf['importance'].sum()
    impdf.index = impdf['feature']

    impdf.plot(kind='bar', title='{0} - Importance Features'.format(importance_type.title()), figsize=(6, 5))",0.4815223217,
2253,survey,"import numpy as np

def star_wars_films(film_name, col_name, new_col_name):
    global star_wars_survey
    true_false={ \
        film_name : True, \
                None : False \
    }
    star_wars_survey[col_name] = star_wars_survey[col_name].map(true_false)
    star_wars_survey = star_wars_survey.rename(columns={col_name:new_col_name})
    return

for i in np.arange(3,9): 
    star_wars_films(star_wars_survey.iloc[:,i].value_counts().index[0], star_wars_survey.columns[i], ""seen_""+str(i-2))

print(star_wars_survey.head())",0.4814917147,
2253,survey,"def r2_in_days(target, pred, n_days):
    '''Evaluate R2 scores in different days
    Args:
        target, pred (array-like): targets and predictions for evaluation.
        n_days: time horizon in number of days for evaluation.
    Returns: a list of R2 scores.
    '''
    from sklearn.metrics import r2_score
    r2s = []
    for d in n_days:
        if d == 'all':
            r2s.append(r2_score(target, pred))
        else:
            r2s.append(r2_score(target[:d], pred[:d]))
    return r2s",0.479359448,
2253,survey,"def run_flat_surface_weathering(params, niter):
    """"""Run weathering and disturbance starting with a flat surface.""""""
    import grain_hill_as_class

    # instantiate a GrainHill model
    num_rows = params['number_of_node_rows']
    num_cols = params['number_of_node_columns']
    grain_hill = grain_hill_as_class.GrainHill((num_rows, num_cols), **params)
    
    dt = params['run_duration'] / float(niter)

    soil_thickness = np.zeros(niter + 1)
    
    for i in range(niter):
        
        grain_hill.run_for(dt)

        # compute and write the results
        (elev_profile, soil) = grain_hill.get_profile_and_soil_thickness(grain_hill.grid, 
                                                                 grain_hill.ca.node_state)
        #plot_hill(grain_hill.ca.grid)
        soil_thickness[i+1] = np.mean(soil)

    return np.arange(0.0, dt * niter + 2, dt), soil_thickness",0.4776122868,
2253,survey,"def bootstrap_statistic(stat_function, y_test, y_hat):
    #Setting random seed ensures paired samples
    np.random.seed(3743)
    N = 10**3
    result = np.empty(N)
    for n in xrange(N):
        I = np.random.choice(len(y_test), len(y_test))
        result[n] = stat_function(y_test[I], y_hat[I])
        
    return result",0.4768852293,
2253,survey,"def kmeans_pp(data, k, weights):
    ''' 
        Function to return final centers for the using k-means++ clustering algorithm
        Input: data, an array of data. k, the number of clusters. weights, weight for each centroid
        Output: C, an array with length k of initial cluster centers. 
    '''
    np.random.seed(663)
    first_random = np.random.choice(data.shape[0], 1)
    C = data[first_random, :]
    
    for i in range(k-1):
        cdist = (distance(data, C))**2
        cdist_min = np.min(cdist, axis = 1)* weights
        prob = cdist_min/np.sum(cdist_min)
        np.random.seed(663)
        new_center = np.random.choice(data.shape[0],1, p=prob)
        C = np.vstack([C, data[new_center,:]])
        
    return C",0.4764870405,
1441,part predicting a categorical response,"def getRa(X, obj):
    ra = obj.predict_proba(X)[:,1]
    ra[ra==1.0] -= 0.0001
    return(ra)",0.5543439984,
1441,part predicting a categorical response,"sum(our_linear_svm.dual_coef_[0] * (our_linear_svm.coef_[0,0] * our_linear_svm.support_vectors_[:,1] - 
                                our_linear_svm.coef_[0,1] * our_linear_svm.support_vectors_[:,0]))",0.5479606986,
1441,part predicting a categorical response,"for model_key in models:
    pyint_model = InMemoryModel(models[model_key].predict_proba, examples=X_train, target_names=['p(Cancer)-malignant', 'p(No Cancer)-benign'])
    axes_list = interpreter.partial_dependence.plot_partial_dependence(['worst area'],
                                                                       pyint_model, 
                                                                       grid_resolution=30, 
                                                                       with_variance=True,
                                                                       figsize = (10, 5))
    ax = axes_list[0][1]
    ax.set_title(model_key)
    ax.set_ylim(0, 1)",0.5471110344,
1441,part predicting a categorical response,"for model_key in models:
    pyint_model = InMemoryModel(models[model_key].predict_proba, examples=X_train, target_names=['p(No Cancer)', 'p(Cancer)'])
    axes_list = interpreter.partial_dependence.plot_partial_dependence(['worst area'],
                                                                       pyint_model, 
                                                                       grid_resolution=30, 
                                                                       with_variance=True,
                                                                       figsize = (10, 5))
    ax = axes_list[0][1]
    ax.set_title(model_key)
    ax.set_ylim(0, 1)",0.5471110344,
1441,part predicting a categorical response,"def get_proba(X, clf):
    return pd.DataFrame(clf.predict_proba(X)[1])
print(X_train.shape, get_proba(X_train, stacking).shape)
print(get_proba(X_train, stacking))
stacking = stacking_2
stacking.fit(X_train, y_train)

print(""Do Nothing roc-auc on train"", roc_auc_score(y_train, get_proba(X_train, stacking)))
print(""Do Nothing roc-auc on test"", roc_auc_score(y_test, get_proba(X_test, stacking)))

stacking.fit(X_train_rus, y_train_rus)
print(""\nstacking on RandomUnderSampled set:"", roc_auc_score(y_train_rus, get_proba(X_train_rus, stacking)))
print(""stacking on RandomUnderSampled test set:"", roc_auc_score(y_test, get_proba(X_test_rus, stacking)))

stacking.fit(X_train_smote, y_train_smote)
print(""\nstacking on SMOTE set:"", roc_auc_score(y_train_smote, get_proba(X_train_smote, stacking)))
print(""stacking on SMOTE test set:"", roc_auc_score(y_test, get_proba(X_test_smote, stacking)))

stacking.fit(X_train_both, y_train_both)
print(""\nstacking on both RUS and SMOTE :"", roc_auc_score(y_train_both, get_proba(X_train_both, stacking)))
print(""stacking on both RUS and SMOTE"", roc_auc_score(y_test, get_proba(X_test_both, stacking)))",0.5469508171,
1441,part predicting a categorical response,"pca_scaled = PCA(n_components=n_components, whiten=True).fit(scaler.transform(X_train))
pca_scaled.explained_variance_ratio_",0.5425499678,
1441,part predicting a categorical response,"model.intercept_[0] + np.dot(Xprime.todense()[0], np.transpose(model.coef_[0]))[0,0]",0.5419412851,
1441,part predicting a categorical response,"#To find Variation explained by 100 components
pca_100 = PCA(n_components=100)
pca_result_100 = pca_100.fit_transform(df_dig[feat_cols].values)
np.sum(pca_100.explained_variance_ratio_)",0.5419039726,
1441,part predicting a categorical response,"pca = PCA().fit(train_pca)
explained = np.cumsum(pca.explained_variance_ratio_)",0.5417782664,
1441,part predicting a categorical response,"skm.roc_auc_score(y_train, Ada.predict_proba(X_train)[:,1])",0.5417085886,
251,comparison,"def make_compare(names, traces, models, ic='WAIC'):
    comparedf=pm.compare(traces,models, method='pseudo-BMA')
    temp=comparedf.sort_index()
    temp['Model']=names
    comparedf = temp.sort_values(ic).set_index('Model')
    return comparedf",0.3665109873,
251,comparison,"# Compute WAIC scores:
def make_compare(names, traces, models, ic='WAIC'):
    comparedf=pm.compare(traces,models, method='pseudo-BMA')
    temp=comparedf.sort_index()
    temp['Model']=names
    comparedf = temp.sort_values(ic).set_index('Model')
    return comparedf",0.3665109873,
251,comparison,"# Compute WAIC scores:
def make_compare(names, traces, models, ic='WAIC'):
    compared=pm.compare(traces,models, method='pseudo-BMA')
    temp=compared.sort_index()
    temp['Model']=names
    compared = temp.sort_values(ic).set_index('Model')
    return compared",0.3665109873,
251,comparison,"def predict(node, row):
    '''
    takes a tree and uses it to predict an outcome for a given set of factors
    '''
    if row[node['index']] < node['value']:
        if isinstance(node['left'], dict):
            return predict(node['left'], row)
        else:
            return node['left']
    else:
        if isinstance(node['right'], dict):
            return predict(node['right'], row)
        else:
            return node['right']",0.3616172373,
251,comparison,"def make_compare(names, traces, models, ic='WAIC'):
    comparedf=pm.compare(traces,models, method='pseudo-BMA')
    temp=comparedf.sort_index()
    temp['Model']=names
    comparedf = temp.sort_values(ic).set_index('Model')
    return comparedf

names=['A1', 'B1']
dfc=make_compare(names, [sample_A1,sample_B1_NC_high_ta],[edu_A1,edu_B1_NC])
dfc",0.3522883356,
251,comparison,"def compare_models(names, traces, models):
    df = pm.compare(traces, models, method='pseudo-BMA')
    df = df.sort_index()
    df.insert(loc=0, column='name', value=names)
    df = df.sort_values('WAIC')
    df = df.set_index('name')
    return df

compare_df = compare_models(['model A', 'model B'], [t1a, t1b_NC], [m1a, m1b_NC])
compare_df",0.3483741879,
251,comparison,"def classify(root,testline):
    if testline[root['best_attr']]<= root['best_attr_val']:
        if isinstance(root['left_tree'],dict):
            return classify(root['left_tree'],testline)
        else:
            return root['left_tree']
    else:
        if isinstance(root['right_tree'],dict):
            return classify(root['right_tree'],testline)
        else:
            return root['right_tree']",0.3413248658,
251,comparison,"def getTopK(df, k, label_value, label_column='groundTruth', operation=operator.eq, value_column='reviewCol'):
    stop = set(stopwords.words('english'))
    #Add possible Stop Words for Hotel Reviews
    stop.add('hotel')
    stop.add('room')
    stop.add('rooms')
    stop.add('stay')
    stop.add('staff')
    counter = Counter()
    for review in df.loc[operation(df[label_column],label_value)][value_column]:
            counter.update([word.lower() 
                            for word 
                            in re.findall(r'\w+', review) 
                            if word.lower() not in stop and len(word) > 2])
    topk = counter.most_common(k)
    return topk",0.3411272168,
251,comparison,"def same_high_location(obj1, obj2):
    return all((obj1.is_high(), obj2.is_high(), obj1.x == obj2.x, obj1.y == obj2.y))",0.3391363621,
251,comparison,"def can_see(i, source_angles, target_angles, source_viewports):
    return abs(source_angles[i] - target_angles[i]) < source_viewports[i]",0.3368469477,
2368,the two regression lines,"# Defining the regression models 

def linear_regression(X_train, y_train, X_test, y_test, plot=True):
    regr = LinearRegression()
    regr.fit(X_train, y_train)

    # Make predictions using the testing set
    y_pred = regr.predict(X_test)

    # Error
    rms_r2_score(y_test, y_pred)
    
    if plot:
        # Plot outputs
        #plot_task1(X_test, y_test, y_pred, 'Linear Regression')
        plot_powergeneration(y_test, y_pred, 'Linear Regression')
    return y_pred
        

def k_nearest_neighbors(X_train, y_train, X_test, y_test, plot=True):
    neigh = KNeighborsRegressor(n_neighbors=800)
    neigh.fit(X_train_selected, y_train) 
    y_pred = neigh.predict(X_test)

    # Error
    rms_r2_score(y_test, y_pred)
    
    if plot:
        # Plot outputs
        #plot_task1(X_test, y_test, y_pred, 'KNN')
        plot_powergeneration(y_test, y_pred, 'KNN')

#def knn_crossval(X,y,n_folds=10):
#    num_neighbors = [1, 5, 20, 50, 100, 500, 800, 1000]
#    #leaf_size = [10, 30, 50, 100, 200, 500]
#    param_grid = [{'n_neighbors': num_neighbors,
#                   'weights':['uniform'],
#                 #  'leaf_size':leaf_size
#                  },
#                 {'n_neighbors': num_neighbors,
#                  'weights':['distance'],
#               #   'leaf_size':leaf_size
#                 }]
#    grid_search = GridSearchCV(KNeighborsRegressor(),
#                                   param_grid,
#                                   cv=n_folds,
#                                   n_jobs=-1)
#    grid_search.fit(X, y)
#    grid_search.best_params_
#    return grid_search.best_params_    
#    
#def k_nearest_neighbors(X_train, y_train, X_test, y_test, plot=True):
#    
#    best_params = knn_crossval(X_train, y_train)    
#    neigh = KNeighborsRegressor().set_params(**best_params)
#    
#    neigh.fit(X_train_selected, y_train) 
#    y_pred = neigh.predict(X_test)
#
#    # The Root mean squared error
#    print(""Root Mean squared error: %.4f""
#          % np.sqrt(mean_squared_error(y_test, y_pred)))
#    # Explained variance score: 1 is perfect prediction
#    print('Variance score: %.2f' % r2_score(y_test, y_pred))
#    
#    if plot:
#        # Plot outputs
#        #plot_task1(X_test, y_test, y_pred, 'KNN')
#        plot_powergeneration(y_test, y_pred, 'KNN')


def support_vector_regression(X_train, y_train, X_test, y_test, plot=True):

    svr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)
    y_pred = svr_rbf.fit(X_train_selected, y_train).predict(X_test_selected)

    # Error
    rms_r2_score(y_test, y_pred)
    
    if plot:
        #plot_task1(X_test, y_test, y_pred, 'SVR')
        plot_powergeneration(y_test, y_pred, 'SVR')
        
        

def ann_model(X_train, y_train, X_test, y_test, plot=True):
    '''
    Trains an artificial neural network. n-input channels and one 
    output(the predicted power).
    '''
    input_shape = X_train.shape[1]
    output_shape = 1
    model = Sequential()

    model.add(InputLayer(input_shape=(input_shape,)))
    model.add(Dense(6, kernel_initializer='lecun_normal',
                    bias_initializer='ones',activation='selu'))
    model.add(Dropout(0.3))
    model.add(Dense(8, kernel_initializer='lecun_normal',
                    bias_initializer='ones',activation='softmax'))
    model.add(Dense(output_shape))

    model.compile(optimizer='rmsprop',
                loss='mean_squared_error')  #batch_size=2000, epochs=100,

    model.fit(X_train, y_train,epochs=10,  verbose=1)


    y_pred = model.predict(X_test)
    
    # Error
    rms_r2_score(y_test, y_pred)

    if plot:
        plot_powergeneration(y_test, y_pred, model='ANN')
 
def rms_r2_score(y_test, y_pred):
    # The Root mean squared error
    print(""Root Mean squared error: %.4f""
          % np.sqrt(mean_squared_error(y_test, y_pred)))
    # Explained variance score: 1 is perfect prediction
    print('R^2: %.2f' % r2_score(y_test, y_pred))

# Store predictions to file function        
def store_predictions_to_file(y_pred, model=None, task=1 , 
                              template='ForecastTemplate.csv'):
    pred = pd.read_csv(template)
    pred['FORECAST'] = y_pred[:len(pred)]
    pred.to_csv('ForecastTemplate{1}-{0}.csv'.format(model,task), index=False)
        
        
        
# Plotting function

def plot_powergeneration(y_test, y_pred, model=None):
    
    plt.figure(figsize=(15,5))
   
    plt.plot(y_test.values, color='darkorange', label='Real')
    
    plt.plot(y_pred, color='navy', label='Predicted')
    
    plt.xlabel('Time')
    plt.ylabel('Wind Power')
    plt.title(model)
    plt.legend()
    #plt.ylim(-0.1,y_test.max().all()+0.1)
    plt.show()
    

def plot_task1(X_test, y_test, y_pred, model=None):
    plt.scatter(X_test, y_test, color='darkorange', 
            marker='.', label='Real', linewidth=0.1)
    
    plt.scatter(X_test, y_pred, color='navy', 
                marker='.', label='Predicted', linewidth=0.1)
    
    plt.xlabel('Wind speed')
    plt.ylabel('Wind Power')
    plt.title(model)
    plt.legend()
    plt.ylim(-0.1,y_test.max().all()+0.1)
    plt.show()",0.4688239396,
2368,the two regression lines,"def linreg(X, Y):
    """"""
    Summary
        Linear regression of y = ax + b
    Usage
        real, real, real = linreg(list, list)
    Returns coefficients to the regression line ""y=ax+b"" from x[] and y[], and R^2 Value
    """"""
    if len(X) != len(Y):  raise ValueError, 'unequal length'
    N = len(X)
    Sx = Sy = Sxx = Syy = Sxy = 0.0
    for x, y in map(None, X, Y):
        Sx = Sx + x
        Sy = Sy + y
        Sxx = Sxx + x*x
        Syy = Syy + y*y
        Sxy = Sxy + x*y
    det = Sxx * N - Sx * Sx
    a, b = (Sxy * N - Sy * Sx)/det, (Sxx * Sy - Sx * Sxy)/det
    meanerror = residual = 0.0
    for x, y in map(None, X, Y):
        meanerror = meanerror + (y - Sy/N)**2
        residual = residual + (y - a * x - b)**2
    RR = 1 - residual/meanerror
    ss = residual / (N-2)
    Var_a, Var_b = ss * N / det, ss * Sxx / det
    #print ""y=ax+b""
    #print ""N= %d"" % N
    #print ""a= %g \\pm t_{%d;\\alpha/2} %g"" % (a, N-2, sqrt(Var_a))
    #print ""b= %g \\pm t_{%d;\\alpha/2} %g"" % (b, N-2, sqrt(Var_b))
    #print ""R^2= %g"" % RR
    #print ""s^2= %g"" % ss
    return a, b, RR",0.4445315301,
2368,the two regression lines,"for ind in range(10):
    exec('xtest = x%d' % (ind + 1))
    clf_simple = linear_model.LinearRegression()
    clf_simple.fit(xtest.reshape((N,1)), yvals.reshape((N,1)))
    print('x%d only R^2: %.2f'
          % ((ind+1), clf_simple.score(xtest.reshape((N,1)), yvals.reshape((N,1)))))",0.4417840242,
2368,the two regression lines,"def get_mnist_common_config():
    kmix,rho_ref_train,pi1_bias,logSigmaZval = 10,0.95,0.0,-2 # 10,0.95,1e-4,0.0,-2
    logsumexp_coef,kl_reg_coef,l2_reg_coef = 1e-2,1e-4,1e-5 # 0.1,0.1,1e-5
    USE_INPUT_BN,USE_RESNET,USE_GAP,USE_KENDALL_LOSS = False,True,False,False
    maxEpoch = 40
    return kmix,rho_ref_train,pi1_bias,logSigmaZval \
            ,logsumexp_coef,kl_reg_coef,l2_reg_coef \
            ,USE_INPUT_BN,USE_RESNET,USE_GAP,USE_KENDALL_LOSS,maxEpoch",0.4369857311,
2368,the two regression lines,"def plotRegression(beta, X, y):  
#     yFit = betaFit[0][0] + betaFit[1][0]*X[:,1:]
    yFit = np.dot(X,betaFit)
    
    MSE = np.sum((y - yFit)**2)/y.shape[0]
    
    plt.plot(X[:,1:], y, 'o', X[:,1:], yFit, '-')
    plt.xlabel(""X"")
    plt.ylabel(""Y"")
    print (""_0:"", betaFit[0][0],
           ""\n_1:"", betaFit[1][0],
           ""\nRegression: Y ="", '{:10.2f}'.format(betaFit[0][0]), '+', '{:10.2f}'.format(betaFit[1][0]), ""X""
           ""\nMSE ="",'{:10.2f}'.format(MSE))
    return plt.show()

def plotConvergence(J_history, iterations):
    plt.plot(np.arange(1, iterations + 1), J_history, '-')
    plt.xlabel(""iterations"")
    plt.ylabel(""J (cost)"")
    plt.show()",0.4307184815,
2368,the two regression lines,"def XGB(train, test, modelParam):
    gbm = xgb.XGBRegressor(**modelParam[1]).fit(train.drop(['SeriousDlqin2yrs'], axis=1), train['SeriousDlqin2yrs'])
    predicted = gbm.predict(test.drop(['SeriousDlqin2yrs'], axis=1))
    return roc_auc_score(test['SeriousDlqin2yrs'], predicted)

kFoldValidation(srch_train, proccessData, XGB, [0, {'max_depth':4, 'n_estimators':500, 'learning_rate':0.05}])",0.4292091727,
2368,the two regression lines,"def linear_regression():
    from sklearn.linear_model import LinearRegression
    lr = LinearRegression()
    lr.fit(X_train, y_train)
    y_pred = lr.predict(X)
    print(""RMSE for Linear Regression: {:0.4f} (+/- {:0.4f})"".format(rmse_cv(lr).mean(), rmse_cv(lr).std() * 2))
    return lr",0.4287355244,
2368,the two regression lines,"# Modified, getting weird results
def linear_regression5():
    from sklearn.linear_model import LinearRegression
    model = LinearRegression()
    model.fit(X_train, y_train)
    y_pred = model.predict(X)
    print(""RMSE for Linear Regression: {:0.4f} (+/- {:0.4f})"".format(rmse_cv(model).mean(), rmse_cv(model).std() * 2))

    out = expanded_scores(model)
    return out",0.4285717309,
2368,the two regression lines,"def plot_input_output_qq(lab1,lab2, mycolor):
    
    # use scikit-learn linear_model
    # build linear model of input vs. output
    ###lab1 = 'SS-E'
    ###lab2 = 'SS-P'
    lmreg = linear_model.LinearRegression( fit_intercept = False )
    dat = df[[lab1, lab2]].dropna()
    lmreg.fit( dat[lab1].values.reshape(-1,1), dat[lab2].values.reshape(-1,1) )

    xx = dat[[lab1,lab2]].dropna()[lab1].values.reshape(-1,1)
    yyhat = lmreg.predict( xx )
    yy = df[[lab1,lab2]].dropna()[lab1].shape

    resid = yyhat - yy
    resid = resid.reshape(len(resid),)

    fig = plt.figure(figsize=(6,4))
    ax = fig.add_subplot(111)

    stats.probplot(resid, dist='norm', plot=ax)
    
    ax.get_lines()[0].set_color(mycolor)
    ax.get_lines()[1].set_color(mycolor)
    
    plt.title(""Quantile-Quantile Plot: ""+lab1+"" vs ""+lab2+"" Residual"",size=14)
    
    plt.show()
    
plot_input_output_qq('SS-E','SS-P',colorz[0])
plot_input_output_qq('SS-P','SS-D',colorz[1])
plot_input_output_qq('SS-D','SS-S',colorz[2])",0.4276787639,
2368,the two regression lines,"def plot_block_input_output_regression(lab1, lab2, mycolor):
    # use scikit-learn linear_model
    # build linear model of input vs. output
    lmreg = linear_model.LinearRegression( fit_intercept = False )
    dat = df[[lab1, lab2]].dropna()
    lmreg.fit( dat[lab1].values.reshape(-1,1), dat[lab2].values.reshape(-1,1) )
    
    xx = np.linspace(0,1300,100)
    yy = lmreg.predict(xx.reshape(-1,1))

    fig = plt.figure(figsize=(6,5))
    
    ax1 = fig.add_subplot(111)
    
    zippy = np.linspace(0,1000,5)
    ax1.plot(zippy,zippy,'k-')
    ax1.scatter(df[lab1].values, df[lab2].values, alpha=0.4, color=mycolor, label=lab1+"" v ""+lab2)
    ax1.plot(xx, yy, '-', color=mycolor, label='Linear Regression')
    
    ax1.set_xlabel(lab1)
    ax1.set_ylabel(lab2)
    ax1.legend(loc='upper left')",0.4249097705,
1396,part data analysis,"# SVD does not accept missing values
try:
    PCAmodelAnalytical = nPYc.multivariate.exploratoryAnalysisPCA(tData, withExclusions=True, scaling=1.0)
    nPYc.reports.multivariateReport.multivariateQCreport(tData, PCAmodelAnalytical, reportType='analytical', withExclusions=True)
except ValueError:
    print('Multivariate analysis is not currently possible with vaues <LLOQ or >ULOQ.')",0.5303134918,
1396,part data analysis,"def fill_drivable_area(file_name, visualize=True):
    
    combined_binary = create_binary_image(file_name, show=False)

    rightx = []
    righty = []
    leftx = []
    lefty = []
    
    x, y = np.nonzero(np.transpose(combined_binary))
    i = 720
    j = 630
    while j >= 0:
        histogram = np.sum(combined_binary[j:i,:], axis=0)
        if visualize:
            plt.figure()
            plt.plot(histogram)
            plt.figure()
        left_peak = np.argmax(histogram[:640])
        x_idx = np.where((((left_peak - 25) < x)&(x < (left_peak + 25))&((y > j) & (y < i))))
        x_window, y_window = x[x_idx], y[x_idx]
        if np.sum(x_window) != 0:
            leftx.extend(x_window.tolist())
            lefty.extend(y_window.tolist())

        right_peak = np.argmax(histogram[640:]) + 640
        x_idx = np.where((((right_peak - 25) < x)&(x < (right_peak + 25))&((y > j) & (y < i))))
        x_window, y_window = x[x_idx], y[x_idx]
        if np.sum(x_window) != 0:
            rightx.extend(x_window.tolist())
            righty.extend(y_window.tolist())
        i -= 90
        j -= 90

    lefty = np.array(lefty).astype(np.float32)
    leftx = np.array(leftx).astype(np.float32)
    righty = np.array(righty).astype(np.float32)
    rightx = np.array(rightx).astype(np.float32)
    left_fit = np.polyfit(lefty, leftx, 2)
    left_fitx = left_fit[0]*lefty**2 + left_fit[1]*lefty + left_fit[2]
    right_fit = np.polyfit(righty, rightx, 2)
    right_fitx = right_fit[0]*righty**2 + right_fit[1]*righty + right_fit[2]
    rightx_int = right_fit[0]*720**2 + right_fit[1]*720 + right_fit[2]
    rightx = np.append(rightx,rightx_int)
    righty = np.append(righty, 720)
    rightx = np.append(rightx,right_fit[0]*0**2 + right_fit[1]*0 + right_fit[2])
    righty = np.append(righty, 0)
    leftx_int = left_fit[0]*720**2 + left_fit[1]*720 + left_fit[2]
    leftx = np.append(leftx, leftx_int)
    lefty = np.append(lefty, 720)
    leftx = np.append(leftx,left_fit[0]*0**2 + left_fit[1]*0 + left_fit[2])
    lefty = np.append(lefty, 0)
    lsort = np.argsort(lefty)
    rsort = np.argsort(righty)
    lefty = lefty[lsort]
    leftx = leftx[lsort]
    righty = righty[rsort]
    rightx = rightx[rsort]
    left_fit = np.polyfit(lefty, leftx, 2)
    left_fitx = left_fit[0]*lefty**2 + left_fit[1]*lefty + left_fit[2]
    right_fit = np.polyfit(righty, rightx, 2)
    right_fitx = right_fit[0]*righty**2 + right_fit[1]*righty + right_fit[2]
    
    # Per pixel in X and Y axis as provided in the lecture code sample
    ym_per_pix = 30./720
    xm_per_pix = 3.7/700
    
    left_fit_cr = np.polyfit(lefty*ym_per_pix, leftx*xm_per_pix, 2)
    right_fit_cr = np.polyfit(righty*ym_per_pix, rightx*xm_per_pix, 2)
    
    # Compute the left and right curve radius
    left_curverad = ((1 + (2*left_fit_cr[0]*np.max(lefty) + left_fit_cr[1])**2)**1.5) \
                                 /np.absolute(2*left_fit_cr[0])
    right_curverad = ((1 + (2*right_fit_cr[0]*np.max(lefty) + right_fit_cr[1])**2)**1.5) \
                                    /np.absolute(2*right_fit_cr[0])
    
    # Calculate the position of the car w.r.t. the center of the lane
    position = (rightx_int+leftx_int)/2
    center= abs((405 - position)*3.7/700) 
    
    img_size = (img.shape[1], img.shape[0])
    
    _x, _y = 360, 258
    offset_1 = 48
    offset_2 = 2
    
    src = np.float32([[int(_x-offset_1),_y],
                      [int(_x+offset_1),_y],
                      [int(0+offset_2),390],
                      [int(720-offset_2),390]])
    dst = np.float32([[0,0],[720,0],[0,405],[720,405]])
    
    M = cv2.getPerspectiveTransform(src, dst)
    Minv = cv2.getPerspectiveTransform(dst, src)
    
    warp_zero = np.zeros_like(combined_binary).astype(np.uint8)
    color_warp = np.dstack((warp_zero, warp_zero, warp_zero))
    pts_left = np.array([np.flipud(np.transpose(np.vstack([left_fitx, lefty])))])
    pts_right = np.array([np.transpose(np.vstack([right_fitx, righty]))])
    pts = np.hstack((pts_left, pts_right))
    cv2.polylines(color_warp, np.int_([pts]), isClosed=False, color=(0,0,255), thickness = 40)
    cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))
    newwarp = cv2.warpPerspective(color_warp, Minv, (combined_binary.shape[1], combined_binary.shape[0]))
    tmp_img = mpimg.imread(file_name)
    tmp_img = cv2.resize(tmp_img, (720, 405))
    result = cv2.addWeighted(tmp_img, 1, newwarp, 0.5, 0)
    f, (ax1, ax2) = plt.subplots(1,2, figsize=(11, 8))
    f.tight_layout()
    processed_img = perspective_transform(file_name, False)
    ax1.imshow(processed_img)
    ax1.set_xlim(0, 720)
    ax1.set_ylim(0, 405)
    ax1.plot(left_fitx, lefty, color='green', linewidth=3)
    ax1.plot(right_fitx, righty, color='green', linewidth=3)
    ax1.set_title('Fit Polynomial to Lane Lines', fontsize=16)
    ax1.invert_yaxis() # to visualize as we do the images
    ax2.imshow(result)
    output_filename = ""output_images\\"" + file_name.split(""\\"",1)[1]
    cv2.imwrite(output_filename, result)
    ax2.set_title('Fill Lane Between Polynomials', fontsize=16)
    if center < 405:
        ax2.text(80, 50, 'The car is {:.2f}m to the left from center'.format(center),
                 style='italic', color='white', fontsize=10)
    else:
        ax2.text(80, 50, 'The car is {:.2f}m to the right from center'.format(center),
                 style='italic', color='white', fontsize=10)
    ax2.text(80, 102, 'Curvature of the lane {}m'.format(int((left_curverad + right_curverad)/2)),
             style='italic', color='white', fontsize=10)",0.5214657784,
1396,part data analysis,"def fill_lanelines(filename):
    # introduce the combined_thresholded_binary_images with 1280x720.
    combined_binary = combined_thresholds(filename, show = False)
    
    leftx = []
    lefty = []
    rightx = []
    righty = []
    
    x,y = np.nonzero(np.transpose(combined_binary))
    i = 720
    j = 630
    while j >= 0:
        histogram = np.sum(combined_binary[j:i,:], axis = 0)
        left_peak = np.argmax(histogram[:640])
        x_idx = np.where((((left_peak - 25)< x)&(x<(left_peak+25))&((y>j)&(y<i))))
        x_window, y_window = x[x_idx], y[x_idx]
        if np.sum(x_window)!= 0:
            leftx.extend(x_window.tolist())
            lefty.extend(y_window.tolist())
        
        right_peak = np.argmax(histogram[640:])+640
        x_idx = np.where((((right_peak - 25)<x)&(x<(right_peak +25))&((y>j)&(y<i))))
        x_window, y_window = x[x_idx], y[x_idx]
        if np.sum(x_window)!=0:
            rightx.extend(x_window.tolist())
            righty.extend(y_window.tolist())
        i -= 90
        j -= 90
    
    lefty = np.array(lefty).astype(np.float32)
    leftx = np.array(leftx).astype(np.float32)
    righty = np.array(righty).astype(np.float32)
    rightx = np.array(rightx).astype(np.float32)
    # do the polynomial fitting
    left_fit = np.polyfit(lefty,leftx,2)
    left_fitx = left_fit[0]*lefty**2 + left_fit[1]*lefty + left_fit[2]
    right_fit = np.polyfit(righty,rightx,2)
    right_fitx = right_fit[0]*righty**2 + right_fit[1]*righty + right_fit[2]
    
    rightx_int = right_fit[0]*720**2 + right_fit[1]*720 + right_fit[2]
    rightx = np.append(rightx,rightx_int)
    righty = np.append(righty,720)
    rightx = np.append(rightx,right_fit[0]*0**2 + right_fit[1]*0 + right_fit[2])
    righty = np.append(righty,0)
    
    leftx_int = left_fit[0]*720**2 + left_fit[1]*720 + left_fit[2]
    leftx = np.append(leftx,leftx_int)
    lefty = np.append(lefty,720)
    leftx = np.append(leftx,left_fit[0]*0**2 + left_fit[1]*0 + left_fit[2])
    lefty = np.append(lefty,0)
    
    lsort = np.argsort(lefty)
    rsort = np.argsort(righty)
    lefty = lefty[lsort]
    leftx = leftx[lsort]
    righty = righty[rsort]
    rightx = rightx[rsort]
    
    left_fit = np.polyfit(lefty,leftx,2)
    left_fitx = left_fit[0]*lefty**2 + left_fit[1]*lefty + left_fit[2]
    right_fit = np.polyfit(righty,rightx,2)
    right_fitx = right_fit[0]*righty**2 + right_fit[1]*righty + right_fit[2]
    
    # calculate the curvature radius for each laneline
    # meters per pixel in y dimension
    ym_per_pix = 30./720 
    # meters per pixel in x dimension
    xm_per_pix = 3.7/700
    
    left_fit_cr = np.polyfit(lefty*ym_per_pix, leftx*xm_per_pix,2)
    right_fit_cr = np.polyfit(righty*ym_per_pix, rightx*xm_per_pix,2)
    left_curverad = ((1+(2*left_fit_cr[0]*np.max(lefty) + left_fit_cr[1])**2)**1.5)/np.absolute(2*left_fit_cr[0])
    right_curvead = ((1+(2*right_fit_cr[0]*np.max(lefty) + right_fit_cr[1])**2)**1.5)/np.absolute(2*right_fit_cr[0])
    
    # calculate the position of vehicle
    center = abs(640 - ((rightx_int + leftx_int)/2))
    
    offset = 0
    img_size = (combined_binary.shape[1], combined_binary.shape[0])
    src = np.float32([[490, 482],[810, 482],
                      [1250, 720],[40, 720]])
    dst = np.float32([[0, 0], [1280, 0], 
                     [1250, 720],[40, 720]])
    Minv = cv2.getPerspectiveTransform(dst, src)
    
    warp_zero = np.zeros_like(combined_binary).astype(np.uint8)
    color_warp = np.dstack((warp_zero, warp_zero, warp_zero))
    pts_left = np.array([np.flipud(np.transpose(np.vstack([left_fitx, lefty])))])
    pts_right = np.array([np.transpose(np.vstack([right_fitx,righty]))])
    pts = np.hstack((pts_left,pts_right))
    cv2.polylines(color_warp, np.int_([pts]), isClosed = False, color = (0,0,255), thickness = 40)
    cv2.fillPoly(color_warp, np.int_([pts]), (0,255,0))
    newwarp = cv2.warpPerspective(color_warp, Minv,img_size)
    result = cv2.addWeighted(mpimg.imread(filename), 1, newwarp, 0.5, 0)
    
    fig,axes = plt.subplots(1,2,figsize = (8,4))
    fig.tight_layout()
    axes[0].imshow(cv2.cvtColor((birds_eye_view(filename, show = False)[0]), cv2.COLOR_BGR2RGB))
    axes[0].set_xlim(0,1280)
    axes[0].set_ylim(0,720)
    axes[0].plot(left_fitx, lefty, color = 'green', linewidth = 3)
    axes[0].plot(right_fitx, righty, color = 'green', linewidth = 3)
    axes[0].set_title('Polynomial fitting to lanelines', fontsize = 12)
    axes[0].invert_yaxis()
    
    axes[1].imshow(result)
    axes[1].set_title('Fill lane between polynomials', fontsize = 12)
    if center < 640:
        axes[1].text(200,100,'Vehicle is {:.2f}m left of the center'.format(center*3.7/700),\
                     style = 'oblique',color = 'yellow', fontsize = 10)
    else:
        axes[1].text(200,100,'Vehicle is {:.2f}m right of the center'.format(center*3.7/700),\
                    style = 'oblique', color = 'yellow', fontsize = 10)
    axes[1].text(200,175,'Curvature radius is {}m'.format(int((left_curverad + right_curvead)/2)),\
                style = 'oblique', color = 'yellow', fontsize = 10) 
    #fig.savefig(os.path.join('output_images','fill_lanelines_'+os.path.split(filename)[1]))",0.5189102888,
1396,part data analysis,"def fill_lane(image):
    
    combined_binary = apply_thresholds(image, show=False)
    
    rightx = []
    righty = []
    leftx = []
    lefty = []
     
    x, y = np.nonzero(np.transpose(combined_binary))
    i = 720
    j = 630
    while (j >= 0):
        histogram = np.sum(combined_binary[j:i,:], axis=0)
        left_peak = np.argmax(histogram[:640])
        x_idx = np.where((((left_peak - 25) < x)&(x < (left_peak + 25))&((y > j) & (y < i))))
        x_window, y_window = x[x_idx], y[x_idx]
        if np.sum(x_window) != 0:
            leftx.extend(x_window.tolist())
            lefty.extend(y_window.tolist())

        right_peak = np.argmax(histogram[640:]) + 640
        x_idx = np.where((((right_peak - 25) < x)&(x < (right_peak + 25))&((y > j) & (y < i))))
        x_window, y_window = x[x_idx], y[x_idx]
        if np.sum(x_window) != 0:
            rightx.extend(x_window.tolist())
            righty.extend(y_window.tolist())
        i -= 90
        j -= 90

    leftx = np.array(leftx).astype(np.float32)
    lefty = np.array(lefty).astype(np.float32)
    
    righty = np.array(righty).astype(np.float32)
    rightx = np.array(rightx).astype(np.float32)
    
    
    left_fit = np.polyfit(lefty, leftx, 2)
    left_fitx = left_fit[0]*lefty**2 + left_fit[1]*lefty + left_fit[2]
    
    right_fit = np.polyfit(righty, rightx, 2)
    right_fitx = right_fit[0]*righty**2 + right_fit[1]*righty + right_fit[2]
    
    rightx_int = right_fit[0]*720**2 + right_fit[1]*720 + right_fit[2]
    
    rightx = np.append(rightx,rightx_int)
    righty = np.append(righty, 720)
    #rightx = np.append(rightx,right_fit[0]*0**2 + right_fit[1]*0 + right_fit[2])
    rightx = np.append(rightx,right_fit[2])
    righty = np.append(righty, 0)
    
    leftx_int = left_fit[0]*720**2 + left_fit[1]*720 + left_fit[2]
    
    leftx = np.append(leftx, leftx_int)
    lefty = np.append(lefty, 720)
    #leftx = np.append(leftx,left_fit[0]*0**2 + left_fit[1]*0 + left_fit[2])
    leftx = np.append(leftx,left_fit[2])
    lefty = np.append(lefty, 0)
    
    lsort = np.argsort(lefty)
    rsort = np.argsort(righty)
    
    lefty = lefty[lsort]
    leftx = leftx[lsort]
    
    righty = righty[rsort]
    rightx = rightx[rsort]
    
    left_fit = np.polyfit(lefty, leftx, 2)
    left_fitx = left_fit[0]*lefty**2 + left_fit[1]*lefty + left_fit[2]
    
    right_fit = np.polyfit(righty, rightx, 2)
    right_fitx = right_fit[0]*righty**2 + right_fit[1]*righty + right_fit[2]
    
    # Measure Radius of Curvature for each lane line
    # meters per pixel in y dimension
    ym_per_pix = 30./720 
    
    # meteres per pixel in x dimension
    xm_per_pix = 3.7/700 
    
    left_fit_cr = np.polyfit(lefty*ym_per_pix, leftx*xm_per_pix, 2)
    right_fit_cr = np.polyfit(righty*ym_per_pix, rightx*xm_per_pix, 2)
    
    left_curverad = ((1 + (2*left_fit_cr[0]*np.max(lefty) + left_fit_cr[1])**2)**1.5) \
                                 /np.absolute(2*left_fit_cr[0])
    right_curverad = ((1 + (2*right_fit_cr[0]*np.max(lefty) + right_fit_cr[1])**2)**1.5) \
                                    /np.absolute(2*right_fit_cr[0])
    
    
    # Calculate the position of the vehicle
    center = abs(640 - ((rightx_int+leftx_int)/2))
    
    offset = 0 
    img_size = (img.shape[1], img.shape[0])
    src = np.float32([[490, 482],[810, 482],
                      [1250, 720],[40, 720]])
    dst = np.float32([[0, 0], [1280, 0], 
                     [1250, 720],[40, 720]])
    Minv = cv2.getPerspectiveTransform(dst, src)
    
    warp_zero = np.zeros_like(combined_binary).astype(np.uint8)
    color_warp = np.dstack((warp_zero, warp_zero, warp_zero))
    pts_left = np.array([np.flipud(np.transpose(np.vstack([left_fitx, lefty])))])
    pts_right = np.array([np.transpose(np.vstack([right_fitx, righty]))])
    pts = np.hstack((pts_left, pts_right))
    cv2.polylines(color_warp, np.int_([pts]), isClosed=False, color=(0,0,255), thickness = 40)
    cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))
    newwarp = cv2.warpPerspective(color_warp, Minv, (combined_binary.shape[1], combined_binary.shape[0]))
    result = cv2.addWeighted(mpimg.imread(image), 1, newwarp, 0.5, 0)
    
    f, (ax1, ax2) = plt.subplots(1,2, figsize=(9, 6))
    f.tight_layout()
    ax1.imshow(cv2.cvtColor((birds_eye(image, display=False)[0]), cv2.COLOR_BGR2RGB))
    ax1.set_xlim(0, 1280)
    ax1.set_ylim(0, 720)
    ax1.plot(left_fitx, lefty, color='blue', linewidth=3)
    ax1.plot(right_fitx, righty, color='red', linewidth=3)
    ax1.set_title('Fit Polynomial to Lane Lines', fontsize=16)
    ax1.invert_yaxis() # to visualize as we do the images
    ax2.imshow(result)
    ax2.set_title('Fill Lane Between Polynomials', fontsize=16)
    if center < 640:
        side = 'left'
    else:
        side = 'right'

    ax2.text(100, 125, 'Vehicle is {:.2f}m '.format(center*3.7/700) + side + ' of center', color='red', fontsize=8)
    ax2.text(100, 200, 'Radius of curvature is {}m'.format(int((left_curverad + right_curverad)/2)), color='red', fontsize=8)",0.5189102888,
1396,part data analysis,"def fill_lane(image):
    
    combined_binary = apply_thresholds(image, show=False)
    
    rightx = []
    righty = []
    leftx = []
    lefty = []
    
    x, y = np.nonzero(np.transpose(combined_binary))
    i = 720
    j = 630
    while j >= 0:
        histogram = np.sum(combined_binary[j:i,:], axis=0)
        left_peak = np.argmax(histogram[:640])
        x_idx = np.where((((left_peak - 25) < x)&(x < (left_peak + 25))&((y > j) & (y < i))))
        x_window, y_window = x[x_idx], y[x_idx]
        if np.sum(x_window) != 0:
            leftx.extend(x_window.tolist())
            lefty.extend(y_window.tolist())

        right_peak = np.argmax(histogram[640:]) + 640
        x_idx = np.where((((right_peak - 25) < x)&(x < (right_peak + 25))&((y > j) & (y < i))))
        x_window, y_window = x[x_idx], y[x_idx]
        if np.sum(x_window) != 0:
            rightx.extend(x_window.tolist())
            righty.extend(y_window.tolist())
        i -= 90
        j -= 90

    lefty = np.array(lefty).astype(np.float32)
    leftx = np.array(leftx).astype(np.float32)
    righty = np.array(righty).astype(np.float32)
    rightx = np.array(rightx).astype(np.float32)
    left_fit = np.polyfit(lefty, leftx, 2)
    left_fitx = left_fit[0]*lefty**2 + left_fit[1]*lefty + left_fit[2]
    right_fit = np.polyfit(righty, rightx, 2)
    right_fitx = right_fit[0]*righty**2 + right_fit[1]*righty + right_fit[2]
    rightx_int = right_fit[0]*720**2 + right_fit[1]*720 + right_fit[2]
    rightx = np.append(rightx,rightx_int)
    righty = np.append(righty, 720)
    rightx = np.append(rightx,right_fit[0]*0**2 + right_fit[1]*0 + right_fit[2])
    righty = np.append(righty, 0)
    leftx_int = left_fit[0]*720**2 + left_fit[1]*720 + left_fit[2]
    leftx = np.append(leftx, leftx_int)
    lefty = np.append(lefty, 720)
    leftx = np.append(leftx,left_fit[0]*0**2 + left_fit[1]*0 + left_fit[2])
    lefty = np.append(lefty, 0)
    lsort = np.argsort(lefty)
    rsort = np.argsort(righty)
    lefty = lefty[lsort]
    leftx = leftx[lsort]
    righty = righty[rsort]
    rightx = rightx[rsort]
    left_fit = np.polyfit(lefty, leftx, 2)
    left_fitx = left_fit[0]*lefty**2 + left_fit[1]*lefty + left_fit[2]
    right_fit = np.polyfit(righty, rightx, 2)
    right_fitx = right_fit[0]*righty**2 + right_fit[1]*righty + right_fit[2]
    
    # Measure Radius of Curvature for each lane line
    ym_per_pix = 30./720 # meters per pixel in y dimension
    xm_per_pix = 3.7/700 # meteres per pixel in x dimension
    left_fit_cr = np.polyfit(lefty*ym_per_pix, leftx*xm_per_pix, 2)
    right_fit_cr = np.polyfit(righty*ym_per_pix, rightx*xm_per_pix, 2)
    left_curverad = ((1 + (2*left_fit_cr[0]*np.max(lefty) + left_fit_cr[1])**2)**1.5) \
                                 /np.absolute(2*left_fit_cr[0])
    right_curverad = ((1 + (2*right_fit_cr[0]*np.max(lefty) + right_fit_cr[1])**2)**1.5) \
                                    /np.absolute(2*right_fit_cr[0])
    
    
    # Calculate the position of the vehicle
    center = abs(640 - ((rightx_int+leftx_int)/2))
    
    offset = 0 
    img_size = (img.shape[1], img.shape[0])
    src = np.float32([[490, 482],[810, 482],
                      [1250, 720],[40, 720]])
    dst = np.float32([[0, 0], [1280, 0], 
                     [1250, 720],[40, 720]])
    Minv = cv2.getPerspectiveTransform(dst, src)
    
    warp_zero = np.zeros_like(combined_binary).astype(np.uint8)
    color_warp = np.dstack((warp_zero, warp_zero, warp_zero))
    pts_left = np.array([np.flipud(np.transpose(np.vstack([left_fitx, lefty])))])
    pts_right = np.array([np.transpose(np.vstack([right_fitx, righty]))])
    pts = np.hstack((pts_left, pts_right))
    cv2.polylines(color_warp, np.int_([pts]), isClosed=False, color=(0,0,255), thickness = 40)
    cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))
    newwarp = cv2.warpPerspective(color_warp, Minv, (combined_binary.shape[1], combined_binary.shape[0]))
    result = cv2.addWeighted(mpimg.imread(image), 1, newwarp, 0.5, 0)
    
    f, (ax1, ax2) = plt.subplots(1,2, figsize=(9, 6))
    f.tight_layout()
    ax1.imshow(cv2.cvtColor((birds_eye(image, display=False)[0]), cv2.COLOR_BGR2RGB))
    ax1.set_xlim(0, 1280)
    ax1.set_ylim(0, 720)
    ax1.plot(left_fitx, lefty, color='green', linewidth=3)
    ax1.plot(right_fitx, righty, color='green', linewidth=3)
    ax1.set_title('Fit Polynomial to Lane Lines', fontsize=16)
    ax1.invert_yaxis() # to visualize as we do the images
    ax2.imshow(result)
    ax2.set_title('Fill Lane Between Polynomials', fontsize=16)
    if center < 640:
        ax2.text(200, 100, 'Vehicle is {:.2f}m left of center'.format(center*3.7/700),
                 style='italic', color='white', fontsize=10)
    else:
        ax2.text(200, 100, 'Vehicle is {:.2f}m right of center'.format(center*3.7/700),
                 style='italic', color='white', fontsize=10)
    ax2.text(200, 175, 'Radius of curvature is {}m'.format(int((left_curverad + right_curverad)/2)),
             style='italic', color='white', fontsize=10)",0.5189102888,
1396,part data analysis,"# Processing images and lines

def lane_filling(image):
    
    comb_binary = thresholds(image, show=False)
    
    leftx = []
    lefty = []
    rightx = []
    righty = []
    
    x, y = np.nonzero(np.transpose(comb_binary))
    i = 720
    j = 630
    
    while j >= 0:
        histogram = np.sum(comb_binary[j:i,:], axis=0)
        left_peak = np.argmax(histogram[:640])
        x_idx = np.where((((left_peak - 25) < x)&(x < (left_peak + 25))&((y > j) & (y < i))))
        x_window, y_window = x[x_idx], y[x_idx]
        
        if np.sum(x_window) != 0:
            leftx.extend(x_window.tolist())
            lefty.extend(y_window.tolist())

        right_peak = np.argmax(histogram[640:]) + 640
        x_idx = np.where((((right_peak - 25) < x)&(x < (right_peak + 25))&((y > j) & (y < i))))
        x_window, y_window = x[x_idx], y[x_idx]
        if np.sum(x_window) != 0:
            rightx.extend(x_window.tolist())
            righty.extend(y_window.tolist())
        i -= 90
        j -= 90

    lefty = np.array(lefty).astype(np.float32)
    leftx = np.array(leftx).astype(np.float32)
    
    righty = np.array(righty).astype(np.float32)
    rightx = np.array(rightx).astype(np.float32)
    
    # Fitting a second order polynomial
    left_fit = np.polyfit(lefty, leftx, 2)  
    left_fitx = left_fit[0]*lefty**2 + left_fit[1]*lefty + left_fit[2]
    
    # Fitting a second order polynomial
    right_fit = np.polyfit(righty, rightx, 2)  
    right_fitx = right_fit[0]*righty**2 + right_fit[1]*righty + right_fit[2]
    
    rightx_int = right_fit[0]*720**2 + right_fit[1]*720 + right_fit[2]
    rightx = np.append(rightx,rightx_int)
    righty = np.append(righty, 720)
    rightx = np.append(rightx,right_fit[0]*0**2 + right_fit[1]*0 + right_fit[2])
    righty = np.append(righty, 0)
    
    leftx_int = left_fit[0]*720**2 + left_fit[1]*720 + left_fit[2]
    leftx = np.append(leftx, leftx_int)
    lefty = np.append(lefty, 720)
    leftx = np.append(leftx,left_fit[0]*0**2 + left_fit[1]*0 + left_fit[2])
    lefty = np.append(lefty, 0)
    
    lsort = np.argsort(lefty)
    rsort = np.argsort(righty)
    
    lefty = lefty[lsort]
    leftx = leftx[lsort]
    
    righty = righty[rsort]
    rightx = rightx[rsort]
    
    # Fitting a second order polynomial
    left_fit = np.polyfit(lefty, leftx, 2)
    left_fitx = left_fit[0]*lefty**2 + left_fit[1]*lefty + left_fit[2]
    
    # Fitting a second order polynomial
    right_fit = np.polyfit(righty, rightx, 2)
    right_fitx = right_fit[0]*righty**2 + right_fit[1]*righty + right_fit[2]
    
    # Radius of Curvature
    ym_per_pix = 30./720 # meters per pixel in y dimension
    xm_per_pix = 3.7/700 # meteres per pixel in x dimension
    
    left_fit_cr = np.polyfit(lefty*ym_per_pix, leftx*xm_per_pix, 2)
    right_fit_cr = np.polyfit(righty*ym_per_pix, rightx*xm_per_pix, 2)
    left_curverad = ((1 + (2*left_fit_cr[0]*np.max(lefty) + left_fit_cr[1])**2)**1.5) /np.absolute(2*left_fit_cr[0])
    right_curverad = ((1 + (2*right_fit_cr[0]*np.max(lefty) + right_fit_cr[1])**2)**1.5) /np.absolute(2*right_fit_cr[0])
    
    
    # vehicle position
    center = abs(640 - ((rightx_int+leftx_int)/2))
    
    offset = 0 
    img_size = (img.shape[1], img.shape[0])
     
    
    src = np.float32([[475, 500],[800, 500],[1250, 720],[100, 720]])
    dst = np.float32([[0, 0], [1250, 0], [1250, 720],[100, 720]])
    #src = np.float32([[490, 482],[810, 482],[1250, 720],[50, 720]])
    #dst = np.float32([[0, 0], [1280, 0], [1250, 720],[50, 720]])
    
    Minv = cv2.getPerspectiveTransform(dst, src)
    
    warp_zero = np.zeros_like(comb_binary).astype(np.uint8)
    color_warp = np.dstack((warp_zero, warp_zero, warp_zero))
    
    pts_left = np.array([np.flipud(np.transpose(np.vstack([left_fitx, lefty])))])
    pts_right = np.array([np.transpose(np.vstack([right_fitx, righty]))])
    pts = np.hstack((pts_left, pts_right))
    
    cv2.polylines(color_warp, np.int_([pts]), isClosed=False, color=(0,0,255), thickness = 40)
    cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))
    
    new_warp = cv2.warpPerspective(color_warp, Minv, (comb_binary.shape[1], comb_binary.shape[0]))
    result = cv2.addWeighted(mpimg.imread(image), 1, new_warp, 0.5, 0)
    
    f, (ax1, ax2) = plt.subplots(1,2, figsize=(12, 9))
    f.tight_layout()
    
    ax1.imshow(cv2.cvtColor((warp(image, display=False)[0]), cv2.COLOR_BGR2RGB))
    ax1.set_xlim(0, 1280)
    ax1.set_ylim(0, 720)
    
    ax1.plot(left_fitx, lefty, color='green', linewidth=3)
    ax1.plot(right_fitx, righty, color='green', linewidth=3)
    ax1.set_title('Line fitting')
    ax1.invert_yaxis() # to visualize as we do the images
    
    ax2.imshow(result)
    ax2.set_title('Lane filling')
    
    if center < 640:
        ax2.text(400, 100, 'Vehicle is {:.2f}m left of center'.format(center*3.7/700), color='white', fontsize=10)
    else:
        ax2.text(400, 100, 'Vehicle is {:.2f}m right of center'.format(center*3.7/700), color='white', fontsize=10)
    ax2.text(400, 150, 'Radius of curvature: {}m'.format(int((left_curverad + right_curverad)/2)), color='white', fontsize=10)
    
images = glob.glob('test_images/test*.jpg')
for image in images:
    lane_filling(image)",0.5185539722,
1396,part data analysis,"def plot_posterior(smc):
    pa = smc.get_particle_approximation()
    pa.resample()
    fig, ax = plt.subplots()
    alpha = pa.alpha
    ax.hist(alpha, normed=True, label='$\alpha$', alpha=0.5)
    ax.set_xlabel('alpha')
    fig, ax = plt.subplots()
    beta = pa.beta
    ax.hist(beta, normed=True, label='$\beta^2$', alpha=0.5)
    ax.set_xlabel('beta')

def logistic(x, beta, alpha=0):
    return 1.0 / (1.0 + np.exp(np.dot(beta, x) + alpha))

def plot_predictive(challenger_data, smc, num_samples=500):
    pa = smc.get_particle_approximation()
    pa.resample()
    alpha = pa.alpha
    beta = pa.beta
    
    temp = challenger_data[:, 0]
    D = challenger_data[:, 1]
    t = np.linspace(temp.min() - 5, temp.max() + 5, 50)[:, None]
    t_scaled = (t - np.mean(temp))/np.std(temp)
    p_t = logistic(t_scaled.T, beta[:,None], alpha[:,None])

    mean_prob_t = p_t.mean(axis=0)
    # vectorized bottom and top 2.5% quantiles for ""confidence interval""
    qs = mquantiles(p_t, [0.025, 0.975], axis=0)
    plt.fill_between(t[:, 0], *qs, alpha=0.7,
                     color=""#7A68A6"")

    plt.plot(t[:, 0], qs[0], label=""95% CI"", color=""#7A68A6"", alpha=0.7)

    plt.plot(t, mean_prob_t, lw=1, ls=""--"", color=""k"",
             label=""average posterior \nprobability of defect"")

    plt.xlim(t.min(), t.max())
    plt.ylim(-0.02, 1.02)
    plt.legend(loc=""lower left"")
    plt.scatter(temp, D, color=""k"", s=50, alpha=0.5)
    plt.xlabel(""temp, $t$"")

    plt.ylabel(""probability estimate"")
    plt.title(""Posterior probability estimates given temp. $t$"");",0.5182472467,
1396,part data analysis,"def plot_intercept_distribution(ens):
    pylab.subplot(1,2,1)
    intercepts = ens.intercepts.sample(ens.n_neurons)
    seaborn.distplot(intercepts, bins=20)
    pylab.xlabel('intercept')

    pylab.subplot(1,2,2)
    pts = ens.eval_points.sample(n=1000, d=ens.dimensions)
    model = nengo.Network()
    model.ensembles.append(ens)
    sim = nengo.Simulator(model)
    _, activity = nengo.utils.ensemble.tuning_curves(ens, sim, inputs=pts)
    p = np.mean(activity>0, axis=0)
    seaborn.distplot(p, bins=20)
    pylab.xlabel('proportion of pts neuron is active for')",0.514272809,
1396,part data analysis,"def fill_lane(image,objpoints, imgpoints):
    
    combined_binary = apply_thresholds(image,objpoints, imgpoints, show=False)
    
    rightx = []
    righty = []
    leftx = []
    lefty = []
    
    x, y = np.nonzero(np.transpose(combined_binary))
    i = 720
    j = 630
    while j >= 0:
        histogram = np.sum(combined_binary[j:i,:], axis=0)
        left_peak = np.argmax(histogram[:640])
        x_idx = np.where((((left_peak - 25) < x)&(x < (left_peak + 25))&((y > j) & (y < i))))
        x_window, y_window = x[x_idx], y[x_idx]
        if np.sum(x_window) != 0:
            leftx.extend(x_window.tolist())
            lefty.extend(y_window.tolist())

        right_peak = np.argmax(histogram[640:]) + 640
        x_idx = np.where((((right_peak - 25) < x)&(x < (right_peak + 25))&((y > j) & (y < i))))
        x_window, y_window = x[x_idx], y[x_idx]
        if np.sum(x_window) != 0:
            rightx.extend(x_window.tolist())
            righty.extend(y_window.tolist())
        i -= 90
        j -= 90

    lefty = np.array(lefty).astype(np.float32)
    leftx = np.array(leftx).astype(np.float32)
    righty = np.array(righty).astype(np.float32)
    rightx = np.array(rightx).astype(np.float32)
    left_fit = np.polyfit(lefty, leftx, 2)
    left_fitx = left_fit[0]*lefty**2 + left_fit[1]*lefty + left_fit[2]
    right_fit = np.polyfit(righty, rightx, 2)
    right_fitx = right_fit[0]*righty**2 + right_fit[1]*righty + right_fit[2]
    rightx_int = right_fit[0]*720**2 + right_fit[1]*720 + right_fit[2]
    rightx = np.append(rightx,rightx_int)
    righty = np.append(righty, 720)
    rightx = np.append(rightx,right_fit[0]*0**2 + right_fit[1]*0 + right_fit[2])
    righty = np.append(righty, 0)
    leftx_int = left_fit[0]*720**2 + left_fit[1]*720 + left_fit[2]
    leftx = np.append(leftx, leftx_int)
    lefty = np.append(lefty, 720)
    leftx = np.append(leftx,left_fit[0]*0**2 + left_fit[1]*0 + left_fit[2])
    lefty = np.append(lefty, 0)
    lsort = np.argsort(lefty)
    rsort = np.argsort(righty)
    lefty = lefty[lsort]
    leftx = leftx[lsort]
    righty = righty[rsort]
    rightx = rightx[rsort]
    left_fit = np.polyfit(lefty, leftx, 2)
    left_fitx = left_fit[0]*lefty**2 + left_fit[1]*lefty + left_fit[2]
    right_fit = np.polyfit(righty, rightx, 2)
    right_fitx = right_fit[0]*righty**2 + right_fit[1]*righty + right_fit[2]
    
    # Measure Radius of Curvature for each lane line
    ym_per_pix = 30./720 # meters per pixel in y dimension
    xm_per_pix = 3.7/700 # meteres per pixel in x dimension
    left_fit_cr = np.polyfit(lefty*ym_per_pix, leftx*xm_per_pix, 2)
    right_fit_cr = np.polyfit(righty*ym_per_pix, rightx*xm_per_pix, 2)
    left_curverad = ((1 + (2*left_fit_cr[0]*np.max(lefty) + left_fit_cr[1])**2)**1.5) \
                                 /np.absolute(2*left_fit_cr[0])
    right_curverad = ((1 + (2*right_fit_cr[0]*np.max(lefty) + right_fit_cr[1])**2)**1.5) \
                                    /np.absolute(2*right_fit_cr[0])

    curvature = (left_curverad + right_curverad) / 2
    
    min_curvature = min(left_curverad, right_curverad)
    
    # Calculate the position of the vehicle
    center = abs(640 - ((rightx_int+leftx_int)/2))
    vehicle_position = center / 12800 
    offset = 0 
    img_size = (img.shape[1], img.shape[0])
    src = np.float32([[490, 482],[810, 482],
                      [1250, 720],[40, 720]])
    dst = np.float32([[0, 0], [1280, 0], 
                     [1250, 720],[40, 720]])
    Minv = cv2.getPerspectiveTransform(dst, src)
    
    warp_zero = np.zeros_like(combined_binary).astype(np.uint8)
    color_warp = np.dstack((warp_zero, warp_zero, warp_zero))
    pts_left = np.array([np.flipud(np.transpose(np.vstack([left_fitx, lefty])))])
    pts_right = np.array([np.transpose(np.vstack([right_fitx, righty]))])
    pts = np.hstack((pts_left, pts_right))
    cv2.polylines(color_warp, np.int_([pts]), isClosed=False, color=(0,0,255), thickness = 40)
    cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))
    newwarp = cv2.warpPerspective(color_warp, Minv, (combined_binary.shape[1], combined_binary.shape[0]))
    result = cv2.addWeighted(mpimg.imread(image), 1, newwarp, 0.5, 0)
    
    f, (ax1, ax2) = plt.subplots(1,2, figsize=(9, 6))
    f.tight_layout()
    ax1.imshow(cv2.cvtColor((birds_eye(image,objpoints, imgpoints, display=False)[0]), cv2.COLOR_BGR2RGB))
    ax1.set_xlim(0, 1280)
    ax1.set_ylim(0, 720)
    ax1.plot(left_fitx, lefty, color='green', linewidth=3)
    ax1.plot(right_fitx, righty, color='green', linewidth=3)
    ax1.set_title('Fit Polynomial to Lane Lines', fontsize=16)
    ax1.invert_yaxis() # to visualize as we do the images
    ax2.imshow(result)
    ax2.set_title('Fill Lane Between Polynomials', fontsize=16)

    
    if center < 640:
        ax2.text(200, 100, 'Vehicle is {:.2f}m left of center'.format(center*3.7/700),
                 style='italic', color='white', fontsize=10)
    else:
        ax2.text(200, 100, 'Vehicle is {:.2f}m right of center'.format(center*3.7/700),
                 style='italic', color='white', fontsize=10)
    ax2.text(200, 175, 'Radius of curvature is {}m'.format(int((left_curverad + right_curverad)/2)),
             style='italic', color='white', fontsize=10)",0.5135058761,
1396,part data analysis,"def plot_rd(univ):  # rd = reaction distance
    univ.calculate_distances(rd=(20,39))
    fig, axes = plt.subplots(1, 2, figsize=(12, 4))
    univ.data.rd.plot(ax=axes[0])
    univ.data.rd.hist(ax=axes[1], grid=False)
    print(f'reaction distance mean: {univ.data.rd.mean():.2f} and sd: {univ.data.rd.std():.2f}')
    return fig, axes

def plot_hist_dist(univ, name, indexes=None):
    if indexes is not None:
        kwargs = {name: indexes}
        univ.calculate_distances(**kwargs)
    fig, axes = plt.subplots(1, 2, figsize=(12, 4))
    univ.data[name].plot(ax=axes[0])
    univ.data[name].hist(ax=axes[1], grid=False)
    print(f'{name} distance mean: {univ.data[name].mean():.2f} and sd: {univ.data[name].std():.2f}')",0.5114357471,
2430,try out your own text,"def getRandomTweetText(tweets):
    tweet_id = random.choice(list(tweets.getIds()))
    return tweets.getText(tweet_id)",0.4090628922,
2430,try out your own text,"def identify(animal):
    if animal.is_vertebrate():
        noise = animal.poke()
        if noise == 'moo':
            return 'cow'
        elif noise == 'woof':
            return 'dog'
    else:
        if animal.is_multicellular():
            return 'Bug!'
        else:
            if animal.is_fungus():
                return 'Yeast'
            else:
                return 'Amoeba'",0.4048897028,
2430,try out your own text,"self.tts.say(""Hello, my name is ""+ self._name + ""Let's play a game called"" ""Where's the ball"")
        
        #self.posture.goToPosture(""StandInit"", 0.5)
        uncover_eyes(self)
        currentPosture = self.posture.getPosture()
        
        if currentPosture != 'Sit':
            self.tts.say(""First, place me in an open space so I can sit down."")
            time.sleep(5.0)
            self.posture.goToPosture(""Sit"", 0.5)
            time.sleep(3.0)
            self.tts.say(""Thank you, now I feel great."")",0.4034470916,
2430,try out your own text,"self.tts.say(""Grab the red ball and hide it from me."" + 
                     "" I will tell you once I see the ball."")
        
        self.lastSeen = 0
        self.lastNotSeen = 0
        
        START = time.time()

        while time.time() - START < 10:

            cover_eyes(self)

            if time.time() - self.lastNotSeen > 5:
                self.tts.say(""Where's the ball? I don't see the ball"")
                self.lastNotSeen = time.time()    
        
        uncover_eyes(self)",0.4000398517,
2430,try out your own text,"def beautiful(tw): #input is a string, e.g single tweet
        remove_html_tags = BeautifulSoup(tw, ""lxml"").get_text() #removes html tags, e.g <html></html>
        return remove_html_tags #returns text without html tags",0.3993270993,
2430,try out your own text,"def get_key_word(r): 
    for x in key_word:
        if x in r.lemmas:
            return x
    return ''",0.3987418711,
2430,try out your own text,"def results(lr):
    print(""Best predictors for motorcycle class (Label 1)"")
    bike = lr.bike_words()
    print([vocab[i] for i in bike])
    
    print(""\nBest predictors for automobile class (Label 0)"")
    car = lr.car_words()
    print([vocab[i] for i in car])
    
    print(""\nWorst predictors for class"")
    bad = lr.bad_words()
    print([vocab[i] for i in bad])

results(lr)",0.3982173204,
2430,try out your own text,"# Administrator: red
# PTO Super Moderator: blue
# PTO Site Moderator: purple
# PTO Moderator: darkgreen
# Moderator On Leave: magenta
# PTO Card Swap Host: MediumTurquoise
# User: black

def get_user_role(el):
    
    try:
        # <font color=""red""><i>'   Patty    '</font>
        user_role = el.a.font['color']
    
    # if user is simply 'registered' then it won't have <font> element
    except TypeError as err:
        
        user_role = u'black'
        
    return user_role

get_user_role(cleaned[0][0]), get_user_role(cleaned[1][0]) # (u'red', 'black')",0.3913896084,
2430,try out your own text,"# Create a function that can extract the names from the website.  We can use these on the Greyhond board website to 
# get the race results for each of these Greyhounds

def get_name(result):
    dog = result.get_text()
    name = dog.split(' [')[0]
    return name

# Testing the function
get_name(name_results[3])",0.3902049661,
2430,try out your own text,"def get_rare_words(processed_tweets):
    """""" use the word count information across all tweets in training data to come up with a feature list
    Inputs:
        processed_tweets: pd.DataFrame: the output of process_all() function
    Outputs:
        list(str): list of rare words, sorted alphabetically.
    """"""
    list_all_words = []
    for thisText in processed_tweets.text:
        list_all_words.extend(thisText)
    word_dict_ctr = dict(Counter(list_all_words))
    rare_word_list = [word for word in word_dict_ctr if word_dict_ctr[word] == 1]
    rare_word_list.sort()

    return rare_word_list
    
print(processed_tweets.head())
rare_words = get_rare_words(processed_tweets)
print(len(rare_words))",0.3900310993,
382,creating dataframes & basic manipulations,"def process_fare():
    
    global combined
    
    # replace missing fare values
    combined.Fare.fillna(combined.Fare.mean(),inplace=True)
    
    status('Fare')",0.4018817544,
382,creating dataframes & basic manipulations,"def process_fares():
    
    global combined
    # there's one missing fare value - replacing it with the mean.
    combined.Fare.fillna(combined.Fare.mean(),inplace=True)
    
    status('fare')",0.4018817544,
382,creating dataframes & basic manipulations,"def process_fares():
    global combined
    # once fare values is misssing, lets replace it with the mean
    combined.Fare.fillna(combined.Fare.mean(), inplace=True)
    status('fare')",0.4018817544,
382,creating dataframes & basic manipulations,"def fill_nans():
    
    nan_list = df.columns[5:]
    
    for i in nan_list:
        
        df.loc[:,i].fillna(0, inplace = True)
        
fill_nans()",0.3834578991,
382,creating dataframes & basic manipulations,"train.loc[:, ['Sex', 'Embarked', 'Title', 'Family_Size']] = \
    train.replace( {'Sex': {'female': 1, 'male': 0},
                    'Family_Size': {'alone': 0, 'small_fam': 1, 'big_fam': 2}
                    }).loc[:, ['Sex', 'Embarked', 'Title', 'Family_Size']]
test.loc[:, ['Sex', 'Embarked', 'Title', 'Family_Size']] = \
    test.replace( {'Sex': {'female': 1, 'male': 0},
                   'Family_Size': {'alone': 0, 'small_fam': 1, 'big_fam': 2}
                   }).loc[:, ['Sex', 'Embarked', 'Title', 'Family_Size']]",0.3824568391,
382,creating dataframes & basic manipulations,"def fill_fare_na(f):
    if f.Fare.isnull().any():
        print('Number rows with Null Fare: {}'.format(f[f.Fare.isnull()].shape[0]))
        cl = f[f.Fare.isnull()].Pclass.values[0]
        cl_mean_fare = ds.groupby('Pclass').Fare.mean()[cl]
        print('Avg fare for class {} set: {}'.format(cl, cl_mean_fare))
        f.Fare.fillna(cl_mean_fare, inplace=True)
    
    if f.Fare.isnull().any():
        print('Number rows with Null Fare again!!!')
        print(f[f.Fare.isnull()].Fare)
        
fill_fare_na(test_ds)",0.3792058527,
382,creating dataframes & basic manipulations,"my_ratings_df = (
  pd.DataFrame(
    data={
      'UserId': [0 for __ in range(len(my_ratings[0]))],
      'Title': my_ratings[0],
      'Rating': my_ratings[1]})
    .pivot(index='UserId', columns='Title', values='Rating')
    .reindex(columns=pd_ratings_frame.columns))
my_ratings_df.dropna(axis=1).T",0.3789969683,
382,creating dataframes & basic manipulations,"def process_fare():
    global combined
    combined.Fare.fillna(combined.Fare.mean(), inplace=True)
    status('fare')
    
# execute the function
process_fare()
combined.head(5)",0.3768459558,
382,creating dataframes & basic manipulations,"# With numeric index
reformat_table(
    input_file=""./data/Small_m5C_Squires_hg38.bed"",
    output_file=""./data/Small_m5C_Squires_hg38_reformat.bed"",
    init_template=[0,""\t"",1,""\t"",2,""\t"",3,""|"",4,""\t"",5,""\t"",6],
    final_template=[0,""\t"",1,""\t"",2,""\tm5C|*|HeLa|22344696\t-\t"",6],
    replace_internal_space='_',
    replace_null_val=""*"",
    keep_original_header=False,
    header=""# New header\n""
    )

linerange (""./data/Small_m5C_Squires_hg38.bed"")
linerange (""./data/Small_m5C_Squires_hg38_reformat.bed"")",0.3752472401,
382,creating dataframes & basic manipulations,"pd.DataFrame(index=returns.columns, 
             data={'Mean':returns.mean(axis='index'),
                   'St.dev.':returns.std()})",0.3747682571,
1601,problem construct the inclusion indicator and append the column to the table,"# Function used to add Average, and Win % to the end of the dataset based on data in that file. This function is created
# in case further analysis down the road needs to utilize these columns. The second function does the same, but is made
# to simply the input for sheets that dont need win %

def add_column(dataset):
    dataset.insert(len(dataset.columns), 'AVG',(dataset['H'] / dataset['AB']))
    dataset.insert(len(dataset.columns), 'W%',(dataset['W'] / (dataset['W'] + dataset['L'])))
    dataset.insert(len(dataset.columns), 'OBP',((dataset['H'] + dataset['BB'] + dataset['HBP']) / (dataset['AB'] + dataset['BB'] + dataset['HBP'] + dataset['SF'])))
    

def add_column_avg_obp(dataset): # Creates a function to only add AVG and OBP
    dataset.insert(len(dataset.columns), 'AVG',(dataset['H'] / dataset['AB']))
    dataset.insert(len(dataset.columns), 'OBP',((dataset['H'] + dataset['BB'] + dataset['HBP']) / (dataset['AB'] + dataset['BB'] + dataset['HBP'] + dataset['SF'])))",0.3803677559,
1601,problem construct the inclusion indicator and append the column to the table,"MisColumn <- function(column, vector){
  # Extracts HDIM numbers of misspelled entries by column.
  # 
  # Args:
  #   column: The name of the target column within colEvent.
  #   vector: A vector of the accepted entries for the target column.
  #   
  # Returns: 
  #   Vector of HDIM numbers of misspelled entries within a column.
  indice.misspelled <- (which(!colEvent[, column] %in% vector))
  return(colEvent[indice.misspelled,]$HDIM)
}",0.3639229834,
1601,problem construct the inclusion indicator and append the column to the table,"%%file simplelist.c
#include <stdlib.h>
#include <stdio.h>

int find_min(int* alist, int size){
    int minctr = 0;
    int min = *alist;
    for(int i =0; i < size; i++){
        if (*(alist+i) <= min) {
            min = *(alist+i);
            minctr=i;
        }
    }
    return minctr;
}

int main() {
    int alist[10];
    int i;
    int minctr;
    
    for (i = 0; i < 10; i++) {
        alist[i] =-10*i+5;
        printf(""alist %d :  %d\n"", i, alist[i]);
    }
    for (i = 0; i < 10; i++) {
        printf(""alist %d :  %d\n"", i, *(alist + i));
    }
    minctr = find_min(alist, 10);
    printf(""min index %d value %d\n"", minctr, alist[minctr] );
}",0.3606912494,
1601,problem construct the inclusion indicator and append the column to the table,"EmptyContin <- function(method, vector){
  # Extracts HDIM numbers of empty entries contingent to method.
  #
  # Args:
  #   method: The name of the target method in the method column.
  #   vector: The vector of the names of the contingent columns
  #           to the target method.
  #   
  # Returns:
  #   Vector of HDIM numbers of empty entries in all columns contingent
  #   to the target method.
  method.ind <- which(colEvent$Method == method)
  method.vec <- apply(colEvent[vector], 2, function(x) which(x == """"))
  empty.ind <- c(method.ind, unique(unlist(method.vec, recursive = TRUE)))
  return(colEvent[unique(empty.ind[duplicated(empty.ind)]), ]$HDIM)
}",0.3562842607,
1601,problem construct the inclusion indicator and append the column to the table,"# Function to prefix column name. This helps to identify columns name after dataframes are merged
def addColumnPrefix(df, prefix):
    df.columns = [prefix+col if col not in ['card_id', 'merchant_id'] else col in df.columns.values]",0.354550302,
1601,problem construct the inclusion indicator and append the column to the table,"def get_VIF(df, incl):
    matrix = df[incl].dropna() 
    print ""Resulting matrix shape "", matrix.shape
    vifs = {}
    for i, col in enumerate(incl):
        vifs[col] = variance_inflation_factor(matrix.values, i)
    return sorted(vifs.items(), key = lambda (key, value): value, reverse=True)",0.350515455,
1601,problem construct the inclusion indicator and append the column to the table,"# Check feature importance
def featImp(modelfit, setVars): 
    featFit = modelfit.feature_importances_
    df = {'Var': pd.Series(setVars.columns.values), 'Imp': pd.Series(featFit)}
    fi = pd.DataFrame(df, columns=['Var','Imp'])
    return fi.sort(['Imp'], ascending=0).head(10)
    

featImp(dt_fit, testSetVars)",0.348839432,
1601,problem construct the inclusion indicator and append the column to the table,"def plot_trend(country, indicator, title='', figsize=(10, 6)):
    data = HFA.ix[(HFA.indicator_id==indicator) & (HFA.country == country), :]
    data.plot(x='year', y='value', title=title, legend=False, figsize=figsize)",0.3487067223,
1601,problem construct the inclusion indicator and append the column to the table,"def move_bike(system, n):
    olin_temp = system.olin - n
    if olin_temp < 0:
        system.olin_empty += 1
        return
    
    wellesley_temp = system.wellesley + n
    if wellesley_temp < 0:
        system.wellesley_empty += 1
        return
    
    system.olin = olin_temp
    system.wellesley = wellesley_temp",0.3476772904,
1601,problem construct the inclusion indicator and append the column to the table,"def move_bike(system, n):
    olin_temp = system.olin - n
    if olin_temp < 0:
        system.olin_empty += 1
        system.first_empty=system.clock
        return
    
    wellesley_temp = system.wellesley + n
    if wellesley_temp < 0:
        system.wellesley_empty += 1
        return
    
    system.olin = olin_temp
    system.wellesley = wellesley_temp",0.3476772904,
76,audio demixing,"import pyaudio
import wave

def record(fileName, duration, silence=False, rate=44100, channels=2, chunk=1024, format=pyaudio.paInt16):
    p = pyaudio.PyAudio()
    stream = p.open(format=format, channels=channels, 
                    rate=rate, input=True, frames_per_buffer=chunk)
    if not silence:
        print('* Start recording')
    frames = []
    for i in range(int(rate / chunk * duration)):
        data = stream.read(chunk)
        frames.append(data)
    if not silence:
        print(""* Done recording"")
    stream.stop_stream()
    stream.close()
    # output
    wf = wave.open(fileName, 'wb')
    wf.setnchannels(channels)
    wf.setsampwidth(p.get_sample_size(format))
    wf.setframerate(rate)
    wf.writeframes(b''.join(frames))
    wf.close()
    p.terminate()",0.4488160014,
76,audio demixing,"import pyaudio
import wave


def record_audio(RECORD_SECONDS, WAVE_OUTPUT_FILENAME):
    #--------- SETTING PARAMS FOR OUR AUDIO FILE ------------#
    FORMAT = pyaudio.paInt16    # format of wave
    CHANNELS = 2                # no. of audio channels
    RATE = 44100                # frame rate
    CHUNK = 1024                # frames per audio sample
    #--------------------------------------------------------#
     
    # creating PyAudio object
    audio = pyaudio.PyAudio()
     
    # open a new stream for microphone
    # It creates a PortAudio Stream Wrapper class object
    stream = audio.open(format=FORMAT,channels=CHANNELS,
                        rate=RATE, input=True,
                        frames_per_buffer=CHUNK)


    #----------------- start of recording -------------------#
    print(""Listening..."")

    # list to save all audio frames
    frames = []

    for i in range(int(RATE / CHUNK * RECORD_SECONDS)):
        # read audio stream from microphone
        data = stream.read(CHUNK)
        # append audio data to frames list
        frames.append(data)

    #------------------ end of recording --------------------#   
    print(""Finished recording."")
      
    stream.stop_stream()    # stop the stream object
    stream.close()          # close the stream object
    audio.terminate()       # terminate PortAudio

    #------------------ saving audio ------------------------#

    # create wave file object
    waveFile = wave.open(WAVE_OUTPUT_FILENAME, 'wb')

    # settings for wave file object
    waveFile.setnchannels(CHANNELS)
    waveFile.setsampwidth(audio.get_sample_size(FORMAT))
    waveFile.setframerate(RATE)
    waveFile.writeframes(b''.join(frames))

    # closing the wave file object
    waveFile.close()


def read_audio(WAVE_FILENAME):
    # function to read audio(wav) file
    with open(WAVE_FILENAME, 'rb') as f:
        audio = f.read()
    return audio",0.4452939332,
76,audio demixing,"from audioop import mul
#new delay function with factor
def delay(audio_bytes,params,offset_ms,factor=1):
    """"""version 2: delay after 'offset_ms' milliseconds amplified by 'factor'""""""
    #calculate the number of bytes which corresponds to the offset in milliseconds
    offset= params.sampwidth*offset_ms*int(params.framerate/1000)
    #create some silence
    beginning= b'\0'*offset
    #remove space from the end
    end= audio_bytes[:-offset]
    #multiply by the factor
    multiplied_end= mul(audio_bytes[:-offset],params.sampwidth,factor)
    return add(audio_bytes, beginning+ multiplied_end, params.sampwidth)",0.4431579411,
76,audio demixing,"cqtsHarm, cqtsPerc = lbr.decompose.hpss(lbr.amplitude_to_db(lbr.magphase(lbr.cqt(
    song, fmin=lbr.note_to_hz('A0'), n_bins=88*nBins, bins_per_octave=12*nBins))[0], ref=np.min))
#, kernel_size=(3, int(31 * 88 * nBins / len(lbr.fft_frequencies()) // 2 * 2 + 1)))

assert 0 <= cqtsHarm.min() < 15 and 20 < cqtsHarm.mean() < 45 and 70 < cqtsHarm.max() < 100
print(cqtsHarm.shape[1], 'frames,', end='\t')
print('Cqts decibels in range [{:.0f} - {:.0f} - {:.0f}]'.format(cqtsHarm.min(), cqtsHarm.mean(), cqtsHarm.max()))",0.4353795648,
76,audio demixing,"from nilearn.image import resample_to_img, math_img
from scipy.ndimage import binary_dilation

def get_mask(mask_type):
    
    # Specify location of the brain and eye image
    brain = '/templates/MNI152_T1_1mm_brain.nii.gz'
    eyes = '/templates/MNI152_T1_1mm_eye.nii.gz'

    # Load region of interest
    if mask_type == 'brain':
        img_resampled = resample_to_img(brain, func)
    elif mask_type == 'eyes':
        img_resampled = resample_to_img(eyes, func)
    elif mask_type == 'both':
        img_roi = math_img(""img1 + img2"", img1=brain, img2=eyes)
        img_resampled = resample_to_img(img_roi, func)

    # Binarize ROI template
    data_binary = np.array(img_resampled.get_data()>=10, dtype=np.int8)

    # Dilate binary mask once
    data_dilated = binary_dilation(data_binary, iterations=1).astype(np.int8)

    # Save binary mask in NIfTI image
    mask = nb.Nifti1Image(data_dilated, img_resampled.affine, img_resampled.header)
    mask.set_data_dtype('i1')
    
    return mask",0.4353001118,
76,audio demixing,"from audioop import add

def delay(audio_bytes,params,offset_ms):
    """"""version 1: delay after 'offset_ms' milliseconds""""""
    #calculate the number of bytes which corresponds to the offset in milliseconds
    offset= params.sampwidth*offset_ms*int(params.framerate/1000)
    #create some silence
    beginning= b'\0'*offset
    #remove space from the end
    end= audio_bytes[:-offset]
    return add(audio_bytes, beginning+end, params.sampwidth)",0.4343160391,
76,audio demixing,"from scipy.io import wavfile
from scipy.signal import resample

def load_wav(filename,samplerate=44100):
    
    # load file
    rate, data = wavfile.read(filename)

    # convert stereo to mono
    if len(data.shape) > 1:
        data = data[:,0]/2 + data[:,1]/2

    # re-interpolate samplerate    
    ratio = float(samplerate) / float(rate)
    data = resample(data, len(data) * ratio)
    
    return samplerate, data.astype(np.int16)",0.4317929149,
76,audio demixing,"f = abs(fft.fft(sound)[0:size(sound)/2]) #the magnitude spectrum, from 0 to nyquist
freqs = (fft.fftfreq(size(sound), 1/44100))[0:(size(sound)/2)] #the frequency values for each bin, from 0 to nyquist
centroid = sum(freqs * f)/sum(f) # the weighted sum of magnitudes divided by the unweighted sum of magnitudes
print centroid #print it out",0.4280596077,
76,audio demixing,"video_ocr.chord_intervals_to_audio_snips(chord_intervals, wav_file, output_dir, video_nb_frames)",0.4265014231,
76,audio demixing,"#Only do this (set it to 1) if you want to replace the file with the cropped version...
if 1:
    f = './data/cropped-raw-from-phone.wav'  
    soundfile.write(f, cropped, samplerate=sample_rate)
    print(""Wrote '%s'"" % (f,))",0.425820142,
2119,step naive bayes implementation using scikit learn,"eval_points = sim.data[x].eval_points
e = sim.data[x].encoders.squeeze()
gain = sim.data[x].gain
bias = sim.data[x].bias
intercepts = sim.data[x].intercepts

if neuron_type == nengo.neurons.Sigmoid():
    # Hack to fix intercepts:
    # https://github.com/nengo/nengo/issues/1211
    intercepts = -np.ones_like(intercepts)

d_alg = sim.data[conn].weights.T",0.4678604901,
2119,step naive bayes implementation using scikit learn,"# Dataset split between train & test
train_data, test_data, train_target, test_target = train_test_split(mnist.data, mnist.target, test_size=1/7.0, random_state=0)",0.4532504082,
2119,step naive bayes implementation using scikit learn,"# test_size: what proportion of original data is used for test set
train_img, test_img, train_lbl, test_lbl = train_test_split(
    mnist.data, mnist.target, test_size=1/7.0, random_state=0)",0.4532504082,
2119,step naive bayes implementation using scikit learn,"evalROCLB1 = Evaluator(linearModel, experiment4.testData[:,0:-1], experiment4.testData[:,-1], experiment4.bestTheta)
tpr_lb1, fpr_lb1, aroc_lb1 = evalROCLB1.getROC()
evalROCLB2 = Evaluator(linearModel, experiment5.testData[:,0:-1], experiment5.testData[:,-1], experiment5.bestTheta)
tpr_lb2, fpr_lb2, aroc_lb2 = evalROCLB2.getROC()
evalROCLB3 = Evaluator(linearModel, experiment6.testData[:,0:-1], experiment6.testData[:,-1], experiment6.bestTheta)
tpr_lb3, fpr_lb3, aroc_lb3 = evalROCLB3.getROC()

plotROCLB = Plotter(1, 1)
plotROCLB.add2DPlot(fpr_lb1, tpr_lb1, 'Exp 4', True, 'blue', 'solid', 2.0)
plotROCLB.add2DPlot(fpr_lb2, tpr_lb3, 'Exp 5', True, 'green', 'solid', 2.0)
plotROCLB.add2DPlot(fpr_lb3, tpr_lb3, 'Exp 6', True, 'black', 'solid', 2.0)
plotROCLB.add2DPlot([0, 1], [0, 1], 'random', True, 'red', 'dashed', 3.0)
plotROCLB.show('False Positive Rate', 'True Positive Rate', 0)

print('==AROC per Experiment==')
print('==Exp 4: %.2f==' %aroc_lb1)
print('==Exp 5: %.2f==' %aroc_lb2)
print('==Exp 6: %.2f==' %aroc_lb3)",0.4486217201,
2119,step naive bayes implementation using scikit learn,"evalROCLS1 = Evaluator(linearModel, experiment1.testData[:,0:-1], experiment1.testData[:,-1], experiment1.bestTheta)
tpr_ls1, fpr_ls1, aroc_ls1 = evalROCLS1.getROC()
evalROCLS2 = Evaluator(linearModel, experiment2.testData[:,0:-1], experiment2.testData[:,-1], experiment2.bestTheta)
tpr_ls2, fpr_ls2, aroc_ls2 = evalROCLS2.getROC()
evalROCLS3 = Evaluator(linearModel, experiment3.testData[:,0:-1], experiment3.testData[:,-1], experiment3.bestTheta)
tpr_ls3, fpr_ls3, aroc_ls3 = evalROCLS3.getROC()

plotROCLS = Plotter(1, 1)
plotROCLS.add2DPlot(fpr_ls1, tpr_ls1, 'Exp 1', True, 'blue', 'solid', 2.0)
plotROCLS.add2DPlot(fpr_ls2, tpr_ls2, 'Exp 2', True, 'green', 'solid', 2.0)
plotROCLS.add2DPlot(fpr_ls3, tpr_ls3, 'Exp 3', True, 'black', 'solid', 2.0)
plotROCLS.add2DPlot([0, 1], [0, 1], 'random', True, 'red', 'dashed', 3.0)
plotROCLS.show('False Positive Rate', 'True Positive Rate', 0)

print('==AROC per Experiment==')
print('==Exp 1: %.2f==' %aroc_ls1)
print('==Exp 2: %.2f==' %aroc_ls2)
print('==Exp 3: %.2f==' %aroc_ls3)",0.4486217201,
2119,step naive bayes implementation using scikit learn,"evalROCLoS1 = Evaluator(linearModel, experiment7.testData[:,0:-1], experiment7.testData[:,-1], experiment7.bestTheta)
tpr_los1, fpr_los1, aroc_los1 = evalROCLoS1.getROC()
evalROCLoS2 = Evaluator(linearModel, experiment8.testData[:,0:-1], experiment8.testData[:,-1], experiment8.bestTheta)
tpr_los2, fpr_los2, aroc_los2 = evalROCLoS2.getROC()
evalROCLoS3 = Evaluator(linearModel, experiment9.testData[:,0:-1], experiment9.testData[:,-1], experiment9.bestTheta)
tpr_los3, fpr_los3, aroc_los3 = evalROCLoS3.getROC()

plotROCLoS = Plotter(1, 1)
plotROCLoS.add2DPlot(fpr_los1, tpr_los1, 'Exp 7', True, 'blue', 'solid', 2.0)
plotROCLoS.add2DPlot(fpr_los2, tpr_los2, 'Exp 8', True, 'green', 'solid', 2.0)
plotROCLoS.add2DPlot(fpr_los3, tpr_los3, 'Exp 9', True, 'black', 'solid', 2.0)
plotROCLoS.add2DPlot([0, 1], [0, 1], 'random', True, 'red', 'dashed', 3.0)
plotROCLoS.show('False Positive Rate', 'True Positive Rate', 0)

print('==AROC per Experiment==')
print('==Exp 7: %.2f==' %aroc_los1)
print('==Exp 8: %.2f==' %aroc_los2)
print('==Exp 9: %.2f==' %aroc_los3)",0.4486217201,
2119,step naive bayes implementation using scikit learn,"# mutate(flights,
#   gain = arr_delay - dep_delay,
#   speed = distance / air_time * 60)

flights.assign(gain=flights.arr_delay - flights.dep_delay,
               speed=flights.distance / flights.air_time * 60)",0.4478456378,
2119,step naive bayes implementation using scikit learn,"reviews_data_train, reviews_data_test, target_train, target_test = train_test_split(reviews_data.data,
                                                                                    reviews_data.target,
                                                                                    test_size=0.20,
                                                                                    random_state=None)",0.445571214,
2119,step naive bayes implementation using scikit learn,"docs_train, docs_test, y_train, y_test = train_test_split(
    dataset.data, dataset.target, test_size=0.25, random_state=None)",0.4450991452,
2119,step naive bayes implementation using scikit learn,"plsr_reg.score(co_data[input_features], co_data.cbrt_PCpc)",0.4450123906,
10,vectors,"def sent_vec(sent):
    wv_res = np.zeros(glove_model.vector_size)
    ctr = 1
    for w in sent:
        if w in glove_model:
            ctr += 1
            wv_res += glove_model[w]
    wv_res = wv_res/ctr
    #return (wv_res, ctr)
    return wv_res",0.472424686,
10,vectors,"def sent_vec_w2v(sent):
    wv_res = np.zeros(w2v.vector_size)
    ctr = 1
    for w in sent:
        if w in w2v:
            ctr += 1
            wv_res += w2v[w]
    wv_res = wv_res/ctr
    return wv_res",0.472424686,
10,vectors,"def cosine_similarity(model):
    sim = model.item_vecs.dot(model.item_vecs.T)
    norms = np.array([np.sqrt(np.diagonal(sim))])
    return sim / norms / norms.T

als_sim = cosine_similarity(best_als_model)
sgd_sim = cosine_similarity(best_sgd_model)",0.4630145729,
10,vectors,"# homogenization of boundary data and solution of linear system
def SolveLinearSystem():
    rhs = gfu.vec.CreateVector()
    rhs.data = f.vec - a.mat * gfu.vec
    update = gfu.vec.CreateVector()
    update.data = a.mat.Inverse(freedofs) * rhs
    gfu.vec.data += update
SolveLinearSystem()
Draw(uh,mesh,""u"")",0.4593757093,
10,vectors,"def f(x):
    g = np.zeros((3, x.size))
    g[0], g[1], g[2] = np.exp(-x), -np.exp(-x), np.exp(-x)
    return g",0.4576120079,
10,vectors,"def mat_to_np(mat):
    elements = mat.A
    return np.array([[elements[j * 4 + i] for i in range(4)] for j in range(4)])

def np_to_mat(np_mat):
    App.Matrix(*np_mat.flatten())

def vec_to_np(vec):
    return np.array(list(vec))",0.4545356631,
10,vectors,"%%add_to UKF

@property
def weights_mean(self):
    
    w_m = np.zeros((2*self.n+1, 1))
    # TODO: Calculate the weight to calculate the mean based on the predicted sigma points
    w_m[0] = self.lam / (self.n + self.lam)
    w_m[1:] = 0.5 / (self.n + self.lam)
    
    self.w_m = w_m
    return w_m

@property
def weights_cov(self):
    
    w_cov = np.zeros((2*self.n+1, 1))
    # TODO: Calculate the weight to calculate the covariance based on the predicted sigma points
    w_cov[0] = self.lam / (self.n + self.lam) + (1 - self.alpha**2 - self.betta**2)
    w_cov[1:] = 0.5 / (self.n + self.lam)
    
    self.w_cov = w_cov
    return w_cov


def h(self,Z):
    return np.matmul(np.array([[0.0, 1.0]]), Z) 
    

def update(self,z_in):
    
    # TODO: Implement the update step 
    mu_bar = self.x_bar @ self.weights_mean
    cov_bar = self.weights_cov.T * (self.x_bar - mu_bar) @ (self.x_bar - mu_bar).T + self.q_t
    z = self.h(self.x_bar)
    mu_z = z @ self.weights_mean
    cov_z = self.weights_cov.T * (z - mu_z) @ (z - mu_z).T + self.r_t
    cov_xz = self.weights_cov.T * (self.x_bar - mu_bar) @ (z - mu_z).T
    k = cov_xz @ np.linalg.pinv(cov_z)
    
    mu_t = mu_bar + k * (z_in - mu_z)
    cov_t = cov_bar - k @ cov_z @ k.T  

    self.mu = mu_t
    self.sigma = cov_t
    
    return mu_t, cov_t",0.4505329132,
10,vectors,"def action_value(value_function, environment):
    q = np.zeros((environment.nS, environment.nA))
    for state0, actions in environment.P.items():
        for action, transitions in actions.items():
            for transition in transitions:
                probability, state1, reward, _ = transition
                q[state0, action] += probability * (reward + value_function[state1])
    return q",0.4499844313,
10,vectors,"def tolst(term):
    return [list(term.mat.Xs),list(term.mat.Zs),term.val]
random.seed(3)
system = SBRG(TFIsing(8,J=1.,K=1.,h=1.,alpha=0.2))
RGdat = []
while system.phybits:
    (H0, Rs, offdiag) = system.nextstep()
    RGdat.append([tolst(H0),[tolst(R) for R in Rs],[tolst(S) for S in offdiag]])
Hbdy = [tolst(term) for term in system.Hbdy]
Heff = [tolst(term) for term in system.Heff]
export('WFO_J1K1h1_a02_8',{'bits':system.size,'RGdat': RGdat,'Hbdy':Hbdy,'Heff':Heff})",0.4473503232,
10,vectors,"# Function to transform training points into these dimensions: (1, x, y, xy, x^2, y^2)
def nonlinear_transform(points):
    # Add columns for xy, x^2, and y^2
    return np.hstack((points, points[:,1, np.newaxis]*points[:,2, np.newaxis] , np.square(points[:,1:])))",0.4466798902,
2647,z scoring,"def mic(param1, param2):
    
    """"""Input is two parameters between which we want to calculate MIC.
        Output is the MIC value.""""""
    
    m = MINE()
    m.compute_score(param1, param2)
    return m.mic()",0.477681309,
2647,z scoring,"def print_score(m):
    res = [m.score(X_train, y_train), m.score(X_valid, y_valid)]
    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)
    print(res)",0.4672170281,
2647,z scoring,"for metric in ['accuracy', 'precision', 'recall', 'roc_auc']:
    scores = cross_val_score(model, X, y, scoring=metric)
    print(""mean {}: {}, all: {}"".format(metric, scores.mean(), scores))",0.4670479298,
2647,z scoring,"def print_score(m):
    res = [m.score(X_train, y_train), m.score(X_val, y_val), m.oob_score_]
    print(res)
    
print_score(m)",0.4662091434,
2647,z scoring,"def preprocess_fn(input_features):
    
    output_features = {}

    # target feature
    output_features['weight_pounds'] = input_features['weight_pounds']

    # normalisation
    output_features['mother_age_normalized'] = tft.scale_to_z_score(input_features['mother_age'])
    output_features['gestation_weeks_normalized'] =  tft.scale_to_0_1(input_features['gestation_weeks'])
    
    # bucktisation based on quantiles
    output_features['mother_age_bucketized'] = tft.bucketize(input_features['mother_age'], num_buckets=5)
    
    # you can compute new features based on custom formulas
    output_features['mother_age_log'] = tf.log(input_features['mother_age'])
    
    # or create flags/indicators
    is_multiple = tf.as_string(input_features['plurality'] > tf.constant(1.0))
    
    # convert categorical features to indexed vocab
    output_features['mother_race_index'] = tft.compute_and_apply_vocabulary(input_features['mother_race'], vocab_filename='mother_race')
    output_features['is_male_index'] = tft.compute_and_apply_vocabulary(input_features['is_male'], vocab_filename='is_male')
    output_features['is_multiple_index'] = tft.compute_and_apply_vocabulary(is_multiple, vocab_filename='is_multiple')
    
    return output_features",0.4629775882,
2647,z scoring,"## Function for feature importance
def get_feature_importance(model):
    Importance = model.get_fscore()
    Importance = list(Importance.items())
    Feature= []
    Score = []
    for each in Importance:
        Feature.append(each[0])
        Score.append(each[1])
    df = pd.DataFrame({'Feature':Feature,'Score':Score}).sort_values(by=['Score'],ascending=[0])
    return df",0.4621186256,
2647,z scoring,"def get_feature_importance(model):
    Importance = model.get_fscore()
    Importance = list(Importance.items())
    Feature= []
    Score = []
    for each in Importance:
        Feature.append(each[0])
        Score.append(each[1])
    df = pd.DataFrame({'Feature':Feature,'Score':Score}).sort_values(by=['Score'],ascending=[0])
    return df",0.4621186256,
2647,z scoring,"def player_breakdown(team, player):
    """""" Returns dict(shot_type: [made, missed] counts)""""""
    flat_list = [item for sublist in team.scoring_record[player] for item in sublist]
    keys, counts = np.unique(flat_list, return_counts=True)
    res = dict((i, [0, 0]) for i in [1, 2, 3])
    for i, count in zip(keys, counts):
        res[abs(i)][i<0] = count
    return res",0.4609129429,
2647,z scoring,"def get_importance(model, threshold): 
    impt = model.get_fscore()
    index = []
    for (key,value) in impt.items():
        if impt[key] >= threshold: index.append(int(str(key)[1:]))
    return index

def select_features(index, dataTrain, dataTest):
    partialTrain = np.column_stack((dataTrain[:,index[0]], dataTrain[:,index[1]]))
    partialTest = np.column_stack((dataTest[:,index[0]], dataTest[:,index[1]]))
    for j in range(2,len(index)):
        i = index[j]
        partialTrain = np.column_stack((partialTrain, dataTrain[:,i]))
        partialTest = np.column_stack((partialTest, dataTest[:,i]))
    return partialTrain, partialTest

def getFeatureNames(headerWhole, indexList):
    resultant_list = []
    for i in indexList:
        resultant_list.append(headerWhole[i])
    return resultant_list",0.4596645832,
2647,z scoring,"# def fcn_scoring_graph(input, config, mode):
    #     in_heatmap, pr_scores = input
    rois_per_image        = KB.int_shape(pr_scores)[2] 
    img_h, img_w          = config.IMAGE_SHAPE[:2]
    batch_size            = config.BATCH_SIZE
    num_classes           = config.NUM_CLASSES  
    heatmap_scale         = config.HEATMAP_SCALE_FACTOR
#     verbose               = config.VERBOSE
    CLASS_COLUMN          = 4
    SCORE_COLUMN          = 5
    DT_TYPE_COLUMN        = 6
    SEQUENCE_COLUMN       = 7
    NORM_SCORE_COLUMN     = 8
        
    print('\n ')
    print('---------------------------------------------')
    print('>>> FCN Scoring Graph  - mode:', mode)
    print('---------------------------------------------')
    logt('in_heatmap.shape  ', in_heatmap, verbose = verbose)
    logt('pr_hm_scores.shape', pr_scores, verbose = verbose )
    # rois per image is determined by size of input tensor 
    #   detection mode:   config.TRAIN_ROIS_PER_IMAGE 
    #   ground_truth  :   config.DETECTION_MAX_INSTANCES

    logt('pr_scores shape ', pr_scores, verbose = verbose)
    logt('rois_per_image  ', rois_per_image , verbose = verbose)
    logt('config.DETECTION_MAX_INSTANCES ', config.DETECTION_MAX_INSTANCES, verbose = verbose)
    logt('config.DETECTIONS_PER_CLASS    ', config.DETECTION_PER_CLASS, verbose = verbose)
    logt('SEQUENCE_COLUMN                ', SEQUENCE_COLUMN, verbose = verbose)
    logt('NORM_SCORE_COLUMN              ', NORM_SCORE_COLUMN, verbose = verbose)
    
    ##---------------------------------------------------------------------------------------------
    ## Stack non_zero bboxes from PR_SCORES into pt2_dense 
    ##---------------------------------------------------------------------------------------------
    # pt2_ind shape  : [?, 3] : [ {image_index, class_index , roi row_index }]
    # pt2_dense shape: [?, 11] : 
    #    pt2_dense[0:3]  roi coordinates 
    #    pt2_dense[4]    is class id 
    #    pt2_dense[5]    is score from mrcnn    
    #    pt2_dense[6]    is bbox sequence id    
    #    pt2_dense[7]    is normalized score (per class)    
    #-----------------------------------------------------------------------------
    pt2_sum = tf.reduce_sum(tf.abs(pr_scores[:,:,:,:CLASS_COLUMN]), axis=-1)
    pt2_ind = tf.where(pt2_sum > 0)
    pt2_dense = tf.gather_nd(pr_scores, pt2_ind)
    logt('in_heatmap       ', in_heatmap, verbose = verbose)
    logt('pr_scores.shape  ', pr_scores , verbose = verbose)
    logt('pt2_sum shape    ', pt2_sum   , verbose = verbose)
    logt('pt2_ind shape    ', pt2_ind   , verbose = verbose)
    logt('pt2_dense shape  ', pt2_dense , verbose = verbose)


    ##---------------------------------------------------------------------------------------------
    ##  Build mean and convariance tensors for bounding boxes
    ##---------------------------------------------------------------------------------------------
    # bboxes_scaled = tf.to_int32(tf.round(pt2_dense[...,0:4])) / heatmap_scale
    bboxes_scaled = pt2_dense[...,0:CLASS_COLUMN] / heatmap_scale
    width  = bboxes_scaled[:,3] - bboxes_scaled[:,1]      # x2 - x1
    height = bboxes_scaled[:,2] - bboxes_scaled[:,0]
    cx     = bboxes_scaled[:,1] + ( width  / 2.0)
    cy     = bboxes_scaled[:,0] + ( height / 2.0)
     
    covar_y = tf.sqrt(height * 0.5)
    covar_x = tf.sqrt(width * 0.5)
    
    covar  = tf.stack((width * 0.5 , height * 0.5), axis = -1)
    covar  = tf.sqrt(covar)          
    

    img_height = tf.ones_like(cy) * img_h
    img_width  = tf.ones_like(cx) * img_w
    
    start_y      = tf.maximum(cy[:]-covar[:,1], 0.0)
    end_y        = tf.minimum(cy[:]+covar[:,1], img_height)

    start_x      = tf.maximum(cx-covar[:,0], 0.0)
    end_x        = tf.minimum(cx+covar[:,0], img_width)    
    stk = tf.floor(tf.stack((start_y, end_y, start_x, end_x, img_height, img_width), axis = -1))
    
    logt(' cy  ',  cy, verbose = verbose)
    logt(' cx  ',  cx, verbose = verbose)
    logt(' covar  ',  covar, verbose = verbose)
    logt(' img_height  ',  img_height, verbose = verbose)
    logt(' img_height  ',  img_width, verbose = verbose)
    logt(' start y     ',  start_y, verbose = verbose)
    logt(' end y       ',  end_y, verbose = verbose)
    logt(' start x     ',  start_x, verbose = verbose)
    logt(' end x       ',  end_x, verbose = verbose)    
    logt(' stk         ',  stk , verbose = verbose)    
    
    masks = tf.map_fn(build_bbox_masks2, stk, dtype=tf.float32)
    logt(' masks         ',  masks , verbose = verbose)    
#     masks = tf.map_fn(build_bbox_masks2, [start_y, end_y, start_x , end_x , img_height, img_width], dtype=tf.float32)
#     masks = tf.map_fn(build_bbox_masks, [cx, cy, covar_x, covar_y, img_height, img_width], dtype=tf.float32)
    masks_exp = tf.expand_dims(masks, axis = 1)
    logt(' masks_exp         ',  masks_exp , verbose = verbose)    
    hm_indices = tf.cast(pt2_ind[:,0],dtype=tf.int32)
    logt('hm_indices  ',  hm_indices, verbose = verbose)
        
    pt2_mask_heatmaps = tf.gather(tf.transpose(in_heatmap, [0,3,1,2]), hm_indices )
    logt('pt2_mask_heatmaps',  pt2_mask_heatmaps, verbose = verbose)
    
    product = tf.multiply(pt2_mask_heatmaps, masks_exp)
    logt('product', product, verbose = verbose)
    mask_reduce_sum = tf.reduce_sum(product, axis = (-2,-1))
    logt('mask_sum_per_class:', mask_reduce_sum, verbose = verbose)
    ##---------------------------------------------------------------------------------------------
    ##  build indices and extract heatmaps corresponding to each bounding boxes' class id
    ##---------------------------------------------------------------------------------------------
    hm_indices = tf.cast(pt2_ind[:, :2],dtype=tf.int32)
    logt('hm_indices  ',  hm_indices, verbose = verbose)
    
    pt2_heatmaps = tf.transpose(in_heatmap, [0,3,1,2])
    logt('pt2_heatmaps',  pt2_heatmaps, verbose = verbose)
    
    pt2_heatmaps = tf.gather_nd(pt2_heatmaps, hm_indices )
    logt('pt2_heatmaps',  pt2_heatmaps, verbose = verbose)

    ##--------------------------------------------------------------------------------------------
    ## (0) Generate scores using prob_grid and pt2_dense
    ##--------------------------------------------------------------------------------------------
    old_style_scores = tf.map_fn(build_hm_score_v2, [pt2_heatmaps, bboxes_scaled, pt2_dense[:, NORM_SCORE_COLUMN]], 
                                 dtype = tf.float32, swap_memory = True)
    logt('old_style_scores',  old_style_scores, verbose = verbose)                      
                                                                      

    ##---------------------------------------------------------------------------------------------
    ## generate score based on gaussian using bounding box masks 
    ##---------------------------------------------------------------------------------------------
    alt_scores_1= tf.map_fn(build_hm_score_v3, [pt2_heatmaps, cy, cx,covar], dtype=tf.float32)    
    logt('alt_scores_1 ', alt_scores_1 , verbose = verbose)
    logt('masks  ', masks , verbose = verbose)

    ##---------------------------------------------------------------------------------------------
    ##  Scatter back to per-class tensor /  normalize by class
    ##---------------------------------------------------------------------------------------------
    alt_scores_1_norm = tf.scatter_nd(pt2_ind, alt_scores_1, 
                                    [batch_size, num_classes, rois_per_image, KB.int_shape(alt_scores_1)[-1]],
                                    name='alt_scores_1_norm')
    logt('alt_scores_1_scattered', alt_scores_1_norm, verbose = verbose)
    
    alt_scores_1_norm = normalize_scores(alt_scores_1_norm)
    logt('alt_scores_1_norm(by_class)', alt_scores_1_norm, verbose = verbose)
    
    alt_scores_1_norm = tf.gather_nd(alt_scores_1_norm, pt2_ind)
    logt('alt_scores_1_norm(by_image)', alt_scores_1_norm, verbose = verbose)

    ##---------------------------------------------------------------------------------------------
    ## Normalize fcn_heatmap (per class) to calculate alt_score_2
    ##--------------------------------------------------------------------------------------------
    logt('Normalize heatmap within each class !-------------------------------------', verbose = verbose)         
    in_heatmap_norm = tf.transpose(in_heatmap, [0,3,1,2])

    logt('in_heatmap_norm  ', in_heatmap_norm, verbose = verbose)
    ## normalize in class
    normalizer = tf.reduce_max(in_heatmap_norm, axis=[-2,-1], keepdims = True)
    normalizer = tf.where(normalizer < 1.0e-15,  tf.ones_like(normalizer), normalizer)
    in_heatmap_norm = in_heatmap_norm / normalizer
    
    # gauss_heatmap_sum_normalized = gauss_heatmap_sum / normalizer
    logt('normalizer shape ', normalizer, verbose = verbose)   
    logt('normalized heatmap  ', in_heatmap_norm, verbose = verbose)

    ##---------------------------------------------------------------------------------------------
    ##  build indices and extract heatmaps corresponding to each bounding boxes' class id
    ##  build alternative scores#  based on normalized/sclaked clipped heatmap
    ##---------------------------------------------------------------------------------------------
    hm_indices = tf.cast(pt2_ind[:, :2],dtype=tf.int32)
    logt('hm_indices shape',  hm_indices, verbose = verbose)
    
    pt2_heatmaps = tf.gather_nd(in_heatmap_norm, hm_indices )
    logt('pt2_heatmaps',  pt2_heatmaps, verbose = verbose)

    alt_scores_2 = tf.map_fn(build_hm_score_v3, [pt2_heatmaps, cy, cx,covar], dtype=tf.float32)    
    logt('alt_scores_2',alt_scores_2, verbose = verbose)
    
    alt_scores_2_norm = tf.scatter_nd(pt2_ind, alt_scores_2, 
                                     [batch_size, num_classes, rois_per_image, KB.int_shape(alt_scores_2)[-1]], name = 'alt_scores_2')  
    logt('alt_scores_2(scattered)', alt_scores_2_norm , verbose = verbose)
    
    alt_scores_2_norm = normalize_scores(alt_scores_2_norm)
    logt('alt_scores_2_norm(by_class)', alt_scores_2_norm, verbose = verbose)
    
    alt_scores_2_norm = tf.gather_nd(alt_scores_2_norm, pt2_ind)
    logt('alt_scores_2_norm(by_image)', alt_scores_2_norm, verbose = verbose)

    
    ##--------------------------------------------------------------------------------------------
    ##  Append alt_scores_1, alt_scores_1_norm to yield fcn_scores_dense 
    ##--------------------------------------------------------------------------------------------
    fcn_scores_dense = tf.concat([pt2_dense[:, : NORM_SCORE_COLUMN+1], old_style_scores, alt_scores_1, alt_scores_1_norm, alt_scores_2, alt_scores_2_norm], 
                                  axis = -1, name = 'fcn_scores_dense')
    logt('fcn_scores_dense    ', fcn_scores_dense , verbose = verbose)

    ##---------------------------------------------------------------------------------------------
    ##  Scatter back to per-image tensor 
    ##---------------------------------------------------------------------------------------------
    seq_ids = tf.to_int32( rois_per_image - pt2_dense[:, SEQUENCE_COLUMN] )
    scatter_ind= tf.stack([hm_indices[:,0], seq_ids], axis = -1, name = 'scatter_ind')

    fcn_scores_by_class = tf.scatter_nd(pt2_ind, fcn_scores_dense, 
                                        [batch_size, num_classes, rois_per_image, fcn_scores_dense.shape[-1]], name='fcn_hm_scores')
    
    # fcn_scores_by_image = tf.scatter_nd(scatter_ind, fcn_scores_dense, 
                                        # [batch_size, rois_per_image, fcn_scores_dense.shape[-1]], name='fcn_hm_scores_by_image')

    ##--------------------------------------------------------------------------------------------
    ##  Generate alt_score_1_logits
    ##--------------------------------------------------------------------------------------------
     
        
        
        
        
        
        
        
        
        
    logt('seq_ids             ', seq_ids, verbose = verbose)
    logt('sscatter_ids        ', scatter_ind, verbose = verbose)
    logt('fcn_scores_by_class ', fcn_scores_by_class, verbose = verbose)
    # logt('fcn_scores_by_image ', fcn_scores_by_image) 
    logt('complete', verbose = verbose)
   
#     return fcn_scores_by_class",0.4590299129,
526,encoding,"# Function to create a age bracket labels
# There is probably a more pythonic way to do this, but this works

def generate_age_label(row):
    age_bracket = row['age']
    if age_bracket < 15:
        return ""0-15""
    elif age_bracket < 25:
        return ""15-25""
    elif age_bracket < 35:
        return ""25-35""
    elif age_bracket < 45:
        return ""35-45""
    else:
        return ""45-""",0.3910042048,
526,encoding,"# Create a function to classify the students among high score, medium score
# or poor score according to the sat_score column
def score(row): 
    mark= row['sat_score']
    if mark >= 1500:
        return 'High score'
    elif (mark > 1100) and (mark < 1500):
        return 'Medium score'
    elif mark <=1100:
        return 'Poor score'  
print(combined.columns)
combined['ranking_school'] = combined.apply(score,axis=1)
# Display schools with High and Medium score
print(combined[combined['ranking_school']== 'High score']['SCHOOL NAME'])
print(combined[combined['ranking_school']== 'Poor score']['SCHOOL NAME'])",0.3853428662,
526,encoding,"# function to return name of model type
def get_model_name(model_type):
    if model_type == log_reg:
        model_name = ""Logistic Regression""
    elif model_type == lda:
        model_name = ""LDA""
    elif model_type == qda:
        model_name = ""QDA""
    elif model_type == knn:
        model_name = ""KNN""
    elif model_type == rfc:
        model_name = ""Random Forests""
    elif model_type == gbc:
        model_name = ""Boosting""
    elif model_type == svm:
        model_name = ""SVM""
    elif model_type == dtc:
        model_name = ""Decision Tree""  
    elif model_type == nb:
        model_name = ""Naive Bayes""          
    else:
        model_name = """"
        
    return model_name",0.3842862844,
526,encoding,"# function to return name of model type
def get_model_name(model_type):
    if model_type == log_reg:
        model_name = ""logistic regression""
    elif model_type == lda:
        model_name = ""LDA""
    elif model_type == qda:
        model_name = ""QDA""
    elif model_type == knn:
        model_name = ""KNN""
    elif model_type == rfc:
        model_name = ""random forests""
    elif model_type == boost:
        model_name = ""boost""
    elif model_type == svm:
        model_name = ""SVM""
    else:
        model_name = """"
        
    return model_name",0.3842862844,
526,encoding,"def my_classifier(row):
    if row['total_UPDRS'] > 50:
        return 0
    elif row['total_UPDRS'] > 40: 
        return 1
    elif row['total_UPDRS'] > 30:
        return 2
    elif row['total_UPDRS'] > 20:
        return 3
    elif row['total_UPDRS'] > 10:
        return 4
    else:
        return 5
    
quantum = df1.apply(my_classifier, axis=1)

df1['updrs']= quantum",0.3821679354,
526,encoding,"# Function that would unify or organize the title based on the titles with missing ages
def unify_title(df):
    '''
    Returns the respective title
    '''
    if df in ['Don', 'Rev', 'Major', 'Sir', 'Col', 'Capt', 'Jonkheer', 'Mr']:
        return 'Mr'
    elif df in ['Lady', 'the Countess', 'Mrs']:
        return 'Mrs'
    elif df in ['Mme', 'Mlle', 'Ms', 'Miss']:
        return 'Miss'
    else:
        return df",0.3817741871,
526,encoding,"def clean_up_bool(x):
    if x in {'No', 'false', 'False'}:
        return ""No""
    elif x in {'Yes', 'true', 'True'}:
        return ""Yes""
    else:
        return x",0.3816809654,
526,encoding,"def get_id(row):
    cd115 = row.CD115FP
    if cd115 == 'ZZ':
        return None
    return mapping.get(str(int(cd115))+fips_map[str(row.STATEFP)])",0.3805263042,
526,encoding,"def map_crime_type(crime):
    """"""
    Violent crimes are murder and nonnegligent manslaughter, forcible rape, robbery, and aggravated assault.     
    Property crime includes the offenses of burglary, larceny-theft, motor vehicle theft, and arson.
    
    Reference:
    FBIs Uniform Crime Reporting (UCR) Program
    https://ucr.fbi.gov/crime-in-the-u.s/2010/crime-in-the-u.s.-2010/violent-crime
    
    """"""
    if crime in ('ASSAULT', 'ROBBERY', 'SEX OFFENSES, FORCIBLE'):
        return 'VIOLENT CRIMES'
    elif crime in ('BURGLARY', 'VANDALISM', 'LARCENY/THEFT', 'VEHICLE THEFT', 'ARSON', 'STOLEN PROPERTY'):
        return 'PROPERTY CRIMES'
    elif crime in ('DRUG/NARCOTIC', 'DRUNKENNESS', 'LIQUOR LAWS', 'DRIVING UNDER THE INFLUENCE'):
        return 'SUBSTANCE-BASED CRIMES'
    elif crime in ('NON-CRIMINAL'):
        return 'NO CRIMES'
    else:
        return 'OTHER CRIMES'",0.3802796006,
526,encoding,"def compute_season(value):
    # Sources for determining season:
    # 1) Googling ""official spring/summer/fall/winter dates""
    # 2) https://simple.wikipedia.org/wiki/Solstice
    if (value > '03-19' and value < '06-21'):
        return 'Spring'
    elif (value > '06-20' and value < '09-23'):
        return 'Summer'
    elif (value > '09-22' and value < '12-22'):
        return 'Fall'
    elif ((value > '12-21' and value <= '12-31') or (value >= '01-01' and value < '3-20')):
        return 'Winter'
    else:
        return 'Error'
    
q2['MONTH-DAY'] = q2.index.map(lambda x: x.strftime(""%m-%d""))
q2['SEASON'] = q2['MONTH-DAY'].map(lambda x: compute_season(x))
q2['SEASON'].value_counts()",0.3778021634,
1563,preliminary visualisation,"def onclick(event):
    '''
    This function performs all of the work in this notebook. We have put all of the
    processing within this function so that all the calculations are done following
    a 'click' and therefore do not need to be in separate cells. This makes the notebook
    smoother and minimises the cells that need to be manually run by the user once a 
    location has been selected. 

    This particular widget function uses the selected to location to plot two images:
    - spectra for the pixel closest to the chosen location
    - spectra for the pixel/s closest to the chosen location using the 60m pixel as
    a bounding box for the higher resolution pixels.
    These two images are run in the subsequent cells.
    '''
    global pixelx, pixely, spectra, spectramin, spectramax, spectramean
    pixelx, pixely = int(event.xdata), int(event.ydata)
    w.value = 'pixelx : {}, pixely : {}'.format(pixelx, pixely)
    plt.plot(pixelx, pixely, 'ro', markersize=5)

    # Find the pixel closest to the chosen pixel at each resolution
    Pixel10m = Resolution10m.sel(y=pixely, x=pixelx, method='nearest')
    Pixel20m = Resolution20m.sel(y=pixely, x=pixelx, method='nearest')
    Pixel60m = Resolution60m.sel(y=pixely, x=pixelx, method='nearest')

    # Grab the pixel spectral values for each band
    spectra = [Pixel60m.nbar_coastal_aerosol.isel(time=mytime).values,
               Pixel10m.nbar_blue.isel(time=mytime).values,
               Pixel10m.nbar_green.isel(time=mytime).values,
               Pixel10m.nbar_red.isel(time=mytime).values,
               Pixel20m.nbar_red_edge_1.isel(time=mytime).values,
               Pixel20m.nbar_red_edge_2.isel(time=mytime).values,
               Pixel20m.nbar_red_edge_3.isel(time=mytime).values,
               Pixel10m.nbar_nir_1.isel(time=mytime).values,
               Pixel20m.nbar_nir_2.isel(time=mytime).values,
               Pixel20m.nbar_swir_2.isel(time=mytime).values,
               Pixel20m.nbar_swir_3.isel(time=mytime).values,
               ]

    # Get the location of the selected pixel at the coursest resolution
    Pixel60m = Resolution60m.sel(y=pixely, x=pixelx, method='nearest')

    # Find the index locations of the lat/lon of that pixel
    xindex = Resolution60m.indexes['x'].get_loc(
        Pixel60m.x.values.item(), method='nearest')
    yindex = Resolution60m.indexes['y'].get_loc(
        Pixel60m.y.values.item(), method='nearest')

    # Get the index for the pixels next to the chosen pixel
    xmax = Resolution60m.x.isel(x=xindex+1)
    xmin = Resolution60m.x.isel(x=xindex-1)
    ymax = Resolution60m.y.isel(y=yindex-1)
    ymin = Resolution60m.y.isel(y=yindex+1)

    # Now work out what the lat/lon is for halfway between pixel +-1 (to keep our resolution at 60 x 60 m)
    latmin = mean((xmin, pixelx))
    latmax = mean((pixelx, xmax))
    lonmin = mean((ymin, pixely))
    lonmax = mean((pixely, ymax))

    # Grab all of the pixels that fall within the 60m pixel bounds
    bluepixels = Resolution10m.nbar_blue.sel(
        x=slice(latmin, latmax), y=slice(lonmax, lonmin))
    greenpixels = Resolution10m.nbar_green.sel(
        x=slice(latmin, latmax), y=slice(lonmax, lonmin))
    redpixels = Resolution10m.nbar_red.sel(
        x=slice(latmin, latmax), y=slice(lonmax, lonmin))
    rededge1pixels = Resolution20m.nbar_red_edge_1.sel(
        x=slice(latmin, latmax), y=slice(lonmax, lonmin))
    rededge2pixels = Resolution20m.nbar_red_edge_2.sel(
        x=slice(latmin, latmax), y=slice(lonmax, lonmin))
    rededge3pixels = Resolution20m.nbar_red_edge_3.sel(
        x=slice(latmin, latmax), y=slice(lonmax, lonmin))
    nir1pixels = Resolution10m.nbar_nir_1.sel(
        x=slice(latmin, latmax), y=slice(lonmax, lonmin))
    nir2pixels = Resolution20m.nbar_nir_2.sel(
        x=slice(latmin, latmax), y=slice(lonmax, lonmin))
    swir2pixels = Resolution20m.nbar_swir_2.sel(
        x=slice(latmin, latmax), y=slice(lonmax, lonmin))
    swir3pixels = Resolution20m.nbar_swir_3.sel(
        x=slice(latmin, latmax), y=slice(lonmax, lonmin))

    # Grab the min, max and mean of the pixels within the 60m bounding box
    spectramin = [Pixel60m.nbar_coastal_aerosol.isel(time=mytime).min().item(),
                  bluepixels.isel(time=mytime).min().item(),
                  greenpixels.isel(time=mytime).min().item(),
                  redpixels.isel(time=mytime).min().item(),
                  rededge1pixels.isel(time=mytime).min().item(),
                  rededge2pixels.isel(time=mytime).min().item(),
                  rededge3pixels.isel(time=mytime).min().item(),
                  nir1pixels.isel(time=mytime).min().item(),
                  nir2pixels.isel(time=mytime).min().item(),
                  swir2pixels.isel(time=mytime).min().item(),
                  swir3pixels.isel(time=mytime).min().item(),
                  ]

    spectramax = [Pixel60m.nbar_coastal_aerosol.isel(time=mytime).max().item(),
                  bluepixels.isel(time=mytime).max().item(),
                  greenpixels.isel(time=mytime).max().item(),
                  redpixels.isel(time=mytime).max().item(),
                  rededge1pixels.isel(time=mytime).max().item(),
                  rededge2pixels.isel(time=mytime).max().item(),
                  rededge3pixels.isel(time=mytime).max().item(),
                  nir1pixels.isel(time=mytime).max().item(),
                  nir2pixels.isel(time=mytime).max().item(),
                  swir2pixels.isel(time=mytime).max().item(),
                  swir3pixels.isel(time=mytime).max().item(),
                  ]

    spectramean = [Pixel60m.nbar_coastal_aerosol.isel(time=mytime).mean().item(),
                   bluepixels.isel(time=mytime).mean().item(),
                   greenpixels.isel(time=mytime).mean().item(),
                   redpixels.isel(time=mytime).mean().item(),
                   rededge1pixels.isel(time=mytime).mean().item(),
                   rededge2pixels.isel(time=mytime).mean().item(),
                   rededge3pixels.isel(time=mytime).mean().item(),
                   nir1pixels.isel(time=mytime).mean().item(),
                   nir2pixels.isel(time=mytime).mean().item(),
                   swir2pixels.isel(time=mytime).mean().item(),
                   swir3pixels.isel(time=mytime).mean().item(),
                   ]",0.4848329127,
1563,preliminary visualisation,"def plotMarker(step):
    plt.clf()
    arrLithI = numpy.where(mIVar.data == lith_id)[0][::5]
    plt.gca().set_aspect('equal')
    plt.xlim(minX,maxX)
    plt.ylim(minY,maxY)
    plt.scatter(mSwarm.particleCoordinates.data[arrLithI,0],mSwarm.particleCoordinates.data[arrLithI,1],alpha=0.5,lw=0)
    plt.scatter(markerSwarm.particleCoordinates.data[:,0],markerSwarm.particleCoordinates.data[:,1],c='red',s=50)
    plt.savefig('IMG/Marker_%i.pdf' %step)
    
plotMarker(0)",0.4756600857,
1563,preliminary visualisation,"def initialize_filter(kf, std_R=None):
    """""" helper function - we will be reinitialing the filter
    many times.
    """"""
    kf.x.fill(0.)
    kf.P = np.eye(kf.dim_x) * .1
    if std_R is not None:
        kf.R = np.eye(kf.dim_z) * std_R",0.474865526,
1563,preliminary visualisation,"# Visualize the weights of the network

def show_net_weights(net):
  W1 = net.params['W1']
  W1 = W1.reshape(28, 28, -1).transpose(2, 0, 1)
  plt.imshow(util.visualize_grid(W1, padding=3).astype('uint8'))
  plt.gca().axis('off')
  plt.show()",0.4747597575,
1563,preliminary visualisation,"def plot_hmm(hmm):
    state_to_label = {s: s.name for i,s in enumerate(hmm.states)}
    nx.draw_networkx(hmm.graph, labels=state_to_label)",0.4710380435,
1563,preliminary visualisation,"def plot_safe_set(lyapunov, show=True):
    """"""Plot the safe set for a given Lyapunov function.""""""
    plt.imshow(lyapunov.safe_set.reshape(num_states).T,
               origin='lower',
               extent=lyapunov.discretization.limits.ravel(),
               vmin=0,
               vmax=1)
    
    if isinstance(lyapunov.dynamics, safe_learning.UncertainFunction):
        X = lyapunov.dynamics.functions[0].X
        plt.plot(X[:, 0], X[:, 1], 'rx')
    
    plt.title('safe set')
    plt.colorbar()
    if show:
        plt.show()
    
lyapunov.update_safe_set()
plot_safe_set(lyapunov)",0.4695179462,
1563,preliminary visualisation,"# Visualize the weights of the network

def show_net_weights(net):
    W1 = net.params['W1']
    W1 = W1.reshape(28, 28, 1, -1).transpose(3, 0, 1, 2)
    plt.imshow(visualize_grid(W1, padding=3).astype('uint8').squeeze(axis=2))
    plt.gca().axis('off')
    plt.show()

show_net_weights(net)",0.4677380025,
1563,preliminary visualisation,"def plotMarker(step):
    plt.clf()
    arrLithI = numpy.where(mIVar.data == lith_id)[0][::2]
    plt.gca().set_aspect('equal')
    plt.xlim(minX,maxX)
    plt.ylim(minY,maxY)
    plt.scatter(mSwarm.particleCoordinates.data[arrLithI,0],mSwarm.particleCoordinates.data[arrLithI,1],alpha=0.5,lw=0,s=5)
    plt.scatter(markerSwarm.particleCoordinates.data[:,0],markerSwarm.particleCoordinates.data[:,1],c='red',s=50)
    plt.savefig(outputPath + 'Marker_%i.pdf' %step)

if size == 1:
    plotMarker(0)",0.4669941664,
1563,preliminary visualisation,"def colourmap_picker(cmap):
    global grey
    
    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.imshow(grey, cmap = cmap)
    ax.axis(""off"")

interact(colourmap_picker,
         cmap = (""gray"", ""Greys"", ""gist_heat"", ""bone"", # sequential
                 ""magma"", ""viridis"", # perceptually uniform
                 ""seismic"", ""PRGn"", ""BrBG_r"", # diverging
                 ""Set3"", ""Accent_r"", ""Pastel1"", # qualitative
                 ""jet"", ""terrain_r"", ""prism"")) # misc",0.4662422836,
1563,preliminary visualisation,"def cam(img_path):
    from keras.applications.vgg16 import VGG16
    import matplotlib.image as mpimg
    from keras import backend as K
    import matplotlib.pyplot as plt
    %matplotlib inline
    K.clear_session()
    
    model = VGG16(weights='imagenet')
    img=mpimg.imread(img_path)
    plt.imshow(img)
    from keras.preprocessing import image
    img = image.load_img(img_path, target_size=(224, 224))
    x = image.img_to_array(img)
    x = np.expand_dims(x, axis=0)
    from keras.applications.vgg16 import preprocess_input
    x = preprocess_input(x)
    preds = model.predict(x)
    predictions = pd.DataFrame(decode_predictions(preds, top=3)[0],columns=['col1','category','probability']).iloc[:,1:]
    argmax = np.argmax(preds[0])
    output = model.output[:, argmax]
    last_conv_layer = model.get_layer('block5_conv3')
    grads = K.gradients(output, last_conv_layer.output)[0]
    pooled_grads = K.mean(grads, axis=(0, 1, 2))
    iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])
    pooled_grads_value, conv_layer_output_value = iterate([x])
    for i in range(512):
        conv_layer_output_value[:, :, i] *= pooled_grads_value[i]
    heatmap = np.mean(conv_layer_output_value, axis=-1)
    heatmap = np.maximum(heatmap, 0)
    heatmap /= np.max(heatmap)
    import cv2
    img = cv2.imread(img_path)
    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))
    heatmap = np.uint8(255 * heatmap)
    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)
    hif = .8
    superimposed_img = heatmap * hif + img
    output = 'G:/output.jpeg'
    cv2.imwrite(output, superimposed_img)
    img=mpimg.imread(output)
    plt.imshow(img)
    plt.axis('off')
    plt.title(predictions.loc[0,'category'].upper())
    return None",0.465603143,
2295,task preprocess the digits dataset,"def get_digits_binary_data(mc=False):
    dataset = sklearn.datasets.load_digits()
    X, y = dataset.data, dataset.target
    if mc:
        return sklearn.model_selection.train_test_split(X, y, random_state=0)
    
    # create digits with 2 classes: negative class 0: not 1,  positive class 1: 1
    y_binary_imbalanced = y.copy()
    y_binary_imbalanced[y_binary_imbalanced != 1] = 0
    return sklearn.model_selection.train_test_split(X, y_binary_imbalanced, random_state=0)

def model_report(title, clf, *args):
    '''
    Display report of a classification model for clf
    if len(args) == 2: X_test, y_test
    if len(args) == 4: X_train, X_test, y_train, y_test
    '''
    assert isinstance(clf, sklearn.base.BaseEstimator), ""2nd arg must be classifier""
    
    X_train, X_test, y_train, y_test = [None] * 4
    if len(args) == 2:
        X_test, y_test = args
    elif len(args) == 4:
        X_train, X_test, y_train, y_test = args

    title = ""{} - {}"".format(type(clf).__name__, title)
    params = ['kernel', 'gamma', 'C', 'strategy']
    d = clf.get_params()
    print(""{}: {}"".format(title, "","".join(""{}={}"".format(p, d[p]) for p in params if p in d)))
    if X_train is not None:
        print('Train Accuracy: {:.2f}'.format(clf.score(X_train, y_train)))
        
    if X_test is not None:

        y_predict = clf.predict(X_test)
        cf = sklearn.metrics.confusion_matrix(y_test, y_predict)
        cols = ['Pred:Neg', 'Pred:Pos']
        labels = ['Class:Neg', 'Class:Pos']
        str_cf = [[f""{cf[0][0]}(TN)"", f""{cf[0][1]}(FP)""], [f""{cf[1][0]}(FN)"", f""{cf[1][1]}(TP)""]]
        df_confusion = pd.DataFrame(str_cf, columns=cols, index=labels)
        print('Confusion martix:\n{}'.format(df_confusion))
        print()
        metrics = (
            ('Test Accuracy (TP+TN)/(ALL)', sklearn.metrics.accuracy_score(y_test, y_predict)),
            ('Precision TP/(TP+FP)', sklearn.metrics.precision_score(y_test, y_predict)),
            ('Recall TP/(TP+FN)', sklearn.metrics.recall_score(y_test, y_predict)),
            ('F1 (2*Prec*Recall)/(Prec+Recall)', sklearn.metrics.f1_score(y_test, y_predict))
            )
        w = max(len(t[0]) for t in metrics) + 1
        for n, v in metrics:
            print('{:{w}}: {:.2f}'.format(n, v, w=w))
        print()
        print(sklearn.metrics.classification_report(y_test, y_predict, target_names=['not 1', '1']))",0.4947754741,
2295,task preprocess the digits dataset,"class Numbers:
    def __init__(self):
        digits = sklearn.datasets.load_digits()
    
        self.train_x, self.test_x, self.train_y, self.test_y = \
            train_test_split(digits.data, digits.target, test_size=0.2)
        
    def report(self):
        """"""
        Report information about the dataset using the print() function
        """"""
        print(""\t#data\t#pixels"")
        print(""train\t{}\t{}"".format(self.train_x.shape[0], self.train_x.shape[1]))
        print(""test\t{}\t{}"".format(self.test_x.shape[0], self.test_x.shape[1]))   
        

    def classify(self):
        """"""
        Create a classifier using the training data and generate a confusion matrix for the test data
        Return errors dict
        """"""
        class_digits = KNNClassifier(self.train_x, self.train_y)
        confMatrix = class_digits.confusionMatrix(self.test_x, self.test_y)
        class_digits.printConfMatrix(confMatrix)
        
        print(""Accuracy: {:.2f}%"".format(class_digits.accuracy(confMatrix)*100))
        
        return class_digits._errors
    
    def viewDigit(self, digitImage):
        """"""
        Display an image of a digit
        PARAMETERS
        digitImage - a data object from the dataset
        """"""
        plt.gray()
        plt.matshow(digitImage)
        plt.show()",0.4944097996,
2295,task preprocess the digits dataset,"def reset_train_full():
    global train_full
    train_full= pd.read_csv('train.csv')
    train_full.set_index('PassengerId',inplace=True)",0.4916095138,
2295,task preprocess the digits dataset,"import pandas as pd
import random
def sample_file():
    global skip_idx
    global train_data
    global X_train
    global y_train
    big_train='lsml-Bigtrainm'

    # Count number of rows entire set
    num_lines = sum(1 for i in open(big_train))

    # Sample size - fraction of the train-set 1/3
    size = int(num_lines / 3)

    # skip indexes and keep indices
    skip_idx = random.sample(range(1, num_lines), num_lines - size)
    train_data = pd.read_csv(big_train, skiprows=skip_idx)  
    X_train=train_data.drop(train_data.columns[[0]], axis=1)
    y_train = train_data.ix[:,0]",0.4865825176,
2295,task preprocess the digits dataset,"def load_digits():
    import sklearn.datasets
    data = sklearn.datasets.load_digits()
    return data['data'].T, data['target']",0.4864455163,
2295,task preprocess the digits dataset,"def load_prediction_model_weights(args):
    try:
        model.load_weights(args.weights_file[0])
        print (""Loaded model weights from: "" + str(args.weights_file[0]))
        return model
        
    except:
        print (""Error loading model weights ..."")
        sys.exit(1)",0.4862628579,
2295,task preprocess the digits dataset,"def plot_lat_lon():
    global training_data
    data = training_data[(training_data.longitude < 0) & (training_data.latitude > 0)]
    plt.figure(figsize=(7,6))
    for klass in ['low', 'medium', 'high']:
        subdata = data[data.interest_level == klass]
        plt.scatter(subdata['longitude'], subdata['latitude'], alpha=0.4)
    plt.legend(['low', 'medium', 'high'])
    plt.show()
plot_lat_lon()",0.4820449054,
2295,task preprocess the digits dataset,"# Create a function that takes a filepath as an input
# and uses a tempory dictionary in order to feed 3 global variables
def process_harvested_file(filepath):
    
    # Set global variables
    global processed_papers
    global corpus_words_freq
    global abs_counter
    
    # Start timer
    _start_time = time.time()
    
    # Load file into a soup
    soup = BeautifulSoup(open(filepath), ""lxml"")
    records = soup.find_all('record')

    # Create the temporary dictionary with the variables needed
    for r in records:
        tmp_paper = dict()
        tmp_paper['identifier'] = r.header.identifier.contents[0].encode('utf-8')
        tmp_paper['title'] = r.metadata.title.contents[0].encode('utf-8')
        tmp_paper['abstract'] = r.metadata.abstract.contents[0].encode('utf-8')
        
        # Create new keys with empty dicts as values in the temporary dictionary
        tmp_paper['tf'] = list()
        tmp_paper['idf'] = dict()
        tmp_paper['tfidf'] = dict()

        # Use the tokenizer() function to tokenize the abstract
        tokenized = tokenizer(tmp_paper['abstract'])
        
        # Use the remove_stopwords() function to clean and lowercase tokens
        valuable_tokens = remove_stopwords(tokenized)
        
        # Use the token_lemmatizer() function to lemmatize tokens
        lemmatized_tokens = token_lemmatizer(valuable_tokens)
        
        # Create a new key in the temporary dictionary with the proccessed tokens
        tmp_paper['words'] = lemmatized_tokens

        # Feed counter with unique number of tokens
        for token in tmp_paper['words']:
            if token not in corpus_words_freq:
                corpus_words_freq[token] = 1
            else:
                corpus_words_freq[token] += 1

        
        # Feed the tf key with the frequency distance of the proccesed tokens
        tmp_paper['tf'] = nltk.FreqDist(tmp_paper['words'])
        processed_papers[tmp_paper['identifier']] = tmp_paper

        # Use this function to track processing state of abstracts
        abs_counter += 1
        if abs_counter % 1000 == 0:
            print abs_counter,'papers have been input into the engine'

    # Stop timer        
    _stop_time = time.time()
    print(""Finished harvesting file %s in: %s"" % (filepath.split(""/"")[-1], str(datetime.timedelta(seconds=_stop_time-_start_time))))",0.4795601964,
2295,task preprocess the digits dataset,"class Numbers2:
    def __init__(self, trainPercentage):
        digits = sklearn.datasets.load_digits()
    
        self.train_x, self.test_x, self.train_y, self.test_y = \
            train_test_split(digits.data, digits.target, test_size=trainPercentage)

    def classify(self, k):
        """"""
        Create a classifier using the training data and generate a confusion matrix for the test data
        Return accuracy
        """"""
        class_digits = KNNClassifier(self.train_x, self.train_y, k)
        confMatrix = class_digits.confusionMatrix(self.test_x, self.test_y)
        accuracy = class_digits.accuracy(confMatrix)
            
        return accuracy
        
    def viewDigit(digitImage):
        """"""
        Display an image of a digit
        PARAMETERS
        digitImage - a data object from the dataset
        """"""
        plt.gray()
        plt.matshow(digitImage)
        plt.show()",0.4792517126,
2295,task preprocess the digits dataset,"# Create a function that calculates the idf
def calculate_idf():
    
    # Set global variables
    global processed_papers
    global corpus_words_freq
    
    # Start timer
    _start_time = time.time()
    
    idf_counter = 0
    total_texts = len(processed_papers)
    for paper in processed_papers.keys():
        for word in processed_papers[paper]['tf'].keys():
            processed_papers[paper]['idf'][word] = np.log(total_texts/corpus_words_freq[word])
        idf_counter += 1
        
        # Use this function to track processing state of IDFs
        if idf_counter % 5000 == 0:
            print idf_counter,'IDF\'s calculated.'
    
    # Stop timer
    _stop_time = time.time()
    print(""Finished IDF in: %s"" % str(datetime.timedelta(seconds=_stop_time-_start_time)))",0.4785823822,
1103,load mnist,"def load_mnist():
    mnist = input_data.read_data_sets('../data/', one_hot=True)
    trainimg,trainlabel = mnist.train.images,mnist.train.labels
    testimg,testlabel = mnist.test.images,mnist.test.labels
    valimg,vallabel = mnist.validation.images,mnist.validation.labels
    return trainimg,trainlabel,testimg,testlabel,valimg,vallabel
# Demo usage of mnist loader
if __name__=='__main__':
    trainimg,trainlabel,testimg,testlabel,valimg,vallabel = load_mnist()
    print (""We have [%d] train, [%d] test, and [%d] validation images.""
           %(trainimg.shape[0],testimg.shape[0],valimg.shape[0]))",0.487754941,
1103,load mnist,"def get_MNIST_data(num_training=50000, num_validation=10000):
    path_to_dataset = 'data/datasets'

    # load raw MNIST data
    # https://raw.githubusercontent.com/amitgroup/amitgroup/master/amitgroup/io/mnist.py
    X_train, y_train = util.load_mnist('training', path=path_to_dataset, return_labels=True)
    X_test, y_test = util.load_mnist('testing', path=path_to_dataset, return_labels=True)
    
    # subsample training data to training and validation data
    mask = range(num_training, num_training + num_validation)
    X_val = X_train[mask]
    y_val = y_train[mask]
    mask = range(num_training)
    X_train = X_train[mask]
    y_train = y_train[mask]

    # reshape the image data into rows
    X_train = np.reshape(X_train, (X_train.shape[0], -1))
    X_val = np.reshape(X_val, (X_val.shape[0], -1))
    X_test = np.reshape(X_test, (X_test.shape[0], -1))
    
    # No 'bias trick' needed here. Our NN already has bias params.

    return X_train, y_train, X_val, y_val, X_test, y_test

X_train, y_train, X_val, y_val, X_test, y_test = get_MNIST_data()
print 'Train data shape: ', X_train.shape
print 'Train labels shape: ', y_train.shape
print 'Validation data shape: ', X_val.shape
print 'Validation labels shape: ', y_val.shape
print 'Test data shape: ', X_test.shape
print 'Test labels shape: ', y_test.shape",0.4708067179,
1103,load mnist,"def get_MNIST_data(num_training=50000, num_validation=10000):
    path_to_dataset = 'data/datasets'

    # load raw MNIST data
    # https://raw.githubusercontent.com/amitgroup/amitgroup/master/amitgroup/io/mnist.py
    X_train, y_train = util.load_mnist('training', path=path_to_dataset, return_labels=True)
    X_test, y_test = util.load_mnist('testing', path=path_to_dataset, return_labels=True)
    
    # subsample training data to training and validation data
    mask = range(num_training, num_training + num_validation)
    X_val = X_train[mask]
    y_val = y_train[mask]
    mask = range(num_training)
    X_train = X_train[mask]
    y_train = y_train[mask]

    # reshape the image data into rows
    X_train = np.reshape(X_train, (X_train.shape[0], -1))
    X_val = np.reshape(X_val, (X_val.shape[0], -1))
    X_test = np.reshape(X_test, (X_test.shape[0], -1))

    # normalize the data by subtracting the mean image
    mean_image = np.mean(X_train, axis=0)
    X_train -= mean_image
    X_val -= mean_image
    X_test -= mean_image

    # add bias dimension and transform into columns
    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])
    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])
    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])

    return X_train, y_train, X_val, y_val, X_test, y_test

X_train, y_train, X_val, y_val, X_test, y_test = get_MNIST_data()
print 'Train data shape: ', X_train.shape
print 'Train labels shape: ', y_train.shape
print 'Validation data shape: ', X_val.shape
print 'Validation labels shape: ', y_val.shape
print 'Test data shape: ', X_test.shape
print 'Test labels shape: ', y_test.shape",0.469647944,
1103,load mnist,"if ""CIFAR10_EXP_1"" not in os.environ:
    sys.stderr.write(""loading mnist...\n"")
    train_data, valid_data, _ = hp.load_mnist(""../../data/mnist.pkl.gz"")
    X_train, y_train = train_data
    X_valid, y_valid = valid_data
    # minimal
    X_train_minimal = X_train[0:200]
    y_train_minimal = y_train[0:200]
    # ---
    X_train = theano.shared(np.asarray(X_train, dtype=theano.config.floatX), borrow=True)
    y_train = theano.shared(np.asarray(y_train, dtype=theano.config.floatX), borrow=True)
    X_valid = theano.shared(np.asarray(X_valid, dtype=theano.config.floatX), borrow=True)
    y_valid = theano.shared(np.asarray(y_valid, dtype=theano.config.floatX), borrow=True)
    # minimal
    X_train_minimal = theano.shared(np.asarray(X_train_minimal, dtype=theano.config.floatX), borrow=True)
    y_train_minimal = theano.shared(np.asarray(y_train_minimal, dtype=theano.config.floatX), borrow=True)
    # ---
    #y_train = T.cast(y_train, ""int32"")
    #y_valid = T.cast(y_valid, ""int32"")
    # minimal
    #y_train_minimal = T.cast(y_train_minimal, ""int32"")
    # ---
else:
    sys.stderr.write(""loading cifar10...\n"")
    dat = deep_residual_learning_CIFAR10.load_data()
    X_train_and_valid = dat[""X_train""]
    y_train_and_valid = dat[""Y_train""]
    
    X_train_minimal = theano.shared(X_train_and_valid[0:100].astype(theano.config.floatX), borrow=True)
    y_train_minimal = theano.shared(y_train_and_valid[0:100].astype(theano.config.floatX), borrow=True)
    
    X_test = theano.shared(dat[""X_test""].astype(theano.config.floatX), borrow=True)
    y_test = theano.shared(dat[""Y_test""].astype(theano.config.floatX), borrow=True)
    n = X_train_and_valid.shape[0]
    X_train = theano.shared(X_train_and_valid[0 : 0.85*n].astype(theano.config.floatX), borrow=True)
    y_train = theano.shared(y_train_and_valid[0 : 0.85*n].astype(theano.config.floatX), borrow=True)
    X_valid = theano.shared(X_train_and_valid[0.85*n :: ].astype(theano.config.floatX), borrow=True)
    y_valid = theano.shared(y_train_and_valid[0.85*n :: ].astype(theano.config.floatX), borrow=True)",0.4562706351,
1103,load mnist,"path = ""./data""

def load_mnist(path, kind='train'):
    import os
    import gzip
    import numpy as np

    """"""Load MNIST data from `path`""""""
    labels_path = os.path.join(path,
                               '%s-labels-idx1-ubyte.gz'
                               % kind)
    images_path = os.path.join(path,
                               '%s-images-idx3-ubyte.gz'
                               % kind)

    with gzip.open(labels_path, 'rb') as lbpath:
        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,
                               offset=8)

    with gzip.open(images_path, 'rb') as imgpath:
        images = np.frombuffer(imgpath.read(), dtype=np.uint8,
                               offset=16).reshape(len(labels), 784)

    return images, labels",0.4553643465,
1103,load mnist,"# Fashion MNIST
def load_mnist(path, kind='train'):
    import os
    import struct
    import gzip
    import numpy as np

    """"""Load MNIST data from `path`""""""
    labels_path = os.path.join(path,
                               '%s-labels-idx1-ubyte.gz'
                               % kind)
    images_path = os.path.join(path,
                               '%s-images-idx3-ubyte.gz'
                               % kind)

    with gzip.open(labels_path, 'rb') as lbpath:
        struct.unpack('>II', lbpath.read(8))
        labels = np.frombuffer(lbpath.read(), dtype=np.uint8)

    with gzip.open(images_path, 'rb') as imgpath:
        struct.unpack("">IIII"", imgpath.read(16))
        images = np.frombuffer(imgpath.read(), dtype=np.uint8).reshape(len(labels), 784)

    return images, labels",0.4487219453,
1103,load mnist,"def load_mnist(path, kind='train'):

    labels_path = os.path.join(path,
                               '%s-labels-idx1-ubyte.gz'
                               % kind)
    images_path = os.path.join(path,
                               '%s-images-idx3-ubyte.gz'
                               % kind)

    with gzip.open(labels_path, 'rb') as lbpath:
        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,
                               offset=8)

    with gzip.open(images_path, 'rb') as imgpath:
        images = np.frombuffer(imgpath.read(), dtype=np.uint8,
                               offset=16).reshape(len(labels), 784)

    return images, labels",0.4415900111,
1103,load mnist,"import os
import struct
import numpy as np
 
def load_mnist(path, kind='train'):
    """"""Load MNIST data from `path`""""""
    labels_path = os.path.join(path, 
                               '%s-labels-idx1-ubyte' 
                                % kind)
    images_path = os.path.join(path, 
                               '%s-images-idx3-ubyte' 
                               % kind)
        
    with open(labels_path, 'rb') as lbpath:
        magic, n = struct.unpack('>II', 
                                 lbpath.read(8))
        labels = np.fromfile(lbpath, 
                             dtype=np.uint8)

    with open(images_path, 'rb') as imgpath:
        magic, num, rows, cols = struct.unpack("">IIII"", 
                                               imgpath.read(16))
        images = np.fromfile(imgpath, 
                             dtype=np.uint8).reshape(len(labels), 784)
 
    return images, labels",0.4413958788,
1103,load mnist,"import os
import struct
import numpy as np

def load_mnist(path, kind='train'):
    """"""Load MNIST data from `path`""""""
    labels_path = os.path.join(path, 
                               '%s-labels.idx1-ubyte' 
                                % kind)
    images_path = os.path.join(path, 
                               '%s-images.idx3-ubyte' 
                               % kind)
        
    with open(labels_path, 'rb') as lbpath:
        magic, n = struct.unpack('>II', 
                                 lbpath.read(8))
        labels = np.fromfile(lbpath, 
                             dtype=np.uint8)

    with open(images_path, 'rb') as imgpath:
        magic, num, rows, cols = struct.unpack("">IIII"", 
                                               imgpath.read(16))
        images = np.fromfile(imgpath, 
                    dtype=np.uint8).reshape(len(labels), 784)
 
    return images, labels",0.4357044697,
1103,load mnist,"def load_data():
    train_data = datasets.MNIST('../data', train=True, download=True, transform=transforms.ToTensor())
    train_loader_scatter_plot = torch.utils.data.DataLoader(train_data, batch_size=1, shuffle=False, **{})
    train_loader_tsne = torch.utils.data.DataLoader(train_data, batch_size=10000, shuffle=False, **{})  
    return train_loader_scatter_plot, train_loader_tsne

def find_latent_coordinates(train_loader_scatter_plot, model):
   
    x_coordinates = defaultdict(lambda: [])
    y_coordinates = defaultdict(lambda: [])
   
    for batch_idx, (data, label) in enumerate(train_loader_scatter_plot):
        model = model.cpu()
        _, z, _ = model(data)
        index = label.data.cpu().numpy()[0]
       
        x_coordinates[index].append(z.data.cpu().numpy()[0][0])
        y_coordinates[index].append(z.data.cpu().numpy()[0][1])
       
        if batch_idx == 10000:
            break
           
    return x_coordinates, y_coordinates

def find_TSNE_coordinates(train_loader_scatter_plot, model):
   
    x_coordinates = defaultdict(lambda: [])
    y_coordinates = defaultdict(lambda: [])
   
    for batch_idx, (data, label) in enumerate(train_loader_scatter_plot):
        model = model.cpu()
        _, z, _ = model(data)
            
        labels = label.data.numpy()
        embedded = TSNE(n_components=2).fit_transform(z.data.cpu().numpy())
        if batch_idx == 0:
            break
           
    assert embedded.shape[0] == labels.shape[0]   
    for i in range(embedded.shape[0]):
        x_coordinates[labels[i]].append(embedded[i][0])
        y_coordinates[labels[i]].append(embedded[i][1])
       
    return x_coordinates, y_coordinates

def plot_loss(losses, labels):
    epoch_space = np.linspace(1,200,200)
    for i in range(len(losses)):
        plt.plot(epoch_space, losses[i], label=labels[i])
    plt.legend()
    plt.show()
   
def plot_grid(VAE):
    N = 15
    if VAE.method == 'Gaussian':
        M1, M2 = -10, 10
    elif VAE.method == 'Gumbel':
        M1, M2 = -100, -0.01
    else:
        M1, M2 = 0.001, 0.999
    rows = np.linspace(M1, M2, N)
    cols = np.linspace(M1, M2, N)
    for i in range (N):
        for j in range(N):
            z = torch.FloatTensor([[rows[i],cols[j]]])
            try:
                x = VAE.decode(z)
            except RuntimeError:
                x = VAE.decode(z.cuda()).cpu()
            plt.subplot(N,N,i*N+j+1)
            plt.axis('off')
            plt.imshow(x.data.numpy().reshape(28,28))
    plt.show()

def visualize_models(paths):
    train_loader_scatter_plot, train_loader_tsne = load_data()
    losses = []
    labels = []
    for path in paths:
        outputs = pickle.load( open( path, ""rb"" ) )
        print(""Model type: {}, latent dimension: {} "".format(outputs[0].method, outputs[0].latent_dim))
        if outputs[0].method == 'logit':
            print(""Variance = {}"".format(path.split(""_"")[1][:-2]))
        if outputs[0].latent_dim == 2:
            plot_grid(outputs[0])
            x, y = find_latent_coordinates(train_loader_scatter_plot, outputs[0])
        else:
            x, y = find_TSNE_coordinates(train_loader_tsne, outputs[0])
        for label in x:
            plt.scatter(x[label], y[label], marker='.')
        plt.axis('off')
        plt.show()
        losses.append(outputs[4])
        label = outputs[0].method + ' ' + str(outputs[0].latent_dim)
        labels.append(label)
    plot_loss(losses, labels)",0.4286135137,
2452,use list comprehensions to find long words,"stuff = [' '.join(w for w in comment.split() if w.lower() not in stopwords)
       for comment in comments[0:400000]]",0.4345334172,
2452,use list comprehensions to find long words,"Counter([word.string.strip().lower() 
         for word in grail 
         if word.prob < -19.5]).most_common(20)",0.4307199717,
2452,use list comprehensions to find long words,"Counter([word.string.strip().lower() 
         for word in war_peace 
         if word.prob < -19.5]).most_common(20)",0.4307199717,
2452,use list comprehensions to find long words,"junior_words = [w for w in (nltk.word_tokenize(junior_clean)) if w.lower() not in stop]
mid_low_words = [w for w in (nltk.word_tokenize(mid_low_clean)) if w.lower() not in stop]
mid_high_words = [w for w in (nltk.word_tokenize(mid_high_clean)) if w.lower() not in stop]
senior_words = [w for w in (nltk.word_tokenize(senior_clean)) if w.lower() not in stop]

print len(nltk.word_tokenize(junior)) #original length of list of words for 'junior' category
print len(junior_words) # so we can see we ended up with relatively short list of meaningful words for the 'junior' bin (in comparison its original length )",0.4282650948,
2452,use list comprehensions to find long words,"list_char = list(set('cogmlcomlalmanachlomhcomlllamanachcocoml'))
dict_char = ['<epsilon>'] + list_char

dict_words = ['coml','almanach']


text_words = 'cogml coml almanach lomh comll lamanach cocoml'.split()

dict_table = fst.SymbolTable(name='words')

for idx,element in enumerate(dict_char):
    dict_table.add_symbol(symbol=element,key=idx)

arrive = 1 
fr_model_tranducer_compiler_1 = fst.Compiler(acceptor=True)

for l in dict_words:
    depart = 0
    for ch in l:
        print >> fr_model_tranducer_compiler_1, ""%d %d %d"" % (depart,arrive,dict_char.index(ch))
        depart = arrive
        arrive = arrive+1
        
    print >> fr_model_tranducer_compiler_1, ""%d"" %(depart)

fst_dict = fr_model_tranducer_compiler_1.compile()
fst_dict.set_input_symbols(dict_table)
fst_dict.rmepsilon()",0.4282577634,
2452,use list comprehensions to find long words,list(set([word.string.strip().lower() for word in grail if word.prob < -19]))[:20],0.4265548587,
2452,use list comprehensions to find long words,"#the only thing is that the prob of each word is from the dictionary itself so it doesn't represent the probability 
#of the word in relation to the text itself. not really useful imo


list(set([word.string.strip().lower() for word in war_peace if word.prob < -19]))[:20]",0.4265548587,
2452,use list comprehensions to find long words,"import collections
all_words = collections.Counter(w.lower() \
        for w in reuters.words(fileids=training_fileids))
word_features = [w for (w, c) in all_words.most_common(500)]
word_features[:10]",0.4260582924,
2452,use list comprehensions to find long words,">>> max([w.lower() for w in word_tokenize(text)]) #1

>>> max(w.lower() for w in word_tokenize(text)) #2 more efficient aka generator expression",0.4259172678,
2452,use list comprehensions to find long words,"%%time

nouns = [word.lower() for word, pos in tagged_all_text if word not in stopwords.words('english') and len(word) > MIN_LENGTH and pos.startswith('N')]",0.4249121547,
372,create training and test sets,"# Loading the data

def load_data():
    from sklearn.datasets import load_boston
    from sklearn.model_selection import train_test_split
    
    boston = load_boston()
    
    train_set_x, test_set_x, train_set_y, test_set_y = train_test_split(boston.data, boston.target, test_size=0.33, random_state=42)

    train_set_y = train_set_y.reshape((1, train_set_y.shape[0]))
    test_set_y = test_set_y.reshape((1, test_set_y.shape[0]))
    
    return train_set_x.T, train_set_y, test_set_x.T, test_set_y, boston

train_set_x, train_set_y, test_set_x, test_set_y, visualization_set = load_data()",0.548620522,
372,create training and test sets,"def split_train_test():
    # TODO: complete this function
        
    X_train = data[:,:] # currently, this selects all rows and all columns
    X_test  =
    
    y_train = target[:] # currently, this selects all rows
    y_test  = 
    
    return X_train, y_train, X_test, y_test",0.5444247127,
372,create training and test sets,"def load_data():
    from sklearn.model_selection import train_test_split
    
    X = np.genfromtxt('mush_features.csv')
    Y = np.genfromtxt('mush_labels.csv')
    
    train_set_x, test_set_x, train_set_y, test_set_y = train_test_split(X, Y, test_size=0.33, random_state=42)
    
    train_set_x = train_set_x[:300].astype(float)
    train_set_y = train_set_y[:300].astype(float)
    
    test_set_x = test_set_x[:100].astype(float)
    test_set_y = test_set_y[:100].astype(float)
    
    x_test = train_set_x[:5]
    y_test = train_set_y[:5]   
    
    train_set_x = train_set_x.reshape(train_set_x.shape[0], -1).T
    test_set_x = test_set_x.reshape(test_set_x.shape[0], -1).T
    
    train_set_y = train_set_y.reshape((1, train_set_y.shape[0]))
    test_set_y = test_set_y.reshape((1, test_set_y.shape[0]))
    
    x_test = x_test.reshape(x_test.shape[0], -1).T
    y_test = y_test.reshape((1, y_test.shape[0]))
    
    return train_set_x, test_set_x, train_set_y, test_set_y, x_test, y_test",0.5393919349,
372,create training and test sets,"def get_digits_binary_data(mc=False):
    dataset = sklearn.datasets.load_digits()
    X, y = dataset.data, dataset.target
    if mc:
        return sklearn.model_selection.train_test_split(X, y, random_state=0)
    
    # create digits with 2 classes: negative class 0: not 1,  positive class 1: 1
    y_binary_imbalanced = y.copy()
    y_binary_imbalanced[y_binary_imbalanced != 1] = 0
    return sklearn.model_selection.train_test_split(X, y_binary_imbalanced, random_state=0)

def model_report(title, clf, *args):
    '''
    Display report of a classification model for clf
    if len(args) == 2: X_test, y_test
    if len(args) == 4: X_train, X_test, y_train, y_test
    '''
    assert isinstance(clf, sklearn.base.BaseEstimator), ""2nd arg must be classifier""
    
    X_train, X_test, y_train, y_test = [None] * 4
    if len(args) == 2:
        X_test, y_test = args
    elif len(args) == 4:
        X_train, X_test, y_train, y_test = args

    title = ""{} - {}"".format(type(clf).__name__, title)
    params = ['kernel', 'gamma', 'C', 'strategy']
    d = clf.get_params()
    print(""{}: {}"".format(title, "","".join(""{}={}"".format(p, d[p]) for p in params if p in d)))
    if X_train is not None:
        print('Train Accuracy: {:.2f}'.format(clf.score(X_train, y_train)))
        
    if X_test is not None:

        y_predict = clf.predict(X_test)
        cf = sklearn.metrics.confusion_matrix(y_test, y_predict)
        cols = ['Pred:Neg', 'Pred:Pos']
        labels = ['Class:Neg', 'Class:Pos']
        str_cf = [[f""{cf[0][0]}(TN)"", f""{cf[0][1]}(FP)""], [f""{cf[1][0]}(FN)"", f""{cf[1][1]}(TP)""]]
        df_confusion = pd.DataFrame(str_cf, columns=cols, index=labels)
        print('Confusion martix:\n{}'.format(df_confusion))
        print()
        metrics = (
            ('Test Accuracy (TP+TN)/(ALL)', sklearn.metrics.accuracy_score(y_test, y_predict)),
            ('Precision TP/(TP+FP)', sklearn.metrics.precision_score(y_test, y_predict)),
            ('Recall TP/(TP+FN)', sklearn.metrics.recall_score(y_test, y_predict)),
            ('F1 (2*Prec*Recall)/(Prec+Recall)', sklearn.metrics.f1_score(y_test, y_predict))
            )
        w = max(len(t[0]) for t in metrics) + 1
        for n, v in metrics:
            print('{:{w}}: {:.2f}'.format(n, v, w=w))
        print()
        print(sklearn.metrics.classification_report(y_test, y_predict, target_names=['not 1', '1']))",0.53677845,
372,create training and test sets,"def train_input_fn():
    ds_tr = dataset.training_dataset(hparams.data_dir, DATA_SET)
    ds_tr_tr, _ = split_datasource(ds_tr, 60000, 0.95)
    ds1 = ds_tr_tr.cache().shuffle(buffer_size=57000).\
        repeat(hparams.train_epochs).\
        batch(hparams.batch_size)
    return ds1

def eval_input_fn():
    ds_tr = dataset.training_dataset(hparams.data_dir, DATA_SET)
    _, ds_tr_ev = split_datasource(ds_tr, 60000, 0.95)
    ds2 = ds_tr_ev.batch(hparams.batch_size)
    return ds2",0.5364596844,
372,create training and test sets,"def load_data():
    devkit_path = os.path.join(cfg.DATA_DIR, 'VOCdevkit/')
    data_set = PascalRoiDB(devkit_path, data_set_name, year, 
                        shuffle_samples=False, mode='test')

    dataloader = torch.utils.data.DataLoader(
        data_set, batch_size=1, shuffle=False, num_workers=3)

    data_iter = dataloader.__iter__()
    return data_set, dataloader, data_iter


try:
    del data_set
    del dataloader
    del data_iter
except:
    pass

data_set, dataloader, data_iter = load_data()
model = None",0.5334396362,
372,create training and test sets,"def plot_lat_lon():
    global training_data
    data = training_data[(training_data.longitude < 0) & (training_data.latitude > 0)]
    plt.figure(figsize=(7,6))
    for klass in ['low', 'medium', 'high']:
        subdata = data[data.interest_level == klass]
        plt.scatter(subdata['longitude'], subdata['latitude'], alpha=0.4)
    plt.legend(['low', 'medium', 'high'])
    plt.show()
plot_lat_lon()",0.5275797844,
372,create training and test sets,"def generate_embeddings():
    # Import data
    mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True, fake_data=FLAGS.fake_data)
    sess = tf.InteractiveSession()
    # Input set for Embedded TensorBoard visualization
    # Performed with cpu to conserve memory and processing power
    with tf.device(""/cpu:0""):
        embedding = tf.Variable(tf.stack(mnist.test.images[:FLAGS.max_steps], axis=0), 
                                trainable=False, name='embedding')
    tf.global_variables_initializer().run()
    saver = tf.train.Saver()
    writer = tf.summary.FileWriter(FLAGS.log_dir + '/projector', sess.graph)
    # Add embedding tensorboard visualization. 
    config = projector.ProjectorConfig()
    embed= config.embeddings.add()
    embed.tensor_name = 'embedding:0'
    embed.metadata_path = os.path.join(FLAGS.log_dir + '/projector/metadata.tsv')
    embed.sprite.image_path = os.path.join(FLAGS.data_dir + '/mnist_10k_sprite.png')
    # Specify the width and height of a single thumbnail.
    embed.sprite.single_image_dim.extend([28, 28])
    projector.visualize_embeddings(writer, config)
    saver.save(sess, os.path.join(
        FLAGS.log_dir, 'projector/a_model.ckpt'), global_step=FLAGS.max_steps)

def generate_metadata_file():
    # Import data
    mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True, fake_data=FLAGS.fake_data)
    def save_metadata(file):
        with open(file, 'w') as f:
            for i in range(FLAGS.max_steps):
                c = np.nonzero(mnist.test.labels[::1])[1:][0][i]
                f.write('{}\n'.format(c))
    save_metadata(FLAGS.log_dir + '/projector/metadata.tsv')",0.5253742337,
372,create training and test sets,"def testOnce(data):
    # split the data into training and testing sets
    (trainingData, testData) = data.randomSplit([1-test_size, test_size])
     # train the random forest
    model = RandomForest.trainClassifier(trainingData, numClasses=3, categoricalFeaturesInfo={},
                                         numTrees=num_trees, featureSubsetStrategy=""auto"",
                                         impurity='gini', maxDepth = max_depth, maxBins=32)
    # test the random forest
    predictions = model.predict(testData.map(lambda x: x.features))
    labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)
    testErr = labelsAndPredictions.filter(lambda (v, p): v != p).count() / float(testData.count())
    Mg = float(labelsAndPredictions.filter(lambda (v, p): v == 0 and p == 1).count())
    Ng = float(labelsAndPredictions.filter(lambda (v, p): v == 0 and p == 0).count())
    Ms = float(labelsAndPredictions.filter(lambda (v, p): v == 1 and p == 0).count())
    Ns = float(labelsAndPredictions.filter(lambda (v, p): v == 1 and p == 1).count())
    probsAndScores = probTest(testData, model)
    threshold_accuracy = probsAndScores[0]
    probs = probsAndScores[1].map(lambda x: x/num_trees)
    labelsAndPredictions = labelsAndPredictions.zip(probs)
    #save(labelsAndPredictions, 'answers')
    print ('Galaxy Purity = ' + str(Ng / (Ng+Ms)))
    print ('Galaxy Completeness = ' + str(Ng / (Ng+Mg)))
    print ('Star Purity = ' + str(Ns / (Ns+Mg)))
    print ('Star Completeness = ' + str(Ns/(Ns+Ms)))
    print ('Accuracy = ' + str(1 - testErr))
    print ('Threshold method accuracy = ' + str(threshold_accuracy))",0.524576962,
372,create training and test sets,"def ensemble_predictions():
    # Empty list of predicted labels for each of the neural networks.
    pred_labels = []

    # Classification accuracy on the test-set for each network.
    test_accuracies = []

    # Classification accuracy on the validation-set for each network.
    val_accuracies = []

    # For each neural network in the ensemble.
    for i in range(num_networks):
        # Reload the variables into the TensorFlow graph.
        saver.restore(sess=session, save_path=get_save_path(i))

        # Calculate the classification accuracy on the test-set.
        test_acc = test_accuracy()

        # Append the classification accuracy to the list.
        test_accuracies.append(test_acc)

        # Calculate the classification accuracy on the validation-set.
        val_acc = validation_accuracy()

        # Append the classification accuracy to the list.
        val_accuracies.append(val_acc)

        # Print status message.
        msg = ""Network: {0}, Accuracy on Validation-Set: {1:.4f}, Test-Set: {2:.4f}""
        print(msg.format(i, val_acc, test_acc))

        # Calculate the predicted labels for the images in the test-set.
        # This is already calculated in test_accuracy() above but
        # it is re-calculated here to keep the code a bit simpler.
        pred = predict_labels(images=data.test.images)

        # Append the predicted labels to the list.
        pred_labels.append(pred)
    
    return np.array(pred_labels), \
           np.array(test_accuracies), \
           np.array(val_accuracies)",0.5241936445,
470,dice simulaiton,"def evolve(population, CXPB, MUTPB, MUTFQ, NGEN, GOAL):
    
    # CXPB = 0.5    # Crossover probability.
    # MUTPB = 0.75  # Chance for individual to mutate.
    # MUTFQ = 0.20  # Frequency of mutate() mutation events.
    # NGEN = 75     # Number of generations.
    # GOAL = 0.95   # Goal AUC.

    POP = len(population)
    
    # Data to use for plots.
    plots = defaultdict(list)

    # Evaluation model.
    linreg = LinearRegression(normalize=True)

    # Evaluate fitnesses of starting population.
    fitness_list = map(lambda x: evaluate(x), population)

    # Assign fitness values.
    for ind, fitness in zip(population, fitness_list):
        ind.fitness.values = fitness

    # EVOLUTION.
    for gen in xrange(NGEN):

        print ""\nGeneration:"", str(gen + 1)
        print ""---------------""
        print ""Mean AUC:"", str(population_fitness(population))
        print ""Mean RMSD:"", str(population_rmsd(population))
        print ""Mean Count:"", str(population_count(population))

        plots[""PopRMSD""].append(population_rmsd(population))
        plots[""PopFitness""].append(population_fitness(population))
        plots[""PopCount""].append(population_count(population))
        plots[""PopFreq""].append(population_lig_freq(population))

        # Goal achieved!
        if population_fitness(population) > GOAL:
            break

        # Select the next generation of individuals.
        offspring = []
        offspring.append(max(population, key=lambda x: x.fitness.values[0]))
        offspring += tools.selTournament(population, POP-1, 10)  # Select the rest with tournament.
        offspring = map(toolbox.clone, offspring)

        # Apply crossovers.
        for child_a, child_b in zip(offspring[::2], offspring[1::2]):  # Staggered.
            if random.random() < CXPB:
                # crossover(child_a, child_b, CXPB)
                sp_crossover(child_a, child_b, CXPB)
                del child_a.fitness.values
                del child_b.fitness.values

        # Apply mutations.
        for child in offspring:
            if random.random() < MUTPB:
                # mutate(child, MUTFQ)
                sp_mutate(child, MUTFQ)
                # child = single_ppl_ind(train_actives)  # Mutation event is just creating a new individual.
                del child.fitness.values

        # Reevaluate fitness of changed individuals.
        new_children = [e for e in offspring if not e.fitness.valid]
        fitness_list = map(lambda x: evaluate(x), new_children)
        for individual, fitness in zip(new_children, fitness_list):
            individual.fitness.values = fitness

        # Replace population with the new generation.
        population[:] = offspring

        best = max(population, key=lambda x: x.fitness.values[0])

        print ""Best Indv:         \
               \n  {count} Poses  \
               \n  {auc} AUC      \
               \n  {rmsd} AvgRMSD"".format(count = individual_size(best),
                                          auc = best.fitness.values[0],
                                          rmsd = individual_rmsd(best)
                                         )

        plots[""BestCount""].append(individual_size(best))
        plots[""BestFitness""].append(best.fitness.values[0])
        plots[""BestRMSD""].append(individual_rmsd(best))
        plots[""BestFrequencies""].append(ligand_freq(best))

        ## Plot Frequencies
        # if gen % 5 == 0:
        #     individual_rmsd_frequencies(best)
        
    plots[""examples""] = best
    return plots",0.4411082268,
470,dice simulaiton,"def slice_sampling(p, init, iters, omega):
    """""" slice_sampling is implemented based on
        http://www.cs.cmu.edu/~epxing/Class/10708-16/slide/lecture16-MCMC.pdf
        
        p (my_dist)
        init (numpy array): intial position
        iters (int): number of iterations to run
        omega (numpy array): array of window size for each dimension
    """"""
    # check if dist and init matches
    D = p.N + 1
    if D != len(init):
        sys.exit(""Distribution and initial position dimension mismatch"")

    # construct holder for samples
    samples = np.zeros((D, iters))

    # initialize position and log-likelihood
    xx = init.copy()
    xx_llh = p.loglikelihood(xx)

    # for each of required number of iterations
    for i in range(iters):

        # for each dimension
        for d in range(D):
            
            # sample u
            u = xx_llh + np.log(np.random.rand())
            
            # Part 1: Stepping Out
            # sample interval (x_r[d], x_l[d] enclosing xx[d])
            r = np.random.rand()
            x_l = xx.copy()
            x_l[d] = x_l[d] - r * omega[d]
            x_r = xx.copy()
            x_r[d] = x_r[d] + (1 - r) * omega[d]

            # expand until endpoints are ""outside"" region under curve
            x_l_llh = p.loglikelihood(x_l)
            while x_l_llh > u:
                x_l[d] = x_l[d] - omega[d]
                x_l_llh = p.loglikelihood(x_l)
            
            x_r_llh = p.loglikelihood(x_r)
            while x_r_llh > u:
                x_r[d] = x_r[d] + omega[d]
                x_r_llh = p.loglikelihood(x_r)

            # Part 2: Sample x (Shrinking)
            x = xx.copy()
            while True:
                # Draw x from within (x_l[d], x_r[d]), then accept or shrink
                xd = x_l[d] + np.random.rand() * (x_r[d] - x_l[d])
                x[d] = xd
                xx_llh = p.loglikelihood(x)
                if xx_llh > u:
                    xx = x.copy()
                    break
                elif xd > xx[d]:
                    x_r[d] = xd
                else:
                    x_l[d] = xd

        if i % 1000 == 0: print i * 1.0 / iters, 'completed'

        samples[:, i] = xx.copy()

    return samples",0.4407752752,
470,dice simulaiton,"def epsilonGreedy(epsilon, Q, board):
    moves = validMoves(board)
    index = np.random.randint(len(moves))
    if np.random.uniform() < epsilon:
        # Random Move
        return moves[index]
    else:
        # Greedy Move
        Qs = np.array([Q.get((stateMoveTuple(board,m)), 0) for m in moves]) 
        return moves[ np.argmax(Qs) ]",0.4277927279,
470,dice simulaiton,"def next_batch(input_sequence, batch_size, n_steps):
    i_first = 0
    i_last = len(input_sequence)
    i_starts = np.random.randint(i_first, high=i_last-n_steps, size=(batch_size, 1))
    i_sequences = i_starts + np.arange(0, n_steps + 1)
    flat_i_sequences = np.ravel(i_sequences[:,:])
    flat_sequences = input_sequence[flat_i_sequences]
    sequences = flat_sequences.reshape(batch_size,-1)
    return sequences[:, :-1].reshape(-1, n_steps, 1), sequences[:, 1:].reshape(-1, n_steps, 1)",0.4234351516,
470,dice simulaiton,"def crossover(p, mother, father):
    split = np.random.randint(p.n)
    child = np.append(mother[:split], father[split:])
    return make_chromosome_great_again(p, child)

def breed(p, population):
    mother = roulette_choice(p, population)
    father = roulette_choice(p, population)
    return crossover(p, mother, father)",0.4231671989,
470,dice simulaiton,"def load_random_data(N, x_dim, nclasses, seed):
    np.random.seed(seed)
    Y = np.random.randint(size=N, low=0, high=nclasses)
    X = np.random.randn(N, x_dim)
    D = (X, Y.astype(int))
    return D

X,Y = load_random_data(5, 5, 4, 256)
Y",0.4231435657,
470,dice simulaiton,"#the shuffle and fit function
@require (get_metric)
def shuffle_and_fit(model, target, predictors, metricType):
    import random
  
    shuffledTarget = list(target)
    random.shuffle(shuffledTarget)
    if sum(target) != sum(shuffledTarget):
        print 'uh-o something went wrong the numbers pr proportions are not the same in the shuffled list'
    
    model.fit(predictors, shuffledTarget)
    Metric = get_metric(model, shuffledTarget, predictors, metricType)
         
    return Metric",0.4212554395,
470,dice simulaiton,"def select_batch(X,y,batchsize):
    random_indices=np.random.randint(X.shape[0],size=batchsize)
    return X[random_indices], y[random_indices]",0.4202544689,
470,dice simulaiton,"def drift_pop(num_msats, pop_size):
    m = SinglePop(gens=100)
    m.num_msats = num_msats
    m.pop_size = pop_size
    BasicView(m, [ExpHe()], ['mean'], min_y=[0], max_y=[1])
    m.run()
    
interact(drift_pop, num_msats=(2, 20, 1), pop_size=(30, 530, 50))",0.4197531343,
470,dice simulaiton,"def getBuy_Dates(history):
    buy_dates = history.index[history.Crossover == 1]
    return buy_dates",0.4188717604,
313,convert to arrays,"def resize(image):
    new_patient = []
    for each_slice in patient:
        
        new_img = cv2.resize(np.array(each_slice),(IMG_SIZE, IMG_SIZE))
        new_patient.append(new_img)
    return np.array(new_patient)",0.467520833,
313,convert to arrays,"%%add_to Network
def initialize_weights(self):
    # YOUR CODE HERE
    self.weights = np.random.randn(self.sizes[2], self.sizes[1])",0.451121062,
313,convert to arrays,"def resize(image):
    new_patient = []
    for each_slice in image:
        new_patient.append(cv2.resize(np.array(each_slice),(IMG_SIZE, IMG_SIZE)))
    return np.array(new_patient)",0.4503857493,
313,convert to arrays,"# Cache_lane initialization by pre-populating the lane end-points in cache_line

def cache_lane_initialization(clip):
    """"""
    `clip` is an handle to video clip 
    """""" 
    global cache_lane
    
    cache_lane = [] 
    
    # Grab first frame and obtain the end points of detected lanes
    frame_t0 = clip.get_frame(t=0)
    img = (np.copy(frame_t0)*255).astype('uint8')
    _, cache_lane = lanedetection_pipeline(img)
    
    return cache_lane",0.4492716789,
313,convert to arrays,"def read_counts():
    adata_counts = sc.read('./data/CountsNorm.csv', cache=True).T
    # this is not yet in sparse format, as the data was in a dense csv file
    from scipy.sparse import csr_matrix
    adata_counts.X = csr_matrix(adata_counts.X)
    adata_counts.write('./write/zebrafish_sparse_counts.h5ad')
# read_counts()",0.4471206367,
313,convert to arrays,"#Function to compute persistence on cubical complexes
def persist_img(img):
    buf = img.get_whole_array()
    lval = []
    for k in range(DEFAULT_SIZE[2]):
        for j in range(DEFAULT_SIZE[1]):
            for i in range(DEFAULT_SIZE[0]):
                lval.append(buf[i,j,k]) 
    cub_cpx = gd.CubicalComplex(DEFAULT_SIZE,lval)
    return cub_cpx, cub_cpx.persistence(homology_coeff_field=2, min_persistence=0)",0.4469916523,
313,convert to arrays,"#Going with option 1
def Add_Project(df):
    project = np.zeros(len(df))
    df['Project'] = project
    return df",0.4428577423,
313,convert to arrays,"def to_img(img_list):
    # Note: only works on square images
    output = []
    for i in range(0,img_list.shape[0]):
        output.append(np.reshape(img_list[i],(28,28)))
    return output",0.4375916719,
313,convert to arrays,"def extract(politician_name):
    tweet_texts = []
    for i in range(0, n_ids):
        author = df.handle[i]
        tweet = df.text[i]
        if author==politician_name:
            tweet_texts.append(tweet.strip())
    return tweet_texts",0.4373697639,
313,convert to arrays,"def build_discriminator(self):

        img_shape = (self.img_rows, self.img_cols, self.channels)
        
        model = Sequential()

        model.add(Flatten(input_shape=img_shape))        #Flattens the image in linear array
        model.add(Dense(512))                            #Fully connected layer (512 Neurons) taking generated image as input in linear format
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dense(256))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dense(1, activation='sigmoid'))        #Returns the output between 0 to 1
        model.summary()

        img = Input(shape=img_shape)
        validity = model(img)                            #Returns the validity of the image passed to the generator

        return Model(img, validity)",0.4367550611,
2008,step assign it to a variable called data and replace the first columns by a proper datetime index,"tl = esdl.get_cleaned_timeline(test_user_id, ct_df.iloc[0].start_ts, ct_df.iloc[-1].end_ts)",0.5355903506,
2008,step assign it to a variable called data and replace the first columns by a proper datetime index,"def cleanup_country(data, region):
    if 'Country' in data.columns.values:
        data.loc[pd.isnull(data['Country']), 'Country'] = 'United Arab Emirates' # replace blank countries with UAE
        data.loc[data['Country']=='', 'Country'] = 'United Arab Emirates' # replace blank countries with UAE
        data.loc[data['Country']==' ', 'Country'] = 'United Arab Emirates' # replace blank countries with UAE
        
        data = data.merge(region, left_on = 'Country', right_on = 'country', how = 'left')
        no_region = data.loc[pd.isnull(data['region_2'])]
        print('\n Region Summary\n')
        print(data['region_2'].value_counts())
        print('\nNumber of No Region -- '+ str(len(no_region)))
        return data, no_region
    else:
        print('There is no Country column')",0.5337064266,
2008,step assign it to a variable called data and replace the first columns by a proper datetime index,"""""""""this function takes the row 'created_at', copies it to the front as the pandas index and converts
it to the local time of the game
parameters:
df= raw dataframe
output= dataframe that is now indexed by local time of the tweets
""""""
def get_indexed_by_time(df):
    df.set_index('created_at', drop=False, inplace=True)
    df.index.names = [None]
    df.index = df.index.tz_localize('GMT').tz_convert('EST')
    df.index = df.index - DateOffset(hours = 0)
    return df",0.5287217498,
2008,step assign it to a variable called data and replace the first columns by a proper datetime index,"def most_popular_route_by_hour(df, title_label):
    df['Count']= 1
    df.index.names = ['Date']
    df1 = df.groupby(['Route']).resample('1H', how='count')
    df1 = df1.reset_index()
    
    df1_pivoted = df1.pivot_table('Count', ['Route'], 'Date')
    df1_pivoted.columns = np.arange(24)
        
    df2 = df1_pivoted.max(axis=0, skipna=True)
    df2 = pd.concat([df1_pivoted.idxmax(axis=0, skipna=True), df1_pivoted.max(axis=0, skipna=True)], axis =1)
    df2=df2.reset_index()
    df2.columns = ['Time','Route','Count']
    df2=df2.drop(df2[df2.Count < 10].index)
    
    ##Plotting##
    c=np.random.rand(3,1)

    marker = itertools.cycle(('s', 'd', '<', 'o', '*','p','^','h')) 
    plt.figure(figsize = (15,10))
    for i, group in df2.groupby('Route'):
        x = group.Time
        y= group.Count
        plt.plot(group.Time, group.Count, marker = marker.next(),markersize= 20, linestyle = '', 
                 color  = np.random.rand(3,1), label = str(i),clip_on=False)
    plt.legend(loc='left', bbox_to_anchor=(1.05, 1.01),
              fancybox=True, shadow=True, ncol=1, fontsize = 20)

    plt.xlabel('Hour', fontsize = 30, fontweight = 'bold')
    plt.ylabel('# of Oyster trips', fontsize = 30, fontweight = 'bold')
    plt.tick_params(labelsize = 20)
    plt.xlim([0,24.5])
    plt.grid(True)
    plt.title(('Most popular routes ' + '('+title_label+')'), fontsize = 30, fontweight = 'bold',y=1.04)
    
    return df2",0.5261545181,
2008,step assign it to a variable called data and replace the first columns by a proper datetime index,"def index_by_datetime_and_station(df):
    return df.set_index([pd.to_datetime(df[['YEAR', 'MONTH', 'DAY']]),
                        df['STATION_ID']])",0.5247844458,
2008,step assign it to a variable called data and replace the first columns by a proper datetime index,"#Convert to timestamp
cancels.iloc[:,2] = pd.to_datetime(cancels.iloc[:,2], format='%Y-%m-%d')

#Create dummy column for CATEGORICAL variables 'event_type'.
cancels_event_type = pd.get_dummies(cancels.iloc[:,1])

# Concatenate dummy variables of event_type
cancels = pd.concat([cancels, cancels_event_type], axis=1)
    
#Group by over 'subscription_id'.
cancels_groupby = cancels.groupby('subscription_id').agg({'event_date':['min', 'max'], 
                                                          'cancellation':['sum'], 'reactivation':['sum']}).reset_index()

#Rename columns
cancels_groupby.columns = ['subscription_id', 'First_event_date', 'Last_event_date' ,'Total_cancellation','Total_reactivation']",0.5225929022,
2008,step assign it to a variable called data and replace the first columns by a proper datetime index,"daymap = {0:'Monday', 1:'Tuesday', 2:'Wednesday', 3:'Thursday', 4:'Friday', 5:'Saturday', 6:'Sunday'}

def add_date_time_weekday( df ):
    df.loc[:,'Timestamp'] = pd.to_datetime(df['Timestamp'])
    df.loc[:,'Time'] = df['Timestamp'].apply(lambda x:x.time())
    df.loc[:,'Date'] = df['Timestamp'].apply(lambda x:x.date())
    df.loc[:,'Weekday'] = df['Date'].apply( lambda x: daymap[x.weekday()] )",0.5218257308,
2008,step assign it to a variable called data and replace the first columns by a proper datetime index,"def convert_to_NYC(df):
    df.timestamp = pd.to_datetime(df.timestamp, utc=True)
    df.set_index('timestamp', drop=True, inplace=True)
    df.index = df.index.tz_convert('America/New_York')
    return df",0.5214235783,
2008,step assign it to a variable called data and replace the first columns by a proper datetime index,"from functools import partial

def recode_and_add_end_labels(df, win):
    """"""Recode and add end labels.""""""
    df.rename(columns={'R^2': 'R2'}, inplace=True)
    df[""CHR1_END""] = df.POS1.apply(lambda i: ""left"" if i < win else ""right"")
    df[""CHR2_END""] = df.POS2.apply(lambda i: ""left"" if i < win else ""right"")

def do_per_chunk(chunk, win):
    """"""Run this code for each chunk.""""""
    recode_and_add_end_labels(df=chunk,win=win)
    return chunk.groupby([""CHR1"",""CHR1_END""])['POS1'].unique().apply(lambda x: len(x)).unstack()

def indep_snps_per_scaff_end(ld_path, chunksize=6000000, ld_window=100000, procs=None):
    """"""Return a per-end count of snps found to be independent.
    
    Runs on each chunk in parallel using `procs` processors for the pool.
    If `procs` is `None`, uses as many as it can find.
    """"""
    ld = pd.read_csv(ld_, sep='\t', chunksize=6000000)
    
    with Pool(procs) as p:
        results = p.map(partial(do_per_chunk, win=ld_window), ld)
    
    results = pd.concat(results).reset_index().fillna(0)
    
    return results.groupby(""CHR1"").sum()",0.5195023417,
2008,step assign it to a variable called data and replace the first columns by a proper datetime index,"test = test.loc[:,'vendor_id':'store_and_fwd_flag']

# Convert to datetime object
test['pickup_datetime'] = pd.to_datetime(test['pickup_datetime'], format='%Y-%m-%d %H:%M:%S') 

# get dummies
test = pd.get_dummies(test,drop_first=True)

# Split datetime objects to separte columns
test['pickup_month'] = test['pickup_datetime'].dt.month.astype(float)
test['pickup_weekday'] = test['pickup_datetime'].dt.weekday.astype(float)
test['pickup_hour'] = test['pickup_datetime'].dt.hour.astype(float)

# # Convert to Float datatype
int_feat = test.dtypes[(test.dtypes != 'object') & (test.dtypes != 'float64') & (test.dtypes != 'datetime64[ns]')].index
test[int_feat] = test[int_feat].astype(float)

test.drop(labels=['pickup_datetime'],axis=1,inplace=True)

test.info()",0.5180741549,
466,design decisions with python functions,"def fn(input):
    if <base case>:
        return ...
    else:
        return <combine input and fn(input_reduced)>",0.4275742173,
466,design decisions with python functions,"c_factorial_generator = lambda r: lambda n: IF(c_lt(n)(c2))(lambda: c1)(lambda: c_multiply(n)(r(c_predecessor(n))))

c_factorial = Y(c_factorial_generator)",0.4238860309,
466,design decisions with python functions,"c_lt = lambda c_num1: lambda c_num2: AND(NOT(c_eq(c_num1)(c_num2)))(c_lte(c_num1)(c_num2))

c_reject(c_lt(c0)(c0))
c_reject(c_lt(c1)(c1))
c_assert(c_lt(c0)(c1))
c_reject(c_lt(c1)(c0))",0.4234521985,
466,design decisions with python functions,"def entry(i, j):
    return 3 if i == 2 else 4",0.4196099639,
466,design decisions with python functions,"algorithms = {
    ""k=3"":lambda A, p, r: merge_sort_kway(A, p, r, 3),
    ""k=4"":lambda A, p, r: merge_sort_kway(A, p, r, 4),
    ""k=5"":lambda A, p, r: merge_sort_kway(A, p, r, 5),
    ""k=7"":lambda A, p, r: merge_sort_kway(A, p, r, 7),
    ""k=10"":lambda A, p, r: merge_sort_kway(A, p, r, 10),
    ""k=20"":lambda A, p, r: merge_sort_kway(A, p, r, 20),
    ""k=30"":lambda A, p, r: merge_sort_kway(A, p, r, 30)
}

t2, s2 = measure_performance(algorithms, trials=30, interval=25)",0.4195487499,
466,design decisions with python functions,"algorithms = {
    ""t=2"":lambda A, p, r: merge_sort_3way_aug(A, p, r, 2),
    ""t=3"":lambda A, p, r: merge_sort_3way_aug(A, p, r, 3),
    ""t=5"":lambda A, p, r: merge_sort_3way_aug(A, p, r, 5),
    ""t=10"":lambda A, p, r: merge_sort_3way_aug(A, p, r, 10),
    ""t=20"":lambda A, p, r: merge_sort_3way_aug(A, p, r, 20),
    ""t=30"":lambda A, p, r: merge_sort_3way_aug(A, p, r, 30)
}

t3, s3 = measure_performance(algorithms, trials=10, interval=10)",0.4183113873,
466,design decisions with python functions,"A = [1,2,3,'apple',lambda x:x**2]
A[4](5)",0.4172614217,
466,design decisions with python functions,"algorithms = {
    ""t=5"":lambda A, p, r: merge_sort_3way_aug(A, p, r, 5),
    ""t=10"":lambda A, p, r: merge_sort_3way_aug(A, p, r, 10),
    ""t=15"":lambda A, p, r: merge_sort_3way_aug(A, p, r, 15),
}

t3_2, s3_2 = measure_performance(algorithms, trials=2, max_size=5000, interval=100)",0.4169414639,
466,design decisions with python functions,"expected_loss_s1 = lambda estimate_s: res_score_lossG(estimate_s, given_score1)
expected_loss_s2 = lambda estimate_s: res_score_lossG(estimate_s, given_score2)
expected_loss_s3 = lambda estimate_s: res_score_lossG(estimate_s, given_score3)

figsize(15, 6)

loss = [expected_loss_s1(e) for e in estimates_s]
plt.plot(estimates_s, loss, label=""True score = %d"" % (given_score1))

loss = [expected_loss_s2(e) for e in estimates_s]
plt.plot(estimates_s, loss, label=""True score = %d"" % (given_score2))

loss = [expected_loss_s3(e) for e in estimates_s]
plt.plot(estimates_s, loss,label=""True score = %d"" % (given_score3))

plt.grid()
plt.vlines(given_score1, 0, 1.1 * np.max(loss), label=""True score = %d"" % (given_score1), linestyles=""-."")
plt.vlines(given_score2, 0, 1.1 * np.max(loss), label=""True score = %d"" % (given_score2), linestyles=""-"")
plt.vlines(given_score3, 0, 1.1 * np.max(loss), label=""True score = %d"" % (given_score3), linestyles="":"")
plt.xlabel(""Reservoir score estimate"")
plt.ylabel(""Expected loss"")
plt.title(""Using IId loss function for 3 given values"")
plt.xlim(estimates_s[0], estimates_s[-1])
plt.ylim(0,1.1 * np.max(loss));
plt.legend(loc=""upper left"", title=""Legend"");",0.4142084122,
466,design decisions with python functions,"expected_loss_sG1 = lambda estimate_s: res_score_lossG(estimate_s, given_score1)
expected_loss_sG2 = lambda estimate_s: res_score_lossG(estimate_s, given_score2)
expected_loss_sG3 = lambda estimate_s: res_score_lossG(estimate_s, given_score3)

figsize(15, 6)

loss = [expected_loss_sG1(e) for e in estimates_s]
plt.plot(estimates_s, loss, label=""True score = %d"" % (given_score1))

loss = [expected_loss_sG2(e) for e in estimates_s]
plt.plot(estimates_s, loss, label=""True score = %d"" % (given_score2))

loss = [expected_loss_sG3(e) for e in estimates_s]
plt.plot(estimates_s, loss,label=""True score = %d"" % (given_score3))

plt.grid()
plt.vlines(given_score1, 0, 1.1 * np.max(loss), label=""True score = %d"" % (given_score1), linestyles=""-."")
plt.vlines(given_score2, 0, 1.1 * np.max(loss), label=""True score = %d"" % (given_score2), linestyles=""-"")
plt.vlines(given_score3, 0, 1.1 * np.max(loss), label=""True score = %d"" % (given_score3), linestyles="":"")
plt.xlabel(""Reservoir score estimate"")
plt.ylabel(""Loss"")
plt.title(""Customized loss function (IV) applied on determined values"")
plt.xlim(estimates_s[0], estimates_s[-1])
plt.ylim(0,1.1 * np.max(loss));
plt.legend(loc=""upper left"", title=""Legend"");",0.4142084122,
2394,train the network,"# train
def train(epoch):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        if config_dict.use_cuda:
            data, target = data.cuda(), target.cuda()
        data, target = Variable(data), Variable(target)
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, target)
        # plot the training loss
        Viz.show_value(loss.item(), name='Training Loss')
        loss.backward()
        optimizer.step()
        if batch_idx % config_dict.log_interval == 0:
            print('Train Epoch: {} [{}/{} samples ({:.0f}%)]\t Batch Loss: {:.6f}'
                  .format(epoch, batch_idx * len(data), 
                          len(train_loader.dataset),
                          100. * batch_idx / len(train_loader), 
                          loss.item()))
            # save the current model weights
            Exp.save_model(model, name=""MNIST_ConvNet"", n_iter=batch_idx)",0.5976291299,
2394,train the network,"def train(epoch):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        if cuda:
            data, target = data.cuda(), target.cuda()
        data, target = Variable(data), Variable(target)
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()
        if batch_idx % log_interval == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.data[0]))",0.5943601131,
2394,train the network,"def train(epoch):
    network.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = Variable(data), Variable(target)
        optimizer.zero_grad()
        output = network(data)
        loss = F.cross_entropy(output, target)
        loss.backward()
        optimizer.step()
        if batch_idx % 100 == 0:
            print(('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.data[0])))

def test():
    network.eval() 
    test_loss = 0
    correct = 0
    for data, target in test_loader:
        data, target = Variable(data, volatile=True), Variable(target)
        output = network(data)
        test_loss += F.cross_entropy(output, target, size_average=False).data[0] # sum up batch loss
        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability
        correct += pred.eq(target.data.view_as(pred)).cpu().sum()

    test_loss /= len(test_loader.dataset)
    print(('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset))))",0.5914494991,
2394,train the network,"def train(epoch):
    network.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = Variable(data), Variable(target)
        optimizer.zero_grad()
        output = network(data)
        loss = F.cross_entropy(output, target)
        loss.backward()
        optimizer.step()
        if batch_idx % 100 == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100 * batch_idx / len(train_loader), loss.data[0]))

def test():
    network.eval()
    test_loss = 0
    correct = 0
    for data, target in test_loader:
        data, target = Variable(data, volatile=True), Variable(target)
        output = network(data)
        test_loss += F.cross_entropy(output, target, size_average=False).data[0] # sum up batch loss
        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability
        correct += pred.eq(target.data.view_as(pred)).cpu().sum()

    test_loss /= len(test_loader.dataset)
    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))",0.5914494991,
2394,train the network,"def train(epoch):
    model.train()
    for batch, (data, target) in enumerate(train_loader):
        data, target = data.cuda(), target.cuda()
        data, target = Variable(data), Variable(target)
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()
        if batch % LOG_INTERVAL == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'
                  .format(epoch, 
                          batch * len(data), 
                          len(train_loader.dataset), 
                          100. * batch / len(train_loader), 
                          loss.data.item()))

def test(epoch):
    model.eval()
    test_loss = 0
    correct = 0
    for data, target in test_loader:
        data, target = data.cuda(), target.cuda()
        with torch.no_grad():
            data, target = Variable(data), Variable(target)
        output = model(data)
        test_loss += F.nll_loss(output, target).data.item()
        pred = output.data.max(1)[1]
        correct += pred.eq(target.data).cpu().sum()
    test_loss /= len(test_loader)
    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'
          .format(test_loss, 
                  correct, 
                  len(test_loader.dataset), 
                  100. * correct / len(test_loader.dataset)))",0.5914494991,
2394,train the network,"def train(epoch):
    model.train()
    for batch_idx, (data,target) in enumerate(train_distributed_dataset):
            
        worker = data.location
        model.send(worker)

        optimizer.zero_grad()
        # update the model
        pred = model(data)
        loss = F.mse_loss(pred, target.float())
        loss.backward()
        model.get()
        optimizer.step()

        if batch_idx % args.log_interval == 0:
            loss.get()
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * args.batch_size, len(train_loader) * args.batch_size,
                100. * batch_idx / len(train_loader), loss.data[0]))",0.5886001587,
2394,train the network,"def train(epoch):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = Variable(data), Variable(target)
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()
        if batch_idx % 10 == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.item()))",0.5857282877,
2394,train the network,"def train(epoch):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()  # zero out the gradients (they accumulate)
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()
        if batch_idx % args.log_interval == 0:
            print('Epoch: {} {}/{} ({:.0f}%)\tLoss: {:.3f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.item()))",0.5857282877,
2394,train the network,"def train(epoch):
    model.train()
    #batch_idx numbers the batch
    for batch_idx, (data, target) in enumerate(train_loader):
        # Move tensors to the configured device
        data = data.reshape(-1,28*28)
        data = data.to(device)
        target = target.to(device)
        
        #############Forward pass###########
        
       
        #feedforward
      
        y = model(data) 
        #cross-entropy regression problems
        current_loss = F.cross_entropy(y, target)
        # Backward and optimize
        model.zero_grad()
        current_loss.backward()
        optimizer.step()
       

        ##########calculate the loss##########
        
        
        #######################################
        # print training loss every 100 batches
        if batch_idx % 100 == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100 * batch_idx / len(train_loader), current_loss.item()))",0.584559679,
2394,train the network,"def training_routine(args, path, train_datalist, val_datalist):
    
    #open files and load content
    
    # Create the network
    if args.load_disk_model==True:
        load_model = torch.load(args.load_checkpoint_filename, map_location=lambda storage, loc: storage)
        #load_model = torch.load('ChexNet_model2.pytorch')
        model = DenseNet121(args)
        new_state_dict = OrderedDict()
        for k, v in load_model.items():
            #print (k)
            name = k[7:] # remove `module.`
            new_state_dict[name] = v
        # load params
        model.load_state_dict(new_state_dict)
    elif args.load_dis_model == False:
        model = DenseNet121(args)  
    
    #Initialize weitgths
    #my_model.initialize_weigths()
    
    
    #Choose the loss function / optimizer
    loss = nn.BCELoss(size_average = True)
    
    #choose optimizer
    optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),
                             lr=args.learn_rate,
                             weight_decay=args.l2)
    scheduler = ReduceLROnPlateau(optim, factor = 0.1, patience = 10, mode = 'min')
                         
    print (""Created Neural Network arquitecture"")
    
    if torch.cuda.is_available():
        # Move the network and the optimizer to the GPU
        print (""Moving to GPU"")
        model = model.cuda()
        model = torch.nn.DataParallel(model).cuda()
        loss = loss.cuda()
    
    dataset = ChestXray(args, train_datalist, path, is_val=False)
    
    
    data_loader = torch.utils.data.DataLoader(
                    dataset, batch_size=args.batch_size, shuffle=True,
                    num_workers=args.num_workers, pin_memory=args.pin_memory)
    
    print (""Created data objects"")
    
    losses= []
    for epoch in range(args.epochs): 
        model.train()
        t0 = datetime.datetime.now()
        losses = []
        for i,(input_val,labels) in enumerate(data_loader): 

            #if transform_1 using, we got a vector of 5 DIM
            #and we only need a 4 DIM
            if args.transform == 'transform_1':  
                bs, n_crops, c, h, w = input_val.size() #e.g(4,10,3,224,244)
                input_val = input_val.view(-1, c, h, w) #e.g(40,3,224,224)
                bs, n_classes = labels.size() #e.g(4,14)
                labels = labels.unsqueeze(2).repeat(1, n_crops, 1 )
                labels = labels.contiguous().view(bs*n_crops, n_classes)
                
            prediction = model(to_variable(input_val))

            #print(""Finished forward pass"")
            #print (prediction.shape)
            
            train_loss = loss(prediction, to_variable(labels))
            optim.zero_grad()# Reset the gradients NEVER FORGET THIS
            train_loss.backward()
            optim.step() # Update the network
            
            losses.append(train_loss.data.cpu().numpy())

            
            if i % 200 == 0:
                print('Minibatch ',i,train_loss.data.cpu().numpy())
            if i % 800 == 0:
                val_loss = validate_routine(model, args, val_datalist, path)
                scheduler.step(np.asscalar(np.mean(val_loss)))
                model.train()
            
            
        print ('EPOCH', end='\t')
        print (""Epoch {} Train Loss: {:.4f}"".format(epoch, np.asscalar(np.mean(losses))), end='\t')
        print (""Epoch {} Validation Loss: {:.4f}"".format(epoch, np.asscalar(np.mean(val_loss))), end='\t')
        print (""Epoch Time: {}"".format(datetime.datetime.now()-t0))
        
        torch.save(model.state_dict(), args.save_checkpoint_filename)
        
    return model,losses",0.5831743479,
715,function to_labeled_point,"def to_season(month):
    if month in [3,4,5]: return 'spring'
    if month in [6,7,8]: return 'summer'
    if month in [9,10,11]: return 'fall'
    if month in [12,1,2]: return 'winter'",0.4696413875,
715,function to_labeled_point,"# A function which maps the season name
def season(month):
    if month in [3,4,5]:
        return 'Spring'
    if month in [6,7,8]:
        return 'Summer'
    if month in [9,10,11]:
        return 'Fall'
    if month in [12,1,2]:
        return 'Winter'",0.4696413875,
715,function to_labeled_point,"def convert_to_season(month):
    if month in [12, 1, 2, 3]:
        return ""winter""
    elif month in [4, 5]:
        return ""spring""
    elif month in [6, 7, 8, 9]:
        return ""summer""
    else:
        return ""fall""",0.4649253488,
715,function to_labeled_point,"def transform(row):
    if (row in [12,1,2]):
        return 'Winter'
    elif (row in [3,4,5]):
        return 'Summer'
    elif (row in [6,7,8]):
        return 'Rainy'
    else:
        return 'Autumn'
    

df_full['Season'] = df_full[""Month""].apply(transform)",0.4647208452,
715,function to_labeled_point,"def season(month):
    if month in [12,1,2]: return 'summer'
    if month in [3,4,5]: return 'autumn'
    if month in [6,7,8]: return 'winter'
    if month in [9,10,11]: return 'spring'
    
def binary_season(month):
    if month in [10,11,12,1,2,3]: return 'hot-season'
    return 'cold-season'",0.4609933496,
715,function to_labeled_point,"def SymbolToNumber(symbol):
    if symbol not in [""a"",""c"",""g"",""t""]:
        return ""keine gueltige Symbol""
    dic={""a"":0,""c"":1,""g"":2,""t"":3}
    return dic[symbol]",0.459440589,
715,function to_labeled_point,"def get_sentiment_from_score(value):
    if value > 0.65:
        return 'positive'
    elif value < 0.45:
        return 'negative'
    return 'neutral'",0.4586139321,
715,function to_labeled_point,"def getRdacGrade(rdac):
    if (rdac < 0.4):
        return 'LOW'
    if (rdac >= 0.4) & (rdac < 1):
        return 'MID'
    if (rdac >= 1) & (rdac < 2):
        return 'HIGH'
    return 'DANGER'",0.4583021998,
715,function to_labeled_point,"def switcharoo(x):
    if x == 0.0:
        return 'Low Risk'
    elif x == 1.0:
        return 'Caution'
    elif x == 2.0:
        return 'High Risk'
    elif x == 3.0:
        return 'Danger'

my_rank_2011['MyRank'] = [x+1 for x in range(my_rank_2011.shape[0])]
my_rank_2011['agg_rank_score'] = my_rank_2011['agg_rank_score'] * -1
my_rank_2011['clutch_rank_score'] = my_rank_2011['clutch_rank_score'] * -1
my_rank_2011['game_risk_score'] = my_rank_2011['game_risk_score'] * -1
my_rank_2011['game_risk_score'] = my_rank_2011['game_risk_score'].apply(switcharoo)

my_rank_2012['MyRank'] = [x+1 for x in range(my_rank_2012.shape[0])]
my_rank_2012['agg_rank_score'] = my_rank_2012['agg_rank_score'] * -1
my_rank_2012['clutch_rank_score'] = my_rank_2012['clutch_rank_score'] * -1
my_rank_2012['game_risk_score'] = my_rank_2012['game_risk_score'] * -1
my_rank_2012['game_risk_score'] = my_rank_2012['game_risk_score'].apply(switcharoo)

my_rank_2013['MyRank'] = [x+1 for x in range(my_rank_2013.shape[0])]
my_rank_2013['agg_rank_score'] = my_rank_2013['agg_rank_score'] * -1
my_rank_2013['clutch_rank_score'] = my_rank_2013['clutch_rank_score'] * -1
my_rank_2013['game_risk_score'] = my_rank_2013['game_risk_score'] * -1
my_rank_2013['game_risk_score'] = my_rank_2013['game_risk_score'].apply(switcharoo)

my_rank_2014['MyRank'] = [x+1 for x in range(my_rank_2014.shape[0])]
my_rank_2014['agg_rank_score'] = my_rank_2014['agg_rank_score'] * -1
my_rank_2014['clutch_rank_score'] = my_rank_2014['clutch_rank_score'] * -1
my_rank_2014['game_risk_score'] = my_rank_2014['game_risk_score'] * -1
my_rank_2014['game_risk_score'] = my_rank_2014['game_risk_score'].apply(switcharoo)

my_rank_2015['MyRank'] = [x+1 for x in range(my_rank_2015.shape[0])]
my_rank_2015['agg_rank_score'] = my_rank_2015['agg_rank_score'] * -1
my_rank_2015['clutch_rank_score'] = my_rank_2015['clutch_rank_score'] * -1
my_rank_2015['game_risk_score'] = my_rank_2015['game_risk_score'] * -1
my_rank_2015['game_risk_score'] = my_rank_2015['game_risk_score'].apply(switcharoo)",0.4582670629,
715,function to_labeled_point,"#rename day_of_week column values, convert from number to string to make value informative
def change_DAY_OF_WEEK(x):
    if x in [1,7]:
        return 'Weekend'
    elif x in range(2,7):
        return 'Weekday'",0.4579832554,
2267,table of contents,"%%HTML
<html>
  <body>
    <h1>Favorite Python Librarires</h1>
    <ul>
      <li>Numpy</li>
      <li>Pandas</li>
      <li>requests</li>
    </ul>
  </body>
</html>",0.3708313704,
2267,table of contents,"def process(response):
    selector = lxml.cssselect.CSSSelector('#main > div.text')
    lx = lxml.html.fromstring(response.body)
    title = lx.find('./head/title').text
    links = [a.attrib['href'] for a in lx.find('./a') if 'href' in a.attrib]
    for link in links:
        yield Request(url=link)
    divs = selector(lx)
    if divs: yield Item(utils.lx_to_text(divs[0]))",0.3698558211,
2267,table of contents,"def printGroupInfo(df):
    # View overview statistics
    print(df.describe())
    # View top 10
    print(""\nTop 10:\n"", df.apply(lambda x: x.sort_values(ascending=False)).head(10))",0.3648142815,
2267,table of contents,"# Minimal, uncluttered notebook display of astropy Tables
def show_no_dtype(astropy_table):
    return HTML('\n'.join(astropy_table.pformat(html=True,
                                                show_dtype=False,
                                                tableclass=""table-condensed table-bordered"")))",0.3598980308,
2267,table of contents,"# Define function to scrape met report 
def ScrapeMetReport(Page):
    ""Scrape MetReport Data from NWS for Site with ID and return df(data), str(time of observation), and (coordinates)""
    soup = BeautifulSoup(Page.content, 'html.parser')
    # Extract data from table body
    table_body = soup.find('table',class_=""inner-timeseries"")
    rows = table_body.find_all('tr')
    tabs=[]
    HeaderLines = 3
    ColumnNames1 = ['Time', 'Temperature', 'Dewpoint', 'Relative Humidity', 'Wind Dir', 'Surface Wind', 'Visibility', 'WX', 'Clouds', 'SLP', 'Altimeter',
               'StationP', '6h TMAX', '6h TMIN', '24h TMAX', '24h TMIN','QC']
    ColumnNames2 = ['Time', 'Temperature', 'Dewpoint', 'Relative Humidity', 'Wind Dir', 'Surface Wind', 'Visibility', 'WX', 'Clouds', 'SLP', 'Altimeter',
               'StationP', 'P1h','P3h','P6h','P24h','6h TMAX', '6h TMIN', '24h TMAX', '24h TMIN','QC']

    for row in rows[HeaderLines:]:
        cols=row.find_all('td')
        cols=[x.text.strip() for x in cols]
        tabs.append(cols)
    # and write data to dataframe
    if (len(cols)==17):
        df = pd.DataFrame(tabs, columns=ColumnNames1) 
    elif (len(cols) == 21):
        df = pd.DataFrame(tabs, columns=ColumnNames2) 
    else:
        raise Exception('Unexpected number of data columns MetReport')
    
    # Convert from string to numeric
    df['Temperature']=pd.to_numeric(df['Temperature'], errors='ignore')
    df['Dewpoint']=pd.to_numeric(df['Dewpoint'], errors='ignore')
    df['6h TMAX']=pd.to_numeric(df['6h TMAX'], errors='ignore')
    df['6h TMIN']=pd.to_numeric(df['6h TMIN'], errors='ignore')    
    df['24h TMAX']=pd.to_numeric(df['24h TMAX'], errors='ignore')
    df['24h TMIN']=pd.to_numeric(df['24h TMIN'], errors='ignore')    
    
    if 'P1h' in df.columns:
        df['P1h'].replace('T','0',inplace = True)
        df['P3h'].replace('T','0',inplace = True)
        df['P6h'].replace('T','0',inplace = True)
        df['P24h'].replace('T','0',inplace = True)
        df['P1h']=pd.to_numeric(df['P1h'], errors='ignore')
        df['P3h']=pd.to_numeric(df['P3h'], errors='ignore')
        df['P6h']=pd.to_numeric(df['P6h'], errors='ignore')
        df['P24h']=pd.to_numeric(df['P24h'], errors='ignore')
    
    
    # Adjust to proper date
    YDiff = YearNow-1900
    df['Time'] = pd.to_datetime(df['Time'], format='%d %b %I:%M %p')
    df['Time'] = df['Time'].apply(lambda x: x + pd.DateOffset(years=YDiff))
    df=df.set_index('Time')

    
    # Extract time of observations from website
    table_body = soup.find_all('table')
    rows = table_body[1].find_all('tr')
    cols=rows[2].find_all('td')
    cols=rows[3].find_all('td')
    TimeObs = cols[1].get_text()
    TimeObs = pd.to_datetime(TimeObs.split(',')[1], format=' %d %b %I:%M %p')
    TimeObs = TimeObs + pd.DateOffset(years=YDiff)
    
    # Extract Coordinates 
    LatLonStr = table_body[1].find_all(text=re.compile(""Latitude""))
    LatLonStr = re.split(' |;',LatLonStr[0] )
    Coord = tuple([w for w in LatLonStr if re.search('-?[0-9]{1,3}(?:\.[0-9]{1,10})', w)])

    

    
    return df, TimeObs, Coord",0.3554099202,
2267,table of contents,"%%HTML
<html>
  <body>
    <h1 class=""text-muted"">Favorite Python Librarires</h1>
    <ul class=""nav nav-pills nav-stacked"">
      <li role=""presentation""><a href=""http://www.numpy.org/"">Numpy</a></li>
      <li role=""presentation""><a href=""http://pandas.pydata.org/"">Pandas</a></li>
      <li role=""presentation""><a href=""http://python-requests.org/"">requests</a></li>
    </ul>
    <h1 class=""text-success"">Favorite JS Librarires</h1>
    <ul class=""nav nav-tabs"">
      <li role=""presentation""><a href=""http://getbootstrap.com/"">Bootstrap</a></li>
      <li role=""presentation""><a href=""https://jquery.com/"">jQuery</a></li>
      <li role=""presentation""><a href=""http://d3js.org/"">d3.js</a></li>
    </ul>
</html>",0.3534580469,
2267,table of contents,"def pivot(df):
    """"""Given a dataframe with columns named Count, State, and Year, returns a
    pivot table data frame where rows are indexed by state, columns represent
    each year found in the Year data series, and numbers are combined using
    the numpy.sum function
    
    Parameters
    ----------
    df : pandas dataframe
    """"""
    return df.pivot_table(values='Count', index='State', columns='Year', aggfunc=np.sum, fill_value=0)",0.3473261595,
2267,table of contents,"def exploreData(data):
    """"""
    Provide Data Exploration
    """"""
    from tabulate import tabulate
    print(""\nData Descriptive Stats:\n{}"".format(tabulate(data.describe(),headers='keys', tablefmt='psql')))
    print(""\nSample Data Head:\n{}"".format(tabulate(data.head(), headers='keys', tablefmt='psql')))
    print(""\nSample Data Tail:\n{}"".format(tabulate(data.tail(), headers='keys', tablefmt='psql')))",0.3448587656,
2267,table of contents,"%%html
<body>
    <h1>This is a heading</h1>
    <p>Normal text usually going in paragraph tags</p>
    <ul>
        <li>Item 1</li>
        <li><strong>We can bold text by putting it inside strong tags</strong></li>
        <li> <a href=""https://developer.mozilla.org/en-US/docs/Web/HTML/Element"">Link to MDN Docs</a> </li>
    </ul>
</body>",0.3429323137,
2267,table of contents,"def show_dataset_info(dataframe):
    """"""
    get a pandas dataframe as input and print the information.
    """"""
    # show basic info of the phase 2 training dataset
    print(dataframe.columns, '\n')
    print(dataframe.describe(), '\n')
    print(dataframe.head(), '\n')",0.3414283991,
2437,types of missingness,"def qualifying_game(game):
    return (game.HS >= 15 and game.FTHG == 0) or (game.AS >= 15 and game.FTAG == 0)
games = [g for g in epl.matches if qualifying_game(g)]
display(HTML(league_analysis.html_games(games)))",0.4317017794,
2437,types of missingness,"# create a list of the mean scores only
# YOUR CODE HERE

assert len(grid_mean_scores) == 30
assert isinstance(grid_mean_scores[0], np.float)",0.4224441051,
2437,types of missingness,"import engarde.decorators as ed

dtypes = dict(
    col1=int,
    col2=int)

@ed.is_shape((None, 10))
@ed.has_dtypes(items=dtypes)
@ed.none_missing()
@ed.within_range({'col3':[0, 150]})
def load_df():
    return df",0.4224043787,
2437,types of missingness,"# np.ones
ones = np.ones([4])
print(ones)
print()

# dtype of the array
print(ones.dtype.type is np.float64)
print()

# Creating 1s matrix, np.ones(lst)
ones_matrix = np.ones([3,3])
print(ones_matrix)",0.4206455946,
2437,types of missingness,"# np.zeros
zilch = np.zeros(3)
print(zilch)
print()

# dtype of the 0s
print(zilch.dtype.type is np.float64)
print()

# zero matrix, insert np.zeros(lst here)
zilch_matrix = np.zeros([3, 2])
print(zilch_matrix)",0.4202367067,
2437,types of missingness,"@ed.has_dtypes(final_types)
@ed.none_missing()
@ed.unique_index()
@ed.is_shape((None, 8))
def calculate_store_sales(sales):
    sales['store_total'] = sales.groupby('store_id').transform(sum)['sale_amount']
    sales['associate_total'] = sales.groupby('associate').transform(sum)['sale_amount']
    return sales",0.4200588465,
2437,types of missingness,"def downcast_dtypes(df):
    '''
        Changes column types in the dataframe: 
                
                `float64` type to `float32`
                `int64`   type to `int32`
    '''
    
    # Select columns to downcast
    float32_cols = [c for c in df if df[c].dtype == ""float64""]
    int32_cols = [c for c in df if df[c].dtype in [""int64"", ""int16"", ""int8""] ]
            
    # Downcast
    df[float32_cols] = df[float32_cols].astype(np.float32)
    df[int32_cols]   = df[int32_cols].astype(np.int32)
        
    return df",0.4198138118,
2437,types of missingness,"def downcast_dtypes(df):
    '''
        Changes column types in the dataframe: 
                
                `float64` type to `float32`
                `int64`   type to `int32`
    '''
    
    # Select columns to downcast
    float_cols = [c for c in df if df[c].dtype == ""float64""]
    int_cols =   [c for c in df if df[c].dtype == ""int64""]
    
    # Downcast
    df[float_cols] = df[float_cols].astype(np.float32)
    df[int_cols]   = df[int_cols].astype(np.int32)
    
    return df",0.4198138118,
2437,types of missingness,"def downcast_dtypes(df):
    '''
        Changes column types in the dataframe: 
                
                `float64` type to `float32`
                `int64`   type to `int32`
    '''
    # Select columns to downcast
    float_cols = [c for c in df if df[c].dtype == ""float64""]
    int_cols =   [c for c in df if df[c].dtype == ""int64""]
    
    # Downcast
    df[float_cols] = df[float_cols].astype(np.float32)
    df[int_cols]   = df[int_cols].astype(np.int32)
    
    if df.loc[:, 'device'].dtype != 'int16':
        df.loc[:, 'device'] = df.loc[:, 'device'].astype(np.int16)
    if df.loc[:, 'os'].dtype != 'int16':
        df.loc[:, 'os'] = df.loc[:, 'os'].astype(np.int16)
    if df.loc[:, 'channel'].dtype != 'int16':
        df.loc[:, 'channel'] = df.loc[:, 'channel'].astype(np.int16)
    if 'is_attributed' in df.columns:
        df.loc[:, 'is_attributed'] = df.loc[:, 'is_attributed'].astype(np.int8)
    
    if 'day' in df.columns and df.loc[:, 'day'].dtype != 'int8':
        df.loc[:, 'day'] = df.loc[:, 'day'].astype(np.int8)
    
    return df",0.4198138118,
2437,types of missingness,"def extract_ids(annotations):
    concepts = defaultdict(set)
    for annot in annotations:
        concepts[annot.stype].add(annot.uid)

    return concepts

def expand_set(vals):
    res = set()
    for v in vals:
        res |= v
        
    return res",0.4196404219,
1059,lets test our model on the set aside test set,"# Defining a helper function to plot a confusion matrix
def plot_confusion_matrix():
    """"""
    
    Plot confusion matrix between true and predicted labels
    
    """"""
    # Get the confusion matrix using scikit-learn
    cm = confusion_matrix(y_true=data.test.cls,
                          y_pred=pred_label)

    # Print the confusion matrix as text
    print(cm)

    # Plot the confusion matrix as an image.
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)

    # Make various adjustments to the plot.
    plt.tight_layout()
    plt.colorbar()
    tick_marks = np.arange(NUM_CLASSES)
    plt.xticks(tick_marks, LABELS)
    plt.yticks(tick_marks, LABELS)
    plt.xlabel('Predicted')
    plt.ylabel('True')",0.4438562393,
1059,lets test our model on the set aside test set,"def show_model_graph(wrapper):
    """"""Utility function to show model graph image in notebook.""""""
    filename = os.path.join(DATA_FOLDER, wrapper.name + '.png')
    plot_keras_model(wrapper.model, to_file=filename)
    return Image(filename=filename) 


def create_network(name, verbose=0, model_params=None, **compile_params):
    wrapper = init_model(name)
    model = wrapper.build(**(model_params or {}))
    if verbose >= 1:
        log.info(""Model '%s' architecture:"", name)
        display(show_model_graph(wrapper))
    if compile_params:
        model.compile(**compile_params)
    return model


def score_network(X, y, name, n_splits=3, nb_epoch=10, batch_size=150, verbose=2, 
                  model_params=None, callbacks=None, **compile_params):
    """"""Scores a specific model using stratified K-fold approach.""""""
    
    _ = create_network(name, model_params=model_params, verbose=1)        
    k_fold = StratifiedKFold(n_splits=n_splits, random_state=RANDOM_STATE)
    y_hot = np_utils.to_categorical(y)
        
    cv_scores = []
    log.info(""Dataset to train model: X.shape = %s, y.shape = %s"", X.shape, y.shape)
        
    if callbacks is None:
        callbacks = [EarlyStopping(min_delta=0.001)]
        
    with Timer() as timer:
        for i, (train, val) in enumerate(k_fold.split(X, y), 1):
            log.info('Split #%d', i)
            model = create_network(name, **compile_params)
            h = model.fit(
                X[train], y_hot[train],
                nb_epoch=nb_epoch,
                batch_size=batch_size,
                validation_data=(X_train_small[val], y_hot[val]), 
                verbose=verbose, 
                callbacks=callbacks).history
            cv_scores.append(h)
    log.info(""Total scoring time (%d folds): %s"", n_splits, str(timer))
        
    df_cv_scores = list(map(pd.DataFrame, cv_scores))
    names = model.metrics_names + ['val_%s' % name for name in model.metrics_names]
    scoring = pd.DataFrame()
    for name in names:
        all_runs = pd.concat([df[name] for df in df_cv_scores], axis=1)
        avg_for_epoch = all_runs.mean(axis=1)
        scoring[name] = avg_for_epoch

    log.info(""Average scores per epoch:"")
    display(scoring)
    return model, scoring",0.4293161035,
1059,lets test our model on the set aside test set,"def main():
    lines = load_lines(data_path)[1:]
    training_set_lines, validation_set_lines = train_test_split(lines, test_size=0.2)
    
    nb_training = len(training_set_lines)*6
    nb_validation = len(validation_set_lines)*6

    training_images, steering_angles = get_data_without_generator(data_path, lines[0:500])
    return (training_images, steering_angles)
data_path = ""data-from-udacity""
#main()",0.4290575683,
1059,lets test our model on the set aside test set,"def print_accuracy():
    test_accuracy = accuracy.eval(feed_dict={
        input_placeholder: mnist.test.images,
        desired_output_placeholder: mnist.test.labels,
        keep_prob: 1.0,
    })
    print ""Test-set accuracy %6.2f%%"" % (test_accuracy * 100,)

for training_step in xrange(5000):
    # Get a mini-batch that consists of (fifty images, fifty labels).
    # Thus, batch[0] has shape (50, 784) and batch[1] has shape (50, 10).
    batch = mnist.train.next_batch(50)

    # Print accuracy periodically.
    if training_step % 250 == 0:
        print ""Step %4i:"" % (training_step,),
        print_accuracy()

    # Do a training step on the batch!
    train_step.run(feed_dict={
        input_placeholder: batch[0],
        desired_output_placeholder: batch[1],
        keep_prob: 0.5,
    })

print ""Final result:"",
print_accuracy()",0.4218916297,
1059,lets test our model on the set aside test set,"def testOnce(data):
    # split the data into training and testing sets
    (trainingData, testData) = data.randomSplit([1-test_size, test_size])
     # train the random forest
    model = RandomForest.trainClassifier(trainingData, numClasses=3, categoricalFeaturesInfo={},
                                         numTrees=num_trees, featureSubsetStrategy=""auto"",
                                         impurity='gini', maxDepth = max_depth, maxBins=32)
    # test the random forest
    predictions = model.predict(testData.map(lambda x: x.features))
    labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)
    testErr = labelsAndPredictions.filter(lambda (v, p): v != p).count() / float(testData.count())
    Mg = float(labelsAndPredictions.filter(lambda (v, p): v == 0 and p == 1).count())
    Ng = float(labelsAndPredictions.filter(lambda (v, p): v == 0 and p == 0).count())
    Ms = float(labelsAndPredictions.filter(lambda (v, p): v == 1 and p == 0).count())
    Ns = float(labelsAndPredictions.filter(lambda (v, p): v == 1 and p == 1).count())
    probsAndScores = probTest(testData, model)
    threshold_accuracy = probsAndScores[0]
    probs = probsAndScores[1].map(lambda x: x/num_trees)
    labelsAndPredictions = labelsAndPredictions.zip(probs)
    #save(labelsAndPredictions, 'answers')
    print ('Galaxy Purity = ' + str(Ng / (Ng+Ms)))
    print ('Galaxy Completeness = ' + str(Ng / (Ng+Mg)))
    print ('Star Purity = ' + str(Ns / (Ns+Mg)))
    print ('Star Completeness = ' + str(Ns/(Ns+Ms)))
    print ('Accuracy = ' + str(1 - testErr))
    print ('Threshold method accuracy = ' + str(threshold_accuracy))",0.4191365242,
1059,lets test our model on the set aside test set,"def fit_predict_model():

    # Get the features and labels from the Boston housing data
    
    X, y = boston.data, boston.target

    # Setup a Decision Tree Regressor
    
    regressor = DecisionTreeRegressor()
    
    parameters = {'max_depth':(1,2,3,4,5,6,7,8,9,10)}
    
    mse_scoring = make_scorer(mean_squared_error, greater_is_better=False)
    
    #using grid search to fine tune the Decision Tree Regressor and
    #obtain the parameters that generate the best training performance. 

    reg = GridSearchCV(regressor, parameters, scoring = mse_scoring)
    reg.fit(X,y)
    
    # Fit the learner to the training data to obtain the best parameter set
    print ""Final Model: ""
    print (reg.fit(X, y))    

    # Using the model to predict the output of a particular sample
    x = [11.95, 0.00, 18.100, 0, 0.6590, 5.6090, 90.00, 1.385, 24, 680.0, 20.20, 332.09, 12.13]
    x = np.array(x)
    x = x.reshape(1, -1)
    y = reg.predict(x)

    #Predict Housing Price:
    print ""\nHouse: "" + str(x)
    print ""\nPredicted: "" + str(y)
    print ""\nBest Score %s:"" % (reg.best_score_)
    
    #DataFrame of Client_Features
    #x = [11.95, 0.00, 18.100, 0, 0.6590, 5.6090, 90.00, 1.385, 24, 680.0, 20.20, 332.09, 12.13]
    #pd.DataFrame(zip(boston.feature_names, x), columns = ['Features', 'Client_Features'])",0.4173176885,
1059,lets test our model on the set aside test set,"def evaluate_accuracy(model):
    global test_set
    
    # Allocate a Distributed Keras Accuracy evaluator.
    evaluator = AccuracyEvaluator(prediction_col=""prediction_index"", label_col=""label_index"")
    # Clear the prediction column from the testset.
    test_set = test_set.select(""features_normalized"", ""label_index"", ""label"")
    # Apply a prediction from a trained model.
    predictor = ModelPredictor(keras_model=trained_model, features_col=""features_normalized"")
    test_set = predictor.predict(test_set)
    # Allocate an index transformer.
    index_transformer = LabelIndexTransformer(output_dim=nb_classes)
    # Transform the prediction vector to an indexed label.
    test_set = index_transformer.transform(test_set)
    # Fetch the score.
    score = evaluator.evaluate(test_set)
    
    return score",0.415307343,
1059,lets test our model on the set aside test set,"def evaluate(model):
    global test_set

    metric_name = ""f1""
    evaluator = MulticlassClassificationEvaluator(metricName=metric_name, predictionCol=""prediction_index"", labelCol=""label_index"")
    # Clear the prediction column from the testset.
    test_set = test_set.select(""features_normalized"", ""label"", ""label_index"")
    # Apply a prediction from a trained model.
    predictor = ModelPredictor(keras_model=trained_model, features_col=""features_normalized"")
    test_set = predictor.predict(test_set)
    # Transform the prediction vector to an indexed label.
    index_transformer = LabelIndexTransformer(output_dim=nb_classes)
    test_set = index_transformer.transform(test_set)
    # Store the F1 score of the SingleTrainer.
    score = evaluator.evaluate(test_set)
    
    return score",0.4131273031,
1059,lets test our model on the set aside test set,"##Measure accuracy of the net
def print_test_accuracy(test_batch_size, mnist):
    data=mnist 
    data.test.cls = np.argmax(data.test.labels, axis=1)
    #data.validation.cls = np.argmax(data.test.labels, axis=1)
    #data.train.cls = np.argmax(data.train.labels, axis=1)
    # Numero de imagenes en test-set.
    num_test = len(data.test.images)

    # Crea arreglo para guardar clases predichas.
    cls_pred = np.zeros(shape=num_test, dtype=np.int)

    # Calcular clases predichas.
    i = 0
    while i < num_test:
        
        j = min(i + test_batch_size, num_test)
        images = data.test.images[i:j, :]
        images = (2*images)-1
        labels = data.test.labels[i:j, :]
        dp=1
        feed_dict = {x: images,
                     y_: labels, keep_prob: dp}

        cls_pred[i:j] = sess.run(y_pred_cls, feed_dict=feed_dict)
        i = j
    
    # Labels reales.
    cls_true = data.test.cls

    # Arreglo booleano de clasificaciones correctas.
    correct = (cls_true == cls_pred)
    
    #Numero de clasificaciones correctas.
    correct_sum = correct.sum()

    # Accuracy
    acc = float(correct_sum) / num_test
    msg = ""Accuracy on Test-Set: {0:.1%} ({1} / {2})""
    print(msg.format(acc, correct_sum, num_test))
    return acc, cls_true,  cls_pred",0.4129520059,
1059,lets test our model on the set aside test set,"def plot_images_comparison(idx):
    plot_images(images=data.test.images[idx, :],
                cls_true=data.test.cls[idx],
                ensemble_cls_pred=ensemble_cls_pred[idx],
                best_cls_pred=best_net_cls_pred[idx])",0.4122091532,
1559,predictions and evaluations,"def compute_accuracy(v_xs,v_ys):
    global prediction
    y_pre = sess.run(prediction,feed_dict={# if it is not neccessary ???
            xs:v_xs,
            ys:v_ys,
            keep_prob:0.5
        })
    correct_prediction = tf.equal(tf.argmax(y_pre,1),tf.argmax(v_ys,1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))
    result = sess.run(accuracy,feed_dict ={
            xs:v_xs,
            ys:v_ys,
            keep_prob:0.5
        })
    return result",0.5312044621,
1559,predictions and evaluations,"def evaluate_performance(model, x_test, y_test):
    test_set_predictions = [model.predict(x_test[i].reshape((1,len(x_test[i]))))[0] for i in range(x_test.shape[0])]
    test_misclassification_percentage = 0
    for i in range(len(test_set_predictions)):
        if test_set_predictions[i]!=y_test[i]:
            test_misclassification_percentage+=1
    test_misclassification_percentage *= 100/len(y_test)
    return test_misclassification_percentage",0.5265632272,
1559,predictions and evaluations,"W = np.ndarray(shape=(2, len(en_swad), en_emb.shape[1]), dtype=np.float32)
    W[0, :, :] = en_emb
    W[1, :, :] = de_emb
    T1, T, A = train(W, num_steps=50000)",0.5212088823,
1559,predictions and evaluations,"def forecast_lstm(model, batch_size, X):
    X = X.reshape(1, 1, len(X))
    yhat = model.predict(X, batch_size=batch_size)
    return yhat[0,0]",0.5207237601,
1559,predictions and evaluations,"# note about resize/reshaping contraints

def forecast_lstm(model, batch_size, X):
    X = X.reshape(1, 1, len(X))
    yhat = model.predict(X, batch_size=batch_size)
    return yhat[0,0]",0.5207237601,
1559,predictions and evaluations,"def draw_test_curve(model, X_test, y_test, name):
    pred_degs = model.predict(X_test,batch_size=X_test.shape[0])
    real_degs = y_test
    
    plt.figure
    plt.plot(pred_degs)
    plt.plot(real_degs)
    plt.ylabel('Steering angle', fontsize=12)
    plt.xlabel('Frame', fontsize=12)
    plt.xlim((0, 2700))
    plt.xticks(np.arange(0, 2701, 300))
    plt.legend(['predicted degree', 'real degree'], loc='upper right')
    plt.grid()
    plt.savefig(""./graphs/{}.png"".format(name), dpi=300)
    plt.show()",0.5203226805,
1559,predictions and evaluations,"# Halper clusters plotting function
# Takes the model, its centroids and the desired plot title
### It is based on previously computed grid which should be in scope! ###
def plotModel(model, centroids, title):
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    plt.figure()
    plt.clf()
    plt.imshow(Z, interpolation='nearest',
               extent=(xx.min(), xx.max(), yy.min(), yy.max()),
               cmap=plt.cm.Paired,
               aspect='auto', origin='lower')

    plt.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=2)
    plt.scatter(centroids[:, 0], centroids[:, 1],
                marker='x', s=169, linewidths=3,
                color='w', zorder=10)
    plt.title(title)
    plt.xlim(x_min, x_max)
    plt.ylim(y_min, y_max)
    plt.xticks(())
    plt.yticks(())
    plt.show()",0.5199161768,
1559,predictions and evaluations,"def gen_adv_boosting_data(model, data, groups):
    ''' 
    model  : is the LightGBM Model
    data   : data matrix with all valid attacks (last column is label)
    groups : grouping of same attacked instance 
    returns the new data matrix and new groups
    
    WARNING: currently works only for binary classification
    '''
    # score the datataset
    labels = data[:,-1]
    
    predictions = model.predict(data[:,:-1]) # exclude labels
    # binarize
    predictions = (predictions>0).astype(np.float)
    predictions = 2*predictions - 1
    
    # check mispredictions
    matchings = labels * predictions
    
    # select original data + attacked instances
    new_selected = [] # id of selected instances
    new_groups   = []
    
    offset = 0
    for g in groups:
        if g==0:
            print (""Error !!!!"")
        elif g==1:
            # there are no attacks, just add original
            new_selected += [offset]
            new_groups   += [1]
        else:
            # get a slice of the matching scores
            g_matchings = matchings[offset:offset+g]

            # most misclassified (smallest margin)
            # skip original
            adv_instance = np.argmin(g_matchings[1:])+1

            # add original and adversarial
            new_selected += [offset, offset+adv_instance]
            new_groups   += [2]
        
        offset += g
    
    new_dataset = data[new_selected,:]
    
    return new_dataset, new_groups",0.5193917751,
1559,predictions and evaluations,"def tf_graph_fullyconnected_softmax():
    global h_state_outputs_flat, y_preds_logit, y_preds_prob
    
    # flatting h_state_outputs
    h_state_outputs_flat = tf.reshape(h_state_outputs, [-1, hStateSize])

    y_preds_logit = layers.fully_connected(
        h_state_outputs_flat,
        alphaSize,
        activation_fn=tf.nn.relu # the default
        # activation_fn=tf.nn.softmax => WARNING: This op expects unscaled logits,
        # since it performs a softmax on logits internally for efficiency.
        # https://www.tensorflow.org/versions/master/api_docs/python/tf/nn/softmax_cross_entropy_with_logits_v2
    )

    y_preds_prob = tf.nn.softmax(y_preds_logit, name=""y_preds_prob"")  # (BS*MSL, AS)",0.5180925727,
1559,predictions and evaluations,"def report_results(model, X, y):
    pred_proba = model.predict_proba(X)[:, 1]
    pred = model.predict(X)        

    auc = roc_auc_score(y, pred_proba)
    acc = accuracy_score(y, pred)
    f1 = f1_score(y, pred)
    prec = precision_score(y, pred)
    rec = recall_score(y, pred)
    result = {'auc': auc, 'f1': f1, 'acc': acc, 'precision': prec, 'recall': rec}
    return result",0.5180703998,
1837,running a local postgres database advanced,"#dbconn = ???.connect('./classicmodels.db') 
dbconn = ???.connect(user='???', password='???',
                              host='127.0.0.1',
                              database='classicmodels')",0.5202678442,
1837,running a local postgres database advanced,"conn = psycopg2.connect(
    database='tradesy',
    port=os.environ['REDSHIFT_PORT'],
    password=os.environ['REDSHIFT_PASS'],
    user=os.environ['REDSHIFT_USER'],
    host=os.environ['REDSHIFT_HOST']
)",0.4922794104,
1837,running a local postgres database advanced,"conn = pymysql.connect(host=host,user=user,password=password,database='enwiki_p',connect_timeout=3600)
conn.cursor().execute('use enwiki_p');",0.4919024408,
1837,running a local postgres database advanced,"from utility import db_connect, query2csv
from settings import  DBNAME, DBPASS, DBUSER, DBHOST

def calculate_est_aadt(flow_detector_id, start_time, 
                       end_time, city, mode):
    query = """"""
with d as (
  select generate_series(0,6) as dayofweek
),
m as (
  select generate_series(1,12) as month
),
-- v_ijmy:Compute an average by day of week for each month.
v_ijmy as (
  select 
      baadv.analysis_area_id,
      date_part('year', baadv.date) as year,
      avg(baadv.volume)::bigint as volume_i,
      avg(baadv.volume) as volume,
      d.dayofweek,
      m.month
  from
      baa_ex_sus.analysis_areas_daily_volume as baadv,
      d,
      m
  where     
      extract(dow from baadv.date) in (d.dayofweek)  
      AND date_part('month', baadv.date) = m.month
      group by baadv.analysis_area_id, year, d.dayofweek, m.month       
),
-- madt: average volume each month, each year for sites
madt as (  
  select 
      analysis_area_id,
      month,
      year,
      avg(volume)::bigint as volume_i,
      avg(volume) as volume
  from 
      v_ijmy
      group by analysis_area_id, year, month
      having count(dayofweek)=7 -- having 7 days of data each week
),
AADT as (
select 
  analysis_area_id, 
  year,
  avg(volume)::bigint as AADT_i,
  round(avg(volume), 2) as AADT
from madt
  group by analysis_area_id, year
  having count(month) = 12 -- having 12 months of data
),
-- daily_exclude_holiday: daily counts for sites excluding holidays
daily_exclude_holiday as (
select
 baaad.analysis_area_id,
 baaad.date,
 baaad.volume,
 date_part('month', baaad.date) as month,
 date_part('dow', baaad.date) as dow
from
  baa_ex_sus.analysis_areas_daily_volume as baaad
  left join baa.holidays as baahd 
    on baaad.date::date = baahd.holiday_date
where
  baahd.holiday_id is null
  group by 1,2,3
),
V_jmyl_exclude_holiday as (
  select
      baadv.analysis_area_id,
      date_part('year', baadv.date) as year,
      avg(baadv.volume) as volume,
      d.dayofweek,
      m.month
  from
      daily_exclude_holiday as baadv,
      d,
      m
  where     
      extract(dow from baadv.date) in (d.dayofweek)  
      AND date_part('month', baadv.date) = m.month
      group by baadv.analysis_area_id, year, d.dayofweek, m.month       
),
-- 84 factors volume count should exclude holiday weeks
factor84 as (
select 
  v_jmyl_nh.analysis_area_id,
  v_jmyl_nh.volume as v_jmyl,
  AADT.aadt as aadt,
  round(v_jmyl_nh.volume/aadt::numeric, 2) as f_jmys,
  v_jmyl_nh.dayofweek,
  v_jmyl_nh.month,
  v_jmyl_nh.year
from
  V_jmyl_exclude_holiday as v_jmyl_nh inner join AADT 
    using(analysis_area_id, year)
where
  AADT.AADT <> 0
),
V_we as (
select 
  baadv.analysis_area_id,
  avg(baadv.volume) vwe
from 
  baa_ex_sus.analysis_areas_daily_volume as baadv
where 
  extract(dow from baadv.date) in (0,6)
  group by baadv.analysis_area_id
),
V_wd as (
select 
  baadv.analysis_area_id,
  avg(baadv.volume) vwd
from 
  baa_ex_sus.analysis_areas_daily_volume as baadv
where 
  extract(dow from baadv.date) in (1,2,3,4,5)
  group by baadv.analysis_area_id
),
grouping as (
select 
  V_we.analysis_area_id,
  round(V_we.vwe, 2) as V_we,
  round(V_wd.vwd, 2) as V_wd,
  round(V_we.vwe/V_wd.vwd, 2) as wwi,
  case 
     when (round(V_we.vwe/V_wd.vwd, 2) <= 0.8) 
       then 'Weekday Commute'
     when (round(V_we.vwe/V_wd.vwd, 2) >  1.2) 
       then 'Weekend Multipurpose'
     ELSE 'Weekly Multipurpose'
  END as grouping   
from 
  V_we inner join V_wd using (analysis_area_id)
),
wwi as (
select
  grouping.analysis_area_id,
  baaa.mode,
  baaa.analysis_area_name,
  baaa.analysis_area_regions_id,
  grouping.v_we,
  grouping.v_wd,
  grouping.wwi,
  grouping.grouping as weekly_group
from
   grouping inner join baa.analysis_areas 
     as baaa using(analysis_area_id)  
),
factorgrp as (
select 
  ar.analysis_area_name as city, 
  wwi.mode,
  wwi.weekly_group,
  array_agg(wwi.analysis_area_id order by analysis_area_id) 
    as analysis_area_id_list
from 
  wwi, baa.analysis_area_regions as ar
where 
  ar.analysis_area_regions_id = wwi.analysis_area_regions_id
  group by 1,2,3
), f84_wwi as (
  select
    fg.city,
    fg.weekly_group,
    fg.mode,
    fg.analysis_area_id_list,
    f84.dayofweek, 
    f84.month,
    f84.year,
    round(avg(f84.f_jmys), 2) as f_jmys_avg
  from 
    factor84 as f84 inner join factorgrp as fg
    on f84.analysis_area_id = Any(fg.analysis_area_id_list::int[])
    group by     
    fg.city,
    fg.weekly_group,
    fg.mode,
    fg.analysis_area_id_list,
    f84.dayofweek, 
    f84.month,
    f84.year
    order by fg.city,
    fg.weekly_group,
    fg.mode,f84.year, f84.month, f84.dayofweek
    ), 
-- start aadt estimator for a flow detector      
fltz as (
   select 
     flow_detector_id,
     bike_ped.get_flow_detector_timezone(flow_detector_id) 
       as timezone
     from 
       bike_ped.flow_detectors
     where
       flow_detector_id = {0}  
  ),
hrly as (  
  select 
    bpd.flow_detector_id,
    bpd.upload_id,
    date_trunc('hour', bpd.start_time) as start_time_utc,
    date_trunc('hour', bpd.start_time at time zone fltz.timezone) 
      as start_time,
    fltz.timezone as timezone,
    date_trunc('hour', bpd.start_time at time zone fltz.timezone)
      +'1 hour'::interval as end_time, 
    '1 hour'::INTERVAL as measure_period,
    sum(volume) as volume
from
    bike_ped.data as bpd inner join fltz using(flow_detector_id)
where 
    measure_period='00:15:00'
    and bpd.flow_detector_id = {0}
    and date_trunc('hour', bpd.start_time at time zone fltz.timezone) 
      >='{1}'
    and date_trunc('hour', bpd.start_time at time zone fltz.timezone) 
      <'{2}'
    group by flow_detector_id, upload_id, 
      date_trunc('hour', start_time), fltz.timezone , 
      date_trunc('hour', bpd.start_time at time zone fltz.timezone)
    having count(start_time) = 4
),
daily as (
select
    flow_detector_id,
    date_trunc('day', start_time) as date,
    sum(volume) as count,
    extract(dow from start_time) as dow 
from
    hrly
    group by 1,2,4
    --Restriction:Do not estimate AADT for days with <22hrs or >26 hours.
    having count(*) > 22 and count(*) < 26
),
-- weekend volume
vwe as (
  select
    avg(count) as vwe
  from
    daily
  where
    dow in (0, 6)  
),
-- weekday volume
vwd as (
  select
    avg(count) as vwd
  from
    daily
  where
    dow in (1,2,3,4,5)  
),
vwwi as (
select
  daily.flow_detector_id,
  daily.date,
  date_part('month', daily.date) as month,
  date_part('year', daily.date) as year,
  daily.count,
  daily.dow,
  --round(vwe.vwe,2) as vwe,
  --round(vwd.vwd,2) as vwd,
  round(vwe.vwe/vwd.vwd, 2) as wwi,
  case 
    when (round(vwe.vwe/vwd.vwd, 2) <= 0.8) 
      then 'Weekday Commute'
    when (round(vwe.vwe/vwd.vwd, 2) >  1.2) 
      then 'Weekend Multipurpose'
    ELSE 'Weekly Multipurpose'
  END as grouping
  from vwe, vwd, daily
),
est_aadt as (
select 
  vwwi.flow_detector_id,
  vwwi.date,
  vwwi.month,
  vwwi.year,
  vwwi.count,
  vwwi.grouping,
  f84.mode,
  round(vwwi.count/f84.f_jmys_avg, 2) as est_aadt
from vwwi,f84_wwi as f84
where 
  f84.city = '{3}'
  and f84.month = vwwi.month
  and f84.year = vwwi.year
  and f84.weekly_group = vwwi.grouping
  and f84.dayofweek = vwwi.dow
  and f84.mode = '{4}'
  order by vwwi.date   
  )
  select
  round(avg(est_aadt), 2) as estaadt
  from
    est_aadt
"""""".format(flow_detector_id, start_time, end_time, city, mode)
    conn = db_connect()
    with conn:
        with conn.cursor() as curs:
            curs.execute(query)
            rows = curs.fetchall()
            return (rows[0][0])",0.4900470376,
1837,running a local postgres database advanced,"gpuFFT.execute(Psi_gpu_k,Psi_gpu_k2)
getNablaPSI.prepared_call(gridDims, blockDims,
                          Nx,Ny,Lx,Ly,Psi_gpu_k2.gpudata)
gpuFFT.execute(Psi_gpu_k2, inverse=True)",0.4893881679,
1837,running a local postgres database advanced,"cvcdbfile = ""C:/users/phobson/Desktop/scratch/cvc/cvc.accdb""
cvcdb = pycvc.Database(cvcdbfile, nsqdata, bmpdb)",0.4852506518,
1837,running a local postgres database advanced,"con = mdb.connect(MySQL_data.my_sql_host, MySQL_data.my_sql_user,\
                    MySQL_data.my_sql_passwd,\
                    MySQL_data.my_sql_database)
#con.close",0.4850154519,
1837,running a local postgres database advanced,"conn = psycopg2.connect(
   database = dbname,
   user = user,
   password = password,
   host = host,
   port = port
)",0.4849162698,
1837,running a local postgres database advanced,"# As when setting up PSQL, the connection will need the password for the database entered here

con = psycopg2.connect(database = dbname, user = username, host='localhost', password=psswd)",0.4836954772,
1837,running a local postgres database advanced,"url = sqlalchemy.engine.url.URL(
    drivername='postgres+psycopg2',
    username=user,
    password=pw,
    host=host,
    port=5432,
    database=dbname
)
sqlalchemy.engine.url.make_url(url)",0.4827476144,
2607,working with arrays,"for x in np.nditer(a, op_flags=['readwrite']):
    x[...] = x * x",0.4768418074,
2607,working with arrays,"a = np.arange(6).reshape(2,3)
for x in np.nditer(a[:,2], op_flags=['readwrite']):
    x[...] = 2 * x
print(a)",0.4612212777,
2607,working with arrays,"bhp = np.sum(bhp_nn_diff,axis=0)
for i in range(len(norm_factor)):
    bhp_nn_diff[i,:,:] = norm_factor[i] * bhp_nn_diff[i,:,:]

bhp.shape",0.4605867267,
2607,working with arrays,"Kl[:,1] =  Tf * M * Matrix([[1/L],[1/L]])   # column 1
Kl[:,1]",0.4600381553,
2607,working with arrays,"# Imports required but not shown in the video lecture.
import numpy

from numpy import (add, array, empty_like, float32, float64, int32, 
                   logical_and, multiply, uint8, zeros)",0.4597935677,
2607,working with arrays,"# create X variable (=features i.e. molecular fingerprints)
X = np.array([x.fp for x in df.fp])
#X.shape

# create Y variable (=activity values i.e. blocker;1 or non-blocker;0)
y = np.array(df.ac)
#y.shape",0.4595111012,
2607,working with arrays,"# base case, time step 0
alpha[0, :] = pi * O[:,UP]
alpha",0.458538115,
2607,working with arrays,"a = np.array([[23, 34, 56],[-2, -13, 12],[-14, 20, 16]])
b = np.array([0,1,0])
a[np.arange(3),b]+=25
print(a)",0.4579746127,
2607,working with arrays,db.op_count(db.arrays.foo)[:],0.4577125311,
2607,working with arrays,"a = array([[n+m for n in range(5)] for m in range(5)])
print(a[1:3, 1:3])",0.4576491714,
1148,make a pandas data frame from csv,"import pandas as pd
def convert_df(datatlist):
    df = pd.DataFrame(datatlist)
    df.columns = ['index', 'actual', 'generated']
    return df",0.5214958191,
1148,make a pandas data frame from csv,"if False:
    df = pd.read_csv('https://raw.githubusercontent.com/1iyiwei/pyml/master/code/datasets/housing/housing.data',
                      header=None, sep='\s+')

    df.columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 
                  'NOX', 'RM', 'AGE', 'DIS', 'RAD', 
                  'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']
    df.head()",0.5211641192,
1148,make a pandas data frame from csv,"import pandas as pd
import numpy as np

def read_data(file):
    data = pd.read_csv(file, sep = ' ', header = None)
    data = data.drop(0, axis = 1)  # drop the first empty column
    
    return data


def generate_data(data):
    """"""Return data point X and lable Y.""""""
    
    X = data.loc[:, 0:1]
    Y = data.loc[:, 2:2].values
    
    N = X.shape[0]
    X0 = np.ones((N, 1))      # add coefficient term X0
    X = np.hstack((X0, X))
    
    return X, Y",0.5210325122,
1148,make a pandas data frame from csv,"import pandas as pd
import numpy as np

def read_data(file):
    data = pd.read_csv(file, sep = ' ', header = None)
    data = data.drop(0, axis = 1)  # drop the first empty column
    
    return data",0.5200803876,
1148,make a pandas data frame from csv,"import pandas as pd
import numpy as np

def read_data(file):
    data = pd.read_csv(file, sep = ' ', header = None).dropna(axis = 1)
    data.columns = [0,1,2]
    
    return data


def generate_data(data, positive):
    """"""Return data point X and lable Y.""""""
    X = np.array(data.loc[:, 1:2])
    Y = np.array(data.loc[:, :0]).flatten()
    Y[Y != positive] = -1
    Y[Y == positive] = 1
    
    return X, Y",0.5190213919,
1148,make a pandas data frame from csv,"#initialize an empty dataframe
df_all = pd.DataFrame(columns=['POS','POS_Counts'])

def AddRowToDF(listoftuples):
    global df_all
    #create a small dataframe
    df = pd.DataFrame(listoftuples)
    #name the columns
    df.columns = ['POS','POS_Counts']
    #append the small dataframe to the master dataframe
    df_all = df_all.append(df)
    
for i in list_of_pos_counts:
    AddRowToDF(i)

Grp_agg = df_all.groupby(['POS'])[['POS_Counts']].agg('sum')
Grp_agg.sort_values('POS_Counts', ascending = False)[:5]",0.5188622475,
1148,make a pandas data frame from csv,"if statson == True:
    data = pd.read_csv(metafile, index_col=['rat', 'diet'])

    data = data[:].stack()
    data = data.to_frame()
    data.reset_index(inplace=True) 
    data.columns = ['rat', 'diet', 'day', 'licks']
    ro.globalenv['r_data'] = data

    ro.r('bodyweight = aov(formula = licks ~ day * diet + Error(rat / day), data = r_data)')

    print(ro.r('summary(bodyweight)'))",0.5175625682,
1148,make a pandas data frame from csv,"import pandas as pd
import numpy as np
from datetime import datetime

# some preprocess
# turn the vader dict columns into four seperate cols.
def vader_score(dataframe):
    text = pd.DataFrame({'vader_negative':[0]*dataframe.shape[0],'vader_neutral':[0]*dataframe.shape[0],
                         'vader_positive':[0] * dataframe.shape[0], 'vader_compound':[0]*dataframe.shape[0]})
    for i in range(dataframe.shape[0]):
        text['vader_negative'].iloc[i] = eval(data_demo['vader_scores'].iloc[i])['neg']
        text['vader_positive'].iloc[i] =  eval(data_demo['vader_scores'].iloc[i])['pos']
        text['vader_neutral'].iloc[i] =  eval(data_demo['vader_scores'].iloc[i])['neu']
        text['vader_compound'].iloc[i] =  eval(data_demo['vader_scores'].iloc[i])['compound']
    return text

def turn_datetime(dataframe):
    t = []
    for i in range(dataframe.shape[0]):
        Month = dataframe['daily_discussion_date'].iloc[i].split(',')[1].split(' ')[1][0:3]
        Day = dataframe['daily_discussion_date'].iloc[i].split(',')[1].split(' ')[2]
        year = dataframe['daily_discussion_date'].iloc[i].split(',')[2].split(' ')[-1]
        datetime_object = datetime.strptime(Month + ' ' + Day + ' ' + year, '%b %d %Y')
        t.append(datetime_object)
    dataframe['daily_discussion_date'] = pd.DataFrame(t)

# get sentiment features

# will do a demo of daily here. Will modify later to see whether we use hourly or 6-hour interval.
def sentiment_normal_avg_byday(df, lexicon_name):
    # lexicon name can only be ""vader"", ""socialsent"".
    sent_catogory = [""compound"", ""neutral"", ""negative"", ""positive""]
    col_names = [lexicon_name+""_""+i for i in sent_catogory]
    groupby_dict = {}
    for key in col_names:
        groupby_dict[key] = ""mean""
    return data_demo.groupby(""daily_discussion_date"").agg(groupby_dict)

def sentiment_votes_avg_byday(df, lexicon_name):
    weighted_mean = lambda x: np.average(x, weights=df.loc[x.index, ""upvotes""])
    # lexicon name can only be ""vader"", ""socialsent"".
    sent_catogory = [""compound"", ""neutral"", ""negative"", ""positive""]
    col_names = [lexicon_name+""_""+i for i in sent_catogory]
    renames = [lexicon_name+""_""+""votes""+""_""+i for i in sent_catogory]
    groupby_dict = {}
    for key in col_names:
        groupby_dict[key] = weighted_mean
    df_result = data_demo.groupby(""daily_discussion_date"").agg(groupby_dict)
    df_result.columns = renames
    return df_result

def sentiment_child_avg_byday(df, lexicon_name):
    weighted_mean = lambda x: np.average(x, weights=df.loc[x.index, ""num_child""])
    # lexicon name can only be ""vader"", ""socialsent"".
    sent_catogory = [""compound"", ""neutral"", ""negative"", ""positive""]
    col_names = [lexicon_name+""_""+i for i in sent_catogory]
    renames = [lexicon_name+""_""+""child""+""_""+i for i in sent_catogory]
    groupby_dict = {}
    for key in col_names:
        groupby_dict[key] = weighted_mean
    df_result = data_demo.groupby(""daily_discussion_date"").agg(groupby_dict)
    df_result.columns = renames
    return df_result

# Bullish, Bearish, Long-term Holder, Bitcoin Skeptic, None
def get_author_opinion(dataframe):
    opinion_types = ['Bullish','Bearish','Long-term Holder','Bitcoin Skeptic','None']
    for type in opinion_types:
        dataframe[type] = 0
    for i in range(dataframe.shape[0]):
        dataframe.set_value(i, dataframe.iloc[i][""author_opinion""], 1)
    return dataframe

def opinion_avg_byday(df):
    opinion_types = ['Bullish','Bearish','Long-term Holder','Bitcoin Skeptic','None']
    groupby_dict = {}
    for type in opinion_types:
        groupby_dict[type] = ""mean""
    return data_demo.groupby(""daily_discussion_date"").agg(groupby_dict)",0.5171907544,
1148,make a pandas data frame from csv,"import pandas as pd
import numpy as np

DATA_DIR = ""data/""

def prepare_data(filename):
    df = pd.read_csv(DATA_DIR+filename, header=0)

    #Add interesting columns -- these did not help
    '''
    df['FamilySize'] = df['SibSp'] + df['Parch']
    df['FancyName'] = np.where(
            ((df.Name.str.contains(""Master."")) 
            | (df.Name.str.contains(""Rev."")) 
            | (df.Name.str.contains(""Dr.""))
            | (df.Name.str.contains(""Dr."")) 
            | (df.Name.str.contains(""Sir.""))),
        1,
        0
    )
    '''
    #Map strings to numbers
    df['EmbarkedCode'] = df['Embarked'].map( {'S':0,'C':1,'Q':2} ) 
    df['Gender'] = df['Sex'].map( {'male':0,'female':1} )
    
    #Set missing Embarked + Fare
    df = df.fillna(value={""EmbarkedCode"":df.EmbarkedCode.mode()[0]})
    df = df.fillna(value={""Fare"":df.Fare.mean()})

    #Set missing Age based on Sex and Pclass
    female_stats = df[(df.Sex==""female"") & (df.Age.notnull())].groupby(['Sex','Pclass']).Age.median()
    for i in xrange(df.Pclass.nunique()):
        df.loc[(df.Age.isnull()) & (df.Sex==""female"") & (df.Pclass==i+1),'Age'] = female_stats[i]
    male_stats = df[(df.Sex==""male"") & (df.Age.notnull())].groupby(['Sex','Pclass']).Age.median()
    for i in xrange(df.Pclass.nunique()):
        df.loc[(df.Age.isnull()) & (df.Sex==""male"") & (df.Pclass==i+1),'Age'] = female_stats[i]
    
    #Drop unnessary columns
    passenger_ids = df['PassengerId'].values
    df = df.drop(['Ticket', 'Cabin', 'Name', 'Embarked', 'Sex', 'PassengerId'], axis=1) 

    #Convert everything to floats
    for col in df.columns:
        df[col] = df[col].astype(np.float32)
    return df.values,passenger_ids
    
trained_data_matrix = prepare_data(""train.csv"")
trained_data_matrix",0.5168384314,
1148,make a pandas data frame from csv,"import numpy as np
import pandas as pd

ratingfile = 'ratings.data'
header = ['user_id', 'item_id', 'rating', 'timestamp']
df = pd.read_csv(ratingfile, sep='\t', names=header)

n_users = df.user_id.unique().shape[0]
n_items = df.item_id.unique().shape[0]
print('Number of users = ' + str(n_users) + ' | Number of movies = ' + str(n_items))

from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split

# split the data set into training set and testing set
train_data, test_data = train_test_split(df, test_size=0.25)
train = pd.DataFrame(train_data)

user_item_matrix = np.zeros((n_users, n_items))
for line in train.itertuples():
    user_item_matrix[line[1] - 1, line[2] - 1] = line[3]",0.5152849555,
1875,select the data,"def handle_click(grid, point):   
    
    # Select the Point
    grid.toggle_select(point)
    
    # If we only have 0 or 1 points selected 
    #  then return right away
    if len(grid.selected) < 2:
        return    
    
    # We have two points selected, get the
    #  board and the first selected point
    board = grid.data
    last_point = grid.selected[0]
   
    # Swap Items
    swap_items(board, last_point, point)
    
    # Deselect both points
    grid.toggle_select(last_point)
    grid.toggle_select(point)
    
    # Find all matches
    (h_matches, v_matches) = find_matches(board, 3)
    match_count = len(h_matches) + len(v_matches)
    
    if(match_count > 0):          
        # We found some matches so remove them and re-fill the board
        remove_matching_tiles(board, h_matches, v_matches)
        # The number of types is the same as the number of images on our grid
        number_of_types = len(grid.images)
        pull_down_cells(board, number_of_types)
    else:
        # We havent found any matches so swap back
        swap_items(board, last_point, point)        
    
    # Update the grid with the new board
    grid.data = board",0.5027937889,
1875,select the data,"def handle_click(grid, point):   
    
    grid.toggle_select(point)
    
    if len(grid.selected) < 2:
        return    
    
    board = grid.data
    last_point = grid.selected[0]
   
    swap_items(board, last_point, point)
    
    grid.toggle_select(last_point)
    grid.toggle_select(point)
    
    number_of_types = len(grid.images)
    # Process all matches and get the total number found
    match_count = process_matches(board, number_of_types)
    
    if(match_count == 0):
        # We havent found any matches so swap back
        swap_items(board, last_point, point)        
    
    
    grid.data = board",0.5017900467,
1875,select the data,"def handle_click(grid, point):   
        
    grid.toggle_select(point)
    
    if len(grid.selected) < 2:
        return    
    
    board = grid.data
    # Create our refresh function that will update the grid
    #  and pause so we can see the update
    refresh = lambda: update_grid(grid, board)
    last_point = grid.selected[0]
   
    swap_items(board, last_point, point)
    # We have made a change so refresh the grid
    refresh()
    
    grid.toggle_select(last_point)
    grid.toggle_select(point)
    
    number_of_types = len(grid.images)
    # We pass the refresh function to process_matches
    #  so that it can update the grid every time it changes
    #  something
    match_count = process_matches(board, number_of_types, refresh)
    
    if(match_count == 0):
        # We havent found any matches so swap back
        swap_items(board, last_point, point)      
        refresh()",0.5005217791,
1875,select the data,"def handle_click(grid, point, required_matches):   
        
    grid.toggle_select(point)
    
    if len(grid.selected) < 2:
        return    
    
    board = grid.data
    refresh = lambda: update_grid(grid, board)
    last_point = grid.selected[0]
   
    swap_items(board, last_point, point)
    refresh()
    
    grid.toggle_select(last_point)
    grid.toggle_select(point)
    
    number_of_types = len(grid.images)
    # Process all matches and get the total number found
    match_count = process_matches(board, number_of_types, required_matches, refresh)
    
    if(match_count == 0):
        # We havent found any matches so swap back
        swap_items(board, last_point, point)      
        refresh()",0.4951059818,
1875,select the data,"def handle_click(grid, point):   
    
    grid.toggle_select(point)
    
    # If we have zero or 1 points selected we dont need to do anymore
    if len(grid.selected) < 2:
        return    
    
    # lets get the current board
    board = grid.data
    last_point = grid.selected[0]
    
    # We have two different points, so we try to swap them
    swap_items(board, last_point, point)
    
    # Deselect both points
    grid.toggle_select(last_point)
    grid.toggle_select(point)
    
    # Update the grid
    grid.data = board",0.4920986295,
1875,select the data,"def permutation_test_means(table, variable, classes, repetitions):
    
    """"""Test whether two numerical samples 
    come from the same underlying distribution, 
    using the absolute difference between the means.
    table: name of table containing the sample
    variable: label of column containing the numerical variable 
    classes: label of column containing names of the two samples
    repetitions: number of random permutations""""""
    
    t = table.select(variable, classes)
    
    # Find the observed test statistic
    means_table = t.group(classes, np.mean) 
    obs_stat = abs(means_table.column(1).item(0) - means_table.column(1).item(1))
    
    # Assuming the null is true, randomly permute the variable 
    # and collect all the generated test statistics
    stats = make_array()
    for i in np.arange(repetitions):
        shuffled_var = t.select(variable).sample(with_replacement=False).column(0)
        shuffled = t.select(classes).with_column('Shuffled Variable', shuffled_var)
        m_tbl = shuffled.group(classes, np.mean)
        new_stat = abs(m_tbl.column(1).item(0) - m_tbl.column(1).item(1))
        stats = np.append(stats, new_stat)
    
    # Find the empirical P-value:
    emp_p = np.count_nonzero(stats >= obs_stat)/repetitions

    # Draw the empirical histogram of the tvd's generated under the null, 
    # and compare with the value observed in the original sample
    Table().with_column('Test Statistic', stats).hist(bins=20)
    plots.title('Empirical Distribution Under the Null')
    print('Observed statistic:', obs_stat)
    print('Empirical P-value:', emp_p)",0.4799239635,
1875,select the data,"def permutation_test_means(table, variable, classes, repetitions):
    
    """"""Test whether two numerical samples 
    come from the same underlying distribution, 
    using the absolute difference between the means.
    table: name of table containing the sample
    variable: label of column containing the numerical variable 
    classes: label of column containing names of the two samples
    repetitions: number of random permutations""""""
    
    t = table.select(variable, classes)
    
    # Find the observed test statistic
    means_table = t.group(classes, np.mean) 
    obs_stat = abs(means_table.column(1).item(0) - means_table.column(1).item(1))
    
    # Assuming the null is true, randomly permute the variable 
    # and collect all the generated test statistics
    stats = make_array()
    for i in np.arange(repetitions):
        shuffled_var = t.select(variable).sample(with_replacement=False).column(0)
        shuffled = t.select(classes).with_column('Shuffled Variable', shuffled_var)
        m_tbl = shuffled.group(classes, np.mean)
        new_stat = abs(m_tbl.column(1).item(0) - m_tbl.column(1).item(1))
        stats = np.append(stats, new_stat)
    
    # Find the empirical P-value:
    emp_p = np.count_nonzero(stats >= obs_stat)/repetitions

    # Draw the empirical histogram of the tvd's generated under the null, 
    # and compare with the value observed in the original sample
    Table().with_column('Test Statistic', stats).hist()
    plots.title('Empirical Distribution Under the Null')
    print('Observed statistic:', obs_stat)
    print('Empirical P-value:', emp_p)",0.4799239635,
1875,select the data,"def bootstrap_ci_means(table, variable, classes, repetitions):
    
    """"""Bootstrap approximate 95% confidence interval
    for the difference between the means of the two classes
    in the population""""""
    
    t = table.select(variable, classes)
    
    mean_diffs = make_array()
    for i in np.arange(repetitions):
        bootstrap_sample = t.sample()
        m_tbl = bootstrap_sample.group(classes, np.mean)
        new_stat = m_tbl.column(1).item(0) - m_tbl.column(1).item(1)
        mean_diffs = np.append(mean_diffs, new_stat)
        
    left = percentile(2.5, mean_diffs)
    right = percentile(97.5, mean_diffs)
    
    # Find the observed test statistic
    means_table = t.group(classes, np.mean) 
    obs_stat = means_table.column(1).item(0) - means_table.column(1).item(1)
    
    Table().with_column('Difference Between Means', mean_diffs).hist(bins=20)
    plots.plot(make_array(left, right), make_array(0, 0), color='yellow', lw=8)
    print('Observed difference between means:', obs_stat)
    print('Approximate 95% CI for the difference between means:')
    print(left, 'to', right)",0.4799239635,
1875,select the data,"def permuted_sample_average_difference(table, label, group_label, repetitions):
    
    tbl = table.select(group_label, label)
    
    differences = make_array()
    for i in np.arange(repetitions):
        shuffled = tbl.sample(with_replacement = False).column(1)
        original_and_shuffled = tbl.with_column('Shuffled Data', shuffled)

        shuffled_means = original_and_shuffled.group(group_label, np.average).column(2)
        simulated_difference = shuffled_means.item(1) - shuffled_means.item(0)
    
        differences = np.append(differences, simulated_difference)
    
    return differences",0.469150424,
1875,select the data,"def render_function(change):
    print_dynamic(""Final: {}, Initial: {}, GT: {}, Image: {}, Subplots: {}"".format(
            res_wid.selected_values['render_final_shape'],
            res_wid.selected_values['render_initial_shape'],
            res_wid.selected_values['render_gt_shape'],
            res_wid.selected_values['render_image'],
            res_wid.selected_values['subplots_enabled']))

res_wid = ResultOptionsWidget(has_gt_shape=True, has_initial_shape=True, has_image=True, 
                              render_function=render_function, style='info')
res_wid",0.4665834308,
846,how many products per category does the catalog contain?,"# Number of complaints per sub-product, grouped by product
    print(df.groupby(['Product','Sub-product']).count().head())",0.4840475321,
846,how many products per category does the catalog contain?,"# create a function that generates a chart of requests per neighborhood
def issues_by_neighborhood(neighborhood):
    """"""Generates a plot of issue categories by neighborhood""""""
    grouped_by_type = merged_311_data[merged_311_data['NEIGHBORHOOD'] == neighborhood].groupby(merged_311_data.Category)
    size = grouped_by_type.size().sort_values()
    size.plot(kind='barh', figsize=(8,6))",0.4839667678,
846,how many products per category does the catalog contain?,"def _get_category_mapping(column):
    """""" Return the mapping of a category """"""
    return dict([(cat, code) for code, cat in enumerate(column.cat.categories)])",0.4783983827,
846,how many products per category does the catalog contain?,"############################
# the product descriptions provided by HomeDepot are not properly spaced, 
# therefore, for each bullet point in a given product's attributes, add a space before it
############################
def addSpaces(row):
    spaceWords = []
    #for each bullet available for the currently given product...
    for bullet in bulletPoints[bulletPoints.product_uid == row.product_uid].value:
        spaceWords.append(bullet.split("" "")[0])   #take the first word in each bullet
    
    #append a space to the beginning of each word in 'spaceWords'
    newWords = ["""".join(pair) for pair in (zip(len(spaceWords)*"" "", spaceWords ))]
    return replaceAll(row.product_description,spaceWords, newWords)",0.4766964316,
846,how many products per category does the catalog contain?,"def get_quality_all(page_data, last_index=DEFAULT_ROW_COUNT):
    """"""
    Gets article quality for all revision IDs in a page data list, up
    to a given maximum.

    @param page_data: A page data list, formatted with revision ID as the
    third element in each row
    @type page_data: list or tuple
    @param last_index: The last index to consider
    @type last_index: int
    @return: article_quality
    @rtype: dict
    """"""

    # Use the the full length of the page data if the last index is less than
    # a minimum number of rows.
    if last_index <= DEFAULT_ROW_COUNT:
        last_index = len(page_data)

    # Declare and initialize the quality dictionary, and determine the number
    # of iterative calls.
    quality_dictionary = {}
    calls = last_index // PER_CALL

    # Declare and initialize the base index, and cycle for the given number of
    # full calls required to retrieve the indicated number of rows.
    base = 1
    for _ in range(0, calls):

        # Calculate the last index, and print a message.
        count = base + PER_CALL
        print('Retrieving quality rating for articles %d to %d...'
              % (base, count - 1))

        # Update the quality dictionary.
        quality_dictionary = make_quality_call(quality_dictionary,
                                               page_data,
                                               base,
                                               count)

        # Update the base index.
        base = count

    # Is the base index less than the last index?  If so, there is
    # a remaining number of rows...
    if base < last_index:

        # Print a message.
        print('Retrieving quality rating for articles %d to %d...' %
              (base, last_index - 1))

        # Update the quality dictionary with the remaining number of rows.
        quality_dictionary = make_quality_call(quality_dictionary,
                                               page_data,
                                               base,
                                               last_index)

    # Describe how long the dictionary is, and return is.
    print('Length of quality dictionary is %d' % len(quality_dictionary))
    return quality_dictionary",0.4722024798,
846,how many products per category does the catalog contain?,"# inside 'featx.py'
import collections

def label_feats_from_corpus(corp, feature_detector=bag_of_words):
    label_feats = collections.defaultdict(list)
    for label in corp.categories():
        for fileid in corp.fileids(categories=[label]):
            feats = feature_detector(corp.words(fileids=[fileid]))
            label_feats[label].append(feats)
    return label_feats",0.4713342786,
846,how many products per category does the catalog contain?,"def show_df(df, field='occupation'):
    df.groupBy(field).count().show()",0.4694204032,
846,how many products per category does the catalog contain?,"def assert_prep(expected, pdp_url):
    df = brands_info[brands_info['pdp_url'] == pdp_url]
    assert(df.product_category_gen.iloc[0] == expected), df.iloc[0]

pdp_array = [
    'https://www.victoriassecret.com/victorias-secret-sport/shop-all/sport-bralette-victoria-sport?productid=313539&cataloguetype=ols',
    'http://www.topshop.com/en/tsus/product/clothing-70483/lingerie-2313852/yasallie-mini-knickers-by-yas-6582953',
    'http://www.topshop.com/en/tsus/product/clothing-70483/lingerie-2313852/boho-lace-triangle-bra-by-bonds-6764089',
    'https://www.victoriassecret.com/victorias-secret-sport/shop-all/sport-bralette-victoria-sport?productid=313539&cataloguetype=ols',
    'http://www.topshop.com/en/tsus/product/clothing-70483/lingerie-2313852/yasallie-mini-knickers-by-yas-6582953',
    'http://www.calvinklein.us/en/womens-clothing/womens-sports-bras/womens-underwear-set-retro-black',
    'http://www.hankypanky.com/panties/signature-lace-brazilian-bikini-one.html',
    'https://www.victoriassecret.com/bras/shop-all-bras/flutter-bandeau-body-by-victoria?productid=335491&cataloguetype=ols',
    'http://www.topshop.com/en/tsus/product/clothing-70483/lingerie-2313852/sporty-branded-bra-6514233',
    'http://www.topshop.com/en/tsus/product/clothing-70483/lingerie-2313852/embroidered-mini-knickers-6331894',
    'https://www.victoriassecret.com/bras/shop-all-bras/lightweight-by-victoria-sport-bra-victoria-sport?productid=306335&cataloguetype=ols', 
    'https://www.victoriassecret.com/pink/panties/super-soft-lace-trim-boyshort-pink?productid=332296&cataloguetype=ols',
    'http://www.calvinklein.us/en/womens-clothing/women-panties/essentials-thong-51563445-699',
    'https://www.victoriassecret.com/pink/panties/lace-trim-cheekster-pink?productid=315760&cataloguetype=ols',
    'https://www.victoriassecret.com/bras/shop-all-bras/bandeau-flounce-bralette-the-bralette-collection?productid=331528&cataloguetype=ols',
    'https://www.victoriassecret.com/pink/shop-all-swim/strappy-high-neck-one-piece-pink?productid=335257&cataloguetype=ols',
    'https://www.victoriassecret.com/pink/apparel-activewear/2-pack-crew-sock-pink?productid=306791&cataloguetype=ols',
    'https://www.victoriassecret.com/pink/apparel-activewear/super-soft-beach-cheeky-cropped-tank-pink?productid=324392&cataloguetype=ols',
    'http://www.calvinklein.us/en/womens-clothing/women-sleepwear/modern-cotton-stretch-zip-hoodie-54015667-001',
    'https://www.victoriassecret.com/pink/all-bras/lace-t-shirt-bra-pink?productid=335657&cataloguetype=ols',
    'http://www1.macys.com/shop/product/wacoal-embrace-lace-soft-cup-wireless-bra-852191?id=1940063&categoryid=55799',
    'https://www.victoriassecret.com/pink/all-bras/ultimate-unlined-mesh-high-neck-sports-bra?productid=335737&cataloguetype=ols',
    'https://www.victoriassecret.com/victorias-secret-sport/shop-all/the-player-lace-up-sport-bra?productid=335811&cataloguetype=ols',
    'https://www.victoriassecret.com/victorias-secret-sport/shop-all/sport-bralette-victoria-sport?productid=313539&cataloguetype=ols',
    'http://www.topshop.com/en/tsus/product/clothing-70483/lingerie-2313852/sporty-lace-triangle-bra-6581791',
'http://www.topshop.com/en/tsus/product/clothing-70483/lingerie-2313852/seamless-sporty-bra-6237774',
'http://www1.macys.com/shop/product/wacoal-sport-high-impact-underwire-bra-855170?id=360198&categoryid=65739',
'http://www.topshop.com/en/tsus/product/clothing-70483/lingerie-2313852/seamless-sporty-branded-bra-6164565',
'http://www.topshop.com/en/tsus/product/clothing-70483/lingerie-2313852/seamless-sporty-thongs-6262929',
'http://www.topshop.com/en/tsus/product/clothing-70483/lingerie-2313852/topshop-branded-sporty-bralet-6816936',
'http://www.topshop.com/en/tsus/product/clothing-70483/lingerie-2313852/sporty-lace-bralet-6374365',
'https://www.victoriassecret.com/lingerie/shop-all-lingerie/macrame-lace-mix-sleep-short?productid=334002&cataloguetype=ols',
'https://www.victoriassecret.com/lingerie/shop-all-lingerie/double-v-sleep-romper?productid=335785&cataloguetype=ols',
'https://www.victoriassecret.com/lingerie/shop-all-lingerie/daisy-lace-sleep-tee-body-by-victoria?productid=334975&cataloguetype=ols',
'http://www.hankypanky.com/bras-and-tops/camisoles-and-tops/violet-spray-chiffon-sleep-top.html',

'http://www.topshop.com/en/tsus/product/clothing-70483/lingerie-2313852/shark-print-boypants-6556686',
'http://www.topshop.com/en/tsus/product/clothing-70483/lingerie-2313852/all-over-lace-ladypants-6527395',
'http://www.topshop.com/en/tsus/product/clothing-70483/lingerie-2313852/branded-waistband-jersey-boyshorts-6820829',
'http://www.topshop.com/en/tsus/product/clothing-70483/lingerie-2313852/ariel-boypants-6205718',
'https://www.victoriassecret.com/panties/shop-all-panties/raw-cut-hiphugger-sexy-smooth?productid=334151&cataloguetype=ols',
'https://www.victoriassecret.com/pink/panties/high-leg-logo-bikini-pink?productid=341004&cataloguetype=ols',
'https://www.victoriassecret.com/pink/panties/shortie-pink?productid=296225&cataloguetype=ols',
'https://www.victoriassecret.com/bras/shop-all-bras/unlined-demi-body-by-victoria?productid=336097&cataloguetype=ols',
'https://www.victoriassecret.com/pink/panties/logo-bikini-pink?productid=301636&cataloguetype=ols',
'https://www.victoriassecret.com/pink/all-bras/wildflower-lace-high-neck-push-up-pink?productid=316589&cataloguetype=ols',
'https://www.victoriassecret.com/pink/all-bras/daisy-lace-racerback-push-up-pink?productid=336295&cataloguetype=ols',
'https://www.victoriassecret.com/pink/all-bras/wear-everywhere-super-push-pink?productid=336277&cataloguetype=ols',
'https://www.victoriassecret.com/pink/all-bras/seamless-lightly-lined-racerback-pink?productid=334019&cataloguetype=ols',
'http://www1.macys.com/shop/product/wacoal-basic-beauty-wireless-contour-856192?id=561760&categoryid=65739',
'http://www1.macys.com/shop/product/wacoal-body-by-wacoal-seamless-underwire-65115?id=546525&categoryid=55799',


'http://www1.macys.com/shop/product/wacoal-body-base-shorty-874228?id=2906509&categoryid=55805',
'https://www.victoriassecret.com/pink/panties/lace-cross-front-unlined-halter?productid=338354&cataloguetype=ols',
'https://www.victoriassecret.com/pink/all-bras/lace-lightly-lined-triangle-pink?productid=335757&cataloguetype=ols',
'https://www.victoriassecret.com/pink/all-bras/bonded-strappy-bralette-pink?productid=331634&cataloguetype=ols',
'https://www.victoriassecret.com/panties/shop-all-panties/the-floral-lace-sexy-shortie-body-by-victoria?productid=336118&cataloguetype=ols',
'https://www.victoriassecret.com/pink/panties/shortie-pink?productid=296225&cataloguetype=ols',
'https://www.victoriassecret.com/bras/shop-all-bras/unlined-demi-body-by-victoria?productid=336097&cataloguetype=ols',
'https://www.victoriassecret.com/pink/panties/logo-bikini-pink?productid=301636&cataloguetype=ols',
'https://www.victoriassecret.com/pink/all-bras/wildflower-lace-high-neck-push-up-pink?productid=316589&cataloguetype=ols',
'https://www.victoriassecret.com/pink/all-bras/daisy-lace-racerback-push-up-pink?productid=336295&cataloguetype=ols',
'https://www.victoriassecret.com/pink/all-bras/wear-everywhere-super-push-pink?productid=336277&cataloguetype=ols',
'https://www.victoriassecret.com/pink/all-bras/seamless-lightly-lined-racerback-pink?productid=334019&cataloguetype=ols',
'http://btemptd.wacoal-america.com/b-tempt-d-lace-kiss-packaged-bikinis-btemptd-970282-p.html',
'http://btemptd.wacoal-america.com/b-splendid-bikini-btemptd-943255-p.html',
    'http://www.hankypanky.com/lingerie/short-european-flannel-robe.html'
]

expec_arrays = ['activewear','panties','bras','activewear','panties','shorts','panties','bralettes','activewear','panties',
                'activewear','panties','panties','panties','bralettes','swimsuits', 'pack','camisoles','sleepwear','bras',
                'bras','activewear', 'activewear', 'activewear', 'bras', 'activewear', 'activewear', 'activewear','panties', 'activewear',
                'bralettes', 'sleepwear', 'sleepwear', 'sleepwear','sleepwear','panties','panties','panties','panties', 'panties', 
                'panties','panties','bras','panties','bras','bras','bras','bras', 'bras', 'bras', 
                'panties', 'bras','bras','bralettes','panties','panties','bras','panties','bras','bras','bras', 'bras',
                'pack', 'panties', 'robe'
               ]

#http://www.topshop.com/en/tsus/product/clothing-70483/lingerie-2313852/sporty-lace-triangle-bra-6581791
# has only sporty underline
for ind, pdp in enumerate(pdp_array):
    #print(ind)
    assert_prep(expec_arrays[ind],pdp)",0.4693571925,
846,how many products per category does the catalog contain?,"def remove_nan(main_df, feat_df):
    """"""
    Identify missing or unknown data values and convert them to NaNs.
    """"""
    for idx in feat_df.index:
        # collumn name
        col_n = feat_df.iloc[idx].attribute
        # missing or unknow values for that collumn. Convert string to list
        m_u = feat_df.iloc[idx].missing_or_unknown[1:-1].split(',')
        # replace/convert values for those collumn/features, whose m_u value is empty list
        if m_u != [""""]:
            # convert values in the list to appropreate data type
            # first get data dype of feture collum
            # >> azdias[col_n].dtypes
            # then convert list to np array
            m_u = np.array(m_u, dtype=main_df[col_n].dtypes)
            # replace any values in m_u array with np.nan
            main_df[col_n] = main_df[col_n].replace(m_u, np.nan)
    return main_df",0.4685945511,
846,how many products per category does the catalog contain?,"def station_total(dataframe, column = 'total'):
    """"""
    station_total takes a pre-processed turnstile dataframe and returns the sum of each station's turnstile use.
    column selected for aggregation can be 'entries', 'exits', or 'total'
    Returns a dataframe of the station name, and the total sum
    """"""
    ## Group by STATION, even though this may end up lumping some smaller stations together.
    grouped = dataframe.groupby(['STATION', 'LINENAME']).sum()
    ## New column for total
    grouped['total'] = grouped.d_Entries + grouped.d_Exits
    
    if column == 'entries':
        total = grouped.d_Entries.sort_values(by=['d_Entries'], axis=0, ascending=False)
    elif column == 'exits':
        total = grouped.d_Exits.sort_values(by=['d_Exits'], axis=0, ascending=False)
    else:
        total = grouped.total.sort_values(ascending=False)

    return pd.DataFrame(total)


top_stations = station_total(subway, 'total')
top_stations.head(10)",0.4679868221,
1420,part implement the exponential linear unit elu activation function,"def inference(images, hidden_units1, hidden_units2):
    hidden1 = build_layer('hidden1', IMAGE_PIXELS, hidden_units1, images, tf.nn.relu)
    hidden2 = build_layer('hidden2', hidden_units1, hidden_units2, hidden1, tf.nn.relu)
    logits = build_layer('softmax_linear', hidden_units2, NUM_CLASSES, hidden2)
    return logits",0.4886678457,
1420,part implement the exponential linear unit elu activation function,"def gaussian_fun(x, mu, sigma):
    return (
        (1 / (sigma * np.sqrt(2 * np.pi))) * np.exp(
            -np.power(x - mu, 2) / (2 * np.power(sigma, 2))
        )
    )",0.4882344902,
1420,part implement the exponential linear unit elu activation function,"# HElPER FUNCTIONS
# sympy has problems of simplifying (V^2 sin(phi)^2)^{1/2} to V sin(phi)
def sympy_sqrt_fix(formula):
#  return sp.simplify(formula.subs((V**2*sp.sin(phi)**2)**(0.5), V*sp.sin(phi)))
  return sp.simplify(formula.subs(sp.sqrt( (V**2) * (sp.sin(phi))**2), V*sp.sin(phi)))
def zero_yb0(formula, val=0):
  return sympy_sqrt_fix(formula.subs(yb0,val))

GRAVITY=9.81

t, yb0, phi, V, g, vr = sp.symbols(""t, b_{y0}, \\varphi, \\nu, g, v_r"")
yb0 = 0 # simplify: y_b=0
yb = V*sp.sin(phi)*t - 0.5*g*t*t + yb0
xb = V*sp.cos(phi)*t

T = sp.solve(yb, t)[1]
R = xb.subs(t, T)
D = sp.symbols(""D"")
a0 = R+D # starting position

# Set-up Eq.(1) - the tan alpha(t), based on yb

a = R+D   # assume is not moving
#a = sp.symbols(""a_0"")

theta =  yb/(a-xb)
theta",0.4858202934,
1420,part implement the exponential linear unit elu activation function,"def fun_generator(amp, gamma, log_period):
    #guess the kernel numbers 
    kernel = amp*george.kernels.ExpSine2Kernel(gamma=gamma, log_period=log_period) #optimal gamma was ~15
    gp = george.GP(kernel)
    
    gp.compute(x,yerr)
    return gp.lnlikelihood(y)
    #pred, pred_var = gp.predict(y, x, return_var=True)

    #optimize the fit
    #gp2, ln_like2, fun = cgp.optimize(y,gp,gp.lnlikelihood, print_results=False)
    #return fun",0.484916687,
1420,part implement the exponential linear unit elu activation function,"def uR(s,u0,u1,x):
    return -s**(-1)*np.log((np.exp(-s*u0(x))+np.exp(-s*u1(x))))",0.4815312028,
1420,part implement the exponential linear unit elu activation function,"def pix_to_wl(pix, sgraph):
    fac = sgraph.xres / (sgraph.maxWl - sgraph.minWl)
    return sgraph.minWl + (pix / fac)
    
def wl_to_pix(wl, sgraph):
    fac = sgraph.xres / (sgraph.maxWl - sgraph.minWl)
    return (wl - sgraph.minWl) * fac
    
def get_peaks(shift, lfc, n):
    peaks = np.empty(0)
    current = shift
    for i in range(0, n):
        peaks = np.append(peaks, current)
        current += lfc.dWl
    return peaks
    
def pred_error(y, pred_y, solution, prediction):
    lse = np.mean(np.abs(y - pred_y))
    error = np.abs(solution - prediction) / solution * 3e8
    return lse, error
    
def guess_min(x, y):
    guess_x = x[y > 2000]
    guess_y = y[y > 2000]
    max_idx = signal.argrelextrema(guess_y, np.greater, order=10)
    guess_max_x = guess_x[max_idx]
    guess_max_y = guess_y[max_idx]
    guess_x = guess_max_x[0]
    guess_y = guess_max_y[0]
    return guess_x, guess_y
    
def find_lfc_peaks(sgraph, lfc):
    fac = sgraph.xres / (sgraph.maxWl - sgraph.minWl)
    lfc_peaks = lfc.getPeaks()
    
    def per_gauss_fit(wl, minWl):
        return per_gauss(wl, lfc.sigma * fac, minWl, minWl + ((len(lfc_peaks) - 1) * lfc.dWl * fac), lfc.dWl * fac, lfc.intensity)
    
    print('Solution is: {}pix or {}nm'.format(wl_to_pix(lfc.minWl, sgraph), lfc.minWl))

    x, y = sgraph.activate(lfc)
    
    # Do a cubic interpolation of x and y 
    spl = interpolate.UnivariateSpline(x, y, k=3)
    # ss stands for supersampled
    ss_x = np.linspace(0, sgraph.xres, 20000, dtype=np.float64)
    ss_y = spl(ss_x)
#     ss_x = x
#     ss_y = y
    ss_y[ss_y <= 0.0] = 1e-15
    
    # Fit on interpolated data
    guess_x, guess_y = guess_min(ss_x, ss_y)
    print('Initial guess for shift is: {}pix or {}nm'.format(np.asarray([guess_x]), pix_to_wl(guess_x, sgraph)))
    
    # Use lmfit for fitting
    gmod = lmfit.Model(per_gauss_fit)
    result = gmod.fit(ss_y, wl=ss_x, minWl=guess_x, method='lbfgsb', 
                      options={'ftol':1e-20, 'gtol':1e-20, 'maxiter':20000*(len(ss_x) + 1), 'factr':1.0, 'disp':True, 
                               'eps':np.finfo(np.float64).eps, 'maxls':100, 'maxfun':100000, 'maxcor':100})
#     result = gmod.fit(ss_y, wl=ss_x, minWl=guess_x, method='leastsq', 
#                       options={'disp': True, 'xtol': 1e-20, 'ftol': 1e-20, 'maxiter':20000000, 'maxfev':200000})
    pred_shift = result.params['minWl'].value
    y_pred = result.best_fit
    nfev = result.nfev
    pred_shift_wl = pix_to_wl(pred_shift, sgraph)
    print('Predicted shift is: {}pix or {}nm'.format(np.asarray([pred_shift]), pred_shift_wl))
    print('Number of calls: {}'.format(nfev))
    
    pred_peaks = get_peaks(pred_shift_wl, lfc, len(lfc_peaks))
    
    print(lfc_peaks[0:3], lfc_peaks[-3:])
    print(pred_peaks[0:3], pred_peaks[-3:])
    
    lse, error = pred_error(ss_y, y_pred, lfc.minWl, pred_shift_wl)
    print('Least squares error is {}'.format(lse))
    print('Error is: {}m/s'.format(error))
    
    lse, error = pred_error(y, per_gauss_fit(x, wl_to_pix(lfc.minWl, sgraph)), lfc.minWl, pred_shift_wl)
    print('Best squares error is {}'.format(lse))
    
    plt.scatter(ss_x, ss_y, color='blue')
    plt.scatter(x, y, color='magenta')
    plt.plot(ss_x, y_pred, color='red')
    for peak in pred_peaks:
        plt.axvline((peak - sgraph.minWl) * fac, color='green')
    for peak in lfc_peaks:
        plt.axvline((peak - sgraph.minWl) * fac, color='purple')
    plt.axvline(guess_x, color='turquoise')
    plt.xlim(guess_x - 3 * (lfc.dWl * fac), guess_x + 12 * (lfc.dWl * fac))
#     plt.xlim(wl_to_pix(lfc.maxWl, sgraph) - 12 * (lfc.dWl * fac), wl_to_pix(lfc.maxWl, sgraph) + 3 * (lfc.dWl * fac))
    fig = plt.show()
    
    return ss_x, ss_y, pred_shift",0.4798726439,
1420,part implement the exponential linear unit elu activation function,"def pixToWl(pix, sgraph):
    fac = sgraph.xres / (sgraph.maxWl - sgraph.minWl)
    return sgraph.minWl + (pix / fac)
    
def wlToPix(wl, sgraph):
    fac = sgraph.xres / (sgraph.maxWl - sgraph.minWl)
    return (wl - sgraph.minWl) * fac
    
def get_peaks(shift, sgraph, lfc, n):
    fac = sgraph.xres / (sgraph.maxWl - sgraph.minWl)
    peaks = np.empty(0)
    current = sgraph.minWl + (shift / fac)
    for i in range(0, n):
        peaks = np.append(peaks, current)
        current += lfc.dWl
    return peaks
    
def pred_error(y, pred_y, lfc_peaks, pred_peaks):
    lse = np.mean(np.abs(y - pred_y))
    error = np.mean(np.abs((lfc_peaks - pred_peaks) / lfc_peaks)) * 3e8
    print(lfc_peaks[0:3], lfc_peaks[-3:])
    print(pred_peaks[0:3], pred_peaks[-3:])
    
    print('Least squares error is ' + str(lse))
    print('Error is: ' + str(error))
    
def guess_min(x, y):
    guess_x = x[y > 2000]
    guess_y = y[y > 2000]
    max_idx = signal.argrelextrema(guess_y, np.greater, order=10)
    guess_max_x = guess_x[max_idx]
    guess_max_y = guess_y[max_idx]
    guess_x = guess_max_x[0]
    guess_y = guess_max_y[0]
    return guess_x, guess_y
    
def find_lfc_peaks(sgraph, lfc):
    fac = sgraph.xres / (sgraph.maxWl - sgraph.minWl)
    lfc_peaks = lfc.getPeaks()
    
    def per_gauss_fit(wl, minWl):
        return per_gauss(wl, lfc.sigma * fac, minWl, minWl + (len(peaks) * lfc.dWl * fac), lfc.dWl * fac, lfc.intensity)

    solution = wlToPix(lfc.minWl, sgraph)
    print('Solution is:', solution)

    x, y = sgraph.activate(lfc)
    
    # Do a cubic interpolation of x and y 
    spl = interpolate.UnivariateSpline(x, y, k=3)
    # ss stands for supersampled
    ss_x = np.linspace(0, sgraph.xres, 20000, dtype=np.float64)
    ss_y = spl(ss_x)
#     ss_x = x
#     ss_y = y
    ss_y[ss_y <= 0.0] = 1e-15
    
    # Fit on interpolated data
    guess_x, guess_y = guess_min(ss_x, ss_y)
    print('initial guess for shift is:', np.asarray([guess_x]))

    
    # Fit using curve_fit
#     popt, pcov = optimize.curve_fit(per_gauss_fit, ss_x, ss_y, p0=[guess_x], sigma=np.sqrt(ss_y), ftol=1e-12, xtol=1e-12, epsfcn=0.0)
    
    # Fit using leastsq
#     err_func = lambda tpl,x,y: (per_gauss_fit(x, tpl) - y)
# #     def jac_errfunc(p, t, y):
# #         ap = algopy.UTPM.init_jacobian(p)
# #         return algopy.UTPM.extract_jacobian(err_func(ap, t, y))
# #     popt, pcov, info = optimize.leastsq(err_func, [guess_x], Dfun=jac_errfunc, args=(ss_x, ss_y), xtol=1e-12, ftol=1e-12, maxfev=1000 * (len(ss_x) + 1))
#     popt, pcov, info, msg, success = optimize.leastsq(err_func, [guess_x], args=(ss_x, ss_y), full_output=1, xtol=1e-20, ftol=1e-20, maxfev=1000*(len(ss_x) + 1))
#     nfev = info['nfev']
#     pred_shift = popt[0]
#     y_pred = per_gauss_fit(ss_x, *popt)

#     Fit using Brent
#     err_func = lambda shift: np.mean(np.abs(per_gauss_fit(ss_x, shift) - ss_y))
#     result = optimize.minimize_scalar(err_func, method='Brent', bracket=(guess_x-5, guess_x+5), tol=1e-15)
#     nfev = result.nit
#     pred_shift = result.x
#     y_pred = per_gauss_fit(ss_x, pred_shift)
    
    # Approx with curve_fit
#     popt, pcov = optimize.curve_fit(per_gauss_fit, ss_x, ss_y, p0=[guess_x], sigma=np.sqrt(ss_y), ftol=1e-12, xtol=1e-12, epsfcn=0.0)
#     second_guess_x = popt[0]
#     print('Second guess is:', second_guess_x)
    
    #####
    # Approx with lmfit
#     ss_x = ss_x[ss_y > 4000]
#     ss_y = ss_y[ss_y > 4000]
#     gmod = lmfit.Model(per_gauss_fit)
#     result = gmod.fit(ss_y, wl=ss_x, minWl=guess_x, method='lbfgsb', 
#                       options={'ftol':1e-20, 'gtol':1e-20, 'maxiter':20000*(len(ss_x) + 1), 'factr':10.0, 'disp':True, 
#                                'eps':np.finfo(np.float64).eps, 'maxls':100, 'maxfun':100000, 'maxcor':100})
#     print('lbfgsb executed # times:', result.nfev)
#     second_guess_x = result.params['minWl'].value
#     print('Second guess is:', second_guess_x)
#     temp_pred = per_gauss_fit(ss_x, second_guess_x)
#     lse = np.mean(np.abs(ss_y - temp_pred))
#     print('Second guess lse is ' + str(lse))
    
#     solution = (lfc.minWl - sgraph.minWl) * fac
#     print('Solution is:', solution)
#     temp_pred = per_gauss_fit(ss_x, solution)
#     lse = np.mean(np.abs(ss_y - temp_pred))
#     print('lse of optimap solution is ' + str(lse))
    
#     # Fit using brute
#     err_func = lambda shift: np.mean(np.abs(per_gauss_fit(ss_x, shift) - ss_y))
#     result = optimize.brute(err_func, ranges=((second_guess_x-1e-3, second_guess_x+1e+3),), Ns=100, full_output=True, finish=None)
#     nfev = 0
#     pred_shift = result[0]
#     y_pred = per_gauss_fit(ss_x, pred_shift)
    ######
    
#     eps = np.finfo(np.float64).eps
#     err_func = lambda tpl,x,y: (per_gauss_fit(x, tpl) - y)
#     result = optimize.least_squares(err_func, [guess_x], args=(ss_x, ss_y), xtol=eps, ftol=eps, gtol=eps,
#                                     max_nfev=1000 * (len(ss_x) + 1), method='dogbox')
#     popt = result.x
#     pred_shift = popt[0]
#     nfev = result.nfev
#     y_pred = per_gauss_fit(ss_x, *popt)
#     print('Terminated because: ' + result.message)

    # Use lmfit for fitting
    gmod = lmfit.Model(per_gauss_fit)
    result = gmod.fit(ss_y, wl=ss_x, minWl=guess_x, method='lbfgsb', 
                      options={'ftol':1e-20, 'gtol':1e-20, 'maxiter':20000*(len(ss_x) + 1), 'factr':10.0, 'disp':True, 
                               'eps':np.finfo(np.float64).eps, 'maxls':100, 'maxfun':100000, 'maxcor':100})
    pred_shift = result.params['minWl'].value
    y_pred = result.best_fit
    nfev = result.nfev
    pred_peaks = np.empty(0)
    print('Predicted shift is:', np.asarray([pred_shift]))
    print('Number of calls: ' + str(nfev))
    
    # Switch to WL space
    pred_peaks = get_peaks(pred_shift, sgraph, lfc, len(lfc_peaks))
    pred_error(ss_y, y_pred, pixToWl(lfc_peaks, sgraph), pred_peaks, sgraph)
    
    plt.scatter(ss_x, ss_y, color='blue')
    plt.scatter(x, y, color='magenta')
    plt.plot(ss_x, y_pred, color='red')
    for peak in pred_peaks:
        plt.axvline((peak - sgraph.minWl) * fac, color='green')
    for peak in peaks:
        plt.axvline((peak - sgraph.minWl) * fac, color='purple')
    plt.axvline(guess_x, color='turquoise')
    plt.xlim(guess_x - 3 * (lfc.dWl * fac), guess_x + 12 * (lfc.dWl * fac))
    fig = plt.show()
    
    return ss_x, ss_y, pred_shift",0.4798726439,
1420,part implement the exponential linear unit elu activation function,"def model(x, act=tf.nn.relu): 
    layer_1 = act(tf.add(tf.matmul(x, weights['h1']), biases['b1']))
    layer_2 = act(tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']))
    out_layer = tf.add(tf.matmul(layer_2, weights['out']), biases['out'], name=""absolute_output"")
    return out_layer

# Construct model
logits = model(X)",0.4789742231,
1420,part implement the exponential linear unit elu activation function,"def gaussian(x, mu, sig2):
    return (1.0 / np.power(2 * sig2 * np.pi, 0.5)) * np.exp(-(np.power(x - mu, 2.)) / (2.0 * sig2))",0.4789415896,
1420,part implement the exponential linear unit elu activation function,"def prelu(x):
    pos = tf.nn.relu(x)
    neg = prelu_alpha * (x - abs(x)) * 0.5

    return pos + neg",0.4786907136,
2431,tuning a regression tree,crTree.getTreeBranch(['FEM']).printStats2(),0.5536754131,
2431,tuning a regression tree,dhcrTree.getTreeBranch(['FEM']).printStats2(),0.5536754131,
2431,tuning a regression tree,"crTree.getTreeBranch(['XFEM', 'simple']).printStats2()",0.5432239175,
2431,tuning a regression tree,"dhcrTree.getTreeBranch(['XFEM', 'simple']).printStats2()",0.5432239175,
2431,tuning a regression tree,"dhcrTree.getTreeBranch(['XFEM', 'crackPartition', 'LinearTet']).printStats2()",0.532980144,
2431,tuning a regression tree,"crTree.getTreeBranch(['XFEM', 'crackPartition', 'LinearTet']).printStats2()",0.532980144,
2431,tuning a regression tree,"# Train tree
tree.fit(train, train_labels)
print(f'Decision tree has {tree.tree_.node_count} nodes with maximum depth {tree.tree_.max_depth}.')",0.518553257,
2431,tuning a regression tree,"import math
import numpy as np

class decision_tree_regressor:
    
    def __init__(self, max_depth = None, criteria='std'):
        """"""
        Builds a decision tree to regress on the target data. The
        decision tree is built by trying to minimize the requested
        criteria at each possible split. Starting with the whole data
        the tree will try every possible split (column and value pair)
        and choose the split which results in the greatest reduction 
        of the criteria. Then for each sub-group of that split, the 
        process is repeated recursively until no more splits are possible
        or no splits cause a reductuion of the criteria.
        ---
        KWargs:
        max_depth: how many splits to allow in the tree (depth not breadth)
        criteria: what metric to use as a measure of split strength 
        ('std'= reduction of standard deviation in the data, 'mae'=
        minimize the mean error size (abs value))
        """"""
        self.tree = self.tree_split()
        self.data_cols = None
        self.max_depth = max_depth
        self.current_depth = 0
        self.criteria = criteria
    
    # Sub class for handling recursive nodes (only makes sense in the scope of a tree)
    class tree_split:
        """"""
        A sub class for handling recursive nodes. Each node will contain the value and column
        for the current split, as well as links to the resulting nodes from the split. The 
        results attribute remains empty unless the current node is a leaf. 
        """"""
        def __init__(self,col=-1,value=None,results=None,label=None,tb=None,fb=None):
            self.col=col # column index of criteria being tested
            self.value=value # vlaue necessary to get a true result
            self.results=results # dict of results for a branch, None for everything except endpoints
            self.tb=tb # true decision nodes 
            self.fb=fb # false decision nodes
    
    def split_data(self, X, y, colnum, value):
        """"""
        Returns: Two sets of data from the initial data. Set 1 contains those that passed
        the condition of data[colnum] >= value
        ----------
        Input: The dataset, the column to split on, the value on which to split
        """"""
        splitter = None
        if isinstance(value, int) or isinstance(value,float):
            splitter = lambda x: x[colnum] >= value
        else:
            splitter = lambda x: x[colnum] == value
        split1 = [i for i,row in enumerate(X) if splitter(row)]
        split2 = [i for i,row in enumerate(X) if not splitter(row)]
        set1X = X[split1]
        set1Y = y[split1]
        set2X = X[split2]
        set2Y = y[split2]
        return set1X, set1Y, set2X, set2Y

    def get_mean_target_value(self, data):
        """"""
        Returns: A dictionary of target variable counts in the data
        """"""
        return np.mean(data)

    def split_criteria(self, y):
        """"""
        Returns the criteria we're trying to minimize by splitting.
        Current options are target Mean Absolute Error (from the target 
        mean) or Standard deviation of the target.
        ---
        Input: targets in the split
        Output: Criteria
        """"""
        if self.criteria == 'mae':
            mu = np.mean(y)
            return np.mean(np.abs(y-mu))
        else:
            return np.std(y)
    
    def pandas_to_numpy(self, x):
        """"""
        Checks if the input is a Dataframe or series, converts to numpy matrix for
        calculation purposes.
        ---
        Input: X (array, dataframe, or series)
        Output: X (array)
        """"""
        if type(x) == type(pd.DataFrame()) or type(x) == type(pd.Series()):
            return x.as_matrix()
        if type(x) == type(np.array([1,2])):
            return x
        return np.array(x) 
        
    def handle_1d_data(self,x):
        """"""
        Converts 1 dimensional data into a series of rows with 1 columns
        instead of 1 row with many columns.
        """"""
        if x.ndim == 1:
            x = x.reshape(-1,1)
        return x
    
    def convert_to_array(self, x):
        """"""
        Takes in an input and converts it to a numpy array
        and then checks if it needs to be reshaped for us
        to use it properly
        """"""
        x = self.pandas_to_numpy(x)
        x = self.handle_1d_data(x)
        return x
    
    def fit(self, X, y):
        """"""
        Helper function to wrap the fit method. This makes sure the full nested, 
        recursively built tree gets assigned to the correct variable name and 
        persists after training.
        """"""
        self.tree = self._fit(X,y)
    
    def _fit(self, X, y, depth=0):
        """"""
        Builds the decision tree via a greedy approach, checking every possible
        branch for the best current decision. Decision strength is measured by
        information gain/score reduction. If no information gain is possible,
        sets a leaf node. Recursive calls to this method allow the nesting. If
        max_depth is met, all further nodes become leaves as well.
        ---
        Input: X (feature matrix), y (labels)
        Output: A nested tree built upon the node class.""""""
        X = self.convert_to_array(X)
        y = self.convert_to_array(y)
       
        if len(X) == 0: return tree_split()
        current_score = self.split_criteria(y)

        best_gain = 0.0
        best_criteria = None
        best_sets = None
        
        self.data_cols = X.shape[1]
        
        
        # Here we go through column by column and try every possible split, measuring the
        # information gain. We keep track of the best split then use that to send the split
        # data sets into the next phase of splitting.
        
        for col in range(self.data_cols):
            column_values = set(X.T[col])
            for value in column_values:
                set1, set1_y, set2, set2_y = self.split_data(X, y, col, value)
                p = float(len(set1)) / len(y)
                gain = current_score - p*self.split_criteria(set1_y) - (1-p)*self.split_criteria(set2_y)
                if gain > best_gain and len(set1_y) and len(set2_y):
                    best_gain = gain
                    best_criteria = (col, value)
                    best_sets = (np.array(set1), np.array(set1_y), np.array(set2), np.array(set2_y))
        
        # Now decide whether it's an endpoint or we need to split again.
        if (self.max_depth and depth < self.max_depth) or not self.max_depth:
            if best_gain > 0:
                self.current_depth += 1
                true_branch = self._fit(best_sets[0], best_sets[1], depth=depth+1)
                false_branch = self._fit(best_sets[2], best_sets[3], depth=depth+1)
                return self.tree_split(col=best_criteria[0], value=best_criteria[1],
                        tb=true_branch, fb=false_branch)
            else:
                return self.tree_split(results=self.get_mean_target_value(y))
        else:
            return self.tree_split(results=self.get_mean_target_value(y))

    def print_tree(self, indent=""---""):
        """"""
        Helper function to make sure the correct tree gets printed.
        ---
        In: indent (how to show splits between nodes)
        """"""
        self.__original_indent = indent
        self._print_tree_(self.tree, indent)
    
    def _print_tree_(self, tree, indent):
        """"""
        Goes through node by node and reports the column and value used to split
        at that node. All sub-nodes are drawn in sequence below the node.
        """"""
        if tree.results: # if this is a end node
            print(str(tree.results))
        else:
            print('Column ' + str(tree.col)+' : '+str(tree.value)+'? ')
            # Print the branches
            print(indent+' True: ', end=' ')
            next_indent = indent+self.__original_indent
            self._print_tree_(tree.tb,indent=next_indent)
            print(indent+' False: ', end=' ')
            self._print_tree_(tree.fb,indent=next_indent)

    def predict(self, newdata):
        """"""
        Helper function to make sure the correct tree is used to
        make predictions. Also manages multiple rows of input data
        since the tree must predict one at a time.
        ---
        In: new data point of the same structure as the training X.
        Out: numpy array of the resulting predictions
        """"""
        results = []
        newdata = self.convert_to_array(newdata)
        for x in newdata:
            results.append(self._predict(x,self.tree))
        return np.array(results)
            
    def _predict(self, newdata, tree):
        """"""
        Uses the reusive structure of the tree to follow each split for
        a new data point. If the node is an endpoint, the available classes
        are sorted by ""most common"" and then the top choice is returned.
        """"""
        if tree.results: # if this is a end node
            return tree.results

        if isinstance(newdata[tree.col], int) or isinstance(newdata[tree.col],float):
            if newdata[tree.col] >= tree.value:
                return self._predict(newdata, tree.tb)

            else:
                return self._predict(newdata, tree.fb)
        else:
            if newdata[tree.col] == tree.value:
                return self._predict(newdata, tree.tb)
            else:
                return self._predict(newdata, tree.fb) 

    def score(self, X, y):
        """"""
        Uses the predict method to measure the accuracy of the model.
        ---
        In: X (list or array), feature matrix; y (list or array) labels
        Out: accuracy (float)
        """"""
        pred = self.predict(X)
        return np.mean((pred-y)**2)",0.5084972978,
2431,tuning a regression tree,"id3_iris.prune(validate)
print(id3_iris.score(train), id3_iris.score(validate), id3_iris.score(test))
id3_iris.display_tree()",0.5065413713,
2431,tuning a regression tree,runner.score_leaves(),0.5018526316,
1476,payload,"# ML Example:
""""""
def _generate_payload(self):
    X = pd.read_csv(self.params['datafile'])
    model = XGboost(X)
    model.fit()
    self.payload['model'] = model
""""""

# Silly Example:
def _generate_payload(self):
    self.payload['sum'] = self.params['a'] + self.params['b']
    self.payload['msg'] = self.params['hello']",0.5520933867,
1476,payload,"def print_midi_message(port, message):
    if message.bytes()[0] == 176:
        print(""[{}] @{:0.6f} {}"".format(port.name, message.time, message.bytes()))
read_midi_messages(device_name)",0.4973732829,
1476,payload,"def callback(data):
    rospy.loginfo(rospy.get_caller_id() + ""I heard %s"", data.data)",0.4929044843,
1476,payload,"def print_response(response_text):
    if (r.ok):
        obj = json.loads(response_text)
        print(""Returned successfully posted object:\n"",obj)
        return obj
    else:
        print(""Error: "",r.text)
    return None",0.4923110306,
1476,payload,"def print_midi_message(port, message):
    if message.bytes()[0] >= 128 and message.bytes()[0] < 160:
        print(""[{}] @{:0.6f} {}"".format(port.name, message.time, message.bytes()))
read_midi_messages(device_name)",0.4907009006,
1476,payload,"def get_adcp_url(params,inst): 
    '''make request for the dataset url'''
    global USERNAME, TOKEN
    subsite = inst[0]
    node = inst[1]
    sensor = inst[2]
    method = inst[3]
    stream = inst[4]
    data_request_url ='/'.join((SENSOR_BASE_URL,subsite,node,sensor,method,stream))
    r = requests.get(data_request_url, params=params, auth=(USERNAME, TOKEN))
    rjson = r.json(); url = rjson['allURLs'][0];
    print(url)
    return url",0.4887400866,
1476,payload,"def f(b, a):
        
    q = np.empty((6,))
    q[0] = a[3]
    q[1] = a[4]
    q[2] = a[5]
    q[3] = delta * (b['bz']*a[4] - b['by']*a[5])
    q[4] = delta * (b['bx']*a[5] - b['bz']*a[3])
    q[5] = delta * (b['by']*a[3] - b['bx']*a[4]) 
    
    return q",0.4842174649,
1476,payload,"# Create child splits for a node or make terminal
def split(node, max_depth, min_size, depth):
    left, right = node['groups']
    del(node['groups'])
    # check for a no split
    if not left or not right:
        node['left'] = node['right'] = leaf(left + right)
        return
    # check for max depth
    if depth >= max_depth:
        node['left'], node['right'] = leaf(left), leaf(right)
        return
    # process left child
    if len(left) <= min_size:
        node['left'] = leaf(left)
    else:
        node['left'] = get_best_split(left)
        split(node['left'], max_depth, min_size, depth+1)
    # process right child
    if len(right) <= min_size:
        node['right'] = leaf(right)
    else:
        node['right'] = get_best_split(right)
        split(node['right'], max_depth, min_size, depth+1)",0.4840421379,
1476,payload,"#Create children splits for a node or get to the end(hit the stop criteria)
def split(node, max_depth, min_size, depth):
    left, right = node['groups']
    del(node['groups'])
    #check for the end(no split)
    if not left or not right:
        node['left'] = node['right'] = to_leaf(left+right)
        return
    #check for max depth
    if depth >= max_depth:
        node['left'], node['right'] = to_leaf(left), to_leaf(right)
        return
    #for left node
    if len(left) <= min_size:
        node['left'] = to_leaf(left)
    else:
        node['left'] = get_split(left)
        split(node['left'], max_depth, min_size, depth + 1)
    #for right node
    if len(right) <= min_size:
        node['right'] = to_leaf(right)
    else:
        node['right'] = get_split(right)
        split(node['right'], max_depth, min_size, depth + 1)",0.4840421379,
1476,payload,"def split(node, max_depth, min_size, depth):
    left, right = node['groups']
    del(node['groups'])
    ### A ###
    if not left or not right:
        node['left'] = node['right'] = to_terminal(left + right)
        return
    ### B ###
    if depth >= max_depth:
        node['left'], node['right'] = to_terminal(left), to_terminal(right)
        return
    ### C ###
    if len(left) <= min_size:
        node['left'] = to_terminal(left)
    else:
        node['left'] = get_split(left)
        split(node['left'], max_depth, min_size, depth+1)
    ### D ###
    if len(right) <= min_size:
        node['right'] = to_terminal(right)
    else:
        node['right'] = get_split(right)
        split(node['right'], max_depth, min_size, depth+1)",0.4840421379,
604,extra credit tests,"def is_leap_baby(day,month,year):
    if year % 4 == 0:
        if year % 100 == 0 and year % 400 != 0:
            return False
        if month == 2 and day == 29:
            return True
    else:
        return False",0.3476009965,
604,extra credit tests,"def shortestPath(G, source, target): 
    
    #Check whether input author exists in graph
    if G.has_node(target) == False:
        return (""Node "" + str(target) + "" is NOT in the graph."")
    
    #Check whether there is a path from input author to Aris 
    if not nx.has_path(G,source,target): 
        return ""There is no path from "" + str(source) + "" to "" + str(target)
    
    #Initialize queue
    Q = []
    heapq.heappush(Q, (0, source))
    
    #Keep track of visited nodes
    D = {aut : None for aut in G.nodes()} 
    
    while Q:
                                   #repeat while Q is not empty 
        t = heapq.heappop(Q)       #pop the current node ,t, associated with minumum distance

        if t[1] == target:
            return ""The weight of the shortest path from "" + str(source) + "" to "" + str(target) + "" is: "" + str(t[0])
        
        #If  t has already been visited, it does not make sense to update all the distances again.
        if D[t[1]] is None:
            #update D to indicate that node t has been visited
            D[t[1]] = t[0]

            # Compute tentative distance for all the unvisited neighbours of current minumum distance node
            # push it into the heap
            for neig in G[t[1]].keys():
                if D[neig] is None:
                    heapq.heappush(Q, (G[t[1]][neig][""weight""] + t[0], neig))",0.3469097018,
604,extra credit tests,"def main():
    print 'donuts'
    # Each line calls donuts, compares its result to the expected for that call.
    test(donuts(4), 'Number of donuts: 4')
    test(donuts(9), 'Number of donuts: 9')
    test(donuts(10), 'Number of donuts: many')
    test(donuts(99), 'Number of donuts: many')

    print
    print 'both_ends'
    test(both_ends('spring'), 'spng')
    test(both_ends('Hello'), 'Helo')
    test(both_ends('a'), '')
    test(both_ends('xyz'), 'xyyz')

  
    print
    print 'fix_start'
    test(fix_start('babble'), 'ba**le')
    test(fix_start('aardvark'), 'a*rdv*rk')
    test(fix_start('google'), 'goo*le')
    test(fix_start('donut'), 'donut')

    print
    print 'mix_up'
    test(mix_up('mix', 'pod'), 'pox mid')
    test(mix_up('dog', 'dinner'), 'dig donner')
    test(mix_up('gnash', 'sport'), 'spash gnort')
    test(mix_up('pezzy', 'firm'), 'fizzy perm')",0.3365793824,
604,extra credit tests,"def report_result(p,a):
    print ('is the p value ' + 
           '{0:.2f} smaller than the critical value {1:.2f}?'.format(p,a))
    if p < a:
        print (""YES!"")
    else: 
        print (""NO!"")
    
    print ('the Null hypothesis is {}'.format(\
                            'rejected' if p < a  else 'not rejected') )

    
report_result(p_3y, alpha)",0.3339545131,
604,extra credit tests,"def calc_distance(dists, beer1, beer2, weights):
    mask = (dists.beer1==beer1) & (dists.beer2==beer2)
    row = dists[mask]
    row = row[['overall_dist', 'aroma_dist', 'palate_dist', 'taste_dist']]
    dist = weights * row
    return dist.sum(axis=1).tolist()[0]

weights = [2, 1, 2, 1]
#fix from PY 2.7
#print(calc_distance(simple_distances), 'Fat Tire Amber Ale', ""Dale's Pale Ale"", weights)
#print(calc_distance(simple_distances), ""Fat Tire Amber Ale"", ""Michelob Ultra"", weights)",0.333411932,
604,extra credit tests,"def calc_distance(dists, beer1, beer2, weights):
    mask = (dists.beer1==beer1) & (dists.beer2==beer2)
    row = dists[mask]
    row = row[['overall_dist', 'aroma_dist', 'palate_dist', 'taste_dist']]
    dist = weights * row
    return dist.sum(axis=1).tolist()[0]

weights = [2, 1, 1, 1] #Give your ratings here
print (calc_distance(simple_distances, ""Fat Tire Amber Ale"", ""Dale's Pale Ale"", weights))
print (calc_distance(simple_distances, ""Fat Tire Amber Ale"", ""Michelob Ultra"", weights))",0.333411932,
604,extra credit tests,"def calc_distance(dists, beer1, beer2, weights):
    mask = (dists.beer1==beer1) & (dists.beer2==beer2)
    row = dists[mask]
    row = row[['overall_dist', 'aroma_dist', 'palate_dist', 'taste_dist']]
    dist = weights * row
    return dist.sum(axis=1).tolist()[0]

weights = [2, 1, 1, 1]
print calc_distance(simple_distances, ""Fat Tire Amber Ale"", ""Dale's Pale Ale"", weights)
print calc_distance(simple_distances, ""Fat Tire Amber Ale"", ""Michelob Ultra"", weights)",0.333411932,
604,extra credit tests,"def calc_distance(dists, beer1, beer2, weights):
    mask = (dists.beer1==beer1) & (dists.beer2==beer2)
    row = dists[mask]
    row = row[['overall_dist', 'aroma_dist', 'palate_dist', 'taste_dist']]
    dist = weights * row
    return dist.sum(axis=1).tolist()[0]

weights = [4, 1, 5, 1]
print calc_distance(simple_distances, ""90 Minute IPA"", ""India Pale Ale"", weights)
print calc_distance(simple_distances, ""90 Minute IPA"", ""Old Rasputin Russian Imperial Stout"", weights)",0.333411932,
604,extra credit tests,"#My code
def make_withdraw3(balance): 
    """"""Return a withdraw function that draws down balance with each call.""""""
    def withdraw(amount):#you are ok if you use balance as a read-only property
        nonlocal balance
        if amount > balance:
            return 'Insufficient funds'
        else:
            balance=balance-amount
            return balance
    return withdraw",0.3320915103,
604,extra credit tests,"#your code here
def make_withdraw(balance):
    """"""Return a withdraw function that draws down balance with each call.""""""
    def withdraw(amount):#you are ok if you use balance as a read-only property
        nonlocal balance
        if amount > balance:
            return 'Insufficient funds'
        balance = balance - amount
        return balance
    return withdraw",0.3320915103,
2279,task examine how many clusters you may need using the elbow method,"# the code below was taken and adapted from https://gist.github.com/calippo/20a147e657ee5e8d8666
def eblow(df, n):
    kMeansVar = [KMeans(n_clusters=k).fit(df.values) for k in range(2, n+1)]
    # define the centroids for 
    centroids = [X.cluster_centers_ for X in kMeansVar]
    # calculate the Euclidian Distance
    k_euclid = [cdist(df.values, cent) for cent in centroids]
    cIdx = [np.argmin(ke,axis=1) for ke in k_euclid]
    dist = [np.min(ke, axis=1) for ke in k_euclid]
    avgWithinSS = [sum(d)/df.values.shape[0] for d in dist]
    
    wcss = [sum(d**2) for d in dist]
    tss = sum(pdist(df.values)**2)/df.values.shape[0]
    bss = tss - wcss
    plt.subplots(figsize=(6,6))
    plt.ylabel('SS')
    plt.xlabel('Number of Clusters')
    plt.plot(np.arange(2,11),wcss)
    plt.scatter(np.arange(2,11),wcss, color='red')
    plt.show()",0.4792467356,
2279,task examine how many clusters you may need using the elbow method,"def evaluate(genome):
    genome.fitness = -math.log(abs((100*(genome.genes[0]**2 - genome.genes[1])**2 + (1 - genome.genes[0])**2)) + 1)
      
# Create interactive sliders for parameters
interact(interactive_iteration, generations=(0,10000,10),
                                mu=(0,1000, 10),
                                lamb=(0,1000, 10),
                                mutation_power=(0, 10, 0.05),
                                prob_mutate_gene=(0,1, 0.05))",0.4791781306,
2279,task examine how many clusters you may need using the elbow method,"# function for finding the best cluster number
def elbow(df, n):
    kMeansVar = [KMeans(n_clusters=k).fit(df.values) for k in range(1, n)]
    centroids = [X.cluster_centers_ for X in kMeansVar]
    k_euclid = [cdist(df.values, cent) for cent in centroids]
    dist = [np.min(ke, axis=1) for ke in k_euclid]
    wcss = [sum(d**2) for d in dist]
    tss = sum(pdist(df.values)**2)/df.values.shape[0]
    bss = tss - wcss
    plt.plot(bss)
    plt.title('best cluster amount')
    plt.grid(True)
    plt.show()",0.4788374305,
2279,task examine how many clusters you may need using the elbow method,"# get info on one cluster
#for cluster_in_question in range(0,4):
    #get center of cluster
    ""centroid"", diabetic_cluster.cluster_centers_[cluster_in_question]
    # grab diabetesMed in dataframe that belong to this cluster
    #diabetic2 = diabetes3[np.where(y_kmeans == cluster_in_question, True, False)]['admission_type_id']
    # look at top five qualities in cluster
    #print sorted(zip(diabetes3.columns[1:], diabetic_cluster.cluster_centers_[cluster_in_question]), key=lambda x:x[1], reverse=True)[1:6]
    #print
 
#from sklearn import metrics
#metrics.silhouette_score(diabetes3.drop('admission_type_id',axis=1), diabetic_cluster.labels_, metric='euclidean')",0.4767911434,
2279,task examine how many clusters you may need using the elbow method,"# Function to evaluate a genome's fitness
def evaluate(genome):
    genome.fitness = 1.0 / (1.0 + abs(math.cos(genome.genes) - math.sin(genome.genes)))

winner = evolution(1, 1, 1000)
print(""Champion -- Genes = "" + str(winner.genes) + "" Fitness = "" + str(winner.fitness))",0.4697918892,
2279,task examine how many clusters you may need using the elbow method,"# Evaluator that returns fitness of the genome
def evaluate(genome): 
    # This fitness is equal to 1 - x^2
    genome.fitness = 1.0 - genome.genes * genome.genes",0.4666067064,
2279,task examine how many clusters you may need using the elbow method,"def plot_agglomerative_clustering(X, clusters=3):
    for linkage in ('ward', 'average', 'complete'):
        agglo = AgglomerativeClustering(linkage=linkage, n_clusters=clusters)
        t0 = time()
        clusters = agglo.fit(X)
        print(""%s : %.2fs"" % (linkage, time() - t0))

        x,y = zip(*X)
        plt.figure(dpi=200)
        plt.scatter(x,y,c=plt.cm.rainbow(agglo.labels_*20),s=14)
        plt.title(""Linkage Type: %s"" % linkage)
        
        return agglo, clusters",0.4629566967,
2279,task examine how many clusters you may need using the elbow method,"def run_kmeans(df, num_clusters):
  kmeans = KMeans(n_clusters=num_clusters)
  kmeans.fit(df[['latitude', 'longitude']])
  df['kmeans_labels'] = kmeans.labels_
  df['centroids'] = df['kmeans_labels'].apply(lambda x: kmeans.cluster_centers_[x])
  return df, kmeans",0.4615595639,
2279,task examine how many clusters you may need using the elbow method,"def get_cluster_number(X):
    score = 0
    best_cluster_number = 0
    for i in range(2,10):
        # kmeans = AgglomerativeClustering(n_clusters = i).fit(X)
        kmeans = KMeans(n_clusters=i).fit(X)
        chs = calinski_harabaz_score(X,kmeans.labels_)
        print 'cluster number -->', i, chs
        if chs>score:
            best_cluster_number = i
            score = chs
    return best_cluster_number-1",0.4587257504,
2279,task examine how many clusters you may need using the elbow method,"def accom(P):
    if not P.accom:
        return 0
    
    path2 = os.getcwd() + '/AccomBoundsA' + str(P.livingArea) + '.csv'
    data2 = pd.read_csv(path2, header=0)
    
    row = 0
    if P.couple:
        if P.numKids > 0: row = 2 
        else: row = 1
    else:
        if P.numKids == 0: row = 0 
        if P.numKids == 1: row = 3 
        else: row = 4

    cutoff = deDollar(data2.iloc[row,4])
    threshold = deDollar(data2.iloc[row,3])
    maxpay = deDollar(data2.iloc[row,6])
    if P.wibt() > cutoff:
        return 0
    
    path3 = os.getcwd() + '/AccomThreshold.csv'
    data3 = pd.read_csv(path3, header=0)
 
    if P.renting: col = 1
    else: col = 2
    rentthreshold = deDollar(data3.iloc[row,col])
    sub = 0
    if P.wHousingCost > rentthreshold:
        sub = (P.wHousingCost - rentthreshold)*0.7
    if sub>maxpay: sub = maxpay
    if P.wibt()>threshold:
        sub = sub - (P.wibt()-threshold)*0.25
    if sub < 0:
        sub = 0

    return sub

# https://www.workandincome.govt.nz/map/income-support/extra-help/accommodation-supplement/income-non-beneficiaries-01.html
Person1.wHousingCost = 800
accom(Person1)",0.4572091997,
950,integrals,"def test_fib():
    assert(fib(1) == 1)
    assert(fib(2) == 1)
    assert(fib(3) == 2)
    assert(fib(6) == 8)
    assert(fib(50) == 12586269025)
    
test_fib()",0.438033402,
950,integrals,"def f3(x): # Assumes x is an array of length 5 or higher
    return 2*x[0] + 3*x[1] + 5*x[2] + 7*x[3] + 11*x[4]**2",0.437723279,
950,integrals,"def calculate_fitness(population):
    return [(i, 2 ** sum([1 for (a, b) in zip(i, TARGET) if a == b])) for i in population]",0.4363533258,
950,integrals,"def f(x):
    return   1e-1   * x[0] ** 2 \
           + 2e1    * x[1] ** 0.5 \
           + 1e-5   * x[2] ** 5.7\
           + 1e4    * x[3] ** -3",0.4323611856,
950,integrals,"%%run_pytest[clean] -qq

def test_my_func():
    assert my_func(0) == 0
    assert my_func(1) == 0
    assert my_func(2) == 2
    assert my_func(3) == 2",0.432038486,
950,integrals,"def fib_test():
    assert fib(2) == 1, 'The 2nd Fibonacci number should be 1'
    assert fib(3) == 1, 'The 3rd Fibonacci number should be 1'
    assert fib(50) == 7778742049, 'Error at the 50th Fibonacci number'",0.4307284653,
950,integrals,"# split panels into two sections based on the flow velocity
def split_panels(panels):
    # positive velocity defines `top` BL
    top = [p for p in panels if p.gamma<=0]      
    # negative defines the `bottom`
    bottom = [p for p in panels if p.gamma>=0]
    # reverse array so panel[0] is stagnation
    bottom = bottom[::-1]

    return top,bottom",0.4293892682,
950,integrals,"def function_to_evaluate(ind):  
    '''Function that we want to optimize -> to calculate fitness'''
    return -0.5*ind[0] + 2*ind[1] + 3*ind[2] + 0*ind[3] + sum(ind[4:]),",0.4286192954,
950,integrals,"# DON'T CHANGE THIS CELL! IT'S JUST HERE TO TEST YOUR CODE

def exercise_3_tester():
    """"""
    This function will test your data_type_handler function
    by putting in some values and making sure you get the
    correct answer!
    
    If you've written your code well, you should see 
    ""you passed all tests!"" as your output of this cell.
    """"""
    
    assert data_type_handler('cat') == 3, ""Failed on First Test!""
    assert data_type_handler([1,2,3,4,5]) == 5, ""Failed on Second Test!""
    assert data_type_handler(1.531) == 1, ""Failed on Third Test!""
    assert data_type_handler(1) == 'integer', ""Failed on Fourth Test!""
    assert data_type_handler(set([1,2,3,4,4])) == ""I don't know this data type"", ""Failed on Fifth Test!""
    return ""You passed all tests!""

exercise_3_tester()",0.4286175668,
950,integrals,"def addone(x):
    if isinstance(x, int):
        return x + 1
    if isinstance(x, float):
        return x + 1.0
    else:
        last_char = x[-1]
        return x[:-1] + chr(ord(last_char) + 1)",0.4285283089,
872,implement each method,"def clean_text(data):
    for i, target in zip(range(len(data.data)), data.target):
        #------------TO LOWER CASE-------------
        file = data.data[i].decode(""utf-8"").lower()
        #------------TOKENIZE-------------
        word_tokens = word_tokenize(file)
        #------------REMOVE STOP WORDS-------------
        filtered_sentence = [w for w in word_tokens if not w in stop_words]
        filtered_sentence = []
        for w in word_tokens:
            if w not in stop_words:
                    filtered_sentence.append(w)
        #------------STEMMING WITH PORTER STEMMER-------------
        ps = PorterStemmer()
        stemmedFile = []
        for word in filtered_sentence:
            for w in word.split("" ""):
                stem = ps.stem(w)
                stemmedFile.append(stem)
                #COUNT THE TERMS PER CATEGORY
                term_per_category[train_set.target_names[target]][word] += 1
        #------------PUT FILE BACK-------------
        data.data[i] = ' '.join(stemmedFile)",0.2949863672,
872,implement each method,"def animate(f):
    for idx, l in enumerate(points):
        line[idx].set_data(l[:f, 1], l[:f, 2])
    return",0.2851946056,
872,implement each method,"def make_Y_from_Z(Z):
    Zb = mtb.PMF(Z.hypos, Z.probs * Z.hypos)
    
    pmfs = [
        (mtb.PMF(range(interval + 1)), prob)
        for interval, prob in Zb.items()
    ]
    
    for pmf, weight in pmfs:
        pmf[0] *= 0
    
    return mtb.PMF.from_mixture(pmfs)
    
Y = make_Y_from_Z(Z)
Y.to_cdf().plot(label='Y')
Z.to_cdf().plot(label='Z')
Zb.to_cdf().plot(label='Zb')
plt.title('Figure 8.2: CDF of gaps between trains')
plt.xlabel('Arrival interval (minutes)')
plt.ylabel('CDF')
plt.legend()
plt.show()

print('E[Z] = %.3lf' % Z.expectation())
print('E[Zb] = %.3lf' % Zb.expectation())
print('E[Y] = %.3lf' % Y.expectation())

# Check against Downey's results from book.
tol = 1
assert abs(Z.expectation() - 7.7) < tol
assert abs(Zb.expectation() - 8.8) < tol
assert abs(Y.expectation() - 4.4) < tol",0.2801251411,
872,implement each method,"def solve_random(self):
    open_idx = list(range(self.F))
    random.shuffle(open_idx)
    open_idx = open_idx[:self.F * 4 // 5] # open 80% of facilities
    
    y = np.zeros(self.F)
    y[open_idx] = 1
    
    assignments = np.empty(problem.C)
    for cli, fac in zip(range(problem.C), cycle(open_idx)):
        assignments[cli] = fac
    
    return y, assignments

Problem.solve_random = solve_random",0.2765622139,
872,implement each method,"def split_data(city_data):
    # Get the features and labels from the Boston housing data
    X, y = city_data.data, boston.target
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30,random_state=None)
    print ""X_training:"", X_train.shape
    print ""X_test:"", X_test.shape 
    return X_train, y_train, X_test, y_test",0.2713503838,
872,implement each method,"def plot_intercept_distribution(ens):
    pylab.subplot(1,2,1)
    intercepts = ens.intercepts.sample(ens.n_neurons)
    seaborn.distplot(intercepts, bins=20)
    pylab.xlabel('intercept')

    pylab.subplot(1,2,2)
    pts = ens.eval_points.sample(n=1000, d=ens.dimensions)
    model = nengo.Network()
    model.ensembles.append(ens)
    sim = nengo.Simulator(model)
    _, activity = nengo.utils.ensemble.tuning_curves(ens, sim, inputs=pts)
    p = np.mean(activity>0, axis=0)
    seaborn.distplot(p, bins=20)
    pylab.xlabel('proportion of pts neuron is active for')",0.2710992098,
872,implement each method,"def best_first_search(problem, f):
    ""Search nodes with minimum f(node) value first.""
    global reached # <<<<<<<<<<< Only change here
    node = Node(problem.initial)
    frontier = PriorityQueue([node], key=f)
    reached = {problem.initial: node}
    while frontier:
        node = frontier.pop()
        if problem.is_goal(node.state):
            return node
        for child in expand(problem, node):
            s = child.state
            if s not in reached or child.path_cost < reached[s].path_cost:
                reached[s] = child
                frontier.add(child)
    return failure


def plot_grid_problem(grid, solution, reached=(), title='Search', show=True):
    ""Use matplotlib to plot the grid, obstacles, solution, and reached.""
    reached = list(reached)
    plt.figure(figsize=(16, 10))
    plt.axis('off'); plt.axis('equal')
    plt.scatter(*transpose(grid.obstacles), marker='s', color='darkgrey')
    plt.scatter(*transpose(reached), 1**2, marker='.', c='blue')
    plt.scatter(*transpose(path_states(solution)), marker='s', c='blue')
    plt.scatter(*transpose([grid.initial]), 9**2, marker='D', c='green')
    plt.scatter(*transpose([grid.goal]), 9**2, marker='8', c='red')
    if show: plt.show()
    print('{} {} search: {:.1f} path cost, {:,d} states reached'
          .format(' ' * 10, title, solution.path_cost, len(reached)))
    
def plots(grid, weights=(1.4, 2)): 
    """"""Plot the results of 4 heuristic search algorithms for this grid.""""""
    solution = astar_search(grid)
    plot_grid_problem(grid, solution, reached, 'A* search')
    for weight in weights:
        solution = weighted_astar_search(grid, weight=weight)
        plot_grid_problem(grid, solution, reached, '(b) Weighted ({}) A* search'.format(weight))
    solution = greedy_bfs(grid)
    plot_grid_problem(grid, solution, reached, 'Greedy best-first search')
    
def transpose(matrix): return list(zip(*matrix))",0.2692472637,
872,implement each method,"def crossValResults(classifierObj):
    results = []
    #iterate through the training and test cross validation segments and
    #run the classifier on each one, aggregating the results into a list
    for traincv, testcv in cv:
        probas = classifierObj.fit(train[traincv], target[traincv]).predict_proba(train[testcv])
        results.append(llfun( target[testcv], [x[1] for x in probas] ))
    return results",0.2686920762,
872,implement each method,"def encode(cols):
    for c in cols:
        lbl = LabelEncoder() 
        lbl.fit(list(data[c].values)) 
        data[c] = lbl.transform(list(data[c].values))
        
cols_to_encode = ['FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', 'ExterQual', 
                 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1','BsmtFinType2', 
                 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope', 'LotShape', 
                 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond','YrSold', 'MoSold']
        
encode(cols_to_encode)",0.2677088678,
872,implement each method,"# divide into train and test sets
def split_data(city_data):
    # Get the features and labels from the Boston housing data
    X, y = city_data.data, city_data.target

    X_train, X_test, y_train, y_test = cross_validation.train_test_split(\
        X, y, test_size=0.3, random_state=0)

    return X_train, y_train, X_test, y_test

X_train, y_train, X_test, y_test = split_data(boston)",0.2673026621,
2030,"step change the name of the columns to bedrs, bathrs, price_sqr_meter","# for s in sorted(list(df['TypeOfEstablishment'].unique())):
#     print s
# print

def collapse_types(df):
    df['school_type'] = df['TypeOfEstablishment']
    df.ix[df.TypeOfEstablishment.astype(str).str[:7]=='Academy', 'school_type'] ='Academy'
    df.ix[df.TypeOfEstablishment.astype(str).str[:9]=='Community', 'school_type'] ='Community'
    df.ix[df.TypeOfEstablishment.astype(str).str[:10]=='Foundation', 'school_type'] ='Foundation'
    df.ix[df.TypeOfEstablishment.astype(str).str[:4]=='Free', 'school_type'] ='Free'
    df.ix[df.TypeOfEstablishment.astype(str).str[:5]=='Other', 'school_type'] ='Independent'
    df.ix[df.TypeOfEstablishment.astype(str).str[:9]=='Voluntary', 'school_type'] ='Voluntary'
    return df
df = collapse_types(df)

## Create table
sch_type = []
edubase_tot = []
url_tot = []
scrape_tot = []
url_perc = []
blurb_tot = []
blurb_perc = []
blurb_perc_of_urls = []
med_tot = []
med_perc = []
for s in sorted(list(df['school_type'].unique())):
    sub = df[df.school_type==s]
    sch_type.append(s)
    edubase_tot.append(len(sub))
    url_tot.append(sum(sub.SchoolWebsite.notnull()))
    scrape_tot.append(sum(sub['scraped_dataset']==True))
    url_perc.append(float(sum(sub['scraped_dataset']==True))/float(len(sub)))
    blurb_tot.append(sum(sub['flag']=='Found_blurb'))
    blurb_perc.append(float(sum(sub['flag']=='Found_blurb'))/float(len(sub)))
    blurb_perc_of_urls.append(float(sum(sub['flag']=='Found_blurb'))/float(sum(sub['scraped_dataset']==True)))
    sub_blurb = sub[sub['flag']=='Found_blurb']
    med_tot.append(len(sub_blurb[(sub_blurb['length']>=250) & (sub_blurb['length']<=4000)]))
    med_perc.append(float(len(sub_blurb[(sub_blurb['length']>=250) & (sub_blurb['length']<=4000)]))/float(len(sub)))
df_sch_type = pd.DataFrame({'sch_type': sch_type})
df_sch_type['edubase_tot'] = edubase_tot
df_sch_type['url_tot'] = url_tot
df_sch_type['scrape_tot'] = scrape_tot
df_sch_type['url_perc'] = url_perc
df_sch_type['blurb_tot'] = blurb_tot
df_sch_type['blurb_perc'] = blurb_perc
df_sch_type['blurb_perc_of_urls'] = blurb_perc_of_urls
df_sch_type['med_tot'] = med_tot
df_sch_type['med_perc'] = med_perc
df_sch_type",0.4920155406,
2030,"step change the name of the columns to bedrs, bathrs, price_sqr_meter","def changeValues(df,column_name,Value):    
    df.rename(columns = {column_name:'Temp_Name'}, inplace = True)
    df.ix[df.Temp_Name == 0, 'Temp_Name'] = Value
    df.rename(columns = {'Temp_Name':column_name}, inplace = True)",0.4766107202,
2030,"step change the name of the columns to bedrs, bathrs, price_sqr_meter","def default_missing(results):
    red_states = [""Alabama"", ""Alaska"", ""Arkansas"", ""Idaho"", ""Wyoming""]
    blue_states = [""Delaware"", ""District of Columbia"", ""Hawaii""]
    results.ix[red_states, [""poll_mean""]] = -100.0
    results.ix[red_states, [""poll_std""]] = 0.1
    results.ix[blue_states, [""poll_mean""]] = 100.0
    results.ix[blue_states, [""poll_std""]] = 0.1
default_missing(avg)
avg.head()",0.4736073315,
2030,"step change the name of the columns to bedrs, bathrs, price_sqr_meter","def default_missing(results):
    red_states = [""Alabama"", ""Alaska"", ""Arkansas"", ""Idaho"", ""Wyoming"",""Louisiana""]
    blue_states = [""Delaware"", ""District of Columbia"", ""Hawaii""]
    results.ix[red_states, [""poll_mean""]] = -100.0
    results.ix[red_states, [""poll_std""]] = 0.1
    results.ix[blue_states, [""poll_mean""]] = 100.0
    results.ix[blue_states, [""poll_std""]] = 0.1
default_missing(avg)",0.4710453153,
2030,"step change the name of the columns to bedrs, bathrs, price_sqr_meter","def default_missing(results):
    red_states = [""Alabama"", ""Alaska"", ""Arkansas"", ""Idaho"", ""Wyoming""]
    blue_states = [""Delaware"", ""District of Columbia"", ""Hawaii""]
    results.loc[red_states, [""poll_mean""]] = -100.0
    results.loc[red_states, [""poll_std""]] = 0.1
    results.loc[blue_states, [""poll_mean""]] = 100.0
    results.loc[blue_states, [""poll_std""]] = 0.1
default_missing(avg)
avg.head()",0.4705649614,
2030,"step change the name of the columns to bedrs, bathrs, price_sqr_meter","# This function adjusts the input stats for pace and outputs per 100 possession metrics
def paceConversion(df, listOfFields):
    for field in listOfFields:
        df['{}_per_100_poss'.format(field)] = (100/df['baseStats_Pace'])*(48/(df['perGameStats_MP']/5))*df[field]
        
    return df

# Select a subset of columns to manage size of dataframe
teamAggDfToAnalyzeSelectedColumns = teamAggDfToAnalyze[[
    'season_start_year',
    'perGameStats_Tm',
    'baseStats_W',
    'baseStats_WLPerc',
    'perGameStats_MP',
    'baseStats_Pace',
    'baseStats_Rel_Pace',
    'baseStats_ORtg',
    'baseStats_Rel_ORtg',
    'baseStats_DRtg',
    'baseStats_Rel_DRtg',
    'perGameStats_PTS',
    'perGameStats_2PA',
    'perGameStats_2PPerc',
    'perGameStats_3PA',
    'perGameStats_3PPerc',
    'perGameStats_FTA',
    'perGameStats_FTPerc',
    'perGameStats_ORB',
    'perGameStats_DRB',
    'perGameStats_AST',
    'perGameStats_STL',
    'perGameStats_BLK',
    'perGameStats_TOV'
]]

# Pace adjust the following metrics
teamAggDfToAnalyzePaceAdjusted = paceConversion(
    teamAggDfToAnalyzeSelectedColumns,
    [
        'perGameStats_PTS',
        'perGameStats_2PA',
        'perGameStats_3PA',
        'perGameStats_FTA',
        'perGameStats_ORB',
        'perGameStats_DRB',
        'perGameStats_AST',
        'perGameStats_STL',
        'perGameStats_BLK',
        'perGameStats_TOV'
    ]
)",0.4680578113,
2030,"step change the name of the columns to bedrs, bathrs, price_sqr_meter","# This function adjusts the input stats for pace and outputs per 100 possession metrics
def paceConversion(df, listOfFields):
    for field in listOfFields:
        df['{}_per_100_poss'.format(field)] = (100/df['baseStats_Pace'])*(48/(df['perGameStats_MP']/5))*df[field]
        
    return df

# Select a subset of columns to manage size of dataframe
teamAggDfToAnalyzeSelectedColumns = teamAggDfToAnalyze[[
    'season_start_year',
    'perGameStats_Tm',
    'baseStats_W',
    'baseStats_WLPerc',
    'perGameStats_MP',
    'baseStats_Pace',
    'baseStats_Rel_Pace',
    'baseStats_ORtg',
    'baseStats_Rel_ORtg',
    'baseStats_DRtg',
    'baseStats_Rel_DRtg',
    'perGameStats_PTS',
    'perGameStats_2PA',
    'perGameStats_2PPerc',
    'perGameStats_3PA',
    'perGameStats_3PPerc',
    'perGameStats_FGA',
    'perGameStats_FGPerc',
    'perGameStats_FTA',
    'perGameStats_FTPerc',
    'perGameStats_ORB',
    'perGameStats_DRB',
    'perGameStats_AST',
    'perGameStats_STL',
    'perGameStats_BLK',
    'perGameStats_TOV',
    'opponentPerGameStats_PTS',
    'opponentPerGameStats_2PA',
    'opponentPerGameStats_2PPerc',
    'opponentPerGameStats_3PA',
    'opponentPerGameStats_3PPerc',
    'opponentPerGameStats_FGA',
    'opponentPerGameStats_FGPerc',
    'opponentPerGameStats_FTA',
    'opponentPerGameStats_FTPerc',
    'opponentPerGameStats_ORB',
    'opponentPerGameStats_DRB',
    'opponentPerGameStats_AST',
    'opponentPerGameStats_STL',
    'opponentPerGameStats_BLK',
    'opponentPerGameStats_TOV'
]]

# Pace adjust the following metrics
teamAggDfToAnalyzePaceAdjusted = paceConversion(
    teamAggDfToAnalyzeSelectedColumns,
    [
        'opponentPerGameStats_PTS',
        'opponentPerGameStats_2PA',
        'opponentPerGameStats_3PA',
        'opponentPerGameStats_FGA',
        'opponentPerGameStats_FTA',
        'opponentPerGameStats_ORB',
        'perGameStats_DRB',
        'perGameStats_STL',
        'perGameStats_BLK',
        'opponentPerGameStats_TOV',
        'opponentPerGameStats_STL',
        'opponentPerGameStats_BLK'
    ]
)",0.4680578113,
2030,"step change the name of the columns to bedrs, bathrs, price_sqr_meter","def clean_data(df):
    # drop predicators that we are not going to use
    df.drop(['Ticket','Cabin'],inplace=True,axis=1)
    
    # encode the sex class to 0, 1
    df['Sex'] = df.Sex.map( {'female': 0, 'male': 1} ).astype(int)

    # simply fill the missing Embarked value with ""S"" since there are only 2 NAs
    df['Embarked'] = df.Embarked.fillna('S')
    
    # there is a 0 value in Fare column, which is unreasonable. Will have to replace it with NA first.
    df.Fare = df.Fare.map(lambda x: np.nan if x==0 else x)
    # build a pivot table to impute NAs with averaged Fare for all Pclass
    fare_pivot_table = df.pivot_table(""Fare"", index='Pclass', aggfunc='mean', dropna=True)
    # use pivot table to impute missing fare values
    df['Fare'] = df[['Fare', 'Pclass']].apply(lambda x: fare_pivot_table[x['Pclass']] if pd.isnull(x['Fare']) else x['Fare'], axis = 1)
    
    #build a pivot table to impute NAs with median Age according to Pclass and Sex
    age_pivot_table = df.pivot_table('Age', index=['Pclass','Sex'], aggfunc='median',dropna=True)
    df['Age'] = df[['Sex', 'Pclass', 'Age']].apply(lambda x: age_pivot_table[x.Pclass, x.Sex] if pd.isnull(x.Age) else x.Age, axis = 1)
    
    # define a age group categorical variable
    df['AgeGroup'] = 'adult'
    df.loc[ (df['Age']<=10) ,'AgeGroup'] = 'child'
    df.loc[ (df['Age']>=60) ,'AgeGroup'] = 'senior'
    
    # define a fare group categorical variable
    df['FareGroup'] = 'low'
    df.loc[ (df['Fare']<=20) & (df['Fare']>10) ,'FareGroup'] = 'mid'
    df.loc[ (df['Fare']<=30) & (df['Fare']>20) ,'FareGroup'] = 'mid-high'
    df.loc[ (df['Fare']>30) ,'FareGroup'] = 'high'
    
    # Here we are adding interaction terms between predicators. I have included a few reasonable interaction terms in my mind 
    df['Family_Size']=train['SibSp']+train['Parch']+1
    
    # Generating the Title predicator
    df['Title'] = df['Name'].apply(lambda x: getTitle(x))
    df['Title'] = df.apply(getAndReplaceTitle, axis=1)

    return df

def getTitle(name):
    # use regex to match the titles in passenger names
    # titles are the characters preceded by a comma and a space ("", "") and succeeded by a period (""."") in a passenger's name.
    # Here we use the positive lookbehind assertion to find the preceding ("", "") and the positive lookahead assertion to match the succeeding (""."").
    m = re.search('(?<=,\s)[a-zA-z\s]+(?=\.)', name)
    if m is None: 
        return np.nan
    else:
        return m.group(0)
    
def getAndReplaceTitle(df):
    # combine similar titles
    # the majority of the titles are pretty rare, which doesn't give much information about the passenger.
    title = df['Title']
    if title in ['Mr','Don', 'Major', 'Capt', 'Jonkheer', 'Rev', 'Col','Sir']:
        return 'Mr'
    elif title in ['the Countess', 'Mme','Mrs','Lady','Dona']:
        return 'Mrs'
    elif title in ['Mlle', 'Ms','Miss']:
        return 'Miss'
    elif title =='Dr':
            if df['Sex']=='Male':
                return 'Mr'
            else:
                return 'Mrs'
    else:
        return title",0.4672264457,
2030,"step change the name of the columns to bedrs, bathrs, price_sqr_meter","def add_outside_moscow_price(df):
    outside_moscow_avg_price = pd.read_csv('outside_avg_price.csv')

    outside_moscow_avg_price = outside_moscow_avg_price.rename(columns={'timestamp':'t_timestamp'})
    df['date'] = df['timestamp'].apply(lambda x: x.strftime('%Y-%m-%d'))
    df = df.merge(outside_moscow_avg_price, left_on='date', right_on='t_timestamp').drop(['date', 't_timestamp'], axis=1)
    
    return df

train_avg_price = add_outside_moscow_price(train_avg_price)
idx = pd.isnull(train_avg_price['sub_area_avg_price_eur'])
train_avg_price.loc[idx, 'sub_area_avg_price_eur'] = train_avg_price.loc[idx, 'outside_moscow_eur']
train_avg_price.loc[idx, 'sub_area_avg_price_rub'] = train_avg_price.loc[idx, 'outside_moscow_rub']
train_avg_price.loc[idx, 'sub_area_avg_price_usd'] = train_avg_price.loc[idx, 'outside_moscow_usd']
train_avg_price = train_avg_price.drop(['outside_moscow_eur', 'outside_moscow_rub', 'outside_moscow_usd'], axis=1)

test_avg_price = add_outside_moscow_price(test_avg_price)
idx = pd.isnull(train_avg_price['sub_area_avg_price_eur'])
test_avg_price.loc[idx, 'sub_area_avg_price_eur'] = test_avg_price.loc[idx, 'outside_moscow_eur']
test_avg_price.loc[idx, 'sub_area_avg_price_rub'] = test_avg_price.loc[idx, 'outside_moscow_rub']
test_avg_price.loc[idx, 'sub_area_avg_price_usd'] = test_avg_price.loc[idx, 'outside_moscow_usd']
test_avg_price = test_avg_price.drop(['outside_moscow_eur', 'outside_moscow_rub', 'outside_moscow_usd'], axis=1)",0.4648717642,
2030,"step change the name of the columns to bedrs, bathrs, price_sqr_meter","# this function takes a row of all_sets and performs the conversion on the DataFrame in cards.
# the entire row is needed because the convert_printings function needs the set code to create the dictionary.
def convert_row(row):
    row[""cards""][""printings""] = row[""cards""].apply(lambda x: {row[""code""] : x[""rarity""]}, axis = 1)
    row[""cards""].set_index(""name"", inplace = True) # the name is a much more natural index than the default index, which was just the first column by alpha order
                                                              
    return row

only_cards = all_sets.apply(convert_row, axis = 1)[""cards""]",0.4643508792,
1113,loading a csv into a dataframe,"import pandas as pd
import csv

df = pd.read_csv('../data/150604_CNT_furnace_ramp_time.txt', skiprows = 1, delimiter = ' ')

labels = next(csv.reader(open('../data/150604_CNT_furnace_ramp_time.txt')))
labels = dict(zip(list(df.columns), labels))

df.rename(columns = labels)",0.5819145441,
1113,loading a csv into a dataframe,"import csv
import pandas as pd

# Read in GloVe word embeddings
embeddings_50d = ""../../word_embeddings/glove.6B/glove.6B.100d.text""
embeddings_100d = ""../../word_embeddings/glove.6B/glove.6B.100d.text""
embeddings_200d = ""../../word_embeddings/glove.6B/glove.6B.200d.text""
embeddings_300d = ""../../word_embeddings/glove.6B/glove.6B.300d.txt""

embeddings = pd.read_table(embeddings_300d, delim_whitespace=True, index_col=0, header=None, quoting=csv.QUOTE_NONE)
embedding_dim = 300",0.5682839155,
1113,loading a csv into a dataframe,"w1=np.loadtxt('well_1.txt',skiprows=1)
w1z=w1[:,0]
w1vp=w1[:,2]

w2=np.recfromcsv('qsiwell2.csv',names=True)
w2z=w2['depth']
w2vp=w2['vp']/1e3

w3=np.recfromcsv('qsiwell3.csv',names=True)
w3z=w3['depth']
w3vp=w3['vp']/1e3

w5=np.recfromcsv('qsiwell5.csv',names=True)
w5z=w5['depth']
w5vp=304.800/w5['dt']

well1_il, well1_xl = 1448, 945
well2_il, well2_xl = 1376, 1776
well3_il, well3_xl =1468, 1847

tops_w1={'Heimdal': 2170}
tops_w2={'Heimdal': 2153,'OWC': 2183}
tops_w3={'Heimdal': 2180}",0.5681242347,
1113,loading a csv into a dataframe,"paths_length=[]
unexist_paths={}
path_df=pd.read_csv(pathfinished_file_path, sep=""\t"",skiprows=15,names=['hashedIpAddress','timestamp','durationInSec','path','rating'],usecols=['path'],encoding='utf-8')
for index, row in path_df.iterrows():
    steps=row.values[0].split(';')
    hum_source=urllib.unquote_plus(str(steps[0]))
    hum_target=urllib.unquote_plus(str(steps[-1]))
    try:
        spath=nx.shortest_path_length(G,source=hum_source,target=hum_target)
        paths_length.append(spath)
    except:
        print (""path does not exist between"",hum_source,hum_target,""at row index"",index)
        unexist_paths[index]=spath

paths_count=Counter(paths_length)
print ('The shortest path histogram counts',paths_count)

#draw plot
# Counter data, counter is your counter object
keys = paths_count.keys()
y_pos = np.arange(len(keys))
# get the counts for each key, assuming the values are numerical
performance = [paths_count[k] for k in keys]

# init plot labels
fig = plt.figure(num=None, figsize=(9, 6), dpi=160, facecolor='w', edgecolor='k')
fig.suptitle('', fontsize=28)
    
plt.subplot(2, 1, 1)
line = plt.plot(keys, performance, label='shortest path')
plt.legend(handles=line)
plt.title('Shortest path histogram LINE')
plt.ylabel('number of paths')
plt.xlabel('path length')

plt.subplot(2, 1, 2)
plt.barh(y_pos, performance, align='center', alpha=0.6)
plt.yticks(y_pos, keys)
plt.xlabel('number of paths')
plt.ylabel('path length')
plt.title('Shortest path histogram BARS')

plt.tight_layout()
plt.show()",0.5667461157,
1113,loading a csv into a dataframe,"ndbcURL = 'http://coastwatch.pfeg.noaa.gov/erddap/tabledap/cwwcNDBCMet.csv?station,longitude,latitude&longitude>=-124&longitude<=-121&latitude>=37&latitude<=47'
junk = pd.read_csv(ndbcURL, skiprows=[1])",0.5659676194,
1113,loading a csv into a dataframe,"# import the kinect data
kinect_moves = pd.read_csv('kinect_moves67.csv',sep=',',dtype=float64,skiprows=1).values

kinect_binary = zeros(len(accm))
lab = zeros(len(accm)) # label each movement as short, medium, or long

for i in np.arange(np.size(kinect_moves,axis=0)):
    tstart = fa*(hr*kinect_moves[i,0]+m*kinect_moves[i,1]+kinect_moves[i,2])
    tend = tstart+fa*kinect_moves[i,3]
    kinect_binary[tstart:tend] = 1
    if kinect_moves[i,3] < 6:
        lab[tstart:tend] = 1
    elif kinect_moves[i,3] > 10:
        lab[tstart:tend] = 3
    else:
        lab[tstart:tend] = 2",0.5658438206,
1113,loading a csv into a dataframe,"# Concatenating loop
df = pd.concat([pd.read_csv(file, encoding='Latin5', sep='\t', header=None, skiprows=1,error_bad_lines=False,warn_bad_lines=True) for file in files])",0.5650254488,
1113,loading a csv into a dataframe,"import pandas as pd

# Load all data
data_summary = pd.read_csv(cbe_intermediate_filename)
data_space = pd.read_csv(test_cogrecon_filename, skiprows=1)
data_nav = pd.read_csv(nav_intermediate_filename)
data_nav_context = pd.read_csv(nav_context_intermediate_filename)
data_missassignment_by_context = pd.read_csv(misassignment_intermediate_filename)

# Sort
[df.sort_values(['subID', 'trial'], inplace=True) for df in [data_summary, data_space, data_nav, data_nav_context, data_missassignment_by_context]]

# Confirm subID and trial match across all data
assert all([a==b==c==d==e for a,b,c,d,e in zip(data_summary['subID'].values, 
                                               data_space['subID'].values, 
                                               data_nav['subID'].values, 
                                               np.transpose([name for name, group in data_nav_context.groupby(['subID', 'trial'])['subID']])[0], # Note: The reason this funny business is needed is because this file is broken down further by context - so we have to group out that column
                                               data_missassignment_by_context['subID'].values)]), 'subIDs do not match in intermediate files'
assert all([a==b==c==d==e for a,b,c,d,e in zip(data_summary['trial'].values, 
                                               data_space['trial'].values, 
                                               data_nav['trial'].values, 
                                               np.transpose([name for name, group in data_nav_context.groupby(['subID', 'trial'])['trial']])[1], # Note: The reason this funny business is needed is because this file is broken down further by context - so we have to group out that column
                                               data_missassignment_by_context['trial'].values)]), 'trials do not match in intermediate files'

data = pd.DataFrame()

# Random Factors
data['subID'] = data_space['subID']
data['trial'] = data_space['trial']

# Study Time Factors (independent variables)

# AS) Simple Path Factors
data['space_travelled'] = data_nav['total_distance']
data['time_travelled'] = data_nav['total_time']

# BS) Complex Path Factors
data['fd_space'] = data_nav['fd_space']
data['lacunarity_space'] = data_nav['lacunarity_space']

# Test Time Factors (dependent variables)

# AT) Simple Factors
data['space_misplacement'] = data_space['Original Misplacement']

# BT) Context Factors
data['across_context_boundary_effect'] = data_summary['context_crossing_dist_exclude_wrong_color_pairs']
data['within_context_boundary_effect'] = data_summary['context_noncrossing_dist_exclude_wrong_color_pairs']
data['context_boundary_effect'] = data_summary['context_crossing_dist_exclude_wrong_color_pairs'] - data_summary['context_noncrossing_dist_exclude_wrong_color_pairs']

# CT) Relational Memory Factors
data['accurate_misassignment_space'] = data_space['Accurate Misassignment']

# DT) Relational Memory and Context Factors
data['within_misassignments'] = data_missassignment_by_context['within_misassignments']
data['across_misassignments'] = data_missassignment_by_context['across_misassignments']

if 'generate_intermediate_files' in vars() and generate_intermediate_files:
    data.to_csv(full_dataset_filename)",0.5649517179,
1113,loading a csv into a dataframe,"import itertools
import ast
import pandas as pd
import numpy as np
from cogrecon.core.batch_pipeline import get_header_labels

# Load the data
data = pandas.read_csv(test_cogrecon_filename, skiprows=1)
misassignment_pairs = [ast.literal_eval(row) for row in data['Accurate Misassignment Pairs']]

# The pairs which share a context (note that order doesn't matter for this)
# These are tuples, but in this analysis, we don't use tuples because then chance level is more complicated (plus they aren't related to the question at hand)
# within_key = [[1, 2], [2, 1], [5, 6], [6, 5], [13, 15], [15, 13], [11, 7], [7, 11]]
# across_key = [[7, 8], [8, 7], [15, 14], [14, 15], [6, 0], [0, 6], [2, 4], [4, 2]]

context_item_indicies = [[4, 7, 11, 12], [8, 9, 13, 15], [0, 1, 2, 10], [3, 5, 6, 14]]
within_key = []
for context in context_item_indicies:
    keys = list(itertools.product(context, context))
    for key in keys:
        if key[0] != key[1]:
            within_key.append(list(key))

all_keys = []
for key in list(itertools.product(list(range(0, 16)), list(range(0, 16)))):
    if key[0] != key[1]:
        all_keys.append(list(key))

across_key = [ast.literal_eval(el) for el in list(set([str(x) for x in all_keys]) - set([str(x) for x in within_key]))]

# The items to exclude because they had no contextual information
# thus if they were given temporal information, they would not be a valid misassignment
exclusion_items = []

within_list = []
across_list = []
totals_list = []
for i, a in enumerate(misassignment_pairs):
    totals_list.append(len(a))
    within_list.append(0)
    across_list.append(0)
    for el in a:
        if all([el_i not in exclusion_items for el_i in el]):
            if el in within_key:
                within_list[-1] += 1
            elif el in across_key:
                across_list[-1] += 1
within_list_proportion = [float(x)/float(y) if y is not 0 else np.nan for x, y in zip(within_list, totals_list)]
across_list_proportion = [float(x)/float(y) if y is not 0 else np.nan for x, y in zip(across_list, totals_list)]",0.5637788773,
1113,loading a csv into a dataframe,"import pandas as pd

def load_csv(path):
    ''' '''
    return pd.read_csv(path, low_memory=False)",0.5635222793,
1085,list sort,"def longest_text():
    text = max((t for t in all_texts), key=lambda x: len(x['alltext']))
    print(text['title'])
    print(text['alltext'])",0.4514731169,
1085,list sort,"store = []
def sort_by_last_letter(strings):
    def last_letter(s):
        return s[-1]
    store.append(last_letter)
    print(last_letter)
    return sorted(strings,key=last_letter)",0.4368990958,
1085,list sort,"def OrderByScore(func):
    def wrappee(q, results):
        return [r for s, r in sorted([
                    (func(q, r), r) for r in results
                ], reverse=True)]
    return wrappee

# Try a real metric
@OrderByScore
def CharOverlap(q, r):
    return len(set(q).intersection(set(r)))

print EvaluateAllByRank(CharOverlap)",0.434378624,
1085,list sort,"def in_random_order(data):
    """"""generator that returns the elements of data in a random order""""""
    indexes = [i for i, _ in enumerate(data)]  # create a list of indexes
    random.shuffle(indexes)                    # shuffle them
    for i in indexes:
        yield data[i]",0.4318306446,
1085,list sort,"def in_random_order(data):
    """""" generator that returns the elements of data in a rnadom order """"""
    indexes = [i for i, _ in enumerate(data)]  # create a list of indices, 
    random.shuffle(indexes)  # shuffle them,
    for i in indexes:
        yield data[i]  # and return the data in that order",0.4318306446,
1085,list sort,"# during each cycle, iterate through the data in a random order
def in_random_order(data):
    """"""generator that returns the elements of data in random order""""""
    indexes = [i for i, _ in enumerate(data)] # create a list of indexes
    random.shuffle(indexes)                   # shuffle them
    for i in indexes:
        yield data[i]",0.4318306446,
1085,list sort,"def in_random_order(data):
    """"""generator that returns the elements of data in random order""""""
    indexes = [i for i, _ in enumerate(data)] # create a list of indexes
    random.shuffle(indexes) # shuffle them
    for i in indexes: # return the data in that order
        yield data[i]",0.4318306446,
1085,list sort,"def capitalize(string):
    string_list=list(string)
    spazi = [i for i, x in enumerate(string_list) if x == "" ""]
    for i in spazi:
        string_list[0]=string_list[0].upper()
        string_list[i+1] = string_list[i+1].upper()
    string=''.join(string_list)
    
    return (string)",0.4316345155,
1085,list sort,"if __name__ == ""__main__"":
    # sort by spam prob (ascending)
    classified.sort(key = lambda row: row[2])
    
    # highest predicted spam probabilities among the non-spams
    spammiest_hams = list(filter(lambda row: not row[1], classified))[-5:]
    
    # the lowest predict spam probabilities among the actual spams
    hammiest_spams = list(filter(lambda row: row[1], classified))[:5]
    
    print(""Spammiest Hams: {0}\n\n"".format(spammiest_hams))
    print(""Hammiest Spams: {0}\n\n"".format(hammiest_spams))",0.4273185134,
1085,list sort,"def count_CoA(auths_nums):
    """"""
    Get the different authors from the series of list of authors.
    Nb: we take non connected authors into account.
    
    Params:
        auths_nums (pandas.core.series.Series) : the series of list of authors.    
    Returns:
        Counter : author_id : nbr of authored articles
    """"""
    concat = []
    for auth_list in auths_nums:
        concat +=  auth_list
    concat.sort()
    return Counter(concat)",0.4269706309,
1149,"make a single dataframe with the encoded columns, plus age , fare and survived from the original dataframe","def mergeByYear(year):
    data = pd.DataFrame(income.ix[year].values, columns = ['Income'])
    data['Country'] = income.columns
    joined = pd.merge(data, countries, how=""inner"", on=['Country'])
    joined.Income = np.round(joined.Income, 2)
    return joined",0.4680178761,
1149,"make a single dataframe with the encoded columns, plus age , fare and survived from the original dataframe","""""""
Function
--------
mergeByYear

Return a merged DataFrame containing the income, 
country name and region for a given year. 

Parameters
----------
year : int
    The year of interest

Returns
-------
a DataFrame
   A pandas DataFrame with three columns titled 
   'Country', 'Region', and 'Income'. 

Example
-------
>>> mergeByYear(2010)
""""""
#your code here

def mergeByYear(year):
    data = pd.DataFrame(income.ix[year].values, columns = ['Income'])
    data['Country'] = income.columns
    joined = pd.merge(data, countries, how=""inner"", on=['Country'])
    joined.Income = np.round(joined.Income, 2)
    return joined

mergeByYear(2010).head()",0.4602323771,
1149,"make a single dataframe with the encoded columns, plus age , fare and survived from the original dataframe","if applyDoubleLineFix:
    transcriptionLineDetailsFirstPass = transcriptionLineDetailsFrame.set_index(
        'bestLineIndex', drop=False, append=True)
    transcriptionLineDetailsFirstPass[
        'oldBestLineIndex'] = transcriptionLineDetailsFirstPass[
            'bestLineIndex'].astype(np.int64)
    transcriptionLineDetailsFirstPass = transcriptionLineDetailsFirstPass.groupby(
        level=[0, 5]).aggregate({
            'y1': np.mean,
            'y2': np.mean,
            'bestLineIndex': 'first',
            'oldBestLineIndex': 'first'
        })
    transcriptionLineDetailsFirstPass['meanY'] = 0.5 * (
        transcriptionLineDetailsFirstPass['y1'] +
        transcriptionLineDetailsFirstPass['y2'])
    transcriptionLineDetailsFirstPass.set_index(
        'meanY', drop=False, append=True)

    oldColumnNames = transcriptionLineDetailsFirstPass.columns
    newColumnNames = [(name if name != 'bestLineIndex' else 'newBestLineIndex')
                      for name in oldColumnNames]
    transcriptionLineDetailsFirstPass.columns = newColumnNames

    thisRow = transcriptionLineDetailsFirstPass.iloc[0]
    #display(thisRow)

    meanYThreshold = 20
    bestLineIndexDecrement = 0

    for index, nextRow in transcriptionLineDetailsFirstPass.iterrows():
        if nextRow['newBestLineIndex'] == 0:  # new subject
            bestLineIndexDecrement = 0  # so reset the decrement
        elif np.abs(thisRow['meanY'] - nextRow['meanY']) < meanYThreshold:
            bestLineIndexDecrement += 1

        transcriptionLineDetailsFirstPass.loc[
            index, 'newBestLineIndex'] -= int(bestLineIndexDecrement)

        thisRow = nextRow

    transcriptionLineDetailsFirstPass.reset_index(
        level='bestLineIndex', drop=True, inplace=True)
    transcriptionLineDetailsFirstPass.columns = oldColumnNames",0.4580754936,
1149,"make a single dataframe with the encoded columns, plus age , fare and survived from the original dataframe","def make_uid(x):
    df = x.reset_index().copy()
    return df['companyAlias'].map(lambda x: str(alias_comp[x])).values+'_'+df['employee'].map(str).values
users_index['uid'] = make_uid(users_index)
inters_clean['uid'] = make_uid(inters_clean)
inters_clean['comid'] = inters_clean['uid'].map(lambda x: x.split('_')[0]).values
votes_clean['uid'] = make_uid(votes_clean)
votes_clean['comid'] = votes_clean['uid'].map(lambda x: x.split('_')[0]).values
coms_clean['uid'] = make_uid(coms_clean)
coms_clean['comid'] = coms_clean['uid'].map(lambda x: x.split('_')[0]).values
lastp_clean['uid'] = make_uid(lastp_clean)
lastp_clean['comid'] = lastp_clean['uid'].map(lambda x: x.split('_')[0]).values
#in case you want to do inverse hashing
inv_comp = dict([(val,key) for key,val in alias_comp.items()])
def inv_company(x):
    c = x.split('_')[0]
    return inv_comp[int(c)]",0.4578516185,
1149,"make a single dataframe with the encoded columns, plus age , fare and survived from the original dataframe","if use_title:
    df_features_train = train.join(pd.DataFrame(features_train.todense()).reset_index(drop=True), how='inner').drop([""ID"", ""title"", ""text"", ""text_and_title"", ""X1"", ""X2"", ""label""], axis=1)
    df_features_test = test.join(pd.DataFrame(features_test.todense()).reset_index(drop=True), how='inner').drop([""ID"", ""title"", ""text"", ""text_and_title"", ""label"", ""X1"", ""X2""], axis=1)
    df_final_features_train = data_training.join(pd.DataFrame(final_features_train.todense()).reset_index(drop=True), how='inner').drop([""ID"", ""title"", ""text"", ""text_and_title"", ""X1"", ""X2""], axis=1)
    df_final_features_test = data_testing.join(pd.DataFrame(final_features_test.todense()).reset_index(drop=True), how='inner').drop([""ID"", ""title"", ""text"", ""text_and_title""], axis=1)
else: 
    df_features_train = train.join(pd.DataFrame(features_train.todense()).reset_index(drop=True), how='inner').drop([""ID"", ""title"", ""text"", ""X1"", ""X2"", ""label""], axis=1)
    df_features_test = test.join(pd.DataFrame(features_test.todense()).reset_index(drop=True), how='inner').drop([""ID"", ""title"", ""text"", ""label"", ""X1"", ""X2""], axis=1)
    df_final_features_train = data_training.join(pd.DataFrame(final_features_train.todense()).reset_index(drop=True), how='inner').drop([""ID"", ""title"", ""text"", ""X1"", ""X2""], axis=1)
    df_final_features_test = data_testing.join(pd.DataFrame(final_features_test.todense()).reset_index(drop=True), how='inner').drop([""ID"", ""title"", ""text""], axis=1)",0.4552676976,
1149,"make a single dataframe with the encoded columns, plus age , fare and survived from the original dataframe","# read and clean data
def clean_data(csv_filename):
    return (
        pd.read_csv(csv_filename)
        .assign(
            age=lambda x: (
                x.groupby(['Pclass', 'Sex'])['Age']
                .transform(lambda x: x.fillna(x.mean()).astype(int))
        ))    
        .assign(sex=lambda x: x['Sex'].map({'female': 1, 'male': 0}))
        .assign(age_band=lambda x: pd.qcut(x.age, 5, labels=False))
        .assign(family_size=lambda x: x.SibSp + x.Parch + 1)
        .assign(is_alone=lambda x: np.where(x.family_size == 1, 1, 0))
        .assign(
            embarked=lambda x: x.Embarked.fillna(x.Embarked.mode()[0]
        ))
        .assign(
            embarked_int=lambda x: x.embarked.map(
                {p:i for i,p in enumerate(np.sort(x.embarked.unique()))}
        ))
        .assign(fare=lambda x: x.Fare.fillna(x.Fare.median()))
        .assign(fare_band=lambda x: pd.qcut(x.fare, 4, labels=False))
        .rename(columns={
            'Pclass': 'passenger_class',
            'Survived': 'survived',
        })
        .drop([
            'Age','Cabin', 'Ticket', 
            'Name', 'Sex', 'SibSp', 
            'Parch', 'family_size',
            'Embarked', 'Fare', 'fare', 
            'age', 'embarked',], 
            axis=1
        )
    )

train_df = clean_data('train.csv')
test_df = clean_data('test.csv')

train_df.head()",0.4544708729,
1149,"make a single dataframe with the encoded columns, plus age , fare and survived from the original dataframe","def cleanDF(df):
    #rename columns
    df = df.rename(columns={u'Unnamed: 44': 'Median_sdt', u'Unnamed: 46': 'Mean_sdt', u'Unnamed: 48':'Gini_sdt'})
    
    #    remove first row, and I guess we can remove combinations
    df = df.ix[1:38].set_index(u'Characteristic')

    # convert everythin to nomeric and return
    return df.convert_objects(convert_numeric=True)",0.4524526,
1149,"make a single dataframe with the encoded columns, plus age , fare and survived from the original dataframe","if reRun:
    id_features.reset_index(inplace=True, drop=True)
    id_df = pd.DataFrame(id_array, columns=['Id'])
    id_df = pd.concat([id_df, id_features], axis=1)
    id_json = id_df.to_json(orient='records')",0.4523693919,
1149,"make a single dataframe with the encoded columns, plus age , fare and survived from the original dataframe","def get_target_and_features(df: pd.DataFrame) -> Tuple[pd.DataFrame]:
    merged_df = df.merge(encoding_df, on='category')
    X = merged_df[['x', 'infold_mean']]
    y = merged_df['y']
    return X, y",0.4488738179,
1149,"make a single dataframe with the encoded columns, plus age , fare and survived from the original dataframe","def probStatus(dataset, group_by):
    df = pd.crosstab(index = dataset[group_by], columns = dataset.No_show).reset_index()
    df['probShowUp'] = df['No'] / (df['Yes'] + df['No'])
    return df[[group_by, 'probShowUp']]",0.4485012591,
1775,refine the data,"def on_click_marker(event):
    if event.dblclick:
        x, y = event.xdata, event.ydata
        ax = event.inaxes
        ax.plot(x, y, 'ko')
        event.canvas.draw()",0.450279355,
1775,refine the data,"def deleteMiddle(node):
    
    temp = node.getNext()
    node.setData(temp.getData())
    node.setNext(temp.getNext())",0.4299095571,
1775,refine the data,"def onclick(event):
    y = event.ydata # gets the ""y"" coordinate of the mouseclick
    threshold.set_ydata([y,y]) # draws the threshold line at the newly set level
    
    # iterates through the bars and applies new coloring
    for bar, avg, y_err in zip(bars, year_avg, y_error):
        
        # sets the 'red' and 'blue' depending on whether respective condition is True and applies faded alpha
        bar.set_color(((y < avg), 0, (y > avg), min(1, abs(y - avg)/y_err)))
    plt.gca().set_title('Barplots for y={:.0f}'.format(y)) # displays the threshold",0.4251438379,
1775,refine the data,"def update_frame(t):
    x = np.linspace(0,1)
    data_line.set_data(x, np.sin(np.pi*x)*np.sin(2*np.pi*t))",0.4239088893,
1775,refine the data,"class nzr(object):
    def __init__(self,_rawdata,_eps=1e-8):
        self.rawdata = _rawdata
        self.eps     = _eps
        self.mu      = np.mean(self.rawdata,axis=0)
        self.std     = np.std(self.rawdata,axis=0)
        self.nzd_data = self.get_nzdval(self.rawdata)
        self.org_data = self.get_orgval(self.nzd_data)
        self.maxerr = np.max(self.rawdata-self.org_data)
    def get_nzdval(self,_data):
        _n = _data.shape[0]
        _nzddata = (_data - np.tile(self.mu,(_n,1))) / np.tile(self.std+self.eps,(_n,1))
        return _nzddata
    def get_orgval(self,_data):
        _n = _data.shape[0]
        _orgdata = _data*np.tile(self.std+self.eps,(_n,1))+np.tile(self.mu,(_n,1))
        return _orgdata
# Demo usage of normalizer
if __name__=='__main__':
    np.set_printoptions(precision=3) # Set precision
    xRand = 100*np.random.rand(100,2)
    print (""Mean and varaince of original X are %s and %s""
               %(np.mean(xRand,axis=0),np.var(xRand,axis=0)))
    nzrX = nzr(_rawdata=xRand) # Noarmalizer
    nzdX = nzrX.get_nzdval(xRand) # Normalize 
    print (""Mean and varaince of normalized X are %s and %s""
               %(np.mean(nzdX,axis=0),np.var(nzdX,axis=0)))",0.4222589433,
1775,refine the data,"# Define a class to receive the characteristics of each line detection
class Line():
    def __init__(self):
        # x values of the last n fits of the line
        self.recent_xfitted = [] 
        #average x values of the fitted line over the last n iterations
        self.bestx = None     
        #polynomial coefficients for the previous fit
        self.previous_fit = np.array([0,0,0], dtype='float') 
        #polynomial coefficients for the most recent fit
        self.current_fit = np.array([0,0,0], dtype='float') 
        #radius of curvature of the line in some units
        self.radius_of_curvature = None 
        #x values for detected line pixels
        self.allx = None  
        #y values for detected line pixels
        self.ally = None",0.4204454124,
1775,refine the data,"#Define a class to process and visualize every moving behavior
class Processing:
    
    #initialize one moving behavior: remove mean, set length, time period, x,y,z, and magnitude
    def __init__(self, data):
        #self.data = self
        self.data = data
        #remove mean
        self.x = (data[[0]] - data[[0]].values.mean()).values[:,0]
        self.y = (data[[1]] - data[[1]].values.mean()).values[:,0]
        self.z = (data[[2]] - data[[2]].values.mean()).values[:,0]
        m = (np.sqrt((data ** 2).sum(axis = 1))).values
        m_mean = m.mean()
        self.m = m - m_mean
        self.length = data.shape[0]
        self.time = self.length / 32.
        self.t = np.linspace(0, self.time, self.length) #define time scale
        print ""Total Samples: "", self.length
        
    #a function to visualize acceleration data from one direction, peaks are optional
    def Plot_data(self, direction, dir_str, title, with_peaks = False, peaks = []):

        #define style for plot
        title_font = {'fontname':'Arial', 'size':'20'}
        axis_font = {'fontname':'Arial', 'size':'15'}

        fig, ax = plt.subplots(1,1,figsize = (20,5))
        #if true show peaks in the plot
        if (with_peaks):
            pplot(self.t, direction, peaks)
            ax.set_title('Peaks')
            ax.set_xlabel(""time/s"", **axis_font)
            ax.set_ylabel(""ACC "" + dir_str, **axis_font)
        
        #otherwise, just show acceleration data
        else:
            #plot
            ax.plot(self.t,direction)
            ax.set_xticks(np.arange(min(self.t), max(self.t), 10.0))
            ax.set_xlabel(""time/s"", **axis_font)
            ax.set_ylabel(""ACC "" + dir_str, **axis_font)
            ax.set_title(title, **title_font)
            
        return ax
        
    #a function to find peaks 
    def Peaks(self, direction, dir_str, thres, min_dist, plot = True):
        
        #define style for plot
        title_font = {'fontname':'Arial', 'size':'20'}
        axis_font = {'fontname':'Arial', 'size':'15'}
        
        indexes = peakutils.indexes(-direction, thres=thres, min_dist=min_dist)
        if plot:
            plt.figure(figsize=(20,5))
            pplot(self.t, direction, indexes[1:-1]) #skip the first and the last peaks
            plt.title('Peaks')
            plt.xlabel(""time/s"", **axis_font)
            plt.ylabel(""ACC "" + dir_str, **axis_font)

        return indexes[1:-1] 
        
    #Segment data by time
    def Clip(self, interval):

        start_time = interval[0]
        end_time = interval[1]
        start_sample = int(start_time/float(self.time) * self.length)
        end_sample = int(end_time/float(self.time) * self.length)
        
        return self.data.iloc[start_sample:end_sample,:]
    
    #plot segmented (repetitive) data and average feature
    def Segment_Feature(self, direction, buffer_size, peaks):
        
        #line up repetitive segments
        window_size = 2*buffer_size+1
        steps = np.zeros((len(peaks),window_size))
        for i,index in enumerate(peaks):
            temp = np.arange(index-buffer_size,index+buffer_size+1)
            steps[i,:] = direction[temp]

        steps = np.transpose(steps)

        
        fig, ax = plt.subplots(1,2,figsize=(20,5))
        
        #plot the repetitive segments
        t = np.linspace(0,window_size/32.0,window_size)
        mean_step = np.mean(steps,axis = 1)
        ax[0].plot(t.reshape(-1,1),steps,color = sns.color_palette()[0])
        ax[0].plot(t.reshape(-1,1),mean_step,color = 'r',linewidth = 4, label = ""Mean Segment"")
        ax[0].set_title(""Plot of All Segments"")
        ax[0].set_ylabel(""Magnitude"")
        ax[0].set_xlim(t[0],t[-1])
        ax[0].set_xlabel('time/s')
        ax[0].legend()
        
        #plot extracted feature
        off_set = np.min(mean_step) 
        mean_step_off_set = mean_step - off_set #move signal above zero
        top = np.max(mean_step_off_set) 
        feature = mean_step_off_set/float(top) #standardize it to have max magnitude 1
        
        
        ax[1].plot(t.reshape(-1,1),feature - feature.mean()) # remove feature mean for matched filter
        ax[1].plot(t[14],feature[14] - feature.mean(),c = 'r', marker = 'o',label = 'peak')
        ax[1].set_title(""Average Feature"")
        ax[1].set_xlim(t[0],t[-1])
        ax[1].set_xlabel('time/s')
        ax[1].set_ylabel('Normalized Magnitude')
        ax[1].legend()
        #ax[1].plot(t.reshape(-1,1),np.flipud(feature))
        
        
        #return np.flipud(feature - feature.mean()), np.transpose(steps)
        return np.flipud(feature), np.flipud(feature - feature.mean()), np.transpose(steps)
    
    def extract_features(self,go_upstairs_normal,go_downstairs_normal,walking_normal,running):
        #going up stairs feature
        peaks_1 = go_upstairs_normal.Peaks(go_upstairs_normal.m,""M"", 0.3, 20, plot = False)
        go_upstairs_normal_feature1, go_upstairs_normal_feature2, go_upstairs_normal_set = go_upstairs_normal.Segment_Feature(go_upstairs_normal.m,14,peaks_1)
        
        peaks_2 = go_downstairs_normal.Peaks(go_downstairs_normal.m,""M"", 0.3, 20,plot = False)
        go_downstairs_normal_feature1,go_downstairs_normal_feature2,go_downstairs_normal_set = go_downstairs_normal.Segment_Feature(go_downstairs_normal.m,14,peaks_2)
        
        peaks_3 = walking_normal.Peaks(walking_normal.m,""Magnitude"", 0.3, 20, plot = False )
        walking_normal_feature1, walking_normal_feature2,walking_normal_set = walking_normal.Segment_Feature(walking_normal.m,14,peaks_3)
        
        peaks_4 = running.Peaks(running.m, ""Magnitude"",0.3, 20, plot = False )
        running_feature1,running_feature2,running_set = running.Segment_Feature(running.m,14,peaks_4)
        
        features1 = [go_upstairs_normal_feature1,go_downstairs_normal_feature1,walking_normal_feature1,running_feature1]
        features2 = [go_upstairs_normal_feature2,go_downstairs_normal_feature2,walking_normal_feature2,running_feature2]
        features_set = [go_upstairs_normal_set,go_downstairs_normal_set,walking_normal_set,running_set]
        return features1, features2, features_set
        
    #delete some part of the signal (e.g. noise)
    #start: start time in seconds
    #end: end time in seconds
    def delete(self, start, end):
        n_start = int(float(start)/self.time * self.length)
        n_end = int(float(end)/self.time * self.length)
       
        return self.data.drop(self.data.index[n_start:n_end])
    def save(self, filename):
        self.data.to_csv(filename, sep = "","", index_label = False )",0.4140556455,
1775,refine the data,"def animate(f):
    for idx, l in enumerate(points):
        line[idx].set_data(l[:f, 1], l[:f, 2])
    return",0.4138821959,
1775,refine the data,"def onclick_polygon_interrogation(event):
    global pixelx, pixely, AlldataMasked, FieldMean, Clickedpolygon
    pixelx, pixely = int(event.xdata), int(event.ydata)
    # Generate a point from the location
    Clickpoint = shapely.geometry.Point(pixelx, pixely)
    IrrigatedShapes = IrrigatedPolygons.shapes()
    # Find the polygon that contains the selected point
    Clickedpolygon = []
    for ix, shapes in enumerate(IrrigatedPolygons.shapes()):
        if shapely.geometry.shape(shapes).contains(Clickpoint) == True:
            Clickedpolygon.append(ix)
    # Colour the chosen polygon on the figure
    x = [i[0] for i in IrrigatedShapes[Clickedpolygon[0]].points[:]]
    y = [i[1] for i in IrrigatedShapes[Clickedpolygon[0]].points[:]]
    
    plt.figure(fig1.number)
    plt.plot(x, y, 'r')
    
    # Grab the geometry from the polygon we want to interrogate
    with fiona.open(shape_file) as shapes:
        crs = geometry.CRS(shapes.crs_wkt)
        first_geometry = shapes[Clickedpolygon[0]]['geometry']
        geom = geometry.Geometry(first_geometry, crs=crs)

    querys2 = {'output_crs': 'EPSG:3577',
               'resolution': (-10, 10),
               'geopolygon': geom,
               'time':(start_date, end_date)
              }
    queryls = {'geopolygon': geom,
               'time':(start_date, end_date)
              }

    # dc.load the data using the polygon as the bounds
    Alldata = dict()
    for Sensor in AllSensors:
        if Sensor[0] == 'l':
            try:
                Alldata[Sensor], LScrs, LSaffine = DEADataHandling.load_nbarx(dc, Sensor, queryls, product = 'nbart')
            except TypeError:
                print('No data available for {}'.format(Sensor))
        if Sensor[0] == 's':
            prodname = '{0}_ard_granule'.format(Sensor)
            try:
                Alldata[Sensor], S2crs, S2affine = DEADataHandling.load_sentinel(dc, prodname, querys2)
            except TypeError:
                print('No data available for {}'.format(Sensor))

    # Tidy up our dict to remove any empty keys
    for Sensor in AllSensors:
        try:
            Alldata[Sensor]
            if Alldata[Sensor] is None:
                del Alldata[Sensor]
            else:
                try:
                    Alldata[Sensor].time
                except AttributeError:
                    del Alldata[Sensor]
        except KeyError:
                pass

    # Mask the returned data with the polygon to remove any extra data
    AlldataMasked = Alldata.copy()
    for Sensor in Alldata.keys():
        mask = rasterio.features.geometry_mask([geom.to_crs(Alldata[Sensor].geobox.crs) for geoms in [geom]],
                                               out_shape=Alldata[Sensor].geobox.shape,
                                               transform=Alldata[Sensor].geobox.affine,
                                               all_touched=False,
                                               invert=True)
        AlldataMasked[Sensor] = Alldata[Sensor].where(mask)

    # Calculate the NDVI for each sensor
    for Sensor in AllSensors:
        try:
            AlldataMasked[Sensor]['NDVI'] = BandIndices.calculate_indices(AlldataMasked[Sensor], 'NDVI')
        except KeyError:
            pass

    # Concatenate all out 
    Allvalues = xr.concat([masked.NDVI for masked in AlldataMasked.values()], dim='time')
    Allvalues = Allvalues.sortby('time')
    Allvalues.values[Allvalues.values == -999] = np.nan
    FieldMean = Allvalues.mean(dim = ('x', 'y')).dropna(dim = 'time')",0.4132879376,
1775,refine the data,"def onclick(event):
    global pixelx, pixely
    x, y = int(event.xdata), int(event.ydata)
    image_coords = data.affine * (x, y)
    pixelx = int(image_coords[0])
    pixely = int(image_coords[1])
    w.value = 'pixelx : {}, pixely : {}'.format(pixelx, pixely)",0.4123244286,
2232,"students calculate median, mode, max, min for the example","def myowndescribe(datacolumn):
    mean = datacolumn.mean()
    median = datacolumn.median()
    mode = datacolumn.mode()
    print(""mean = "", mean)
    print(""median = "", median)
    
    variance = datacolumn.var()
    std = datacolumn.std()
    stderror = datacolumn.std()/np.sqrt(len(datacolumn))
    print(""variance = "", variance)
    print(""standard deviation = "", std)
    print(""standard error ="", stderror)",0.5439952612,
2232,"students calculate median, mode, max, min for the example","def mean_median_feature(df):
        print('# Mean & Median range')
        dmean = df.mean()
        dmedian = df.median()
        #q0_1 = df.quantile(0.1)
        #q0_99 = df.quantile(0.99)
        q1 = df.quantile(0.25)
        d2 = df.quantile(0.5)
        q3 = df.quantile(0.75)
        col = df.columns
        del_col = ['id','formation_energy_ev_natom','bandgap_energy_ev']
        col = [w for w in col if w not in del_col]
        
        for c in col:
            df['mean_'+c] = (df[c] > dmean[c]).astype(np.uint8)
            df['median_'+c] = (df[c] > dmedian[c]).astype(np.uint8)
            #df['q0_1_'+c] = (df[c] < q0_1[c]).astype(np.uint8)
            #df['q0_99_'+c] = (df[c] > q0_99[c]).astype(np.uint8)
            df['q1_'+c] = (df[c] < q1[c]).astype(np.uint8)
            df['q2_'+c] = (df[c] < q1[c]).astype(np.uint8)
            df['q3_'+c] = (df[c] > q3[c]).astype(np.uint8)
            
        print('Shape',df.shape)


mean_median_feature(train)
mean_median_feature(test)",0.5399775505,
2232,"students calculate median, mode, max, min for the example","def early_stopping(history):
    index = np.argmin(np.array(history['val_YP_loss']) + np.array(history['val_YT_loss']))
    print('The validation YT loss is {}.'.format(history['val_YT_loss'][index]))
    print('The validation YP loss is {}.'.format(history['val_YP_loss'][index]))
    print('The validation YT accuracy is {}.'.format(history['val_YT_acc'][index]))
    print('The validation YP accuracy is {}.'.format(history['val_YP_acc'][index]))",0.5367306471,
2232,"students calculate median, mode, max, min for the example","def fit_predict_model(city_data, print_output=True):
    """"""Find and tune the optimal model. Make a prediction on housing data.""""""

    # Get the features and labels from the Boston housing data
    X, y = city_data.data, city_data.target

    # Setup a Decision Tree Regressor
    regressor = DecisionTreeRegressor()

    parameters = {'max_depth':(1,2,3,4,5,6,7,8,9,10)}

    ###################################
    ### Step 4. YOUR CODE GOES HERE ###
    ###################################

    # 1. Find an appropriate performance metric. This should be the same as the
    # one used in your performance_metric procedure above:
    # http://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html
    scorer = make_scorer(mean_squared_error, greater_is_better=False)
    
    # 2. We will use grid search to fine tune the Decision Tree Regressor and
    # obtain the parameters that generate the best training performance. Set up
    # the grid search object here.
    # http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html#sklearn.grid_search.GridSearchCV
    reg = grid_search.GridSearchCV(regressor, parameters, scoring=scorer)
                             

    # Fit the learner to the training data to obtain the best parameter set
    reg = reg.fit(X, y)
    best_model = reg.best_estimator_
    if print_output:
        print ""Final Model: ""
        print best_model

        print ""All scores: "" + str(reg.grid_scores_)
    
    # I have added additional print statements to help undertand output of the Grid Search.     
    optim_max_depth = reg.best_params_['max_depth']
    score = reg.best_score_
    
    if print_output:
        print ""The optimal max_depth parameter found by Grid Search: "" + str(optim_max_depth)
        print ""The score given by Grid Search for the optimal model: "" + str(score)
    
    # Use the model to predict the output of a particular sample
    x = [11.95, 0.00, 18.100, 0, 0.6590, 5.6090, 90.00, 1.385, 24, 680.0, 20.20, 332.09, 12.13]
    y = best_model.predict([x])
    
    if print_output:
        print ""House: "" + str(x)
        print ""Prediction: "" + str(y)
    
    # By returning this tuple of max_depth, model score and predictin, it is possible to analyse a sample of results.    
    return (optim_max_depth, score, y[0])",0.5334065557,
2232,"students calculate median, mode, max, min for the example","#----------------------------------------------------------------
# Question 1 - Lists & graphs of the best and worst places to swim
#----------------------------------------------------------------
def processQ1(river_df1):
    display(Markdown(""## <font color='red'>Question - 1</font>""))
    display(Markdown(""### <font color='red'>Create lists & graphs of the best and worst places to swim in the dataset.</font>""))
    river_means_df = river_df1.groupby(river_df1['Site'])['EnteroCount'].mean().reset_index()
    river_best_df = river_means_df.sort_values('EnteroCount').reset_index(drop=True)[0:10]
    river_worst_df = river_means_df.sort_values('EnteroCount', ascending=False).reset_index(drop=True)[0:10]
    
    display(Markdown(""### <font color='blue'>Best places to swim</font>""))
    display(river_best_df)
   
    display(Markdown(""### <font color='blue'>Worst places to swim</font>""))
    display(river_worst_df)
      
    plt.figure(1)
    plt.rcParams['figure.figsize'] = (15,10)
    p1 = sns.barplot(x='Site', y='EnteroCount', data=river_best_df, order=river_best_df['Site'].values, palette=sns.color_palette('BuGn', 10))
    p1.set_ylabel('EnteroCount')
    p1.set_xlabel('Mean Enterococcus Count')
    p1.set_title('Best 10 places to Swim')
    for item in p1.get_xticklabels():
        item.set_rotation(45)
    sns.plt.show()
            
    plt.figure(2)
    plt.rcParams['figure.figsize'] = (15,10)
    p2 = sns.barplot(x='Site', y='EnteroCount', data=river_worst_df, order=river_worst_df['Site'].values, palette=sns.color_palette('OrRd', 10))
    p2.set_ylabel('EnteroCount')
    p2.set_xlabel('Mean Enterococcus Count')
    p2.set_title('Worst 10 places to Swim')
    for item in p2.get_xticklabels():
        item.set_rotation(45)
    sns.plt.show()",0.5320234895,
2232,"students calculate median, mode, max, min for the example","# common functions
from statistics import mean, median
def inspect_votes(qs):
    print('[Votes]')
    votes = [q['score'] for q in qs]
    print('  votes sum      : ', sum(votes))
    print('  votes mean     :  {:.2f}'.format(mean(votes)))
    print('  total questions:  ', len(qs))
def inspect_questions(qs, vs_all=False, owner=False, answer=True):
    answered = [q for q in qs if q['is_answered']]
    prc = lambda v, total: len(v) * 100 / len(total)
    print('[Questions]')
    print('  total questions:  ', len(qs))
    if answer:
        print('  % answered     :  {:.2f}'.format(prc(answered, qs)))
        score0 = [q for q in qs if q['score'] == 0]
        print('  % 0 score      :  {:.2f}'.format(prc(score0, qs)))
        score0_answered = [q for q in answered if q['score'] == 0]
        print('    of answered  :  {:.2f}'.format(prc(score0_answered, answered)))
    if vs_all:
        print('compared to all questions:')
        print('  total             :', '{}/{}'.format(len(qs), len(questions)))
        print('  % of all questions: {:.2f}'.format(prc(qs, questions)))
    if owner:
        print('question owner data:')
        from datetime import datetime
        qs_with_time = [q for q in qs if q['owner'].get('creation_date')]
        times = []
        for q in qs_with_time:
            owner = datetime.fromtimestamp(q['owner']['creation_date'])
            qdate = datetime.fromtimestamp(q['creation_date'])
            time = qdate - owner
            times.append(time.days)
        print('  average asker account age: {:.2f} days'.format(mean(times)))
        print('  median: {:.2f} days'.format(median(times)))",0.5295218229,
2232,"students calculate median, mode, max, min for the example","def print_bench_k_means(estimator, name, data, labels):
    t0 = time()
    estimator.fit(data)
    print('%-9s\t%.2fs\t%i\t%.3f\t%.3f\t%.3f\t%.3f\t%.3f\t%.3f\t%.3f'
          % (name, (time() - t0), estimator.inertia_,
             metrics.homogeneity_score(labels, estimator.labels_),
             metrics.completeness_score(labels, estimator.labels_),
             metrics.v_measure_score(labels, estimator.labels_),
             metrics.adjusted_rand_score(labels, estimator.labels_),
             metrics.adjusted_mutual_info_score(labels, estimator.labels_),
             metrics.calinski_harabaz_score(data, estimator.labels_) ,
             metrics.silhouette_score(data, estimator.labels_, metric='euclidean', sample_size=1000))
         )",0.5277459025,
2232,"students calculate median, mode, max, min for the example","def get_mean(x):
    print(all_in_titanic.groupby(x).mean().sort_values(by='Survived',ascending=False))
    all_in_titanic.groupby(x)['Survived'].mean().plot(kind = 'bar')
    plt.ylabel('Survival Proportions')
    plt.title('Survival Probability by ' + str(x))",0.5270473957,
2232,"students calculate median, mode, max, min for the example","def score_model(model, alpha=False):
    ''' 
    This function fits a model using the training set, predicts using the test set, and then calculates 
    and reports goodness of fit metrics and alpha if specified and available.
    
    All of the model parameters are also reported, which I find extremely useful.
    
    I wanted to include all of the available regression metrics to see how they compare and comove.
    I ran into an ValueError when trying to include MSLE (mean squared log error). 
    Could be related to ln0 being undefined?
    '''
    model.fit(Xtrain, ytrain)
    yhat = model.predict(Xtest)
    r2 = r2_score(ytest, yhat)
    me = mse(ytest, yhat)
    ae = mae(ytest, yhat)
    mede = medae(ytest, yhat)
    ev = evs(ytest, yhat)
    
    if alpha == True:
        print(""Results from {}: \nr2={:0.3f} \nMSE={:0.3f} \
              \nMAE={:0.3f} \nMEDAE={:0.3f} \nEVS={:0.3f} \nalpha={:0.3f}"".format(model, r2, me, 
                                                                                  ae, mede, ev, model.alpha_))
    else:
        print(""Results from {}: \nr2={:0.3f} \nMSE={:0.3f} \
              \nMAE={:0.3f} \nMEDAE={:0.3f} \nEVS={:0.3f}"".format(model, r2, me, ae, mede, ev))",0.5265202522,
2232,"students calculate median, mode, max, min for the example","#created graph to plot TAYP in individual years
def TAYPGraph(Year):
    Year.sort_values(""TAYP"",ascending = False, inplace = True)
    Y = int(Year[""YR""].mean())
    T = (""TAYP "" + str(Y))
    fig, ax = plt.subplots()
    Year.head(5).plot(y = ""TAYP"", x = ""Player"", ax = ax, kind='bar', figsize=(9, 5), legend=False, fontsize=8)
    ax.set_xlabel(""Player"", fontsize = 12, weight = ""bold"")
    ax.set_ylabel((""TAYP""), fontsize=12, weight = ""bold"")
    ax.set_ylim(0,15)
    ax.set_title(T, weight = ""bold"", fontsize = 14)
    avg = Year[""TAYP""].mean()
    fig.text(.40, .32, (""Average: "" + str(round(avg,3))) ,  fontsize=10, color=""blue"")
    ax.axhline(avg, color='blue', linewidth=2)    
    ax.spines['right'].set_color('none')
    ax.spines['top'].set_color('none')
    
    plt.show()",0.5257140398,
2080,step get your tools setup,"def load_prediction_model_weights(args):
    try:
        model.load_weights(args.weights_file[0])
        print (""Loaded model weights from: "" + str(args.weights_file[0]))
        return model
        
    except:
        print (""Error loading model weights ..."")
        sys.exit(1)",0.4557762742,
2080,step get your tools setup,"%%file tutorial4_beamline_vls_grating_1.py
# m=1 reflection grating 

def get_beamline(ekev):
    import os
    import wpg
    from wpg import Beamline
    from wpg.optical_elements import Aperture, Drift, Lens, Empty, Use_PP, Mirror_plane, VLS_grating
    from wpg.wpg_uti_oe import show_transmission
    from wpg.srwlib import SRWLOptMirPl
    import numpy as np

    wpg_path = os.path.abspath(os.path.dirname(wpg.__file__))
    data_path = 'data_wpg_tutorial_04'

    # S1 beamline layout
    # Geometry ###
    src_to_hom1 = 268.8 #274.  # Distance source to HOM 1 [m]
    src_to_hom2 = 271.7 #284.  # Distance source to HOM 2 [m]
    
    src_to_m3a = 287.4    # high energy 1-3 keV #Distance source to vertical focusing mirror of grating mono
    src_to_m3b = 288.2    # low energy 0.27 1.2 keV #Distance source to vertical focusing mirror of grating mono
    src_to_vls_gr = 288.8 #
    src_to_m5 = 326.8   # SCS distribution mirror
    src_to_pslit = 387.9  #400. # distance source to slit in focus 
    
    src_to_m3 = src_to_m3b #low energy mono

    theta_om = 9.e-3   # [rad] check!
    theta_m3 = 20.e-3  # [rad]
    alpha_gr1 = 16.239e-3 # [rad] for 0.8 keV
    vls_gr1_par = [50., 0.00101, 0, 0] #Polynomial coefficients for VLS Grating Groove Density
    vls_gr2_par = [150., 0.00101, 0, 0] #Polynomial coefficients for VLS Grating Groove Density
    # calculate grating geometry
    d = 1.e-3/vls_gr1_par[0]; #period
    wl = 12.39e-10/ekev
    m = 1 #grating order
    dtheta = np.arcsin(m*wl/(2*d*np.sin(theta_m3))) #for constant deflection angle geo
    alpha = theta_m3-dtheta; beta = theta_m3+dtheta
    print('alpha, beta: {:.1f}, {:.1f} mrad'.format(alpha*1e3,beta*1e3))
    print('{:.4g}'.format(np.cos(alpha)-np.cos(beta)))
    print('{:.4g}'.format(m*wl/d))    
    om_mirror_length = 0.8  # [m]
    om_clear_ap = om_mirror_length * theta_om
    
    m3_mirror_length = 0.58
    m3_clear_ap = m3_mirror_length * theta_m3
    gr_length = 0.5
    gr_width = 0.2
    gr_clear_ap = gr_length * theta_m3
    # define the beamline:
    bl0 = Beamline()

    # Define HOM1.
    aperture_x_to_y_ratio = 1
    hom1 = Aperture(
        shape='r', ap_or_ob='a', Dx=om_clear_ap, Dy=om_clear_ap / aperture_x_to_y_ratio)
    bl0.append(
        hom1, Use_PP(semi_analytical_treatment=0, zoom=1,sampling = 1/0.6))

    # Define mirror profile
    # Apply distortions.
    mirrors_path = os.path.join(wpg_path, '..','samples', 'data_common')
    hom1_wavefront_distortion = Mirror_plane(orient='x', 
                                             theta=theta_om, 
                                             length=om_mirror_length, 
                                             range_xy=om_clear_ap/aperture_x_to_y_ratio, 
                                             filename=os.path.join(mirrors_path, 'mirror1.dat'), 
                                             scale=1, delim=' ',bPlot=True)
    print('HOM1 WF distortion'); show_transmission(hom1_wavefront_distortion);

    zoom = 1.2

    bl0.append(hom1_wavefront_distortion,
               Use_PP(semi_analytical_treatment=0, zoom=zoom, sampling=zoom/0.8))

    # Free space propagation from hom1 to hom2
    hom1_to_hom2_drift = Drift(src_to_hom2 - src_to_hom1)
    bl0.append(hom1_to_hom2_drift, Use_PP(semi_analytical_treatment=0))

    # Define HOM2 as aperture.
    zoom = 1.0
    hom2 = Aperture('r', 'a', om_clear_ap, om_clear_ap / aperture_x_to_y_ratio)
    bl0.append(hom2, Use_PP(semi_analytical_treatment=0,
                            zoom=zoom, sampling=zoom / 1))

    # drift to M3 mirror
    hom2_to_m3 = Drift(src_to_m3 - src_to_hom2)

    bl0.append(hom2_to_m3, Use_PP(semi_analytical_treatment=1))
    m3 = Aperture('r', 'a', m3_clear_ap, m3_clear_ap / aperture_x_to_y_ratio)
    bl0.append(m3, Use_PP(semi_analytical_treatment=0,
                            zoom=zoom, sampling=zoom / 1))
    # Define mirror profile
    # Apply distortions.
    mirrors_path = 'data_common'
    # Define mirror profile
    pm3_wavefront_distortion = Mirror_plane(orient='y', 
                                             theta=theta_m3, 
                                             length=m3_mirror_length, 
                                             range_xy=m3_clear_ap/aperture_x_to_y_ratio, 
                                             filename=os.path.join(
                                             mirrors_path, 'mj37_2.dat'), 
                                             scale=5.,
                                             bPlot=True)
    print('M3 WF distortion'); show_transmission(pm3_wavefront_distortion);
    bl0.append(pm3_wavefront_distortion,
               Use_PP(semi_analytical_treatment=0))
    q_m3 = src_to_pslit - src_to_m3
    bl0.append(Lens(1e23,1./(1./src_to_m3 + 1./q_m3)), 
            Use_PP(semi_analytical_treatment=1))
    m3_to_vls_gr = Drift(src_to_vls_gr - src_to_m3)
    bl0.append(m3_to_vls_gr, Use_PP(semi_analytical_treatment=1))
    vls_gr_0 = Aperture('r', 'a', m3_clear_ap, m3_clear_ap / aperture_x_to_y_ratio)
    #Grating Substrate (plane mirror, deflecting in vertical plane):
    gr_sub = SRWLOptMirPl(_size_tang=gr_length, _size_sag=gr_width, _ap_shape='r',
                    _nvx=0, _nvy=np.cos(alpha_gr1), _nvz=-np.sin(alpha_gr1), _tvx=0, _tvy=np.sin(alpha_gr1))    
    vls_gr_1 = VLS_grating(_mirSub=gr_sub, 
                           _m=1, 
                           _grDen=vls_gr1_par[0], 
                           _grDen1=vls_gr1_par[1], 
                           _grDen2=vls_gr1_par[2], 
                           _grDen3=vls_gr1_par[3])
    
    bl0.append(vls_gr_0, Use_PP(semi_analytical_treatment=0,
                            zoom=zoom, sampling=zoom / 1.))
    bl0.append(vls_gr_1, Use_PP(semi_analytical_treatment=0,
                            zoom=zoom, sampling=zoom / 1.))
    vls_gr_to_foc = Drift(src_to_pslit - src_to_vls_gr)
    bl0.append(vls_gr_to_foc, Use_PP(semi_analytical_treatment=1))
    bl0.append(Empty(),Use_PP(zoom_v=0.1, sampling_v=0.1/0.1))
    return bl0",0.439987123,
2080,step get your tools setup,"def plot_gt_vs_pred(df):
    hover = HoverTool(tooltips=[
    (""(x,y)"", ""($x, $y)"")])
    p = figure(plot_width=800, plot_height=400, tools=[hover, BoxZoomTool(), ResetTool()],
           title =""bicycle positions vs predicted positions"")


    p.circle(df[""x_gt""], df[""y_gt""], size=3, color='navy', legend='ground truth')
    p.square(df[""p_x""], df[""p_y""], size=3, color='firebrick', legend= 'predicted')
    p.legend.location = ""top_left""
    p.legend.click_policy=""hide""
    show(p)",0.4365008175,
2080,step get your tools setup,"def test_ham_or_spam(message):
    bowNewMess = bow_transformer.transform([message]) # turns new message into bag of words
    tfidfNewMess = tfidf_transformer.transform(bowNewMess)
    print('Prediction: ', spam_detect_model.predict(tfidfNewMess)[0])",0.4353546202,
2080,step get your tools setup,"def prepare_data():
    """"""
    Loads imdb data and splits it into train / val / test sets.
    """"""
    (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=MAX_FEATURES)
    print(""Number of training samples:"", len(x_train))

    x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=MAX_REVIEW_LENGTH)
    x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=MAX_REVIEW_LENGTH)

    # Split a part of the training dataset for validation
    x_val = x_train[:10000]
    y_val = y_train[:10000]
    x_train = x_train[10000:]
    y_train = y_train[10000:]

    print(""x_train.shape:"", x_train.shape)
    print(""x_val.shape:"", x_val.shape)
    print(""x_test.shape:"", x_test.shape)
    
    return ((x_train, y_train), (x_val, y_val), (x_test, y_test))",0.432803452,
2080,step get your tools setup,"#Parse the Input XML File to Extract the Class Tag as Labels and the text tag as Input Sentences Into an Intermediate File Format
#Note that the Input Sentences are first pre-processed using the Spacy NLP Library to remove all Extraneous Words and 
# reduce the feature vector Size from ~47000 to ~37000
def extract_from_file(file,output_file_handle):
    nlp = spacy.load('en')
    print('==================================================================================================================')
    print('Processing File: ',file)
    parser = etree.XMLParser(recover=True)
    root=etree.parse(file).getroot()
    rootChildren=root.getchildren()
    for rootChild in rootChildren:
        if rootChild.tag=='citations':
            citations=rootChild.getchildren()
            for citation in citations:
                citationChildren=citation.getchildren()
                for citationChild in citationChildren:
                    if citationChild.tag=='class':
                        #print('Class: ',citationChild.text)
                        output_file_handle.write(citationChild.text)
                        output_file_handle.write('\t')
                    if citationChild.tag=='text':
                        #print('Sentence: ',citationChild.text)
                        if citationChild.text is not None:
                            doc = nlp(citationChild.text, disable=['parser', 'ner'])
                            # Only Select the lematized and lower case version of a token if it is not a stop-word, punctuation, currencly Symbol, Bracket, Number , URL, or Email
                            cleaned = [token.lemma_.lower().strip() for token in doc if token.is_punct==False and token.is_stop==False and token.is_bracket==False and token.is_currency==False and token.is_quote==False and token.is_digit==False and len(token)>2 and token.like_url==False and token.like_num==False and token.like_email==False]
                            cleaned_text=' '.join(cleaned)
                            to_write=cleaned_text
                            to_write=re.sub(""\n"", "" "",to_write)
                            output_file_handle.write(to_write)
                            output_file_handle.write('\n')
                        else:
                            output_file_handle.write("" "")
                            output_file_handle.write('\n')",0.4312163591,
2080,step get your tools setup,"def main(config):
    # load vocabs
    vocab_words = load_vocab(config.words_filename)
    vocab_tags  = load_vocab(config.tags_filename)
    vocab_chars = load_vocab(config.chars_filename)

    # get processing functions
    processing_word = get_processing_word(vocab_words, vocab_chars,
                    lowercase=True, chars=config.chars)
    processing_tag  = get_processing_word(vocab_tags, 
                    lowercase=False)

    # get pre trained embeddings
    embeddings = get_trimmed_glove_vectors(config.trimmed_filename)

    # create dataset
    dev   = CoNLLDataset(config.dev_filename, processing_word,
                        processing_tag, config.max_iter)
    test  = CoNLLDataset(config.test_filename, processing_word,
                        processing_tag, config.max_iter)
    train = CoNLLDataset(config.train_filename, processing_word,
                        processing_tag, config.max_iter)

    # build model
    model = NERModel(config, embeddings, ntags=len(vocab_tags),
                                         nchars=len(vocab_chars))
    model.build()

    # train, evaluate
    model.train(train, dev, vocab_tags)
    model.evaluate(test, vocab_tags)",0.4310900867,
2080,step get your tools setup,"def findCars(img):
    # Define vairables
    filename = './model/bestFit_0.9932.pkl'
    clf = joblib.load(filename)
    filename = './model/scaler_0.9932.pkl'
    scaler = joblib.load(filename)
    color_space = 'LAB'
    orient = 12
    pix_per_cell = 8
    cell_per_block = 2
    hog_channel = 'ALL'
    spatial_feat = True
    hist_feat = True
    hog_feat = True
    
    overlap = 0.8
    windowSizes = [64,96,128,160] #32
    windowROI = [[[500,400],[1268,528]], [[400,350],[1264,542]], [[350,350], [1246,606]], [[300,350],[1260,670]]] #[[600,400],[984,496]], 
    
    
    images_roi, allWindows, scales = getWindows(img, windowSizes, windowROI, overlap)
    
    
    hot_windows = []
    for image_roi, windows, roi, scale in zip(images_roi, allWindows, windowROI, scales):
        carWindows = search_windows(image_roi, windows, clf, scaler, color_space=color_space,
                                    orient=orient, pix_per_cell=pix_per_cell, cell_per_block=cell_per_block, 
                                    hog_channel=hog_channel, spatial_feat=spatial_feat, hist_feat=hist_feat, hog_feat=hog_feat)

#         b = draw_boxes(image_roi, windows, thick=2)
#         plt.imshow(b)
#         plt.axis('off')
#         plt.title('Regoin Of Interest: {} | Window Size {}'.format(roi, scale*64))
#         plt.show()
        
        # Resize the boxes for use on main image
        outputWindows = []
        for box in carWindows:
            box[0][0] = int(box[0][0]*scale + roi[0][0])
            box[0][1] = int(box[0][1]*scale + roi[0][1])
            box[1][0] = int(box[1][0]*scale + roi[0][0])
            box[1][1] = int(box[1][1]*scale + roi[0][1])
            hot_windows.append(box)
#         if len(carWindows) > 0:
#         hot_windows.append(carWindows)
    return np.array(hot_windows)",0.429566443,
2080,step get your tools setup,"### TODO: Obtain bottleneck features from another pre-trained CNN.
def features_from_pre_trained(network):
    print('Loading bottleneck features for: {} ...'.format(network))
    bottleneck_features = np.load(f'bottleneck_features/Dog{network}Data.npz')
    print('Done.')
    train = bottleneck_features['train']
    valid = bottleneck_features['valid']
    test = bottleneck_features['test']
    return train, valid, test",0.4290909767,
2080,step get your tools setup,"#Define the Image Processing Function
def image_process(img):
    ##Load the distortion coefficients
    dist_pickle = pickle.load(open( ""camera_cal/wide_dist_pickle.p"", ""rb"" ))
    mtx=dist_pickle[""mtx""]
    dist = dist_pickle[""dist""] 
    #print(mtx.shape,dist.shape)
    
    #Edge Finding Function outputting Binary Image
    output_image = edge_finding(img)
    output_image = output_image.astype(np.uint8)
    
    #Warp the Binary Image  
    warped, M_img, Minv_img = warp_image(output_image, mtx, dist)
    
    # window settings
    window_width = 40
    window_height = 80 # Break image into 8 vertical layers since image height is 720
    margin = 30 # How much to slide left and right for searching
    frames= 9 #Frames over which to average coefficients
    
    #Window Centroids used for left and right lane
    leftx = []  
    rightx = [] 
    
    
    # Define conversions in x and y from pixels space to meters
    ym_per_pix = 30/720 # meters per pixel in y dimension
    xm_per_pix = 3.7/700 # meters per pixel in x dimension
    
    #Find Lanes using Window Sliding Search and Convolutions
    window_centroids = find_window_centroids(warped, window_width, window_height, margin)
    
    for level in range(0,len(window_centroids)):
        #Add the window center points to the list of left and right lane points 
        leftx.append(window_centroids[level][0])
        rightx.append(window_centroids[level][1])
    
    #print(mean_left,mean_right)
    #Average centroids
        
    mean_left = np.mean(leftx)
    mean_right = np.mean(rightx)
    average_lane_width = mean_right - mean_left
    
    
        
    #print(track.leftLaneAppender, track.rightLaneAppender)
    #Curve fitting
    y_value = range(0, warped.shape[0])
    res_yvalue = np.arange(warped.shape[0]-(window_height/2),0,-window_height)#define the range of 

    # Fit a second order polynomial to pixel positions of left and right lane lines respectively
    left_fit = np.polyfit(res_yvalue, leftx, 2)
    right_fit = np.polyfit(res_yvalue, rightx, 2)


    #Track Lane Coefficients
    track.framesCounter += 1
    
    
    if track.framesCounter < frames:
        track.lineWidthAppender(average_lane_width)
        track.lineCoef([left_fit],[right_fit])
        left_fitx = left_fit[0]*y_value*y_value + left_fit[1]*y_value + left_fit[2]
        left_fitx = left_fitx.astype(np.int32)
        right_fitx = right_fit[0]*y_value*y_value + right_fit[1]*y_value + right_fit[2]
        right_fitx = right_fitx.astype(np.int32)
        
        #Calculate the Radius of Curvature
        curve_fit_left = np.polyfit(np.array(res_yvalue,np.float32)*ym_per_pix, np.array(leftx,np.float32)*xm_per_pix, 2)
        left_curverad = ((1 + (2*curve_fit_left[0]*y_value[-1]*ym_per_pix + curve_fit_left[1])**2)**1.5) / np.absolute(2*curve_fit_left[0])
        track.radius.append(left_curverad)
              
    else:
        #Average coefficients over last 7 frames
        if (np.mean(track.lineWidthAppend[(len(track.leftCoef)-frames):])-20) < average_lane_width and (np.mean(track.lineWidthAppend[(len(track.leftCoef)-frames):])+20) > average_lane_width:
            track.lineWidthAppender(average_lane_width)
            track.lineCoef([left_fit],[right_fit])
            left_fit = np.mean(track.leftCoef[(len(track.leftCoef)-frames):],axis=0)
            right_fit = np.mean(track.rightCoef[(len(track.rightCoef)-frames):],axis=0) 
            left_fitx = left_fit[0]*y_value*y_value + left_fit[1]*y_value + left_fit[2]
            left_fitx = left_fitx.astype(np.int32)
            right_fitx = right_fit[0]*y_value*y_value + right_fit[1]*y_value + right_fit[2]
            right_fitx = right_fitx.astype(np.int32)
            
            #Calculate the Radius of Curvature
            curve_fit_left = np.polyfit(np.array(res_yvalue,np.float32)*ym_per_pix, np.array(leftx,np.float32)*xm_per_pix, 2)
            left_curverad = ((1 + (2*curve_fit_left[0]*y_value[-1]*ym_per_pix + curve_fit_left[1])**2)**1.5) / np.absolute(2*curve_fit_left[0])
            track.radius.append(left_curverad)              
        
        else:
            left_fit = np.mean(track.leftCoef[(len(track.leftCoef)-frames):],axis=0)
            right_fit = np.mean(track.rightCoef[(len(track.rightCoef)-frames):],axis=0) 
        
            #determine lines
            left_fitx = left_fit[0]*y_value*y_value + left_fit[1]*y_value + left_fit[2]
            left_fitx = left_fitx.astype(np.int32)
            right_fitx = right_fit[0]*y_value*y_value + right_fit[1]*y_value + right_fit[2]
            right_fitx = right_fitx.astype(np.int32)


    #print(track.leftCoef, track.rightCoef)
    
    
    left_lane = np.array(list(zip(np.concatenate((left_fitx-window_width/4,left_fitx[::-1]+window_width/4), axis=0),np.concatenate((y_value,y_value[::-1]),axis=0))), np.int32)
    right_lane = np.array(list(zip(np.concatenate((right_fitx-window_width/4,right_fitx[::-1]+window_width/4), axis=0),np.concatenate((y_value,y_value[::-1]),axis=0))), np.int32)
    middle_marker = np.array(list(zip(np.concatenate((left_fitx+window_width/4,right_fitx[::-1]-window_width/4), axis=0),np.concatenate((y_value,y_value[::-1]),axis=0))), np.int32)

    road= np.zeros_like(img)

    cv2.fillPoly(road,[left_lane], color=[255,0,0])
    cv2.fillPoly(road,[right_lane], color=[0,0,255])
    cv2.fillPoly(road,[middle_marker], color=[0,150,0])


    #plt.imshow(road)
    img_size = (img.shape[1], img.shape[0])
    road_warped = cv2.warpPerspective(road,Minv_img,img_size, flags=cv2.INTER_LINEAR)
    result = cv2.addWeighted(img, 1, road_warped, 1.0, 0.0)
    

    #Calculate the offset of the car on the road
    camera_center = (left_fitx[-1]+right_fitx[-1])/2
    center_diff = (camera_center - warped.shape[1]/2)*xm_per_pix
    side_pos = 'left'
    if center_diff <= 0:
        side_pos = 'right'

    #print(left_curverad)
    #Draw the Radius of Curvature, Offset and Speed
    
    cv2.putText(result, 'Radius of Curvature = '+str(np.round(np.mean(track.radius[(len(track.leftCoef)-frames):]),3))+'(m)',(50,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255),2)
    cv2.putText(result, 'Vehicle is '+str(abs(np.round(center_diff,3)))+'m '+side_pos+' of center',(50,100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255),2)
    
    return result",0.4289448261,
798,heatmap,"def plot_experiment(df):
    """"""
    Utility function to plot a heatmap of the resulting p-values
    """"""
    plt.figure(figsize=(10,10))
    ax = sns.heatmap(df, cmap=plt.cm.Blues, linecolor='gray',
                     xticklabels=df.columns.values.round(2),
                     linewidths=.02
                    )
    ax.set(xlabel='CR delta', ylabel='N')
    ax.set_yticklabels(rotation=0, labels=Ns);",0.5546966195,
798,heatmap,"def display_cmap(cmap): #Display  a colormap cmap
    plt.imshow(np.linspace(0, 100, 256)[None, :],  aspect=25, interpolation='nearest', cmap=cmap) 
    plt.axis('off')",0.5360743999,
798,heatmap,"def display_cmap(cmap):
    plt.imshow(np.linspace(0, 100, 256)[None, :],  aspect=25, interpolation='nearest', cmap=cmap) 
    plt.axis('off')",0.5360743999,
798,heatmap,"# Use the seaborn library to quickly plot the confusion matrix
def plot_confusion(mat):
    import seaborn as sn
    import matplotlib as mpl
    ax = sn.heatmap(mat)
    mpl.rcParams['figure.figsize'] = (20,10)
    plt.show()
plot_confusion(confusion_mat)",0.5249950886,
798,heatmap,"def interactive_distance_plot(save=False):
    ""Create an interactive distance plot""
    words, distances = tsne_distances()
    
    trace = go.Heatmap(z=distances,
                        x=words,
                        y=words,
                        zmin=0,
                        zmax=40)
    data=[trace]

    fig = go.Figure(data=data, layout=go.Layout(title=""Distances for {} words"".format(len(words))))
    
    # if Save is set, save as .html:
    if save:
        py.offline.plot({
            ""data"": data, 
            ""layout"": go.Layout(title=""Distances for {} words"".format(len(words)))
        }, auto_open=False, filename=""output/words-distances.html"")
    
    py.offline.iplot(fig)",0.5202568769,
798,heatmap,"###########################################################################################
# 5 -> Basic pipeline
###########################################################################################   
prev_heatmaps = []
def find_cars(image, vis=False):
    global prev_heatmaps
    
    windows_1 = get_windows(image, sizes=[80], y_start_stop=(.4, .7))
    windows_2 = get_windows(image, sizes=[120, 180])
    windows = windows_1 + windows_2
    
    car_windows = []
    for window in windows:
        if contains_car(image, window):
            car_windows.append(window)
            
    heatmap = get_heatmap(image, car_windows, thresholded=False)     
    prev_heatmaps.insert(0, heatmap)
    if len(prev_heatmaps) > 5:
        prev_heatmaps.pop()
    aggregate_heatmap = get_aggregate_heatmap(threshold=4) 
    if vis == True:
        two_plots(heatmap, aggregate_heatmap, 'Heatmap', 'Agg.Heatmap')
    return draw_heatmap_boxes(np.copy(image), aggregate_heatmap)

image = mpimg.imread('./test_images/test4.jpg')
one_plot(find_cars(image, True))",0.5159611702,
798,heatmap,"def make_heatmap(data, equities=False):
    heatmap = sns.heatmap(
        data, 
        mask=np.zeros_like(data, dtype=np.bool), 
        cmap=sns.diverging_palette(
            h_neg=140, 
            h_pos=374,
            l=45,
            as_cmap=True
        ),
        square=True,
        annot=True,
        xticklabels=equities,
        yticklabels=equities
    )

    heatmap.set_yticklabels(heatmap.get_yticklabels(), rotation=0)
    plt.show()
    
make_heatmap(data, [equity.symbol for equity in data.index])",0.5151711106,
798,heatmap,"def Average(self):
    maps = np.asarray(self.heatmap)
    if len(maps)>0:
        avg=0
        for map in maps:
            avg +=map
        avg = avg / len(maps)
    else:
        avg = np.zeros_like(self[:,:,0]).astype(np.float)
    return avg",0.5128340721,
798,heatmap,"# Create a Class to Store Information between Video Frames
class heat():
    def __init__(self):
        
        self.heatmap = np.zeros_like(image[:,:,0]).astype(np.float)
        self.heatmap_list = []",0.5075028539,
798,heatmap,"def testPipeline(path):
    global heatmaps # detected heatmaps from the last images
    heatmaps = deque() # source: https://docs.python.org/2/library/collections.html
    global car_windows
    car_windows = []
    images = []
    
    # read test images
    test_images = readImages(path)
    
    for test_image in test_images: # go through all test images
        test_image_detected = pipeline(test_image) # detect cars with the pipeline
        images.append(test_image_detected)

    plotImage(images, True) # plot images with detected cars and save them to folder
    
#testPipeline('test_images/test*.jpg')",0.5065610409,
1485,perform the regression,"# helper functions    
def test_classifier_performance(clf):
    """"""
    clf will be fitted on the newsgroup train data, measured on test
    clf can be a sklearn pipeline
    """"""

    clf.fit(newsgroups_train.data, newsgroups_train.target)
    pred = clf.predict(newsgroups_test.data)
    print('Accuracy: {:.3f}'.format(metrics.accuracy_score(newsgroups_test.target, pred)))",0.5395370722,
1485,perform the regression,"def tokenize_test(vect):
    X_train_dtm = vect.fit_transform(X_train)
    X_test_dtm = vect.transform(X_test)
    print('Features: ', X_train_dtm.shape[1])
    nb = MultinomialNB()
    nb.fit(X_train_dtm, y_train)
    y_pred_prob = nb.predict_proba(X_test_dtm)[:,1]
    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_prob)
    print('AUC: ', metrics.auc(fpr, tpr))
    
# Check with default settings
tokenize_test(vect)",0.5237691998,
1485,perform the regression,"#def gpPredictFix(x):
#    return np.array([x[i] for i in x.keys()]).T

def posterior(bo, x):
    # Fit gaussian process
    bo.gp.fit(bo.X, bo.Y)
    # Create predict value (mu,sigma)
    Xinput = gpPredictFix(x)
    return bo.gp.predict(Xinput, return_std=True)",0.5215688348,
1485,perform the regression,"def print_metrics(classifier):
    classifier.fit(X_train,Y_train)
    y_pred = classifier.predict(X_test)
    y_pred_proba = classifier.predict_proba(X_test)[:,1]
    cm = confusion_matrix(Y_test,y_pred)
    ac = met.accuracy_score(Y_test,y_pred)
    pc = met.precision_score(Y_test,y_pred)
    rc = met.recall_score(Y_test,y_pred)
    auc = met.roc_auc_score(Y_test,y_pred_proba)
    print ('Confusion Matrix is ')
    print  cm
    print ('Accuracy is ' + str(ac))
    print ('Precision is ' + str(pc))
    print ('Recall is ' + str(rc))
    print ('AUC score is ' + str(auc))",0.5192109346,
1485,perform the regression,"def vect_test(vect):
    # transform the unstructured data using the vectorizer
    X_train_dtm = vect.fit_transform(X_train)
    X_test_dtm = vect.transform(X_test)

    # print the number of features generated as a result of the transformation
    print 'Features: ', X_train_dtm.shape[1]

    # train Logistic Regression model and measure accuracy
    lr = LogisticRegression()
    lr.fit(X_train_dtm, y_train)
    y_pred_class_lr = lr.predict(X_test_dtm)
    print 'Logistic Regression Accuracy: ', metrics.accuracy_score(y_test, y_pred_class_lr)

    # train Naive Bayes model and measure accuracy
    nb = MultinomialNB()
    nb.fit(X_train_dtm, y_train)
    y_pred_class_nb = nb.predict(X_test_dtm)
    print 'Naive Bayes Accuracy: ', metrics.accuracy_score(y_test, y_pred_class_nb)
    
    # train Random Forest model and measure accuracy
    rf = RandomForestClassifier()
    rf.fit(X_train_dtm, y_train)
    y_pred_class_rf = rf.predict(X_test_dtm)
    print 'Random Forest Accuracy: ', metrics.accuracy_score(y_test, y_pred_class_rf)",0.5164256692,
1485,perform the regression,"def posterior(optimizer, x_obs, y_obs, grid):
    optimizer._gp.fit(x_obs, y_obs)

    mu, sigma = optimizer._gp.predict(grid, return_std=True)
    return mu, sigma

def plot_gp(optimizer, x, y):
    fig = plt.figure(figsize=(16, 10))
    steps = len(optimizer.space)
    fig.suptitle(
        'Gaussian Process and Utility Function After {} Steps'.format(steps),
        fontdict={'size':30}
    )
    
    gs = gridspec.GridSpec(2, 1, height_ratios=[3, 1]) 
    axis = plt.subplot(gs[0])
    acq = plt.subplot(gs[1])
    
    x_obs = np.array([[res[""params""][""x""]] for res in optimizer.res])
    y_obs = np.array([res[""target""] for res in optimizer.res])
    
    mu, sigma = posterior(optimizer, x_obs, y_obs, x)
    axis.plot(x, y, linewidth=3, label='Target')
    axis.plot(x_obs.flatten(), y_obs, 'D', markersize=8, label=u'Observations', color='r')
    axis.plot(x, mu, '--', color='k', label='Prediction')

    axis.fill(np.concatenate([x, x[::-1]]), 
              np.concatenate([mu - 1.9600 * sigma, (mu + 1.9600 * sigma)[::-1]]),
        alpha=.6, fc='c', ec='None', label='95% confidence interval')
    
    axis.set_xlim((-2, 10))
    axis.set_ylim((None, None))
    axis.set_ylabel('f(x)', fontdict={'size':20})
    axis.set_xlabel('x', fontdict={'size':20})
    
    utility_function = UtilityFunction(kind=""ucb"", kappa=5, xi=0)
    utility = utility_function.utility(x, optimizer._gp, 0)
    acq.plot(x, utility, label='Utility Function', color='purple')
    acq.plot(x[np.argmax(utility)], np.max(utility), '*', markersize=15, 
             label=u'Next Best Guess', markerfacecolor='gold', markeredgecolor='k', markeredgewidth=1)
    acq.set_xlim((-2, 10))
    acq.set_ylim((0, np.max(utility) + 0.5))
    acq.set_ylabel('Utility', fontdict={'size':20})
    acq.set_xlabel('x', fontdict={'size':20})
    
    axis.legend(loc=2, bbox_to_anchor=(1.01, 1), borderaxespad=0.)
    acq.legend(loc=2, bbox_to_anchor=(1.01, 1), borderaxespad=0.)",0.5155830979,
1485,perform the regression,"def crossValResults(classifierObj):
    results = []
    #iterate through the training and test cross validation segments and
    #run the classifier on each one, aggregating the results into a list
    for traincv, testcv in cv:
        probas = classifierObj.fit(train[traincv], target[traincv]).predict_proba(train[testcv])
        results.append(llfun( target[testcv], [x[1] for x in probas] ))
    return results",0.5155810118,
1485,perform the regression,"def calc_fitness(model):
    
    epochs = 1000
    train_losses, test_losses, times = model.fit(X, Y, epochs=epochs)    

    train = sys.maxsize if len(train_losses) == 0 else train_losses[-1] 
    test = sys.maxsize if len(test_losses) == 0 else test_losses[-1]    
    
    return train + test + np.mean(times) * 1000",0.5129199624,
1485,perform the regression,"# define a function that accepts a vectorizer and calculates the accuracy
def tokenize_test(vect):
    X_train_dtm = vect.fit_transform(X_train)
    print('Features: ', X_train_dtm.shape[1])
    X_test_dtm = vect.transform(X_test)
    nb = MultinomialNB()
    nb.fit(X_train_dtm, y_train)
    y_pred_class = nb.predict(X_test_dtm)
    print('Accuracy: ', metrics.accuracy_score(y_test, y_pred_class))",0.5127699971,
1485,perform the regression,"# define a function that accepts a vectorizer and calculates the accuracy
def tokenize_test(vect):
    X_train_dtm = vect.fit_transform(X_train)
    print 'Features: ', X_train_dtm.shape[1]
    X_test_dtm = vect.transform(X_test)
    nb = MultinomialNB()
    nb.fit(X_train_dtm, y_train)
    y_pred_class = nb.predict(X_test_dtm)
    print 'Accuracy: ', metrics.accuracy_score(y_test, y_pred_class)",0.5127699971,
616,features,"def get_model_pkl(): 
    conts = [get_contin_pkl(feat) for feat in contin_map_fit.features]
    embs = [get_emb_pkl(feat) for feat in cat_map_fit.features]
    x = merge([emb for inp,emb in embs] + [contin_out], mode='concat')

    x = Dropout(0.02)(x)
    x = Dense(1000, activation='relu', init='uniform')(x)
    x = Dense(500, activation='relu', init='uniform')(x)
    x = Dense(1, activation='sigmoid')(x)

    model_pkl = Model([inp for inp,emb in embs] + [contin_inp], x)
    model_pkl.compile('adam', 'mean_absolute_error')
    #model.compile(Adam(), 'mse')
    return model_pkl",0.4725069702,
616,features,"# TODO: Replace <FILL IN> with appropriate code
import itertools

def twoWayInteractions(lp):
    cartesian_product_iterator = [item for item in itertools.product(lp.features, lp.features)]
    cartesian_product = map(lambda x: np.prod(x), cartesian_product_iterator)
    return LabeledPoint(lp.label, np.hstack((lp.features, cartesian_product)))

print twoWayInteractions(LabeledPoint(0.0, [2, 3]))

# Transform the existing train, validation, and test sets to include two-way interactions.
trainDataInteract = parsedTrainData.map(twoWayInteractions)
valDataInteract = parsedValData.map(twoWayInteractions)
testDataInteract = parsedTestData.map(twoWayInteractions)",0.4646158814,
616,features,"def feature_extraction(df):
    feature_list = []
    for i in df.features:
        feature_list.extend(i)
    names = list(set(feature_list))
    voc2id = dict(zip(names, range(len(names))))
    rows, cols, vals = [], [], []
    for r, d in enumerate(df.features):
        for e in d:
            if voc2id.get(e) is not None:
                rows.append(r)
                cols.append(voc2id[e])
                vals.append(1)
    features = sp.csr_matrix((vals, (rows, cols)))
    svd = TruncatedSVD(n_components=8)
    features_svd = pd.DataFrame(svd.fit_transform(features),columns = ['f-1','f-2','f-3','f-4','f-5','f-6','f-7','f-8'])
    return features_svd",0.4638861418,
616,features,"def evaluateResults(model, data):
    """"""Calculates the log loss for the data given the model.

    Args:
        model (LogisticRegressionModel): A trained logistic regression model.
        data (RDD of LabeledPoint): Labels and features for each observation.

    Returns:
        float: Log loss for the data.
    """"""
    
    return data.map(lambda x: computeLogLoss(getP(x.features, model.weights, model.intercept),x.label)).mean()
    
logLossTrLR0 = evaluateResults(model0, OHETrainData)
print ('OHE Features Train Logloss:\n\tBaseline = {0:.3f}\n\tLogReg = {1:.3f}'
       .format(logLossTrBase, logLossTrLR0))",0.4628302753,
616,features,"# TODO: Replace <FILL IN> with appropriate code
def evaluateResults(model, data):
    """"""Calculates the log loss for the data given the model.

    Args:
        model (LogisticRegressionModel): A trained logistic regression model.
        data (RDD of LabeledPoint): Labels and features for each observation.

    Returns:
        float: Log loss for the data.
    """"""
    #<FILL IN>
    return data.map(lambda p: computeLogLoss(getP(p.features, model.weights, model.intercept), p.label)).mean()
    

logLossTrLR0 = evaluateResults(model0, OHETrainData)
print ('OHE Features Train Logloss:\n\tBaseline = {0:.3f}\n\tLogReg = {1:.6f}'
       .format(logLossTrBase, logLossTrLR0))",0.4628302753,
616,features,"def get_results(project):
    """"""Args: A DataRobot project object
    
       returns: A dataframe sorted by log loss for cross-validation""""""
    #extract featurelist
    feature_lists = project.get_featurelists()
    
    #get informative features, the default for autopilot
    f_list = [lst for lst in feature_lists if lst.name == 'Informative Features'][0]
    
    #get models
    models = project.get_models()
    flist_models = [model for model in models if model.featurelist_id == f_list.id]
    
    #print results
    val_scores = pd.DataFrame([{'model_type': model.model_type,
                           'model_id': model.id,
                           'sample_pct': model.sample_pct,
                           'featurelist': model.featurelist_name,
                           'val_logloss': model.metrics['LogLoss']['validation'],
                           'cross_val_logloss': model.metrics['LogLoss']['crossValidation']}
                           for model in flist_models if model.metrics['LogLoss'] is not None])
    
    return val_scores.sort_values(by='cross_val_logloss')",0.4622984529,
616,features,"import itertools
def twoWayInteraction(lp):
    features = lp.features.toArray().tolist()
    twoWays = map(lambda x: x[0] * x[1], itertools.product(features, features))
    features.extend(twoWays)
    newPoint = LabeledPoint(lp.label, Vectors.dense(features))
    return newPoint",0.4617019892,
616,features,"# feature-wize normalization between 0.0 and 1.0
def normalize_in_place(extracted_features):
    X = [x[0] for x in extracted_features if x is not None]
    scaler = MinMaxScaler(feature_range=(0, 1))
    X = scaler.fit_transform(X)
    for i, x in enumerate(X):
        if extracted_features[i] is not None:
            extracted_features[i][0] = x

normalize_in_place(extracted_features)",0.46032691,
616,features,"# TODO: Replace <FILL IN> with appropriate code
def transformOrderTwo(lp):
    """"""Transforms the features in the LabeledPoint object lp into higher order features.

    Args:
        lp - LabeledPoint object 

    Returns:
        LabeledPoint: The object contains the label and the higher order features
    """"""
    features = lp.features
    new_features = [features[0], features[1], 
                    features[0]**2, features[1]**2,
                    features[0] * features[1]]
    
    label = lp.label
    return LabeledPoint(label, DenseVector(new_features))

orderTwoParsedSamplePoints = parsedSamplePoints.map(transformOrderTwo) 
pprint(orderTwoParsedSamplePoints.take(2))
normalizedOrderTwoSamplePoints = getNormalizedRDD(orderTwoParsedSamplePoints) 
pprint(normalizedOrderTwoSamplePoints.take(2))",0.4592863619,
616,features,"def evaluateResults(model, data):
    """"""Calculates the log loss for the data given the model.

    Args:
        model (LogisticRegressionModel): A trained logistic regression model.
        data (RDD of LabeledPoint): Labels and features for each observation.

    Returns:
        float: Log loss for the data.
    """"""
    return (data
            .map(lambda x: computeLogLoss(getP(x.features, model.weights, model.intercept), x.label))
            .reduce(lambda x, y: x + y)) / data.count()",0.4590102434,
2131,step print only the column veterans,"def describeMetric(df, metric):
    print(df[metric].describe())
    print(""90%: {}"".format(df[metric].quantile(0.9)))
    print(""95%: {}"".format(df[metric].quantile(0.95)))
    print(""99%: {}"".format(df[metric].quantile(0.99)))",0.4769042134,
2131,step print only the column veterans,"def q6(df):
    salary_list = ['low', 'medium', 'high']
    for salary in salary_list:
        df_std = df['average_monthly_hours'].std()
        df_mean = df['average_monthly_hours'].mean()
        two_sd = 2*df_std + df_mean
        total_count = df['salary'].count()
        
        prob_salary_hours = df['salary'][(df['salary'] == salary) & (df['average_monthly_hours'] > two_sd)].count() / df['salary'][df['average_monthly_hours'] > two_sd].count()
        prob_hours = df['average_monthly_hours'][df['average_monthly_hours'] > two_sd].count() / total_count
        prob_salary = df['salary'][df['salary'] == salary].count() / total_count
        total_prob = prob_salary_hours * prob_hours / prob_salary
        print(""""""probability that a {} salary employee worked more than 2 standard 
deviations of the average monthly hours across all groups: {}%"""""".format(salary, round(total_prob * 100, 4)))",0.4747996926,
2131,step print only the column veterans,"def DoctorModule(self):
#1
        """"""Initialize and/or change treatment block""""""
        self.InitializeCorrectTreatment()
#2
        #ExitCode will be returned from one of treatment block.
        #ExitCode == True means either  
        #no more medication left (if in MedicationBlock)
        #or IOP > IOP target in surgeries Block (Trabeculectomy/Implant)
        if self.medicalRecords['ExitCode'] == True:
            self.ChangeTreatmentPlan()
            self.InitializeCorrectTreatment()
            self.medicalRecords['ExitCode'] = False            
        
        self.medicalRecords['PatientVisits'] += 1",0.470251292,
2131,step print only the column veterans,"def describe_columns(df):
    for i in df.columns:
        print('Column: ' + i)
        titanic.select(i).describe().show()",0.4665508866,
2131,step print only the column veterans,"def get_column_info(df,column):
    # print general info about column
    print df[column].describe()
    
    # show histogram amd box plot
    plt.figure()
    df[df.TARGET==0][column].plot(kind=""hist"", label=""satisfied customers"")
    df[df.TARGET==1][column].plot(kind=""hist"", label=""unsatisfied customers"")
    plt.legend();
    
    plt.figure()
    df[df.TARGET==0][column].plot(kind=""kde"", label=""satisfied customers"")
    df[df.TARGET==1][column].plot(kind=""kde"", label=""unsatisfied customers"")
    plt.legend();
    
    plt.figure()
    df[column].plot(kind='box')",0.4638954401,
2131,step print only the column veterans,"def flip_images_augmentation(df):
    new_df = df[df.steering != 0].sample(frac=0.4)
    df.loc[:,'is_flipped'] = False
    new_df.loc[:,'is_flipped'] = True
    left_rows = (new_df.steering < 0)
    right_rows = (new_df.steering > 0)
    new_df.loc[left_rows,'steering'] = new_df[left_rows].steering.abs()
    new_df.loc[right_rows, 'steering'] = new_df[right_rows].steering * -1
    return pd.concat([df, new_df])

augmented = flip_images_augmentation(new_drive_log)
plt.figure(figsize=(10,4))
augmented.steering.hist(bins=100, color='r')
plt.xlabel('steering angle bins')
plt.ylabel('counts')
plt.show()
print(""Current Dataset Size: "", len(augmented.steering))",0.4631635547,
2131,step print only the column veterans,"def flip_images_augmentation(df):
    new_df = df[df.steering != 0].sample(frac=0.4)
    df.loc[:,'is_flipped'] = False
    new_df.loc[:,'is_flipped'] = True
    left_rows = (new_df.steering < 0)
    right_rows = (new_df.steering > 0)
    new_df.loc[left_rows,'steering'] = new_df[left_rows].steering.abs()
    new_df.loc[right_rows, 'steering'] = new_df[right_rows].steering * -1
    return pd.concat([df, new_df])

augmented = flip_images_augmentation(new_drive_log)

plt.figure(figsize=(10,4))
augmented.steering.hist(bins=100, color='b')
plt.xlabel('steering angle bins')
plt.ylabel('counts')
plt.show()
print(""Current Dataset Size: "", len(augmented.steering))",0.4631635547,
2131,step print only the column veterans,"def CalMeanVar(df):
    '''
    This function calculates mean and variance of all features (excluding target and 
    dummy columns)
    Input: data frame
    Output: None
    
    '''
    
    for i in range(2, len(df.columns)):
        print(""{} | mean: {} | var: {}"".format(df.columns[i], 
                                               df.iloc[:, i].mean(), 
                                               df.iloc[:, i].var()))
    return

CalMeanVar(data_train[data_train.y == 1.0])",0.4627987146,
2131,step print only the column veterans,"def describe_treatment_control(df, col_name):
    print('Descriptive statistics for the matched cases in the treatment group\n')
    print(df[df.treat==1][col_name].describe())

    print('\n\nDescriptive statistics for the matched cases in the control group')
    print(df[df.treat==0][col_name].describe())",0.4624010324,
2131,step print only the column veterans,"def plotAll(evaluation, envId):
    evaluation.printFinalRanking(envId)
    # Rewards
    evaluation.plotRewards(envId)
    # Fairness
    #evaluation.plotFairness(envId, fairness=""STD"")
    # Centralized regret
    evaluation.plotRegretCentralized(envId, subTerms=True)
    #evaluation.plotRegretCentralized(envId, semilogx=True, subTerms=True)
    # Number of switches
    #evaluation.plotNbSwitchs(envId, cumulated=False)
    evaluation.plotNbSwitchs(envId, cumulated=True)
    # Frequency of selection of the best arms
    evaluation.plotBestArmPulls(envId)
    # Number of collisions - not for Centralized* policies
    #evaluation.plotNbCollisions(envId, cumulated=False)
    #evaluation.plotNbCollisions(envId, cumulated=True)
    # Frequency of collision in each arm
    #evaluation.plotFrequencyCollisions(envId, piechart=True)",0.4614171982,
1456,part sentiment analysis on movie review data,"opinions = set()
fileids = corpus.movie_reviews.fileids()
for fileid in fileids:
    doc = corpus.movie_reviews.raw(fileids=[fileid]
    doc = re.sub('\s+', ' ', doc))
    parsed = spc(doc)
    for ent in parsed.ents:
        if is_person(ent) and is_opinion(ent):
            left_child = next(ent.root.lefts)
            first = left_child.orth_.capitalize()
            last = ent.root.orth_.capitalize()
            right_of_head = next(ent.root.head.rights)
            opinions.add((first, last,
                          right_of_head.orth_))",0.5430527925,
1456,part sentiment analysis on movie review data,"from nltk.corpus import movie_reviews
documents = [(list(movie_reviews.words(fileid)), category)
            for category in movie_reviews.categories()
            for fileid in movie_reviews.fileids(category)]
random.seed(1)
random.shuffle(documents)",0.5390141606,
1456,part sentiment analysis on movie review data,"def genre(x):
    m = tmdb.Movies(x)
    time.sleep(1)      ## slow down by 1sec the code to give time to the API to answer
    g_id = m.info()['genres'][0][""id""]
    g_name = m.info()['genres'][0][""name""]
    budget = m.info()[""budget""]
    revenue = m.info()[""revenue""]
    return g_id, g_name, budget, revenue

df[""main genre""], df[""main genre name""], df[""budget""], df[""revenue""] = zip(*df[""id""].apply(genre,1))

df[[""title"", ""genre_ids"", ""main genre"", ""main genre name"", ""budget"", ""revenue""]].head()",0.5379534364,
1456,part sentiment analysis on movie review data,"id_to_name = load_mid_to_name('../Musicdata/id-to-name')
 uri_to_id = load_uri_to_id('../Musicdata/uri-to-id')
 drake_uri = 'spotify:artist:3TVXtAsR1Inumwj472S9r4'
 def most_similar_music(uri, vectors, max=30, least=False):
    id = uri_to_id[uri]
    vec = vectors[id]
    top_sims = []
    for (i, vec_b) in enumerate(vectors):
        if np.isnan(scipy.spatial.distance.cosine(vec, vec_b)):
            continue
        top_sims.append((i, (scipy.spatial.distance.cosine(vec, vec_b))))
    top_sims = sorted(top_sims, key = lambda x: x[1])[:max]
    return [(id_to_name[i], sim) for (i, sim) in top_sims]
 sims = most_similar_music(drake_uri, m.item_vectors, least=False)
 plot_sims(id_to_name[uri_to_id[drake_uri]], sims[:20], 'Artists', 'Implicit Matrix Factorization', True)",0.5367289782,
1456,part sentiment analysis on movie review data,"''
getArticle(url)

#Description
##############
This function collects the article of a specific URL and produces some rudementaty features. 

#Parameters
##############
url = the URL to collect

#Output
#############
A JSON consiting of the following structure:

url: the url of the article
text: the full text of the article
keywords: the keywords of the article as returned by newspaper.nlp.keywords()
authors: a list of authors of the article
summary: the article summary as returned by newspaper.nlp.summary()
publish_date: the publishing data of thwe article as returned by nespaper.parse()


'''



def getArticle(url):
    article = newspaper.Article(url, fetch_images = False)
    article.download()
    article.parse()
    article.nlp()
    return {""url"":article.url, ""text"":article.text,""keywords"":newspaper.nlp.keywords(article.text), 
            ""authors"":article.authors,""summary"":article.summary,
            ""publish_date"":str(article.publish_date)}",0.535282433,
1456,part sentiment analysis on movie review data,"# Functions goes here
def grab_poster_tmdb(movie):
    response = search.movie(query=movie)
    id=response['results'][0]['id']
    movie = tmdb.Movies(id)
    posterp=movie.info()['poster_path']
    title=movie.info()['original_title']
    url='image.tmdb.org/t/p/original'+posterp
    title='_'.join(title.split(' '))
    strcmd='wget -O '+poster_folder+title+'.jpg '+url
    os.system(strcmd)
    #print(response['results'])
    
def get_movie_id_tmdb(movie):
    response = search.movie(query=movie)
    movie_id=response['results'][0]['id']
    return movie_id

def get_movie_info_tmdb(movie):
    response = search.movie(query=movie)
    movie_id=response['results'][0]['id']
    movie=tmdb.Movies(movie_id)
    info = movie.info()
    return info

def get_movie_genres_tmdb(movie):
    response=search.movie(query=movie)
    id=response['results'][0]['id']
    movie=tmdb.Movies(id)
    genres=movie.info()['genres']
    return genres",0.5349573493,
1456,part sentiment analysis on movie review data,"# get movie, genre pairs from TMDb
latest_id = tmdb.Movies().latest()['id']

def worker(i):
    try:
        movie = tmdb.Movies(i).info()
    except:
        movie = """"
    return [i, movie]

ids = list(range(1,latest_id))
random.shuffle(ids)
ids = ids[1:25000] #take random sample of 25,000 IDs 

p = multiprocessing.Pool()
ans = p.map(worker,ids)

global movie_dictionary 
movie_dictionary = {}
for i in ans:
    if i[1]: #only add the non-deleted movies to movie dictionary
        movie_dictionary[i[0]] = i[1]",0.5348410606,
1456,part sentiment analysis on movie review data,"# Load the corpus
from nltk.corpus import movie_reviews
# Check the categories
print(""Categories in the movie review corpus: {0}"".format(movie_reviews.categories()))
# Load the first files of each category
movie_pos = movie_reviews.sents(movie_reviews.fileids(""pos"")[0])
movie_neg = movie_reviews.sents(movie_reviews.fileids(""neg"")[0])

# Look at each of the sentences
print(""\nPositive review: {0}"".format(movie_pos[:3]))
print(""\nNegative review: {0}"".format(movie_neg[:3]))",0.5328581333,
1456,part sentiment analysis on movie review data,"#Functions that take in a movie and return ID, Genre, info and Posters
def get_poster(movie):
    response = search.movie(query=movie)
    id=response['results'][0]['id']
    movie = tmdb.Movies(id)
    posterpath=movie.info()['poster_path']
    title=movie.info()['original_title']
    url='http://image.tmdb.org/t/p/original'+posterpath
    title='_'.join(title.split(' '))
    r = requests.get(url)
    completeName = os.path.join(poster_folder, title) 
    with open(completeName,'wb') as w:
        w.write(r.content)

def getmovie_id(movie):
    resp= search.movie(query=movie)
    movie_id=resp['results'][0]['id']
    return movie_id

def getmovie_info(movie):
    resp= search.movie(query=movie)
    id=resp['results'][0]['id']
    movie = tmdb.Movies(id)
    info=movie.info()
    return info

def getmovie_genre(movie):
    resp = search.movie(query=movie)
    id=resp['results'][0]['id']
    movie = tmdb.Movies(id)
    genres=movie.info()['genres']
    return genres",0.5326256752,
1456,part sentiment analysis on movie review data,"numPartitions = 2
rawRatings = sc.textFile(ratingsFilename).repartition(numPartitions)
rawMovies = sc.textFile(moviesFilename)

def get_ratings_tuple(entry):
    """""" Parse a line in the ratings dataset
    Args:
        entry (str): a line in the ratings dataset in the form of UserID::MovieID::Rating::Timestamp
    Returns:
        tuple: (UserID, MovieID, Rating)
    """"""
    items = entry.split('::')
    return int(items[0]), int(items[1]), float(items[2])


def get_movie_tuple(entry):
    """""" Parse a line in the movies dataset
    Args:
        entry (str): a line in the movies dataset in the form of MovieID::Title::Genres
    Returns:
        tuple: (MovieID, Title)
    """"""
    items = entry.split('::')
    return int(items[0]), items[1]


ratingsRDD = rawRatings.map(get_ratings_tuple).cache()
moviesRDD = rawMovies.map(get_movie_tuple).cache()

ratingsCount = ratingsRDD.count()
moviesCount = moviesRDD.count()

print 'There are %s ratings and %s movies in the datasets' % (ratingsCount, moviesCount)
print 'Ratings: %s' % ratingsRDD.take(3)
print 'Movies: %s' % moviesRDD.take(3)

assert ratingsCount == 487650
assert moviesCount == 3883
assert moviesRDD.filter(lambda (id, title): title == 'Toy Story (1995)').count() == 1
assert (ratingsRDD.takeOrdered(1, key=lambda (user, movie, rating): movie)
        == [(1, 1, 5.0)])",0.5293967724,
1897,set hyperparameters,"""""""
Arc Constructor
An arc is composed of a CS basis, a radius and a sweep angle. The starting 
point of the resulting arc lies along the x-axis of the given CS.
""""""
def __init__(self, cs, radius, sweep_angle):
    self._basis = cs
    self.rad = radius
    self.angle = sweep_angle",0.4940985739,
1897,set hyperparameters,"class NeuralNetwork:
    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):
        # Set number of nodes in input, hidden and output layers.
        self.input_nodes = input_nodes
        self.hidden_nodes = hidden_nodes
        self.output_nodes = output_nodes

        # Initialize weights
        self.weights_input_to_hidden = np.random.normal(0.0, self.hidden_nodes**-0.5, 
                                                (self.hidden_nodes, self.input_nodes))
    
        self.weights_hidden_to_output = np.random.normal(0.0, self.output_nodes**-0.5, 
                                                (self.output_nodes, self.hidden_nodes))
        
        self.learning_rate = learning_rate
        
        #### Set this to your implemented sigmoid function ####
        # TODO: Activation function is the sigmoid function
    def sigmoid(self, w, x):        
        self.activation_function = 1.0 / (1.0 + np.exp(-np.dot(x, w)))
    
    def train(self, inputs_list, targets_list):
        # Convert inputs list to 2d array
        inputs = np.array(inputs_list, ndmin=2).T
        targets = np.array(targets_list, ndmin = 2).T
        
        #### Implement the forward pass here ####
        ### Forward pass ###
        # TODO: Hidden layer
        hidden_inputs = self.sigmoid(self.weights_input_to_hidden, inputs)
        hidden_outputs = self.sigmoid(self.weights_hidden_to_output, hidden_inputs)
        
        # TODO: Output layer
        final_inputs = # signals into final output layer
        final_outputs = # signals from final output layer
        
#         #### Implement the backward pass here ####
#         ### Backward pass ###
        
#         # TODO: Output error
#         output_errors = # Output layer error is the difference between desired target and actual output.
        
#         # TODO: Backpropagated error
#         hidden_errors = # errors propagated to the hidden layer
#         hidden_grad = # hidden layer gradients
        
#         # TODO: Update the weights
#         self.weights_hidden_to_output += # update hidden-to-output weights with gradient descent step
#         self.weights_input_to_hidden += # update input-to-hidden weights with gradient descent step
        
#     def run(self, inputs_list):
#         # Run a forward pass through the network
#         inputs = np.array(inputs_list, ndmin=2).T
        
#         #### Implement the forward pass here ####
#         # TODO: Hidden layer
#         hidden_inputs = # signals into hidden layer
#         hidden_outputs = # signals from hidden layer
        
#         # TODO: Output layer
#         final_inputs = # signals into final output layer
#         final_outputs = # signals from final output layer
        
#         return final_outputs",0.485485822,
1897,set hyperparameters,"%R library(xgboost)

# Prepare x and y vars
%R xTrain = data.matrix(playerAggDfAllNbaAllStarTrain[,c('advancedStats_WS', 'advancedStats_VORP')])
%R yTrain = data.matrix(ifelse(playerAggDfAllNbaAllStarTrain[,c('accolades_all_nba')] == 'All-NBA', 1, 0))
%R xTest = data.matrix(playerAggDfAllNbaAllStarTest[,c('advancedStats_WS', 'advancedStats_VORP')])

# Model
%R gbModel = xgboost(\
    data = xTrain,\
    label = yTrain,\
    nround = 120,\
    objective = 'binary:logistic',\
    eval_metric = 'auc',\
    eta = 0.1,\
    max_depth = 1\
)

# Predict
%R gbModelProb = predict(gbModel, xTest)
%R gbModelPred = ifelse(gbModelProb >= 0.047340, 'All-NBA', 'Not All-NBA')

# Check results
%R detach(package:xgboost, unload = T)
%R print(head(gbModelPred))",0.4830881655,
1897,set hyperparameters,"def grid_tune(param_test,scores):
    
    for score in scores:
        print(""# Tuning hyper-parameters for %s"" % score)
        print

        clf = GridSearchCV(LogisticRegression(penalty = 'l1', C=1,random_state = 2), param_test, cv=5,
                           scoring=score)
        smt=SMOTE(ratio=0.1, random_state=2, k=None, k_neighbors=5, 
          m=None, m_neighbors=10, out_step=0.5, kind='regular', svm_estimator=None, n_jobs=2)
        X_smt, y_smt=smt.fit_sample(train[predictors],train[response])
        X_smt=pd.DataFrame(X_smt)
        columns=train[predictors].columns
        X_smt.columns=columns
        
        clf.fit(X_smt[predictors], y_smt)

        print(""Best parameters set found on development set:"")
        print
        print(clf.best_params_,clf.best_score_)
        print
        print(""Grid scores on development set:"")
        print
        means = clf.cv_results_['mean_test_score']
        stds = clf.cv_results_['std_test_score']
        for mean, std, params in zip(means, stds, clf.cv_results_['params']):
            print(""%0.3f (+/-%0.03f) for %r""
                  % (mean, std * 2, params))",0.4816852808,
1897,set hyperparameters,"def set_parameters(parameters=None):
    if parameters is None: return
    global model
    model.params = parameters",0.4805281162,
1897,set hyperparameters,"def __init__(self, input_dim, hidden_dim, output_dim):
    #Initialize our neural network with a given number of input, hidden, and output neurons
    #Notice that all variables start with ""self."", I'll explain this in a moment
    
    
    #Create an array of input, hidden, and output activities, and initialize them to all ones
    
    #Create a matrix of random connection weights (drawn from a gaussian distribution)
    
    #Create a matrix of weight updates, and initialize them to all zeros",0.477809906,
1897,set hyperparameters,"# SVM grid search

param_grid = { 
    'kernel': ['poly', 'rbf', 'sigmoid'],
    'degree' : [1, 2, 3, 4, 5],
    'C' : [.5, 1.0, 10],
    'gamma': [1e-3, 1e-4, 1e-5]
            }

gs_svm = GridSearchCV(estimator=sv, 
                     param_grid=param_grid, 
                     cv= 5,
                     verbose=True,
#                      scoring='roc_auc',
                     n_jobs=-1)

gs_svm.fit(X_train, y_train)

print
print ""Best parameters"" 
print gs_svm.best_params_ 
print 
xval_score(gs_svm.best_estimator_)",0.4757013917,
1897,set hyperparameters,"def _compute_network_model(self) : 
    """""" Build the network, loss, grad_loss and sgd_update theano functions.
        More work than is strictly nessecary is done here as the only thing
        that is really needed in order to run sgd (stochastic gradient 
        descent) is the sgd_update function. The network, loss and grad_loss
        functions are compiled since this is experimental code.
    """"""

    # build the network
    self.i = T.vector('i',dtype = self.int_dtype)

    self.network_outputs = compute_network_outputs(self.i,self.s0,self.V,
                                                   self.U,self.W,self.b)


    # build mean log likelyhood loss

    # variables for a batch of sentences
    self.I = T.matrix('I',dtype = self.int_dtype)
    self.J = T.matrix('J',dtype = self.int_dtype) # for embedding I = J

    self.loss_outputs = compute_mean_log_lklyhd_outputs(self.I,self.J,
                                                        self.s0,self.V,
                                                        self.U,self.W,
                                                        self.b)

    # set up the accumulator for computing the loss in batches

    n_minibatch = T.cast(self.I.shape[0],self.float_dtype)
    loss_accum_ipnm = self.loss_accum_i + n_minibatch

    self.loss_updates = ((self.loss_accum,
                          (self.loss_outputs*n_minibatch/loss_accum_ipnm
                           + (self.loss_accum 
                             * self.loss_accum_i/loss_accum_ipnm))),
                         (self.loss_accum_i,loss_accum_ipnm))

    # get the gradient of the loss

    (self.dV,
     self.dU,
     self.dW,
     self.db) = theano.grad(self.loss_outputs,
                            [self.V,self.U,self.W,self.b])

    # get the gradient magnitudes

    self.dV_mag = T.sqrt(T.sum(self.dV*self.dV))
    self.dU_mag = T.sqrt(T.sum(self.dU*self.dU))
    self.dW_mag = T.sqrt(T.sum(self.dW*self.dW))
    self.db_mag = T.sqrt(T.sum(self.db*self.db))

    # get the sgd update function

    # this is the learning parameter
    self.eta = T.scalar('eta',dtype = self.float_dtype)

    # also including a running average of the gradient magnitudes

    self.sgd_i = T.scalar('sgd_i',dtype = self.float_dtype)

    dV_mag_accum = (self.dV_mag/(self.sgd_i+1.)
                        + self.m_dV_mag*(self.sgd_i/(self.sgd_i+1.)))
    dU_mag_accum = (self.dU_mag/(self.sgd_i+1.) 
                        + self.m_dU_mag*(self.sgd_i/(self.sgd_i+1.)))
    dW_mag_accum = (self.dW_mag/(self.sgd_i+1.) 
                        + self.m_dW_mag*(self.sgd_i/(self.sgd_i+1.)))
    db_mag_accum = (self.db_mag/(self.sgd_i+1.) 
                        + self.m_db_mag*(self.sgd_i/(self.sgd_i+1.)))

    # adding here since we are taking a max of the loss - accumulators
    # do not include the latest values
    self.sgd_updates = ((self.V,self.V + self.eta*self.dV),
                        (self.U,self.U + self.eta*self.dU),
                        (self.W,self.W + self.eta*self.dW),
                        (self.b,self.b + self.eta*self.db),
                        (self.m_dV_mag,dV_mag_accum),
                        (self.m_dU_mag,dU_mag_accum),
                        (self.m_dW_mag,dW_mag_accum),
                        (self.m_db_mag,db_mag_accum))

    # pointers for the compiled functions
    self.network = None
    self.loss = None
    self.grad_loss = None
    self.sgd_update = None
    self.sgd_update_w_loss = None

# bind the method to the class - this is just so the class definition
# can be broken up across several Jupyter notebook cells
embedding_model._compute_network_model = _compute_network_model",0.4747592807,
1897,set hyperparameters,"def fit_random(job_name, hyperparameters):
    m = MXNet('cifar10.py', 
              role=role, 
              train_instance_count=1, 
              train_instance_type='ml.p3.8xlarge',
              hyperparameters=hyperparameters)
    m.fit(inputs, wait=False, job_name=job_name)",0.4713321924,
1897,set hyperparameters,"# the robot class
class robot:

    # --------
    # init: 
    #   creates a robot with the specified parameters and initializes 
    #   the location (self.x, self.y) to the center of the world
    #
    def __init__(self, world_size = 100.0, measurement_range = 30.0,
                 motion_noise = 1.0, measurement_noise = 1.0):
        self.measurement_noise = 0.0
        self.world_size = world_size
        self.measurement_range = measurement_range
        self.x = world_size / 2.0
        self.y = world_size / 2.0
        self.motion_noise = motion_noise
        self.measurement_noise = measurement_noise
        self.landmarks = []
        self.num_landmarks = 0


    # returns a positive, random float
    def rand(self):
        return random.random() * 2.0 - 1.0


    # --------
    # move: attempts to move robot by dx, dy. If outside world
    #       boundary, then the move does nothing and instead returns failure
    #
    def move(self, dx, dy):

        x = self.x + dx + self.rand() * self.motion_noise
        y = self.y + dy + self.rand() * self.motion_noise

        if x < 0.0 or x > self.world_size or y < 0.0 or y > self.world_size:
            return False
        else:
            self.x = x
            self.y = y
            return True
    

    # --------
    # sense: returns x- and y- distances to landmarks within visibility range
    #        because not all landmarks may be in this range, the list of measurements
    #        is of variable length. Set measurement_range to -1 if you want all
    #        landmarks to be visible at all times
    #
    
    ## TODO: complete the sense function
    def sense(self):
        ''' This function does not take in any parameters, instead it references internal variables
            (such as self.landamrks) to measure the distance between the robot and any landmarks
            that the robot can see (that are within its measurement range).
            This function returns a list of landmark indices, and the measured distances (dx, dy)
            between the robot's position and said landmarks.
            This function should account for measurement_noise and measurement_range.
            One item in the returned list should be in the form: [landmark_index, dx, dy].
            '''
           
        measurements = []
        
        ## TODO: iterate through all of the landmarks in a world
        
        ## TODO: For each landmark
        ## 1. compute dx and dy, the distances between the robot and the landmark
        ## 2. account for measurement noise by *adding* a noise component to dx and dy
        ##    - The noise component should be a random value between [-1.0, 1.0)*measurement_noise
        ##    - Feel free to use the function self.rand() to help calculate this noise component
        ##    - It may help to reference the `move` function for noise calculation
        ## 3. If either of the distances, dx or dy, fall outside of the internal var, measurement_range
        ##    then we cannot record them; if they do fall in the range, then add them to the measurements list
        ##    as list.append([index, dx, dy]), this format is important for data creation done later
        
        ## TODO: return the final, complete list of measurements
        return measurements

    
    # --------
    # make_landmarks: 
    # make random landmarks located in the world
    #
    def make_landmarks(self, num_landmarks):
        self.landmarks = []
        for i in range(num_landmarks):
            self.landmarks.append([round(random.random() * self.world_size),
                                   round(random.random() * self.world_size)])
        self.num_landmarks = num_landmarks
    
    
    # called when print(robot) is called; prints the robot's location
    def __repr__(self):
        return 'Robot: [x=%.5f y=%.5f]'  % (self.x, self.y)",0.4710316062,
406,data import and inspection,"try:
    # read in data from H1 and L1, if available:
    strain_H1, time_H1, chan_dict_H1 = rl.loaddata(fn_H1, 'H1')
    strain_L1, time_L1, chan_dict_L1 = rl.loaddata(fn_L1, 'L1')
except:
    print(""Cannot find data files!"")
    print(""You can download them from https://losc.ligo.org/s/events/""+eventname)
    print(""Quitting."")
    quit()",0.4323008657,
406,data import and inspection,"class AnalyzeData:
    def __init__(self, fname):
        self.fname = fname
        self.import_data()
        self.analyze_data()
        
    def import_data(self):
        valid_extensions = {'.csv': self._import_csv,
                            '.tab': self._import_tab,
                            '.dat': self._import_dat}
        file_extension = os.path.splitext(self.fname)[-1]
        importer_function = valid_extensions[file_extension]
        importer_function()

    def _import_csv(self):
        print(""Import comma-separated data"")
        # many lines of code, perhaps with function calls
        
    def _import_tab(self):
        print(""Import tab-separated data"")
        # many lines of code, perhaps with function calls
        
    def _import_dat(self):
        print(""Import data with | delimiters (old-school)"")
        # many lines of code, perhaps with function calls
        
    def analyze_data(self):
        """"""Do some super-awesome data analysis!""""""

        
a = AnalyzeData('data.tab')
# a = AnalyzeData('data.xls')   # unknown file type, throws exception!",0.4184893966,
406,data import and inspection,"for im in imgs:
    print(im.shape)
    markers = ar_markers.detect_markers(im)
    print(len(markers))
    print(markers)
    for m in markers:
        print('Marker ID:', m.id)",0.4176024795,
406,data import and inspection,"nb_classes = 10
 
def load_dataset():
    (X_train, y_train), (X_test, y_test) = cifar10.load_data()
    print 'X_train shape:', X_train.shape
    print X_train.shape[0], 'train samples'
    print X_test.shape[0], 'test samples'
 
    Y_train = np_utils.to_categorical(y_train, nb_classes)
    Y_test = np_utils.to_categorical(y_test, nb_classes)
 
    X_train = X_train.astype('float32')
    X_test = X_test.astype('float32')
    X_train /= 255
    X_test /= 255
 
    return X_train, Y_train, X_test, Y_test

X_train, y_train, X_test, y_test = load_dataset()",0.4163478017,
406,data import and inspection,"nb_classes = 10
 
def load_dataset():
    # the data, shuffled and split between train and test sets
    (X_train, y_train), (X_test, y_test) = cifar10.load_data()
    print 'X_train shape:', X_train.shape
    print X_train.shape[0], 'train samples'
    print X_test.shape[0], 'test samples'
 
    # convert class vectors to binary class matrices
    Y_train = np_utils.to_categorical(y_train, nb_classes)
    Y_test = np_utils.to_categorical(y_test, nb_classes)
 
    X_train = X_train.astype('float32')
    X_test = X_test.astype('float32')
    X_train /= 255
    X_test /= 255
 
    return X_train, Y_train, X_test, Y_test

X_train, y_train, X_test, y_test = load_dataset()",0.4163478017,
406,data import and inspection,"procedureIDs = procedureNames = [] # The variable names must be assigned to something but are overwritten in the next call
_, procedureIDs, procedureNames = my_vf_pcie.GetProcedures(procedureIDs, procedureNames)
procedureIDs = [int(id) for id in procedureIDs] # Convert to a python list of integers
procedureNames = [str(name) for name in procedureNames] # Convert to a python list of strings
procs = zip(procedureIDs, procedureNames) # Zip the id's & names together. A dictionary may also be a useful way to store these.
for proc_id,proc_name in procs:
    print proc_id, proc_name",0.4140596092,
406,data import and inspection,"m = wasmfun.Module(
    wasmfun.ImportedFuncion('print_charcode', ['i32'], [], 'js', 'print_charcode'),
    wasmfun.Function('$main', [], [], ['i32'], instructions),
    wasmfun.MemorySection((1, 1)),
    wasmfun.DataSection(),
    )",0.4098554254,
406,data import and inspection,"if IDifTask.config.doMeasurement:
    print ""Doing measurement of dipoles""
    if len(diaSources) < IDifTask.config.maxDiaSourcesToMeasure:
         IDifTask.dipoleMeasurement.run(subtractedExposure, diaSources)
    else:
         IDifTask.measurement.run(subtractedExposure, diaSources)",0.4097483158,
406,data import and inspection,"def removeUnusedProcedures():
    global instructions, compiledProcedures
    currentInstructions = instructions[curlev]
    procedureNames = []
    temp = []
    for i in range(len(currentInstructions)):
        (l,ins,target) = currentInstructions[i]   
        if (ins == 'jal'):
            recursiveProcedureSearch(procedureNames, temp, target)
    compiledProcedures = temp",0.4094874859,
406,data import and inspection,"trainX, trainY, testX, testY = u.import_data(path_to_root)",0.4069354534,
2603,word counting,"# Complete the code 
def count_words(train):
    """"""
    Returns a tuple of tables, first is spam word counts and second is ham word counts
    
    train is a table of training data
    """"""
    # Spam
    Spam_data_table_spamonly = train.where('Spam',True)
    #
    # Apply the token function to each subject line
    Spam_data_table_token = Spam_data_table_spamonly.with_columns(""Subject"",Spam_data_table_spamonly.\
    apply(token,""Subject Line"")).drop(0).select(""Subject"",""Spam"").relabel(""Subject"",""Subject Line"")
    #
    # Get counts and frequency
    # Flatten first
    spam_words = [item for sublist in Spam_data_table_token.column(0) for ... in sublist]
    #
    # Put into a table
    spam_table = ds.Table().with_columns(""Words"",...)
    spam_word_counts = spam_table.group('Words').sort('count',descending=...).relabel('count','Spam Count')
    #
    # Now for the ham
    Ham_data_table_hamonly = train.where('Spam',...)
    Ham_data_table_token = ...
    # Get counts and frequency
    # Flatten first
    ham_words = ...
    # Put into a table
    ham_table = ...
    ham_word_counts = ...
    return spam_word_counts,ham_word_counts",0.5198257565,
2603,word counting,"def most_used_words(blob, n):
    word_counts = sorted(blob.word_counts.items(), key=lambda p: p[1], reverse=True)
    return list(filter(lambda p: p[0].lower() not in stopwords, word_counts))[:n]

for speaker, speech in speeches.items():
    print(speaker, most_used_words(speech, 10), ""\n"")",0.4996029735,
2603,word counting,"# A simple function to obtain the overall sentimate of a text chunk
# Method: tokenise the text chunk, obtain the sentiment score of each token, then take mean average.
# Note: you may need to separately install sentiwordnet: nltk.download('sentiwordnet')

## synsets based on context
## phrases/tokens 

## classify words as noun/adjectives

# unsupervised split between adjectives 

def simple_sentiment(text_chunk):
    cumulative_pos_sentiment = 0
    cumulative_neg_sentiment = 0
    index = 0
    
    # Tokenizing the sample text
    tokens=nltk.word_tokenize(text_chunk)
    # Removing words of lenght 2 or less
    tokens = [i for i in tokens if len(i)>=3]
    # remove stop words
    tokens = [word for word in tokens if word not in stopwords.words('english')]
    
    # a/n/v/r represent adjective/noun/verb/adverb respectively. They are used to index the sentinet dictionary.
    for i in tokens:
        if len(list(swn.senti_synsets(i, 'a')))>0:
            cumulative_pos_sentiment += list(swn.senti_synsets(i, 'a'))[0].pos_score()
            cumulative_neg_sentiment += list(swn.senti_synsets(i, 'a'))[0].neg_score()
            index +=1
        elif len(list(swn.senti_synsets(i, 'n')))>0:
            cumulative_pos_sentiment += list(swn.senti_synsets(i, 'n'))[0].pos_score()
            cumulative_neg_sentiment += list(swn.senti_synsets(i, 'n'))[0].neg_score()
            index +=1
        elif len(list(swn.senti_synsets(i, 'v')))>0:
            cumulative_pos_sentiment += list(swn.senti_synsets(i, 'v'))[0].pos_score()
            cumulative_neg_sentiment += list(swn.senti_synsets(i, 'v'))[0].neg_score()
            index +=1
        elif len(list(swn.senti_synsets(i, 'r')))>0:
            cumulative_pos_sentiment += list(swn.senti_synsets(i, 'r'))[0].pos_score()
            cumulative_neg_sentiment += list(swn.senti_synsets(i, 'r'))[0].neg_score()
            index +=1
        
    avg_pos_sentiment = cumulative_pos_sentiment / float(index)
    avg_neg_sentiment = cumulative_neg_sentiment / float(index)
    
#     print('Positive sentiment:',avg_pos_sentiment)
#     print('Negative sentiment:',avg_neg_sentiment)
    
    return (avg_pos_sentiment,avg_neg_sentiment)",0.4920060039,
2603,word counting,"print 'shared words %d\neng1000 only %d\ncorpus only %d' % (len(set(eng1000.vocab).intersection(set(corpus_vocab))),
                                                            len(set(corpus_vocab) - set(eng1000.vocab)),
                                                            len(set(eng1000.vocab) - set(corpus_vocab)))",0.4857570231,
2603,word counting,"def isWinner(board, letter):
    isWinner.win = 0

# Check if either player has won by having all his/her letters across a row horizontally
    for i in range(size):
        if board[i].count(letter) == size:
            isWinner.win = 1

        else:
           None

# Check if either player has won by having all his/her letters across a column vertically
    if isWinner.win != 1:
        for ii in range(size):
            cols = []
                       
            for i in range(size):
                if board[i][ii] == letter:
                    cols.append(1)
                else:
                    None
                if sum(cols) == size:
                    isWinner.win = 1
                    break
            else:
                None
    else:
        None

# Check if either player has won by having all his/her letters across the diagonal lines
# First, check the diagonal line from upper left corner to botton right corner
    if isWinner.win !=1:
        diag1 = []
        for i in range(size):
            if board[i][i] == letter:
                diag1.append(1)
            else:
                None
        if sum(diag1) == size:
            isWinner.win = 1
        else:
            None
    else:
        None

# Then, check the diagonal line from upper right corner to bottom left corner
    if isWinner.win !=1:
        diag2 = []
        for i in range(size):
            if board[i][size-1-i] == letter:
                diag2.append(1)
            else:
                None
        if sum(diag2) == size:
                isWinner.win = 1
        else:
            None
    else:
        None
    
    if isWinner.win == 1:
        return True",0.4789253175,
2603,word counting,"%%time
# pass function into load_balanced view
from glob import glob
from collections import Counter
from tqdm import tqdm

def word_counter(file_name):
    from collections import Counter # can pass in all imports
    counter = Counter()
    with open(file_name) as f:
        for line in f:
            counter.update(line.lower().split())
        return counter

num_pieces = 5
!split --number=l/{num_pieces} encyclopedia_britannica.txt temp_file.

async = load_balanced_view.map(word_counter, 
    glob(""/Users/Eugene/Desktop/Repos/ipyparallel-vs-MRJob/temp_file*""),
    ordered=False, chunksize=1)

global_counter1 = Counter() 
for engine_result in tqdm(async):
    global_counter1.update(engine_result)
    
!rm temp_file*",0.4768536687,
2603,word counting,"from string import punctuation, ascii_letters

reference_chars = punctuation + ascii_letters

def char_counts(text):
    ""Function returning relative character counts for alphanumeric characters and punctuation marks.""
    
    text = text.lower()
    # First count the characters
    
    char_counts = # YOUR CODE HERE
    
    # Then determine the total amount of characters
    total_chars = # YOUR CODE HERE
    
    relative_values = []
    for char in reference_chars:
        # YOUR CODE HERE
    
    # And return the relative values.
    return relative_values",0.4757063985,
2603,word counting,"def tf(word, blob):
    return blob.words.count(word) / len(blob.words)",0.4728147686,
2603,word counting,"for fileid in nltk.corpus.state_union.fileids():
    cnt = 0
    for word in nltk.corpus.state_union.words(fileid):
        if word.isalpha() and gematria(word.lower()) == 666:
            cnt += 1
    print(fileid, cnt)",0.472386986,
2603,word counting,"def count_occurences(phrase, sublist):
    counter = 0
    for sub in sublist:
        counter += phrase.count(sub)
    return counter",0.4697815776,
680,for chapter strings,"# Define the class book which takes the title of the book, name of the author and Genre
class book:
    def __init__(self,title,author,genre):
        self.title = title
        self.author = author
        self.genre = genre
#returns the good reads results of the book 

    def goodread_results(self):
        return f""https://www.goodreads.com/search?q={self.title}""",0.4184272587,
680,for chapter strings,"def simple_interactions(table):
    ''' 
    Collect characters with context where they act (NOT PAIR LIST!)
    >>> simple_interactions(table)
    >>> {0: [[17160, 17169, 1184], [17160, 17169, 1184]],
         1: [[1169, 1224, 71],
          [1169, 1224, 71],
          [1242, 1245, 75],
          [1246, 1249, 76],
          [1250, 1266, 77], ...}
    '''
    character_list = dict()
    for i in table.index:
        if table.CharacterID.loc[i] is not None:
            sent_numb = table.loc[i, 'SentenceID']
            character_list.setdefault(table.CharacterID.loc[i], []).append(
                                                [table[table.SentenceID == sent_numb].index[0], 
                                                  table[table.SentenceID == sent_numb].index[-1], 
                                                     table.SentenceID.loc[i]])
    return character_list",0.4182744622,
680,for chapter strings,"def title_mapping(x):
    #if x in set(['Capt', 'Col', 'Don', 'Major', 'Rev', 'Sir', 'Jonkheer']):
    if x.Title in set(['Don', 'Rev', 'Sir', 'Jonkheer']):
        return 'Mr'
    elif x.Title in set(['Lady', 'the Countess']):
        return 'Mrs'
    elif x.Title in set(['Mlle', 'Mme', 'Dona', 'Ms']):
        return ""Miss""
    elif x.Title in set(['Major', 'Col', 'Capt']):
        return ""Officer""
    elif x.Title == 'Dr' and x.Sex == 'female':
        return 'Mrs'
    elif x.Title == 'Dr' and x.Sex == 'male':
        return 'Mr'
    else:
        return x.Title

union.Title = union.apply(title_mapping, axis=1)
union.Title.unique()",0.4171380997,
680,for chapter strings,"def name_to_idx(name):
    ''' From an authors, return the list of contributions '''
    contrib = [f for f in sources if name in f.authors.keys()]
    return [sources.index(elt) for elt in contrib]
    
my_contrib = name_to_idx('Clement Thorey')",0.4139605761,
680,for chapter strings,"def lower_text(self):
    """"""
    Converts the text of all imported files to lowercase.
    """"""
    self.lower_corpus_list = []

    for char in self.unset_apostrophe_list:
        lower_character = char.lower()
        self.lower_corpus_list.append(lower_character)

    return self.lower_corpus_list",0.4127620459,
680,for chapter strings,"class Book:
    def __init__(self, title, author):
        self.title = title
        self.author = author
        self.chapters = []",0.4113715291,
680,for chapter strings,"class Book:         
    def __init__(self, title, author): # <-- this is a method
        self.title = title # <-- this is an atribute
        self.author = author # <-- so is this
        self.state = ""shelf""",0.4113715291,
680,for chapter strings,"# here is the whole chapter in a couple functions that don't work

def book_info(td):
    title = td.find('div', 'thumbheader').a.text
    by_author = td.find('div', 'AuthorName').text
    authors = [x.strip() for x in re.sub(""^By "", """", by_author).split("","")]
    isbn = re.match(""/product/(.*)\.do,"" isbn_link).groups()[0]
    date = td.find(""span"", ""directorydate"").text.strip()
    
    return {
        'title' : title,
        'authors' : authors,
        'isbn' : isbn,
        'date' : date
    }

from bs4 import BeautifulSoup
import requests
from time import sleep
base_url = 'http://shop.oreilly.com/category/browse-subjects/data.do?sortby=publicationDate&page='

books = []
NUM_PAGES = 31 # some number that was treu at the time

for page_num in range(1, NUM_PAGES + 1):
    print('souping page', page_num, "","", len(books), ""found so far"")
    
    url = base_url + str(page_num)
    soup = BeautifulSoup(requests.get(url).text, 'html5lib')
    
    for td in soup('td', 'thumbtext'):
        if not is_video(td):
            books.append(book_info(td)) # i never wrote this function so this is just a placeholder
    
    sleep(30) # respecting the robots.txt
                
        
# there are many ways to scrape data and it is a bit of an art. He just goes on to plot the number of books which I'll skip",0.4104747474,
680,for chapter strings,"def extract_region_1(row):
    if row.region_1 == 'nan':
        return row.region_1
    if not row.title.endswith(')'):
        return None
    return row.title[row.title.rindex('(')+1:-1]

df.region_1 = df.apply(extract_region_1, axis=1)",0.4102567732,
680,for chapter strings,"def get_title(soup):
    """"""Takes a BeautifulSoup object of an individual game page and returns the
    cleaned up title of the game as a string""""""
    pagetitle = soup.title.text
    clean_game_title = re.sub(r'\ for .*MobyGames', '', pagetitle)
    return clean_game_title",0.4071882367,
302,convert categorical variables,"def optimize_memory(df):
    for int8 in []:
        #Byte (-128 to 127)
        df[int8] = df[int8].astype('int8') 
    for int16 in ['id_game','number_of_bets']:
        #Integer (-32768 to 32767)
        df[int16] = df[int16].astype('int16')
    for int32 in []:
        #Integer (-2147483648 to 2147483647)
        df[int32] = df[int32].astype('int32')
    for float16 in []:
        #Half precision float: sign bit, 5 bits exponent, 10 bits mantissa
        df[float16] = df[float16].astype('float16')
    for float32 in []:
        #Single precision float: sign bit, 8 bits exponent, 23 bits mantissa
        df[float32] = df[float32].astype('float32')
    return df

fact_bets_df = optimize_memory(fact_bets_df)
fact_bets_df.info()",0.5371852517,
302,convert categorical variables,"def clean_memberships(df, col):
    '''Cleans memberships by consolidating and converting to categories
    INPUT: df - pandas Dataframe containing membership columns
           col - column name to be consolidated
    RETURNS: pandas DataFrame with consolidated memberships'''
    
    # Replace the various memberships with their groupings
    df[col] = df[col].astype(str)
    df[col] = df[col].replace(['RideScout Single Ride','Try Before You Buy Special'], value='single')
    df[col] = df[col].replace(['^24-Hour.*$', '^Explorer.*$', '^Walk Up$'], value='day', regex=True)
    df[col] = df[col].replace(['^ACL Weekend Pass.*$', '^FunFunFun.*$', '^Weekender.*$'], value='weekend', regex=True)
    df[col] = df[col].replace(['^7-Day.*$'], value='week', regex=True)
    df[col] = df[col].replace(['^Local30.*$'], value='month', regex=True)
    df[col] = df[col].replace(['^Semester.*$'], value='semester', regex=True)
    df[col] = df[col].replace(['^Annual.*$', '^Local365.*$', 'Republic Rider.*$', '^Membership: pay once  one-year.*$'], value='year', regex=True)
    df[col] = df[col].replace(['^Founding Member.*$', '^.*Founder.*$'], value='triannual', regex=True)

    # Drop the remaining trips (PROHIBITED and RESTRICTED)
    drop_mask = (df['membership'] == 'PROHIBITED') | (df['membership'] == 'RESTRICTED')
    df = df[~drop_mask]
    
    # Finally convert to categorical
    df[col] = df[col].astype('category')
    return df
    
trip_df = clean_memberships(trip_df, 'membership')

print(trip_df.groupby('membership').size())
print(trip_df.info())",0.5336723328,
302,convert categorical variables,"for field_id, field in {'CHANNEL_CODE' : 'CHANNEL_NAME', 'SUB_CHANNEL_CODE' : 'SUB_CHANNEL_NAME',
                        'ELEMENT_CODE' : 'ELEMENT_NAME', 'SUB_ELEMENT_CODE' : 'SUB_ELEMENT_NAME'}.items():
    df[field] = df[field].astype('category')
    df[field_id] = df[field].cat.codes",0.5313927531,
302,convert categorical variables,"for df in combine_lst:
    df['GarageYrBlt'] = df['GarageYrBlt'].astype('str')
    print(df['GarageYrBlt'].describe())
    df['YrSold'] = df['YrSold'].astype('str')
    print(df['YrSold'].describe())
combine_df = pd.concat(combine_lst)",0.5295549035,
302,convert categorical variables,"def modify_mosold(df):
    """""" Turn MoSold feature into a string""""""
    
    df[""MoSold""] = df[""MoSold""].astype(str)
    return df",0.5288745165,
302,convert categorical variables,"for col in [""Interest.Rate"",""Debt.To.Income.Ratio""]:
    ld[col]=ld[col].astype(""str"")
    ld[col]=[x.replace(""%"","""") for x in ld[col]]",0.5274697542,
302,convert categorical variables,"# *****test code*******
# # column ' performance_rating' needs to be change to catagory type
# # column ' tenure_yr' needs to be change to interger type

# df['performance_rating'] = df['performance_rating'].astype('category')
# df['sex'] = df['sex'].astype('category')

# # check new data types
# df.dtypes
# **** test code end******





# create function to convert columns to category type
def change_data_type(columns):
#     df = df
    for column in columns:
        df[column] = df[column].astype('category')
    return df.dtypes
        
    
columns = ['employee_id', 'dept_id', 'full_part', 'reg_temp', 'age_bracket', 'edu_lvl', 'flsa_stat', 
           'ethnic_group', 'sex', 'performance_rating']   

change_data_type(columns)",0.5263835788,
302,convert categorical variables,"def change_data_type(columns):
#     df = df
    for column in columns:
        df[column] = df[column].astype('int64')
    return df.dtypes
        
    
columns = ['salary', 'tenure_yr']   

change_data_type(columns)",0.5263835788,
302,convert categorical variables,"def moneyLaundering(df, c):
    newCol = df[c].astype(str).apply(lambda x: (x.replace('$', '')))
    newCol = newCol.astype(str).apply(lambda x: (x.replace(',', '')))
    newCol = newCol.astype(str).apply(lambda x: (x.replace('nan', '0')))
    df = df.drop([c], axis=1)
    df[c] = pd.to_numeric(newCol.values, errors='raise')
    return df

df = moneyLaundering(df, 'cleaning_fee')
df = moneyLaundering(df, 'security_deposit')
df = moneyLaundering(df, 'price')
df = moneyLaundering(df, 'extra_people')

df['host_is_superhost'] = df['host_is_superhost'].str.contains('t', regex=False)
df.host_is_superhost = df.host_is_superhost.apply(lambda x: x*1)
# Use ""has secutiry_deposit"" instead of security_deposit
# df['security_deposit'] = np.where(df['security_deposit']>0, True, False)",0.5242682695,
302,convert categorical variables,"for field_id, field in {'LOCALITY_CODE' : 'LOCALITY_NAME', 'SLOCALITY_CODE' : 'SLOCALITY_NAME'}.items():
    df[field] = df[field].astype('category')
    df[field_id] = df[field].cat.codes",0.522039175,
143,building the network,"def compile_network(self) : 
    """""" Compile the network  
    """"""
    if self.network is not None : 
        return

    self.network = theano.function(inputs = [self.i],
                                   outputs = self.network_outputs)

# bind the method to the class
embedding_model.compile_network = compile_network

def compile_loss(self) : 
    """""" Compile the loss
    """"""

    if self.loss is not None : 
        return

    self.loss = theano.function(inputs = [self.I,self.J],
                                outputs = self.loss_outputs,
                                updates = self.loss_updates)

# bind the method to the class
embedding_model.compile_loss = compile_loss
        
def compile_sgd_update(self) : 
    """""" Compile SGD update function
    """"""

    if self.sgd_update is not None : 
        return

    self.sgd_update = theano.function(inputs = [self.I,self.J,
                                                self.eta,self.sgd_i],
                                      outputs = [],
                                      updates = self.sgd_updates)

# bind the method to the class
embedding_model.compile_sgd_update = compile_sgd_update",0.5675191879,
143,building the network,"def _compute_network_model(self) : 
    """""" Build the network, loss, grad_loss and sgd_update theano functions.
        More work than is strictly nessecary is done here as the only thing
        that is really needed in order to run sgd (stochastic gradient 
        descent) is the sgd_update function. The network, loss and grad_loss
        functions are compiled since this is experimental code.
    """"""

    # build the network
    self.i = T.vector('i',dtype = self.int_dtype)

    self.network_outputs = compute_network_outputs(self.i,self.s0,self.V,
                                                   self.U,self.W,self.b)


    # build mean log likelyhood loss

    # variables for a batch of sentences
    self.I = T.matrix('I',dtype = self.int_dtype)
    self.J = T.matrix('J',dtype = self.int_dtype) # for embedding I = J

    self.loss_outputs = compute_mean_log_lklyhd_outputs(self.I,self.J,
                                                        self.s0,self.V,
                                                        self.U,self.W,
                                                        self.b)

    # set up the accumulator for computing the loss in batches

    n_minibatch = T.cast(self.I.shape[0],self.float_dtype)
    loss_accum_ipnm = self.loss_accum_i + n_minibatch

    self.loss_updates = ((self.loss_accum,
                          (self.loss_outputs*n_minibatch/loss_accum_ipnm
                           + (self.loss_accum 
                             * self.loss_accum_i/loss_accum_ipnm))),
                         (self.loss_accum_i,loss_accum_ipnm))

    # get the gradient of the loss

    (self.dV,
     self.dU,
     self.dW,
     self.db) = theano.grad(self.loss_outputs,
                            [self.V,self.U,self.W,self.b])

    # get the gradient magnitudes

    self.dV_mag = T.sqrt(T.sum(self.dV*self.dV))
    self.dU_mag = T.sqrt(T.sum(self.dU*self.dU))
    self.dW_mag = T.sqrt(T.sum(self.dW*self.dW))
    self.db_mag = T.sqrt(T.sum(self.db*self.db))

    # get the sgd update function

    # this is the learning parameter
    self.eta = T.scalar('eta',dtype = self.float_dtype)

    # also including a running average of the gradient magnitudes

    self.sgd_i = T.scalar('sgd_i',dtype = self.float_dtype)

    dV_mag_accum = (self.dV_mag/(self.sgd_i+1.)
                        + self.m_dV_mag*(self.sgd_i/(self.sgd_i+1.)))
    dU_mag_accum = (self.dU_mag/(self.sgd_i+1.) 
                        + self.m_dU_mag*(self.sgd_i/(self.sgd_i+1.)))
    dW_mag_accum = (self.dW_mag/(self.sgd_i+1.) 
                        + self.m_dW_mag*(self.sgd_i/(self.sgd_i+1.)))
    db_mag_accum = (self.db_mag/(self.sgd_i+1.) 
                        + self.m_db_mag*(self.sgd_i/(self.sgd_i+1.)))

    # adding here since we are taking a max of the loss - accumulators
    # do not include the latest values
    self.sgd_updates = ((self.V,self.V + self.eta*self.dV),
                        (self.U,self.U + self.eta*self.dU),
                        (self.W,self.W + self.eta*self.dW),
                        (self.b,self.b + self.eta*self.db),
                        (self.m_dV_mag,dV_mag_accum),
                        (self.m_dU_mag,dU_mag_accum),
                        (self.m_dW_mag,dW_mag_accum),
                        (self.m_db_mag,db_mag_accum))

    # pointers for the compiled functions
    self.network = None
    self.loss = None
    self.grad_loss = None
    self.sgd_update = None
    self.sgd_update_w_loss = None

# bind the method to the class - this is just so the class definition
# can be broken up across several Jupyter notebook cells
embedding_model._compute_network_model = _compute_network_model",0.5569583178,
143,building the network,"class Network(object):
    
    # Initializing method
    def __init__(self, sizes):
        self.num_layers = len(sizes) # getting number of layers
        self.sizes = sizes # number of neurons in each layers
        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]  # assign bias
        self.weights = [np.random.randn(y, x) 
                        for x, y in zip(sizes[:-1], sizes[1:])] # assign random weights
    
    # Method for feedforward
    def feedforward(self, a):
        """"""Return the output of the network if ``a`` is input.""""""
        for b, w in zip(self.biases, self.weights):
            a = sigmoid(np.dot(w, a)+b)
        return a

    #stochastic gradient descent
    def SGD(self, training_data, epochs, mini_batch_size, eta,
            test_data=None):
        if test_data: # If test data is present then assign n_test with the size of test data
            n_test = len(test_data)
        n = len(training_data)
        for j in range(epochs):
            random.shuffle(training_data) # shuffle the training data
            mini_batches = [training_data [k: k + mini_batch_size] #taking samples from the training data if size mini_batch size
                for k in range(0, n, mini_batch_size)]
            for mini_batch in mini_batches:
#updates the network weights and biases according to a single iteration of gradient descent, using just the training data in mini_batch
                self.update_mini_batch(mini_batch, eta) #eta is the learning rate
            if test_data:                                  # test data present then evaluate the accuracy
                print (""Epoch {0}: {1} / {2}"".format(
                    j, self.evaluate(test_data), n_test))
            else:
                print (""Epoch {0} complete"".format(j))
                
    # update the mini batches created above
    """"""Update the network's weights and biases by applying
    gradient descent using backpropagation to a single mini batch.
    The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``
    is the learning rate.""""""
    def update_mini_batch(self, mini_batch, eta):
        nabla_b = [np.zeros(b.shape) for b in self.biases]
        nabla_w = [np.zeros(w.shape) for w in self.weights]
        for x, y in mini_batch:
            delta_nabla_b, delta_nabla_w = self.backprop(x, y)
            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]
            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]
        self.weights = [w-(eta/len(mini_batch))*nw 
                        for w, nw in zip(self.weights, nabla_w)]
        self.biases = [b-(eta/len(mini_batch))*nb 
                       for b, nb in zip(self.biases, nabla_b)]
        
    def backprop(self, x, y):
        """"""Return a tuple ``(nabla_b, nabla_w)`` representing the
        gradient for the cost function C_x.  ``nabla_b`` and
        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar
        to ``self.biases`` and ``self.weights``.""""""
        nabla_b = [np.zeros(b.shape) for b in self.biases]
        nabla_w = [np.zeros(w.shape) for w in self.weights]
        # feedforward
        activation = x
        activations = [x] # list to store all the activations, layer by layer
        zs = [] # list to store all the z vectors, layer by layer
        for b, w in zip(self.biases, self.weights):
            z = np.dot(w, activation)+b
            zs.append(z)
            activation = sigmoid(z)
            activations.append(activation)
        # backward pass
        delta = self.cost_derivative(activations[-1], y) * \
            sigmoid_prime(zs[-1])
        nabla_b[-1] = delta
        nabla_w[-1] = np.dot(delta, activations[-2].transpose())
        # Note that the variable l in the loop below is used a little
        # differently to the notation in Chapter 2 of the book.  Here,
        # l = 1 means the last layer of neurons, l = 2 is the
        # second-last layer, and so on.  It's a renumbering of the
        # scheme in the book, used here to take advantage of the fact
        # that Python can use negative indices in lists.
        for l in range(2, self.num_layers):
            z = zs[-l]
            sp = sigmoid_prime(z)
            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp
            nabla_b[-l] = delta
            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())
        return (nabla_b, nabla_w)

    def evaluate(self, test_data):
        """"""Return the number of test inputs for which the neural
        network outputs the correct result. Note that the neural
        network's output is assumed to be the index of whichever
        neuron in the final layer has the highest activation.""""""
        test_results = [(np.argmax(self.feedforward(x)), y)
                        for (x, y) in test_data]
        return sum(int(x == y) for (x, y) in test_results)

    def cost_derivative(self, output_activations, y):
        """"""Return the vector of partial derivatives \partial C_x /
        \partial a for the output activations.""""""
        return (output_activations-y)
    
# Derivative of sigmoid function
def sigmoid_prime(z):
        return sigmoid(z)*(1-sigmoid(z))
    
# Sigmoid used as activation function function
def sigmoid(z):
        """"""The sigmoid function.""""""
        return 1.0/(1.0+np.exp(-z))",0.5479130149,
143,building the network,"class nnetwork():
    
    def __init__(self, layers )
        self.nlayers = len(layers)
        self.inlayer = layers[0]
        self.outlayer = layers[-1]
        for ii in range(1,len(layers)-1):",0.5428881645,
143,building the network,"def create_example_net():
    # An empty network object is created, that will be filled in the next steps.
    net = pp.create_empty_network()

    #create slack nodes
    hvb = pp.create_buses(net, 2, vn_kv=110.)
    pp.create_ext_grid(net, hvb[0], s_sc_min_mva=2500, rx_min=0.3)
    pp.create_ext_grid(net, hvb[1], s_sc_min_mva=3000, rx_min=0.2)

    #create a first bus
    lb = pp.create_bus(net, vn_kv=10.)
    # we start with the transformer at the first high voltage bus
    pp.create_transformer(net, hvb[0], lb, std_type=""40 MVA 110/10 kV"", df=0.1)
    # define the standard types for this network
    std_types = [""70-AL1/11-ST1A 10.0"", ""NA2XS2Y 1x240 RM/25 6/10 kV""]
    # create several lines and busses below the first transformer
    for length, std_type in zip([1.8, 2.4, 3.6, 4.8, 4.8, 1.2], [0, 1, 0, 1, 1, 1]):
        lb = create_bus_with_line(net, lb, length_km=length, std_type=std_types[std_type])
    lb = 5
    # create several lines and busses below bus 5
    for length, std_type in zip([1.2, 3., 4.8, 3.6, 1.2, 0.6], [1, 1, 1, 0, 1, 0]):
        lb = create_bus_with_line(net, lb, length_km=length, std_type=std_types[std_type])
    # we create another the transformer (this time a 3w transformer) at the other high voltage bus
    # and connect it to bus 8
    pp.create_transformer3w(net, hvb[1], 8, lb, ""63/25/38 MVA 110/10/10 kV"")
    net.line.loc[net.line.index[net.line.type == ""ol""], ""endtemp_degree""] = 100 #overhead lines
    net.line.loc[net.line.index[net.line.type == ""cs""], ""endtemp_degree""] = 20 #cables

    #add constraints
    net.line[""max_loading_percent""] = 50
    net.trafo[""max_loading_percent""] = 50
    net.bus[""max_vm_pu""] = 1.03
    net.bus[""min_vm_pu""] = 0.97

    #create load and generation elements
    for bus in [3, 6, 10, 13]:
        pp.create_sgen(net, bus, p_kw=-1280, sn_kva=1280, k=1.2)
    for bus in [3, 4, 5, 6, 7, 9, 10, 11, 12, 13]:
        pp.create_load(net, bus, p_kw=3.5e3)

    # create a valid original switching state
    net.switch.closed.loc[[6, 14]] = False
    
    return net",0.5372663736,
143,building the network,"class Network():
    def __init__(self):
        self._model = models.Sequential()
        self._loss = np.ones(500)
        self._mse = np.ones(500)
    def _compile(self):
        self._model.add(layers.Dense(1))
        self._model.compile(optimizer='rmsprop', loss='mse', metrics=['mse'])        
    def baseline(self):
        self._model.add(layers.Dense(64, activation='relu', input_shape=(train_data.shape[1],)))
        self._model.add(layers.Dense(64, activation='relu'))
        self._compile()
    def dropout(self):
        self._model.add(layers.Dense(64, activation='relu', input_shape=(train_data.shape[1],)))
        self._model.add(layers.Dropout(0.5))
        self._model.add(layers.Dense(64, activation='relu'))
        self._model.add(layers.Dropout(0.5))
        self._compile()
    def l1(self):
        self._model.add(layers.Dense(64, kernel_regularizer=l1(0.001), activation='relu', input_shape=(train_data.shape[1],)))
        self._model.add(layers.Dense(64, kernel_regularizer=l1(0.001), activation='relu'))
        self._compile()
    def l2(self):
        self._model.add(layers.Dense(64, kernel_regularizer=l2(0.001), activation='relu', input_shape=(train_data.shape[1],)))
        self._model.add(layers.Dense(64, kernel_regularizer=l2(0.001), activation='relu'))
        self._compile()    
    def layer3(self):
        self._model.add(layers.Dense(64, activation='relu', input_shape=(train_data.shape[1],)))
        self._model.add(layers.Dense(64, activation='relu'))
        self._model.add(layers.Dense(64, activation='relu'))
        self._compile()
    def layer2b(self):
        self._model.add(layers.Dense(200, activation='relu', input_shape=(train_data.shape[1],)))
        self._model.add(layers.Dense(200, activation='relu'))
        self._compile()        
    def kfold_validate(self):
        k = 10
        num_val_samples = len(train_data) // k
        num_epochs = 500
        all_mse_histories = []
        for i in range(k):
            print('processing fold #', i)
            val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]
            val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]
            partial_train_data = np.concatenate(
                [train_data[:i * num_val_samples],
                 train_data[(i + 1) * num_val_samples:]],
                axis=0)
            partial_train_targets = np.concatenate(
                [train_targets[:i * num_val_samples],
                 train_targets[(i + 1) * num_val_samples:]],
                axis=0)

            model = self._model
            history = model.fit(partial_train_data, partial_train_targets,
                                validation_data=(val_data, val_targets),
                                epochs=num_epochs, batch_size=128, verbose=0)
            mse_history = history.history['mean_squared_error']
            all_mse_histories.append(mse_history)
        self._histories = [np.mean([x[i] for x in all_mse_histories]) for i in range(num_epochs)]
    def history(self):
        return self._loss[1:,].mean(axis=0), self._mse[1:,].mean(axis=0)
    def scores(self):
        return self._histories
    @property
    def model(self):
        return self._model",0.5357052684,
143,building the network,"class Network():
    def __init__(self, inputLayer, hiddenLayers, outputLayer):
        self.layers = []
        self.x = inputLayer.x
        
        inputLayer.link()
        self.layers.append(inputLayer)

        curx = inputLayer.h
        for hl in hiddenLayers:
            hl.link(curx)
            self.layers.append(hl)
            curx = hl.h
            print curx

        outputLayer.link(curx)
        self.layers.append(outputLayer)
        self.y = outputLayer.h
    
    def setupTrainingForClassification(self, costFunction=""cross-entropy"", optimizer=""Adam""):
        self.target = tf.placeholder(tf.float32, self.y.get_shape(), ""Target"")
        if costFunction == ""cross-entropy"":
            self.cost = -tf.reduce_sum(self.target*tf.log(self.y))
        elif costFunction == ""squared-diff"":
            self.cost = tf.reduce_mean(tf.square(self.target-self.y))
        
        if optimizer == ""Adam"":
            self.trainingStep = tf.train.AdamOptimizer(1e-4).minimize(self.cost)
        elif optimizer == ""GradientDescent"":
            self.trainingStep = tf.train.GradientDescentOptimizer(0.01).minimize(self.cost)
        self.correct_prediction = tf.equal(tf.argmax(self.y,1), tf.argmax(self.target,1))
        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, ""float""))
    
    def train(self, x, y):
        self.trainingStep.run(feed_dict={self.x: x, self.target: y})
    
    def evaluate(self, x, y):
        return self.accuracy.eval(feed_dict={self.x: x, self.target: y})
    
    def predict(self, x):
        return self.y.eval(feed_dict={self.x: x})",0.5297859311,
143,building the network,"def flow_with_demands(G):
    global a
    
    G.add_nodes_from(['super_s', 'super_t'])
    G.node['super_s']['demand']=0
    G.node['super_t']['demand']=0
    
    
    for state in G.nodes():
        if G.node[state]['demand'] > 0:
            G.add_edge(state, 'super_t')
            G.edge[state]['super_t']['capacity'] = G.node[state]['demand']
        elif G.node[state]['demand'] < 0:
            G.add_edge('super_s', state)
            G.edge['super_s'][state]['capacity'] = abs(G.node[state]['demand'])
            
    check=[]
    chksum=0
    for state in G.nodes():
        check= (G.node[state]['demand'])
        demandsum = dict([(state, check)])
        chksum+=(demandsum[state])
    
    if chksum !=0 :
        print ""NetworkX unfeasible, flow cannot exist""
            
    import networkx as nx
    from networkx.algorithms.flow import ford_fulkerson

    a= ford_fulkerson(G, 'super_s','super_t')
    #print a.graph['flow_value']
    #print a.graph['flow_dict']

    def flow(state1,state2):
         
             return a.graph['flow_dict'][state1][state2]
        
    
    
    pass",0.5288963914,
143,building the network,"class Network:
    def __init__(self, sizes):
        """"""
        Initialize the neural network 
        
        :param sizes: a list of the number of neurons in each layer 
        """"""
        # save the number of layers in the network 
        self.L = len(sizes) 
        
        # store the list of layer sizes 
        self.sizes = sizes  
        
        # initialize the bias vectors for each hidden and output layer 
        self.b = [np.random.randn(n) for n in self.sizes[1:]]
        
        # initialize the matrices of weights for each hidden and output layer 
        self.W = [np.random.randn(n, m) for (m,n) in zip(self.sizes[:-1], self.sizes[1:])]
        
        # initialize the derivatives of biases for backprop 
        self.db = [np.zeros(n) for n in self.sizes[1:]]
        
        # initialize the derivatives of weights for backprop 
        self.dW = [np.zeros((n, m)) for (m,n) in zip(self.sizes[:-1], self.sizes[1:])]
        
        # initialize the activities on each hidden and output layer 
        self.z = [np.zeros(n) for n in self.sizes]
        
        # initialize the activations on each hidden and output layer 
        self.a = [np.zeros(n) for n in self.sizes]
        
        # initialize the deltas on each hidden and output layer 
        self.delta = [np.zeros(n) for n in self.sizes]
        
    def g(self, z):
        """"""
        sigmoid activation function 
        
        :param z: vector of activities to apply activation to 
        """"""
        return 1.0/(1.0 + np.exp(-z))
    
    def g_prime(self, z):
        """"""
        derivative of sigmoid activation function 
        
        :param z: vector of activities to apply derivative of activation to 
        """"""
        return self.g(z) * (1.0 - self.g(z))
    
    def gradC(self, a, y):
        """"""
        evaluate gradient of cost function for squared-loss C(a,y) = (a-y)^2/2 
        
        :param a: activations on output layer 
        :param y: vector-encoded label 
        """"""
        return (a - y)
    
    def forward_prop(self, x):
        """"""
        take an feature vector and propagate through network 
        
        :param x: input feature vector 
        """"""
        
        # Initialize activation on initial layer to x 
        # TODO 
        
        # Loop over layers and compute activities and activations 
        # TODO 
            
        self.a = self.a
            
    def back_prop(self, x, y):
        """"""
        Back propagation to get derivatives of C wrt weights and biases for given training example
        
        :param x: training features  
        :param y: vector-encoded label 
        """"""
        
        # forward prop training example to fill in activities and activations 
        # TODO 
        
        # compute deltas on output layer 
        # TODO 
        
        # loop backward through layers, backprop deltas, compute dWs and dbs
        # TODO 
        for ll in range(self.L-2, -1, -1):
            pass 
            
            
    def train(self, X_train, y_train, X_valid=None, y_valid=None, eta=0.25, num_epochs=10, isPrint=True, isVis=False):
        """"""
        Train the network with SGD 
        
        :param X_train: matrix of training features 
        :param y_train: matrix of vector-encoded labels 
        """"""
        
        # initialize shuffled indices 
        shuffled_inds = list(range(X_train.shape[0]))
        
        # loop over training epochs 
        for ep in range(num_epochs):
            
            # shuffle indices 
            np.random.shuffle(shuffled_inds)
            
            # loop over training examples 
            for ind in shuffled_inds:
                
                # back prop to get derivatives 
                # TODO 
                
                # update weights and biases 
                self.W = self.W #TODO 
                self.b = self.b #TODO 
                
            # print mean loss every 10 epochs if requested 
            if isPrint and (ep%25)==0:
                print(""epoch {:3d}/{:3d}: "".format(ep,num_epochs),end="""")
                print(""  train loss: {:8.3f}"".format(self.compute_loss(X_train, y_train)), end="""")
                if X_valid is not None:
                    print(""  validation loss: {:8.3f}"".format(self.compute_loss(X_valid, y_valid)))
                else:
                    print("""")
                    
            if isVis and (ep%50)==0:
                self.pretty_pictures(X_train, y_train, decision_boundary=True, epoch=ep)
                    
    def compute_loss(self, X, y):
        """"""
        compute average loss for given data set 
        
        :param X: matrix of features 
        :param y: matrix of vector-encoded labels 
        """"""
        loss = 0 
        for x, y in zip(X, y):
            nn.forward_prop(x)
            loss += 0.5 * np.linalg.norm(nn.a[-1]-y)**2
        return loss 
            
    def pretty_pictures(self, X, y, decision_boundary=False, epoch=None):
        """"""
        Function to plot data and neural net decision boundary
        
        :param X: matrix of features 
        :param y: matrix of vector-encoded labels 
        :param decision_boundary: whether or not to plot decision 
        :param epoch: epoch number for printing 
        """"""
        
        mycolors = {""blue"": ""steelblue"", ""red"": ""#a76c6e""}
        colorlist = [c for (n,c) in mycolors.items()]
        colors = [colorlist[np.argmax(yk)] for yk in y]

        fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,8))
        
        if decision_boundary:
            xx, yy = np.meshgrid(np.linspace(-1.25,1.25,300), np.linspace(-1.25,1.25,300))
            grid = np.column_stack((xx.ravel(), yy.ravel()))
            grid_pred = np.zeros_like(grid[:,0])
            for ii in range(len(grid_pred)):
                self.forward_prop(grid[ii,:])
                grid_pred[ii] = np.argmax(self.a[-1])
            grid_pred = grid_pred.reshape(xx.shape)
            cmap = ListedColormap([
                colorConverter.to_rgba('steelblue', alpha=0.30),
                colorConverter.to_rgba('#a76c63', alpha=0.30)])
            plt.contourf(xx, yy, grid_pred, cmap=cmap)
            if epoch is not None: plt.text(-1.23,1.15, ""epoch = {:d}"".format(epoch), fontsize=16)

        plt.scatter(X[:,0], X[:,1], color=colors, s=100, alpha=0.9)
        plt.axis('off')",0.528321147,
143,building the network,"class Network(object):
    def __init__(self, nInput):
        self.layers = [Layer(nInput)]
        self.nLayers = 1
        
    def addLayer(self, nUnits):
        self.layers.append(Layer(nUnits, self.layers[-1]))
        self.nLayers += 1
        
    def drive_wake(self, input_state):
        self.layers[0].state = input_state
        for i in range(self.nLayers-1):
            drive_layer_wake(self.layers[i], self.layers[i+1])
        
    def train_wake(self):
        train_layer_wake(self.layers[-1], None)
        for i in range(self.nLayers-2, -1, -1):
            train_layer_wake(self.layers[i], self.layers[i+1])

    def drive_sleep(self):
        p = sigmoid(-self.layers[-1].b_gen)
        self.layers[-1].state = np.random.random_sample(self.layers[-1].state.shape) < p
        for i in range(self.nLayers-2, -1, -1):
            drive_layer_sleep(self.layers[i], self.layers[i+1])

    def train_sleep(self):
        train_layer_sleep(None, self.layers[0])
        for i in range(self.nLayers-1):
            train_layer_sleep(self.layers[i], self.layers[i+1])
            
    def get_C(self):
        """"""Get description length of whole network.""""""
        return np.sum([layer.get_C() for layer in self.layers])",0.5251330733,
1068,linear algebra intro,"displayed = r""""""
    \begin{equation}
    \text{Using implicit differentiation}\\
    y^3 + x3y^2\frac{dy}{dx}+ 3x^2y + x^3\frac{dy}{dx} = 0\\
    \frac{dy}{dx}(3xy^2+x^3)= -3y^3 - 3x^2y\\
    \frac{dy}{dx}=\frac{-3y^3 - 3x^2y}{3xy^2+x^3} = 0\\
    3y^3=-3x^2y\\
    y=\sqrt{-x^2}\text{, Sympy answer: }%(solution)s\\
    \text{Answer: As a square from a negative number never has a solution there exists no such x that will have derivative equal to 0 and, thus, won't have horizontal tangent}
    \end{equation}
    """"""
    x, y= symbols('x y')
#     y = Function('y')(x)
    equation = x * y ** 3 + x **3 * y - 4
    solution = Derivative(y, x).eq(idiff(equation, y, x)).eq(0)
    
    
    display_args = {
        ""solution"": solution.latex()
    }
    
    display_latex(displayed % display_args)
    
    plot_implicit(equation)",0.44727391,
1068,linear algebra intro,"%%file process.py
import sympy
import numpy as np

def parseExpr(expr=''):
    '''Helper function to iterate through a list of equations'''
    err = 'Malformed expression! Does not match ""y = f(x)""\n  {0:s}'
    for s in expr.strip().split('\n'):
        # Parse anything that looks like an equation and isn't commented out
        if ('=' in s) and (s[0] != '#'):
            # convert expression to sympy
            y, f = map(str.strip, s.split('=',1))
            y = sympy.Symbol(y)
            f = sympy.sympify(f)
            assert type(y) == sympy.symbol.Symbol, err.format(s)
            yield (y, f)
                
class Block(object):
    '''Block(expr, inputs='', outputs='', functions={})
Generic processing block that performs specified calculations on given inputs
in such a way to ease documentation
    '''
    
    verbose    = True   # Enable verbose output
    outputMode = 'dict' # eval() return type ('dict' or 'tuple')
    
    def __init__(self, expr, name='', inputs='', outputs='', functions={}):
        '''Create a Block instance'''
        
        if Block.verbose:
            s = 'Creating Block(expr={:s}, name={:s}, inputs={:s}, outputs={:s}, functions='
            s = s.format(*map(repr,[expr, name, inputs, outputs]))+'{'
            if functions:
                s_functions = []
                for k, v in functions.iteritems():
                    s2 = k+':'+v.__name__
                    args, varargs, keywords, defaults = getargspec(v)
                    if varargs != None:
                        args += ['*args']
                    if keywords != None:
                        args += ['*kwargs']
                    s2 += '('+','.join(args)+')'
                    s_functions.append(s2)
                s += ','.join(s_functions)
            print s+'})'
        
        self.name = name
        self.user = functions
        
        # save in list form for ordered outputs
        eqn = tuple(parseExpr(expr))
        self.eqn = tuple([ sympy.Eq(k, v) for k, v in eqn ])
        # placeholder for compiled expressions
        self.lambdas = {}
        
        # Extract inputs and functions used
        expr_inputs = set()
        expr_functs = set()
        for k, v in eqn:
            for arg in sympy.preorder_traversal(v):
                if arg.is_Symbol:
                    expr_inputs.add(arg)
                elif arg.is_Function:
                    expr_functs.add(arg.func)
        
        # save SymPy style .args and .func attributes
        self.args = tuple(expr_inputs)
        self.func = tuple(expr_functs)     
        
        if inputs:
            self.ins = sympy.symbols(inputs)
        else: # extract inputs from expr
            self.ins = tuple(self.args)
            
        outs = tuple([ i[0] for i in eqn ])
        if outputs:
            self.outs = sympy.symbols(outputs)
            print 'outs='+repr(outs)
            self.hidden = tuple([ i for i in outs if i not in self.outs ])
        else: # extract inputs from expr
            self.outs = outs
            self.hidden = tuple()
            if Block.verbose:
                print '  Extracting outputs:', self.outs
                
        # create _eval() wrapper function with useful docstring        
        self.eval = self._wrap_eval()
                        
    def _wrap_eval(self):
        '''create _eval() wrapper function with useful docstring'''
        
        if type(self.ins) == tuple:
            ins = self.ins
            s = repr(self.ins)
        else:
            ins = (self.ins,)
            s = '('+repr(self.ins)+')'

        # not sure how to do a decorator with non-generic dynamic args
        func = sympy.lambdify(self.ins, '_eval'+s, {'_eval':self._eval})
        func.__name__ = 'Block.eval'
        s = repr(self.outs)
        if type(self.outs) != tuple:
            s = '('+s+')'
        func.__doc__ = 'Calculate outputs '+s+' as follows:\n  '
        func.__doc__ += '\n  '.join(map(str,self.eqn))
        func.__doc__ += '\n\nBlock.outputMode = '+repr(Block.outputMode)+'\n'
        
        return func
        
    def __str__(self):
        '''Support str calls on Block instances'''
        s  = 'Block: '+str(self.name)+'\n'
        s += '\n'.join( str(e).replace('==','=') for e in self.eqn )
        return s
    
    def __repr__(self):
        '''return representation that sympy will automatically show as LaTeX'''
        s = 'Block'
        if self.name:
            s += '('+self.name +')'
        return repr({s:[ sympy.Eq(sympy.Symbol(k), v) for k,v in self.eqn.items() ]})
    
    # method doesn't work currently
    __repr__ = __str__
    
    def pretty(self):
        '''Show a SymPy pretty print version of Block equations'''
        print '\nBlock:', self.name
        print '\n'.join( sympy.pretty(e) for e in self.eqn )
        
    @property
    def latex(self):
        '''generate a latex version of Block equations'''
        # \verb;*; leaves contents unformatted
        s  = r'\underline{\verb;Block: '+str(self.name)+';} \\\\ \n'
        s += ' \\\\ \n'.join( sympy.latex(e) for e in self.eqn )
        return s 
        
    def lambdify(self, unknowns=None):
        '''generate an executable function using NumPy'''
        lookup='numpy'
        
        # by default, unknowns are everything but outputs
        if unknowns == None:
            unknowns = self.outs+self.hidden
        
        if self.verbose:
            print 'Compiling Block '+repr(self.name)+' expressions:'
            
        # check for missing functions
        defined = set(dir(np) + self.user.keys())
        missing = [ f for f in self.func if str(f) not in defined ]
        if missing:
            s = 'Unable to find functions '+str(missing)+' in '+str(lookup)
            s += ' or user-defined functions ' + self.user.keys()
            raise LookupError, s
        
        # solve equations in terms of unknowns
        eqn = sympy.solve(self.eqn, unknowns, dict=True)
        if self.verbose:
            print 'Solving equations for unknowns:', unknowns 
            print self.eqn
            print ' =', eqn, '\n'
        
        s = ""  {0:s} = sympy.lambdify({1:s}, {2:s}, '{3:s}')"" # verbose string
        for k, v in eqn[0].iteritems():   
            if k in self.hidden: # skip intermediate values
                continue
            
            k = str(k) # convert output variable from sympy.Symbol
            
            if self.verbose:
                print s.format(*map(str, (k, self.ins, v, (lookup, self.user.keys()) )))
            f = sympy.lambdify(self.ins, v, (lookup, self.user))
            # Update the function name and doc string from ""<lambda> lambda x, y, theta""
            f.__name__ = ""Block""
            if self.name:
                f.__name__ += '('+self.name+')'
            f.__name__ += '.lambdas['+k+']'
            f.__doc__ = k+' = '+str(v)
            self.lambdas[k] = f
            
        return self.lambdas    
        
    def _eval(self, *args, **kwargs):
        '''evaluate outputs for given inputs'''
        
        # make sure in/ouputs are iterable
        if hasattr(self.ins, '__iter__'):
            ins = self.ins
        else:
            ins = [self.ins]
            
        # make sure all inputs are given
        assert len(args) == len(ins)

        # default kwargs values
        if not kwargs.has_key('output'):
            output=Block.outputMode
                   
        # compile expressions if needed
        if not self.lambdas:
            self.lambdify()
            
        if self.verbose:
            s = '\nBlock.eval('
            s += ', '.join( map(lambda k, v: str(k)+'='+str(v), ins, args) )
            s += ', output='+repr(output)
            print s+')\n'
        
        if output == 'dict':            
            if self.name:
                prefix=self.name+'.'
            else:
                prefix=''
            return { prefix+k:v(*args) for k, v in self.lambdas.iteritems() }
        else: # output tuple in order of .outs
            return tuple( self.lambdas[k](*args) for k in self.outs )",0.4365166426,
1068,linear algebra intro,"class linear_system_identifier:
    """"""LTI system identification class
    This class is used to identify systems of a particular structure
    given input-output signals (u(t), y(t))
    """"""
    def __init__(self, input_data, output_data, n, m):
        """"""Constructor
        This is a constructor for `linear_system_identifier`
        Syntax: 
        ssid = linear_system_identifier(input_signal, output_signal, n, m)
        
        where
        input_signal    :   input signal
        output_signal   :   output signal
        n               :   number of past y's affecting the output
        m               :   number of past u's affecting the output
        
        Parameters `n` and `m` determine the model order.
        
        Note: If `input_data` is of lenth `N`, output_data must be of 
              length (at least) `N + 1`
        """"""
        self._u = input_data
        self._y = output_data[:input_data.size+1]
        self._n = n
        self._m = m
        
    def get_n(self):
        return self._n
    
    def get_m(self):
        return self._m
        
    def _inverse_range(self, low, high):
        return np.arange(high, low-1, -1)

    def construct_phi(self):
        num_data = self._u.size + 1        
        num_rows_phi = num_data - self._n
        num_cols_phi = self._n + self._m
        phi = np.zeros([num_rows_phi, num_cols_phi])
        for i_row_phi in range(0, num_rows_phi):
            phi[i_row_phi][:self._n] =                                        \
                    self._y[                                                  \
                            self._inverse_range(                              \
                                    i_row_phi, i_row_phi + self._n - 1)       \
                            ]
            phi[i_row_phi][self._n:] =                                        \
                    self._u[                                                  \
                            self._inverse_range(                              \
                                    i_row_phi + 1, i_row_phi + self._m)       \
                            ]          
        return np.matrix(phi)
    
    def estimate_params_rls(self, lam=0, verbose=False,                       \
                              regularizer=cp.sum_squares):
        """"""Estimates the system parameters by ReLS
        Solves a regularized least squares problem to determine the system
        parameters; the ReLS problem is of the form
        
        Minimize_theta |Phi * theta - Y|^2 + lambda * R(theta)
        
        Syntax:
        ssid.estimate_params_rls(lam=10, regularizer=cp.norm1, verbose=False)
        
        where
        ssid         :  object of type `linear_system_identifier`
        lam          :  regularization parameters (default = 0)
        regularizer  :  regularizer function
        verbose      :  whether to print out solver details (default=False)
        """"""
        phi      = self.construct_phi()
        theta    = cp.Variable(phi.shape[1])        
        cost_fun = cp.sum_squares(phi*theta - self._y[self._n:])              \
                   + lam * regularizer(theta)
        prob  = cp.Problem(cp.Minimize(cost_fun))
        prob.solve(verbose=verbose, solver=""SCS"")
        alpha = theta.value[:self._n]
        beta = theta.value[self._n:]        
        return linear_system(np.array(alpha), np.array(beta), 0)",0.4352912307,
1068,linear algebra intro,"%matplotlib inline
xhat_plt = amp.amp_solver_results(amp.y, amp.A, amp.X, amp.X_orig, amp.m, amp.N, xhat)",0.4223692715,
1068,linear algebra intro,"class ForwardEuler:
    ...
    def solve(self, time_points):
        """"""Compute u for t values in time_points list.""""""
        self.t = np.asarray(time_points)
        self.u = np.zeros(len(time_points))

        self.u[0] = self.U0

        for k in range(len(self.t)-1):
            self.k = k
            self.u[k+1] = self.advance()
        return self.u, self.t

    def advance(self):
        """"""Advance the solution one time step.""""""
        u, f, k, t = self.u, self.f, self.k, self.t

        dt = t[k+1] - t[k]
        unew = u[k] + dt*f(u[k], t[k])
        return unew",0.4184108675,
1068,linear algebra intro,"# Define gradient operator for imaging
from devito import TimeFunction, Operator
from examples.seismic import PointSource

from sympy import solve, Eq

def ImagingOperator(model, image):
    # Define the wavefield with the size of the model and the time dimension
    v = TimeFunction(name='v', grid=model.grid, time_order=2, space_order=4)

    u = TimeFunction(name='u', grid=model.grid, time_order=2, space_order=4,
                     save=time_range.num)
    
    # Define the wave equation, but with a negated damping term
    eqn = model.m * v.dt2 - v.laplace - model.damp * v.dt

    # Use SymPy to rearranged the equation into a stencil expression
    stencil = Eq(v.backward, solve(eqn, v.backward)[0])
    
    # Define residual injection at the location of the forward receivers
    dt = model.critical_dt
    residual = PointSource(name='residual', grid=model.grid,
                           time_range=time_range,
                           coordinates=rec.coordinates.data)    
    res_term = residual.inject(field=v, expr=residual * dt**2 / model.m,
                               offset=model.nbpml)

    # Correlate u and v for the current time step and add it to the image
    image_update = Eq(image, image - u * v)

    return Operator([stencil] + res_term + [image_update],
                    subs=model.spacing_map)",0.4183467627,
1068,linear algebra intro,"sconst = 1

def TaylorSS(A):
    taylordegree = 15 # Use order 15 Taylor approximation
    s = np.ceil(sp.log2(la.norm(A))) + sconst # Find s such that norm(A/2^s) is small.
    X = A/(2**s)
    eX = np.eye(np.shape(A)[0])
    for k in range(taylordegree): # Compute the Taylor series
        eX = eX + X/sp.misc.factorial(k+1) 
        X = sp.dot(X, X)
        
    for k in range(np.int64(s)):
        eX = sp.dot(eX, eX) # Do the squaring phase of the algorithm
        
    return eX

#Let's test it against la.expm
A = np.random.randn(4, 4)
E1 = TaylorSS(A)
Eexact = la.expm(A)
print la.norm(E1 - Eexact)/la.norm(Eexact) # Relative error",0.4142099023,
1068,linear algebra intro,"def findClosure(R, F, printflag = False):
        """"""Finds closure by repeatedly applying the three Armstrong Axioms, until there is no change""""""

        # Start with adding all trivial dependencies generated by using Reflexivity
        F = F.union(applyreflexivity(R))
        powersetR = list(chain.from_iterable(combinations(list(R), r) for r in range(1, len(R)+1)))

        # Repeat application of the other two rules until no change
        done = False;
        while done == False:
                if printflag: print ""Trying to find new FDs using Transitivity""
                F2 = applytransitivity(F, printflag)
                if printflag: print ""Trying to find new FDs using Augmentation""
                F2 = applyaugmentation(F2, powerset(R), printflag)
                done = len(F2) == len(F)
                F = F2
        if printflag: print ""Finished""
        return F",0.4128018022,
1068,linear algebra intro,"def advection_LW_pbc(m, nsteps, nplot=None, verbose=False):
    """"""
    Solve u_t + au_x = 0  on [ax,bx] with periodic boundary conditions,
    using the Lax-Wendroff method with m interior points.
     
    Returns k, h, and the max-norm of the error
     
    """"""

    a = 2             # advection velocity
    
    ax = 0.
    bx = 1.
    tfinal = 1.                # final time

    def eta(x):
        """"""Initial conditions""""""
        beta = 600.
        return exp(-beta*(x - 0.5)**2)

    def utrue(x,t):
        """"""
        True solution for comparison.
        For periodic BC's, we need the periodic extension of eta(x).
        Map x-a*t-ax back to interval of length bx-ax
        and then evaluate initial data at this point.
        """"""
        xat = ax + mod(x - a*t - ax, bx-ax)
        return eta(xat)


    h = (bx-ax)/float(m+1)     # h = delta x
    k = tfinal / float(nsteps) # time step
    nu = a*k/h                 # Courant number
    
    xgrid = linspace(ax, bx, m+2)
            
    I = array(range(m+1), dtype=int)   # indices of unknowns 0, 1, ..., m
    Im1 = mod(I-1, m+1)
    Ip1 = mod(I+1, m+1)

    if verbose:
        print ""Will take %i time steps with k = %g, h = %g"" % (nsteps,k,h)
        if nplot is not None:
            print ""    and plot every %i time steps"" % nplot

        print ""The Courant number is nu = a*k/h = %g"" % nu
        if abs(nu) > 1:
                print ""*** Warning: the method is unstable!""

    
    # initial conditions:
    u0 = eta(xgrid)

    # initial data on fine grid for plotting:
    xfine = linspace(ax,bx,1001)
    ufine = utrue(xfine,0)

    if nplot is not None:
        
        # make plot directory for storing png files:
        J.make_plotdir(plotdir, clobber=True)  # ok to clobber if it already exists
        
        # plot initial data:
        plot(xgrid,u0,'b.-', label='computed')
        plot(xfine,ufine,'r', label='true')
        axis([ax,bx,-.2,1.2])
        legend()
        title('Initial data at time = 0')
    
        # Save this frame:
        J.save_frame(frameno=0, plotdir=plotdir, verbose=False)

        
    # main time-stepping loop:
    
    tn = 0.
    u = eta(xgrid)  # initial data

    for n in range(1,nsteps+1):
        tnp = tn + k   # = t_{n+1}
        
        # Lax-Wendroff:
        u[I] = u[I] - 0.5*nu*(u[Ip1] - u[Im1]) + \
                      0.5*nu**2 * (u[Im1] - 2*u[I] + u[Ip1])
            
        u[m+1] = u[0]  # for plotting

        # plot results at desired times:
        if nplot is not None:
            if mod(n,nplot)==0 or n==nsteps:
                uint = u[0:m+2]  # points on the interval (drop ghost cell on right)
                ufine = utrue(xfine,tnp)
                clf()
                plot(xgrid,u,'b.-', label='computed')
                plot(xfine,ufine,'r', label='true')
                axis([ax,bx,-.2,1.2])
                title('t = %9.5e  after %4i time steps with %5i grid points' \
                               % (tnp,n,m+1))
                    # Save this frame:
                J.save_frame(n, plotdir,verbose=False)
                
        tn = tnp   # for next time step
        close('all')  # close figure
        
    error = max(abs(u[0:m+2] - utrue(xgrid,tfinal)))
    return h,k,error",0.4111354351,
1068,linear algebra intro,"class Hamiltonian:
    """"""
    This is a Hamiltonian operator class that allows us to construct a matrix representation 
    for a two-spin Hamiltonian. The two spins correspond to electronic and nuclear spins of 
    arbitrary size. The interactions that can be included are the zero-field splitting (ZFS) 
    term (for electronic spin), the Zeeman interaction, the Quadrupolar interaction 
    (including the quadrupole coupling constant and a non-zero asymmetry parameter), and a 
    dipolar coupling term.        
        
    """"""
    
    def __init__(self,spin_e,spin_n,Egyro,Ngyro):
        """"""
        The __init__ method creates a Hamiltonian object. The 'e', 'n', 'Egyro' and 'Ngyro 
        fields are initialized. The set containing term labels is initialized to the empty 
        set and the matrix representation is initialized to the zero matrix. The method 
        calls function 'createSpinOperators' to construct the generalized Pauli spin vector 
        operators for the electronic and nuclear spins. 
        
        Args: 
            spin_e: Spin quantum number of electronic spin.
            spin_n: Spin quantum number of nuclear spin.
            Egyro: Electronic spin gyromagnetic ratio.
            Ngyro: Nuclear spin gyromagnetic ratio.
            
        """"""
        
        self.e = spin_e
        self.n = spin_n
        
        self.Egyro = Egyro
        self.Ngyro = Ngyro
        
        # create spin operators
        self.eSpinOperators = createSpinOperators(spin_e)
        self.nSpinOperators = createSpinOperators(spin_n)
        
        # initialize matrix representation
        HilbSpace_size = int(2*spin_e+1)*int(2*spin_n+1) # dimension of Hilbert space
        self.H_MatrixRep = npm.zeros((HilbSpace_size,HilbSpace_size), dtype = np.complex)
        self.terms = set()
    
    def __str__(self):
        """"""
        This method prints a string containing the current terms in the Hamiltonian
        
        Args: 
            None
            
        """"""
        
        outstring = 'Current Hamiltonian terms: '
        for item in self.terms:
            outstring += item + ', '
        outstring = outstring[0:-2]
        return outstring
    
    def add_ZFS(self,D):
        """"""
        This method creates a ZFS term in the Hamiltonian. Asymmetry parameter is not 
        considered here. The ZFS term is added to the Hamiltonian matrix representation 
        and a separate matrix is created. 
        
        Args: 
            D: Electronic ZFS magnitude [Hz]. 
            
        """"""
        if 'ZFS' not in self.terms:
            self.ZFS_MatrixRep = C['hbar']*2*np.pi*D* \
                np.kron(self.eSpinOperators[2]**2,self.nSpinOperators[3])

            # add to total Hamiltonian matrix representation
            self.H_MatrixRep += self.ZFS_MatrixRep
            self.terms.add('ZFS') # add label to terms list
        else:
            print('Hamiltonian already contains ZFS term.')
        
    def add_Zeeman(self,B):
        """"""
        This method creates a Zeeman term in the Hamiltonian. Both the electron and 
        nuclear parts are included. The Zeeman term is added to the Hamiltonian matrix 
        representation and a separate matrix is created. 
        
        Args: 
            B: List of components of external applied magnetic field [G]. 
            
        """"""
        if 'Zeeman' not in self.terms:
            self.Zeeman_MatrixRep = npm.zeros(self.H_MatrixRep.shape, dtype = np.complex)
            for i in range(3):
                self.Zeeman_MatrixRep += C['hbar']*2*np.pi*B[i]* \
                    (self.Egyro*np.kron(self.eSpinOperators[i],self.nSpinOperators[3])) 
                self.Zeeman_MatrixRep += C['hbar']*2*np.pi*B[i]* \
                    (self.Ngyro*np.kron(self.eSpinOperators[3],self.nSpinOperators[i]))

            # add to total Hamiltonian matrix representation
            self.H_MatrixRep += self.Zeeman_MatrixRep
            self.terms.add('Zeeman') # add label to terms list
        else: 
            print('Hamiltonian already contains Zeeman term.')
            
    def add_nuclearQuadrupolar(self,Q,eta,NVang):
        """"""
        This method creates a nuclear Quadrupolar term in the Hamiltonian. The 
        quadrupolar term is added to the Hamiltonian matrix representation and a separate 
        matrix is created. 
        
        Args: 
            Q: Quadrupole coupling constant [Hz].
            eta: Asymmetry parameter.
            NVang: Symmetry axis orientation (in radians) for electron spin.
            
        """"""
        
        if 'Nuclear Quadrupolar' not in self.terms:
            # create rotation matrix for transformation into NV coordinate system
            RotMatrix = rotateMatrix([NVang,0,0])

            # Initialize Quadrupolar Matrix
            Cmat = C['hbar']*2*np.pi*Q/(4*self.n*(2*self.n-1))* \
                np.array([[eta,0,0],[0,-eta,0],[0,0,3]])
            Cmat = np.linalg.inv(RotMatrix)*Cmat*RotMatrix # transform to NV coordinates

            # populate Quadrupolar terms
            self.nuclearQuadrupolar_MatrixRep = npm.zeros(self.H_MatrixRep.shape, \
                                                          dtype = np.complex)
            for i in range(3):
                for j in range(3):
                    self.nuclearQuadrupolar_MatrixRep += Cmat[i,j]* \
                        np.kron(self.eSpinOperators[3],self.nSpinOperators[i]* \
                                self.nSpinOperators[j])

            # add to total Hamiltonian matrix representation
            self.H_MatrixRep += self.nuclearQuadrupolar_MatrixRep
            self.terms.add('Nuclear Quadrupolar') # add label to terms list
        else: 
            print('Hamiltonian already contains nuclear quadrupolar term.')
        
    def add_dipolarInteraction(self,disp,NVang):
        """"""
        This method creates a dipolar interaction term in the Hamiltonian. The dipolar term 
        is added to the Hamiltonian matrix representation and a separate matrix is created. 
        
        Args: 
            Q: Displacement of electronic and nuclear spins [nm].
            NVang: Symmetry axis orientation (in radians) for electron spin.
            
        """"""

        if 'Dipolar Interaction' not in self.terms:
            disp = np.asmatrix(disp) # convert displacement into NumPy matrix 
            disp = disp*10**-9 # convert displacement vector to nanometers

            # create rotation matrix for transformation into NV coordinate system
            RotMatrix = rotateMatrix([NVang,0,0])
            nr = RotMatrix*disp.T/np.linalg.norm(disp) # unit displacement vector

            # dipolar interaction strength
            Interaction_strength = (C['u_0']*(2*np.pi*10**4*C['hbar'])**2* \
                                    self.Egyro*self.Ngyro)/(4*np.pi*np.linalg.norm(disp)**3)

            # populate anisotropic dipolar terms
            self.dipolarInteraction_MatrixRep = npm.zeros(self.H_MatrixRep.shape, \
                                                          dtype = np.complex)
            for i in range(3):
                for j in range(3):
                    self.dipolarInteraction_MatrixRep -= 3*Interaction_strength* \
                        float(nr[i])*np.kron(self.eSpinOperators[i],self.nSpinOperators[j])

            # populate isotropic dipolar terms
            for i in range(3):
                self.dipolarInteraction_MatrixRep += Interaction_strength* \
                    np.kron(self.eSpinOperators[i],self.nSpinOperators[i])

            # add to total Hamiltonian matrix representation
            self.H_MatrixRep += self.dipolarInteraction_MatrixRep
            self.terms.add('Dipolar Interaction') # add label to terms list   
        else: 
            print('Hamiltonian already contains dipolar interaction term.')

    def remove_ZFS(self):
        """"""
        This method removes the ZFS term from the Hamiltonian and the terms list set.
        
        Args: 
            None 
            
        """"""
        if 'ZFS' in self.terms:
            self.H_MatrixRep -= self.ZFS_MatrixRep # remove from Hamiltonian
            self.ZFS_MatrixRep = None
            self.terms.remove('ZFS') # remove label from terms list set
        else:
            print('Hamiltonian has no ZFS term.')
            
    def remove_Zeeman(self):
        """"""
        This method removes the Zeeman term from the Hamiltonian and the terms list set.
        
        Args: 
            None 
            
        """"""
        if 'Zeeman' in self.terms:
            self.H_MatrixRep -= self.Zeeman_MatrixRep # remove from Hamiltonian
            self.Zeeman_MatrixRep = None
            self.terms.remove('Zeeman') # remove label from terms list set
        else:
            print('Hamiltonian has no Zeeman term.')
            
    def remove_nuclearQuadrupolar(self):
        """"""
        This method removes the nuclear quadrupolar term from the Hamiltonian and the terms 
        list set.
        
        Args: 
            None 
            
        """"""
        if 'Nuclear Quadrupolar' in self.terms:
            self.H_MatrixRep -= self.nuclearQuadrupolar_MatrixRep # remove from Hamiltonian
            self.nuclearQuadrupolar_MatrixRep = None
            self.terms.remove('Nuclear Quadrupolar') # remove label from terms list set
        else:
            print('Hamiltonian has no nuclear quadrupolar term.')
            
    def remove_dipolarInteraction(self):
        """"""
        This method removes the dipolar interaction term from the Hamiltonian and the terms 
        list set.
        
        Args: 
            None 
            
        """"""
        if 'Dipolar Interaction' in self.terms:
            self.H_MatrixRep -= self.dipolarInteraction_MatrixRep # remove from Hamiltonian
            self.dipolarInteraction_MatrixRep = None
            self.terms.remove('Dipolar Interaction') # remove label from terms list set
        else:
            print('Hamiltonian has no dipolar interaction term.')",0.4111083746,
1540,positive and negative tweets,"# sort tweets into positive/negative lists for each candidate 

store_pos_polarity = []
store_ind_pos_polarity = []
store_tweet_pos_polarity = []
store_neg_polarity = []
store_ind_neg_polarity = []
store_tweet_neg_polarity = []
pos_neg_ratio = []
pos_tweets = []
neg_tweets = []
avg_pos = []
avg_neg = []

# for each candidate
for iperson, val in enumerate(candidates):
    # sort each sentiment score into positive or negative sentiment categories, outputs are:
    # lists of each pos. and neg. sentiment score: pos_polarity, neg_polarity
    # lists of the indices for each tweet w. a pos/neg score: ind_pos_polarity, ind_neg_polarity
    # lists of the texts for each tweet that has a pos/neg tscore: 
                                        #tweet_pos_polarity, tweet_neg_polarity
    pos_polarity, neg_polarity, ind_pos_polarity, ind_neg_polarity, \
        tweet_pos_polarity, tweet_neg_polarity = \
        fns.sort_tweets_pos_neg(iperson, polarity_store, tweet_index_store, tweet_list)
    
    # store each candidates lists in a master list
    store_pos_polarity.append(pos_polarity)              # positive sentiment scores
    store_ind_pos_polarity.append(ind_pos_polarity)      # indices of tweets w. pos. scores
    store_tweet_pos_polarity.append(tweet_pos_polarity)  # positive tweet texts
    store_neg_polarity.append(neg_polarity)              # negative sentiment scores
    store_ind_neg_polarity.append(ind_neg_polarity)      # indices of tweets w. neg. scores 
    store_tweet_neg_polarity.append(tweet_neg_polarity)  # negative tweet texts
    
    # get the ratio of positive to negative tweets
    pos_neg_ratio.append(len(ind_pos_polarity)/len(ind_neg_polarity))
    
    # get counts of # of pos/neg tweets, and the average pos/neg sentiment scores
    pos_tweets.append(len(ind_pos_polarity))  # number positive tweets
    neg_tweets.append(len(ind_neg_polarity))  # number negative tweets
    avg_pos.append(np.average(pos_polarity))  # avg positive tweet sentiment score
    avg_neg.append(np.average(neg_polarity))  # avg negative tweet sentiment score
    
    # print number of positive/negative tweets per person, 
    # and their average positive/negative sentiment score
    print(candidates[iperson], '# Tweets: Positive', len(ind_pos_polarity), \
          'Negative', len(ind_neg_polarity), ' Avg. Sentiment: Positive %.2f' \
          % np.average(pos_polarity), 'Negative %.2f' % np.average(neg_polarity))",0.4302573204,
1540,positive and negative tweets,"sample_size = 5000

values = {
    ""negative"":{""found"":0, ""text"":""""},
    ""neutral"":{""found"":0, ""text"":""""},
    ""positive"":{""found"":0, ""text"":""""}
}

for tweet in tweets:
    sent = tweet[""sentiment""]
    if sent[""probability""] > 0.6:
        if values[sent[""label""]][""found""] < sample_size:
            values[sent[""label""]][""text""] += ""\n"" + tweet[""text""]
            values[sent[""label""]][""found""] += 1

    if values[""negative""][""found""] >= sample_size and values[""neutral""][""found""] >= sample_size and values[""positive""][""found""] >= sample_size:
        break

module_id = 'ex_y7BPYzNG' # This is the id of the keyword extractor

for sentName, sentDict in values.items():
    print(sentName keywords: )
    print()
    res = ml.extractors.extract(module_id, [sentDict[""text""]])
    for d in res.result[0]:
        print(d[""keyword""])
    sentDict[""keywords""] = res.result",0.4239760637,
1540,positive and negative tweets,"counts = tweets['ysjagan'].value_counts()[True]+tweets['Jagan Mohan Reddy'].value_counts()[True]+tweets['ysjagan'].value_counts()[True]+tweets['Jagan'].value_counts()[True]+
tweets['YSRC']value_counts()[True]+tweets['YSRCP'].value_counts()[True]
Print(counts)",0.4009127617,
1540,positive and negative tweets,"strong =     # put True here if you think it's a strong correlation and False if it's weak
positive =     # put True here if you think the data is positively correlated, and False if it's negatively correlated

checker(strong, positive)",0.3952211738,
1540,positive and negative tweets,"# we can have the classification of positive, negative, neutral tweets as mentioned earlier and the percentage of each
    ptweets = [tweet for tweet in text if data['Sentiment'] == 'positive']
    ntweets = [tweet for tweet in text if data['Sentiment'] == 'negative']
    neutraltweets = [tweet for tweet in text if data['Sentiment'] == 'neutral']

    print(""Percentage of positive tweets: {}%"".format(len(ptweets)*100/len(text)))
    print(""Percentage of neutral tweets: {}%"".format(len(ntweets)*100/len(text)))
    print(""Percentage de negative tweets: {}%"".format(len(neutraltweets)*100/len(text)))",0.3927204907,
1540,positive and negative tweets,"def analogy(model,a,b,c):
    return model.wv.most_similar(positive=[c, b], negative=[a])",0.3918878734,
1540,positive and negative tweets,"def keyword_func(**kwargs) : 
    print ('Noun:', kwargs['noun'])
    print ('Verb:', kwargs['verb'])
    
keyword_func(noun='spork', verb='eat')",0.3903675973,
1540,positive and negative tweets,"class TweetAnalyzer():
    def tweets_to_dataframe(self, tweets):
        df = pd.DataFrame(data=[t.text for t in tweets], columns=['Tweets'])
        df['id'] = np.array([t.id for t in tweets]) # <<<<<<<<<<<<

        return df",0.388902843,
1540,positive and negative tweets,"class TweetAnalyzer():
    def tweets_to_dataframe(self, tweets):
        df = pd.DataFrame(data=[t.text for t in tweets], columns=['Tweets'])

        df['id'] = np.array([t.id for t in tweets])
        df['len'] = np.array([len(t.text) for t in tweets])
        df['date'] = np.array([t.created_at for t in tweets])
        df['source'] = np.array([t.source for t in tweets])
        df['likes'] = np.array([t.favorite_count for t in tweets])
        df['retweets'] = np.array([t.retweet_count for t in tweets])

        return df",0.388902843,
1540,positive and negative tweets,"rCoaddCalib = butler_coadd.get('deepCoadd_calexp_calib',  {'filter': 'HSC-R', 'tract': 0, 'patch': '1,1'})
iCoaddCalib = butler_coadd.get('deepCoadd_calexp_calib',  {'filter': 'HSC-I', 'tract': 0, 'patch': '1,1'})

rCoaddCalib.setThrowOnNegativeFlux(False)
iCoaddCalib.setThrowOnNegativeFlux(False)

rMags_coadd = rCoaddCalib.getMagnitude(rSources['base_PsfFlux_flux'])
iMags_coadd = iCoaddCalib.getMagnitude(iSources['base_PsfFlux_flux'])",0.3871375024,
1060,level problems,"def my_max(l):
    current_max = None
    for x in l:
        if current_max is None or x >= current_max:
            current_max = x
    return current_max",0.3197606802,
1060,level problems,"x = -32
if x < 0:
    if abs(x) > 5:
        print('{} is less than 0 and has magnitude greater than 5'.format(x))
    else:
        print('{} is not interesting'.format(x))
        
#or you can do:
if x < 0 and abs(x) > 5:
  print('{} is less than 0 and has magnitude greater than 5'.format(x))
if x < 0 and not abs(x) > 5:
        print('{} is not interesting'.format(x))",0.3119857609,
1060,level problems,"def first_negative(l):
    for x in l:
        if x < 0:
            return x
    return None",0.3042155802,
1060,level problems,"def my_min(sequence):
    """"""return the minimum element of sequence""""""
    low = sequence[0] # need to start with some value
    for i in sequence:
        if i < low:
            low = i
    return low

features = []
for i in range(len(top_corr)-1):
    features = top_corr[0:i+1]
    young.dropna(subset=[""Alcohol""],inplace=True)
    young.dropna(subset=features,inplace=True)
    
    X_train, X_test, y_train, y_test = train_test_split(young[features],young[""Alcohol""], test_size=0.3, random_state=20)
    kf = KFold(len(X_train), n_folds=5)
    
    error_rate = []
    #Find the optimal K by finding the errors from 1 to 40 and picking the K with lowest error
    for i in range(1,40):
    
        knn = KNeighborsClassifier(n_neighbors=i)
        knn.fit(X_train,y_train)
        pred_i = knn.predict(X_test)
        error_rate.append(np.mean(pred_i != y_test))
    ideal_K = error_rate.index(my_min(error_rate))
    
    print(""-----------------------------------------------------------------------------------"")
    knn=KNeighborsClassifier(n_neighbors=ideal_K)
    knn.fit(X_train,y_train)
    print(""Ideal K:{}"".format(ideal_K))
    print(features)
    print('Average accuracy score on cv (KFold) set: {:.3f}'.format(np.mean(cross_val_score(knn, X_train, y_train, cv=kf))))
    print('Accuracy score on test set is: {:.3f}'.format(knn.score(X_test, y_test)))",0.3012414277,
1060,level problems,"def find_min_freq(L, f_min):
    for f in L :
        if f >= f_min :
            return f
            break",0.2978475988,
1060,level problems,"# hiddencode
def list_problems():
    for p in sorted(problems.values(), key=lambda x: x.attributes):
        if not p.superproblems:
            p.print_structure()
            print """"",0.2977582514,
1060,level problems,"def create_graph(self):
    """""" 
    build a Planning Graph as described in Russell-Norvig 3rd Ed 10.3 or 2nd Ed 11.4

    The S0 initial level has been implemented for you.  It has no parents and includes all of
    the literal fluents that are part of the initial state passed to the constructor.  At the start
    of a problem planning search, this will be the same as the initial state of the problem.  However,
    the planning graph can be built from any state in the Planning Problem

    This function should only be called by the class constructor.

    :return:
        builds the graph by filling s_levels[] and a_levels[] lists with node sets for each level
    """"""
    # the graph should only be built during class construction
    if (len(self.s_levels) != 0) or (len(self.a_levels) != 0):
        raise Exception(
            'Planning Graph already created; construct a new planning graph for each new state in the planning sequence')

    # initialize S0 to literals in initial state provided.
    # 0 .
    leveled = False
    level = 0 #
    self.s_levels.append(set())  # S0 set of s_nodes - empty to start
    # for each fluent in the initial state, add the correct literal PgNode_s
    for literal in self.fs.pos: #positive  loop
        self.s_levels[level].add(PgNode_s(literal, True)) #PgNode_s : state type  . init(symbol, is_pos)
        #s_levels  () PgNode_s .  set   .
    for literal in self.fs.neg: #negative  loop
        self.s_levels[level].add(PgNode_s(literal, False)) #PgNode_s : state type  . init(symbol, is_pos)
        #s_levels  () PgNode_s .  set   .
    # no mutexes at the first level
    #   .  :     

    # continue to build the graph alternating A, S levels until last two S levels contain the same literals,
    # i.e. until it is ""leveled""
    while not leveled: #planning Graph      .
        self.add_action_level(level) #action 
        self.update_a_mutex(self.a_levels[level]) #mutex 

        level += 1 # 
        self.add_literal_level(level)
        self.update_s_mutex(self.s_levels[level])

        if self.s_levels[level] == self.s_levels[level - 1]: #   .
            # ,            .
            leveled = True",0.2969411314,
1060,level problems,"# filter after applying the double_reverse function to the list not filter the original list
# if it's string, then exclude
def filter_5(x):
    if type(x) == int or type(x) == float: #can use if type(x) in (float,int):
        return x >= 5
    else:
        return False
    
list(filter(filter_5, map(double_reverse, fix_me)))",0.2962042987,
1060,level problems,"# [ ] Write an expression to test if a string s contains a numerical value
# then test if the value is greater than the value stored in x
# HINT: Use the functions `s.isnumeric()` and `float(s)`

def check_stringnum(s, x):
    if s.isnumeric():
        return float(s) >= x
    else:
        return False

# Test your expression with
# s = ""39""
# x = 24
# Expression should yield True

print(check_stringnum(""39"", 24))

# s = ""a39""
# x = 24
# Expression should yield False

print(check_stringnum(""a39"", 24))",0.2944019139,
1060,level problems,"def pretty_print_compl(c):
    if p_imag(c) >= 0:
        rep_ext = str(p_real(c)) + '+' + str(p_imag(c)) + 'i'
    else:
        rep_ext = str(p_real(c)) + '-' + str(abs(p_imag(c))) + 'i'
    print(rep_ext)",0.2932259738,
547,event metadata,"eventsWithSpecificHashtagRowList = []

def applyToMakeEventDf(row):
    '''
    This function will be applied to each row of each dataframe of hashtags.
    If a row is detected as an event, it will be added to the locaRowsList which will
    be used to make a dataframe of all the events.
    '''
    if row.event:
        rowToAdd = {'date': row.name, 'hashtag': row.hashtag, 'text': row.text,
                    'longitude': row.longitude, 'latitude':row.latitude, 'numberOfTweets': row.numberOfTweets, }
        global eventsWithSpecificHashtagRowList
        eventsWithSpecificHashtagRowList.append(rowToAdd)",0.4736953378,
547,event metadata,"def get_updates(element):
    '''This function will process each element from the xml file, and if it represents an 'associatedStreet' type relation,
    it will store all the information from the node in a usable form for the next function which will update the mongoDB.'''
    node = {}
    node['member'] = []
    street = False
    if element.tag != 'relation':
        return None
    
    #Check if this relation is an 'associatedStreet' type
    for child in elem:
        try:
            if child.attrib['v'] == 'associatedStreet':
                street = True
        except KeyError:
            pass
    
    # If it's not an 'associatedStreet' type, just ignore it
    if not street:
        return None
    
    # If it is an 'associatedStreet' type:
    if street:
        for child in element:
            #Add the information from the relation tags in the node dict
            if child.tag == 'tag':
                try:
                    node[child.attrib['k']] = child.attrib['v']
                except KeyError:
                    pass
            #Put all the members in a list
            elif child.tag == 'member':
                try:
                    node['member'].append(child.attrib['ref'])
                except KeyError:
                    pass
        return node",0.4723918438,
547,event metadata,"def add_features(result,features):
    result['Type Autocall'] = features.result[0]
    result['Maturite'] = features.result[1]
    result['Barriere Coupon'] = features.result[2]
    result['Barriere PDI'] = features.result[3]
    
    result_Bender = dp.data_for_pricing(result, number_years,strike_values)
    result_XGBoost = test.match_columns(result_Bender,train_columns)
    
    return result_XGBoost, result_Bender",0.4533611536,
547,event metadata,"def shape_element(element):
    data = []
    node = {}
    node[""pos""]=[]
    if element.tag == ""node"" or element.tag == ""way"" :
        node[""type""] = element.tag
        for key in element.keys():
            if key in CREATED:
                if ""created"" not in node.keys():
                    node[""created""] = {}
                node[""created""][key] = element.attrib[key]
            elif key in (""lat"", ""lon""):
                try:
                    node['pos']=[float(element.attrib['lat']),float(element.attrib['lon'])]
                except KeyError:
                    pass
            else:
                node[key] = element.attrib[key]

        for tag in element.iter(""tag""):
            if not(problemchars.search(tag.attrib['k'])):
                if tag.attrib['k'].startswith(""addr:""):
                        addr_fields = tag.attrib['k'].split(':')
                        if len(addr_fields) == 2:
                            if 'address' not in node.keys():
                                node['address'] = {}
                            if addr_fields[1] == ""street"":
                                node[""address""][addr_fields[1]] = update_streets(tag.attrib['v'], mapping, st_types)
                            elif addr_fields[1] == ""postcode"":
                                node[""address""][addr_fields[1]] = update_pcodes(tag.attrib['v'], pcode_types)
                            else:
                                node[""address""][addr_fields[1]] = tag.attrib['v']
                else:
                    node[tag.attrib['k']] = tag.attrib['v']
        
        for nd in element.iter(""nd""):
            if ""node_refs"" not in node.keys(): 
                node[""node_refs""] = []
            node[""node_refs""].append(nd.attrib['ref'])
        
        return node
    else:
        return None",0.452676177,
547,event metadata,"def parse_row(row):
    columns = row.find_all(""td"")
    return {
        'name': columns[0].find('p').string,
        'plug_type': columns[1].find('a').string,
        'voltage': columns[2].string,
        'frequency': columns[3].string
    }

parse_row(rows[0])",0.451033771,
547,event metadata,"# convert the way into dict so that it will be converted into json
def way_to_dict(filename):
    data = []
    for _,elem in cET.iterparse('singapore.osm',events=('start',)):
        if elem.tag == 'way':
            way = {}
            way['uniqueId'] = int(elem.attrib['id'])
            way['uid'] = int(elem.attrib['uid'])
            nds = []
            for nd in elem.iter('nd'):
                nds.append(int(nd.attrib['ref']))
            way['nd'] = nds
            tags = []
            flag = 0
            for tag in elem.iter('tag'):
                flag = 1
                temp = {}
                if is_street(tag):
                    temp['k']='street'
                    temp['v']=inconsistent_to_correct_name(tag.attrib['v'])
                elif is_postcode(tag):
                    if is_valid_postcode(tag):
                        temp['k']='postcode'
                        temp['v']=tag.attrib['v']
                elif is_address(tag):
                    temp['k']=correct_address(tag.attrib['k'])
                    temp['v']=tag.attrib['v']
                else:
                    temp['k']=tag.attrib['k']
                    temp['v']=tag.attrib['v']
                tags.append(temp)
            if flag == 1:
                way['tag'] =tags
            data.append(way)
    return data
way_data = way_to_dict('singapore.osm')",0.4500604868,
547,event metadata,"def shape_element(element):
    node = {}
    address = {}
    node_refs = []
    info = {}
    
    if element.tag == ""node"" or element.tag == ""way"" :
        created = {}
        pos = [None, None]
        for attr in element.attrib.keys():
            if attr == 'id':
                node[""id""] = element.attrib[attr]
            if attr == ""visible"":
                node[""visible""] = element.attrib[attr]
            if attr in CREATED:
                created[attr] = element.attrib[attr]
            # pos
            if attr == 'lat':
                pos[0] = float(element.attrib[attr])
            if attr == 'lon':
                pos[1] = float(element.attrib[attr])
                
        node[""type""] = element.tag
        node[""created""] = created
        if pos[0]:
            node[""pos""] = pos
       
        for tag in element.iter('tag'):
            k = tag.attrib['k']
            # address standarize
            if k.startswith(""addr:""):
                if lower_colon.search(k):
                    v = tag.attrib['v']
                    matcher = street_type_re.search(v)
                    if matcher:
                        streetname = matcher.group(0) 
                        if streetname in mapping:
                            # set standarized street name
                            address[k[5:]] = v.replace(streetname, mapping[streetname])
                        else:
                            address[k[5:]] = v
                    else:
                        address[k[5:]] = v
                    
            # not address
            elif lower.search(k):
                # exclude unnecessary tag defined in 'EXCLUDE'
                if not k in EXCLUDE:
                    info[k] = tag.attrib['v']
                    
            # contains symbols
            elif problemchars.search(k):
                pass

        if address:
            node[""address""] = address
        if info:
            node[""point_info""] = info 
            
        # node_refs
        for tag in element.iter('nd'):
            ref = tag.attrib['ref']
            node_refs.append(ref)
            
        if node_refs:
            node[""node_refs""] = node_refs
            
        return node
        
    else:
        return None

if __name__ == ""__main__"":
    data = process_map('south-lake-tahoe_california.osm', True)
    #data = process_map('test.osm', True)
    pprint.pprint(data[0])",0.4491806626,
547,event metadata,"def count_tags(filename):
    tags_counted = {}
    context = ET.iterparse(filename, events=(""start"", ))
    for event, elem in context:
        if elem.tag not in tags_counted:
            tags_counted[elem.tag] = 1
        else:
            tags_counted[elem.tag] = tags_counted[elem.tag]+1
    return tags_counted",0.4483527541,
547,event metadata,"def get_authors(root):
    authors = []
    for author in root.findall('./fm/bibl/aug/au'):
        data = {
                ""fnm"": None,
                ""snm"": None,
                ""email"": None
        }
        # MY CODE HERE

        fnm = author.find('fnm')
        snm = author.find('snm')
        email = author.find('email')
        
        authors.append({'fnm': fnm.text, 'snm': snm.text, 'email': email.text})

    return authors",0.4480734169,
547,event metadata,"def get_author(root):
    authors = []
    for author in root.findall('./fm/bibl/aug/au'):
        data = {
                ""fnm"": None,
                ""snm"": None,
                ""email"": None
        }
        data[""fnm""] = author.find('./fnm').text
        data[""snm""] = author.find('./snm').text
        data[""email""] = author.find('./email').text

        authors.append(data)

    return authors",0.4480734169,
2413,transform weight column,"def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        init.orthogonal(m.weight.data, gain=1.4)",0.5399971008,
2413,transform weight column,"def init_randn(m):
    if type(m) == nn.Linear:
        m.weight.data.normal_(0,1)",0.5365071297,
2413,transform weight column,"# And a way to set the filters to ones
def zero_first_filter(layer):
    W, b = layer.get_weights()
    W[0] = np.ones(W.shape[1:])
    b[0] = 0.0
    layer.set_weights([W, b])",0.5235490799,
2413,transform weight column,"# takes in a module and applies the specified weight initialization
def weights_init_uniform_center(m):
    classname = m.__class__.__name__
    # for every Linear layer in a model..
    if classname.find('Linear') != -1:
        # apply a centered, uniform distribution to the weights
        m.weight.data.uniform_(-0.5, 0.5)
        m.bias.data.fill_(0)

# create a new model with these weights
model_centered = Net()
model_centered.apply(weights_init_uniform_center)",0.5225205421,
2413,transform weight column,"def initialize_weights(m):
    weight_shape = list(m.weight.data.size())
    fan_in = weight_shape[1]
    fan_out = weight_shape[0]
    w_bound = np.sqrt(6. / (fan_in + fan_out))
    m.weight.data.uniform_(-w_bound, w_bound)
    m.bias.data.fill_(0)",0.5216721296,
2413,transform weight column,"%%add_to Network
def initialize_weights(self):
    self.weights = [np.random.randn(y, x)
                    for x, y in zip(self.sizes[:-1], self.sizes[1:])]",0.5201124549,
2413,transform weight column,"%%add_to Network
def initialize_weights(self):
    # YOUR CODE HERE
    self.weights = [np.random.randn(y, x)
                    for x, y in zip(self.sizes[:-1], self.sizes[1:])]",0.5201124549,
2413,transform weight column,"# Calculate the weighted average over the palmitic column
def wfunc(f):
    return (f.palmitic*f.weight).sum()/f.weight.sum()
smallerdf.groupby('region').apply(wfunc)",0.5171462297,
2413,transform weight column,"# Define initialization function for linear modules
def init_weight(m):
    if isinstance(m, torch.nn.Linear):
        m.weight.data.fill_(0.1)
        m.bias.data.fill_(0.1)
#         print(m.weight.data, m.bias.data.item())",0.5161756277,
2413,transform weight column,"def normalizar2D(X):
    X[0] = (X[0,:] - X[0,:].mean())/X[0,:].std()
    X[1] = (X[1,:] - X[1,:].mean())/X[1,:].std()
    return X
normalizar2D(X2D)
X2D",0.5149863362,
433,day in class assignment shotgun sequencing,"def load_training_data():
    data = fetch_rcv1(subset='train')
    return data.data, data.target.toarray(), data.sample_id",0.4274595976,
433,day in class assignment shotgun sequencing,"def classification_rates(caretracker_patients,low_risk_segment):
    true_positives = len(caretracker_patients.loc[caretracker_patients.readmit30 == 1])
    false_positives = len(caretracker_patients.loc[caretracker_patients.readmit30 == 0])
    true_negatives = len(low_risk_segment.loc[low_risk_segment.readmit30 == 0])
    false_negatives = len(low_risk_segment.loc[low_risk_segment.readmit30 == 1])
    
    return true_positives,false_positives, true_negatives, false_negatives",0.4250565171,
433,day in class assignment shotgun sequencing,"# Get Data
def get_cancer_data():
    cancer = datasets.load_breast_cancer()
    data = cancer.data
    labels = cancer.target
    pd_data = pd.DataFrame(data= cancer.data,
                     columns= cancer.feature_names)
    return cancer,data,labels",0.4087285697,
433,day in class assignment shotgun sequencing,"##Write a process function to process the necessary columns such as cabin, relative and etc..
def process(df):
    
    ##Process the Cabin columns 
    df.Cabin.fillna('U', inplace=True)
    
    df['Cabin'] = df['Cabin'].map(lambda c : c[0])

    cabin_dummies = pd.get_dummies(df['Cabin'], prefix='Cabin')
    
    df = pd.concat([df,cabin_dummies], axis=1)
    
    df.drop('Cabin', axis=1, inplace=True)
    
    ##Process the Embarked column

    embark_dummies = pd.get_dummies(df['Embarked'], prefix='Embarked')
    
    df = pd.concat([df,embark_dummies], axis=1)
    
    df.drop('Embarked', axis=1, inplace=True)
    
    ##Process the PClass column
    
    pclass_dummies = pd.get_dummies(df['Pclass'], prefix='Pclass')
    
    df = pd.concat([df,pclass_dummies], axis=1)
    
    df.drop('Pclass', axis=1, inplace=True)

    ##Process the relaitve column 

    df = get_relative(df)
    
    ##Process gender
    
    df['Sex'] = df['Sex'].map({'male':1,'female':0})
    
    ##Drop unused columns 
    
    df.drop('SibSp', axis=1, inplace=True)
    df.drop('Parch', axis=1, inplace=True)
    df.drop('Name', axis=1, inplace=True)
    df.drop('Ticket', axis=1, inplace=True)
    df.drop('PassengerId', axis=1, inplace=True)
    
    return df",0.406301856,
433,day in class assignment shotgun sequencing,"def loadTestData():
    mnist23_test = pickle.load( open( ""./datasets/mnist23.data"", ""rb"" ) )
    test_x = mnist23.data
    test_y = np.array([mnist23.target])
    return test_x,test_y",0.4018577337,
433,day in class assignment shotgun sequencing,"def players_involved_oh(X_train, X_test):
    # Players involved in the game using a one-hot encoding
    pitcher_onehot = CountVectorizer(binary=True).fit(X_train.pitchers.values)
    pitcher_train = pitcher_onehot.transform(X_train.pitchers.values).todense()
    pitcher_test = pitcher_onehot.transform(X_test.pitchers.values).todense()

    batter_onehot = CountVectorizer(binary=True).fit(X_train.batters.values)
    batter_train = batter_onehot.transform(X_train.batters.values).todense()
    batter_test = batter_onehot.transform(X_test.batters.values).todense()
    
    return pitcher_train, pitcher_test, batter_train, batter_test",0.399980098,
433,day in class assignment shotgun sequencing,"def construct_pairs(lexicon, df_test):
    dict_test = list(zip(df_test.target, df_test.binary))
    return [(target, binary, lexicon[target.strip().lower()]) 
            for target, binary in dict_test]",0.3998098671,
433,day in class assignment shotgun sequencing,"def getBuy_Dates(history):
    buy_dates = history.index[history.Crossover == 1]
    return buy_dates",0.3991525173,
433,day in class assignment shotgun sequencing,"def plot_each_feature_vs_target(city_data):
    feature_names = city_data['feature_names'] 
    data = city_data.data
    targets = city_data.target
    
    for i in xrange(len(feature_names)):
        feature_data = [ datum[i] for datum in data]
        feature_name = feature_names[i]
        pl.figure()
        pl.title(""Boston House Data: "" + feature_name + "" vs MEDV"")
        pl.scatter(feature_data, city_data.target, alpha=0.2)
        pl.xlabel(feature_name)
        pl.ylabel('MEDV ($1000)')
        pl.show()",0.3973272443,
433,day in class assignment shotgun sequencing,"def mask_clouds(tile):
    # Use the Landsat QA band to mask out cloud values
    qa = tile.cells[bands[""QA""]]
    cloud = np.right_shift(qa, 14)
    result_bands = []
    for band in tile.cells[:-1]:
        band[cloud == 3] = 0
        result_bands.append(band)
    return gps.Tile.from_numpy_array(np.array(result_bands), no_data_value=0)

cloud_masked = layer.to_numpy_rdd().mapValues(mask_clouds)",0.3961405754,
669,fit the kmeans,"def find_centroids(clustered_data, num_of_clusters):
    """""" Use scipy.cluster.vq.kmeans to determine centroids of clusters
    Parameters:
        clustered_data: the data projected onto the new basis
        num_of_clusters: the expected number of clusters in the data
    Returns: 
        The centroids of the clusters
    Hint 1: make sure to first 'whiten' the data (refer to docs)
    """"""
    whitened_data = scipy.cluster.vq.whiten(clustered_data)
    return scipy.cluster.vq.kmeans(whitened_data, num_of_clusters)[0]",0.601524353,
669,fit the kmeans,"def kde(x):
    # Kernel density estimation, to get P(dI/dx | on edge) and P(dI/dx | off edge) from data
    from scipy.stats import gaussian_kde
    f = gaussian_kde(x, bw_method=0.01 / x.std(ddof=1))
    return f
    
def ponEdge(im, edgeMap):
    # Compute on edge histogram
    # im is filtered image
    
    # Convert edge map to pixel index
    flattenEdgeMap = edgeMap.flatten()
    edgeIdx = [i for i in range(len(flattenEdgeMap)) if flattenEdgeMap[i]]
    
    # find edge pixel in 3x3 region, shift the edge map a bit, in case of inaccurate boundary labeling
    [offx, offy] = np.meshgrid(np.arange(-1,2), np.arange(-1,2)); offx = offx.flatten(); offy = offy.flatten()
    maxVal = np.copy(im)
    for i in range(9):
        im1 = np.roll(im, offx[i], axis=1) # x axis
        im1 = np.roll(im1, offy[i], axis=0) # y axis    
        maxVal = np.maximum(maxVal, im1)

    vals = maxVal.flatten()
    onEdgeVals = vals[edgeIdx]
    
    bins = np.linspace(0,0.5, 100)
    [n, bins] = np.histogram(onEdgeVals, bins=bins)
    # n = n+1 # Avoid divide by zero

    pon = kde(onEdgeVals)

    return [n, bins, pon]


def poffEdge(im, edgeMap):
    flattenEdgeMap = edgeMap.flatten()
    noneEdgeIdx = [i for i in range(len(flattenEdgeMap)) if not flattenEdgeMap[i]]
    
    vals = im.flatten()
    offEdgeVals = vals[noneEdgeIdx] 

    bins = np.linspace(0,0.5, 100)
    n, bins = np.histogram(offEdgeVals, bins=bins)

    # n = n+1
    # p = n / sum(n)
    
    poff = kde(offEdgeVals)
    
    return [n, bins, poff]

dx = dIdx(im)
[n1, bins, pon] = ponEdge(dx, edgeMap)
[n2, bins, poff] = poffEdge(dx, edgeMap)

plt.figure(); # Plot on edge
# title('(Normalized) Histogram of on/off edge pixels')
plt.plot((bins[:-1] + bins[1:])/2, n1.astype(float)/sum(n1), '-', lw=2, label=""p(f|y=1)"")
plt.plot((bins[:-1] + bins[1:])/2, n2.astype(float)/sum(n2), '--', lw=2, label=""p(f|y=-1)"")
plt.legend()

plt.figure()
# title('Density function of on/off edge pixels')
plt.plot(bins, pon(bins), '-', alpha=0.5, lw=3, label=""p(f|y=1)"")
plt.plot(bins, poff(bins), '--', alpha=0.5, lw=3, label=""p(f|y=-1)"")
plt.legend()",0.5969701409,
669,fit the kmeans,"class Kmeans:
    
    # Kmeans++ initialization
    def init(self, X, n):
        (N, d) = X.shape
        P = np.empty((n, d))
        P[0] = X[np.random.randint(0, N)]

        D = math.inf * np.ones(N)

        for i in range(n - 1):
            T = X - P[i]
            D = np.minimum(D, np.sum(T * T, axis=1))

            W = D.cumsum()

            z = np.random.uniform(0, W[-1])
            j = np.searchsorted(W > z, True)
            P[i + 1] = X[j]

        return P

    def __init__(self, X, n, max_iter = 100, restarts=10, e=0.1):
        H_best = math.inf
        for i in range(restarts):
            (mu, Y, H, n_iter) = self.run(X, n, max_iter, e)
            if (H < H_best):
                H_best = H
                self.mu = mu
                self.Y = Y
                self.H = H
                self.n_iter = i
        

    def _assign(self, X, mu):

        D = X[:, None, :] - mu[None, :, :]
        D = np.sum(D * D, axis=2)

        Y = np.argmin(D, axis=1)
        H = np.sum(np.min(D, axis=1))
        
        return (Y, H)
        
            
    def run(self, X, n, max_iter = 100, e = 0.1):
        (N, d) = X.shape;

        mu = self.init(X, n)
        #mu = X[np.random.choice(N, n, replace=False)]

        for i in range(max_iter):

            (Y, H) = self._assign(X, mu)
            
            Z = np.zeros((N, d))
            Z[np.arange(N), Y] = 1

            old_mu = mu
            mu = Z.T.dot(X) / Z.sum(axis = 0)[:,None]

            if (np.max(mu - old_mu) < e):
                break

        (Y, H) = self._assign(X, mu)
                
        return (mu, Y, H, i)",0.5914086103,
669,fit the kmeans,"def get_pdf(data):
    from scipy.stats.kde import gaussian_kde
    from numpy import linspace
    
    kde = gaussian_kde(data)

    return kde",0.589386642,
669,fit the kmeans,"def returnHighlightedLane(img):
    global l_fit
    global r_fit
#     fig, axis = plt.subplots(1,2,figsize=(16,16))
    undistorted_img = undist(img)
    warped = cv2.warpPerspective(undistorted_img, M, (X_size, Y_size), flags=cv2.INTER_LINEAR)

    binary_output = getLanes(warped)
    
    l_fit, r_fit, out_img = firstFrame(binary_output)
    result, curve_highlight, ploty,l_fit_s, r_fit_s = unwarpedLanes(img, binary_output, l_fit, r_fit)
    
    resized_boxes = cv2.resize(curve_highlight,( int(X_size/8), int(Y_size/8)), interpolation = cv2.INTER_CUBIC)
    resized_warp = cv2.resize(warped,( int(X_size/8), int(Y_size/8)), interpolation = cv2.INTER_CUBIC)
    resized_binary = cv2.resize(binary_output,( int(X_size/8), int(Y_size/8)), interpolation = cv2.INTER_CUBIC)
    
    return result, out_img, ploty, l_fit_s, r_fit_s, curve_highlight
    
result, out_img, ploty, l_fit_s, r_fit_s, curve_highlight = returnHighlightedLane(test_images[7])
fig, axis = plt.subplots(1,2,figsize=(20,20))
axis[0].imshow(out_img)
axis[0].plot(l_fit_s, ploty,'y')
axis[0].plot(r_fit_s, ploty,'y')
axis[1].imshow(curve_highlight)
axis[1].plot(l_fit_s, ploty,'y')
axis[1].plot(r_fit_s, ploty,'y')",0.5892307758,
669,fit the kmeans,"def getDensity(df):
    data = df#_log_prob['log.prior']
    density = scipy.stats.gaussian_kde(data)
    width = np.max(data) - np.min(data)
    xs = np.linspace(np.min(data)-width/5, np.max(data)+width/5,600)
    density.covariance_factor = lambda : .25
    density._compute_covariance()
    return xs, density(xs)",0.5887598991,
669,fit the kmeans,"# The elbow method looks at the percentage of variance explained as a function of the number of clusters
# We're looking for the point at which marginal gain drops, giving an angle in the graph
# Our methodology is inspired by Lab 8 from Applied Data Science with Prof. Sobolevsky
data = econ

def elbow(data,K):
    KM = [cluster.KMeans(n_clusters=k).fit(data) for k in K]
    centroids = [k.cluster_centers_ for k in KM] # Save the centroids for each model with a increasing k 
    D_k = [cdist(data, cent, 'euclidean') for cent in centroids] # For each k, get distance b/w data & each center
    globals()['dist'] = [np.min(D,axis=1) for D in D_k] # distance to nearest centroid
    avgWithinSS = [sum(d)/data.shape[0] for d in dist]  # calculate average sum of squared error (SSE)
    
    # ""elbow"" curve
    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.plot(K, avgWithinSS, 'k*-')
    plt.grid(True)
    plt.xlabel('Number of clusters', fontsize = 16)
    plt.ylabel('Avg. Within-Cluster SSE', fontsize = 16)
    plt.title('Elbow for K-Means Clustering of Socioeconomic Features', fontsize = 18)
    plt.show()
    
elbow(data, range(1,30))",0.5861679316,
669,fit the kmeans,"class KMeans():
    """"""Implementation of unsupervised Machine Learning algorithm K-Means used for data 
    clustering. Data is aggregated into 'k' groups. Data sample is assigned to group based
    in it's distance from centroid which is initialized randomly at start and then changed
    by calculating mean of samples assigned to it's group.
    
     Arguments:
    ---------
    k: int
        Number of centroids/data clusters.
    iteration_limit: int
        Maximum numbers of iterations that algorithm can perform in order to reach convergence.     
    """"""
    @staticmethod
    def _l2_euclidean_distance(p, q):
        """"""Implementation of similarity function using L2 - Euclidean Distance.""""""
        return np.sqrt(np.sum(np.square(p - q)))
    
    def __init__(self, k=2, iteration_limit=300):
        self.k = k
        self.iteration_limit = iteration_limit 
        self.centroids = dict()
        self.iter_done = 0
        
    def _init_centroids(self, data):
        """"""Picking self.k random samples from data and setting them as centroids.""""""
        samples_num, _ = data.shape
        for i in range(self.k):
            # new centroids are randomly picked samples from inserted data
            self.centroids[i] = data[np.random.randint(0, samples_num)]
    
    def _create_clusters(self, data):
        """"""Assigning data samples to clusters based on their distance to centroids.""""""
        
        # init cluster containers with empty lists
        clusters = dict()
        for k in range(self.k):
            clusters.setdefault(k, list())
            
        # get closest cluster to each sample and store it's index in clusters dictionary
        for sample_index, sample in enumerate(data):
            assigned_centroid_index = self._assign_closest_centroid(sample)
            clusters[assigned_centroid_index].append(sample_index)
        return clusters
    
    def _assign_closest_centroid(self, sample):
        """"""Returning index of closest centroid to inserted sample.""""""
        closest_centroid_index = 0
        closest_dist = None
        for centroid_index, centroid in self.centroids.items():
            
            # calculating distance of samples from cluster
            dist = self._l2_euclidean_distance(sample, centroid)
            
            # updated closest centroid index based on calculated dist value
            if closest_dist is None:
                closest_dist = dist
                closest_centroid_index = centroid_index
            elif dist < closest_dist:
                closest_centroid_index = centroid_index
                closest_dist = dist
                
        return closest_centroid_index
    
    def _calculate_centroids(self, data, clusters):
        """""" Calculate new centroids based on mean of the samples in every cluster.""""""
        for k in range(self.k):
            # new centroid is an artificial sample created from mean values of features
            # of all samples in cluster
            _, features_num = data.shape
            self.centroids[k] = np.mean(data[clusters[k]], axis=0) if clusters[k] else np.zeros(features_num)        

    def predict(self, data):
        """"""Assigning indexes of inserted data samples into self.k clusters.""""""
        
        # randomly initializing clusters
        self._init_centroids(data)
        
        # updating clusters until interation_limit is reached or nothing was changed
        for i in range(self.iteration_limit):
            
            # tracking number of iterations
            self.iter_done = i
            
            clusters = self._create_clusters(data)
            previous_centroids = copy.deepcopy(self.centroids)
            self._calculate_centroids(data, clusters)
            
            # stopping if centroids didn't change
            if all(previous_centroids[k].tolist() == self.centroids[k].tolist() for k in self.centroids.keys()):
                break
                
        return clusters",0.5859166384,
669,fit the kmeans,"def kpca_transformed(dataset_name, kernel_name, gamma):
  kpca = KernelPCA(n_components=2, kernel=kernel_name, gamma=gamma)

  X, y = datasets[dataset_name]
  X_transformed = kpca.fit_transform(X)
  
  return X, X_transformed, y",0.5855808854,
669,fit the kmeans,"def contraction(sigma,alpha):
    expdelta0 = deltam
    expdelta0.shape = (nrow,1)
    draws = v * np.kron(np.ones((ns,1)),sigma)
    expmu = (np.exp(np.dot(X,draws.T) - np.dot(price,np.ones((1,ns))) * alpha / y))
    dif = 1
    it = 0
    while(dif > tol and it < 1000):
        rat = s_data/s_comp(expdelta0,expmu)
        rat.shape = (nrow,1)
        expdelta1 = expdelta0 * rat
        dif = (abs(expdelta1/expdelta0-1)).max()
        expdelta0 = expdelta1
        it += 1
    return(np.log(expdelta0))",0.5850578547,
1523,plot the rolling_mean for weekly_sales what general trends do you observe?,"pd.rolling_mean(test_series.sort_index(),freq='10ms',window=1).plot(figsize=(12,6))
pd.expanding_mean(test_series.sort_index(),freq='10ms').plot(figsize=(12,6))",0.5818424225,
1523,plot the rolling_mean for weekly_sales what general trends do you observe?,"BAC['Close'].loc['2008-01-01':'2009-01-01'].rolling(window = 10).mean().plot(figsize=(20,10), label = '10 day MA')
BAC['Close'].loc['2008-01-01':'2009-01-01'].rolling(window = 30).mean().plot(figsize=(20,10), label = '30 day MA')
BAC['Close'].loc['2008-01-01':'2009-01-01'].rolling(window = 50).mean().plot(figsize=(20,10), label = '50 day MA')

BAC['Close'].ix['2008-01-01':'2009-01-01'].plot(figsize=(20,10),label ='BAC Close');
plt.legend()",0.5786244869,
1523,plot the rolling_mean for weekly_sales what general trends do you observe?,"gs.Close.plot(label='Raw', figsize=(14, 6))
gs.Close.rolling(28).mean().plot(label='28D MA')
gs.Close.expanding().mean().plot(label='Expanding Average')
gs.Close.ewm(alpha=0.03).mean().plot(label='EWMA($\\alpha=.03$)')

plt.legend(bbox_to_anchor=(1.25, .5))
plt.tight_layout()
plt.ylabel(""Close ($)"")",0.5750232935,
1523,plot the rolling_mean for weekly_sales what general trends do you observe?,"def plot_t_series_last_sign_up(cohort, city_name=None):
    '''
    I:DataFrame, city name (string)
    O:None, just plots the daily resampled value of the time-series
    '''
    if city_name:
        cohort = cohort[churned_cohort.city == city_name]
    cohort_t_series = cohort[['last_trip_date']]
    cohort_t_series['count'] = 1
    cohort_t_series.index = cohort_t_series.last_trip_date
    del cohort_t_series['last_trip_date']

    # time-frame it by window
    cohort_t_series.resample('D', how='sum').plot()
    ylabel('number of last sign-ins')
    if city_name: 
        title(city_name)
    else:
        title('All Cities')
    ;",0.5741300583,
1523,plot the rolling_mean for weekly_sales what general trends do you observe?,"dfTrain['Mean_TemperatureF'].resample('D').mean().rolling(window=7, center=True).mean().plot(title='Average for Daily and Monthly')
dfTrain['Mean_TemperatureF'].resample('W').mean().rolling(window=7, center=True).mean().plot().legend(loc='center left', bbox_to_anchor=(1, 0.5))",0.5729572773,
1523,plot the rolling_mean for weekly_sales what general trends do you observe?,"@interact
def byMonth(month=month_list, interval=['H','D','W']):
    df.loc[month].resample(interval).sum().plot()",0.5717545748,
1523,plot the rolling_mean for weekly_sales what general trends do you observe?,"stock_data.Close.plot(label='Raw Close Data')
stock_data.Close.rolling(15).mean().plot(label='15-Day Moving Average')
stock_data.Close.expanding().mean().plot(label='Expanding Average')
stock_data.Close.ewm(alpha=0.05).mean().plot(label='EWMA($\\alpha=.05$)')

plt.legend(bbox_to_anchor=(1.25, .5))
plt.tight_layout()
plt.ylabel(""Closing Price"")
sns.despine()",0.5711565018,
1523,plot the rolling_mean for weekly_sales what general trends do you observe?,"bt.plot_trades()
ohlc.C.rolling(short_ma).mean().plot(c='green')
ohlc.C.rolling(long_ma).mean().plot(c='blue')
plt.legend(loc='upper left')
pass",0.5706398487,
1523,plot the rolling_mean for weekly_sales what general trends do you observe?,"BAC[""Close""].loc[""2008-01-01"" : ""2009-01-01""].rolling(window = 30).mean().plot(label = ""30 day moving average"")
BAC[""Close""].loc[""2008-01-01"" : ""2009-01-01""].plot(label = ""BAC Close"")
plt.legend()",0.567111969,
1523,plot the rolling_mean for weekly_sales what general trends do you observe?,"def plot_freq_on_log_log_scale(value_counts_series, title):
    my_alpha = 0.25
    plt.scatter(value_counts_series.index.values, value_counts_series.values, c=""b"", alpha=my_alpha)
    plt.xscale(""log"")
    plt.yscale(""log"")
    plt.xlabel('Traffic volume')
    plt.ylabel('Frequency')
    plt.grid(True)
    plt.title(title);",0.5670726299,
1383,part basic stats,"def print_statistics_fast(corpus):
    # data structures for counting words
    word_list = {}

    # calculate statistics of words
    for word in corpus:
        try:
            word_list[word] += 1
        except KeyError:
            word_list[word] = 1
            
    # create a list of indices sorted according to word occurrances
    sorted_indices = sorted(word_list.items(), key=lambda (k,v): v)

    # only keep last ten entries of the index list
    sorted_indices = sorted_indices[-10:]

    # switch order of list
    sorted_indices = sorted_indices[::-1]

    print('most common words:')
    for word, count in sorted_indices:
        print '%s (%d)' % (word, count)
    print '\nOverall there are %d unique words in the corpus.' % len(word_list)

%lprun -f print_statistics_fast print_statistics_fast(brown)",0.470908612,
1383,part basic stats,"def printInfo(track):
    print(""\n#####################"")
    print(""Name: "", track.name)
    print(""isDrum: "", track.is_drum)
    print(""Program: "", track.program)
    if len(track.pianoroll):
        print(""Active length: "", track.get_active_length())
        print(""Active pitch range: "", track.get_active_pitch_range())
    else:
        print(""Empty pianoroll."")
    print("""")

# Pick a file to look at
filename = random.choice(os.listdir(DATA_DIR))
print(filename)
# Load pianoroll file as a multitrack object
multi = pypianoroll.Multitrack(os.path.join(DATA_DIR, filename))
print(""Tempo: "", multi.tempo)
print(""Beat resolution: "", multi.beat_resolution)
# Show some info about the pianoroll
for track in multi.tracks:
    printInfo(track)",0.4698674679,
1383,part basic stats,"def marginal_probabilities(mcmc_result):
    """"""Return estimators for marginal probabilities knowing some set of evidences
    :param mcmc_result: An object of class pymc.MCMC
    :return: a dictionary where each key is the marginal probability that the node is True knowing some set of evidence """"""
    dict_mean = {}
    for key, value in mcmc_result.stats().iteritems():
        dict_mean[key] = value['mean']
    return dict_mean

def result_data_frame(mcmc_result):
    """"""
    Concatenate every sampled Random Variables with computed probabilities.
    :param mcmc_result: An object of class pymc.MCMC
    :return: a dataframe containing every sampled Random Variables.
    """"""
    trace_matrix = {}
    for name in mcmc_result.stats().keys():
        trace_matrix[name] = mcmc_result.trace(name)[:]
    return pd.DataFrame.from_dict(trace_matrix)

def query_prob(mcmc_result, a, b, value):
    """"""
    Compute the probabilities of succeeding at question or capsule a knowing that you succeeded another capsule or question.
    :param a: Query parameter : What you want to know
    :param b: Information parameter : With what information
    :value: Value of parameter b : True or False
    :param mcmc_result: Result of the MCMC procedure
    :return: P(a|b = value)
    """"""
    df = result_data_frame(mcmc_result)
    b_values = (df[b] == value)
    a_values = (df[a] == True)
    return float(df[b_values & a_values].shape[0]) / df[b_values].shape[0]

def query_prob_post_p_ii(mcmc_result, H, N):
    """"""
    Compute every a posteriori probabilities for questions, knowing the outcome for parent capsule
    :param mcmc_result: Result of the MCMC procedure
    :evidences: A list containing for each question whether the answer has already been answered or not
    :H: Number of capsules
    :N: List of number of questions for each capsule
    :result: A tuple containing the probability of answering correctly 
    knowing that the capsule is not mastered 
    and the probability of answering correctly
    knowing that the capsule is mastered.
    If one question is already answered it will return -1e100 in order to make fisher info as small as possible.
    """"""
    p_0 = []
    p_1 = []
    for i in range(H):
        capsule = ""Theta_%d"" % (i+1)
        p_0i = []
        p_1i = []
        for j in range(N[i]):
            p_0i.append(query_prob(mcmc_result, ""U_%d%d"" % (i+1, j+1), capsule, False))
            p_1i.append(query_prob(mcmc_result, ""U_%d%d"" % (i+1, j+1), capsule, True))
        p_0.append(p_0i)
        p_1.append(p_1i)
    return p_0, p_1",0.4693185985,
1383,part basic stats,"def getMaskNums(df):
    # pre-formatted dict to write to
    d = {0:{'hap':{}, 'sad':{}, 'ang':{}, 'fea':{}, 'dis':{}, 'sup':{}, 'ntr':{} },
         1:{'hap':{}, 'sad':{}, 'ang':{}, 'fea':{}, 'dis':{}, 'sup':{}, 'ntr':{} }
        }

    # loop trough all rows
    for i,ident in enumerate(df.index.labels[0]):
        # get the current emotion and trial number
        express = df.index.labels[1][i]
        trial =  df.index.labels[2][i]
        # get the string emotion name corresponding to the number
        actual = myLabels[express]
        # get the decision
        decision = df.ix[ident].ix[express].ix[trial][""button""]
        # get the number of revealed tiles
        maskNum = df.ix[ident].ix[express].ix[trial][""maskNum""]
    
        # put number of revealed tiles into dict, according to what face it
        # belongs to and what answer was given
        try:
            d[ident][actual][decision].append(maskNum)
        except:
            d[ident][actual][decision] = [maskNum]

    return d",0.4629706144,
1383,part basic stats,"class Stats():
    
    # initializes the statistics bureau
    def __init__(self):
        self.data = {} # a dictionary that holds different data structures
        self.data['u'] = 1 # the latest unemployment rate
        self.data['U'] = 1 # the latest unemployment level
        self.data['V'] = 1 # the number of vacancies
        self.data['Vs'] = [1 for firm in firms]
        self.data['M'] = 1 # the latest number of matches
        self.data['ts_u'] = [] # list that stores the time series of the unemployment rate
        self.data['ts_v'] = [] # list that stores the time series of the vacancy rate
        
    # counts the number of employed
    def count_employed(self):
        return sum([worker.is_employed() for worker in workers])
    
    # counts the number of unemployed
    def count_unemployed(self):
        return sum([not worker.is_employed() for worker in workers])
    
    # counts the vacancies per firm
    def count_vacancies(self):
        return [firm.V for firm in firms]
    
    # counts the total vacancies
    def sum_vacancies(self):
        return sum(count_vacancies())
        
    # computes the unemployment rate
    def unemployment_rate(self):
        return self.count_unemployed()/L
    
    # computes the average wage
    def average_wage(self):
        return sum([worker.w for worker in workers if worker.is_employed()])/self.count_employed()
    
    # computes the average firm size
    def average_firm_size(self):
        return sum([firm.size() for firm in firms])/N
    
    # what the statistics bureau does every period
    def step(self):
        self.data['Vs'] = self.count_vacancies()
        self.data['V'] = sum(self.data['Vs'])
        self.data['U'] = self.count_unemployed()
        self.data['M'] = market.matches(self.data['U'], self.data['V'])
        self.data['u'] = self.data['U']/L
        self.data['ts_u'].append(self.data['u'])
        self.data['ts_v'].append(self.data['V']/L)
    
    # the theoretical unemployment rate from Pissarides' model
    def theory_u(self):
        return (Lambda*(w-w*Alpha+y*(-1+Alpha+c*(r+Lambda))))/(-(w-y)*(-1+Alpha)*Lambda+c*y*(r+Lambda)*(Alpha+Lambda))
    
    # the theoretical unemployment rate from Pissarides' model
    def theory_v(self):
        return ((w-y)*Alpha*Lambda)/((w-y)*(-1+Alpha)*Lambda-c*y*(r+Lambda)*(Alpha+Lambda))",0.4601336718,
1383,part basic stats,"def dataInfo(tmp):
    
    #Data Summary by Column, Null v Not Null Counts
    print ('{0:3} {1!s:16} {2:8} {3:8} {4:8} {5:8} {6:8}'.format('idx','dtype','  Null', 'Not Null', 'Available','Unique','Column Name'))
    for i, column in enumerate(tmp.columns):
        print ('{0:3} {1!s:16} {2:8} {3:8} {4:8} {5:8} {6:8}'.format(i, tmp[column].dtype,
                                                             tmp[column].isnull().sum(), 
                                                             tmp[column].notnull().sum(),
                                                             round(tmp[column].notnull().sum()/tmp[column].shape[0],3),
                                                             tmp[column].unique().shape[0],
                                                              column))
    del tmp

#define confusion matrix plotting function
labels = ['Catastrophic','Bronze','Silver','Gold','Platinum']
def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues,labels=labels):
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title, fontsize = 16)
    plt.colorbar()
    tick_marks = np.arange(len(labels))
    plt.xticks(tick_marks, labels, rotation=45, fontsize = 14)
    plt.yticks(tick_marks, labels, fontsize = 14)
    plt.tight_layout()
    plt.ylabel('True label', fontsize = 16)
    plt.xlabel('Predicted label', fontsize = 16)",0.4495266378,
1383,part basic stats,"def get_positions_statistics(promoters_dictionary):
    """"""
    Receives a dictionary representing a promoters fasta file,
    and returns the frequencies of possible nucleotides in 
    each variable position.
    """"""
    # define a  dictionary for each position, to store the nucleotide frequencies
    # D position
    D_dict = {'A':0, 'G':0, 'T':0}
    # M position
    M_dict = {'A':0, 'C':0}
    # R position
    R_dict = {'A':0, 'G':0}
    # S position
    S_dict = {'C':0, 'G':0}
    
    # iterate over promoters
    for p in promoters_dictionary: #iterates over the keys ==> p is a different key in every iteration!
        # if promoter includes the GATA-4 motif
        sequence = promoters_dictionary[p]
        if check_for_GATA4(sequence):
            # get variable nucleotides in promoter
            motif_regex = re.compile(r'AGATA([AG])([AC])AG([AG])([CG])A') # notice the parentheses
            result = re.search(motif_regex,sequence)
            # insert to dictionaries
            D = result.group(1)
            D_dict[D] += 1
            M = result.group(2)
            M_dict[M] += 1
            R = result.group(3)
            R_dict[R] += 1
            S = result.group(4)
            S_dict[S] += 1
            
    return D_dict, M_dict, R_dict, S_dict",0.4471447468,
1383,part basic stats,"def SurgeryTE(self):
    #reassign the IOP based on the normal distribution reported by van Gestel
    self.Attribute['IOP'] = random.normal(12.5,0.3)
    self.medicalRecords['ContinueTreatment'] = False
    self.medicalRecords['OnTrabeculectomy'] = True
    self.medicalRecords['MedicationIntake'] = 0
    self.medicalRecords['NumberTrabeculectomy'] +=1
    self.medicalRecords['CurrentMedicationType'] = 30
    self.medicalRecords['TreatmentOverallStatus'] = 0
    #nullify any side effect left as patients no longer on medication
    self.params['SideEffect'] = 0",0.4458505213,
1383,part basic stats,"# Two Sample T Test
    from scipy import stats

    # General Stats

    ## Sample Data 1
    mu_1 = 595
    s_1 = 29
    n_1 = 5
    data1 = stats.t.rvs(loc=mu_1, scale=s_1, df=(n_1-1), size=n_1)

    ## Sample Data 2
    mu_2 = 512
    s_2 = 19 
    n_2 = 5
    data2 = stats.t.rvs(loc=mu_1, scale=s_1, df=(n_1-1), size=n_1)


    df_ = n_1 + n_2 - 2
    alpha = alpha

    # plot distribution for normality
    #f, (ax1, ax2) = plt.subplots(2, figsize=(15,8))
    #sns.distplot(data1, kde=True, fit=stats.norm, ax = ax1);
    #sns.distplot(data2, kde=True, fit=stats.norm, ax = ax2);
    mystats.descriptive_statistics_plots(data1)
    mystats.descriptive_statistics_plots(data2)


    # tt = (sm-m)/np.sqrt(sv/float(n))  # t-statistic for mean
    tt = (mu_1 - mu_2) / (np.sqrt(((s_1**2)/n_1) + ((s_2**2)/n_2)))
    print(""T Test Score: "", tt)

    #Studnt, n=14, p<0.05, 2-tail
    t_crit = stats.t.ppf(1-alpha/2, df_)
    print(""T Critical Value: "", t_crit)

    # p value two sided 
    pval = stats.t.sf(np.abs(tt), df_)*2
    print(""p-value: "", pval)

    # confidence interval
        ## Pooled Standard Deviaiton
    sp = np.sqrt((((n_1-1)*(s_1**2)) + ((n_2-1)*(s_2**2)))/df_ ) # pooled variance (ratio of variances is less than 1)
    se = sp * np.sqrt((1/n_1 + 1/n_2))

    diff_ = mu_1 - mu_2

    ci_low = diff_ - (t_crit * se)
    ci_high = diff_ + (t_crit * se)
    print(""Confidence Interval from: %.3f to %.3f"" % (ci_low, ci_high))",0.4457642734,
1383,part basic stats,"def get_features(data):
    roads = collections.defaultdict(
        lambda: {'num_truck': 0, 'num_pv': 0, 'speeds': [],
                 'label': -1})

    prev = None
    data_tqdm = tqdm(data)
    data_len = len(data)
    for idx, datum in enumerate(data_tqdm):
        data_tqdm.set_description_str('Processing datum {}/{}'
                                      .format(idx, data_len))

        date, trip_char, road_id, lon, lat, vehicle_type, label =\
            datum

        roads[road_id]['label'] = label

        # add to vehicle count
        vehicle_key = 'num_truck' if vehicle_type == 'truck' else 'num_pv'
        roads[road_id][vehicle_key] += 1

        # add to speeds if not last point
        part_of_same_trip = (prev
                             and (prev[Column.TRIP_CHAR] == 's'
                                  or prev[Column.TRIP_CHAR] == 'm')
                             and (trip_char == 'm' or trip_char == 'e'))
        if part_of_same_trip:
            time_diff = date - prev[Column.DATE]
            hours = time_diff.seconds / 60 / 60

            prev_lat = prev[Column.LAT]
            prev_lon = prev[Column.LON]
            miles = distance_on_unit_sphere(lat, lon, prev_lat,
                                            prev_lon)

            # things happen
            if miles == 0 or hours == 0:
                continue

            speed = miles / hours
            roads[road_id]['speeds'].append(speed)

        prev = datum

    features = []
    labels = []
    roads_len = len(roads)
    roads_tqdm = tqdm(enumerate(roads.items()))
    for i, (k, v) in roads_tqdm:
        roads_tqdm.set_description_str('Creating feature vector {}/{}'
                                       .format(i, roads_len))

        num_truck, num_pv, speeds, label = (v['num_truck'], v['num_pv'],
                                            v['speeds'], v['label'])
        speeds.sort()

        # for some reason some roads have no speeds?
        if not speeds:
            continue

        features.append([
            # 1/4 speed
            speeds[int(len(speeds) / 4)],
            # median speed
            statistics.median(speeds),
            # 3/4 speed
            speeds[int((len(speeds) / 4) * 3)],
            # average speed
            statistics.mean(speeds),
            # existance of pv
            1 if num_pv > 0 else 0,
            # existance of truck
            1 if num_truck > 0 else 0,
            # percentage of pv on road
            num_pv / (num_pv + num_truck),
            # percentage of truck on road
            num_truck / (num_pv + num_truck)])
        labels.append(label)

    return (features, labels)",0.4435294569,
600,exponentialy weighted rolling functions,"def limit(x, function=function):
    lower_limit = f(x - .00000001, function=function)
    upper_limit = f(x + .00000001, function=function)
    if np.abs(upper_limit - lower_limit) > 100000:
        print('No limit exists')
        return None
    else:
        L = (lower_limit + upper_limit)/2
        return round(L, 4)",0.5422492623,
600,exponentialy weighted rolling functions,"def logistic(z, k=1.0):
    return 1/(1 + np.exp(-k*z))",0.5374687314,
600,exponentialy weighted rolling functions,"@custom_model
def LogConst1D(x, log_amplitude=0.0):
    return np.ones_like(x) * np.exp(log_amplitude)",0.5362232327,
600,exponentialy weighted rolling functions,"def phi(x,slope=1.0):
    return 1.0/(1.0+np.exp(-slope*x))",0.5351412296,
600,exponentialy weighted rolling functions,"def p(t,a=1.,t0=0.): return 1./(1.+np.exp(-a*(t-t0)))

plt.figure(figsize=(12.5, 4))
plt.xlim(-7,7)
xr=np.arange(-7.,7.1,.1)
plt.plot(xr,p(xr),'r--');",0.533416152,
600,exponentialy weighted rolling functions,"# This function to generate evidence is used for the first example
def relu_evidence(logits):
    return tf.nn.relu(logits)

# This one usually works better and used for the second and third examples
# For general settings and different datasets, you may try this one first
def exp_evidence(logits): 
    return tf.exp(tf.clip_by_value(logits,-10,10))

def KL(alpha):
    beta=tf.constant(np.ones((1,K)),dtype=tf.float32)
    S_alpha = tf.reduce_sum(alpha,axis=1,keep_dims=True)
    S_beta = tf.reduce_sum(beta,axis=1,keep_dims=True)
    lnB = tf.lgamma(S_alpha) - tf.reduce_sum(tf.lgamma(alpha),axis=1,keep_dims=True)
    lnB_uni = tf.reduce_sum(tf.lgamma(beta),axis=1,keep_dims=True) - tf.lgamma(S_beta)
    
    dg0 = tf.digamma(S_alpha)
    dg1 = tf.digamma(alpha)
    
    kl = tf.reduce_sum((alpha - beta)*(dg1-dg0),axis=1,keep_dims=True) + lnB + lnB_uni
    return kl

def mse_loss(p, alpha, global_step, annealing_step): 
    S = tf.reduce_sum(alpha, axis=1, keep_dims=True) 
    E = alpha - 1
    m = alpha / S
    
    A = tf.reduce_sum((p-m)**2, axis=1, keep_dims=True) 
    B = tf.reduce_sum(alpha*(S-alpha)/(S*S*(S+1)), axis=1, keep_dims=True) 
    
    annealing_coef = tf.minimum(1.0,tf.cast(global_step/annealing_step,tf.float32))
    
    alp = E*(1-p) + 1 
    C =  annealing_coef * KL(alp)
    return (A + B) + C",0.5330743194,
600,exponentialy weighted rolling functions,"# This has everything fixed except for the functionparams a,b,c which are amplitude, mean and standard deviation
def expo(x,expoparams=[1,1]):
    return np.multiply(expoparams[0],np.exp(np.divide(-x,expoparams[1])))

def expo_plus_gaus(x,params=[1,1,1,1,1]):
    return np.add(np.multiply(params[0],np.exp(np.divide(-x,params[1]))),np.multiply(params[2],np.exp(-np.divide(np.power(np.subtract(x,params[3]),2),2*np.power(params[4],2)))))

#The sum of both individual chi2 is the combined chi2
def combined_chisquare(a,b,c,d,e):
    return chisquare_1d(expo,[a,b],xdata,ydata,ydataerr)[0]+chisquare_1d(expo_plus_gaus,[a,b,c,d,e],xdata,ydata,ydataerr)[0] 

describe(combined_chisquare) # What does minuit see?",0.5323890448,
600,exponentialy weighted rolling functions,"def pi(x,mean=np.zeros(2),V=example_V):
    return np.exp(-.5*np.dot(x,np.dot(V,x)))",0.5314015746,
600,exponentialy weighted rolling functions,"def gaussian(mean=0.0,sigma=1.0,x=x):
    ## gaussian function 
    z = (tf.exp(tf.negative(tf.pow(x - mean, 2.0) /
                       (2.0 * tf.pow(sigma, 2.0)))) *
         (1.0 / (sigma * tf.sqrt(2.0 * 3.1415))))
    return z",0.5307955742,
600,exponentialy weighted rolling functions,"def identity_basis_function(x):
    return x


def gaussian_basis_function(x, mu, sigma=0.1):
    return np.exp(-0.5 * (x - mu) ** 2 / sigma ** 2)


def polynomial_basis_function(x, degree):
    return x ** degree


def expand(x, bf, bf_args=None):
    if bf_args is None:
        return np.concatenate([np.ones(x.shape), bf(x)], axis=1)
    else:
        return np.concatenate([np.ones(x.shape)] + [bf(x, bf_arg) for bf_arg in bf_args], axis=1)",0.5275839567,
2021,step by step guide of machine learning in python,"# Cross Entropy cost function
cost_fn = tf.reduce_sum(tf.pow(y_ - y, 2))/(2*batch_size)

# Define model optimiser
optimiser = tf.train.AdamOptimizer(learning_rate).minimize(cost_fn)

correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

# Initialize variables and tensorflow session
init = tf.global_variables_initializer()
sess.run(init)",0.5676215887,
2021,step by step guide of machine learning in python,"learning_rate = tf.train.exponential_decay(init_lr,
 global_step,
 decay_steps,
 decay_rate,
 staircase=True)
optimizer = tf.train.AdamOptimizer(learning_rate)",0.5664184093,
2021,step by step guide of machine learning in python,"learning_rate = tf.train.exponential_decay(init_lr,
                                           global_step,
                                           decay_steps,
                                           decay_rate,
                                           staircase=True)
optimizer = tf.train.AdamOptimizer(learning_rate)",0.5664184093,
2021,step by step guide of machine learning in python,"optimizer = tf.train.AdamOptimizer(lr).minimize(loss, global_step=global_step)",0.5656868815,
2021,step by step guide of machine learning in python,"optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss, global_step=global_step)",0.5656868815,
2021,step by step guide of machine learning in python,"learning_rate = 0.003
train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy, global_step=global_step)",0.5642534494,
2021,step by step guide of machine learning in python,"# create optimizer
adam_optimizer = paddle.optimizer.Adam(
    learning_rate=2e-3,
    regularization=paddle.optimizer.L2Regularization(rate=8e-4),
    model_average=paddle.optimizer.ModelAverage(average_window=0.5))

# create trainer
trainer = paddle.trainer.SGD(cost=cost,
                                parameters=parameters,
                                update_equation=adam_optimizer)",0.5641813278,
2021,step by step guide of machine learning in python,"optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(loss, global_step=global_step)",0.56401968,
2021,step by step guide of machine learning in python,"#all network weights
all_weights = [w_i, b_i, w_o,b_o]

#weight updates. maximizing J is same as minimizing -J. Adding negative entropy.
loss = -J -0.1 * entropy

update = tf.train.AdamOptimizer().minimize(loss,var_list=all_weights)",0.5625122786,
2021,step by step guide of machine learning in python,"optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost, global_step=global_step)",0.5624450445,
984,investor flow of funds us,"def compute_exact_solution(gD, gN): 
    x = sym.Symbol('x')
    b, c = sym.symbols('b, c')
    gen = x**2 + b*x + c

    # apply BCs to general solution
    system = []
    system.append(ddx(gen).subs(x, 0) - gN) # du/dx @ x=0 = 1
    system.append(gen.subs(x, 1) - gD)      # u     @ x=1 = 0

    coeffSolution = linsolve(system, (b, c))
    c1, c2 = next(iter(coeffSolution))
    u_exact = gen.subs([(b, c1),(c, c2)])
    u_ex = sym.lambdify(x, u_exact, 'numpy') # vectorize exact solution
    return u_exact, u_ex",0.4208990633,
984,investor flow of funds us,"class Wallet:
    
    def __init__(self, owner, cash):
        self.owner = owner
        self.cash = cash",0.4093409479,
984,investor flow of funds us,"def color_threshold(img,color_thresholds):
        # Read in the saved objpoints and imgpoints
        dist_pickle = pickle.load( open( ""calibration_pickle.p"", ""rb"" ))
        mtx=dist_pickle[""mtx""]
        dist=dist_pickle[""dist""]
        undistorted_img_bgr = cv2.undistort(img, mtx, dist, None, mtx) # bgr image

        # Initialize color channels    
        hls = cv2.cvtColor(undistorted_img_bgr, cv2.COLOR_BGR2HLS)
        hsv = cv2.cvtColor(undistorted_img_bgr, cv2.COLOR_BGR2HSV)
        l_channel = hls[:,:,1]
        s_channel = hls[:,:,2]
        v_channel = hsv[:,:,2]
        r_channel = undistorted_img_bgr[:,:,2]
        g_channel = undistorted_img_bgr[:,:,1]

        # Threshold color channel
        s_binary = np.zeros_like(s_channel)
        s_binary[(s_channel >= color_thresholds[0]) & (s_channel <= color_thresholds[1])] = 1
        l_binary = np.zeros_like(l_channel)
        l_binary[(l_channel >= color_thresholds[2]) & (l_channel <=color_thresholds[3])] = 1
        v_binary= np.zeros_like(v_channel)
        v_binary[(v_channel >= color_thresholds[4])&(v_channel<=color_thresholds[5])] = 1
        r_binary= np.zeros_like(r_channel)
        r_binary[(r_channel >= color_thresholds[6])&(r_channel<=color_thresholds[7])] = 1
        g_binary= np.zeros_like(r_channel)
        g_binary[(g_channel >= color_thresholds[8])&(g_channel<=color_thresholds[9])] = 1

        yellow_rec = np.zeros_like(s_binary)
        yellow_rec[((r_binary==1)&(g_binary==1))|(s_binary==1)]=1
        
        finalcolor_combined = np.zeros_like(yellow_rec)
        finalcolor_combined[(yellow_rec==1) & (l_binary==1)]=1
        
        return finalcolor_combined

def color_images_plotter(s_min=(0,255),s_max=(0,255),l_min=(0,255),l_max=(0,255),v_min=(0,255),v_max=(0,255),
                         r_min=(0,255),r_max=(0,255),g_min=(0,255),g_max=(0,255)):
    
#     images1 = glob.glob('test_images/exp_output/undistorted/straight_lines*.jpg')
#     for index,file_name in enumerate(images1):
#         if index==1:
#             img1 = cv2.imread(file_name)
#             title = file_name.split('/')
#             title = title[-1]
#             f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))
#             f.tight_layout()
#             img1=img1[:,:,::-1] # for matplotlib display we need rgb
#             ax1.imshow(img1)
#             ax1.set_title(title, fontsize=30)
#             img2 = color_threshold(img1,color_thresholds=(s_min,s_max,l_min,l_max,v_min,v_max,
#                          r_min,r_max,g_min,g_max))
#             ax2.imshow(img2,cmap='gray') # for matplotlib display we need rgb
#             ax2.set_title(""gradient_thresholded"", fontsize=30)
#             plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)
#             plt.show()
    
#     images2 = glob.glob('test_images/exp_output/undistorted/test*.jpg')
#     for index,file_name in enumerate(images2):
    file_names=[""test_images/exp_output/undistorted/test_1undistorted.jpg"",
                        ""test_images/exp_output/undistorted/test_4undistorted.jpg"",
                        ""test_images/exp_output/undistorted/test_5undistorted.jpg""]
    for file_name in file_names:
            img1 = cv2.imread(file_name)
            title = file_name.split('/')
            title = title[-1]
            f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))
            f.tight_layout()
            img1=img1[:,:,::-1] # for matplotlib display we need rgb
            ax1.imshow(img1)
            ax1.set_title(title, fontsize=30)
            img2 = color_threshold(img1,color_thresholds=(s_min,s_max,l_min,l_max,v_min,v_max,
                          r_min,r_max,g_min,g_max))
            ax2.imshow(img2,cmap='gray') # for matplotlib display we need rgb
            ax2.set_title(""color_thresholded"", fontsize=30)
            plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)
            plt.show()",0.4081490636,
984,investor flow of funds us,"#Define the Image Processing Function
def image_process(img):
    ##Load the distortion coefficients
    dist_pickle = pickle.load(open( ""camera_cal/wide_dist_pickle.p"", ""rb"" ))
    mtx=dist_pickle[""mtx""]
    dist = dist_pickle[""dist""] 
    #print(mtx.shape,dist.shape)
    
    #Edge Finding Function outputting Binary Image
    output_image = edge_finding(img)
    output_image = output_image.astype(np.uint8)
    
    #Warp the Binary Image  
    warped, M_img, Minv_img = warp_image(output_image, mtx, dist)
    
    # window settings
    window_width = 40
    window_height = 80 # Break image into 8 vertical layers since image height is 720
    margin = 30 # How much to slide left and right for searching
    frames= 9 #Frames over which to average coefficients
    
    #Window Centroids used for left and right lane
    leftx = []  
    rightx = [] 
    
    
    # Define conversions in x and y from pixels space to meters
    ym_per_pix = 30/720 # meters per pixel in y dimension
    xm_per_pix = 3.7/700 # meters per pixel in x dimension
    
    #Find Lanes using Window Sliding Search and Convolutions
    window_centroids = find_window_centroids(warped, window_width, window_height, margin)
    
    for level in range(0,len(window_centroids)):
        #Add the window center points to the list of left and right lane points 
        leftx.append(window_centroids[level][0])
        rightx.append(window_centroids[level][1])
    
    #print(mean_left,mean_right)
    #Average centroids
        
    mean_left = np.mean(leftx)
    mean_right = np.mean(rightx)
    average_lane_width = mean_right - mean_left
    
    
        
    #print(track.leftLaneAppender, track.rightLaneAppender)
    #Curve fitting
    y_value = range(0, warped.shape[0])
    res_yvalue = np.arange(warped.shape[0]-(window_height/2),0,-window_height)#define the range of 

    # Fit a second order polynomial to pixel positions of left and right lane lines respectively
    left_fit = np.polyfit(res_yvalue, leftx, 2)
    right_fit = np.polyfit(res_yvalue, rightx, 2)


    #Track Lane Coefficients
    track.framesCounter += 1
    
    
    if track.framesCounter < frames:
        track.lineWidthAppender(average_lane_width)
        track.lineCoef([left_fit],[right_fit])
        left_fitx = left_fit[0]*y_value*y_value + left_fit[1]*y_value + left_fit[2]
        left_fitx = left_fitx.astype(np.int32)
        right_fitx = right_fit[0]*y_value*y_value + right_fit[1]*y_value + right_fit[2]
        right_fitx = right_fitx.astype(np.int32)
        
        #Calculate the Radius of Curvature
        curve_fit_left = np.polyfit(np.array(res_yvalue,np.float32)*ym_per_pix, np.array(leftx,np.float32)*xm_per_pix, 2)
        left_curverad = ((1 + (2*curve_fit_left[0]*y_value[-1]*ym_per_pix + curve_fit_left[1])**2)**1.5) / np.absolute(2*curve_fit_left[0])
        track.radius.append(left_curverad)
              
    else:
        #Average coefficients over last 7 frames
        if (np.mean(track.lineWidthAppend[(len(track.leftCoef)-frames):])-20) < average_lane_width and (np.mean(track.lineWidthAppend[(len(track.leftCoef)-frames):])+20) > average_lane_width:
            track.lineWidthAppender(average_lane_width)
            track.lineCoef([left_fit],[right_fit])
            left_fit = np.mean(track.leftCoef[(len(track.leftCoef)-frames):],axis=0)
            right_fit = np.mean(track.rightCoef[(len(track.rightCoef)-frames):],axis=0) 
            left_fitx = left_fit[0]*y_value*y_value + left_fit[1]*y_value + left_fit[2]
            left_fitx = left_fitx.astype(np.int32)
            right_fitx = right_fit[0]*y_value*y_value + right_fit[1]*y_value + right_fit[2]
            right_fitx = right_fitx.astype(np.int32)
            
            #Calculate the Radius of Curvature
            curve_fit_left = np.polyfit(np.array(res_yvalue,np.float32)*ym_per_pix, np.array(leftx,np.float32)*xm_per_pix, 2)
            left_curverad = ((1 + (2*curve_fit_left[0]*y_value[-1]*ym_per_pix + curve_fit_left[1])**2)**1.5) / np.absolute(2*curve_fit_left[0])
            track.radius.append(left_curverad)              
        
        else:
            left_fit = np.mean(track.leftCoef[(len(track.leftCoef)-frames):],axis=0)
            right_fit = np.mean(track.rightCoef[(len(track.rightCoef)-frames):],axis=0) 
        
            #determine lines
            left_fitx = left_fit[0]*y_value*y_value + left_fit[1]*y_value + left_fit[2]
            left_fitx = left_fitx.astype(np.int32)
            right_fitx = right_fit[0]*y_value*y_value + right_fit[1]*y_value + right_fit[2]
            right_fitx = right_fitx.astype(np.int32)


    #print(track.leftCoef, track.rightCoef)
    
    
    left_lane = np.array(list(zip(np.concatenate((left_fitx-window_width/4,left_fitx[::-1]+window_width/4), axis=0),np.concatenate((y_value,y_value[::-1]),axis=0))), np.int32)
    right_lane = np.array(list(zip(np.concatenate((right_fitx-window_width/4,right_fitx[::-1]+window_width/4), axis=0),np.concatenate((y_value,y_value[::-1]),axis=0))), np.int32)
    middle_marker = np.array(list(zip(np.concatenate((left_fitx+window_width/4,right_fitx[::-1]-window_width/4), axis=0),np.concatenate((y_value,y_value[::-1]),axis=0))), np.int32)

    road= np.zeros_like(img)

    cv2.fillPoly(road,[left_lane], color=[255,0,0])
    cv2.fillPoly(road,[right_lane], color=[0,0,255])
    cv2.fillPoly(road,[middle_marker], color=[0,150,0])


    #plt.imshow(road)
    img_size = (img.shape[1], img.shape[0])
    road_warped = cv2.warpPerspective(road,Minv_img,img_size, flags=cv2.INTER_LINEAR)
    result = cv2.addWeighted(img, 1, road_warped, 1.0, 0.0)
    

    #Calculate the offset of the car on the road
    camera_center = (left_fitx[-1]+right_fitx[-1])/2
    center_diff = (camera_center - warped.shape[1]/2)*xm_per_pix
    side_pos = 'left'
    if center_diff <= 0:
        side_pos = 'right'

    #print(left_curverad)
    #Draw the Radius of Curvature, Offset and Speed
    
    cv2.putText(result, 'Radius of Curvature = '+str(np.round(np.mean(track.radius[(len(track.leftCoef)-frames):]),3))+'(m)',(50,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255),2)
    cv2.putText(result, 'Vehicle is '+str(abs(np.round(center_diff,3)))+'m '+side_pos+' of center',(50,100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255),2)
    
    return result",0.4081450701,
984,investor flow of funds us,"#reset in case the below case ""no loans allowed"" is run previously
def budget(self, current):
    '''
    The budget forms the constraint for a Person in his/her consumption
    '''
    repayments = 0.0
    for loan in self.loans:
        if current == loan.repay_at:
            repayments += loan.repay_amount()
    return self.creditworthiness()*self.consumerism + self.savings  - (1-self.consumerism) * repayments
Person.budget = budget
central_bank = Bank(interest_rate, reserve_rate)

n_steps = 60
participant_count = 1000
max_price = 100
price_steps = 200
basket = 7
m = Market(participant_count, basket, central_bank, max_price, price_steps)
print(central_bank.holding)
print(len(m.participants))
mp = m.market_price()

fig,(ax1,ax2, ax3) = plt.subplots(3,1, sharex=True)
ax1.set_xlabel('Time')
ax1.set_ylabel('Money Supply')
ax1.set_xlim(0,n_steps)
ax1.set_ylim(0,700000)
ax2.set_xlabel('Time')
ax2.set_ylabel('Prices')
ax2.set_xlim(0,n_steps)
ax2.set_ylim(0,35)
ax3.set_ylabel(""Participants"")
ax3.set_ylim(0,participant_count*1.1)


steps =[0]
m1s = [m.m1()]
m2s = [m.m2()]
participants = [participant_count]
prices = [[p] for p in mp]
colors = ['r', 'b', 'g', 'c', 'm', 'y','k']

ax1.plot(steps, m1s, c='r')
ax1.plot(steps, m2s, c='b')
ax3.plot(steps, participants)
for i, p in enumerate(prices):
    ax2.plot(steps, p, c=colors[i])
fig.canvas.draw()

for i in range(n_steps):
    mp = m.market_price()
    m.step()

    steps.append(i)
    m1 = m.m1()
    m2 = m.m2()
    
    # plotting shit
    m1s.append(m1)
    m2s.append(m2)
    for i, p in enumerate(mp):
        prices[i].append(p)

    participants.append(m.actives())
    
    maxY1 = max(m1s+m2s)*1.1
    maxY2 = max([p for pl in prices for p in pl ])*1.1
    ax1.set_ylim(0, maxY1)
    ax2.set_ylim(0, maxY2)
    plotmoney(ax1, steps, m1s, m2s)
    plotprices(ax2, steps, prices)
    plotpeople(ax3, steps, participants)
    
    fig.canvas.draw()
#     print(""Time {}. M1: {}, M2: {}, Total: {}"".format(i, m1, m2, m1+m2))",0.4074260592,
984,investor flow of funds us,"def process_image(test_img):
    
    dist_pickle = pickle.load(open(""calibration_pickle.p"", ""rb""))
    mtx = dist_pickle[""mtx""]
    dist = dist_pickle[""dist""]
    
    test_img_u = cv2.undistort(test_img, mtx, dist, None, mtx)
    
    preprocessImage = np.zeros_like(test_img_u[:, :, 0])
    test_grad_binary_x = abs_sobel_thresh(test_img_u, orient='x', grad_thresh=(40, 255))
    test_grad_binary_y = abs_sobel_thresh(test_img_u, orient='y', grad_thresh=(25, 100))
    mag_binary = mag_thresh(test_img_u, sobel_kernel=3, mag_thresh=(30, 255))
    dir_binary = dir_threshold(test_img_u, sobel_kernel=15, dir_thresh=(0.7, 1.3))
    
    hls = cv2.cvtColor(test_img_u, cv2.COLOR_RGB2HLS)
    L = hls[:,:,1]
    S = hls[:,:,2]
    
    thresh_L = (100, 255)
    binary_L = np.zeros_like(L)
    binary_L[(L > thresh_L[0]) & (L <= thresh_L[1])] = 1
    
    thresh_S = (170, 255)
    binary_S = np.zeros_like(S)
    binary_S[(S > thresh_S[0]) & (S <= thresh_S[1])] = 1
    
    hsv = cv2.cvtColor(test_img_u, cv2.COLOR_RGB2HSV)
    V = hsv[:,:,2]
    thresh_V = (160, 255)
    binary_V = np.zeros_like(V)
    binary_V[(V > thresh_V[0]) & (V <= thresh_V[1])] = 1
    
    R = test_img_u[:,:,0]
    thresh_R = (230, 255)
    binary_R = np.zeros_like(R)
    binary_R[(R > thresh_R[0]) & (R <= thresh_R[1])] = 1
    
    G = test_img_u[:,:,1]
    thresh_G = (150, 255)
    binary_G = np.zeros_like(G)
    binary_G[(G > thresh_G[0]) & (G <= thresh_G[1])] = 1
    
    #preprocessImage[((test_grad_binary_x == 1) | (binary_S == 1) & (binary_R == 1) & (binary_V == 1))] = 255
    preprocessImage[((test_grad_binary_x == 1) | (binary_G == 1) & (binary_R == 1) | (binary_L == 1) & (binary_S == 1))] = 255

    img_size = (test_img_u.shape[1], test_img_u.shape[0])

    w,h = 1280,720
    x,y = 0.5*w, 0.8*h
    src = np.float32([[200./1280*w,720./720*h],
                  [453./1280*w,547./720*h],
                  [835./1280*w,547./720*h],
                  [1100./1280*w,720./720*h]])
    
    dst = np.float32([[(w-x)/2.,h],
                  [(w-x)/2.,0.82*h],
                  [(w+x)/2.,0.82*h],
                  [(w+x)/2.,h]])
    
    M = cv2.getPerspectiveTransform(src, dst)
    Minv = cv2.getPerspectiveTransform(dst, src)
    warped = cv2.warpPerspective(preprocessImage, M, img_size, flags=cv2.INTER_LINEAR)
 
    '''PREPROCESSING COMPLETED'''

    # Assuming you have created a warped binary image called ""warped""
    # Take a histogram of the bottom half of the image
    histogram = np.sum(warped[warped.shape[0]//2:,:], axis=0)
    # Create an output image to draw on and visualize the result
    out_img = np.dstack((warped, warped, warped))*255
    # Find the peak of the left and right halves of the histogram
    # These will be the starting point for the left and right lines
    midpoint = np.int(histogram.shape[0]/2)
    leftx_base = np.argmax(histogram[:midpoint])
    rightx_base = np.argmax(histogram[midpoint:]) + midpoint

    # Choose the number of sliding windows
    nwindows = 9
    # Set height of windows
    window_height = np.int(warped.shape[0]/nwindows)
    # Identify the x and y positions of all nonzero pixels in the image
    nonzero = warped.nonzero()
    nonzeroy = np.array(nonzero[0])
    nonzerox = np.array(nonzero[1])
    # Current positions to be updated for each window
    leftx_current = leftx_base
    rightx_current = rightx_base
    # Set the width of the windows +/- margin
    margin = 100
    # Set minimum number of pixels found to recenter window
    minpix = 50
    # Create empty lists to receive left and right lane pixel indices
    left_lane_inds = []
    right_lane_inds = []

    # Step through the windows one by one
    for window in range(nwindows):
        # Identify window boundaries in x and y (and right and left)
        win_y_low = warped.shape[0] - (window+1)*window_height
        win_y_high = warped.shape[0] - window*window_height
        win_xleft_low = leftx_current - margin
        win_xleft_high = leftx_current + margin
        win_xright_low = rightx_current - margin
        win_xright_high = rightx_current + margin
        # Draw the windows on the visualization image
        cv2.rectangle(out_img,(win_xleft_low,win_y_low),(win_xleft_high,win_y_high),(0,255,0), 2) 
        cv2.rectangle(out_img,(win_xright_low,win_y_low),(win_xright_high,win_y_high),(0,255,0), 2) 
        # Identify the nonzero pixels in x and y within the window
        good_left_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & (nonzerox >= win_xleft_low) & (nonzerox < win_xleft_high)).nonzero()[0]
        good_right_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & (nonzerox >= win_xright_low) & (nonzerox < win_xright_high)).nonzero()[0]
        # Append these indices to the lists
        left_lane_inds.append(good_left_inds)
        right_lane_inds.append(good_right_inds)
        # If you found > minpix pixels, recenter next window on their mean position
        if len(good_left_inds) > minpix:
            leftx_current = np.int(np.mean(nonzerox[good_left_inds]))
        if len(good_right_inds) > minpix:        
            rightx_current = np.int(np.mean(nonzerox[good_right_inds]))

    # Concatenate the arrays of indices
    left_lane_inds = np.concatenate(left_lane_inds)
    right_lane_inds = np.concatenate(right_lane_inds)

    # Extract left and right line pixel positions
    leftx = nonzerox[left_lane_inds]
    lefty = nonzeroy[left_lane_inds] 
    rightx = nonzerox[right_lane_inds]
    righty = nonzeroy[right_lane_inds] 

    # Fit a second order polynomial to each
    left_fit = np.polyfit(lefty, leftx, 2)
    right_fit = np.polyfit(righty, rightx, 2)

    '''RECTANGLES'''
    # Generate x and y values for plotting
    ploty = np.linspace(0, warped.shape[0]-1, warped.shape[0] )
    left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]
    right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]
    
    out_img[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 0, 0]
    out_img[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [0, 0, 255]
    
    '''SMOOTH CURVE'''
    # Create an image to draw on and an image to show the selection window
    out_img2 = np.dstack((warped, warped, warped))*255
    window_img = np.zeros_like(out_img2)
    # Color in left and right line pixels
    out_img2[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 0, 0]
    out_img2[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [0, 0, 255]

    # Generate a polygon to illustrate the search window area
    # And recast the x and y points into usable format for cv2.fillPoly()
    left_line_window1 = np.array([np.transpose(np.vstack([left_fitx-margin, ploty]))])
    left_line_window2 = np.array([np.flipud(np.transpose(np.vstack([left_fitx+margin, ploty])))])
    left_line_pts = np.hstack((left_line_window1, left_line_window2))
    right_line_window1 = np.array([np.transpose(np.vstack([right_fitx-margin, ploty]))])
    right_line_window2 = np.array([np.flipud(np.transpose(np.vstack([right_fitx+margin, ploty])))])
    right_line_pts = np.hstack((right_line_window1, right_line_window2))

    # Draw the lane onto the warped blank image
    cv2.fillPoly(window_img, np.int_([left_line_pts]), (0,255, 0))
    cv2.fillPoly(window_img, np.int_([right_line_pts]), (0,255, 0))
    result = cv2.addWeighted(out_img2, 1, window_img, 0.3, 0)
    
    # Define y-value where we want radius of curvature
    # I'll choose the maximum y-value, corresponding to the bottom of the image
    y_eval = np.max(ploty)
    left_curverad = ((1 + (2*left_fit[0]*y_eval + left_fit[1])**2)**1.5) / np.absolute(2*left_fit[0])
    right_curverad = ((1 + (2*right_fit[0]*y_eval + right_fit[1])**2)**1.5) / np.absolute(2*right_fit[0])
    
    # Define conversions in x and y from pixels space to meters
    ym_per_pix = 30/720 # meters per pixel in y dimension
    xm_per_pix = 3.7/700 # meters per pixel in x dimension

    # Fit new polynomials to x,y in world space
    left_fit_cr = np.polyfit(ploty*ym_per_pix, left_fitx*xm_per_pix, 2)
    right_fit_cr = np.polyfit(ploty*ym_per_pix, right_fitx*xm_per_pix, 2)
    # Calculate the new radii of curvature
    left_curverad = ((1 + (2*left_fit_cr[0]*y_eval*ym_per_pix + left_fit_cr[1])**2)**1.5) / np.absolute(2*left_fit_cr[0])
    right_curverad = ((1 + (2*right_fit_cr[0]*y_eval*ym_per_pix + right_fit_cr[1])**2)**1.5) / np.absolute(2*right_fit_cr[0])
    avg_curvaturerad = np.sqrt((left_curverad**2)+(right_curverad**2))
    # Now the radius of curvature is in meters
    
    camera_center = (left_fitx[-1] + right_fitx[-1])/2
    center_diff = (camera_center - warped.shape[1]/2)*xm_per_pix
    side_pos = 'left'
    if center_diff <= 0:
        side_pos = 'right'
    
    # Create an image to draw the lines on
    warp_zero = np.zeros_like(warped).astype(np.uint8)
    color_warp = np.dstack((warp_zero, warp_zero, warp_zero))
    color_warp[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 0, 0]
    color_warp[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [0, 0, 255]

    # Recast the x and y points into usable format for cv2.fillPoly()
    pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])
    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])
    pts = np.hstack((pts_left, pts_right))

    # Draw the lane onto the warped blank image
    cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))

    # Warp the blank back to original image space using inverse perspective matrix (Minv)
    newwarp = cv2.warpPerspective(color_warp, Minv, (warped.shape[1], warped.shape[0])) 
    # Combine the result with the original image
    result2 = cv2.addWeighted(test_img, 1, newwarp, 0.3, 0)
    
    cv2.putText(result2, 'Radius of Curvature = '+str(round(avg_curvaturerad, 3))+'(m)', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    cv2.putText(result2, 'Vehicle is '+str(abs(round(center_diff, 3)))+'m ' +side_pos+' of center', (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
        
    return result2 # just return the same frame",0.4071629345,
984,investor flow of funds us,"def hit(deck,hand): 
    '''
    Inputs:
    deck - Deck() object (the current deck)
    hand - Hand() object (the current hand)
    '''
    hand.add_card(card = deck.deal())
    hand.adjust_for_ace()",0.4061041176,
984,investor flow of funds us,"def hit (deck,hand):
    
    single_card = deck.deal()
    hand.add_card(single_card)
    hand.adjust_for_ace()",0.4052817225,
984,investor flow of funds us,"class Account:
    def __init__(self, initDatetime, symbols, cashAmt, acctType='return'):
        """"""
        input
        initDate is start date of simulation
        symbols is list of universe for simulation
        
        variables
        txns is DataFrame which contains tranaction history
        eq is DataFrame which contains current value of account
        
        For initial implementation, this class only values account based on return data of security. 
        Valuing account with price of security will be implemented later.
        """"""
        self.txns = DataFrame(np.zeros((1,len(symbols))), index = [initDatetime], columns=symbols)
        self.eq = DataFrame(np.zeros((1,len(symbols)+1)), index = [initDatetime], columns=['Cash'] + symbols)
        self.eq.ix[initDatetime, 'Cash'] = cashAmt
        self.accType = acctType

    def addTxnDate(self, txnDate):
        """"""
        add one row for txns and eq with index as txnDate
        """"""
        self.txns.ix[txnDate] = self.txns.tail(1).values
        self.eq.ix[txnDate] = self.eq.tail(1).values
        
    def addTxn(self, txnDate, symbol, txnAmt):
        """"""
        addTxn add transaction data to txns DataFrame
        """"""
        self.txns.ix[txnDate, symbol] = self.txns.ix[txnDate, symbol] + txnAmt
    
    def updateAcct(self, currDate, value):
        """"""
        update account based on market data.
        value is market data dataframe containing either return or price data
        
        Transaction cost should be implemented here. many options should be considered.
        """"""
        shifted_txns = self.txns.shift(1)
        # for each symbol in txns
        for symbol in self.txns.columns:
            # if there's no position, skip.
            if self.eq.ix[currDate, symbol] != 0.0 or self.txns.ix[currDate, symbol] != 0.0:
                # if any position exist for this symbol before, then eq should be multiplyied by return of symbol
                # return of symbol for today is return from previous day to today
                self.eq.ix[currDate, symbol] = self.eq.ix[currDate, symbol] * (1.0 + value[symbol])
                # Check if transaction occur
                before_txns = self.eq.ix[currDate, symbol]
                if self.txns.ix[currDate, symbol] != shifted_txns.ix[currDate, symbol]:
                    """"""
                    Cases: same logic can be applied to case 1, 2
                    1. Sell all existing position
                    2. Sell partial existing position
                    3. add to existing position
                    """"""
                    # case 3, add to eq
                    if self.txns.ix[currDate, symbol] > shifted_txns.ix[currDate, symbol]:
                        self.eq.ix[currDate, symbol] = before_txns + (self.txns.ix[currDate, symbol] - shifted_txns.ix[currDate, symbol])
                        self.eq.ix[currDate, 'Cash'] = self.eq.ix[currDate, 'Cash'] - (self.txns.ix[currDate, symbol] - shifted_txns.ix[currDate, symbol])
                    # case 1, 2
                    else :
                        if self.txns.ix[currDate, symbol] < shifted_txns.ix[currDate, symbol]:
                            self.eq.ix[currDate, symbol] = before_txns + before_txns * (self.txns.ix[currDate, symbol] - shifted_txns.ix[currDate, symbol]) / shifted_txns.ix[currDate, symbol]
                            self.eq.ix[currDate, 'Cash'] = self.eq.ix[currDate, 'Cash'] + before_txns - self.eq.ix[currDate, symbol]

    def getEq(self, currDate):
        """"""
        get equity value of account at current date(currDate)
        """"""
        return self.eq.ix[currDate].sum()
    def getPos(self, currDate, symbol):
        """"""
        get position information of symbol
        """"""
        return self.txns.ix[currDate, symbol]
    def getUniverse(self):
        """"""
        get list of all symbols in universe
        """"""
        return list(self.eq.columns[1:])
    def getEqCurve(self):
        """"""
        get equty curve of account
        """"""
        return self.eq.sum(axis=1)",0.4035573006,
984,investor flow of funds us,"def initialize_contract(web3):
    contract = web3.eth.contract(
        abi=CONTRACT_ABI,
        bytecode=CONTRACT_BIN,
        bytecode_runtime=CONTRACT_BIN_RUNTIME
    )

    deployer_address = input(""Enter the address of the deployer: "")
    deployer_address = web3.eth.accounts[0] if deployer_address == """" else deployer_address

    gas = input(""Enter the desired gas for the contract: "")
    gas = 1000000 if gas == """" else int(gas)

    contract_address = deploy_contract(web3, contract, deployer_address, gas)
    print(""The contract address is: "" + contract_address)
    return contract_address",0.4030403495,
842,how many orders in total?,"# Passion Score
def passion_score(df):
    if len(df.customer_id.unique()) == 0:
        passion_score = 0
    else:
        passion_score = round(len(df)/len(df.customer_id.unique()),2)
    return passion_score",0.4758899212,
842,how many orders in total?,"exchange.create_order(symbol2, amount=balances[symbol2.split('/')[0]], price=exchange.fetch_order_book(symbol2)['bids'][0][0] * (1 - 0.0025))
# exchange.create_limit_sell_order('', amount)",0.4455068111,
842,how many orders in total?,"# objective function
def Obj_fn(m):
    return sum((m.priceBuy[i]*m.posNetLoad[i]) + (m.priceSell[i]*m.negNetLoad[i]) for i in m.Time)  
m.total_cost = en.Objective(rule=Obj_fn,sense=en.minimize)",0.4422017336,
842,how many orders in total?,"def calc_order_numbers_to_sum((last_order_n,order_number)):
    order_numbers_to_sum = range(last_order_n+1, order_number+1)
    return order_numbers_to_sum",0.4353814125,
842,how many orders in total?,"def to_zero_coupon_price(bond):
    price = bond.price
    for f in bond.fixings:
        price -= bond.coupon * np.exp(- usd_rates[f] * f / 360.)
    return price",0.433396101,
842,how many orders in total?,"def get_topic_wc(row):
    topic_pc = row['percent'] / 100
    total_wc = int(df_raw.loc[df_raw.delivered == row['delivered'], 'words'])
    return topic_pc * total_wc",0.4280977249,
842,how many orders in total?,"oanda.get_order(account_id, order_id=id)",0.423081696,
842,how many orders in total?,"class UpdateOrder(ICommand, IOrder):
    name = '--update-quantity'
    description = 'Update Quantity <number>'
    
    def __init__(self, args):
        self.newqty = args[1]
        
    def execute(self):
        oldqty = 5
        
        print(""Updated Database"")
        
        print(f""Logging: Updated quantity from {oldqty} to {self.newqty}\n"")",0.4217099845,
842,how many orders in total?,"order_id = order.get_orderId()
order_id",0.4214528799,
842,how many orders in total?,"def new_price_function(x):
    if x.foreign == 'Foreign':
        return x.price * 1.5
    else:
        return x.price",0.4208896458,
192,check accuracy / score for a logistic classifier,"for name, clf in zip(names,classifiers):
    #clf.fit(x_train_scaled,y_train)
    #print(clf.score(x_test_scaled,y_test))
    score = cross_validation.cross_val_score(clf, x_train_scaled[:,:5], y_train[0], cv=10,scoring='accuracy')
    print'{}: accuracy {}+/-{}'.format(name,score.mean(),score.std())",0.6001706123,
192,check accuracy / score for a logistic classifier,"def accuracy(p, y):
    """"""
    Calculate the accuracy of predictions against truth labels.

        Accuracy = # of correct predictions / # of data.

    Args:
        p: predictions of shape (n, 1)
        y: true labels of shape (n, 1)

    Returns:
        accuracy: The ratio of correct predictions to the size of data.
    """"""
    return np.mean((p > 0.5) == (y == 1))

def train(X, y, num_hidden=100, num_classes=1, num_iterations=2000,
            loss_function=loss_l2,
            learning_rate=1e-2):
    """"""
    Make prediction on X using model by doing forward feeding.

    Args:
        X: The data matrix of shape (n, p)
        y: The label for data of shape (n, p)
        num_hidden: number of neurons in the hidden layer.
        num_classes: 1 as we are doing binary classification.
        num_iterations: number of iterations to do training.
        loss_function: what loss function to use.
        learning_rate: learning rate for updating parameters.

    Returns:
        model: tuple of trained parameters (W1, b1, W2, b2).
        all_loss: a list of losses after each iteration.
        accuracies: a list of training accuracies after each iteration.

    """"""
    n, p = X.shape

    # Initialize parameters.
    # Weights and bias two layers, random numbers centered at 0.
    W1 = 2*np.random.randn(p, num_hidden) - 1
    b1 = 2*np.random.randn(num_hidden, ) - 1
    W2 = 2*np.random.randn(num_hidden, num_classes) - 1
    b2 = 2*np.random.randn(num_classes, ) - 1

    all_loss = []
    accuracies = []

    for it in xrange(num_iterations):
        # Feed forward.
        Z, params1 = feed_forward(X, W1, b1)
        y_out, params2 = feed_forward(Z, W2, b2)

        # Calculate the loss.
        loss, dy = loss_function(y_out, y)

        # Back propagation.
        dZ, dW2, db2 = back_prop(dy, params2)
        dX, dW1, db1 = back_prop(dZ, params1)

        # Update parameters along the gradients.
        W1 -= (learning_rate * dW1)
        b1 -= (learning_rate * db1)
        W2 -= (learning_rate * dW2)
        b2 -= (learning_rate * db2)

        # Save loss and accuracy for plotting the curves.
        all_loss.append(loss)
        accuracies.append(accuracy(y_out, y))

        # Print progress every 100 iterations.
        if it % 100 == 0:
            print 'After iteration %d, loss = %f, accuracy = %f' % (
                it, loss, accuracies[-1])

    # Print the training accuracy at the very last.
    print 'Accuracy on training data: %f' % accuracies[-1]

    # Pack the trained parameters and return it.
    model = (W1, b1, W2, b2)
    return model, all_loss, accuracies",0.5937360525,
192,check accuracy / score for a logistic classifier,"def eval_accuracy(current_model, truth_X, truth_Y):
    truth = denormalize(np.expand_dims(truth_Y, axis=1))
    
    global preds = current_model.predict(truth_X, batch_size=1024) # Global for debugging purposes
    preds = np.clip(denormalize(preds), 0.5, 5.0)
    
    print(np.mean(np.square(preds - truth)))",0.5922214389,
192,check accuracy / score for a logistic classifier,"def EvaluateModel(tree, validationSet,expectedResults):
    predictions = tree.predict(validationSet)
    results = predictions==expectedResults
    return results
def Accuracy(results):
    nbrTrue = results[results==True].count()
    nbrTotal = results.count()    
    return nbrTrue/float(nbrTotal)*100.0
resltClassifierTree = EvaluateModel(classifierTree, tidyValidateData, survivedValidate)
resltLimitedTree = EvaluateModel(limitedTree, tidyValidateData, survivedValidate)
resltMengthTree = EvaluateModel(lengthTree, tidyValidateData, survivedValidate)
print ('Accuracy for Classifier Tree : %s' % Accuracy(resltClassifierTree))
print ('Accuracy for Classifier Tree with a minimal class size : %s' % Accuracy(resltLimitedTree))
print ('Accuracy for Classifier Tree with a fixed length : %s' % Accuracy(resltMengthTree))",0.5904550552,
192,check accuracy / score for a logistic classifier,"def classifier_report(clf, X, y):
    S_0 = np.count_nonzero(y) # Total number of signal events in dataset X
    B_0 = np.shape(y)[0] - S_0
    S_f, B_f = 0,0
    
    y_pred = clf.predict(X)
    onehots = np.equal(y,y_pred)
    accuracy = np.count_nonzero(onehots)/np.shape(y)[0]
    
    for i in range(0,np.shape(y)[0]):
        if (y[i] and onehots[i]):
            S_f+=1
        elif ((not y[i]) and (not onehots[i])):
            B_f+=1
            
    signal_efficiency = S_f/S_0
    background_rejection = B_0/B_f
    
    print(""Accuracy: %.3f\nSignal Efficiency: %.3f \nBackground Rejection: %.3f"" 
          %(accuracy, signal_efficiency, background_rejection))",0.588182807,
192,check accuracy / score for a logistic classifier,"def check_FPs(model,testX,testY,testIDs):
    predY = model.predict(testX)
    index=predY==testY
    bins=np.linspace(1000,7000,13)
    print bins
    plt.hist(testIDs[index],bins=bins)
    plt.hist(testIDs[~index],bins=bins)
    return",0.5860146284,
192,check accuracy / score for a logistic classifier,"# Predict values using testing data
    nb_predict_test = nb_model.predict(x_test)
    
    from sklearn import metrics
    
    nb_accuracy_result = metrics.accuracy_score(y_test,nb_predict_test)
    
    print(""Navie Byes accuracy: "",nb_accuracy_result)
    
    
    nb_clasification_report = metrics.classification_report(y_test,nb_predict_test)",0.5848234892,
192,check accuracy / score for a logistic classifier,"def sentiment(text):
    feats = find_features(text)
    return voted_classifier.classify(feats),voted_classifier.confidence(feats)",0.5845666528,
192,check accuracy / score for a logistic classifier,"all_labels = ['POS','NEG','NEU']
def evalClassifier(weights,outfilename,testfile=devkey):    
    with open(outfilename,'w') as outfile:
        for counts,label in dataIterator(testfile): #iterate through eval set
            print >>outfile, predict(counts,weights,all_labels)[0] #print prediction to file
    return scorer.getConfusion(testfile,outfilename) #run the scorer on the prediction file",0.5813929439,
192,check accuracy / score for a logistic classifier,"def eval_accuracy(predicted, twenty_test):
    np.mean(predicted == twenty_test.target)
    print(metrics.classification_report(twenty_test.target, predicted,
                                        target_names=twenty_test.target_names))

    print metrics.confusion_matrix(twenty_test.target, predicted)",0.5789927244,
1920,sieve of eratosthenes,sum(euler.sieve_of_eratosthenes(2000000)),0.4925663471,
1920,sieve of eratosthenes,"import primes
primes.eratosthenes(100)",0.4662228227,
1920,sieve of eratosthenes,"limit = 1000000
run_length = 8
all_primes = set(euler.sieve_of_eratosthenes(limit))
sorted_primes = sort(list(all_primes.copy()))
digits10 = {str(i) for i in range(10)}
digits9 = {str(i) for i in range(1,10)}
def replace(input_string,target,replacement):
    output_string = [replacement if c==target else c for c in input_string]
    return ''.join(output_string)
w_dict = {}
for p in sorted_primes:
    #find all positions of wildcard
    p_str = str(p)
    wildcards = unique([i for i in p_str])
    for w in wildcards:
        w_key = replace(p_str,target=w,replacement=""*"")
        if w_key in w_dict:
            continue
        digit_list = digits9 if w_key[0] == ""*"" else digits10
        rep_list = set(int(replace(p_str,target=w,replacement=d)) for d in digit_list )
        intersection_size = len(rep_list & all_primes)
        w_dict[w_key] = intersection_size
        
        if intersection_size >= run_length:
            print(""length: {}, prime:{}, pattern:{}, family:{}"".format(intersection_size,p,w_key,rep_list))
            break",0.4601778984,
1920,sieve of eratosthenes,epileptor = models.Epileptor(),0.4573667049,
1920,sieve of eratosthenes,1 / (((2*(2)**0.5) / 9801) * (1103 + (4*3*2*((1103+26390) / 396**4)))),0.4569875002,
1920,sieve of eratosthenes,"limit = 1000000
primes = set(euler.sieve_of_eratosthenes(limit))
sorted_primes = sort(list(primes)) 
for seq_len in range(limit):
    if sum(sorted_primes[0:seq_len]) > limit:
        break
    for i in range(len(sorted_primes) - seq_len):
        sequence = sorted_primes[i:i+seq_len]
        s_sum = sum(sequence)
        if s_sum in primes:
            seq_found = sequence
            break
print(""{0}-length sequence sums to {1}"".format(len(seq_found),sum(seq_found)))",0.455512166,
1920,sieve of eratosthenes,"def support(X):
  return support_count(X) / len(T)",0.4539272189,
1920,sieve of eratosthenes,expr = (b**2/(12*a*c)) * (5/3 + ln(4*a*c/b**2)),0.4535589218,
1920,sieve of eratosthenes,"var_women = ((14*pow((1-0.636),2))+(8*pow((0-0.636),2)))/22",0.4531588554,
1920,sieve of eratosthenes,"def Ek2(k):
    return((1/6)*(1-(1/3)**k)+(4/15)*((-1)**(k+1)*(1/2)**k+(1/3)**k))

def V(k):
    return((1/10)*(1/3)**k+(1/18)-(1/9)*(1/2)**(2*k)+(-1)**(k+1)*(2/45)*(1/2)**k)

for j in range(0, k):
    def Ej(j):
        return((1/6)*(1-(1/3)**(j))+(4/15)*((-1)**(j+1)*(1/2)**(j)+(1/3)**(j)))
              
    def T1(j):
        return((1/2)*((1/3)**(j-1)*(1-(1/3)**(k-j))))

    def T2(j):
        return((1/2)*((1/3)**j-(1/3)**k))
    
    def T3(j):
        return((3/5)*(4*(1/3)**(k+1)+(-1)**(j+k+1)*(1/2)**(k-j-2)*(1/3)**(j+1)))
    
    def T4(j):
        return((1/5)*((-1)**(j+2)*(1/3)*(1/2)**(j-2)-4*(1/3)**(j+1)))
    
    def T5(j):
        return((1/5)*((-1)**(k+1)*(1/3)*(1/2)**(k-2)+(-1)**(k+j)*(1/3)**(j+1)*(1/2)**(k-j-2)))
    
def A1ST(j):
    return((Ek2(k)-Ej(j)-(1/4)*(T1(j)-2*T2(j)+T3(j))-T4(j)-T5(j))/V(k))

#EA2

def VjA2(j):
    return((1/3)/(1+a2[j])**2)

def VA2(j):
    productA2 = []
    for j in range(0, k):
        productA2.append(1+VjA2(j))
    return np.product(productA2)-1

def VnA2(j):
    return((VA2(j)+1)/(1+VjA2(j)))

def VTjA2(j):
    return VjA2(j)*(VnA2(j))

def A2ST(j):
    return(VTjA2(j)/VA2(j))

#AEB1

def q(j):
    return(12*(k-0.5)**2)
        
def B1ST(j):
    return((q(j)+1)**k/((q(j)+1)*((q(j)+1)**k-q(j)**k)))

#AEB2

def B2ST(j):
    return((k+1)**(2*k-2)/(((k+1)**(2*k)-(k**2+2*k)**k)))

#AEB3

def VjB3(j):
    return((1/3)/(1+b3[j])**2)

def VB3(j):
    productB3 = []
    for j in range(0, k):
        productB3.append(1+VjB3(j))
    return np.product(productB3)-1

def VnB3(j):
    return((VB3(j)+1)/(1+VjB3(j)))

def VTjB3(j):
    return VjB3(j)*(VnB3(j))

def B3ST(j):
    return(VTjB3(j)/VB3(j))

#AEC1

def C1ST(j):
    return 4**(k-1)/(4**k-3**k)

#AEC2
        
def C2ST(j):
    return 4**(k-1)/(4**k-3**k)

def create_dict(key, values):
    return dict(zip(key, values))

analyticalValues = [A1ST,A2ST,B1ST,B2ST,B3ST,C1ST,C2ST]

AE = []
AE_l = []
AE_names = []
AE_namesd = []
for iw,w in enumerate(analyticalValues):
    AE_names.append(str(w.__name__)[:2])
    for j in range (0,k):
        AE.append(w(j))
        AE_namesd.append('AE' + str(w.__name__) + str(j))
AE_df = pd.DataFrame([AE[k*iw:k*(iw+1)] for iw in range(len(analyticalValues))],AE_names)
AE_dic = create_dict(AE_namesd, AE)",0.4518542886,
1186,matrix matrix multiplication,"b = np.matrix([1,2])
a*b",0.5092107058,
1186,matrix matrix multiplication,"A = np.array([[1,2]])
B = np.array([[1],[1]])
np.matmul(A,B)",0.5090956092,
1186,matrix matrix multiplication,"v1 = np.matrix(""10; 100"") # Matlab stlye notation
m1 = np.matrix([[1, 2], [3, 4], [5, 6]]) # Python style notation

print(m1*v1) # , ""|"", v1*m1)",0.5059632659,
1186,matrix matrix multiplication,"A = np.matrix([[1, 2], [3, 4]])
C = np.matrix([[1, 2, 3], [4, 5, 6]])


A*A              # ** on matrix this does matrix multiplication **
np.dot(A, A)     # this also does matrix multiplication

A*C              # ** works with matrix as it is looking for alignment for matmul

A.dot(C)         # these four all give the same result, matrix multiplication
np.dot(A,C)
np.matmul(A,C)
A@C",0.5051808357,
1186,matrix matrix multiplication,"u_matrix = np.array([[1],[1],[1],[1]])
np.matmul(C_matrix, u_matrix)",0.5042017102,
1186,matrix matrix multiplication,"class MatrixMul(Operation):
    def __init__(self, x, y):
        self.super(self,[x, y])
        
    def compute(self, x_var, y_var):
        self.inputs = [x_var, y_var]
        return np.dot(x_var, y_var)",0.5032510757,
1186,matrix matrix multiplication,"mat1 = np.matrix('1 2; 3 4')  # matrices can be created with a matlab-like syntax
mat2 = np.matrix([[10, 20], [30, 40]])

print(mat1 * mat2)  # matrix multiplication",0.5031164885,
1186,matrix matrix multiplication,"A = np.matrix([[4,2], [1,2]])
x = np.matrix([[1],[0]])

y = A*x

fig1 = plt.figure()
ax1 = plt.axes()

ax1.grid(True)
plt.gca().set_aspect('equal', adjustable='box')
ax1.set_ylim(-5, 5)
ax1.set_xlim(-5, 5)

ax1.arrow(0,0, float(x[0]) , float(x[1]), width=0.1, color='b')
ax1.arrow(0,0, float(y[0]) , float(y[1]), width=0.1, color='r')

plt.show()",0.5012599826,
1186,matrix matrix multiplication,"A = array([[1,3],[5,3]])
x = array([-2,3])
np.dot([2j, 3j], [2j, 3j])",0.5007697344,
1186,matrix matrix multiplication,"class Matrix:
    def __init__(self, matrix):
        self.matrix = matrix

m1 = Matrix([[1, 2], [3, 4]])
m2 = Matrix([[5, 6], [7, 8]])
m1 + m2  # <-- by default no method for addition is defined",0.4989134073,
2015,step assign it to a variable called users and use the user_id as index,"# Adding tax rates to datasets
def tax_rate(df,year):
    pos = tax[tax['YEAR'] == year].index.tolist()[0]
    df['TAX_RATE'] = np.where(df['TAX_CLASS_AT_TIME_OF_SALE']==1,
                              tax.iloc[pos,1],tax.iloc[pos,2])
    return df",0.4532465339,
2015,step assign it to a variable called users and use the user_id as index,"def RandomMovieFromYear(df,year):
    randomchoice = np.random.choice(movies_df.loc[movies_df['Year'] ==year,'Year'].index)
    return randomchoice",0.4531642497,
2015,step assign it to a variable called users and use the user_id as index,"def get_users_restaurants(df, user_id):
    dfuser=df[df.user_id==user_id]
    dfuserdedup=dfuser.drop_duplicates('business_id')
    return dict(zip(dfuserdedup.business_id.values, dfuserdedup.stars.values))",0.4528555274,
2015,step assign it to a variable called users and use the user_id as index,"def plot_for_field(ax, field):
    datasets = {
        'Naive': pd.read_json(
            f'data/naive_performance_search_{field}.json')[['step', field]].rename(
            columns={field: 'objective'}),
        'Controlled': pd.read_json(
            f'data/parameterised_performance_search_{field}.json')[['step', field]].rename(
            columns={field: 'objective'})}
    ax = compare_trajectory(datasets, ax, palette)
    return ax",0.4518840313,
2015,step assign it to a variable called users and use the user_id as index,"def pick_users_books(df, num_users, num_books):
    user_counts = pd.DataFrame(df.user_id.value_counts()).sort_values('user_id', ascending=False)
    top_10K_users = list(user_counts[0:num_users].index)
    user_filtered_df = df[df.user_id.isin(top_10K_users)]
    filtered_book_counts = pd.DataFrame(user_filtered_df.book_id.value_counts()).sort_values('book_id', 
                                                                                             ascending = False)
    top_100_filtered_books = list(filtered_book_counts[0:num_books].index)
    filtered_df = user_filtered_df[user_filtered_df.book_id.isin(top_100_filtered_books)]
    train, test = train_test_split(filtered_df, test_size = 0.2, random_state=42)
    return train, test
    
def get_all_subsets(df):
    train_500_20, test_500_20 = pick_users_books(df, 500, 20)
    train_1000_35, test_1000_35 = pick_users_books(df, 1000, 35)
    train_2000_50, test_2000_50 = pick_users_books(df, 2000, 50)
    train_5000_70, test_5000_70 = pick_users_books(df, 5000, 70)
    train_7500_85, test_7500_85 = pick_users_books(df, 7500, 85)
    train_10000_100, test_10000_100 = pick_users_books(df, 10000, 100)
    return train_500_20, test_500_20, train_1000_35, test_1000_35, train_2000_50, test_2000_50, \
        train_5000_70, test_5000_70, train_7500_85, test_7500_85, train_10000_100, test_10000_100",0.4510677159,
2015,step assign it to a variable called users and use the user_id as index,"def user_action(u, x, t, n):
    ""Defines a user action that stores the current simulated state for further usage.""
    global user_data
    if 'x' not in user_data:
        user_data['x'] = x
    if 'u' not in user_data:
        user_data['u'] = [(n, u)]
    else:
        user_data['u'].append((t[n], u.copy()))",0.4509224892,
2015,step assign it to a variable called users and use the user_id as index,"for f in glob.glob(data_path + '*.csv'):
    user_id = os.path.basename(f)[:-4]
    
    df = pd.read_csv(f)
    
    df.drop('antenna_id', axis=1, inplace=True)
    df.drop_duplicates(inplace=True)
    txt = df[df.interaction == 'text'].copy()
    cal = df[df.call_duration.notnull()].copy()
    cal = df[df.call_duration !=]
    
    cal['call_duration'] = cal['call_duration'].astype('int')
    cal['call_duration'] = cal['call_duration'].astype('object')
    df = txt.append(cal)
    df.to_csv(data_path_clean + user_id + '.csv', index=False)",0.4503302574,
2015,step assign it to a variable called users and use the user_id as index,"#moviecluster=clusterFilms(titles)

#ratingsdf.head()
def getUserInfo(ratingsdf,id):
    df = ratingsdf[ratingsdf.userId == id][['movieId','rating']]
    a=pd.DataFrame(df.sort('movieId'))
    a=a.reset_index(drop=True)    
    return a",0.4502106905,
2015,step assign it to a variable called users and use the user_id as index,"def recommend(city, n):
    if city in location_counts.index:
        for workshop in wksp_pmf.loc[city].sort_values(ascending = False).head(n).index:
            print(workshop)
            
    else:
        for i in (locations.T
                  .loc['Ticket name']
                  .value_counts()
                  .drop(['General Registration', 'Pravega Accommodation for 2 nights'])
                  .head(n)
                  .index):
            print(i)",0.4476060867,
2015,step assign it to a variable called users and use the user_id as index,"#define the function to make recommendation given guest_id, the input parameter includes guest_id, number of item recommendations,purchase data, and item data
#The output is items recommendation in DataFrame
def item_based_recommend(guest_id,n_recommend,purchase_merge,item_data):

    #get most popular item the guest has purchased most
    popular_item=purchase_merge[purchase_merge['gst_i']==guest_id].sort_values(by='qty',ascending=False,axis=0).reset_index()['item_i'][0]
    
    #convert the purchase data to pivoit table, with row representing item id and columns representing guest id
    purchase_pivot=pd.pivot_table(purchase_merge,values='qty',index='item_i',columns='gst_i').fillna(0)

    #find the items that the guest has not purchased and most popular item into not brought table.
    #This table will be used to calculate smiliarity, recommendations items will be from not brought table
    not_bought=purchase_pivot[purchase_pivot[guest_id]==0]
    not_bought=not_bought.append(purchase_pivot[purchase_pivot.index==popular_item])
    
    #get the index of most popular item
    item_index=np.where(not_bought.index==popular_item)[0][0]
    
    
    #convert to sparse matrix
    not_bought_sparse=csr_matrix(not_bought.values)
    
    #train knn model using cosine to calculate the similarities between item vectors 
    model_knn=knn(metric='cosine',algorithm='brute')
    model_knn.fit(not_bought_sparse)
    
    
    #get the number of n_recommend similar item vectors, store the distance(similarity) and item ids into distances and indices respectively
    distances,indices=model_knn.kneighbors(not_bought.iloc[item_index,:].reshape(1,-1),n_recommend)
    
    #select the items from item table based on item id get from previous steps
    #start from the second element in indice for first element is  popular item itself
    recommendations=item_data[item_data['anon_sku'].isin(indices[1:].flatten())]

    return recommendations",0.4466359615,
1401,part dictionaries,"def table(a_list):
    counter = {}
    for value in a_list:
        add_occurrence(value, counter)
        
    return counter",0.4150149226,
1401,part dictionaries,"def gen_dic(lst):
    dic1, dic2, dic3 = {},{},{}
    for row in lst:
        dic1[row[0]] = row[1]
        dic2[row[0]] = row[2]
        dic3[row[0]] = row[3]
    return dic1, dic2, dic3

rate, verbal, math = gen_dic(num_sat_scores)
print rate.items()[:3] # .items() return both the key and the value of a dictionary in a tuple.
print verbal.items()[:3]
print math.items()[:3]",0.4146288633,
1401,part dictionaries,"def makeDictOfPoints(listUkpol):
	""""""
	makeDict is a function that create a Python's dictionary, which contain verts as key, and point's lists, that incident verts, as value.
	Return a dictionary.
	@param listUkpol: List that contain verts list and cells list.
	@return dictionary: Dictionary that contain verts as key and point's list as value
	""""""
	dictionary = {}
	verts = listUkpol[0]
	cells = listUkpol[1]
	for cell in cells: 
		for label in cell:
			point = str(normalize_value_in_list(verts)[int(label)-1])
			if(point not in dictionary):
				dictionary[point] = []
			dictionary[point].append(label)
	return dictionary",0.4075809419,
1401,part dictionaries,"def CreateDictionary():
    global Dict_defs
    data = ReadDefinitions()
    for concept in data:
        Dict_defs[concept[0]] = TranslateFeats(concept[1])",0.4040058851,
1401,part dictionaries,"def all_pairs_length(G):
    dist = {}
    for v in G:
        dist[v] = shortest_path_length(G, v)
    return dist",0.4036336541,
1401,part dictionaries,"# function that maps a labeled example to features.
#
def encode_sample(name_toks):
    fs = []
    for i,(k,v) in enumerate(name_toks):
        fs.append({
            'tag': token_tags[k],
            'length': len(v),
            'has_nonalpha' : 1 if any([not ch.isalnum() for ch in v]) else 0,
            'known_name' : -1 if type(v) != str else name_mapping.get(v.lower(), -1)})

    # add an 'empty' tag if only one token is given
    if len(name_toks) < 2:
        fs.append({'tag' : 4,
                   'length' : 0,
                   'has_nonalpha' : 0,
                   'known_name' : -1})
    return fs


# example
encode_sample([('FirstInitial','J.'), ('Surname', 'Smith')])",0.4030576348,
1401,part dictionaries,"def perfume_dict(perfume_list):
    """"""
    input: perfume_list, list of perfume urls
    returns: dictionary where the keys are designers and the values are number of perfumes per designer
    """"""
    
    # Create perfume dictionary
    perfumes_dict = {}
    for i in perfume_list:
        try:
            perfumes_dict[i.split('/')[4]] += 1
        except KeyError:
            perfumes_dict[i.split('/')[4]] = 1
            
    return perfumes_dict",0.4026682377,
1401,part dictionaries,"def find_palindromes(seq, n):
    palindromes = []
    
    for _________________:
        
        
        if _________________:
            palindromes.append(___________)
            
            
            
            
    return palindromes

DNA_seq = 'GGAGCTCCCAAAGCCATCAATATTCATCAAAACGAATTCAACGGAGCTCGATATCGCATCGCAAAAGACACC'
palindromic_sequences = find_palindromes(DNA_seq,6)
assert palindromic_sequences == ['GAGCTC', 'AATATT', 'GAATTC', 'GAGCTC', 'GATATC']",0.4025236666,
1401,part dictionaries,"def red2(xz):
    red_sum = []
    for key in xz:
        red_sum.append(((key), sum(xz[key])))
    return red_sum",0.4021676183,
1401,part dictionaries,"def get_contents(contents):
    res = {}
    for c, p in contents:
        res[c] = max(res.get(c, 0), p)
    return res",0.4012447,
2096,step how many times were a veggie salad bowl ordered?,100How is the main dish typically cooked?How is the main dish typically cooked?. * data.groupby('What kind of stuffing/dressing do you typically have?').size() / len(data),0.3844748139,
2096,step how many times were a veggie salad bowl ordered?,"def evalCost():
    csum = 0
    for i in range(m):
        csum += (evalHypothesisAt(i) - output_vector[i])**2
    return csum/(2*m)",0.3760256171,
2096,step how many times were a veggie salad bowl ordered?,"# transcription 
auto = quail.load_egg('../data/auto_egg.egg')
man = quail.load_egg('../data/man_egg.egg')

# number of subjects    
n_subs = len(auto.pres[0]) / len(man.rec.ix[0])",0.3715556264,
2096,step how many times were a veggie salad bowl ordered?,"def lyricist_count(x):
    if x == 'no_lyricist':
        return 0
    else:
        return sum(map(x.count, ['|', '/', '\\', ';'])) + 1
    return sum(map(x.count, ['|', '/', '\\', ';']))

train['lyricist'] = train['lyricist'].cat.add_categories(['no_lyricist'])
train['lyricist'].fillna('no_lyricist',inplace=True)
train['lyricists_count'] = train['lyricist'].apply(lyricist_count).astype(np.int8)
test['lyricist'] = test['lyricist'].cat.add_categories(['no_lyricist'])
test['lyricist'].fillna('no_lyricist',inplace=True)
test['lyricists_count'] = test['lyricist'].apply(lyricist_count).astype(np.int8)

def composer_count(x):
    if x == 'no_composer':
        return 0
    else:
        return sum(map(x.count, ['|', '/', '\\', ';'])) + 1

train['composer'] = train['composer'].cat.add_categories(['no_composer'])
train['composer'].fillna('no_composer',inplace=True)
train['composer_count'] = train['composer'].apply(composer_count).astype(np.int8)
test['composer'] = test['composer'].cat.add_categories(['no_composer'])
test['composer'].fillna('no_composer',inplace=True)
test['composer_count'] = test['composer'].apply(composer_count).astype(np.int8)

def is_featured(x):
    if 'feat' in str(x) :
        return 1
    return 0",0.3658612967,
2096,step how many times were a veggie salad bowl ordered?,"def temp(x):
    return((x.singles+2*x.doubles+3*x.triples+4*x.home_runs)/x.at_bats)
batting_summary['slg']=batting_summary.apply(temp, axis=1)",0.3651382923,
2096,step how many times were a veggie salad bowl ordered?,"def F0(x):
    sum = 0.
    for w, f in zip(w0, fs):
        sum += w*f(x)
    return sum
def F1(x):
    sum = 0.
    for w, f in zip(w1, fs):
        sum+= w*f(x)
    return sum",0.3597432673,
2096,step how many times were a veggie salad bowl ordered?,"import time

def fib(n, dicter):
    if n == 0:
        return 1
    if n == 1:
        return 1
    if n in dicter:
        return dicter[n]
    else:
        return fib(n-1, dicter) + fib(n-2, dicter)
    
def fib_calc(n, dicter):
    if n in dicter:
        return dicter[n]
    else:
        return fib(n, dicter)

start = time.time()
dicter = {}
for elem in range(200):
    dicter[elem] = fib_calc(elem, dicter)
time.time() - start",0.3591338694,
2096,step how many times were a veggie salad bowl ordered?,"total = 0
for i in range(len(items)):
    thing = items[i].lower()
    if thing != 'tarantulas':
        if thing != 'peanuts':
            #calculate the cost of getting the number of things at this index times the price/each",0.3576082289,
2096,step how many times were a veggie salad bowl ordered?,"for transit in transits: 
    transit.fluxes += quarterly_maxes[transit.quarters[0]]
    transit.fluxes /= quarterly_maxes[transit.quarters[0]]
    transit.errors /= quarterly_maxes[transit.quarters[0]]
transits[0].plot()",0.3572835028,
2096,step how many times were a veggie salad bowl ordered?,"def loglh_func(params):
    llh = kalman_filter(params[:k_1], data[aggregate_vars], model = 'restricted_3')[2]
    return -llh",0.3570607603,
23,add vectors,"def pretty_print_policy(policy, world, goal):
    cols = len(world[0])
    rows = len(world)
    for i in range(rows):
        for j in range(cols):
            if world[i][j] == 'x':
                sys.stdout.write('x')
            elif (j,i) == goal:
                sys.stdout.write('G')
            elif policy[(j, i)] == (0, -1):
                sys.stdout.write('^')
            elif policy[(j, i)] == (0, 1):
                sys.stdout.write('v')
            elif policy[(j, i)] == (1, 0):
                sys.stdout.write('>')
            elif policy[(j, i)] == (-1, 0):
                sys.stdout.write('<')
        print",0.4665426016,
23,add vectors,"# solution
def nn(x, W_list, h_list):
    assert len(W_list) == len(h_list)
    for W,h in zip(W_list, h_list):
        x = h(affine(x, W))
    return h(x)",0.4663417935,
23,add vectors,"def nn(x, W_list, h_list):
    assert len(W_list) == len(h_list)
    for W,h in zip(W_list, h_list):
        aff = affine(x, W)
        x = h(aff)
    return x",0.464853406,
23,add vectors,"def stringToInt(arr, position, values):
    for i, v in enumerate(values):
        arr[arr[:,position] == v, position] = i
    return arr
np.set_printoptions(precision=4)",0.4643067718,
23,add vectors,"def stringToInt(arr, position, values):
    for i, v in enumerate(values):
        arr[arr[:,position] == v, position] = i
    return arr",0.4643067718,
23,add vectors,"def plot_transmission_p(syst, energy, params):
    # Compute conductance
    trans = []
    for param in params:
        smatrix = kwant.smatrix(syst, energy, args=[param])
        trans.append(smatrix.transmission(1, 0))
    pyplot.plot(params, trans)
    
def plot_transmission_e(syst, energies, param):
    # Compute conductance
    trans = []
    for energy in energies:
        smatrix = kwant.smatrix(syst, energy, args=[param])
        trans.append(smatrix.transmission(1, 0))
    pyplot.plot(energies, trans, '+')",0.4625625014,
23,add vectors,"# evaluate a single model using the walk-forward validation approach
def evaluate_model(model_func, train, test):
    from numpy import array
    # history is a list of weekly data
    history = [x for x in train] # this is a list comprehension that creates a list from an array
    # walk-forward validation over each week
    predictions = list()
    for i in range(len(test)):
        # predict the week
        yhat_sequence = model_func(history)
        # store the predictions
        predictions.append(yhat_sequence)
        # get real observation and add to history for predicting the next week
        history.append(test[i, :])
    predictions = array(predictions)
    # evaluate predictions days for each week
    score, scores = evaluate_forecasts(test[:, :, 0], predictions)
    return score, scores",0.4618769884,
23,add vectors,"class composite_sed:
    def __init__(self,seds):
        self.sedlist = np.vstack(seds)
        self.X = self.sedlist.T
    def fit(self,obssed,verbose=False):
        a,resid,rank,s = np.linalg.lstsq(self.X,obssed)
        r2 = 1-resid/(len(obssed)*obssed.var())
        if verbose:
            print ""a= "",a
            print ""resd= "",resid
            print ""r2= "",r2
        return a",0.4590247869,
23,add vectors,"def plot_probabilities(pred_history, prob_history, labels):
    for l in labels:
        probs = prob_history[:, l]
        ppt.plot(probs, label=imagenet_labels[l])
        ppt.legend(loc='upper center')
    
    ppt.xlabel('iterations')
    ppt.ylabel('probability')

plot_probabilities(pred_history, prob_history, [panda_label, vulture_label, cat_label])",0.4551234245,
23,add vectors,"import math
def batches(batch_size, features, labels):
    """"""
    Create batches of features and labels
    :param batch_size: The batch size
    :param features: List of features
    :param labels: List of labels
    :return: Batches of (Features, Labels)
    """"""
    assert len(features) == len(labels)
    
    return [
        [
            [features[j] 
                for j in range(i*batch_size, min((i+1)*batch_size, len(features)))
            ],
            [labels[j] 
                for j in range(i*batch_size, min((i+1)*batch_size, len(features)))
            ]
        ] for i in range((len(features)//batch_size) + 1)
    ]
    
from pprint import pprint
# PPrint prints data structures like 2d arrays, so they are easier to read
pprint(batches(3, example_features, example_labels))",0.4488095641,
1812,retrieving data,"def generate_bam_view(pysam_iter):

    pysam_iter.reset()
    reads = pysam_iter.fetch(until_eof=True)

    records = []
    for r in reads:
        if r.get_tag('NH') > 0:
            xm = r.get_tag('XM')
            qn = r.query_name
            ref = r.reference_name
            start = r.reference_start
            nh = r.get_tag('NH')
            records.append((xm, qn, ref, start, nh))
    df = pd.DataFrame(records, columns=['XM', 'QName', 'Ref', 'Start', 'NH'])

    return df",0.4473995566,
1812,retrieving data,"def getPages(driver):
    rows = []
    offset = 0
    while True:
        driver.get(""https://finance.yahoo.com/cryptocurrencies?offset="" + str(offset) + ""&count=25"")
        next_button_element = driver.find_element_by_xpath('//*[@id=""fin-scr-res-table""]/div[2]/div[2]/button[3]')
        rows.extend(scrapePage(driver))
        outer_html = next_button_element.get_attribute(""outerHTML"")
        if ""disabled"" in outer_html:
            break
        offset = offset + 25
    return rows",0.4404754639,
1812,retrieving data,"def loadFASTQ():
    global reads
    
    start_time = time.time()    
    
    f = open(filenameFASTQ)

    reads = []

    try:
        while 1:
            name = f.readline().rstrip()
            sequence = f.readline().rstrip()
            f.readline()
            quality = f.readline().rstrip()

            if len(name) == 0:
                break

            union = name, sequence

            reads.append(union)           

        end_time = time.time()
        delta_time = end_time - start_time

        text_time.delete('1.0', tk.END)
        text_time.insert('1.0', str(delta_time))  

        text_readNum.delete('1.0', tk.END)
        text_readNum.insert('1.0', str(len(reads)))  

    except:
        messagebox.showwarning(""File Loading Failed"", ""Sorry, file loading failed! Please check the file format."")
    f.close()",0.4400448799,
1812,retrieving data,"clean_data = []

for r in resp:
    data = []
    try:
        user_name = r['user']['name'].encode('ascii', 'ignore')    # username
        tweet_text = r['text'].encode('ascii', 'ignore')            # tweet message
        data.append(""{}: {}"".format(user_name, tweet_text))
        data.append(r['geo']['coordinates'][0])    # lat
        data.append(r['geo']['coordinates'][1])    # long
        clean_data.append(data)
    except TypeError, e:
        print ""lat, long not availabe. """,0.4315343499,
1812,retrieving data,"# reads data
    list_of_dataframes = []
    for file in data_paths:
        # read file
        df = pd.read_table(file, skiprows=header_size, delim_whitespace=True)

        # naming columns
        df.columns = ['theta', 'phi', 'Year', 'Month', 'Time', 'r', 'Br', 'Bt', 'Bp', 'N_{data}']

        list_of_dataframes.append(df)

    # concatenate dataframes into one
    data = pd.concat(list_of_dataframes, axis=0, ignore_index=True)",0.4273821712,
1812,retrieving data,"@asyncio.coroutine
def stream(gc):
    results = []
    resp = yield from gc.submit(""x + x"", bindings={""x"": 1})
    while True:
        result = yield from resp.stream.read()
        if result is None:
            break
        results.append(result)
    return results
loop = asyncio.get_event_loop()
gc = aiogremlin.GremlinClient()
results = loop.run_until_complete(stream(gc))",0.4263546467,
1812,retrieving data,"#loading notMNIST datasheet from http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html
if(not os.path.isfile(""./notMNIST_small.mat"")):
    #get the dataset
    sys.stdout.write(b""Fetching the dataset from the internet"")
    import requests
    s = requests.Session()
    while(True):
        r = s.get(""http://yaroslavvb.com/upload/notMNIST/notMNIST_small.mat"", stream=""True"")
        if(r.status_code == 200):
            f = open(""./notMNIST_small.mat"", 'wb')
            for chunk in r.iter_content(chunk_size=512 * 1024): 
                if chunk: # filter out keep-alive new chunks
                    f.write(chunk)
                    sys.stdout.write(b""."")
                    sys.stdout.flush()
            f.close()
            print(""Done"")
            break
        os.sleep(200) #do not dos the server :)
    del s #close sesion
    del requests
    
data = io.loadmat(""./notMNIST_small.mat"")

#preparation of the data
X = data['images']
y = data['labels']
resolution = 28
classes = 10

X = np.transpose(X, (2, 0, 1))

y = y.astype('int32')
X = X.astype('float32') / 255.

Y = np_utils.to_categorical(y,10)",0.4260264039,
1812,retrieving data,"def wget_python(url_name, loc_file_name):
    """"""
         Purpose: Implementation of wget

         Arguments:
              - url_name: url pointing to the remote file name
              - loc_file_name: local file name     
    """"""
    try:
        # import for Python 2.7
        import urllib
        retrieve = urllib.URLopener().retrieve
    except AttributeError:
        # import for Python 3.6
        import urllib.request
        retrieve = urllib.request.urlretrieve
    testfile = retrieve(url_name, loc_file_name)",0.4250422716,
1812,retrieving data,"def import_iris_dataset():
    data = datasets.load_iris()

    target = data.target
    data = data.data
#     print(""----------------Dane Wejciowe--------------------"")
#     print(data[0])
#     print(data[1])
#     print(data[2])
#     print(data[3])

    return data, target",0.4237861931,
1812,retrieving data,"def get_historical_closes(ticker, start_date, end_date):
    # get the data for the tickers.  This will be a panel
    p = web.DataReader(ticker, ""yahoo"", start_date, end_date)    
    # convert the panel to a DataFrame and selection only Adj Close
    # while making all index levels columns
    d = p.to_frame()['Adj Close'].reset_index()
    # rename the columns
    d.rename(columns={'minor': 'Ticker', 'Adj Close': 'Close'}, inplace=True)
    # pivot each ticker to a column
    pivoted = d.pivot(index='Date', columns='Ticker')
    # and drop the one level on the columns
    pivoted.columns = pivoted.columns.droplevel(0)
    return pivoted",0.4234921038,
2359,the plot function extends naturally to an entire dataframe,"def plot_fi(fi, ax):
    return fi.plot('cols', 'imp', 'barh', ax=ax, legend=False)",0.5976073742,
2359,the plot function extends naturally to an entire dataframe,"# we define a plotting function for convenience
def pie(df, **kwargs):
    graph = df.plot.pie(subplots=True, figsize=(5,5), **kwargs)
    return graph",0.5973557234,
2359,the plot function extends naturally to an entire dataframe,"def plot_bin_features(count_df, count_df_norm):
    # Two subplots, unpack the axes array immediately
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=((15,5)))
    count_df.plot.bar(ax = ax1,
                      legend=True,
                      title= ""Distribution for binary features"")


    colors = sns.color_palette(""Paired"", n_colors=5)


    count_df_norm[['control_0','control_1']].plot.bar(ax=ax2,
                                                      legend=True,
                                                      title=""Normalized distribution for binary features"",
                                                      label = 'control',
                                                      stacked=True,
                                                      color = colors,
                                                      yerr= count_df_norm['error_control'],
                                                      position=0.25)

    count_df_norm[['treatment_0','treatment_1']].plot.bar(ax=ax2,
                                                          legend=True, 
                                                          title=""Normalized distribution for binary features"",
                                                          label='treatment',
                                                          stacked=True,
                                                          color = colors[2:],
                                                          yerr= count_df_norm['error_treatment'], 
                                                          position=0.75)

    # Change the width of the bars
    h,l = ax2.get_legend_handles_labels() 
    width=0.3
    for i in range(0, 4, 2): 
        for container in h[i:i+2]:
            for bar in container.patches: 
                bar.set_width(width)


    plt.suptitle(""comparison between normalized distribution and global distribution for binary features"",
                 fontsize=20,
                 y = -0.1,
                )


    plt.show()
plot_bin_features(count_df, count_df_norm)",0.5962253809,
2359,the plot function extends naturally to an entire dataframe,"def plot_forecast(m, fc):
    fig, ax = plt.subplots(figsize=figsize)
    m.plot(fc, ax=ax)
    ax.set_xlabel('Date')
    ax.set_ylabel('# ipynb files')
    ax.minorticks_on()
    ax.legend(loc='upper left')
    ax.set_title(f'GitHub search hits predicted until {fc.iloc[-1].ds.date()} (95% confidence interval)')",0.5925049782,
2359,the plot function extends naturally to an entire dataframe,"def plot_greeks(column, title):
    ax = data[column].plot(grid=True, title=title)
    ax.axhline(0, color=""black"")",0.5868468285,
2359,the plot function extends naturally to an entire dataframe,"def plot_bar(df, title, show_values=False):
    #display(df)        
    ax = df.plot(kind='bar', figsize=(12,6))
    plt.title(title)
    # if False: display legend; otherwise, display the value above each bar
    if (not show_values):
        plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))
    else:
        ax.legend_.remove() # hide legend
        x_offset = -0.03
        y_offset = 0.02
        for p in ax.patches:
            b = p.get_bbox()
            val = ""{:.0f}"".format(b.y1 + b.y0)        
            ax.annotate(val, ((b.x0 + b.x1)/2 + x_offset, b.y1 + y_offset), fontsize=14)
    plt.show()",0.5857895613,
2359,the plot function extends naturally to an entire dataframe,"sv_fig = plt.figure()
def fit_changed(bunch):
    sv_fig.clear()
    fits[bunch['new']].plot(fig=sv_fig,
                            data_kws=dict(color='black', marker='o', markersize=1, markerfacecolor='none'),
                            fit_kws=dict(color='red', linewidth=4))

# Widget for the spectrum index
sv_label = widgets.Label('spectrum index', layout=widgets.Layout(width='10%'))
sv_int_text = widgets.BoundedIntText(value=0, min=0, max=(n_spectra - 1),
                                     layout=widgets.Layout(width='20%'))
sv_int_text.observe(fit_changed, 'value')
sv_hbox_l = [widgets.HBox([sv_label, sv_int_text])]

sv_vertical_layout = widgets.VBox(sv_hbox_l)
display(sv_vertical_layout)",0.5854691267,
2359,the plot function extends naturally to an entire dataframe,"# Scatterplots of variables v. SalePrice 

# Note that SaleCondition is categorical, so this representation wouldn't be ideal
def scatter(var):
    data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)
    data.plot.scatter(x=var,y='SalePrice')",0.5853896141,
2359,the plot function extends naturally to an entire dataframe,"def plot_timings(performance_table, classifier_name):
    ax = performance_table.table[[""TRAIN_TIME"", ""PREDICT_TIME_TRAIN"", ""PREDICT_TIME_TEST""]].plot(title=classifier_name,
                                                                                grid=True,figsize=(9,5.06))

plot_timings(perf_1, clf.__class__.__name__)",0.5846897364,
2359,the plot function extends naturally to an entire dataframe,"def onclick(event):
    plt.cla()
    plt.plot(data)
    plt.gca().set_title('Event at pixels {},{} \nand data {},{}'.format(event.x, event.y, event.xdata, event.ydata))

# tell mpl_connect we want to pass a 'button_press_event' into onclick when the event is detected
plt.gcf().canvas.mpl_connect('button_press_event', onclick)",0.5839704871,
40,aggregation,"def model(X, w):
    return tf.add(tf.mul(X, w[1]), w[0])",0.3809947371,
40,aggregation,"def function_to_minize(variables):
    beta = variables[0]
    N = variables[1]

    final_size = compute_final_size(beta=beta, N=N)
    peak = compute_peak(beta=beta, N=N)
    return (final_size-outbreak_final_size)**2 + 20*(peak-epidemic_peak)**2

minimize(function_to_minize, [2.3, 440])",0.3704998493,
40,aggregation,"def function_to_minize(variables):
    beta = variables[0]
    N = variables[1]

    final_size = compute_final_size(beta=beta, N=N)
    peak = compute_peak(beta=beta, N=N)
    return (final_size-outbreak_final_size)**2 + 20*(peak-epidemic_peak)**2

minimize(function_to_minize, [2.3, 440], bounds=[(0, 10), (outbreak_final_size, population)])",0.3704998493,
40,aggregation,"def function_to_minize(variables):
    beta = variables[0]
    N = variables[1]

    final_size = compute_final_size(beta=beta, N=N)
    peak = compute_peak(beta=beta, N=N)
    return (final_size-outbreak_final_size)**2 + (peak-epidemic_peak)**2

minimize(function_to_minize, [2.3, 440])",0.3704998493,
40,aggregation,"def function_to_minize(variables):
    beta = variables[0]
    N = variables[1]

    final_size = compute_final_size(beta=beta, N=N)
    return (final_size-outbreak_final_size)**2

minimize(function_to_minize, [0.9, 400])",0.3700833917,
40,aggregation,"def variational_step(state, params):
    state = state.propagate(v.data, params[0])
    return state.propagate(k.data, params[1])",0.367179513,
40,aggregation,"def model(X, w_h, w_o):
    h = tf.nn.relu(tf.matmul(X, w_h))
    return tf.matmul(h, w_o)

py_x = model(X, w_h, w_o)",0.3662678003,
40,aggregation,"# Setup analysis
def run_and_plot(cond_ind_test, fig_ax):
    pcmci = PCMCI(dataframe=dataframe, cond_ind_test=cond_ind_test, var_names=var_names)
    results = pcmci.run_pcmci(tau_max=2,pc_alpha=0.2, )
    link_matrix = pcmci._return_significant_parents(pq_matrix=results['p_matrix'],
            val_matrix=results['val_matrix'], alpha_level=0.01)['link_matrix']
    tp.plot_graph(fig_ax = fig_ax,  val_matrix=results['val_matrix'],
                  link_matrix=link_matrix, var_names=var_names,
    )",0.3655810952,
40,aggregation,"def model(X, w_h, w_o):
    h = tf.nn.relu(tf.matmul(X, w_h))
    return tf.matmul(h, w_o)",0.3638495505,
40,aggregation,"def multilayer_perceptron(x, weights, biases, keep_prob):
    layer_1 = tf.nn.dropout(tf.nn.relu(tf.add(tf.matmul(x, weights['h1']), biases['b1'])), keep_prob)
    layer_2 = tf.nn.dropout(tf.nn.relu(tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])), keep_prob)
    layer_3 = tf.nn.dropout(tf.nn.relu(tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])), keep_prob)
    layer_4 = tf.nn.dropout(tf.nn.relu(tf.add(tf.matmul(layer_3, weights['h4']), biases['b4'])), keep_prob)
    return (tf.matmul(layer_4, weights['out']) + biases['out'])",0.3627262115,
1402,part dimensionality reduction,"from scipy.linalg import fractional_matrix_power
def diagElements(m):
    size = m.shape[0]
    return np.matrix(np.diag([m[i, i] for i in range(size)]))

def corrMat(m):
    sqrt_diag = fractional_matrix_power(diagElements(m), -0.5)
    return np.array(sqrt_diag * m  * sqrt_diag)

def plot_corrMat(corrMat_input, varNames, figs, ax):
    sc=ax.matshow(corrMat_input, cmap=plt.cm.seismic, vmin=-1, vmax=1)
    _=ax.set_xticks(range(len(varNames)))
    _=ax.set_xticklabels(varNames, rotation='vertical')
    _=ax.set_yticks(range(len(varNames))) 
    _=ax.set_yticklabels(varNames)
    _=figs.colorbar(sc, ax=ax, orientation='vertical', fraction=0.046, pad=0.04)",0.4419592619,
1402,part dimensionality reduction,"function formH(n, p = n)
  # Default is to return n*n matrix, otherwise n*p portion of H
  H = zeros(n, n)
  H[:, 1] = 1 / sqrt(n)
  for j = 2:n
    for i = 1:(j-1)
      H[i, j] = 1/sqrt(j * (j-1))
    end
    H[j, j] = -(j-1)/sqrt(j * (j-1))
  end
  return H[:, 1:p]
end

p = 1000
n = 10000
X = formH(n, p)
true = zeros(p)
true[1:5] = 1:5
srand(2015790003)
y = X * true + randn(n);",0.4401532412,
1402,part dimensionality reduction,"def rank_revealing_QR(A):
    Q, R, _ = la.qr(A, pivoting=True)
    Rd = np.diag(R)
    tol = 1e-10  # not actually compared with |R_22|
    return Q[:,:sum(np.abs(np.diag(R)) > tol)]",0.4388057888,
1402,part dimensionality reduction,"def formatResult(graph):
    graph_copy = graph.copy()
    # Remove center node
    graph_copy.remove_node('pysparkling(repo)')

    dc = sorted(nx.degree_centrality(graph_copy).items(), 
                key=itemgetter(1), reverse=True)

    bc = sorted(nx.betweenness_centrality(graph_copy).items(), 
                key=itemgetter(1), reverse=True)
    cc = sorted(nx.closeness_centrality(graph_copy).items(), 
                key=itemgetter(1), reverse=True)
    return (dc, bc, cc)

dc, bc, cc = formatResult(graph)

print (""Degree Centrality"")
print (dc[:5],'\n')

print (""Betweenness Centrality"")
print (bc[:5],'\n')

print (""Closeness Centrality"")
print (cc[:5])",0.4325911701,
1402,part dimensionality reduction,"divPT = np.identity(6)/np.diag(PT.A1)
     divPT[np.isnan(divPT)] = 0
     P_D_DADO_T = (PTyD.transpose()*divPT).transpose()
     print(P_D_DADO_T)",0.4312950969,
1402,part dimensionality reduction,"def S_new_method(idxs):
    component = (V.covariantD(idxs[0], idxs[1], idxs[2]) - V.covariantD(idxs[0], idxs[2], idxs[1])).simplify() # definition
    S.components.update({idxs: component}) # memoization
    return component
S._compute_covariant_component = S_new_method # _compute_covariant_component method was overriden",0.4300290942,
1402,part dimensionality reduction,"def centrality_measures(g):
    print ""Number of Nodes:""
    print g.number_of_nodes()
    print
    
    print ""Number of Edges:""
    print g.number_of_edges()
    print
    
    degree = nx.degree(g)
    degree = dict(list(degree))
    sort_degree = sorted(degree.iteritems(), key=lambda(k,v): (-v,k))
    print ""Degree:""
    print sort_degree[0:10]
    print
    
    closeness = nx.closeness_centrality(g)
    print ""Closeness:""
    print sorted(closeness.iteritems(), key=lambda(k,v): (-v,k))[0:10]
    print
    
    betweenness = nx.betweenness_centrality(g)
    print ""Betweenness:""
    print sorted(betweenness.iteritems(), key=lambda(k,v): (-v,k))[0:10]
    print
    
    eigenvector = nx.eigenvector_centrality(g)
    print ""Eigenvector:""
    print sorted(eigenvector.iteritems(), key=lambda(k,v): (-v,k))[0:10]
    print
    
    pagerank = nx.pagerank(g)
    print ""Pagerank:""
    print sorted(pagerank.iteritems(), key=lambda(k,v): (-v,k))[0:10]",0.429513216,
1402,part dimensionality reduction,"def jacobi_solver(A):
    N = np.diag(np.diag(A))
    Dinv = np.diag(1.0/np.diag(A))    
    P = N - A
    evals = np.linalg.eigvals(Dinv.dot(P))
    print(""Spectral radius (rho) is: {}"".format(np.max(abs(evals))))
    
    I = np.identity(A.shape[0])

    b = np.ones(A.shape[1])
    x = np.zeros(A.shape[1])
    r0_norm = np.linalg.norm(b - A.dot(x), 2)
    for k in range(15):
        x = Dinv.dot(b) + (I - Dinv.dot(A)).dot(x)
        r = b - A.dot(x)
        print(""  Step: {}, relative norm of residual (l2): {}"".format(k + 1, np.linalg.norm(r, 2)/r0_norm))
        
jacobi_solver(create_mass_matrix(50) )",0.4276936352,
1402,part dimensionality reduction,"def ditrad2(x):
    """"""radix-2 DIT FFT
    x: list or array of N values to perform FFT on, can be real or imaginary, x must be of size 2^n
    """"""
    ox = np.asarray(x, dtype='complex') # assure the input is an array of complex values
    # INSERT: assign a value to N, the size of the FFT
    N = #??? 1 point
    
    if N==1: return ox # base case

    # INSERT: compute the 'even' and 'odd' components of the FFT,
    # you will recursively call ditrad() here on a subset of the input values
    # Hint: a binary tree design splits the input in half
    even = #??? 2 points
    odd = #??? 2 points
    
    twiddles = np.exp(-2.j * cmath.pi * np.arange(N) / N) # compute the twiddle factors

    # INSERT: apply the twiddle factors and return the FFT by combining the even and odd values
    # Hint: twiddle factors are only applied to the odd values
    # Hint: combing even and odd is different from the way the inputs were split apart above.
    return #??? 3 points",0.4275349677,
1402,part dimensionality reduction,"def rectify(xy):
    x = xy[:, 0]
    y = xy[:, 1]
    
    # We need to provide the backward mapping, from the target
    # image to the source image.
    HH = np.linalg.inv(H)
    
    # You must fill in your code here to take
    # the matrix HH (given above) and to transform
    # each coordinate to its new position.
    # 
    # Hint: handy functions are
    #
    # - np.dot (matrix multiplication)
    # - np.ones_like (make an array of ones the same shape as another array)
    # - np.column_stack
    # - A.T -- type .T after a matrix to transpose it
    # - x.reshape -- reshapes the array x
    
    # ... your code
    # ... your code
    # ... your code
    # ... your code
    
    return ...

    
image = plt.imread('images/chapel_floor.png')
out = transform.warp(image, rectify, output_shape=(400, 400))

f, (ax0, ax1) = plt.subplots(1, 2, figsize=(8, 4))
ax0.imshow(image)
ax1.imshow(out)",0.4264293909,
2217,stochastic gradient descent,"def grad_descent_step(learning_rate, theta_0, theta_1, x, y):
    ''''''
    # Initialize variables
    m = size(x)
    dtheta_0 = 0.0
    dtheta_1 = 0.0
    
    # Do m gradient descent steps 
    for i in range(m):
        h = x[i] * theta_1 + theta_0
        dtheta_0 = dtheta_0 + # Insert code here!
        dtheta_1 = dtheta_1 + # Insert code here!
        
    # Insert code here!
    return theta_0, theta_1

# Insert code here!",0.5803903341,
2217,stochastic gradient descent,"if False: #never runs
    threshold = 1.0 #hyperparameter
    optimizer = tf.train.GradientDescentOptimizer(learning_rate)
    grads_and_vars = optimizer.compute_gradients(loss)
    capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var) for grad, var in grads_and_vars]
    training_op = optimizer.apply_gradients(capped_gvs)",0.5604762435,
2217,stochastic gradient descent,"### (4) Objective (and solver)

alpha = 0.01
def backprop(cost, w):
    grads = T.grad(cost=cost, wrt=w)
    updates = []
    for wi, grad in zip(w, grads):
        updates.append([wi, wi - grad * alpha])
    return updates

update = backprop(cost, params)
train = theano.function(inputs=[X, Y], outputs=cost, updates=update, allow_input_downcast=True)
y_pred = T.argmax(y_hat_predict, axis=1)
predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)

miniBatchSize = 10 

def gradientDescent(epochs):
    for i in range(epochs):
        for start, end in zip(range(0, len(train_data), miniBatchSize), range(miniBatchSize, len(train_data), miniBatchSize)):
            cc = train(train_data[start:end], train_labels_b[start:end])
        clear_output(wait=True)
        print ('%d) accuracy = %.4f' %(i+1, np.mean(np.argmax(test_labels_b, axis=1) == predict(test_data))) )

gradientDescent(50)

### How to decide what # to use for epochs? epochs in this case are how many rounds?
### plot costs for each of the 50 iterations and see how much it decline.. if its still very decreasing, you should
### do more iterations; otherwise if its looking like its flattening, you can stop",0.5569118261,
2217,stochastic gradient descent,"def train():
    pyro.clear_param_store()
    for j in range(num_iterations):
        # calculate the loss and take a gradient step
        loss = svi.step(x_data, y_data)
        if j % 100 == 0:
            print(""[iteration %04d] loss: %.4f"" % (j + 1, loss / len(data)))

train()",0.5561022758,
2217,stochastic gradient descent,"import numpy as np 
# Initializing the variables
def train(learning_rate, training_epochs, batch_size, display_step, optimizer_method=tf.train.GradientDescentOptimizer,activation_function=tf.nn.sigmoid):
    tf.reset_default_graph()
    # Initializing the session 
    
    logs_path = 'log_files/'  # useful for tensorboard

    # tf Graph Input:  mnist data image of shape 28*28=784
    x = tf.placeholder(tf.float32, [None,28, 28,1], name='InputData')
    # 0-9 digits recognition,  10 classes
    y = tf.placeholder(tf.float32, [None, 10], name='LabelData')
    
    keep_prob = tf.placeholder(tf.float32)

    # Construct model and encapsulating all ops into scopes, making Tensorboard's Graph visualization more convenient
    with tf.name_scope('Model'):
        # Model
        pred = LeNet5_Model(x,keep_prob,activation_function=activation_function)
    with tf.name_scope('Loss'):
        # Minimize error using cross entropy
                # Minimize error using cross entropy
        if activation_function == tf.nn.sigmoid:
            cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))
        else:    
            cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(tf.clip_by_value(pred,-1.0,1.0)), reduction_indices=1))
            #cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))
    with tf.name_scope('SGD'):
        # Gradient Descent
        optimizer = optimizer_method(learning_rate).minimize(cost)
    with tf.name_scope('Accuracy'):
        # Accuracy
        acc = evaluate(pred, y)

        
    # Initializing the variables
    init = tf.global_variables_initializer()
    # Create a summary to monitor cost tensor
    tf.summary.scalar(""Loss"", cost)
    # Create a summary to monitor accuracy tensor
    tf.summary.scalar(""Accuracy"", acc)
    # Merge all summaries into a single op
    merged_summary_op = tf.summary.merge_all()

    saver = tf.train.Saver()
    
    print (""Start Training!"")
    t0 = time()
    X_train,Y_train = mnist.train.images.reshape((-1,28,28,1)), mnist.train.labels
    X_val,Y_val = mnist.validation.images.reshape((-1,28,28,1)), mnist.validation.labels
    
    # Launch the graph for training
    with tf.Session() as sess:
        sess.run(init)
        # op to write logs to Tensorboard
        summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())

        # Training cycle
        for epoch in range(training_epochs):
            avg_cost = 0.
            total_batch = int(mnist.train.num_examples/batch_size)

            # Loop over all batches
            for i in range(total_batch):
                # train_next_batch shuffle the images by default
                batch_xs, batch_ys = mnist.train.next_batch(batch_size)
                batch_xs = batch_xs.reshape((-1,28,28,1))

                # Run optimization op (backprop), cost op (to get loss value)
                # and summary nodes
                _, c, summary = sess.run([optimizer, cost, merged_summary_op],
                                         feed_dict={x: batch_xs, 
                                                    y: batch_ys,keep_prob:0.75})
                
                # Write logs at every iteration
                summary_writer.add_summary(summary, epoch * total_batch + i)
                # Compute average loss
                avg_cost += c / total_batch
                

            # Display logs per epoch step
            if (epoch+1) % display_step == 0:
                print(""Epoch: "", '%02d' % (epoch+1), ""=====> Loss="", ""{:.9f}"".format(avg_cost))
                                                
                acc_train = acc.eval({x: X_train, y: Y_train,keep_prob:1.0})
                print(""Epoch: "", '%02d' % (epoch+1), ""=====> Accuracy Train="", ""{:.9f}"".format(acc_train))
                
                acc_val = acc.eval({x: X_val, y: Y_val,keep_prob:1.0})
                print(""Epoch: "", '%02d' % (epoch+1), ""=====> Accuracy Validation="", ""{:.9f}"".format(acc_val))
        print (""Training Finished!"")
        t1 = time()
        # Save the variables to disk.
        save_path = saver.save(sess, ""model.ckpt"")
        print(""Model saved in file: %s"" % save_path)

        #Your implementation for testing accuracy after training goes here
        
        X_test,Y_test = mnist.test.images.reshape((-1,28,28,1)),mnist.test.labels

        acc_test = acc.eval({x: X_test, y: Y_test,keep_prob:1.0})
        print(""Accuracy Test="", ""{:.9f}"".format(acc_test))
        
        return acc_train,acc_val,acc_test,t1-t0",0.5559110641,
2217,stochastic gradient descent,"def RMSprop(cost, params, lr=0.001, rho=0.9, epsilon=1e-6):
    grads = T.grad(cost=cost, wrt=params)
    updates = []
    for p, g in zip(params, grads):
        acc = theano.shared(p.get_value() * 0.)
        acc_new = rho * acc + (1 - rho) * g ** 2
        gradient_scaling = T.sqrt(acc_new + epsilon)
        g = g / gradient_scaling
        updates.append((acc, acc_new))
        updates.append((p, p - lr * g))
    return updates",0.5541687012,
2217,stochastic gradient descent,"def gradient_clip(loss, params, opt, clip_th=1e-3):
    grads_and_vars = opt.compute_gradients(loss, params)
    clipped_grads_and_vars = [(tf.clip_by_norm(grad, clip_th), var) for grad, var in grads_and_vars]
    solver = opt.apply_gradients(clipped_grads_and_vars)
    return solver

lr = 1e-4
bsize = 256
# define model
model = image_caption_model(embedding_matrix=embedding_matrix, bsize=bsize)
inputs, output, loss, nxt_pred, params = model
# define solver
opt = tf.train.AdamOptimizer(learning_rate=lr, beta1=0.5)
# apply gradient clipping
solver = gradient_clip(loss, params, opt, clip_th=1e-3)",0.5538868308,
2217,stochastic gradient descent,"def train(total_loss,learning_rate=0.01):
    #learning_rate = 0.01
    return tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)",0.5519607067,
2217,stochastic gradient descent,"def backward_pass_external_errorGradient(nn, err_grad):
    nn[-1].top_error_grad = err_grad
    nn[-1].backward_pass()
    for layerNo in range(len(nn)-2, -1, -1):
        nn[layerNo].top_error_grad = nn[layerNo+1].bottom_error_grad
        nn[layerNo].backward_pass()",0.5515418053,
2217,stochastic gradient descent,"def dev_eval(epoch, train_loss):
  dev_mae, dev_rmse, dev_label, dev_pred, dev_loss = sess.run([mae, rmse, label, pred, loss], dev_feed_dict)

  pearson = scipy.stats.pearsonr(dev_pred, dev_label)
  spearman = scipy.stats.spearmanr(dev_pred, dev_label)

  print('dev results: pearson: %.3e, spearman: %.3e, mae: %.3e, rmse: %.3e, loss: %.3e, train_loss: %.3e'
        % (pearson[0], spearman[0], dev_mae, dev_rmse, dev_loss, train_loss))
  return pearson[0]",0.5513689518,
1321,open high low close resampling often seen with stocks,"minute_return = bars.close / bars.open - 1
minute_return.describe()",0.5528771281,
1321,open high low close resampling often seen with stocks,"df.Close.resample('M').mean().plot()
df.Close.asfreq('D', method = 'backfill').plot()",0.5524410009,
1321,open high low close resampling often seen with stocks,"minute_return = bars.close / bars.open - 1     # Summary of Minute Return, we can also use log(return) 
pxopen = bars.open
minute_return.describe()",0.5518680811,
1321,open high low close resampling often seen with stocks,"data.resample('w').Close.mean().pct_change().plot(kind='bar', figsize=(16, 4));",0.5516192913,
1321,open high low close resampling often seen with stocks,"data.resample('w').Close.mean().pct_change().plot(figsize=(16, 4));",0.5506094694,
1321,open high low close resampling often seen with stocks,"# Create box plots of features vs change in market value for categorical variables (ie the dummy variable categories)

for variable in ['age_strings']:

    # Boxplot to explore market value by position
    xy = df.groupby([variable]).marketval_change.mean()
    xy_df = pd.DataFrame(xy)
    xy_df = xy_df.sort_values(by = 'marketval_change', ascending= False)
    sns.set_style('whitegrid')
    plt.figure(figsize=(15,5))
    plt.ticklabel_format(style='plain', axis='x')
    ax = sns.boxplot(y=variable, x=""marketval_change"", data=df)
    ax.set_ylabel('Feature', fontsize = 10)
    ax.set_xlabel('Change in market value', fontsize = 10)
    plt.xticks(rotation=30)
    ax.tick_params(labelsize=10)",0.5495306253,
1321,open high low close resampling often seen with stocks,"# Broker
ndf[ndf.channel=='B'].interest.mean()",0.5438441634,
1321,open high low close resampling often seen with stocks,"# Retail
ndf[ndf.channel=='R'].interest.mean()",0.5438441634,
1321,open high low close resampling often seen with stocks,"# Correspondent
ndf[ndf.channel=='C'].interest.mean()",0.5438441634,
1321,open high low close resampling often seen with stocks,"# Show Traded Volume
tx.expected_btc_traded.head()
_ = tx.expected_btc_traded.resample(ts, how='sum').plot()
plt.ylabel('BTC')
plt.title('Traded Volume')
plt.show()",0.5432972908,
594,exploring the dataset,"from sklearn import svm
from sklearn import datasets
iris = datasets.load_iris()
X, y = iris.data, iris.target",0.4038268328,
594,exploring the dataset,c.explorer.explain('2011acs5'),0.4036874771,
594,exploring the dataset,c.explorer.explain('2010acs5'),0.4036874771,
594,exploring the dataset,c.explorer.explain('DECENNIALSF12010'),0.4036874771,
594,exploring the dataset,"prot_perc, fat_perc, carb_perc = pickle.load( open('prot_perc.obj','rb')), pickle.load(open('fat_perc.obj', 'rb')), pickle.load(open('carb_perc.obj', 'rb'))",0.401155442,
594,exploring the dataset,"iris = datasets.load_iris()
X, y = iris.data[:, 0:2], iris.target
    
clf1 = DecisionTreeClassifier(criterion='entropy', max_depth=1)
clf2 = KNeighborsClassifier(n_neighbors=1)    

bagging1 = BaggingClassifier(base_estimator=clf1, n_estimators=10, max_samples=0.8, max_features=0.8)
bagging2 = BaggingClassifier(base_estimator=clf2, n_estimators=10, max_samples=0.8, max_features=0.8)",0.4011108875,
594,exploring the dataset,"from sklearn import datasets

iris = datasets.load_iris()
X, y = iris.data[:, 1:3], iris.target",0.3986313939,
594,exploring the dataset,"iris = datasets.load_iris()
X, y = iris.data[:, 0:2], iris.target
    
#XOR dataset
#X = np.random.randn(200, 2)
#y = np.array(map(int,np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)))
    
clf = DecisionTreeClassifier(criterion='entropy', max_depth=1)

num_est = [1, 2, 3, 10]
label = ['AdaBoost (n_est=1)', 'AdaBoost (n_est=2)', 'AdaBoost (n_est=3)', 'AdaBoost (n_est=10)']",0.3975375295,
594,exploring the dataset,"iris = datasets.load_iris()
X, y = iris.data[:, 1:3], iris.target",0.3967989087,
594,exploring the dataset,"from sklearn import datasets

iris = datasets.load_iris()
X_iris, y_iris = iris.data, iris.target",0.39551422,
1004,k nearest neighbors knn classification,"for i in range(1,21):
    knnclf = neighbors.KNeighborsClassifier(i,weights='distance')
    knnclf.fit(bank_train_norm,bank_target_train)
    knnpred_test = knnclf.predict(bank_test_norm)
    score = knnclf.score(bank_test_norm,bank_target_test)
    print ""K=%s, Score = %s""%(i,score)",0.5600199699,
1004,k nearest neighbors knn classification,"for k in [5, 10, 15]:
    knn = neighbors.KNeighborsClassifier(n_neighbors=k)
    
    knn.fit(X_train, y_train)
    preds = knn.predict(X_val)
    
    report = ""k: "" + str(k) + "" gives validation accuracy: "" + str(accuracy(preds, y_val))
    print(report)",0.5599862337,
1004,k nearest neighbors knn classification,"for n in range(1, 5):
    clf = neighbors.KNeighborsClassifier(n_neighbors=n)
    cv_scores = cross_val_score(clf, X_train, y_train, cv=10)
    print (n, cv_scores.mean())",0.5552183986,
1004,k nearest neighbors knn classification,"for k in range(30):
    knn = neighbors.KNeighborsClassifier(n_neighbors=k+1)
    y_pred = knn.fit(X_train, y_train).predict(X_test)
    print('Test Error for K = {}:'.format(k),
          round(1 - (y_pred == y_test).mean(),3))",0.5551753044,
1004,k nearest neighbors knn classification,"import scipy.spatial.distance as ssd

def knn(k, X_train, X_test, q_train):
    """""" k-nearest neighbors """"""
    # initialize list to store predicted class
    pred_class = []
    # for each instance in data testing,
    # calculate distance in respect to data training
    for ii, di in enumerate(X_test):
        distances = []  # initialize list to store distance
        for ij, dj in enumerate(X_train):
            # calculate distances
            distances.append((calc_dist(di,dj), ij))
        # k-neighbors
        k_nn = sorted(distances)[:k]
        # predict the class for the instance
        pred_class.append(classify(k_nn, q_train))
    # return prediction class
    return pred_class
 
def calc_dist(di,dj):
    return ssd.euclidean(di,dj) # built-in Euclidean fn
 
def evaluate(result):
    # create eval result array to store evaluation result
    eval_result = np.zeros(2,int)
    for x in result:
        # increment the correct prediction by 1
        if x == 0:
            eval_result[0] += 1
        # increment the wrong prediction by 1
        else:
            eval_result[1] += 1
    # return evaluation result
    return eval_result

def classify(k_nn, q_train):
    qlabel = []
    for dist, idx in k_nn:
        # retrieve label class and store into qlabel
        qlabel.append(q_train[idx])
    # return prediction class
    return np.argmax(np.bincount(qlabel))
 
def main():
    N = data.shape[0] #get tupple (numRows, numCols)
    np.random.shuffle(data)
  
    Nfolds = 10
    sizes = np.tile(np.floor(N/10),(1,Nfolds))
    sizes[-1] = sizes[-1] + N - sizes.sum()
    c_sizes = np.hstack((0,np.cumsum(sizes)))
    X = np.copy(data[:,:11])# change to data here if you dont want to run the cv on independent test data
    t = np.copy(data[:,11])
    
    # initialize K
    K = [1,3,7,11,19]
    cv_loss = np.zeros((Nfolds, len(K)))
    print ""Started Cross Validation for 10 folds. This may take a few minutes...""
    for i in range(len(K)):
        for fold in range(Nfolds):
            X_fold = X[c_sizes[fold]:c_sizes[fold+1],:]
            X_train = np.delete(X,np.arange(c_sizes[fold],c_sizes[fold+1],1),0)

            t_fold = t[c_sizes[fold]:c_sizes[fold+1]]
            t_train = np.delete(t,np.arange(c_sizes[fold],c_sizes[fold+1],1),0)

            # predict the data test into class
            pred_class = knn(K[i], X_train, X_fold, t_train)
            # evaluate the predicted result
            eval_result = evaluate(pred_class-t_fold)
            # Calculate accuracy
            cv_loss[fold,i] = float(eval_result[1])/float(eval_result[0]+eval_result[1])
        print ""Processing... K,"",K[i]
    
    plt.plot(K,cv_loss.mean(axis=0),'r-',label=""CV results"")
    plt.legend()
    plt.xlabel('K')
    plt.ylabel('Accuracy')
    print ""Optimal K value found"", K[np.argmin(cv_loss.mean(axis=0))]
    return K[np.argmin(cv_loss.mean(axis=0))]
    
def confusionMatrix(optimalK):
    N = data.shape[0] #get tupple (numRows, numCols)
    np.random.shuffle(data)
    
    train = data[:int(N*0.7)]
    test = data[int(N*0.7):]
    X_train = train[:,:11]
    q_train = train[:,11]
    
    X_test = test[:,:11]
    q_test = test[:,11]
    
    confusion_matrix = np.zeros((6,6)) # map class 3 to 0, 4 to 1, 5 to 2, 6 to 3, 7 to 4, 8 to 5
    m = {3:0, 4:1, 5:2, 6:3, 7:4, 8:5}

    # predict the data test into class
    pred_class = knn(optimalK, X_train, X_test, q_train)
    # build the confusion matrix
    for p in range(len(pred_class)):
        confusion_matrix[m[pred_class[p]]][m[q_test[p]]] += 1.0
    
    print ""Confusion Matrix. X-axis is the true class, Y-axis is the predicted class""
    print
    print "" "",3,"" "",4,"" "",5,"" "",6,"" "",7,"" "",8
    for i in range(6):
        line = str(i+3) + "" ""
        for j in range(6):
            line += str(confusion_matrix[i][j]) + "" ""
        print line    
print ""KNN started...""  
optimalK = main()",0.55420506,
1004,k nearest neighbors knn classification,"for n in range(1,50):
    clf = neighbors.KNeighborsClassifier(n_neighbors=n)
    cv_scores = cross_val_score(clf, all_features_scaled, classes, cv = 10)
    print (n, cv_scores.mean())",0.5537401438,
1004,k nearest neighbors knn classification,"for n in range(1, 50):
    clf = neighbors.KNeighborsClassifier(n_neighbors=n)
    cv_scores = cross_val_score(clf, all_features_scaled, all_classes, cv=10)
    print (n, cv_scores.mean())",0.5537401438,
1004,k nearest neighbors knn classification,"for i in range(1,50):
    knn = neighbors.KNeighborsClassifier(n_neighbors=i)
    svm_knn_scores = cross_val_score(knn, all_features_scaled, classes, cv=10)
    print((i),':',svm_knn_scores.mean())",0.5537401438,
1004,k nearest neighbors knn classification,"for n in range(1, 50):
    knn = neighbors.KNeighborsClassifier(n_neighbors=n)
    scores = cross_val_score(knn, X_train, y_train, cv=10)
    print('KNN Score:', '{:.2%}'.format(scores.mean()))",0.5537401438,
1004,k nearest neighbors knn classification,"def knn_optimizer(neighbours = 5):
    knn_clf = neighbors.KNeighborsClassifier(n_neighbors = neighbours)
    knn_clf.fit(x_train, y_train)
    y_pred_knn = knn_clf.predict(x_test)

    knn_accuracy = accuracy_score(y_test, y_pred_knn)
    print(""No of neighbours = %s :"" %neighbours, knn_accuracy)
    
for i in range(1, 26):
    knn_optimizer(neighbours = i)",0.5524488688,
542,estimating the sharpe ratio in python,"# Use the STAPLE algorithm to obtain the reference segmentation. This implementation of the original algorithm
# combines a single label from multiple segmentations, the label is user specified. The result of the
# filter is the voxel's probability of belonging to the foreground. We then have to threshold the result to obtain
# a reference binary segmentation.
foregroundValue = 1
threshold = 0.95
reference_segmentation_STAPLE_probabilities = sitk.STAPLE(segmentations, foregroundValue) 
# We use the overloaded operator to perform thresholding, another option is to use the BinaryThreshold function.
reference_segmentation_STAPLE = reference_segmentation_STAPLE_probabilities > threshold

manual_plus_staple = list(segmentations)  
# Append the reference segmentation to the list of manual segmentations
manual_plus_staple.append(reference_segmentation_STAPLE)

interact(display_with_overlay, segmentation_number=(0,len(manual_plus_staple)-1), 
         slice_number = (0, image.GetSize()[1]-1), image = fixed(image),
         segs = fixed(manual_plus_staple), window_min = fixed(-1024), window_max=fixed(976));",0.515065372,
542,estimating the sharpe ratio in python,sea_level = water_pressure2 / (sea_water_density * g) - href,0.5139936805,
542,estimating the sharpe ratio in python,"H0 = 100.0 # Hubble constant in h=1 units of km/s/Mpc
pi_max = 3*presumed_vvir/H0
rp_max = 2*presumed_rvir",0.5115513206,
542,estimating the sharpe ratio in python,"depthre10_NF_copy = first100_analysis_depthre0.depthre10_NF
estimated_true_NF = depthre10_NF_copy / first100_analysis_depthre0.percent_greater10x
print(""Depth Threshold 10: "", pearsonr(estimated_true_NF, first100_analysis_depthre0.depthre10_NF))
print(""Depth Threshold 5: "", pearsonr(estimated_true_NF, first100_analysis_depthre5.depthre5_NF))
print(""Depth Threshold 0: "", pearsonr(estimated_true_NF, first100_analysis_depthre0.depthre0_NF))",0.511457324,
542,estimating the sharpe ratio in python,"#NBVAL_IGNORE_OUTPUT
# Define acquisition geometry: source
from examples.seismic import RickerSource, Receiver

# Define time discretization according to grid spacing
t0 = 0.
tn = 1000.  # Simulation lasts 1 second (1000 ms)
dt = model.critical_dt  # Time step from model grid spacing
nt = int(1 + (tn-t0) / dt)  # Discrete time axis length
time = np.linspace(t0, tn, nt)  # Discrete modelling time

f0 = 0.010  # Source peak frequency is 10Hz (0.010 kHz)
src = RickerSource(name='src', grid=model.grid, f0=f0, time=np.linspace(t0, tn, nt))
src.coordinates.data[0, :] = np.array(model.domain_size) * .5
src.coordinates.data[0, 0] = 20.  # 20m from the left end

# We can plot the time signature to see the wavelet
src.show()",0.5107771158,
542,estimating the sharpe ratio in python,"# Calculate test statistic, Z, assuming null hypothesis holds
z = (p_w - p_b)/(p*(1-p)*(1/n_w + 1/n_b))**0.5",0.5088573694,
542,estimating the sharpe ratio in python,"pts = (games.TEAM_OFF_RTG + games.TEAM_DEF_RTG + games.TEAM_OFF_RTG_AWAY + games.TEAM_DEF_RTG_AWAY) / 2
fig = plt.figure(figsize=(10, 6))
plt.scatter(pts, games['OVER_UNDER'])
plt.annotate('Website Error', xy=(203, 82), xytext=(208, 110), fontsize=16,
             arrowprops=dict(facecolor='black'))
plt.annotate('Website Error', xy=(220, 25), xytext=(211, 60), fontsize=16,
             arrowprops=dict(facecolor='black'))
plt.ylim(-15)

title = 'Over/under lines increase with team ratings'
subtitle = '''Comparison of over/under lines with the sum of home and away
mean ratings (average of offensive and defensive ratings)'''
format_538(fig, 'NBA Stats & Covers.com', xlabel='Sum of Mean Team Ratings', ylabel='Over/Under Line',
           title=title, subtitle=subtitle, xoff=(-0.11, 1.01), yoff=(-0.14, -0.2),
           toff=(-0.09, 1.18), soff=(-0.09, 1.04))
plt.show()",0.5087199211,
542,estimating the sharpe ratio in python,"lannih = 6*r0
aluminumdislvmax = 1000
mydislvmax = aluminumdislvmax*(lengthfac/timefac)
aluminumdisldrag = 1e-4
mydisldrag = aluminumdisldrag*(stressfac*timefac)",0.508453548,
542,estimating the sharpe ratio in python,"p_a_buy=(b_A*0.3)/(b_A*0.3+b_B*0.7)
p_a_buy",0.5081461668,
542,estimating the sharpe ratio in python,"t_mf = (m_mean-f_mean)/(poolstd*(1/m_n + 1/f_n)**0.5)
t_mf",0.5070533156,
473,difference in means,"def grad2(Itf,dx):
    I_0 = (tf.manip.roll(Itf,shift=-1,axis=0) - tf.manip.roll(Itf,shift=1,axis=0))/2.0/dx[0]
    I_1 = (tf.manip.roll(Itf,shift=-1,axis=1) - tf.manip.roll(Itf,shift=1,axis=1))/2.0/dx[1]
    #out[0,:] = out[1,:]-out[0,:] # this doesn't work in tensorflow
    # generally you cannot assign to a tensor
    return I_0, I_1",0.4053032696,
473,difference in means,"def beta(df):
    x_bar = df.x.mean()
    y_bar = df.y.mean()
    numerator = 0.
    for x_i, y_i in zip(df.x, df.y):
        numerator += (x_i - x_bar) * (y_i - y_bar)
    denominator = 0.
    for x_i in df.x:
        denominator += (x_i - x_bar)**2
    return numerator / denominator
        
b = beta(df)
print('b = {:.2f}'.format(b))",0.3960347176,
473,difference in means,"# we also will need gradients
def grad2d(Itf,dx):
    I_0 = (tf.manip.roll(Itf,shift=-1,axis=0) - tf.manip.roll(Itf,shift=1,axis=0))/2.0/dx[0]
    I_1 = (tf.manip.roll(Itf,shift=-1,axis=1) - tf.manip.roll(Itf,shift=1,axis=1))/2.0/dx[1]
    #out[0,:] = out[1,:]-out[0,:] # this doesn't work in tensorflow
    # generally you cannot assign to a tensor
    return I_0, I_1
I_0,I_1 = grad2d(Itf,dx)
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    I_0np = I_0.eval()
    I_1np = I_1.eval()
plt.figure()
plt.imshow(np.sqrt(I_0np**2 + I_1np**2),cmap='gray',aspect='equal',interpolation='none')",0.3934517503,
473,difference in means,"def mean_iou_tf(y_true, y_pred):
   score, up_opt = tf.metrics.mean_iou(y_true, y_pred, NUM_CLASSES)
   K.get_session().run(tf.local_variables_initializer())
   with tf.control_dependencies([up_opt]):
       score = tf.identity(score)
   return score",0.389333427,
473,difference in means,"# Return the intercept and slope of output with respect to input_feature
def simple_linear_regression(input_feature, output, verbose=False):    
    # Average all values of input and output into x and y
    x = input_feature.mean()
    y = output.mean()
    
    n = input_feature.size()
    
    # Average x * y and x * x
    yx = (input_feature * output).sum() / n
    xx = (input_feature * input_feature).sum() / n
    
    # Find the slope, w1 and the y intercept, w0
    w1 = (x * y - yx) / (x * x - xx)
    w0 = y - (w1 * x)
     
    slope = w1
    intercept = w0
    
    if verbose is True:
        print(""Simple Linear Regression"")
        print(""input feature: "" + str(input_feature.head()))
        print(""output: "" + str(output.head()))
        print(""intercept: %f"" % intercept)
        print(""slope: %f"" % slope)
        
    return (intercept, slope)",0.3883783221,
473,difference in means,"def std_diff(X_control, X_test, Z):
    differences = [(X_test.mean()[zi] - X_control.mean()[zi]) for zi in Z]
    root_avg_vars = [np.sqrt((X_test.var()[zi] + X_control.var()[zi])/2.) for zi in Z]
    return [di / ravi for di, ravi in zip(differences, root_avg_vars)]",0.3869255185,
473,difference in means,"def calc_disc_loss(input_imgs, gen_imgs):
    
    loss1 = tf.reduce_mean(tf.squared_difference(input_imgs, 1))
    loss2 = tf.reduce_mean(tf.squared_difference(gen_imgs, 0))
    
    return (loss1 + loss2) / 2",0.3853411674,
473,difference in means,"def makeLabelled(column):
    mean = column.mean()
    for i in range (0,len(column)):
        column[i] = int(column[i]>=mean) 
    return column",0.3832031488,
473,difference in means,"from helpers import calculate_mse

def baseline_global_mean(train, test):
    """"""baseline method: use the global mean.""""""
    global_mean = np.mean([val for sublist in train.data for val in sublist])
    return rmse(convert_to_array(test), global_mean)
    
baseline_global_mean(train, test)",0.3830071092,
473,difference in means,"def diff_of_means(data1, data2): 
    diff = np.mean(data1) - np.mean(data2)
    return diff",0.3822011352,
2336,the distinct operator,"def count_order(expr):
    order = 0
    if not expr.func is sympy.Mul:
        return 2*sympy.count_ops(expr)
    else:
        for i in expr.args:
            if i.func is not sympy.Pow:
                order += 1
            else:
                if i.args[1].is_Integer:
                    order += i.args[1]
                else:
                    order += 1
    #print (expr, order)
    return order",0.4743875265,
2336,the distinct operator,"def solution(tree):
    
    tree.right
    tree.left
    tree.value
    
    return True/False",0.4579514861,
2336,the distinct operator,"import operator
def evaluate(parseTree):
    opers = {'+':operator.add, '-':operator.sub, '*':operator.mul, '/':operator.truediv}
    
    leftC = parseTree.getLeftChild()
    rightC = parseTree.getRightChild()
    
    if leftC and rightC:
        fn = opers[parseTree.getRootVal()]
        return fn(evaluate(leftC),evaluate(rightC))
    else: #both children are None
        return parseTree.getRootVal()",0.4541226625,
2336,the distinct operator,"def fibonacci_undocumented(n):
    if n == 1:
        return 1
    elif n == 2:
        return 1
    else:
        return fibonacci(n-1) + fibonacci(n-2)",0.4456838369,
2336,the distinct operator,"def is_balanced(root):
    height_diff = abs(height(root.left) - height(root.right))
    return height_diff <= 1",0.4447585642,
2336,the distinct operator,"from linear_algebra import scalar_multiply

def project(v, w):
    """""" return the projection of v on to the direction w """"""
    projection_length = dot(v, w)
    return scalar_multiply(projection_length, w)",0.4425084591,
2336,the distinct operator,"def check_prime(n):
    if n <= 1:
        return False
    if n == 2:
        return True
    if n & 1 == 0:
        return False
    i = 3
    while (i*i <= n):
        if n%i == 0:
            return False
        i += 2
    return True",0.4419421554,
2336,the distinct operator,"import operator


def accumulate2(iterable, func=operator.add):
    """""" 
    1. convert the 'iterable' input to a type iter.  (i'm not sure why this is needed, 
            we'll learn more in )
    2. set the total as the first element in 'it' (within try except in case there is no
            element)
    3. yield the total (this is always only the first element of the 'iterable'
    4. for each 'element' in the 'it'
    5. pass the 'total' and 'element' values to the 'func' parameter
    6. yield the total (this is always the second  to the nth element of the 'it'
    """"""
    # 1.
    it = iter(iterable)
    try:
        # 2.
        total = next(it)
    except StopIteration:
        return
    # 3. 
    yield total
    # 4. 
    for element in it:
        # 5. 
        total = func(total, element)
        # 6. 
        yield total",0.4408726096,
2336,the distinct operator,"import math
def abs_dist(point):
    if type(point) != type((1,2)):
        raise ValueError (""Not a tuple"")
    else:
        return abs(math.sqrt(point[0]**2 + point[1]**2))

print(abs_dist((3,4)))",0.43697083,
2336,the distinct operator,"def tree_depth(tree):
    if tree is None:
        return 0
    return 1 + max(tree_depth(tree.left), tree_depth(tree.right))",0.4358965158,
1749,read a number n and print the numbers from to n,"def firstn_gen(n):
    count = 0
    print(""before loop"")
    while count < n:
        print(""yielding number"")
        yield count
        print(""i'm back. incrementing count"")
        count += 1",0.5580800772,
1749,read a number n and print the numbers from to n,"def Gen_mult3(n):
    print( ' this is the first line of the generator function' )
    k=0
    while (k<n):
        print ( 'line before yield' )
        yield 3*k
        print ( 'line after yield' )
        k+=1
    print( 'this is the last line of the generator function')",0.5526749492,
1749,read a number n and print the numbers from to n,"def fib(n):
    """"""
    The Fibonacci sequence starts with the numbers 0 and 1. From then on,
    every number in the sequence is the sum of the two numbers before it. 
    The result looks something like this:
    
    0, 1, 1, 2, 3, 5, 8, 13, 21, 34...
    
    The function fib takes in the number n as a parameter, and returns 
    a list of all Fibonacci numbers that are smaller than n. 
    """"""
    
    fiblist = [0] #Creates a variable fiblist, which is what 
    #will ultimately be returned as an output of the function. We will
    #gradually enlarge the fiblist to include all Fibonacci numbers < n. 
    
    if n<=1:            #In the case that our input is <=1, the only Fibonacci number 
        print(fiblist)  #smaller than n is 0, so we can return fiblist = [0].
    else:               
        fiblist = [0,1,1] #If n > 1, at least the first three numbers of the Fibonacci 
        #sequence, [0, 1, 1] will be smaller than n, so we enlarge 
        #fiblist accordingly. 
        
        while fiblist[-1] + fiblist[-2] <n: #Makes sure that the next Fibonacci number #is <n. 
            fiblist += [fiblist[-1]+fiblist[-2]] #Adds the next number to fiblist. 
        
        print(fiblist) #Once the while condition ceases to hold, we know that the last 
        #Fibonacci number on our list is the final one before we reach n. Therefore we
        #can print fiblist.",0.5523626804,
1749,read a number n and print the numbers from to n,"# Example solution




















def print_rocket(head_length = 3, body_length = 4):
    # Start by printing the head
    print(""  ^  "")
    for idx in range(head_length):
        print(""  |  "")
        
    # Print the part connecting the head and body
    print("" ^|^ "")
    
    # Print the body
    for idx in range(body_length):
        print("" ||| "")
        
    # Print the sweet flames at the end
    print("" AAA "")
    print(""AAAAA"")
    print(""AAAAA"")

    
print_rocket()",0.5516759753,
1749,read a number n and print the numbers from to n,"def my_gen():
    n = 1
    print('This is printed first')
    # Generator function contains yield statements
    yield n

    n += 1
    print('This is printed second')
    yield n

    n += 1
    print('This is printed at last')
    yield n",0.5510057807,
1749,read a number n and print the numbers from to n,"def display_prime_numbers(parameter: int):
    output(""First {} prime numbers are:"".format(parameter))
    for prime in primes(parameter):
        output(str(prime), endWith="" "")",0.5466320515,
1749,read a number n and print the numbers from to n,"def scrapePages(noOfPages):
    print(""BEGIN SCRAPING\n-------------\n"")
    
    for page in range(1, noOfPages + 1):
        scrapeKijijiData(page)
    
    print(""\n---------------\nEND SCRAPING"")",0.544534862,
1749,read a number n and print the numbers from to n,"for e in Input('03'):
    
    x = re.match(r'#(\d+) @ (\d+),(\d+): (\d+)x(\d+)', e)
    
    sum = 0
    
    for j in range(int(x.group(5))):
        for i in range(int(x.group(4))):
            t = int(x.group(2)) + i
            u = int(x.group(3)) + j
            key = (t, u)
            sum += coordinates[key]
            
    if sum == int(x.group(4)) * int(x.group(5)):
        print(e)",0.54306674,
1749,read a number n and print the numbers from to n,"def my_series(f, t):
    print ""my series started!""
    while f < t:
        yield f
        f += 1
    print ""game over""
    # using the return is optionally
    
print type(my_series)

stuff = my_series(0, 10)
print type(stuff), stuff

a = stuff.next()
print a
print next(stuff)

for a in stuff:
    print a,
    
# print stuff.next()",0.5423407555,
1749,read a number n and print the numbers from to n,"for num in range(2, 10):
    if num % 2 == 0:
        print ""Found an even number"", num, ""\n""
        continue # this will cause the anything that follows after this statment that is inside the for loop to be skipped,
                 # basically it branches back to the for loop, skipping the two print statements below.
    print ""Found a number"", num
    print ""Life is good\n""",0.5420541763,
70,array indexing,"sliceim2 = sliceim1[extendbox[0,0]:extendbox[0,1],
                    extendbox[1,0]:extendbox[1,1],
                    extendbox[2,0]:extendbox[2,1]]",0.4726524949,
70,array indexing,"ndvi_im = ndvi_raster[ndvi_pix_origin[0]:ndvi_pix_extent[0], ndvi_pix_origin[1]:ndvi_pix_extent[1]]",0.4719282091,
70,array indexing,"contrast_matrix = np.eye(design_matrix.shape[1])
basic_contrasts = dict([(column, contrast_matrix[i])
                  for i, column in enumerate(design_matrix.columns)])",0.4644829631,
70,array indexing,"contrast_matrix = np.eye(design_matrix.shape[1])
contrasts = dict([(column, contrast_matrix[i])
                  for i, column in enumerate(design_matrix.columns)])",0.4644829631,
70,array indexing,"obama_features = map(lambda x: features_names[x], obama_tfidf.indices)",0.4629211426,
70,array indexing,[feature_names[i] for i in x.indices],0.4585895538,
70,array indexing,"# add the single timestep to the training data
train_data = {inp: train_data[0][:, None, :],
              out_p: train_data[1][:, None, :]}

# when testing our network with spiking neurons we will need to run it 
# over time, so we repeat the input/target data for a number of 
# timesteps. we're also going to reduce the number of test images, just 
# to speed up this example.
n_steps = 30
test_data = {
    inp: np.tile(test_data[0][:minibatch_size*2, None, :],
                 (1, n_steps, 1)),
    out_p_filt: np.tile(test_data[1][:minibatch_size*2, None, :],
                        (1, n_steps, 1))}",0.4504609704,
70,array indexing,"surface_wind[{0, 2, 3, 5}, :, 150:350, 50:250]",0.4493314028,
70,array indexing,"fake_solar_data[:,:,0,0].flat[idx]",0.4489159584,
70,array indexing,"common_boy_names[common_boy_names == 100]  # Common for 100 years, but not 129 years.",0.4473671317,
530,end of project solution,"def solve_primal_update(problem_data):
    index, problem_i, p_ang, z_i, ld_i, x = problem_data
    
    # solve problem
    problem_i.solve(verbose=False, solver=cvxpy.ECOS, abstol=1e-9)

    # get solution
    x_i = np.array(x.value).flatten()
    z_i = np.array(z_i.value).flatten()
    # calculate residual
    r_i = norm(p_ang * x_i - z_i, ord=inf)
    
    return index, x_i, r_i, prob.objective.value",0.3504495621,
530,end of project solution,"class Solution(object):
    def reverseBetween(self, head, m, n):
        """"""
        :type head: ListNode
        :type m: int
        :type n: int
        :rtype: ListNode
        """"""
        ps = []
        h = ListNode(0)
        h.next = head
        def re(head,m,n,p):
            while p < m - 1:
                p += 1
                head = head.next
            if p == m - 1:
                re(head.next,m,n,p+1)
                head.next = ps[0][0]
            if p >= m and p < n:
                re(head.next,m,n,p+1).next = head
                if p == m:
                    head.next = ps[0][1]
                    return
                return head
            elif p == n:
                p.append([head,head.next])
                return head
        re(h,m,n,0)
        return h.next",0.3353431225,
530,end of project solution,"%%writefile -a RosenbrockOpt.cpp

    // solve the optimization problem
    Eigen::VectorXd xOpt = Solver->solve(x0);

    // Get the termination status
    int optStat = Solver->GetStatus();

    std::cout << ""Optimal solution = "" << xOpt.transpose() << std::endl;
    
    return 0;
} // End of ""int main()""",0.3311339617,
530,end of project solution,"def closedform(X,T,l):

    ### TODO: REPLACE BY YOUR CODE
    import solutions
    w,t = solutions.closedform(X,T,l)
    ### --------------------------
        
    return w,t",0.3216102719,
530,end of project solution,"def independent_set_ip(graph):
    prob = pulp.LpProblem(""Min Cost Flow"", pulp.LpMaximize)
    
    nodes = [n for n in graph.nodes()]
    node_vars = pulp.LpVariable.dicts(""node"", nodes, 0, 1, pulp.LpInteger)
    
    prob += pulp.lpSum(node_vars)
    
    for e in graph.edges():
        prob += node_vars[e[0]] + node_vars[e[1]] <= 1
        
    prob.solve()
    
    return [(var, pulp.value(node_vars[var])) for var in node_vars]",0.320720315,
530,end of project solution,"def independent_set_lp(graph):
    prob = pulp.LpProblem(""Min Cost Flow"", pulp.LpMaximize)
    
    nodes = [n for n in graph.nodes()]
    node_vars = pulp.LpVariable.dicts(""node"", nodes, 0, 1)
    
    prob += pulp.lpSum(node_vars)
    
    for e in graph.edges():
        prob += node_vars[e[0]] + node_vars[e[1]] <= 1
        
    prob.solve()
    
    return [(var, pulp.value(node_vars[var])) for var in node_vars]",0.320720315,
530,end of project solution,"def evaluate(model):
    score = model.evaluate(imgTest, onehotTest, verbose=0)
    print('Test loss     :', score[0])
    print('Test accuracy :', score[1])
    
evaluate(model)",0.3206495941,
530,end of project solution,"print(""Solve status: "" + msol.get_solve_status())
if msol.is_solution():
    stdout.write(""Solve time: "" + str(msol.get_solve_time()) + ""\n"")
    # Sort tasks in increasing begin order
    ltasks = []
    for hs in HOUSES:
        for tsk in TASKS:
            (beg, end, dur) = msol[tasks[(hs, tsk)]]
            ltasks.append((hs, tsk, beg, end, dur))
    ltasks = sorted(ltasks, key = lambda x : x[2])
    # Print solution
    print(""\nList of tasks in increasing start order:"")
    for tsk in ltasks:
        print(""From "" + str(tsk[2]) + "" to "" + str(tsk[3]) + "", "" + tsk[1].name + "" in house "" + str(tsk[0]))
else:
    stdout.write(""No solution found\n"")",0.3184758425,
530,end of project solution,"class Solution(object):
    def findAnagrams(self, s, p):
        """"""
        :type s: str
        :type p: str
        :rtype: List[int]
        """"""
        indices=[]
        p_dict={}
        # create a dictionary of p
        for c in p:
            if p_dict.get(c)==None:
                p_dict[c]=1
            else:
                p_dict[c]+=1
        # update the dict value of p when passing through s.
        match=0
        for i in xrange(len(s)):
            # check correspondence 
            if p_dict.get(s[i])!=None:
                p_dict[s[i]]-=1
                if p_dict[s[i]]==0:
                    match+=1
            if i>=len(p):
                print match,i
                if p_dict.get(s[i-len(p)])!=None:
                    p_dict[s[i-len(p)]]+=1
                    if p_dict[s[i-len(p)]]==1:
                        match-=1
                print match
            if match==len(p_dict):
                indices.append(i-len(p)+1)
        return indices",0.3182896972,
530,end of project solution,"def print_results(model):
    print ('|=================================================================|')
    print ('|                   Feature     Coefficient                       |')
    print ('|                   -------     -----------                       |')
    for idx in range(len(model[0])):
        print ('|', '{:>25}'.format(model[0][idx]), ' : ', '{:<25}'.format(
                model[1][idx]), '        |')
    print ('|                                                                 |')
    print ('|                                   Training       Test           |')
    print ('|         Area Under the ROC Curve:', '{:<10}'.format(
            round(float(model[2]), 5)), '   ', '{:<10}'.format(
                    round(float(model[3]), 5)), '    |')
    if model[4] != None:
        print ('|           Minimum Cost Per Event:', '{:<10}'.format(
                round(float(model[5]), 5)), '   ', '{:<10}'.format
            (round(float(model[6]), 5)), '    |')
        print ('|                     at threshold:', '{:<20}'.format
               (round(float(model[4]), 10)), '         |')
        print ('|              Condition Incidence:', '{:<10}'.format(
                round(float(model[7]), 5)), '   ', '{:<10}'.format(
                        round(float(model[8]), 5)), '    |')
        print ('|    Probability of True Positives:', '{:<10}'.format(
                round(float(model[9]), 5)), '   ', '{:<10}'.format(
                        round(float(model[10]), 5)), '    |')
        print ('|  Test (Classification) Indidence:', '{:<10}'.format
               (round(float(model[11]), 5)), '   ', '{:<10}'.format(
                       round(float(model[12]), 5)), '    |')
    print ('|=================================================================|')
    print ('')
    
    return",0.3173658252,
2048,"step create a scatter plot with the day as the y axis and tip as the x axis, differ the dots by sex","def plot_night_seeing(df, night, fig, axes):
    night_df = df[df.night == night]
    
    night_df.plot(y='log_r0', drawstyle='steps-mid',ax=axes)

    # Add an axes with the DIMM measured seeing on the right
    right_axes = axes.twinx()
    night_df.plot(y='log_r0', drawstyle='steps-mid', ax=right_axes)

    seeing_ticks = np.concatenate((np.arange(0.4, 2.0, 0.2), np.arange(2.0, 4.0, 0.4)), axis=0)
    logr0_ticks = np.log10(0.98*5e-7/np.radians(seeing_ticks/(60*60)))
    right_axes.set_yticks(logr0_ticks)
    right_axes.set_yticklabels(seeing_ticks)
    right_axes.set_ylabel('seeing ("")')

    right_axes.set_ylim(axes.get_ylim())
    right_axes.grid(False)
    axes.set_ylabel(""$\log(r_{0})$, $r_{0}$ in meters"")
    
    axes.xaxis.set_major_formatter(mpl.dates.DateFormatter('%H:%m:%SZ'))
    
    axes.legend().set_visible(False)
    right_axes.legend().set_visible(False)
    
    axes.set_title('Night MJD %d' % night)
    return fig, axes",0.5550245643,
2048,"step create a scatter plot with the day as the y axis and tip as the x axis, differ the dots by sex","def plot_class_broke_down_hist(df, var, xlog=False, ylog=False, **histkwargs):
    df[var][df.died_hosp == 0].hist(alpha=.5, label='survived', **histkwargs)
    df[var][df.died_hosp == 1].hist(alpha=.5, label='died', **histkwargs)
    plt.xlabel(var)
    if xlog:
        plt.xscale('log')
    if ylog:
        plt.yscale('log')
    plt.legend();",0.5464372635,
2048,"step create a scatter plot with the day as the y axis and tip as the x axis, differ the dots by sex","def get_px(tickers_ls, start_dt, end_dt):
    px_data=pd.DataFrame() #create dataframe
    for ticker in tickers_ls:
        print (ticker, end="""", flush=True),
        px_data_raw=pd.DataFrame(Share(ticker).get_historical(start_dt, end_dt))
        if len(px_data_raw)!=0:
            px_data=px_data.append(px_data_raw)
        px_data=px_data.reindex()
    px_data['Adj_Close']=px_data.Adj_Close.astype(float)
    px_data=px_data.sort_values(by=['Symbol','Date'])
    px_data['ret']=px_data.groupby('Symbol')['Adj_Close'].pct_change().fillna(0)
    px_data['Date']=pd.to_datetime(px_data['Date'])
    px_data=px_data.set_index('Date')
    return px_data",0.5353136659,
2048,"step create a scatter plot with the day as the y axis and tip as the x axis, differ the dots by sex","def get_column_info(df,column):
    # print general info about column
    print df[column].describe()
    
    # show histogram amd box plot
    plt.figure()
    df[df.TARGET==0][column].plot(kind=""hist"", label=""satisfied customers"")
    df[df.TARGET==1][column].plot(kind=""hist"", label=""unsatisfied customers"")
    plt.legend();
    
    plt.figure()
    df[df.TARGET==0][column].plot(kind=""kde"", label=""satisfied customers"")
    df[df.TARGET==1][column].plot(kind=""kde"", label=""unsatisfied customers"")
    plt.legend();
    
    plt.figure()
    df[column].plot(kind='box')",0.5334239602,
2048,"step create a scatter plot with the day as the y axis and tip as the x axis, differ the dots by sex","def plot_sp(t='percent'):
    plt.figure(figsize=(6,3))
    g = sns.barplot(percent_stat_df.season.sort_values(), percent_stat_df[t], color=""b"")
    for item in g.get_xticklabels():
        item.set_rotation(90)

plot_sp()",0.5333902836,
2048,"step create a scatter plot with the day as the y axis and tip as the x axis, differ the dots by sex","def generate_and_save_images(model, epoch, test_input):
  predictions = model.sample(test_input)
  fig = plt.figure(figsize=(4,4))

  for i in range(predictions.shape[0]):
      plt.subplot(4, 4, i+1)
      plt.imshow(predictions[i, :, :, 0], cmap='gray')
      plt.axis('off')

  # tight_layout minimizes the overlap between 2 sub-plots
  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))
  plt.show()",0.5323119164,
2048,"step create a scatter plot with the day as the y axis and tip as the x axis, differ the dots by sex","def plot_sim_states(start, end, **kwargs): 
    fig, axes = plt.subplots(3, 2, figsize=(10,15))
    nbody.plot_positions(axes[0,0], start[""x""], start[""y""], **kwargs)
    nbody.plot_positions(axes[0,1], end[""x""], end[""y""], **kwargs)
    
    nbody.hist_velocities(axes[1,0], start[""vx""], label=r""$v_x$"", xlabel="""", ylabel="""", **kwargs)
    nbody.hist_velocities(axes[1,1], end[""vx""],   label=r""$v_x$"", xlabel="""", ylabel="""", **kwargs)
    nbody.hist_velocities(axes[2,0], start[""vy""], label=r""$v_y$"", xlabel="""", ylabel="""", **kwargs)
    nbody.hist_velocities(axes[2,1], end[""vy""],   label=r""$v_y$"", xlabel="""", ylabel="""", **kwargs)

    #pos plots
    axes[0,0].set_xlabel(""x"")
    axes[0,0].set_ylabel(""y"")
    axes[0,1].set_xlabel(""x"")
    
    #vel plots
    axes[1,0].set_ylabel(r""$N_{\mathrm{particles}}$"")
    axes[2,0].set_ylabel(r""$N_{\mathrm{particles}}$"")
    axes[2,0].set_xlabel(r""$v [1/v_s]$"")
    axes[2,1].set_xlabel(r""$v [1/v_s$"")
    #axes[0,1].set_title(""End of sim."")
    
    axes[0,0].set_title(""Begining of sim."")
    axes[0,1].set_title(""End of sim."")

folderpath = ""100p_5s_ordered_ekinconstr_normal""
start = nbody.snapshot_data(join(folderpath, ""snapshot_0.0""))
end   = nbody.snapshot_data(join(folderpath, ""snapshot_4.991885228""))

plot_sim_states(start, end, bins=20, ylim=(0, 60), xlim=(-0.5, 0.5))
plt.tight_layout()",0.5316011906,
2048,"step create a scatter plot with the day as the y axis and tip as the x axis, differ the dots by sex","def new_actions(test):
    ''' samples 5 possible actions from the given test set'''
    C = test[test.position_sign == 'C'].sample(1)
    G = test[test.position_sign == 'G'].sample(2)
    F = test[test.position_sign == 'F'].sample(2)
    return pd.concat([C,G,F])",0.5311635733,
2048,"step create a scatter plot with the day as the y axis and tip as the x axis, differ the dots by sex","def printplots(df, fields):
    plt.figsize=(15,9)
    sns.pairplot(df[fields]);
    plt.figure()
    sns.heatmap(df[fields].corr(), annot=True, cmap=""YlGnBu"");
printplots(df, pred + personal)",0.5307123661,
2048,"step create a scatter plot with the day as the y axis and tip as the x axis, differ the dots by sex","# plot
def plot_single_data(df):
    for i in range(1,6):
        pl.plot(df.Male[i],df.Female[i],'o',ms=15,c = colors[i-1], label = 'selected ' + keys[i-1])
    pl.legend(loc ='upper left')",0.5294941068,
630,filtering data,"epochs._data = mne.filter.filter_data(epochs._data, epochs.info['sfreq'], None, 5)",0.4180208445,
630,filtering data,"""""""Bootstrap A/B test for the difference in the mean response
Assumes A=0, B=1""""""

def bootstrap_AB_test(table, categories, values, repetitions):
    
    # Sort the sample table according to the A/B column; 
    # then select only the column of effects.
    response = table.sort(categories).select(values)
    
    # Find the number of entries in Category A.
    n_A = table.where(categories, 0).num_rows
      
    # Calculate the observed value of the test statistic.
    meanA = np.mean(response.column(values)[:n_A])
    meanB = np.mean(response.column(values)[n_A:])
    observed_difference = meanA - meanB
    
    # Run the bootstrap procedure and get a list of resampled differences in means
    diffs = []
    for i in range(repetitions):
        resample = response.sample(with_replacement=True)
        d = np.mean(resample.column(values)[:n_A]) - np.mean(resample.column(values)[n_A:])
        diffs.append([d])
    
    # Compute the bootstrap empirical P-value
    diff_array = np.array(diffs)
    empirical_p = np.count_nonzero(abs(diff_array) >= abs(observed_difference))/repetitions
    
    # Display results
    differences = Table().with_column('diff_in_means', diffs)
    differences.hist(bins=20,normed=True)
    plots.xlabel('Approx null distribution of difference in means')
    plots.title('Bootstrap A/B Test')
    print(""Observed difference in means: "", observed_difference)
    print(""Bootstrap empirical P-value: "", empirical_p)",0.4124480784,
630,filtering data,"def run_pfilter(pfilter, inputs):
    """"""Apply a particle filter to a time series of inputs,
    and return the particles, their weights, and the mean
    estimated state""""""    
    # reset filter
    pfilter.init_filter()
    particles, weights, means = [], [], []
    # apply to each element of the time series
    for i in range(len(inputs)):           
        pfilter.update([inputs[i]])
        particles.append(pfilter.original_particles)    
        weights.append(pfilter.weights)
        means.append(pfilter.mean_state)        
    return np.array(particles), np.array(weights), np.array(means)",0.4106161892,
630,filtering data,"%matplotlib inline
# Take the absolute value, then low-pass filter
envelope = audio.copy()
envelope._data = np.abs(envelope._data)
envelope._data = mne.filter.filter_data(envelope._data,
                                        envelope.info['sfreq'],
                                        None,
                                        10, filter_length='1s')
envelope.resample(sfreq_new)",0.4062585533,
630,filtering data,"def agg(df, n):
    df['tile'] = pd.qcut(df['rs6ma'], n, labels = range(n))
    res = df.groupby('tile')['rs6mb'].mean()
    return res

agg(df, 10)",0.4034340382,
630,filtering data,"def agg(df, n):
    df['tile'] = pd.qcut(df['rs6ma'], n, labels = range(n))
    res = df.groupby('tile')['rs6mb'].mean()
    return res",0.4034340382,
630,filtering data,"####### The delay ratio of the top 20 busiest airports ########
K = 20

# extract top_20_airports from stat_airport_traffic
top_20_airports = [item[0] for item in stat_airport_traffic.take(K)]

# select the statistic of source airports
statistic_ratio_delay_airport = (
    df_with_delay
        # select only flights that depart from one of top 20 ariports
        .filter(df_with_delay.src_airport.isin(top_20_airports))
        # group by source airport
        .groupBy('src_airport')
        #  calculate the delay ratio
        .agg((func.sum('is_delay')/func.count('*')).alias('delay_ratio'))
        # sort by name of airport
        .orderBy(['src_airport'])
    )
statistic_ratio_delay_airport.show(20)",0.4025577307,
630,filtering data,"# Convolutional autoencoder
def deeper_nodense(conf):
    num_filters = conf.num_filters or 64
    # Encoder

    unpool = DePool2D if conf.use_depool else Upsampling2DPadded

    kwargs = { 'data_format': ""channels_first"" }
    pool_size = 2

    # (64)5c-2p-(64)3c-2p-(64)3c-2p-10fc
    x = Input(shape=conf.original_img_size)

    conv1 = conv2d(num_filters, 5, name=""conv1"", use_batch_normalization=conf.use_batch_normalization, **kwargs)(x)
    maxp1 = MaxPooling2D(pool_size=(pool_size, pool_size), **kwargs)(conv1)
    conv2 = conv2d(num_filters, 3, name=""conv2"", use_batch_normalization=conf.use_batch_normalization, **kwargs)(maxp1)
    
    if conf.num_layers == 3:
        maxp2 = MaxPooling2D(pool_size=(pool_size, pool_size), **kwargs)(conv2)
        conv3 = conv2d(num_filters, 3, name=""conv3"", use_batch_normalization=conf.use_batch_normalization, **kwargs)(maxp2)
        maxp3 = MaxPooling2D(name=conf.encoded_name, pool_size=(pool_size, pool_size), **kwargs)(conv3)
        z = maxp3
    else:
        maxp2 = MaxPooling2D(name=conf.encoded_name, pool_size=(pool_size, pool_size), **kwargs)(conv2)
        z = maxp2
        
    y = z
    if conf.num_layers == 3:
        y = unpool(size=(pool_size, pool_size), **kwargs)([conv3, maxp3, y])
        y = deconv2d(num_filters, 3, **kwargs)(y)
    
    y = unpool(size=(pool_size, pool_size), **kwargs)([conv2, maxp2, y])
    y = deconv2d(num_filters, 3, use_batch_normalization=conf.use_batch_normalization, **kwargs)(y)
    y = unpool(size=(pool_size, pool_size), **kwargs)([conv1, maxp1, y])
    y = deconv2d(1, 5, use_batch_normalization=conf.use_batch_normalization, activation=None, **kwargs)(y)

    # AE
    ae = Model(inputs=x, outputs=y)
    optimizer = Adam(lr=conf.learning_rate, decay=conf.decay)
    ae.compile(optimizer=optimizer, loss='mse')

    encoder = Model(x, z)

    return ae, encoder",0.4007640183,
630,filtering data,"def parsplit(DF, input, quantiles):
    qvalues = pd.qcut(DF[input], quantiles, labels = range(1, quantiles+1))
    return qvalues",0.4001256526,
630,filtering data,"def housing_price_heat_map(my_df, feature_name, bin_num):
    # binning price into 20 bins
    my_df.loc[:, 'price_bin'] = pd.cut(my_df.loc[:, 'price'], bins = bin_num, labels = range(bin_num))
    my_df.loc[:, 'price_bin'].head(10)
    # count how many houses within each bin
    feature_price = pd.crosstab(my_df.loc[:, 'price_bin'], my_df.loc[:, feature_name])
    # normalize the count numbers, so we can get the density value 
    total_num_houses = feature_price.apply(sum, axis = 0)
    feature_price = feature_price.div(total_num_houses, axis = 1)
    # plot the density map with seaborn
    ax = plt.figure(figsize=(4, 4)).gca() # define axis
    ax.pcolor(feature_price, cmap = 'Blues')
    ax.set_xticks(range(feature_price.shape[1])) 
    ax.set_xticklabels(feature_price.columns, rotation=90)
    ax.set_xlabel(feature_name)
    ax.set_ylabel('House Price Bin')
    title_name = 'House price ranges by ' + feature_name
    ax.set_title(title_name)",0.4000256062,
2535,waveform data seismometer time series,"hp, hc = pycbc.waveform.get_td_waveform(
    approximant=""SEOBNRv2"",
    mass1=h1_bank.table['mass1'][0],
    mass2=h1_bank.table['mass2'][0],
    spin1z=h1_bank.table['spin1z'][0],
    spin2z=h1_bank.table['spin2z'][0],
    f_lower=search_f_low,
    delta_t=h1_bank.delta_t)

template = h1_bank[0].to_timeseries() / DYN_RANGE_FAC
template.roll(int(-template.start_time / template.delta_t))

plt.plot(template.sample_times, template,
         color='r',label='Search template')
plt.plot(hp.sample_times, hp, linestyle=':', label='SEOBNRv2')

np.savetxt('SearchTemplate_hp.txt', np.c_[template.sample_times.numpy(), 
                                          template.numpy()])

plt.xlim([-0.3,0.05])
plt.title('Search template at 1 Mpc')
plt.legend(loc='best')
plt.ylabel('Strain'.format(DYN_RANGE_FAC))
plt.xlabel('Time from waveform peak (seconds)')",0.5201243162,
2535,waveform data seismometer time series,"features_and_response, heart_group = load_wav_data_into_features(
    wave_folder_path='set_b/normal/', training_segmentation_file='./set_b/Btraining_normal_seg.csv',
    time_point_spec=final_confirmed_window_spec,
    time_point_avg_window=final_confirmed_window_amplitude,
    noise_filter=final_noise_pass_filter,
    normalize_raw_signal='standard',
    l_time_trim = 0.1, r_time_trim=0.1
)

features_and_response_null_drop = features_and_response[features_and_response.isnull().sum(axis=1)==0]
X = features_and_response_null_drop[amp_col + spec_col].values
y = features_and_response_null_drop['info']['response'].map({'S1': 0, 'S2': 1, 'NOISE': 2}).values

scores = cross_val_score(estimator=pipe_bc, X=X, y=y, cv=10, n_jobs=1)",0.5134814382,
2535,waveform data seismometer time series,"import pywt as pywt

def extractRMSVWavelet(signal,waveFamily):
	C = pywt.wavedec(signal,waveFamily,mode='sym',level=5)
	RMS = np.array([np.sqrt(np.linalg.norm(wave)**2/(len(wave)-1)) for wave in C])
 	return RMS

def extractRMS(line,timeg1,timeg2,timeg3,dataid):
    if line in dataid:
        n1 = timeg1.ix[line].sum()
        n2 = timeg2.ix[line].sum()
        n3 = timeg3.ix[line].sum()
        if n1 > (n2 + n3):
            return extractRMSVWavelet(timeg1.ix[line],'db7')
        elif n2 >= (n1 + n3):
            return extractRMSVWavelet(timeg2.ix[line],'db7')
        else :
            return extractRMSVWavelet(timeg3.ix[line],'db7')
    else:
        return np.array([0.,0.,0.,0.,0.,0.])

colum = train['bidder_id'].apply(lambda x: extractRMS(x,time_group1 ,time_group2 ,time_group3,dataIdList))
columtest = test['bidder_id'].apply(lambda x: extractRMS(x,time_group1 ,time_group2 ,time_group3,dataIdList))",0.5022912621,
2535,waveform data seismometer time series,"def view_harmonics(freq, framerate):
    signal = thinkdsp.SawtoothSignal(freq)
    wave = signal.make_wave(duration=0.5, framerate=framerate)
    spectrum = wave.make_spectrum()
    spectrum.plot(color='blue')
    thinkplot.show(xlabel='frequency', ylabel='amplitude')
    
    display(wave.make_audio())",0.5001956224,
2535,waveform data seismometer time series,"def view_harmonics(freq, framerate):
    signal = thinkdsp.SawtoothSignal(freq)
    wave = signal.make_wave(duration=0.5, framerate=framerate)
    spectrum = wave.make_spectrum()
    spectrum.plot(color='blue')
    thinkplot.config(xlabel='frequency', ylabel='amplitude',title='freq:{} ,framerate:{}'.format(freq,framerate))
    
    display(wave.make_audio())",0.5001956224,
2535,waveform data seismometer time series,"def calc_syn0norm(model):
    """"""since syn0norm is now depricated""""""
    return (model.wv.syn0 / np.sqrt((model.wv.syn0 ** 2).sum(-1))[..., np.newaxis]).astype(np.float32)

def smart_procrustes_align_gensim(base_embed, other_embed, words=None):
    """"""Procrustes align two gensim word2vec models (to allow for comparison between same word across models).
    Code ported from HistWords <https://github.com/williamleif/histwords> by William Hamilton <wleif@stanford.edu>.
    (With help from William. Thank you!)
    First, intersect the vocabularies (see `intersection_align_gensim` documentation).
    Then do the alignment on the other_embed model.
    Replace the other_embed model's syn0 and syn0norm numpy matrices with the aligned version.
    Return other_embed.
    If `words` is set, intersect the two models' vocabulary with the vocabulary in words (see `intersection_align_gensim` documentation).
    """"""

    # make sure vocabulary and indices are aligned
    in_base_embed, in_other_embed = intersection_align_gensim(base_embed, other_embed, words=words)

    # get the embedding matrices
    base_vecs = calc_syn0norm(in_base_embed)
    other_vecs = calc_syn0norm(in_other_embed)

    # just a matrix dot product with numpy
    m = other_vecs.T.dot(base_vecs) 
    # SVD method from numpy
    u, _, v = np.linalg.svd(m)
    # another matrix operation
    ortho = u.dot(v) 
    # Replace original array with modified one
    # i.e. multiplying the embedding matrix (syn0norm)by ""ortho""
    other_embed.wv.syn0norm = other_embed.wv.syn0 = (calc_syn0norm(other_embed)).dot(ortho)
    return other_embed
    
def intersection_align_gensim(m1,m2, words=None):
    """"""
    Intersect two gensim word2vec models, m1 and m2.
    Only the shared vocabulary between them is kept.
    If 'words' is set (as list or set), then the vocabulary is intersected with this list as well.
    Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).
    These indices correspond to the new syn0 and syn0norm objects in both gensim models:
        -- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0
        -- you can find the index of any word on the .index2word list: model.index2word.index(word) => 2
    The .vocab dictionary is also updated for each model, preserving the count but updating the index.
    """"""

    # Get the vocab for each model
    vocab_m1 = set(m1.wv.vocab.keys())
    vocab_m2 = set(m2.wv.vocab.keys())

    # Find the common vocabulary
    common_vocab = vocab_m1&vocab_m2
    if words: common_vocab&=set(words)

    # If no alignment necessary because vocab is identical...
    if not vocab_m1-common_vocab and not vocab_m2-common_vocab:
        return (m1,m2)

    # Otherwise sort by frequency (summed for both)
    common_vocab = list(common_vocab)
    common_vocab.sort(key=lambda w: m1.wv.vocab[w].count + m2.wv.vocab[w].count,reverse=True)

    # Then for each model...
    for m in [m1,m2]:
        # Replace old syn0norm array with new one (with common vocab)
        indices = [m.wv.vocab[w].index for w in common_vocab]
        old_arr = calc_syn0norm(m)
        new_arr = np.array([old_arr[index] for index in indices])
        m.wv.syn0norm = m.wv.syn0 = new_arr

        # Replace old vocab dictionary with new one (with common vocab)
        # and old index2word with new one
        m.index2word = common_vocab
        old_vocab = m.wv.vocab
        new_vocab = {}
        for new_index,word in enumerate(common_vocab):
            old_vocab_obj=old_vocab[word]
            new_vocab[word] = gensim.models.word2vec.Vocab(index=new_index, count=old_vocab_obj.count)
        m.wv.vocab = new_vocab

    return (m1,m2)",0.4975101948,
2535,waveform data seismometer time series,"def calc_syn0norm(model):
    """"""since syn0norm is now depricated""""""
    return (model.wv.syn0 / np.sqrt((model.wv.syn0 ** 2).sum(-1))[..., np.newaxis]).astype(np.float32)

def smart_procrustes_align_gensim(base_embed, other_embed, words=None):
    """"""Procrustes align two gensim word2vec models (to allow for comparison between same word across models).
    Code ported from HistWords <https://github.com/williamleif/histwords> by William Hamilton <wleif@stanford.edu>.
    (With help from William. Thank you!)
    First, intersect the vocabularies (see `intersection_align_gensim` documentation).
    Then do the alignment on the other_embed model.
    Replace the other_embed model's syn0 and syn0norm numpy matrices with the aligned version.
    Return other_embed.
    If `words` is set, intersect the two models' vocabulary with the vocabulary in words (see `intersection_align_gensim` documentation).
    """"""
    base_embed = copy.copy(base_embed)
    other_embed = copy.copy(other_embed)
    # make sure vocabulary and indices are aligned
    in_base_embed, in_other_embed = intersection_align_gensim(base_embed, other_embed, words=words)

    # get the embedding matrices
    base_vecs = calc_syn0norm(in_base_embed)
    other_vecs = calc_syn0norm(in_other_embed)

    # just a matrix dot product with numpy
    m = other_vecs.T.dot(base_vecs) 
    # SVD method from numpy
    u, _, v = np.linalg.svd(m)
    # another matrix operation
    ortho = u.dot(v) 
    # Replace original array with modified one
    # i.e. multiplying the embedding matrix (syn0norm)by ""ortho""
    other_embed.wv.syn0norm = other_embed.wv.syn0 = (calc_syn0norm(other_embed)).dot(ortho)
    return other_embed
    
def intersection_align_gensim(m1,m2, words=None):
    """"""
    Intersect two gensim word2vec models, m1 and m2.
    Only the shared vocabulary between them is kept.
    If 'words' is set (as list or set), then the vocabulary is intersected with this list as well.
    Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).
    These indices correspond to the new syn0 and syn0norm objects in both gensim models:
        -- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0
        -- you can find the index of any word on the .index2word list: model.index2word.index(word) => 2
    The .vocab dictionary is also updated for each model, preserving the count but updating the index.
    """"""

    # Get the vocab for each model
    vocab_m1 = set(m1.wv.vocab.keys())
    vocab_m2 = set(m2.wv.vocab.keys())

    # Find the common vocabulary
    common_vocab = vocab_m1&vocab_m2
    if words: common_vocab&=set(words)

    # If no alignment necessary because vocab is identical...
    if not vocab_m1-common_vocab and not vocab_m2-common_vocab:
        return (m1,m2)

    # Otherwise sort by frequency (summed for both)
    common_vocab = list(common_vocab)
    common_vocab.sort(key=lambda w: m1.wv.vocab[w].count + m2.wv.vocab[w].count,reverse=True)

    # Then for each model...
    for m in [m1,m2]:
        # Replace old syn0norm array with new one (with common vocab)
        indices = [m.wv.vocab[w].index for w in common_vocab]
        old_arr = calc_syn0norm(m)
        new_arr = np.array([old_arr[index] for index in indices])
        m.wv.syn0norm = m.wv.syn0 = new_arr

        # Replace old vocab dictionary with new one (with common vocab)
        # and old index2word with new one
        m.index2word = common_vocab
        old_vocab = m.wv.vocab
        new_vocab = {}
        for new_index,word in enumerate(common_vocab):
            old_vocab_obj=old_vocab[word]
            new_vocab[word] = gensim.models.word2vec.Vocab(index=new_index, count=old_vocab_obj.count)
        m.wv.vocab = new_vocab

    return (m1,m2)",0.4975101948,
2535,waveform data seismometer time series,"class WaveformData(object):
    def __init__(self, filename, anomaly_class, val_size):
        random.seed(42)
        f = open(filename,""r"")
        data = f.read()
        lines = data.split('\n')

        self.data = []
        for line in lines[:-1]:
            self.data.append([float(x) for x in line.split(',')])
        self.val_size = val_size
        
        random.shuffle(self.data)
        self.anomaly_data = [x[:-1] for x in self.data if x[-1] == anomaly_class][:val_size]
        classes = [x[-1] for x in self.data if x[-1] != anomaly_class]
        self.data = [x[:-1] for x in self.data if x[-1] != anomaly_class]
        self.normal_val_data = self.data[:val_size]
        self.normal_classes = classes[:val_size]
        self.data = self.data[val_size:]
        self.classes = classes[val_size:]
        
    def normalize(self):
        # Normalize based on our training data
        self.mean, self.std = np.mean(self.data), np.std(self.data)
        self.data = (self.data - self.mean) / self.std
        self.normal_val_data = (self.normal_val_data - self.mean) / self.std
        self.anomaly_data = (self.anomaly_data - self.mean) / self.std",0.490126878,
2535,waveform data seismometer time series,"# Sum up the intensity of onset activation function
def onsetDensity(FILEPATH):
    signal, sample_rate = madmom.audio.signal.load_wave_file(FILEPATH, num_channels = 1)
    duration = len(signal) / float(sample_rate) # duration = samples / sample_rate (sample_rate = sample / time)
    print ""Duration: {} seconds"".format(duration)
    
    log_filt_spec = \
    madmom.audio.spectrogram.LogarithmicFilteredSpectrogram(FILEPATH,\
                num_bands=24, num_channels = 1, norm_filters = True) # Normalizing the magnitudes
    onset = madmom.features.onsets.superflux(log_filt_spec)
    sumActivation = sum(onset)
    print ""Intensity : {}"".format(sumActivation)
    
    return sumActivation / duration

onsetDensity = onsetDensity(FILEPATH)

print 'Onset Density : {} unit / sec'.format(onsetDensity)",0.4896134734,
2535,waveform data seismometer time series,"def show_dye_emission_enclosed_by_filters(dye, filtercube, objective, camera, title='dummy title'):
    """"""Given dye and filtercube, give visual indication of overlap""""""
    
    dlambda = 0.5
    dem = utils.interpolateSpectrum(dye.emissionSpectrum, dlambda)
    fem = utils.interpolateSpectrum(filtercube.emissionFilter.getSpectrum(), dlambda)
    fdiem = utils.interpolateSpectrum(filtercube.dichroicFilter.getSpectrum(), dlambda)
    spectra = [dem, fem, fdiem]
    
    lowerLimit = max( [min(spectrum[:,0]) for spectrum in spectra] )
    upperLimit = min( [max(spectrum[:,0]) for spectrum in spectra] )
    
    trimmedSpectra = [spectrum[(spectrum[:,0] >= lowerLimit) & (spectrum[:,0] <= upperLimit)] for spectrum in spectra]

    ovrlp = np.ones((trimmedSpectra[0][:,1].shape))
    
    for spectrum in trimmedSpectra:
        ovrlp = np.multiply(ovrlp, spectrum[:,1])
    
    hfig = plt.figure()
    plt.title(filtercube.channel + ' ' + title)
    plt.ylabel('Transmission (or normalised emission)')
    plt.xlabel('Wavelength, nm')
    utils.displaySpectra(trimmedSpectra)
    plt.fill_between(trimmedSpectra[0][:,0], ovrlp)
    plt.legend(['Dye emission', 'Emission filter', 'Dichroic filter'], loc='upper right')
        
    return np.sum(ovrlp)*dlambda",0.4888672233,
1973,split up train and test sets,"def split_train_test():
    # TODO: complete this function
        
    X_train = data[:,:] # currently, this selects all rows and all columns
    X_test  =
    
    y_train = target[:] # currently, this selects all rows
    y_test  = 
    
    return X_train, y_train, X_test, y_test",0.4979840517,
1973,split up train and test sets,"def load_data():
    from sklearn.model_selection import train_test_split
    
    X = np.genfromtxt('mush_features.csv')
    Y = np.genfromtxt('mush_labels.csv')
    
    train_set_x, test_set_x, train_set_y, test_set_y = train_test_split(X, Y, test_size=0.33, random_state=42)
    
    train_set_x = train_set_x[:300].astype(float)
    train_set_y = train_set_y[:300].astype(float)
    
    test_set_x = test_set_x[:100].astype(float)
    test_set_y = test_set_y[:100].astype(float)
    
    x_test = train_set_x[:5]
    y_test = train_set_y[:5]   
    
    train_set_x = train_set_x.reshape(train_set_x.shape[0], -1).T
    test_set_x = test_set_x.reshape(test_set_x.shape[0], -1).T
    
    train_set_y = train_set_y.reshape((1, train_set_y.shape[0]))
    test_set_y = test_set_y.reshape((1, test_set_y.shape[0]))
    
    x_test = x_test.reshape(x_test.shape[0], -1).T
    y_test = y_test.reshape((1, y_test.shape[0]))
    
    return train_set_x, test_set_x, train_set_y, test_set_y, x_test, y_test",0.4932080507,
1973,split up train and test sets,"# Loading the data

def load_data():
    from sklearn.datasets import load_boston
    from sklearn.model_selection import train_test_split
    
    boston = load_boston()
    
    train_set_x, test_set_x, train_set_y, test_set_y = train_test_split(boston.data, boston.target, test_size=0.33, random_state=42)

    train_set_y = train_set_y.reshape((1, train_set_y.shape[0]))
    test_set_y = test_set_y.reshape((1, test_set_y.shape[0]))
    
    return train_set_x.T, train_set_y, test_set_x.T, test_set_y, boston

train_set_x, train_set_y, test_set_x, test_set_y, visualization_set = load_data()",0.4908441007,
1973,split up train and test sets,"def get_data():
    """"""
    Get some data, either pretend user data or the Iris dataset
    ------
    """"""
    # An oversimplified dataset for easy visualization while learning
#     headers = ['referral','country','faq','age','service']
#     my_data=[['slashdot','USA','yes',18,'None'],
#         ['google','France','yes',17,'Premium'],
#         ['google','France','yes',20,'Premium'],
#         ['reddit','USA','yes',24,'Basic'],
#         ['wiki','France','yes',23,'Basic'],
#         ['google','UK','no',21,'Premium'],
#         ['(direct)','New Zealand','no',12,'None'],
#         ['(direct)','UK','no',21,'Basic'],
#         ['google','USA','no',24,'Premium'],
#         ['slashdot','France','yes',19,'None'],
#         ['slashdot','UK','yes',31,'Basic'],
#         ['reddit','USA','no',18,'None'],
#         ['google','UK','no',18,'None'],
#         ['wiki','UK','no',19,'None'],
#         ['reddit','New Zealand','yes',12,'Basic'],
#         ['slashdot','UK','no',21,'None'],
#         ['google','UK','yes',18,'Basic'],
#         ['wiki','France','yes',19,'Basic']]
#     my_data = np.array(my_data)
#     X = my_data.T[:4]
#     y = my_data.T[-1]
#     return X.T,y.T
    # The Iris data set from SKLearn 
    # (http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html)
    from sklearn.datasets import load_iris
    iris = load_iris()
    return iris.data, iris.target",0.4882551432,
1973,split up train and test sets,"def split_training_validation(xt, yt, ratio):
    """"""
    Split original Test set into actual traing set and validation set.
    Returns: new X_train, y_train, X_validation and y_validation
    """"""
    from sklearn.model_selection import train_test_split

    # X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.2, random_state=5, stratify=y_train)
    xt, xv, yt, yv = train_test_split(xt, yt, test_size=ratio, random_state=5)
    
    return xt, xv, yt, yv
    

# Split input training data into training and validation set of 4 to 1 ratio
X_train, X_validation, y_train, y_validation = split_training_validation(X_train, y_train, 0.2)

print()
print(""New Training Set:   {} samples"".format(len(X_train)))
print(""New Validation Set: {} samples"".format(len(X_validation)))
print(""Test Set:           {} samples"".format(len(X_test)))",0.4806761444,
1973,split up train and test sets,"def load_data():
    from sklearn.model_selection import train_test_split
    from sklearn import datasets
    
    iris = datasets.load_iris()
    
    train_set_x, test_set_x, train_set_y, test_set_y = train_test_split(iris.data, iris.target, test_size=0.33, random_state=42)
    
    return train_set_x, test_set_x, train_set_y, test_set_y, iris

train_set_x, test_set_x, train_set_y, test_set_y, visualization_set = load_data()",0.4773982167,
1973,split up train and test sets,"# Loading the data

def load_data():
    from sklearn.model_selection import train_test_split

    data = np.genfromtxt('kangaroo.csv', delimiter=',')
    
    x = data[:, 0]
    y = data[:, 1]
    
    train_set_x, test_set_x, train_set_y, test_set_y = train_test_split(x, y, test_size=0.33, random_state=42)
        
    return train_set_x, test_set_x, train_set_y, test_set_y

train_set_x, test_set_x, train_set_y, test_set_y = load_data()",0.4770382941,
1973,split up train and test sets,"def split_data(X, y):
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)
    
    return X_train, X_test, y_train, y_test",0.4765693545,
1973,split up train and test sets,"# Loading the data

def load_data():
    from sklearn.model_selection import train_test_split

    data = np.genfromtxt('time_temp_2016.tsv', delimiter='\t')
    
    x = data[:, 0]
    x = x.reshape((x.shape[0], 1))
    y = data[:, 1]
    
    train_set_x, test_set_x, train_set_y, test_set_y = train_test_split(x, y, test_size=0.33, random_state=42)
    
    train_set_y = train_set_y.reshape((1, train_set_y.shape[0]))
    test_set_y = test_set_y.reshape((1, test_set_y.shape[0]))
    
    return train_set_x.T, test_set_x.T, train_set_y, test_set_y, x.T

train_set_x, test_set_x, train_set_y, test_set_y, full_feature_set_for_plot = load_data()",0.4742547572,
1973,split up train and test sets,"def approx_silhouette(df, cluster_numbers):
    from sklearn.model_selection import train_test_split
    (Xuse, Xignore, Yuse, Yignore) = train_test_split(df, cluster_numbers, test_size=0.9)
    return sklearn.metrics.silhouette_score(Xuse, Yuse)

approx_silhouette(locations[['Longitude', 'Latitude']], clusters)",0.4740573168,
677,floating point numbers float,float(5) + 5.5,0.6114766598,
677,floating point numbers float,float(4.567),0.5971871018,
677,floating point numbers float,"#answer
def get_decimal(in_val):
    new_val = str(float(in_val)) #cast to float first, just in case it wasn't already, then to string
    deci = new_val.split(""."")[-1] #split on the decimal
    return float('0.'+deci) #add a '0.' to the front of the string, cast as float, then return

get_decimal(3.14159265359)",0.5962182283,
677,floating point numbers float,100 * .5,0.5926870704,
677,floating point numbers float,0.002 * 5,0.5907660127,
677,floating point numbers float,float(3.14),0.590713501,
677,floating point numbers float,float(4.5556),0.5902527571,
677,floating point numbers float,"def radians(img):
    return img.toFloat().multiply(math.pi).divide(180)",0.58893013,
677,floating point numbers float,0.5 * 0.5 * 0.5 * 0.5,0.5888421535,
677,floating point numbers float,"def convert_votes(string):
    if not isinstance(string, float):
        string = string.replace(',', '')
        return float(string)

omdb_df['imdb_votes'] = omdb_df_orig['imdb_votes'].apply(convert_votes)
omdb_df['imdb_votes'].sample(5)",0.5883996487,
1426,part introduction to numpy,"Sigma_inv = np.zeros(M.shape[::-1])
reverted_diagonal = [0 if i == 0 else 1/i for i in np.sqrt(sorted(eigenvalues_v, reverse=True))]
np.fill_diagonal(Sigma_inv, reverted_diagonal)
Sigma_inv",0.5104863048,
1426,part introduction to numpy,"create_convolution_animation(np.sinc(np.linspace(-2 * np.pi, 2 * np.pi, 42)[1:-1]), np.array([0, -1] + [0] * 16 + [1, 0]), normalize=True)",0.5091317892,
1426,part introduction to numpy,"h = np.zeros((2, A),int)
h[0, :-1] = np.arange(1, A)",0.5043575764,
1426,part introduction to numpy,"M_G = np.array([
        [0, 1/3, 1/3, 1/3, 0],
        [1/2, 0, 0, 1/2, 0],
        [0, 0, 0, 0, 1],
        [0, 1/2, 1/2, 0, 0],
        [0, 0, 0, 0, 0]
    ]).T
M_G",0.5010658503,
1426,part introduction to numpy,"# Shuffle data to randomize the entries
np.random.shuffle(data)

# Get all data except the last row as the training data
X = data[:,:-1].T

# Make sure that Y dtype is int64 to not run into errors with one_hot_encoding
Y = np.array(data[:,-1].T.reshape(1,data.shape[0]), dtype=np.int64)
print(X.shape)
print(Y.shape)",0.5006343126,
1426,part introduction to numpy,"a = np.floor(10 * np.random.random((8, 3)))
print(a)
print(""--------------"")
print(np.vsplit(a, (3, 4, 7))[0])                      # VERTICALLY SPLIT
print(""--------------"")
print(np.vsplit(a, (3, 4, 7))[1])                      # ON PARTICULAR POSITIONS
print(""--------------"")
print(np.vsplit(a, (3, 4, 7))[2])                      # VERTICALLY
print(""--------------"")
print(np.vsplit(a, (3, 4, 7))[3])",0.5006016493,
1426,part introduction to numpy,"from numpy.random import random_integers
nsample = 30

xsamp = random_integers(0, img.shape[0] - 2, nsample)
ysamp = random_integers(0, img.shape[1] - 2, nsample)

vals = []
for i in range(nsample):
    vals.append(len(
            new_df[
                (new_df[""x""] >= XX[xsamp[i]][ysamp[i]]) & (new_df[""x""] < XX[xsamp[i] + 1][ysamp[i] + 1]) & \
                (new_df[""y""] >= YY[xsamp[i]][ysamp[i]]) & (new_df[""y""] < YY[xsamp[i] + 1][ysamp[i] + 1])
            ]
        ))",0.5004893541,
1426,part introduction to numpy,"# build the GHZ state
k = np.zeros((1, d**chi))
k[:,0] = np.sqrt(1/2.)
k[:,-1] = np.sqrt(1/2.)

# feed GHZ state through model (try to reconstruct it from predicted MPS coefficients)
feed = {X: k}
k_hat = y_hat.eval(feed) # get reconstructed state

# compare the actual state with the reconstructed state
print k.ravel()
print k_hat.ravel()

# plot the vectors so it's easier to visualize
plt.figure(0) ; plt.imshow(k, interpolation='none') ; plt.title('true $\psi$')
plt.figure(1) ; plt.imshow(np.stack([k_hat]), interpolation='none') ; plt.title('MPS autoencoded $\psi$')
plt.show()",0.4995365739,
1426,part introduction to numpy,"matrx_c=np.arange(1,17,1,dtype=np.int).reshape(4,4)
    # [[ 1  2  3  4]
    #  [ 5  6  7  8]
    #  [ 9 10 11 12]
    #  [13 14 15 16]]

np.vsplit(matrx_c,2)[0][0,0:3]
m1=np.array([[1,2],[1,2]])
m2=np.array([[2,3],[2,3]])
print(m1, "":"", m2)
np.dot(m1,m2)",0.4981798828,
1426,part introduction to numpy,"theta_test = np.array([-2, -1, 1, 2]).reshape(-1, 1)

a = np.ones(5).reshape(-1, 1)
b = np.arange(1, 16).reshape(3, 5).transpose() / 10
X_test = np.concatenate((a, b), axis=1)
y_test = np.array([1, 0, 1, 0, 1]).reshape(-1, 1)
display(X_test)
y_test",0.4979829192,
2055,step create the dataframes based on the followin raw data,"class Line:
    def __init__(self):
        # The first frame of video has been processed
        self.first_frame_processed = True          
        
        #  The value of left and right line
        self.left_fit = [np.array([False])] 
        self.right_fit = [np.array([False])] 
        #  The deque to store last frame n fits
        self.recent_left_fit = deque([],maxlen=10)
        self.recent_right_fit = deque([],maxlen=10)
        
        # The needed parameters which defined by hotcode
        self.y_eval = 719
        self.midx = 640
        self.ym_per_pix = 30.0/720.0 # meters per pixel in y dimension
        self.xm_per_pix = 3.7/700.0 # meters per pixel in x dimension
        self.curvature = 0
        
       
    def add_fit_left(self, left_fit):
        """"""
        Add the left fit to the deque
        """""" 
        if (len(self.recent_left_fit))<=10:
            self.recent_left_fit.append(left_fit)
        else:
            self.recent_left_fit.leftpop()
            self.recent_left_fit.append(left_fit)
                
    def avg_fit_left(self):
        """"""
        Average left fits 
        """""" 
        fit = np.array(self.recent_left_fit)
        avg_previous_left = fit.mean(axis = 0)
        return avg_previous_left
        
    def add_fit_right(self, right_fit):
        """"""
        Add the right fit to the deque
        """"""         
        if (len(self.recent_right_fit))<=10:
            self.recent_right_fit.append(right_fit)
        else:
            self.recent_right_fit.leftpop()
            self.recent_right_fit.append(right_fit)

    def avg_fit_right(self):
        """"""
        Average right fits 
        """"""         
        fit = np.array(self.recent_right_fit)
        avg_previous_right = fit.mean(axis = 0)
        return avg_previous_right            
        
    
    def update_bilateral_fits(self, left_fit, right_fit):
        """"""
        Update cofficient of bilateral fitting polynomial
        """""" 
        
        if self.first_frame_processed:   
            # The first frame set the values directly
            self.right_fit = right_fit
            self.left_fit = left_fit
            self.recent_left_fit.append(left_fit)
            self.recent_right_fit.append(right_fit)            
        else:
            # Sanity check by MSE 
            # Now, the self value is the previous frame values, then the left_fit and right_fit are current frame values
            left_error = ((self.left_fit[0] - left_fit[0]) ** 2).mean(axis=None)      
            right_error = ((self.right_fit[0] - right_fit[0]) ** 2).mean(axis=None)   
            
            # If the polynomial parameter succeed, adding it to deque and set average value
            # Othervise, using the average current  polynomial values to assign to the self 
            if left_error < 0.01:
                self.add_fit_left(left_fit)
                self.left_fit = self.avg_fit_left()
            else:
                self.left_fit = left_fit
            
            if right_error < 0.01:
                self.add_fit_right(right_fit)
                self.right_fit = self.avg_fit_right()
            else:
                self.right_fit = right_fit       
        
        # update curvature
        self.update_curvature(self.right_fit, self.left_fit)
     
     
    def update_curvature(self, left_fit, right_fit):
        """"""
        Update radius of curvature 
        """"""
        #  Calculate the curvature
        y_eval = self.y_eval
        xm_per_pix = self.xm_per_pix
        ym_per_pix = self.ym_per_pix
        # Calculate the new radii of curvature    
        left_y1 = (2*left_fit[0]*y_eval + left_fit[1])*xm_per_pix/ym_per_pix
        left_y2 = 2*left_fit[0]*xm_per_pix/(ym_per_pix*ym_per_pix)
        left_curverad = ((1 + left_y1*left_y1)**(1.5))/np.absolute(left_y2)
    
        right_y1 = (2*right_fit[0]*y_eval + right_fit[1])*xm_per_pix/ym_per_pix
        right_y2 = 2*right_fit[0]*xm_per_pix/(ym_per_pix*ym_per_pix)
        right_curverad = ((1 + right_y1*right_y1)**(1.5))/np.absolute(right_y2)
        
        curvature = (left_curverad + right_curverad) / 2
        
        self.curvature = curvature
        

    def get_position_from_center(self):
        """"""
        Get position from the center 
        """"""        
        #  Calculate the curvature position from center
        x_left_pix = self.left_fit[0]*(self.y_eval**2) + self.left_fit[1]*self.y_eval + self.left_fit[2]
        x_right_pix = self.right_fit[0]*(self.y_eval**2) + self.right_fit[1]*self.y_eval + self.right_fit[2]
        
        return ((x_left_pix + x_right_pix)/2.0 - self.midx) * self.xm_per_pix",0.496643126,
2055,step create the dataframes based on the followin raw data,"def number_of_specific_attribs(way_or_node, regex):
    '''Returns the attributes specified through the input and how often they occur
    
    Takes as input a primary XML tag that holds tags in this dataset (""way"" or ""node"")
    and a regular expression to match the desired attributes of these tags
    Returns a pandas Series object mapping the attributes to the amount of their occurence.
    '''
    import re
    import pandas as pd
    regex_keys = {}
    for event, elem in ET.iterparse(las_vegas_osm, events=('start',)):
        if elem.tag == way_or_node:
            for tag in elem.iter('tag'):
                if re.search(regex, tag.attrib['k']):
                    if tag.attrib['k'] not in regex_keys:
                        regex_keys[tag.attrib['k']] = 1
                    else:
                        regex_keys[tag.attrib['k']] += 1
    regex_series = pd.Series(regex_keys, name='tag attributes for ""%s"" matching ""%s""' %(way_or_node, regex.pattern))
    return regex_series",0.4936335087,
2055,step create the dataframes based on the followin raw data,"def train_classifier(dataset):
    '''
    DataFrame - > Scikit Classifier Object
    
    Takes the generated dataset and trains a logistic regression classifier on it.
    '''
    X_train = dataset.drop(['Label'], 1)
    Y_train = dataset['Label']

    logistic_regression = LogisticRegression()
    params = {'C': np.logspace(start=-5, stop=3, num=9)}
    clf = GridSearchCV(logistic_regression, params, scoring='neg_log_loss', refit=True)
    #clf = RandomForestClassifier(n_jobs=7)
    model = clf.fit(X_train, Y_train)
    return model

clf = train_classifier(dataset)",0.4874275327,
2055,step create the dataframes based on the followin raw data,"def find_bestselling_directors(year, df):
    df1 = df
    df1['director'].dropna(inplace=True)  # I chose to drop NaNs in this situation because the amount of NaNs is insignificant
    df_temp = df1[df1['release_year'] == year]
    max_revenue = df_temp['revenue'].max()
    name = df_temp[df_temp['revenue'] == max_revenue].iloc[0]['director']
    if '|' in name:
        name = name.replace('|', ' and ')
        print('The best-selling directors for {0} are {1} with a revenue of {2}'.format(year, name, max_revenue))
    else:
        print('The best-selling director for {0} is {1} with a revenue of {2}'.format(year, name, max_revenue))",0.4854903817,
2055,step create the dataframes based on the followin raw data,"# create the submission file and store the results
def saveResults(predictions, filename):
    submission = pd.DataFrame.from_dict({'id': test['id']})
    for idx, col in enumerate(label_cols):
        submission[col] = predictions[:,idx]
    submission.to_csv(filename, index=False)",0.4854692221,
2055,step create the dataframes based on the followin raw data,"def predict_class(classifier, data_subset):
    """"""
    Predict class of data, and append predictions to data frame
    """"""
    try:
        # drop old predictions before reclassifying (if they exist)
        data_subset = data_subset.drop('predictions', axis = 1)
        data_to_classify = data_subset.copy()
    except:
        data_to_classify = data_subset.copy()
        pass
    
    # convert to numpy and classify
    columns = list(data_to_classify.columns)
    columns.remove('slave_logs')
    data_matrix = data_to_classify.as_matrix(columns)
    predictions = classifier.predict(data_matrix[::,1::])
    
    # revalue slave_log ID column to indicate classification
    data_to_classify['slave_logs'] = predictions + 4
    
    # print statstics
    counts = collections.Counter(predictions)
    
    for key in counts:
        percent = (counts[key]/(len(predictions))* 100)
        print(round(percent, 2), 'of data was classified as ', key)
        
    # update slave_log columns

    return data_to_classify",0.4848165512,
2055,step create the dataframes based on the followin raw data,"# Run the model with new parameter set. Change the parameter set in the input map
def change_model_parameters(imap, param_df):
    """"""
    the intent of this function is to replace the values in imap (your current configuration) 
    by values in a new parameter dataframe (param_df)
    """"""
    
    import pandas as pd
    
    variable_names = param_df['MemberName']
    variable_value = param_df['value']
      
    for k,v in zip(variable_names, variable_value):
        imap[k.lower()].args['value'] = v
        
change_model_parameters(input_map, new_params)",0.4819937348,
2055,step create the dataframes based on the followin raw data,"# reads data
    list_of_dataframes = []
    for file in data_paths:
        # read file
        df = pd.read_table(file, skiprows=header_size, delim_whitespace=True)

        # naming columns
        df.columns = ['theta', 'phi', 'Year', 'Month', 'Time', 'r', 'Br', 'Bt', 'Bp', 'N_{data}']

        list_of_dataframes.append(df)

    # concatenate dataframes into one
    data = pd.concat(list_of_dataframes, axis=0, ignore_index=True)",0.4812532067,
2055,step create the dataframes based on the followin raw data,"def average_revenue(categories, df):
    result = {}
    
    # I chose to fill all the NaNs of column genre because the NaNs don't count towards the final conclusion.
    df1 = df
    df1['genres'].dropna(inplace=True)
    
    for category in categories:
        df_temp = df1[df1['genres'].str.contains(category)]
        total = float(df_temp['revenue'].sum())
        if total == 0.0:
          result[category] = 0.0
        else:
          count = len(df_temp.index)
          result[category] = total / count
            
    return result",0.4780743718,
2055,step create the dataframes based on the followin raw data,"def get_xval_preds(model):
    xval_pred = None
    for xval_pred_info in model._model_json['output']['cross_validation_predictions']:
        single_pred = h2o.get_frame(xval_pred_info['name'])
        if xval_pred is None:
            xval_pred = single_pred[""predict""]
        else:
            xval_pred[""predict""] = xval_pred[""predict""] + single_pred[""predict""]
            
    return xval_pred

def sensor_preds(frame):
    frame[""predict""] = h2o.ifelse(frame[""predict""] < 0., 0., frame[""predict""])
    return frame

def append_score(target_frame):
    target_frame[""diff""] = target_frame[""predict""] - target_frame[""RemainingUsefulLife""]
    target_frame[""score""] = target_frame.apply(lambda r: h2o.ifelse(r['diff'] < 0,r['diff']/(-10.),r['diff']/13.), axis=1)
    target_frame[""score""] = target_frame[""score""].expm1()
    target_frame = target_frame.drop(""diff"")
    return target_frame

def get_median_score(target_frame):
    return target_frame[""score""].mean()

def get_val_preds_with_info(model):
    xval_pred = get_xval_preds(model)
    xval_pred = sensor_preds(xval_pred)
    xval_pred = xval_pred.cbind(train_with_predictor[""UnitNumber""])
    xval_pred = xval_pred.cbind(train_with_predictor[""Cycle""])
    xval_pred = xval_pred.cbind(train_with_predictor[""RemainingUsefulLife""])
    xval_pred = append_score(xval_pred)
    return xval_pred

def get_models_preds_scores(models):
    result = []
    for model in models:
        xval_pred = get_val_preds_with_info(model)
        median = get_median_score(xval_pred)
        result.append((model, xval_pred, median))
    return result

def extract_best_model(models):
    bestError = models[0][2]
    result = models[0]
    for model in models:
        if model[2] < bestError:
            bestError = model[2]
            result = model
    return result",0.4775744677,
1693,python data type list exercises,"other_set = set('this is my string')
pprint(other_set)
upper_case = [x.upper() for x in other_set]
pprint(uppser_case)",0.4885393381,
1693,python data type list exercises,"chr(65), chr(122)",0.4866425991,
1693,python data type list exercises,"stemmer = SnowballStemmer('english')

# Define a function that accepts text and returns a list of stems.
def split_into_stems(text):
    text = str(text).lower()
    words = TextBlob(text).words
    return [stemmer.stem(word) for word in words]",0.4858158827,
1693,python data type list exercises,[line.upper() for line in open('data/lorem-ipsum')],0.4845643044,
1693,python data type list exercises,"print(type('asdf'), type(True), type(42))",0.4831805825,
1693,python data type list exercises,"solution = [expr(""Eat(Cake)""),
            expr(""Bake(Cake)"")]

for action in solution:
    cakeProblem.act(action)",0.4828329086,
1693,python data type list exercises,"solution = [expr(""Bake(cake)""),
            expr(""Eat(cake)"")]

for action in solution:
    have_cake_and_eat_cake_too.act(action)",0.4828329086,
1693,python data type list exercises,"solution = [expr(""MoveToTable(C, A)""),
            expr(""Move(B, Table, C)""),
            expr(""Move(A, Table, B)"")]

for action in solution:
    three_block_tower.act(action)",0.4828148782,
1693,python data type list exercises,"solution = [expr(""MoveToTable(C, A)""),
            expr(""Move(B, Table, C)""),
            expr(""Move(A, Table, B)"")]

for action in solution:
    threeBlockTower.act(action)",0.4828148782,
1693,python data type list exercises,"solution = [expr(""Remove(Flat, Axle)""),
            expr(""Remove(Spare, Trunk)""),
            expr(""PutOn(Spare, Axle)"")]

for action in solution:
    spareTire.act(action)",0.4828148782,
2301,tensor tests,"data = h.simulate_data(200, 5, 4)

# This will check that your dataset appears to match ours before moving forward
t.test_question_1(data)",0.5148794055,
2301,tensor tests,"data = h.simulate_data(200, 5, 4)# Create a dataset with 200 points, 5 features and 4 centers

# This will check that your dataset appears to match ours before moving forward
t.test_question_1(data)",0.5148794055,
2301,tensor tests,df = ip.test_data(20),0.5081982017,
2301,tensor tests,"class TestSensor(unittest.TestCase):
    
    def test_class(self): 
        mock_bytes = (1024).to_bytes(2, byteorder='big')
        count = 4096
        with mock.patch('socket.socket') as mock_socket:
            mock_socket.return_value.recv.return_value = ""test_return""
            t = TCPSocket()
            t.connect('example.com', 12345)  # t.sock is a mock object, not a Socket
            print(t.recv_some())
            
            self.assertEqual(t.recv_some(), ""test_return"")
            t.sock.connect.assert_called_with(('example.com', 12345))
            
unittest.main(argv=['ignored', '-v'], exit=False)",0.5071867704,
2301,tensor tests,"# Experiment w/ different layers here.  You'll need to change this if you 
# use another network!
content_layer = 'net/conv3_2/conv3_2:0'

with tf.Session(graph=g) as sess, g.device('/cpu:0'):
    content_features = g.get_tensor_by_name(content_layer).eval(
            session=sess,
            feed_dict={x: content_img,
                'net/dropout_1/random_uniform:0': [[1.0]],
                'net/dropout/random_uniform:0': [[1.0]]})",0.5058117509,
2301,tensor tests,"content_layer = 'net/conv3_2/conv3_2:0'

with tf.Session(graph=g) as sess, g.device('/cpu:0'):
    content_features = g.get_tensor_by_name(content_layer).eval(
            session=sess,
            feed_dict={x: content_img,
                    # We need to avoid using dropout to get the content features, thus:
                    'net/dropout_1/random_uniform:0': np.ones([1,g.get_tensor_by_name('net/dropout_1/random_uniform:0').get_shape().as_list()[1]]),
                    'net/dropout/random_uniform:0': np.ones([1,g.get_tensor_by_name('net/dropout/random_uniform:0').get_shape().as_list()[1]])
                      })",0.5058117509,
2301,tensor tests,x = torch.Tensor([3]).requires_grad_(True),0.5023815632,
2301,tensor tests,fmnist.train.num_examples,0.5010462999,
2301,tensor tests,"# Train
mnist.train.num_examples",0.5010462999,
2301,tensor tests,mnist.train.num_examples,0.5010462999,
2251,supervised learning support vector machine,"%%time
from sklearn.svm import LinearSVC

clf_linsvm, _, _, _, _ = do_classify(LinearSVC(loss=""hinge""), {""C"": [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0, 50.0, 100.0]}, 
                                       indf=dftouse, featurenames=lcols, targetname='winner', 
                                       target1val=1, reuse_split=reuse_split, score_func='f1')",0.4794105291,
2251,supervised learning support vector machine,"%%time
from sklearn.svm import LinearSVC

clf_linsvmBalanced, _, _, _, _ = do_classify(LinearSVC(loss=""hinge""), {""C"": [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0, 50.0, 100.0]}, 
                                       indf=dftouse, featurenames=lcols, targetname='winner', 
                                       target1val=1, reuse_split=reuse_split_new, score_func='f1')",0.4794105291,
2251,supervised learning support vector machine,"run_simulation(1.0, model=m3, y0={'A': 60}, structures={'S': sphere},
               solver='spatiocyte', observers=(obs1, obs2), return_type=None)",0.4743404686,
2251,supervised learning support vector machine,"if __name__ == '__main__':
    res = []
    layers = 8
    for n_la in range(0,layers):
        accuracy = test_mlp(learning_rate=0.01, L1_reg=0.00, L2_reg=0.001, n_epochs=1000,
             batch_size=20, n_hidden=50, verbose=False, n_layers = n_la)
        res.append(accuracy)
    print(res)",0.473990202,
2251,supervised learning support vector machine,"if doGridSearch:
    gbmModels = gridSearchGBM(xCols, yCol, foldCol, train_hex,\
                              learning_rates=[0.03,0.01,0.003], ntrees=[100,300,1000,3000], max_depth=[1,2,3,5])
else:
    gbmModels = [trainGBM(xCols, yCol, foldCol, train_hex, \
                        ntrees=300, max_depth=5)]",0.4728274047,
2251,supervised learning support vector machine,"if __name__ == '__main__':
    res = []
    layers = 8
    for n_la in range(4,layers):
        accuracy = test_mlp(learning_rate=0.01, L1_reg=0.00, L2_reg=0.001, n_epochs=1000,
             batch_size=20, n_hidden=50, verbose=False, n_layers = n_la)
        res.append(accuracy)
    print(res)",0.471850723,
2251,supervised learning support vector machine,"# Try Linear SVC
from sklearn.svm import LinearSVC
clfsvm, X_train_svm, y_train_svm, X_test_svm, y_test_svm = do_classify(LinearSVC(loss=""hinge""), {""C"": [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]}, full_df_copy_w_response,all_variable_names,'response',True)",0.4711188078,
2251,supervised learning support vector machine,"from sklearn.svm import LinearSVR
estimator = LinearSVR(loss=""squared_epsilon_insensitive"",
            dual=False,
            random_state=5)
tuned_parameters = {'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100], 
                    'epsilon': [0.0001, 0.001, 0.01, 0.1, 1, 2, 5]}

gridsearch = GridSearchCV(estimator,
                        tuned_parameters,
                        scoring=""r2"",
                        n_jobs=-1,
                        cv=7,
                        verbose=0)
gridsearch.fit(X_scaled1, y_train)
_hyper_C = gridsearch.best_params_
_hyper_epsilon = gridsearch.best_params_
_best_clf_score = gridsearch.best_score_

_clf = best_clf = gridsearch.best_estimator_
_coef = best_clf.coef_
_bias = -best_clf.intercept_[0]
_L1 = np.linalg.norm(_coef, ord=1)
prediction = best_clf.predict(X_test_scaled1)
_loss = np.sum(np.abs(y_test - prediction))
print (_clf)
print ('\n')
print (_coef)
print ('\n')
print (_bias)
print ('\n')
print (_L1)
print ('\n')
print (_loss)
print (_best_clf_score)",0.4685923457,
2251,supervised learning support vector machine,"class perceptron(object):
    def __init__(self, input_size, output_size, hl_neurons, hl_layers, learningrate):
        self.learningrate = learningrate
        self.input_size = input_size
        self.output_size = output_size
        self.hl_neurons = hl_neurons
        self.hl_layers = hl_layers
        
        # Now setup the weights in this format (inputs x neurons)
        self.weights = []
        self.ths = []
        self.weights.append(self.get_weights((hl_neurons, input_size)))
        self.ths.append(self.get_weights((hl_neurons, 1)))
        for i in xrange(1, hl_layers):
            self.weights.append(self.get_weights((hl_neurons, hl_neurons)))
            self.ths.append(self.get_weights((hl_neurons, 1)))
        self.weights.append(self.get_weights((output_size, hl_neurons)))
        self.ths.append(self.get_weights((output_size, 1)))
        self.delta_func = self.delta_func1
        self.sigmoid = np.vectorize(self.sigmoid)
        self.sigmoid_delta = np.vectorize(self.sigmoid_delta)
        
    def get_weights(self, dimensions):
        return np.random.normal(0,1,dimensions)
        
    def delta_func1(self, output, truths):
        return (output-truths)
    
    def sigmoid(self, z):
        return 1/(1+np.exp(-z))
    
    def sigmoid_delta(self, z):
        return (np.exp(-z))/((1+np.exp(-z))**2)
    
    def push_forward(self, inputs):
        A = []
        Z = []
        output = inputs
        for i in xrange(0, len(self.weights)):
#             print ""performing %f * %f + %f"" % (np.max(self.weights[i]), np.max(output), np.max(self.ths[i]))
            normno = len(output)
            output = np.dot(self.weights[i], output)
            output /= normno
#             print ""Dividing by %d neurons"" % normno
            output += self.ths[i]
            Z.append(output)
            output = self.sigmoid(output)
            A.append(output)
        return A, Z, output
    
    def feed_forward(self, inputs):
        print ""input = %s"" % (inputs)
        _, _, output = self.push_forward(inputs)
        return output
    
    def multiply_and_average(self, delta, A):
        At = np.transpose(A)
        # Multiply the two, see if they're compatible (faster than throwing an exception)
        _ = np.dot(delta,At)
        ws = []
        for i in xrange(0, len(delta[0])):
            di = delta[:,[i]]
            Ai = At[[i],:]
            ws.append(np.dot(di, Ai))
        return np.mean(ws,axis=0)
    
    def train(self, inputs, truths):
        # First get the error and delta for output layer
        A, Z, output = self.push_forward(inputs)
#         curdelta = np.multiply((output - truths),self.sigmoid_delta(Z[len(Z)-1]))
        curdelta = np.multiply(self.delta_func(output, truths),self.sigmoid_delta(Z[len(Z)-1]))
        deltas = [curdelta]
        for i in reversed(xrange(0, len(self.weights)-1)):
            curdelta = np.multiply(np.dot(np.transpose(self.weights[i+1]), curdelta),
                                   self.sigmoid_delta(Z[i]))
            deltas.insert(0, curdelta)        
        A.insert(0,inputs)
        for i in reversed(xrange(0, len(self.weights))):
            self.weights[i] -= (self.learningrate * np.dot(deltas[i], A[i].T))
            tmp = np.mean(deltas[i],axis=1)
            tmp = np.matrix(tmp)
            tmp = tmp.T
            try:
                self.ths[i] -= (self.learningrate * tmp)
            except ValueError:
#                 print ""tmp - %s"" % tmp
#                 print ""ths - %s"" % self.ths[i]
                self.ths[i] -= (self.learningrate * tmp.T)
            
        return np.mean(abs(truths - output))",0.4671483934,
2251,supervised learning support vector machine,"from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import LinearSVC

def draw_svm_roc(x, y, x_test, y_test, classifier_title):
    classifier = OneVsRestClassifier(LinearSVC(random_state=0))
    svm_y_score = classifier.fit(x, y).decision_function(x_test)
    draw_roc(y_test, svm_y_score, classifier_title)",0.4646662772,
1820,review of the iris dataset,"import pandas as pd
    import seaborn as sns
    import matplotlib.pyplot as plt
    
    iris = pd.read_csv(""/home/subham/Downloads/DATA VISUALISATION ON IRIS DATASET/Iris.csv"")
    print (iris.shape)",0.4664183855,
1820,review of the iris dataset,"def que2():
    iris = datasets.load_iris() 
    X = iris.data
    target = iris.target 
    names = iris.target_names
    acc_rf = cross_val(RandomForestClassifier(n_estimators = 20), X, target, 5)
    acc_svm = cross_val(SVC(), X, target, 5)
    # skLearn function from cross validation accuracy score
    sk_learn_cross_val_rf = cross_val_score(RandomForestClassifier(n_estimators = 20), X, target, cv = 10, scoring = 'accuracy')
    sk_learn_cross_val_svc = cross_val_score(SVC(), X, target, cv = 10, scoring = 'accuracy')
    # Answer 2-b -START
    print(""Non aggregated accuracy of user defined cross validation function v/s sk learn cross valtion function"")
    print(""Accuraccy score of user defined cross val function"")
    print(acc_rf)
    
    print(""Accuraccy score of sk learn cross val function"")
    print(sk_learn_cross_val_rf)
    # Answer 2-b -FINISH
    
    # Answer 2-c - START
    print(""Comparing Accuracy score between Random Forest and SVM Classification :"")
    print(""Random Forest accuracy (Mean) :"")
    print(np.mean(sk_learn_cross_val_rf))
    print(""SVM Classification accuracy (Mean) :"")
    print(np.mean(sk_learn_cross_val_svc))
    # Answer 2-c FINISH

que2()",0.4505906105,
1820,review of the iris dataset,"def visuvalize_sepal_data():
    iris = datasets.load_iris()
    X = iris.data[:, :2]
    y = iris.target
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)
    plt.xlabel('Sepal length')
    plt.ylabel('Sepal width')
    plt.title('Sepal Width & Length')
    plt.show()",0.4421685338,
1820,review of the iris dataset,"def visuvalize_sepal_data():
    iris = datasets.load_iris()
    X = iris.data[:, :2]  # we only take the first two features.
    y = iris.target
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)
    plt.xlabel('Sepal length')
    plt.ylabel('Sepal width')
    plt.title('Sepal Width & Length')
    plt.show()
 
visuvalize_sepal_data()",0.4421685338,
1820,review of the iris dataset,"def visuvalize_petal_data():
    iris = datasets.load_iris()
    X = iris.data[:, 2:]  # we only take the last two features.
    y = iris.target
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)
    plt.xlabel('Petal length')
    plt.ylabel('Petal width')
    plt.title('Petal Width & Length')
    plt.show()
 
visuvalize_petal_data()",0.4401438832,
1820,review of the iris dataset,"def visuvalize_petal():
    iris = datasets.load_iris()
    X = iris.data[:, 2:]  # we only take the last two features.
    y = iris.target
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)
    plt.xlabel('Petal length')
    plt.ylabel('Petal width')
    plt.title('Petal Width & Length')
    plt.show()
 
visuvalize_petal()",0.4401438832,
1820,review of the iris dataset,"def visuvalization_petal_data():
    iris = datasets.load_iris()
    X = iris.data[:, 2:]
    y = iris.target
    plt.scatter(X[:, 0],X[:, 1],c=y,cmap=plt.cm.BrBG)
    plt.xlabel('Petal length')
    plt.ylabel('Petal width')
    plt.title('Petal Width & Length')
    plt.show()",0.4401438832,
1820,review of the iris dataset,"if IDifTask.config.doMeasurement:
    print ""Doing measurement of dipoles""
    if len(diaSources) < IDifTask.config.maxDiaSourcesToMeasure:
         IDifTask.dipoleMeasurement.run(subtractedExposure, diaSources)
    else:
         IDifTask.measurement.run(subtractedExposure, diaSources)",0.427134186,
1820,review of the iris dataset,"from arcgis.gis import GIS

def print_map_info(map):
    print(""Mode =\t\t{}"".format(map.mode))
    if map.mode == ""2D"":
        print(""Zoom =\t\t{}\n"".format(map.zoom) + \
              ""Rotation =\t{}"".format(map.rotation))
    elif map.mode == ""3D"":
        print(""Zoom =\t\t{}\n"".format(map.zoom) + \
              ""Tilt =\t\t{}\n"".format(map.tilt) + \
              ""Heading =\t{}"".format(map.heading))
    else:
        raise Exception(""Not supported argument"")

usa_map = GIS().map(""USA"")
usa_map.mode = ""3D""
usa_map",0.4252077937,
1820,review of the iris dataset,"def load_dataset():
    iris = datasets.load_iris()
    X = iris.data
    y = np_utils.to_categorical(iris.target) #i transformed the target (different integers for each flower class) into a one-hot vector 


    xy = np.hstack((X,y)) #stacking the features and classes vectors horizontally, so during the shuffle phase i'll not get lost
    np.random.shuffle(xy) #shuffling the lines

    #there are four iris features and 150 samples. I'm spliting the now suffled vector into 4. Training vectors with 105 samples and 45 samples to model validation.
    X_train, X_test = xy[:105,:4], xy[105:,:4] 
    y_train, y_test = xy[:105,4:], xy[105:,4:]

    return { 'data':{'train': X_train, 'test': X_test},
             'classes':{'train': y_train, 'test': y_test} }",0.4244734049,
716,functions,"def deadStoreElimination():
    global compiledProcedures
    instructions[curlev] = deadStoreEliminationFromBlock(instructions[curlev])",0.429423213,
716,functions,"import copy

def has_passed(special_add, tests):
    for test in tests:
        current_func = copy.deepcopy(test.function)
        current_func.func_globals['special_add'] = special_add
        try:
            current_func()
        except Exception as e:
            return False
    return True",0.4210967422,
716,functions,"def p2_5_2():
    return lambda x: 0.1*x**4-1.5*x**3+0.53*x**2+2*x+1",0.408513248,
716,functions,"def reverse(f):
    def g(a,b):
        return f(b,a)
    return g",0.4045718014,
716,functions,"def test14 () :
    class A :
        def __init__ (self, i, f) :
            self.i = i
            self.f = f

    z = A(2, 3.45)
    assert z       != A(2, 3.45)
    assert type(z) is A
    assert type(A) is type

    assert isinstance(z, A)
    assert isinstance(z, object)

    assert issubclass(A, object)

    assert type(type) is type
    
test14()",0.4038594961,
716,functions,"def outer():
    def inner(x, y):
        return x+y
    return inner

#outer is invoked when this cell is ran (or when a module is loaded)
#inner will not be invoked until outer() is called",0.4029667079,
716,functions,"def callnsize(nsize, func):
    x, y = zip(*((n, func(n)) for n in nsize))
    df = pd.DataFrame()
    df[""n""] = x
    df[""time""] = y
    df.plot.scatter(x=""n"", y=""time"", xlim=(0, max(x)), ylim=(0, max(y)))
    return df",0.3959315419,
716,functions,"def plot_cdfs(cdf3):
    thinkplot.PrePlot(2)
    thinkplot.Cdf(cdf1)
    thinkplot.Cdf(cdf2)
    thinkplot.Cdf(cdf3, color='orange')
    thinkplot.Config(xlabel='sentence (months)', ylabel='CDF', loc='lower right')",0.3918085396,
716,functions,"def validate_many(*validators):
    def checker(input):
        return all(validator(input) for validator in validators)
    return checker",0.3897699118,
716,functions,"def prep_data(df, prep_funcs, verbose=False) :
    for name,func in prep_funcs.items() :
        if verbose : print 'Applying function ' + name
        df = func(df)
        
    return df",0.3888691664,
1913,"shell, pipes, and csvkit","%%writefile scripts/install_docker.sh
#!/bin/bash

if [ """" != ""$(lsblk | grep -F '/mnt')"" ]; then
    if [ -d /var/lib/docker ]; then
        sudo mv /var/lib/docker /mnt/
    elif [ ! -d /mnt/docker ]; then
        sudo mkdir /mnt/docker
    fi

    sudo ln -s /mnt/docker /var/lib/docker
fi

sudo apt-get install -y docker docker.io git
sudo service docker start
sudo usermod -aG docker $USER",0.5126014352,
1913,"shell, pipes, and csvkit","## Compile using pystan
#stan_model_compiled = pystan.StanModel(model_code=stan_code)

### Compile using cmdstan
### Script expects cmdstan installation at cmdstan_path
subprocess.call(""mkdir ""+cmdstan_path+""user-models"", shell=1)
subprocess.call(""cp gp_model_final.stan "" + cmdstan_path+""user-models/"", shell=1)
subprocess.call(""make user-models/gp_model_final"", cwd=cmdstan_path, shell=1)",0.4917150438,
1913,"shell, pipes, and csvkit","[keyboard]i

export PATH=""/home/ec2-user/anaconda3/bin:$PATH""

[keyboard]esc
[keyboard]:wq",0.48237288,
1913,"shell, pipes, and csvkit","# install_v06.sh, Linux VM 1Gb RAM

if [[ `uname` == 'Linux' ]]; then
  sudo apt-get install g++

  # http://stackoverflow.com/questions/26575587/cant-install-scipy-through-pip
  sudo apt-get install libatlas-base-dev gfortran

  # http://www.reddit.com/r/linux4noobs/comments/32sova/trying_to_install_python_packages_and_getting_an/
  # will also install gcc!!
  sudo apt-get install liblapack-dev

  # http://matplotlib.1069221.n5.nabble.com/matplotlib-devel-1-4-3-does-not-build-on-Ubuntu-14-with-python3-td45301.html
  sudo apt-get install libfreetype6-dev
  sudo apt-get install pkg-config
fi
pip install -r requirements.txt",0.4822276831,
1913,"shell, pipes, and csvkit","process = subprocess.run('ls *.py', stdout=subprocess.PIPE, encoding='utf8', shell=True)",0.4801955819,
1913,"shell, pipes, and csvkit","p | df.Create(['hello', 'world']) | df.io.Write(df.io.TextFileSink('./test.txt'))",0.4795041084,
1913,"shell, pipes, and csvkit","grib_dir=""./""+folder+""/""+date+""_""+simu_starttime+""/grib2/""
if os.path.isdir(grib_dir+""/"") is False : 
    dtn.exec_command(date,simu_starttime,folder,graph_mode)
else :
    if len([name for name in os.listdir(grib_dir) if os.path.isfile(grib_dir+""/""+name)]) < 6 :
        #download
        dtn.exec_command(date,simu_starttime,folder,graph_mode)
    else : 
        print(""Skip download.. , ""+date+"" is already existed..."")",0.4778627753,
1913,"shell, pipes, and csvkit","from subprocess import Popen, PIPE
import os

def extrai_imagens(arquivo, nome='image', formato='png'):    
    fmt = '-{}'.format(formato)
    proc = Popen(['pdfimages', fmt, arquivo, nome])
    out, err = proc.communicate()
    print(err, out)
    return out

def extrai_texto(arquivo, txtout=None):
    cmd = ['pdftotext', arquivo]
    if txtout:
        cmd.append(txtout)
    proc = Popen(cmd)
    out, err = proc.communicate()
    return out

extrai_texto(""bt.pdf"", 'teste.txt')
extrai_imagens('bt.pdf', nome='im')",0.4750828445,
1913,"shell, pipes, and csvkit","output = subprocess.check_output(
    ['utl/vw-varinfo', os.path.abspath(UNLABELED_DATA)], 
    stderr=subprocess.STDOUT,
    cwd=VW_CODE_DIR,
    env=dict(os.environ, PATH=os.environ['PATH'] + ':' + VW_EXEC_DIR)
)",0.4724924564,
1913,"shell, pipes, and csvkit","import shlex
return_value = subprocess.run(shlex.split('ls -l *.ipynb'), stdout=subprocess.PIPE, shell=True)
print(return_value)",0.4724806547,
1735,questions start here,"show_count = 3
    for i in range(1, show_count+1):
        print('\n----- Sample %d start -----\n' % i)
        print(train['comment_text'][np.random.randint(len(train))])
        print('\n------ Sample %d end ------\n' % i)",0.3269901872,
1735,questions start here,"# Include this package for parallel processing
import multiprocessing as mp

def parallel_merge_all_sorting(dataset, n_processor, buffer_size):
    """"""
    Perform a parallel merge-all sorting method

    Arguments:
    dataset -- entire record set to be sorted
    n_processor -- number of parallel processors
    buffer_size -- buffer size determining the size of each sub-record set

    Return:
    result -- the merged record set
    """"""
    if (buffer_size <= 2):
        print(""Error: buffer size should be greater than 2"")
        return
    
    result = []

    ### START CODE HERE ### 
    
    # Pre-requisite: Perform data partitioning using round-robin partitioning
    subsets = rr_partition(dataset, n_processor)
    
    # Pool: a Python method enabling parallel processing. 
    pool = mp.Pool(processes = n_processor)

    # ----- Sort phase -----
    sorted_set = []
    for s in subsets:
        # call the serial_sorting method above
        sorted_set.append(*pool.apply(serial_sorting, [s, buffer_size]))
    pool.close()
    
    # ---- Final merge phase ----
    print(""sorted entire set:"" + str(sorted_set))
    result = k_way_merge(sorted_set)
    ### END CODE HERE ###
    
    return result",0.3267191648,
1735,questions start here,"# Include this package for parallel processing
import multiprocessing as mp

def parallel_binary_merge_sorting(dataset, n_processor, buffer_size):
    """"""
    Perform a parallel binary-merge sorting method

    Arguments:
    dataset -- entire record set to be sorted
    n_processor -- number of parallel processors
    buffer_size -- buffer size determining the size of each sub-record set

    Return:
    result -- the merged record set
    """"""
    
    if (buffer_size <= 2):
        print(""Error: buffer size should be greater than 2"")
        return
    
    result = []

    ### START CODE HERE ### 
    
    # Pre-requisite: Perform data partitioning using round-robin partitioning
    subsets = rr_partition(dataset, n_processor)
    
    # Pool: a Python method enabling parallel processing. 
    pool = mp.Pool(processes = n_processor)

    # ----- Sort phase -----
    sorted_set = []
    for s in subsets:
        # call the serial_sorting method above
        sorted_set.append(*pool.apply(serial_sorting, [s, buffer_size]))
    pool.close()
    
    # ---- Final merge phase ----
    print(""sorted entire set:"" + str(sorted_set))
    dataset = sorted_set
    while True:
        merged_set = []

        N = len(dataset)
        start_pos = 0
        pool = mp.Pool(processes = N//2)

        while True:
            if ((N - start_pos) > 2): 
                subset = dataset[start_pos:start_pos + 2]
                merged_set.append(pool.apply(k_way_merge, [subset]))
                start_pos += 2
            else:
                subset = dataset[start_pos:]
                merged_set.append(pool.apply(k_way_merge, [subset]))
                break
        
        pool.close()
        dataset = merged_set
        
        if (len(dataset) == 1): # if the size of merged record set is 1, then stop 
            result = merged_set
            break
    ### END CODE HERE ###
    
    return result",0.3267191648,
1735,questions start here,"# Counter for total number of iterations performed so far.
total_iterations = 0

def optimize(num_iterations = 0):
    # Ensure we update the global variable rather than a local copy.
    global total_iterations
    # Build the summary operation based on the TF collection of Summaries.
    

    print ('Initialized!')
    # Loop through training steps.
    print ('Total number of iterations = ' + str(int(num_epochs * train_size / BATCH_SIZE)))

    training_indices = range(train_size)
    
    # Start-time used for printing time-usage below.
    start_time = time.time()
    
    
    for iepoch in range(num_epochs):

        # Permute training indices
        perm_indices = numpy.random.permutation(training_indices)

        for step in range (int(train_size / BATCH_SIZE)):

            offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)
            batch_indices = perm_indices[offset:(offset + BATCH_SIZE)]

            # Compute the offset of the current minibatch in the data.
            # Note that we could use better randomization across epochs.
            batch_data = train_data[batch_indices, :, :, :]
            batch_labels = train_labels[batch_indices]
            # This dictionary maps the batch data (as a numpy array) to the
            # node in the graph is should be fed to.
            feed_dict = {train_data_node: batch_data,
                         train_labels_node: batch_labels}

            if step % RECORDING_STEP == 0:

                summary_str, _, l, lr, predictions = s.run(
                    [summary_op, optimizer, loss, learning_rate, y_pred],
                    feed_dict=feed_dict)

                summary_str = s.run(summary_op, feed_dict=feed_dict)
                summary_writer.add_summary(summary_str, step)
                summary_writer.flush()

                print ('global step:', iepoch*int(train_size / BATCH_SIZE)+step,\
                        ' over ',num_epochs*int(train_size / BATCH_SIZE))
                print ('Epoch: ', iepoch, '   || Step',float(step))
                print ('Minibatch loss: %.3f, learning rate: %.6f' % (l, lr))
                print ('Minibatch error: %.1f%%' % error_rate(predictions,
                                                             batch_labels))

                sys.stdout.flush()
            else:
                # Run the graph and fetch some of the nodes.
                _, l, lr, predictions = s.run(
                    [optimizer, loss, learning_rate, y_pred],
                    feed_dict=feed_dict)

    # Ending time.
    end_time = time.time()

    # Difference between start and end-times.
    time_dif = end_time - start_time

    # Print the time-usage.
    print(""Time usage: "" + str(timedelta(seconds=int(round(time_dif)))))",0.3209158182,
1735,questions start here,"while True:
    try:
        x = int(input())
        print(':', x)
    except:
        print('')
        continue
    break",0.3171532452,
1735,questions start here,"def take_bet(chip):
    '''
    Input:
    chip - a Chip() object
    '''
    while True:   
        try:
            chip.bet = int(input(""Please provide your bet: ""))
        except:
            print(""It looks that you have not provided a number. Please, try again by providing an integer value!"")
        else:
            if chip.bet > chip.total:
                print(""You exceeded your total amount of chips! You have: {} chips"".format(chip.total))
            else:
                print(""Thank you! Your bet is equal to {}"".format(chip.bet))
                break",0.3159021139,
1735,questions start here,"for word in ['and', 'he', 'she', 'when', 'john', 'never', 'i', 'how']:
    print (""Start word: %s"" % word)

    print (""2-gram sentence: \"""",
    get2GramSentence(word, 20))
    print (""\"""")",0.3149906993,
1735,questions start here,"num = 407
if num > 1:
    for i in range(2,num):
        if (num % i) = 0:
            print(num,""is not a prime number"")
            print(i,""times"",num//i,""is"" num)
            break
    else:
        print(num ""is a prime number"")",0.3104563653,
1735,questions start here,"for branch, words_tf_idf in philosophers_branches_idf.iteritems():
    words_tf_idf_sorted = sorted(words_tf_idf, key=lambda (w, tf_idf, tf, idf): -tf_idf)
    print ""Top 10 words for branch %s : %s\n"" % (branch, words_tf_idf_sorted[:10])",0.3096622527,
1735,questions start here,"def take_bet(chips):
    
    while True:
        
        try:
            chips.bet = int(input(""How many chips would you like to bet? ""))
        except:
            print(""Sorry, please use an integer"")
        else:
            if chips.bet > chips.total:
                print('Sorry, you do not have enough chips! you have: {}'.format(chips.total))
            else:
                break",0.3094466329,
339,create a list of all words,"import itertools
words = [""abc"", ""cat"", ""moto""]

def wordz():
    global words
    for w in words:
        yield ["""".join(x) for x in itertools.permutations(w , len(w))]
        
[i for i in wordz()]",0.4993405342,
339,create a list of all words,"def tokenize_stem(text):
    return [stemmer.stemWord(tok.text) for tok in nlp(text) if tok.is_alpha and not tok.is_stop]

tokenize_stem(""Hello vilg. Itt vagyok."")",0.4984436035,
339,create a list of all words,"from nltk.corpus import stopwords
from collections import defaultdict
import operator

stopwords = set(stopwords.words(""english""))

ham_words_lowercase = ham_messages_one_string.lower().split()

ham_words_nostop = []
for word in ham_words_lowercase:
    if word not in stopwords:
        ham_words_nostop.append(word)

ham_words_freq = Counter(ham_words_nostop).most_common()
ham_words_freq[:25]",0.498136878,
339,create a list of all words,"#Prepare NLTK stopwords
from nltk.corpus import stopwords
stop_words = stopwords.words('english')
stop_words.extend(['hotel'])



# Tokenize and clean sentences to words 
def sent_to_words(sentences):
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations



def process_text(sentences):
    # Create Corpus
    txt_tokenized=list(sent_to_words(sentences))
    # Remove stopwords
    txt_tokenized = [[w for w in doc if w not in stop_words] for doc in txt_tokenized]
    return txt_tokenized


import warnings
warnings.filterwarnings('ignore')

import timeit
start_time = timeit.default_timer()


rev_tokenized=process_text(rev_gens)
    # Create Dictionary-association word to numericID
dictionary = gensim.corpora.Dictionary(rev_tokenized )
    # Term Document Frequency-transform collections of texts to numeric form
corpus = [dictionary.doc2bow(txt) for txt in rev_tokenized ]

    # #View the first document:[(word_id,count ),()...]
print(corpus[:1])


    #save dictionary and corpus
pickle.dump(corpus, open('corpus.pkl', 'wb'))
dictionary.save('dictionary.gensim')

elapsed = timeit.default_timer() - start_time #10:41-10:10:43
print ('time', elapsed)",0.4978363514,
339,create a list of all words,"stops = set(stopwords.words(""english""))
def remove_stopwords(post):
    post = post.lower().split()
    p_post = []
    for word in post:
        if word not in stops:
            p_post.append(word)
    final_post = "" "".join(p_post)
    return final_post",0.4952850938,
339,create a list of all words,"# Step 2
from nltk.corpus import stopwords

en_stopwords = set(stopwords.words('english'))

def remove_stopwords(words):
    # TODO: return a new list of words with any stopwords removed
     return [word.lower() for word in words if word.lower() not in en_stopwords]",0.493929863,
339,create a list of all words,"from nltk.corpus import stopwords
import re
import string

stoplist = stopwords.words('english')

def filtered_words(a_list):
    word_list = []
    for tweet in a_list:
        word_list += preprocess(tweet) # preprocessing tweets
    text_list = Text(word_list)
    text_with_no_stopwords = [] # removing stopwords
    for word in text_list:
        if word not in stoplist:
            text_with_no_stopwords.append(word)
    return [w for w in text_with_no_stopwords if len(w)>2 and w not in string.punctuation] # removing punctuation 
    
def extract_freq_words (a_list):
    get_freq = filtered_words(a_list)
    fdist = FreqDist(get_freq) # getting frequency
    print(fdist.most_common(30)) 
    fdist.plot(30, cumulative=True)

print ""Trump:""
extract_freq_words(trump)
print ""Clint:""
extract_freq_words(clinton)",0.4930097461,
339,create a list of all words,"words =[]
for f in inaugural.fileids():
    words = words + [w.lower() for w in inaugural.words(f) if len(w)>7]
fdist = FreqDist([w for w in words])
fdist.plot(10)",0.4900179207,
339,create a list of all words,"from nltk.corpus import stopwords

tokens = [w for w in word_tokenize(text.lower()) if w.isalpha()]
no_stops = [t for t in tokens if t not in stopwords.words('english')]

Counter(no_stops).most_common(3)",0.4885635972,
339,create a list of all words,"new_stop = list(stop_words.ENGLISH_STOP_WORDS)

new_stop.extend(custom_stopwords)",0.4876771867,
971,introduction to factorials and variations,"from itertools import combinations_with_replacement
from math import factorial

def salads():

    List = ""TBL""
    k = 4
    n = len(List)
    cnt = 0

    for c in combinations_with_replacement(List, k):
        #print("""".join(c))
        cnt += 1
    
    formua_answer = combination_with_rep(n, k)
    
    return (cnt, cnt == formua_answer , formua_answer)

salads()",0.4617025256,
971,introduction to factorials and variations,"def factorials():
    index = 0
    while True:
        yield math.factorial(index)
        index += 1",0.4537177086,
971,introduction to factorials and variations,"def my_generator():
    yield 4
    yield 9
    yield 7
    yield 20
    yield 6
    yield 33
    yield 13
    yield 23
    yield 16
    yield 62
    yield 8",0.4407007098,
971,introduction to factorials and variations,"def test_fib():
    assert(fib(1) == 1)
    assert(fib(2) == 1)
    assert(fib(3) == 2)
    assert(fib(6) == 8)
    assert(fib(50) == 12586269025)
    
test_fib()",0.4400579929,
971,introduction to factorials and variations,"def test_fib():
    assert(fib(1) == 1)
    assert(fib(2) == 1)
    assert(fib(3) == 2)
    assert(fib(6) == 8)
    assert(fib(50) == 12586269025)
    print(""Tests passed!"")

test_fib()",0.438452661,
971,introduction to factorials and variations,"def factorial(n):
    result = ????  #fill in the missing value
    for i in range(1,n+1):
        #do something here to get 1*2*3*...*n
    return(result)",0.4343467057,
971,introduction to factorials and variations,"# An example of a generator function
def gen_func():
    print('First item')
    yield 10
    print('Second item')
    yield 20
    print('Third item')
    yield 30
    print('No more items left!')",0.4335353374,
971,introduction to factorials and variations,"def test_fibonacci():
    assert fibonacci(0) == 0
    assert fibonacci(8) == 21
    assert fibonacci(12) == 144
    assert fibonacci(21) == 10946",0.4328900278,
971,introduction to factorials and variations,"### R code ###
        
# Program to find n!        
factorial <- function(n) {    
  if (n == 1) {
    return(n)
  } else {
    return(n * factorial(n-1))
  }
}

factorial(5)",0.4326717257,
971,introduction to factorials and variations,"def factorials():
    index = 0
    while True:
        print(f'yielding factorial({index})...')
        yield math.factorial(index)
        index += 1",0.43242535,
897,importing data,"import csv
import sys

def readQuizBowlData(filename):
    train,dev,test = [],[],[]
    data = csv.reader(io.open(filename, 'r', encoding='iso-8859-1').readlines())
    header = next(data, None)
    if set(header) != set(['Question ID', 'Fold', 'Category', 'Answer', 'Text']):
        raise Exception('data improperly formatted')
    for item in iter(data):
        y = item[3]        
        x = sanify(' '.join(tokenize(item[4].replace('|||',''))))
        if   item[1] == 'train': train.append( (x,y) )
        elif item[1] == 'dev'  :   dev.append( (x,y) )
        elif item[1] == 'test' :  test.append( (x,y) )
    return train,dev,test

def makeLabelIDs(train, outputFile):
    labelIds = { label: k+1 for k,label in enumerate(set([y for x,y in train])) }
    labelIds['***UNKNOWN***'] = len(labelIds)+1
    with io.open(outputFile, 'w') as h:
        for label,k in labelIds.items():
            print(""{0}\t{1}"".format(k, label), file=h)
    return labelIds

def writeVWFile(filename, data, labelIds):
    unknownId = labelIds['***UNKNOWN***']
    with io.open(filename,'w') as h:
        for x,y in data:
            print(""{0} | q {1}"".format(labelIds.get(y, unknownId), x), file=h)
            
train,dev,test = readQuizBowlData('data/question_data/questions.csv')
traindev = train + dev
random.seed(9876)
random.shuffle(traindev)
labelIds = makeLabelIDs(train, 'data/quizbowl.labels')
writeVWFile('data/quizbowl.trde', traindev, labelIds)
writeVWFile('data/quizbowl.te',   test    , labelIds)
print('maximum label id = {0}'.format(len(labelIds)+1))
!wc data/quizbowl.t[re]*",0.443189919,
897,importing data,"%%writefile -a byoa/train     

        #Read training data from CSV and load into a data frame
        data=pd.read_csv(data_filename, sep=',', names = [""Name"", ""Gender""])
        data = shuffle(data)
        print(""Training data loaded"")

        #number of names
        num_names = data.shape[0]

        # length of longest name
        max_name_length = (data['Name'].map(len).max())

        #Separate data and label
        names = data['Name'].values
        genders = data['Gender']

        #Determine Alphabets in the input
        names = data['Name'].values
        txt = """"
        for n in names:
            txt += n.lower()

        #Alphabet derived as an unordered set containing unique entries of all characters used in name
        chars = sorted(set(txt))
        alphabet_size = len(chars)

        #Assign index values to each symbols in Alphabet
        char_indices = dict((str(chr(c)), i) for i, c in enumerate(range(97,123)))
        alphabet_size = 123-97
        char_indices['max_name_length'] = max_name_length

        #One hot encoding to create training-X
        X = np.zeros((num_names, max_name_length, alphabet_size))
        for i,name in enumerate(names):
            name = name.lower()
            for t, char in enumerate(name):
                X[i, t,char_indices[char]] = 1

        #Encode training-Y with 'M' as 1 and 'F' as 0
        Y = np.ones((num_names,2))
        Y[data['Gender'] == 'F',0] = 0
        Y[data['Gender'] == 'M',1] = 0

        #Shape of one-hot encoded array is equal to length of longest input string by size of Alphabet
        data_dim = alphabet_size
        timesteps = max_name_length
        print(""Training data prepared"")

        #Consider this as a binary classification problem
        num_classes = 2

        #Initiate a sequential model
        model = Sequential()

        # Add an LSTM layer that returns a sequence of vectors of dimension sequence size (512 by default)
        model.add(LSTM(sequence_size, return_sequences=True, input_shape=(timesteps, data_dim)))

        # Drop out certain percentage (20% by default) to prevent over fitting
        if dropout_ratio > 0 and dropout_ratio < 1:
            model.add(Dropout(dropout_ratio))

        # Stack another LSTM layer that returns a single vector of dimension sequence size (512 by default)
        model.add(LSTM(sequence_size, return_sequences=False))

        # Drop out certain percentage (20% by default) to prevent over fitting
        if dropout_ratio > 0 and dropout_ratio < 1:
            model.add(Dropout(dropout_ratio))

        # Finally add an activation layer with a chosen activation function (Sigmoid by default)
        model.add(Dense(num_classes, activation=activation_function))

        # Compile the Stacked LSTM Model with a loss function (binary_crossentropy by default),
        #optimizer function (rmsprop) and a metric for measuring model effectiveness (accuracy by default)
        model.compile(loss=loss_function, optimizer=optimizer_function, metrics=[metrics_measure])
        print(""Model compiled"")

        # Train the model for a number of epochs (50 by default), with a batch size (1000 by default)
        # Split a portion of trainining data (20% by default) to be used a validation data
        model.fit(X, Y, validation_split=split_ratio, epochs=num_epochs, batch_size=batch_records)
        print(""Model trained"")

        # Save the model artifacts and character indices under /opt/ml/model
        model_type='lstm-gender-classifier'
        model.save(os.path.join(model_path,'{}-model.h5'.format(model_type)))
        char_indices['max_name_length'] = max_name_length
        np.save(os.path.join(model_path,'{}-indices.npy'.format(model_type)), char_indices)

        print('Training complete.')
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255)

if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)",0.4176974893,
897,importing data,"def extract_business_categories(pid, lines):
    """"""
    Yield the 13th column of the yelp_business.csv dataset
    which contains the categories in which a business can 
    be categorized
    """"""
    import csv
    
    if pid == 0:
        lines.next()
        
    reader = csv.reader(lines)
    for row in reader:
        yield row[12]",0.4173199534,
897,importing data,"def extract_row(index, lines):
    """"""
    Yield a row of strings that has been parsed
    correctly by the CSV reader.
    
    Only rows with nine elements are valid as per
    the schema of the original CSV file
    """"""
    import csv
    
    if index == 0:
        lines.next()
        
    reader = csv.reader(lines)
    for row in reader:
        if len(row) == 9:
            yield row",0.4173199534,
897,importing data,"import pandas as pd

## pandas crashed while reading more than one million lines in one batch, so I split 
## the data in two files and read them seperately then append them into one DataFrame
def getFullDataFrame(features):
    df1 = pd.read_csv(""largeFirst.tsv"", delimiter=""\t"", low_memory=False, usecols=features)
    df2 = pd.read_csv(""largeSecond.tsv"", delimiter=""\t"", low_memory=False, usecols=features)
    df = df1.append(df2, ignore_index = True)
    return df

## Returns new dataframe with features selected
def pickColumnsDataFrame(df, features):
    df2 = df.copy()
    return df2[features]",0.4148684144,
897,importing data,"# reads data
    list_of_dataframes = []
    for file in data_paths:
        # read file
        df = pd.read_table(file, skiprows=header_size, delim_whitespace=True)

        # naming columns
        df.columns = ['theta', 'phi', 'Year', 'Month', 'Time', 'r', 'Br', 'Bt', 'Bp', 'N_{data}']

        list_of_dataframes.append(df)

    # concatenate dataframes into one
    data = pd.concat(list_of_dataframes, axis=0, ignore_index=True)",0.414124012,
897,importing data,"%%writefile utils.py

import pandas as pd

def read_csv(filename):
    df = pd.read_csv(filename, skiprows=8)
    df.rename(columns={'# timestamp': 'timestamp'}, inplace=True)
    df['timestamp'] = pd.to_datetime(df['timestamp'])  #might not want this here
    df.set_index('timestamp', inplace=True)
    return df",0.4132029116,
897,importing data,"# %load utils.py

import pandas as pd

def read_csv(filename):
    df = pd.read_csv(filename, skiprows=8)
    df.rename(columns={'# timestamp': 'timestamp'}, inplace=True)
    df['timestamp'] = pd.to_datetime(df['timestamp'])  #might not want this here
    df.set_index('timestamp', inplace=True)
    return df",0.4132029116,
897,importing data,"%%writefile utils.py

import pandas as pd

def read_csv(filename):
    df = pd.read_csv(filename, skiprows=8)
    df.rename(columns={'# timestamp': 'timestamp'}, inplace=True)
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df.set_index('timestamp', inplace=True)
    return df",0.4132029116,
897,importing data,"def get_tx_y_standardized(path):
    y, X, ids = proj1_helpers.load_csv_data(path)
    X_s, _, _ = feature_processing.standardize(X)
    tx = feature_processing.add_polynomial(X_s, [])
    return tx, y, ids",0.4117743373,
891,import packages,"# Import all PTR-FD algorithms and accompanying utility functions
import packages",0.4759173095,
891,import packages,"# Version checks
import importlib
def version_check(libname, min_version):
    m = importlib.import_module(libname)
    print(""{} version {} is "".format(libname, m.__version__), end='')
    print(""OK"" if m.__version__ >= min_version 
           else ""out-of-date. Please upgrade!"")
    
version_check(""numpy"", ""1.12"")
version_check(""matplotlib"", ""2.0"")
version_check(""pandas"", ""0.20"")
version_check(""nltk"", ""3.2"")
version_check(""tensorflow"", ""1.1.0"")",0.4746078849,
891,import packages,"## Code to install any package via python

def install_and_import(package):
    import importlib
    try:
        importlib.import_module(package)
    except ImportError:
        import pip
        pip.main(['install', package])
    finally:
        globals()[package] = importlib.import_module(package)

install_and_import('mlxtend')",0.4720779657,
891,import packages,"def pip_install(*packages):
    """"""
    Install packages using pip
    Alternatively just use command line
    pip install package_name
    """"""
    try:
        import pip
        for package in packages:
            pip.main([""install"", ""--upgrade"", package, ""--user""])
    except Exception as e:
        print(""Unable to install {} using pip."".format(package))
        print(""Exception:"", e)",0.4695003033,
891,import packages,from packages import *,0.4668546915,
891,import packages,"from py_files.importing_packages import *
%matplotlib inline",0.4605497718,
891,import packages,import package1,0.4595366716,
891,import packages,from mypkg import *,0.4592090249,
891,import packages,"import sys
sys.path.append("".."")
import grading
import download_utils",0.4586265087,
891,import packages,"#!/usr/bin/env python
import os
import zipfile

def zipdir(path, ziph):
    # ziph is zipfile handle
    for root, dirs, files in os.walk(path):
        for file in files:
            ziph.write(os.path.join(root, file))

if __name__ == '__main__':
    zipf = zipfile.ZipFile('/home/jovyan/ArchiveNotebook.zip', 'w', zipfile.ZIP_DEFLATED)
    zipdir('/home/jovyan', zipf)
    zipf.close()
    print(""The notebook is saved in Archive.zip"")",0.4586126506,
240,combining what we ve learned,"def selfdot(v):
    global waste
    for i in range(waste): #waste time - represents extra calculation
        np.dot(v,v)
    return np.dot(v,v)

def multi_process(L=int(1e7),vals=np.arange(1,4.5,0.25)):

    times={}
    chunks=[]
    for ps in range(1,6):
        print ps,
        p=Pool(ps)   #create a pool of ps processes
        times[ps]=[]
        for e in vals:
            chunk=int(L*10**(-e))
            t1=time()
            ############# map = perform a command on each element of a list, using parallel processes.
            X=p.map(selfdot,[np.arange(i,i+chunk-1) for i in xrange(0,L,chunk)],)
            #############
            t2=time()
            ratio=(t2-t1)/(L*(1+waste))
            times[ps].append(ratio)
            if ps==1:
                chunks.append(chunk*8e-3)
        p.close() # close the pool

    for i in times.keys():
        plt.semilogx(chunks,times[i],label=str(i))

    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
    plt.grid()",0.382633388,
240,combining what we ve learned,"def probe_network(W, p):
    aep_all = False # ""the probe and activation vector match for all elements""
    aep_sum = True # ""a[i] and p[i] have matched for all elements so far in a given cycle""
    i = 0
    while not aep_all and i < max_updates:
        print ""\tstep {}, activation: {}"".format(i, p)
        index_asynchronous = 2 + i%4 # cyclically update indices 2,3,4,5
        p, aep = dynamic_rule(W, p, ix_async=index_asynchronous)
        i += 1
        aep_sum = aep_sum and aep
        if i%4 == 0:
            aep_all = aep_sum ; aep_sum = True
    a = p ; print ""{} updates produced {}\n"".format(i, a)
    return a",0.3753291965,
240,combining what we ve learned,"def getResult(multipliedElements):
    ################## COMPLETE HERE FOLLOWING THE INSTRUCTIONS ##################
    
    # Apply a transformation to the input RDD so that all values with the same key are summed.
    R = 
    return R
    
    ################## END MODIFICATIONS ##################

nice(""A"", A)
nice(""B"", B)
C = getResult(multipliedElements)
nice(""C"", C)

############################################################## 
#YOU SHOULD OBTAIN THE FOLLOWING MATRIX C AS RESULT
# 98.00      144.00       56.00     1038.00      869.00
# 241.00      353.00      133.00     1767.00     1315.00
# 3513.00     5169.00     1869.00    10683.00     6621.00
##############################################################",0.3658710122,
240,combining what we ve learned,"def clean_text(data):
    for i, target in zip(range(len(data.data)), data.target):
        #------------TO LOWER CASE-------------
        file = data.data[i].decode(""utf-8"").lower()
        #------------TOKENIZE-------------
        word_tokens = word_tokenize(file)
        #------------REMOVE STOP WORDS-------------
        filtered_sentence = [w for w in word_tokens if not w in stop_words]
        filtered_sentence = []
        for w in word_tokens:
            if w not in stop_words:
                    filtered_sentence.append(w)
        #------------STEMMING WITH PORTER STEMMER-------------
        ps = PorterStemmer()
        stemmedFile = []
        for word in filtered_sentence:
            for w in word.split("" ""):
                stem = ps.stem(w)
                stemmedFile.append(stem)
                #COUNT THE TERMS PER CATEGORY
                term_per_category[train_set.target_names[target]][word] += 1
        #------------PUT FILE BACK-------------
        data.data[i] = ' '.join(stemmedFile)",0.3645709753,
240,combining what we ve learned,"def loadValidationData():
    validation_x = mnist23.data[training_samples:]
    validation_y = np.array([mnist23.target[training_samples:]]) 
    return validation_x,validation_y",0.3614777327,
240,combining what we ve learned,"#This version of autoline with shift the emission lines to the rest frame based on the
#spectrum's redshift. It also flattens it to zero intercept.
def file_autolineS(infile):
    match_flag = 0
    #g_lower,g_upper = 10.0,100.0
    #x0_lower,x0_upper = 6800.0,7200.0
    #A_lower,A_upper = -1.0,4.0
    #m_lower,m_upper = -0.5,10.0
    #b_lower,b_upper = -4.0,5.0
    global g_lower,g_upper,x0_lower,x0_upper,A_lower,A_upper,m_lower,m_upper,b_lower,b_upper
    g_lower,g_upper = 1.0,50.0
    A_lower,A_upper = -2.0,3.0
    m_lower,m_upper = -1.0,1.0
    b_lower,b_upper = -1.0,1.0
    
    
    spec_array = fits.open(infile)[1].data
    spec_hdr = fits.open(infile)[0].header
    logwave = spec_array['loglam'] #Wavelength as log10(lambda)
    wave_arr = spec_array['lam'] #Recast the wavelengths in Angstroms
    flux_raw = spec_array['flux_raw'] #Flux at a given wavelength. Units of 10^(-17) erg/s/cm^(2)/Ang
    flux_err = spec_array['ivar']
    zobj = float(spec_hdr['Z_VI'])
    obj_class = int(spec_hdr['OBJ_CLAS'])
    if zobj < 0.0:
        zobj = 0.0
    
    wave_shift = wave_arr / (1 + zobj)
    
    #Fit the continuum to a polynomial using Legendre polynomials
    box_continuum = spec_array['cont_flux']
    param_lgd = np.array([float(spec_hdr['LGD_PRM0']),float(spec_hdr['LGD_PRM1']),
                          float(spec_hdr['LGD_PRM2']),float(spec_hdr['LGD_PRM3'])])
    continuum_fit = spec_array['cont_fit'] #continuum fit
    flux_reduced = spec_array['flux_reduced'] #Reduce the flux
    err_reduced = spec_array['err_reduced'] #Reduce the ivar
    
    flux_reducedS = flux_reduced - 1.0
        
    #Pick out the emission lines
    line_flux,line_err = spec_array['line_flux'],spec_array['line_err']
    line_fluxS = line_flux - 1.0 #Flattened down to 0.
    line_errS = line_err - 1.0 #Flattened down to 0.
    
    line_arr = line_load()
    lower_shift = 3700 / (1 + zobj)
    upper_shift = 9500 / (1 + zobj)
    for i in range(len(line_arr)):
        line_arr['OBS_WAVE'][i] = line_arr['REST_WAVE'][i] * (zobj + 1)
    wlines = np.where((line_arr['REST_WAVE']>=lower_shift)&(line_arr['REST_WAVE']<=upper_shift))[0]
    vis_lines = line_arr[wlines]
    
    for i in range(len(wlines)):
        lam_center = vis_lines['REST_WAVE'][i]
        x0_lower = lam_center - 30.0
        x0_upper = lam_center + 30.0
        lam_lower = lam_center - 100.0
        lam_upper = lam_center + 100.0
        g_true,x0_true,A_true,m_true,b_true = 20.0,lam_center,1.5,0.0,0.0
        result = [g_true,x0_true,A_true,m_true,b_true]
        ndim, nwalkers = 5, 50
        p0 = [result + 1e-4*np.random.randn(ndim) for i in range(nwalkers)]
    
        wem = np.where((wave_shift>=lam_lower)&(wave_shift<=lam_upper))[0]
        x_em = wave_shift[wem]
        y_em = line_fluxS[wem]
        err_em = line_errS[wem]
        sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob, args=(x_em,y_em,err_em),a=3.5)
    
        #Burn-in
        pos,prob,state = sampler.run_mcmc(p0, 100) #----Ran well with 500
        sampler.reset()
        pos, prob, state = sampler.run_mcmc(pos, 1000) #----Ran well with 1000
        samples = sampler.flatchain
        g_mcmc, x0_mcmc, A_mcmc, m_mcmc,b_mcmc = map(lambda v: (v[1], v[2]-v[1], v[1]-v[0]),
                                 zip(*np.percentile(samples, [16, 50, 84],
                                                    axis=0)))
        acmean = np.mean(sampler.acceptance_fraction)
        
        #print('\n')
        #print('Central Wavelength: {0:6.1f}'.format(lam_center))
        #print('----------------PARAMETERS------------------')
        #print('Parameter |   50th   |   84th   |   16th   |')
        #print('--------------------------------------------')
        #print('    g     |  {0:5.2f}   |  {1:5.2f}   | {2:5.2f}    |'.format(g_mcmc[0],g_mcmc[1],g_mcmc[2]))
        #print('    x0    |  {0:4d}    |  {1:4d}    | {2:4d}     |'.format(int(x0_mcmc[0]),int(x0_mcmc[1]),int(x0_mcmc[2])))
        #print('    A     |  {0:5.2f}   |  {1:5.2f}   | {2:5.2f}    |'.format(A_mcmc[0],A_mcmc[1],A_mcmc[2]))
        #print('    m     |  {0:6.2f}  |  {1:6.2f}  | {2:6.2f}   |'.format(m_mcmc[0],m_mcmc[1],m_mcmc[2]))
        #print('    b     |  {0:.4f} | {1:0.6f} | {2:0.6f} |'.format(b_mcmc[0],b_mcmc[1],b_mcmc[2]))
        #print('--------------------------------------------')
        #print('Mean Acceptance Fraction: {0:6.4f}'.format(acmean))
        
        #y_lor = line_func(x_em,g_mcmc[0],x0_mcmc[0],A_mcmc[0],m_mcmc[0],b_mcmc[0])
        #max_amp = np.amax(y_lor)
        
        try:
            y_lor = line_func(x_em,g_mcmc[0],x0_mcmc[0],A_mcmc[0],m_mcmc[0],b_mcmc[0])
            max_amp = np.amax(y_lor)
        except Exception:
            continue
        
        #print(max_amp)
        if ((g_mcmc[0]>=5.0)&(g_mcmc[0]<50.0)&(acmean>0.22)&(acmean<0.5)&(max_amp>0.35)):
            match_flag = 1
            #print('\n')
            #print('TARGET IS QSO')
            #print('\n')
            #print('Max. Relative Flux: {0:06.4f}'.format(max_amp))
            break
        '''
        x_lower = np.amin(wave_shift)
        x_upper = np.amax(wave_shift)
        fig1,ax1 = plt.subplots(figsize=(15,12))
        ax1.plot(wave_shift,flux_reducedS,color='0.75')
        ax1.plot(wave_shift,line_fluxS,linewidth=0.6,color='black')
        ax1.plot(x_em,y_lor,linewidth=1.5,color='orange')
        ax1.set_xlim((x_lower,x_upper))
        ax1.set_xlabel(r'Wavelength (\AA)')
        ax1.set_ylim((-2.0,5))
        ax1.set_ylabel(r'Rel. Flux')
        ax1.tick_params(axis='both',direction='in')
        ax1.tick_params(axis='both',which='minor',direction='in')
        ax1.xaxis.set_minor_locator(ticker.MultipleLocator(100))
        ax1.yaxis.set_minor_locator(ticker.MultipleLocator(1))
        
        fig2 = corner.corner(samples[:,:], labels=[""$\sigma_1$"", ""$\mu_1$"", ""$A_1$"",""$m$"",""$b$""],
                          label_kwargs={""fontsize"": 15},quantiles=[0.16, 0.5, 0.84],
                          show_titles=True, title_kwargs={""fontsize"": 15})
        '''
    return match_flag,obj_class",0.3591231704,
240,combining what we ve learned,"def file_autoline(infile):
    match_flag = 0
    #g_lower,g_upper = 10.0,100.0
    #x0_lower,x0_upper = 6800.0,7200.0
    #A_lower,A_upper = -1.0,4.0
    #m_lower,m_upper = -0.5,10.0
    #b_lower,b_upper = -4.0,5.0
    global g_lower,g_upper,x0_lower,x0_upper,A_lower,A_upper,m_lower,m_upper,b_lower,b_upper
    g_lower,g_upper = 10.0,100.0
    A_lower,A_upper = -2.0,6.0
    m_lower,m_upper = -1.0,9.0
    b_lower,b_upper = -5.0,5.0
    
    
    spec_array = fits.open(infile)[1].data
    spec_hdr = fits.open(infile)[0].header
    logwave = spec_array['loglam'] #Wavelength as log10(lambda)
    wave_arr = spec_array['lam'] #Recast the wavelengths in Angstroms
    flux_raw = spec_array['flux_raw'] #Flux at a given wavelength. Units of 10^(-17) erg/s/cm^(2)/Ang
    flux_err = spec_array['ivar']
    zobj = float(spec_hdr['Z_VI'])
    obj_class = int(spec_hdr['OBJ_CLAS'])
    if zobj < 0.0:
        zobj = 0.0
    
    #Fit the continuum to a polynomial using Legendre polynomials
    box_continuum = spec_array['cont_flux']
    param_lgd = np.array([float(spec_hdr['LGD_PRM0']),float(spec_hdr['LGD_PRM1']),
                          float(spec_hdr['LGD_PRM2']),float(spec_hdr['LGD_PRM3'])])
    continuum_fit = spec_array['cont_fit'] #continuum fit
    flux_reduced = spec_array['flux_reduced'] #Reduce the flux
    err_reduced = spec_array['err_reduced'] #Reduce the ivar
        
    #Pick out the emission lines
    line_flux,line_err = spec_array['line_flux'],spec_array['line_err']
    
    line_arr = line_load()
    for i in range(len(line_arr)):
        line_arr['OBS_WAVE'][i] = line_arr['REST_WAVE'][i] * (zobj + 1)
    wlines = np.where((line_arr['OBS_WAVE']>=3700)&(line_arr['OBS_WAVE']<=9000))[0]
    vis_lines = line_arr[wlines]
    
    for i in range(len(wlines)):
        lam_center = vis_lines['OBS_WAVE'][i]
        x0_lower = lam_center - 100.0
        x0_upper = lam_center + 100.0
        lam_lower = lam_center - 300.0
        lam_upper = lam_center + 300.0
        g_true,x0_true,A_true,m_true,b_true = 50.0,lam_center,1.5,0.0,1.0
        result = [g_true,x0_true,A_true,m_true,b_true]
        ndim, nwalkers = 5, 50
        p0 = [result + 1e-4*np.random.randn(ndim) for i in range(nwalkers)]
    
        wem = np.where((wave_arr>=lam_lower)&(wave_arr<=lam_upper))[0]
        x_em = wave_arr[wem]
        y_em = line_flux[wem]
        err_em = line_err[wem]
        sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob, args=(x_em,y_em,err_em),a=3)
    
        #Burn-in
        pos,prob,state = sampler.run_mcmc(p0, 100) #----Ran well with 500
        sampler.reset()
        pos, prob, state = sampler.run_mcmc(pos, 1000) #----Ran well with 1000
        samples = sampler.flatchain
        g_mcmc, x0_mcmc, A_mcmc, m_mcmc,b_mcmc = map(lambda v: (v[1], v[2]-v[1], v[1]-v[0]),
                                 zip(*np.percentile(samples, [16, 50, 84],
                                                    axis=0)))
        acmean = np.mean(sampler.acceptance_fraction)
        '''
        print('\n')
        print('----------------PARAMETERS------------------')
        print('Parameter |   50th   |   84th   |   16th   |')
        print('--------------------------------------------')
        print('    g     |  {0:5.2f}   |  {1:5.2f}   | {2:5.2f}    |'.format(g_mcmc[0],g_mcmc[1],g_mcmc[2]))
        print('    x0    |  {0:4d}    |  {1:4d}    | {2:4d}     |'.format(int(x0_mcmc[0]),int(x0_mcmc[1]),int(x0_mcmc[2])))
        print('    A     |  {0:5.2f}   |  {1:5.2f}   | {2:5.2f}    |'.format(A_mcmc[0],A_mcmc[1],A_mcmc[2]))
        print('    m     |  {0:6.2f}  |  {1:6.2f}  | {2:6.2f}   |'.format(m_mcmc[0],m_mcmc[1],m_mcmc[2]))
        print('    b     |  {0:.4f} | {1:0.6f} | {2:0.6f} |'.format(b_mcmc[0],b_mcmc[1],b_mcmc[2]))
        print('--------------------------------------------')
        print('Mean Acceptance Fraction: {0:6.4f}'.format(acmean))
        '''
        #fwhm = 2.355 * g_mcmc[0] #FWHM definition.
        #broad_check = (fwhm / x0_mcmc[0]) * 299792.458 #velocity dispersion in 
        y_lor = line_func(x_em,g_mcmc[0],x0_mcmc[0],A_mcmc[0],m_mcmc[0],b_mcmc[0])
        max_amp = np.amax(y_lor)
        if ((g_mcmc[0]>=20.0)&(g_mcmc[0]<100.0)&(acmean>0.2)&(acmean<0.5)&(max_amp>1.2)):
            match_flag = 1
            #print('\n')
            #print('TARGET IS QSO')
            #print('\n')
            #print('Max. Relative Flux: {0:06.4f}'.format(max_amp))
            break
    return match_flag,obj_class",0.3585613072,
240,combining what we ve learned,"%%cython
def ill_write_my_own_cython_sum_thank_you_very_much(list arr):
    cdef int N = len(arr)
    cdef float x = arr[0]
    cdef int i
    for i in range(1 ,N):
        x += arr[i]
    return x",0.3565592468,
240,combining what we ve learned,"# ENN SMOTE
# http://contrib.scikit-learn.org/imbalanced-learn/stable/auto_examples/combine/plot_smote_enn.html
from imblearn.combine import SMOTEENN

def ennSMOTE(train_features, train_labels) :
    """"""
    Takes training set with imbalanced data and creates
    synthentic samples that upsample the minority class
    by applying SMOTE + Tomek links
    Input: 
        train_features -- array of training features
        train_labels -- array of training labels
    Output: 
        x_train_res -- array of resampled features
        y_train_res -- array of resampled labels
    """"""
    sm = SMOTEENN(random_state=12)
    x_train_res, y_train_res = sm.fit_sample(train_features, train_labels)
    return  x_train_res, y_train_res

x_train_res, y_train_res = ennSMOTE(X_train, y_train) 
x_test_res, y_test_res = ennSMOTE(X_test, y_test)",0.3540828228,
240,combining what we ve learned,"#your code here
# Maybe there is a more elegant way, without looping using vectorization, but I wasn't able to get it...
def getmats(indf):
    mx = np.zeros((len(indf),len(features)))
    i = 0
    for uid, bid in zip(indf['user_id'],indf['business_id']):
        j = 0
        for x in features:
            if uid==x or bid==x:
                mx[i,j] = 1
            j = j + 1
        i = i + 1
    return (mx,indf['stars'])",0.3527660966,
857,hyper parameter marginalisation,"#
    # Model Parameters
    #
    r0, kappa_r, theta_r, sigma_r = [0.01, 0.1, 0.03, 0.2]
    T  = 2.0  # time horizon
    M  = 50   # time steps
    dt = T / M
    I  = 50000  # number of MCS paths
    np.random.seed(50000)  # seed for RNG

    r = CIR_generate_paths_exact(r0, kappa_r, theta_r, sigma_r, T, M, I)",0.490007937,
857,hyper parameter marginalisation,"def back_propagation(params, nn_shape, X, y, lambda_val):
    debug = 0
    m = X.shape[0]
    X = np.matrix(X)
    y = np.matrix(y)
    
    #Theta config #################################################################
    #l = len(nn_shape)
    theta = expand_theta(nn_shape,params,debug)       
   
    #Config the Gradient Accumulators #############################################
    delta = {}
    for s in range(1,len(theta)+1):
        #print ""theta["",s,""].shape = "",theta[s].shape
        delta[s] = (np.zeros(theta[s].shape))
        if debug: print ""deltaset: delta["",s,""] = "", delta[s].shape
    
    #Forward Propogate ############################################################
    a,z  = generalized_forward_propagate(X, theta,debug)
    if debug: print ""backprop -> forward_prop -> len(a)"", len(a)
    if debug: print ""backprop -> forward_prop -> len(z)"", len(z)
    
    #Calculate Cost of this iteration's theta #######################################
    J = cost_function(a,y,lambda_val,m,theta,debug)
    print ""J is "", J
    
    # Iterate through training set to determine partial derivatives##################
    #note setting m=2 breaks the algorithm, but allows for the debug functions to 
    # convey the operations in the process.
    if debug: m=2 
    if debug: print ""backprop: setting m to:"", m
    for i in range(m):
        
        if debug: print ""i = "",i
        
        # Forward Propogate ############################################################
        a,z  = generalized_forward_propagate(X[i], theta,debug)
        if debug:
            for i in a.keys():
                print ""backprop -> a["",i,""].shape = "",a[i].shape
            for i in z.keys():
                    print ""backprop -> z["",i,""].shape = "",z[i].shape

                    
        # Find the errors ###############################################################
        
        #the 'd' represents the lower case delta, or the error between 'reality' and the activation values
        d = {}
              
        
        #the first d calculated is for the last layer; it is simply the difference between the
        # predicted values, aka hypothese and the label(or truth) data.
        a_len = len(a)
        h = a[a_len]
        if debug: print 'backprop -> d[',a_len,'] = (',h.shape,' - ', y[i,:].shape,')', '= ',
        d[a_len] = (h - y[i,:])
        if debug: print ""backprop -> d["", a_len, ""].shape = "", d[a_len].shape
        
        #subsequent d values are calculated by propagating the errors found in the last layer to all previous layers up to
        #but exluding the last layer, which represents the input values, as these are considered truth and without error.
        for k in range(a_len-1,1,-1):
            #print ""k="",k
            if debug: print ' backprop -> z[',k,'] =    np.insert(',z[k].shape,', 0, values=np.ones(1)) = ',
            z[k] =    np.insert(z[k], 0, values=np.ones(1))  # (1, 26), activation_function_gradient(zt[ti])
            if debug: print z[k].shape
            #if 1==1:   #ti == a_len-1:
            if k == a_len-1:    
                if debug: print 'backprop -> d[',k,'] = np.multiply((',theta[k].T.shape,' * ', d[k+1].T.shape, ').T, activation_function_gradient(', z[k].shape,')) =',
                d[k] = np.multiply((theta[k].T * d[k+1].T).T, activation_function_gradient(z[k]))
                if debug: print d[k].shape
            else:
                d[k] = np.multiply((theta[k].T * d[k+1][:,1:].T).T, activation_function_gradient(z[k]))
                
        #the partial derivatives of the cost function  with respect the the theta values are calculated by
        #accumulating over each iteration of the training set, aka the gradient
        l = len(d)
        for li in range(1,l):  
            if debug: print ""backprop -> iteration "", li, "" of "", len(theta)+1
            if debug: print ""backprop ->delta: delta["",li,""] += d["",li+1,""][:,1:].T * a["",li,""] = ""
            if debug: print delta[li].shape, d[li+1][:,1:].T.shape, "" * "", a[li].shape
            delta[li] += d[li+1][:,1:].T * a[li]

        if debug: print ""backprop -> delta: delta["",li,""] += d["",li+1,""].T * a["",li,""] = ""
        if debug: print delta[li].shape, d[li+1].T.shape, "" * "", a[li].shape
        delta[l] += d[l+1].T * a[l]
    
    grad_list = []
    if debug: print ""backprop -> keys delta"", delta.keys()
    
    #delta values are normalized based on the number of iterations on the dataset.
    for li in range(1,len(delta)+1):
        if debug: print ""backprop: accumulator iteration li= "",li, ""of "", len(delta)+1
        if debug: print ""len delta[li] = "", l, li, len(delta[li])
        delta[li] /= m
        if debug: print ""li="",li,"" delta["",li,""][:,1:] = "",delta[li][:,1:].shape, "" theta["",li,""] = "",theta[li][:,1:].shape
        delta[li][:,1:] += np.multiply(lambda_val,theta[li][:,1:])
        grad_list.append(np.ravel(delta[li]))
    
    grad = np.concatenate(grad_list)
    if debug: print ""grad.shape="", grad.shape, ""flat theta shape"", flatten_theta(theta,debug)
    return J, grad",0.4876908362,
857,hyper parameter marginalisation,"# Physical parameters
    
    epsilon = 1.0  # non-spinning BH
    B_0 = 3e12     # gausss
    Mbh = 10     # solar masses
    Mns = 1.4      # solar masses
    r_NS = 10.0    # km
    LRtime = time_to_coal(30,10,1.4)  # seconds
    steps = 1000
    sep = np.linspace(0.0,70.0, steps)
    longsep = np.linspace(30.0,1000.0, steps)
    mass_range = np.linspace(8.0,32.0, steps)

    # Declaring arrays
    
    lum_vec = np.empty([steps])
    time_to_LR = np.empty([steps])
    time_to_LR_BIGB = np.empty([steps])
    coal_time = np.empty([steps])  
    el_over_el = np.empty([steps])
    el_over_el2 = np.empty([steps])
    el_over_el3 = np.empty([steps])
    mag_test15 = np.empty([steps])
    rel_strength = np.empty([steps])
    
    # Filling arrays
    for k in range (steps):
        # Strong B Field Test
        mag_test15[k] = luminosity(epsilon, 1e15, 10., longsep[k], 10., 1e-2)
        time_to_LR_BIGB[k] = time_to_coal(longsep[k],10,Mns) - LRtime
        # L/L_0 fractions
        el_over_el[k] = lum_frac(mass_range[k], 1.4, -0.5e-3)
        el_over_el2[k] = lum_frac(mass_range[k], 1.4, -1e-3)
        el_over_el3[k] = lum_frac(mass_range[k], 1.4, -5e-3)
        # Battery luminosity for typical parameters
        lum_vec[k] = luminosity(epsilon, 3e12, 10., sep[k], 10., 1e-2) # luminosity(epsilon, B_0, Mbh, r_sep, r_NS, eff)
        time_to_LR[k] = time_to_coal(sep[k],10,Mns) - LRtime
        coal_time[k] = time_to_coal(sep[k],10,Mns)   #time to coalescence 
        rel_strength[k] = lum_frac(10., 1.4, time_to_LR[k])",0.4799253941,
857,hyper parameter marginalisation,"%%stata --mata
    void logLikelihood2DynaA(M,todo,b,lnf,g,H)
    {
        real scalar mu_l,mu_o,mu_n,mu_c,eta_l,eta_n,
            sdsta,sdmar,sdmod,i,T,N,lam1,
            lam2,lam3,lam4,phi22,phi32
        real matrix X,mt,id,mtp,idp,yp,Xp,Jn,Jt,En,Et,
            Q1,Q2,Q3,Q4,OmegaInv,beta
        struct dynoInfo scalar Dy

        mu_l =(moptimize_util_xb(M,b,1))^2
        mu_o =(moptimize_util_xb(M,b,2))^2
        mu_n =(moptimize_util_xb(M,b,3))^2
        mu_c =(moptimize_util_xb(M,b,4))^2
        eta_l=moptimize_util_xb(M,b,5)
        eta_o=moptimize_util_xb(M,b,6)
        eta_n=moptimize_util_xb(M,b,7)
        eta_ll=moptimize_util_xb(M,b,8)
        eta_ln=moptimize_util_xb(M,b,9)
        eta_nl=moptimize_util_xb(M,b,10)
        eta_nn=moptimize_util_xb(M,b,11)

        lam_own=moptimize_util_xb(M,b,12)
        lam_ll=moptimize_util_xb(M,b,13)
        lam_ln=moptimize_util_xb(M,b,14)

        lam_nl=moptimize_util_xb(M,b,15)
        lam_nn=moptimize_util_xb(M,b,16)
        rho_l=moptimize_util_xb(M,b,17)
        rho_n=moptimize_util_xb(M,b,18)
        omega=moptimize_util_xb(M,b,19)
        zeta_l=moptimize_util_xb(M,b,20)
        zeta_o=moptimize_util_xb(M,b,21)
        zeta_n=moptimize_util_xb(M,b,22)
        zeta_c=moptimize_util_xb(M,b,23)
        sdsta=exp(moptimize_util_xb(M,b,24))
        sdmar=exp(moptimize_util_xb(M,b,25))
        sdmod=exp(moptimize_util_xb(M,b,26))

        alpha=moptimize_util_xb(M,b,27)

        y     =moptimize_util_depvar(M,1)
        lnswg =moptimize_util_depvar(M,2)
        lnews =moptimize_util_depvar(M,3)
        otherl=moptimize_util_depvar(M,4)
        nnews =moptimize_util_depvar(M,5)
        otherc=moptimize_util_depvar(M,6)
        l_ACS_HH=moptimize_util_depvar(M,7)

        id=moptimize_util_userinfo(M,1)
        mt=moptimize_util_userinfo(M,2)
        m =moptimize_util_userinfo(M,3)
        Dy =moptimize_util_userinfo(M,4)
        si =moptimize_util_userinfo(M,5)

        beta=mu_l,mu_o,mu_n,mu_c,eta_l,eta_o,eta_n,
            eta_ll,eta_ln,eta_nl,eta_nn,lam_own,lam_ll,
            lam_ln,lam_nl,lam_nn,rho_l,rho_n,omega,zeta_l,zeta_o,zeta_n,zeta_c,alpha

        X=lnews:*lnswg,otherl:*lnswg,nnews:*lnswg,otherc:*lnswg,
            lnews,otherl,nnews,
            Dy.lnewslnews,Dy.lnewsnnews,Dy.nnewslnews,Dy.nnewsnnews,
            Dy.lsi,Dy.siXlnln,Dy.siXlnnn,Dy.siXnnln,Dy.siXnnnn,
            Dy.lnewstot,Dy.nnewstot,l_ACS_HH,Dy.lnewsn,Dy.otherln,Dy.nnewsn,Dy.othercn,J(rows(lnews),1,1)

        B=lnews,otherl,nnews,otherc

        lnf=J(rows(m),1,.)
        lnDetF=J(rows(m),6,.)
        muVec=mu_l,mu_o,mu_n,mu_c
        if (runiform(1,1)>.96) muVec,eta_l,eta_n,omega,zeta_l,zeta_o,zeta_n,zeta_c
            for (i=1;i<=rows(m);i++) {
                mtp=panelsubmatrix(mt,i,m)
                idp=panelsubmatrix(id,i,m)
                T=rows(uniqrows(mtp))
                N=rows(uniqrows(idp))
                yp=panelsubmatrix(y,i,m)
                Xp=panelsubmatrix(X,i,m)
                sip=panelsubmatrix(si,i,m)
                Bp=panelsubmatrix(B,i,m)
            
                lam1=sdmod^2
                lam2=T*sdsta^2+sdmod^2
                lam3=N*sdmar^2+sdmod^2
                lam4=T*sdsta^2+N*sdmar^2+sdmod^2
                phi22=sdmod^2/lam2
                phi32=sdmod^2/lam3
                phi42=sdmod^2/lam4

                Jn=J(N,N,1/N)
                Jt=J(T,T,1/T)
                En=I(N)-Jn
                Et=I(T)-Jt

                Q1=En#Et
                Q2=En#Jt
                Q3=Jn#Et
                Q4=Jn#Jt
                OmegaInv=(Q1/lam1+Q2/lam2+Q3/lam3+Q4/lam4)
                lnDetOmega=-2*N*T*ln(sdmod)+(N-1)*ln(phi22)+(T-1)*ln(phi32)+ln(phi42)

                lnf[i]=1/2*lnDetOmega-1/2*(yp-Xp*beta')'*OmegaInv*(yp-Xp*beta')

                TimeVars=uniqrows(mtp)	

                for (z=1;z<=rows(TimeVars);z++) {
                    siz=select(sip,mtp:==TimeVars[z])
                    Bz=select(Bp,mtp:==TimeVars[z])
                    sizBz=siz:*Bz
                    sg=colsum(sizBz)
                    Ng=colsum(sizBz:!=0)
                    term=(Ng:>0):*(Ng:-1):*ln(1:-muVec)
                    if (hasmissing(term)) lnDetF[i,z]=.
                    else lnDetF[i,z]=rowsum(term)
                }
            }

        lnDetF=B:*ln(1:-muVec)
        if (hasmissing(lnf)) lnf=.
        else lnf=colsum(lnf)
        if (hasmissing(lnDetF)) lnf=.
        else lnf=colsum(lnf) +colsum(rowsum(lnDetF))
        if (runiform(1,1)>.96) colsum(rowsum(lnDetF)),lnf
    }",0.4703124762,
857,hyper parameter marginalisation,"def SVR_model (train_norm, test_norm, features, target,kern,eps):
    svr_rbf = SVR(kernel=kern,C=1000,gamma=0.1,epsilon=eps)
    pred = svr_rbf.fit(train_norm[features],train_norm[target]).predict(test_norm[features])
    sns.kdeplot(pred)
    sns.kdeplot(test_norm[""cnt""])
    plt.legend([""Pred"",""Test""])
    plt.show()
    mse = np.mean((test_norm[""cnt""] - pred)**2)
    return mse

rbf_svr = SVR_model(train_norm,test_norm,features,""cnt"",""rbf"",0.1)
rbf_svr",0.4700084925,
857,hyper parameter marginalisation,"def svmTrain(dataTrain, labelTrain, cost, kernel, gamma, degree):
    model = SVC(C=cost, kernel=kernel, gamma=gamma, degree=degree)
    model.fit(dataTrain, labelTrain)
    return model, model.support_.shape[0]",0.4661208689,
857,hyper parameter marginalisation,"def svmTrain(dataTrain, labelTrain, cost, kernel, gamma, degree):
    model = SVC(C=cost, kernel=kernel, gamma=gamma, degree = degree)
    model.fit(dataTrain, labelTrain)
    return model, model.support_.shape[0]",0.4661208689,
857,hyper parameter marginalisation,"def fitL2LogisticRegression(ftrMtrx, rspVctr, initCff, stpSize, l2Penalty, maxItr):
    coefficients = np.array(initCff) # make sure it's a numpy array
    for itr in xrange(maxItr):
        # Predict P(y_i = +1|x_i,w) using your predctClassProbability() function
        ## YOUR CODE HERE
        predictions = predctClassProbability(ftrMtrx, coefficients)
        
        # Compute indicator value for (y_i = +1)
        indicator = (rspVctr == +1)
        
        # Compute the errors as indicator - predictions
        errors = indicator - predictions
        for j in xrange(len(coefficients)): # loop over each coefficient
            isIntercept = (j == 0)
            # Recall that feature_matrix[:,j] is the feature column associated with coefficients[j].
            # Compute the derivative for coefficients[j]. Save it in a variable called derivative
            ## YOUR CODE HERE
            derivative = cmptFeatureL2Derivative(errors, ftrMtrx[:,j], coefficients[j], 
                                                 l2Penalty, isIntercept)
            
            # add the step size times the derivative to the current coefficient
            ## YOUR CODE HERE
            coefficients[j] += stpSize * derivative
        
        # Checking whether log likelihood is increasing
        if  (itr <=    10) or                     \
            (itr <=   100 and itr %   10 == 0) or \
            (itr <=  1000 and itr %  100 == 0) or \
            (itr <= 10000 and itr % 1000 == 0) or \
            (itr %  10000 == 0):            
            lp = cmptL2LogLikelihood(ftrMtrx, rspVctr, coefficients, l2Penalty)
            print 'iteration %*d: log likelihood of observed labels = %.8f' % \
                (int(np.ceil(np.log10(maxItr))), itr, lp)
                
    return coefficients",0.4651799202,
857,hyper parameter marginalisation,"# Euclidean distance based cost function
def costEuclidean((U_in, C_in), Q, initialT, TK, dt, testT):
    U_inv, F, C, nN, nM = mU1C1.mU1C1(U_in, C_in, dt)
    # Calculate the future temperatures
    T = simfun.futureT(Q, initialT, TK, U_inv, F, C, nN, dt)
    err = T - testT                    # setpoint error
    return np.sqrt(np.dot(err.T, err)) # sqrt[sum(error^2)]

# NMBE based cost function [normalized mean bias error]
def costNMBE((U_in, C_in), Q, initialT, TK, dt, testT):
    U_inv, F, C, nN, nM = mU1C1.mU1C1(U_in, C_in, dt)
    # Calculate the future temperatures
    T = simfun.futureT(Q, initialT, TK, U_inv, F, C, nN, dt)
    err = T - testT                    # setpoint error
    return np.abs(np.sum(err))/(np.mean(testT)*(len(testT)-1))

# CV(RMSE) based cost function 
# [coefficient of variance of the root mean square error]
def costCVRMSE((U_in, C_in), Q, initialT, TK, dt, testT):
    U_inv, F, C, nN, nM = mU1C1.mU1C1(U_in, C_in, dt)
    # Calculate the future temperatures
    T = simfun.futureT(Q, initialT, TK, U_inv, F, C, nN, dt)
    err = T - testT                    # setpoint error
    return 1/np.mean(testT)*np.sqrt(np.dot(err.T, err)/(len(testT)-2))",0.4647839665,
857,hyper parameter marginalisation,"from scipy.optimize import minimize

def neg_log_like(params, y, gp):
    gp.set_parameter_vector(params)
    return -gp.log_likelihood(y)

initial_params = gp.get_parameter_vector()
bounds = gp.get_parameter_bounds()

r = minimize(neg_log_like, initial_params, method=""L-BFGS-B"", bounds=bounds, args=(y, gp))
gp.set_parameter_vector(r.x)
print(r)",0.4626735449,
129,build term document matrix,"import textmining
def term_document_dataframe(test_df):
    """""" Transforms a column with text into a set of
        dummy variables for each word in a dictionary.
    """"""
    
    # Initialize class to create term-document matrix
    tdm = textmining.TermDocumentMatrix()
    
    # Get word frequency in each document
    for doc in test_df:
        tdm.add_doc(doc)
        
    # Sets tdm into list data structure
    tdmMatrix = list(tdm.rows(cutoff=1))
    
    return pd.DataFrame(tdmMatrix[1:], columns=tdmMatrix[0])

#def retrieve_ta",0.5345659256,
129,build term document matrix,"def createIndex(docs):
    #builds inverted index based on documents
    index = {""$ N $"":0} # amount of documents in corpus
    print ""initializing index of"",len(docs),""users"" ,time.ctime(), 
    for i in docs:
        index[""$ N $""]+=1
        tfdic = docs[i]
        for w in tfdic:
            if index.has_key(w):
                index[w][i] = tfdic[w] # add document to the inverted index
            else:
                index[w] = {i:tfdic[w]}
    print    
    print ""index created with"",len(index),""words"", time.ctime()
    return index",0.5081665516,
129,build term document matrix,"def init():
    ""Initialize vector 'null' and symbol matrix""
    global Dict, Symb_mat, labels
    Dict = {}; Symb_mat = [] ; labels = []
    # Null vector 
    null = HDvector(N, 'null')  # Initialize by size and label
    # Store vector in global array, this has to be done only once...
    Symb_mat = np.array([null.getVec()])",0.507499814,
129,build term document matrix,"def tfidf_q(tqv):
    tqv_tfidf = []
    for i, t in enumerate(voc):
        # TODO compute TFIDF
        tf =
        # tqv[i] holds the raw frequency for term t
        tfidf = tqv[i]
        tqv_tfidf.append(tfidf)
    return tqv_tfidf",0.5026899576,
129,build term document matrix,"def ExtractTopicLMMatrix(lda):
    # Initialize the matrix
    docTopicProbs = numpy.zeros((lda.num_topics,lda.num_terms))
    for topicID in range(0,lda.num_topics):
        termProbsList = lda.get_topic_terms(topicID,lda.num_terms)
        for termProb in termProbsList:
            docTopicProbs[topicID,termProb[0]]=termProb[1]
    return docTopicProbs
    
# topicTermProbs[topicID,termID] --> P(term|topic)
topicTermProbs = ExtractTopicLMMatrix(lda)",0.4968271852,
129,build term document matrix,"# Create a function that calculates the idf
def calculate_idf():
    
    # Set global variables
    global processed_papers
    global corpus_words_freq
    
    # Start timer
    _start_time = time.time()
    
    idf_counter = 0
    total_texts = len(processed_papers)
    for paper in processed_papers.keys():
        for word in processed_papers[paper]['tf'].keys():
            processed_papers[paper]['idf'][word] = np.log(total_texts/corpus_words_freq[word])
        idf_counter += 1
        
        # Use this function to track processing state of IDFs
        if idf_counter % 5000 == 0:
            print idf_counter,'IDF\'s calculated.'
    
    # Stop timer
    _stop_time = time.time()
    print(""Finished IDF in: %s"" % str(datetime.timedelta(seconds=_stop_time-_start_time)))",0.4912915826,
129,build term document matrix,"#Counting the number of terms in each document D
     St = TD.transpose()*np.matrix('1; 1; 1; 1; 1; 1')
     #Making the sum of terms a diagonal matrix
     divMtx = np.identity(5)/np.diag(St.A1)
     divMtx[np.isnan(divMtx)] = 0
     print(divMtx)",0.4907572269,
129,build term document matrix,"def compute_tf_idf(docs):
    totalDocs = docs.count()
    docIds = docs.zipWithIndex()
    
    tokens = docs.flatMap(tokenize).distinct()
    tokenIds = tokens.zipWithIndex()

    tokensWithDocIds = docIds.flatMap(lambda (doc, docId): [(token, docId) for token in tokenize(doc)])
    idPairs = tokensWithDocIds.join(tokenIds).map(lambda (token, ids): (ids, 1))
    occurences = idPairs.reduceByKey(add)

    tokensByDoc = occurences.map(lambda ((docId, tokenId), occs): (docId, occs)).reduceByKey(add)
    tf = occurences.map(lambda ((docId, tokenId), occs): (docId, (tokenId, occs))) \
                    .join(tokensByDoc) \
                    .map(lambda (docId, ((tokenId, occs), totalOccs)): ((docId, tokenId), 1.0 * occs / totalOccs))

    docsWithToken = occurences.map(lambda ((docId, tokenId), occs): (tokenId, 1)).reduceByKey(add)
    idf = occurences.map(lambda ((docId, tokenId), occs): (tokenId, (docId, occs))) \
                    .join(docsWithToken) \
                    .map(lambda (tokenId, ((docId, occs), totalOccs)): ((docId, tokenId), log(totalDocs / totalOccs))) \
                    .filter(lambda ((docId, tokenId), idf): idf > 0.0)

    tf_idf = tf.join(idf).map(lambda ((docId, tokenId), (tf, idf)): ((docId, tokenId), tf * idf))
    
    return tf_idf, docIds, tokenIds",0.4901170731,
129,build term document matrix,"# Create a function that takes a filepath as an input
# and uses a tempory dictionary in order to feed 3 global variables
def process_harvested_file(filepath):
    
    # Set global variables
    global processed_papers
    global corpus_words_freq
    global abs_counter
    
    # Start timer
    _start_time = time.time()
    
    # Load file into a soup
    soup = BeautifulSoup(open(filepath), ""lxml"")
    records = soup.find_all('record')

    # Create the temporary dictionary with the variables needed
    for r in records:
        tmp_paper = dict()
        tmp_paper['identifier'] = r.header.identifier.contents[0].encode('utf-8')
        tmp_paper['title'] = r.metadata.title.contents[0].encode('utf-8')
        tmp_paper['abstract'] = r.metadata.abstract.contents[0].encode('utf-8')
        
        # Create new keys with empty dicts as values in the temporary dictionary
        tmp_paper['tf'] = list()
        tmp_paper['idf'] = dict()
        tmp_paper['tfidf'] = dict()

        # Use the tokenizer() function to tokenize the abstract
        tokenized = tokenizer(tmp_paper['abstract'])
        
        # Use the remove_stopwords() function to clean and lowercase tokens
        valuable_tokens = remove_stopwords(tokenized)
        
        # Use the token_lemmatizer() function to lemmatize tokens
        lemmatized_tokens = token_lemmatizer(valuable_tokens)
        
        # Create a new key in the temporary dictionary with the proccessed tokens
        tmp_paper['words'] = lemmatized_tokens

        # Feed counter with unique number of tokens
        for token in tmp_paper['words']:
            if token not in corpus_words_freq:
                corpus_words_freq[token] = 1
            else:
                corpus_words_freq[token] += 1

        
        # Feed the tf key with the frequency distance of the proccesed tokens
        tmp_paper['tf'] = nltk.FreqDist(tmp_paper['words'])
        processed_papers[tmp_paper['identifier']] = tmp_paper

        # Use this function to track processing state of abstracts
        abs_counter += 1
        if abs_counter % 1000 == 0:
            print abs_counter,'papers have been input into the engine'

    # Stop timer        
    _stop_time = time.time()
    print(""Finished harvesting file %s in: %s"" % (filepath.split(""/"")[-1], str(datetime.timedelta(seconds=_stop_time-_start_time))))",0.4888704121,
129,build term document matrix,"def optimize(fixed_cost = 0):
    F = fixed_cost * np.ones(N)
    obj = LinExpr(0.0);
    obj.addTerms((c[i,j] for i in DC for j in DA), (x[i,j] for i in DC for j in DA))
    obj.addTerms((F[i] for i in DC), (z[i] for i in DC))
    m.setObjective(obj, GRB.MINIMIZE)
    m.setParam('OutputFlag', 0)
    m.update()
    m.optimize()",0.4846853614,
415,data retrieval and dataset preprocessing,"def PreprocessDataset():
    from sklearn import preprocessing
    ## Load MNIST dataset of handwritten digits
    (x_train, y_train), (x_test, y_test) = mnist.load_data()
    ## Transform labels to one-hot encoding
    ## i.e., from '7' to [0,0,0,0,0,0,0,1,0,0]
    y_train = np_utils.to_categorical(y_train, 10)
    y_test = np_utils.to_categorical(y_test, 10)
    
    ## Process features. Set numeric type
    x_train = x_train.astype('float32')
    x_test = x_test.astype('float32')
    
    ## X_train has 6000 samples with 28x28 features
    ## Reshape from a matrix of 28 x 28 pixels to 1-D vector of 784 dimensions
    x_train = np.reshape(x_train, (60000, 784))
    x_test = np.reshape(x_test, (10000, 784))

    ################################################################
    # Activity 1 (Pre-processing):
    # Group A: w/o pre-processing datasets.
    #
    # Group B: Min-Max Normalize value to [0, 1]
    # x_train /= 255
    # x_test /= 255
    #
    # Group C: proceed w/ standardizing datasets by z-scoring (de-mean, uni-variance).
    # x_train = preprocessing.scale(x_train)
    # x_test = preprocessing.scale(x_test)
    ################################################################  
    ## YOUR TURN: CHANGE HERE
    x_train /= 255
    x_test /= 255

    return x_train, x_test, y_train, y_test

x_train, x_test, y_train, y_test = PreprocessDataset()
print(""x_train type: "" + str(x_train.shape))
print(""x_test type: "" + str(x_test.shape))
print(""y_train type: "" + str(y_train.shape))
print(""y_test type: "" + str(y_test.shape))",0.476398319,
415,data retrieval and dataset preprocessing,"%%time
if cfg.redo_preprocessing:
    # featurized data gets written here
    feature_data_split = os.path.join(cfg.data_dir, ""feature_data_split"")

    threads = []
    for x in [""training"", ""validation"", ""test""]:
        # create_local_dir(feature_data_split, x)
        for word in cfg.words_core + cfg.words_negative:
            # folder = create_local_dir(feature_data_split, x, word)
            print (""Feature engineering for set <%s> word <%s>"" % (x, word))
            wav_dir = ""/"".join([raw_data_split,     x, word])
            # full path gets created automatically inside extract_features function
            out_dir = ""/"".join([feature_data_split, x, word])
            thread = threading.Thread(name = x + ' ' + word, args = (wav_dir, out_dir, ), target = extract_features)
            threads += [thread]
            thread.start()

    # block the calling thread (this notebook cell) until all threads in the list complete
    for thread in threads:
        thread.join()",0.4609675407,
415,data retrieval and dataset preprocessing,"def PreprocessDataset():
    # Load dataset
    (x_train, y_train), (x_test, y_test) = mnist.load_data()
    # Set numeric type
    x_train = x_train.astype('float32')
    x_test = x_test.astype('float32')
    # Normalize value to [0, 1]
    x_train /= 255
    x_test /= 255
    # Transform lables to one-hot
    y_train = np_utils.to_categorical(y_train, 10)
    y_test = np_utils.to_categorical(y_test, 10)
    # Reshape: here x_train is re-shaped to [channel]  [width]  [height]
    # In other environment, the orders could be different; e.g., [height]  [width]  [channel].
    x_train = x_train.reshape(x_train.shape[0], 1, 28, 28)
    x_test = x_test.reshape(x_test.shape[0], 1, 28, 28)
    return [x_train, x_test, y_train, y_test]

x_train, x_test, y_train, y_test = PreprocessDataset()",0.4566820264,
415,data retrieval and dataset preprocessing,"def PreprocessDataset():
    # Load dataset
    (x_train, y_train), (x_test, y_test) = mnist.load_data()
    # Set numeric type
    x_train = x_train.astype('float32')
    x_test = x_test.astype('float32')
    # Normalize value to [0, 1]
    x_train /= 255
    x_test /= 255
    # Transform lables to one-hot
    y_train = np_utils.to_categorical(y_train, 10)
    y_test = np_utils.to_categorical(y_test, 10)
    
    # Reshape: here x_train is re-shaped to [width]  [height] x [channel]
    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)
    x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)
    return [x_train, x_test, y_train, y_test]

x_train, x_test, y_train, y_test = PreprocessDataset()",0.4566820264,
415,data retrieval and dataset preprocessing,"def load_dataset():
    # the data, shuffled and split between train and test sets
    (X_train, y_train), (X_test, y_test) = cifar10.load_data()
    
    # Allocate last 5000 training examples for validation.
    X_train, X_val = X_train[:-5000], X_train[-5000:]
    y_train, y_val = y_train[:-5000], y_train[-5000:]
    
    # convert class vectors to binary class matrices
    y_train = np_utils.to_categorical(y_train, nb_classes)
    y_test = np_utils.to_categorical(y_test, nb_classes)
    y_val = np_utils.to_categorical(y_val, nb_classes)
    
    # preprocess data
    X_train = X_train.astype('float32')
    X_test = X_test.astype('float32')
    X_val = X_val.astype('float32')
    X_train /= 255
    X_test /= 255
    X_val /= 255
    
    print('X_train shape:', X_train.shape)
    print(X_train.shape[0], 'train samples')
    print(X_test.shape[0], 'test samples')
    print(y_train.shape[0], 'training labels')
    print(y_test.shape[0], 'test labels')
    print(X_val.shape[0], 'validation samples')
    print(y_val.shape[0], 'validation labels')

    return X_train, y_train, X_test, y_test, X_val, y_val",0.4555329084,
415,data retrieval and dataset preprocessing,"def prepare_data():
    (x_train, y_train), (x_test, y_test) = mnist.load_data()
    x_train = x_train.reshape(60000, 784)
    x_test = x_test.reshape(10000, 784)
    x_train = x_train.astype(np.float32)/255.0
    x_test = x_test.astype(np.float32)/255.0
    y_train = np_utils.to_categorical(y_train)
    y_test = np_utils.to_categorical(y_test)
    return x_train, y_train, x_test, y_test",0.4494186044,
415,data retrieval and dataset preprocessing,"def data():
    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()
    # unroll image into a vector
    x_train = x_train.reshape(-1, 28 * 28)
    x_test = x_test.reshape(-1, 28 * 28)
    # scaling all values into [0, 1]
    x_train = x_train.astype('float32') / 255
    x_test = x_test.astype('float32') / 255
    
    nb_classes = 10
    y_train = np_utils.to_categorical(y_train, nb_classes)
    y_test = np_utils.to_categorical(y_test, nb_classes)
    return x_train, y_train, x_test, y_test",0.4484446347,
415,data retrieval and dataset preprocessing,"def load_mnist_dataset():
    import torchvision.datasets as datasets

    mnist_train = datasets.MNIST(root='./data/mnist', train=True, download=True, transform=None)
    mnist_test = datasets.MNIST(root='./data/mnist', train=False, download=True, transform=None)
    test_labels = np.array([mnist_test[i][1].numpy() for i in range(len(mnist_test))], dtype=np.int)
    train_labels = np.array([mnist_train[i][1].numpy() for i in range(len(mnist_train))], dtype=np.int)
    test = np.array([np.asarray(mnist_test[i][0]).reshape(28*28) for i in range(len(mnist_test))], dtype=np.float)
    train = np.array([np.asarray(mnist_train[i][0]).reshape(28*28) for i in range(len(mnist_train))], dtype=np.float)
    train /= 255.  # normalize data to be in range [0,1]
    test /= 255.
    return train, train_labels, test, test_labels, [28, 28]",0.4477418959,
415,data retrieval and dataset preprocessing,"def load_data():
    # load data
    (X_train, y_train), (X_test, y_test) = mnist.load_data()
    # reshape to be [samples][width][height][channels]
    X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')
    X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32')
    
    #One-hot encoding
    y_train = np_utils.to_categorical(y_train)
    y_test = np_utils.to_categorical(y_test)
    num_classes = y_test.shape[1]
    return (X_train, y_train, X_test, y_test, num_classes)",0.4470453858,
415,data retrieval and dataset preprocessing,"# Note: For whatever reason, I've experienced a bug with hyperas that
# prevents me from using any kind of comment in either the data() or
# model() function. For this reason I will attempt to describe the 
# code in both of these functions through comments and explanations
# outside of the functions themselves.
def data():
    (X_train, y_train), (X_test, y_test) = mnist.load_data()
    X_train = X_train.reshape(60000, 784)
    X_test = X_test.reshape(10000, 784)
    X_train = X_train.astype('float32')
    X_test = X_test.astype('float32')
    X_train /= 255
    X_test /= 255
    nb_classes = 10
    y_train = to_categorical(y_train, nb_classes)
    y_test = to_categorical(y_test, nb_classes)
    return X_train, y_train, X_test, y_test",0.4447437525,
1395,part d working with pandas dataframes,"data[['party', 'votes']].groupby('party').sum().head(10)",0.3909596801,
1395,part d working with pandas dataframes,"#didn't finish. just example of lambda
pd.pivot_table(df, index=['sex'], aggfunc=[lambda x: x.nunique(),np.mean, np.",0.3880379796,
1395,part d working with pandas dataframes,"table = pivot_table(df, values='D', index=['A', 'B'],
...                     columns=['C'], aggfunc=np.sum)",0.3830784559,
1395,part d working with pandas dataframes,df.groupby('date').sum()['n'].head(3),0.382722497,
1395,part d working with pandas dataframes,"%%R -o r_df2

r_df2 <- r_df %>% group_by(tailnum) %>% 
  summarise(count = n(), 
            dist = mean(distance, na.rm = TRUE),
            delay = mean(arr_delay, na.rm = TRUE)) %>%
  as.data.frame()

# r_df %>% group_by(tailnum, year, month) ...
# for more than one group column",0.3822119534,
1395,part d working with pandas dataframes,"#Review if a World Series was played in any particular year

years_ws = teams_df[['yearID','WSWin']].groupby('yearID', as_index=False).sum()
years_ws.head(16)",0.382155329,
1395,part d working with pandas dataframes,"(WA_bg_with_CountyName_wgs84
 .groupby([""CountyName""])
 .sum()
 .head(15))",0.3815606833,
1395,part d working with pandas dataframes,"for i in range(3):
    companies[i][""Volume""] = companies[i][""Volume""].str.replace("","", """").astype(int)
    companies[i].fillna(method=""ffill"", inplace=True)
    companies[i][""Volume""].replace(0, method=""ffill"", inplace=True)
    companies[i].dropna(how=""any"", inplace=True)
    to_csv(companies[i], abbr[i])",0.3812588453,
1395,part d working with pandas dataframes,"print data.corr().applymap(lambda x: ""{0:.3f}"".format(x))   # map makes it all fit on one line",0.3810846806,
1395,part d working with pandas dataframes,"union[['Sex','Title','Survived']].groupby(['Title','Sex']).sum().unstack('Sex')",0.3810546994,
1183,matrix,"# Function to return dummy variables for certain columns

# Company name was not included

def get_features(dataframe):
    
   features = patsy.dmatrix('~ C(State) + C(City)', dataframe)
   X_df = pd.DataFrame(features, columns=features.design_info.column_names)
   X_df.drop('Intercept', axis=1, inplace=True)
   return X_df",0.4668126404,
1183,matrix,"def with_bias(df):
    X = df.as_matrix()
    return np.hstack([np.ones((X.shape[0], 1)), X])",0.4650871158,
1183,matrix,"#Returns dense matrix
def get_dense_matrix(term_frquency):
    from scipy.sparse import csr_matrix
    dense_trm_frq=term_frquency.todense()

    return dense_trm_frq",0.4650238156,
1183,matrix,"def get_matrix(n):
    return np.matrix(np.zeros((n, n)))",0.4620772004,
1183,matrix,"def UnitA(A):
    A[:]=tnp.eye(A.shape[0])",0.4586775899,
1183,matrix,"def vortex_contribution_normal(panels):
    A = numpy.empty((panels.size, panels.size), dtype=float)
    numpy.fill_diagonal(A, 0.0)
    for i, panel_i in enumerate(panels):
        for j, panel_j in enumerate(panels):
            if i != j:
                A[i, j] = -0.5 / numpy.pi * integral(panel_i.xc, panel_i.yc, 
                                                     panel_j,
                                                     numpy.sin(panel_i.beta),
                                                     -numpy.cos(panel_i.beta))
    return A",0.4524715543,
1183,matrix,"def mat_to_np(mat):
    elements = mat.A
    return np.array([[elements[j * 4 + i] for i in range(4)] for j in range(4)])

def np_to_mat(np_mat):
    App.Matrix(*np_mat.flatten())

def vec_to_np(vec):
    return np.array(list(vec))",0.4521350861,
1183,matrix,"def convert_biom_to_pandas(table):


    feature_table = pd.DataFrame(np.array(table.matrix_data.todense()).T,index=table.ids(axis='sample'),columns=table.ids(axis='observation'))
    feature_ids = table.ids(axis='observation')
    mapping = {i: table.metadata(id=i, axis='observation')['taxonomy'] for i in feature_ids}
    for key, value in mapping.items():
        nvalue = ';'.join(value[1:])
        mapping.update({key:nvalue})
    taxonomy = pd.DataFrame(mapping, index=['taxonomy']).T
    
    
    return feature_table, taxonomy",0.4508924186,
1183,matrix,"def compare_tails_to_normal(X):
    # Define matrix to store comparisons
    A = np.zeros((2,4))    
    for k in range(4):             
        #stores tail probabilities of the sample series vs a normal series
        A[0, k] = len(X[X > (k + 1)]) / float(len(X)) # Estimate tails of X        
        A[1, k] = 1 - stats.norm.cdf(k + 1) # Compare to Gaussian distribution
    print 'Frequency of std events in X \n1: %s\t2: %s\t3: %s\t4: %s' % tuple(A[0])
    print 'Frequency of std events in a normal process \n1: %s\t2: %s\t3: %s\t4: %s' % tuple(A[1])
    return A

compare_tails_to_normal(X);",0.4494034648,
1183,matrix,"def read_counts():
    adata_counts = sc.read('./data/CountsNorm.csv', cache=True).T
    # this is not yet in sparse format, as the data was in a dense csv file
    from scipy.sparse import csr_matrix
    adata_counts.X = csr_matrix(adata_counts.X)
    adata_counts.write('./write/zebrafish_sparse_counts.h5ad')
# read_counts()",0.4454360604,
1114,loading and saving data,"def PreprocessDataset():
    # Load dataset
    (x_train, y_train), (x_test, y_test) = mnist.load_data()
    # Set numeric type
    x_train = x_train.astype('float32')
    x_test = x_test.astype('float32')
    # Normalize value to [0, 1]
    x_train /= 255
    x_test /= 255
    # Transform lables to one-hot
    y_train = np_utils.to_categorical(y_train, 10)
    y_test = np_utils.to_categorical(y_test, 10)
    
    # Reshape: here x_train is re-shaped to [width]  [height] x [channel]
    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)
    x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)
    return [x_train, x_test, y_train, y_test]

x_train, x_test, y_train, y_test = PreprocessDataset()",0.4371684194,
1114,loading and saving data,"def PreprocessDataset():
    # Load dataset
    (x_train, y_train), (x_test, y_test) = mnist.load_data()
    # Set numeric type
    x_train = x_train.astype('float32')
    x_test = x_test.astype('float32')
    # Normalize value to [0, 1]
    x_train /= 255
    x_test /= 255
    # Transform lables to one-hot
    y_train = np_utils.to_categorical(y_train, 10)
    y_test = np_utils.to_categorical(y_test, 10)
    # Reshape: here x_train is re-shaped to [channel]  [width]  [height]
    # In other environment, the orders could be different; e.g., [height]  [width]  [channel].
    x_train = x_train.reshape(x_train.shape[0], 1, 28, 28)
    x_test = x_test.reshape(x_test.shape[0], 1, 28, 28)
    return [x_train, x_test, y_train, y_test]

x_train, x_test, y_train, y_test = PreprocessDataset()",0.4371684194,
1114,loading and saving data,"def load_dataset():
    # the data, shuffled and split between train and test sets
    (X_train, y_train), (X_test, y_test) = cifar10.load_data()
    
    # Allocate last 5000 training examples for validation.
    X_train, X_val = X_train[:-5000], X_train[-5000:]
    y_train, y_val = y_train[:-5000], y_train[-5000:]
    
    # convert class vectors to binary class matrices
    y_train = np_utils.to_categorical(y_train, nb_classes)
    y_test = np_utils.to_categorical(y_test, nb_classes)
    y_val = np_utils.to_categorical(y_val, nb_classes)
    
    # preprocess data
    X_train = X_train.astype('float32')
    X_test = X_test.astype('float32')
    X_val = X_val.astype('float32')
    X_train /= 255
    X_test /= 255
    X_val /= 255
    
    print('X_train shape:', X_train.shape)
    print(X_train.shape[0], 'train samples')
    print(X_test.shape[0], 'test samples')
    print(y_train.shape[0], 'training labels')
    print(y_test.shape[0], 'test labels')
    print(X_val.shape[0], 'validation samples')
    print(y_val.shape[0], 'validation labels')

    return X_train, y_train, X_test, y_test, X_val, y_val",0.4358944595,
1114,loading and saving data,"def load_mnist():
    (X_train, y_train), (X_test, y_test) = mnist.load_data()
    return X_train, X_test, y_train, y_test",0.4284810722,
1114,loading and saving data,"def load_data():
    """"""
    This function loads the MNIST digit data using Keras inbuilt function \
    and returns the train and test set
    """"""
    
    (X_train, y_train), (X_test, y_test) = mnist.load_data()
    return (X_train, y_train), (X_test, y_test)",0.4273109436,
1114,loading and saving data,"def load_and_prepare_data():
    print('Loading data...')
    
    # Keras provides MNIST dataset as two tuples for training and testing sets.
    # https://keras.io/datasets/#mnist-database-of-handwritten-digits
    (X_train, y_train), (X_test, y_test) = mnist.load_data()
    
    print('Done.')
    print('Preparing data...')

    # The input datasets should have values between 0-1 instead of having a range of 0-255.
    # This requires typecasting them from int to float at first.
    X_train = X_train.astype('float32')
    X_test = X_test.astype('float32')
    X_train /= 255
    X_test /= 255

    # The output datasets should be converted from simply having a value between 0-9 to having
    # a matrix of categorical one-hot encoding. Example:
    # 2 -> [0 0 1 0 0 0 0 0 0 0]
    # 9 -> [0 0 1 0 0 0 0 0 0 9]
    y_train = np_utils.to_categorical(y_train, 10)
    y_test = np_utils.to_categorical(y_test, 10)

    # The MINST input is 28x28 grayscale images. Let's convert them to single row vectors.
    X_train = np.reshape(X_train, (60000, 784))
    X_test = np.reshape(X_test, (10000, 784))

    print('Done.')
    return [X_train, X_test, y_train, y_test]",0.4261793494,
1114,loading and saving data,"def prepare_data():
    (x_train, y_train), (x_test, y_test) = mnist.load_data()
    x_train = x_train.reshape(60000, 784)
    x_test = x_test.reshape(10000, 784)
    x_train = x_train.astype(np.float32)/255.0
    x_test = x_test.astype(np.float32)/255.0
    y_train = np_utils.to_categorical(y_train)
    y_test = np_utils.to_categorical(y_test)
    return x_train, y_train, x_test, y_test",0.4132888317,
1114,loading and saving data,"def load_data():
    (x_train, y_train), (x_test, y_test) = mnist.load_data()
    
    # plot 4 images as gray scale
    plt.subplot(221)
    plt.imshow(x_train[0], cmap=plt.get_cmap('gray'))
    plt.subplot(222)
    plt.imshow(x_train[1], cmap=plt.get_cmap('gray'))
    plt.subplot(223)
    plt.imshow(x_train[2], cmap=plt.get_cmap('gray'))
    plt.subplot(224)
    plt.imshow(x_train[3], cmap=plt.get_cmap('gray'))
    # show the plot
    plt.show()
    
    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)
    x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)
    x_train = x_train.astype('float32')
    x_test = x_test.astype('float32')
    x_train /= 255
    x_test /= 255
    y_train = keras.utils.to_categorical(y_train, 10)
    y_test = keras.utils.to_categorical(y_test, 10)
    return x_train, y_train, x_test, y_test",0.4068384469,
1114,loading and saving data,"filename = 'FIWT_Exp014_20150601105647.dat.npz'

def loadData():
    # Read and parse raw data
    global exp_data
    exp_data = np.load(filename)

    # Select colums
    global T_rig, phi_rig
    T_rig = exp_data['data44'][:,0]
    phi_rig = exp_data['data44'][:,2]

loadData()",0.4066384435,
1114,loading and saving data,"try:
    # read in data from H1 and L1, if available:
    strain_H1, time_H1, chan_dict_H1 = rl.loaddata(fn_H1, 'H1')
    strain_L1, time_L1, chan_dict_L1 = rl.loaddata(fn_L1, 'L1')
except:
    print(""Cannot find data files!"")
    print(""You can download them from https://losc.ligo.org/s/events/""+eventname)
    print(""Quitting."")
    quit()",0.4064782262,
509,due 11 00 p m january,"def make_birthday_intro(name, birth_date):
    """"""function takes in two arguments: a name (string), and a birth date (date), and returns:
    a string of the format ""Hello, my name is {NAME} and I'm {AGE} years old. 
    In {N} days I'll be {NEW_AGE}"" (replacing {NAME}, {AGE}, {N}, and {NEW_AGE} with appropriate values).""""""
    
    age = relativedelta(date.today(), birth_date)    # calculate the age
    
    days_to_new_age = (birth_date + relativedelta(years = age.years + 1) - date.today()).days    # days to go for next birthday  
    
    # print statement
    return(make_introduction(name, age.years) + "" In "" + str(days_to_new_age) + "" days I'll be "" + str(age.years + 1) + ""."")",0.4206089973,
509,due 11 00 p m january,"def before_holiday(date_time):
    '''
    Finds out whether a given date falls 5 days or less before a holiday (an adoption that took place up to 5 days before
    the holiday, is considered here as a ""holiday gift"").
    If yes, returns the holiday's name, otherwise None.
    '''
    i = 0
    for holiday_date in holidays.HolidayDate:
        dates_diff = holiday_date - date_time
        if dates_diff.days > 5:  
            return 'No Holiday' 
        if dates_diff.days >= 0:
            return holidays.iloc[i,0]
        i += 1
    return 'No Holiday'

outcomes['Holiday'] = outcomes['DateTime'].apply(before_holiday)",0.4143175483,
509,due 11 00 p m january,"def generate_random_birthday(min_age, max_age):
    """""" Generate a random birthday between min and max age""""""
    today = datetime.date.today() # Get todays date
    startdate = today - datetime.timedelta(max_age * 365) # Subtract max_age to get startdate for random bdays
    enddate = today - datetime.timedelta(min_age * 365)
    number_of_days = enddate - startdate
    
    return startdate+datetime.timedelta(np.random.randint(1,number_of_days.days))

def lower_rand_int(min_int, max_int, power):
    """""" Generate a random int with a higher probability of a lower number""""""
    return int(min_int+(max_int-min_int)*np.random.rand()**power)",0.4118010402,
509,due 11 00 p m january,"def current_month_start():
    start = dt.date.today()
    if start.day > 25:
        start += dt.timedelta(7)
    return start.replace(day=1)

use_slider = True
if use_slider:
#     period = get_period(df, start_slider.value, end_slider.value)
    period = get_period(df, pd.Timestamp('2017-03-01'), pd.Timestamp('2017-04-30'))
else:
    period = get_period(df, current_month_start())",0.4102064967,
509,due 11 00 p m january,"#Function to create a monthdelta method which returns the latest 20 months
def monthdelta(date, delta):
    m, y = (date.month+delta) % 12, date.year + ((date.month)+delta-1) // 12
    if not m: m = 12
    d = min(date.day, [31,29 if y%4==0 and not y%400==0 else 28,31,30,31,30,31,31,30,31,30,31][m-1])
    return date.replace(day=d,month=m, year=y)

List_of_last_20_months = []
for m in range(-20, 0):
    List_of_last_20_months.append('{0:%Y%m}'.format(monthdelta(datetime.datetime.now(), m)))
    
    
List_of_links = [""https://s3.amazonaws.com/tripdata/JC-""+s+ '-citibike-tripdata.csv.zip' for s in List_of_last_20_months]
#Changing the link manually since it doesnt follow the pattern
List_of_links[4]=""https://s3.amazonaws.com/tripdata/JC-201708%20citibike-tripdata.csv.zip""
List_of_links",0.4093024731,
509,due 11 00 p m january,"def get_configuration_day_data(date):
    date_formatted = date.strftime('%Y-%m-%d')
    logger.info('Downloading data for {0}'.format(date_formatted))
    
    url = 'https://rata.digitraffic.fi/api/v1/compositions/{0}'.format(date_formatted)
    logger.info('\tURL: {0}'.format(url))
    
    r = requests.get(url)
    data = json.loads(r.text)
    return data",0.4069461823,
509,due 11 00 p m january,"def get_day_data(date):
    date_formatted = date.strftime('%Y-%m-%d')
    logger.info('Downloading data for {0}'.format(date_formatted))
    
    url = 'https://rata.digitraffic.fi/api/v1/trains/{0}'.format(date_formatted)
    logger.info('\tURL: {0}'.format(url))
    
    r = requests.get(url)
    data = json.loads(r.text)
    return data",0.4069461823,
509,due 11 00 p m january,"!ssh mlb@10.195.223.53 ""hive -e \""use cpk; select * from mlb_bobcat_raw \
                        where model = 'Agera'\
                        and day between '2015-10-26' and '2015-11-07';\"""" \
                        > Data/Bobcat_20151026.log",0.4066425562,
509,due 11 00 p m january,"def Pnosame(n):
    '''Probability no one out of n people have the same birthday.'''
    P = 1
    for a in range(365-n+1, 365):
        P *= a/365
    return P",0.4005752504,
509,due 11 00 p m january,"@z.curry
def new_year(newyear, d):
    ""Given date object, return copy with same date, but year `newyear`""
    (_, m, day) = d.timetuple()[:3]
    return dt.date(newyear, m, day)


plt.figure(figsize=(16, 6))
for yr, yeardf_ in dfwkday.groupby('Year'):
    yeardf = yeardf_.copy()
    yeardf.index = yeardf.index.map(new_year(2000))  # Changing all years to 2000 for easier overlay
    
    am = get_peak(yeardf[['6', '7', '8', '9', '10']])
    lunch = get_peak(yeardf[['10', '11', '12', '13', '14']])
    pm = get_peak(yeardf[['14', '15', '16', '17', '18', '19']])
    
    plot_peak_smoothed(am)
    plot_peak_smoothed(lunch)
    plot_peak_smoothed(pm)
rotate(75)",0.3991743922,
1363,parse a single incident entry,"def add_dependency_child_feats(result, event):
    """"""
    Append to the `result` dictionary features based on the syntactic dependencies of the event trigger word of
    `event`. The feature keys should have the form ""Child: [label]->[word]"" where ""[label]"" is the syntactic label
    of the syntatic child (e.g. ""det"" in the case above), and ""[word]"" is the word of the syntactic child (e.g. ""The"" 
    in the case above).
    Args:
        result: a defaultdict that returns `0.0` by default. 
        event: the event for which we want to populate the `result` dictionary with dependency features.
    Returns:
        Nothing, but populates the `result` dictionary. 
    """"""
    index = event.trigger_index
    if len(event.sent.children[index]) > 0: 
        for child,label in event.sent.children[index]:
            child_name = event.sent.tokens[child]['word']
            result[""Child: "" + label + ""->"" + child_name] += 1.0",0.3679642081,
1363,parse a single incident entry,"# This creates a set of street names for different types of streets where the name does not fit to the 
# expected street name pattern. Thus the function helps to identify streetname of bad quality.
# INPUT: type_set - set of different streets
# INPUT: value - streetname
def audit_street_type(type_set, value):
    naming = street_type_re.search(value)
    if naming:
        stype = naming.group()
        found = False
        for k in street_name_mapping.keys():
            if ((k in value) or (street_name_mapping[k] in value) or (stype in street_name_expected)):
                found = True
        if not found:
            type_set[stype].add(value)",0.360437274,
1363,parse a single incident entry,"def audit_street_type(street_types, street_name):
    m = get_last_word.search(street_name)
    if m:
        street_type = m.group()
        if street_type not in expected:
            street_types[street_type].add(street_name)",0.3599960208,
1363,parse a single incident entry,"# Sorting zipcodes in different forms and save them to dict
def audit_zip_type(zip_types, zip):
    m = zip_type_re.search(zip)
    if m:
        zip_type = m.group()
        if zip_type not in zip_types:
            zip_types[zip_type].add(zip)
    else:
        zip_types['unknown'].add(zip)",0.3599960208,
1363,parse a single incident entry,"def audit_street_type(street_types, street_name):
    m = street_type_re.search(street_name)
    if m:
        street_type = m.group()
        street_types[street_type]+=1",0.3599960208,
1363,parse a single incident entry,"# Adding street names in dictionary by type
# Takes 2 arguments: dictionary and string. If string doesn't match pattern adds it to dictionary.
def audit_street_type(street_types, street_name):
    m = street_type_re.search(street_name)
    if m:
        street_type = m.group()
        if street_type not in expected:
            street_types[street_type].add(street_name)",0.3599960208,
1363,parse a single incident entry,"def audit_street_type(street_types, street_name):
    m = street_type_re.search(street_name)
    if m:
        street_type = m.group()
        if street_type not in EXPECTED_STREETS:
            street_types[street_type].add(street_name)",0.3599960208,
1363,parse a single incident entry,"def audit_street_type(street_types, street_name):
    m = street_type_re.search(street_name)
    if m:
        street_type = m.group()
        street_types[street_type] += 1",0.3599960208,
1363,parse a single incident entry,"def audit_phone_type(phone_types, phone):
    m = phone_type_re.search(phone)
    if m:
        phone_type = m.group()
        if phone_type not in phone_types:
            new_phone = phone_re.sub('', phone_type)
            new_phone = ('+1-' + new_phone[:3] + '-' +
                         new_phone[3:6] + '-' + new_phone[6:])
            phone_types[new_phone].add(phone)
    else:
        phone_types['unknown'].add(phone)",0.3599960208,
1363,parse a single incident entry,"def audit_street_type(street_types, street_name):
    m = street_type_re.search(street_name)
    if m:
        street_type = m.group()
        if street_type not in expected:
            street_types[street_type].add(street_name)",0.3599960208,
1139,machine learning and statistics for physicists,"print ('At timestep 0: ',sum(s1.history.ism_elem_yield[0])/1e7,sum(s2.history.ism_elem_yield[0])/1e8,sum(s3.history.ism_elem_yield[0])/1e9)
print ('At timestep 0: ',sum(s1.history.ism_iso_yield[0])/1e7,sum(s2.history.ism_iso_yield[0])/1e8,sum(s3.history.ism_iso_yield[0])/1e9)",0.4744466841,
1139,machine learning and statistics for physicists,"print ('At last timestep, should be the same fraction: ',sum(s1.history.ism_elem_yield[-1])/1e7,sum(s2.history.ism_elem_yield[-1])/1e8,sum(s3.history.ism_elem_yield[-1])/1e9)
print ('At last timestep, should be the same fraction: ',sum(s1.history.ism_iso_yield[-1])/1e7,sum(s2.history.ism_iso_yield[-1])/1e8,sum(s3.history.ism_iso_yield[-1])/1e9)",0.4650020003,
1139,machine learning and statistics for physicists,"em = fairnessMeasures.envyMatrix(p2)
print(em)
print(""There are "", fairnessMeasures.nbEnviousAgents(em), "" envious agents"")
print(""The maximum envy among two agents is "", fairnessMeasures.maxEnvy(em))",0.4455037713,
1139,machine learning and statistics for physicists,"pt_leptons = []
for event_num in xrange(1000):              # loop over the events
    data.GetEntry(event_num)                # read the next event into memory
    num_leptons = data.lep_n                # number of leptons in the event
    for lepton_num in xrange(num_leptons):  # loop over the leptons within this event
        pt_lepton = data.lep_pt[lepton_num] # get the pT of the next lepton...
        pt_leptons.append(pt_lepton)        # ... and add it to the list
        
n, bins, patches = plt.hist(pt_leptons)
plt.xlabel('Lepton pT [MeV]')
plt.ylabel('Events per bin')

n_entries = int(sum(n))
print(""Number of entries = {}"".format(n_entries))",0.4372214079,
1139,machine learning and statistics for physicists,"r_insulation_score, c_insulation_score, dnases = [], [], []
for celltype in celltypes:
    r_insulation_score.append(numpy.load('insulation/chr21.{}.insulation.rambutan.5000.npy'.format(celltype))[3500:9500])
    c_insulation_score.append(numpy.load('insulation/chr21.{}.insulation.5000.npy'.format(celltype))[3500:9500])
    
    d = numpy.load('dnase/chr21.{}.dnase.npy'.format(celltype))
    dnase = numpy.zeros(9626)
    for i in range(9626):
        dnase[i] = d[i*5000:(i+1)*5000].mean()
    dnase[dnase == 0] = 1
    dnase = numpy.log(dnase) / numpy.log(dnase).std()
    dnases.append(dnase[3500:9500])
    
idx = [(c != 0) & (r != 0) for c, r in zip(c_insulation_score, r_insulation_score)] 
r_insulation_score = [r_insulation_score[i][idx[i]] for i in range(6)]
c_insulation_score = [c_insulation_score[i][idx[i]] for i in range(6)]
dnases = [dnases[i][idx[i]] for i in range(6)]

r_insulation_score = [numpy.log(r_insulation_score[i]) - numpy.log(r_insulation_score[i].mean()) for i in range(6)]
c_insulation_score = [numpy.log(c_insulation_score[i]) - numpy.log(c_insulation_score[i].mean()) for i in range(6)]

zr_insulation_score = [(r_insulation_score[i] - r_insulation_score[i].mean()) / r_insulation_score[i].std() for i in range(6)]
zc_insulation_score = [(c_insulation_score[i] - c_insulation_score[i].mean()) / c_insulation_score[i].std() for i in range(6)]",0.4314760268,
1139,machine learning and statistics for physicists,"IncomeA=ProjectA*10+ProjectA*ResPerProject*20-Risk*ProjectA*50
IncomeB=ProjectB*20+ProjectB*ResPerProject*20-Risk*ProjectB*40
TotalA2=TotalA1+dt*IncomeA # Euler integration
TotalB2=TotalB1+dt*IncomeB # Euler integration
TotalA.append(TotalA2)
TotalB.append(TotalB2)",0.4312734306,
1139,machine learning and statistics for physicists,"#MAE
UBC_MAE=(1/4)*(abs(rCBD-1)+abs(rCEC-2)+abs(rCBL-2)+abs(rCEL-4))
UBP_MAE=(1/4)*(abs(rPBD-1)+abs(rPEC-2)+abs(rPBL-2)+abs(rPEL-4))
IBC_MAE=(1/4)*(abs(rCDB-1)+abs(rCCE-2)+abs(rCLB-2)+abs(rCLE-4))
IBP_MAE=(1/4)*(abs(rPDB-1)+abs(rPCE-2)+abs(rPLB-2)+abs(rPLE-4))
UBC_MAE,UBP_MAE,IBC_MAE,IBP_MAE",0.4304927588,
1139,machine learning and statistics for physicists,"basis_functions = [lambda t:cos(2*pi*(t-1985)), \
                   lambda t:sin(2*pi*(t-1985)), \
                   lambda t:1, \
                   lambda t:t-1985, \
                   lambda t:(t-1985)**2]

A = make_A(t, basis_functions)
print ""A has shape %i by %i"" % A.shape

Q,R = qr(A)
c = solve(R, dot(Q.T,y))
print ""Coefficients using QR: "",c

# Make fit and plot:

y_fit = dot(A,c)

figure(figsize=(13,4))
subplot(1,2,1)
plot(t,y,'b',label='data')
plot(t,y_fit,'r',label='fit')


subplot(1,2,2)
plot(t, y - y_fit)
title(""Residual"")
print ""Sum of squares of resisuals is %g"" % norm(y-y_fit,2)**2

# Remove oscillations and plot trend:
c2 = c.copy()
c2[0] = 0.
c2[1] = 0.
y_trend = dot(A,c2)
subplot(1,2,1)
plot(t, y_trend, 'g', label='trend')
legend(loc='upper left');",0.4290315211,
1139,machine learning and statistics for physicists,"print 'Grouped MSE on MLPC: ', mean_squared_error(grouped_by_business.yelp_avg_rating, grouped_by_business.MLPC_predicted_avg_rating)
print 'Grouped MSE on MLPR: ', mean_squared_error(grouped_by_business.yelp_avg_rating, grouped_by_business.MLPR_predicted_avg_rating)",0.4282518923,
1139,machine learning and statistics for physicists,"show(0, cef_yield, get_yield_rolling(ntr(YYY)), get_yield_rolling(ntr(PCEF)), get_yield_rolling(ntr(cjb))-0.23, get_yield_rolling(ntr(JNK)), ta=False)",0.4279738665,
430,dataset brain csv,"with h5py.File('FillValue.hdf5', 'w') as f:
    dseti = f.create_dataset('empty_ints', shape = (1, 2), dtype = 'int16')
    dsetf = f.create_dataset('empty_floats', shape = (1, 2), dtype = 'float32')
    arri = dseti[:]
    arrf = dsetf[:]
    print('1. Using Defaults: arri, ', arri, 'arrf, ', arrf)
    dseti = f.create_dataset('fillv_ints', shape = (1, 2), dtype = 'int16', fillvalue = -1)
    dsetf = f.create_dataset('fillv_floats', shape = (1, 2), dtype = 'float32', fillvalue = float('nan'))
    arri = dseti[:]
    arrf = dsetf[:]
    print('2. Using FillValues: arri, ', arri, 'arrf, ', arrf)",0.5261508822,
430,dataset brain csv,"if rank == 0:
    data = numpy.loadtxt(outputPath + ""ParticlePosition.txt"",skiprows=1)
    arrTime = data[:,0]
    arrW = data[:,1]
    arrdWdt = data[:,2]

    plt.clf()
    plt.plot(arrTime,arrW,label='w')
    plt.plot(arrTime,arrdWdt,label='dw/dt')
    plt.xlabel('Time')
    plt.ylabel('Perturbation Displacement / Velocity')
    plt.legend(loc=2)
    
    plt.savefig(outputPath + 'VelocityDisplacementTime.pdf')",0.5226350427,
430,dataset brain csv,"def get_feature_dataframe(sample_file, features):
    """"""
    Create feature table from the sample file which has flow information
    
    Parameters
    ----------
    
    sample_file: string
        file(full path) containing IP layer information destination address, 
        protocol, time stamp.
        
    features: list
        list of feature we required for anlysis.
    """"""
    df = pd.read_csv(sample_file, index_col=0)
    #Filter Columns
    df = df[['ip.dst', 'ip.proto', 'sniff_timestamp', 'sample']]
    #Remove null destinations
    df = df[df['ip.dst'].notnull()]
    #Rename Columns
    df.columns = ['ip', 'protocol', 'time_stamp', 'sample']
    #Get count for each ip
    df = df.groupby(['ip', 'protocol']).size().unstack().fillna(0).astype(int)
    #Drop row for given IP
    #df = filter_ip(df, '147.32.84.165')        
    if(set(df.columns) != set(features)):
        non_columns = set(features) - set(df.columns)
        for c in non_columns:
            df.insert(loc=features.index(c), column=c, value=0)
    #Select only required protocols which would be used as features
    df = df[features]
    return df",0.5222955942,
430,dataset brain csv,"def readRNA(filepath):
    """"""
    Takes the path to preprocessed Illumina HiSeq RNASeq data, 
    reads in the data and drops unneccesary variables
    Input: string-- path to file 
    Output: pandas dataframe
    """"""
    rna = pd.read_table(filepath,
                  sep='\t', na_values='NA',low_memory=False )
    rna = rna.drop(['pat.bcr','stage', 'class.y', 
                    'patient.stage_event.pathologic_stage'],axis=1)
    return rna

rnafile = 'D:\Insight\RNASeq2.txt'
rna = readRNA(rnafile)
print(rna.head())
print(rna.shape)",0.5213403702,
430,dataset brain csv,"#group all fields by city and bin in days calculating the mean and errors for taxi and delay
def create_airport_histogram(path_csv, sort_field):
    """"""""Takes a filename for a CSV file which gets parsed into a pandas
    frame. Then the rows gets grouped according to DAY_OF_MONTH values
    from which a mean is generated for DEP_DELAY and a count sum for 
    CRS_DEP_TIME. The group index, mean and sum get returned as new frame.
    params:
    path_csv = file path to csv file
    output: returns new frame
    """"""
    df = pd.read_csv(path_csv)
    df = df.drop('Unnamed: 0', 1)
    df['CARRIER_STRING'] = df['CARRIER_STRING'].str.replace(r'US Airways.+', 'US Airways Inc.')
    df_out = df.groupby('CARRIER_STRING').agg([np.mean, np.std]).sort(
        [(sort_field, 'mean')])
    new_header = [""Taxi Mean"", ""Taxi Error"", 
                  ""Carrier Mean"", ""Carrier Error""]
    df_out = df_out[1:] #take the data less the header row
    df_out.columns = new_header #set the header row as the df header
    return df_out",0.5206036568,
430,dataset brain csv,"#How far from edge of hand to make box?
boxBorder = 8
    
for imageDict in iDLabeled:
    minY = min(imageDict['handPoints'][:,0])
    minX = min(imageDict['handPoints'][:,1])
    maxY = max(imageDict['handPoints'][:,0])
    maxX = max(imageDict['handPoints'][:,1])
    
    xRange = arange(minX-boxBorder, maxX+1+boxBorder)
    yRange = arange(minY-boxBorder, maxY+1+boxBorder)

    top = np.vstack(([(minY-boxBorder)*np.ones(len(xRange)), xRange])).T
    bottom = np.vstack(([(maxY+boxBorder)*np.ones(len(xRange)), xRange])).T
    left = np.vstack(([yRange, (minX-boxBorder)*np.ones(len(yRange))])).T
    right = np.vstack(([yRange, (maxX+boxBorder)*np.ones(len(yRange))])).T
    
    box = np.vstack(([top, bottom, left, right]))
    
    imageDict['handEdges'] = [minX, maxX, minY, maxY]
    imageDict['boxEdges'] = [minX-boxBorder, maxX+boxBorder, minY-boxBorder, maxY+boxBorder]
    boxHeight = ((maxY+boxBorder)- (minY-boxBorder))
    boxWidth = ((maxX+boxBorder)- (minX-boxBorder))
    
    imageDict['boxHeight'] = boxHeight
    imageDict['boxWidth'] = boxWidth
    imageDict['numPointsInBox'] = boxHeight*boxWidth
                                  
    imageDict['box'] = box",0.5203632116,
430,dataset brain csv,"with h5py.File(CROWDASTRO_H5_PATH, 'r') as crowdastro_h5:
    atlas_images = crowdastro_h5['/atlas/cdfs/numeric'][:, 2 : 2 + IMAGE_SIZE]
    
    images = []
    one_maximum = []

    for i, image in enumerate(atlas_images):
        image = image.reshape((200, 200))[60:140, 60:140]
        maxima = skimage.feature.peak_local_max(image, threshold_rel=0.2)
        
        if len(maxima) == 1:
            images.append(image)
            one_maximum.append(i)

print('Found {} radio sources with one local maximum.'.format(len(one_maximum)))",0.5193037391,
430,dataset brain csv,"export_graphviz(tree, 
                out_file=""./images/iris_tree.dot"",
                class_names = iris[""target_names""],
                feature_names = iris[""feature_names""],
                impurity = False,
                filled = True)",0.5186569691,
430,dataset brain csv,"with h5py.File(CROWDASTRO_H5_PATH) as f_h5:
    positions = f_h5['/swire/cdfs/catalogue'][:, :2]
    
    times = []
    for i in range(1000):
        now = time.time()
        sx, sy = f_h5['/atlas/cdfs/positions'][i]

        lt_x = positions[:, 0] <= sx + ARCMIN
        gt_x = positions[:, 0] >= sx - ARCMIN
        lt_y = positions[:, 1] <= sy + ARCMIN
        gt_y = positions[:, 1] >= sy - ARCMIN
        enclosed = numpy.all([lt_x, gt_x, lt_y, gt_y], axis=0)
        potential_hosts = positions[enclosed]
        total = time.time() - now
        times.append(total)

    print('{:.02} +- {:1.1} s'.format(numpy.mean(times), numpy.std(times)))",0.5185279846,
430,dataset brain csv,"''Function to load the HMDA dataset'''
def hmda_init(loan_filepath,institution_filepath):

    inst_data = pd.read_csv(institution_filepath,low_memory= False)
    loan_data = pd.read_csv(loan_filepath,low_memory= False)

    tmp_filter_df = pd.DataFrame()
    tmp_filter_df['Respondent_Name_TS'] = pd.DataFrame(inst_data['Respondent_Name_TS'])
    loan_col_list = list(loan_data)
    inst_col_list = list(inst_data)
    common_list = list(set(loan_col_list).intersection(inst_col_list))
    for common_column in common_list:
        tmp_filter_df[str(common_column)] = pd.DataFrame(inst_data[str(common_column)])

    print ""Merging two datasets""
    join_data = loan_data.merge(tmp_filter_df, how = 'left', on = common_list)
    print ""Loaded successfully.""
    
    return join_data,inst_data",0.5185028315,
341,create a list to store guesses,"import time

#-- Sequence of angles
seq = [40, 0, 20, -40, -80, 0]

#-- Repeat the sequence n times
for n in range(2):
    for ang in seq:
        a.pos = ang
        time.sleep(0.8)",0.4130865932,
341,create a list to store guesses,"def add_thing_to_some_list(thing, some_list=[]):
    some_list.append(thing)
    return some_list

list_with_one_thing = add_thing_to_some_list(99)
list_with_one_thing",0.4123357534,
341,create a list to store guesses,"def unique_pos(Series):
    
    allpos = [token.pos_ for token in Series]
        
    return (len(set(allpos)))",0.4104164839,
341,create a list to store guesses,"sflag = [True]
imgs  = [img_L, img_L + img_hpf]

def toggle(event):
    """"""
    Toggle between original image and sharpened images.
    """"""
    
    # -- if the ""n"" key is pressed
    if event.key == "" "":

        # flip the display flag
        sflag[0] = ~sflag[0]
        
        # reset the data
        im6.set_data(imgs[sflag[0]])
        fig6.canvas.draw()
        
dum = fig6.canvas.mpl_connect(""key_press_event"", toggle)",0.409217,
341,create a list to store guesses,"def simple(x, y=[]):
    y.append(x)
    return y",0.4071908593,
341,create a list to store guesses,"def add_to_list(val, seq=[]):
    seq.append(val)
    return seq",0.4071908593,
341,create a list to store guesses,"def append_to(element, to=[]):
    to.append(element)
    return to",0.4071908593,
341,create a list to store guesses,"def bad_append(new_item, a_list=[]):
    a_list.append(new_item)
    return a_list",0.4071908593,
341,create a list to store guesses,"def f4(a, n=[]):
    n.append(a)
    return n",0.4071908593,
341,create a list to store guesses,"def cartesian(G):
    return [{'cartesianLayout': [
        {'node': n, 'x': float(G.pos[n][0]), 'y': float(G.pos[n][1])}
        for n in G.pos
        ]}]

def apply_spring_layout(network):
    my_networkx = network.to_networkx()
    my_networkx.pos = nx.drawing.spring_layout(my_networkx)
    #my_networkx.pos = nx.drawing.circular_layout(my_networkx)
    cartesian_aspect = cartesian(my_networkx)
    network.set_opaque_aspect(""cartesianCoordinates"", cartesian_aspect)",0.4054692388,
1394,part d subtask point,"def outer_crossing_probability(transition):
    tcp = transition.tcp
    interface_outer_lambda = transition.interfaces.lambdas[-1]
    return tcp(interface_outer_lambda)",0.4561576545,
1394,part d subtask point,"def q_minus_one(row):
    r""""""Subtract 1 from q.""""""
    sol = row.data[row.index[0]]
    xc = sol.state.grid.p_centers[0] + 0
    q = sol.state.q[0,:] - 1
    return xc, q",0.4503200054,
1394,part d subtask point,"def aggregate(shop_id):
    df_shop_ts_filtered = df_shop_ts.loc[(shop_id,""2016-07-01""):,:]

    pay_mean = df_shop_ts_filtered.total_pay.mean()
    view_mean = df_shop_ts_filtered.total_view.mean()

    df_shop_ts_filtered = df_shop_ts_filtered.fillna({""total_view"":pay_mean, ""total_pay"":view_mean})

    df_shop_ts_filtered.loc[shop_id,].plot(subplots = True)

    df_shop_ts_filtered.loc[:,""total_today""] = df_shop_ts_filtered.loc[:,""total_pay""] + df_shop_ts_filtered.loc[:,""total_view""]
    df_shop_ts_filtered = df_shop_ts_filtered.drop([""total_view"", ""total_pay""], axis = 1)
    print(df_shop_ts_filtered)

    print(df_shop_ts_filtered.total_today.describe())
    return df_shop_ts_filtered",0.4430485368,
1394,part d subtask point,"# Function to find optimal parameters based on 'aic' criterion
def get_arma_params(grp):
    ddf = df_prediction.ix[df_prediction.ad == grp, {'shown'}].copy(deep = True)
    x = arma_order_select_ic(ddf, ic=['aic'], trend='nc',
                             max_ar = 10, max_ma = 5, fit_kw = {'method': 'css'})
    return x['aic_min_order']",0.4413987398,
1394,part d subtask point,"def threshold_likes(df, uid_min, mid_min):
    n_users = df.uid.unique().shape[0]
    n_items = df.mid.unique().shape[0]
    sparsity = float(df.shape[0]) / float(n_users*n_items) * 100
    print('Starting likes info')
    print('Number of users: {}'.format(n_users))
    print('Number of models: {}'.format(n_items))
    print('Sparsity: {:4.3f}%'.format(sparsity))
    
    done = False
    while not done:
        starting_shape = df.shape[0]
        mid_counts = df.groupby('uid').mid.count()
        df = df[~df.uid.isin(mid_counts[mid_counts < mid_min].index.tolist())]
        uid_counts = df.groupby('mid').uid.count()
        df = df[~df.mid.isin(uid_counts[uid_counts < uid_min].index.tolist())]
        ending_shape = df.shape[0]
        if starting_shape == ending_shape:
            done = True
    
    assert(df.groupby('uid').mid.count().min() >= mid_min)
    assert(df.groupby('mid').uid.count().min() >= uid_min)
    
    n_users = df.uid.unique().shape[0]
    n_items = df.mid.unique().shape[0]
    sparsity = float(df.shape[0]) / float(n_users*n_items) * 100
    print('Ending likes info')
    print('Number of users: {}'.format(n_users))
    print('Number of models: {}'.format(n_items))
    print('Sparsity: {:4.3f}%'.format(sparsity))
    return df",0.4388262331,
1394,part d subtask point,"# Shows the trace with a vertical line at the mean of the trace
def plot_trace(trace):
    # Traceplot with vertical lines at the mean value
    ax = pm.traceplot(trace, figsize=(12, len(trace.varnames)*1.5),
                      lines={k: v['mean'] for k, v in pm.summary(trace).iterrows()})
    
    plt.rcParams['font.size'] = 12
    # Labels with the mean value
    for i, mn in enumerate(pm.summary(trace)['mean']):
        ax[i, 0].annotate('{:0.2f}'.format(mn), xy = (mn, 0), xycoords = 'data', size = 8,
                          xytext = (5, 10), textcoords = 'offset points', rotation = 90,
                          va = 'bottom', fontsize = 'large', color = 'red')",0.4386019111,
1394,part d subtask point,"# Shows the trace with a vertical line at the mean of the trace
def plot_trace(trace):
    # Traceplot with vertical lines at the mean value
    ax = pm.traceplot(trace, figsize=(14, len(trace.varnames)*1.8),
                      lines={k: v['mean'] for k, v in pm.df_summary(trace).iterrows()})
    
    matplotlib.rcParams['font.size'] = 16
    
    # Labels with the median value
    for i, mn in enumerate(pm.df_summary(trace)['mean']):
        ax[i, 0].annotate('{:0.2f}'.format(mn), xy = (mn, 0), xycoords = 'data', size = 8,
                          xytext = (-18, 18), textcoords = 'offset points', rotation = 90,
                          va = 'bottom', fontsize = 'large', color = 'red')",0.4383765459,
1394,part d subtask point,"#class LearningAgent(Agent):
    
def update(self, t):
    #some other code
        
    #Learn and update Q-value based on state, action and reward
    new_state = (self.env.sense(self)[""light""],\
                 self.planner.next_waypoint())
        
    if new_state not in self.q_table:
        q_hat = reward + self.gamma * 0
    else:
        q_hat = reward + \
                self.gamma * max(self.q_table[new_state].values())
            
    self.q_table[self.state][action] = \
        self.alpha*q_hat + (1-self.alpha)*self.q_table[self.state][action]",0.4383340776,
1394,part d subtask point,"def SampleP(particle, fieldset, time, dt):  # Custom function that samples fieldset.P at particle location
    particle.p = fieldset.P[time, particle.lon, particle.lat, particle.depth]

k_sample = pset.Kernel(SampleP)    # Casting the SampleP function to a kernel.",0.4382235408,
1394,part d subtask point,"def calcProperties(s):
    structure_id = s[0]
    space_group = s[1].space_group
    a, b, c, alpha, beta, gamma = s[1].unit_cell

    return Row(structure_id, space_group, a, b, c, alpha, beta, gamma)",0.4375478029,
812,histograms,"def make_hist( X, xmax=None, xmin=None, binw=1, xlabel='Input', ylabel='Counts', xunit='', edgecolor='black', tightLabel=False, centerLabel=False, debug=False, log=False, **hist_kwds ):    
    if not xmax:
        xmax = max(X)+binw
    if not xmin:
        xmin = min(X) if (min(X) < 0) or (min(X) >=1) else 0 
        
    if xmax <= xmin: 
        xmax = max(X)+binw
        xmin = min(X) if (min(X) < 0) or (min(X) >=1) else 0 

    hist_info_ = plt.hist( 
                           x = X[ (X <= xmax) & (X >= xmin) ],
                           bins = np.arange(xmin, xmax+binw, binw), # Due to end 2 bins are 1, hist will combined them to a bin. 
                           edgecolor = edgecolor,
                           log=log,
                           **hist_kwds
                         )
    
    if xunit == '': 
        ylabel = ylabel+' / %.2f'%(binw)
    else:
        xlabel = xlabel+' [%s]'%(xunit)
        ylabel = ylabel+' / %.2f %s'%(binw, xunit)
    plt.tick_params(labelsize=20)
    plt.xlabel(xlabel, fontsize=25)
    plt.ylabel(ylabel, fontsize=25)
    plt.ylim(ymin = 0.5 if log else 0)
    
    # Show label be bin by bin
    if tightLabel: 
        plt.xticks(hist_info_[1])
    
    # Make label be in bins' center
    if tightLabel and centerLabel:
        ax_min = min(hist_info_[1])
        ax_max = max(hist_info_[1])
        ax_wth = (hist_info_[1][1]-hist_info_[1][0])/2.
        newrange = np.arange(ax_min, ax_max+ax_wth, ax_wth)
        newnames, n = [], 0
        for i in range(len(newrange)):
            if i%2 == 0: 
                newnames.append('')
            elif n < len(hist_info_[1]): 
                v = hist_info_[1][n] if hist_info_[1][n]%1 != 0 else int(hist_info_[1][n])
                newnames.append(v)
                n+=1   
        plt.xticks(newrange, newnames)
        if debug: print ax_min, ax_max, ax_wth, newrange

    return hist_info_",0.4793895483,
812,histograms,"# Plots a histogram of the image, splitting into individual channels if necessary.
def plot_multichannel_histo(img):
    if img.ndim == 2: # plot grayscale histo
        plt.hist(img.flatten(), 256,  range=(0,255), color='k', histtype='step')
    elif img.ndim == 3: # print rgb histo
        plt.hist(img.reshape(-1,3), 256,  range=(0,255), color=['r','g','b'],histtype='step')
    else: # Not an image
        print(""Must pass a valid RGB or grayscale image"")
    plt.xlim([0,255])",0.4791707993,
812,histograms,"# Plots a histogram of the image, splitting into individual channels if necessary.
## Plots a histogram of the basic colors used in the image. It splits it up into
## r, g, b.
def plot_multichannel_histo(img):
    if img.ndim == 2: # plot grayscale histo
        plt.hist(img.flatten(), 256,  range=(0,255), color='k', histtype='step')
    elif img.ndim == 3: # print rgb histo
        plt.hist(img.reshape(-1,3), 256,  range=(0,255), color=['r','g','b'],histtype='step')
    else: # Not an image
        print(""Must pass a valid RGB or grayscale image"")
    plt.xlim([0,255])",0.4791707993,
812,histograms,"# if number
#     skip
# else 
#     plot histogram (full data)
#     plot kde (subsample if n is given else full data)

def plot_sample(sample, n=None):
    if isinstance(sample, Number):
        return
    plt.hist(sample, normed=True, bins=20)
    x = np.linspace(*plt.gca().get_xlim(), 1000)
    if n is not None:
        sample = np.random.choice(sample, size=int(n))
    kde = st.gaussian_kde(sample)
    y = kde.pdf(x)
    plt.plot(x, y, lw=3)",0.4783065021,
812,histograms,"def plot_simulated_data():
    plt.hist(heights_simulated, normed=True, bins=100, range=[130, 180], alpha=0.5, color='coral');
    plt.hist(adult_heights, normed=True, bins=100, range=[130, 180], alpha=0.5, color='steelblue');
    plt.xlabel('height');
    plt.ylabel('P(height)');

plot_simulated_data()",0.4768753648,
812,histograms,"def drawWeightHistogram(x):
    ## the histogram of the data
    fig = plt.subplots()
    n, bins, patches = plt.hist(x, 50)
    plt.xlim(-0.5, 0.5)
    plt.xlabel('Weight')
    plt.ylabel('Count')
    zero_counts = (x == 0.0).sum()
    plt.title(""Weight Histogram. Num of '0's: %d"" % zero_counts)",0.4761844277,
812,histograms,"def make_sideHistogram(varY):
    traceHside = go.Histogram(
        y = varY,
        name = 'events',
        histnorm = 'probability',
        autobinx = True,
        #ybins = dict(
        #        start=zmax,
        #        end=0,
        #        size=50
        #    ),
        marker = dict(color='#2d1e3e'),
        xaxis = 'x2'
    )
    return traceHside",0.472135663,
812,histograms,"def data_errors():
    from numpy import random, mean, std
    from matplotlib. pyplot import hist
    N = 10000
    
    # Draw random numbers for I and a
    I_values = random. normal(1, 0.2, N)
    a_values = random. uniform(0.5, 1.5, N)
    
    # Compute corresponding u values for some t values
    t = [0, 1, 3]
    u_values = {} # samples for various t values
    u_mean = {}
    u_std = {}

    for t_ in t:    
        # Compute u samples corresponding to I and a samples
        u_values[t_] = [model(t_, I, a)
        for I, a in zip(I_values, a_values)]
        u_mean[t_] = mean(u_values[t_])
        u_std[t_] = std(u_values[t_])
        figure()
        dummy1, bins, dummy2 = hist(
            u_values[t_], bins=30, range=(0, I_values. max()),
            normed=True, facecolor='green' )        
        #plot(bins)
        title('t=%g' % t_)
        savefig('tmp_%g.png' % t_); savefig('tmp_%g.pdf' % t_)
    
    # Table of mean and standard deviation values
    print 'time mean st.dev.'
    for t_ in t:
        print '%3g %.2f %.3f' % (t_, u_mean[t_], u_std[t_])",0.4717691541,
812,histograms,"def plotRainfallDistribution(rainfallSimulated):
  
    # Create Figure.
    fig = plt.figure(figsize=(20, 10))

    # Plot histogram.
    plt.hist(rainfallSimulated,facecolor='steelblue',bins=100, density=True,
           histtype='stepfilled', edgecolor = 'black' , hatch = '+')

    # Add axis names.
    plt.title('Rainfall Simulation')
    plt.xlabel('Rainfall Amount [mm]')
    plt.ylabel('Probability ')
    plt.grid()
    plt.show()",0.4715001583,
812,histograms,"def Neighbor_profile(r):
    from analysis import hist, Neighbors
    neighbors = Neighbors(r)
    xs, ys = hist(neighbors, boundary=(0,6))
    mean_neighbors = round(neighbors.mean(),3)
    
    fig, ax = plt.subplots()
    ax.plot(xs, ys)
    ax.annotate(f'mean :{mean_neighbors}', xy=(0.7,0.5), xycoords='axes fraction')
    ax.set(xlabel='number of neighbors', ylabel='frequency')
    
    plt.show(fig)
    fig.savefig('statistics/neighbors.pdf')
    
Neighbor_profile(sys.r)",0.4705871344,
2352,the most famous quote in regex dom,??MLPRegressor,0.4655483961,
2352,the most famous quote in regex dom,"spec = construct.Select(
    construct.Regex(re.compile(b'goodbye (?P<farewell_person>[A-Za-z]*)')),
    construct.Regex(re.compile(b'hello (?P<greet_person>[A-Za-z]*)')),
)

spec.parse(b'\x01\x02hello bob\x00\x03')",0.4576490521,
2352,the most famous quote in regex dom,"# remove name from one card 

re.sub(cards_no_nulls.name[3] , ""This"", cards_no_nulls.text[3] )",0.457307905,
2352,the most famous quote in regex dom,"spec = construct.Select(
    construct.Regex(re.compile(b'hello (?P<greet_person>[A-Za-z]*)')),
    construct.Regex(re.compile(b'goodbye (?P<farwell_person>[A-Za-z]*)')),
)

spec.parse(b'\x01\x02hello bob\x00\x03\x04goodbye george\x00\x05\x06')",0.4572797418,
2352,the most famous quote in regex dom,"noun_regex = RegexNgramMatch(label='Nouns', regex_pattern=r'[A-Z]?NN[A-Z]?', ignore_case=True, match_attrib='poses')
up_regex = RegexFilterAll(noun_regex, label='Upper', regex_pattern=r'[A-Z]+([0-9]+)?([A-Z]+)?([0-9]+)?$', ignore_case=False, match_attrib='words')
multi_regex = RegexFilterAll(up_regex, label='Multi', regex_pattern=r'[a-z0-9]{3,}', ignore_case=True)",0.4566510022,
2352,the most famous quote in regex dom,"spec = construct.Select(
    construct.Regex(re.compile(b'goodbye (?P<farwell_person>[A-Za-z]*)')),
    construct.Regex(re.compile(b'hello (?P<greet_person>[A-Za-z]*)'))
)

spec.parse(b'\x01\x02hello bob\x00\x03\x04goodbye george\x00\x05\x06')",0.4558763504,
2352,the most famous quote in regex dom,"product = list(section2.strings)[1].replace('\u2003','')",0.4551769197,
2352,the most famous quote in regex dom,"tree.findall(""person/name/last"")[1].text",0.4510681331,
2352,the most famous quote in regex dom,"re.search(regex, string), we can check whether string is a match for regex. 
# return a match object. If it isn't, it will return None. 

re.sub(""yo"", ""hello"", ""yo world"") 
# replace the ""yo"" in ""yo world"" with ""hello"" and return the result ,OR original string if not found",0.4495404959,
2352,the most famous quote in regex dom,"parse = gram.parse_input(""2015 05 04 US"")[0]

print(parse)

# Here's how to get the timezone string:
print(parse.children[1].rule.rhs[0])

# Here's how to get the top-level $Y vs. $MonthDay ordering:
print(parse.children[0].rule.rhs)

# Here's how to get the $MonthDay child names:
print(parse.children[0].children[1].rule.rhs)

def lemmas(parse):
    """"""Returns a list of (category, word) pairs""""""
    labs = []
    for t in parse.children:
        if len(t.rule.rhs) == 1:
            labs.append((t.rule.lhs, t.rule.rhs[0]))
        else:
            labs += lemmas(t)
    return labs

print(lemmas(parse))",0.4480729103,
609,fdsn client exercise,"def hparams_dqn_cartpole():
    params = base_hparams.base_dqn()

    params.env_id = 'CartPole-v1'

    params.rollout_steps = 1
    params.num_processes = 1
    params.actor_lr = 1e-3
    params.gamma = 0.99
    params.target_update_interval = 10
    params.eps_min = 1e-2
    params.buffer_size = 1000
    params.batch_size = 32
    params.num_total_steps = 5000
    params.num_eps_steps = 500

    return params",0.4781663716,
609,fdsn client exercise,"#From the readme.txt
#
#III. FORMAT OF DATA FILES ("".dly"" FILES)
#
#Each "".dly"" file contains data for one station.  The name of the file
#corresponds to a station's identification code.  For example, ""USC00026481.dly""
#contains the data for the station with the identification code USC00026481).
#
#Each record in a file contains one month of daily data.  The variables on each
#line include the following:
#
#------------------------------
#Variable   Columns   Type
#------------------------------
#ID            1-11   Character
#YEAR         12-15   Integer
#MONTH        16-17   Integer
#ELEMENT      18-21   Character
#VALUE1       22-26   Integer
#MFLAG1       27-27   Character
#QFLAG1       28-28   Character
#SFLAG1       29-29   Character
#VALUE2       30-34   Integer
#MFLAG2       35-35   Character
#QFLAG2       36-36   Character
#SFLAG2       37-37   Character
#  .           .          .
#  .           .          .
#  .           .          .
#VALUE31    262-266   Integer
#MFLAG31    267-267   Character
#QFLAG31    268-268   Character
#SFLAG31    269-269   Character
#------------------------------
#
#retrieves ghcnd_all.tar.gz from noaa's website
def get_ghcnd_all():
    print(""GRABBING ALL STATION DATA"")

    ftp = FTP('ftp.ncdc.noaa.gov')
    ftp.login()
    ftp.cwd('pub/data/ghcn/daily')
    ftp.retrbinary('RETR ghcnd_all.tar.gz', open('ghcnd_all.tar.gz', 'wb').write)
    ftp.quit()",0.4727572203,
609,fdsn client exercise,"#From the readme.txt:  
#    
#VII. FORMAT OF ""ghcnd-inventory.txt""  
#  
#------------------------------  
#Variable   Columns   Type  
#------------------------------  
#ID            1-11   Character  
#LATITUDE     13-20   Real  
#LONGITUDE    22-30   Real  
#ELEMENT      32-35   Character  
#FIRSTYEAR    37-40   Integer  
#LASTYEAR     42-45   Integer  
#------------------------------  
#  
#ELEMENT    is the element type.   There are five core elements as well as a number
#           of addition elements.  
#   
#           The five core elements are:  
#  
#           PRCP = Precipitation (tenths of mm)  
#           SNOW = Snowfall (mm)  
#           SNWD = Snow depth (mm)  
#           TMAX = Maximum temperature (tenths of degrees C)  
#           TMIN = Minimum temperature (tenths of degrees C)  
#         
#FIRSTYEAR and LASTYEAR are the first and last years the weather station recorded that element.
#
#creates a dataframe for ghcnd-inventory.txt from noaa's website
def df_ghcnd_inventory():
    print(""GRABBING LATEST STATION INVENTORY FILE"")

    ftp = FTP('ftp.ncdc.noaa.gov')
    ftp.login()
    ftp.cwd('pub/data/ghcn/daily')
    ftp.retrbinary('RETR ghcnd-inventory.txt', open('ghcnd-inventory.txt', 'wb').write)
    ftp.quit()

    # Read in GHCND-D INVENTORY File
    ghcnd_invfile='ghcnd-inventory.txt'
    ghcnd_inventory= np.genfromtxt(ghcnd_invfile,delimiter=(11,1,8,1,9,1,4,1,4,1,4),dtype=str)
    
    #creates a dataframe form the data in the text file
    dataframe = pd.DataFrame(ghcnd_inventory)
    
    #deletes empty columns
    deleteCols = [1,3,5,7,9]
    for i in deleteCols:
        del dataframe[i]
    
    #gives columns header names
    headers = [""ID"", ""LAT"", ""LON"", ""ELEM"" , ""FIRST"", ""LAST""]
    dataframe.columns = headers
    
    return dataframe",0.4698145986,
609,fdsn client exercise,"#From the readme.txt:  
#  
#IV. FORMAT OF ""ghcnd-stations.txt""  
#  
#------------------------------  
#Variable   Columns   Type  
#------------------------------  
#ID            1-11   Character  
#LATITUDE     13-20   Real  
#LONGITUDE    22-30   Real  
#ELEVATION    32-37   Real  
#STATE        39-40   Character  
#NAME         42-71   Character  
#GSN FLAG     73-75   Character  
#HCN/CRN FLAG 77-79   Character  
#WMO ID       81-85   Character  
#------------------------------
#
#creates a dataframe for ghcnd-stations.txt from noaa's website
def df_ghcnd_stations():
    print(""GRABBING LATEST STATION METADATA FILE"")

    ftp = FTP('ftp.ncdc.noaa.gov')
    ftp.login()
    ftp.cwd('pub/data/ghcn/daily')
    ftp.retrbinary('RETR ghcnd-stations.txt', open('ghcnd-stations.txt', 'wb').write)
    ftp.quit()

    # Read in GHCND-D Stations File
    ghcnd_stnfile = 'ghcnd-stations.txt'
    ghcnd_stations = np.genfromtxt(ghcnd_stnfile,delimiter=(11,9,10,7,1,2,1,30,1,3,1,3,1,5),dtype=str)
    
    #creates a dataframe form the data in the text file
    dataframe = pd.DataFrame(ghcnd_stations)
    
    #deletes empty columns
    deleteCols = [4,6,8,10,12]
    for i in deleteCols:
        del dataframe[i]
    
    #gives columns header names
    headers = [""ID"", ""LAT"", ""LON"", ""ELEV"", ""ST"", ""NAME"",""GSN"", ""HCN"", ""WMOID""]
    dataframe.columns = headers
    
    return dataframe",0.4698145986,
609,fdsn client exercise,"class Network:
    
    def __init__(self, session, w_in, h_in, c_in , n_out):
        self.session = session
        self.w_in = w_in
        self.h_in = h_in
        self.c_in = c_in
        self.n_out = n_out
        # data placeholders
        self.x = tf.placeholder(tf.float32, [None, self.w_in, self.h_in, self.c_in], name='x')
        self.y = tf.placeholder(tf.float32, [None, self.n_out], name='y')
        #self.pkeep = tf.placeholder(tf.float32, name='pkeep')
        
        self.x_in = tf.reshape(self.x, [-1,self.w_in,self.h_in,self.c_in])
        
        self.n1 = 8
        self.n2 = 16
        self.n3 = 32
        with tf.device(""/device:GPU:0""):
             # cnn layer 1
            self.W_conv1 = tf.get_variable('W_conv1', shape=[7, 7, 1, self.n1])
            self.h_conv1 = tf.nn.conv2d(self.x_in,self.W_conv1,strides=[1,1,1,1],padding='SAME',name='conv1') 
            
            # cnn layer 2
            self.W_conv2 = tf.get_variable('W_conv2', shape=[5, 5, self.n1, self.n2])
            self.h_conv2 = tf.nn.conv2d(self.h_conv1,self.W_conv2,strides=[1,1,1,1],padding='SAME',name='conv2')
            # pool 2
            self.h_pool = maxpool(self.h_conv2)
            # fc layer 
            self.h_pool_flat = tf.reshape(self.h_pool, [-1, 42*42*self.n2])
            self.W_fc1 = tf.get_variable('W_fc1', shape=[42*42*self.n2, self.n3])
            self.b_fc1 = tf.get_variable('b_fc1', shape=[self.n3])
            self.h_fc1 = tf.nn.relu(tf.add(tf.matmul(self.h_pool_flat, self.W_fc1), self.b_fc1),name='fc1')
            
            # one more fc layer
            self.W_fc2 = tf.get_variable('W_fc2', shape=[self.n3,self.n_out])
            self.b_fc2 = tf.get_variable('b_fc2', shape=[self.n_out])
            self.q = tf.add(tf.matmul(self.h_fc1, self.W_fc2), self.b_fc2, name='fc2')
            self.loss = tf.reduce_sum(tf.square(self.y - self.q),1)
            self.train_step = tf.train.AdamOptimizer(2*1e-4).minimize(self.loss)
        
    def compute(self, x):
        # evaluate the network and return the action values [q(s,a=0),q(s,a=1)]
        return self.session.run(self.q, feed_dict={self.x:np.reshape(x,[-1,self.w_in,self.h_in,self.c_in])})#self.pkeep:0.5
    
    def train(self, x_batch, y_batch):
        # take a training step
        _ = self.session.run(self.train_step, feed_dict={self.x: x_batch, self.y: y_batch})# self.pkeep: 0.5",0.4457816482,
609,fdsn client exercise,"from milanuncios import MilAnunciosLoginError

# delay parameter indicates how many seconds to wait loading pages and before perform actions
# Is 1.5 as default, if you have runtime troubles try to increase it
with MilAnuncios(delay=3) as ma:
    # Login in milanuncios
    ma.login(""tu_email@proveedor.com"", ""tu_contrasea"")  # If login fails MilAnunciosLoginError will be raised
    assert ma.logged == True
    
    # Obtain your ads
    ma.my_ads(dataframe=False) # As default returns a pandas' DataFrame, but you can retrieve a list also
    
    # Renew your ads
        # You can renew by title of by number of adverts
        # The program will ignore that adverts wich can't be renewed yet
    ma.renew_ads(title=[""Ttulo de mi anuncio"", ""Otro, da igual si es en minscula o mayscula""])  # Por nombre
    
    ma.renew_ads(number=3)  # First 3 that can be renewed in your adverts list
    
    # This method returns the number of adverts renewed",0.443457514,
609,fdsn client exercise,testsimdata = pt.simPermDsn(k=1000),0.4424639344,
609,fdsn client exercise,"with tf.Session() as sess:
    
    sess, saver, writer_train, writer_valid = net.init_session(sess, output_dir)
    print('\n\nTraining Statistics:\n')

    try:
        coord = tf.train.Coordinator()
        threads = tf.train.start_queue_runners(coord=coord)
        metrics = net.init_metrics(losses)

        for i in range(iterations):
            
            # --- Run a single iteration of training
            X_batch, y_batch = sess.run([batch['train']['X'], batch['train']['y']])
            y_batch = np.max(y_batch > 0, axis=(1,2)).astype('float32')
            _, metric, summary, step  = sess.run(
                [ops['train'], losses, ops['summary'], global_step],
                feed_dict={
                    X: X_batch, 
                    y: y_batch, 
                    mode: True})

            writer_train.add_summary(summary, step)
            metrics = net.update_ema(metrics, metric, mode='train', iteration=i)
            net.print_status(metrics, step, metric_names=['sce', 'topk'])

            # --- Every 10th iteration run a single validation batch
            if not i % 10:

                X_batch, y_batch = sess.run([batch['valid']['X'], batch['valid']['y']])
                y_batch = np.max(y_batch > 0, axis=(1,2)).astype('float32')
                metric, summary = sess.run(
                    [losses, ops['summary']],
                    feed_dict={
                        X: X_batch, 
                        y: y_batch, 
                        mode: False})

                writer_valid.add_summary(summary, step)
                metrics = net.update_ema(metrics, metric, mode='valid', iteration=i)
                net.print_status(metrics, step, metric_names=['sce', 'topk'])

        saver.save(sess, '%s/checkpoint/model.ckpy' % output_dir)

    finally:
        coord.request_stop()
        coord.join(threads)
        saver.save(sess, '%s/checkpoint/model.ckpy' % output_dir)",0.4388777614,
609,fdsn client exercise,"def getHeaders(address,password,folder):
    '''
    Function to get the email header data with only read emails. 
    '''
    
    # imap connection
    mail = IMAP4_SSL('imap.gmail.com')
    
    # login
    mail.login(address,password)
    
    # select the folder we want to anlyize 
    mail.select(folder) 
    
    # retrieving the uids
    result, data = mail.uid('search', None, 'SEEN')
    
    # you can also select the emails by using other filters, i.e. by time, by from etc
    # checkout the internet message protcol here:
    # http://tools.ietf.org/html/rfc3501.html
    ####### But be careful here!!! ########
    # if I use sentsince, which select emails from today to a past date, 
    # then all the unread emails before today will become read. 
    # so if you have many unread emails, you need be careful of using the time
    #interval = (datetime(2016,1,1) - timedelta(n_days)).strftime(""%d-%b-%Y"")
    #result, data = mail.uid('search', None, '(SENTSINCE {date})'.format(date=interval))
    
    # retrieving the headers
    result, data = mail.uid('fetch', data[0].replace(' ',','), 
                         '(BODY[HEADER.FIELDS (DATE)])')
    mail.close()
    mail.logout()
    return data

def headers2df(headers):
    '''
    Function to convert the header data from the gmails to pandas dataframe
    '''
    t = []
    for h in headers: 
        if len(h) > 1:
            try:
                timestamp = mktime(parsedate(h[1][5:].replace('.',':')))
            except:
                continue
            mailstamp = datetime.fromtimestamp(timestamp)
            t.append([mailstamp, 1])
       
    df = pd.DataFrame(t, columns=['Datetime', 'Count'])
    df = df.set_index('Datetime')
    return df",0.4387003183,
609,fdsn client exercise,"# This Functions converts the SQL databases into pandas and returns the database of |Object|Frame|X_positions|Y_positions
delta_t=1./30
def SQL_to_Pandas(filename):
    
    con = sqlite3.connect(path+filename)

    #Defining The number of objects we have
    sql_command = ""SELECT COUNT(*) FROM objects""
    max_objects_sql = con.execute(sql_command)
    max_objects = max_objects_sql.fetchone()
    max_objects=max_objects[0]

    # Getting the positions of every object
    df_positions = pd.read_sql_query(""SELECT a.object_id,  b.frame_number, x_coordinate, y_coordinate\
    FROM objects_features AS a \
    JOIN positions AS b ON a.trajectory_id = b.trajectory_id"",con)

    # Merging the velocities and positions
    df = pd.DataFrame()

    df=df_positions.copy()
    
    df.columns = ['object', 'frame', 'x', 'y']
    
    # Compute the average of trajectories based on the features
    df=df.groupby(['object','frame']).mean().reset_index()
    
    #Dropping elements with nan
    df=df.dropna(axis=0, how='any')
    df_positions=df_positions.dropna(axis=0, how='any')
    
    return df, df_positions, max_objects",0.4370199442,
248,comparing regression trees and classification trees,"dhcrTree.getTreeBranch(['XFEM', 'crackPartition', 'LinearTet']).printStats2()",0.4835990369,
248,comparing regression trees and classification trees,"crTree.getTreeBranch(['XFEM', 'crackPartition', 'LinearTet']).printStats2()",0.4835990369,
248,comparing regression trees and classification trees,"dhcrTree.getTreeBranch(['XFEM', 'simple']).printStats2()",0.4830411077,
248,comparing regression trees and classification trees,"crTree.getTreeBranch(['XFEM', 'simple']).printStats2()",0.4830411077,
248,comparing regression trees and classification trees,dhcrTree.getTreeBranch(['FEM']).printStats2(),0.4820994139,
248,comparing regression trees and classification trees,crTree.getTreeBranch(['FEM']).printStats2(),0.4820994139,
248,comparing regression trees and classification trees,"id3_iris.prune(validate)
print(id3_iris.score(train), id3_iris.score(validate), id3_iris.score(test))
id3_iris.display_tree()",0.4743615389,
248,comparing regression trees and classification trees,"class Forest(object):
    
    def __init__(self, max_depth, gini_tol, num_trees):
        self.trees = []
        self.N = num_trees
        for i in xrange(self.N):
            self.trees.append(Node(max_depth, gini_tol))
        self.tol = gini_tol
        self.max_depth = max_depth
        
    def __str__(self):
        s = '\n%d Trees\n'%(self.N)
        for i,node in enumerate(self.trees):
            s += 'Tree #%d:\n'%(i+1)
            s += str(node)+'\n'
        return s
        
    def train(self, data, labels, col_names=None, categorical=None, num_vars=None):
        if num_vars is None:
            num_vars = int(data.shape[1]**0.5)+1
            print num_vars
        self.r = num_vars
        count = 1
        for node in self.trees:
            print ""\rtraining tree #"",count,'/',self.N,
            node.train(data, labels, col_names, categorical, self.r)
            count += 1
            
    def classify(self, sample):
        predictions = []
        for node in self.trees:
            predictions.append(node.classify(sample))
        return np.argmax(np.bincount(predictions))
    
    def classify_many(self,samples):
        sample_labels = np.zeros(samples.shape[0])
        for i,s in enumerate(samples):
            sample_labels[i] = self.classify(samples[i])
        return sample_labels
    
    def accuracy(self,samples,actual_labels):
        predicted_labels = self.classify_many(samples)
        incorrect_labels = predicted_labels.astype(bool)^actual_labels.astype(bool)
        correct_count = actual_labels.size-np.sum(incorrect_labels)
        accuracy = correct_count/float(actual_labels.size)
        return accuracy",0.4734638035,
248,comparing regression trees and classification trees,"# This random forest
my_rf = Random_forest(x_train, y_train, num_trees=1, min_leaf_samples=1, max_depth = 3)
my_rf.trees[0], my_rf.trees[0].left_child_tree, my_rf.trees[0].right_child_tree
my_rf.trees[0].print_tree()",0.4692219496,
248,comparing regression trees and classification trees,"import math
import numpy as np

class decision_tree_regressor:
    
    def __init__(self, max_depth = None, criteria='std'):
        """"""
        Builds a decision tree to regress on the target data. The
        decision tree is built by trying to minimize the requested
        criteria at each possible split. Starting with the whole data
        the tree will try every possible split (column and value pair)
        and choose the split which results in the greatest reduction 
        of the criteria. Then for each sub-group of that split, the 
        process is repeated recursively until no more splits are possible
        or no splits cause a reductuion of the criteria.
        ---
        KWargs:
        max_depth: how many splits to allow in the tree (depth not breadth)
        criteria: what metric to use as a measure of split strength 
        ('std'= reduction of standard deviation in the data, 'mae'=
        minimize the mean error size (abs value))
        """"""
        self.tree = self.tree_split()
        self.data_cols = None
        self.max_depth = max_depth
        self.current_depth = 0
        self.criteria = criteria
    
    # Sub class for handling recursive nodes (only makes sense in the scope of a tree)
    class tree_split:
        """"""
        A sub class for handling recursive nodes. Each node will contain the value and column
        for the current split, as well as links to the resulting nodes from the split. The 
        results attribute remains empty unless the current node is a leaf. 
        """"""
        def __init__(self,col=-1,value=None,results=None,label=None,tb=None,fb=None):
            self.col=col # column index of criteria being tested
            self.value=value # vlaue necessary to get a true result
            self.results=results # dict of results for a branch, None for everything except endpoints
            self.tb=tb # true decision nodes 
            self.fb=fb # false decision nodes
    
    def split_data(self, X, y, colnum, value):
        """"""
        Returns: Two sets of data from the initial data. Set 1 contains those that passed
        the condition of data[colnum] >= value
        ----------
        Input: The dataset, the column to split on, the value on which to split
        """"""
        splitter = None
        if isinstance(value, int) or isinstance(value,float):
            splitter = lambda x: x[colnum] >= value
        else:
            splitter = lambda x: x[colnum] == value
        split1 = [i for i,row in enumerate(X) if splitter(row)]
        split2 = [i for i,row in enumerate(X) if not splitter(row)]
        set1X = X[split1]
        set1Y = y[split1]
        set2X = X[split2]
        set2Y = y[split2]
        return set1X, set1Y, set2X, set2Y

    def get_mean_target_value(self, data):
        """"""
        Returns: A dictionary of target variable counts in the data
        """"""
        return np.mean(data)

    def split_criteria(self, y):
        """"""
        Returns the criteria we're trying to minimize by splitting.
        Current options are target Mean Absolute Error (from the target 
        mean) or Standard deviation of the target.
        ---
        Input: targets in the split
        Output: Criteria
        """"""
        if self.criteria == 'mae':
            mu = np.mean(y)
            return np.mean(np.abs(y-mu))
        else:
            return np.std(y)
    
    def pandas_to_numpy(self, x):
        """"""
        Checks if the input is a Dataframe or series, converts to numpy matrix for
        calculation purposes.
        ---
        Input: X (array, dataframe, or series)
        Output: X (array)
        """"""
        if type(x) == type(pd.DataFrame()) or type(x) == type(pd.Series()):
            return x.as_matrix()
        if type(x) == type(np.array([1,2])):
            return x
        return np.array(x) 
        
    def handle_1d_data(self,x):
        """"""
        Converts 1 dimensional data into a series of rows with 1 columns
        instead of 1 row with many columns.
        """"""
        if x.ndim == 1:
            x = x.reshape(-1,1)
        return x
    
    def convert_to_array(self, x):
        """"""
        Takes in an input and converts it to a numpy array
        and then checks if it needs to be reshaped for us
        to use it properly
        """"""
        x = self.pandas_to_numpy(x)
        x = self.handle_1d_data(x)
        return x
    
    def fit(self, X, y):
        """"""
        Helper function to wrap the fit method. This makes sure the full nested, 
        recursively built tree gets assigned to the correct variable name and 
        persists after training.
        """"""
        self.tree = self._fit(X,y)
    
    def _fit(self, X, y, depth=0):
        """"""
        Builds the decision tree via a greedy approach, checking every possible
        branch for the best current decision. Decision strength is measured by
        information gain/score reduction. If no information gain is possible,
        sets a leaf node. Recursive calls to this method allow the nesting. If
        max_depth is met, all further nodes become leaves as well.
        ---
        Input: X (feature matrix), y (labels)
        Output: A nested tree built upon the node class.""""""
        X = self.convert_to_array(X)
        y = self.convert_to_array(y)
       
        if len(X) == 0: return tree_split()
        current_score = self.split_criteria(y)

        best_gain = 0.0
        best_criteria = None
        best_sets = None
        
        self.data_cols = X.shape[1]
        
        
        # Here we go through column by column and try every possible split, measuring the
        # information gain. We keep track of the best split then use that to send the split
        # data sets into the next phase of splitting.
        
        for col in range(self.data_cols):
            column_values = set(X.T[col])
            for value in column_values:
                set1, set1_y, set2, set2_y = self.split_data(X, y, col, value)
                p = float(len(set1)) / len(y)
                gain = current_score - p*self.split_criteria(set1_y) - (1-p)*self.split_criteria(set2_y)
                if gain > best_gain and len(set1_y) and len(set2_y):
                    best_gain = gain
                    best_criteria = (col, value)
                    best_sets = (np.array(set1), np.array(set1_y), np.array(set2), np.array(set2_y))
        
        # Now decide whether it's an endpoint or we need to split again.
        if (self.max_depth and depth < self.max_depth) or not self.max_depth:
            if best_gain > 0:
                self.current_depth += 1
                true_branch = self._fit(best_sets[0], best_sets[1], depth=depth+1)
                false_branch = self._fit(best_sets[2], best_sets[3], depth=depth+1)
                return self.tree_split(col=best_criteria[0], value=best_criteria[1],
                        tb=true_branch, fb=false_branch)
            else:
                return self.tree_split(results=self.get_mean_target_value(y))
        else:
            return self.tree_split(results=self.get_mean_target_value(y))

    def print_tree(self, indent=""---""):
        """"""
        Helper function to make sure the correct tree gets printed.
        ---
        In: indent (how to show splits between nodes)
        """"""
        self.__original_indent = indent
        self._print_tree_(self.tree, indent)
    
    def _print_tree_(self, tree, indent):
        """"""
        Goes through node by node and reports the column and value used to split
        at that node. All sub-nodes are drawn in sequence below the node.
        """"""
        if tree.results: # if this is a end node
            print(str(tree.results))
        else:
            print('Column ' + str(tree.col)+' : '+str(tree.value)+'? ')
            # Print the branches
            print(indent+' True: ', end=' ')
            next_indent = indent+self.__original_indent
            self._print_tree_(tree.tb,indent=next_indent)
            print(indent+' False: ', end=' ')
            self._print_tree_(tree.fb,indent=next_indent)

    def predict(self, newdata):
        """"""
        Helper function to make sure the correct tree is used to
        make predictions. Also manages multiple rows of input data
        since the tree must predict one at a time.
        ---
        In: new data point of the same structure as the training X.
        Out: numpy array of the resulting predictions
        """"""
        results = []
        newdata = self.convert_to_array(newdata)
        for x in newdata:
            results.append(self._predict(x,self.tree))
        return np.array(results)
            
    def _predict(self, newdata, tree):
        """"""
        Uses the reusive structure of the tree to follow each split for
        a new data point. If the node is an endpoint, the available classes
        are sorted by ""most common"" and then the top choice is returned.
        """"""
        if tree.results: # if this is a end node
            return tree.results

        if isinstance(newdata[tree.col], int) or isinstance(newdata[tree.col],float):
            if newdata[tree.col] >= tree.value:
                return self._predict(newdata, tree.tb)

            else:
                return self._predict(newdata, tree.fb)
        else:
            if newdata[tree.col] == tree.value:
                return self._predict(newdata, tree.tb)
            else:
                return self._predict(newdata, tree.fb) 

    def score(self, X, y):
        """"""
        Uses the predict method to measure the accuracy of the model.
        ---
        In: X (list or array), feature matrix; y (list or array) labels
        Out: accuracy (float)
        """"""
        pred = self.predict(X)
        return np.mean((pred-y)**2)",0.4684818983,
388,credible interval vs confidence interval,"# Determine the 95% and 99% confidence interval for the mean of body temperature using t and z-score
def confidence_interval(data, ci, two_tails= None):
    mu = np.mean(data)
    sigma = np.std(data)
    n = len(data)
    if two_tails:
        t = stats.t.ppf(ci+(1-ci)/2, n-1)
        z = stats.norm.ppf(ci+(1-ci)/2)
    else:
        t = stats.t.ppf(ci, n-1)
        z = stats.norm.pdf(ci+(1-ci)/2)
    hight = mu + t*sigma/np.sqrt(n)
    lowt = mu - t*sigma/np.sqrt(n)
    highz = mu + z*sigma/np.sqrt(n)
    lowz = mu - z*sigma/np.sqrt(n)
    return round(hight, 2), round(lowt,2), round(highz, 2), round(lowz,2) 

cf95 = confidence_interval(df['temperature'], 0.95, True)
print (""The range of true mean of body temperature using 95% confidence interval and t-statistic is from "",\
       cf95[1], ' to ', cf95[0])
print (""The range of true mean of body temperature using 95% confidence interval and z-statistic is from "",\
       cf95[3], ' to ', cf95[2])

cf99 = confidence_interval(df['temperature'], 0.99, True)
print (""The range of true mean of body temperature using 99% confidence interval and t-statistic is from "",\
       cf99[1], ' to ', cf99[0])
print (""The range of true mean of body temperature using 99% confidence interval and z-statistic is from "",\
       cf99[3], ' to ', cf99[2])",0.4899325967,
388,credible interval vs confidence interval,"## equal tailed 95% credible intervals, and posterior distribution means:
def summary(samples, varname, p=95):
    values = samples[varname][0]
    ci = np.percentile(values, [100-p, p])
    print('{:<6} mean = {:>5.1f}, {}% credible interval [{:>4.1f} {:>4.1f}]'.format(
      varname, np.mean(values), p, *ci))

for varname in samples_Nm1_trace_Stan:
    summary(samples_Nm1_trace_Stan, varname)",0.4841949344,
388,credible interval vs confidence interval,"## equal tailed 95% credible intervals, and posterior distribution means:
def summary(samples, varname, p=95):
    values = samples[varname][0]
    ci = np.percentile(values, [100-p, p])
    print('{:<6} mean = {:>5.1f}, {}% credible interval [{:>4.1f} {:>4.1f}]'.format(
      varname, np.mean(values), p, *ci))

for varname in samples_Nm1:
    summary(samples_Nm1, varname)",0.4841949344,
388,credible interval vs confidence interval,"def reject_outliers_iqr(df, col='roc_cv', scale=1.5):
    q1, q3 = np.percentile(df[col], [25, 75])
    iqr = q3 - q1

    lower_bound = q1 - (iqr * scale)
    upper_bound = q3 + (iqr * scale)
    print('Length before removing outliers', len(df))
    print('Rejecting items from ', col, 'lower than ', lower_bound, ' and higher than', upper_bound)
    d = df.where((df[col] > lower_bound) & (df[col] < upper_bound))
    d.dropna(inplace=True)
    print('Length after removing outliers', len(d))
    return d

df_res_clean = pd.DataFrame(df_res)
df_res_clean = reject_outliers_iqr(df_res_clean, col='cv_train_diff')",0.4794436693,
388,credible interval vs confidence interval,"def display_outliers(dataframe, col, param=1.5):
    Q1 = np.percentile(dataframe[col], 25)
    Q3 = np.percentile(dataframe[col], 75)
    tukey_window = param*(Q3-Q1)
    less_than_Q1 = dataframe[col] < Q1 - tukey_window
    greater_than_Q3 = dataframe[col] > Q3 + tukey_window
    tukey_mask = (less_than_Q1 | greater_than_Q3)
    return dataframe[tukey_mask]",0.4764308929,
388,credible interval vs confidence interval,"def get_lo_conf_index(model):
    conf_int_lo = model.cv_results_['mean_test_score'][model.best_index_] - 1.96 * model.cv_results_['std_test_score'][model.best_index_] / np.sqrt(n_folds)
    I = np.where(model.cv_results_['mean_test_score'] > conf_int_lo)[0]
    I_min = I[np.argmin(model.cv_results_['mean_test_score'][I])]
    return I_min, conf_int_lo",0.4747126997,
388,credible interval vs confidence interval,"def Section2Eval(PredValue, TrueValue, cutoff=0.5):
    df = pd.DataFrame({})
    y_hat = PredValue>=cutoff
    y_actual = TrueValue 
    TP, FP, TN, FN = [0, 0, 0, 0]
    FN = 0
    for i in range(len(y_hat)): 
        if y_actual[i]==y_hat[i]==1:
           TP += 1
    for i in range(len(y_hat)): 
        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:
           FP += 1
    for i in range(len(y_hat)): 
        if y_actual[i]==y_hat[i]==0:
           TN += 1
    for i in range(len(y_hat)): 
        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:
           FN += 1
    df['True Y'] = [TP, FN]
    df['True N'] = [FP, TN]
    df.index=['Predict Y','Predict N']
    return  'Sen.:{:.4f}'.format(TP/(TP+FP*1.0+0.00001)),\
            'PPV.:{:.4f}'.format(TP/(TP+FN*1.0+0.00001)), \
            'Spe.:{:.4f}'.format(TN/(TN+FN*1.0+0.00001)),\
            'NPV.:{:.4f}'.format(TN/(TN+FP*1.0+0.00001)),\
            df",0.4727768898,
388,credible interval vs confidence interval,"print '99% confidence interval:', stats.t.interval(0.99, df=9,
                                                   loc=mean_height, scale=SE)
print '95% confidence interval:', stats.t.interval(0.95, df = 9, 
                                                   loc=mean_height, scale=SE)
print '80% confidence interval:', stats.t.interval(0.8, df = 9, 
                                                   loc=mean_height, scale=SE)",0.4721652865,
388,credible interval vs confidence interval,"def test_model_cv(model, x, y, cv=10):
    scores = cross_validation.cross_val_score(model, x, y, cv=cv, n_jobs=-1, 
                                            scoring='neg_mean_absolute_error')

    scores = -1*scores

    return scores.mean()


def test_fingerprints(fp_list, model, y, verbose = True):

    fingerprint_scores = {}

    for fp in fp_list:
        if verbose: print(""Generating "", fp.name, ""fingerprints"")
        fingerprint_scores[fp.name] = test_model_cv(model, fp.x, y)

    sorted_names = sorted(fingerprint_scores, key=fingerprint_scores.__getitem__, reverse=False)

    #print(""\n Using Linear Regression Model: "", model)
    print(""          ----------------------------------"")
    print(""                name        CV avg abs error"")
    print(""          ----------------------------------"")
    for i in range(len(sorted_names)):
        name = sorted_names[i]
        print(""%26s    %5.3f "" % (name, fingerprint_scores[name]))
    print(""          ----------------------------------"")

# Tikhonov estimation
test_fingerprints(fp_list, BayesianRidge(n_iter=300, tol=0.001, alpha_1=1e-03, alpha_2=1e-03, lambda_1=1e-03, lambda_2=1e-03), y, verbose=True)
test_fingerprints(fp_list, Ridge(alpha=1e-3), y, verbose=True)",0.4718196392,
388,credible interval vs confidence interval,"def test_model(model, cv=10):
    cv_scores = cross_validate(model, X, y, cv=cv)
    
    mean_train_acc = mean(cv_scores[""train_score""]) 
    mean_test_acc = mean(cv_scores[""test_score""])
    
    print()
    print(""Train Accuracy: "", mean_train_acc)
    print(""Test Accuracy: "", mean_test_acc)
    print()
    
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    print(classification_report(y_test, y_pred))
    print()
    print(""Normal split accuracy score: "", accuracy_score(y_test, y_pred))",0.4691391587,
2040,step create a function called majority that return a boolean value to a new column called legal_drinker,"def multiply(func):
    def inner(x):
        ret = func(x)
        return ret * 100
    return inner",0.4852145314,
2040,step create a function called majority that return a boolean value to a new column called legal_drinker,"def log_likelihood_given_pump(d):
    def likelihood_of_single_measurement(measurement):
        return pdf_given_pump(measurement['timedelta'])[measurement['wattdelta']]

    likelihoods = d.apply(likelihood_of_single_measurement, axis=1)

    return np.log(likelihoods).sum()


def log_likelihood_given_no_pump(d):
    def likelihood_of_single_measurement(measurement):
        return pdf_given_no_pump(measurement['timedelta'])[measurement['wattdelta']]

    likelihoods = d.apply(likelihood_of_single_measurement, axis=1)

    return np.log(likelihoods).sum()


def log_odds_of_pump(d):
    return log_likelihood_given_pump(d) - log_likelihood_given_no_pump(d)",0.4844077826,
2040,step create a function called majority that return a boolean value to a new column called legal_drinker,"def record_max(f):
    record_max.maxval = -1
    def _f(*args):
        ret = f(*args)
        record_max.maxval = max(record_max.maxval, ret)
        return ret
    return _f

@record_max
def eval_expr(lhs, op, rhs):
    expr = '{} {} {}'.format(lhs, op, rhs)
    return eval(expr)

run(program)
print(record_max.maxval)",0.482096374,
2040,step create a function called majority that return a boolean value to a new column called legal_drinker,"def compose(fs):
    return lambda start: reduce( lambda x, f: f( x), fs, start)",0.479191184,
2040,step create a function called majority that return a boolean value to a new column called legal_drinker,"def currythis(my_function):
    def f1(a):
        def f2(b):
            return my_function(a, b)  
        return f2
    return f1

def func1(a, b):
    return a + b",0.4777379036,
2040,step create a function called majority that return a boolean value to a new column called legal_drinker,"def ar_incon_normalizer(size):
    params50 = (-5.97004965, -0.53482601,  4.7198059)
    params75 = (-5.36819674, -0.54483589,  4.7198059)
    fx = lambda x, a, b, c: a*x**b + c
    x50 = fx(size, *params50)
    x75 = fx(size, *params75)
    return lambda x: (logistic2(x, x50, x75, 0, 1) - logistic2(1, x50, x75, 0, 1)) / (1 - logistic2(1, x50, x75, 0, 1))

def ar_incon_normalized(mat):
    raw = ar_consistency(mat)
    normalizer = ar_incon_normalizer(len(mat))
    return normalizer(raw)",0.4772777557,
2040,step create a function called majority that return a boolean value to a new column called legal_drinker,"def standardize(df):
    def standardize_c(values):
        return (values - values.mean())/values.std(ddof=0)
    return df.apply(standardize_c)
print(standardize(df))",0.4759463072,
2040,step create a function called majority that return a boolean value to a new column called legal_drinker,"measure = make_pipeline(segmentation.s_one_one,
                        probability_estimation.p_boolean_sliding_window,
                        direct_confirmation_measure.log_ratio_measure,
                        aggregation.arithmetic_mean)",0.4736582637,
2040,step create a function called majority that return a boolean value to a new column called legal_drinker,"class SimpleRanker(metapy.index.RankingFunction):                                            
    """"""                                                                          
    Create a new ranking function in Python that can be used in MeTA.             
    """"""                                                                          
    def __init__(self, some_param=1.0):                                             
        self.param = some_param
        # You *must* invoke the base class __init__() here!
        super(SimpleRanker, self).__init__()                                        
                                                                                 
    def score_one(self, sd):
        """"""
        You need to override this function to return a score for a single term.
        For fields available in the score_data sd object,
        @see https://meta-toolkit.org/doxygen/structmeta_1_1index_1_1score__data.html
        """"""
        return (self.param + sd.doc_term_count) / (self.param * sd.doc_unique_terms + sd.doc_size)",0.4735766351,
2040,step create a function called majority that return a boolean value to a new column called legal_drinker,"def mutiplication(func):
    def muti(c,d):
        result = func(c,d)
        print("":%d""%(result))
        return ("":%d""%(c*d))
    return muti
@mutiplication
def add(a,b):
    return(a+b)
add(3,5)",0.4713557363,
1847,"save the genes to a file, copy to your laptop, and do go analyses","!pyannote-face.py track --verbose --every=0.5 /home/buddha/thesis/pyannote-data/willi.mp4 \
                                              /home/buddha/thesis/pyannote-data/willi.shots.json \
                                              /home/buddha/thesis/pyannote-data/willi.track.txt",0.4520635009,
1847,"save the genes to a file, copy to your laptop, and do go analyses","def update(self):
        """"""
        Interface: 
        After trabeculectomy is done, nothing is done besides set number of visit
        until IOP > IOP target
        """"""
        if self.medicalRecords['OnTrabeculectomy'] == False:
            self.SurgeryTE()
        else:
            self.SetNumberofVisits()
        #Check whether IOP > IOP target yet. Then evaluate whether to leave the block    
        self.DeterminetoExitTEblock()",0.4475286007,
1847,"save the genes to a file, copy to your laptop, and do go analyses","def do_ifcformant(row, indir, outdir, errors='raise'):
    '''Perform formant analysis with the ifcformant command.
    
    Parameters
    ----------
    
    row : namedtuple that contains formant analysis parameters
          in fields:
        'relpath' (relative path to audio file),
        'fname' (name of .wav file),
        'barename' (name of .wav file without extension)
        'speaker' (ifcformant speaker type, one of 'female',
            'male', 'child')
             
    indir : str
        Base pathname to input .wav file. The path to the input file
        will be: indir/row.relpath/row.fname_wav.
             
    outdir : str
        Base pathname to ifcformant output. The output file will
        be written to: outdir/row.relpath/row.barename + '.ifc'.
             
    errors : str (default 'raise')
        How to handle errors if `check_call()` fails. If
        'ignore', print debug statement to STDERR and return the
        ifcformant return code; if 'raise' immediately reraise
        the CalledProcessError.
        
    Returns
    -------
    
    The `ifcformant` return code is returned by this function,
    0 for success or non-zero for errors.
    '''
    ifcargs = [
        'ifcformant',
        '--speaker', row.sex,
        '--print-header',
        '--output', os.path.join(
            outdir, row.relpath, row.barename + '.ifc'
        ),
        os.path.join(indir, row.relpath, row.fname_wav)
    ]
    try:
        subprocess.check_call(ifcargs)
    except subprocess.CalledProcessError as e:
        if errors == 'ignore':
            msg = 'Caught error while invoking ifcformant:\n{:}'.format(e)
            sys.stderr.write(msg)
            return e.returncode
        else:
            raise e
    return 0",0.4428622127,
1847,"save the genes to a file, copy to your laptop, and do go analyses","# MAIN FUNCTION
def main(path, new_location):
    '''
    Input: 	path: Path of Photos Library Masters folder, i.e. ""/Users/USER/Desktop/Masters""
                    new_location: Path of desired new location, i.e. ""/Users/USER/Pictures""
    Output: No file output -- reformats Masters folder
    Dependencies:   Libraries: os
                    Functions: listdir_nh(), listdirz(), recursivedirz()
    '''
    
    # LIBRARY
    import os
    
    # CREATE MASTERS FOLDER COPY
    copy = """"
    print(""A temporary Masters folder has been successfully created."")
    
    # ACQUIRE ALL FILE NAMES IN THE MASTERS FOLDER
    all_files = recursivedirz(copy)
    folders_created = []
    
    # FOR EACH NAMED FILE IN MASTERS FOLDER
    for this_file in all_files:
        # OBTAIN DOWNLOAD DATE
        yyyymmdd = this_file.split(""/"")[-2].split(""-"")[0]
        folder_name = ""-"".join([yyyymmdd[0:4], yyyymmdd[4:6], yyyymmdd[6:8]])
        folder_name = new_location+""/""+folder_name
        
        # TRY TO MAKE THE FOLDER
        try:
            os.makedirs(folder_name)
            folders_created.append(folder_name)
            print(""\nA folder named ""+folder_name+"" was created."")
        except:
            pass
        # SAVE FILE BY ITS FILENAME UNDER A FOLDER WITH FORMAT YYYY-MM-DD
        filename = this_file.split(""/"")[-1]
        os.rename(this_file, folder_name+""/""+filename)
        print(""The file ""+filename+"" was placed under the above directory."")
    
    # DELETE THE MASTERS FOLDER COPY
    # 
    print(""The temporary Masters folder has been successfully deleted."")
    
    # PRINT SUMMARY
    print(""\n""+str(len(folders_created))+"" folders were created under ""+new_location+""."")",0.4423812628,
1847,"save the genes to a file, copy to your laptop, and do go analyses","# Note: you'll need to change the ffmpeg path to use this on your machine
def embed_animation_as_gif(_animation, ffmpeg_path=r'C:\mingw2\bin\ffmpeg.exe'):
    if not os.path.exists(ffmpeg_path):
        return _animation
    _animation.save('animation.mp4')
    os.system(""ffmpeg -i animation.mp4 animation.gif -y"")
    data='0'
    IMG_TAG = '<img src=""data:image/gif;base64,{0}"">'
    with open('animation.gif', 'rb') as f:
        data = f.read()
        data = data.encode('base64')
    return HTML(IMG_TAG.format(data))",0.4417543709,
1847,"save the genes to a file, copy to your laptop, and do go analyses","# make a new directory to save data
    if not os.path.exists(sav_path):
        os.makedirs(sav_path)
        
    end_time = timeit.default_timer()
    cst_time = (end_time - start_time) / 60.
    print(sys.stderr, ('\n The code for file ' + os.path.split(__file__)[1] +
                          ' ran for %.2fm' % ((end_time - start_time) / 60.)))
     
    sav_text = StringIO();
    for layer_idx in range(len(n_nodes)-2):
        if layer_idx==len(n_nodes)-3:
            sav_text.write(""%d"" % (n_nodes[layer_idx+1]))
        else:
            sav_text.write(""%d-"" % (n_nodes[layer_idx+1]))

    if flag_nodewise==1:
       sav_name = '%s/mlp_rst_node_%s.mat' % (sav_path,sav_text.getvalue())
    else: 
       sav_name = '%s/mlp_rst_layer_%s.mat' % (sav_path,sav_text.getvalue())

    sav_text.close()
        
    data_variable = {}; 

    for i in range(len(n_nodes)-1):
        if (i==len(n_nodes)-2): 
            W_name = ""w%d"" %(i+1); b_name = ""b%d"" % (i+1); 
            data_variable[W_name] = classifier.logRegressionLayer.W.get_value(borrow=True)
            data_variable[b_name] = classifier.logRegressionLayer.b.get_value(borrow=True)
        else:
            W_name = ""w%d"" %(i+1); b_name = ""b%d"" % (i+1)
            data_variable[W_name] = classifier.hiddenLayer[i].W.get_value(borrow=True)
            data_variable[b_name] = classifier.hiddenLayer[i].b.get_value(borrow=True)
            
    data_variable['hsp_vals'] = all_hsp_vals;  
    data_variable['L1_vals'] =  all_L1_beta_vals;
    data_variable['train_errors'] = train_errors;    data_variable['test_errors'] = test_errors;
    data_variable['l_rate'] = lrs;
    
    data_variable['momtentum'] = momentum_val;    data_variable['beginAnneal'] = beginAnneal;    data_variable['decay_lr'] = decay_rate;
    data_variable['beta_lrates'] = beta_lrates;    data_variable['max_beta'] = max_beta;    data_variable['tg_hspset'] = tg_hspset;
    data_variable['batch_size'] = batch_size;    data_variable['n_epochs'] = n_epochs;    data_variable['min_annel_lrate'] = min_annel_lrate;
    data_variable['n_nodes'] = n_nodes; data_variable['lrate_list'] = lrate_list;
    
    sio.savemat(sav_name,data_variable)

    print('...done!')",0.4401991665,
1847,"save the genes to a file, copy to your laptop, and do go analyses","%%R -i datapath
returnpath <- getwd()
setwd(datapath)
if(!file.exists(""tomancak_exprs.csv"")) {
    source(""http://www.bioconductor.org/biocLite.R"")
    biocLite(""puma"")
    library(puma)
    print(""Processing data with PUMA"")
    expfiles <- c(paste(""embryo_tc_4_"", 1:12, "".CEL"", sep=""""), paste(""embryo_tc_6_"", 1:12, "".CEL"", sep=""""), paste(""embryo_tc_8_"", 1:12, "".CEL"", sep=""""))
    library(puma)
    drosophila_exp_set <- justmmgMOS(filenames=expfiles, celfile.path=datapath)
    pData(drosophila_exp_set) <- data.frame(""time.h"" = rep(1:12, 3), row.names=rownames(pData(drosophila_exp_set)))    
    write.reslts(drosophila_exp_set, file='tomancak')
}
else {
    print(""Processed data found on disk."")
}
setwd(returnpath)",0.4359071851,
1847,"save the genes to a file, copy to your laptop, and do go analyses","def get_goeaobj_nbt3102(method='fdr_bh'):
    """"""Return GOEA Object ready to run Nature data.""""""
    from goatools.obo_parser import GODag
    from goatools.associations import read_ncbi_gene2go
    from goatools.test_data.genes_NCBI_10090_ProteinCoding import GeneID2nt as GeneID2nt_mus
    from goatools.go_enrichment import GOEnrichmentStudy
    from goatools.base import download_go_basic_obo, download_ncbi_associations
    # Load Ontologies
    obo_fname = download_go_basic_obo()
    obodag = GODag(""go-basic.obo"")
    # Load Associations
    download_ncbi_associations() # Get ftp://ftp.ncbi.nlm.nih.gov/gene/DATA/gene2go.gz
    geneid2gos_mouse = read_ncbi_gene2go(""gene2go"", taxids=[10090])
    # GOE Object holds Ontologies, Associations, and Background gene set
    return GOEnrichmentStudy(
        GeneID2nt_mus.keys(), # Background gene set: mouse protein-coding genes
        geneid2gos_mouse, # geneid/GO Associations
        obodag, # Ontologies
        propagate_counts = False,
        alpha = 0.05, # default significance cut-off
        methods = [method]) # defult multipletest correction method",0.4355190992,
1847,"save the genes to a file, copy to your laptop, and do go analyses","def run_one_model(LM, X=X, y=y, degree=0, n_splits=5, random_state=71, lm_params={}):
    """"""Run linear model LM(lm_params) using 
       k-fold splits for cross validation and return 
       the scores.
    
       Optionally try polynomial features if degree > 0.
    """"""
     
    lm_scores = []
    kf = KFold(n_splits=n_splits, shuffle=True, 
               random_state=random_state)
    for train_ind, val_ind in kf.split(X, y):
        X_train, y_train = X[train_ind], y[train_ind]
        X_val, y_val = X[val_ind], y[val_ind] 
    
        lm = LM(**lm_params)
        
        if degree > 0:
            poly = PolynomialFeatures(degree=degree) 
            X_train = poly.fit_transform(X_train)
            X_val = poly.transform(X_val)
        
        lm.fit(X_train, y_train)
        lm_scores.append(lm.score(X_val, y_val))
    return lm_scores",0.4301946759,
1847,"save the genes to a file, copy to your laptop, and do go analyses","def gs_ol(species, gses, gpls, genes, ref_species, ref_genes):
    try:
        pgmap = io.read_obj(os.path.join(PLATFORM_PATH, 'probe_gene_map.pkl'))
    except Exception as e:
        pgmap = None
    for species, gse, gpl, gss in zip(species, gses, gpls, genes):
        has_pgmap = pgmap is not None and pgmap.has_key(gpl)
        try:
            spcs_idx = ref_species.index(species)
        except ValueError as e:
            print e
            continue
        for ref_gs in ref_genes[spcs_idx]:
            for gs in gss:
                if (len(gs) == 0 or not gs[0]): continue
                if (has_pgmap):
                    gs = func.flatten_list(map(lambda x: pgmap[gpl].loc[x].split(SC) if x and x in pgmap[gpl].index else x, gs))
                    gs = [x if x.strip() != '///' else 0 for x in gs]
                    gs = [x for x in gs if float(x) != 0]
                gs_sim = difflib.SequenceMatcher(None, gs, ref_gs).ratio()
                if (gs_sim > 0.2): print 'Found %f%% similar gene set with size %i in series %s' % (gs_sim, len(gs), gse)
                    
numprocs = psutil.cpu_count()
task_bnd = njobs.split_1d(len(pred_gses), split_num=numprocs, ret_idx=True)
# gs_ol(pred_species, pred_gses, pred_gpls, pred_genes, ref_species=wkpw_species, ref_genes=wkpw_genes)
_ = njobs.run_pool(gs_ol, n_jobs=numprocs, dist_param=['species', 'gses', 'gpls', 'genes'], species=[pred_species[task_bnd[i]:task_bnd[i+1]] for i in range(numprocs)], gses=[pred_gses[task_bnd[i]:task_bnd[i+1]] for i in range(numprocs)], gpls=[pred_gpls[task_bnd[i]:task_bnd[i+1]] for i in range(numprocs)], genes=[pred_genes[task_bnd[i]:task_bnd[i+1]] for i in range(numprocs)], ref_species=wkpw_species, ref_genes=wkpw_genes)",0.4301455617,
687,"four axes, returned as a array","def Jacobean(X):
    J =np.array([[0, np.cos(X[1])],
                 [1- 3*X[0]**2, 0]])
    return J",0.4655534923,
687,"four axes, returned as a array","def normalize_data(X):
    """"""Normalize data such that it lies in range [0, 1] along every dimension.
    
    Parameters
    ----------
    X : np.array, shape [N, D]
        Data matrix, each row represents a sample.
        
    Returns
    -------
    X_norm : np.array, shape [N, D]
        Normalized data matrix. 
    """"""
    X_norm = np.empty_like(X)
    X_norm[:, 0] = (X[:, 0] - X[:, 0].min())/(X[:, 0].max() - X[:, 0].min())
    X_norm[:, 1] = (X[:, 1] - X[:, 1].min())/(X[:, 1].max() - X[:, 1].min())
    return X_norm",0.4641839266,
687,"four axes, returned as a array","# We'll rescale the epochs to show the increase over baseline using a
# ""z"" score. This subtracts the baseline mean, and divides by baseline
# standard deviation
_ = mne.baseline.rescale(epochs._data, epochs.times, [-.5, 0], 'zscore', copy=False)",0.4602504969,
687,"four axes, returned as a array","%%add_to UKF

@property
def weights_mean(self):
    
    w_m = np.zeros((2*self.n+1, 1))
    # TODO: Calculate the weight to calculate the mean based on the predicted sigma points
    w_m[0] = self.lam / (self.n + self.lam)
    w_m[1:] = 0.5 / (self.n + self.lam)
    
    self.w_m = w_m
    return w_m

@property
def weights_cov(self):
    
    w_cov = np.zeros((2*self.n+1, 1))
    # TODO: Calculate the weight to calculate the covariance based on the predicted sigma points
    w_cov[0] = self.lam / (self.n + self.lam) + (1 - self.alpha**2 - self.betta**2)
    w_cov[1:] = 0.5 / (self.n + self.lam)
    
    self.w_cov = w_cov
    return w_cov


def h(self,Z):
    return np.matmul(np.array([[0.0, 1.0]]), Z) 
    

def update(self,z_in):
    
    # TODO: Implement the update step 
    mu_bar = self.x_bar @ self.weights_mean
    cov_bar = self.weights_cov.T * (self.x_bar - mu_bar) @ (self.x_bar - mu_bar).T + self.q_t
    z = self.h(self.x_bar)
    mu_z = z @ self.weights_mean
    cov_z = self.weights_cov.T * (z - mu_z) @ (z - mu_z).T + self.r_t
    cov_xz = self.weights_cov.T * (self.x_bar - mu_bar) @ (z - mu_z).T
    k = cov_xz @ np.linalg.pinv(cov_z)
    
    mu_t = mu_bar + k * (z_in - mu_z)
    cov_t = cov_bar - k @ cov_z @ k.T  

    self.mu = mu_t
    self.sigma = cov_t
    
    return mu_t, cov_t",0.4599403441,
687,"four axes, returned as a array","def iso_vector(n_loop):
    """"""Generates n_loop unit vectors in the isotropic phase""""""
    u = np.empty((n_loop, 3))
    u[:,2] = np.random.uniform(0,1,n_loop)
    x = np.random.uniform(0,twopi,n_loop)
    fac = np.sqrt(1.0-u[:,2]*u[:,2])
    u[:,0] = fac*np.cos(x)
    u[:,1] = fac*np.sin(x)
    return u",0.4579985738,
687,"four axes, returned as a array","def compare_tails_to_normal(X):
    # Define matrix to store comparisons
    A = np.zeros((2,4))    
    for k in range(4):             
        #stores tail probabilities of the sample series vs a normal series
        A[0, k] = len(X[X > (k + 1)]) / float(len(X)) # Estimate tails of X        
        A[1, k] = 1 - stats.norm.cdf(k + 1) # Compare to Gaussian distribution
    print 'Frequency of std events in X \n1: %s\t2: %s\t3: %s\t4: %s' % tuple(A[0])
    print 'Frequency of std events in a normal process \n1: %s\t2: %s\t3: %s\t4: %s' % tuple(A[1])
    return A

compare_tails_to_normal(X);",0.456258595,
687,"four axes, returned as a array","def mat_to_np(mat):
    elements = mat.A
    return np.array([[elements[j * 4 + i] for i in range(4)] for j in range(4)])

def np_to_mat(np_mat):
    App.Matrix(*np_mat.flatten())

def vec_to_np(vec):
    return np.array(list(vec))",0.4552989006,
687,"four axes, returned as a array","#Use numpy to speed up
def cvt2grayscale_np(img):
    img_size = img.size//3
    grayImage = np.zeros(img_size)
    
    grayImage[i] = [int(0.3* img[3 * i] + 0.59 *
                        img[3 * i + 1] + 0.11 * img[3 * i + 2]) for i in range(0, img_size )][0]

    return np.array(grayImage)",0.4545945525,
687,"four axes, returned as a array","def cov_to_pts( cov ):
    circ = np.linspace( 0, 2*np.pi, 100 )
    sf = np.asarray( [ np.cos( circ ), np.sin( circ ) ] )
    [u,s,v] = np.linalg.svd( cov )
    pmat = u*2.447*np.sqrt(s) # 95% confidence
    return np.dot(  pmat, sf )",0.4542201757,
687,"four axes, returned as a array","#Function to get covariances
def cov_to_pts( cov ):
    circ = np.linspace( 0, 2*np.pi, 100 )
    sf = np.asarray( [ np.cos( circ ), np.sin( circ ) ] )
    [u,s,v] = np.linalg.svd( cov )
    pmat = u*2.447*np.sqrt(s) # 95% confidence
    return np.dot(  pmat, sf )",0.4542201757,
340,create a list of retweet count and status tuples,"def unique_user_mentions():
    result = db.tweets.aggregate([
        {""$unwind"": ""$entities.user_mentions""},
        {""$group"": {
            ""_id"": ""$user.screen_name"",
            ""mset"": {
                ""$addToSet"": ""$entities.user_mentions.screen_name""
            }
        }},
        {""$unwind"": ""$mset""},
        {""$group"": {""_id"": ""$_id"",
                   ""count"": {""$sum"": 1}}},
        {""$sort"": {""count"": -1}},
        {""$limit"": 10}
    ])
    
    return result",0.5514940619,
340,create a list of retweet count and status tuples,"# Creating object for location
def main():
    api = TwitterClient()
    tweets = api.get_tweets(query = '', count = 200)

    for tweet in tweets:
        print(tweet['text'])
        
if __name__ == ""__main__"":
    # calling main function
    main()",0.5389380455,
340,create a list of retweet count and status tuples,"# make this a function to be able to call it at start and end
def get_followers(tag):
    follower_limit=10000
    users=[]
    for row in dbclient.db_followers.top_rt_followers.find(
        {},sort=[('initial_count', -1)]).limit(10):
        users.append(row['_id'])
    for (num,user) in enumerate(users):
        print(""Processing user"", num)
        cursor=tweepy.Cursor(
                        api.followers_ids,
                        count=5000,
                        user_id=user).items(follower_limit)
        while True:
            try: 
                followers=[]
                for follower in cursor:
                    followers.append(follower)
                dbclient.db_followers.top_rt_followers.update_one(
                    {'_id': user},
                    {'$set': { tag+'_followers': followers,
                               tag+'_date': datetime.datetime.now()}},
                    upsert=True)
                break
            except tweepy.TweepError, e:
                s=""""
                if e.response and e.response.status:
                    s=e.response.status
                print(e, s)
                if s == 104:
                    continue
                break
            except Exception, e:
                print(e)
                break",0.5357399583,
340,create a list of retweet count and status tuples,"def highest_ratio():
    result = db.tweets.aggregate([
        {""$match"": {""user.friends_count"": {""$gt"": 0},
                   ""user.follower_count"": {""$gt"": 0}}},
        {""$project"": {""ratio"":{""$divide"": [""$user.followers_count"",
                                          ""$user.friends_count""]},
                     ""screen_name"": ""$user.screen_name""}},
        {""$sort"": {""ratio"": -1}},
        {""$limit"": 1}
    ])
    
    return result",0.5355705023,
340,create a list of retweet count and status tuples,"def arc(i,reference):
    #A function for finding friends
    result = []
    ffs = get_friends_followers(twitter_api,user_id = i,limit=5000,fr_or_fl=True,plimit = 5000)
    for i in range(0,len(ffs) / 100):
            tmp = twitter_func(user_id = ffs[i*100:(i+1)*100],include_user_entities = True)
            if (len(tmp) > 0):
                for r in tmp:
                    if (r['id'] in reference and r['id'] != i): #Someone that a mutual friend is following is my mutual friend
                        result.append((r['id'],r['friends_count'],r['followers_count']))
    return result
                  

def get_network_scores(user_id,init,frac):
    #First thing to do is to limit the search span
    ffs = get_friends_followers(twitter_api,user_id = user_id,limit=5000,fr_or_fl=True,plimit = 5000)
    fo_ids = get_friends_followers(twitter_api,user_id = user_id,limit=5000,fr_or_fl=False,plimit = 5000)
    mutual_friends = list(set(ffs).intersection(set(fo_ids)))[:init]
    print ""now constructing graphs""
    graph = construct_graph(mutual_friends,user_id)
    #print ""now collecting scores""
     ##Calculating base scores and flatten Score dictionary
    #scores = {}
    #vector = []
    #for i in graph.columns.values.tolist():
        #u = twitter_api.users.show(user_id = i,include_user_entities = True)
        #scores[i] = 1.0 * u['followers_count'] / u['friends_count']
        #vector.append(scores[i])
        #print >> sys.stderr, 'Fetched Scores for {0}'.format(i)
    #Normalization
    for i in graph.index:
        n = sum(graph.loc[:,i])
        graph.loc[:,i] = 1.0 * graph.loc[:,i] / n
    rank_score = get_ranks(graph,frac)
    return (user_id, rank_score, graph.index)
    #if method == ""Prank"":
    #    return (user_id,rank_score[0])
    #else:
    #    return (user_id,rank_score[1])

def get_ranks(graph,frac):
    matrix = pd.DataFrame.as_matrix(graph) * frac
    np.fill_diagonal(matrix, 1 - frac)
    print matrix
    #ranks = np.linalg.inv(np.transpose(matrix) -  np.eye(matrix.shape[0])).dot(vector)
    #ranks - min(ranks)
    err = 1
    vector = [1.0 / len(matrix)] * len(matrix)
    oldm = np.asarray(vector)
    count = 0
    while (err > 1e-4 and count <100):
        newm = oldm.dot(np.transpose(matrix))
        err = np.linalg.norm(newm - oldm)
        oldm = newm
        count = count +1
    prank = oldm
    return prank
    
    
def construct_graph(mutual_friends,user_id):
    allids = mutual_friends
    allids.append(user_id)
    graph = pd.DataFrame(0, index = allids,columns = allids)
    get_r = partial(make_twitter_request, twitter_api.friendships.show,count=5000)
    graph.loc[:,user_id] = 1
    graph.loc[user_id,:] = 1
    graph.loc[user_id,user_id] = 0
    #Now looping the realtionships to construct the graph
    i = 0
    while (i < len(mutual_friends)):
        for otid in mutual_friends[i:]:
            tmp = get_r(source_id = mutual_friends[i],target_id = otid)
            if tmp is not None:
                if (tmp['relationship']['source']['followed_by'] == True):
                    graph.loc[otid,mutual_friends[i]] = 1
                if (tmp['relationship']['source']['following'] == True):
                    graph.loc[mutual_friends[i],otid] = 1
        print >> sys.stderr, 'Graph Construction for {0} is finished'.format(mutual_friends[i])
        i = i + 1
    return graph",0.5347366333,
340,create a list of retweet count and status tuples,"class Tweets:
    
    
    def __init__(self,term="""",corpus_size=100):
        self.tweets={}
        if term !="""":
            self.searchTwitter(term,corpus_size)
    
    def searchTwitter(self,term,corpus_size):
        searchTime=datetime.now()
        while (self.countTweets() < corpus_size):
            new_tweets = api.search(term,lang=""en"",tweet_mode='extended',count=corpus_size)
            for nt_json in new_tweets:
                nt = nt_json._json
                if self.getTweet(nt['id_str']) is None and self.countTweets() < corpus_size:
                    self.addTweet(nt,searchTime,term)
            time.sleep(120)
                
    def addTweet(self,tweet,searchTime,term="""",count=0):
        id = tweet['id_str']
        if id not in self.tweets.keys():
            self.tweets[id]={}
            self.tweets[id]['tweet']=tweet
            self.tweets[id]['count']=0
            self.tweets[id]['searchTime']=searchTime
            self.tweets[id]['searchTerm']=term
        self.tweets[id]['count'] = self.tweets[id]['count'] +1
        
    def combineTweets(self,other):
        for otherid in other.getIds():
            tweet = other.getTweet(otherid)
            searchTerm = other.getSearchTerm(otherid)
            searchTime = other.getSearchTime(otherid)
            self.addTweet(tweet,searchTime,searchTerm)
        
    def getTweet(self,id):
        if id in self.tweets:
            return self.tweets[id]['tweet']
        else:
            return None
    
    def getTweetCount(self,id):
        return self.tweets[id]['count']
    
    def countTweets(self):
        return len(self.tweets)
    
    # return a sorted list of tupes of the form (id,count), with the occurrence counts sorted in decreasing order
    def mostFrequent(self):
        ps = []
        for t,entry in self.tweets.items():
            count = entry['count']
            ps.append((t,count))  
        ps.sort(key=lambda x: x[1],reverse=True)
        return ps
    
    # reeturns tweet IDs as a set
    def getIds(self):
        return set(self.tweets.keys())
    
    # save the tweets to a file
    def saveTweets(self,filename):
        json_data =jsonpickle.encode(self.tweets)
        with open(filename,'w') as f:
            json.dump(json_data,f)
    
    # read the tweets from a file 
    def readTweets(self,filename):
        with open(filename,'r') as f:
            json_data = json.load(f)
            incontents = jsonpickle.decode(json_data)   
            self.tweets=incontents
        
    def getSearchTerm(self,id):
        return self.tweets[id]['searchTerm']
    
    def getSearchTime(self,id):
        return self.tweets[id]['searchTime']
    
    def getText(self,id):
        tweet = self.getTweet(id)
        text=tweet['full_text']
        if 'retweeted_status'in tweet:
            original = tweet['retweeted_status']
            text=original['full_text']
        return text",0.5241026878,
340,create a list of retweet count and status tuples,"class Tweets:
    
    
    def __init__(self,term="""",corpus_size=100):
        self.tweets={}
        if term !="""":
            self.searchTwitter(term,corpus_size)
                
    def searchTwitter(self,term,corpus_size):
        searchTime=datetime.now()
        while (self.countTweets() < corpus_size):
            new_tweets = api.search(term,lang=""en"",tweet_mode='extended',count=corpus_size)
            for nt_json in new_tweets:
                nt = nt_json._json
                if self.getTweet(nt['id_str']) is None and self.countTweets() < corpus_size:
                    self.addTweet(nt,searchTime,term)
            time.sleep(30)
                
    def addTweet(self,tweet,searchTime,term="""",count=0):
        id = tweet['id_str']
        if id not in self.tweets.keys():
            self.tweets[id]={}
            self.tweets[id]['tweet']=tweet
            self.tweets[id]['count']=0
            self.tweets[id]['searchTime']=searchTime
            self.tweets[id]['searchTerm']=term
        self.tweets[id]['count'] = self.tweets[id]['count'] +1
        
        
    def combineTweets(self,other):
        for otherid in other.getIds():
            tweet = other.getTweet(otherid)
            searchTerm = otherid.getSearchTerm(id)
            searchTime = otherid.getSearchTime(id)
            self.addTweet(tweet,searchTime,searchTerm)
        
    def getTweet(self,id):
        if id in self.tweets:
            return self.tweets[id]['tweet']
        else:
            return None
    
    def getTweetCount(self,id):
        return self.tweets[id]['count']
    
    def countTweets(self):
        return len(self.tweets)
    
    # return a sorted list of tupes of the form (id,count), with the occurrence counts sorted in decreasing order
    def mostFrequent(self):
        ps = []
        for t,entry in self.tweets.items():
            count = entry['count']
            ps.append((t,count))  
        ps.sort(key=lambda x: x[1],reverse=True)
        return ps
    
    # reeturns tweet IDs as a set
    def getIds(self):
        return set(self.tweets.keys())
    
    # save the tweets to a file
    def saveTweets(self,filename):
        json_data =jsonpickle.encode(self.tweets)
        with open(filename,'w') as f:
            json.dump(json_data,f)
    
    # read the tweets from a file 
    def readTweets(self,filename):
        with open(filename,'r') as f:
            json_data = json.load(f)
            incontents = jsonpickle.decode(json_data)   
            self.tweets=incontents
        
    def getSearchTerm(self,id):
        return self.tweets[id]['searchTerm']
    
    def getSearchTime(self,id):
        return self.tweets[id]['searchTime']
    
    def getText(self,id):
        tweet = self.getTweet(id)
        text=tweet['full_text']
        if 'retweeted_status'in tweet:
            original = tweet['retweeted_status']
            text=original['full_text']
        return text",0.5241026878,
340,create a list of retweet count and status tuples,"class Tweets:
    
    
    def __init__(self,term="""",corpus_size=100):
        self.tweets={}
        if term !="""":
            self.searchTwitter(term,corpus_size)
                
    def searchTwitter(self,term,corpus_size):
        searchTime=datetime.now()
        while (self.countTweets() < corpus_size):
            new_tweets = api.search(term,lang=""en"",tweet_mode='extended',count=corpus_size)
            for nt_json in new_tweets:
                nt = nt_json._json
                if self.getTweet(nt['id_str']) is None and self.countTweets() < corpus_size:
                    self.addTweet(nt,searchTime,term)
            time.sleep(120)
                
    def addTweet(self,tweet,searchTime,term="""",count=0):
        id = tweet['id_str']
        if id not in self.tweets.keys():
            self.tweets[id]={}
            self.tweets[id]['tweet']=tweet
            self.tweets[id]['count']=0
            self.tweets[id]['searchTime']=searchTime
            self.tweets[id]['searchTerm']=term
        self.tweets[id]['count'] = self.tweets[id]['count'] +1

    def combineTweets(self,other):
        for otherid in other.getIds():
            tweet = other.getTweet(otherid)
            searchTerm = other.getSearchTerm(otherid)
            searchTime = other.getSearchTime(otherid)
            self.addTweet(tweet,searchTime,searchTerm)
        
    def getTweet(self,id):
        if id in self.tweets:
            return self.tweets[id]['tweet']
        else:
            return None
    
    def getTweetCount(self,id):
        return self.tweets[id]['count']
    
    def countTweets(self):
        return len(self.tweets)
    
    # return a sorted list of tupes of the form (id,count), with the occurrence counts sorted in decreasing order
    def mostFrequent(self):
        ps = []
        for t,entry in self.tweets.items():
            count = entry['count']
            ps.append((t,count))  
        ps.sort(key=lambda x: x[1],reverse=True)
        return ps
    
    # reeturns tweet IDs as a set
    def getIds(self):
        return set(self.tweets.keys())
    
    # save the tweets to a file
    def saveTweets(self,filename):
        json_data =jsonpickle.encode(self.tweets)
        with open(filename,'w') as f:
            json.dump(json_data,f)
    
    # read the tweets from a file 
    def readTweets(self,filename):
        with open(filename,'r') as f:
            json_data = json.load(f)
            incontents = jsonpickle.decode(json_data)   
            self.tweets=incontents
        
    def getSearchTerm(self,id):
        return self.tweets[id]['searchTerm']
    
    def getSearchTime(self,id):
        return self.tweets[id]['searchTime']
    
    def getText(self,id):
        tweet = self.getTweet(id)
        text=tweet['full_text']
        if 'retweeted_status'in tweet:
            original = tweet['retweeted_status']
            text=original['full_text']
        return text
                
    ### NEW ROUTINE - add a code to a tweet
    def addCode(self,id,code):
        tweet=self.getTweet(id)
        if 'codes' not in tweet:
            tweet['codes']=set()
        tweet['codes'].add(code)
        
    ### NEW ROUTINE  - add multiple  codes for a tweet
    def addCodes(self,id,codes):
        for code in codes:
            self.addCode(id,code)
        
    ### NEW ROUTINE get codes for a tweet
    def getCodes(self,id):
        tweet=self.getTweet(id)
        if 'codes' in tweet:
            return tweet['codes']
        else:
            return None",0.5229102969,
340,create a list of retweet count and status tuples,"def user_mentions():
    result = db.tweets.aggregate([
        {""$unwind"": ""$entities.user_mentions""},
        {""$group"": {""_id"": ""$user.screen_name"",
                   ""count"": {""$sum"": 1}}},
        {""$sort"": {""count"": -1}},
        {""$limit"": 1}
    ])
    
    return result",0.518753171,
340,create a list of retweet count and status tuples,"def hashtag_retweet_avg():
    result = db.tweets.aggregate([
        {""$unwind"": ""$entities.hastags""},
        {""$group"": {""_id"": ""$entities.hastags.text"",
                   ""retweet_avg"": {""$avg"": ""$retweet_count""}}},
        {""$sort"": {""retweet_avg"": -1}}
    ])
    
    return result",0.5180945992,
1592,problem attributes of geometries points,"""""""
PGon Initialization
The PGon constructor takes a defined basis, a collection of vertices, or both. 
If only a collection of verts is given, these are interpreted as world-space 
Points, and an attempt is made to find a Plane in which they lie.
""""""
class PGon(HasPts):
    def __init__(self, vertices=None, basis=None):
        if basis is None and vertices is None : raise GeometricError()
        # if vertices have been provided, but no basis:
        if basis is None:
            # find the plane that best fits the given world-space Points
            # define a CS using the first three Points given
            # initialize using the CS and given Points via HasPts constructor
        # if only a basis or a basis and vertices have been provided:
        else:
            #strip the z-coordinate and initialize via HasPts constructor
            super(PGon,self).__init__([Vec(v.x,v.y) for v in vertices],basis)
            # initialize the the basis 
            self.basis = basis",0.4615182281,
1592,problem attributes of geometries points,"print('CRS:', nbh.crs)
print('Sape:', nbh.shape)",0.4505026937,
1592,problem attributes of geometries points,"import shapely.geometry as geom

def pdtoGeo(df):
    lonlat = [geom.Point(lon, lat) for lon, lat in zip(df.Lon, df.Lat)]
    df = df.drop([""Lon"", ""Lat""], axis = 1) #drop column
    df = gpd.GeoDataFrame(df, geometry = lonlat, crs = {'init' :'epsg:4326'})
    return df",0.4502837956,
1592,problem attributes of geometries points,"print 'Diameter: ', nx.diameter(core22)  # the length of the ""longest shortest path""
#print 'Eccentricity: ', nx.eccentricity(core22) # the maximum distance from some node to all other nodes
print 'Radius: ', nx.radius(core22) # the minimum eccentricity
 
center = nx.center(core22) # the set of nodes where eccentricity is minimal i.e. equal to radius 
periphery = nx.periphery(core22) # the set of nodes with eccentricity equal to the diameter
core22.order() == (len(center) + len(periphery))",0.4494152665,
1592,problem attributes of geometries points,"""""""
HasPts Initialization
The abstract HasPts constructor takes a set of vertices in local coordinates 
and a CS basis.
""""""
    def __init__(self, verts=None,basis=None):
        self._verts = []
        # append verts before basis to ensure local interpretation
        if verts is not None: self.append(verts)
        self._basis = basis",0.4429789186,
1592,problem attributes of geometries points,"# combine subgraphs with a shared coordinate

if tuple(df.coord.iloc[0])==tuple(df.coord.iloc[1]):
    #pull the closest coordinate and get it prepped to be added (or called) as a node
    origin_coord = tuple(df.coord.iloc[0])

    #loop through nodes, creating an edge to each with the origin coordinate
    for listed_edge in df.edge:
        for node_coord in listed_edge:
            if gu.has_edge(origin_coord, node_coord) is False:
                #add the edge connecting existing node to origin
                gu.add_edge(node_coord, origin_coord)

                #store the two coordinates in the Json property
                my_dict = {'coordinates': [node_coord, origin_coord]}
                gu[node_coord][origin_coord]['Json'] = json.dumps(my_dict)",0.4416921437,
1592,problem attributes of geometries points,"geo_pri = op.geometry.GenericGeometry(network=net,
                                      pores=net.pores('primary'),
                                      throats=net.throats('primary'))
geo_sec = op.geometry.GenericGeometry(network=net,
                                      pores=net.pores('secondary'),
                                      throats=net.throats('secondary'))
geo_inter = op.geometry.GenericGeometry(network=net,
                                        throats=net.throats('interconnect'))",0.4415485561,
1592,problem attributes of geometries points,"mesh = uw.mesh.FeMesh_Cartesian( elementType = (""Q1/dQ0""), 
                                 elementRes  = (resx, resy, resy), 
                                 minCoord    = (0., 0., 0.), 
                                 maxCoord    = (boxLength, boxHeight, boxHeight))",0.4404951334,
1592,problem attributes of geometries points,"# Time reversal
TR = qsymm.PointGroupElement(sympy.eye(2), True, False, np.eye(2))

# Chiral symmetry
C = qsymm.PointGroupElement(sympy.eye(2), False, True, np.array([[1, 0], [0, -1]]))

# Atom A rotates into A, B into B.
sphi = 2*sympy.pi/3
RC3 = sympy.Matrix([[sympy.cos(sphi), -sympy.sin(sphi)],
                  [sympy.sin(sphi), sympy.cos(sphi)]])
C3 = qsymm.PointGroupElement(RC3, False, False, np.eye(2))

symmetries = [C, TR, C3]",0.4402614534,
1592,problem attributes of geometries points,"def geo(dat):
    '''
    Since we will do a lot of transformation from normal pandas dataframe to geo dataframe,
    I have made a function to dry my code, 
    automatically return the geo dataframe by inputting normal dataframe.
    '''
    lonlat = [geom.Point(lon, lat) for lon, lat in zip(dat.Lon, dat.Lat)]
    dat.drop([""Lon"", ""Lat""], axis = 1)
    geodat = gpd.GeoDataFrame(dat, geometry = lonlat, crs = {'init' :'epsg:4326'})
    return geodat",0.4398055673,
1794,relationships between dataframes,"for dataset in [train, test]:
    age_avg = dataset.Age.mean()
    age_std = dataset.Age.std()
    age_null_cnt = dataset.Age.isnull().sum()

    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_cnt)
    dataset.Age[np.isnan(dataset.Age)] = age_null_random_list
    dataset.Age = dataset.Age.astype(int)",0.4947158098,
1794,relationships between dataframes,"rxn_fwd.update(verbose=False)
rxn_rev.update(verbose=False)
print(me.reactions.a_to_b_FWD_complex_ab.reaction)
print(me.reactions.a_to_b_REV_complex_ab.reaction)",0.4931960106,
1794,relationships between dataframes,"s.s.keys(), s.g.keys()",0.4924261272,
1794,relationships between dataframes,"oxbridge_orcid_coverage = pandas.DataFrame([
    { oxford.DOI.isin(oxford_orcids_noj).count()/oxford.DOI.count()},
    { cambridge.DOI.isin(cambridge_orcids_noj).count()/cambridge.DOI.count()}
])
oxbridge_orcid_coverage.columns=[ """"]
oxbridge_orcid_coverage = oxbridge_orcid_coverage.rename(index={0:'Oxford', 1: 'Cambridge'})
explode = (0, 0.1, 0, 0)
ax = oxbridge_orcid_coverage.plot.pie("""", figsize=(10,10), colors=['#000036','#B50000'], fontsize=18)
prop = fm.FontProperties(fname='/Library/Fonts/GillSans.ttc')

ax.text(1,0.5,""Cambridge vs Oxford ORCID coverage fight!"", color='#D3A153', fontproperties=prop, size=40)
ax.set_facecolor('#4F6B70')
ax.legend( fontsize=""x-large"")
ax.tick_params(labelsize=18)",0.4911095798,
1794,relationships between dataframes,"m = read_excel(""xls/iLL672.xls"",
               rxn_id_key=""auto"", met_sheet_name=""Appendix 3 iLL672 metabolites"",\
               rxn_str_key=""REACTION"", rxn_gpr_key=""skip"", verbose=False,
               rxn_sheet_name='Appendix 3 iLL672 reactions')
m.reactions[-1].objective_coefficient = 1
m.metabolites.BM.remove_from_model()
add_exchanges(m, ""xt"")
models.append(m)",0.4896767735,
1794,relationships between dataframes,"# ds, df, dp, dn

# if dp and dn are smaller than ds and df
if dp.shape[0] < ds.shape[0]:
    
    # downsample ds and df
    ds = ds[ds[""itest_id""].isin(dn[""itest_id""])]
    df = df[df[""itest_id""].isin(dn[""itest_id""])]
    
    # reset index
    ds = ds.reset_index(drop=True)
    df = df.reset_index(drop=True)


ds = ds.drop([""itest_id"", ""isstem""], axis=1)
df = df.drop([""itest_id"", ""isstem""], axis=1)
dp = dp.drop([""itest_id"", ""isstem""], axis=1)
dn = dn.drop([""itest_id""], axis=1)

print(ds.shape)
print(df.shape)
print(dn.shape)
print(dp.shape)",0.4876580238,
1794,relationships between dataframes,(trsetsum27_31.ID.count()+trsetsum127_131.ID.count())/trset.ID.count(),0.4865159988,
1794,relationships between dataframes,(tsetsum27_31.ID.count()+tsetsum127_131.ID.count())/tset.ID.count(),0.4865159988,
1794,relationships between dataframes,"if len(df_attacks_extended)>0 and len(df_logins_extended)>0:
    merged_attacks_logins = pd.merge(df_attacks_extended,
                                  df_logins_extended,
                                  how = 'left',
                                  left_on = 'date',
                                  right_on = 'date')[['targetcountry','srccountry']]

    who_against_whom = merged_attacks_logins.groupby(['targetcountry','srccountry'])\
                            .size()\
                            .reset_index()\
                            .pivot('srccountry','targetcountry',0)
else:
    who_against_whom =""""",0.4809976816,
1794,relationships between dataframes,"tbd_manager.set_allowed_interaction_types(
    [InteractionTypes.Strong, InteractionTypes.EM])
graph_interaction_settings_groups = tbd_manager.prepare_graphs()
(solutions, violated_rules) = tbd_manager.find_solutions(
        graph_interaction_settings_groups)

print(""found "" + str(len(solutions)) + "" solutions!"")
print_intermediate_states(solutions)",0.4794627428,
916,independent practice python lists,"list1 = list2 = [1,2]
list1 is list2
id(list1), id(list2)",0.4244095087,
916,independent practice python lists,"[a,b] = [3,4]
print(a)
print(b)
[a,b] = [b,a]
print(a)
print(b)",0.4188108444,
916,independent practice python lists,"a=[1,2]
print(a)
assignFiveList(a)
print(a)


a=[1,2]
print(a)
assignPlusFiveList(a)
print(a)",0.4168457985,
916,independent practice python lists,"x = [1,2]
y = x
z = [1,2]
print(x is y)
print(x is z)
print(x is not z) # x and z are not identical...
print(x==y)
print(x==z) # ... yet their contents is the same",0.4152285457,
916,independent practice python lists,"nums = [1, 2, 3]
nums1 = nums
id(nums)     # return a long integer representing the address of the object in memory
id(nums1)    # return the same integer",0.4145638049,
916,independent practice python lists,"my_list_1 = [1, 2, 3]
my_list_2 = my_list_1
print(my_list_1)
print(my_list_2)",0.4145638049,
916,independent practice python lists,"aList = [1,2,3]
bList = aList
print(aList)
print(bList)",0.4145638049,
916,independent practice python lists,"l1 = [1,2]
l2 = [1,2]
l1 is l2",0.4144283235,
916,independent practice python lists,"# the network connection is an iterable and iterator

cur is iter(cur)",0.4128504097,
916,independent practice python lists,"# similar to a file descriptor - 
# the network connection itself is an iterator

cur is iter(cur)",0.4128504097,
137,building a regression tree by hand,dhcrTree.getTreeBranch(['FEM']).printStats2(),0.5145856142,
137,building a regression tree by hand,crTree.getTreeBranch(['FEM']).printStats2(),0.5145856142,
137,building a regression tree by hand,"dhcrTree.getTreeBranch(['XFEM', 'simple']).printStats2()",0.5103395581,
137,building a regression tree by hand,"crTree.getTreeBranch(['XFEM', 'simple']).printStats2()",0.5103395581,
137,building a regression tree by hand,"import math
import numpy as np

class decision_tree_regressor:
    
    def __init__(self, max_depth = None, criteria='std'):
        """"""
        Builds a decision tree to regress on the target data. The
        decision tree is built by trying to minimize the requested
        criteria at each possible split. Starting with the whole data
        the tree will try every possible split (column and value pair)
        and choose the split which results in the greatest reduction 
        of the criteria. Then for each sub-group of that split, the 
        process is repeated recursively until no more splits are possible
        or no splits cause a reductuion of the criteria.
        ---
        KWargs:
        max_depth: how many splits to allow in the tree (depth not breadth)
        criteria: what metric to use as a measure of split strength 
        ('std'= reduction of standard deviation in the data, 'mae'=
        minimize the mean error size (abs value))
        """"""
        self.tree = self.tree_split()
        self.data_cols = None
        self.max_depth = max_depth
        self.current_depth = 0
        self.criteria = criteria
    
    # Sub class for handling recursive nodes (only makes sense in the scope of a tree)
    class tree_split:
        """"""
        A sub class for handling recursive nodes. Each node will contain the value and column
        for the current split, as well as links to the resulting nodes from the split. The 
        results attribute remains empty unless the current node is a leaf. 
        """"""
        def __init__(self,col=-1,value=None,results=None,label=None,tb=None,fb=None):
            self.col=col # column index of criteria being tested
            self.value=value # vlaue necessary to get a true result
            self.results=results # dict of results for a branch, None for everything except endpoints
            self.tb=tb # true decision nodes 
            self.fb=fb # false decision nodes
    
    def split_data(self, X, y, colnum, value):
        """"""
        Returns: Two sets of data from the initial data. Set 1 contains those that passed
        the condition of data[colnum] >= value
        ----------
        Input: The dataset, the column to split on, the value on which to split
        """"""
        splitter = None
        if isinstance(value, int) or isinstance(value,float):
            splitter = lambda x: x[colnum] >= value
        else:
            splitter = lambda x: x[colnum] == value
        split1 = [i for i,row in enumerate(X) if splitter(row)]
        split2 = [i for i,row in enumerate(X) if not splitter(row)]
        set1X = X[split1]
        set1Y = y[split1]
        set2X = X[split2]
        set2Y = y[split2]
        return set1X, set1Y, set2X, set2Y

    def get_mean_target_value(self, data):
        """"""
        Returns: A dictionary of target variable counts in the data
        """"""
        return np.mean(data)

    def split_criteria(self, y):
        """"""
        Returns the criteria we're trying to minimize by splitting.
        Current options are target Mean Absolute Error (from the target 
        mean) or Standard deviation of the target.
        ---
        Input: targets in the split
        Output: Criteria
        """"""
        if self.criteria == 'mae':
            mu = np.mean(y)
            return np.mean(np.abs(y-mu))
        else:
            return np.std(y)
    
    def pandas_to_numpy(self, x):
        """"""
        Checks if the input is a Dataframe or series, converts to numpy matrix for
        calculation purposes.
        ---
        Input: X (array, dataframe, or series)
        Output: X (array)
        """"""
        if type(x) == type(pd.DataFrame()) or type(x) == type(pd.Series()):
            return x.as_matrix()
        if type(x) == type(np.array([1,2])):
            return x
        return np.array(x) 
        
    def handle_1d_data(self,x):
        """"""
        Converts 1 dimensional data into a series of rows with 1 columns
        instead of 1 row with many columns.
        """"""
        if x.ndim == 1:
            x = x.reshape(-1,1)
        return x
    
    def convert_to_array(self, x):
        """"""
        Takes in an input and converts it to a numpy array
        and then checks if it needs to be reshaped for us
        to use it properly
        """"""
        x = self.pandas_to_numpy(x)
        x = self.handle_1d_data(x)
        return x
    
    def fit(self, X, y):
        """"""
        Helper function to wrap the fit method. This makes sure the full nested, 
        recursively built tree gets assigned to the correct variable name and 
        persists after training.
        """"""
        self.tree = self._fit(X,y)
    
    def _fit(self, X, y, depth=0):
        """"""
        Builds the decision tree via a greedy approach, checking every possible
        branch for the best current decision. Decision strength is measured by
        information gain/score reduction. If no information gain is possible,
        sets a leaf node. Recursive calls to this method allow the nesting. If
        max_depth is met, all further nodes become leaves as well.
        ---
        Input: X (feature matrix), y (labels)
        Output: A nested tree built upon the node class.""""""
        X = self.convert_to_array(X)
        y = self.convert_to_array(y)
       
        if len(X) == 0: return tree_split()
        current_score = self.split_criteria(y)

        best_gain = 0.0
        best_criteria = None
        best_sets = None
        
        self.data_cols = X.shape[1]
        
        
        # Here we go through column by column and try every possible split, measuring the
        # information gain. We keep track of the best split then use that to send the split
        # data sets into the next phase of splitting.
        
        for col in range(self.data_cols):
            column_values = set(X.T[col])
            for value in column_values:
                set1, set1_y, set2, set2_y = self.split_data(X, y, col, value)
                p = float(len(set1)) / len(y)
                gain = current_score - p*self.split_criteria(set1_y) - (1-p)*self.split_criteria(set2_y)
                if gain > best_gain and len(set1_y) and len(set2_y):
                    best_gain = gain
                    best_criteria = (col, value)
                    best_sets = (np.array(set1), np.array(set1_y), np.array(set2), np.array(set2_y))
        
        # Now decide whether it's an endpoint or we need to split again.
        if (self.max_depth and depth < self.max_depth) or not self.max_depth:
            if best_gain > 0:
                self.current_depth += 1
                true_branch = self._fit(best_sets[0], best_sets[1], depth=depth+1)
                false_branch = self._fit(best_sets[2], best_sets[3], depth=depth+1)
                return self.tree_split(col=best_criteria[0], value=best_criteria[1],
                        tb=true_branch, fb=false_branch)
            else:
                return self.tree_split(results=self.get_mean_target_value(y))
        else:
            return self.tree_split(results=self.get_mean_target_value(y))

    def print_tree(self, indent=""---""):
        """"""
        Helper function to make sure the correct tree gets printed.
        ---
        In: indent (how to show splits between nodes)
        """"""
        self.__original_indent = indent
        self._print_tree_(self.tree, indent)
    
    def _print_tree_(self, tree, indent):
        """"""
        Goes through node by node and reports the column and value used to split
        at that node. All sub-nodes are drawn in sequence below the node.
        """"""
        if tree.results: # if this is a end node
            print(str(tree.results))
        else:
            print('Column ' + str(tree.col)+' : '+str(tree.value)+'? ')
            # Print the branches
            print(indent+' True: ', end=' ')
            next_indent = indent+self.__original_indent
            self._print_tree_(tree.tb,indent=next_indent)
            print(indent+' False: ', end=' ')
            self._print_tree_(tree.fb,indent=next_indent)

    def predict(self, newdata):
        """"""
        Helper function to make sure the correct tree is used to
        make predictions. Also manages multiple rows of input data
        since the tree must predict one at a time.
        ---
        In: new data point of the same structure as the training X.
        Out: numpy array of the resulting predictions
        """"""
        results = []
        newdata = self.convert_to_array(newdata)
        for x in newdata:
            results.append(self._predict(x,self.tree))
        return np.array(results)
            
    def _predict(self, newdata, tree):
        """"""
        Uses the reusive structure of the tree to follow each split for
        a new data point. If the node is an endpoint, the available classes
        are sorted by ""most common"" and then the top choice is returned.
        """"""
        if tree.results: # if this is a end node
            return tree.results

        if isinstance(newdata[tree.col], int) or isinstance(newdata[tree.col],float):
            if newdata[tree.col] >= tree.value:
                return self._predict(newdata, tree.tb)

            else:
                return self._predict(newdata, tree.fb)
        else:
            if newdata[tree.col] == tree.value:
                return self._predict(newdata, tree.tb)
            else:
                return self._predict(newdata, tree.fb) 

    def score(self, X, y):
        """"""
        Uses the predict method to measure the accuracy of the model.
        ---
        In: X (list or array), feature matrix; y (list or array) labels
        Out: accuracy (float)
        """"""
        pred = self.predict(X)
        return np.mean((pred-y)**2)",0.5063203573,
137,building a regression tree by hand,"crTree.getTreeBranch(['XFEM', 'crackPartition', 'LinearTet']).printStats2()",0.5031177998,
137,building a regression tree by hand,"dhcrTree.getTreeBranch(['XFEM', 'crackPartition', 'LinearTet']).printStats2()",0.5031177998,
137,building a regression tree by hand,"import math
import numpy as np

class decision_tree_classifier:
    
    def __init__(self, max_depth = None):
        """"""
        Builds a decision tree to classify target data. The
        decision tree is built by trying to minimize the requested
        criteria at each possible split. Starting with the whole data
        the tree will try every possible split (column and value pair)
        and choose the split which results in the greatest reduction 
        of the criteria. Then for each sub-group of that split, the 
        process is repeated recursively until no more splits are possible
        or no splits cause a reduction of the criteria. In this class
        entropy is used as the criteria.
        ---
        KWargs:
        max_depth: how many splits to allow in the tree (depth not breadth)
        """"""
        self.tree = self.tree_split()
        self.max_depth = max_depth
    
    # Sub class for handling recursive nodes (only makes sense in the scope of a tree)
    class tree_split:
        """"""
        A sub class for handling recursive nodes. Each node will contain the value and column
        for the current split, as well as links to the resulting nodes from the split. The 
        results attribute remains empty unless the current node is a leaf. 
        """"""
        def __init__(self,col=-1,value=None,results=None,label=None,tb=None,fb=None):
            self.col=col # column index of criteria being tested
            self.value=value # vlaue necessary to get a true result
            self.results=results # dict of results for a branch, None for everything except endpoints
            self.tb=tb # true decision nodes 
            self.fb=fb # false decision nodes
    
    def split_data(self, X, y, colnum, value):
        """"""
        Returns: Two sets of data from the initial data. Set 1 contains those that passed
        the condition of data[colnum] >= value
        ----------
        Input: The dataset, the column to split on, the value on which to split
        """"""
        splitter = None
        if isinstance(value, int) or isinstance(value,float):
            splitter = lambda x: x[colnum] >= value
        else:
            splitter = lambda x: x[colnum] == value
        split1 = [i for i,row in enumerate(X) if splitter(row)]
        split2 = [i for i,row in enumerate(X) if not splitter(row)]
        set1X = X[split1]
        set1Y = y[split1]
        set2X = X[split2]
        set2Y = y[split2]
        return set1X, set1Y, set2X, set2Y

    def count_target_values(self, data):
        """"""
        Returns: A dictionary of target variable counts in the data
        """"""
        results = {}
        counts = np.unique(data, return_counts=True)
        for i,j in zip(*counts):
            results[i] = j
        return results

    def entropy(self, y):
        """"""
        Returns: Entropy of the data set, based on target values. 
        ent = Sum(-p_i Log(p_i), i in unique targets) where p is the percentage of the
        data with the ith label.
        Sidenote: We're using entropy as our measure of good splits. It corresponds to 
        information gained by making this split. If the split results in only one target type
        then the entropy new sets entropy is 0. If it results in a ton of different targets, the
        entropy will be high. 
        """"""
        results = self.count_target_values(y)
        log_base = len(results.keys())
        if log_base < 2:
            log_base = 2
        logb=lambda x:math.log(x)/math.log(log_base)
        ent=0.
        for r in results.keys():
            p=float(results[r])/len(y) 
            ent-=p*logb(p)
        return ent  
    
    def pandas_to_numpy(self, x):
        """"""
        Checks if the input is a Dataframe or series, converts to numpy matrix for
        calculation purposes.
        ---
        Input: X (array, dataframe, or series)
        Output: X (array)
        """"""
        if type(x) == type(pd.DataFrame()) or type(x) == type(pd.Series()):
            return x.as_matrix()
        if type(x) == type(np.array([1,2])):
            return x
        return np.array(x) 
    
    def handle_1d_data(self,x):
        """"""
        Converts 1 dimensional data into a series of rows with 1 columns
        instead of 1 row with many columns.
        """"""
        if x.ndim == 1:
            x = x.reshape(-1,1)
        return x
    
    def convert_to_array(self, x):
        """"""
        Takes in an input and converts it to a numpy array
        and then checks if it needs to be reshaped for us
        to use it properly
        """"""
        x = self.pandas_to_numpy(x)
        x = self.handle_1d_data(x)
        return x
    
    def fit(self, X, y):
        """"""
        Helper function to wrap the fit method. This makes sure the full nested, 
        recursively built tree gets assigned to the correct variable name and 
        persists after training.
        """"""
        self.tree = self._fit(X,y)
    
    def _fit(self, X, y, depth=0):
        """"""
        Builds the decision tree via a greedy approach, checking every possible
        branch for the best current decision. Decision strength is measured by
        information gain/entropy reduction. If no information gain is possible,
        sets a leaf node. Recursive calls to this method allow the nesting. If
        max_depth is met, all further nodes become leaves as well.
        ---
        Input: X (feature matrix), y (labels)
        Output: A nested tree built upon the node class.""""""
        X = self.convert_to_array(X)
        y = self.convert_to_array(y)

        if len(X) == 0: return tree_split()
        current_score = self.entropy(y)

        best_gain = 0.0
        best_criteria = None
        best_sets = None
        
        self.data_cols = X.shape[1]
        
        
        # Here we go through column by column and try every possible split, measuring the
        # information gain. We keep track of the best split then use that to send the split
        # data sets into the next phase of splitting.
        
        for col in range(self.data_cols):
            
            # find different values in this column
            column_values = set(X.T[col])
            # for each possible value, try to divide on that value
            for value in column_values:
                set1, set1_y, set2, set2_y = self.split_data(X, y, col, value)

                # Information gain
                p = float(len(set1)) / len(y)
                gain = current_score - p*self.entropy(set1_y) - (1-p)*self.entropy(set2_y)
                if gain > best_gain and len(set1_y) and len(set2_y):
                    best_gain = gain
                    best_criteria = (col, value)
                    best_sets = (np.array(set1), np.array(set1_y), np.array(set2), np.array(set2_y))
        
        
        # Now decide whether it's an endpoint or we need to split again.
        if (self.max_depth and depth < self.max_depth) or not self.max_depth:
            if best_gain > 0:
                true_branch = self._fit(best_sets[0], best_sets[1], depth=depth+1)
                false_branch = self._fit(best_sets[2], best_sets[3], depth=depth+1)
                return self.tree_split(col=best_criteria[0], value=best_criteria[1],
                        tb=true_branch, fb=false_branch)
            else:
                return self.tree_split(results=self.count_target_values(y))
        else:
            return self.tree_split(results=self.count_target_values(y))

    def print_tree(self, indent=""---""):
        """"""
        Helper function to make sure the correct tree gets printed.
        ---
        In: indent (how to show splits between nodes)
        """"""
        self.__original_indent = indent
        self._print_tree_(self.tree, indent)
    
    def _print_tree_(self, tree, indent):
        """"""
        Goes through node by node and reports the column and value used to split
        at that node. All sub-nodes are drawn in sequence below the node.
        """"""
        if tree.results: # if this is a end node
            print(str(tree.results))
        else:
            print('Column ' + str(tree.col)+' : '+str(tree.value)+'? ')
            # Print the branches
            print(indent+' True: ', end=' ')
            next_indent = indent+self.__original_indent
            self._print_tree_(tree.tb,indent=next_indent)
            print(indent+' False: ', end=' ')
            self._print_tree_(tree.fb,indent=next_indent)

    def predict(self, newdata):
        """"""
        Helper function to make sure the correct tree is used to
        make predictions. Also manages multiple rows of input data
        since the tree must predict one at a time.
        ---
        In: new data point of the same structure as the training X.
        Out: numpy array of the resulting predictions
        """"""
        results = []
        for x in newdata:
            results.append(self._predict(x,self.tree))
        return np.array(results)
            
    def _predict(self, newdata, tree):
        """"""
        Uses the reusive structure of the tree to follow each split for
        a new data point. If the node is an endpoint, the available classes
        are sorted by ""most common"" and then the top choice is returned.
        """"""
        newdata = self.pandas_to_numpy(newdata)
        if tree.results: # if this is a end node
            return sorted(list(tree.results.items()), key=lambda x: x[1],reverse=True)[0][0]

        if isinstance(newdata[tree.col], int) or isinstance(newdata[tree.col],float):
            if newdata[tree.col] >= tree.value:
                return self._predict(newdata, tree.tb)

            else:
                return self._predict(newdata, tree.fb)
        else:
            if newdata[tree.col] == tree.value:
                return self._predict(newdata, tree.tb)
            else:
                return self._predict(newdata, tree.fb) 

    def score(self, X, y):
        """"""
        Uses the predict method to measure the accuracy of the model.
        ---
        In: X (list or array), feature matrix; y (list or array) labels
        Out: accuracy (float)
        """"""
        pred = self.predict(X)
        correct = 0
        for i,j in zip(y,pred):
            if i == j:
                correct+=1
        return float(correct)/float(len(y))",0.4857211411,
137,building a regression tree by hand,"rf = sess.decisionTree.forestTrain(
  table={
    ""name"":""hr_part"",
    ""where"":""strip(put(_partind_, best.))='1'""
  },
  inputs=all_inputs,
  nominals=class_vars,
  target=""left"",
  nTree=20,
  nBins=20,
  leafSize=5,
  maxLevel=21,
  crit=""GAIN"",
  varImp=True,
  missing=""USEINSEARCH"",
  vote=""PROB"",
  OOB=True,
  casOut={""name"":""forest_model"", ""replace"":True}
)

# Output model statistics
render_html(rf)

# Score 
sess.decisionTree.forestScore(
  table={""name"":""hr_part""},
  modelTable={""name"":""forest_model""},
  casOut={""name"":""_scored_rf"", ""replace"":True},
  copyVars={""left"", ""_partind_""},
  vote=""PROB""
)

# Create p_b_tgt0 and p_b_tgt1 as _rf_predp_ is the probability of event in _rf_predname_
sess.dataStep.runCode(
  code=""""""data _scored_rf; set _scored_rf; if _rf_predname_=1 then do; p_left1=_rf_predp_; 
    p_left0=1-p_left1; end; if _rf_predname_=0 then do; p_left0=_rf_predp_; p_left1=1-p_left0; end; run;""""""
)",0.4775550961,
137,building a regression tree by hand,"# Train tree
tree.fit(train, train_labels)
print(f'Decision tree has {tree.tree_.node_count} nodes with maximum depth {tree.tree_.max_depth}.')",0.4772791862,
1846,save the data and perform an enrichment analysis,"def update(self):
        """"""
        Interface: 
        After trabeculectomy is done, nothing is done besides set number of visit
        until IOP > IOP target
        """"""
        if self.medicalRecords['OnTrabeculectomy'] == False:
            self.SurgeryTE()
        else:
            self.SetNumberofVisits()
        #Check whether IOP > IOP target yet. Then evaluate whether to leave the block    
        self.DeterminetoExitTEblock()",0.5055013895,
1846,save the data and perform an enrichment analysis,"def autocount_tracks():
    # Otsu
    print('* counting tracks in Otsu binary images.')
    ds.count_and_save(regotsu_mica,
                      filename='auto_count/autootsu_mica.csv',
                      show_exectime=False)
    ds.count_and_save(regotsu_apte,
                      filename='auto_count/autootsu_apatite.csv',
                      show_exectime=False)

    # Yen
    print('* counting tracks in Yen binary images.')
    ds.count_and_save(regyen_mica,
                      filename='auto_count/autoyen_mica.csv',
                      show_exectime=False)
    ds.count_and_save(regyen_apte,
                      filename='auto_count/autoyen_apatite.csv',
                      show_exectime=False)

    # Li
    print('* counting tracks in Li binary images.')
    ds.count_and_save(regli_mica,
                      filename='auto_count/autoli_mica.csv',
                      show_exectime=False)
    ds.count_and_save(regli_apte,
                      filename='auto_count/autoli_apatite.csv',
                      show_exectime=False)

    # ISODATA
    print('* counting tracks in ISODATA binary images.')
    ds.count_and_save(regiso_mica,
                      filename='auto_count/autoiso_mica.csv',
                      show_exectime=False)
    ds.count_and_save(regiso_apte,
                      filename='auto_count/autoiso_apatite.csv',
                      show_exectime=False)

    # MLSS
    print('* counting tracks in MLSS binary images.')
    ds.count_and_save(regmlss_mica,
                      filename='auto_count/automlss_mica.csv',
                      show_exectime=False)
    ds.count_and_save(regmlss_apte,
                      filename='auto_count/automlss_apatite.csv',
                      show_exectime=False)

    return None",0.5007377863,
1846,save the data and perform an enrichment analysis,"def get_blob_coord(train_img):
    t = time.time()
    print ('Getting Color Blobs...')
    colors = ['red','magenta','brown','blue','green','error']
    df_blob = pd.DataFrame(index=df.index, columns=colors)

    count = 0
    R, G, B, color = [], [], [], []
    for fname in range(train_img[0],train_img[1]):
        count += 1
        if fname in list(df_skip['train_id']): continue
            
        img_1 = mpimg.imread('./input/Train/' + str(fname) + '.jpg')
        img_2 = mpimg.imread('./input/TrainDotted/' + str(fname) + '.jpg')
        img_3 = cv2.absdiff(img_2,img_1)
    
        mask_2 = cv2.cvtColor(img_2, cv2.COLOR_RGB2GRAY)
        mask_2[mask_2 < 20] = 0
        mask_2[mask_2 > 0] = 255

        img_4 = cv2.bitwise_or(img_3, img_3, mask=mask_2)
        img_5 = cv2.cvtColor(img_4, cv2.COLOR_RGB2GRAY)

        blobs = blob_log(img_5, min_sigma=3, max_sigma=4, num_sigma=1, threshold=0.02)
        red, magenta, green, blue, brown, error = [], [], [], [], [], []
        
        #cut = np.copy(img_2)
        for blob in blobs:
            y, x, s = blob
            r,g,b = img_2[int(y)][int(x)][:]
        
            pred = predict_color(r,g,b)
            
            if pred[0] == 'red':
                red.append((int(y),int(x)))
                color.append('red')
            elif pred[0] == 'magenta':
                magenta.append((int(y),int(x)))
                color.append('magenta')
            elif pred[0] == 'brown':
                brown.append((int(y),int(x)))
                color.append('brown')
            elif pred[0] == 'blue':
                blue.append((int(y),int(x)))
                color.append('blue')
            elif pred[0] == 'green':
                green.append((int(y),int(x)))
                color.append('green')
            else:
                error.append((int(y),int(x)))
                color.append('error')
                
            R.append(int(r))
            G.append(int(g))
            B.append(int(b))
            #cv2.rectangle(cut, (int(x)-32,int(y)-32),(int(x)+32,int(y)+32), 0,-1)
        
        if count % 25 == 0:
            print ('File progress:',fname,len(red),len(magenta),len(brown),len(blue),len(green),len(error))
            
        df_blob.loc[fname,'red'] = red
        df_blob.loc[fname,'magenta'] = magenta
        df_blob.loc[fname,'brown'] = brown
        df_blob.loc[fname,'blue'] = blue
        df_blob.loc[fname,'green'] = green
        df_blob.loc[fname,'error'] = error
    
    df_color = pd.DataFrame({'R':R, 'G':G, 'B':B,'color':color})
    hf.check_colors(df,df_color,train_img)
    print ('\nTotal Training Images:',train_img[1]-train_img[0])
    print ('Time for getting Sea Lion coordinates:', np.round(time.time() - t, 4),'\n')
    return R,G,B,color,df_color,df_blob",0.4959599376,
1846,save the data and perform an enrichment analysis,"def test(isTrue): #Runs CNN on test images
    pred=fitAndPredict()
    print(""creating test file"")
    df = pd.DataFrame(pred, columns=['Type_1','Type_2','Type_3']) #Instantiates dataframe
    df['image_name'] = test_id #image_name holds the .jpg file name
    if (isTrue): #if(True), it will create a .csv file with the dataframe
        df.to_csv('test.csv', index=False)
        print(""Test file created in users/keerat/..."")
    else: #if(False), it will just show the dataframe
        print(df.to_string())",0.4953256547,
1846,save the data and perform an enrichment analysis,"def run_analysis(project_file, annotation_file, pimp_file):
    
    # load m2lda object and do thresholding
    ms2lda = Ms2Lda.resume_from(project_file)
    ms2lda.do_thresholding(th_doc_topic=0.05, th_topic_word=0.01)

    # read the annotation file
    print
    print ""Annotation file""
    motif_annotation = {}
    motif_idx = {}
    i = 0
    for item in csv.reader(open(annotation_file), skipinitialspace=True):
        key = int(item[0])
        val = item[1]
        print str(key) + ""\t"" + val
        motif_annotation[key] = val
        motif_idx[key] = i
        i += 1

    motifs_of_interest = motif_annotation.keys()    
    norm = mpl.colors.Normalize(vmin=min(motif_idx.values()), vmax=max(motif_idx.values()))
    cmap = cm.gist_rainbow
    motif_colour = cm.ScalarMappable(norm=norm, cmap=cmap)    
    
    # get the network graph out for the motifs of interest
    G = get_network_graph(ms2lda, motifs_of_interest)
    print ""\n"" + nx.info(G)  
    print

    # print out some info
    ms1_count = 0
    nodes = G.nodes(data=True)
    for node_id, node_data in nodes:
        # 1 for doc, 2 for motif
        if node_data['group'] == 1: 
            ms1_count += 1
    print ""%d (out of %d) MS1 peaks found in the graph"" % (ms1_count, ms2lda.ms1.shape[0])    
    
    # load the pimp differential analysis file for matching
    de_peaks = []
    with open(pimp_file, ""rb"") as infile:
       reader = csv.reader(infile)
       next(reader, None)  # skip the headers
       for row in reader:
        PiMP_ID = int(row[0])
        polarity = row[1]
        mz = float(row[2])
        rt = float (row[3])
        mh_intensity = float(row[4])
        tup = (PiMP_ID, polarity, mz, rt, mh_intensity)
        de_peaks.append(tup)

    print
    print ""PiMP list: ""
    for tup in de_peaks:
        print tup
        
    # do the matching
    mass_tol = 3
    rt_tol = 12

    std = np.array(de_peaks)
    std_mz = np.array([x[2] for x in de_peaks])
    std_rt = np.array([x[3] for x in de_peaks])
    matches = {}

    ms1_label = {}
    for row in ms2lda.ms1.itertuples(index=True):
        peakid = row[1]
        mz = row[5]
        rt = row[4]

        # the following line is hacky for pos mode data
        mass_delta = mz*mass_tol*1e-6
        mass_start = mz-mass_delta
        mass_end = mz+mass_delta
        rt_start = rt-rt_tol
        rt_end = rt+rt_tol

        match_mass = (std_mz>mass_start) & (std_mz<mass_end)
        match_rt = (std_rt>rt_start) & (std_rt<rt_end)
        match = match_mass & match_rt

        res = std[match]
        if len(res) == 1:
            closest = tuple(res[0])
            matches[closest] = row
            ms1_label[row[1]] = closest[1]        
        elif len(res)>1:
            closest = None
            min_dist = sys.maxint
            for match_res in res:
                match_mz = float(match_res[2])
                match_rt = float(match_res[3])
                dist = math.sqrt((match_rt-rt)**2 + (match_mz-mz)**2)
                if dist < min_dist:
                    min_dist = dist
                    closest = match_res
            closest = tuple(closest)
            matches[closest] = row
            ms1_label[row[1]] = closest[1]

    print ""Matches found %d/%d"" % (len(matches), len(std))
    print

    ms1_list = []
    for match in matches:
        key = str(match)
        ms1_row = matches[match]
        value = str(ms1_row)
        pid = ms1_row[1]
        print ""Standard %s"" % key
        print ""MS1 %s"" % value
        print
        ms1_list.append(pid)
        
    # print the motifs and count their occurences
    m2m_list = motifs_of_interest
    word_map, motif_words = ms2lda.print_motif_features(quiet=True)

    c = Counter() # count the motif occurences
    for i in range(len(ms1_list)):

        ms1 = ms1_list[i]
        df = print_report(ms2lda, G, ms1, motif_annotation, motif_words, motif_colour, motif_idx, word_map, xlim_upper=770)
        if df is not None:

            # display(df) # show the table to see the mz, annotations, etc

            # get the motif ids in the dataframe
            fragment_motif_ids = df[['fragment_motif']].values.flatten()
            loss_motif_ids = df[['loss_motif']].values.flatten()

            # get rid of nan values
            fragment_motif_ids = fragment_motif_ids[~np.isnan(fragment_motif_ids)]
            loss_motif_ids = loss_motif_ids[~np.isnan(loss_motif_ids)]

            # store the unique counts
            combined = np.append(fragment_motif_ids, loss_motif_ids).astype(int)
            combined = set(combined.tolist())
            c.update(combined)
            
    return c",0.4921550751,
1846,save the data and perform an enrichment analysis,"def myexamples(preprocess_flag):
    global saver
    global sess
    arr=[]
    X_myex=[]
    y_ex=[]
    num=1
    plt.figure(figsize=(15,15))
    saver.restore(sess, tf.train.latest_checkpoint('./'))
    for filename in os.listdir(""myexamples/""):
        arr=[]
        image = cv2.imread('myexamples/'+filename)
        image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)
        image= cv2.resize(image,(32,32))
        plt.figure(figsize=(15,15))
        plt.subplot(10,2,num)
        plt.imshow(image,interpolation=""nearest"", cmap=""gray"")
        
        image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)
        h,w,c=image.shape
        image.resize(1,h,w,c)
        out=pipeline(image,preprocess_flag)
        arr.append(out[0])
        X_myex.append(arr[0])
        arr=np.array(arr)
        values,indices =softmax(arr)
        e_x = np.exp(values - np.max(values))
        soft_out = e_x / e_x.sum()
        index=[]
        global df
        index=[df['SignName'][int(key)] for key in tuple(indices.tolist()[0])]
        plt.subplot(10,2,num+1)
        plt.xlim(0,1)
        input_x = np.arange(len(index))
        plt.yticks(input_x, index)
        plt.barh(input_x,soft_out.tolist()[0])    
    #for usage by showfeature
    X_myex=np.array(X_myex)
    return X_myex",0.4913132191,
1846,save the data and perform an enrichment analysis,"def lots_of_edges(match):
    df = pd.read_csv(match['match.csv'].abspath)
    df = df[['Champion','Win']]
    winners = df[df['Win'] == True]
    winners_names = list(winners['Champion'].unique())
    losers = df[df['Win'] == False]
    losers_names = list(losers['Champion'].unique())
    mixwinnerslosers = pd.DataFrame(columns=['Champion','Win'])
    for winner in winners_names:
        df1 = pd.DataFrame({df.keys()[0]: [winner]*5, df.keys()[1]: [losers_names[0], losers_names[1], losers_names[2], losers_names[3], losers_names[4]]})
        mixwinnerslosers = pd.concat([mixwinnerslosers,df1])
    
    return mixwinnerslosers",0.4908418059,
1846,save the data and perform an enrichment analysis,"def print_revenues_description(data):
    treated = data.re78[data.treat == True]
    not_treated = data.re78[data.treat == False]   
    print(""Treated revenues in 1978:\n"", treated.describe())
    print(""\nNot treated revenues in 1978:\n"", not_treated.describe())",0.4900709093,
1846,save the data and perform an enrichment analysis,"def print_1978_earnings(data):
    """""" This function print the box and ccdf of the 1978 earnings""""""
    treated = data.re78[data.treat == True]
    not_treated = data.re78[data.treat == False]   
    print_box_1978_earnings(treated, not_treated)
    print_ccdf_1978_earnings(treated,not_treated)",0.4883524179,
1846,save the data and perform an enrichment analysis,"#Get all the vocabulary in the corpus and its features:
def vocab_pl(data):
    #get dictionary data
    pl_data = pd.read_csv('pl_dict_clean.csv')
    pl_data = pl_data[['word','fam','conc','imag','kf_wf']]
    pl_data = pl_data.dropna(thresh=2).drop_duplicates()
    with warnings.catch_warnings():
        warnings.simplefilter(""ignore"", category=RuntimeWarning)
        pl_data = pl_data.groupby('word').agg({'fam': np.nanmean,'conc': np.nanmean,'imag': np.nanmean, 
                                               'kf_wf':np.nanmean}).reset_index() 

    vocab = []
    for i in data.index.values:
        # get rid of newlines, and non alpha-numeric characters
        text = data.loc[i,'X']
        text = text.strip().replace(""\n"", "" "").replace(""\r"", "" "")
        text = re.sub(r'([^\s\w]|_)+', ' ', text)
        text = text.replace(' +',' ')
        parsed_text = nlp(text)
        token_text = [token.orth_ for token in parsed_text]
        for token in token_text:
            vocab.append(token.upper())
    vocab = list(set(vocab))
    
    vocab_in_pl = [word for word in vocab if word in list(pl_data.word)]
    pl_data =  pl_data[pl_data['word'].isin(vocab_in_pl)]
    
    return pl_data",0.4859477878,
1024,lab independent component analysis,"def pca_sklearn():
    pca = PCA()
    pca.fit(X)

    V = pca.components_
    S = pca.explained_variance_
    X_mean = pca.mean_

    plt.scatter(*X.T, marker='o', edgecolors='b', c='none', s=15);
    plt.plot(*array([X_mean, X_mean + S[0] * V[:,0]]).T, 'r-')
    plt.plot(*array([X_mean, X_mean + S[1] * V[:,1]]).T, 'r-')
    plt.xlim(0,8)
    plt.ylim(2,8)
    
    pca.set_params(n_components=1)
    pca.fit(X)
    X_redux = pca.transform(X)

    plt.scatter(*(pca.inverse_transform(X_redux)).T, marker='o', edgecolors='g', c='none', s=15);
    plt.axis('scaled');
    return S, V

pca_sklearn()",0.5171123743,
1024,lab independent component analysis,"import math
from matplotlib import (cm, pyplot as plt, mlab)

def visualize(word, model):
    """""" visualize the input model for a particular word """"""
    variance=np.array([np.diag(model.covars_[i]) for i in range(model.n_components)])
    figures = []
    for parm_idx in range(len(model.means_[0])):
        xmin = int(min(model.means_[:,parm_idx]) - max(variance[:,parm_idx]))
        xmax = int(max(model.means_[:,parm_idx]) + max(variance[:,parm_idx]))
        fig, axs = plt.subplots(model.n_components, sharex=True, sharey=False)
        colours = cm.rainbow(np.linspace(0, 1, model.n_components))
        for i, (ax, colour) in enumerate(zip(axs, colours)):
            x = np.linspace(xmin, xmax, 100)
            mu = model.means_[i,parm_idx]
            sigma = math.sqrt(np.diag(model.covars_[i])[parm_idx])
            ax.plot(x, mlab.normpdf(x, mu, sigma), c=colour)
            ax.set_title(""{} feature {} hidden state #{}"".format(word, parm_idx, i))

            ax.grid(True)
        figures.append(plt)
    for p in figures:
        p.show()",0.492321223,
1024,lab independent component analysis,"import math
from matplotlib import (cm, pyplot as plt, mlab)

def visualize(word, model):
    """""" visualize the input model for a particular word """"""
    variance=np.array([np.diag(model.covars_[i]) for i in range(model.n_components)])
    figures = []
    for parm_idx in range(len(model.means_[0])):
        xmin = int(min(model.means_[:,parm_idx]) - max(variance[:,parm_idx]))
        xmax = int(max(model.means_[:,parm_idx]) + max(variance[:,parm_idx]))
        fig, axs = plt.subplots(model.n_components, sharex=True, sharey=False)
        colours = cm.rainbow(np.linspace(0, 1, model.n_components))
        for i, (ax, colour) in enumerate(zip(axs, colours)):
            x = np.linspace(xmin, xmax, 100)
            mu = model.means_[i,parm_idx]
            sigma = math.sqrt(np.diag(model.covars_[i])[parm_idx])
            ax.plot(x, mlab.normpdf(x, mu, sigma), c=colour)
            ax.set_title(""{} feature {} hidden state #{}"".format(word, parm_idx, i))

            ax.grid(True)
        figures.append(plt)
    for p in figures:
        p.show()
        
visualize('CHOCOLATE', model)",0.492321223,
1024,lab independent component analysis,"import math
from matplotlib import (cm, pyplot as plt, mlab)

def visualize(word, model):
    """""" visualize the input model for a particular word """"""
    variance=np.array([np.diag(model.covars_[i]) for i in range(model.n_components)])
    figures = []
    for parm_idx in range(len(model.means_[0])):
        xmin = int(min(model.means_[:,parm_idx]) - max(variance[:,parm_idx]))
        xmax = int(max(model.means_[:,parm_idx]) + max(variance[:,parm_idx]))
        #xmin = -10
        #xmax = 10
        fig, axs = plt.subplots(model.n_components, sharex=True, sharey=False)
        colours = cm.rainbow(np.linspace(0, 1, model.n_components))
        for i, (ax, colour) in enumerate(zip(axs, colours)):
            x = np.linspace(xmin, xmax, 100)
            mu = model.means_[i,parm_idx]
            sigma = math.sqrt(np.diag(model.covars_[i])[parm_idx])
            ax.plot(x, mlab.normpdf(x, mu, sigma), c=colour)
            ax.set_title(""{} feature {} hidden state #{}"".format(word, parm_idx, i))

            ax.grid(True)
        figures.append(plt)
    for p in figures:
        p.show()
    
model, logL = train_a_word('CHOCOLATE', 3, features_ground)
visualize('CHOCOLATE', model)",0.4901305735,
1024,lab independent component analysis,"import math
from matplotlib import (cm, pyplot as plt, mlab)

def visualize(word, model):
    
    """""" visualize the input model for a particular word """"""
    variance=np.array([np.diag(model.covars_[i]) for i in range(model.n_components)])
    figures = []
    for parm_idx in range(len(model.means_[0])):
        xmin = int(min(model.means_[:,parm_idx]) - max(variance[:,parm_idx]))
        xmax = int(max(model.means_[:,parm_idx]) + max(variance[:,parm_idx]))
        fig, axs = plt.subplots(model.n_components, sharex=True, sharey=False)
        colours = cm.rainbow(np.linspace(0, 1, model.n_components))
        for i, (ax, colour) in enumerate(zip(axs, colours)):
            x = np.linspace(xmin, xmax, 100)
            mu = model.means_[i,parm_idx]
            sigma = math.sqrt(np.diag(model.covars_[i])[parm_idx])
            ax.plot(x, mlab.normpdf(x, mu, sigma), c=colour)
            ax.set_title(""{} feature {} hidden state #{}"".format(word, parm_idx, i))

            ax.grid(True)
        figures.append(plt)
    for p in figures:
        p.show()
        
visualize(my_testword, model)",0.4898224771,
1024,lab independent component analysis,"import math
from matplotlib import (cm, pyplot as plt, mlab)

def visualize(word, model):
    """""" visualize the input model for a particular word """"""
    variance=np.array([np.diag(model.covars_[i]) for i in range(model.n_components)])
    figures = []
    for parm_idx in range(len(model.means_[0])):
        xmin = int(min(model.means_[:,parm_idx]) - max(variance[:,parm_idx]))
        xmax = int(max(model.means_[:,parm_idx]) + max(variance[:,parm_idx]))
        fig, axs = plt.subplots(model.n_components, sharex=True, sharey=False)
        colours = cm.rainbow(np.linspace(0, 1, model.n_components))
        for i, (ax, colour) in enumerate(zip(axs, colours)):
            x = np.linspace(xmin, xmax, 100)
            mu = model.means_[i,parm_idx]
            sigma = math.sqrt(np.diag(model.covars_[i])[parm_idx])
            ax.plot(x, mlab.normpdf(x, mu, sigma), c=colour)
            ax.set_title(""{} feature {} hidden state #{}"".format(word, parm_idx, i))

            ax.grid(True)
        figures.append(plt)
    for p in figures:
        p.show()
        
visualize(my_testword, model)",0.4898224771,
1024,lab independent component analysis,"import math
from matplotlib import (cm, pyplot as plt, mlab)

def visualize(word, model):
    """""" visualize the input model for a particular word """"""
    
    # Variance of Hidden States
    variance = np.array([np.diag(model.covars_[i]) for i in range(model.n_components)])
 
    figures = []
    for parm_idx in range(len(model.means_[0])):
        
        # print(""parma_idx: {} means: {} variance: {}"".format(parm_idx, model.means_[:,parm_idx], variance[:,parm_idx]))
        
        # xmin = int(min(model.means_[:,parm_idx]) - max(variance[:,parm_idx]))
        # xmax = int(max(model.means_[:,parm_idx]) + max(variance[:,parm_idx]))
        
        xmin = int((min(model.means_[:,parm_idx]) - max(variance[:,parm_idx]))*100)/100
        xmax = int((max(model.means_[:,parm_idx]) + max(variance[:,parm_idx]))*100)/100
        
        fig, axs = plt.subplots(model.n_components, sharex=True, sharey=False)
        colours = cm.rainbow(np.linspace(0, 1, model.n_components))
        for i, (ax, colour) in enumerate(zip(axs, colours)):
            x = np.linspace(xmin, xmax, 100)
            mu = model.means_[i, parm_idx]
            sigma = math.sqrt(np.diag(model.covars_[i])[parm_idx])
            
            # print(""parma_idx: {} i: {} mu: {} sigma: {} xmin: {} xmax: {}"".format(parm_idx, i, mu, sigma, xmin, xmax))
            
            ax.plot(x, mlab.normpdf(x, mu, sigma), c=colour)
            ax.set_title(""{} feature {} hidden state #{}"".format(word, parm_idx, i))
            ax.grid(True)
        figures.append(plt)
    for p in figures:
        p.show()
        
visualize(my_testword, model)",0.4898224771,
1024,lab independent component analysis,"def get_components(data):
    pca = PCA().fit(data)
    C2 = pca.components_[:2].T # for 2D projection
    C3 = pca.components_[:3].T # for 3D projection
    data2d, data3d = data.dot(C2), data.dot(C3)
    return C2, C3, data2d, data3d, pca.explained_variance_ratio_",0.4834077656,
1024,lab independent component analysis,"# Complete code below this comment for FA
# ----------------------------------
def FA_explained_variance_ratio(fa):
    fa.explained_variance_ = np.flip(np.sort(np.sum(fa.components_**2, axis=1)), axis=0)
    total_variance = np.sum(fa.explained_variance_) + np.sum(fa.noise_variance_)
    fa.explained_variance_ratio_ = fa.explained_variance_ / total_variance

FA_explained_variance_ratio(fa)
print('FA', fa.explained_variance_ratio_)",0.4733110368,
1024,lab independent component analysis,"import numpy as np
from sklearn.decomposition import PCA

def plot_pca_variances():
    #################################
    # Calculated PCA variance ratios
    #################################
    pca = PCA()
    pca.fit(X)
    variances = pca.explained_variance_ratio_
    x = range(1, len(variances)+1,1)
    
    ###################
    # Create barcharts
    ###################
    plt.plot(x, variances, color='b')
    plt.title('Scree Plot')
    plt.xlabel('Components Number')
    plt.ylabel('Eigenvalue')
    plt.xticks(range(0,len(variances)+1,2))
    plt.show()

plot_pca_variances()",0.4729776382,
165,calculating the mean of a vector with nans,"np.nanmean([1.0,np.NaN,5.9,6])",0.6554069519,
165,calculating the mean of a vector with nans,"# You can tell numpy to do calculations, ignoring NaN values, but you have to explicitly tell it to do so
np.nanmean(np.array([1, 2, 3, np.nan]))",0.6548050642,
165,calculating the mean of a vector with nans,"import scipy
scipy.nanmean(x)",0.6545928717,
165,calculating the mean of a vector with nans,"np.mean([1.0,np.NaN,5.9,6])",0.653426528,
165,calculating the mean of a vector with nans,"print(c[~np.isnan(c)])
print(np.mean(c[~np.isnan(c)]))",0.651476264,
165,calculating the mean of a vector with nans,"data = np.array([ 10.2, np.nan, 8.0, 7.5, 11.3, 7.5, np.nan, 12.1, 12.5, 9.3 ])

print(""average of unfiltered data is:"", np.average(data),""\n"")
print(""average of filtered data is:  "", np.average(data[~ np.isnan(data)]), ""\n"")",0.6511561871,
165,calculating the mean of a vector with nans,"mean_week = np.nanmean(df, axis=0)
mean_week",0.650123179,
165,calculating the mean of a vector with nans,"# the numpy method nanmean will give you the column means (if axis = 0) is used by ignoring the nans
colmean = np.nanmean(temp, axis = 0)
colmean",0.650123179,
165,calculating the mean of a vector with nans,"theta_mean = np.nanmean(thetas, axis=0)
theta_mean",0.650123179,
165,calculating the mean of a vector with nans,"# NaN values won't fail (unlike None) but they will return undefined (NaN) answers
dat_a = np.array([1, 2, 3, np.nan])
print(np.mean(dat_a))",0.6497546434,
2238,subsample dataset to make sure classes are balanced,"def balance_dataset(dataset, label, undersample=True):
    # function to downsample dataset
    # gets list of datasets split by class in label column
    data_split = [dataset[dataset[label] == l].copy() for l in list(set(dataset[label].values))]
    sizes = [f.shape[0] for f in data_split]  # list of dataset lengths
    dataset = pd.concat([f.sample(n=(min(sizes) if undersample else max(sizes)),
                               replace=(not undersample), random_state=42).copy() for f in data_split], axis=0).sample(frac=1)

    print 'Balanced dataset by undersampling dominant class, now have dataset of length: ' + \
          str(len(dataset))

    return dataset",0.5598471165,
2238,subsample dataset to make sure classes are balanced,"def data_allocation_train_dev(df):
    # Define X,Y
    df = df.sample(frac=1).reset_index(drop=True)
    X, y = df.drop([""action""], axis=1), df[""action""]
    y = y.replace(labels_dict)
    
    # Divide to training, and validation 90%, 10%
    num_training = int(df.shape[0] * 0.9)
    X_train, y_train = X[:num_training], y[:num_training]
    X_vald, y_vald = X[num_training:], y[num_training:]
    
    return X_train, y_train, X_vald, y_vald",0.4978880584,
2238,subsample dataset to make sure classes are balanced,"def dataframes_for_modeling(df):
    n = 0.90 # 90% of the dataframe for training
    df_sample = df.sample(n=int(round(df.shape[0]*n)))
    labels = df_sample['Survived']
    leftovers_labels = df.drop(df_sample.index)['Survived']
    leftovers = df.drop('Survived',axis=1).drop(df_sample.index)
    df_sample = df_sample.drop('Survived',axis=1)
    return (df_sample,labels,leftovers,leftovers_labels)",0.4973832965,
2238,subsample dataset to make sure classes are balanced,"def data_allocation_only_train(df):
    # Define X,Y
    df = df.sample(frac=1).reset_index(drop=True)
    X, y = df.drop([""action""], axis=1), df[""action""]
    y = y.replace(labels_dict)
    
    return X, y",0.4970313609,
2238,subsample dataset to make sure classes are balanced,"def data_allocation_only_split_feature_labels(df):
    # Define X,Y
    df = df.sample(frac=1).reset_index(drop=True)
    X, y = df.drop([""action""], axis=1), df[""action""]
    y = y.replace(labels_dict)
    
    return X, y",0.4970313609,
2238,subsample dataset to make sure classes are balanced,"def data_allocation(df):
    # Define X,Y
    df = df.sample(frac=1).reset_index(drop=True)
    X, y = df.drop([""action""], axis=1), df[""action""]
    y = y.replace(labels_dict)
    
    # Divide to training, validation and test set 70%, 10%, 20%
    num_training = int(df.shape[0] * 0.7)
    num_validation = int(df.shape[0] * 0.1)
    X_train, y_train = X[:num_training], y[:num_training]
    X_vald, y_vald = X[num_training:num_training + num_validation], y[num_training:num_training + num_validation]
    X_test, y_test = X[num_training + num_validation:], y[num_training + num_validation:]
    
    return X_train, y_train, X_vald, y_vald, X_test, y_test",0.4962024689,
2238,subsample dataset to make sure classes are balanced,"# holdout method: All data is randomly divided into same equal size data sets:
# 1) training set: Data set helps in the prediction of the model
# 2) validation set: Data set used to assess the performance of model built during the training

def holdout_method(data):
    validation_num = int(data.shape[0] * 0.5)
    data_random = data.sample(frac=1)
    tup = (data_random[:validation_num], data_random[validation_num:])
    return (tup)",0.4910550416,
2238,subsample dataset to make sure classes are balanced,"def loadValidationData():
    validation_x = mnist23.data[training_samples:]
    validation_y = np.array([mnist23.target[training_samples:]]) 
    return validation_x,validation_y",0.4909581542,
2238,subsample dataset to make sure classes are balanced,"def get_data(dataset):
    complete_data = get_values_to_number(dataset.get_data())
    complete_data = complete_data.sample(frac=1)
    data = complete_data.drop(['label'], axis=1).values.tolist()
    targets = complete_data['label'].values.tolist()
    return complete_data, data, targets",0.4881547093,
2238,subsample dataset to make sure classes are balanced,"def loadTestData():
    mnist23_test = pickle.load( open( ""./datasets/mnist23.data"", ""rb"" ) )
    test_x = mnist23.data
    test_y = np.array([mnist23.target])
    return test_x,test_y",0.4880950749,
1466,part using logistic regression with categorical features,"for CReg in np.logspace(0,6,10):
    logreg = linear_model.LogisticRegression(C=CReg,penalty='l1')
    EQ_default_params_fit = logreg.fit(X_train, y_train)
    predicted = EQ_default_params_fit.predict(X_test)
    probs = EQ_default_params_fit.predict_proba(X_test)
    ScoreMetric =  metrics.accuracy_score(y_test, predicted)
    print ""Num of zero coefs = %d "" % (EQ_default_params_fit.coef_[0,:] == 0).sum()
    print ""Value of Creg = %3.3e"" % CReg
    print ""Score is %2.2f"" % ScoreMetric
    num_classes = EQ_default_params_fit.classes_.shape[0]
    fig,ax =plt.subplots(1,num_classes)
    fig.set_size_inches((15,3))
    for j in range(num_classes):
        #ax[j].scatter(x =np.array(range(nsample*3)), y=EQ_default_params_fit.coef_[j,:],c=point_colors,
                      #edgecolors=""none"")
        ax[j].plot(EQ_default_params_fit.coef_[j,:])
        #ax[j].plot(x =range(0,nsample,1), y=EQ_default_params_fit.coef_[j,0:255],color='r')
        #x[j].plot(x =range(nsample,2*nsample,1),y=EQ_default_params_fit.coef_[j,256:511],color='b')
        #ax[j].plot(x =range(2*nsample,3*nsample,1),y=EQ_default_params_fit.coef_[j,512:767],color='g')

        plt.title(""Num of zero coefs = %d , Creg= %2.2f"" % ((EQ_default_params_fit.coef_[0,:] == 0).sum(),CReg))
        
        ax[j].set_ylim(1.05*np.min(EQ_default_params_fit.coef_[j,:]),1.05*np.max(EQ_default_params_fit.coef_[j,:]))",0.6123751402,
1466,part using logistic regression with categorical features,"for x in range(1,9):
    logistic1 = linear_model.LogisticRegression()
    logistic1.fit(xtrain2[:], ytrain[:,x])
    print(""null model MSE:"", mean_squared_error(ytrain[:,x], np.zeros(ytrain[:,0].shape)), ""train MSE: "",
     mean_squared_error(ytrain[:,x], logistic1.predict(xtrain2[:])))",0.5986924171,
1466,part using logistic regression with categorical features,"log_reg=sklearn.linear_model.LogisticRegression(random_state=0).fit(X_train[:,1:],Y_train)
log_reg.score(X_train[:,1:],Y_train),log_reg.score(X_test[:,1:],Y_test)",0.5959240198,
1466,part using logistic regression with categorical features,"from sklearn import cross_validation
from sklearn.linear_model import LogisticRegression

log_model = LogisticRegression(random_state=1)

scores = cross_validation.cross_val_score(log_model,titanic[predictors],titanic[""Survived""],cv=3)  #because we already created the predictors numpy array above, we can re-use it. All logistic regression does is replace or rudimentary mapping with a logit function for mapping values to 0 or 1

print scores.mean()  #scores is created with 3 seperate instances because the cross validation is done thrice",0.5956169367,
1466,part using logistic regression with categorical features,"## Define and fit the logistic regression model
log_mod_5 = linear_model.LogisticRegression(C = 10.0, class_weight = {0:0.45, 1:0.55}) 
log_mod_5.fit(Comps, y_train)
print(log_mod_5.intercept_)
print(log_mod_5.coef_)",0.5932757854,
1466,part using logistic regression with categorical features,"from sklearn.linear_model import LogisticRegression

scores_region, region_best_models = cross_validate(
    X, ys['region'].values, 
    LogisticRegression(
        solver='lbfgs', multi_class='multinomial'),
    param_grid={'C': np.logspace(-2.0, 3, 10)})",0.5921698809,
1466,part using logistic regression with categorical features,"# logistic regression model, ridge regression turned off
logreg = linear_model.LogisticRegression(fit_intercept=False, C=1, solver='newton-cg')

# coefficient of limit_rating variable
logreg.fit(X, y).coef_[0][4]",0.5915663242,
1466,part using logistic regression with categorical features,"huber_model.fit(X_train.values.reshape(-1,1),y_train.ravel())
print('Huber Model: {:f}R + {:.3}\nTest Data:\t\t\tFemale CEO Data:\nMSE: {:.5e}\t\tMSE: {:.5e}\
\nr^2: {:.5e}\t\tr^2: {:.5e}\n\nLinear Model: {:f}R + {:.3}\nTest Data:\t\t\t\
Female CEO Data:\nMSE: {:.5e}\t\tMSE: {:.5e}\nr^2: {:.5e}\t\tr^2: {:.5e}'.format(
    huber_model.coef_[0],
    huber_model.intercept_,
        mean_squared_error(y_test.values.reshape(-1,1),
                           huber_model.predict(X_test.values.reshape(-1,1))),
        mean_squared_error(huber_model.predict(ceo_pay_by_year_f['Rank'].values.reshape(-1,1)),
                           ceo_pay_by_year_f['log(Compensation)'].ravel()),
        
        huber_model.score(X_test.values.reshape(-1,1), y_test.ravel()),
        huber_model.score(ceo_pay_by_year_f['Rank'].values.reshape(-1,1),
                          ceo_pay_by_year_f['log(Compensation)'].ravel()),
        
    lin_model.coef_[0],
    lin_model.intercept_,
        mean_squared_error(y_test.values.reshape(-1,1),
                           lin_model.predict(X_test.values.reshape(-1,1))),
        mean_squared_error(lin_model.predict(ceo_pay_by_year_f['Rank'].values.reshape(-1,1)),
            ceo_pay_by_year_f['log(Compensation)'].ravel()),
        
        lin_model.score(X_test.values.reshape(-1,1), y_test.values.reshape(-1,1)),
        lin_model.score(ceo_pay_by_year_f['Rank'].values.reshape(-1,1),
                        ceo_pay_by_year_f['log(Compensation)'].ravel()),))",0.5914996862,
1466,part using logistic regression with categorical features,"reg.intercept_, reg.coef_[0], reg.coef_[1]",0.5912365913,
1466,part using logistic regression with categorical features,"if string_classifier=='logistic_reg':

    from sklearn.linear_model import LogisticRegression
    score_cv = dict()
    C_list = [0.1,1.0,5.0,10.0,50.0,100.0]
    
    # Bag of Words
    X=X_train_bag_of_words
    for C_cv in C_list:
            print 'Cost : {0}/{1}'.format(C_list.index(C_cv)+1,len(C_list))
            clf=LogisticRegression(C=C_cv,class_weight='auto') # class_weight=None gives better results
            score_cv[(C_cv)] = np.mean(cross_validation.cross_val_score(clf, X, Y, cv=cv, scoring='f1_weighted'))
    C_opt = max(score_cv.iteritems(), key=operator.itemgetter(1))[0]
    print 'Maximum f1 score obtained for Cost ' + str(C_opt)+' : '+ str(100*max(score_cv.values()))+ ' %'

    clf_bag=LogisticRegression(C=C_opt,class_weight='auto')
    clf_bag.fit(X,Y)

    # Graph of Words
    X=X_train_graph_of_words
    for C_cv in C_list:
            print 'Cost : {0}/{1}'.format(C_list.index(C_cv)+1,len(C_list))
            clf=LogisticRegression(C=C_cv,class_weight='auto') # class_weight=None gives better results
            score_cv[(C_cv)] = np.mean(cross_validation.cross_val_score(clf, X, Y, cv=cv, scoring='f1_weighted'))
    C_opt = max(score_cv.iteritems(), key=operator.itemgetter(1))[0]
    print 'Maximum f1 score obtained for Cost ' + str(C_opt)+' : '+ str(100*max(score_cv.values()))+ ' %'

    clf_graph=LogisticRegression(C=C_opt,class_weight='auto')
    clf_graph.fit(X,Y)",0.5908168554,
1478,pca example random data,"data = pd.read_csv('./data/tweets.csv', encoding='latin1', usecols=['Sentiment', 'SentimentText'])
data.columns = ['sentiment', 'text']
data = data.sample(frac=1, random_state=42)
print(data.shape)",0.4754309058,
1478,pca example random data,"multipleImages = glob('../input/chest-xray-pneumonia/chest_xray/chest_xray/train/NORMAL/**')
def plotThreeImages(images):
    r = random.sample(images, 3)
    plt.figure(figsize=(16,16))
    plt.subplot(131)
    plt.imshow(cv2.imread(r[0]))
    plt.subplot(132)
    plt.imshow(cv2.imread(r[1]))
    plt.subplot(133)
    plt.imshow(cv2.imread(r[2])); 
plotThreeImages(multipleImages)",0.4753447771,
1478,pca example random data,"multipleImages = glob('../input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/B/**')
def plotThreeImages(images):
    r = random.sample(images, 3)
    plt.figure(figsize=(16,16))
    plt.subplot(131)
    plt.imshow(cv2.imread(r[0]))
    plt.subplot(132)
    plt.imshow(cv2.imread(r[1]))
    plt.subplot(133)
    plt.imshow(cv2.imread(r[2])) 
plotThreeImages(multipleImages)",0.4753447771,
1478,pca example random data,"multipleImages = glob('../input/caltech256/256_objectcategories/256_ObjectCategories/228.triceratops/**')
def plotThreeImages(images):
    r = random.sample(images, 3)
    plt.figure(figsize=(16,16))
    plt.subplot(131)
    plt.imshow(cv2.imread(r[0]))
    plt.subplot(132)
    plt.imshow(cv2.imread(r[1]))
    plt.subplot(133)
    plt.imshow(cv2.imread(r[2])); 
plotThreeImages(multipleImages)
plotThreeImages(multipleImages)
plotThreeImages(multipleImages)",0.4753447771,
1478,pca example random data,"multipleImages = glob('../input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/A/**')
def plotThreeImages(images):
    r = random.sample(images, 3)
    plt.figure(figsize=(16,16))
    plt.subplot(131)
    plt.imshow(cv2.imread(r[0]))
    plt.subplot(132)
    plt.imshow(cv2.imread(r[1]))
    plt.subplot(133)
    plt.imshow(cv2.imread(r[2]))
    #;
plotThreeImages(multipleImages)",0.4753447771,
1478,pca example random data,"multipleImages = glob('../input/kermany2018/oct2017/OCT2017 /train/NORMAL/**')
def plotThreeImages(images):
    r = random.sample(images, 3)
    plt.figure(figsize=(16,16))
    plt.subplot(131)
    plt.imshow(cv2.imread(r[0]))
    plt.subplot(132)
    plt.imshow(cv2.imread(r[1]))
    plt.subplot(133)
    plt.imshow(cv2.imread(r[2])); 
plotThreeImages(multipleImages)",0.4753447771,
1478,pca example random data,"# read_table() is like read_csv(), but generalized for delimited file formats
# and calling .sample() with a fraction of 1 shuffles the data
dataset = pd.read_table(DATASET, sep=',', engine='c', header=0).sample(frac=1)

# verify dataset shape
print(""Dataset shape: "", dataset.shape)",0.4734560549,
1478,pca example random data,"raw_df = data.import_raw_data()
raw_sample_df = raw_df.sample(4, random_state=42)
display(raw_sample_df.set_index('Name').T)",0.4711779356,
1478,pca example random data,"import random
def singleTrial(depthlist):
    depth_sample = random.sample(depthlist,154)
    est_well_cost = [12.3635*float(x[0]) - 2211.21 for x in depth_sample]
    total_est_cost = sum(est_well_cost)
    return total_est_cost",0.4680290818,
1478,pca example random data,"## Simulate the binary tables
import numpy as np
import numpy.random as nr
import pandas as pd

def sim_bernoulli(p, n = 25):
    """"""
    Function to compute the vectors with probabilities for each 
    condition (input value) of the dependent variable using the Bernoulli
    distribution. 
    
    The arguments are:
    p - a vector of probabilites of success for each case.
    n - The numer of realizations. 
    """"""
    temp = np.zeros(shape = (len(p), n))
    for i in range(len(p)): 
        temp[i,:] = nr.binomial(1, p[i], n)
    return(temp)

def selec_dist_1(sims, var, lg):
    """"""
    Function to integrate the conditional probabilities for
    each of the cases of the parent variable. 
    
    The arguments are:
    sims - the array of simulated realizations with one row for each state of the
           parent variable. 
    var - the vector of values of parent variable used to select the value from the 
          sims array.
    lg - vector of states of possible states of the parent variable. These must be
         in the same order as for the sims array. 
    """"""
    out = sims[0,:] # Copy of values for first parent state
    var = np.array(var).ravel()
    for i in range(1, sims.shape[0]): # loop over other parent states
        out = [x if u == lg[i] else y for x,y,u in zip(sims[i,:], out, var)]
    return([int(x) for x in out])

def set_class_2(x):
    """"""
    Function to flatten the array produced by the numpy.random.multinoulli function. 
    The function tests which binary value of the array of output states is true
    and substitutes an integer for that state. This function only works for up to three
    output states. 
    
    Argument:
    x - The array produced by the numpy.random.multinoulli function. 
    """"""
    out = []
    for i,j in enumerate(x):
        if j[0] == 1: out.append(0)
        elif j[1] == 1: out.append(1)
        else: out.append(2)       
    return(out)   

def sim_multinoulli(p, n = 25):
    """"""
    Function to compute the vectors with probabilities for each 
    condition (input value) of the dependent variable using the multinoulli
    distribution. 
    
    The arguments are:
    p - an array of probabilites of success for each possible combination
        of states of the parent variables. Each row in the array are the 
        probabilities for each state of the multinoulli distribution for 
        that combination of parent values.
    n - The numer of realizations. 
    """"""
    temp = np.zeros(shape = (p.shape[0], n))
    for i in range(p.shape[0]):  
        ps = p[i,:]
        mutlis = nr.multinomial(1, ps, n) 
        temp[i,:] = set_class_2(mutlis)
    return(temp)

def selec_dist_4(sims, var1, var2, var3, var4, lg1, lg2, lg3, lg4):
    """"""
    Function to integrate the conditional probabilities for
    each of the cases of four parent variables. 
    
    The arguments are:
    sims - the array of simulated realizations with one row for each state of the
           union of the parent variables. 
    var1 - the vector of values of first parent variable used to select the value from the 
          sims array.
    var2 - the vector of values of second parent variable used to select the value from the 
          sims array.
    var3 - the vector of values of third parent variable used to select the value from the 
          sims array.
    var4 - the vector of values of fourth parent variable used to select the value from the 
          sims array.
    lg1 - vector of states of possible states of the first parent variable. These must be
         in the same order as for the sims array. 
    lg2 - vector of states of possible states of the second parent variable. These must be
         in the same order as for the sims array. 
    lg3 - vector of states of possible states of the third parent variable. These must be
         in the same order as for the sims array. 
    lg4 - vector of states of possible states of the fourth parent variable. These must be
         in the same order as for the sims array. 
    """"""
    out = sims[0,:] # Copy values for first combination of states for parent variables
    
    ## make sure the parent variables are 1-d numpy arrays.
    var1 = np.array(var1).ravel() 
    var2 = np.array(var2).ravel()
    var3 = np.array(var3).ravel() 
    var4 = np.array(var4).ravel()
    for i in range(1, sims.shape[0]): # Loop over all combination of states of the parent variables
        out = [x if t == lg1[i] and u == lg2[i] and v == lg3[i] and w == lg4[i] else y for x,y,t,u,v,w in 
              zip(sims[i,:], out, var1, var2, var3, var4)]                         
    return([int(x) for x in out])


## set the sample size
nsamp = 20

## First the conditionally independent variables
nr.seed(22234)
B_samps = pd.DataFrame(nr.binomial(1, 0.8, nsamp), columns = ['B'])
nr.seed(2355)
C_samps = pd.DataFrame(nr.binomial(1, 0.3, nsamp), columns = ['C'])

## Two variables conditionally depenent on one other
probs = [0.5, 0.1]
nr.seed(2134)
bern = sim_bernoulli(probs, nsamp)
BW_samps = pd.DataFrame(selec_dist_1(bern, B_samps, [0,1]), columns = ['BW'])

probs = [0.5, 0.7]
nr.seed(22234)
bern = sim_bernoulli(probs)
CW_samps = pd.DataFrame(selec_dist_1(bern, C_samps, [0,1]), columns = ['CW'])


probs = np.array([[0.0, 0.7, 0.5,  0.7, 0.0, 0.6, 0.5,  0.6, 0.0, 0.7, 0.5,  0.7, 0.0, 0.6, 0.5,  0.6],
                           [1.0, 0.0, 0.0,  0.0, 1.0, 0.0, 0.0,  0.0, 1.0, 0.0, 0.0,  0.0, 1.0, 0.0, 0.0,  0.0],
                           [0.0, 0.1, 0.4,  0.2, 0.0, 0.1, 0.3,  0.2, 0.0, 0.1, 0.4,  0.2, 0.0, 0.1, 0.3,  0.2]]).transpose()

nr.seed(2334)
sims = sim_multinoulli(probs, nsamp)

C_lg = [0,0,1,1,0,0,1,1,0,0,1,1,0,0,1,1]
B_lg = [0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1]
BW_lg = [0,0,0,0,1,1,1,1,0,0,0,0,1,1,1,1]
CW_lg = [0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1]
M_samps = pd.DataFrame(selec_dist_4(sims, B_samps, C_samps, BW_samps, CW_samps, B_lg, C_lg, BW_lg, CW_lg), columns = ['M'])



## Now concatenate the columns into one data frame
dats = pd.concat([B_samps, C_samps, BW_samps, CW_samps, M_samps], axis = 1)
print(dats.shape)
dats",0.4666726887,
199,chi squared test,"# chi2 function, from Federica's

def chi2(data, model, errors = None):
    '''Calculates the chi sq given data, model and errors
    Arguments:
    data: series of datapoints (endogenous variable)
    model: series of predicted values corresponding to the observed data
    errors: serie of errors (optional). 
    If errors are not passes all errors are set to 1
    '''
    if errors is None:
        errors = np.ones_like(data)
    if data.shape == model.shape and data.shape == errors.shape:
        return (((data - model)**2) / errors**2).sum()
    else: 
        print ('''ERROR:
must pass arrays of identical dimension for data, model and (optional) error)''')
    return -1",0.5685682297,
199,chi squared test,"#defining the chi-square test
def chi2(data, model, errors = None):
    '''Calculates the chi sq given data, model and errors
    Arguments:
    data: series of datapoints (endogenous variable)
    model: series of predicted values corresponding to the observed data
    errors: serie of errors (optional). 
    If errors are not passes all errors are set to 1
    '''
    if errors is None:
        errors = np.ones_like(data)
    if data.shape == model.shape and data.shape == errors.shape:
        return (((model - data)**2) / errors**2).sum()
    else: 
        print ('''ERROR:
must pass arrays of identical dimension for data, model and (optional) error)''')
    return -1",0.5613216758,
199,chi squared test,"#Chi Square
def chi2(data, model, errors = None):
    '''Calculates the chi sq given data, model and errors
    Arguments:
    data: series of datapoints (endogenous variable)
    model: series of predicted values corresponding to the observed data
    errors: serie of errors (optional). 
    If errors are not passes all errors are set to 1
    '''
    if errors is None:
        errors = np.ones_like(data)
    if data.shape == model.shape and data.shape == errors.shape:
        return (((data - model)**2) / errors**2).sum()
    else: 
        print ('''ERROR:
must pass arrays of identical dimension for data, model and (optional) error)''')
    return -1",0.5611408949,
199,chi squared test,"def chi_square_independence_test(data, group):
    '''
    Function takes the titanic dataset, a column name from the dataset and returns the result of a 
    chi-square test for independence for a specified variable (column name) and the ""Survived"" variable from
    the dataset.
    
    data: titanic dataset (data frame)
    group: name of the column from the titanic dataset to be used as specified variable (string)
    '''
    # creates a contingency table of the observed frequencies of variables ""Survived"" and ""Specified Column""
    contingency_table_observed = pd.crosstab(index = data[""Survived""], columns = data[group], margins = True)
    contingency_table_observed.index = np.append(contingency_table_observed.index[:-1], ""coltotal"")
    contingency_table_observed.columns = np.append(contingency_table_observed.columns[:-1], ""rowtotal"")
    
    rowtotal_not_survived = contingency_table_observed[""rowtotal""].loc[""not survived""]
    rowtotal_survived = contingency_table_observed[""rowtotal""].loc[""survived""]
    total = contingency_table_observed[""rowtotal""].loc[""coltotal""]
    
    # creates a contingency table of the expected frequencies of variables ""Survived"" and ""Specified Column""
    # expected values are based on the marginal frequencies for the total row and column entries
    expected_not_survived = (contingency_table_observed.loc[""coltotal""]*rowtotal_not_survived)/total
    expected_survived = (contingency_table_observed.loc[""coltotal""]*rowtotal_survived)/total
    contingency_table_expected = pd.DataFrame()
    contingency_table_expected = contingency_table_expected.append([expected_not_survived,expected_survived,contingency_table_observed.loc[""coltotal""]], ignore_index = True)
    contingency_table_expected.index = contingency_table_observed.index
    
    # calculates the chi-square statistic
    chi_square = sum( (((contingency_table_observed-contingency_table_expected)**2)/contingency_table_expected).loc[""survived""] ) + \
    sum( (((contingency_table_observed-contingency_table_expected)**2)/contingency_table_expected).loc[""not survived""] )
    
    # calculate Cramer's V as measure of association between the tested variables
    min_dimension = min(len(contingency_table_observed.columns[:-1])-1,len(contingency_table_observed.index[:-1])-1)    
    cramers_v = np.sqrt(chi_square/(total*min_dimension))
    
    print (""chi-square statistic for correlation between survival rate and {0}: {1}"".format(group,round(chi_square,2))+\
           ""\nCramer's V: {}"".format( round(cramers_v,2) ))",0.5601993799,
199,chi squared test,"def Chi_report_result(p,a):
    print ('Is the chi-square statistic ' + 
           '{0:.2f} greater than the critical value {1:.2f}?'.format(p,a))
    if p > a:
        print (""YES!"")
    else: 
        print (""NO!"")
    
    print ('The Null hypothesis is {}'.format(\
                            'rejected' if p > a  else 'not rejected') )

    
Chi_Val = evalChisq(sample_values)
Chi_Sig = 3.84
Chi_report_result(Chi_Val, Chi_Sig)",0.5576428771,
199,chi squared test,"def chi2(data, model, errors = None):
    '''Calculates the chi sq given data, model and errors
    Arguments:
    data: series of datapoints (endogenous variable)
    model: series of predicted values corresponding to the observed data
    errors: serie of errors (optional). 
    If errors are not passes all errors are set to 1
    '''
    if errors is None:
        errors = np.ones_like(data)
    if data.shape == model.shape and data.shape == errors.shape:
        return (((data - model)**2) / errors**2).sum()
    else: 
        print ('''ERROR:
must pass arrays of identical dimension for data, model and (optional) error)''')
    return -1",0.5541883707,
199,chi squared test,"#Defining the Chi-Square test

def chi2(data, model, errors = None):
    '''Calculates the chi sq given data, model and errors
    Arguments:
    data: series of datapoints (endogenous variable)
    model: series of predicted values corresponding to the observed data
    errors: serie of errors (optional). 
    If errors are not passes all errors are set to 1
    '''
    if errors is None:
        errors = np.ones_like(data)
    if data.shape == model.shape and data.shape == errors.shape:
        return (((model - data)**2) / errors**2).sum()
    else: 
        print ('''ERROR:
must pass arrays of identical dimension for data, model and (optional) error)''')
    return -1",0.550008893,
199,chi squared test,"# TO HIDE

from scipy.stats import chi2

def calculate_error_ellipse_2D_projection(covariance, p):
    """"""
    Calculates the 2D projections of an ND error ellipse.
    The ellipse contains p * 100 percent of the population 
    based on the sample covariance matrix estimate.
    
    Parameters:
        covariance (np.ndarray (N,N)) : covariance matrix
        p (float) 0 <= p <= 1 : probability 
    
    Returns:
        ellipse_parameters ([(int,int,float, float, float, float)]) :
            (int, int) : the indices of the projection axes, 
            (float, float) : axes of the ellipse
            (float) : rotation angle of  the major axis wert the first projection axis
            
        The quintuplet is calculated for each projection.
    """"""

    r2 = chi2.ppf(p, covariance.shape[0])

    ellipse_parameters = []

    # loop over doublets of indices
    
    n_dims = covariance.shape[0]
    gen = ((x, y) for x in range(n_dims) for y in range(x + 1, n_dims))
    
    for i1, i2 in gen:
        
        # select relevant elements of the covariance matrix
        # W. P. Karl, CVGIP: Graphical Models and Image Processing 56(2), 124-139 (1994)
        cov_2d = covariance[[i1, i1, i2, i2], [i1, i2, i1, i2]].reshape(2,2)
    
        # calculate axes
        w, v = np.linalg.eigh(cov_2d)
        axis2, axis1 = 2 * np.sqrt(w * r2)
        
        # angle of rotation
        angle = np.degrees(np.arctan2(v[1,1], v[0,1]))
    
        ellipse_parameters.append((i1, i2, axis1, axis2, angle))
        
    return ellipse_parameters",0.5428298712,
199,chi squared test,"# A function to visualize the schi-squared dist. scores
def visualize_chi_squared(score, cutoff, dof, visual_mode=True):

    # Set up the x axis
    x = np.linspace(0,10,100)
    # Here's the normal distribution
    y = stats.chi2.pdf(x, df = dof)
    
    fig, ax = plt.subplots(1,1, sharex=True)
    ax.plot(x,y, label='$\chi^2$ Distribution, '+str(dof)+' Degrees of Freedom')

    # shade our rejection regions
    ax.fill_between(x, 0, y, where = x > stats.chi2.ppf(1-(cutoff), df=dof), alpha=0.7)
    
    plt.vlines(score, ymin=0, ymax=np.max(y)*1.1,colors='r', linestyles='dashed', label='score='+str(score))
    # plt.vlines(stats.norm.ppf(1-(cutoff/2)), mu, sigma, colors='r', linestyles='dashed')

    plt.title('Rejection Regions and score')
    plt.ylabel('Density')
    if score > 10:
        plt.xscale('log')
        plt.xlabel('Log Scaled')
    plt.legend(loc='best');",0.5403053164,
199,chi squared test,"scipy.stats.chi2.ppf(1 - alpha/2, 1)",0.5398981571,
1199,miscellaneous,"class nnetwork():
    
    def __init__(self, layers )
        self.nlayers = len(layers)
        self.inlayer = layers[0]
        self.outlayer = layers[-1]
        for ii in range(1,len(layers)-1):",0.372517556,
1199,miscellaneous,"assignCount = 0

def genAssign(x, y):
    """"""Assume x is Var, generate x := y""""""
    global assignCount, regs
    if type(y) == Cond:
        #putB(condOp(negate(y.cond)), y.left, y.right, y.labA[0])
        put('cmp', y.left, y.right)
        putInstr(condOp(negate(y.cond)) + "" "" + y.labA[0])
        releaseReg(y.left); releaseReg(y.right); r = obtainReg()
        putLab(y.labB); put('mov', r, '#1') # load true
        lab = '.A' + str(assignCount); assignCount += 1
        putInstr('B ' + lab)
        putLab(y.labA); put('mov', r, '#0') # load false 
        putLab(lab)
    elif type(y) != Reg: 
        y = loadItem(y); r = y.reg
    else: r = y.reg
    # Need to load x to reg because of label
    if x.reg == ZR:
        # TODO: Int's here mean stack var
        #print(x.adr)
        s = loadAddressOfLabel(x.adr)
    else:
        s = x.reg
    #print(x.reg)
    #putInstr('str ' + r + ', [x' + s[1:] + ']')
    if x.reg == FP:
        putM('str', r, s, x.adr)
    else:
        putM('str', r, s)
    releaseReg(s) 
    releaseReg(r)",0.3713215888,
1199,miscellaneous,"def evaluate_basis(basis=None):
    
    # Optimization
    if basis is None:
        basis = morpher.optimize_basis()

    # Evaluate basis
    squared_weights = []

    for theta in theta_evaluation:
        wi = morpher.calculate_morphing_weights(theta, basis)
        squared_weights.append(np.sum(wi*wi)**0.5)

    squared_weights = np.array(squared_weights).reshape((n_resolution,n_resolution))
    
    return basis, squared_weights",0.3661255836,
1199,miscellaneous,"def evaluate_basis(basis=None):
    
    # Optimization
    if basis is None:
        basis = morpher.optimize_basis(n_bases=1)

    # Evaluate basis
    squared_weights = []

    for theta in theta_evaluation:
        wi = morpher.calculate_morphing_weights(theta, basis)
        squared_weights.append(np.sum(wi*wi)**0.5)

    squared_weights = np.array(squared_weights).reshape((n_resolution,n_resolution))
    
    return squared_weights",0.3637179136,
1199,miscellaneous,"def explore_city_data(city_data):
    """"""Calculate the Boston housing statistics.""""""

    # Get the labels and features from the housing data
    housing_prices = city_data.target
    housing_features = city_data.data

    ###################################
    ### Step 1. YOUR CODE GOES HERE ###
    ###################################

    # Uncomment this line for further details about the data set.    
    # print city_data['DESCR']
    
    # Please calculate the following values using the Numpy library
    # Size of data (number of houses)?
    num_houses = len(city_data.data)
    print ""Size of data (number of houses)? "" + str(num_houses)
    
    # Number of features?
    num_features = len(city_data.data[0])
    print ""Number of features? "" + str(num_features)
    
    # Minimum price?
    min_price = np.min(housing_prices)
    print ""Minimum price (in $1000's)? "" + str(min_price)
    
    # Maximum price?
    max_price = np.max(housing_prices)
    print ""Maximum price (in $1000's)? "" + str(max_price)
    
    # Calculate mean price?
    mean_price = np.mean(housing_prices)
    print ""Calculate mean price (in $1000's)? "" + str(mean_price)
    
    # Calculate median price?
    median_price = np.median(housing_prices)
    print ""Calculate median price (in $1000's)? "" + str(median_price)
    
    # Calculate standard deviation?
    std_price = np.std(housing_prices)
    print ""Calculate standard deviation (in $1000's)? "" + str(std_price)
    
    
    # I added a plot to help illsutrate these diescriptive statistics.    
    pl.figure()
    pl.title('Distribution of Boston House Prices')
    pl.hist(city_data.target, bins=50., color='c')
    pl.xlabel('Price ($1000\'s)')
    pl.ylabel('Number of Records')
    median_label = 'Median of %2.2f' % median_price
    pl.axvline(median_price, color='r', linestyle='solid', linewidth=2, label=median_label)
    mean_label = 'Mean of %2.2f' % mean_price
    pl.axvline(mean_price, color='b', linestyle='solid', linewidth=2, label=mean_label)
    pl.legend()
    pl.show()",0.3591860533,
1199,miscellaneous,"def _compute_network_model(self) : 
    """""" Build the network, loss, grad_loss and sgd_update theano functions.
        More work than is strictly nessecary is done here as the only thing
        that is really needed in order to run sgd (stochastic gradient 
        descent) is the sgd_update function. The network, loss and grad_loss
        functions are compiled since this is experimental code.
    """"""

    # build the network
    self.i = T.vector('i',dtype = self.int_dtype)

    self.network_outputs = compute_network_outputs(self.i,self.s0,self.V,
                                                   self.U,self.W,self.b)


    # build mean log likelyhood loss

    # variables for a batch of sentences
    self.I = T.matrix('I',dtype = self.int_dtype)
    self.J = T.matrix('J',dtype = self.int_dtype) # for embedding I = J

    self.loss_outputs = compute_mean_log_lklyhd_outputs(self.I,self.J,
                                                        self.s0,self.V,
                                                        self.U,self.W,
                                                        self.b)

    # set up the accumulator for computing the loss in batches

    n_minibatch = T.cast(self.I.shape[0],self.float_dtype)
    loss_accum_ipnm = self.loss_accum_i + n_minibatch

    self.loss_updates = ((self.loss_accum,
                          (self.loss_outputs*n_minibatch/loss_accum_ipnm
                           + (self.loss_accum 
                             * self.loss_accum_i/loss_accum_ipnm))),
                         (self.loss_accum_i,loss_accum_ipnm))

    # get the gradient of the loss

    (self.dV,
     self.dU,
     self.dW,
     self.db) = theano.grad(self.loss_outputs,
                            [self.V,self.U,self.W,self.b])

    # get the gradient magnitudes

    self.dV_mag = T.sqrt(T.sum(self.dV*self.dV))
    self.dU_mag = T.sqrt(T.sum(self.dU*self.dU))
    self.dW_mag = T.sqrt(T.sum(self.dW*self.dW))
    self.db_mag = T.sqrt(T.sum(self.db*self.db))

    # get the sgd update function

    # this is the learning parameter
    self.eta = T.scalar('eta',dtype = self.float_dtype)

    # also including a running average of the gradient magnitudes

    self.sgd_i = T.scalar('sgd_i',dtype = self.float_dtype)

    dV_mag_accum = (self.dV_mag/(self.sgd_i+1.)
                        + self.m_dV_mag*(self.sgd_i/(self.sgd_i+1.)))
    dU_mag_accum = (self.dU_mag/(self.sgd_i+1.) 
                        + self.m_dU_mag*(self.sgd_i/(self.sgd_i+1.)))
    dW_mag_accum = (self.dW_mag/(self.sgd_i+1.) 
                        + self.m_dW_mag*(self.sgd_i/(self.sgd_i+1.)))
    db_mag_accum = (self.db_mag/(self.sgd_i+1.) 
                        + self.m_db_mag*(self.sgd_i/(self.sgd_i+1.)))

    # adding here since we are taking a max of the loss - accumulators
    # do not include the latest values
    self.sgd_updates = ((self.V,self.V + self.eta*self.dV),
                        (self.U,self.U + self.eta*self.dU),
                        (self.W,self.W + self.eta*self.dW),
                        (self.b,self.b + self.eta*self.db),
                        (self.m_dV_mag,dV_mag_accum),
                        (self.m_dU_mag,dU_mag_accum),
                        (self.m_dW_mag,dW_mag_accum),
                        (self.m_db_mag,db_mag_accum))

    # pointers for the compiled functions
    self.network = None
    self.loss = None
    self.grad_loss = None
    self.sgd_update = None
    self.sgd_update_w_loss = None

# bind the method to the class - this is just so the class definition
# can be broken up across several Jupyter notebook cells
embedding_model._compute_network_model = _compute_network_model",0.3590751886,
1199,miscellaneous,"class MLP:
    """"""
    Multi-layer perceptron class.
    """"""
    def __init__(self, desc):
        self.layers = desc
        self.reshaped = False
        
        # Here we aggregate all tunable parameters into a single list.
        self.params = []
        for l in self.layers:
            try:
                for v in l.params.values():
                    self.params.append(v)
            except Exception, e:
                pass
    
    def reshape(self, input_shape):
        """"""
        Invokes reshape methods of all layers.
        """"""
        batch_size = input_shape[0]
        self.data = {
            'data': np.zeros(input_shape), 
            'diff': np.zeros(input_shape)
        }
        self.labels = {
            'data': np.zeros((batch_size, 1), dtype=np.int32)
        }
        
        self.blobs = [] # Holds all internal blobs created by reshape methods.
        
        # Your code goes here. ################################################ 
        self.blobs.append(self.data)
        for layer in desc:
            new_bottom = [{'data': None, 'diff': None}]
            layer.reshape(self.blobs[-1:], new_bottom)
            self.blobs.append(new_bottom[0])
        # Your code goes here. ################################################ 
        
        self.reshaped = True
    
    def set_input(self, data, labels):
        if not self.reshaped:
            self.reshape(data.shape)
            
        # Populate self.data and self.labels.
        # Your code goes here. ################################################
        self.data = {'data':data, 'diff':np.zeros(data.shape)}
        self.labels = {'data':labels}
        # Your code goes here. ################################################
        
    def fprop(self):
        """"""
        Conducts forward-propagation through the network.
        
        (i.e. fills self.blobs[:]['data'])
        """"""
        # Your code goes here. ################################################ 
        for i in xrange(len(self.layers) - 1):
            self.layers[i].fprop(self.blobs[i:i+1], self.blobs[i+1:i+2])
        self.layers[-1].fprop([self.blobs[len(self.layers) - 1], self.labels], \
                               self.blobs[len(self.layers):])
        # Your code goes here. ################################################ 
        # NOTE: Keep in mind that the last layer should receive ground-truth 
        #       labels as well as blob from the lower layer.
        
    def bprop(self):
        """"""
        Conducts backward-propagation through the network.
        
        (i.e. fills self.blobs[:]['diff'] and updates 'diff's of the internal 
        weight blobs)
        """"""
        # Your code goes here. ################################################ 
        self.layers[-1].bprop(self.blobs[len(self.layers):], [self.blobs[len(self.layers)-1], self.labels])
        for i in xrange(1, len(self.layers)):
            self.layers[len(self.layers) - 1 - i].bprop(\
                                    self.blobs[len(self.blobs) - 1 - i:len(self.blobs) - i], \
                                    self.blobs[len(self.blobs) - 2 - i:len(self.blobs) - 1 - i])
        for i in xrange(len(self.params)):
            self.params[i]['data'] -= self.params[i]['diff']
        # Your code goes here. ################################################ 
        
    def get_loss(self):
        """""" Return the value of the objective function """"""
        return self.blobs[-1]['data'].mean()
    
    def test(self, data, labels):
        """"""
        Helper function for evaluating the performance of the network on a test
        set (which can be larger than the batch size).
        
        Returns accuracy.
        """"""
        
        batch_size = self.data['data'].shape[0]
        preds = []
        for start in xrange(0, data.shape[0], batch_size):
            self.set_input(data[start : start + batch_size, :], labels[start : start + batch_size])
            self.fprop()
            preds += [self.layers[-1].probs.argmax(axis=1)]
        preds = np.hstack(preds)
        return np.mean(preds == labels)",0.3583013415,
1199,miscellaneous,"def explore_city_data(city_data):
    
    
    #get the labels and features from the houssing data
    housing_prices = city_data.target
    housing_features = city_data.data
     # Size of data
    print ""No. of data points: {}"".format(housing_features.shape[0])

    # Number of features
    print ""No. of features: {}"".format(housing_features.shape[1])
    
    # Minimum housing price
    print ""Minimum housing price: {}"".format(min(housing_prices))
    
    # Maximum housing price
    print ""Maximum housing price: {}"".format(max(housing_prices))
    
    # Mean housing price
    print ""Mean housing price: {}"".format(np.mean(housing_prices))
    
    # Calculate median?
    print ""Median housing price: {}"".format(np.median(housing_prices))
    
    # Calculate standard deviation?
    print ""Standard deviation of housing prices: {}"".format(np.std(housing_prices))",0.352971524,
1199,miscellaneous,"def is_palindrom(str1):
    
    #YOUR CODE HERE
    
    #Do not use reverse function
    

def main():
    str1 = raw_input(""Enter string : "")
    res = is_palindrom(str1)
    print res
    
main()",0.3513328433,
1199,miscellaneous,"from learntools.python import roulette
import random

def random_and_superstitious(wheel):
    """"""Interact with the given wheel over 100 spins with the following strategy:
    - if the wheel lands on 4, don't bet on the next spin
    - otherwise, bet on a random number on the wheel (from 0 to 10)
    """"""
    last_number = 0
    while wheel.num_remaining_spins() > 0:
        if last_number == 4:
            # Unlucky! Don't bet anything.
            guess = None
        else:
            guess = random.randint(0, 10)
        last_number = wheel.spin(number_to_bet_on=guess)

roulette.evaluate_roulette_strategy(random_and_superstitious)",0.3511167169,
741,get familiar with real data,"def faster_function():
    d = cis.read_data(""../resources/WorkshopData2016/od550aer.nc"", ""od550aer"")
    return d.collapsed(['x','y'], how='mean')",0.3517796099,
741,get familiar with real data,"def loadTestData():
    mnist23_test = pickle.load( open( ""./datasets/mnist23.data"", ""rb"" ) )
    test_x = mnist23.data
    test_y = np.array([mnist23.target])
    return test_x,test_y",0.3480522931,
741,get familiar with real data,"def init():
    line.set_data([], [])
    return (line,)",0.3458166718,
741,get familiar with real data,"#intial function for the base frame
def init():
    line.set_data([], [])
    return line,",0.3458166718,
741,get familiar with real data,"def init():
    line.set_data([],[])
    return line,",0.3458166718,
741,get familiar with real data,"# initialization function: plot the background of each frame
def init():
    line.set_data([], [])
    return (line,)",0.3458166718,
741,get familiar with real data,"sess.get_data(['ibm us equity','aa us equity','vod ln equity'],['px last','px open'])",0.3375082612,
741,get familiar with real data,"%%time

# As xarray DataSet class.
E = era_interim.evaporation.read_all_xarray(DATA_FOLDER + ""evaporation/"")",0.3361036777,
741,get familiar with real data,"# Define a class to receive the characteristics of each line detection
class Line():
    def __init__(self):
        # x values of the last n fits of the line
        self.recent_xfitted = [] 
        #average x values of the fitted line over the last n iterations
        self.bestx = None     
        #polynomial coefficients for the previous fit
        self.previous_fit = np.array([0,0,0], dtype='float') 
        #polynomial coefficients for the most recent fit
        self.current_fit = np.array([0,0,0], dtype='float') 
        #radius of curvature of the line in some units
        self.radius_of_curvature = None 
        #x values for detected line pixels
        self.allx = None  
        #y values for detected line pixels
        self.ally = None",0.3332736492,
741,get familiar with real data,"self.tts.say(""Grab the red ball and hide it from me."" + 
                     "" I will tell you once I see the ball."")
        
        self.lastSeen = 0
        self.lastNotSeen = 0
        
        START = time.time()

        while time.time() - START < 10:

            cover_eyes(self)

            if time.time() - self.lastNotSeen > 5:
                self.tts.say(""Where's the ball? I don't see the ball"")
                self.lastNotSeen = time.time()    
        
        uncover_eyes(self)",0.3315633535,
1974,splitting criteria for classification trees,"names = [""Decision tree high confidenceFactor"", ""Decision tree low confidenceFactor""]
classifiers = [sk.tree.DecisionTreeClassifier(criterion=""entropy"", min_impurity_split=0.5), 
               sk.tree.DecisionTreeClassifier(criterion=""entropy"", min_impurity_split=0.00000000000001)]

ml.plots.plot_classifiers(names, classifiers, figuresize=(20,8))",0.5211618543,
1974,splitting criteria for classification trees,"#Building the max-tree
mxt = iamxt.MaxTreeAlpha(data,Bc)

# Area attribute extraction and computation of area extinction values
area = mxt.node_array[3,:]
Aext = mxt.computeExtinctionValues(area,""area"")
    
# Max-tree profile
for n in nextrema:
    mxt2 = mxt.clone()
    mxt2.extinctionFilter(Aext,n)
    ep[:,:,i] = mxt2.getImage()
    i+=1",0.4937440753,
1974,splitting criteria for classification trees,"# Create a new array with the added features: features_two
target = train[""Survived""].values
features_two = train[[""Pclass"",""Age"",""Sex"",""Fare"", ""SibSp"", ""Parch"", ""Embarked""]].values

# Control overfitting by setting ""max_depth"" to 10
max_depth = 10

# Control overfitting by setting ""min_samples_split"" to 5
my_tree_two = tree.DecisionTreeClassifier(max_depth = 10, min_samples_split = 5, random_state = 1)

# Create the my_tree_two model
start = time()
my_tree_two = my_tree_two.fit(features_two, target)

#Print the score of the new decison tree
print ""Our new Decision Tree prediction took {:.2f} seconds."".format(time() - start)
print ""Importance:\n"", my_tree_two.feature_importances_
print ""Score:\n"", my_tree_two.score(features_two, target)",0.4904564023,
1974,splitting criteria for classification trees,"# Variable respuesta= target 
target = train[""Survived""].values
features = train[[""Pclass"", ""Age"", ""Sex"", ""Fare"", ""SibSp"", ""Parch"", ""Embarked"",""family_size""]].values

# Crear el modelo 
my_tree = tree.DecisionTreeClassifier(max_depth =10, min_samples_split = 5, random_state = 1)
my_tree = my_tree.fit(features,target)",0.4887740612,
1974,splitting criteria for classification trees,"bbSymbolRecognition = [""QBBNamePlasmid"", ""QBBFunctionTER"", ""QBBNamePromoter"", ""QBBFunctionGameCDS"", ""QBBNameTerminator"", ""QBBFunctionBiologyCDS"", ""QBBNameRBS"", ""QBBExampleCDS"", ""QBBNameCDS"", ""QBBFunctionPR"", ""QBBFunctionRBS"", ""QBBFunctionPlasmid"", ""QBBNameOperator""]
getPerformanceFromQuestionGroup(bbSymbolRecognition, thresholdPercentage = 0.6, extraTreesClassifier = True, randomForestClassifier = True, lasso = True)",0.4883970618,
1974,splitting criteria for classification trees,"from sklearn import tree

dt_classifier = tree.DecisionTreeClassifier(criterion='gini',  # or 'entropy' for information gain
                       splitter='best',  # or 'random' for random best split
                       max_depth=None,  # how deep tree nodes can go
                       min_samples_split=2,  # samples needed to split node
                       min_samples_leaf=1,  # samples needed for a leaf
                       min_weight_fraction_leaf=0.0,  # weight of samples needed for a node
                       max_features=None,  # number of features to look for when splitting
                       max_leaf_nodes=None,  # max nodes
                       min_impurity_split=1e-07)  # early stopping

model = dt_classifier.fit(X_train, y_train)
print(model.score(X_test, y_test))",0.4870153069,
1974,splitting criteria for classification trees,"estimator = sklearn.ensemble.ExtraTreesClassifier()
param_grid = {'criterion': ['gini', 'entropy'],
                  'n_estimators':[5, 7, 10, 11, 13, 15, 17, 19, 23, 25, 29],
                  'max_features':['auto', 'log2', 10, 25, 50, 75, 100, 150, 200]  }",0.4867790639,
1974,splitting criteria for classification trees,"from sklearn import tree

dt_classifier = tree.DecisionTreeClassifier(criterion='gini',  # or 'entropy' for information gain
                       splitter='best',  # or 'random' for random best split
                       max_depth=None,  # how deep tree nodes can go
                       min_samples_split=2,  # samples needed to split node
                       min_samples_leaf=1,  # samples needed for a leaf
                       min_weight_fraction_leaf=0.0,  # weight of samples needed for a node
                       max_features=None,  # number of features to look for when splitting
                       max_leaf_nodes=None,  # max nodes
                       min_impurity_split=1e-07, #early stopping
                       random_state = 10) #random seed",0.4867436886,
1974,splitting criteria for classification trees,"synthesisQuestions = [""QDeviceRbsPconsFlhdcTer"",
                      ""QDevicePconsRbsFlhdcTer"",
                      ""QDevicePbadRbsGfpTer"",                      
                      ""QDevicePbadGfpRbsTer"",
                      ""QDeviceGfpRbsPconsTer"",
                      ""QDevicePconsGfpRbsTer"",
                      ""QDeviceAmprRbsPconsTer"",
                      ""QDeviceRbsPconsAmprTer"",
                     ]
getPerformanceFromQuestionGroup(synthesisQuestions, thresholdPercentage = 1.0, extraTreesClassifier = True, randomForestClassifier = True, lasso = True, histTarget = 14)",0.4845904112,
1974,splitting criteria for classification trees,"clf = tree.DecisionTreeClassifier(splitter='best', criterion='entropy')
test_classifier(clf, data_dict, features_list)",0.4831226468,
2007,step assign it to a variable called crime,"def fill_in_outcome(df, fight_id, newval):
    index = df[df.fightid == fight_id].index[0]
    df.at[index,'f1_outcome'] = newval
    return df",0.3934119642,
2007,step assign it to a variable called crime,"dxl_io.set_goal_position({1: 0})
dxl_io.set_goal_position({2: 0})
dxl_io.set_goal_position({3: 0})
dxl_io.set_goal_position({4: 0})",0.3925663829,
2007,step assign it to a variable called crime,"def sample_reward(env, policy, t_max=100):
    """"""
    Interact with an environment, return sum of all rewards.
    If game doesn't end on t_max (e.g. agent walks into a wall), 
    force end the game and return whatever reward you got so far.
    Tip: see signature of env.step(...) method above.
    """"""
    s = env.reset()
    total_reward = 0
    
    <play & get reward>
    return total_reward",0.3915053904,
2007,step assign it to a variable called crime,"def get_fighter_page_info(fighter_id):
    try:
        value = fighter_page_info.loc[fighter_page_info.fighterid==fighter_id,'weight'].iloc[0]
    except:
        value = 0
    
    return value",0.3909454346,
2007,step assign it to a variable called crime,"class State:
    def __init__(self, moves):
        self.to_move = 'O'
        self.utility = 0
        self.board = {}
        self.moves = cp.copy(moves)",0.3909004331,
2007,step assign it to a variable called crime,"The environment's step function returns four values:
 - observation (object): an environment-specific object representing your observation of the environment. 
        For example, pixel data from a camera, joint angles and joint velocities of a robot, or the board state in a board game.

 - reward (float): amount of reward achieved by the previous action. 
    The scale varies between environments, but the goal is always to increase your total reward.
    
 - done (boolean): whether it's time to reset the environment again. 
    Most (but not all) tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. 
    (For example, perhaps the pole tipped too far, or you lost your last life.)

 - info (dict): diagnostic information useful for debugging. 
    It can sometimes be useful for learning (for example, it might contain the raw probabilities behind the environment's last state change). However, official evaluations of your agent are not allowed to use this for learning.",0.3871581554,
2007,step assign it to a variable called crime,"def heuristic_cost_estimate(self, node):
    """""" Returns the heuristic cost estimate of a node """"""
    # TODO: Return the heuristic cost estimate of a node
    
    return distance(self,node,self.goal)",0.3826647997,
2007,step assign it to a variable called crime,"class State:
    def __init__(self, moves):
        self.to_move = 'X'
        self.utility = 0
        self.board = {}
        self.moves = cp.copy(moves)

        
class TicTacToe:
    
    def __init__(self, nrow=3, ncol=3, nwin=3, nexp=0):
        self.nrow = nrow
        self.ncol = ncol
        self.nwin = nwin
#        moves = # insert your general list of nrow x ncol moves here
        moves = [(row, col) for row in range(1, nrow + 1) for col in range(1, ncol + 1)]
        self.state = State(moves)
        self.nexp = nexp

    def result(self, move, state):
        '''
        What is the hypothetical result of move `move` in state `state` ?
        move  = (row, col) tuple where player will put their mark (X or O)
        state = a `State` object, to represent whose turn it is and form
                the basis for generating a **hypothetical** updated state
                that will result from making the given `move`
        '''

        # your code goes here
        
        # Solution:
        # Don't do anything if the move isn't a legal one
        if move not in state.moves:
            return state
        # Return a copy of the updated state:
        #   compute utility, update the board, remove the move, update whose turn
        new_state = cp.deepcopy(state)
        new_state.utility = self.compute_utility(move, state)
        new_state.board[move] = state.to_move
        new_state.moves.remove(move)
        new_state.to_move = ('O' if state.to_move == 'X' else 'X')
        return new_state

        
    def compute_utility(self, move, state):
        '''
        What is the utility of making move `move` in state `state`?
        If 'X' wins with this move, return 1;
        if 'O' wins return -1;
        else return 0.
        '''        

        # your code goes here

        # Solution:
        row, col = move
        player = state.to_move
        
        # create a hypothetical copy of the board, with 'move' executed
        board = cp.deepcopy(state.board)
        board[move] = player

        # what are all the ways 'player' could with with 'move'?
        
        # check for row-wise win
        in_a_row = 0
        for c in range(1,self.ncol+1):
            in_a_row += board.get((row,c))==player

        # check for column-wise win
        in_a_col = 0
        for r in range(1,self.nrow+1):
            in_a_col += board.get((r,col))==player

        # check for NW->SE diagonal win
        in_a_diag1 = 0
        for r in range(row,0,-1):
            in_a_diag1 += board.get((r,col-(row-r)))==player
        for r in range(row+1,self.nrow+1):
            in_a_diag1 += board.get((r,col-(row-r)))==player

        # check for SW->NE diagonal win
        in_a_diag2 = 0
        for r in range(row,0,-1):
            in_a_diag2 += board.get((r,col+(row-r)))==player
        for r in range(row+1,self.nrow+1):
            in_a_diag2 += board.get((r,col+(row-r)))==player
        
        if self.nwin in [in_a_row, in_a_col, in_a_diag1, in_a_diag2]:
            return 1 if player=='X' else -1
        else:
            return 0
        

    def game_over(self, state):
        '''game is over if someone has won (utility!=0) or there
        are no more moves left'''

        # your code goes here
        
        # Solution:
        return state.utility!=0 or len(state.moves)==0    

    
    def utility(self, state, player):
        '''Return the value to player; 1 for win, -1 for loss, 0 otherwise.'''

        # your code goes here
        
        # Solution:
        return state.utility if player=='X' else -state.utility        
        
        
    def display(self):
        for row in range(1, self.nrow+1):
            for col in range(1, self.ncol+1):
                print(self.state.board.get((row, col), '.'), end=' ')
            print()
        
    def play_game(self, player1, player2):
        '''Play a game of tic-tac-toe!'''

        # your code goes here

        # Solution:
        turn_limit = self.nrow*self.ncol  # limit in case of buggy code
        turn = 0
        while turn<=turn_limit:
            for player in [player1, player2]:
                turn += 1
                move = player(self)
                self.state = self.result(move, self.state)
                if self.game_over(self.state):
                    #self.display()
                    return self.state.utility",0.3822771609,
2007,step assign it to a variable called crime,"class State:
    def __init__(self, moves):
        self.to_move = 'O' #Who goes first?
        self.utility = 0 #Running score of winner
        self.board = {} #state of the board
        self.moves = cp.copy(moves) #moves will be a list of available movess
    

        
class TicTacToe:

    def __init__(self, nrow = 3, ncol = 3, nwin = 3):
        #initialize dimensions of the board
        self.nrow = nrow
        self.ncol = ncol
        
        #number of consecutive tiles necessary to win
        self.nwin = nwin
        
        #create a initial state of the board in the form of tuples
        #indexing beginning at (1,1) to (n,n)
        initial_state_of_board = self.create_moves(nrow, ncol)
        
        #create a State object as a member of the TicTacToe class.
        self.state = State(initial_state_of_board)
                 
    def create_moves(self, nrow, ncol):
        #generates a list of initially available moves
        moves = []
        for i in range(1, nrow+1):
            for j in range(1, ncol+1):
                moves.append((i,j))
        return moves

                 
    def result(self, move, state):
        '''
        What is the hypothetical result of move `move` in state `state` ?
        move  = (row, col) tuple where player will put their mark (X or O)
        state = a `State` object, to represent whose turn it is and form
                the basis for generating a **hypothetical** updated state
                that will result from making the given `move`
        '''
        
        new_state = cp.deepcopy(state)
        new_state.utility = self.compute_utility(move, new_state)
        #player makes a move


        #remove move from list of possible moves, as it has been used
        new_state.to_move = 'O' if new_state.to_move == 'X' else 'X'
        
        
        #give the turn to the other player
        new_state.board[move] = new_state.to_move
        new_state.moves.remove(move)
        
        return new_state
        
        

    def compute_utility(self, move, state):
        '''
        What is the utility of making move `move` in state `state`?
        If 'X' wins with this move, return 1;
        if 'O' wins return -1;
        else return 0.
        '''        
        #state.board[move] = state.to_move
        total = 0
        for i in range(1, self.nrow+1):
            total = 0 
            for j in range(1, self.ncol+1):
                if state.to_move == state.board.get((i,j)):
                    total += 1
                else: total = 0
                if total == self.nwin:
                    if state.to_move == 'X':
                        return 1
                    else: return -1
        
        for i in range(1, self.ncol+1):
            total = 0
            for j in range(1, self.nrow +1):
                if state.to_move == state.board.get((j,i)): total += 1
                else: total = 0
                    
                if total == self.nwin:
                    if state.to_move == 'X':
                        return 1
                    else: return -1
        total = 0
        
        for i in range(1, self.nrow + 1):
            if state.to_move == state.board.get((i,i)): total += 1
            else: total = 0
            if total == self.nwin:
                if state.to_move == 'X':
                    return 1
                else: return -1
                
        for i in range(1, self.nrow + 1):
            if state.to_move == state.board.get((i, self.nrow+1 - i)): total += 1
            else: total = 0
            if total == self.nwin:
                if state.to_move == 'X':
                    return 1
                else: return -1
        
        return 0



    def game_over(self):
        '''
        Is the game over?  Return True/False.
        The game is over if someone has won (utility!=0)
        or there are no more moves left.
        '''
        if len(self.state.moves) == 0:
            return self.compute_utility(self.state.to_move, self.state)
        return not len(self.state.moves) or self.compute_utility(self.state.to_move, self.state)
        

    
    def utility(self, state, player):
        '''
        Return the value to the given player; 1 for win, -1 for loss, 0 otherwise.
        '''
        return self.compute_utility(self.state.to_move, self.state)
            
    def display(self):
        '''Display the current game state.'''
        for row in range(1, self.nrow+1):
            for col in range(1, self.ncol+1):
                print(self.state.board.get((row, col), '.'), end=' ')
            print()
   
    def play_game(self, player1, player2):
        '''
        Play a game of tic-tac-toe!
        player1 and player2 are function names, either `random_player`
        (see 2b) or `alphabeta_player (see 2d)
        '''
        to_move = ""p1""
        while(not bool(self.game_over()) and len(self.state.moves) > 0):
            if to_move == 'p1':
                self.state = self.result(player1(self), self.state)
                to_move = 'p2'
            elif to_move == 'p2':
                self.state = self.result(player2(self), self.state)
                to_move = 'p1'            
        return self.game_over()",0.381524682,
2007,step assign it to a variable called crime,"# compute the overall survival rate (what fraction of passengers survived the shipwreck)

<YOUR CODE>",0.3813453615,
2451,use both a backfill and a forward fill in order to fill all the nas,"# handle_missing_values - Basic data imputation of missing values
def handle_missing_values(df):
    df['category_name'].fillna(value='missing', inplace=True)
    df['brand_name'].fillna(value='None', inplace=True)
    df['item_description'].fillna(value='None', inplace=True)",0.4395015836,
2451,use both a backfill and a forward fill in order to fill all the nas,"def fillna_masvnr(df):
    """""" Fill missing values for MasVnrType and for MasVnrArea""""""
    
    df[""MasVnrType""].fillna(value=""None"", inplace=True)
    df[""MasVnrArea""].fillna(value=0, inplace=True)
    return df",0.4372359514,
2451,use both a backfill and a forward fill in order to fill all the nas,"def handle_missing_inplace(dataset): 
    dataset['category_name'].fillna(value='missing', inplace=True) 
    dataset['brand_name'].fillna(value='missing', inplace=True) 
    dataset['item_description'].replace('No description yet,''missing', inplace=True) 
    dataset['item_description'].fillna(value='missing', inplace=True)",0.4261948764,
2451,use both a backfill and a forward fill in order to fill all the nas,"# Handle missing data.
def fill_missing_values(df):
    df.category_name.fillna(value=""Other"", inplace=True)
    df.brand_name.fillna(value=""missing"", inplace=True)
    df.item_description.fillna(value=""None"", inplace=True)
    return df

train_df = fill_missing_values(train_df)
test_df = fill_missing_values(test_df)",0.4219183028,
2451,use both a backfill and a forward fill in order to fill all the nas,"def FillMissingValues(df):
    df.category_name.fillna(value=""missing"", inplace=True)
    df.brand_name.fillna(value=""missing"", inplace=True)
    df.item_description.fillna(value=""missing"", inplace=True)
    df.item_description.replace('No description yet',""missing"", inplace=True)
    return df

print(""Filling missing data..."")
full_df = FillMissingValues(fullDF)",0.4217241406,
2451,use both a backfill and a forward fill in order to fill all the nas,"%%time

# Filling missing values
def fill_missing_values(df):
    df.category_name.fillna(value=""missing"", inplace=True)
    df.brand_name.fillna(value=""missing"", inplace=True)
    df.item_description.fillna(value=""missing"", inplace=True)
    df.item_description.replace('No description yet',""missing"", inplace=True)
    return df

print(""Filling missing data..."")
full_df = fill_missing_values(full_df)
print(full_df.category_name[1])",0.4217241406,
2451,use both a backfill and a forward fill in order to fill all the nas,"def clean_data(data, age_filler):
    data.Age = data.Age.fillna(age_filler)
    data.Fare = data.Fare.fillna(data.Fare.median())
    data.loc[data.Sex == ""female"", ""Sex""] = 1
    data.loc[data.Sex == ""male"", ""Sex""] = 0
    data.Embarked = data.Embarked.fillna(""S"")
    data.loc[data.Embarked == ""S"", ""Embarked""] = 0
    data.loc[data.Embarked == ""C"", ""Embarked""] = 1
    data.loc[data.Embarked == ""Q"", ""Embarked""] = 2
    return data

titanic_train = pd.read_csv('../../datasets/titanic_train.csv')
titanic_train = clean_data(titanic_train, titanic_train.Age.median())

titanic_test = pd.read_csv('../../datasets/titanic_test.csv')
titanic_test = clean_data(titanic_test, titanic_train.Age.median())",0.4198492765,
2451,use both a backfill and a forward fill in order to fill all the nas,"def clean_data_dummies(data, age_filler):
    data.Age = data.Age.fillna(age_filler)
    data.Fare = data.Fare.fillna(data.Fare.median())
    data = pd.concat((data, pd.get_dummies(data.Sex, prefix=""sex"")), axis=1)
    data.Embarked = data.Embarked.fillna(""S"")
    data = pd.concat((data, pd.get_dummies(data.Embarked, prefix=""embarked"")), axis=1)
    return data

titanic_train_dummies = pd.read_csv('../../datasets/titanic_train.csv')
titanic_train_dummies = clean_data_dummies(titanic_train_dummies,
                                           titanic_train_dummies.Age.median())

titanic_test_dummies = pd.read_csv('../../datasets/titanic_test.csv')
titanic_test_dummies = clean_data_dummies(titanic_test_dummies,
                                          titanic_train_dummies.Age.median())

titanic_train_dummies.columns",0.4198492765,
2451,use both a backfill and a forward fill in order to fill all the nas,"def fill_missing_data(data):
    data.category_name.fillna(value = ""Other/Other/Other"", inplace = True)
    data.brand_name.fillna(value = ""Unknown"", inplace = True)
    data.item_description.fillna(value = ""No description yet"", inplace = True)
    return data",0.4162116051,
2451,use both a backfill and a forward fill in order to fill all the nas,"def clean_2(df):
    for col in ['genre_ids', 'source_screen_name', 'source_type', 'source_system_tab']:
        df[col].fillna('-1',inplace=True)
    df['language'].fillna(-1,inplace=True)
    return df",0.4156944752,
2090,step how many items were ordered?,"def predict(self, X):
    X_transformed = X
    for step in self.steps[:-1]:
        # iterate over all but the final step
        # transform the data
        X_transformed = step[1].transform(X_transformed)
    # predict using the last step
    return self.steps[-1][1].predict(X_transformed)",0.4797011018,
2090,step how many items were ordered?,"def predict(self, X):
    X_transformed = X
    for step in self.steps[:-1]:
        # iterate over all but the final step
        # transform the data
        X_transformed = step[1].transform(X_transformed)
    # fit the last step
    return self.steps[-1][1].predict(X_transformed)",0.4797011018,
2090,step how many items were ordered?,"def get_omega_dot(self):
    p_dot = self.tau_x/self.i_x - self.r * self.q *(self.i_z - self.i_y)/self.i_x
    q_dot = self.tau_y/self.i_y - self.r * self.p *(self.i_x - self.i_z)/self.i_y
    r_dot = self.tau_z/self.i_z - self.q * self.p *(self.i_y - self.i_x)/self.i_z

    return np.array([p_dot,q_dot,r_dot])",0.4788302779,
2090,step how many items were ordered?,"def plot_progress(optimizer):
    progress = np.array(optimizer.progress).T
    plot(progress[0], progress[1] / num_episodes)",0.4739354253,
2090,step how many items were ordered?,"def velocity(self):
       self.avg_velocity = float((self.x[-1] - self.x[0])/(self.dt*len(self.x)))
       return self.avg_velocity",0.4678402543,
2090,step how many items were ordered?,"def plotstep(G):
    _, ys_step = G.step(T=ts)
    plt.plot(ts, ys_step);",0.4677967429,
2090,step how many items were ordered?,"# Percentage of re-orders

def reorders(df, name):
    reorder = df.reordered.sum() / df.shape[0]
    print (""Percentage of reorders in {} set: {}"".format(name,reorder))
    
reorders(df_order_products__prior, 'train')
reorders(df_order_products__train, 'test')",0.4665253162,
2090,step how many items were ordered?,"def normalize(self):
    mag = self.distance(Point()) # compute the distance from point to the origin
    if mag > 0:
        # if the magnitude is not 0, scale by 1 over magnitude
        self.scale(1/mag)",0.4616353214,
2090,step how many items were ordered?,"#class LearningAgent(Agent):
    
def update(self, t):
    #some other code
        
    #update state
    self.state = (inputs[""light""], self.next_waypoint)  
    #done in section 2
    
    if self.state not in self.q_table:
        #initialize q_table[self.state]
        self.q_table[self.state] = \
                    {None:0, ""left"":0, ""right"":0, ""forward"":0}
            
    #some other code",0.4616111517,
2090,step how many items were ordered?,"# class LearningAgent(Agent):
    
def update(self, t):
    #some other code
        
    #update self.t, self.epsilon, self.alpha
    self.t += 1
    self.alpha = 1.0/self.t
    self.epsilon -= 0.00046
        
    #some other code",0.4613206387,
1002,k means clustering,"def create_estimator(run_config, hparams):
    
    estimator = tf.contrib.learn.KMeansClustering(
        num_clusters = hparams.num_clusters,
        initial_clusters= tf.contrib.factorization.RANDOM_INIT,
        distance_metric= tf.contrib.factorization.SQUARED_EUCLIDEAN_DISTANCE,
        use_mini_batch=True,
        mini_batch_steps_per_iteration=1,
        kmeans_plus_plus_num_retries=10,
        relative_tolerance=None,
        config= run_config
    )

    print("""")
    print(""Estimator Type: {}"".format(type(estimator)))
    print("""")
    
    return estimator",0.5302420855,
1002,k means clustering,"def k_means(train_x):  
    from sklearn.cluster import KMeans
    model = KMeans(10)  
    model.fit(train_x)  
    return model",0.5240443349,
1002,k means clustering,"def doKMeans(data, num_clusters=0, verbose = 0):
    
    from sklearn.cluster import KMeans
    
    model = KMeans(n_clusters = num_clusters)
    
    # print(data)
    
    # fit the data
    
    model.fit(data.loc[:,['TowerLat', 'TowerLon']])
    
    # plotting function
    
    if verbose == 1:
        
        fig = plt.figure()
        ax = fig.add_subplot(111)
        ax.scatter(model.cluster_centers_[:,1],
                   model.cluster_centers_[:,0],
                   s=169,
                   c='r',
                   marker='x',
                   alpha=0.8,
                   linewidths=2)

        ax.scatter(data.TowerLon, data.TowerLat, c = ""g"", alpha = .2)
        
        ax.set_title('Weekday Calls Centroids, User: ' + str(data['In'].iloc[0]))

    plt.show()
    
    return model",0.52336514,
1002,k means clustering,"if (param.clustering == clust_kneams):
            if (param.affine == 'cosine'):
                # k-means cosine dist
                clusters, kmns_class = clst.kmeans_cosine_dist(emails_representation, param.k)
            elif (param.affine == 'euclidean'):  # k-means euclidean
                clusters, kmns_class = clst.kmeans(emails_representation, param.k)
                print('kmeans sum of distanses = %f' % kmns_class.inertia_)
        elif (param.clustering == clust_hirarchical):
            # hirarchical
            (clusters, _) = clst.hirarchical(emails_representation, param.k, aff=param.affine, link=param.linkage)",0.52054739,
1002,k means clustering,"if (param.clustering == clust_kneams):
            if (param.affine == 'cosine'):
                # k-means cosine dist
                clusters, kmns_class = clst.kmeans_cosine_dist(emails_representation, param.k)
            elif (param.affine == 'euclidean'):  # k-means euclidean
                clusters, kmns_class = clst.kmeans(emails_representation, param.k)
                print('kmeans sum of distanses = %f' % kmns_class.inertia_)
            else:
                raise ValueError('kmenas is not supported with affinity: ' + param.affine)
        elif (param.clustering == clust_hirarchical):
            # hirarchical
            (clusters, _) = clst.hirarchical(emails_representation, param.k, aff=param.affine, link=param.linkage)
        else:
            raise ValueError('clustering is not supported with: ' + param.clustering)",0.52054739,
1002,k means clustering,"def evaluate_kmeans(k):
    model = sklearn.cluster.KMeans(n_clusters=k, n_jobs=-1)
    model.fit_predict(X)
    
    return model.inertia_

x = range(1, 21)
y = [evaluate_kmeans(k) for k in x]

plt.plot(x, y)
plt.xticks(x)
plt.show()",0.5143203139,
1002,k means clustering,"class kmeansclustering:
    def __init__(self, data, k=2, maxiter=100, epsilon=1e-12):
        '''
        data: input data, numpy ndarray
        k: the number of centroids
        '''
        self.data = data
        self.k = k
        self.maxiter = maxiter
        self.epsilon = epsilon
        self.N = len(data)
        self.colors = cm.rainbow(np.linspace(0, 1, self.k))
        self.classes = np.zeros(self.N, dtype=int)        
    
    def getdistmat(self):
        data_sqrowsum = np.sum(self.data * self.data, axis=1)
        cen_sqrowsum = np.sum(self.centroids * self.centroids, axis=1)
        return np.outer(data_sqrowsum, np.ones((1, self.k))) - 2 * np.dot(self.data, self.centroids.T) \
                    + np.outer(np.ones((self.N, 1)), cen_sqrowsum)
    
    def compute_obj(self):
        tempsum = 0
        for i in xrange(self.N):
            tempsum += np.sum(np.square(self.data[i] - self.centroids[self.classes[i]]))
        return tempsum
    
    def kmeans(self, plot=False):
        numiter = 0
        # initialize centroids
        self.centroids = self.data[np.random.choice(self.N, self.k, replace=False), :]
        if plot:
            self.draw(False)
        preval = self.compute_obj()
        while numiter < self.maxiter:
            distmat = self.getdistmat()
            # assign datapoints to clusters
            self.classes = np.argmin(distmat, axis=1)
            # update centroids
            for c in xrange(self.k):
                self.centroids[c] = np.mean(self.data[self.classes == c], axis=0)
            objval = self.compute_obj()
            # check convergence
            if preval - objval < self.epsilon:
                print 'exit before max iterations'
                break
            preval = objval
            if plot:
                self.draw(True)
            numiter += 1
    
    def draw(self, plotcen=True):
        '''
        only for 2-dimension cases
        '''
        plt.figure(figsize=(10,10), facecolor='white')
        colors_data = [self.colors[c] for c in self.classes]
        plt.scatter(self.data[:, 0], self.data[:, 1], color=colors_data, marker='.', alpha=0.9, s=80)
        plt.axis('equal')
        if plotcen:
            plt.scatter(self.centroids[:, 0], self.centroids[:, 1], marker='o', color=self.colors, s=120)
        plt.show()",0.5092687607,
1002,k means clustering,"def kmeans_cluster(df):
    #Determine amount of clusters
    number_of_cluster = np.round(np.sqrt(len(df)/2))
    
    kmeans_model = skc.KMeans(n_clusters=int(number_of_cluster))
    kmeans_model.fit(df)
    centroids = kmeans_model.cluster_centers_
    
    #Create column for K-means label for each flavoprotein
    df['cluster_labels_kmeans'] = kmeans_model.labels_
    
    return(df)",0.5089979172,
1002,k means clustering,"## choosing n_clusters = 2
    clusterer = KMeans(n_clusters=2, random_state=0).fit(reduced_data)

    # TODO: Predict the cluster for each data point
    preds = clusterer.predict(reduced_data)

    # TODO: Find the cluster centers
    centers = clusterer.cluster_centers_

    # TODO: Predict the cluster for each transformed sample data point
    sample_preds = clusterer.predict(pca_samples)",0.5068058968,
1002,k means clustering,"for k in range (1, 11):
 
        # Create a kmeans model on our data, using k clusters.  random_state helps ensure that the algorithm returns the same
        # results each time.
        kmeans_model = KMeans(n_clusters=k, random_state=1).fit(X)
 
        # These are our fitted labels for clusters -- the first cluster has label 0, and the second has label 1.
        labels = kmeans_model.labels_
        
	    # Sum of distances of samples to their closest cluster center
	    interia = kmeans_model.inertia_
        print ""k:"",k, "" cost:"", interia",0.5063171387,
187,character class negation,"# Sex

# As we see, children(age < ~16) on aboard seem to have a high chances for Survival.
# So, we can classify passengers as males, females, and child
def get_person(passenger):
    age,sex = passenger
    if age < 16:
        return 'child'
    elif age>= 65:
        return 'senior'
    else: 
        return sex
    
titanic_df['Person'] = titanic_df[['Age','Sex']].apply(get_person,axis=1)
test_df['Person']    = test_df[['Age','Sex']].apply(get_person,axis=1)

# No need to use Sex column since we created Person column
titanic_df.drop(['Sex'],axis=1,inplace=True)
test_df.drop(['Sex'],axis=1,inplace=True)

# create dummy variables for Person column, & drop Male as it has the lowest average of survived passengers
person_dummies_titanic  = pd.get_dummies(titanic_df['Person'])
person_dummies_titanic.columns = ['Child','Senior','Female','Male']
person_dummies_titanic.drop(['Male'], axis=1, inplace=True)

person_dummies_test  = pd.get_dummies(test_df['Person'])
person_dummies_test.columns = ['Child','Senior','Female','Male']
person_dummies_test.drop(['Male'], axis=1, inplace=True)

titanic_df = titanic_df.join(person_dummies_titanic)
test_df    = test_df.join(person_dummies_test)",0.4918588996,
187,character class negation,"def person_type(x):
  if x <=16:
    return 'C'
  elif x <= 48:
    return 'A'
  elif x <= 90:
    return 'S'
  else:
    return 'U'",0.4882148802,
187,character class negation,"def AgeClassifier(age):
    if age != age:
        return 'Unknown'
    elif age <= 2:
        return 'Infant'
    elif age <= 13:
        return 'Child'
    elif age <= 20:
        return 'Teen'
    elif age <= 64:
        return 'Adult'
    else: return '65+'

titanic_data['AgeCategory'] = titanic_data['Age'].map(AgeClassifier) # New column which I will use for analysis of age",0.4881239533,
187,character class negation,"def Label_Age(age):
    #Function that converts an age to a label.
    if age >= 0 and age <27:
        return ""Young""
    elif age>=27 and age <54:
        return ""Middle-Aged""
    else:
        return ""Old""
    
labeled_Age = cleaned_titanic.Age.apply(Label_Age)
cleaned_titanic[""Labeled_Age""] = labeled_Age
cleaned_titanic.head()",0.4866414666,
187,character class negation,"# Survived by class of age
# Let's divide the people in classes of age: children (under 14), adolescent(14-20), adult(21-65), senior(65+)
def group_age(value):
    if value <= 14:
        return ""Children""
    elif value <= 20:
        return ""Adolescent""
    elif value <= 65:
        return ""Adult""
    elif value <= 100:
        return ""Senior""
    else:
        return ""No data""
    
train_data['GroupAge'] = train_data['Age'].apply(group_age)

Survived_groupAge = train_data.groupby('GroupAge').sum()
Total_groupAge = train_data.groupby('GroupAge').size()
print(Survived_groupAge)
print(Total_groupAge)
print('----------')
print(round(Survived_groupAge['Survived']/Total_groupAge, 2))
print('----------')",0.4854563773,
187,character class negation,"def cat_Status(scr):
    if(scr < 151):
        return 'A'
    elif(scr < 251):
        return 'B'
    else:
        return 'C'",0.4838635325,
187,character class negation,"def compute_season(value):
    # Sources for determining season:
    # 1) Googling ""official spring/summer/fall/winter dates""
    # 2) https://simple.wikipedia.org/wiki/Solstice
    if (value > '03-19' and value < '06-21'):
        return 'Spring'
    elif (value > '06-20' and value < '09-23'):
        return 'Summer'
    elif (value > '09-22' and value < '12-22'):
        return 'Fall'
    elif ((value > '12-21' and value <= '12-31') or (value >= '01-01' and value < '3-20')):
        return 'Winter'
    else:
        return 'Error'
    
q2['MONTH-DAY'] = q2.index.map(lambda x: x.strftime(""%m-%d""))
q2['SEASON'] = q2['MONTH-DAY'].map(lambda x: compute_season(x))
q2['SEASON'].value_counts()",0.4811096191,
187,character class negation,"def simpleThreshold(x):
    if x <= 5:
        return 0
    elif x <= 49:
        return 1
    elif x <= 78:
        return 2
    else:
        return 3

x = range(-100,100)
y = map(simpleThreshold, x)
plt.plot(x, y)
plt.ylim([-5, 5])
plt.show()",0.4807130098,
187,character class negation,"def replace_diag_code(code):
    if (code <= '459' and code >= '390') or code == '785':
        return 'Circulatory'
    elif (code <= '519' and code >= '460') or code == '786':
        return 'Respiratory'
    elif (code <= '579' and code >= '520') or code == '787':
        return 'Digestive'
    elif (code < '251' and code > '249'):
        return 'Diabetes'
    elif (code <= '999' and code >= '800'):
        return 'Injury'
    elif (code <= '739' and code >= '710'):
        return 'Musculoskeletal'
    elif (code <= '629' and code >= '580') or code == '788':
        return 'Genitourinary'
    elif (code <= '239' and code >= '140'):
        return 'Neoplasms'
    elif code == '?': 
        return None
    else:
        return 'Others'",0.4806450605,
187,character class negation,"def season_label(month):
    if(12 == month or 1 <= month <= 2):
        return ""Winter""
    elif(3 <= month <= 5):
        return ""Spring""
    elif(6 <= month <= 8):
        return ""Summer""
    elif(9 <= month <= 11):
        return ""Fall""",0.4803841114,
2181,step training and testing sets,"def train(model, epoch, criterion, optimizer, data_loader):
    model.train()
    for batch_idx, (data, target) in enumerate(data_loader):
        if cuda_gpu:
            data, target = data.cuda(), target.cuda()
            model.cuda()
        data, target = Variable(data), Variable(target)
        output = model(data)
        
        optimizer.zero_grad()
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        if (batch_idx+1) % 400 == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, (batch_idx+1) * len(data), len(data_loader.dataset),
                100. * (batch_idx+1) / len(data_loader), loss.data[0]))",0.5317697525,
2181,step training and testing sets,"def plot_lat_lon():
    global training_data
    data = training_data[(training_data.longitude < 0) & (training_data.latitude > 0)]
    plt.figure(figsize=(7,6))
    for klass in ['low', 'medium', 'high']:
        subdata = data[data.interest_level == klass]
        plt.scatter(subdata['longitude'], subdata['latitude'], alpha=0.4)
    plt.legend(['low', 'medium', 'high'])
    plt.show()
plot_lat_lon()",0.5304428339,
2181,step training and testing sets,"def testOnce(data):
    # split the data into training and testing sets
    (trainingData, testData) = data.randomSplit([1-test_size, test_size])
     # train the random forest
    model = RandomForest.trainClassifier(trainingData, numClasses=3, categoricalFeaturesInfo={},
                                         numTrees=num_trees, featureSubsetStrategy=""auto"",
                                         impurity='gini', maxDepth = max_depth, maxBins=32)
    # test the random forest
    predictions = model.predict(testData.map(lambda x: x.features))
    labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)
    testErr = labelsAndPredictions.filter(lambda (v, p): v != p).count() / float(testData.count())
    Mg = float(labelsAndPredictions.filter(lambda (v, p): v == 0 and p == 1).count())
    Ng = float(labelsAndPredictions.filter(lambda (v, p): v == 0 and p == 0).count())
    Ms = float(labelsAndPredictions.filter(lambda (v, p): v == 1 and p == 0).count())
    Ns = float(labelsAndPredictions.filter(lambda (v, p): v == 1 and p == 1).count())
    probsAndScores = probTest(testData, model)
    threshold_accuracy = probsAndScores[0]
    probs = probsAndScores[1].map(lambda x: x/num_trees)
    labelsAndPredictions = labelsAndPredictions.zip(probs)
    #save(labelsAndPredictions, 'answers')
    print ('Galaxy Purity = ' + str(Ng / (Ng+Ms)))
    print ('Galaxy Completeness = ' + str(Ng / (Ng+Mg)))
    print ('Star Purity = ' + str(Ns / (Ns+Mg)))
    print ('Star Completeness = ' + str(Ns/(Ns+Ms)))
    print ('Accuracy = ' + str(1 - testErr))
    print ('Threshold method accuracy = ' + str(threshold_accuracy))",0.5271722674,
2181,step training and testing sets,"def train(epoch, callback):
    model.train()
    
    for batch_idx, (data, target) in enumerate(train_loader):
        if torch.cuda.is_available():
            data, target = data.cuda(), target.cuda()
        data, target = Variable(data), Variable(target)

        optimizer.zero_grad()
        
        output = model(data)
        
        loss = F.cross_entropy(output, target)
        loss.backward()
        
        optimizer.step()
        
        callback(batch_idx, output, target)
        
        print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),
                100. * (batch_idx + 1) / len(train_loader), loss.data[0]))",0.5251941681,
2181,step training and testing sets,"class trainer():
    def __init__(self,model,optimizer,loss,trainloader):
        self.model=model
        self.optimizer=optimizer
        self.loss=loss
        if torch.cuda.is_available():
            self.loss=self.loss.cuda()
        self.trainloader=trainloader
        
    def stopcon(self):
        def deriv(ns):
            return [ns[i+1]-ns[i] for i in range(len(ns)-1)]
        return sum(deriv(self.errors['val_error'][-10:]))>0
    
    def save_model(self,path):
        torch.save(self.model.state_dict(),path)
        
    def train(self,epoch,val_loader,val_size,train_size):
        print(""Start training..."")
        print(self.optimizer,self.loss)
        print(len(self.trainloader), self.trainloader)
        self.errors={'val_error':[],'loss':[],'train_error':[]}
        schedule=StepLR(self.optimizer,step_size=1,gamma=0.5)
        for i in range(epoch):
            if self.stopcon():
                return
            correct = 0
            epoch_loss = 0
            t1=time.time()
            schedule.step()
            for index,(data,label) in enumerate(self.trainloader):
                self.optimizer.zero_grad()
                inputs=to_variable(data.view(-1,inputsize))
                outputs=self.model.forward(inputs)
                labels=to_variable(label)
                loss=self.loss(outputs,labels)
                loss.backward()
                self.optimizer.step()
                pred = outputs.data.max(1, keepdim=True)[1]
                predicted = pred.eq(labels.data.view_as(pred))
                correct += predicted.sum()
                epoch_loss+=loss.data[0]
            t2=time.time()
            total_loss=epoch_loss/train_size
            train_error=1-correct/train_size
            val_accu=inference(self.model,val_loader,val_size)
            print('epoch:{0},loss:{1:.8f},validate accuracy:{2:.8f},train error:{3:.8f},time:{4:.2f}'.format(i+1,total_loss,val_accu,train_error,t2-t1))
            self.errors['val_error'].append(1-val_accu)
            self.errors['train_error'].append(train_error)
            self.errors['loss'].append(total_loss)
        self.model.eval()",0.522313118,
2181,step training and testing sets,"def train(epoch):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        if cuda:
            data, target = data.cuda(), target.cuda()
        data, target = Variable(data), Variable(target)
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()
        if batch_idx % log_interval == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.data[0]))",0.5211908221,
2181,step training and testing sets,"def benchmark_model(model, X_train, y_train, X_test, y_test):
    model.fit = timing(model.fit)
    model.predict_proba = timing(model.predict_proba)
    
    #fit
    model.fit(X_train, y_train)
    
    #prediction
    y_pred_probs = model.predict_proba(X_test)
    y_pred = np.argmax(y_pred_probs, axis=1)
    
    #scores
    roc = roc_auc_score(y_test, y_pred_probs[:, 1])
    prec = precision_score(y_test, y_pred)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    
    print(""ROC-AUC: {}\nPrecision: {}\nRecall: {}\nF1: {}\n"".format(roc, prec, rec, f1))
    
    print(""Confusion matrix\nRows  true labels\nColumns - predicted labels"")
    conf_matrix = confusion_matrix(y_test, y_pred)
    cm_df = pd.DataFrame(conf_matrix)
    display(cm_df)",0.5190149546,
2181,step training and testing sets,"class agent():
    def __init__(self, lr, s_size, a_size, h_shape):
        # Dropout probability
        #self.keep_prob = tf.placeholder(tf.float32)
        self.trainig = False
        
        #These lines established the feed-forward part of the network. The agent takes a state and produces an action.
        self.state_in= tf.placeholder(shape=[None,s_size],dtype=tf.float32)
        self.hidden = slim.stack(self.state_in, slim.fully_connected, h_shape, biases_initializer=None, activation_fn=tf.nn.relu)
        self.dropout = slim.dropout(self.hidden, is_training=self.trainig)
        self.output = slim.fully_connected(self.dropout,a_size,activation_fn=tf.nn.softmax,biases_initializer=None)
        self.chosen_action = tf.argmax(self.output,1)

        #The next six lines establish the training proceedure. We feed the reward and chosen action into the network
        #to compute the loss, and use it to update the network.
        self.reward_holder = tf.placeholder(shape=[None],dtype=tf.float32)
        self.action_holder = tf.placeholder(shape=[None],dtype=tf.int32)
        
        self.indexes = tf.range(0, tf.shape(self.output)[0]) * tf.shape(self.output)[1] + self.action_holder
        self.responsible_outputs = tf.gather(tf.reshape(self.output, [-1]), self.indexes)

        self.loss = -tf.reduce_mean(tf.log(self.responsible_outputs)*self.reward_holder)
        
        tvars = tf.trainable_variables()
        self.gradient_holders = []
        for idx,var in enumerate(tvars):
            placeholder = tf.placeholder(tf.float32,name=str(idx)+'_holder')
            self.gradient_holders.append(placeholder)
        
        self.gradients = tf.gradients(self.loss,tvars)
        
        optimizer = tf.train.AdamOptimizer(learning_rate=lr)
        self.update_batch = optimizer.apply_gradients(zip(self.gradient_holders,tvars))",0.5182192326,
2181,step training and testing sets,"def do_fit(classifier, X, y):
    classifier.n_iter = np.ceil(10**6 / len(y))
    classifier.fit(X, y)
    predict_y = classifier.predict(X=X)
    success = 1-sum(abs(predict_y - y))/len(y)
    print(""Hypothesis prediction success rate is %.2f"" %success)",0.5173076987,
2181,step training and testing sets,"class Trainer():

    def __init__(self, batch_size, checkpoints_path, dropout):
        self.batch_size = batch_size
        self.checkpoints = checkpoints_path
        self.path_to_graph = checkpoints_path + 'seq2seq'
        self.dropout = dropout

    def train(self, model, train_data, train_size, num_steps, num_epochs, min_loss=0.3):
        """"""
        Trains a given model architecture with given train data.
        """"""
        tf.set_random_seed(1234)
        
        with tf.Session() as sess:
            sess.run(tf.global_variables_initializer())
            total_loss = []
            timings = []
            steps_per_epoch = int(train_size / self.batch_size)
            num_epoch = 1
            
            for step in range(1, num_steps):
                beg_t = timeit.default_timer()
                X, L = train_data.next()
                seq_len = np.max(L)

                # For anomaly detection problem we reconstruct input data, so
                # targets and inputs are identical.
                feed_dict = {
                    model.inputs: X,
                    model.targets: X,
                    model.lengths: L,
                    model.dropout: self.dropout,
                    model.batch_size: self.batch_size,
                    model.max_seq_len: seq_len}
                
                fetches = [model.loss, model.decoder_outputs, model.train_optimizer]
                step_loss, _, _ = sess.run(fetches, feed_dict)

                total_loss.append(step_loss)
                timings.append(timeit.default_timer() - beg_t)

                if step % steps_per_epoch == 0:
                    num_epoch += 1

                if step % 200 == 0 or step == 1:
                    print_progress(
                        int(step / 200),
                        num_epoch,
                        np.mean(total_loss),
                        np.mean(step_loss),
                        np.sum(timings))
                    timings = []

                if step == 1:
                    _ = tf.train.export_meta_graph(filename=self.path_to_graph + '.meta')
                
                if np.mean(total_loss) < min_loss or num_epoch > num_epochs:
                    model.saver.save(sess, self.path_to_graph, global_step=step)
                    print(""Training is finished."")
                    break",0.516684711,
2446,united states crime rates,"def recategorize_part1_part2(df):
    list_crimes = list()
    for crime in df.crime_category:
        if crime in part1_crimes:
            list_crimes.append(0)
        else:
            list_crimes.append(1)
    df.crime_category = list_crimes
    return df",0.474858582,
2446,united states crime rates,"def Predictiveness(cname):
    tVals = numpy.array(oriDF.loc[oriDF.highCrime == True, cname])
    fVals = numpy.array(oriDF.loc[oriDF.highCrime == False, cname])
    nume = abs(tVals.mean() - fVals.mean())
    deno = tVals.std() + fVals.std()
    val = float('%.4f' % (nume/deno))
    return val

def PredictiveFeatures(df):
    tupList = []
    colNames = list(df.columns.values)
    for cname in colNames:
        tup = (cname, Predictiveness(cname))
        tupList.append(tup)
    return tupList

labels = ['Feature', 'Predictiveness']
PredictiveFeaturesList = pandas.DataFrame.from_records(PredictiveFeatures(modDF), columns=labels).sort_values(""Predictiveness"", ascending=False)
PredictiveFeaturesList[:10]",0.4514326751,
2446,united states crime rates,"#crimes vs mean max temperature per month
    dfCrMoUnformat = df['Month'].value_counts()

    dfCrMo = pd.concat([dfCrMoUnformat,dfCrMoUnformat.index.to_series()], axis=1).reset_index(drop=True)
    dfCrMo.columns = ['CrimeCount', 'Month']


    dfGraph = pd.merge(ad, dfCrMo, on='Month')


    fit = np.polyfit(dfGraph['Temperature'],dfGraph['CrimeCount'],1)
    fit_fn = np.poly1d(fit)



    fig, ax = plt.subplots()
    ax.scatter(dfGraph['Temperature'], dfGraph['CrimeCount'])
    plt.plot(dfGraph['Temperature'], fit_fn(dfGraph['Temperature']), '--k')
    plt.xlabel('Maximum Monthly Average Temperature in 2014 [F]', fontsize=12)
    plt.ylabel('Monthly Number of Offenses in Seattle 2014', fontsize=12)

    for i, txt in enumerate(['Dec','Nov','Oct','Sep','Agu','Jul','Jun','May','Apr','Mar','Feb','Jan']):
        ax.annotate(txt, (dfGraph['Temperature'][i]+0.5,dfGraph['CrimeCount'][i]))

    plt.show()",0.4501845837,
2446,united states crime rates,"#Most Relevant Crimes

    dfCrCo = 100 * df['Summarized Offense Description'].value_counts() / len(df['Summarized Offense Description'])

    #print the 10 most relevant crimes
    dfCrCoOrder   = dfCrCo.order(ascending = False) #sort a series

    table = pd.DataFrame(dfCrCoOrder.head(10))
    table.columns = ['Offense Ratio [%]']
    table.columns.name = 'Offense'
    table",0.4446867704,
2446,united states crime rates,"if PROCESS_FULLY:
    sns.distplot(df.where(df.State=='TX').vcrime_rate.dropna(), label=""Violent Crime Rates in Texas"")
    sns.distplot(df.vcrime_rate.dropna(), axlabel=""Violent Crime Rates in US and Texas"")",0.4194681346,
2446,united states crime rates,"def generate_record(university):
    """"""
    get amount for this university
    
    params:
        university (str): universitye name
        
    return:
        a tuple: university and its amount
    """"""
    university_df = data[data.University == university].copy()
    university_df = university_df[university_df['Approved Amount'] != 'data not included in P3']
    university_df['amount'] = university_df['Approved Amount'].astype('float')

    return university, ""{0:.2f}"".format(university_df['amount'].sum())",0.416231811,
2446,united states crime rates,"def holt_winters_model(train=train_data, test=test_data, seasonal_components=12):
    
    """""" train_data - the data subsetted in the previous func
    to train the model on.
        test data - the data to evaluate the results of the model
    against. 
        seasonal_components - a parameter within the exponential 
    smoothing model to spicfy the number of seasonal components. 
    This will be default to 4 for the four seasons typically
    experienced in NYC. 
    
    """"""
    # container for errors
    errors = []
    # run the model on each borough in nyc
    for boro in train.Borough.unique():
        
        #subset data 
        train_boro = (train
                 .query(""Borough == '%s'"" %boro)
                 .reset_index(drop=True)
                 .sort_values(""Date"")
                 .set_index([""Date""])
                )
        test_boro = (test
                .query(""Borough == '%s'"" %boro)
                .reset_index(drop=True)
                .sort_values(""Date"")
                .set_index(""Date"")
               )

        yhat_avg = test_boro.copy()

        # fit the model
        model = ExponentialSmoothing(np.asarray(train_boro.Count), 
                                     seasonal_periods=seasonal_components,
                                     trend=""add"",
                                     seasonal=""add"",
                                    ).fit()

        # create predictions
        yhat_avg[""Holt_Winter""] = model.forecast(len(test_boro))

        # create plot
        plt.figure(figsize=(15,7))
        plt.plot(train_boro.Count, label=""Train"")
        plt.plot(test_boro.Count, label=""Test"")
        plt.plot(yhat_avg.Holt_Winter, label=""Holt Winter"")
        plt.legend(loc=""best"")
        plt.title(""Holt Winters Model for %s"" %boro)
        plt.show()
        
        # get errors
        errors.append(mean_absolute_error(test_boro.Count,
                                          yhat_avg.Holt_Winter
                                         )
                     )
    return errors
    
# run model
errors = holt_winters_model()",0.4113365412,
2446,united states crime rates,"(ds[ds.crime_solved == 'Yes'].state.value_counts().sort_index() / ds.state.value_counts().sort_index()).plot.bar(figsize=(16,4))",0.4085354507,
2446,united states crime rates,"for city in cities.values():
    print(city.name)
    city.load_weather().load_crime().merge_dfs()
    df = city.df[['Temperature', 'Crime']].dropna()
    sns.jointplot(x='Temperature', y='Crime', data=df)
    plt.title(city.name)",0.4060634971,
2446,united states crime rates,"#create a stacked bar graph for the demograpgic breakdown of these states with the highest per-capita gun ratio
    
    t_race.set_index('Race')\
      .reindex(t_race.set_index('Race').sum().sort_values().index, axis=1)\
      .T.plot(kind='bar', stacked=True,
              colormap=ListedColormap(sns.color_palette(""GnBu"", 10)), 
              figsize=(12,8)).set_title('Demographic Breakdown of States with the Highest Per-Capita Gun Ratio')",0.4055433273,
546,evaluating the models,"def plot_approximations(output_object):
    
    output_object[""sigmoid""][""model""].eval()
    output_object[""relu""][""model""].eval()

    inputs = Variable(output_object[""x_sampled""])
    sigmoid_output = output_object[""sigmoid""][""model""](inputs)
    relu_output = output_object[""relu""][""model""](inputs)
    
    plt.figure(figsize=(10, 10))
    plt.subplot(211)
    plt.plot(output_object[""x_linear""].squeeze().numpy(), output_object[""y_linear""].squeeze().numpy(), 'g', label=""true function"")
    plt.scatter(output_object[""x_sampled""].squeeze().numpy(), sigmoid_output.data.squeeze().numpy(), alpha=0.5, c='r', s=5, label=""sigmoid approximation"")
    plt.legend()
    
    plt.subplot(212)
    plt.plot(output_object[""x_linear""].squeeze().numpy(), output_object[""y_linear""].squeeze().numpy(), 'g', label=""true function"")
    plt.scatter(output_object[""x_sampled""].squeeze().numpy(), relu_output.data.squeeze().numpy(), alpha=0.5, c='b', s=5, label=""relu approximation"")
    plt.legend()
    
    print(""MSE for sigmoid {}"".format(torch.norm(sigmoid_output.data - output_object[""y_sampled""])))
    print(""MSE for RELU {}"".format(torch.norm(relu_output.data - output_object[""y_sampled""])))
    
    plt.show()",0.3686636686,
546,evaluating the models,"class Analyzer():
    def __init__(self,history_path,evaluate_result_path):
        self.history = None
        self.evaluate_result = None
        self.load_history(history_path)
        self.load_evaluate_result(evaluate_result_path)

    # training history
    def load_history(self,path):
        with open(path,'rb') as f:
            self.history = pkl.load(f)
             
    
    def load_evaluate_result(self,path):
        with open(path,'rb') as f:
            self.evaluate_result = pkl.load(f)
        
    
    def analyze(self):
        loss ,reward_sum = self.history
        test_success_history,test_reward_history = self.evaluate_result
        
        average_success = np.array(test_success_history).astype(float).mean()
        average_reward = np.array(test_reward_history).mean()
        
        episodes = list(range(0,len(loss)))

        plt.figure(figsize=(20,10))
        plt.subplots_adjust(wspace=0.25, hspace=0.25)
        
        
        plt.subplot(221)
        plt.xlabel('epsidoe')
        plt.ylabel('loss')
        plt.xticks(np.arange(0,max(episodes)+1, 50))
        plt.title('training loss')
        plt.plot(loss)
        
        
       
        
        plt.subplot(222)
        plt.xlabel('epsidoe')
        plt.ylabel('reward sum')
        plt.xticks(np.arange(0,max(episodes)+1, 50))
        plt.title('trainig reward sum ')
        plt.plot(reward_sum)

        episodes = list(range(0,len(test_success_history)))
        
        plt.subplot(223)
        plt.xlabel('epsidoe')
        plt.ylabel('success rate')
        plt.xticks(np.arange(0,max(episodes)+1, 10))
        plt.title('evaluation success rate')
        plt.plot(test_success_history)
        
        
        plt.subplot(224)
        plt.xlabel('epsidoe')
        plt.ylabel('reward sum')
        plt.xticks(np.arange(0,max(episodes)+1, 10))
        plt.title('evaluation reward sum')
        plt.plot(test_reward_history)
        
        print('average reward is %.3f success rate is %.3f'%(average_reward,average_success))",0.3535333872,
546,evaluating the models,"with tf.Session() as sess:
    sess.run(init)
                
    print(y_add.eval(session=sess))
    print(y_mul.eval(session=sess))
    print('-'*30)
    print(sess.run(y_add))
    print(sess.run(y_mul))",0.3522962928,
546,evaluating the models,"class Model:
    def __init__(self, env, feature_transformer):
        self.env = env
        self.models = []
        self.feature_transformer = feature_transformer
        for i in range(env.action_space.n):
            model = SGDRegressor(feature_transformer.dimensions)
            self.models.append(model)

    def predict(self, s):
        X = self.feature_transformer.transform(np.atleast_2d(s))
        return np.array([m.predict(X)[0] for m in self.models])

    def update(self, s, a, G):
        X = self.feature_transformer.transform(np.atleast_2d(s))
        self.models[a].partial_fit(X, [G])

    def sample_action(self, s, eps):
        if np.random.random() < eps:
            return self.env.action_space.sample()
        else:
            return np.argmax(self.predict(s))",0.3395619392,
546,evaluating the models,"class evaluation:
    def __init__(self, nbEpisodes, nbTimesteps):
        self.nbTimesteps=nbTimesteps
        self.nbEpisodes=nbEpisodes
        self.episodesEvaluated=100
        if nbEpisodes < self.episodesEvaluated:
            self.episodesEvaluated=int(nbEpisodes/2)+1
        self.rewardLimit=-110

        self.cycleFirstArrival=[]
        self.cycleArrivedNb=[]
        self.cycleArrivePrct=[]
        self.cycleRewardMax=[]
        self.cycleRewardMaxEpi=[]
        self.cycleAverageMax=[]
        self.cycleAverageMaxEpi=[]
        self.cycleRewAverages=[]
        self.cycleRewards=[]
        self.cycleProblemSolved=[]
        
        
        
        
        self.reset()
        print ('evaluation initialized')
        
    def reset(self):
        self.lastEpisode=-1
        self.arrivedNb=0                 #how often did the car arrive
        self.arrivedFirst=0              #which episode for the first time 
        self.rewardMax=-200              #max reward seen
        self.rewardMaxEpisode=0          #in which episode did we see the max reward
        self.rewardTracker = np.zeros(self.episodesEvaluated)
        self.rewardAverages=[]
        self.rewards=[]
        self.problemSolved=False
        self.averageMax=-200
        self.averageMaxEpisode=0
        self.lastAverage=-200
        print ('evaluation reset')
        
    def EpiTrack (self,episode, step, rwdAcc):
        self.lastEpisode=episode
        if rwdAcc>self.rewardMax:
            self.rewardMax=rwdAcc
            self.rewardMaxEpisode=episode
        self.rewards.append(rwdAcc)
            
        self.rewardTracker[episode%self.episodesEvaluated]=rwdAcc
        
        if episode>self.episodesEvaluated:
            self.lastAverage=np.average(self.rewardTracker)
        else:
            self.lastAverage=np.average(self.rewardTracker[0:episode+1])
            
        self.rewardAverages.append(self.lastAverage)
        if self.lastAverage>=self.rewardLimit:
            self.problemSolved=True
            
        if self.lastAverage>self.averageMax:
            self.averageMax=self.lastAverage
            self.averageMaxEpisode=episode
            
        if step < self.nbTimesteps-1:
            self.arrivedNb+=1
            if self.arrivedFirst==0:
                self.arrivedFirst=episode
                
    def EpiEval(self,episode,step):
        print('\rEpisode {} done: steps: {}, r-average: {:.1f}, arrived {}({:.1%}), maxReward: {} in {}, best average: {:.1f} in {}'.\
              format(episode, step+1, self.lastAverage, self.arrivedNb,self.arrivedNb/(episode+1), \
                     self.rewardMax, self.rewardMaxEpisode, self.averageMax,self.averageMaxEpisode),\
              end='')
        
    def CycleStart(self,i):
        print ('cycle {} starts:'.format(i))
        
    def CycleTrack(self):
        self.cycleFirstArrival.append(self.arrivedFirst)
        self.cycleArrivedNb.append(self.arrivedNb)
        self.cycleArrivePrct.append(self.arrivedNb/(self.lastEpisode+1))
        self.cycleRewardMax.append(self.rewardMax)
        self.cycleRewardMaxEpi.append(self.rewardMaxEpisode)
        self.cycleAverageMax.append(self.averageMax)
        self.cycleAverageMaxEpi.append(self.averageMaxEpisode)
        self.cycleRewAverages.append(self.rewardAverages)
        self.cycleProblemSolved.append(self.problemSolved)
        self.cycleRewards.append(self.rewards)
        
    def CycleEval(self,i,tileModel):
        print ('\n\ncycle {} done:'.format(i))
        print ('number episodes: {}'.format(self.lastEpisode+1))
        print ('first time arrived in episode: {}'.format(self.arrivedFirst))
        print ('car arrived: {} ({:.1%})'.format(self.arrivedNb, self.arrivedNb/(self.lastEpisode+1)))
        print ('best reward: {} in episode {}'.format(self.rewardMax,self.rewardMaxEpisode))
        print ('best average: {:.1f} in episode {}'.format(self.averageMax, self.averageMaxEpisode))
        print ('problem solved:{}'.format(self.problemSolved))
        print ('states: max:{:.2f}, mean: {:.2f}'.format(np.max(tileModel.states), np.mean(tileModel.states)))

        fig = plt.figure(figsize =(15,5))
        ax = fig.add_subplot(131)
        ax.set_autoscaley_on(False)
        ax.set_ylim([-210,-60])
        ax.set_xlim([0,self.nbEpisodes])
        ax.plot(range(len(self.rewardAverages)),self.rewardAverages,'-')
        ax.set_title(""development of rewardAverages"")
        
        ax2 = fig.add_subplot(132)
        ax2.set_autoscaley_on(False)
        ax2.set_ylim([-210,-60])
        ax2.set_xlim([0,self.nbEpisodes])
        ax2.plot(range(len(self.rewards)),self.rewards,'-')
        ax2.set_title(""development of rewards"")
        
        ax3 = fig.add_subplot(133)
        tileModel.plot(ax3)
        ax3.set_title(""tilemodel: states"")
        plt.show()
        
    def TestEval(self,nbCycles, policy, sarsaLambda,tileModel):
        print ('test finished after {} runs'.format(nbCycles))
        print ('input: policy: {}, lambda: {}'.format(policy,sarsaLambda))        
        print ('first arrival: min: {:.0f}, max: {:.0f}, mean: {:.0f}, std: {:.1f}'.\
               format(np.min(self.cycleFirstArrival),np.max(self.cycleFirstArrival),np.mean(self.cycleFirstArrival), np.std(self.cycleFirstArrival)))
        print ('nb of arrival: min: {:.0f}, max: {:.0f}, mean: {:.0f}, std: {:.1f}'.\
               format(np.min(self.cycleArrivedNb),np.max(self.cycleArrivedNb),np.mean(self.cycleArrivedNb), np.std(self.cycleArrivedNb)))
        print ('nb of arrival: min: {:.0%}, max: {:.0%}, mean: {:.0%}, std: {:.1%}'.\
               format(np.min(self.cycleArrivePrct),np.max(self.cycleArrivePrct),np.mean(self.cycleArrivePrct), np.std(self.cycleArrivePrct)))
        print ('best reward  : min: {:.0f}, max: {:.0f}, mean: {:.0f}, std: {:.1f}'.\
               format(np.min(self.cycleRewardMax),np.max(self.cycleRewardMax),np.mean(self.cycleRewardMax), np.std(self.cycleRewardMax)))
        print ('in episode   : min: {:.0f}, max: {:.0f}, mean: {:.0f}, std: {:.1f}'.\
               format(np.min(self.cycleRewardMaxEpi),np.max(self.cycleRewardMaxEpi),np.mean(self.cycleRewardMaxEpi), np.std(self.cycleRewardMaxEpi)))
        print ('best average : min: {:.0f}, max: {:.0f}, mean: {:.0f}, std: {:.1f}'.\
               format(np.min(self.cycleAverageMax),np.max(self.cycleAverageMax),np.mean(self.cycleAverageMax), np.std(self.cycleAverageMax)))
        print ('in episode   : min: {:.0f}, max: {:.0f}, mean: {:.0f}, std: {:.1f}'.\
               format(np.min(self.cycleAverageMaxEpi),np.max(self.cycleAverageMaxEpi),np.mean(self.cycleAverageMaxEpi), np.std(self.cycleAverageMaxEpi)))
        print ('prblem solved: sum: {:.0f}'.\
               format(np.sum(self.cycleProblemSolved)))
               
        #self.cycleRewAverages.append(self.rewardAverages)
        fig = plt.figure(figsize =(15,5))
        ax = fig.add_subplot(131)
        ax.set_autoscaley_on(False)
        ax.set_ylim([-210,-60])
        ax.set_xlim([0,self.nbEpisodes])
        for avgs in self.cycleRewAverages:
            ax.plot(range(len(avgs)),avgs,'-')
        ax.set_title(""development of rewardAverages"")
        
        ax2 = fig.add_subplot(132)
        ax2.set_autoscaley_on(False)
        ax2.set_ylim([-210,-60])
        ax2.set_xlim([0,self.nbEpisodes])
        for rwds in self.cycleRewards:
            ax2.plot(range(len(rwds)),rwds,'-')
        ax2.set_title(""development of rewards"")
        
        ax3 = fig.add_subplot(133)
        tileModel.plot(ax3)
        ax3.set_title(""tilemodel: states"")
        plt.show()",0.3383518159,
546,evaluating the models,"def score_model(model):
    test_score = model.evaluate(x_test, y_test, verbose=0)
    train_score = model.evaluate(x_train, y_train, verbose=0)
    model_score = {""test loss"" : test_score[0], 
                        ""test accuracy"" : test_score[1],
                        ""train accuracy"" : train_score[1]
                  }
    
    return model_score",0.3312993646,
546,evaluating the models,"def evaluate_score(model, x, y):
    model_score = model.evaluate(x, y, verbose=1)
    print('\nCategorical Cross-entropy Loss: {}'.format(model_score[0]))
    print('Test Set Accuracy: {}\n\n'.format(model_score[1]))",0.3305748999,
546,evaluating the models,"# Test the Model
def testing(net):
    net.eval() 
    correct = 0
    total = 0
    for images, labels in test_loader:
        images = images.cuda()
        labels = labels.cuda()
        images = Variable(images)
        outputs = net(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum()

    print('Test Accuracy of the network on the 10000 test images: %.2f %%' % (100.0 * correct / total))",0.3303354084,
546,evaluating the models,"## TODO: Create train and evaluate function using tf.estimator
def train_and_evaluate(output_dir, num_train_steps):
  #ADD CODE HERE
  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)",0.3290498257,
546,evaluating the models,"def e10s_status_mismatch(row):
    branch_status = True if row.e10sCohort == ""test"" else False
    e10sEnabled = json.loads(row.settings)[""e10sEnabled""]
    return (row.e10sCohort, branch_status != e10sEnabled)",0.3274821639,
1356,pandas plotting documentation,"# Plotting Histograms and printing Summary Statistics for all attributes
df.plot(kind='box', subplots=True, layout=(3,4), figsize=(15,15), grid=True)
plt.show()
df.describe()",0.5040098429,
1356,pandas plotting documentation,"loan['loan_amnt'].plot(kind='kde',title='Density plot of loan amount',figsize=(12, 6),xlim=0,ylim=0)
print(loan['loan_amnt'].describe())",0.500726819,
1356,pandas plotting documentation,"loan['funded_amnt'].plot(kind='kde',title='Density plot of funded amount',figsize=(12, 6),xlim=0,ylim=0)
print(loan['funded_amnt'].describe())",0.500726819,
1356,pandas plotting documentation,"project_df['Chg_from_50davg'].plot(kind=""density"", title=""50day Moving Average"", figsize=(4,4))
project_df['Chg_from_50davg'].describe()",0.4982980192,
1356,pandas plotting documentation,"yiwu_SUBJECT2.plot(use_index=True,y=Rea_type_total.index[2:5], kind='line',figsize=(12, 7))
plt.show()",0.4970650077,
1356,pandas plotting documentation,"print(snotel_df['2006':'2017'].describe())

#years = ['2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017']

#for j in years:
plt.figure(figsize=(15,5))
    
ax1 = snotel_df['SNWD.I-1 (in) ']['2006':'2017'].plot(use_index=True, linewidth=1)
ax1.set_ylabel(""Date"")
ax1.set_ylabel(""Snow Depth (inches)"")
ax1.set_title(""Timeseries of SNOTEL Snow Depth Data"")
  
#ax2 = snotel_df['TOBS.I-1 (degC) '].plot(use_index=True, secondary_y = True, marker='.', markersize = 0.25)
#ax2.set_ylabel(""Observed Temperature (deg C)"", rotation = -90)
    
#Create legend
#h1, l1 = ax1.get_legend_handles_labels()
#h2, l2 = ax2.get_legend_handles_labels()
#ax1.legend(h1+h2, l1+l2, loc=2)
      
#    plt.savefig('SNWD_TOBS_'+ '%04s' % (j) +'.png')

plt.show()",0.4970074892,
1356,pandas plotting documentation,"yiwu_SUBJECT2.plot(use_index=True,y=Rea_type_total.index[1:2], kind='line',figsize=(12, 7))
plt.show()",0.4961142242,
1356,pandas plotting documentation,"yiwu_SUBJECT2.plot(use_index=True,y=sub_rea_total.index[5:8], kind='area',figsize=(12, 7))
plt.show()",0.4955115318,
1356,pandas plotting documentation,"utils.df_plot(pn_df, xaxis='n', 
              columns=['n', 'CNB I', 'CNB II'],
              xlim=(0, 550), xlabel='$n$', ylabel=""$p_n$"", 
              style=['-', '--'], loc=1, 
              title='Probability Density of Boson Numbers')",0.4908729792,
1356,pandas plotting documentation,"wks_to_peak__vs__rank__ROCK.plot(kind='scatter',x='rank',y='wks_to_peak',fontsize=25,title='Average Ranking of Track vs Number of Weeks to Peak for Rock only',figsize=(12,5))
plt.show()",0.4905952215,
1561,preliminaries,"def TestTwoObservations(observation_sequence, sequence_probabilities, tolerance):
    # This flag will stay True if your test passes
    test_passes = True

    # Pull out the two observations in the sequence
    obs1 = observation_sequence.seq[0]
    obs2 = observation_sequence.seq[1]

    token_count = 0
    
    # Loop through possible symbols (letter keys) for the first observation
    for symbol_obs1 in symbols:
        # Find the key that generates the symbol
        key1 = get_key_for_symbol(symbol_obs1)

        # TODO compute the prior, likelihood and accumulated probability after the first observation
        obs1_prior = 0
        obs1_likelihood = 0
        acc_prob_obs1 = 0

        # Loop through possible symbols (letter keys) for the second observation
        for symbol_obs2 in symbols:
            # Find the key that generates the symbol
            key2 = get_key_for_symbol(symbol_obs2)
            
            # TODO compute the prior, likelihood and accumulated probability after the second observation
            obs2_prior = 0
            obs2_likelihood = 0
            acc_prob = 0

            # Compare accumulated probability to that reported from our recursive token passing method
            if not (abs(obs_seq2_prob[token_count] - acc_prob) < tol):           
                test_passes = False
            token_count += 1
    return test_passes",0.395449996,
1561,preliminaries,"def TestTwoObservations(observation_sequence, sequence_probabilities, tolerance):
    # This flag will stay True if your test passes
    test_passes = True

    # Pull out the two observations in the sequence
    obs1 = observation_sequence.seq[0]
    obs2 = observation_sequence.seq[1]

    token_count = 0
    
    # Loop through possible symbols (letter keys) for the first observation
    for symbol_obs1 in symbols:
        # Find the key that generates the symbol
        key1 = get_key_for_symbol(symbol_obs1)

        # TODO compute the prior, likelihood and accumulated probability after the first observation
        obs1_prior = get_prior(symbol_obs1, lm)
        obs1_likelihood = get_likelihood(obs1.x, obs1.y, key1)
        acc_prob_obs1 = obs1_prior * obs1_likelihood

        # Loop through possible symbols (letter keys) for the second observation
        for symbol_obs2 in symbols:
            # Find the key that generates the symbol
            key2 = get_key_for_symbol(symbol_obs2)
            
            # TODO compute the prior, likelihood and accumulated probability after the second observation
            obs2_prior = get_prior(symbol_obs2, lm)
            obs2_likelihood = get_likelihood(obs2.x, obs2.y, key2)
            acc_prob = acc_prob_obs1 * obs1_prior * obs2_likelihood

            # Compare accumulated probability to that reported from our recursive token passing method
            if not (abs(obs_seq2_prob[token_count] - acc_prob) < tol):           
                test_passes = False
            token_count += 1
    return test_passes",0.395449996,
1561,preliminaries,"def getSynonyms_usingPOS(word_tuple):
    #print(word_tuple)
    word_tagged = word_tuple[0]
    word_pos = get_wordnet_pos(word_tuple[1])
    syns = wn.synsets(word_tagged, pos=word_pos)
    
    set1 = set()
    for syn in syns:
        for lem in syn.lemmas():
            set1.add(lem.name())
    #print(syns)
    return set1",0.3881137669,
1561,preliminaries,"fidPairs = pygsti.alg.find_sufficient_fiducial_pairs(
            target_model, prep_fiducials, meas_fiducials, germs,
            searchMode=""random"", nRandom=100, seed=1234,
            verbosity=1, memLimit=int(2*(1024)**3), minimumPairs=2)

# fidPairs is a list of (prepIndex,measIndex) 2-tuples, where
# prepIndex indexes prep_fiducials and measIndex indexes meas_fiducials
print(""Global FPR says we only need to keep the %d pairs:\n %s\n""
      % (len(fidPairs),fidPairs))

gfprStructs = pc.make_lsgst_structs(
    opLabels, prep_fiducials, meas_fiducials, germs, maxLengths,
    fidPairs=fidPairs)

print(""Global FPR reduction"")
for L,strct in zip(maxLengths,gfprStructs):
    print(""L=%d: %d operation sequences"" % (L,len(strct.allstrs)))
    
gfprExperiments = pc.make_lsgst_experiment_list(
    opLabels, prep_fiducials, meas_fiducials, germs, maxLengths,
    fidPairs=fidPairs)
print(""\n%d experiments to run GST."" % len(gfprExperiments))",0.3871236444,
1561,preliminaries,"def embed_synset(synset, words, word_index, emb):
    word_set = [lemma.name() for lemma in synset.lemmas()]
    indices = filter(None, map(word_index.get, word_set))
    vecs = np.array([emb[i] for i in indices])
    if len(vecs) == 0:
        return None
    return np.mean(vecs, axis=0)

def embed_synsets(words, word_index, emb):
    return {synset: embed_synset(synset, words, word_index, emb) for synset in wn.all_synsets()}",0.3848135769,
1561,preliminaries,"def get_positive_feats(doc, i, a, feats=coref_features.minimal_features):
    return [k for k,v in feats(doc,i,a).items() if v > 0.0]",0.3844889998,
1561,preliminaries,"def find_palindromes(seq, n):
    palindromes = []
    
    for _________________:
        
        
        if _________________:
            palindromes.append(___________)
            
            
            
            
    return palindromes

DNA_seq = 'GGAGCTCCCAAAGCCATCAATATTCATCAAAACGAATTCAACGGAGCTCGATATCGCATCGCAAAAGACACC'
palindromic_sequences = find_palindromes(DNA_seq,6)
assert palindromic_sequences == ['GAGCTC', 'AATATT', 'GAATTC', 'GAGCTC', 'GATATC']",0.3742927313,
1561,preliminaries,"def add_bigrams(tweet):
    # Currently, tweet has an attribute called tweet.tokenList which is a list of tokens.
    # You want to add a new attribute to tweet called tweet.bigramList which is a list of bigrams.
    # Each bigram should be a pair of strings. You can define the bigram like this: bigram = (token1, token2).
    # In Python, this is called a tuple. You can read more about tuples here: https://www.programiz.com/python-programming/tuple

    ##### YOUR CODE STARTS HERE #####

    tweet.bigramList = [(tweet.tokenList[i], tweet.tokenList[i+1]) for i in range(len(tweet.tokenList)-1)]
    
    ##### YOUR CODE ENDS HERE #####


tweets, test_tweets = lib.read_data()
for tweet in tweets+test_tweets:
    add_bigrams(tweet)
print(""Checking if bigrams are correct..."")
for tweet in tweets+test_tweets:
    assert tweet._bigramList==tweet.bigramList, ""Error in your implementation of the bigram list!""
print(""Bigrams are correct.\n"")

prior_probs, token_probs = lib.learn_nb(tweets)
predictions = [(tweet, lib.classify_nb(tweet, prior_probs, token_probs)) for tweet in test_tweets]
lib.evaluate(predictions)",0.3741747439,
1561,preliminaries,"def plot_by_starting_amount(starting_amount_scores):
    for score in starting_amount_scores:
        if len(starting_amount_scores[score]) > 2:
            current_scores = starting_amount_scores[score]
            passed = len(current_scores[current_scores > 0.9])
            f = starting_amount_scores[score].plot(kind='hist', xlim=[0, 1])
            f.set_title(""{}: {} passed of {} experiments ({:.1%})"".format(
                    score, passed, len(current_scores), passed/len(current_scores)))
            f.set_xlabel('Spearman')
            f.set_ylabel('Number of Samples')
            f.axvline(0.9)
            pyplot.show(f)
        else:
            print(""Skipping {} only {} samples"".format(score, len(starting_amount_scores[score])))",0.3719167113,
1561,preliminaries,"def plot_by_replication(replication_scores):
    for score in replication_scores:
        if len(replication_scores[score]) > 2:
            current_scores = replication_scores[score]
            passed = len(current_scores[current_scores > 0.9])
            f = replication_scores[score].plot(kind='hist', xlim=[0, 1])
            f.set_title(""{}: {} passed of {} experiments ({:0.1%})"".format(
                    score, passed, len(current_scores), passed/len(current_scores)))
            f.set_xlabel('Spearman')
            f.set_ylabel('Number of Samples')
            f.axvline(0.9)
            pyplot.show(f)
        else:
            print(""Skipping {} only {} samples"".format(score, len(replication_scores[score])))",0.3719167113,
1240,multiplication of two polynomials,"plotMyData(liquid_flow_table.Primary,liquid_flow_table.myMultiplication)",0.4552157819,
1240,multiplication of two polynomials,simplify(M_sample * left)  == r_L * exp(I * phi_L) * left,0.4545163214,
1240,multiplication of two polynomials,simplify(M_sample * right) == r_R * exp(I * phi_R) * right,0.4545163214,
1240,multiplication of two polynomials,"a = nfa_to_dfa((RE.from_string('123456') * RE('') * RE.from_string('789')).m)
b = nfa_to_dfa(RE.from_string('123456789').m)",0.448243618,
1240,multiplication of two polynomials,"# Confirming order of G is n
from ecc import G
n = 0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEBAAEDCE6AF48A03BBFD25E8CD0364141
print(n*G)",0.4456198215,
1240,multiplication of two polynomials,"# Confirming order of G is n
from ecc import G

n = 0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEBAAEDCE6AF48A03BBFD25E8CD0364141
print(n*G)",0.4456198215,
1240,multiplication of two polynomials,"test_a = 65
test_b = 8921

assert judge(generator(test_a, factor_a),
             generator(test_b, factor_b), 5) == 1
assert judge(generator(test_a, factor_a),
             generator(test_b, factor_b)) == 588",0.4451524019,
1240,multiplication of two polynomials,"exp_cards = PExp(alphabet=['A','2','3','4','5','6','7','8','9','10','J','Q','K'])
exp_suits = PExp(alphabet=['','','',''])
exp_deck = exp_cards * exp_suits
exp_deck.info()",0.4430891275,
1240,multiplication of two polynomials,"exp_cards = PExp(alphabet=['A','2','3','4','5','6','7','8','9','J','Q','K'])
exp_suits = PExp(alphabet=['','','',''])
exp_deck = exp_cards * exp_suits
exp_deck.info()",0.441939652,
1240,multiplication of two polynomials,"height = 2
width = 128
channels = 1
n_features = height * width

feature_map1 = 128
ksize_conv1 = 2
stride_conv1 = 1

feature_map2 = 128
ksize_conv2 = ksize_conv1
stride_conv2 = stride_conv1

feature_map3 = 128
ksize_conv3 = ksize_conv1
stride_conv3 = stride_conv1

pool_layer_maps = 128

n_fully_conn1 = 128
n_fully_conn2 = 128
n_fully_conn3 = 128

n_classes = 8
  
X = tf.placeholder(tf.float32, shape=[None, height, width])
X_reshaped = tf.reshape(X, shape=[-1, height, width, channels])
labels = tf.placeholder(tf.int32, shape=[None])

weight_init = tf.contrib.layers.xavier_initializer()
activation_func = tf.nn.relu

# ------------------ Convolutional and pooling layers ----------------------------

def convolutional_layer(X, filter_, ksize, kernel_init, strides, padding):
    convolutional_layer = tf.layers.conv2d(X, filters = filter_, kernel_initializer = kernel_init,
                                           kernel_size = ksize, strides = strides,
                                          padding = padding, activation = activation_func)
    return convolutional_layer

def pool_layer(convlayer, ksize, strides, padding, pool_maps):
    pool = tf.nn.max_pool(convlayer, ksize, strides, padding)
    dim1, dim2 = int(pool.get_shape()[1]), int(pool.get_shape()[2])
    pool_flat = tf.reshape(pool, shape = [-1, pool_maps * dim1 * dim2])
    return pool_flat

conv_layer1 = convolutional_layer(X_reshaped, feature_map1, ksize_conv1, weight_init, stride_conv1, padding = ""SAME"")

conv_layer2 = convolutional_layer(conv_layer1, feature_map2, ksize_conv2, weight_init, stride_conv2, padding = ""SAME"")

conv_layer3 = convolutional_layer(conv_layer2, feature_map3, ksize_conv3, weight_init, stride_conv3, padding = ""SAME"")

pool_layer_flat = pool_layer(conv_layer3, [1,2,2,1], [1,2,2,1], ""VALID"", pool_layer_maps)

# ----------------- Fully connected layers -------------------

scale_val = 0.001
new_dense_layer = partial(tf.layers.dense, kernel_initializer = weight_init,
                          kernel_regularizer = tf.contrib.layers.l1_regularizer(scale_val), 
                          activation = activation_func)

dense_layer1 = new_dense_layer(pool_layer_flat, n_fully_conn1)

dense_layer2 = new_dense_layer(dense_layer1, n_fully_conn2)

dense_layer3 = new_dense_layer(dense_layer2, n_fully_conn3)

# ----------------- Output softmax layer ---------------------------

logits = tf.layers.dense(dense_layer3, n_classes)
softmax_activations = tf.nn.softmax(logits)

# ----------------- Specify performance measure -------------------------------

cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits, labels = labels)

loss = tf.reduce_mean(cross_entropy)
regularization_loss = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)
total_loss = tf.add_n([loss] + regularization_loss)

optimizer = tf.train.AdamOptimizer()
train_operation = optimizer.minimize(total_loss)

correct_predictions = tf.nn.in_top_k(logits, labels, 1)
accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))

# ---------------- Execution phase -------------------------------------------
    
n_epochs = 10
batch_size = 1024
n_train = X_train.shape[0]
n_iter = n_train//batch_size

acc_test = defaultdict(list)

path = ""./CNN_regtech_l1.1""  
saver = tf.train.Saver()

start = time()

with tf.Session() as sess:
    tf.global_variables_initializer().run()
    for epoch in range(n_epochs):
        for iteration in range(n_iter):
            rand_indices = np.random.choice(n_train,batch_size)    
            X_batch, y_batch = X_train[rand_indices], y_train[rand_indices]
            sess.run(train_operation, feed_dict={X: X_batch, labels: y_batch})
        acc_train = accuracy.eval(feed_dict={X: X_batch, labels: y_batch})
        print(""Epoch {} training accuracy : {}"".format(epoch, acc_train))
        save_path = saver.save(sess, path)
        saver.restore(sess, path)
    saver.restore(sess, path)
    for snr in snrs:
        acc_test[snr] = accuracy.eval(feed_dict={X: X_test[snr], labels: y_test[snr]})

print(""Training took %f minutes""%(float(time() - start)/60.0))",0.4396233559,
1628,problem ipython notebook as json,"import json, nimare
import pandas as pd
from nilearn.plotting import plot_glass_brain",0.5019752979,
1628,problem ipython notebook as json,"from IPython.lib.pretty import pprint

pprint(wards['features'][0], max_seq_length = 10)",0.4872441292,
1628,problem ipython notebook as json,"from pymongo import MongoClient
from bson.code import Code
import plotly, pymongo
plotly.offline.init_notebook_mode()
from plotly.graph_objs import Bar

client = MongoClient(CONNECTION_STRING)
db = client[DATABASE_NAME]
openfood = db[COLLECTION_NAME]",0.4831067324,
1628,problem ipython notebook as json,"# Show the application
# Make sure the URL matches your Jupyter instance
show(app, notebook_url=""localhost:8888"")",0.4829654694,
1628,problem ipython notebook as json,"from IPython.display import display
import pandas as pd

def Vega(spec):
    bundle = {}
    bundle['application/vnd.vega.v3+json'] = spec
    display(bundle, raw=True)
    
def VegaLite(spec):
    bundle = {}
    bundle['application/vnd.vegalite.v2+json'] = spec
    display(bundle, raw=True)",0.4827665389,
1628,problem ipython notebook as json,"%matplotlib inline

import pyes
import json
import pandas as pd
import seaborn as sns

def pp(o): 
    print json.dumps(o, indent=1)

def boxPlotFBFansPageComp(fids, host, pngFilePath, outlierFilter):
    data = []
    for fid in fids:
        conn = pyes.es.ES(host)
        tq = pyes.query.TermQuery(field=""fid"", value=fid)
        tagg = pyes.aggs.TermsAgg('pid', field= 'fid_pid')
        qsearch = pyes.query.Search(tq) 
        qsearch.agg.add(tagg) 
        result = conn.search(query=qsearch, indices='facebook_nested', doc_types='comment')
        #print json.dumps(result.aggs,indent=2) 
        for fidpid_summary in result.aggs[""pid""][""buckets""]:
            #print fidpid_summary[""key""] + ',' + str(fidpid_summary[""doc_count""])
            if fidpid_summary[""doc_count""] < outlierFilter or outlierFilter < 0:
                data.append([fidpid_summary[""doc_count""], fnames[fids.index(fid)], fidpid_summary[""key""]])

    df = pd.DataFrame(data,columns={'count', 'fname', 'fidpid'})
    sns.set(style=""ticks"")
    sns_plot = sns.boxplot(x=""fname"", y=""count"", data=df, palette=""PRGn"")
    fig = sns_plot.get_figure()
    fig.savefig(pngFilePath)",0.481140852,
1628,problem ipython notebook as json,"import folium
from folium import plugins
import pandas as pd
WeatherDataofCities = pd.read_json(""weathers.json"", lines =True)",0.4789110422,
1628,problem ipython notebook as json,"# Send to Javascript
import json
from IPython.display import Javascript
Javascript(""""""
window.edgeData1=%s;
window.nodeData1=%s;
"""""" % (json.dumps(edges), json.dumps(nodes)))",0.4783654809,
1628,problem ipython notebook as json,"from pybis.db import Db as db
from pybis.tess import Tess as tess
from IPython.display import display
from datetime import datetime
import pandas as pd
import requests
from bs4 import BeautifulSoup",0.4783560336,
1628,problem ipython notebook as json,"from IPython.display import Javascript
import json

def speak(msg):
    return Javascript(""""""
            var msg = new SpeechSynthesisUtterance(%s);
            window.speechSynthesis.cancel();
            window.speechSynthesis.speak(msg);
        """""" % json.dumps(msg))",0.478118062,
617,fetch_and_cache helper,"def simple_monte_carlo_integration(a1, a2, a3, a4, S, nsamples=10**6):
    sigma = 0.05
    masses = prior.fetch(true_z).rvs(nsamples)
    mu_lum = np.exp(a1) * ((masses / a3) ** a2) * ((1 + true_z) ** (a4))
    lums = lognorm(S, scale=mu_lum).rvs()
    return np.sum(p1(true_lum_obs, lums, sigma)) / (nsamples)",0.5491271019,
617,fetch_and_cache helper,"def train_graph(vocab_size, train_batch_size, training_file_pattern, embedding_size=1024, lstm_size=512):
    """"""Build the training graph.""""""
    
    train_instances_size = get_instances_size(training_file_pattern)
    g = tf.Graph()
    with g.as_default():
        input_queue = prefetch_input_data(training_file_pattern, batch_size=train_batch_size)
        serialized_sequence_example = input_queue.dequeue()
        total_loss, _, _, global_step = build_graph(
            serialized_sequence_example, vocab_size, train_batch_size, embedding_size, lstm_size, 'train')
        learning_rate = tf.constant(2.0)   # initial_learning_rate
        learning_rate_decay_factor = 0.5
        num_batches_per_epoch = (train_instances_size / train_batch_size)
        decay_steps = int(num_batches_per_epoch * 8)    # num_epochs_per_decay
    
        def _learning_rate_decay_fn(learning_rate, global_step):
            return tf.train.exponential_decay(
                learning_rate,
                global_step,
                decay_steps=decay_steps,
                decay_rate=learning_rate_decay_factor,
                staircase=True)
      
        train_op = tf.contrib.layers.optimize_loss(
            loss=total_loss,
            global_step=global_step,
            learning_rate=learning_rate,
            optimizer='SGD',
            clip_gradients=5.0,
            learning_rate_decay_fn=_learning_rate_decay_fn)   
        saver = tf.train.Saver(max_to_keep=5)
            
    return g, train_op, global_step, saver",0.5340346694,
617,fetch_and_cache helper,"cache = {}
def optval2(underlying, strike, t_exp, t=0, put=False):
    global cache
    if t > t_exp: return 0
    
    key = (t, underlying.price)
    if key in cache: return cache[key]
    
    ex_call = underlying.price - strike
    ex_put  = -ex_call
    exercise = ex_put if put else ex_call
    
    dt = underlying.dt
    C_up = optval2(underlying.up(),   strike, t_exp, t + dt, put)
    C_dn = optval2(underlying.down(), strike, t_exp, t + dt, put)
    
    p = underlying.p
    
    E_C_next = p * C_up + (1 - p) * C_dn
    E_C_next_d = E_C_next / exp(underlying.r * dt)
    
    cache[key] = max(E_C_next_d, exercise)
    return cache[key]",0.5317683816,
617,fetch_and_cache helper,"def fit(x, start_anti_fit, a_anti_fit, latency_fit, tau_fit, maxi_fit, step_fit, bruit,
        TargetOn, StimulusOf, start_sac=770, stop_sac=820):

    np.random.seed(7)
    

    param_fit={'tau':tau_fit, 'maxi':maxi_fit, 'a_anti':a_anti_fit, 'latency':latency_fit,
               'start_anti':start_anti_fit}
    
    result_fit={'start_anti_true':[], 'start_anti_fit':[], 'a_anti_true':[], 'a_anti_fit':[],
                'latency_true':[], 'latency_fit':[], 'bino':[], 'test':[],
                'tau_true':[], 'tau_fit':[], 'maxi_true':[], 'maxi_fit':[],
                'old_latency':[],'old_max':[]}
    
    
    for trial in range(25):
        bino = exp['p'][trial][0][0]
        start_anti_true = param['start_anti'][0][trial]+TargetOn
        a_anti_true = param['a_anti'][0][trial]
        latency_true = param['latency'][0][trial]+TargetOn
        tau_true = param['tau'][0][trial]
        maxi_true = param['maxi'][0][trial]

        test = ANEMO.fct_velocity(x, bino, start_anti_true, a_anti_true, latency_true,
                                     tau_true, maxi_true)+np.random.rand(len(x))*bruit
        test = test-np.random.rand(len(x))*bruit
        test[start_sac:stop_sac] = np.arange(0, (stop_sac-start_sac), 1)*np.nan

        result_deg = ANEMO.Fit_velocity(test, x, bino, param_fit=param_fit, sup=None, step=step_fit)

        result_fit['test'].append(test)
        result_fit['bino'].append(bino)
        
        result_fit['start_anti_true'].append(start_anti_true)
        result_fit['a_anti_true'].append(a_anti_true)
        result_fit['latency_true'].append(latency_true)
        result_fit['tau_true'].append(tau_true)
        result_fit['maxi_true'].append(maxi_true)
        
        result_fit['start_anti_fit'].append(result_deg.values['start_anti'])
        result_fit['a_anti_fit'].append(result_deg.values['a_anti'])
        result_fit['latency_fit'].append(result_deg.values['latency'])
        result_fit['tau_fit'].append(result_deg.values['tau'])
        result_fit['maxi_fit'].append(result_deg.values['maxi'])
        
        StimulusOn = 0
        a, b = StimulusOn, StimulusOf
        c, seuil = TargetOn+100, 3
        lat = np.where(np.abs(test[c:]) > np.abs(np.nanmean(test[a:b]))+np.abs(np.nanstd(test[a:b])*seuil))
        old_latency = lat[0][0]+c
        result_fit['old_latency'].append(old_latency)
        
        
        a1, b1 = TargetOn+400, TargetOn+600
        maxmax = np.where(np.abs(test) > np.abs(np.nanmean(test[a1:b1])))
        old_max = np.abs(test[maxmax[0][0]])
        result_fit['old_max'].append(old_max)
            
    return result_fit",0.531661272,
617,fetch_and_cache helper,"def model_evaluate(model,X,y,param_grid,cv=10,output=True):
    regressor = model()
    grid = GridSearchCV(estimator=regressor,param_grid=param_grid,verbose=4,cv=cv,n_jobs=-1)
    grid.fit(X,y)
    print(""Best Parameter Combo: "",grid.best_params_)
    print(""Best Mean Score on Training Set: "",grid.best_score_)
    if output:
          return grid.best_estimator_",0.5289148092,
617,fetch_and_cache helper,"from hashlib import sha1
from datasketch import MinHash

def mh_digest(s):
    m = MinHash(num_perm=512) # memory allocation
    for x in s:
        m.update(x.encode('utf8'))
    return m

jabber_digest = mh_digest(jabber_words)
parker_digest = mh_digest(parker_words)

inter = len(set(jabber_words) & set(parker_words))
union = len(set(jabber_words) | set(parker_words))

print(""estimated vs actual"", jabber_digest.jaccard(parker_digest),
                            inter / union)",0.5283372402,
617,fetch_and_cache helper,"def numerical_integration(a1, a2, a3, a4, S, nsamples=10**4):
    masses = midpoints(prior.fetch(true_z).mass)
    delta_masses = np.diff(prior.fetch(true_z).mass)
    lums = np.linspace(np.min(data.lum_obs), np.max(data.lum_obs), nsamples)
    delta_lum = 10.57916582
    sigma = 0.05
    integral = 0
    for lum in lums:
        integral += np.sum(delta_masses * delta_lum * p1(true_lum_obs, lum, sigma) * \
            p2(lum, masses, a1, a2, a3, a4, S, true_z) * p3(masses, true_z))
    return integral",0.5245947838,
617,fetch_and_cache helper,"# Create a new evaluation method where the data is passed in

def evaluate_model(model, X_tr, y_tr, X_te, y_te, optimizer, epochs=20, batch=256, cv_split=None, verbose=False):
    """"""
    Wrapper method to create, train and optionally CV, and check performance on test set
    """"""

    if verbose:
        print('\nCompiling model')
        model.summary()
        
    model.compile(optimizer=optimizer,
                   loss='categorical_crossentropy', 
                   metrics=['accuracy'])

    if verbose:
        print('\nTraining model')
    history = model.fit(X_tr, y_tr, validation_split=cv_split, 
                        epochs=epochs, batch_size=batch, verbose=1 if verbose else 0)

    if verbose:
        print('\nEvaluating model')
        
    train_score = model.evaluate(X_tr, y_tr, batch_size=batch)
    test_score = model.evaluate(X_te, y_te, batch_size=batch)

    if verbose:
        print('\nTest results: Loss = {:.2f}, Error = {:.2f}'.format(100.0 * test_score[0], 100.0 * (1.0 - test_score[1])))
    
    results = {'model': model, 'history': history.history, 
               'train_loss': train_score[0], 'train_acc': train_score[1], 'train_err': 1.0 - train_score[1],
               'test_loss': test_score[0], 'test_acc': test_score[1], 'test_err': 1.0 - test_score[1],
              }
    return results",0.5233255625,
617,fetch_and_cache helper,"def connect(user, password, db, host='localhost', port=5432):
    '''Returns a connection and a metadata object'''
    # We connect with the help of the PostgreSQL URL
    # postgresql://federer:grandestslam@localhost:5432/tennis
    url = 'postgresql://{}:{}@{}:{}/{}'
    url = url.format(user, password, host, port, db)

    # The return value of create_engine() is our connection object
    engine = sqlalchemy.create_engine(url, client_encoding='utf8')

    ## Commections
    connection = engine.connect()

    # We then bind the connection to MetaData()
    meta = sqlalchemy.MetaData(bind=engine, reflect=True)

    return engine, connection, meta",0.5232928395,
617,fetch_and_cache helper,"def make_profit(name, clf, ytest, xtest, util, color=None, ax=None, threshold=True, labe=600, proba=True):
    initial=False
    if not ax:
        ax=plt.gca()
        initial=True
    if proba:
        fpr, tpr, thresholds=roc_curve(ytest, clf.predict_proba(xtest)[:,1])
    else:
        fpr, tpr, thresholds=roc_curve(ytest, clf.decision_function(xtest))
    priorp=np.mean(ytest)
    priorn=1. - priorp
    ben=[]
    percs=[]
    for i,t in enumerate(thresholds):
        perc=percentage(tpr[i], fpr[i], priorp, priorn)
        ev = av_profit(tpr[i], fpr[i], util, priorp, priorn)
        ben.append(ev)
        percs.append(perc*100)
    ax.plot(percs, ben, '-', alpha=0.5, markersize=5, color=color, label='utlity curve for %s' % name)
    if threshold:
        label_kwargs = {}
        label_kwargs['bbox'] = dict(
        boxstyle='round,pad=0.3', alpha=0.5, color=color
        )
        for k in xrange(0, fpr.shape[0],labe):
            #from https://gist.github.com/podshumok/c1d1c9394335d86255b8
            threshold = str(np.round(thresholds[k], 2))
            ax.annotate(threshold, (percs[k], ben[k]), **label_kwargs)
    ax.legend(loc=""lower right"")
    return ax, ben",0.5216595531,
1389,part challenge,"def test_board():
     """"""
    >>> build_board(1, 1, 1)
    [['B']]
    >>> build_board(1,3, 3)
    [['B', 'B', 'B']]
    >>> build_board(3, 1, 0, non_bomb_character = ""X"")
    [['X'], ['X'], ['X']]
    >>> build_board(3, 6, 0, non_bomb_character = ""y"")
    [['y', 'y', 'y', 'y', 'y', 'y'], ['y', 'y', 'y', 'y', 'y', 'y'], ['y', 'y', 'y', 'y', 'y', 'y']]
    >>> build_board(3, 3, 9)
    [['B', 'B', 'B'], ['B', 'B', 'B'], ['B', 'B', 'B']]
    >>> build_board(3, 3, 300)
    [['B', 'B', 'B'], ['B', 'B', 'B'], ['B', 'B', 'B']]
    >>> build_board(4, 3, 0)
    [['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-']]
    """"""
    import doctest
    doctest.testmod()
    print(""TESTING COMPLETE... if you see nothing, (other than this message) that means all tests passed."")",0.4765325189,
1389,part challenge,"def real_estate_value(customer_record):
    """"""Compute a customer's overall real-estate value.""""""
    
    # We need to look at both assets and liabilities
    assets, liabilities = customer_record
    
    # assets = (banks, real_estates, retirements, stocks, forex)
    real_estate_assets = assets[1]
    
    print 'Real estate assets =', real_estate_assets
    
    # This is a list; compute total real-estate asset value
    # Python provides a function for this: sum(real_estate_assets),
    # but let's do it directly.
    asset_value = 0
    for value in real_estate_assets:
        asset_value += value
    
    print 'Asset value =', asset_value
    
    # liabilities = (mortgages, credit_cards)
    real_estate_liabilities = liabilities[0]
    
    print 'Real estate liabilities =', real_estate_liabilities
    
    # This is again a list
    liability_value = 0
    for value in real_estate_liabilities:
        liability_value += value
    
    print 'Liability value =', liability_value
    
    overall_value = asset_value - liability_value    
    print 'Overall value =', overall_value
    
    # Return everything
    return asset_value, liability_value, overall_value",0.4741333723,
1389,part challenge,"def print_doc(doc):
    if doc[""doc_id""] <= 5:  # print only the first 5 documents
        print(""docID:"", doc[""doc_id""])
        print(""date:"", doc[""date""])
        print(""title:"", doc[""title""])
        print(""body:"", doc[""body""])
        print(""--"")",0.4732090533,
1389,part challenge,"def test_fibonacci():
    assert fibonacci(0) == 0
    assert fibonacci(8) == 21
    assert fibonacci(12) == 144
    assert fibonacci(21) == 10946",0.4676588178,
1389,part challenge,"def p3_7():
    
    c =  [3,2]
    A = [[1,-1], 
         [3,1],
         [4,3]]
    b =  [2,  ,7]

    problem = Simplex(c,A,b)
    
    print(""c:\n"",c,""\nA:\n"",A,""\nb:\n"",b)
    print(""\n--> SOLUTION:"")
    print(problem.solve())

    dat = np.load(""productMix.npz"")

    a,p,m,d = dat[""A""],dat[""p""],dat[""m""],dat[""d""]
    A = np.row_stack([a, np.eye((4))])
    b = np.concatenate([m, d])

    print(""\n\n--> SOLUTION for productMix data:"")
    print(Simplex(p,A,b).solve())",0.4676421285,
1389,part challenge,"## Build the basic model..
def build_basic_gru_model(layers):
        model = Sequential()
        
        model.add(GRU(2048, input_shape=(layers[0]+1, layers[1]-1), return_sequences=True))
        model.add(GRU(1024, return_sequences=True))
        model.add(GRU(512, return_sequences=False))       
        
        model.add(Dense(1, activation='linear'))
        
        model.compile(loss='mse',optimizer='adam', metrics=['accuracy'])
        return model",0.4665707052,
1389,part challenge,"def on_epoch_end(epoch, logs):
    # Function invoked at end of each epoch. Prints generated text.
    print()
    print('----- Generating text after Epoch: %d' % epoch)

    start_index = random.randint(0, len(text) - maxlen - 1)
    for diversity in [0.2, 0.5, 1.0, 1.2]:
        print('----- diversity:', diversity)

        generated = ''
        sentence = text[start_index: start_index + maxlen]
        generated += sentence
        print('----- Generating with seed: ""' + sentence + '""')
        sys.stdout.write(generated)

        for i in range(400):
            x_pred = np.zeros((1, maxlen, len(chars)))
            for t, char in enumerate(sentence):
                x_pred[0, t, char_indices[char]] = 1.

            preds = model.predict(x_pred, verbose=0)[0]
            next_index = sample(preds, diversity)
            next_char = indices_char[next_index]

            generated += next_char
            sentence = sentence[1:] + next_char

            sys.stdout.write(next_char)
            sys.stdout.flush()
        print()",0.4653496742,
1389,part challenge,"def test_spiral_sum(f):
    assert(f(1) == 2)
    assert(f(2) == 4)
    assert(f(5) == 10)
    assert(f(330) == 351)
    assert(f(500) == 747)

def spiral_sum(n):
    # Store the value at each location (i,j)
    locs = {}
    locs[(0,0)] = 1

    # Helper for accessing the location dictionary
    def get_loc(x,y):
        return locs[(x,y)] if (x,y) in locs else 0

    # Helper for calculating what the value of a location should be
    def record_loc(x,y):
        value = (get_loc(x-1,y+1) + get_loc(x,y+1) + get_loc(x+1,y+1) + 
                 get_loc(x-1,y)   +                  get_loc(x+1,y) + 
                 get_loc(x-1,y-1) + get_loc(x,y-1) + get_loc(x+1,y-1))
        locs[(x,y)] = value
        return value

    for (x, y) in spiral_indices():
        if x == 0 and y == 0: # Skip the initial index
            continue
        value = record_loc(x, y)
        if value > n:
            return value

test_spiral_sum(spiral_sum)

%timeit spiral_sum(1000)
%timeit spiral_sum(10000)
%timeit spiral_sum(100000)",0.4646251798,
1389,part challenge,"def setupMinuit(params):
    print 'Setup Minuit...'
    PRINT_LEVEL = 1  # -1 => quiet, 1 => loud
    UP = 0.5 # 1: appropriate for 68% CL using chisq (use 0.5 for log-likelihood)
    npar = len(params)
    minuit = rt.TMinuit(npar)
    minuit.SetFCN(nnl)
    minuit.SetErrorDef(UP)
    minuit.SetPrintLevel(PRINT_LEVEL)

    status = rt.Long() # needed for integers passed by refence (int& ii)
    print ""%-20s %10s %10s %10s %10s"" % \
    ('param', 'guess', 'step', 'min', 'max')
    
    for ii, t in enumerate(params):
        print ""%-20s %10.2e %10.3e %10.3e %10.3e"" % t
        name, guess, step, pmin, pmax = t
        minuit.mnparm(ii, name, guess, step, pmin, pmax, status)
        if status != 0:
            sys.exit(""** mnparm(%s) status = %d"" % (name, status))
    return minuit",0.4643812776,
1389,part challenge,"# The function that displays
def display_freq_bases_v2 (counts):
    """"""
    displays the result of count_bases
    """"""
    # we extract the 5 values from count_bases
    nbTotal, nbA, nbC, nbG, nbT = counts
    # and we print them
    print(""Total sequence length"", nbTotal)
    print(""A = {:.2%}"".format(nbA / nbTotal))
    print(""C = {:.2%}"".format(nbC / nbTotal))
    print(""G = {:.2%}"".format(nbG / nbTotal))
    print(""T = {:.2%}"".format(nbT / nbTotal))
    # optionnally we could as well display
    # the proportions of CG and TA
    print(""CG = {:.2%}"".format((nbC + nbG) / nbTotal))
    print(""TA = {:.2%}"".format((nbT + nbA) / nbTotal))",0.4636314213,
676,flight delay prediction,"def sanity_check(fit, fitx, curvature, line):
    is_sane = False
    curvature_similar = abs(curvature[0] - curvature[1]) < 1500
    proper_lane_separation = (430 < (fitx[1][0] - fitx[0][0])) and ((fitx[1][0] - fitx[0][0]) < 850)
    if curvature_similar and proper_lane_separation:
        is_sane = True
    return is_sane",0.4592408538,
676,flight delay prediction,"def create_forecasts_vs_observed(dt_ranges,df_forecast_day,df_observed_day):
    ## Forecasts
    df_forecast_day_sunday = df_forecast_day[(df_forecast_day.updated>=dt_ranges['date_of_prediction'])&\
                                         (df_forecast_day.updated<=dt_ranges['date_of_prediction']+timedelta(days=1))]
    #Filter on the latest updated time for each date_hour:
    df_forecast_day_sunday_latest = df_forecast_day_sunday.groupby('date_hour').apply(lambda x:x.ix[x.updated.idxmax()])
    #df_forecast_day_sunday_latest=df_forecast_day_sunday_latest.reset_index()
    
    ## Observations
    df_observed_day_period = df_observed_day[(df_observed_day.date_hour>=dt_ranges['start_period'])&(df_observed_day.date_hour<dt_ranges['end_period'])]
    df_observed_day_period_latest = df_observed_day_period.groupby('date_hour').apply(lambda x:x.ix[x.updated.idxmax()])
    #df_observed_day_period_latest = df_observed_day_period_latest.reset_index()
    
    
    ## Merge forecasts and observations
    df_forecast_vs_observed = pd.merge(df_forecast_day_sunday_latest,df_observed_day_period_latest[['date_hour', u'tt', u'tn',
       u'tx', u'ne', u'ww', u'rrr', u'prrr']],on='date_hour',suffixes=['_forecasted','_observed'])
    df_forecast_vs_observed.drop('data_type',1)
    #df_forecast_vs_observed = df_forecast_vs_observed.set_index('date_hour')
    
    return df_forecast_vs_observed",0.4442514777,
676,flight delay prediction,"# Let us first test each of the above models against our test data (split from the training data)
def selectModel(models,X,y,testData):
    bestModel = ''
    lowestMSE = 1000 # a relatively large value
    testActual = testData.target
    testFeatures = testData[columnNames[1:]]
    #choose the same top 10% of best features
    testFeatures = selectFeat.transform(testFeatures) 
    for model in models:
        model.fit(X,y)
        testPredicted = model.predict(testFeatures)
        mse = mean_squared_error(testActual,testPredicted)
        if mse < lowestMSE:
            lowestMSE = mse
            bestModel = model
    print ""The best performing model with the lowest MSE of %f"" %(lowestMSE) + "" is %s"" % (str(bestModel))
    return bestModel",0.4441772699,
676,flight delay prediction,"def mean_queue_length(recs, max_sim_time, warm_up):
    arrival_time_stamps = [(r.arrival_date, r.queue_size_at_arrival + 1) for r in recs]
    exit_time_stamps = [(r.exit_date, r.queue_size_at_departure) for r in recs]
    time_stamps = arrival_time_stamps + exit_time_stamps + [(0, 0)]
    time_stamps.sort(key=lambda r: r[0])
    numerator = sum([time_stamps[i][1] * (time_stamps[i][0] - time_stamps[i - 1][0]) for i in range(1, len(time_stamps)) if time_stamps[i][0] > warm_up])
    mean_queue_length = numerator / (max_sim_time - warm_up)
    return mean_queue_length",0.4441772699,
676,flight delay prediction,"#class LearningAgent(Agent):
    
def update(self, t):
    #some other code
        
    #update state
    self.state = (inputs[""light""], self.next_waypoint)  
    #done in section 2
    
    if self.state not in self.q_table:
        #initialize q_table[self.state]
        self.q_table[self.state] = \
                    {None:0, ""left"":0, ""right"":0, ""forward"":0}
            
    #some other code",0.4424178004,
676,flight delay prediction,"#Clean dataset
def clean_df(df):
    return df[(df.fare_amount > 0) & 
            (df.pickup_longitude > -80) & (df.pickup_longitude < -70) &
            (df.pickup_latitude > 35) & (df.pickup_latitude < 45) &
            (df.dropoff_longitude > -80) & (df.dropoff_longitude < -70) &
            (df.dropoff_latitude > 35) & (df.dropoff_latitude < 45) &
            (df.passenger_count > 0) & (df.passenger_count < 10)]

train_df = clean_df(train_df)
print(len(train_df))",0.4411337674,
676,flight delay prediction,"def get_K(wteam_elo, lteam_elo, wteam_score, lteam_score):
    diff = wteam_score - lteam_score
    if(diff>25):
        return 30
    if(diff>15):
        return 25
    return 20

def new_elo(wteam_elo=1600, lteam_elo=1600, wteam_score=80, lteam_score=80):
    eloDiff = wteam_elo - lteam_elo
    odds = 1 / (10**(-eloDiff/400) + 1)
    K = get_K(wteam_elo, lteam_elo, wteam_score, lteam_score)
    diff = round(K * (1-odds))
    return (wteam_elo + diff, lteam_elo - diff)

def get_elo(season, teamnum, game_day=-1):
    if (season,teamnum) in statsdict and ('Elo' in statsdict[season,teamnum]):
        if game_day<0:
            return statsdict[season,teamnum]['Elo'][-1]
        else:
            gameNum = statsdict[season, teamnum]['GameDict'][game_day]
            return statsdict[season,teamnum]['Elo'][gameNum]
    elif ((season-1,teamnum) in statsdict) and ('Elo' in statsdict[season-1,teamnum]):
        return (1600*(1/4)) + (3/4)*statsdict[season-1,teamnum]['Elo'][-1]
    else:
        return 1600",0.4396515787,
676,flight delay prediction,"# Helper functions to pinpoint issues

def find_prev_cpu(trace, taskname, time):
    sdf = trace.data_frame.trace_event('sched_switch')
    sdf = sdf[sdf.prev_comm == taskname]
    sdf = sdf[sdf.index <= time]
    sdf = sdf.tail(1)
    
    wdf = trace.data_frame.trace_event('sched_wakeup')
    wdf = wdf[wdf.comm == taskname]
    wdf = wdf[wdf.index <= time]
    # We're looking for the previous wake event,
    # not the one related to the wake latency
    wdf = wdf.tail(2)
    
    stime = sdf.index[0]
    wtime = wdf.index[1]
    
    if stime > wtime:
        res = wdf[""target_cpu""].values[0]
    else:
        res = sdf[""__cpu""].values[0]
    
    return res
    
def find_next_cpu(trace, taskname, time):
    wdf = trace.data_frame.trace_event('sched_wakeup')
    wdf = wdf[wdf.comm == taskname]
    wdf = wdf[wdf.index <= time].tail(1)
    return wdf[""target_cpu""].values[0]

def trunc(value, precision):
    offset = pow(10, precision)
    res = int(value * offset)
    return float(res) / offset",0.438538909,
676,flight delay prediction,"def step_radial_src(solver,state,dt):
    """"""
    Geometric source terms for SW equations with cylindrical symmetry.
    Integrated using a 2-stage, 2nd-order Runge-Kutta method.
    This is a Clawpack-style source term routine, which approximates
    the integral of the source terms over a step.
    """"""
    dt2 = dt/2.

    q = state.q
    rad = state.grid.r.centers

    h = q[0,:]
    u = q[1,:]/h

    qstar = np.empty(q.shape)

    qstar[0,:] = q[0,:] - dt2/rad * h*u
    qstar[1,:] = q[1,:] - dt2/rad * h*u*u

    h = qstar[0,:]
    u = qstar[1,:]/h

    q[0,:] = q[0,:] - dt/rad * h*u
    q[1,:] = q[1,:] - dt/rad * h*u*u
    
def inner_state(state,dim,t,qbc,auxbc,num_ghost):
    h =  state.problem_data['lower_bc_data'][0]
    u =  state.problem_data['lower_bc_data'][1]
    qbc[0,:num_ghost] = h
    qbc[1,:num_ghost] = h*u
    
def outer_state(state,dim,t,qbc,auxbc,num_ghost):
    h =  state.problem_data['upper_bc_data'][0]
    u =  state.problem_data['upper_bc_data'][1]
    qbc[0,-num_ghost:] = h
    qbc[1,-num_ghost:] = h*u
    
def setup(r_jump=1.,r_inner=0.5,r_outer=3.,num_cells=501,g=1.):
    r = pyclaw.Dimension(r_inner, r_outer, num_cells=num_cells, name='r')
    h, u, upper_bc_data, lower_bc_data, _ = \
            initial_and_boundary_data(r_jump=r_jump,g=g, r_inner=r_inner,
                                      r_outer=r_outer, num_cells=num_cells)
    
    solver = pyclaw.ClawSolver1D(riemann_solver=riemann.shallow_roe_with_efix_1D)
    solver.bc_lower[0] = pyclaw.BC.custom
    solver.user_bc_lower = inner_state
    solver.bc_upper[0] = pyclaw.BC.custom
    solver.user_bc_upper = outer_state
    solver.step_source = step_radial_src
    domain = pyclaw.Domain([r])
    state = pyclaw.State(domain,solver.num_eqn)
    state.problem_data['grav'] = g
    state.problem_data['lower_bc_data'] = lower_bc_data
    state.problem_data['upper_bc_data'] = upper_bc_data

    state.q[0,:] = h
    state.q[1,:] = h*u

    claw = pyclaw.Controller()
    claw.solver = solver
    claw.solution = pyclaw.Solution(state,domain)
    claw.tfinal = 15.0
    claw.keep_copy = True
    claw.num_output_times = 50
    return claw",0.4385121465,
676,flight delay prediction,"def get_jsqNO(data, w2):
    df_jsqNO = pd.DataFrame()

    df_jsqNO['small_arrival'] = data[data.dispatcher == 'jsqNO'].small_arrival
    df_jsqNO['avg_resp_time'] = data[data.dispatcher == 'jsqNO'].avg_resp_time
    df_jsqNO['avg_power'] = data[data.dispatcher == 'jsqNO'].avg_power
    df_jsqNO['wgtd_sum_cost'] = data[data.dispatcher == 'jsqNO'].avg_num_jobs + w2 * 1000 * data[data.dispatcher == 'jsqNO'].avg_power
    
    df_jsqNO = df_jsqNO.groupby('small_arrival',as_index=False).agg({'avg_resp_time':np.mean, 'avg_power':np.mean, 'wgtd_sum_cost':[np.mean]})

    return df_jsqNO",0.4375809431,
1237,multiple independent variables,"# Hyperparameter grid
param_grid = {
    'boosting_type': ['gbdt', 'goss', 'dart'],
    'num_leaves': list(range(20, 150)),
    'learning_rate': list(np.logspace(np.log10(0.005), np.log10(0.5), base = 10, num = 1000)),
    'subsample_for_bin': list(range(20000, 300000, 20000)),
    'min_child_samples': list(range(20, 500, 5)),
    'reg_alpha': list(np.linspace(0, 1)),
    'reg_lambda': list(np.linspace(0, 1)),
    'colsample_bytree': list(np.linspace(0.6, 1, 10)),
    'subsample': list(np.linspace(0.5, 1, 100)),
    'is_unbalance': [True, False]
}",0.4505681694,
1237,multiple independent variables,"param_grid = {
    'boosting_type': ['gbdt', 'goss', 'dart'],
    'num_leaves': list(range(20, 150)),
    'learning_rate': list(np.logspace(np.log10(0.005), np.log10(0.5), base = 10, num = 1000)),
    'subsample_for_bin': list(range(20000, 300000, 20000)),
    'min_child_samples': list(range(20, 500, 5)),
    'reg_alpha': list(np.linspace(0, 1)),
    'reg_lambda': list(np.linspace(0, 1)),
    'colsample_bytree': list(np.linspace(0.6, 1, 10)),
    'subsample': list(np.linspace(0.5, 1, 100)),
    'is_unbalance': [True, False]
}",0.4505681694,
1237,multiple independent variables,"class Jump():
    def __init__(self,left,right):
        #self.weights = random.random((left, right))
        self.weights = np.random.uniform(low=-1,high=1,size=(left,right))",0.4501281381,
1237,multiple independent variables,"it, w = gen_points_and_pla(10,1)",0.4495194554,
1237,multiple independent variables,"ndim = 2

# reduce the dimension of data to specific dimension using PCA
mnistDataND, eigenVectors, eigenValues = dimReducePCA(mnistData, ndim)",0.446315974,
1237,multiple independent variables,"p1, p2, p3 = Point(1, 1), Point(3, 4), Point(3, 4)",0.4450792372,
1237,multiple independent variables,"mean_u_pc, var_u_pc = cp.E(u_hat_pc, dist_a), cp.Var(u_hat_pc, dist_a),",0.4439463615,
1237,multiple independent variables,"nc = NetCDFDataset(file_path)

lat = nc.variables['lat'][:]
lon = nc.variables['lon'][:]
dataset_variable_data = nc.variables[dataset_variable][:][0,:,:]",0.4438742101,
1237,multiple independent variables,"%%R
# Compute variables
vars <- carb  (flag=menu_flag, var1=var1, var2=var2, S=salt, T=temp, Patm=1, P=press, Pt=Phos, Sit=Sil, 
                    k1k2='w14', kf='dg', ks=""d"", pHscale=""T"", b=""u74"", gas=""potential"", warn='n')

H = 10^(-1*vars$pH)         # H+ concentration (mol/kg)
vars <- data.frame(H, vars) # Add H+ as new column to vars data frame

# Absolute errors: propagated uncertainties
if (menu_flag == 15) {
    absEt <- errors (flag=menu_flag, var1=var1, var2=var2, S=salt, T=temp, 
                      Patm=1, P=press, Pt=Phos, Sit=Sil, 
                      evar1=dat$Var2, evar2=dat$Var1, eS=0, eT=0, ePt=0, eSit=0, epK=epKstd, #HUOM!!!                
                      k1k2='w14', kf='dg', ks=""d"", pHscale=""T"", b=""u74"", gas=""potential"", warn='no')
    
} else if (menu_flag == 8 || menu_flag == 21 || menu_flag == 25) { ##NB: SWAP dat$Var2 and dat$Var1 !!!!!!
    absEt <- errors (flag=menu_flag, var1=var1, var2=var2, S=salt, T=temp, 
                      Patm=1, P=press, Pt=Phos, Sit=Sil, 
                      evar1=dat$Var1, evar2=dat$Var2, eS=0, eT=0, ePt=0, eSit=0, epK=epKstd, #HUOM!!!
                      k1k2='w14', kf='dg', ks=""d"", pHscale=""T"", b=""u74"", gas=""potential"", warn='no')
}


# Keep only key columns in vars for consistency with columns in absEt
vars <- vars[,colnames(absEt)] 
vars

# Duplicate rows in *vars* until same as number of members of error vector ALK_e
numerrs <- length(dat$Var1)
vars <- vars[rep(row.names(vars), numerrs), ]

#Relative errors (in percent)
relEt <- 100* absEt / vars            #Total relative error (from constants and other input variables)
# relEt",0.4435016513,
1237,multiple independent variables,"t,y=integrate_interval(10, 2)",0.4426760972,
1270,normalize,"def normalizar(X):
    X[0] = (X[0,:] - X[0,:].mean())/X[0,:].std()
    X[1] = (X[1,:] - X[1,:].mean())/X[1,:].std()
    X[2] = (X[2,:] - X[2,:].mean())/X[2,:].std()
    return X
normalizar(X)
X",0.4282863736,
1270,normalize,"def normalizar2D(X):
    X[0] = (X[0,:] - X[0,:].mean())/X[0,:].std()
    X[1] = (X[1,:] - X[1,:].mean())/X[1,:].std()
    return X
normalizar2D(X2D)
X2D",0.4209316969,
1270,normalize,"#test = data[['O3PM', 'OFG_pct', 'DREB_Pct']]

def feature_normalization(x):
    for feature in x:
        x[feature] = (x[feature]-np.mean(x[feature]))/(np.max(x[feature]) - np.min(x[feature]))
    return x",0.418345958,
1270,normalize,"# helper function
def Init(key, arr):
    if ""weight"" in key:
        arr[:] = mx.random.uniform(-0.07, 0.07, arr.shape)
        # or
        # arr[:] = np.random.uniform(-0.07, 0.07, arr.shape)
    elif ""gamma"" in key:
        # for batch norm slope
        arr[:] = 1.0
    elif ""bias"" in key:
        arr[:] = 0
    elif ""beta"" in key:
        # for batch norm bias
        arr[:] = 0

# Init args
for key, arr in args.items():
    Init(key, arr)",0.4148691595,
1270,normalize,"def assignment(df, centroids):

    for i in centroids.keys():
        # sqrt((x1 - x2)^2 - (y1 - y2)^2)
        df['distance_from_{}'.format(i)] = (
            np.sqrt(
                (df['x'] - centroids[i][0]) ** 2
                + (df['y'] - centroids[i][1]) ** 2
            )
        )
    centroid_distance_cols = ['distance_from_{}'.format(i) for i in centroids.keys()]
    df['closest'] = df.loc[:, centroid_distance_cols].idxmin(axis=1)
    df['closest'] = df['closest'].map(lambda x: int(x.lstrip('distance_from_')))
    df['color'] = df['closest'].map(lambda x: colmap[x])
    return df


df = assignment(df, centroids)
print(df.head())

fig = plt.figure(figsize=(5, 5))
plt.scatter(df['x'], df['y'], color=df['color'], alpha=0.5, edgecolor='k')

for i in centroids.keys():
    plt.scatter(*centroids[i], color=colmap[i])
plt.xlim(0, 80)
plt.ylim(0, 80)
plt.show()",0.4146099687,
1270,normalize,"def zeroArray(testArray):
    testArray[np.isclose(testArray, np.zeros_like(testArray))] = 0.0
    return testArray",0.4107421339,
1270,normalize,"def checked(x):
    for b in range(len(x)):
        if x[b]<=0.:
            x[b] = sum(x)*0.001
    return x",0.4093796015,
1270,normalize,"def handle_skewness(df):
    df['Item_Outlet_Sales'] = np.sqrt(df_train['Item_Outlet_Sales'])
    return df",0.4064526558,
1270,normalize,"def normalize_data(X):
    """"""Normalize data such that it lies in range [0, 1] along every dimension.
    
    Parameters
    ----------
    X : np.array, shape [N, D]
        Data matrix, each row represents a sample.
        
    Returns
    -------
    X_norm : np.array, shape [N, D]
        Normalized data matrix. 
    """"""
    X[..., 0] = X[..., 0]/np.amax(X[..., 0]);
    X[..., 1] = X[..., 1]/np.amax(X[..., 1]);
    return X",0.4053645134,
1270,normalize,"def show_digit(x):
    # Make sure all entries of x are in the range [0,255]
    for i in range(784):
        x[i] = max(0.0, x[i])
        x[i] = min(255.0, x[i])
    # Now display
    plt.axis('off')
    plt.imshow(x.reshape((28,28)), cmap=plt.cm.gray)
    plt.show()
    return",0.405205369,
1574,principal components,"def lnprior(theta):
    a1, a2, a3, p1, p2, p3, T0 = theta
    if #apply conditions on priors here
        return 0.0
    else:
        return -np.inf",0.3841205835,
1574,principal components,"def lnprob1( # complete
    lp = lnprior1(theta)
    if np.isfinite(lp):
        return # complete
    return -np.inf",0.3766564131,
1574,principal components,"function formH(n, p = n)
  # Default is to return n*n matrix, otherwise n*p portion of H
  H = zeros(n, n)
  H[:, 1] = 1 / sqrt(n)
  for j = 2:n
    for i = 1:(j-1)
      H[i, j] = 1/sqrt(j * (j-1))
    end
    H[j, j] = -(j-1)/sqrt(j * (j-1))
  end
  return H[:, 1:p]
end

p = 1000
n = 10000
X = formH(n, p)
true = zeros(p)
true[1:5] = 1:5
srand(2015790003)
y = X * true + randn(n);",0.3755707145,
1574,principal components,"def ext_rep_poly(f):
    return [ f.coeffs(), [ list(x) for x in f.monoms() ] ]",0.3743260503,
1574,principal components,"def heat_balance_main_rule(model):
    return model.F*model.H_F + sum(model.L[s]*model.H_L_[s] + model.V[s]*model.H_V_[s] for s in model.inlet) \
            + model.Q_main - sum(model.L[s]*model.H_L + model.V[s]*model.H_V for s in model.outlet) == 0
model.heat_balance_main_con = pe.Constraint(rule=heat_balance_main_rule)",0.3728869259,
1574,principal components,"def heat_balance_main_rule(model):
    return model.F*model.H_F + sum(model.L[s]*model.H_L_[s] + model.V[s]*model.H_V_[s] for s in model.inlet) \
            + model.Q_main - sum(model.L[s]*model.H_L + model.V[s]*model.H_V for s in model.outlet) - model.W*model.energy_block.dH_L['H2O'] == 0
model.heat_balance_main_con = pe.Constraint(rule=heat_balance_main_rule)",0.3728869259,
1574,principal components,"def mat_to_np(mat):
    elements = mat.A
    return np.array([[elements[j * 4 + i] for i in range(4)] for j in range(4)])

def np_to_mat(np_mat):
    App.Matrix(*np_mat.flatten())

def vec_to_np(vec):
    return np.array(list(vec))",0.3725294471,
1574,principal components,"def signum(u):
    return Vec(u.D, {k: 1 if v >= 0 else -1 for k, v in u.f.items()})",0.3715616763,
1574,principal components,"# Fetch results
def results(ucp):
    N, T, p, u, v = ucp.N, ucp.T, ucp.p, ucp.u, ucp.v
    # Check if problem is feasible
    if ucp.M.getProblemStatus(SolutionType.Default) in [ProblemStatus.PrimalFeasible]:
        # For time-constrained optimization it may be wise to accept any feasible solution
        ucp.M.acceptedSolutionStatus(AccSolutionStatus.Feasible)
        
        # Some statistics:
        print('Solution status: {0}'.format(ucp.M.getPrimalSolutionStatus()))
        print('Relative optimiality gap: {:.2f}%'.format(100*ucp.M.getSolverDoubleInfo(""mioObjRelGap"")))
        print('Total solution time: {:.2f}s'.format(ucp.M.getSolverDoubleInfo(""optimizerTime"")))

        return p.level().reshape([N,T+1]), u.level().reshape([N,T+1])
    else:
        raise ValueError(""No solution"")

pVal, uVal = results(ucp)",0.3698861003,
1574,principal components,"def find_depth(v):
    if v == v.p:
        return (v.d, v)
    (pd, p) = find_depth(v.p)
    d = v.d + pd
    v.d = d - p.d
    v.p = p
    return (d, p)",0.3671836555,
2559,what happened?,"# Function to convert list of numbers into sequence
def list_to_str(lst):
    ss = """"
    for i in lst:
        ss += str(i)
    return ss",0.4681122899,
2559,what happened?,"def display_bits(bits):
    bit_string = ''
    for b in bits:
        bit_string += str(b)
    return bit_string",0.4681122899,
2559,what happened?,"def flattenStrings(ls):
    r = """"
    for el in ls:
        r += str(el)
    return r",0.4681122899,
2559,what happened?,"def conc(X):
    res = []
    for x in X:
        res.append(str(x))
    return(''.join(res))

df['concat'] = df[['int','string']].apply(conc,axis=1)",0.4670054913,
2559,what happened?,"def family_or_not(passenger):
    alone = passenger
    if alone>0:
        return 'Yes'
    else:
        return 'No'",0.4655121565,
2559,what happened?,"def pass_or_fail(row):
    print(row)
    if row != ""F"":
        return 'Pass'
    else:
        return 'Fail'",0.4613181949,
2559,what happened?,"### TODO: Write your algorithm.
### Feel free to use as many code cells as needed.

def similar_breed(path):

    if dog_detector(path):
        result = ""It's dog and breed is "" + readBreed(path)

    elif face_detector(path) :
        result = ""It's human and looks like "" + readBreed(path)

    else:
        return ""error: No human and dog in picture.""

    return result",0.4607996643,
2559,what happened?,"### TODO: Write your algorithm.
### Feel free to use as many code cells as needed.

def identify_breed(image_link):
    if dog_detector(i)==True:
        breed=str(return_breed(image_link))
        return (""Our system indicates that a dog is in this image! Most likely, this dog is a "" +breed)
    else:
        if face_detector(i)==True:
            breed=str(return_breed(image_link))
            return (""Our system tells us that this is a human! The human most closely resembles a ""+ breed)
        else:
            return (""Error!! We suspect that you might have included a picture that is neither a dog nor a human."")",0.4582971931,
2559,what happened?,"def df_to_list(dataframe_column):
    # change a dataframe column to a list wrapped in parentheses
    # (to input in SQL, Vertica queries)
    result = ""(-2)""
    if len(dataframe_column) > 0:
        result = "", "".join(map(str, list(set(dataframe_column))))
        result = ""("" + result + "")""
    return result",0.458150506,
2559,what happened?,"def convert_to_label(arr):
    label = [digit for digit in arr if digit != 10]
    return ''.join(str(digit) for digit in label)",0.4577507973,
1831,run naive bayes classifier,"# Train naive bayes by computing several conditional probabilities in training data
def train():
    # reminder: we will need pSpam and pNotSpam here ;) 


    # Initiailize our variables: the total number of comment and the number of spam comments 

    
    # Go over each comment in training data 
    for ...
        
       # check if comment is spam or not 
    
       # increment the values depending if comment is spam or not
        
       # update dictionary of spam and not spam comments
    
    
    # Compute prior probabilities, P(spam), P(ham)
    pSpam = 
    pNotSpam =",0.5651196837,
1831,run naive bayes classifier,"def train_model(model, batchsize = 100, num_epochs = 20):
    """"""
    Trains a Chainer model.
    
    Arguments:
    
    model -- The model to train
    
    Keyword arguments:
    
    batchsize -- The batchsize to use during training
    epoch -- The number of epochs to train
    """"""
    
    optimizer = optimizers.Adam()
    optimizer.setup(model)
    accuracy = 0

    for epoch in six.moves.range(1, num_epochs + 1):
        print('epoch', epoch)

        # training
        perm = np.random.permutation(N)
        sum_accuracy = 0
        sum_loss = 0
        
        for i in six.moves.range(0, N, batchsize):
            x = chainer.Variable(np.asarray(data_train[perm[i:i + batchsize]]))
            t = chainer.Variable(np.asarray(labels_train[perm[i:i + batchsize]]))

            #run the forward pass, compute the loss, do backprop and then update the weights
            optimizer.update(model, x, t)

            sum_loss += float(model.loss.data) * len(t.data)
            sum_accuracy += float(model.accuracy.data) * len(t.data)
        
        print('train mean loss={}, accuracy={}'.format(sum_loss / N, sum_accuracy / N))

        # evaluation
        sum_accuracy = 0
        sum_loss = 0
        for i in six.moves.range(0, N_test, batchsize):
            x = chainer.Variable(np.asarray(data_test[i:i + batchsize]),
                                 volatile='on')
            t = chainer.Variable(np.asarray(labels_test[i:i + batchsize]),
                                 volatile='on')
            loss = model(x, t)
            sum_loss += float(loss.data) * len(t.data)
            sum_accuracy += float(model.accuracy.data) * len(t.data)

        print('test  mean loss={}, accuracy={}'.format(sum_loss / N_test, sum_accuracy / N_test))
        accuracy = sum_accuracy / N_test
    return accuracy",0.5649032593,
1831,run naive bayes classifier,"def iterative_RFC(X,Y):
    start_time = timeit.default_timer()
    rfc = ensemble.RandomForestClassifier()
    rfecv = RFECV(estimator=rfc, step=1, cv=10, scoring='accuracy')
    rfecv.fit(X, Y)
    print(""Optimal Number of Features: {}."".format(rfecv.n_features_))
    print(""Model Performance: {:.2%}."".format(rfecv.grid_scores_.mean()))
    print(""--- Runtime: %s seconds. ---"" % (timeit.default_timer() - start_time))",0.5528239608,
1831,run naive bayes classifier,"def naive_bayes(test_set, train_set, func):     
    classifier = nltk.NaiveBayesClassifier.train(train_set) 
    accuracy = nltk.classify.accuracy(classifier, test_set)
    print ""Naive Bayes accuracy: {:0.2}"".format(accuracy)
    return classifier, accuracy

def decision_tree(test_set, train_set, func):    
    classifier = nltk.DecisionTreeClassifier.train(train_set)
    accuracy = nltk.classify.accuracy(classifier, test_set)
    print ""Decision Tree accuracy: {:0.2}"".format(accuracy)
    return classifier, accuracy",0.5516902208,
1831,run naive bayes classifier,"# Train the Model

def training(net):
    
    net, criterion, optimizer = reset_model()
    
    net.train()
    for epoch in range(num_epochs):
        total_loss = 0
        accuracy = []
        for i, (images, labels) in enumerate(train_loader):
            images = images.cuda()
            labels = labels.cuda()
            temp_labels = labels
            images = Variable(images)
            labels = Variable(labels)

            # Forward + Backward + Optimize
            optimizer.zero_grad()
            outputs = net(images)
            loss = criterion(outputs, labels)
            
            if use_reg == True :
        
                loss = loss + alpha*li_regularizer(net) + beta*lo_regularizer(net)
                
            loss.backward()
            optimizer.step()

            total_loss += loss.data[0]
            _, predicted = torch.max(outputs.data, 1)
            correct = (predicted == temp_labels).sum()
            accuracy.append(correct/float(batch_size))

        print('Epoch: %d, Loss: %.4f, Accuracy: %.4f' %(epoch+1,total_loss, (sum(accuracy)/float(len(accuracy)))))
    
    return net",0.551676631,
1831,run naive bayes classifier,"%%cython 
import numpy as np
cimport numpy as np

def naivebayes( exprs, trained_params ):

    cdef int i, j, ind_from, ind_to, ip
    cdef int n_var = exprs.n_vars
    
    cdef np.float_t lbf, ap, an
    
    cdef int[:] indptr = exprs.X.indptr
    cdef int[:] indices = exprs.X.indices
    cdef np.float_t[:] data = exprs.X.data

    cdef float[:] pos_mean = trained_params.pos_mean.values
    cdef float[:] pos_sd   = trained_params.pos_sd.values
    cdef float[:] neg_mean = trained_params.neg_mean.values
    cdef float[:] neg_sd   = trained_params.neg_sd.values
    
    ans_np = np.zeros( (exprs.n_obs,) )
    cdef np.float_t[:] ans = ans_np
    
    cdef float norm_term =  \
      np.log( trained_params.pos_sd.values ).sum() -  \
      np.log( trained_params.neg_sd.values ).sum()

    for i in range(exprs.n_obs):

        ind_from  = indptr[i]
        ind_to    = indptr[i+1]
        ip = ind_from
        lbf = 0
        
        for j in range( n_var ):
            
            ap = pos_mean[j]
            an = neg_mean[j]
            if ip < ind_to and indices[ip] == j:
                ap -= data[ip]
                an -= data[ip]
                ip += 1
            ap /= pos_sd[j]
            an /= neg_sd[j]
            lbf += ( ap * ap - an * an ) / (-2)
        
        ans[i] = lbf - norm_term   
        
        if i % 1000 == 0:
            print(i // 1000, end="" "")
 
    print()
    return ans_np",0.5513960123,
1831,run naive bayes classifier,"def naive_bayes(trainingData,testData):
    from pyspark.ml.classification import NaiveBayes
    print(""Naive Bayes Classifier"")
    nb = NaiveBayes(smoothing=1)
    model = nb.fit(trainingData)
    predictions = model.transform(testData)
    predictions.select(""value"",""Category"",""probability"",""label"",""prediction"") \
        .orderBy(""probability"", ascending=False) \
        .show(n = 30, truncate = 30)
    evaluator = MulticlassClassificationEvaluator(predictionCol=""prediction"")
    print(""Accuracy :- "" + str(100*evaluator.evaluate(predictions)) +"" %""+""\n"")",0.5507100224,
1831,run naive bayes classifier,"class gaussianBayes():
    def __init__(self, train_data, train_labels):
        train_labels = train_labels.astype(int)
        self.classes = np.unique(train_labels)
        self.nclasses = self.classes.size
        self.nfeatures = train_data.shape[1]
        self.means = np.zeros((self.nclasses, self.nfeatures))
        self.variances = np.zeros((self.nclasses, self.nfeatures))
        self.probs = np.zeros(self.nclasses)
        for i in xrange(self.nclasses):
            mask = train_data[train_labels==self.classes[i]]
            self.means[i,:] = np.mean(mask, axis=0)
            self.variances[i,:] = np.var(mask, axis=0)
            self.probs[i] = mask.shape[0]/float(train_labels.shape[0])

            
    def predict(self, test_data):
        log_probs = np.zeros((test_data.shape[0], self.nclasses))
        log_probs += np.log(self.probs)
        for c in xrange(self.nclasses):
            for f in xrange(self.nfeatures):
                mu = self.means[c, f]
                var = self.variances[c,f]
                x = test_data[:,f]
                #gaussian log density function
                log_probs[:,c] += - (x-mu)**2/(2*var) - .5 * np.log(2*np.pi*var)
        self.log_probs = log_probs
        return self.classes[np.argmax(log_probs, axis=1)]
    
    def validate(self, test_data, test_labels):
        return np.sum(self.predict(test_data)==test_labels)/float(test_labels.size)

def seed_features_and_labels(data):
    return data[:,:-1], data[:,-1]
train_features, train_labels = seed_features_and_labels(seed_data[40:,:])
test_features, test_labels = seed_features_and_labels(seed_data[:40,:])
gb = gaussianBayes(train_features, train_labels)
print ""Accuracy on test data: %f "" % gb.validate(test_features,test_labels)",0.5496348143,
1831,run naive bayes classifier,"def sample_model(m, s, l=50):
    t = TEXT.numericalize(s)
    m[0].bs=1
    m.eval()
    m.reset()
    res,*_ = m(t)
    print('...')

    for i in range(l):
        r = torch.multinomial(res[-1].exp(), 2)
        if r.data[0] == 0:
            r = r[1]
        else:
            r = r[0]
        word = TEXT.vocab.itos[to_np(r)[0]]
        res, *_ = m(r[0].unsqueeze(0))
        print(word, end='')
    m[0].bs=bs",0.5481472015,
1831,run naive bayes classifier,"class naiveBayes:
    def __init__(self, toggle, smoothing):
        self.toggle = toggle
        self.smoothing = smoothing
        
    def normalizePixels(self, data):
        for i in range(len(data)):
            data[i] = np.floor(data[i]/8)

    def createBinsAndCounts(self, X, y):
        self.counts = np.empty(10)
        self.bins = np.zeros([10, 32, 784])
        self.bins += self.smoothing
        for i in range(10):
            imgs = X[y == i]
            l = len(imgs)
            self.counts[i] = l
            for j in range(l):
                for p in range(784):
                    self.bins[i][imgs[j][p]][p] += 1
   
    def discreteModelTrain(self, X_train, y_train):
        self.createBinsAndCounts(X_train, y_train)
        ## Calculate likelihood.
        for i in range(10):
            for j in range(32):
                    self.bins[i][j] = (self.bins[i][j]) / (self.counts[i]) ## bins will become likelihood.
        ## Calculate prior.
        self.priors = np.zeros(10)
        for i in range(10):
            self.priors[i] = self.counts[i] / 60000

    def discreteModelPredict(self, X_test, y_test):
        self.posteriorList = np.zeros([10000, 10])
        error = 0
        for i in range(10000):
            for j in range(10):
                for k in range(784):
                      self.posteriorList[i][j] += np.log(self.bins[j][X_test[i][k]][k])
            predict = np.argmax(self.posteriorList[i])
            if predict != y_test[i]:
                error += 1
        self.errorRate = (error / 10000) * 100 # percentage format

    def continuousModelTrain(self, X_train, y_train):
        pixelValueSum = np.zeros([10, 784])
        self.pixelValueMean = np.empty([10, 784]) ## means
        self.pixelValueVar = np.empty([10, 784]) ## variances
        self.priors = np.zeros(10)
        ## This for loop generate every pixels mean
        for i in range(10):
            imgs = X_train[y_train == i]
            l = len(imgs)
            self.priors[i] = l / 60000
            for j in range(l):
                pixelValueSum[i] += imgs[j]
            self.pixelValueMean[i] = pixelValueSum[i] / l

        ## This for loop generate every pixels var
        pixelValueSum = np.zeros([10, 784])
        for i in range(10):
            imgs = X_train[y_train == i]
            l = len(imgs)
            for j in range(l):
                pixelValueSum[i] += np.power((imgs[j] - self.pixelValueMean[i]), 2)
            self.pixelValueVar[i] = pixelValueSum[i] / l

    def continuousModelPredict(self, X_test, y_test):
        constant = 1 / np.sqrt(2 * np.pi)
        self.posteriorList = np.zeros([10000, 10])
        error = 0
        for i in range(10000):
            for j in range(10):
                var = self.pixelValueVar[j] + self.smoothing
                power = -((X_test[i] - self.pixelValueMean[j]) ** 2) / (2 * var)
                prob = constant * (1 / np.sqrt(var)) * np.exp(power)
                self.posteriorList[i][j] = np.sum(np.log(prob))        
            predict = np.argmax(self.posteriorList[i])
            if predict != y_test[i]:
                error += 1
        self.errorRate = (error / 10000) * 100 # percentage format
    
    def fit(self, X_train, y_train):        
        if self.toggle == 0:
            self.normalizePixels(X_train)
            self.normalizePixels(X_test)
            self.discreteModelTrain(X_train, y_train)
        else:
            self.continuousModelTrain(X_train, y_train)
            
    def predict(self, X_test, y_test):
        if self.toggle == 0:
            self.discreteModelPredict(X_test, y_test)
            return self.posteriorList, self.errorRate
        else:
            self.continuousModelPredict(X_test, y_test)
            return self.posteriorList, self.errorRate",0.5463302135,
457,definition,"def nengo_obj():
    with nengo.Network():
        ens = nengo.Ensemble(n_neurons=10, dimensions=1)
        p = nengo.Probe(ens)
        p.target = ens
nengo.rc.set('exceptions', 'simplified', str(True))
print_exc(nengo_obj)",0.2867928743,
457,definition,"def nengo_obj():
    with nengo.Network():
        ens = nengo.Ensemble(n_neurons=10, dimensions=1)
        p = nengo.Probe(ens)
        p.target = ens
nengo.rc.set('exceptions', 'simplified', str(False))
print_exc(nengo_obj)",0.2867928743,
457,definition,"def get_params(p, ini_state):
    # Function to optimimize.
    def fun(x):
        # We minimize f to find max for F.
        return F_function(p=p, ini_state=ini_state, params=x)[0]
    
    # Starting point.
    params_0   = [0.25 * np.pi for i in range(2*p)]
    params_min = [0 for i in range(2*p)]
    params_max = [2*np.pi if i%2 == 0 else np.pi for i in range(2*p)]
    
    # The bounds required by L-BFGS-B.
    bounds = [(low, high) for low, high in zip(params_min, params_max)]

    # Use method L-BFGS-B because the problem is smooth and bounded.
    minimizer_kwargs = dict(method=""L-BFGS-B"", bounds=bounds)

    result = scipy.optimize.basinhopping(fun, params_0, minimizer_kwargs=minimizer_kwargs)

    return result.x",0.2814315557,
457,definition,"def tooltip(obj):
    try:
        print(obj.__qualname__, ""\n   "", obj.__doc__.splitlines()[0])
    except AttributeError:
        print(obj.__qualname__, ""\n    No docstring available"")
        
tooltip(compound_interest_v4)
tooltip(compound_interest_v5)",0.2804498076,
457,definition,"def is_palindrom(str1):
    
    #YOUR CODE HERE
    
    #Do not use reverse function
    

def main():
    str1 = raw_input(""Enter string : "")
    res = is_palindrom(str1)
    print res
    
main()",0.2788046896,
457,definition,"## complete this function
def weights_init_normal(m):
    '''Takes in a module and initializes all linear layers with weight
       values taken from a normal distribution.'''
    
    classname = m.__class__.__name__
    # for every Linear layer in a model
    # m.weight.data shoud be taken from a normal distribution
    # m.bias.data should be 0
    if classname.find('Linear') != -1:
        # get the number of the inputs
        n = m.in_features
        y = 1.0/np.sqrt(n)
        m.weight.data.normal_(0, y)
        m.bias.data.fill_(0)",0.2760574818,
457,definition,"# Example of usage:
def dummy_function(image,params):
    if(""dummy_function"" in params[0]):
        print(params[1])
        print(params[2])
    return ""I did it""


file_name = ""../data/videos/video_human.avi""
dummy_params = [];
dummy_params.append(""dummy_function"");
dummy_params.append(""Hello from the video file:"")
dummy_params.append(file_name);


transformVideoFile(file_path=file_name,function=dummy_function,params=dummy_params)",0.2758677006,
457,definition,"def prepdata(...): 
    '''Function to prepare train, test and validation data given X and Y data set given
    '''
    ...
    
...",0.2736214399,
457,definition,"def ensemble():
    with nengo.Network():
        ens = nengo.Ensemble(n_neurons=10, dimensions=1)
        ens.neurons = None
print_exc(ensemble)",0.271643132,
457,definition,"def explore(vertex, count):
    '''Recursive DFS generator used to explore one vertex.'''
    vertex.visited = True
    # previsit
    count[0] = count[0] + 1
    yield (""previsit"", count[0], vertex)
    
    for edge in vertex.edges:
        child = edge[0]
        if not child.visited:
            # visit children
            yield from explore(child, count)
    
    
    # postvisit
    count[0] = count[0] + 1
    yield (""postvisit"", count[0], vertex)",0.2714362442,
2630,write a python program to convert a tuple to a dictionary,"t = () # empty tuple
t = 'hello', # a tuple with one item is constructed by following a value with a comma
print(t)

d = {(x, x+1):x for x in range(10)} # Create a dictionary with tuple keys
print(d)

t = (5, 6) # Create a tuple
print(type(t))
print(d[t])
print(d[(1,2)])",0.5322067738,
2630,write a python program to convert a tuple to a dictionary,"# Based on wikipedia dataset, construct a dictionary as following:
# Key: revision id
# Value: (country name, article name)

country_article = {}
for line in wiki_data:
    country_article[line[2]] = (line[1], line[0])",0.5302667618,
2630,write a python program to convert a tuple to a dictionary,"d = {(x, x + 1): x for x in range(10)}
print(type(d))
print(d)

t = (1, 2)
print(type(t))
print(d[t])
print(d[(2, 3)])",0.5298629999,
2630,write a python program to convert a tuple to a dictionary,"f = {1 : 2, '1': 2, ('a','non','mutable', 'tuple') : 2}
print(f)",0.5296503305,
2630,write a python program to convert a tuple to a dictionary,"import timeit

keys = ('name', 'age', 'food')
values = ('Monty', 42, 'spam')

dic = {k:v for k,v in zip(keys, values)}
print(dic)  
dict = {keys[i]: values[i] for i in range(len(keys))}
print(dict)

print(min(timeit.repeat(lambda: {k: v for k, v in zip(keys, values)})))

print(min(timeit.repeat(lambda: {keys[i]: values[i] for i in range(len(keys))})))",0.5284302235,
2630,write a python program to convert a tuple to a dictionary,"# create a dictionary from our list
d = {}  # reset dict
for name, temp, score in lst:
    d[name] = (temp, score)
d",0.5282683969,
2630,write a python program to convert a tuple to a dictionary,"# An aside on sorting tuples in python:

# E.g., storing CFG rules as a list of tuples
l = [ ('VP', ['V', 'NP']),
      ('Nom', ['Nom', 'Noun'])]
sorted(l)",0.5242769718,
2630,write a python program to convert a tuple to a dictionary,"dict((i, i) for i in (1, 2, 3, 3))",0.524220407,
2630,write a python program to convert a tuple to a dictionary,"#Tuples
eip = {(x, x + 1): x for x in range(10)}
eip_tupple=(4,5)
print(type(eip_tupple))
print(eip[eip_tupple])
print(eip[(1, 2)])",0.5234753489,
2630,write a python program to convert a tuple to a dictionary,"year_counts ={}
dicter(1, year_counts)
year_counts",0.5230144262,
879,implement the parse_training_data function,"DATA_PATH = '/data/imgs/train'


def process_image(img_file):
    """"""
    Resize image to 128x128, flatten color and normalize
    """"""
    # Load and resize test image
    img = Image.open(img_file).convert('L')
    img.thumbnail((128,128), Image.ANTIALIAS)

    # Create numpy array from image and normalize data
    data = np.array(img.getdata()).astype(np.float32)
    data_std = (data - min(data)) / (max(data) - min(data))

    return data_std

# Create one hot vector
def vectorize_label(label):
    one_hot = np.zeros(10).astype(np.float32)
    one_hot[label] = 1.0
    return one_hot


def process_folder((label, folder)):
    img_paths = os.path.join(DATA_PATH, folder, '*.jpg')
    img_files = glob.glob(img_paths)
    print('Processing folder: {} {} items'.format(folder, len(img_files)))
    
    imgs = []
    labels = []
    
    for j in range(len(img_files)):
        if j % 500 is 0 and j is not 0:
            ratio = float(j) / float(len(img_files))
            print('Processed {:.0%} of folder {}'.format(ratio, folder)) 
    

        processed_image = process_image(img_files[j])
        imgs.append(processed_image)
        labels.append(label)
        
    return imgs, labels


def process_testing(files):
    img_paths = os.path.join('/data/imgs/', 'test', '*.jpg')
    img_files = glob.glob(img_paths)
    print('Processing testing data... {} images'.format(amount))
    
    imgs = []
    
    for i in range(amount):
        if i % 500 is 0 and i is not 0:
            ratio = float(i) / float(amount)
            print('Processed {:.0%} of data'.format(ratio))
            
        processed_img = process_image(img_files[i])
        imgs.append(processed_img)
        
    return imgs",0.4194119573,
879,implement the parse_training_data function,"%%writefile -a byoa/train     

        #Read training data from CSV and load into a data frame
        data=pd.read_csv(data_filename, sep=',', names = [""Name"", ""Gender""])
        data = shuffle(data)
        print(""Training data loaded"")

        #number of names
        num_names = data.shape[0]

        # length of longest name
        max_name_length = (data['Name'].map(len).max())

        #Separate data and label
        names = data['Name'].values
        genders = data['Gender']

        #Determine Alphabets in the input
        names = data['Name'].values
        txt = """"
        for n in names:
            txt += n.lower()

        #Alphabet derived as an unordered set containing unique entries of all characters used in name
        chars = sorted(set(txt))
        alphabet_size = len(chars)

        #Assign index values to each symbols in Alphabet
        char_indices = dict((str(chr(c)), i) for i, c in enumerate(range(97,123)))
        alphabet_size = 123-97
        char_indices['max_name_length'] = max_name_length

        #One hot encoding to create training-X
        X = np.zeros((num_names, max_name_length, alphabet_size))
        for i,name in enumerate(names):
            name = name.lower()
            for t, char in enumerate(name):
                X[i, t,char_indices[char]] = 1

        #Encode training-Y with 'M' as 1 and 'F' as 0
        Y = np.ones((num_names,2))
        Y[data['Gender'] == 'F',0] = 0
        Y[data['Gender'] == 'M',1] = 0

        #Shape of one-hot encoded array is equal to length of longest input string by size of Alphabet
        data_dim = alphabet_size
        timesteps = max_name_length
        print(""Training data prepared"")

        #Consider this as a binary classification problem
        num_classes = 2

        #Initiate a sequential model
        model = Sequential()

        # Add an LSTM layer that returns a sequence of vectors of dimension sequence size (512 by default)
        model.add(LSTM(sequence_size, return_sequences=True, input_shape=(timesteps, data_dim)))

        # Drop out certain percentage (20% by default) to prevent over fitting
        if dropout_ratio > 0 and dropout_ratio < 1:
            model.add(Dropout(dropout_ratio))

        # Stack another LSTM layer that returns a single vector of dimension sequence size (512 by default)
        model.add(LSTM(sequence_size, return_sequences=False))

        # Drop out certain percentage (20% by default) to prevent over fitting
        if dropout_ratio > 0 and dropout_ratio < 1:
            model.add(Dropout(dropout_ratio))

        # Finally add an activation layer with a chosen activation function (Sigmoid by default)
        model.add(Dense(num_classes, activation=activation_function))

        # Compile the Stacked LSTM Model with a loss function (binary_crossentropy by default),
        #optimizer function (rmsprop) and a metric for measuring model effectiveness (accuracy by default)
        model.compile(loss=loss_function, optimizer=optimizer_function, metrics=[metrics_measure])
        print(""Model compiled"")

        # Train the model for a number of epochs (50 by default), with a batch size (1000 by default)
        # Split a portion of trainining data (20% by default) to be used a validation data
        model.fit(X, Y, validation_split=split_ratio, epochs=num_epochs, batch_size=batch_records)
        print(""Model trained"")

        # Save the model artifacts and character indices under /opt/ml/model
        model_type='lstm-gender-classifier'
        model.save(os.path.join(model_path,'{}-model.h5'.format(model_type)))
        char_indices['max_name_length'] = max_name_length
        np.save(os.path.join(model_path,'{}-indices.npy'.format(model_type)), char_indices)

        print('Training complete.')
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255)

if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)",0.4191833735,
879,implement the parse_training_data function,"def read_file():
    
    conversation = [] 
    
    with open(constants.FILE_TRAINING_DATA) as f:
        reader = csv.reader(f, delimiter='\t')
        _ = next(reader)
        for row in reader:
            try:
                scene_id = int(row[1].strip())
            except:
                continue 
                
            person = row[2].strip() 
            line = row[5].strip() 
            
            conversation.append({
                'scene_id': scene_id, 
                'actor': person, 
                'text': line})
            
    return conversation",0.4014497399,
879,implement the parse_training_data function,"def load_data(filename): 
    imported_data = read_data(filename);
    random.shuffle(imported_data);
    data = {};
    for i in range(len(imported_data)):
        data[i] = {};
        data[i][""label""] = imported_data[i][0];
        data[i][""cap_shape""] = imported_data[i][2];
        data[i][""cap_surface""] = imported_data[i][4];
        data[i][""cap_color""] = imported_data[i][6];
        data[i][""bruises""] = imported_data[i][8];
        data[i][""odor""] = imported_data[i][10];
        data[i][""gill_attachment""] = imported_data[i][12];
        data[i][""gill_spacing""] = imported_data[i][14];
        data[i][""gill_size""] = imported_data[i][16];
        data[i][""gill_color""] = imported_data[i][18];
        data[i][""stalk_shape""] = imported_data[i][20];
        data[i][""stalk_root""] = imported_data[i][22];
        data[i][""stalk_surface_above_ring""] = imported_data[i][24];
        data[i][""stalk_surface_below_ring""] = imported_data[i][26];
        data[i][""stalk_color_above_ring""] = imported_data[i][28];
        data[i][""stalk_color_below_ring""] = imported_data[i][30];
        data[i][""veil_type""] = imported_data[i][32];
        data[i][""veil_color""] = imported_data[i][34];
        data[i][""ring_number""] = imported_data[i][36];
        data[i][""ring_type""] = imported_data[i][38];
        data[i][""spore_print_color""] = imported_data[i][40];
        data[i][""population""] = imported_data[i][42];
        data[i][""habitat""] = imported_data[i][44];
    return data;

data = load_data(""agaricus-lepiota.data"");
labels = [""e"", ""p""];
default = labels[0];

print ""Data example:"", data.get(0);
print ""\nLabels:"", labels",0.4007749557,
879,implement the parse_training_data function,"def show_random_mnist_train_example(mnist):
    """"""Draws a random training image from MNIST dataset and displays it.
    
    Args:
        mnist: MNIST dataset.
    """"""
    random_idx = random.randint(0, mnist.train.num_examples)
    image = mnist.train.images[random_idx].reshape(28, 28)
    imgplot = plt.imshow(image, cmap='Greys')
    ### [TASK] Get a correct label for the image 
    label = None
    ###
    print('Correct label for image #{0}: {1}'.format(random_idx, label))

show_random_mnist_train_example(mnist)",0.3945767283,
879,implement the parse_training_data function,"def build_dataset():
    global data, num_epochs, words_per_epoch, words_to_train
    with open(datapickle, 'rb') as handle:
        data = pickle.load(handle)
    words_per_epoch = len(data)
    words_to_train = num_epochs * words_per_epoch",0.3924567699,
879,implement the parse_training_data function,"import os
############################################
# Get train image list, not numpy array    #
############################################
def get_train_img_list(train_img_path):
    train_img_path_list = []
    
    type_path_list = map(lambda type_name: ""/"".join([train_img_path, type_name]), os.listdir(train_img_path))
    print(""all type files below:"")
    for p_idx in xrange(len(type_path_list)):
        print """".join(
            map(str, [""type"", (p_idx+1), "" dir:"", type_path_list[p_idx]])
        )

    for p_idx in xrange(len(type_path_list)):
        type_img_name_list = map(lambda img_name:
                                 ""/"".join([type_path_list[p_idx], img_name]),
                                 os.listdir(type_path_list[p_idx]))
        print """".join(
            map(str, [""type"", (p_idx+1), "" file number:"", len(type_img_name_list)])
        )
        train_img_path_list.extend(type_img_name_list)
        
    # filter no-image files
    train_img_path_list = filter(lambda f: f[-4:] == "".jpg"", train_img_path_list)
    
    for p_idx in xrange(len(train_img_path_list[:10])): 
        print """".join(
            map(str, [""sample "", p_idx+1, "":"", train_img_path_list[p_idx]])
        )    
    
    return train_img_path_list",0.3919591308,
879,implement the parse_training_data function,"def image_to_input(model, img_path):
    target_size=model.input_shape[1:]
    img = keras_preprocessing_image.load_img(img_path, target_size=target_size)
    
    x = keras_preprocessing_image.img_to_array(img)
    x = np.expand_dims(x, axis=0)
    x = preprocess_input(x)

    return x",0.3904665709,
879,implement the parse_training_data function,"# Sepration function of image binary and label from tfrecord

def parse_record(serialized_example):
    features = tf.parse_single_example(
        serialized_example,
        features={
            'image': tf.FixedLenFeature([], tf.string),
            'label': tf.FixedLenFeature([], tf.int64),
        })
    
# array of the image from tfrecord
# chage into one-dimention --> three-dimention(RGB * vertical * horizontal) -> three dimention(vertical * horizontal * RGB)
    image = tf.decode_raw(features['image'], tf.uint8)
    image.set_shape([IMAGE_DEPTH * IMAGE_HEIGHT * IMAGE_WIDTH])
    print('Image.shape 1 : ' + str(image.shape))
    image = tf.reshape(image, [IMAGE_DEPTH, IMAGE_HEIGHT, IMAGE_WIDTH])
    print('Image.shape 2 : ' + str(image.shape))
    image = tf.cast(tf.transpose(image, [1,2,0]), tf.float32)

# read label and apply One Hod Encoding
    label = tf.cast(features['label'], tf.int32)
    label = tf.one_hot(label, NUM_CLASSES)
    print ('Label.shape : ' + str(label.shape))

    return image, label",0.3898822665,
879,implement the parse_training_data function,"def parse_record(serialized_example):
  features = tf.parse_single_example(
    serialized_example,
    features={
      'image': tf.FixedLenFeature([], tf.string),
      'label': tf.FixedLenFeature([], tf.int64),
    })
  
  image = tf.decode_raw(features['image'], tf.uint8)
  image.set_shape([IMAGE_DEPTH * IMAGE_HEIGHT * IMAGE_WIDTH])
  image = tf.reshape(image, [IMAGE_DEPTH, IMAGE_HEIGHT, IMAGE_WIDTH])
  image = tf.cast(tf.transpose(image, [1, 2, 0]), tf.float32)
  
  label = tf.cast(features['label'], tf.int32)
  label = tf.one_hot(label, NUM_CLASSES)

  return image, label",0.3887098432,
2140,step read in your data set,"def display_importances(grid_search):
    clf = grid_search.best_estimator_.named_steps.clf
    
    feature_names = text_df_train.columns
    importances_df = pd.DataFrame({'importance': clf.feature_importances_}, index=feature_names)
    display(importances_df[importances_df['importance'] > 0.0].sort_values(by='importance', ascending=False))",0.4715745747,
2140,step read in your data set,"class SpikeData:

    def __init__(self, file):

        self.time, self.voltage, self.lightstim, self.drugstim = SDfunctions.read_spike_train(file)",0.4647267461,
2140,step read in your data set,"class SpikeData:

    def __init__(self, file):

        self.time, self.voltage, self.lightstim, self.drugstim = SDfunctions.read_spike_train(file)


    def plotdata(self):

        # Set up figure and plot the spikes and stimuli with appropriate legends
        plt.clf()
        f, axarr = plt.subplots(2, sharex=True)
        axarr[1].plot(self.time, self.voltage)
        axarr[1].set_ylim([-10,10])
        plt.xlabel('Voltage (mV)')
        axarr[1].legend(['spike trace'])
        axarr[0].plot(self.time, self.lightstim, 'b', self.time, self.drugstim, 'r')
        axarr[0].legend([""light stimulus"", ""acetylcholine""])
        plt.xlabel('Voltage (mV)')
        axarr[0].set_ylim([1,4])
        plt.draw()
        plt.show()

        return",0.4573682249,
2140,step read in your data set,"def get_ratio_df(grid): 
    
    grid_clf = grid.best_estimator_.named_steps['multinomialnb'] 
    count_vect = grid.best_estimator_.named_steps['countvectorizer']
    names = count_vect.get_feature_names()
    
    country = (grid_clf.feature_count_[0] + 1)/grid_clf.class_count_[0]
    hip_hop = (grid_clf.feature_count_[1] + 1)/grid_clf.class_count_[1]
    pop = (grid_clf.feature_count_[2] + 1)/grid_clf.class_count_[2]
    rock = (grid_clf.feature_count_[3] + 1)/grid_clf.class_count_[3]
    
    country_ratio = country/(hip_hop+pop+rock)
    hip_hop_ratio = hip_hop/(country+pop+rock)
    pop_ratio = pop/(hip_hop+country+rock)
    rock_ratio = rock/(hip_hop+pop+country)
    
    ratio_df = pd.DataFrame({'Token':names, 'country':country_ratio, 'hip-hop':hip_hop_ratio,
                             'pop':pop_ratio, 'rock':rock_ratio})
    return ratio_df",0.4526619911,
2140,step read in your data set,"class DataGen:
    def __init__(self, sr=16000, batch_size=128):
        np.random.seed(1209)
        self.pitches = [440., 466.2, 493.8, 523.3, 554.4, 587.3,
                        622.3, 659.3, 698.5, 740., 784.0, 830.6]

        self.sr = sr
        self.n_class = len(self.pitches)  # 12 pitches
        self.secs = 1.
        self.batch_size = batch_size
        self.sins = []
        self.labels = np.eye(self.n_class)[range(0, self.n_class)]  # 1-hot-vectors

        for freq in self.pitches:
            cqt = librosa.cqt(sin_wave(self.secs, freq, self.sr, gain=0.5), sr=sr,
                              fmin=220, n_bins=36, filter_scale=2)[:, 1]  # use only one frame!
            cqt = librosa.amplitude_to_db(cqt, ref=np.min)
            cqt = cqt / np.max(cqt)
            self.sins.append(cqt)

        self.cqt_shape = cqt.shape  # (36, )

    def __next__(self):
        choice = np.random.choice(12, size=self.batch_size, # pick pitches for this batch
                                  replace=True)
        noise_gain = 0.1 * np.random.random_sample(1)  # a random noise gain 
        noise = whitenoise(noise_gain, self.cqt_shape)  # generate white noise
        xs = [noise + self.sins[i] for i in choice]  # compose a batch with additive noise
        ys = [self.labels[i] for i in choice] # corresponding labels

        return np.array(xs, dtype=np.float32), np.array(ys, dtype=np.float32)

    next = __next__",0.4503879547,
2140,step read in your data set,"def top_features(pipe, n):
    """"""Prints features with the highest coefficient values, per class""""""
    vectorizer =  pipe.named_steps['vectorize']
    clf =  pipe.named_steps['clf']
    print(clf.coef_.shape)
    feature_names = vectorizer.get_feature_names()
    for i, class_label in enumerate(clf.classes_):
        if i >= clf.coef_.shape[0]:
            break
        top = np.argsort(clf.coef_[i])
        reversed_top = top[::-1]
        print(""%s:\n%s"" % (class_label,
              ""\n"".join(feature_names[j] for j in reversed_top[:n])))",0.4490505457,
2140,step read in your data set,"def plot_lat_lon():
    global training_data
    data = training_data[(training_data.longitude < 0) & (training_data.latitude > 0)]
    plt.figure(figsize=(7,6))
    for klass in ['low', 'medium', 'high']:
        subdata = data[data.interest_level == klass]
        plt.scatter(subdata['longitude'], subdata['latitude'], alpha=0.4)
    plt.legend(['low', 'medium', 'high'])
    plt.show()
plot_lat_lon()",0.445181042,
2140,step read in your data set,"class Church_Outdoors():
    
    def __init__(self, direc):
        
        
        self._training_data = self._load(direc)
        
        
        np.random.seed(0)
        samples_n = self._training_data.shape[0]
        random_indices = np.random.choice(samples_n, 256, replace = False)
        np.random.seed()
        
        self._validation_data = self._training_data[random_indices]
        
        self._training_data = np.delete(self._training_data, random_indices, axis = 0)
        
    def _load(self, file_name):
        
        images = []
        for file in os.listdir(file_name):
            extension = os.path.splitext(file)[1]
            imagePath = os.path.join(file_name, file)
            images.append(cv2.imread(imagePath, cv2.IMREAD_UNCHANGED))
        return np.array(images)
    
    def get_training_batch(self, batch_size):
        return self._get_batch(self._training_data, batch_size)
    
    def get_validation_batch(self, batch_size):
        return self._get_batch(self._validation_data, batch_size)
    
    def _get_batch(self, data, batch_size):
        samples_n = data.shape[0]
        if batch_size <= 0:
            batch_size = samples_n
            
        random_indices = np.random.choice(data.shape[0], samples_n, replace = False)
        data = data[random_indices]
        
        for i in range(samples_n // batch_size):
            on = i * batch_size
            off = on + batch_size
            yield data[on:off]",0.4449164569,
2140,step read in your data set,"def most_informative_features(pipeline, n=15000):
    vectorizer = pipeline.named_steps[""vectorizer""]
    classifier = pipeline.named_steps[""classifier""]
    
    class_labels = classifier.classes_
    
    feature_names = vectorizer.get_feature_names()
    
    top_n_class_1 = sorted(zip(classifier.coef_[0], feature_names))[:n]
    top_n_class_2 = sorted(zip(classifier.coef_[0], feature_names))[-n:]
    
    return {class_labels[0]: pd.DataFrame({""Weight"": [tup[0] for tup in top_n_class_1], 
                                           ""Feature"": [tup[1] for tup in top_n_class_1]}), 
            class_labels[1]: pd.DataFrame({""Weight"": [tup[0] for tup in reversed(top_n_class_2)],
                                           ""Feature"": [tup[1] for tup in reversed(top_n_class_2)]})}",0.4449107647,
2140,step read in your data set,"class PreprocessData:
    """"""
    Reads data from a CSV file
    
    """"""
    def __init__(self, datafile):
        self.datafile = datafile
        self.data = pd.read_csv(datafile).fillna('')
        self.data.Rating = self.data.Rating.apply(lambda x: int(x))
        
    
    def binarise(self):
        """"""
        Converts star ratings into 1 (>3 stars) or 0
        
        """"""
        self.data.Rating = np.where(self.data.Rating >= 3, 1, 0)
        
        
    def three_way_split(self):
        """"""
        Converts star ratings into 2 (>3 stars), 1 (3 stars) or 0 (<3 stars)
        
        """"""
        def replace_rating(col):
            if col > 3:
                return 2
            if col == 3:
                return 1
            if col < 3:
                return 0
            
        self.data.Rating = self.data.Rating.apply(replace_rating)
        
    
    def unify_columns(self):
        """"""
        Merges Title and Review columns
        
        """"""
        self.data['Corpus'] = self.data.Title + ' ' + self.data.Review
        self.data.drop(['Title', 'Review'], axis=1, inplace=True)
        
        
    def remove_stopwords(self, stem=False):
        """"""
        Removes stop words and numbers from strings. Stems each word using NLTK's PorterStemmer

        Required modules: nltk, re

        """"""
        def rmstp(feature, stem=stem):
            tokens = word_tokenize(feature)
            stop_words = stopwords.words('english') + \
                       [u'ive', u'dont', u'wouldnt', u'im', u'couldnt', u'shouldnt',
                        u'cant', u'mightnt', u'mustnt', u'neednt', u'amazon', u'kindle']
            if stem:
                stemmer = PorterStemmer()
                stemmed_words = ' '.join([stemmer.stem(token) for token in tokens
                                          if token not in stop_words
                                          and not bool(re.search(r'\d', token))])
                return stemmed_words

            unstemmed_words = ' '.join([token for token in tokens
                                        if token not in stop_words
                                        and not bool(re.search(r'\d', token))])
            return unstemmed_words
        
        try:
            self.data.Title = self.data.Title.apply(rmstp)
            self.data.Review = self.data.Review.apply(rmstp)
        except AttributeError:
            self.data.Title = self.data.Corpus.apply(rmstp)
    
    
    def word_cloud(self, pos_neg_split=False):
        """"""
        Generates a word map
        
        """"""
        stopwords = STOPWORDS.copy()
        if pos_neg_split:
            stopwords.add(""book"")
            stopwords.add(""books"")
            stopwords.add(""read"")

            pos_rev = ' '.join(self.data.Corpus.where(self.data.Rating==1).dropna().tolist())
            neg_rev = ' '.join(self.data.Corpus.where(self.data.Rating==0).dropna().tolist())

            wordcloud_pos = WordCloud(max_font_size=50, relative_scaling=.5,
                                      stopwords=stopwords).generate(pos_rev)
            plt.figure(figsize=(15, 15), dpi=80)
            gs = gridspec.GridSpec(1, 2)
            plt.subplot(gs[0, 0])
            plt.imshow(wordcloud_pos)
            plt.axis(""off"")
            plt.title('Positive Reviews', fontsize=18)

            wordcloud_neg = WordCloud(max_font_size=50, relative_scaling=.5,
                                      stopwords=stopwords).generate(neg_rev)
            plt.subplot(gs[0, 1])
            plt.imshow(wordcloud_neg)
            plt.axis(""off"")
            plt.title('Negative Reviews', fontsize=18)
            
        else:
            rev = ' '.join(self.data.Corpus.dropna().tolist())
            wordcloud_all = WordCloud(max_font_size=50, relative_scaling=.5,
                                      stopwords=stopwords).generate(rev)
            plt.figure(figsize=(15, 15))
            plt.imshow(wordcloud_all)
            plt.axis(""off"")
            plt.title('All Reviews', fontsize=18)",0.443877697,
419,data types,"def detect_numerals(data):
    num = data.dtypes.index[(data.dtypes == 'int64')| (data.dtypes == 'int32')|(data.dtypes == 'float64')].tolist()
    if len(data.isnull().sum()[data.isnull().sum() > 0].index.tolist()) > 0:
        indexes = data.isnull().sum()[data.isnull().sum() > 0].index.tolist()
        data = remove_nan(data,indexes)
    non_num = list()
    for i in num:
        if np.unique(data[i]).shape[0] <= 10:
            non_num.append(i)
        
    return set(num) - set(non_num),data",0.5127308965,
419,data types,"def count_nulls(df):
    null_counts = []          #make an empty list to hold our results
    for col in df.dtypes:     #iterate through the column data types we saw above, e.g. ('C0', 'bigint')
        cname = col[0]        #splits out the column name, e.g. 'C0'    
        ctype = col[1]        #splits out the column type, e.g. 'bigint'
        if ctype != 'string': #skip processing string columns for efficiency (can't have nulls)
            nulls = df.where( df[cname].isNull() ).count()
            result = tuple([cname, nulls])  #new tuple, (column name, null count)
            null_counts.append(result)      #put the new tuple in our result list
    return null_counts

null_counts = count_nulls(df)",0.4921718836,
419,data types,"def get_type_lists(frame, rejects=['Id', 'SalePrice']):

    """"""Creates lists of numeric and categorical variables.
    
    :param frame: The frame from which to determine types.
    :param rejects: Variable names not to be included in returned lists.
    :return: Tuple of lists for numeric and categorical variables in the frame.
    
    """"""
    
    nums, cats = [], []
    for key, val in frame.types.items():
        if key not in rejects:
            if val == 'enum':
                cats.append(key)
            else: 
                nums.append(key)
                
    print('Numeric =', nums)                
    print()
    print('Categorical =', cats)
    
    return nums, cats",0.4840584993,
419,data types,"def encode(x):
    if x.dtype is np.dtype('O') and x.name != 'country':
        return x.astype('category').cat.codes
    return x

def normalize(df):
    return (df - df.mean()) / (df.max() - df.min())",0.4818784297,
419,data types,"def dummy_variables(df):
    df_type = df.dtypes
    for col in df_type.keys():
        if df_type[col] == 'object':
            df = pd.concat([df, pd.get_dummies(df[col]).rename(columns=lambda x: col + '_' + str(x))], axis=1)
            df.drop([col], axis=1, inplace=True)
    return df",0.4799771905,
419,data types,"def change_datatype_float(df):
    float_cols = list(df.select_dtypes(include=['float']).columns)
    for col in float_cols:
        df[col] = df[col].astype(np.float32)",0.4762678742,
419,data types,"def type_finder(data_dict):
    """""" data_dict=dictionary, used to determine datatypes of the data associated to label """"""
    for key in data_dict:                          # Go through each key in the dictionary
        for i, val in enumerate(data_dict[key]):   # Values of dict are lists, use to enumerate list
            if i == 0:                             # If the first item of a list
                print key, type(val)               # Print the label and the type of data for first item of list",0.4760475755,
419,data types,"# Function to automatically infer data types for every single feature in a raw data set
# Input: Pandas Dataframe created directly from the raw data with the pd.read_csv function
# Output: List of data types, one data type for each feature

def autoDetectDataTypes(raw_data):
    result = []
    
    for column in raw_data:
        if (raw_data.dtypes[column] == ""object""):
            #print (""Trying to automatically infer the data type of the"",column,""feature..."") #For debugging purposes
            inferredType = autoInferObject(raw_data[column])
            result.append(inferredType)
            #print (""Result:"",inferredType) #For debugging purposes
        elif (raw_data.dtypes[column] == ""int64""):
            if (len(pd.unique(raw_data[column])) == 2):
                result.append(""bool"")
            else:
                result.append(""int64"")
        else:
            # The only remaining data type is 'float64', which needs no special treatment
            result.append(""float64"")
        
    return result",0.4758448601,
419,data types,"# Function used to display the data distribution
def data_viz(df):
    # Plot histograms for numeric columns
    for idx, col in enumerate(df.select_dtypes(include=[np.number]).keys()):
        df[col].plot.hist(title = col)
        plt.show()

    # Plot bar plots  for categorical/boolean columns        
    for idx, col in enumerate(df.select_dtypes(include=[np.object, np.bool]).keys()):
        # Only draw bar plots for variables with less than 30 levels
        if len(df[col].unique()) < 30:
            df[col].value_counts().plot.bar(title = col)
            plt.show()",0.4730160832,
419,data types,"#--- Function to replace Nan values in columns of type float with -5 ---
def replace_Nan_non_object(df):
    object_cols = list(df.select_dtypes(include=['float']).columns)
    for col in object_cols:
        df[col]=df[col].fillna(np.int(-5))
       
replace_Nan_non_object(train_merged) 
replace_Nan_non_object(test_merged)",0.4726405144,
2017,step bag of words,"def main(config):
    # load vocabs
    vocab_words = load_vocab(config.words_filename)
    vocab_tags  = load_vocab(config.tags_filename)
    vocab_chars = load_vocab(config.chars_filename)

    # get processing functions
    processing_word = get_processing_word(vocab_words, vocab_chars,
                    lowercase=True, chars=config.chars)
    processing_tag  = get_processing_word(vocab_tags, 
                    lowercase=False)

    # get pre trained embeddings
    embeddings = get_trimmed_glove_vectors(config.trimmed_filename)

    # create dataset
    dev   = CoNLLDataset(config.dev_filename, processing_word,
                        processing_tag, config.max_iter)
    test  = CoNLLDataset(config.test_filename, processing_word,
                        processing_tag, config.max_iter)
    train = CoNLLDataset(config.train_filename, processing_word,
                        processing_tag, config.max_iter)

    # build model
    model = NERModel(config, embeddings, ntags=len(vocab_tags),
                                         nchars=len(vocab_chars))
    model.build()

    # train, evaluate
    model.train(train, dev, vocab_tags)
    model.evaluate(test, vocab_tags)",0.4919530749,
2017,step bag of words,"def add_bigrams(tweet):
    # Currently, tweet has an attribute called tweet.tokenList which is a list of tokens.
    # You want to add a new attribute to tweet called tweet.bigramList which is a list of bigrams.
    # Each bigram should be a pair of strings. You can define the bigram like this: bigram = (token1, token2).
    # In Python, this is called a tuple. You can read more about tuples here: https://www.programiz.com/python-programming/tuple

    ##### YOUR CODE STARTS HERE #####

    tweet.bigramList = [(tweet.tokenList[i], tweet.tokenList[i+1]) for i in range(len(tweet.tokenList)-1)]
    
    ##### YOUR CODE ENDS HERE #####


tweets, test_tweets = lib.read_data()
for tweet in tweets+test_tweets:
    add_bigrams(tweet)
print(""Checking if bigrams are correct..."")
for tweet in tweets+test_tweets:
    assert tweet._bigramList==tweet.bigramList, ""Error in your implementation of the bigram list!""
print(""Bigrams are correct.\n"")

prior_probs, token_probs = lib.learn_nb(tweets)
predictions = [(tweet, lib.classify_nb(tweet, prior_probs, token_probs)) for tweet in test_tweets]
lib.evaluate(predictions)",0.488953948,
2017,step bag of words,"%%add_to Network
def initialize_weights(self):
    # YOUR CODE HERE
    self.weights = np.random.randn(self.sizes[2], self.sizes[1])",0.4848073721,
2017,step bag of words,"def dirichlet(t, fields, pars):
    # fields.u[:] = np.sin(t * 2 * np.pi * 2) * gaussian(x.size, 10) - fields.u[:]
    fields.u[0] = 0
    fields.u[-1] = 0
    fields.v[0] = 0
    fields.v[-1] = 0
    return fields, pars",0.4846848845,
2017,step bag of words,"# class LearningAgent(Agent):
    
def update(self, t):
    #some other code
        
    #update self.t, self.epsilon, self.alpha
    self.t += 1
    self.alpha = 1.0/self.t
    self.epsilon -= 0.00046
        
    #some other code",0.4781422019,
2017,step bag of words,"def simulator(tic_tac):    
        
    with tf.Session() as sess:
        
        ### initialise the variables:
        sess.run(tic_tac.model.init_g)
        sess.run(tic_tac.model.init_l)
        
        rollouts, rewards = tic_tac.rollouts(sess) 
        
        print(rewards)
            
  
simulator(tic_tac)",0.4778568745,
2017,step bag of words,"def compute_tsne(embeddings_model):
    embeddings_model.init_sims(replace=True)
    X = embeddings_model[embeddings_model.wv.vocab]
    tsne = TSNE(n_components=2)
    return tsne.fit_transform(X)

tsne = compute_tsne(embeddings)",0.4767971039,
2017,step bag of words,"def fit(self, X, y):
    """"""Train the model

    Args:
        X: the training set containing the tips
        y: the list of passwords matching the tips
    """"""
    self.word_dicts_before = [{} for _ in range(0, self.before_cutoff + self.margin_cutoff)]
    self.word_dicts_after = [{} for _ in range(0, self.after_cutoff + self.margin_cutoff)]

    for i, tip in X.iteritems():
        try:
            tip_before, tip_after = self.get_tip_before_after_password(tip, y[i])
        except ValueError:
            continue

        _, words_before = self.get_words(tip_before, reverse=True, bol=True)
        _, words_after = self.get_words(tip_after, eol=True)
        self.word_dicts_before = self.add_words(
            words_before, self.word_dicts_before, self.before_cutoff + self.margin_cutoff)
        self.word_dicts_after = self.add_words(
            words_after, self.word_dicts_after, self.after_cutoff + self.margin_cutoff)

    self.word_dicts_before = self.normalize(self.word_dicts_before)
    self.word_dicts_after = self.normalize(self.word_dicts_after)
    return self

PasswordEstimator.fit = fit",0.4761642218,
2017,step bag of words,"def forward(self, input_array):
        self.input_array = input_array.reshape(self.input_1dim)
        self.output_array = np.dot(self.trans_matrix, self.input_array) + self.bias_array
        self.output_array = np.array([self.activator.forward(value) for value in self.output_array])",0.4757848084,
2017,step bag of words,"def set_map(self, M):
    """"""Method used to set map attribute """"""
    self._reset(self)
    self.start = None
    self.goal = None
    # TODO: Set map to new value. 
    self.M = M",0.4731949866,
2224,strings as function arguments,"import functools

def add3(x, y, z):
    return x + y + z

two_add2 = functools.partial(add3, 2)
print(two_add2(3, 4))

three_add2 = lambda x, y: add3(3, x, y)
print(three_add2(4, 5))",0.5081920624,
2224,strings as function arguments,"run_topic_page_rank = functools.partial(
    page_rank_driver, 'TopicPageRankSingleIterationJob.py',
    'CheckTopicPageRankConvergenceJob.py')",0.504607141,
2224,strings as function arguments,"run_page_rank = functools.partial(
    page_rank_driver, 'PageRankSingleIterationJob.py', 'CheckPageRankConvergenceJob.py')",0.504607141,
2224,strings as function arguments,import Functions as f,0.5026528239,
2224,strings as function arguments,"import functools

# first layer decorator arguments
def check(*argtypes):
    """"""Function argument type checker.
    """"""
    # second layer return function
    def _check(func):
        """"""Takes the function.
        """"""
        @functools.wraps(func)
        # third layer actual decorator
        def __check(*args):
            """"""Takes the arguments
            """"""
            if len(args) != len(argtypes):
                msg = 'Expected %d but got %d arguments' % (len(argtypes),len(args))
                raise TypeError(msg)
            for arg, argtype in zip(args, argtypes):
                if not isinstance(arg, argtype):
                    msg = 'Expected %s but got %s' % (argtypes, tuple(type(arg) for arg in args))
                    raise TypeError(msg)
            return func(*args)
        return __check
    return _check
@check(int, int)
def add(x, y):
    """"""Add two integers.""""""
    return x + y",0.4980146587,
2224,strings as function arguments,"import functools

def function_one(x, y, z):
    return x + y + z


function_two = functools.partial(function_one, 5)
print(function_two(6,7))
function_three = functools.partial(function_one, 5, 6)
print(function_three(8))
function_four = functools.partial(function_one, 5, 8, 9)
print(function_four())",0.4976136684,
2224,strings as function arguments,"def functionname(var1,var2,......varn):   #var1,var2,varn => parameters
    """"""doc str""""""
    #some bussiness logic
    [return value]",0.4967048764,
2224,strings as function arguments,"ltnw.function(""first"",2,fun_definition=lambda d:d[:,:2]);
ltnw.function(""second"",2,fun_definition=lambda d:d[:,2:]);",0.4957094193,
2224,strings as function arguments,"def function_name(input_1, input_2, ...):

    x = do_something_with_inputs

    return x",0.495655179,
2224,strings as function arguments,"ltnw.function(""f"",4,2,fun_definition=lambda x,y: x-y);",0.4951252639,
1349,pandas apply,"r = vPlusCv.applyfunc(lambda f: integrate(f,t))
r",0.5552197695,
1349,pandas apply,"v = a.applyfunc(lambda f: integrate(f,t))
v",0.5552197695,
1349,pandas apply,"for col in ['type', '2nd type', '3rd type']:
    df[col] = df[col].apply(clean_type)",0.5547139049,
1349,pandas apply,"for column in ['search_term','product_title','product_description']:
    df_all[column]=df_all[column].apply(string_stemmer)",0.5547139049,
1349,pandas apply,"square = lambda var: var**2
func = len

print left['A'].apply(func)
print left['A'].apply(len)

print (left['A'].apply(len)).apply(square)",0.554440856,
1349,pandas apply,"K = G_p.applyfunc(gain)
K",0.554367125,
1349,pandas apply,"# Problem 1: apply a lambda function that gives the square root of every element
def square_root(n):
    return n**0.5

df.applymap(lambda x: x**.5)
df.applymap(square_root)",0.5520519018,
1349,pandas apply,"# rescale data so that all values are positive and non-zero
rescaler = lambda x : x + 3

df[['PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']] \
= df[['PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']].apply(rescaler)",0.5506955385,
1349,pandas apply,"#rescale column for deferment of payment in months to abstract 'payment deferment scale' with categories 1 to 11
rescaler = lambda x : x + 3

df[['PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']] \
= df[['PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']].apply(rescaler)",0.5506955385,
1349,pandas apply,"def testiii(x):
    return x.apply(add_one)

testiii(s)",0.5506316423,
1884,sentiment analysis,"def vader_comparison(texts):
    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
    headers = ['pos','neg','neu','compound']
    print(""Name\t"",'  pos\t','neg\t','neu\t','compound')
    analyzer = SentimentIntensityAnalyzer()
    for i in range(len(texts)):
        name = texts[i][0]
        sentences = sent_tokenize(texts[i][1])
        pos=compound=neu=neg=0
        for sentence in sentences:
            vs = analyzer.polarity_scores(sentence)
            pos+=vs['pos']/(len(sentences))
            compound+=vs['compound']/(len(sentences))
            neu+=vs['neu']/(len(sentences))
            neg+=vs['neg']/(len(sentences))
        print('%-10s'%name,'%1.2f\t'%pos,'%1.2f\t'%neg,'%1.2f\t'%neu,'%1.2f\t'%compound)",0.5125356913,
1884,sentiment analysis,"from nltk.sentiment.vader import SentimentIntensityAnalyzer
#function of getting the sentiment value
def get_sentiment(tbl,text):
    """"""
    description: analyze the sentiment of sentence
    input:sentence
    output:sentiment value   
    """"""
    compound = []
    neg = []
    neu = []
    pos = []
    for i in range(len(text)):
        analyzer = SentimentIntensityAnalyzer()
        answer = analyzer.polarity_scores(text[i])
        compound.append(answer['compound'])
        neg.append(answer['neg'])
        neu.append(answer['neu'])
        pos.append(answer['pos'])
    tbl['compound'] = compound
    tbl['negative'] = neg
    tbl['neutral'] = neu
    tbl['positive'] = pos
    return tbl",0.5112473369,
1884,sentiment analysis,"def P6():
    # Keep this random seed here to make comparison easier.
    np.random.seed(0)
    # Generate feature vector
    vectorizer = CountVectorizer(stop_words='english')
    vectorizer.fit(train_data)
    train_X = vectorizer.transform(train_data)
    test_X = vectorizer.transform(test_data)

    # fit logistic regression using L2 regulation as the baseline
    lgr_l2 = LogisticRegression(penalty='l2', C=1.0)
    lgr_l2.fit(train_X,train_labels)
    lgr_l2_pred = lgr_l2.predict(test_X)
    coef_l2 = lgr_l2.coef_  # shape (4,26577)
    # find how many features with at least one non-zero weight
    feature_l2_sum = abs(coef_l2).sum(axis=0)
    nonzero_weight_cnt = coef_l2[coef_l2 != 0].size
    print(""Unpruned model with L2 regulation has %d featues and the accuracy is %.3f.""
          % (feature_l2_sum[feature_l2_sum != 0].shape[0], metrics.f1_score(test_labels, lgr_l2_pred, average='weighted')))

    # pruning features using C parameter
    C_value = [0.1, 0.5, 1.0, 1.5, 2.0, 3.0, 4.0, 10.0]
    nonzero_weight_count = []
    accuracy = []
    vocabulary_size = []
    for i in C_value:
        # using model with L1 regulation to pruning features
        # and then fit model with L2 regulation
        lgr_l1 = LogisticRegression(penalty='l1', C=i)
        lgr_l1.fit(train_X,train_labels)
        lgr_l1_pred = lgr_l1.predict(test_X)
        coef_l1 = lgr_l1.coef_
        feature_l1_sum = abs(coef_l1).sum(axis=0)

        # build a new feature vector using smaller vocabulary
        big_vocabulary = np.array(vectorizer.get_feature_names())
        small_vocabulary = list(big_vocabulary[feature_l1_sum != 0])
        vectorizer2 = CountVectorizer(vocabulary=small_vocabulary, stop_words='english')
        vectorizer2.fit(train_data)
        train_X2 = vectorizer2.transform(train_data)
        test_X2 = vectorizer2.transform(test_data)

        # retain a model using L2 regulation and smaller vocabulary
        lgr_l2b = LogisticRegression(penalty='l2', C=i, tol=.01)
        lgr_l2b.fit(train_X2,train_labels)
        lgr_l2b_pred = lgr_l2b.predict(test_X2)
        coef_l2b = lgr_l2b.coef_
        feature_l2b_sum = abs(coef_l2b).sum(axis=0)
        accuracy.append(metrics.f1_score(test_labels, lgr_l2b_pred, average='weighted'))
        vocabulary_size.append(feature_l2b_sum[feature_l2b_sum != 0].shape)
        nonzero_weight_count.append(coef_l2b[coef_l2b != 0].size)
        print()
        print(""Non-zero weight count: "", coef_l2b[coef_l2b != 0].size)
        print(""Vocabulary Size: "", len(vectorizer2.vocabulary_ )) # the same as feature_l2b_sum[feature_l2b_sum != 0].shape
        print(""Accuracy with %.1f value of C is %.3f.""
              % (i, metrics.f1_score(test_labels, lgr_l2b_pred, average='weighted')))
    # print(accuracy)
    # print(size)
    size = [i[0] for i in vocabulary_size]
    plt.plot(size, accuracy)
    plt.title(""Accuracy vs. Vocabulary Size"")
    
P6()",0.5067468286,
1884,sentiment analysis,"def train_model(fileid):
    """"""
        training a gensim model, see also: https://radimrehurek.com/gensim/models/word2vec.html
    """"""
    return gensim.models.Word2Vec(gutenberg.sents(fileid), min_count=5, size=300, 
                                  workers=4, window=10, sg=1, negative=5, iter=5)",0.5060719252,
1884,sentiment analysis,"text_col = 'sentences'
def create_month_df(data_loc):
    return ss.create_sentiment_df(data_loc, sid.polarity_scores, ner_set=NAMED_ENTITIES,
                                  non_players_set=NON_PLAYERS, UPPER_NAMES = UPPER_ENTITIES, TEXT_COL = text_col)[1]",0.5046619177,
1884,sentiment analysis,"text_col = 'sentences'
def create_month_df(data_loc):
    return ss.create_sentiment_df(data_loc, sid.polarity_scores, ner_set=NAMED_ENTITIES,
                                  non_players_set=non_players_set, UPPER_NAMES = UPPER_ENTITIES, TEXT_COL = text_col)[1]",0.5046619177,
1884,sentiment analysis,"def run_analysis(project_file, annotation_file, pimp_file):
    
    # load m2lda object and do thresholding
    ms2lda = Ms2Lda.resume_from(project_file)
    ms2lda.do_thresholding(th_doc_topic=0.05, th_topic_word=0.01)

    # read the annotation file
    print
    print ""Annotation file""
    motif_annotation = {}
    motif_idx = {}
    i = 0
    for item in csv.reader(open(annotation_file), skipinitialspace=True):
        key = int(item[0])
        val = item[1]
        print str(key) + ""\t"" + val
        motif_annotation[key] = val
        motif_idx[key] = i
        i += 1

    motifs_of_interest = motif_annotation.keys()    
    norm = mpl.colors.Normalize(vmin=min(motif_idx.values()), vmax=max(motif_idx.values()))
    cmap = cm.gist_rainbow
    motif_colour = cm.ScalarMappable(norm=norm, cmap=cmap)    
    
    # get the network graph out for the motifs of interest
    G = get_network_graph(ms2lda, motifs_of_interest)
    print ""\n"" + nx.info(G)  
    print

    # print out some info
    ms1_count = 0
    nodes = G.nodes(data=True)
    for node_id, node_data in nodes:
        # 1 for doc, 2 for motif
        if node_data['group'] == 1: 
            ms1_count += 1
    print ""%d (out of %d) MS1 peaks found in the graph"" % (ms1_count, ms2lda.ms1.shape[0])    
    
    # load the pimp differential analysis file for matching
    de_peaks = []
    with open(pimp_file, ""rb"") as infile:
       reader = csv.reader(infile)
       next(reader, None)  # skip the headers
       for row in reader:
        PiMP_ID = int(row[0])
        polarity = row[1]
        mz = float(row[2])
        rt = float (row[3])
        mh_intensity = float(row[4])
        tup = (PiMP_ID, polarity, mz, rt, mh_intensity)
        de_peaks.append(tup)

    print
    print ""PiMP list: ""
    for tup in de_peaks:
        print tup
        
    # do the matching
    mass_tol = 3
    rt_tol = 12

    std = np.array(de_peaks)
    std_mz = np.array([x[2] for x in de_peaks])
    std_rt = np.array([x[3] for x in de_peaks])
    matches = {}

    ms1_label = {}
    for row in ms2lda.ms1.itertuples(index=True):
        peakid = row[1]
        mz = row[5]
        rt = row[4]

        # the following line is hacky for pos mode data
        mass_delta = mz*mass_tol*1e-6
        mass_start = mz-mass_delta
        mass_end = mz+mass_delta
        rt_start = rt-rt_tol
        rt_end = rt+rt_tol

        match_mass = (std_mz>mass_start) & (std_mz<mass_end)
        match_rt = (std_rt>rt_start) & (std_rt<rt_end)
        match = match_mass & match_rt

        res = std[match]
        if len(res) == 1:
            closest = tuple(res[0])
            matches[closest] = row
            ms1_label[row[1]] = closest[1]        
        elif len(res)>1:
            closest = None
            min_dist = sys.maxint
            for match_res in res:
                match_mz = float(match_res[2])
                match_rt = float(match_res[3])
                dist = math.sqrt((match_rt-rt)**2 + (match_mz-mz)**2)
                if dist < min_dist:
                    min_dist = dist
                    closest = match_res
            closest = tuple(closest)
            matches[closest] = row
            ms1_label[row[1]] = closest[1]

    print ""Matches found %d/%d"" % (len(matches), len(std))
    print

    ms1_list = []
    for match in matches:
        key = str(match)
        ms1_row = matches[match]
        value = str(ms1_row)
        pid = ms1_row[1]
        print ""Standard %s"" % key
        print ""MS1 %s"" % value
        print
        ms1_list.append(pid)
        
    # print the motifs and count their occurences
    m2m_list = motifs_of_interest
    word_map, motif_words = ms2lda.print_motif_features(quiet=True)

    c = Counter() # count the motif occurences
    for i in range(len(ms1_list)):

        ms1 = ms1_list[i]
        df = print_report(ms2lda, G, ms1, motif_annotation, motif_words, motif_colour, motif_idx, word_map, xlim_upper=770)
        if df is not None:

            # display(df) # show the table to see the mz, annotations, etc

            # get the motif ids in the dataframe
            fragment_motif_ids = df[['fragment_motif']].values.flatten()
            loss_motif_ids = df[['loss_motif']].values.flatten()

            # get rid of nan values
            fragment_motif_ids = fragment_motif_ids[~np.isnan(fragment_motif_ids)]
            loss_motif_ids = loss_motif_ids[~np.isnan(loss_motif_ids)]

            # store the unique counts
            combined = np.append(fragment_motif_ids, loss_motif_ids).astype(int)
            combined = set(combined.tolist())
            c.update(combined)
            
    return c",0.5042073131,
1884,sentiment analysis,"def scores_dataset(model, model_name, parameters, train, test, dataset_name):
    model.fit(train.data(), train.sentiment_data(), params=parameters)
    predictions = model.predict(test.data())
    test_sentiment = test.sentiment_data()
    num_classes = len(set(test_sentiment))
    eval_results = evaluation.scores(test_sentiment, predictions, num_classes)
    return model_name, dataset_name, eval_results, predictions

datasets = {'Election Twitter' : (election_train, election_test), 
            'Dong Twitter' : (dong_train, dong_test)}
score_dataset_params = []
for dataset_name, dataset in datasets.items():
    train, test = dataset
    for model_name, model in models.items():
        parameters = {**model_parameters[model_name]}
        if dataset_name == 'Dong Twitter':
            parameters['C'] = model_c_value_dong[model_name][0]
        else:
            parameters['C'] = model_c_value_elections[model_name][0]
        
        parameters = model.get_params(**parameters)
        score_dataset_params.append((model, model_name, parameters, 
                                     train, test, dataset_name))
with Pool(6) as pool:
    model_results = pool.starmap(scores_dataset, score_dataset_params)",0.5009846091,
1884,sentiment analysis,"def P2():
### STUDENT START ###

    vectorizer = CountVectorizer()
    X = vectorizer.fit_transform(train_data)

    # a. 
    print(""A)\n\nThe size of the vocabulary is %d "" % X.shape[1])
    print(""The average number of non-zero features per example is %.2f"" % (X.nnz/X.shape[0]))
    print(""The fraction of the entries in the matrix that are non-zero %.4f"" 
          % (X.nnz/(X.shape[0]*X.shape[1])))

    # b.
    features = sorted(vectorizer.get_feature_names())
    print(""B)\n\nThe 0th feature string is "", features[0])
    print(""The last feature string is "", features[-1])

    # c.
    my_vocabs = [""atheism"", ""graphics"", ""space"", ""religion""]

    vectorizer2c = CountVectorizer()
    vectorizer2c.vocabulary = my_vocabs
    X2c = vectorizer2c.fit_transform(train_data)


    print(""\n\nC) With my own vocabulary set: \n"")
    print(""The size of the vocabulary is %d "" % X2c.shape[1])
    print(""The size of the training data is %d "" % X2c.shape[0])
    print(""The average number of non-zero features per example is %.2f"" % (X2c.nnz/X2c.shape[0]))

    # d.

    print(""\n\nD) Bigram and Trigram characters extraction: \n"")
    vectorizer2d = CountVectorizer(ngram_range=(2,3), analyzer=""char"")
    X2d = vectorizer2d.fit_transform(train_data)
    print(""The size of the vocabulary for bigram and trigram characters is %d "" % X2d.shape[1])

    # e.
    vectorizer2e = CountVectorizer(min_df=10)
    X2e = vectorizer2e.fit_transform(train_data)
    print(""\n\nE) When min_df is 10, the size of the vocabulary is %d "" % X2e.shape[1])

    # f.

    vectorizer_dev = CountVectorizer(min_df=1)
    X_dev = vectorizer_dev.fit_transform(dev_data)
    print(""\nF) The fraction of the words in the dev data that are missing from the vocabulary is %.3f"" % (1-(X_dev.shape[1]/X.shape[1])))

### STUDENT END ###
P2()",0.5003239512,
1884,sentiment analysis,"def demo_vader_instance_mod(text):
    """"""
    Output polarity scores for a text using Vader approach.

    :param text: a text whose polarity has to be evaluated.
    """"""
    from nltk.sentiment import SentimentIntensityAnalyzer
    vader_analyzer = SentimentIntensityAnalyzer()

    map=vader_analyzer.polarity_scores(text)
    score=map.get('compound')
    return score",0.5000186563,
1230,mrjob,"def pass_primes(stop):
    global primer
    primer = get_primes(1)
    primer.send(None)
    for prime in primer:
        if prime <= stop:
            pass
        else:
            break",0.3935174942,
1230,mrjob,"def isUnique(words):
    if len(words) < 1:
        print 'The string is not valid!'
        return False
    
    charDic = {}
    words = words.lower()
    for w in words:
        if w != ' ':
            if w in charDic:
                print '\'' + words + '\' is not unique' 
                return False
            charDic[w] = 1
            
    print '\'' + words + '\' is unique'         
    return True


isUnique('apple')",0.3913008273,
1230,mrjob,"# Cleaning song duration column

# Function to return duration in seconds of songs, taking a string as input
# This function checks to make sure format of duration in ""m,s,ms"", since without more information,
# we can't assume the format of the data.
def get_duration(str_time):
    str_time = str_time.split(',')
    
    # Returns a message if the seconds portion is in decimals instead of seconds
    if float(str_time[1]) > 60:
        print ""seconds out of range""
    
    # Returns a message if the microseconds portion is not in microseconds
    if float(str_time[2].split()[0]) > 60:
        print ""micro seconds out of range""
             
    output = (float(str_time[0])* 60.) + (float(str_time[1])) + (float(str_time[2].split()[0]) / 60)
    return output",0.3829852343,
1230,mrjob,"def allreduce(kvs):
    
     os.environ[""PMI_PORT""] = kvs[""PMI_PORT""]
     os.environ[""PMI_ID""] = str(kvs[""PMI_ID""])
    
     from mpi4py import MPI
    
     comm = MPI.COMM_WORLD
     rank = comm.Get_rank()
   
     # image

     n = 2*1000000
     sendbuf = np.arange(n, dtype=np.float32)
     recvbuf = np.arange(n, dtype=np.float32)
        
     sendbuf[n-1] = 5.0;

     t1 = datetime.now()    
     comm.Allreduce(sendbuf, recvbuf, op=MPI.SUM)     
     t2 = datetime.now()
    
     out = {
        'rank' : rank,
        'time' : (t2-t1), 
        'sum'  : recvbuf[n-1]
     }
     return out",0.3825932145,
1230,mrjob,"def stringCompress(words):
    if len(words) < 1:
        print 'Invalid input'
        return False
    
    if len(words) == 1:
        return words + '1'
    
    words = words.lower()
    preChar = words[0]    
    resultChar = [preChar]
    resultCount = [1]
    
    for w in words[1:]:
        if w == ' ':
            continue
        
        if w == preChar:
            resultCount[-1] += 1
        else:
            preChar = w
            resultChar.append(w)
            resultCount.append(1)
    
    return ''.join([v + str(w) for v,w in zip(resultChar, resultCount)])            

stringCompress('mississippi')",0.3819389939,
1230,mrjob,"def addtodictionary(raw_email):
    email_message = email.message_from_string(raw_email)
    m = str(email_message['Date']).split(' ')[2]
    y = str(email_message['Date']).split(' ')[3]
    k = m +"" ""+ y
    if k in datatoanalyse:
        datatoanalyse[k] +=1
    else:
        datatoanalyse[k] = 1",0.3794034421,
1230,mrjob,"from MRJob5_3 import job

def runJob5_3(filename, part, s3bucket):

    #mr_job = job(args=[filename, '--part', str(part)])
    #mr_job = job(args=[filename, '--part', str(part), '-r', 'hadoop', '--hadoop-home', '/usr/'])
    mr_job = job(args=[filename, '--part', str(part), '--no-output', '--output-dir', s3bucket,
                       '-r', 'emr', '--emr-job-flow-id', clusterId])
    
    output = []

    with mr_job.make_runner() as runner: 
        # Run MRJob
        runner.run()

        # Write stream_output to file
        for line in runner.stream_output():
            output.append(mr_job.parse_output_line(line))
    
    return output",0.3788090646,
1230,mrjob,"# download from https://pypi.python.org/pypi/rmsd/1.2.5
import rmsd


def myRMSDmetric(arr1, arr2):
    """"""
    This function is built under the assumption that the space dimension is 3!!!
    Requirement from sklearn radius_neighbors_graph: The callable should take two arrays as input and return one value indicating the distance between them.
     Input: One row from reshaped XYZ trajectory as number of steps times nDOF
     Inside: Reshape to XYZ format and apply rmsd as r=rmsd(X[i], X[j])
     Output: rmsd distance
    """"""
    
    nParticles = len(arr1) / 3;
    assert (nParticles == int(nParticles))

    X1 = arr1.reshape(int(nParticles), 3 )
    X2 = arr2.reshape(int(nParticles), 3 )

    X1 = X1 -  rmsd.centroid(X1)
    X2 = X2 -  rmsd.centroid(X2)

    return rmsd.kabsch_rmsd(X1, X2)",0.3783528805,
1230,mrjob,"def profit2(stock_prices):
    """""" Same as before, BUT start max_profit from #1-#0.  Still works with a normal up/down day.""""""
    # Take care of edge case where list has no profit calculatability.  ie, less than 2 prices.
    if len(stock_prices) < 2:
        raise Exception(""Must provide at least 2 prices!"")
    
    # Minimum tracker starting from first element
    min_price = stock_prices[0] 
    
    # Profit tracker
    max_profit = stock_prices[1]-stock_prices[0]  ## DIFFERENT THAN PREVIOUS $!!!!!!!!!
    
    for price in stock_prices[2:]:
        
        ## RE-ORDER FINDING MAX PROFIT AND MIN, $!!!!!!!!!
        ## TO ALLOW FOR NEGATIVE PROFITS        $!!!!!!!!!
        
        # update max profit
        max_profit = max (price-min_price, max_profit)
        
        # update min price if current price is lower
        min_price = min(min_price, price)       
    
    return max_profit",0.377520442,
1230,mrjob,"def profit (stock_prices):
    """"""  Go through list once, tracking minimum found so far, and max profit so far.  Return max profit.
    """"""
    # Take care of edge case where list has no profit calculatability.  ie, less than 2 prices.
    if len(stock_prices) < 2:
        raise Exception(""Must provide at least 2 prices!"")
    
    # Minimum tracker starting from first element
    min_price = stock_prices[0]
    
    # Profit tracker
    max_profit = 0
    
    for price in stock_prices[1:]:
        
        # update min price if current price is lower
        min_price = min(min_price, price)
        
        # update max profit
        max_profit = max (price-min_price, max_profit)
    
    return max_profit",0.377520442,
1443,part predictions,"def integrateData(data):
    return data.map(lambda x: (x,clusters.predict(x),list(centroids[clusters.predict(x)]),\
                               euclidean_distance(x,list(centroids[clusters.predict(x)])),\
                               thresholds[clusters.predict(x)][1],find_anomalous(x,centroids)))
integratedNormalizedAllData = integrateData(normalizedAllData)",0.5200349092,
1443,part predictions,"def predict_test_set(predictor, test_set):
    """"""Compute the prediction for every element of the test set.""""""
    predictions = [predictor.predict(test_set[i, :, :]) 
                   for i in xrange(len(test_set))]
    return predictions

# Choose a subset of the test set. Otherwise this will never finish.
test_set = test_images[0:100, :, :]
all_predictions = [predict_test_set(predictor, test_set) for predictor in predictors]",0.513201952,
1443,part predictions,"def pca(split):
    from sklearn.decomposition import PCA
    pca = PCA()
    new_train = pca.fit_transform(split.get_train_vstack())
    new_test = pca.transform(split.get_test_vstack())
    return(new_train, new_test)",0.5093038082,
1443,part predictions,"def predict(models, X):
    """"""
    Voting classifier that works by averaging predicted probabilities among models.
    Reminder: you can use model.predict_proba(X) to get probabilities for each class
    """"""

    <implement prediction by voting among models>
    
    return <your code: array of averaged probabilities>
    
prediction = predict(models,X_test)

assert prediction.ndim==2 and prediction.shape[1]==2, ""Predicted probabilities must be a matrix[n,2]""
assert np.allclose(prediction.sum(axis=-1),1)",0.5077620745,
1443,part predictions,"def results(preds, test_batch, preds_beam=None):
    print ('  Phonemes_________________________________predictions___________beam_________________label')
    for index in range(400, 500):
        phoneme = '-'.join([phonemes[p] for p in test_batch[0][index]])
        prediction = [letters[l] for l in preds[index]]
        real = [letters[l] for l in test_batch[1][index]]
        beamed = [letters[l] for l in preds_beam[index]]
        print ('  ',phoneme.strip('-_').ljust(40), ''.join(prediction).strip('_').ljust(20), ''.join(beamed).strip('_').ljust(20),
               ''.join(real).strip('_'))",0.5062386394,
1443,part predictions,"def getTrainedNaiveBayesClassifier(extractFeatures, trainingData):
    #Turn the training data into a list of feature vectors
    trainingFeatures = nltk.classify.apply_features(extractFeatures, trainingData) 
    return nltk.NaiveBayesClassifier.train(trainingFeatures);",0.5024228096,
1443,part predictions,"def __predict_ratings(self, user_and_movie_RDD):
    """"""Gets predictions for a given (userID, movieID) formatted RDD
    Returns: an RDD with format (movieTitle, movieRating, numRatings)
    """"""
    predicted_RDD = self.model.predictAll(user_and_movie_RDD)
    predicted_rating_RDD = predicted_RDD.map(lambda x: (x.product, x.rating))
    predicted_rating_title_and_count_RDD = \
        predicted_rating_RDD.join(self.movies_titles_RDD).join(self.movies_rating_counts_RDD)
    predicted_rating_title_and_count_RDD = \
        predicted_rating_title_and_count_RDD.map(lambda r: (r[1][0][1], r[1][0][0], r[1][1]))

    return predicted_rating_title_and_count_RDD
    
def get_top_ratings(self, user_id, movies_count):
    """"""Recommends up to movies_count top unrated movies to user_id
    """"""
    # Get pairs of (userID, movieID) for user_id unrated movies
    user_unrated_movies_RDD = self.ratings_RDD.filter(lambda rating: not rating[1]==user_id).map(lambda x: (user_id, x[1]))
    # Get predicted ratings
    ratings = self.__predict_ratings(user_unrated_movies_RDD).filter(lambda r: r[2]>=25).takeOrdered(movies_count, key=lambda x: -x[1])

    return ratings

# Attach the functions to class methods
RecommendationEngine.__predict_ratings = __predict_ratings
RecommendationEngine.get_top_ratings = get_top_ratings",0.5009455085,
1443,part predictions,"def create_model(self):
    """"""
    Creates and returns the PyMC3 model.

    Returns the model and the output variable. The latter is for use in ADVI minibatch.
    """"""
    model_input = theano.shared(np.zeros([1, self.num_pred]))

    model_output = theano.shared(np.zeros(1))

    model_cats = theano.shared(np.zeros(1, dtype='int'))

    self.shared_vars = {
        'model_input': model_input,
        'model_output': model_output,
        'model_cats': model_cats
    }

    model = pm.Model()

    with model:
        # Both alpha and beta are drawn from Normal distributions
        mu_alpha = pm.Normal(""mu_alpha"", mu=0, sd=10)
        sigma_alpha = pm.HalfNormal(""sigma_alpha"", sd=10)

        mu_beta = pm.Normal(""mu_beta"", mu=0, sd=10)
        sigma_beta = pm.HalfNormal(""sigma_beta"", sd=10)

        alpha = pm.Normal(
            'alpha',
            mu=mu_alpha,
            sd=sigma_alpha,
            shape=(self.num_cats,)
        )

        beta = pm.Normal(
            'beta',
            mu=mu_beta,
            sd=sigma_beta,
            shape=(self.num_cats, self.num_pred)
        )

        c = model_cats

        temp = alpha[c] + T.sum(beta[c] * model_input, 1)

        p = pm.invlogit(temp)

        o = pm.Bernoulli('o', p, observed=model_output)

    return model, o",0.5007712245,
1443,part predictions,"def pred_batch(imgs):
    predictions = model.predict(imgs)
    idxs = np.argmax(predictions, axis=1) # Find the maximum value of from the array of predictions
    print('Shape: {}'.format(predictions.shape))
    print('First 5 classes: {}'.format(classes[:5]))
    print('First 5 probabilities: {}\n'.format(predictions[0, :5]))
    print('Predictions prob/class: ')
    
    for i in range(len(idxs)):
        idx = idxs[i]
        print ('  {:.4f}/{}'.format(predictions[i, idx], classes[idx])) # Prints out all predictions in descending order",0.5003022552,
1443,part predictions,"def max_points(contours):
    points = []
    for c in contours:
        left = tuple(c[c[:, :, 0].argmin()][0] )
        right = tuple(c[c[:, :, 0].argmax()][0] )
        top = tuple(c[c[:, :, 1].argmin()][0] )
        bottom = tuple(c[c[:, :, 1].argmax()][0] )
        points.append([left, right, top, bottom])
    return np.array(points)

def calc_centroids(image, contours):
    centroids = []
    cv2.circle(image, (372, 240), 20, (0, 255, 255), 2)
    for c in contours:
        M = cv2.moments(c)
        cx = int(M[""m10""] / M[""m00""])
        cy = int(M[""m01""] / M[""m00""])
        v_max = tuple(c[c[:, :, 1].argmin()][0] )
        v_min = tuple(c[c[:, :, 1].argmax()][0] )
        centroids.append([cx, cy])
        cv2.line(image, v_max, v_min, (0, 0, 255), 3)
        cv2.line(image, (372, 240), (cx, cy), (0, 255, 0), 3)
    plt.figure(figsize=(10, 6) )
    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB) )
    plt.show()",0.4984138608,
1717,implement the squared loss function,"def neural_net_pass(net, x, y):
    pred = net.forwardpass(x)
    loss = net.loss(pred, y)
    net.backpropogate(loss)
    return pred",0.491056174,
1717,implement the squared loss function,"def compute_loss(y,pred,parameters):
    """"""
    softmax cross entropy loss with l2 regularization 
    params: placeholder: y (labels) and pred (predicition) and Variable: weights. 
    returns: loss tensor object
    """"""
    beta = 0.000005
    regularizer = (beta * tf.nn.l2_loss(parameters[""w1""]) + 
                   beta * tf.nn.l2_loss(parameters[""w2""]) +
                   beta * tf.nn.l2_loss(parameters[""w3""]) +
                   beta * tf.nn.l2_loss(parameters[""w4""]))
    
    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=pred)) + regularizer  # softmax ce loss",0.4775018692,
1717,implement the squared loss function,"# TODO: this doesn't work properly when you predict 1 or 0. Write a test and fix it
def loss_classification(predictions: np.array, actual: np.array) -> float:
    positives = -np.multiply(actual, np.log(predictions))
    negatives = -np.multiply((1 - actual), np.log(1 - predictions))
    return positives + negatives
loss_classification(np.array([.5, .5, 0]), np.array([1, 0, 0]))
loss_classification(np.array([.5, .5, .2]), np.array([1, 0, 0]))",0.4766264558,
1717,implement the squared loss function,"def cost(AL, Y):
    return np.squeeze((1./Y.shape[1])*(-np.dot(Y, np.log(AL).T)- np.dot(1-Y, np.log(1-AL).T)))",0.4763718247,
1717,implement the squared loss function,"def define_cost_function(y, y_tensor, batch_size):
    cost = -tf.reduce_sum(y_tensor * tf.log(y), name='cross_entropy') / batch_size
    return cost",0.4735071063,
1717,implement the squared loss function,"def compute_loss_wrt_image(X, y, model):
    dimage = tf.gradients(model.loss, model.image)
    sal = tf.squeeze(tf.reduce_max(tf.abs(dimage), axis=4))
    
    saliency = sess.run(sal, feed_dict={
        model.image: X,
        model.labels: y,
    })
    return saliency",0.471167922,
1717,implement the squared loss function,"def cost(y, t):
    return - np.sum(np.multiply(t, np.log(y)) + np.multiply((1-t), np.log(1-y)))

def cost_for_param(x, wh, wo, t):
    return cost(nn(x,wh,wo), t)",0.4701841474,
1717,implement the squared loss function,"def merge(vgg16avg, content_loss, style_loss, shape):
    loss = content_loss/ 10. + style_loss
    grads = K.gradients(loss, vgg16avg.input)
    transfer_fn = K.function([vgg16avg.input], [loss]+grads) 
    evaluator = Evaluator(transfer_fn, shape)
    return evaluator",0.4684829116,
1717,implement the squared loss function,"def costFunction(X, y, theta):
    h= X.dot(theta)
    err= h-y
    return 0.5/X.shape[0]*sum(err*err)",0.4656264782,
1717,implement the squared loss function,"def compute_cost(A2, Y, parameters):
    m = Y.shape[1]  # number of samples
    logprobs = np.multiply(Y, np.log(A2)) + np.multiply((1-Y), np.log(1-A2))
    cost = - 1.0/m * np.sum(logprobs)
    
    cost = np.squeeze(cost ) # makes sure cost is the dimension we expect. 
                                # E.g., turns [[17]] into 17",0.4655863643,
1294,numpy arrays,"1+np.asarray([1,2,3])",0.4833664298,
1294,numpy arrays,"o = np.empty(3)
o",0.4810328484,
1294,numpy arrays,"z = np.empty(3)
z",0.4810328484,
1294,numpy arrays,"# For your convenience, we have created a list containing
# 98 copies of the number 0 (to represent good syringes)
# and 2 copies of the number 1 (to represent two bad syringes).
# This may be useful if you run a simulation.  Feel free
# to delete it.
faultiness = np.append(0*np.arange(98), [1,1])
 
chance_to_find_syringe = ...
chance_to_find_syringe",0.4806088209,
1294,numpy arrays,"outputImageSize = outputResolution[:]
outputImageSize.append(3)

mappedImage = np.empty(outputImageSize)
mappedImage[:] = NAN",0.4805225134,
1294,numpy arrays,"pts = empty(X.shape + (3,))
pts[..., 0] = X
pts[..., 1] = Y
pts[..., 2] = Z
probed_vals = np.ma.masked_array([probe(f, pts) for f in dolfin_funcs])",0.4796213806,
1294,numpy arrays,"pts = np.empty(z.shape + (3,))
pts[...,0] = x
pts[...,1] = y
pts[...,2] = z",0.4783713222,
1294,numpy arrays,"# assign from another array
b[...] = np.zeros((2,2))
print(repr(a))",0.4780900478,
1294,numpy arrays,"test = np.zeros((200,32))
test[:] = dt[:]
print(test)",0.4779512882,
1294,numpy arrays,"g = np.ones((3,))
r[1] = g
print(r)",0.4769128859,
2400,training a model,"class Demo():    
    def __init__(self, image_list, score_list):
        self.image_list=image_list
        self.score_list=score_list
        
    def show_result(self, query_id, ranked_list,topk=5, debug=False):
        import matplotlib.pyplot as plt
        fig=plt.figure(figsize=(12,6))
        query_name=self.image_list[query_id]
        ax=plt.subplot(1,topk+2,1);
        ax.tick_params(labelbottom='off', labelleft='off')
        reid.Plotter.show_image(query_name)
        title_name=reid.Plotter.person_id(query_name)
        plt.title(title_name,fontsize=10)
        if debug:
            print 'Query filename: ',query_name
            print 'Ranked gallery filename:'
        #show the topk ranking results
        for i in range(topk):
            ax=plt.subplot(1,topk+2,i+3);
            ax.tick_params(labelbottom='off', labelleft='off')
            rank_idx=ranked_list[i]
            ranked_filename=image_list[rank_idx]
            score=self.score_list[query_id][rank_idx]
            if debug:
                print ranked_filename
            plt.title('%.4f'%score,fontsize=10)
            reid.Plotter.show_image(ranked_filename)
        plt.show()
        
    def select_show(self,rand_num=10):
        all_query_list=range(len(self.score_list))
        #randomly select the queries to show
        random_list=np.random.choice(all_query_list,rand_num, replace=False)
        print 'Randomly show %d results for the query ids in all_query_list (total number is %d)'%(len(random_list),len(all_query_list))
        print 'Some examples of the query_list', random_list[:10]
        return random_list, all_query_list
    
    def show_demo(self, query_list, topk=5, debug=False):
        for query_id in query_list:
            ranked_list=np.argsort(self.score_list[query_id])[::-1]
            cur_score_list=np.sort(self.score_list[query_id])[::-1]
            self.show_result(query_id,ranked_list[1:],topk, debug) #ignore the top-1 (always the query itself)",0.5053492785,
2400,training a model,"## TODO: Create train and evaluate function using tf.estimator
def train_and_evaluate(output_dir, num_train_steps):
  #ADD CODE HERE
  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)",0.4979383945,
2400,training a model,"def train(trainX, trainY):
    print('starting train')
    trainer = dn.AdamTrainer(model)
    for _ in range(REPEATS):
        for X, Y in zip(trainX, trainY):
            probs = get_probs(X)
            loss = dn.sum_batches(dn.pickneglogsoftmax_batch(probs, Y))
            loss_value = loss.value()
            loss.backward()
            trainer.update()
    print('done training!')",0.497680366,
2400,training a model,"def train(epoch):
    losses = []
    network.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = Variable(data), Variable(target)
        optimizer.zero_grad()
        output = network(data)
        loss = F.cross_entropy(output, target)
        losses.append(loss.data[0])
        loss.backward()
        optimizer.step()
        if batch_idx % 100 == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.data[0]))
    
    return losses

def test():
    network.eval()
    test_loss = 0
    correct = 0
    for data, target in test_loader:
        data, target = Variable(data, volatile=True), Variable(target)
        output = network(data)
        test_loss += F.cross_entropy(output, target, size_average=False).data[0] # sum up batch loss
        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability
        correct += pred.eq(target.data.view_as(pred)).cpu().sum()

    test_loss /= len(test_loader.dataset)
    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))
    
    incorrect_perc = (len(test_loader.dataset) - correct) / len(test_loader.dataset)
    
    return incorrect_perc",0.4938234389,
2400,training a model,"# define training function
def train(epoch, model):
    """"""
        Train the model
        Inputs:
            epoch - number of the current epoch
            
        Outputs:
            
    """"""
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = Variable(data), Variable(target)
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()
        if batch_idx % LOG_INTERVAL == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.data[0]))",0.4920627475,
2400,training a model,"def train(epoch):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        if torch.cuda.is_available():
            data, target = data.cuda(), target.cuda()
        data, target = Variable(data), Variable(target)
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()

    print('Train Epoch: {} \tLoss: {:.6f}'.format(epoch, loss.data[0]))

def test():
    model.eval()
    test_loss = 0
    correct = 0
    for data, target in test_loader:
        if torch.cuda.is_available():
            data, target = data.cuda(), target.cuda()
        data, target = Variable(data, volatile=True), Variable(target)
        output = model(data)
        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss
        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability
        correct += pred.eq(target.data.view_as(pred)).cpu().sum()

    test_loss /= len(test_loader.dataset)
    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))",0.490445435,
2400,training a model,"def train(epoch, loader):
    loader = ellipse_loader
    model.train()
    train_loss = 0
    for batch_idx, (data, _) in enumerate(loader):
        #just need the data, don't need the dummy label for VAE :)
        data = Variable(data)
        optimizer.zero_grad()
        (mu_x, logvar_x), mu_z, logvar_z = model(data)
        loss = loss_function_Gaussian(mu_x, logvar_x, data, mu_z, logvar_z) #minimize the - loss = maximize lower bound
        loss.backward()
        train_loss += loss.data[0]
        optimizer.step()
        if batch_idx % log_interval == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(loader.dataset),
                100. * batch_idx / len(loader),
                loss.data[0] / len(data)))
            

#     print('====> Epoch: {} Average loss: {:.4f}'.format(
#           epoch, train_loss / len(loader.dataset)))
    return (-train_loss/len(loader.dataset))  #record average loss at current iteration",0.4882339537,
2400,training a model,"def train(epoch):
    model.train()
    for batch, (data, target) in enumerate(train_loader):
        data, target = data.cuda(), target.cuda()
        data, target = Variable(data), Variable(target)
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()
        if batch % LOG_INTERVAL == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'
                  .format(epoch, 
                          batch * len(data), 
                          len(train_loader.dataset), 
                          100. * batch / len(train_loader), 
                          loss.data.item()))

def test(epoch):
    model.eval()
    test_loss = 0
    correct = 0
    for data, target in test_loader:
        data, target = data.cuda(), target.cuda()
        with torch.no_grad():
            data, target = Variable(data), Variable(target)
        output = model(data)
        test_loss += F.nll_loss(output, target).data.item()
        pred = output.data.max(1)[1]
        correct += pred.eq(target.data).cpu().sum()
    test_loss /= len(test_loader)
    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'
          .format(test_loss, 
                  correct, 
                  len(test_loader.dataset), 
                  100. * correct / len(test_loader.dataset)))",0.4878677726,
2400,training a model,"def train(epoch):
    network.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = Variable(data), Variable(target)
        optimizer.zero_grad()
        output = network(data)
        loss = F.cross_entropy(output, target)
        loss.backward()
        optimizer.step()
        if batch_idx % 100 == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100 * batch_idx / len(train_loader), loss.data[0]))

def test():
    network.eval()
    test_loss = 0
    correct = 0
    for data, target in test_loader:
        data, target = Variable(data, volatile=True), Variable(target)
        output = network(data)
        test_loss += F.cross_entropy(output, target, size_average=False).data[0] # sum up batch loss
        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability
        correct += pred.eq(target.data.view_as(pred)).cpu().sum()

    test_loss /= len(test_loader.dataset)
    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))",0.4878677726,
2400,training a model,"def train(epoch):
    network.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = Variable(data), Variable(target)
        optimizer.zero_grad()
        output = network(data)
        loss = F.cross_entropy(output, target)
        loss.backward()
        optimizer.step()
        if batch_idx % 100 == 0:
            print(('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.data[0])))

def test():
    network.eval() 
    test_loss = 0
    correct = 0
    for data, target in test_loader:
        data, target = Variable(data, volatile=True), Variable(target)
        output = network(data)
        test_loss += F.cross_entropy(output, target, size_average=False).data[0] # sum up batch loss
        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability
        correct += pred.eq(target.data.view_as(pred)).cpu().sum()

    test_loss /= len(test_loader.dataset)
    print(('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset))))",0.4878677726,
1556,prediction,"def stack_tensor(model):
    pred_cpb_all = np.empty((0,8))
    for i in range(1,model.config.NUM_CLASSES):
    if pred_cls_cnt[i] > 0:
        pred_cpb_all = np.vstack((pred_cpb_all, pred_cpb[i,0:pred_cls_cnt[i]] ))",0.514144063,
1556,prediction,"def train_conv(batch_images,batch_labels):
    _,acc_tc, loss_tc = sess.run([minimizer,accuracy,loss],feed_dict={inputs:batch_images,labels: batch_labels})
    return acc_tc,loss_tc",0.5119901896,
1556,prediction,"def compute_accuracy(v_xs,v_ys):
    global prediction
    y_pre = sess.run(prediction,feed_dict={# if it is not neccessary ???
            xs:v_xs,
            ys:v_ys,
            keep_prob:0.5
        })
    correct_prediction = tf.equal(tf.argmax(y_pre,1),tf.argmax(v_ys,1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))
    result = sess.run(accuracy,feed_dict ={
            xs:v_xs,
            ys:v_ys,
            keep_prob:0.5
        })
    return result",0.5034373999,
1556,prediction,"def predict_point_by_point(model, data):
    predicted = model.predict(data, batch_size=1)
    predicted = np.reshape(predicted, (predicted.size,))
    return predicted",0.497861743,
1556,prediction,"#This function takes the dataset and the trained classifier to return the prediction obtained.

def predict(data,clf):

    data_x = data[:,:-1]
    data_y = data[:,-1]
    predicted_y = clf.predict(data_x)
    
    return predicted_y",0.4972348213,
1556,prediction,"#Function

def evaluateNetwork(v_xs):
    global prediction
    y_pre = sess.run( tf.cast(prediction, tf.float32), feed_dict={xs: v_xs}) # Forward pass using v_xs as input to network
    result = np.matrix(y_pre, dtype=float)
    return result

# Calculate mean absolute error (MAE) and mean square error (MSE) for evaluation
def compute_accuracy(v_xs, v_ys):
    global prediction
    y_label = tf.reshape(v_ys, [-1, 43, 43, 1])
    y_label = tf.cast(y_label, tf.float32)
    y_pre = sess.run(prediction, feed_dict={xs: v_xs})
    y_pre = tf.cast(y_pre, tf.float32)#Force float32
    #correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1))
    diff = tf.subtract(tf.reduce_sum(y_label,[1,2]),tf.reduce_sum(y_pre,[1,2]))
    accuracyMAE = tf.cast(tf.reduce_mean(tf.abs(diff)),tf.float32)
    accuracyMSE = tf.cast(tf.sqrt(tf.reduce_mean(tf.square(diff))),tf.float32)
    MAE = sess.run(accuracyMAE, feed_dict={xs: v_xs, ys: v_ys})
    MSE = sess.run(accuracyMSE, feed_dict={xs: v_xs, ys: v_ys})
    
    return MAE,MSE

def weight_variable(shape, nameIn):
    initial = tf.truncated_normal(shape=shape, stddev=0.1)
    return tf.Variable(initial, name=nameIn)

def bias_variable(shape, nameIn):
    initial = tf.constant(0.1, shape=shape)
    return tf.Variable(initial, name=nameIn)

def conv2d(x, W):
    # stride [1, x_movement, y_movement, 1]
    # Must have strides[0] = strides[3] = 1
    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')

def max_pool_2x2(x):
    # stride [1, x_movement, y_movement, 1]
    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')

def max_pool_3x3(x):
    # stride [1, x_movement, y_movement, 1]
    return tf.nn.max_pool(x, ksize=[1,3,3,1], strides=[1,3,3,1], padding='SAME')",0.4970687032,
1556,prediction,"def AutoEncoder(Inputs,Weights,biases):
    h=tf.nn.relu(tf.add(tf.matmul(Inputs,Weights['Wh']),biases['bh']))
    Outputs=tf.nn.tanh(tf.add(tf.matmul(h,Weights['Wo']),biases['bo']))
    return Outputs,h # h is the final results that we want!",0.4966657758,
1556,prediction,"def multilayer_perceptron(x, weights, biases):
    # Layer 1, ReLU activation, 4 neurons
    layer_1 = tf.nn.relu(tf.add(tf.matmul(x, weights['h1']), biases['b1']))
    # Layer 2, ReLU activation, 4 Neurons
    layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']))
    return tf.matmul(layer_2, weights['out']) + biases['out']",0.4966657758,
1556,prediction,"def score_ensemble(ensemble, X, y):
    return roc_auc_score(y, ensemble.predict_proba(X)[:,1])",0.4960415959,
1556,prediction,"def classify_test(train_x, train_y, test_x, test_y, clf):
    clf.predict(test_x)
    return clf.predict_proba(test_x)",0.4954296947,
902,imports,"@new_method
def _getter(self, attr):
    """"""Extract a function from the imports based on a name.
    """"""
    return pipe(self._imports, filter(lambda x: x.__name__ == attr), first)",0.4000420868,
902,imports,"import types
def imports():
    for name, val in globals().items():
        if isinstance(val, types.ModuleType):
            yield val.__name__
            
import pdb",0.3679853082,
902,imports,"def get_responses_for_module(m, module_name):
    
    # Data for all responses
    module = m.modules[module_name]
    # Reactions, species and rate equations this module
    rxns = list(module[""reactions""])
    sps = list(module[""species""])
    rates = {rx: m.rateDict[rx] for rx in rxns}
    # Get the stochiometry submatrix: module species x module reaction
    spIdx = np.array([m.idSp.index(sp) for sp in sps]) # indices of species
    rnIdx = np.array([m.idRs.index(rx) for rx in rxns]) # indices of reactions
    S_module = m.S[spIdx, :][: , rnIdx]

    # Output species of the module (can be only 1)
    output_node = [sp for sp in sps if sp in nodes]
    assert len(output_node) == 1
    output_node = output_node[0]
    #input nodes to the module
    input_nodes = [n for n in module['input_from'] if n in nodes]
    
    # Parameters related to the module
    # Treat input nodes as parameter. 
    # These will be perturbed to get local response coefficient
    param = dict()
    for sp in module['input_from']:
        param[sp] = dict(m.refSS)[sp]
    for p in module['param']:
        param[p] = m.param[p]
    
    ss_module = [dict(m.refSS)[sp] for sp in sps]

    # Calculate responses
    response_lst = []
    for node_perturbed in input_nodes:
        param_perturbed = copy.deepcopy(param)
        param_perturbed[node_perturbed] = param_perturbed[node_perturbed]*1.001

        v_perturbed = moduleForOdeModeling.createRateVector(
            sps, rxns, rates, param_perturbed
        )
        def odes_perturbed(x, t):
            return np.dot(S_module, v_perturbed(x))   
        simTime = np.arange(0, 500, 0.01)
        tsol = dict(zip(sps, scipy.integrate.odeint(odes_perturbed, ss_module, simTime)[-1]))
        xof, xoi = tsol[output_node], dict(m.refSS)[output_node]
        xif, xii = param_perturbed[node_perturbed], dict(m.refSS)[node_perturbed]

        response_scaled = (xii/xoi)*(xof-xoi)/(xif-xii)
        
        response_lst.append((
            m.get_species_names(output_node),
            m.get_species_names(node_perturbed),
            response_scaled
            )
        )
        
    return response_lst

def build_rloc(m):
    # NODES_ORDERED = [m.get_species_names(sp) for sp in m.idSp if sp in nodes]
    rloc = -1*np.identity(len(nodes))
    for module in m.modules.keys():
        response_lst = get_responses_for_module(m, module)
        for response in response_lst:
            i = NODES_ORDERED.index(response[0])
            j = NODES_ORDERED.index(response[1])
            rloc[i, j] = response[2]
    rloc = pd.DataFrame(rloc)
    rloc.index = NODES_ORDERED
    rloc.columns = NODES_ORDERED
    return rloc",0.3628446162,
902,imports,"import copy

def has_passed(special_add, tests):
    for test in tests:
        current_func = copy.deepcopy(test.function)
        current_func.func_globals['special_add'] = special_add
        try:
            current_func()
        except Exception as e:
            return False
    return True",0.3611092567,
902,imports,"# Version checks
import importlib
def version_check(libname, min_version):
    m = importlib.import_module(libname)
    print(""{} version {} is "".format(libname, m.__version__), end='')
    print(""OK"" if m.__version__ >= min_version 
           else ""out-of-date. Please upgrade!"")
    
version_check(""numpy"", ""1.12"")
version_check(""matplotlib"", ""2.0"")
version_check(""pandas"", ""0.20"")
version_check(""nltk"", ""3.2"")
version_check(""tensorflow"", ""1.1.0"")",0.3610974252,
902,imports,"def lookup(boo):
    import inspect
    print inspect.getmembers(boo, predicate=inspect.ismethod)
    print inspect.getmembers(boo,lambda a:not(inspect.isroutine(a)))
#lookup(sim)
#lookup(rebound)",0.3607014418,
902,imports,"def predict_regressor(array, A, functions=[]):
    new_X, Y = features_targets(array)
    new_X = extend(new_X, functions)
    return new_X.dot(A), Y",0.3606452942,
902,imports,"#from sys.stdout import write
import sys
import string

def printNicely(x, num_tokens):
    num_tokens, words = printNicelyTitle(x, num_tokens)
    printNicelyCorpus(x, num_tokens, words)
    
def printNicelyTitle(x, num_tokens):
    words = reuters.words(reuters.fileids()[x])
    sys.stdout.write(' ')
    for i,word in enumerate(words):
        if len(word) > 2:
            if word[1].islower():
                sys.stdout.write('\n')
                return num_tokens-i, words[i+1:]
        if i < len(words)-1:
            if words[i+1] not in set(string.punctuation) and len(words[i+1]) != 1:
                sys.stdout.write(' ')
        sys.stdout.write(word)
    return 0, ['']

def printNicelyCorpus(x, num_tokens, words):
    for i,word in enumerate(words[:num_tokens]): 
        sys.stdout.write(word)
        if i < len(words)-1:
            if words[i+1] not in set(string.punctuation):
                sys.stdout.write(' ')
    sys.stdout.write('\n\n')
    
printNicely(123, 50)

def print_results(scores, corpus, num_results, num_chars=50):
    for pair in scores[:num_results]:
        sys.stdout.write('%f ' % pair[0])
        printNicely(pair[1], num_chars)",0.3593358099,
902,imports,"import sys
def get_rules(sess, parserm, b_print=True):
    all_attention_operators, all_attention_memories, query = get_attentions(sess)
    
    all_listed_rules = {}
    all_printed_rules = []
    for i, q in enumerate(query):
        if not False:
            if (i+1) % max(1, (len(query) / 5)) == 0:
                sys.stdout.write(""%d/%d\t"" % (i, len(query)))
                sys.stdout.flush()
        else: 
            # Tuple-ize in order to be used as dict keys
            q = tuple(q)
            
        #if q in attention_operators:
        #    if q in all_attention_memories:
        all_listed_rules[q] = list_rules(all_attention_operators[q], 
                                         all_attention_memories[q],
                                         0.01,)
        if b_print:
            all_printed_rules += print_rules(q, 
                                         all_listed_rules[q], parser)

    pickle.dump(all_listed_rules, 
                open(os.path.join(model_path, ""rules.pckl""), ""w""))
    if b_print:
        with open(os.path.join(model_path, ""rules.txt""), ""w"") as f:
            for line in all_printed_rules:
                f.write(line + ""\n"")
    print(""\nRules listed and printed."")",0.3556911349,
902,imports,"def getRow(status, row ):
    row = getBasicInfo(status, row)
    row = getEntities(status, row)
    row = getUser(status, row)
    row = getOriginalStats(status, row)
    row = getOriginalUser(status, row)
    return row;",0.3556155264,
1012,lab,"def one_hot_encode(x):
    """"""
    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.
    : x: List of sample Labels
    : return: Numpy array of one-hot encoded labels
    """"""
    #allows binarized label array to be saved outside of function
    global binarizer
    
    #initialize LabelBinarizer
    binarizer = LabelBinarizer() 
    binarizer.fit(x)
    binarizer.classes_ = np.array(list(range(10)))
    output = binarizer.transform(x)
    return output


""""""
DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE
""""""
tests.test_one_hot_encode(one_hot_encode)",0.4472540617,
1012,lab,"### Fill your code here
import matlab.engine

def SketchToken(IMAGE_PATH):
    eng = matlab.engine.start_matlab()
    eng.addpath(r'../ref/SketchTokens/',nargout=0)
    eng.addpath(r'../ref/SketchTokens/toolbox/channels/',nargout=0)
    edge = eng.SketchToken(IMAGE_PATH)
    edge = np.array(edge)
    eng.quit()  
    return edge

### Report the accuracy obtained
Edge_Detector_Predict(SketchToken, 'data/img', 'data/edge/SketchToken')
acc_SketchToken = Edge_Detector_Eval(SketchToken, 'data/edge/SketchToken', 'data/ground_truth')  
print('Sketch Token: Average accuracy of test images: %f'%np.mean(acc_SketchToken))

### Report any improvements you have tried",0.4393677711,
1012,lab,"def npred(text):
    t_hot = tokenizer.texts_to_matrix([text], mode='binary')
    pc = model.predict_classes(t_hot, batch_size=1)
    isco = labelencoder.inverse_transform(pc)
    return isco[0]",0.4371564984,
1012,lab,"loader = Loader()
loader.load_binaries(""../sample_data/dict"")",0.4358766675,
1012,lab,"cobra.io.load_matlab_model(
    join(data_dir, ""mini.mat""), variable_name=""mini_textbook"")",0.4313144088,
1012,lab,"cobra.io.load_matlab_model(join(data_dir, ""mini.mat""),
                           variable_name=""mini_textbook"")",0.4313144088,
1012,lab,"def load_label_map():
  label_map = label_map_util.load_labelmap(PATH_TO_LABELS)
  categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)
  category_index = label_map_util.create_category_index(categories)  
  return categories, category_index",0.4293239117,
1012,lab,"import scipy.io
def read_mat_data(recordName):
    return scipy.io.loadmat(recordName,appendmat=True)['val'][0]",0.4262678027,
1012,lab,"def pre_proc(df):
    '''Create dummy variables for all non-numerical columns'''
   
    # Get dummy variables for Sex and Embarked
    # Remove Sex and Embarked columns
    df = pd.get_dummies(df, columns=['Sex', 'Embarked'])
  
  
    return df",0.4246090651,
1012,lab,"cobra.io.load_matlab_model(join(data_dir, ""mini.mat""))",0.4240761101,
1011,knn with sklearn,"cons = Construction(nodes)
G = cons.knn(k=5, sigma=1)",0.5818443298,
1011,knn with sklearn,"cons = Construction(nodes)

## change the graph type with the options as follows.
# G = cons.approximate_knn(k =5, sigma = 1, n_tree = 100)
G = cons.knn(k=5, sigma=1)
# G = cons.rng()",0.5818443298,
1011,knn with sklearn,"# A:
#1.import

from sklearn.neighbors import KNeighborsClassifier as knn

#2. instantiate

knn =KNeighborsClassifier()

#3. fit (aka ""train"" on your labeled data)

knn.fit(wine_df, y)

#4. Mae new predictions on score
wine_sample = wine_df.sample(5)

print ""prediction:"", knn.predict(wine_sample)
print ""actual"", y[wine_sample.index]",0.5812137127,
1011,knn with sklearn,from sklearn.neighbors import KNeighborsClassifier as knn,0.5794196129,
1011,knn with sklearn,"# The label function from skimage.measure module finds every connected set of pixels other than the background, 
# and relabels these sets with a unique increasing integer.
from skimage.measure import label
markers_nuc = label(nmask, connectivity=2, return_num=True)",0.5765115023,
1011,knn with sklearn,"from sklearn import datasets
from sklearn import metrics
from sklearn.naive_bayes import GaussianNB
# load the iris datasets
dataset = datasets.load_iris()
# fit a Naive Bayes model to the data
model = GaussianNB()
model.fit(dataset.data, dataset.target)
print(model)
# make predictions
expected = dataset.target
predicted = model.predict(dataset.data)
# summarize the fit of the model
print(metrics.classification_report(expected, predicted))
print(metrics.confusion_matrix(expected, predicted))",0.5754571557,
1011,knn with sklearn,"# Gaussian Naive Bayes
from sklearn import datasets
from sklearn import metrics
from sklearn.naive_bayes import GaussianNB
# load the iris datasets
dataset = datasets.load_iris()
# fit a Naive Bayes model to the data
model = GaussianNB()
model.fit(dataset.data, dataset.target)
print(model)
# make predictions
expected = dataset.target
predicted = model.predict(dataset.data)
# summarize the fit of the model
print(metrics.classification_report(expected, predicted))
print(metrics.confusion_matrix(expected, predicted))",0.5754571557,
1011,knn with sklearn,"#create and fit classifier
from skmultilearn.adapt import MLkNN
classifier = MLkNN(k=2)
classifier.fit(X_train, Y_train)",0.5749256015,
1011,knn with sklearn,"%%time
from sklearn.decomposition import NMF

# Number of topics
n_components = 5

# Instantiate NMF model
nmf_model = NMF(n_components=n_components)

# Fit model
nmf_model.fit(X_raw)",0.5742120147,
1011,knn with sklearn,"from sklearn.decomposition import NMF
# remember you need to specify number of components for NMF
model = NMF(n_components=2)
model.fit(samples)
nmf_features = model.transform(samples)

print(model.components_)",0.5736464262,
381,creating dataframe structures,"# creates frames using an existing function from the previous workshop

	frames = [workshop_01.structureFrame(beamDimensions[0], pillarDimensions[0], beamDistances[0], pillarDistances[0])]
	for i in range(1, len(frameDistancesX)):
		frames.extend([T(1)(frameDistancesX[i-1]), workshop_01.structureFrame(beamDimensions[i], pillarDimensions[i], beamDistances[i], pillarDistances[i])])
	frames = STRUCT(frames)",0.4539017677,
381,creating dataframe structures,"class MyDataFrame(object):
    def __init__(self, df):
        self.df = df
        self.list_df = [df]
        self.n_updates = 0
    def update(self,df):
        self.df = df
        self.list_df+=[df]
        self.n_updates += 1
    def last_indices_removed(self):
        return self.list_df[-2].index.difference(self.list_df[-1].index)
    def __str__(self):            
        return str(len(self.df))  if (len(self.df)<=999) else "">999""

# Style should be an attribute of the dataframe
    
def highlight_not_null(df_colors=['green'], null_color='red',unknown_color='white'):
    def style_function(s):
        colors = []
        for p in s: 
            if pd.isnull(p):
                colors+=[null_color]
            elif type(p) is MyDataFrame:
                colors+=[df_colors[p.n_updates]]
            else:
                colors+=[unknown_color]
        return ['background-color: '+color for color in colors]
    return style_function",0.4346121252,
381,creating dataframe structures,"from pymks.datasets import make_microstructure
dataset = np.concatenate([make_microstructure(n_samples=samps,
                              size=size, grain_size=grains, seed=seed, 
                               volume_fraction=v_f, percent_variance=per_ch)
                          for samps, grains, v_f in zip(n_samples, grain_size, v_frac)])
dataset.shape",0.4333907962,
381,creating dataframe structures,"class DataIterator(object):

    def __init__(self, df):
        self.size = 26
        self.df = df
        self.n_input = self.size*self.size

    def __iter__(self):

        for unit, df_unit in self.df.groupby(""unit""):            
            arr = df_unit[df_unit.columns.difference([""unit"", ""time""])].as_matrix()
            for i in range(arr.shape[0] - self.size):
                yield arr[i:i+self.size].reshape(-1)

    def iter_series(self):

        for unit, df_unit in self.df.groupby(""unit""):
            time_values = df_unit[""time""].values            
            arr = df_unit[df_unit.columns.difference([""unit"", ""time""])].as_matrix()            
            vectors = [arr[i:i+self.size].reshape(-1) for i in range(arr.shape[0] - self.size)]
            yield unit, time_values[-len(vectors):], vectors


def autoencoder_train_set(sets, with_rul=False, with_rul_class=False):
    arrs = []
    for i in sets:
        x = np.array(list(DataIterator(train_data(i, with_rul, with_rul_class))))
        arrs.append(x)
    return np.vstack(arrs)


def autoencoder_test_set(sets):
    arrs = []
    for i in sets:
        x = np.array(list(DataIterator(test_data(i))))
        arrs.append(x)
    return np.vstack(arrs)

def standard_scale(X_train, X_test):
    preprocessor = preprocessing.StandardScaler().fit(X_train)
    X_train = preprocessor.transform(X_train)
    X_test = preprocessor.transform(X_test)
    return X_train, X_test

def get_random_block_from_data(data, batch_size):
    start_index = np.random.randint(0, len(data) - batch_size)
    return data[start_index:(start_index + batch_size)]",0.4332668185,
381,creating dataframe structures,"# Build the DataFrame for player stats per year
# Because of table formatting first will create a dict and then convert to a dataFrame
Yearly_df = {}
Yearly_df_adv = {}
Yearly_df_100pos = {}
# iterate through all years
for curr_year in years_to_grab:
    print(curr_year)
    output_df = ptd.build_dataframe([yearly_data, yearly_data_adv, yearly_data_100pos], 
                    [yearly_headers, yearly_headers_adv, yearly_headers_100pos], curr_year)
    Yearly_df[str(curr_year)] = output_df[0]
    Yearly_df_adv[str(curr_year)] = output_df[1]
    Yearly_df_100pos[str(curr_year)] = output_df[2]",0.4330705404,
381,creating dataframe structures,"# make sure both have same index
X = ef_df.loc[in_both, :]
Y = ct_df.loc[in_both,:]
threshold = 50
vc = Y.structure.value_counts()

structures_to_keep = vc[vc >= threshold].index.values
sc = Y.spines.value_counts()
spines_to_keep = sc[sc >= threshold].index.values
to_keep = Y['structure'].isin(structures_to_keep) & Y['spines'].isin(spines_to_keep)
Y = Y.loc[to_keep, :]
X = X.loc[to_keep, :]",0.4318137765,
381,creating dataframe structures,"args = {
    'region': ['CVL', 'IDF', 'NAQ', 'ARA', 'PAC'],
    'with_gdp': [False, True],
    'trend_model': ['Holt', 'ARIMA'],
    'lag': range(1,6),
    # periods is a tuple of (training start year, training end year, test year)
    'periods': [('2010-01-01', f'{end_year}-01-01', f'{end_year + 1}-01-01')
                 for end_year in range(2012, 2017)]
}

error_df = pd.DataFrame(columns=list(args.keys()) + ['MAE', 'RMSE', 'Max_error', 'MASE'])

for region, with_gdp, trend_model, lag, periods in product(*args.values()):
    start_year, end_year, predict_year = periods
    input_data = pd.concat([nights, temp, daysoff, gdp], axis=1).dropna()
    input_data = input_data[[f'nights_{region}',
                             f'avgtemp_{region}',
                             f'gdp_{region}',
                             'daysoff']]
    target = nights[[f'nights_{region}']]

    training_period = (input_data.index>=start_year)&(input_data.index<end_year) 
    testing_period = (target.index>=end_year)&(target.index<predict_year)

    train_data = input_data[training_period]
    test_data = np.array(target[testing_period])

    if not with_gdp:
        train_data.drop(f'gdp_{region}', axis=1)

    # --------Seasonal decompose----------
    decomposed_data = seasonal_decompose(
        train_data, model='multiplicative', extrapolate_trend='freq')  
    seasonal_data, trend_data, residual_data = (decomposed_data.seasonal,
                                                decomposed_data.trend, decomposed_data.resid)

    # ---------Seasonal prediction (Same as last year of training data)---------
    seasonal_train = seasonal_data[
        (seasonal_data.index>=start_year)&(seasonal_data.index<end_year)]
    seasonal_forecast = pd.DataFrame(seasonal_train[f'nights_{region}'].values[-12:])

    # --------Trend prediction (Linear Holt)----------
    if trend_model == 'Holt':
        trend = Holt(trend_data[f'nights_{region}']).fit(optimized=True)
        trend_forecast = pd.DataFrame(trend.predict(start=0, end=11).values)

    elif trend_model == 'ARIMA':
        trend = auto_arima(trend_data[f'nights_{region}'], 
                     start_p=0, start_q=0, max_p=20, max_q=20, m=12, 
                     start_P=0, max_P=20, seasonal=True, max_d=2, max_D=2, 
                     trace=False, error_action='ignore', suppress_warnings=True, 
                     stepwise=True, random=True, random_state=42) 
        trend_forecast = pd.DataFrame(trend.predict(n_periods=12))

    # --------Residual prediction (VAR)----------
    resid_model = VAR(residual_data, dates=residual_data.index)    
    resid_results = resid_model.fit(lag)
    lag_order = resid_results.k_ar
    resid_forecast = pd.DataFrame(
        resid_results.forecast(residual_data.values[-lag_order:], 12))

    # --------Recomposing results----------
    forecast = seasonal_forecast[0] * trend_forecast[0] * resid_forecast[0]

    # --------Calculated error measures for each CV step----------
    rmse_test = np.sqrt(mean_squared_error(test_data, forecast))
    mae_test = mean_absolute_error(test_data, forecast)
    years_train_data = len(train_data.index.strftime('%Y').unique().tolist())

    test = pd.Series(test_data[:,0],
                     index=target[f'nights_{region}'][testing_period].index)
    forecast.index = target[f'nights_{region}'][testing_period].index
    
    mase = seasonal_MASE(target[str(int(end_year[:4]) - 1)], target[end_year[:4]], forecast[0])
    max_error = np.max(np.abs(forecast - test)) / test[forecast.idxmax(
        np.abs(forecast - test))]
    new_row = {
        'region': region,
        'lag': lag,
        'periods': years_train_data,
        'trend_model': trend_model, 
        'with_gdp': with_gdp,
        'MAE': mae_test,
        'RMSE': rmse_test,
        'Max_error': max_error,
        'MASE': mase,
    }
    error_df = error_df.append(new_row, ignore_index=True)",0.4314538538,
381,creating dataframe structures,"testing_set = [(r'.\lingspam_public\processed\ham_test\*', 0),
               (r'.\lingspam_public\processed\spam_test\*', 1)]

class analyze:
    def __init__(self):
        # total number of mails
        self.total = 0
        # Dict containing counts corresponding to each category
        self.category = [0]*4
        # List of mails which were misclassified
        self.misclassified = []
        self.accuracy = 0.0
        self.precision = 0.0
        self.recall = 0.0
        self.Fscore = 0.0
    
    def calculate(self):
        '''Calculate statistics'''
        self.accuracy = ((self.category[3] + self.category[0])*
                        100/self.total)
        self.precision  = (self.category[3]/
                          (self.category[3] + self.category[2]))
        self.recall = (self.category[3]/
                      (self.category[3] + self.category[1]))
        self.Fscore = (2*self.precision*self.recall/
                       (self.precision+self.recall)) 
        
    def __repr__(self):
        str_show = """"
        str_show += 'Total mails : ' + str(self.total)
        str_show += '\n' + 'Accuracy : ' + ('%.2f' % self.accuracy)
        str_show += '\n' + 'Precision : ' + ('%.2f' % self.precision)
        str_show += '\n' + 'Recall : ' + ('%.2f' % self.recall)
        str_show += '\n' + 'F Score : ' + ('%.4f' % self.Fscore)
        return str_show

def categorize(predicted, actual):
    '''Categories:
    00: True Negative
    01: False Negative
    10: False Positive
    11: True Positive 
    '''
    return predicted*2 + actual

def test_model(model, emails): 
    res = analyze()   
    for path, is_spam in emails:
        for path_name in glob.iglob(path, recursive=True):
            with open(path_name, ""r"") as f:
                categ = categorize(model.classify(f.read(),
                                                  preprocess=False),
                                   is_spam)
                res.category[categ] += 1
                if categ==1 or categ==2:
                    res.misclassified.append(path_name)
                res.total += 1 
    res.calculate()
    return res",0.430238843,
381,creating dataframe structures,"import geode
from collections import OrderedDict
d_comparison_cd = {} # To store top up/down genes
cd_results = pd.DataFrame(index=expr_df.index) # To store the full CD vectors

sample_classes = OrderedDict() # Specify control and treatment conditions

# The 5 comparisons
comparisons = [('Advanced_DN', 'Ctrl_Batch_A'), 
               ('Early_DN', 'Ctrl_Batch_B')]

for comp in comparisons:
    comp_name = '%s-vs-%s' % comp
    sample_classes[comp_name] = (meta_df['Group'].values==comp[0]).astype(int) + (meta_df['Group'].values==comp[1]).astype(int) * 2
    
# print(sample_classes)
# for comp, sample_class in sample_classes.items():
#     print(comp)
#     print(sample_class)

for comp, sample_class in sample_classes.items():
    print(comp)
    cd_res = geode.chdir(expr_df.values, sample_class, expr_df.index, gamma=.5, sort=False, calculate_sig=False)
    cd_coefs = np.array(list(map(lambda x: x[0], cd_res)))
    cd_results[comp] = cd_coefs
    
    # sort CD in by absolute values in descending order
    srt_idx = np.abs(cd_coefs).argsort()[::-1]
    cd_coefs = cd_coefs[srt_idx][:600]
    sorted_DEGs = expr_df.index[srt_idx][:600]
    # split up and down
    up_genes = dict(zip(sorted_DEGs[cd_coefs > 0], cd_coefs[cd_coefs > 0]))
    dn_genes = dict(zip(sorted_DEGs[cd_coefs < 0], cd_coefs[cd_coefs < 0]))
    d_comparison_cd[comp+'-up'] = up_genes
    d_comparison_cd[comp+'-dn'] = dn_genes",0.4300780892,
381,creating dataframe structures,"ctab1 = {}
for i in range(0, 5):
    ctab1[countryNames[i]]= pd.crosstab(
        index=countries[i]['EDATTAIN'],
        columns=countries[i]['SEX'],
        values=countries[i]['INCTOT'],
        aggfunc=np.mean
    ).round(2)
    ctab1[countryNames[i]][""Difference""] = ctab1[countryNames[i]][""Male""] - ctab1[countryNames[i]][""Female""]
    
ctab2 = {}
for i in range(0, 5):
    ctab2[countryNames[i]]= pd.crosstab(
        index=countries[i]['EDATTAIN'],
        columns=countries[i]['SEX'],
        values=countries[i]['INCTOT'],
        aggfunc=np.size
    ).apply(lambda x: x/np.sum(x)).round(2)
    ctab2[countryNames[i]][""Difference""] = ctab2[countryNames[i]][""Male""] - ctab2[countryNames[i]][""Female""]

ctab3 = {}
for i in range(0, 5):
    ctab3[countryNames[i]]= pd.crosstab(
        index=countries[i]['INDGEN'],
        columns=countries[i]['SEX'],
        values=countries[i]['INCTOT'],
        aggfunc=np.mean
    ).round(2)
    ctab3[countryNames[i]][""Difference""] = ctab3[countryNames[i]][""Male""] - ctab3[countryNames[i]][""Female""]
    ctab3[countryNames[i]] = ctab3[countryNames[i]].sort_values(""Difference"", ascending=False)
    
ctab4 = {}
for i in range(0, 5):
    ctab4[countryNames[i]]= pd.crosstab(
        index=countries[i]['INDGEN'],
        columns=countries[i]['SEX'],
        values=countries[i]['INCTOT'],
        aggfunc=np.size
    ).apply(lambda x: x/np.sum(x)).round(2)
    ctab4[countryNames[i]][""Difference""] = ctab4[countryNames[i]][""Male""] - ctab4[countryNames[i]][""Female""]
    ctab4[countryNames[i]] = ctab4[countryNames[i]].reindex(ctab3[countryNames[i]].index)",0.4292863905,
1348,pandas and memory,"df = bq_assistant.query_to_pandas_safe(QUERY)
print('Size of dataframe: {} Bytes'.format(int(df.memory_usage(index=True, deep=True).sum())))",0.4120061994,
1348,pandas and memory,"q = """"""
SELECT  TIMESTAMP_MILLIS((timestamp - MOD(timestamp,
          86400000))) as Timestamp , count(Timestamp) as output_count from 
    `bigquery-public-data.bitcoin_blockchain.transactions` group by timestamp
""""""
print (str(round((bq_assistant.estimate_query_size(q)),2))+str("" GB""))
transaction_count=bq_assistant.query_to_pandas(q)
transaction_count=transaction_count.sort_values(by=""Timestamp"")
transaction_count.head()",0.4012998343,
1348,pandas and memory,"q = """"""
SELECT  TIMESTAMP_MILLIS((timestamp - MOD(timestamp,
          86400000))) as Timestamp, sum(o.output_satoshis) as output_price from 
    `bigquery-public-data.bitcoin_blockchain.transactions`JOIN
    UNNEST(outputs) as o group by timestamp
""""""
print (str(round((bq_assistant.estimate_query_size(q)),2))+str("" GB""))
results3=bq_assistant.query_to_pandas(q)
results3[""output_price""]=results3[""output_price""].apply(lambda x: float(x/100000000))
results3=results3.sort_values(by=""Timestamp"")
results3.head()",0.4007393718,
1348,pandas and memory,"# Get a pandas dataframe for the cell tally data
df = tally.get_pandas_dataframe()

# Print the first twenty rows in the dataframe
df.head(100)",0.3991112113,
1348,pandas and memory,"''Merging the Training and the Test Dataset before creating the dummies'''
train_test_merged = train_raw.append(test_raw)
train_test_merged.head(2)",0.3967605829,
1348,pandas and memory,"data = sm.datasets.engel.load_pandas().data
data.head(10)",0.3964053988,
1348,pandas and memory,"# import comments
path_to_data = '../data/reddit_comments_askDocs_2014_to_2018_03.gz'
df_comments = load_comments(path_to_data)
df_comments.head(2)",0.3931720853,
1348,pandas and memory,"q = """"""
SELECT column_name, data_type FROM information_schema.columns
WHERE table_name = 'listings'
""""""
datatypes = query(q)
datatypes.head(2)",0.3931720853,
1348,pandas and memory,"df = flux.get_pandas_dataframe()
df.head(10)",0.3927984238,
1348,pandas and memory,"df = scattering.get_pandas_dataframe()
df.head(10)",0.3927984238,
2191,step what is the data type of each column?,"def cols_of_dtype(df, dtype):
    return sorted([c for c in df.columns if df[c].dtype == dtype])",0.4526847601,
2191,step what is the data type of each column?,"def change_datatype(df):
    float_cols = list(df.select_dtypes(include=['int']).columns)
    for col in float_cols:
        if ((np.max(df[col]) <= 127) and(np.min(df[col] >= -128))):
            df[col] = df[col].astype(np.int8)
        elif ((np.max(df[col]) <= 32767) and(np.min(df[col] >= -32768))):
            df[col] = df[col].astype(np.int16)
        elif ((np.max(df[col]) <= 2147483647) and(np.min(df[col] >= -2147483648))):
            df[col] = df[col].astype(np.int32)
        else:
            df[col] = df[col].astype(np.int64)",0.4431103766,
2191,step what is the data type of each column?,"# Transforming categorical variables
def label_transform(df):
    for col in df.columns:
        if df[col].dtypes == object:
            le = LabelEncoder()
            df[col] = le.fit_transform(df[col])
    return df
# label_transform(df)


# Check ratio of Contracts vs. No-Contracts (approx. 1200 to 700)
# df[df['wb_k'] == True].count()",0.4417154491,
2191,step what is the data type of each column?,"def change_datatype(df):
    float_cols = list(df.select_dtypes(include=['int']).columns)
    for col in float_cols:
        if ((np.max(df[col]) <= 127) and(np.min(df[col] >= -128))):
            df[col] = df[col].astype(np.int8)
        elif ((np.max(df[col]) <= 32767) and(np.min(df[col] >= -32768))):
            df[col] = df[col].astype(np.int16)
        elif ((np.max(df[col]) <= 2147483647) and(np.min(df[col] >= -2147483648))):
            df[col] = df[col].astype(np.int32)
        else:
            df[col] = df[col].astype(np.int64)

change_datatype(df_pokemon)",0.4413202405,
2191,step what is the data type of each column?,"def change_datatype(df):
    float_cols = list(df.select_dtypes(include=['int']).columns)
    for col in float_cols:
        if ((np.max(df[col]) <= 127) and(np.min(df[col] >= -128))):
            df[col] = df[col].astype(np.int8)
        elif ((np.max(df[col]) <= 32767) and(np.min(df[col] >= -32768))):
            df[col] = df[col].astype(np.int16)
        elif ((np.max(df[col]) <= 2147483647) and(np.min(df[col] >= -2147483648))):
            df[col] = df[col].astype(np.int32)
        else:
            df[col] = df[col].astype(np.int64)

change_datatype(df_train)
change_datatype(df_test)",0.4393761158,
2191,step what is the data type of each column?,"def change_datatype(df):
    float_cols = list(df.select_dtypes(include=['float']).columns)
    for col in float_cols:
        if ((np.max(df[col]) <= 127) and(np.min(df[col] >= -128))):
            df[col] = df[col].astype(np.int8)
        elif ((np.max(df[col]) <= 32767) and(np.min(df[col] >= -32768))):
            df[col] = df[col].astype(np.int16)
        elif ((np.max(df[col]) <= 2147483647) and(np.min(df[col] >= -2147483648))):
            df[col] = df[col].astype(np.int32)
        else:
            df[col] = df[col].astype(np.int64)

change_datatype(train_merged)
change_datatype(test_merged)",0.4393761158,
2191,step what is the data type of each column?,"# Utility function to change case
def change_type(df):
    df.columns = df.columns.str.upper()
    for col in df.columns:
        if df[col].dtypes == 'object':
            df[col] = df[col].str.upper()
    return df


# Add country specific profile from dataset from GAPMINDER project
data_url = 'http://www.stat.ubc.ca/~jenny/notOcto/STAT545A/examples/gapminder/data/gapminderDataFiveYear.txt'
gapmndr_ctry = pd.read_csv(data_url, sep='\t')
gapmndr_ctry.drop(['year','pop', 'lifeExp', 'gdpPercap'], axis=1, inplace=True)
gapmndr_ctry.columns = ['COUNTRY_NAME', 'CONTINENT_NAME']
gapmndr_ctry = change_type(gapmndr_ctry)
# print(gapmndr_ctry.head(3))


# Update `augmented_ip_geodata` with `gapmndr_ctry` with `augmented_ip_geodata` prioritized.
df_augmented_ip_geodata = df_augmented_ip_geodata.combine_first(gapmndr_ctry)
df_augmented_ip_geodata.head()",0.4387693107,
2191,step what is the data type of each column?,"def dummy_variables(df):
    df_type = df.dtypes
    for col in df_type.keys():
        if df_type[col] == 'object':
            df = pd.concat([df, pd.get_dummies(df[col]).rename(columns=lambda x: col + '_' + str(x))], axis=1)
            df.drop([col], axis=1, inplace=True)
    return df",0.4379043579,
2191,step what is the data type of each column?,"#--- for integer type columns ---
def change_datatype(df):
    float_cols = list(df.select_dtypes(include=['int']).columns)
    for col in float_cols:
        if ((np.max(df[col]) <= 127) and(np.min(df[col] >= -128))):
            df[col] = df[col].astype(np.int8)
        elif ((np.max(df[col]) <= 32767) and(np.min(df[col] >= -32768))):
            df[col] = df[col].astype(np.int16)
        elif ((np.max(df[col]) <= 2147483647) and(np.min(df[col] >= -2147483648))):
            df[col] = df[col].astype(np.int32)
        else:
            df[col] = df[col].astype(np.int64)

#--- for float type columns ---
def change_datatype_float(df):
    float_cols = list(df.select_dtypes(include=['float']).columns)
    for col in float_cols:
        df[col] = df[col].astype(np.float32)",0.4346630573,
2191,step what is the data type of each column?,"def change_datatype(df):
    int_cols = list(df.select_dtypes(include=['int']).columns)
    for col in int_cols:
        if ((np.max(df[col]) <= 127) and(np.min(df[col] >= -128))):
            df[col] = df[col].astype(np.int8)
        elif ((np.max(df[col]) <= 32767) and(np.min(df[col] >= -32768))):
            df[col] = df[col].astype(np.int16)
        elif ((np.max(df[col]) <= 2147483647) and(np.min(df[col] >= -2147483648))):
            df[col] = df[col].astype(np.int32)
        else:
            df[col] = df[col].astype(np.int64)
            
def change_datatype_float(df):
    float_cols = list(df.select_dtypes(include=['float']).columns)
    for col in float_cols:
        df[col] = df[col].astype(np.float32)",0.4327005744,
500,download and visualize main shock,"def load_next():
    """"""
    Example showing how to:
    
      (1) Initialize client
      (2) Load first study 
      (3) Load random study
      
    """"""
    client = init_client()
    
    vol = client.load_next(random=False)
    assert vol['dat'].shape == (1, 28, 28, 1)
    
    vol = client.load_next(random=True)
    assert vol['dat'].shape == (1, 28, 28, 1)
    
%time load_next()",0.4926033616,
500,download and visualize main shock,"def load_data_shared(filename=""../../neural-networks-and-deep-learning/data/mnist.pkl.gz""):
    f = gzip.open(filename, 'rb')
    training_data, validation_data, test_data = cPickle.load(f)
    f.close()
    def shared(data):
        """"""Place the data into shared variables.  This allows Theano to copy
        the data to the GPU, if one is available.

        """"""
        shared_x = theano.shared(
            np.asarray(data[0], dtype=theano.config.floatX), borrow=True)
        shared_y = theano.shared(
            np.asarray(data[1], dtype=theano.config.floatX), borrow=True)
        return shared_x, T.cast(shared_y, ""int32"")
    return [shared(training_data), shared(validation_data), shared(test_data)]",0.4846166074,
500,download and visualize main shock,"def load_and_prepare_data():
    print('Loading data...')
    
    # Keras provides MNIST dataset as two tuples for training and testing sets.
    # https://keras.io/datasets/#mnist-database-of-handwritten-digits
    (X_train, y_train), (X_test, y_test) = mnist.load_data()
    
    print('Done.')
    print('Preparing data...')

    # The input datasets should have values between 0-1 instead of having a range of 0-255.
    # This requires typecasting them from int to float at first.
    X_train = X_train.astype('float32')
    X_test = X_test.astype('float32')
    X_train /= 255
    X_test /= 255

    # The output datasets should be converted from simply having a value between 0-9 to having
    # a matrix of categorical one-hot encoding. Example:
    # 2 -> [0 0 1 0 0 0 0 0 0 0]
    # 9 -> [0 0 1 0 0 0 0 0 0 9]
    y_train = np_utils.to_categorical(y_train, 10)
    y_test = np_utils.to_categorical(y_test, 10)

    # The MINST input is 28x28 grayscale images. Let's convert them to single row vectors.
    X_train = np.reshape(X_train, (60000, 784))
    X_test = np.reshape(X_test, (10000, 784))

    print('Done.')
    return [X_train, X_test, y_train, y_test]",0.4786312878,
500,download and visualize main shock,"def test():
    origin_data_array, origin_label_array = load_all_data_label([arrayds_dir + 'origin.ndarray'])
    origin_data_array, origin_label_array = origin_data_array[:128], origin_label_array[:128]
    origin_ds = MyArrayDataset2([origin_data_array, origin_label_array], transform=_transform_test)
    origin_data = gluon.data.DataLoader(origin_ds, 128, shuffle=False, num_workers=0)
    
    datas = [[origin_data_array, origin_data_array.copy()], [origin_label_array, origin_label_array.copy()]]
    train_ds = MyArrayDataset2(datas, transform=_transform_test)
    train_data = gluon.data.DataLoader(train_ds, 128, shuffle=False, num_workers=2)
    
    print 'origin-train[:half]:', nd.mean(origin_ds._data[0] - train_ds._data[0][0]).asscalar()
    print 'origin-train[half:]:', nd.mean(origin_ds._data[0] - train_ds._data[0][1]).asscalar()
    data1 = None
    for d, l in train_data:
        if data1 is None:
            data1 = d.copy()
        else:
            print nd.mean(nd.abs(d - data1)).asscalar()
    
    net = ResNet(10)
    net.load_params('../../models/resnet18_me_backgrad_prob_online_iter10_lr01', ctx=ctx)
    # net.load_params('../../models/resnet18_me_200e', ctx=ctx)
    print utils.evaluate_accuracy(train_data, net, ctx)
    
    a, b, c, d = BGG.generate_data_for_out(net, origin_data, max_iters=10, lr=0.1,use_batch_mean_std=True, use_statistic=True, show_per_iters=150,
                                loss_f = gluon.loss.SoftmaxCrossEntropyLoss(), out_data=train_ds._data[0][1], begin_index=0)
    print ""\n"".join([str(a), str(b), str(c), str(d)])
    
    print utils.evaluate_accuracy(train_data, net, ctx)
    
    print 'origin-train[:half]:', nd.mean(origin_ds._data[0] - train_ds._data[0][0]).asscalar()
    print 'origin-train[half:]:', nd.mean(origin_ds._data[0] - train_ds._data[0][1]).asscalar()
    print nd.mean(origin_ds._data[0]).asscalar(), nd.mean(train_ds._data[0][0]).asscalar()
    print np.std(origin_ds._data[0].asnumpy()), np.std(train_ds._data[0][1].asnumpy())
    
    data1 = None
    for d, l in train_data:
        show_data(d[:10, :, :, :])
        if data1 is None:
            data1 = d.copy()
        else:
            print nd.mean(nd.abs(d - data1)).asscalar()
test()",0.4746142328,
500,download and visualize main shock,"def load_data():
    (x_train, y_train), (x_test, y_test) = mnist.load_data()
    
    # plot 4 images as gray scale
    plt.subplot(221)
    plt.imshow(x_train[0], cmap=plt.get_cmap('gray'))
    plt.subplot(222)
    plt.imshow(x_train[1], cmap=plt.get_cmap('gray'))
    plt.subplot(223)
    plt.imshow(x_train[2], cmap=plt.get_cmap('gray'))
    plt.subplot(224)
    plt.imshow(x_train[3], cmap=plt.get_cmap('gray'))
    # show the plot
    plt.show()
    
    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)
    x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)
    x_train = x_train.astype('float32')
    x_test = x_test.astype('float32')
    x_train /= 255
    x_test /= 255
    y_train = keras.utils.to_categorical(y_train, 10)
    y_test = keras.utils.to_categorical(y_test, 10)
    return x_train, y_train, x_test, y_test",0.4721145034,
500,download and visualize main shock,"#def getCatPositions(filename = ""/Users/jmsantander/fermi/cats/gll_psch_v13.fit""):
def getCatPositions(filename = ""gll_psch_v13.fit""):
    hdulist = pyfits.open(filename)
    h, data, columns = hdulist[1].header, hdulist[1].data, hdulist[1].columns
    try:
        return (data['RAJ2000'], data['DEJ2000'], data['ASSOC1'])
    except KeyError:
        return (data['RAJ2000'], data['DEJ2000'], data['ASSOC'])

def drawCircle(ax, ra, dec, radius, **kwargs):
    circle = pl.Circle((ra, dec), radius, **kwargs, fill=False)
    ax.add_artist(circle)",0.4639446139,
500,download and visualize main shock,"def load_pickled_data():
    word_dicts_path = './checkpointed_data/word_dicts.p'
    model_input_data_path = './checkpointed_data/model_input_data.p'
    vocab_to_int, int_to_vocab, word_embedding_matrix = pickle.load(open(word_dicts_path, mode='rb'))
    sorted_summaries, sorted_reviews = pickle.load(open(model_input_data_path, mode='rb'))
    return vocab_to_int, int_to_vocab, word_embedding_matrix, sorted_summaries, sorted_reviews",0.4633178413,
500,download and visualize main shock,"def load_dataset():
    # the data, shuffled and split between train and test sets
    (X_train, y_train), (X_test, y_test) = cifar10.load_data()
    
    # Allocate last 5000 training examples for validation.
    X_train, X_val = X_train[:-5000], X_train[-5000:]
    y_train, y_val = y_train[:-5000], y_train[-5000:]
    
    # convert class vectors to binary class matrices
    y_train = np_utils.to_categorical(y_train, nb_classes)
    y_test = np_utils.to_categorical(y_test, nb_classes)
    y_val = np_utils.to_categorical(y_val, nb_classes)
    
    # preprocess data
    X_train = X_train.astype('float32')
    X_test = X_test.astype('float32')
    X_val = X_val.astype('float32')
    X_train /= 255
    X_test /= 255
    X_val /= 255
    
    print('X_train shape:', X_train.shape)
    print(X_train.shape[0], 'train samples')
    print(X_test.shape[0], 'test samples')
    print(y_train.shape[0], 'training labels')
    print(y_test.shape[0], 'test labels')
    print(X_val.shape[0], 'validation samples')
    print(y_val.shape[0], 'validation labels')

    return X_train, y_train, X_test, y_test, X_val, y_val",0.4630573392,
500,download and visualize main shock,"def countycrimeRates3d():
    import matplotlib.image as mpimg
    img = mpimg.imread('../county3d.png')
    fig = plt.figure(figsize = (30,10)) 
    ax3 = fig.add_subplot(111)
    ax3.imshow(img, interpolation='none')
    plt.show()",0.4621030688,
500,download and visualize main shock,"def load_data():
    """"""
    This function loads the MNIST digit data using Keras inbuilt function \
    and returns the train and test set
    """"""
    
    (X_train, y_train), (X_test, y_test) = mnist.load_data()
    return (X_train, y_train), (X_test, y_test)",0.4620195627,
2207,step what was the most ordered item in the choice_description column?,"def elongate_description_items(df):
    desc = df.choice_description.iloc[0]
    if not type(desc) == str:
        desc = 'None'
        desc_len = 1
    else:
        desc = desc.replace('[','').replace(']','')
        desc = [item.strip() for item in desc.split(',')]
        desc_len = len(desc)
    
    long_order = pd.DataFrame({
            'order_customizations':desc,
            'order_customization_id':np.arange(desc_len)
        })
    
    for col in df.columns:
        long_order[col] = df[col].iloc[0]
        
    return long_order

chip_long = chip.groupby(['order_id','sub_order_id']).apply(elongate_description_items).reset_index(drop=True)",0.5495940447,
2207,step what was the most ordered item in the choice_description column?,"# Loop through each parameter
for param in params_list:
    if param.is_choice_param():        # If the parameter is a choice param
        print( param.get_name() )      # Print the parameter's name
        
        choices = param.get_choices()  # Get a list of valid choices 
        for choice in choices:         # Print the label and value for each choice
            print( choice['label'] + "" = "" + choice['value'] )
            
        # Print the default selected value for each choice
        print( param.get_choice_selected_value() )",0.5199490786,
2207,step what was the most ordered item in the choice_description column?,"def highlight_fewer(s):
    """"""Helper function to highlight dataframe cells.""""""
    if s.name in ['Combos_with_min_longest', 'Combos_with_min_diameter']:
        return ['background-color: lime' if b else '' for b in min_longest_summary.Fewer_choices]
    else:
        return ['' for b in min_longest_summary.Fewer_choices]",0.5035443306,
2207,step what was the most ordered item in the choice_description column?,"def get_description(column_name, schema=schema):
    '''
    INPUT - schema - pandas dataframe with the schema of the developers survey
            column_name - string - the name of the column you would like to know about
    OUTPUT - 
            desc - string - the description of the column
    '''
    desc = schema[schema.Column == column_name].Question.values[0]
    return desc

#test your code
#Check your function against solution - you shouldn't need to change any of the below code
get_description(df.columns[0]) # This should return a string of the first column description",0.497023046,
2207,step what was the most ordered item in the choice_description column?,"def get_top10(row):
    sort = row.sort_values(ascending=False)
    return ' '.join(sort.index[:10])",0.4948096871,
2207,step what was the most ordered item in the choice_description column?,"def majority_class(neighbors):
    """"""Return the class that's most common among all these neighbors.""""""
    return neighbors.group('Class').sort('count', descending=True).column('Class').item(0)",0.4915910065,
2207,step what was the most ordered item in the choice_description column?,"def getDfSummary(input_data):
    output_data = input_data.describe().transpose()
    output_data[""spread""] = output_data[""max""] - output_data[""min""]
    # Place your code here
    return output_data",0.4893590212,
2207,step what was the most ordered item in the choice_description column?,"def identify_topic(row):
    scores = row.order(ascending=False)
    if scores[0] - scores[1] >= 1.0:
        label = scores.index[0]
        # remove _score
        return label[:-6]
    else:
        return np.NaN

score_cols = ['taste_score', 'palate_score', 'aroma_score', 'appearance_score']
beer['topic'] = beer[score_cols].apply(identify_topic, axis=1)",0.487622261,
2207,step what was the most ordered item in the choice_description column?,"def ancient_langs(data):
    '''
    Input: asjp languages metadata 
    Output: a list of ancient language names
    '''
    #store names of languages that are extinct
    names = data[data.long_extinct == True].name.values
    
    #replace spaces by '_' and changes everything to uppercase
    names = [re.sub(r' ', r'_', name).upper() for name in names] 
    
    print(""%d ancient languages stored"" % len(names))
    print(names)
    print(""\n"")
    
    return [(x, 'ancient') for x in names]
    
def recently_extinct(data):
    '''
    Input: asjp language metadata 
    Output: a list of names of recently extinct languages
    '''
    #store names of languages that are extinct
    names = data[data.recently_extinct == True].name.values
    
    #replace spaces by '_' and changes everything to uppercase
    names = [re.sub(r' ', r'_', name).upper() for name in names] 
    
    print(""%d recently extinct languages stored"" % len(names))
    print(names)
    print(""\n"")
    
    return [(x, 'recently_extinct') for x in names]

def new_languages(data):
    '''
    Input: asjp language metadata 
    Output: a short subset of list of names of recent languages (non-extinct)
    '''
    random.seed(12)
    
    #store names of languages that are extinct
    names = data[(data.recently_extinct == False) & \
                (data.long_extinct == False)].name.values
    
    #Select random 10 language names (as the whole list is too long)
    random.shuffle(names)
    names = names[:13]
    
    #replace spaces by '_' and changes everything to uppercase
    names = [re.sub(r' ', r'_', name).upper() for name in names] 
    
    random.shuffle(names)
    
    print(""%d new languages stored"" % len(names))
    print(names)
    print(""\n"")
    
    return [(x, 'non_extinct') for x in names]

def vocab(language):
    '''
    Input: language name (a string) 
    Output: list of words in that language
    '''
    vocab = set(list(lang_data[language].Parameter_name.values))
    
    print(""%d words of %s stored"" % (len(vocab), language))
    
    return list(vocab)",0.4826599956,
2207,step what was the most ordered item in the choice_description column?,"def getDfSummary(input_data):
    output_data = input_data.describe()
    output_data = output_data.transpose()
    output_data['spread'] = output_data['max'] - output_data['min']
    
    return output_data",0.4807189107,
1879,selecting features,"def select_features(data, data_std):
    mean_vec = np.mean(data_std, axis=0)
    cov_mat = (data_std - mean_vec).T.dot((data_std - mean_vec)) / (data_std.shape[0]-1)
    #print('Covariance matrix \n%s' %cov_mat)

    eig_vals, eig_vecs = np.linalg.eig(cov_mat)
    cor_mat1 = np.corrcoef(data_std.T)

    eig_vals, eig_vecs = np.linalg.eig(cor_mat1)

    #print('Eigenvectors \n%s' %eig_vecs)
    #print('\nEigenvalues \n%s' %eig_vals)

    cor_mat2 = np.corrcoef(data.T)

    eig_vals, eig_vecs = np.linalg.eig(cor_mat2)
    u,s,v = np.linalg.svd(data_std.T)

    # Make a list of (eigenvalue, eigenvector) tuples
    eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]

    #eig_pairs.sort()
    #eig_pairs.reverse()
    
    #print('Eigenvalues in descending order:')
    for index, i in enumerate(eig_pairs):
        #if index<=2:
            print ""Eigen Value : "" + str(i[0])# max(i[1]), np.mean(i[1]), np.std(i[1]), min(i[1]) )
            print data_std.columns[np.abs(i[1]) > 0.25]",0.4573353231,
1879,selecting features,"def rank_features_with_header(X_train,y_train):
    lr = LinearRegression()
    rfe = RFE(lr, n_features_to_select=1)
    selector = rfe.fit(X_train,y_train)
    rank = selector.ranking_
    
    headers = list(X_train.head(0))

    l = [i for i in range(1,len(headers)+1)]
    a = np.array(l)

    feature_sequence = []
    for i in a:
        index = np.where(rank==i)
        k = list(index)
        position = int(k[0])
        feature_sequence.append(headers[position])

    return feature_sequence",0.4419406652,
1879,selecting features,"def getOptimalFeatures(alg):
    scores = []
    for feat_count in range(1, 60):
        rfe_alg = RFE(estimator=alg, n_features_to_select=feat_count, step=1)
        rfe_alg.fit(X_train, y_train)
        rfe_score = rfe_alg.score(X_test, y_test)
        print(feat_count)

        scores.append({'feat_count': feat_count, 'score': rfe_score})
        
    scores = sorted(scores, key=lambda x: x['score'], reverse=True)
    return scores",0.438947618,
1879,selecting features,"print ""Selected features:"", indices[:8]    # [48, 9, 71, 53, 70, 101, 49, 8]",0.4375004768,
1879,selecting features,"# # Feature Selection Class # #

class Feature_Selector:
    """"""This transformer is not compatible with the current sklearn Pipeline.
    
    This feature selection transformer works only with the following boosters as base learners:
        -gbtree
        -dart
                                                                            
    Sklearn.metrics scoring functions can be passed to parameter:
        scorer = 'scoring_function'
    
    Built-in XGBoost evaluation metrics will be used by passing:
        scorer = 'Auto'                                                    
    
    This class requires that X (features or independent variables) has already been encoded and Imputed.
    
    Tuning requires the target (y) to also be passed.
        If target = 'insert target name' is passed, X = dataframe disregarding position of target column.
        If the fit_transform method is given one positional argument (X) and target=None, 
            the class assumes that it is a DataFrame with the last
            column as the target variable and the rest as the features.""""""
    
    # Compatible model evaluation metrics: Sklearn metrics
    scoring_functions = {'accuracy_score':accuracy_score,'f1_score':f1_score, 'hamming_loss':hamming_loss,
        'jaccard_similarity_score':jaccard_similarity_score, 'log_loss':log_loss, 'matthews_corrcoef':matthews_corrcoef,
                 'precision_score':precision_score, 'recall_score':recall_score, 'zero_one_loss':zero_one_loss,
                'explained_variance_score':explained_variance_score, 'mean_absolute_error':mean_absolute_error,
                 'mean_squared_error':mean_squared_error, 'mean_squared_log_error':mean_squared_log_error,
                 'median_absolute_error':median_absolute_error, 'r2_score':r2_score}

    classification_objectives = ['reg:logistic','binary:logistic', 'binary:logitraw',
                                'multi:softmax', 'multi:softprob', 'rank:pairwise' ]
    regression_objectives = ['reg:linear','count:poisson','reg:gamma', 'reg:tweedie']
    
    # metrics with which higher value = higher model performance
    the_higher_the_better = ['accuracy_score','f1_score','jaccard_similarity_score',
                                  'precision_score','recall_score',
                                'explained_variance_score','r2_score','Best test auc',
                                'Best test ndcg','Best test map']
    # metrics with which lower value = higher model performance
    the_lower_the_better = ['hamming_loss', 'log_loss','matthews_corrcoef','zero_one_loss','mean_absolute_error',
                           'mean_squared_error','mean_squared_log_error','median_absolute_error',
                           'Best test error','Best test rmse','Best test mae','Best test log loss',
                           'Best test merror','Best test mlogloss']

    # metrics that require encoded target variable
    need_encoded_y = ['recall_score','precision_score','f1_score']
    
    booster='N/A'
    best_params='N/A'
    objective='N/A'
    random_state='N/A'
    test_size='N/A'
    scorer='N/A'

    def __init__(self, booster='gbtree', objective='reg:logistic', 
                 random_state=69, test_size=0.2, best_params={}, scorer='Auto',
                
                 # values passed for these parameters will never be used unless: class.parameter is made None
                 scoring_functions=None, classification_objectives=None, regression_objectives=None,
                the_higher_the_better=None, the_lower_the_better=None, need_encoded_y=None):
        
        
        # assigning parameters as instance variables
        varses = list(vars(Feature_Selector).keys())
        self.variables = varses[len(varses) - varses[::-1].index('__doc__') : varses.index('__init__')]
        class_name = ""Feature_Selector""+"".""
        for v in self.variables:
            # if the class variable for the argument is not empty, assign its value as the instance variable
            if eval(""%s != 'N/A'"" % (class_name+v)) is True:
                exec(""self.%s = %s"" % (v,class_name+v))
            # if the class variable is empty, assign to instance the value passed as argument during instantiation
            else:
                exec(""self.%s = %s"" % (v, v))
        
    
        # overriding colsample parameters
        self.best_params['colsample_bytree'] = 1
        self.best_params['colsample_bylevel'] = 1
        
        # determining the model build used for feature selection
        if self.booster == 'gbtree':
            if self.objective in Feature_Selector.classification_objectives:
                self.best_params['objective'] = self.objective
                if 'booster' in list(self.best_params.keys()):
                    del self.best_params['booster']
                self.model = xgb.XGBClassifier(**self.best_params)
            elif self.objective in Feature_Selector.regression_objectives:
                self.best_params['objective'] = self.objective
                if 'booster' in list(self.best_params.keys()):
                    del self.best_params['booster']
                self.model = xgb.XGBRegressor(**self.best_params)
        elif self.booster == 'dart':
            self.best_params['booster'] = self.booster
            self.best_params['objective'] = self.objective
        
        
        # making sure there is a scorer
        if self.scorer not in Feature_Selector.the_higher_the_better\
                            and scorer not in Feature_Selector.the_lower_the_better:
            self.scorer = 'Auto'
        else:
            self.performance_scores = None
            self.best_perf = None
            self.best_threshold = None
    
    def fit(self, X, y=None, target=None):
        if y is None:
            if target is None:
                y = pd.DataFrame(X.iloc[:,-1])
                X = pd.DataFrame(X.drop(X.columns[[-1,]], axis=1))
            else:
                y = pd.DataFrame(X[target])
                X = X.drop(self.target, axis =1)
        
        # Encode labels if string when needed
        if ((self.booster == 'dart' or self.booster == 'gblinear')\
                        and self.objective in Feature_Selector.classification_objectives)\
                                                                or self.scorer in self.need_encoded_y:
            if y.iloc[:,0].dtype == object:
                Target_is_string = True
            else:
                Target_is_string = False
            
            if Target_is_string == True:
                L_E = LabelEncoder()
                L_E = L_E.fit(y.iloc[:,0].astype(str))
                y.iloc[:,0] = L_E.transform(y.iloc[:,0].astype(str))

        # fit model on all training data            
        X_train, X_test, y_train, y_test = train_test_split(X, y,
                                            test_size=self.test_size, random_state=self.random_state, stratify=y)
        
        # Initialize lists of building performance metrics table
        Threshold = []
        ns = []
        Scores = []
        
        # tree boosters: feature selection
        if self.booster == 'gbtree':
            self.test_model = self.model
            self.test_model.fit(X_train, y_train.values.ravel())

            # Fit model using each importance as a threshold
            thresholds = sorted(self.test_model.feature_importances_)
            
            for thresh in thresholds:
                # select features using threshold
                selection = SelectFromModel(self.test_model, threshold=thresh, prefit=True)
                select_X_train = selection.transform(X_train)
                select_X_test = selection.transform(X_test)
                # train model
                if self.objective in Feature_Selector.classification_objectives:
                    estimator = xgb.XGBClassifier
                if self.objective in Feature_Selector.regression_objectives:
                    estimator = xgb.XGBRegressor
                selection_model = estimator(**self.best_params)
                selection_model.fit(select_X_train, y_train.values.ravel(),
                                   eval_set=[(select_X_test,y_test.values.ravel())], verbose=False)
                
                # eval model
                # using built-in evaluation metrics automatically matched with objective
                if self.scorer == 'Auto':
                    result = selection_model.evals_result()
                    scorer_used = list(result['validation_0'].keys())[0]
                    #score = np.mean(result['validation_1'][scorer_used])
                    score = result['validation_0'][scorer_used][-1]
                    Scores.append(score)
                    ns.append(len(pd.DataFrame(select_X_train).columns))
                    Threshold.append(thresh)

                # using Sklearn metrics
                else:
                    scoring_function = Feature_Selector.scoring_functions[self.scorer]
                    select_X_test = selection.transform(X_test)
                    y_pred = selection_model.predict(select_X_test)
                    if y_pred.dtype != object:
                        predictions = [round(value) for value in y_pred]
                        y_test_used = [round(value) for value in y_test.iloc[:,0]]
                    else:
                        predictions = y_pred
                        y_test_used = y_test.iloc[:,0]
                    score = scoring_function(y_test_used, predictions)
                    if self.scorer == 'accuracy_score':
                        score = score*100.00
                    Scores.append(score)
                    Threshold.append(thresh)
                    ns.append(select_X_train.shape[1])
        
        # dart boosters: feature selection
        elif self.booster == 'dart':
            dtrain = xgb.DMatrix(data=X_train, label=y_train)
            
            # train the dart booster model
            xg_reg = xgb.train(dtrain=dtrain, params=self.best_params, num_boost_round=10)
            importance = xg_reg.get_fscore()
            importance = sorted(importance.items(), key=operator.itemgetter(1), reverse=True)
            importance = pd.DataFrame(importance)
        
            for value in range(1, len(importance)):
                columns = importance.iloc[:,0][:value]
                dtrain_trim = xgb.DMatrix(data=X_train.loc[:,columns], label=y_train)
                dtest_trim = xgb.DMatrix(data=X_test.loc[:,columns], label=y_test)
                
                # eval model
                # using built-in evaluation metrics automatically matched with objective
                if self.scorer == 'Auto':
                    result = {}
                    selection_model = xgb.train(dtrain=dtrain_trim, params=self.best_params,
                                                evals=[(dtest_trim, 'eval')], evals_result=result,
                                               verbose_eval=False)
                    scorer_used = list(result['eval'].keys())[0]
                    #score = np.mean(result['eval'][scorer_used])
                    score = result['eval'][scorer_used][-1]
                    Scores.append(score)
                    ns.append(len(columns))
                    Threshold.append(importance.iloc[value-1,1])
                
                # using Sklearn metrics
                else:
                    scoring_function = Feature_Selector.scoring_functions[self.scorer]

                    selection_model = xgb.train(dtrain=dtrain_trim, params=self.best_params)
                    y_pred = selection_model.predict(dtest_trim)
                    if y_pred.dtype != object:
                        predictions = [round(value) for value in y_pred]
                        y_test_used = [round(value) for value in y_test.iloc[:,0]]
                    else:
                        predictions = y_pred
                        y_test_used = y_test.iloc[:,0]
                    score = scoring_function(y_test_used, predictions)
                    if self.scorer == 'accuracy_score':
                        score = score*100.00
                    Scores.append(score)
                    ns.append(len(columns))
                    Threshold.append(importance.iloc[value-1,1])
        
        # building table of performance scores
        if self.scorer == 'Auto':
            self.scorer = 'Best test ' + scorer_used
        self.performance_scores = pd.DataFrame()
        self.performance_scores['Threshold'] = Threshold
        self.performance_scores['n'] = ns
        self.performance_scores[self.scorer] = Scores

        # Best cut-off of top features: minumum number giving the max possible model performance 
        if self.scorer in Feature_Selector.the_higher_the_better:
            self.best_perf = self.performance_scores[self.performance_scores[self.scorer]\
                                                     == max(self.performance_scores[self.scorer])]
            self.best_threshold = self.best_perf[(self.best_perf['n'] == min(self.best_perf['n']))].drop_duplicates()
        elif self.scorer in Feature_Selector.the_lower_the_better:
            self.best_perf = self.performance_scores[self.performance_scores[self.scorer]\
                                                     == min(self.performance_scores[self.scorer])]
            self.best_threshold = self.best_perf[(self.best_perf['n'] == min(self.best_perf['n']))].drop_duplicates()
        
        # store the best number of features
        self.best_n = self.best_threshold.n.iloc[0]
        
        # saving selected columns for use on .predict()
        if self.booster == 'gbtree':
            self.selected_columns = X.columns[np.argsort(self.test_model.feature_importances_)\
                                          [-(self.best_n):]]
        if self.booster == 'dart':
            self.selected_columns = list(columns[:self.best_n])   
        
        # reverse label encoding
        if ((self.booster == 'dart' or self.booster == 'gblinear')\
                        and self.objective in Feature_Selector.classification_objectives)\
                                                                or self.scorer in self.need_encoded_y:
            if Target_is_string == True:
                y.iloc[:,0] = L_E.inverse_transform(y.iloc[:,0])
        
        return self
        
    def transform(self, X, y=None, target=None):
        if y is None:
            if target is None:
                y = pd.DataFrame(X.iloc[:,-1])
                X = pd.DataFrame(X.drop(X.columns[[-1,]], axis=1))
            else:
                y = pd.DataFrame(X[target])
                X = X.drop(target, axis =1)
        # Drop unwanted features
        X = X[self.selected_columns]
        
        return pd.DataFrame(X), pd.DataFrame(y)
    
    def fit_transform(self, X, y=None, target=None):
        self.fit(X, y, target)
        return self.transform(X, y, target)",0.4316240847,
1879,selecting features,"# SelectKBest for selecting top-scoring features

from sklearn import datasets
from sklearn.feature_selection import SelectKBest, chi2

iris = datasets.load_iris()
X, y = iris.data, iris.target

print(X.shape)

# Do feature selection
#  input is scoring function (here chi2) to get univariate p-values
#  and number of top-scoring features (k) - here we get the top 2
X_t = SelectKBest(chi2, k = 2).fit_transform(X, y)

print(X_t.shape)",0.4212758541,
1879,selecting features,"#Considering he labeled set of features 
X, y = features_labeled,target_labeled[""TOTAL_COST""]
estimator = LinearRegression()
selector = RFE(estimator, n_features_to_select = 5, step=1)
selector = selector.fit(X, y)
selector.support_",0.4210749567,
1879,selecting features,"print(selector.selected_feature_indices)
print(selector.selected_features)",0.4199552536,
1879,selecting features,"fet_ind=[]
def plot_important_features(rf,nFeatures) :
    global fet_ind
    plt.figure(figsize=(8, 4))
    fet_ind = np.argsort(rf.feature_importances_)[::-1]
    fet_imp = rf.feature_importances_[fet_ind][:nFeatures]
    feature_names = column_names_test.split("","")
    feature_names =feature_names[1:]
    feature_names_array=np.array(feature_names)[fet_ind]
    feature_names=feature_names_array[:nFeatures].tolist()
    
    print ""%d most important features are\n"" % nFeatures
    print fet_ind[:nFeatures]
    print feature_names
    
    #plot the bar chart for most important features 
    plt.bar(np.arange(len(fet_imp)), fet_imp, width=1, lw=2)
    plt.grid(False)
    xt = np.arange(len(fet_imp)+0.5)
    ax = plt.subplot(111)
    ax.set_xticks(xt)
    xlabels=feature_names_array.tolist()
    ax.set_xticklabels(xlabels, rotation=90)
    plt.xlim(0, len(fet_imp))

plot_important_features(rf,15)",0.4196513295,
1879,selecting features,"# both columns will have the same window-roi: the roi found from
# the first tracker in listTrackers - this should be the ""more correct"" one.
compareTrackers([diffGS[0]]
                ,diffTrackers
                ,roiSelectFunc=True
                ,bMarkedFrame=True
                ,bTrackScore=True
                ,bFirstTrackerRoi=True
                ,expand_factor=5.0
               )

# columns will have different window-roi: in this case,
# second column has a non-sensical roi and plots fail to
# be viewable
compareTrackers([diffGS[0]]
                ,diffTrackers
                ,roiSelectFunc=True
                ,bMarkedFrame=True
                ,bTrackScore=True
                ,bFirstTrackerRoi=False
                ,expand_factor=5.0
               )",0.419090569,
170,data grouping and aggregation,"cms17_stats = pd.pivot_table(cms_2017_opioids_fin,index=[""State"", ""Quarter"", ""startdate"", ""product_name"", ""DOSAGEFORMNAME"", ""LABELERNAME"", ""ROUTENAME"", ""DEASCHEDULE"", ""ACTIVE_NUMERATOR_STRENGTH"",""ACTIVE_INGRED_UNIT""], 
                                values=['units_reimb', 'num_rx', 'total_amt_reimb', 
                                        'med_amt_reimb', 'nonmed_amt_reimb', 'days_marketing'], 
               aggfunc={'units_reimb': lambda x: x.sum(), 
                        'num_rx': lambda x: x.sum(), 
                        'total_amt_reimb': lambda x: x.sum(),
                        'med_amt_reimb': lambda x: x.sum(),
                        'nonmed_amt_reimb':lambda x: x.sum(),
                        'days_marketing': lambda x: x.mean()})

cms17_stats_fin = cms17_stats.reset_index()
cms17_stats_fin.rename(columns={'units_reimb': 'units_reimb_sum', 
                                'num_rx':'num_rx_sum', 
                                'total_amt_reimb':'total_amt_reimb_sum',
                                'med_amt_reimb':'med_amt_reimb_sum',
                                'nonmed_amt_reimb':'nonmed_amt_reimb_sum'}, inplace=True)
cms17_stats_fin.reset_index(inplace=True)",0.4763923287,
170,data grouping and aggregation,"ind_percent = ind_df.groupby('industry').apply(lambda x: ((x['revenue'].sum()-x['prev_revenue'].sum())/x['prev_revenue'].sum())*100).sort_values(ascending=False)
ind_percent",0.4744599462,
170,data grouping and aggregation,"df_act_time = df_activities.loc[:,['userid', 'catQuest', 'actTime']]\
.groupby(['userid', 'catQuest']).agg([np.sum, np.mean, np.std])\
.rename(columns={'sum':'actTimeSum', 'mean':'actTimeMean', 'std':'actTimeStd'})",0.4701887369,
170,data grouping and aggregation,"df_artifact_duration = df_activities.loc[:,['userid', 'catQuest', 'artId', 'actTime']]\
.groupby(['userid', 'catQuest', 'artId']).sum()\
.groupby(['userid', 'catQuest']).agg([np.sum, np.mean, np.std])\
.rename(columns={'sum':'artTimeSum', 'mean':'artTimeMean', 'std':'artTimeStd'})",0.4697127342,
170,data grouping and aggregation,"df.pivot_table(index = ""Group"", aggfunc = (lambda x: len([i for i in x if i > 100])))",0.4678338468,
170,data grouping and aggregation,"df.pivot_table(index='Group', aggfunc = (lambda x: len([i for i in x if i > 100])))",0.4678338468,
170,data grouping and aggregation,"weather_final_hour = weather_final.groupby(['Year','Month','Day','Hour']).agg({'Temp(F)': np.mean,
                                                          'Precipitation(in)': np.mean,
                                                          'WindSpeed(kts)': np.mean}).reset_index()",0.4639569223,
170,data grouping and aggregation,"data.groupby(['race','education'],as_index=False).agg({'call':[np.sum,np.size,np.mean]})",0.4638199806,
170,data grouping and aggregation,"data.groupby(['Pclass','Age_Cat']).apply(lambda x: (x['Survived'].sum() / len(x['Survived']))* 100)",0.4635887146,
170,data grouping and aggregation,"weather_final_day = weather_final_hour.groupby(['Year','Month','Day']).agg({'Temp(F)': [np.mean, np.max, np.min],
                                                          'Precipitation(in)': np.sum,
                                                          'Hour' : np.size,
                                                          'WindSpeed(kts)': np.mean}).reset_index()
weather_final_day.columns = weather_final_day.columns.get_level_values(0)
weather_final_day.columns = ['Year','Month','Day','Temp(F) Mean','Temp(F) High'
                             ,'Temp(F) Low','Precipitation(in)','CountOfObs','WindSpeed(kts)']",0.461409539,
1544,practical concepts,"def doPCA():
    from sklearn.decomposition import PCA
    pca = PCA(n_components=2)
    pca.fit(data)
    return pca",0.3675439954,
1544,practical concepts,"class Stratification:
    def __init__(self):
        self.pj = self.cal_pj()
    
    def _sampling(self, M, mu, sigma, bins, nums):
        samples = [[] for _ in nums]
        N = np.sum(nums)
        n_bins = len(nums)
        while (np.any(np.array([len(_) for _ in samples]) < nums)):
            x = np.random.normal(loc=mu, scale=sigma, size=N)
            y = np.random.uniform(0, 1, size=N)
            x = x[y < np.array([p(_x) for _x in x]) / M / normpdf(x, loc=mu, scale=sigma)]
            ind = np.digitize(x, bins, right=True)
            s = [x[ind == i + 1] for i in range(n_bins)]
            samples = [np.concatenate((_s, s[i])) for i, _s in enumerate(samples)]
        return [_s[:nums[i]] for i, _s in enumerate(samples)]
    
    def sampling(self, params1=(0.51, 3, 1.2), params2=(1, 7, 1.1), N=10000):
        bins1 = [1, 2, 3, 4, 5]
        bins2 = [5, 6, 7, 8, 9]
        n_bins = len(bins1) + len(bins2) - 2
        nums1 = [N // n_bins for _ in bins1][:-1]
        nums2 = [N // n_bins for _ in bins2][:-1]
        return self._sampling(params1[0], params1[1], params1[2], bins1, nums1) \
    + self._sampling(params2[0], params2[1], params2[2], bins2, nums2)
    
    def cal_pj(self):
        return np.array([CDF(i + 1) - CDF(i) for i in range(1, 9)])
    
    def estimate_eh(self):
        return np.dot(self.pj, [np.mean(h(s)) for s in self.sampling()])",0.3662744164,
1544,practical concepts,"def PropertyTypeTable(PropertyType):
    ProperTable = CollectionConceptsDF[CollectionConceptsDF.Concept == PropertyType]
    return ProperTable",0.3649149537,
1544,practical concepts,"def solve():
    Clauses = allClauses()
    return dp.solve(Clauses, set())",0.3647642434,
1544,practical concepts,"def doPCA():
    from sklearn.decomposition import PCA
    pca = PCA(n_components=5)
    pca.fit(train_tiles)
    return pca",0.3632256687,
1544,practical concepts,"class Chips:
    
    def __init__(self):
        self.total = 100  # This can be set to a default value or supplied by a user input
        self.bet = 0
        
    def win_bet(self):
        self.total += self.bet
    
    def lose_bet(self):
        self.total -= self.bet",0.3617667556,
1544,practical concepts,"def raceAB():
    
    ''' initializes two racers and simulates one race returning the winner or a tie'''
    a = racer(prob = [.9, .1], step = 5)
    b = racer(prob = [.4, .6], step = 1)
    
    while not (a.win() or b.win()):
        a.turn()
        b.turn()
    if (a.win() and b.win()):
        return ""tie""
    elif a.win():
        return ""a""
    elif b.win():
        return ""b""",0.357589215,
1544,practical concepts,"def reducate(srs):
    if(srs.Education==""Graduate""):
        srs.Education=1
    elif(srs.Education==""Not Graduate""):
        srs.Education=2
    return srs

train_df=train_df.apply(reducate, axis='columns')
test_df=test_df.apply(reducate, axis='columns')
train_df",0.3563736975,
1544,practical concepts,"def test_add():
    expected = 1 + 1
    computed = add(1, 1)
    assert computed == expected, '1+1=%g' % computed",0.3563140631,
1544,practical concepts,"def expression():
    """"""
    Parses
        expression = simpleExpression
                     {(""="" | ""<>"" | ""<"" | ""<="" | "">"" | "">="") simpleExpression}.
    Generates code for the expression if no error is reported
    """"""
    x = simpleExpression()
    while SC.sym in {EQ, NE, LT, LE, GT, GE}:
        op = SC.sym
        getSym(); y = simpleExpression() # x op y
        if x.tp == Int == y.tp:
            x = CG.genRelation(op, x, y)
        else: mark('bad type')
    return x

def compoundStatement(l):
    """"""
    Parses
        compoundStatement(l) =
            ""begin"" 
            statement(l + 1) {"";"" statement(l + 1)}
            ""end"" 
    Generates code for the compoundStatement if no error is reported
    """"""
    if SC.sym == BEGIN: getSym()
    else: mark(""'begin' expected"")
    x = statement(l + 1)
    while SC.sym == SEMICOLON or SC.sym in FIRSTSTATEMENT:
        if SC.sym == SEMICOLON: getSym()
        else: mark(""; missing"")
        y = statement(l + 1); x = CG.genSeq(x, y)
    if SC.sym == END: getSym()
    else: mark(""'end' expected"")
    return x

def statement(l):
    """"""
    Parses
        statement =
            ident selector "":="" expression |
            ident ""("" [expression
                {"",""  expression}] "")""  |
            compoundStatement(l) |
            ""if""  expression
                ""then"" statement(l + 1)
                [""else"" statement(l + 1)] |
            ""while"" expression ""do"" statement.
    Generates code for the statement if no error is reported
    """"""
    if SC.sym not in FIRSTSTATEMENT:
        mark(""statement expected""); getSym()
        while SC.sym not in FIRSTSTATEMENT | STRONGSYMS | FOLLOWSTATEMENT:
            getSym()
    if SC.sym == IDENT:
        x = find(SC.val); getSym()
        x = CG.genVar(x)
        if type(x) in {Var, Ref}:
            x = selector(x)
            if SC.sym == BECOMES:
                getSym(); y = expression()
                if x.tp == y.tp: #in {Bool, Int}: # and not SC.error: type(y) could be Type 
                    #if type(x) == Var: ### and type(y) in {Var, Const}: incomplete, y may be Reg
                    x = CG.genAssign(x, y)
                    #else: mark('illegal assignment')
                elif type(x.tp) == type(y.tp) == Array:
                    x = CG.genDeferredAssign(x, y)
                else: mark('incompatible assignment')
            elif SC.sym == EQ:
                mark(':= expected'); getSym(); y = expression()
            else: mark(':= expected')
        elif type(x) in {Proc, StdProc}:
            fp, i = x.par, 0  #  list of formals, count of actuals
            if SC.sym == LPAREN:
                getSym()
                if SC.sym in FIRSTEXPRESSION:
                    y = expression()
                    if i < len(fp):
                        if (type(fp[i]) == Var or type(y) == Var) and \
                           fp[i].tp == y.tp:
                            if type(x) == Proc: CG.genActualPara(y, fp[i], i)
                            i = i + 1
                        else: mark('illegal parameter mode')
                    else: mark('extra parameter')
                    while SC.sym == COMMA:
                        getSym()
                        y = expression()
                        if i < len(fp):
                            if (type(fp[i]) == Var or type(y) == Var) and \
                               fp[i].tp == y.tp:
                                if type(x) == Proc: CG.genActualPara(y, fp[i], i)
                                i = i + 1
                            else: mark('illegal parameter mode')
                        else: mark('extra parameter')
                if SC.sym == RPAREN: getSym()
                else: mark(""')' expected"")
            if i < len(fp): mark('too few parameters')
            if type(x) == StdProc:
                if x.name == 'read': x = CG.genRead(y)
                elif x.name == 'write': x = CG.genWrite(y)
                elif x.name == 'writeln': x = CG.genWriteln()
            else: x = CG.genCall(x)
        else: mark(""variable or procedure expected"")
    elif SC.sym == BEGIN: x = compoundStatement(l + 1)
    elif SC.sym == IF:
        getSym(); x = expression();
        if x.tp == Bool: x = CG.genCond(x)
        else: mark('boolean expected')
        if SC.sym == THEN: getSym()
        else: mark(""'then' expected"")
        y = statement(l + 1)
        if SC.sym == ELSE:
            if x.tp == Bool: y = CG.genThen(x, y);
            getSym()
            z = statement(l + 1);
            if x.tp == Bool: x = CG.genIfElse(x, y, z)
        else:
            if x.tp == Bool: x = CG.genIfThen(x, y)
    elif SC.sym == WHILE:
        getSym(); t = CG.genTarget(); x = expression()
        if x.tp == Bool: x = CG.genCond(x)
        else: mark('boolean expected')
        if SC.sym == DO: getSym()
        else: mark(""'do' expected"")
        y = statement()
        if x.tp == Bool: x = CG.genWhile(t, x, y)
    else: x = None
    return x

def typ():
    """"""
    Parses
        type = ident |
               ""array"" ""["" expression "".."" expression ""]"" ""of"" type |
               ""record"" typedIds {"";"" typedIds} ""end"".
    Returns a type descriptor 
    """"""
    if SC.sym not in FIRSTTYPE:
        getSym(); mark(""type expected"")
        while SC.sym not in FIRSTTYPE | STRONGSYMS | FOLLOWTYPE:
            getSym()
    if SC.sym == IDENT:
        ident = SC.val; x = find(ident); getSym()
        if type(x) == Type: x = Type(x.tp)
        else: mark('not a type'); x = Type(None)
    elif SC.sym == ARRAY:
        getSym()
        if SC.sym == LBRAK: getSym()
        else: mark(""'[' expected"")
        x = expression()
        if SC.sym == PERIOD: getSym()
        else: mark(""'.' expected"")
        if SC.sym == PERIOD: getSym()
        else: mark(""'.' expected"")
        y = expression()
        if SC.sym == RBRAK: getSym()
        else: mark(""']' expected"")
        if SC.sym == OF: getSym()
        else: mark(""'of' expected"")
        z = typ().tp;
        if type(x) != Const or x.val < 0:
            mark('bad lower bound'); x = Type(None)
        elif type(y) != Const or y.val < x.val:
            mark('bad upper bound'); y = Type(None)
        else: x = Type(CG.genArray(Array(z, x.val, y.val - x.val + 1)))
    elif SC.sym == RECORD:
        getSym(); openScope(); typedIds(Var)
        while SC.sym == SEMICOLON:
            getSym(); typedIds(Var)
        if SC.sym == END: getSym()
        else: mark(""'end' expected"")
        r = topScope(); closeScope()
        x = Type(CG.genRec(Record(r)))
    else: x = Type(None)
    return x

def typedIds(kind):
    """"""
    Parses
        typedIds = ident {"","" ident} "":"" type.
    Updates current scope of symbol table
    Assumes kind is Var or Ref and applies it to all identifiers
    Reports an error if an identifier is already defined in the current scope
    """"""
    if SC.sym == IDENT: tid = [SC.val]; getSym()
    else: mark(""identifier expected""); tid = []
    while SC.sym == COMMA:
        getSym()
        if SC.sym == IDENT: tid.append(SC.val); getSym()
        else: mark('identifier expected')
    if SC.sym == COLON:
        getSym(); tp = typ().tp
        if tp != None:
            for i in tid: newObj(i, kind(tp))
    else: mark(""':' expected"")

def declarations(allocVar):
    """"""
    Parses
        declarations =
            {""const"" ident ""="" expression "";""}
            {""type"" ident ""="" type "";""}
            {""var"" typedIds "";""}
            {""procedure"" ident [""("" [[""var""] typedIds {"";"" [""var""] typedIds}] "")""] "";""
                declarations compoundStatement "";""}.
    Updates current scope of symbol table.
    Reports an error if an identifier is already defined in the current scope.
    For each procedure, code is generated
    """"""
    if SC.sym not in FIRSTDECL | FOLLOWDECL:
        getSym(); mark(""'begin' or declaration expected"")
        while SC.sym not in FIRSTDECL | STRONGSYMS | FOLLOWDECL: getSym()
    while SC.sym == CONST:
        getSym()
        if SC.sym == IDENT:
            ident = SC.val; getSym()
            if SC.sym == EQ: getSym()
            else: mark(""= expected"")
            x = expression()
            if type(x) == Const: newObj(ident, x)
            else: mark('expression not constant')
        else: mark(""constant name expected"")
        if SC.sym == SEMICOLON: getSym()
        else: mark(""; expected"")
    while SC.sym == TYPE:
        getSym()
        if SC.sym == IDENT:
            ident = SC.val; getSym()
            if SC.sym == EQ: getSym()
            else: mark(""= expected"")
            x = typ(); newObj(ident, x)  #  x is of type ST.Type
            if SC.sym == SEMICOLON: getSym()
            else: mark(""; expected"")
        else: mark(""type name expected"")
    start = len(topScope())
    while SC.sym == VAR:
        getSym(); typedIds(Var)
        if SC.sym == SEMICOLON: getSym()
        else: mark(""; expected"")
    varsize = allocVar(topScope(), start)
    while SC.sym == PROCEDURE:
        getSym()
        if SC.sym == IDENT: getSym()
        else: mark(""procedure name expected"")
        ident = SC.val; newObj(ident, Proc([])) #  entered without parameters
        sc = topScope()
        CG.procStart(); openScope() # new scope for parameters and body
        if SC.sym == LPAREN:
            getSym()
            if SC.sym in {VAR, IDENT}:
                if SC.sym == VAR: getSym(); typedIds(Ref)
                else: typedIds(Var)
                while SC.sym == SEMICOLON:
                    getSym()
                    if SC.sym == VAR: getSym(); typedIds(Ref)
                    else: typedIds(Var)
            else: mark(""formal parameters expected"")
            fp = topScope()
            sc[-1].par = fp[:] #  procedure parameters updated
            if SC.sym == RPAREN: getSym()
            else: mark("") expected"")
        else: fp = []
        parsize = CG.genFormalParams(fp)
        if SC.sym == SEMICOLON: getSym()
        else: mark(""; expected"")
        localsize = declarations(CG.genLocalVars)
        CG.genProcEntry(ident, parsize, localsize)
        x = compoundStatement(); CG.genProcExit(x, parsize, localsize)
        closeScope() #  scope for parameters and body closed
        if SC.sym == SEMICOLON: getSym()
        else: mark(""; expected"")
    return varsize

def program():
    """"""
    Parses
        program = ""program"" ident "";"" declarations compoundStatement(1).
    Generates code if no error is reported
    """"""
    newObj('boolean', Type(Bool)); Bool.size = 4
    newObj('integer', Type(Int)); Int.size = 4
    newObj('true', Const(Bool, 1))
    newObj('false', Const(Bool, 0))
    newObj('read', StdProc([Ref(Int)]))
    newObj('write', StdProc([Var(Int)]))
    newObj('writeln', StdProc([]))
    CG.progStart()
    if SC.sym == PROGRAM: getSym()
    else: mark(""'program' expected"")
    ident = SC.val
    if SC.sym == IDENT: getSym()
    else: mark('program name expected')
    if SC.sym == SEMICOLON: getSym()
    else: mark('; expected')
    declarations(CG.genGlobalVars); CG.progEntry(ident)
    x = compoundStatement(1)
    return CG.progExit(x)

def compileString(src, dstfn = None, target = 'armv8'):
    """"""Compiles string src; if dstfn is provided, the code is written to that
    file, otherwise printed on the screen""""""
    global CG
    #  used for init, genRec, genArray, progStart, genGlobalVars, \
    #  progEntry, progExit, procStart, genFormalParams, genActualPara, \
    #  genLocalVars, genProcEntry, genProcExit, genSelect, genIndex, \
    #  genVar, genConst, genUnaryOp, genBinaryOp, genRelation, genSeq, \
    #  genAssign, genCall, genRead, genWrite, genWriteln, genCond, \
    #  genIfThen, genThen, genIfElse, genTarget, genWhile
    if target == 'mips': import CGmips as CG
    elif target == 'ast': import CGast as CG
    elif target == 'armv8': import CGARMv8 as CG
    else: print('unknown target'); return
    SC.init(src)
    ST.init()
    CG.init()
    p = program()
    if p != None and not SC.error:
        if dstfn == None: print(p)
        else:
            with open(dstfn, 'w') as f: f.write(p);

def compileFile(srcfn):
    if srcfn.endswith('.p'):
        with open(srcfn, 'r') as f: src = f.read()
        dstfn = srcfn[:-2] + '.s'
        compileString(src, dstfn)
    else: print(""'.p' file extension expected"")",0.355627358,
801,helper to create two separate dataframes for classification and regression,"def createDF():
    df = pd.DataFrame(np.random.randn(96, 4), columns=list('ABCD'))
    df[""well""] = np.arange(96)+1
    # change the scaling
    df[""A""] *= 255
    df[""B""] *= 10e3
    df[""C""] *= 30e6
    return df",0.5550609827,
801,helper to create two separate dataframes for classification and regression,"# Let's split our data into training data and testing data
    trainTest = df.randomSplit([0.5, 0.5])
    trainingDF = trainTest[0]
    testDF = trainTest[1]

    # Now create our linear regression model
    lir = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)

    # Train the model using our training data
    model = lir.fit(trainingDF)
    
    import  findspark
findspark.init()
import pyspark as ps
import warnings
from pyspark.sql import SQLContext

    # Now see if we can predict values in our test data.
    # Generate predictions using our linear regression model for all features in our
    # test dataframe:
    fullPredictions = model.transform(testDF).cache()

    # Extract the predictions and the ""known"" correct labels.
    predictions = fullPredictions.select(""prediction"").rdd.map(lambda x: x[0])
    labels = fullPredictions.select(""label"").rdd.map(lambda x: x[0])

    # Zip them together
    predictionAndLabel = predictions.zip(labels).collect()

    # Print out the predicted and actual values for each point
    for prediction in predictionAndLabel:
      print(prediction)",0.5538066626,
801,helper to create two separate dataframes for classification and regression,"def consolidate(df, ratio, ur):
    
    df = pd.concat([ur, ratio], axis=1).fillna(0.0)
    
    df['Date'] = df['Date'].str.rsplit('-', 1).str[0]
    
    round(df['Minwage/CPI Ratio'], 3)
    
    df = df.replace(to_replace=[0.000000], value=[None])
    
    df = df.dropna()
    
    return df

NY = consolidate(NY, ratio_ny, ur_ny)
LA = consolidate(LA, ratio_la, ur_la)
CH = consolidate(CH, ratio_ch, ur_ch)
HS = consolidate(HS, ratio_hs, ur_hs)",0.5524417162,
801,helper to create two separate dataframes for classification and regression,"def regression(df):   
    #### Write your code here while following the instructions
    
    #### Splitting Data into Train and Test
    train,test = 
    
    #### Separating label from features
    y_train = 
    y_test = 
    x_train = 
    x_test = 
    
    #### Building model
    ### Code to build a linear regression model
    reg=
    reg.fit()
    
    #### Doing predictions on training and test data
    predicted_train=reg.predict()
    predicted_test=reg.predict()
    
    #### Train and Test MAE Calculation of model
    
    #### Train and Test RMSE Calculation of model
    
    #### Train and Test R Squared error calculation
    
    
    #### Train and Test Calculating MAPE
    
    ### Getting coefficients for features 
    
    
    ### return all the metrics for the model to store in a dataframe
    return train_r2,test_r2,train_mae,test_mae,train_rmse,test_rmse,train_mape,test_mape,Mkt_RF_coefficient,HML_coefficient,SMB_coefficient,RMW_coefficient,CMA_coefficient",0.5468276739,
801,helper to create two separate dataframes for classification and regression,"if LCI:
    if fu!=0:
#         print('Selected functional unit and its units')
#         chosen_fu_units#Uncomment to view
#         chosen_fu.head()#Uncomment to view
#         co2_emissions.head()#Uncomment to view
#         co2_units#Uncomment to view
#         co2_fossil.head()#Uncomment to view
#         co2_fossil_units#Uncomment to view
        co2_lci=pd.DataFrame(columns=['time','co2_biogenic','co2_fossil'])
        co2_lci[['time','co2_fossil']]=co2_fossil.copy()
        co2_lci['co2_biogenic']=co2_emissions['co2_emission'].values-co2_fossil['co2_emission_fossil'].values
#         co2_lci#Uncomment to view
        co2_lci.loc[1:,['co2_biogenic','co2_fossil']]=co2_lci.loc[1:,['co2_biogenic','co2_fossil']].values/\
        chosen_fu.iloc[1:,1].values.reshape((-1,1))
    print('Units: kg CO2/FU')
    co2_lci.head()
    if not time_intervals:
        for column in ['co2_biogenic','co2_fossil']:
            print('{:s}: {:0.2e}'.format(column,co2_lci.loc[1,column]))",0.5466413498,
801,helper to create two separate dataframes for classification and regression,"# Uncomment following when submitting
if Validation == False:
    test['date_block_num'] = 34
    all_data = pd.concat([all_data, test], axis = 0)
    all_data = all_data.drop(columns = ['ID'])",0.5457948446,
801,helper to create two separate dataframes for classification and regression,"def getCoefs(X,model):
    model_coefficients = pd.DataFrame([X.columns,model.coef_[0]], 
                                      index = ['feat','coef'])

    model_coefficients.columns = ['beta_'+str(i).rjust(3,'0') for i in range(1,model_coefficients.shape[1]+1)]
    model_coefficients['beta_0'] = ['Intercept',model.intercept_[0]]
    model_coefficients = model_coefficients[sorted(model_coefficients.columns)]

    non_zero_coefs = model_coefficients[[col for col in model_coefficients.columns if all(model_coefficients[col] != 0)]]
    non_zero_coefs.columns = ['beta_'+str(i).rjust(3,'0') for i in range(0,len(non_zero_coefs.columns))]

    return non_zero_coefs

getCoefs(X,best_model_gs)",0.5456750989,
801,helper to create two separate dataframes for classification and regression,"# Defining the regression models 

def linear_regression(X_train, y_train, X_test, y_test, plot=True):
    regr = LinearRegression()
    regr.fit(X_train, y_train)

    # Make predictions using the testing set
    y_pred = regr.predict(X_test)

    # Error
    rms_r2_score(y_test, y_pred)
    
    if plot:
        # Plot outputs
        #plot_task1(X_test, y_test, y_pred, 'Linear Regression')
        plot_powergeneration(y_test, y_pred, 'Linear Regression')
    return y_pred
        

def k_nearest_neighbors(X_train, y_train, X_test, y_test, plot=True):
    neigh = KNeighborsRegressor(n_neighbors=800)
    neigh.fit(X_train_selected, y_train) 
    y_pred = neigh.predict(X_test)

    # Error
    rms_r2_score(y_test, y_pred)
    
    if plot:
        # Plot outputs
        #plot_task1(X_test, y_test, y_pred, 'KNN')
        plot_powergeneration(y_test, y_pred, 'KNN')

#def knn_crossval(X,y,n_folds=10):
#    num_neighbors = [1, 5, 20, 50, 100, 500, 800, 1000]
#    #leaf_size = [10, 30, 50, 100, 200, 500]
#    param_grid = [{'n_neighbors': num_neighbors,
#                   'weights':['uniform'],
#                 #  'leaf_size':leaf_size
#                  },
#                 {'n_neighbors': num_neighbors,
#                  'weights':['distance'],
#               #   'leaf_size':leaf_size
#                 }]
#    grid_search = GridSearchCV(KNeighborsRegressor(),
#                                   param_grid,
#                                   cv=n_folds,
#                                   n_jobs=-1)
#    grid_search.fit(X, y)
#    grid_search.best_params_
#    return grid_search.best_params_    
#    
#def k_nearest_neighbors(X_train, y_train, X_test, y_test, plot=True):
#    
#    best_params = knn_crossval(X_train, y_train)    
#    neigh = KNeighborsRegressor().set_params(**best_params)
#    
#    neigh.fit(X_train_selected, y_train) 
#    y_pred = neigh.predict(X_test)
#
#    # The Root mean squared error
#    print(""Root Mean squared error: %.4f""
#          % np.sqrt(mean_squared_error(y_test, y_pred)))
#    # Explained variance score: 1 is perfect prediction
#    print('Variance score: %.2f' % r2_score(y_test, y_pred))
#    
#    if plot:
#        # Plot outputs
#        #plot_task1(X_test, y_test, y_pred, 'KNN')
#        plot_powergeneration(y_test, y_pred, 'KNN')


def support_vector_regression(X_train, y_train, X_test, y_test, plot=True):

    svr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)
    y_pred = svr_rbf.fit(X_train_selected, y_train).predict(X_test_selected)

    # Error
    rms_r2_score(y_test, y_pred)
    
    if plot:
        #plot_task1(X_test, y_test, y_pred, 'SVR')
        plot_powergeneration(y_test, y_pred, 'SVR')
        
        

def ann_model(X_train, y_train, X_test, y_test, plot=True):
    '''
    Trains an artificial neural network. n-input channels and one 
    output(the predicted power).
    '''
    input_shape = X_train.shape[1]
    output_shape = 1
    model = Sequential()

    model.add(InputLayer(input_shape=(input_shape,)))
    model.add(Dense(6, kernel_initializer='lecun_normal',
                    bias_initializer='ones',activation='selu'))
    model.add(Dropout(0.3))
    model.add(Dense(8, kernel_initializer='lecun_normal',
                    bias_initializer='ones',activation='softmax'))
    model.add(Dense(output_shape))

    model.compile(optimizer='rmsprop',
                loss='mean_squared_error')  #batch_size=2000, epochs=100,

    model.fit(X_train, y_train,epochs=10,  verbose=1)


    y_pred = model.predict(X_test)
    
    # Error
    rms_r2_score(y_test, y_pred)

    if plot:
        plot_powergeneration(y_test, y_pred, model='ANN')
 
def rms_r2_score(y_test, y_pred):
    # The Root mean squared error
    print(""Root Mean squared error: %.4f""
          % np.sqrt(mean_squared_error(y_test, y_pred)))
    # Explained variance score: 1 is perfect prediction
    print('R^2: %.2f' % r2_score(y_test, y_pred))

# Store predictions to file function        
def store_predictions_to_file(y_pred, model=None, task=1 , 
                              template='ForecastTemplate.csv'):
    pred = pd.read_csv(template)
    pred['FORECAST'] = y_pred[:len(pred)]
    pred.to_csv('ForecastTemplate{1}-{0}.csv'.format(model,task), index=False)
        
        
        
# Plotting function

def plot_powergeneration(y_test, y_pred, model=None):
    
    plt.figure(figsize=(15,5))
   
    plt.plot(y_test.values, color='darkorange', label='Real')
    
    plt.plot(y_pred, color='navy', label='Predicted')
    
    plt.xlabel('Time')
    plt.ylabel('Wind Power')
    plt.title(model)
    plt.legend()
    #plt.ylim(-0.1,y_test.max().all()+0.1)
    plt.show()
    

def plot_task1(X_test, y_test, y_pred, model=None):
    plt.scatter(X_test, y_test, color='darkorange', 
            marker='.', label='Real', linewidth=0.1)
    
    plt.scatter(X_test, y_pred, color='navy', 
                marker='.', label='Predicted', linewidth=0.1)
    
    plt.xlabel('Wind speed')
    plt.ylabel('Wind Power')
    plt.title(model)
    plt.legend()
    plt.ylim(-0.1,y_test.max().all()+0.1)
    plt.show()",0.5451216698,
801,helper to create two separate dataframes for classification and regression,"#     num_month = 12
#     
#     n_pred = 1
#     batch_s=12
#     num_epochs=100

def flu_pred(num_month = 12,n_pred = 1,batch_s=12,num_epochs=100, dropout_rate = 0):
    n_features = len(dataset.columns) - 1
    
    values = dataset.values
    # integer encode direction
    values = values.astype('float32')
    # normalize features

    ## use previous ""num_month"" data points (all features)
    ## to predict the next ""n_pred"" incidence data

    # frame as supervised learning
    reframed = series_to_supervised(values, num_month, n_pred)
    #reframed.drop(reframed.columns[23:45], axis=1, inplace=True)
    #reframed.drop(reframed.columns[(n_features):-n_pred], axis=1, inplace=True)
    X = reframed[reframed.columns[:num_month*(n_features+1)]]
    y_col = ['var%d(t)' % (n_features+1)] + [('var%d(t+%d)' % (n_features+1, i)) for i in range(1,n_pred)]
    y = reframed[y_col]
    reframed = pd.concat([X,y], axis = 1)
    # ('var%d(t+%d)' % (j+1, i))

    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled = scaler.fit_transform(reframed)
    # specify the number of lag hours


    #num_month = 3
    num_pred = n_pred

    values = dataset.values
    # integer encode direction
    values = values.astype('float32')
    # normalize features

    n_month = num_month
    n_features = len(dataset.columns) - 1
    n_pred = num_pred
    # frame as supervised learning
    reframed = series_to_supervised(values, n_month, n_pred)
    #reframed.drop(reframed.columns[23:45], axis=1, inplace=True)
    #reframed.drop(reframed.columns[(n_features):-n_pred], axis=1, inplace=True)
    X = reframed[reframed.columns[:n_month*(n_features+1)]]
    y_col = ['var%d(t)' % (n_features+1)] + [('var%d(t+%d)' % (n_features+1, i)) for i in range(1,n_pred)]
    y = reframed[y_col]
    reframed = pd.concat([X,y], axis = 1)
    # ('var%d(t+%d)' % (j+1, i))

    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled = scaler.fit_transform(reframed)
    # specify the number of lag hours

    # split into train and test sets
    #values = reframed.values
    values = scaled

    n_train_month = 12*8
    train = values[:n_train_month, :]
    test = values[n_train_month:, :]

    # split into input and outputs
    train_X, train_y = train[:, :-num_pred], train[:, -num_pred:]
    test_X, test_y = test[:, :-num_pred], test[:, -num_pred:]
    # reshape input to be 3D [samples, timesteps, features]
    train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))
    test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))
    print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)

    # design network
    model = Sequential()
    ## note differences : here specifying batch_size not here but in ""fit""
    #model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))
    model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))
    model.add(Dropout(dropout_rate))
    model.add(Dense(30))
    model.add(Dense(num_pred))
    model.compile(loss='mae', optimizer='adam')

    # fit network
    history = model.fit(train_X, train_y, epochs=num_epochs, batch_size=batch_s, validation_data=(test_X, test_y), verbose=0, shuffle=False)
    ## important to note that use test data as validation (of course shuffle=False)
    # validation_data: tuple (x_val, y_val) or tuple (x_val, y_val, val_sample_weights) 
    # on which to evaluate the loss and any model metrics at the end of each epoch. 
    # The model will not be trained on this data. This will override validation_split.

    # plot history
    pyplot.plot(history.history['loss'], label='train')
    pyplot.plot(history.history['val_loss'], label='test')
    pyplot.legend()
    pyplot.show()


    # make a prediction
    yhat = model.predict(test_X)
    test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))
    # invert scaling for forecast
    # inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)
    # inv_yhat = concatenate((yhat, test_X), axis=1)
    inv_yhat = concatenate((test_X,yhat), axis=1)

    inv_yhat = scaler.inverse_transform(inv_yhat)
    #inv_yhat = inverse_transform(inv_yhat)

    # inv_yhat = inv_yhat[:,:num_pred]
    inv_yhat = inv_yhat[:,-num_pred:]

    #inv_yhat = inv_yhat[0,:]

    # invert scaling for actual
    #test_y = test_y.reshape((len(test_y), 1))
    #inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)
    # inv_y = concatenate((test_y, test_X), axis=1)
    inv_y = concatenate((test_X,test_y), axis=1)

    inv_y = scaler.inverse_transform(inv_y)
    # inv_y = inv_y[:,:num_pred]
    inv_y = inv_y[:,-num_pred:]

    # calculate RMSE
    rmse = sqrt(mean_squared_error(inv_y, inv_yhat))
    print('Test RMSE: %.3f' % rmse)

    # #pyplot.plot(inv_y[1,:])
    # #pyplot.plot(inv_yhat[1,:])

    # for i in range(61):
    #     pyplot.plot(inv_y[i,:])
    #     pyplot.plot(inv_yhat[i,:])
    #     pyplot.show()

    # for i in range(num_pred):
    #     pyplot.plot(inv_y[:,i])
    #     pyplot.plot(inv_yhat[:,i])
    #     pyplot.show()

    # make a prediction
    yhat_t = model.predict(train_X)
    train_X = train_X.reshape((train_X.shape[0], train_X.shape[2]))
    # invert scaling for forecast
    # inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)
    # inv_yhat = concatenate((yhat, test_X), axis=1)
    inv_yhat_t = concatenate((train_X,yhat_t), axis=1)

    inv_yhat_t = scaler.inverse_transform(inv_yhat_t)
    #inv_yhat = inverse_transform(inv_yhat)

    # inv_yhat = inv_yhat[:,:num_pred]
    inv_yhat_t = inv_yhat_t[:,-num_pred:]

    #inv_yhat = inv_yhat[0,:]

    # invert scaling for actual
    #test_y = test_y.reshape((len(test_y), 1))
    #inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)
    # inv_y = concatenate((test_y, test_X), axis=1)
    inv_y_t = concatenate((train_X,train_y), axis=1)

    inv_y_t = scaler.inverse_transform(inv_y_t)
    # inv_y = inv_y[:,:num_pred]
    inv_y_t = inv_y_t[:,-num_pred:]

    # calculate RMSE
    rmse = sqrt(mean_squared_error(inv_y_t, inv_yhat_t))
    print('Train RMSE: %.3f' % rmse)

    #pyplot.plot(inv_y[1,:])
    #pyplot.plot(inv_yhat[1,:])

    # for i in range(61):
    #     pyplot.plot(inv_y_t[i,:])
    #     pyplot.plot(inv_yhat_t[i,:])
    #     pyplot.show()

    # for i in range(num_pred):
    #     pyplot.plot(inv_y_t[:,i])
    #     pyplot.plot(inv_yhat_t[:,i])
    #     pyplot.show()
    return(inv_y_t, inv_yhat_t, inv_y, inv_yhat)",0.5435804129,
801,helper to create two separate dataframes for classification and regression,"if data_balancing:
    covariates = df.columns[0:df.shape[1]-1]
    df_covar_dumm = pd.get_dummies(df.loc[:, covariates])
    label = df.columns.tolist()[-1]
    smote = SMOTE(ratio='auto', random_state=42, k=None, k_neighbors=5, m=None,
                  m_neighbors=10, out_step=0.5, kind='regular', svm_estimator=None, n_jobs=1)
    data_resampled, label_resampled = smote.fit_sample(
        df_covar_dumm, df.loc[:, label])
    balancd_data = pd.DataFrame(data_resampled)
    balancd_data.loc[:, balancd_data.shape[1]] = label_resampled
    balancd_data.columns = df_covar_dumm.columns.tolist()+[label]",0.5428347588,
844,how many orders per year?,"# Passion Score
def passion_score(df):
    if len(df.customer_id.unique()) == 0:
        passion_score = 0
    else:
        passion_score = round(len(df)/len(df.customer_id.unique()),2)
    return passion_score",0.5207687616,
844,how many orders per year?,"def utility_matrix_fullness(reviews):
    return len(reviews) / float(len(reviews.business_id.unique()) * len(reviews.user_id.unique()))",0.4918929636,
844,how many orders per year?,"FEATURE_INTERNAL_INDEX = 'internal_index'
def create_first_magic_feature(tr, test, new_col=FEATURE_INTERNAL_INDEX):
    counts_mapping = tr.groupby(['date_block_num', 'shop_id']).shop_id.count()
    counts = pd.Series(list(zip(tr.date_block_num, tr.shop_id))).map(counts_mapping)
    tr[new_col] = (tr.groupby(['date_block_num', 'shop_id']).shop_id.cumcount() / counts).astype(np.float32)

    counts_mapping = test.groupby('shop_id').shop_id.count()
    counts = test.shop_id.map(counts_mapping)
    test[new_col] = (test.groupby('shop_id').shop_id.cumcount() / counts).astype(np.float32)

create_first_magic_feature(train, sales_test)",0.4804820418,
844,how many orders per year?,"exchange.create_order(symbol2, amount=balances[symbol2.split('/')[0]], price=exchange.fetch_order_book(symbol2)['bids'][0][0] * (1 - 0.0025))
# exchange.create_limit_sell_order('', amount)",0.4774004221,
844,how many orders per year?,"def get_topic_wc(row):
    topic_pc = row['percent'] / 100
    total_wc = int(df_raw.loc[df_raw.delivered == row['delivered'], 'words'])
    return topic_pc * total_wc",0.474811852,
844,how many orders per year?,"def assign_pct_of_house(df):
    return df.house_seats.div(df.house_seats.sum()).mul(100).round(2)",0.474766016,
844,how many orders per year?,"def calculate_sparsity(itemlimit, userlimit, df):
    product = df.groupBy(""itemId"").count()
    product_filter = product.filter(product['count'] > itemlimit)
    Data = df.join(product_filter, ['itemId'], 'leftsemi')
    
    user = Data.groupBy(""userId"").count()
    user_filter = user.filter(user['count'] > userlimit)
    DF = Data.join(user_filter, ['userId'], 'leftsemi')
    
    available = DF.count()
    product_total = DF.select(""itemId"").distinct().count()
    user_total = DF.select(""userId"").distinct().count()
    
    print(""available rating: "" + str(available))
    print(""distinct product: "" + str(product_total))
    print(""distinct user: "" + str(user_total))
    
    result = (float(available)/(float(product_total) * float(user_total)))*100
    print(result)
    
    return DF",0.4716353416,
844,how many orders per year?,"def broken_birth_year(b_instance):
    import datetime
    current=datetime.datetime.now().year
    return current-b_instance.age",0.4703985453,
844,how many orders per year?,"def calculateStandradRating(year):
    
    return round(dataset[dataset.YEAR == year]['RATING'].mean(),2)",0.4700173736,
844,how many orders per year?,"def calc_order_numbers_to_sum((last_order_n,order_number)):
    order_numbers_to_sum = range(last_order_n+1, order_number+1)
    return order_numbers_to_sum",0.4688923359,
1916,shotgun assembly program,"def setupMinuit(params):
    print 'Setup Minuit...'
    PRINT_LEVEL = 1  # -1 => quiet, 1 => loud
    UP = 0.5 # 1: appropriate for 68% CL using chisq (use 0.5 for log-likelihood)
    npar = len(params)
    minuit = rt.TMinuit(npar)
    minuit.SetFCN(nnl)
    minuit.SetErrorDef(UP)
    minuit.SetPrintLevel(PRINT_LEVEL)

    status = rt.Long() # needed for integers passed by refence (int& ii)
    print ""%-20s %10s %10s %10s %10s"" % \
    ('param', 'guess', 'step', 'min', 'max')
    
    for ii, t in enumerate(params):
        print ""%-20s %10.2e %10.3e %10.3e %10.3e"" % t
        name, guess, step, pmin, pmax = t
        minuit.mnparm(ii, name, guess, step, pmin, pmax, status)
        if status != 0:
            sys.exit(""** mnparm(%s) status = %d"" % (name, status))
    return minuit",0.4599232674,
1916,shotgun assembly program,"def main():
    print 'donuts'
    # Each line calls donuts, compares its result to the expected for that call.
    test(donuts(4), 'Number of donuts: 4')
    test(donuts(9), 'Number of donuts: 9')
    test(donuts(10), 'Number of donuts: many')
    test(donuts(99), 'Number of donuts: many')

    print
    print 'both_ends'
    test(both_ends('spring'), 'spng')
    test(both_ends('Hello'), 'Helo')
    test(both_ends('a'), '')
    test(both_ends('xyz'), 'xyyz')

  
    print
    print 'fix_start'
    test(fix_start('babble'), 'ba**le')
    test(fix_start('aardvark'), 'a*rdv*rk')
    test(fix_start('google'), 'goo*le')
    test(fix_start('donut'), 'donut')

    print
    print 'mix_up'
    test(mix_up('mix', 'pod'), 'pox mid')
    test(mix_up('dog', 'dinner'), 'dig donner')
    test(mix_up('gnash', 'sport'), 'spash gnort')
    test(mix_up('pezzy', 'firm'), 'fizzy perm')",0.4529255629,
1916,shotgun assembly program,"from qiskit import QuantumRegister, ClassicalRegister, QuantumCircuit

def initialize_program ():
    
    qubit = QuantumRegister(2)
    A = qubit[0]
    B = qubit[1]
    
    bit = ClassicalRegister(2)
    a = bit[0]
    b = bit[1]
    
    qc = QuantumCircuit(qubit, bit)
    
    return A, B, a, b, qc",0.4378557801,
1916,shotgun assembly program,"def listMut():
    a = [1, 2, 3]
    b = [1, 2, 3]
    c = a
    a[0] = 5
    print(""a = "", a)
    print(""b = "", b)
    print(""c = "", c)",0.4320168197,
1916,shotgun assembly program,"MAX = 5000000
def measure(n):
    """"""Measurement model, return two coupled measurements.""""""
    PcmX, PcmY , PcmZ , Pmiss = [], [] , [] , []
    
    for i in range(MAX):        
        x = np.random.uniform(-0.8,0.8)
        y = np.random.uniform(-0.8,0.8)
        z = np.random.uniform(-0.3,1.4)
        pm = np.random.uniform(-0.3,1.)
    
        probability = Gaussian3DwithOffset( (x , y, z , pm) , amplitude=1,
                                       sigma_t=0.155, a1=0.143 , a2=0.159 , b1=0.562 , b2=0.159 )
        uniform = np.random.uniform(0,1.5)
        if uniform<probability:        
            PcmX.append(x)
            PcmY.append(y) 
            PcmZ.append(z) 
            Pmiss.append(pm)
        if (len(Pmiss)%(n/5)==0): print len(Pmiss),'samples so far...'
        if len(Pmiss)>n: break
    return np.array(PcmX), np.array(PcmY) , np.array(PcmZ) , np.array(Pmiss)",0.4311603308,
1916,shotgun assembly program,"%%file files/mc.grmpy.yml
---
SIMULATION:
    seed: 5133
    agents: 10000
    source: mc
ESTIMATION:
    file: mc.grmpy.txt
    start: auto
    agents: 165
    optimizer: SCIPY-BFGS
    maxiter: 6383
    dependent: wage
    indicator: state
    output_file: mc_rslt.grmpy.info
    comparison: 0
TREATED:
    params:
    - 0.99
    - 0.555
    - -0.555
    - 0.755
    - 0.155
    order:
    - const
    - X2
    - X3
    - X4
    - X5
UNTREATED:
    params:
    - 0.5
    - 0.255
    - -0.255
    - 0.1768
    - 0.0987
    
    order:
    - const
    - X2
    - X3
    - X4
    - X5
CHOICE:
    params:
    - 0.28
    - -0.39
    - 0.59
    - -0.89
    - -0.73

    order:
    - const
    - X6
    - X7
    - X8
    - X9

DIST:
    params:
    - 0.2
    - 0.0
    - 0.0
    - 0.2
    - 0.0
    - 1.0
VARTYPES:
    const: nonbinary
    X2: nonbinary
    X3: nonbinary
    X4: nonbinary
    X5: nonbinary
    X6: nonbinary
    X7: nonbinary
    X8: nonbinary
    X9: nonbinary
SCIPY-BFGS:
    gtol: 1.0e-05
    eps: 1.4901161193847655e-08
SCIPY-POWELL:
    xtol: 9.147777614048603e-05
    ftol: 9.749582129043358e-05",0.4233554304,
1916,shotgun assembly program,"def allreduce(kvs):
    
     os.environ[""PMI_PORT""] = kvs[""PMI_PORT""]
     os.environ[""PMI_ID""] = str(kvs[""PMI_ID""])
    
     from mpi4py import MPI
    
     comm = MPI.COMM_WORLD
     rank = comm.Get_rank()
   
     # image

     n = 2*1000000
     sendbuf = np.arange(n, dtype=np.float32)
     recvbuf = np.arange(n, dtype=np.float32)
        
     sendbuf[n-1] = 5.0;

     t1 = datetime.now()    
     comm.Allreduce(sendbuf, recvbuf, op=MPI.SUM)     
     t2 = datetime.now()
    
     out = {
        'rank' : rank,
        'time' : (t2-t1), 
        'sum'  : recvbuf[n-1]
     }
     return out",0.4224678874,
1916,shotgun assembly program,"%%file files/tutorial_eh.grmpy.yml
---
SIMULATION:
    seed: 2356
    agents: 10000
    source: data_eh
ESTIMATION:
    file: data_eh.grmpy.txt
    start: auto
    agents: 165
    optimizer: SCIPY-BFGS
    maxiter: 6383
    dependent: Y
    indicator: D
    output_file: output/est.grmpy.info
    comparison: 1
TREATED:
    params:
    - 1.0
    - 0.555
    order:
    - const
    - X2
UNTREATED:
    params:
    - 0.5
    - 0.25
    order:
    - const
    - X2
CHOICE:
    params:
    - 0.378
    - -0.39
    order:
    - const
    - X3
DIST:
    params:
    - 0.1
    - 0.0
    - 0.0524
    - 0.1
    - -0.0216
    - 1.0
VARTYPES:
    const: nonbinary
    X2: nonbinary
    X3: nonbinary
SCIPY-BFGS:
    gtol: 1.0e-05
    eps: 1.4901161193847655e-08
SCIPY-POWELL:
    xtol: 9.147777614048603e-05
    ftol: 9.749582129043358e-05",0.4221512675,
1916,shotgun assembly program,"#%matplotlib notebook
def plot_3Dscatter(sensor_number, sample):
    """"""
    Plots an interactive 3D scatter plot of a sample sensor data for the each of the 4 sensors 
    mounted on waist, left thigh, right ankle and right upper arm in X, Y and Z directions.
    
    Only a sample of the total data is used for visualization because the 3D plot animation
    if found to be smooth with lesser data points.
    
    Parameters
    ----------------
    sensor_number: The sensor number which determines the position of the sensor
    sample       : size of the sample to be used for plotting
    
    """"""
    # Input validation
    if(sensor_number > 4):
        print(""Sensor number out of range"")
        return
    
    # Determine sensor position from the sensor number input
    sensor_position = get_sensor_pos(sensor_number)
    
    # initialize the sensor X, Y and Z 
    sensor1 = 'x' + str(sensor_number)
    sensor2 = 'y' + str(sensor_number)
    sensor3 = 'z' + str(sensor_number)
    
    # Select the specified sample from the entire dataset
    sensor_data_sample = sensor_data.sample(sample)
    
    # Separate the data based on the target class 
    data_C1 = get_class_data(sensor_data_sample, C1)
    data_C2 = get_class_data(sensor_data_sample, C2)
    data_C3 = get_class_data(sensor_data_sample, C3)
    data_C4 = get_class_data(sensor_data_sample, C4)
    data_C5 = get_class_data(sensor_data_sample, C5)
    
    # PLOT
    fig,ax = plt.subplots()
    ax = fig.add_subplot(111, projection='3d')
    
    scatter1_proxy = get_proxy_plot(ax, data_C1[sensor1], data_C1[sensor2], data_C1[sensor3], color=color_C1)
    scatter2_proxy = get_proxy_plot(ax, data_C2[sensor1], data_C2[sensor2], data_C2[sensor3], color=color_C2)
    scatter3_proxy = get_proxy_plot(ax, data_C3[sensor1], data_C3[sensor2], data_C3[sensor3], color=color_C3)
    scatter4_proxy = get_proxy_plot(ax, data_C4[sensor1], data_C4[sensor2], data_C4[sensor3], color=color_C4)
    scatter5_proxy = get_proxy_plot(ax, data_C5[sensor1], data_C5[sensor2], data_C5[sensor3], color=color_C5)

    ax.set_title(""Sensor mounted on "" + sensor_position)
    ax.set_xlabel(""X"")
    ax.set_ylabel(""Y"")
    ax.set_zlabel(""Z"")
    ax.legend([scatter1_proxy, scatter2_proxy, scatter3_proxy, scatter4_proxy, scatter5_proxy], [C1, C2, C3, C4, C5], numpoints = 1)

    plot_in_html = mpld3.fig_to_html(fig)
    return plot_in_html
#     # rotate the axes and update
#     for angle in range(0, 360):
#         ax.view_init(30, angle)
#         plt.draw()",0.421971947,
1916,shotgun assembly program,"async def main():
    c = ac.from_iter([0,0,0,1,1,2,2,2,2,3,3,4,4,4,5,4,4,3,3,2,1,1,1,0])
    print(await c.distinct().collect())
    
ac.run(main())",0.4209151864,
2375,"timing, numpy, plotting","data = qc.Measure(
    v1.trace).run()
plot = qc.QtPlot(data.VNA_magnitude, figsize=(700, 500))
plot.save()",0.4968995154,
2375,"timing, numpy, plotting","nl.plotting.plot_epi(mean_epi, display_mode='y', cut_coords=5, vmin=10)",0.4916006923,
2375,"timing, numpy, plotting",learn.recorder.plot_metrics(),0.4902549684,
2375,"timing, numpy, plotting","helpers.plot_measures(retweets_filename, full_reg_rt_vol,
                      label=key, descr = ""retweets"",
                      save_dir = 'results/figures/')",0.488972187,
2375,"timing, numpy, plotting","helpers.plot_measures(replies_filename, full_reg_rt_vol, descr = ""replies"",
                      save_dir = 'results/figures/')",0.4871221185,
2375,"timing, numpy, plotting","from nipy.algorithms.diagnostics import screen, plot_tsdiffs",0.4838911891,
2375,"timing, numpy, plotting",sensor_exercice.plot_measurements(),0.4827439189,
2375,"timing, numpy, plotting","mlpl = model_dict.MlPlot(models.model_dict, x_test = x_test, y_test = y_test, evaluation_metrics = ['f1', 'recall', 'precision', 'f1_weighted'])
mlpl.display_performance()",0.4825690389,
2375,"timing, numpy, plotting",pie.pi_timer.time_plot(),0.4812826216,
2375,"timing, numpy, plotting","ipe.plot_diagnostics(weighted_output, weighted_path)",0.4805169404,
586,explore database,"def filter_snps(data):
    ## get h5 database
    io5 = h5py.File(data.database, 'r')
    hsnps = io5[""snps""]     ## snp locations
    hfilt = io5[""filters""]  ## locus filters
    #hedge = io5[""edges""]    ## edge filters

    ## read in a local copy of the full snps and edge arrays
    snps = hsnps[:]
    #edge = hedge[:]

    ## get locus filter by summing across all filters
    locfilter = hfilt[:].sum(axis=1).astype(np.bool)

    ## apply filter to snps array
    fsnps = snps[~locfilter, ...]

    ## clean up big objects
    del snps
    
    ## return what we want
    return fsnps",0.4122794271,
586,explore database,"def motion2(atom, window):
    #
    # Input parameters:
    #
    #   atom - the turtle object to be moved
    #   window - the window to move the turtle object in
    #     
    
    # First reset it
    atom.home()
    atom.clear()

    # Initial velocities in x and y diretions
    x_vel = 20
    y_vel = -10
    
    # Atom radius, its constant and defined above
    # in atom creation. This is just hack =(
    atom_radius = 50
    
    # We will want to store these numbers in a variable
    height = window.window_height()
    width = window.window_width()

    # The amount of time each iteration moves us forward
    dt = 1

    # Max number of steps
    max_steps = 1000

    for i in range(max_steps):
    
        # Get the current position of the atom
        (x,y) = atom.pos()
    
        print x, y
    
        # Check if moving left or right will put our atom beyond the wall
        if abs(x + dt * x_vel) >= width/2.0 - atom_radius:
        
            # We have moved too far right or left, so flip the x_vel
            x_vel = -x_vel           
    
        # Check if moving up or down will put our atom beyond the wall
        if abs(y + dt * y_vel) >= height/2.0 - atom_radius:
        
            # We have moved too far up or down, so flip the y_vel
            y_vel = -y_vel
        
        # We won't move out of the box, so update the new position
        atom.goto(x + dt*x_vel, y + dt*y_vel)",0.4121689498,
586,explore database,"def download_pdf(term, k):
    """"""
    This function uses the arxiv package and the axriv api to query a search term ""term"" and
    randomly sample k terms and download the pdf.
    """"""
    search_query = arxiv.query(search_query=""earth"", max_results=10000)
    rand_items = random.sample(search_query, k)
    for i in range(0, k):
        arxiv.download(obj=rand_items[i], dirname='./pdf/', slugify=True)


def pdf_path():
    """"""
    Returns the names of the downloaded pdf files
    """"""
    pdfFiles = []
    for filename in os.listdir('./pdf/'):
        if filename.endswith('.pdf'):
            pdfFiles.append(filename)
    return pdfFiles


def read_data(pdfFiles):
    """"""
    This function extracts text from pdf and stores in a dataframe along with filename. 
    Returns the dataframe
    """"""
    df = pd.DataFrame([])
    invalid_pdf = []
    i = 0
    for filename in pdfFiles:
        try:
            pdf = pdfx.PDFx((os.path.join('./pdf/', filename)))
            df = df.append(
                pd.DataFrame(
                    {
                        'papername': filename,
                        'text': pdf.get_text()
                    }, index=[i]))
            i = i + 1
        except:
            invalid_pdf.append(filename)
            continue
    return df


def clean(df):
    """"""
    This function cleans the text extracted from the pdf and returns it.
    """"""
    #make all the text to lower case
    df.text = df.text.str.lower()
    #replace new-line with # for easier cleaning
    df['cleaned'] = df.text.apply(lambda x: re.sub(r""\n"", ""#"", x))
    #Remove references from the text
    df['cleaned'] = df.cleaned.apply(lambda x: re.sub(r'references#.*', """", x))
    #Remove Bibliography from the text
    df['cleaned'] = df.cleaned.apply(
        lambda x: re.sub(r'bibliography#.*', """", x))
    #Replace pound unwanted charecters with space
    df['cleaned'] = df.cleaned.apply(lambda x: re.sub(r""#"", "" "", x))
    df['cleaned'] = df.cleaned.apply(lambda x: re.sub(r""\t"", "" "", x))
    df['cleaned'] = df.cleaned.apply(lambda x: re.sub(r""\r"", "" "", x))
    #Keep only alphabets, fullstops and white spaces
    df['cleaned'] = df.cleaned.apply(
        lambda x: re.sub(r'([^a-zA-Z.""\s]+)', "" "", x))
    #Remove all text inside quotes
    df['cleaned'] = df.cleaned.apply(lambda x: re.sub(r'\""(.+?)\""', "" "", x))
    #Strip all extra white spaces
    df['cleaned'] = df.cleaned.apply(lambda x: re.sub(r""([\s]{2,})"", "" "", x))
    return df


def clean_step2(df):
    """"""
    This function performs, stemming and removes all words with length less than 2
    and splits the text into sentences
    """"""
    stemmer = SnowballStemmer(""english"")
    df.cleaned = df['cleaned'].apply(
        lambda x: ' '.join([word for word in x.split() if len(word) > 2]))
    df.cleaned = df['cleaned'].apply(
        lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))
    df['cleaned'] = df.cleaned.apply(lambda x: re.sub(r""([\s]{2,})"", "" "", x))
    df['cleaned_split'] = df.cleaned.apply(lambda x: x.split('. '))
    return df


def avg_wordpl(doc):
    """"""
    Function calculates and returns average number of words per sentence.
    """"""
    wrd = 0
    ln = 0
    for line in doc:
        wrd += len(line.split())
        ln += 1
    return (round(wrd / ln))


def drop_sen(doc, k):
    """"""
    Function to drop all sentences that have words less than k-grams
    """"""
    sentence = [line for line in doc if len(line.split()) > 5]
    return sentence


def apply_kgram(doc, k):
    """"""
    Takes a document and the number of grams as input and returns the kgram list as output
    """"""
    doc_kgram = []
    for line in doc:
        doc_kgram.append(make_kgrams(line, k))
    return doc_kgram


def make_kgrams(sentence, k):
    """"""
    Takes a sentence and number of grams as input and returns the k-gram for that sentence as output
    """"""
    words = sentence.split()
    zip_list = zip(*[words[i:] for i in range(k)])
    list_z = [list(a) for a in zip_list]
    return [' '.join(list(i)) for i in list_z]


def kgram_indexer(df):
    """"""
    Creates a dataframe of K_grams and corresponding document number and sentence number
    """"""
    kgram_index = pd.DataFrame([])
    d = 1
    for doc in df.kgrams:
        s = 1
        for sen in doc:
            for kgram in sen:
                kgram_index = kgram_index.append(
                    pd.DataFrame(
                        {
                            'kgram': kgram,
                            'sentence': s,
                            'doc': d
                        }, index=[0]),
                    ignore_index=True)
            s += 1
        d += 1
    return kgram_index


def fingerprinting(df, w):
    """"""
    This Function takes the windows size and performs the winnowing algorithm. Returns the fingerprint.
    """"""
    kg = []
    fingerprint = pd.DataFrame([])
    for d in range(max(df.doc) + 1):
        doc = df[df.doc == d]
        for i in range(max(doc.sentence) + 1):
            kg = list(doc[doc.sentence == i].hash)
            if (len(kg) < w):
                fingerprint = fingerprint.append(
                    pd.DataFrame(
                        {
                            'fingerprint': min(kg),
                            'doc_num': d,
                            'sentence_no': i
                        },
                        index=[0]),
                    ignore_index=True)
            else:
                for j in range((len(kg) - w + 1)):
                    fingerprint = fingerprint.append(
                        pd.DataFrame(
                            {
                                'fingerprint': min(kg[j:j + w]),
                                'doc_num': d,
                                'sentence_no': i
                            },
                            index=[0]),
                        ignore_index=True)
    return fingerprint


def shifting(df):
    """"""
    Function removes similar consecutive hashes and outputs the final fingerprint dataframe 
    """"""
    shifted = df.fingerprint.loc[df.fingerprint.shift(-1) != df.fingerprint]
    df_final = pd.DataFrame([])
    for i in shifted.index:
        df_final = df_final.append(
            pd.DataFrame(
                {
                    'fingerprint': df_fingerprint.iloc[i].fingerprint,
                    'doc_num': df_fingerprint.iloc[i].doc_num,
                    'sentence_no': df_fingerprint.iloc[i].sentence_no
                },
                index=[0]),
            ignore_index=True)
    return df_final


def test_path():
    """"""
    Returns the names of the test files
    """"""
    testFiles = []
    for filename in os.listdir('./Test//'):
        if filename.endswith('.txt'):
            testFiles.append(filename)
    return testFiles


def read_testdata(testFiles):
    """"""
    This function extracts text from pdf and stores in a dataframe along with filename. 
    Returns the dataframe
    """"""
    df = pd.DataFrame([])
    i = 0
    for filename in testFiles:
        try:
            file = open((os.path.join('./Test/', filename)), ""r"")
            df = df.append(
                pd.DataFrame(
                    {
                        'papername': filename,
                        'text': file.read()
                    }, index=[i]))
            i = i + 1
        except:
            invalid_pdf.append(filename)
            continue
    return df


def comparison(result):
    """"""
    Checks for overlaps in fingerprints
    """"""
    dk = []
    for i in range(max(result.doc_num) + 1):
        dk.append(
            result[result.doc_num == i].fingerprint.values.flatten().tolist())

    setdk = []
    for i in dk:
        setdk.append(set(i))

    dic = {}
    for i in range(0, len(setdk)):
        list_of_matches = []
        for j in range(i + 1, len(setdk)):
            fir = setdk[i].intersection(setdk[j])
            if fir:
                list_of_matches.append((j, fir))
        dic[i] = list_of_matches
        plag = pd.DataFrame([])
    for doc, match in dic.items():
        for l in match:
            if len(l[1]) > 2:
                plag = plag.append(
                    pd.DataFrame(
                        {
                            'doc_num': doc,
                            'match_doc': l[0],
                            'match_print': l[1]
                        },
                        index=[0]),
                    ignore_index=True)
    for i, row in plag.iterrows():
        plag.match_print[i] = list(plag.match_print[i])
    return plag


def getcommonsentence(plag):
    """"""
    Gets the sentence which was plagiarised, depending on the fingerprint
    """"""
    plag['CopiedSentence'] = """"
    for i, row in plag.iterrows():
        sen = []
        for fprint in row.match_print:
            z = row.doc_num
            x = min(kgram_index[kgram_index.hash == fprint].sentence)
            sen.append(df.dropped[z][x])
        plag['CopiedSentence'][i] = sen
    plag.CopiedSentence = plag.CopiedSentence.apply(lambda x: list(set(x)))
    plag['NumberOfCopied'] = plag.CopiedSentence.apply(lambda x: len(x))
    return plag",0.4075449705,
586,explore database,"nmc_cache = None
def power_ratios(data):
    # Convert NMC table to dictionary
    query = '''
        SELECT stabb, year, milexp, milpers, totalpop, urbanpop FROM MIDNMC WHERE year >= 1945;
        '''
    global nmc_cache
    if not nmc_cache:
        df_nmc = pd.read_sql_query(query, cnx)
        df_nmc.replace(-9, 0, inplace=True) # replace 'unknown' with zero
        nmc_cache = df_nmc.set_index(['stabb', 'year']).to_dict(orient='index')
    
    nmc_ = nmc_cache
    def get_ratio(row, item=None, dict_=None):
        """"""
        Given dataframe row and datapoint name, return the ratio of state B
        to state A.
        """"""
        result = np.nan
        a_val = get_value(row.a_abb, row.year, item, dict_)
        if a_val:
            b_val = get_value(row.b_abb, row.year, item, dict_)
            if b_val:
                result = b_val / a_val
        return result

    def get_value(stabb, year, item, dict_):
        """"""
        Get NMC value from dictionary. If no value is present for the
        requested year, check N=4 years before or after and return the
        first value found.
        """"""
        result = dict_.get((stabb, year), None)
        if not result:
            for i in range(1, 5):
                result = dict_.get((stabb, year - i), None)
                if not result:
                    result = dict_.get((stabb, year + i), None)
                if result:
                    break
        return result[item] if result else None
    
    result = data.copy()
    for col in ['milexp', 'milpers', 'totalpop', 'urbanpop']:
        result['r_' + col] = result.apply(get_ratio, axis=1, item=col, dict_=nmc_)
#        result['r_' + col].replace(np.nan, np.median(result['r_' + col].dropna()), inplace=True)

    return result",0.3988348246,
586,explore database,"if GRABREPORTS:
    conn = sq.connect(os.path.join(RADDIR,""DBs"",
                                   ""criticalFindingsAll.sqlite""))
    cursor = conn.cursor()

    r_date  = re.compile(r""""""([0-9]{1,2}(/[0-9]{1,2})?/[0-9]{2,4})"""""")
    r_time = re.compile(r""""""([0-9]{1,2}:\d\d(pm|am)?)"""""")
    cursor.execute(""""""SELECT rowid,impression from reports"""""")
    data = [(d[0],d[1].lower()) for d in cursor.fetchall()]
    data =[(d[0],r_date.sub("""",r_time.sub("""",d[1]))) for d in data]

    reports = [d[1] for d in data]

    def report2sentences(report):
        return TextBlob(report).raw_sentences

    sentences = list(itertools.chain.from_iterable(dview.map_async(report2sentences,reports).get()))",0.3979113102,
586,explore database,"# %load analyse_tweeter.py

def analyse_tweeter(user_handle):
    # latest tweet_id
    largest_id = 0

    # Counter
    counter = 1
    
    while counter < 100 :
       # Loop through 5 pages of tweets (total 100 tweets)
        for page in tweepy.Cursor(api.search, q=target_term, max_id=largest_id, count = 25).pages(4):

            if not largest_id:
                largest_id = page[0].id
            if counter < 100 :
                # Loop through all tweets
                for tweet in page:
                # Run Vader Analysis on each tweet
                    score = analyze_tweet(tweet)
                    tweets_ago = counter

                    # Add sentiments for each tweet into an array
                    sentiments.append({ ""Source"": user_handle,
                                       ""Tweet Text"": tweet.text,
                                       ""Tweet Date"" : tweet.created_at,
                                       ""Compound"": score[""compound""],
                                       ""Positive"": score[""pos""],
                                       ""Negative"": score[""neg""],
                                       ""Neutral"": score[""neg""],
                                       ""Tweets Ago"": counter})

                    # Add to counter 
                    counter = counter + 1


    return sentiments",0.3958062828,
586,explore database,"self.tts.say(""Grab the red ball and hide it from me."" + 
                     "" I will tell you once I see the ball."")
        
        self.lastSeen = 0
        self.lastNotSeen = 0
        
        START = time.time()

        while time.time() - START < 10:

            cover_eyes(self)

            if time.time() - self.lastNotSeen > 5:
                self.tts.say(""Where's the ball? I don't see the ball"")
                self.lastNotSeen = time.time()    
        
        uncover_eyes(self)",0.3955875039,
586,explore database,"def qwi_features(year, db_name = db_name, hostname = hostname, overwrite = False):
    conn = psycopg2.connect(database=db_name, host = hostname) #database connection
    cursor = conn.cursor()
    
    sql_script = '''
    DROP TABLE IF EXISTS ada_kcmo.features_qwi_{year};
    CREATE TABLE ada_kcmo.features_qwi_{year} AS
    SELECT a.*
            , b.nb_jobs_current_qtr AS m1_nb_jobs_current_qtr
            , b.emp_current_qtr AS m1_emp_current_qtr
            , b.emp_4qtrs_ago AS m1_emp_4qtrs_ago
            , b.emp_3qtrs_ago AS m1_emp_3qtrs_ago
            , b.emp_2qtrs_ago AS m1_emp_2qtrs_ago
            , b.emp_prev_qtr AS m1_emp_prev_qtr
            , b.emp_next_qtr AS m1_emp_next_qtr
            , b.emp_begin_qtr AS m1_emp_begin_qtr
            , b.emp_end_qtr AS m1_emp_end_qtr
            , b.emp_full_qtr AS m1_emp_full_qtr
            , b.accessions_current AS m1_accessions_current
            , b.accessions_consecutive_qtr AS m1_accessions_consecutive_qtr
            , b.accessions_full_qtr AS m1_accessions_full_qtr
            , b.separations AS m1_separations
            , b.new_hires AS m1_new_hires
            , b.recalls AS m1_recalls
    FROM(
        SELECT * 
        FROM ada_kcmo.qwi_ein_{year}_1
    ) AS a
    LEFT JOIN (
        SELECT *
        FROM ada_kcmo.qwi_ein_{year_m1}_1
    ) AS b
    ON a.ein = b.ein;
    
    ALTER TABLE ada_kcmo.features_qwi_{year} OWNER TO ada_kcmo_admin; 
    
    COMMIT;
    '''.format(year = year, year_m1 = year-1)
    
    # Let's check if the table already exists:
    cursor.execute('''
    SELECT * FROM information_schema.tables 
    WHERE table_name = 'features_qwi_{year}'
    AND table_schema = 'ada_kcmo';
    '''.format(year = year))
    # Let's write table if it does not exist (or if overwrite = True)
    if not(cursor.rowcount) or overwrite:
        cursor.execute(sql_script)
    
    cursor.close()
    
    df = pd.read_sql('SELECT * FROM ada_kcmo.features_qwi_{};'.format(year), conn)
    
    for var in ['nb_jobs_current_qtr', 'emp_current_qtr'
                , 'emp_4qtrs_ago', 'emp_3qtrs_ago', 'emp_2qtrs_ago', 'emp_prev_qtr', 'emp_next_qtr'
                , 'emp_begin_qtr', 'emp_end_qtr', 'emp_full_qtr'
                , 'accessions_current', 'accessions_consecutive_qtr', 'accessions_full_qtr'
                , 'separations', 'new_hires', 'recalls']:
        m1_var = 'm1_{}'.format(var)
        change_var = 'change_{}'.format(var)
        df[change_var] = df[var] - df[m1_var]
   
    # Remove NULL rows
    isnan_rows = df.isnull().any(axis=1)
    df = df[~isnan_rows]
    
    return df",0.3954587579,
586,explore database,"def wages_features(year, db_name = db_name, hostname = hostname, overwrite = False):
    
    conn = psycopg2.connect(database=db_name, host = hostname) #database connection
    cursor = conn.cursor()

    sql_script = '''
    DROP TABLE IF EXISTS ada_kcmo.features_wages_{year};
    CREATE TABLE ada_kcmo.features_wages_{year} AS    
    SELECT ein, run, ui_acct
            , mon1_empl+mon2_empl+mon3_empl AS total_empl
            , total_wage 
    FROM kcmo_lehd.mo_qcew_employers 
    WHERE year = {year} AND qtr = 1;
    
    ALTER TABLE ada_kcmo.features_wages_{year} OWNER TO ada_kcmo_admin; 
    
    COMMIT;
    '''.format(year = year)
    
    # Let's check if the table already exists:
    cursor.execute('''
    SELECT * FROM information_schema.tables 
    WHERE table_name = 'features_wages_{year}'
    AND table_schema = 'ada_kcmo';
    '''.format(year = year))
    
    # Let's write table if it does not exist (or if overwrite = True)
    if not(cursor.rowcount) or overwrite:
        cursor.execute(sql_script)
    
    cursor.close()    
    
    df = pd.read_sql('SELECT * FROM ada_kcmo.features_wages_{}'.format(year), conn)
    df['avg_wage'] = df['total_wage']/df['total_empl']
    
    # Flag null, infinite average wage values
    mask = ((df['avg_wage'].isnull()) | (df['avg_wage'] == inf))
    vals_to_replace = df[mask]['avg_wage'].values
    df['avg_wage'].replace(vals_to_replace,np.NaN, inplace=True)
    
    # Impute the median wage value
    df['avg_wage'].fillna(df['avg_wage'].median(), inplace=True)
    
    # Remove Outliers
    outlier_rows = ((df['avg_wage'] == 0) | (df['avg_wage'] > 50000))
    df_wages = df[~outlier_rows]
    
    # Scaling values
    df['total_wage_scaled'] = scaling_var(df, 'total_wage')
    df['total_empl_scaled'] = scaling_var(df, 'total_empl')
    df['avg_wage_scaled'] = scaling_var(df, 'avg_wage')
    
    return df",0.3949247301,
586,explore database,"def run_query(query_name):
    """"""
    opens odbc connection and runs query
     saves results as table, closes connection
    :param query_name: name of query statement to run
    :param out_db: name of output database
    :param out_name: name of output table name
    :return: table of results saved on impala in specified output dir
    """"""
    # create connection object
    conn=connect(host=impala_host, port=21050, timeout=120)
    cur = conn.cursor()
    # run query
    print 'Running the query on impala....'
    cur.execute(query_name)
    # download results as pandas dataframe
    results_df = as_pandas(cur)
    if len(results_df) > 0:
        print 'Query finished. Closing connection.'
        return results_df
    else:
        print 'No results found.'
    cur.close()
    conn.close()",0.3897403777,
1497,pivot table,"# Jake Kara's code
def time_series(df, cols=""Year"", vals=""Rate per 100000"", index=""Geography""):
    
    """""" Convert a dataframe to a timeseries with one column per year """"""
    
    return pd.pivot_table(df, columns=cols, 
                         values=vals,
                         index=index)",0.485873431,
1497,pivot table,"air_reserve = extract_dates(pd_df = air_reserve, target_var = 'visit_datetime', prefix='target')
air_reserve = extract_dates(pd_df = air_reserve, target_var = 'reserve_datetime')
air_reserve.pivot_table(columns='reserve_datetime_weekday')",0.4844336212,
1497,pivot table,"def pivot(df):
    """"""Given a dataframe with columns named Count, State, and Year, returns a
    pivot table data frame where rows are indexed by state, columns represent
    each year found in the Year data series, and numbers are combined using
    the numpy.sum function
    
    Parameters
    ----------
    df : pandas dataframe
    """"""
    return df.pivot_table(values='Count', index='State', columns='Year', aggfunc=np.sum, fill_value=0)",0.4786617756,
1497,pivot table,"def pivot_by_ending(names):
    """"""Find the last letter of each name in the given table and pivot by
    that value and by gender.
    
    Args:
        names (pd.DataFrame): A table like `names_in_2014`, having at least
                              the following columns:
                               * 'Name': A column of strings
                               * 'Gender': A column of strings 'F' or 'M'
                               * 'Count': A column of integers, the number
                                 of individuals represented by each row
    Returns:
        pd.DataFrame: A table pivoted by last letter of name and by gender.
                      See above for an example.""""""
    # The next line will create a copy of names with an
    # extra column called ""Ending"".  Replace the ... with
    # a Series or array containing the last letters of
    # each name.
    with_ending = names.assign(Ending = ...)

    # Fill in the ellipses here to pivot your table.
    return with_ending.pivot_table(
        index=[...], # column(s) that will index the pivoted table
        columns=[...], # Each value in this column (or columns) will get its own column in the pivoted table.
        values=..., # the field to process in each group
        aggfunc=..., # the operation to apply to the collection of field values in each group
        fill_value=... # the default value for an empty cell in the resulting table
    )

sample_names_2014_by_ending = pivot_by_ending(sample_names_2014)",0.4779612422,
1497,pivot table,"# Just run this cell.

def tvd(t, conditions, values):
    """"""Compute the total variation distance 
    between proportions of values under two conditions.
    
    t          (Table) -- a table
    conditions (str)   -- a column label in t; should have only two categories
    values     (str)   -- a column label in t
    """"""
    counts = t.pivot(conditions, values)
    categoryA = np.array(counts.labels).item(1)
    categoryB = np.array(counts.labels).item(2)
    props = proportions(counts, [categoryA, categoryB])
    a = props.column(1)
    b = props.column(2)
    return 0.5*sum(abs(a - b))

tvd(couples, 'Gender', 'Relationship Rating')",0.4696782529,
1497,pivot table,"def max_deviation(t, conditions, values):
    """"""Compute the maximum difference for any value
    between proportions of values under two conditions.
    
    t          (Table) -- a table
    conditions (str)   -- a column label in t; should have only two categories
    values     (str)   -- a column label in t
    """"""
    counts = t.pivot(conditions, values) #SOLUTION
    categoryA = np.array(counts.labels).item(1) #SOLUTION
    categoryB = np.array(counts.labels).item(2) #SOLUTION
    props = proportions(counts, [categoryA, categoryB]) #SOLUTION
    a = props.column(1) #SOLUTION
    b = props.column(2) #SOLUTION
    return max(abs(a - b)) #SOLUTION

max_deviation(couples, 'Gender', 'Relationship Rating')",0.4696782529,
1497,pivot table,"# Function that takes movielens dataframe and return the user-movie dataframe with rating as values
def user_movie_df(data):
    d = data.pivot_table(index = 'userid', columns = 'movietitle', values = 'rating')
    del d.index.name
    return d",0.4647223353,
1497,pivot table,"def crosstab(df, index, columns):
    """"""Compute a cross-tabulation.
    
    df: DataFrame
    index: variable(s) that will label the rows
    columns: variable(s) that will label the columns
    
    returns: DataFrame
    """"""

    xtab = df.pivot_table(index=index, 
                          columns=columns,
                          values='id',
                          aggfunc='count')
    
    return xtab",0.4622294605,
1497,pivot table,"def cleanup_nulls(df, variablename):
  df2 = df.pivot_table(variablename, 'date', 'stationid', fill_value=np.nan)
  print('Before: {} null values'.format(df2.isnull().sum().sum()))
  df2.fillna(method='ffill', inplace=True)
  df2.fillna(method='bfill', inplace=True)
  df2.dropna(axis=1, inplace=True)
  print('After: {} null values'.format(df2.isnull().sum().sum()))
  return df2",0.4609963298,
1497,pivot table,"def create_user_item_df(ratings_df):
    """"""
    Convert books_amazon_whole dataframe into user-item ratings dataframe.
    """"""
    
    # books_amazon_whole.drop_duplicates(subset = ['BookTitle', 'UserID'], inplace = True)
    user_item_df = pd.pivot_table(data = ratings_df, index = 'UserID', columns = 'BookTitle', values = 'Score')

    ## Fill NAs with 0 and get the new list of book-titles:
    user_item_df.fillna(0, inplace=True)

    print ('Shape: ', user_item_df.shape)
    
    return user_item_df",0.4606665969,
15,activation functions,"def network_activations(w1,w2,x):
    a1 = # ... YOUR CODE CALCULATING THE ACTIVATIONS OF THE HIDDEN LAYER COMES HERE ...
    a2 = # ... YOUR CODE CALCULATING THE ACTIVATIONS OF THE OUTPUT LAYER COMES HERE ...
    return (a1,a2)",0.4398279786,
15,activation functions,"def genFiles(data,cat,subcats,supercat):
    
    global activities
    
    # Create Dirs
    createAssetDir(cat)
    createSourceDir(cat)
    
    # Iterate oer the keys
    for header in subcats:
        
        # SubCat name
        conteudo = header+'\n'

        # Generate content
        for item in data[header]:
            conteudo += genContent(item)

        # Remove remaing lines
        conteudo = conteudo[0:-2]

        # Generate file name
        filename = genFileName(header)

        # Generate class name
        nomeClasse = genClassName(filename)

        # Generate class content
        classeData = generateClass(cat,nomeClasse,filename,header)

        # Save Data
        saveFile(getAssetDir(cat),filename,conteudo)

        # Save class
        saveFile(getSourceDir(cat),nomeClasse+'.java',classeData)

        # Create manifest
        activities.append(generateSubCatActivity(cat,nomeClasse,supercat))",0.4186868668,
15,activation functions,"#plot graph for different ""values"" of the activation function
def gpNN(X):
    NN = MLPClassifier(activation=X)
    NN.fit(X_train_mnist, y_train_mnist)
    return NN

plt.plot([1, 2, 3, 4], [av_accuracy(gpNN('logistic'),X_train_mnist, y_train_mnist), av_accuracy(gpNN('relu'), X_train_mnist, y_train_mnist), av_accuracy(gpNN('identity'), X_train_mnist, y_train_mnist), av_accuracy(gpNN('tanh'), X_train_mnist, y_train_mnist)], ""ro"")
plt.ylabel('accuracy')
plt.xlabel('value of for the activation function') #here 1 represents ""logistic"", 2 ""relu"", 3 ""identity"" and 4 ""tahn
plt.title('plot different values for different activation functions ')
plt.show()",0.4095560312,
15,activation functions,"class Layer:
    def __init__(self, n_inputs, n_outputs, lr, activation):
        self.lr = lr
        self.activation = activation
        self.n_inputs = n_inputs
        self.n_outputs = n_outputs
        self.weights = []
        self.bias = []
        for i in range(n_outputs):
            self.weights.append([np.random.randn() for _ in range(n_inputs)])
            self.bias.append(np.random.randn())
            
    def activation_fn(self, x):
        if self.activation == 'sigmoid':
            return 1.0/(1.0+np.exp(-x))
        elif self.activation == 'tanh':
            return np.tanh(x)
                            
    def activate(self, inputs):
        outputs = []
        for i in range(self.n_outputs): 
            # multiply weights by inputs
            outputs.append(sum(np.multiply(self.weights[i], inputs)))
        # add bias and apply activation function
        self.outputs = np.add(outputs, self.bias)
        self.outputs = self.activation_fn(self.outputs)
        return outputs
    
    def update_weights(self, inputs, gammas, lr):
        for o in range(self.n_outputs):
            for i in range(len(self.weights[o])):
                self.weights[o][i] -= lr * gammas[o] * inputs[i]                
                self.bias[o] -= lr * gammas[o]",0.3900027275,
15,activation functions,"class Activation(Layer):
    """"""
    An activation layer just applies a function
    elementwise to its inputs
    """"""
    def __init__(self, f: Func, f_prime: Func) -> None:
        super().__init__()
        self.f = f
        self.f_prime = f_prime

    def forward(self, inputs: Tensor) -> Tensor:
        self.inputs = inputs
        return self.f(inputs)

    def backward(self, grad: Tensor) -> Tensor:
        """"""
        if y = f(x) and x = g(z)
        then dy/dz = f'(x) * g'(z)
        """"""
        return self.f_prime(self.inputs) * grad",0.3858308792,
15,activation functions,"def compile_network(self) : 
    """""" Compile the network  
    """"""
    if self.network is not None : 
        return

    self.network = theano.function(inputs = [self.i],
                                   outputs = self.network_outputs)

# bind the method to the class
embedding_model.compile_network = compile_network

def compile_loss(self) : 
    """""" Compile the loss
    """"""

    if self.loss is not None : 
        return

    self.loss = theano.function(inputs = [self.I,self.J],
                                outputs = self.loss_outputs,
                                updates = self.loss_updates)

# bind the method to the class
embedding_model.compile_loss = compile_loss
        
def compile_sgd_update(self) : 
    """""" Compile SGD update function
    """"""

    if self.sgd_update is not None : 
        return

    self.sgd_update = theano.function(inputs = [self.I,self.J,
                                                self.eta,self.sgd_i],
                                      outputs = [],
                                      updates = self.sgd_updates)

# bind the method to the class
embedding_model.compile_sgd_update = compile_sgd_update",0.3830058575,
15,activation functions,"class Agent:
    def __init__(self, bandit, epsilon):
        # epsilon to control the type of agent (Explorer or greedy)
        self.epsilon = epsilon
        # number of times action was chosen
        self.n_actions = np.zeros(bandit.k, dtype=np.int) 
        # Estimated values
        self.Q = np.zeros(bandit.k, dtype=np.float)
    
    # Update Q action-value based on the formula
    # Q(a) = Q(a) + 1/(k+1) * (r(a) - Q(a))
    def update_Q(self, action, reward):
        self.n_actions[action] += 1
        self.Q[action] += (1/self.n_actions[action]) * (reward - self.Q[action])
        
    def choose_action(self, bandit, force_explore=False):
        rand = np.round(np.random.random(),2)
#         print(' rand: ', rand)
        if (rand < self.epsilon) or force_explore:
#             print(' I wanna explore')
            action_explore = np.random.randint(bandit.k) #Explore random bandits?
            return action_explore
        else:
#             print(' I wanna stay')
#             print(' Q', self.Q)
#             print(' Q_max:', self.Q.max())
#             print (' flat: ',np.flatnonzero(self.Q == self.Q.max()))
            action_greedy = np.random.choice(np.flatnonzero(self.Q == self.Q.max()))
            return action_greedy",0.3784198463,
15,activation functions,"def feedforward(self, inputs):
    #Feed inputs into the network and perform a forward pass to get activities of all neurons
    
    #Just in case we enter the wrong number of inputs

    
    #Loop over the number of input neurons and set the activities equal to the inputs

    
    #Loop over the number of hidden neurons

        
        #For each hidden neuron j, loop over the input neurons and calculate the total input into j

            
        #return the sigmoid of the total input

        
    #Do the same thing with the output neurons

        
    #Return the activities of the output neurons",0.3744775355,
15,activation functions,"def FCBlock(model):
    model.add(Dense(4096, activation='relu'))
    model.add(Dropout(0.5))",0.3743676543,
15,activation functions,"#Why 4096? Why relu?
def FCBlock(model):
    model.add(Dense(4096, activation='relu'))
    model.add(Dropout(0.5))",0.3743676543,
2061,step delete the unnamed column,"#let's remove all the columns that match items in the stopword list:
def remove_stopwords(df):
    for col in df.columns:
        if col in stopwords.words('danish'):
            df.drop(col,inplace=True,axis=1)
    return df",0.5216999054,
2061,step delete the unnamed column,"def drop_cols(colnames):
    for df in [X_train, X_val, X_test, X]:
        df.drop(colnames, axis=1, inplace=True)
drop_cols('customerID')
X_train.tail()",0.5215229988,
2061,step delete the unnamed column,"# Learning algorithm 
def learn(trainingData, features_to_drop):


    # trainingY = pd.get_dummies(trainingData['Condition'], 'Condition')
    trainingY = trainingData['label']

    trainingData.drop(features_to_drop, axis=1, inplace=True)

    # Split my data into train and test to avoid overfiting
    X_train, X_test, Y_train, Y_test = train_test_split(trainingData, trainingY)

    #  I will train a Support Vector Machine classifier
    #     note: I tried with a Logistic Regression but I only got 68% accuracy

    classifier = SVC()
    # classifier = SVC(kernel='rbf', verbose=True)
#     classifier = SVC(kernel='poly',degree=2)
    # classifier = LogisticRegression(C=1e5)
    # classifier = KNeighborsClassifier()
    classifier.fit(X=X_train, y=Y_train)

    # Now I'll check the accuracy of my model
    train_ac = classifier.score(X=X_train, y=Y_train)
    test_ac = classifier.score(X=X_test, y=Y_test)
    print('Training accuracy: {}   -  Testing accuracy: {} '.format(train_ac,test_ac))
    
    return classifier",0.5203243494,
2061,step delete the unnamed column,"if train[""Fare""][0] != 0:
    train['Fare'].fillna(train['Fare'].dropna().mode()[0], inplace=True)
    fare = []
    for i in train[""Fare""]:
        if i <= 7.910400:
            fare = np.append(fare,0)
        elif i < 14.454200:
            fare = np.append(fare,1)
        elif i < 31.000000:
            fare = np.append(fare,2)
        else:
            fare = np.append(fare,3)
train[""Fare""] = fare = fare.astype(int)",0.516970396,
2061,step delete the unnamed column,"def missingEmbark(data):
    ModeEmb = data['Embarked'].mode()[0]
    data['Embarked'].loc[data['Embarked'].isnull()] = ModeEmb
    return(data)",0.5137599111,
2061,step delete the unnamed column,"def train_holdout_split(dataframe, target_variable):
    feature_cols = [x for x in dataframe.columns.values]
    feature_cols.remove(target_variable)
    features = dataframe[feature_cols]
    target = dataframe[target_variable]
    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.33, random_state=42)
    training = pd.concat([X_train,y_train], axis=1)
    hold_out = pd.concat([X_test,y_test], axis=1)
    return [training, hold_out]",0.512288332,
2061,step delete the unnamed column,"# function takes in the dataframe and the columns need to be transformed in the dataframe
def onehotencoding(df, col_list):
    df_totransform = df[col_list]
    
    df.drop(col_list, axis = 1)
    
    ohe = OneHotEncoder(sparse=False)
    array_ohe = ohe.fit_transform(df_totransform)
    df_categorical_ohe = pd.DataFrame(array_ohe)
    
    df_categorical_ohe = df_categorical_ohe.reset_index()
    df_noncategorical = df.reset_index()
    df_ohe = pd.concat([df_categorical_ohe, df_noncategorical], axis=1)
    
    return df_ohe",0.5097317696,
2061,step delete the unnamed column,"# reverse beta parameters for shock and loss to match gain
def reverseBeta(model_param_df):
    model_param_df.loc[(model_param_df.task == 'shock') & (model_param_df.parameter == 'mag_diff_rl'), 'beta'] = model_param_df.loc[(model_param_df.task == 'shock')& (model_param_df.parameter == 'mag_diff_rl'),'beta']*-1
    model_param_df.loc[(model_param_df.task == 'shock') & (model_param_df.parameter == 'prob_diff_rl'), 'beta'] = model_param_df.loc[(model_param_df.task == 'shock') & (model_param_df.parameter == 'prob_diff_rl'),'beta']*-1
    model_param_df.loc[(model_param_df.task == 'loss') & (model_param_df.parameter == 'prob_diff_rl'), 'beta'] = model_param_df.loc[(model_param_df.task == 'loss') & (model_param_df.parameter == 'prob_diff_rl'),'beta']*-1
    return(model_param_df)",0.509309113,
2061,step delete the unnamed column,"def clean_titanic_df(df):
    # just to avoir issue
    if 'Ticket' in df.columns:
        df.drop(['PassengerId'], axis=1, inplace=True)
        
        # next keep Name ! Use Mrs. Miss. and Mr.
        df.loc[df['Name'].str.contains('Mrs.'), 'Name'] = 'Mrs'
        df.loc[df['Name'].str.contains('Mme.'), 'Name'] = 'Mrs'
        df.loc[df['Name'].str.contains('Mr.'), 'Name'] = 'Mr'
        df.loc[df['Name'].str.contains('Mlle.'), 'Name'] = 'Miss'
        df.loc[df['Name'].str.contains('Ms.'), 'Name'] = 'Miss'
        df.loc[df['Name'].str.contains('Miss.'), 'Name'] = 'Miss'
        
        # Important states ?
        df.loc[df['Name'].str.contains('Master.'), 'Name'] = 'Master'
        df.loc[df['Name'].str.contains('Rev.'), 'Name'] = 'Rev'
        df.loc[df['Name'].str.contains('Dr.'), 'Name'] = 'Dr'
        df.loc[df['Name'].str.contains('Don.'), 'Name'] = 'Don'
        df.loc[df['Name'].str.contains('Countess.'), 'Name'] = 'Countess'
        df.loc[df['Name'].str.contains('Jonkheer.'), 'Name'] = 'Jonkheer'
        df.loc[df['Name'].str.contains('Col.'), 'Name'] = 'Col'
        
        # Boat states ?
        df.loc[df['Name'].str.contains('Major.'), 'Name'] = 'Boat'
        df.loc[df['Name'].str.contains('Capt.'), 'Name'] = 'Boat'
        
    df['Embarked'].fillna(method='ffill', inplace=True)
    df['Age'].fillna(df['Age'].mean(), inplace=True)
    df['Fare'].fillna(df['Fare'].mean(), inplace=True)
    df['Cabin'].fillna('Unknown', inplace=True)
    df['Ticket'].fillna('Unknown', inplace=True)

clean_titanic_df(titanic_train)
clean_titanic_df(titanic_test)
clean_titanic_df(titanic)
titanic_train.isnull().any()",0.5085018277,
2061,step delete the unnamed column,"def formalizePersonalInfo(rawDF, identity):
    """"""
    Select valid ages and rename some columns of the dataframe containin personal information
    Args:
        rawDF (DataFrame): DataFrame to be formalized
        df (DataFrame): Formalized dataframe
    """"""
    return rawDF[rawDF['age_id'] == 5.0].drop('age_id', axis=1)\
.rename(columns={""age_num"": identity + ""_age"",""sex_code"": identity + ""_sex"", ""race_desc"": identity + ""_race"", \
                ""ethnicity_name"": identity + ""_ethnicity"", ""resident_status_code"": identity + ""_resident_status""})

# First select the data frame with victim_id and offender_id, and merge other dataframes into it by matching victim_id.
dataframe = formalizePersonalInfo(SC2016[""nibrs_victim_offender_rel""].merge(SC2016['nibrs_victim'], how='left', on='victim_id')\
                                  .merge(SC2016['nibrs_incident'], how='left', on='incident_id')\
                                  [['victim_id', 'offender_id', 'incident_id', 'relationship_name',\
                                    'victim_seq_num', 'victim_type_name','age_id', 'age_num', 'sex_code','month_num', \
                                    'race_desc', 'ethnicity_name', 'resident_status_code', 'incident_date']], 'victim')

# Continue to merge dataframes to diversify the information.
dataframe = formalizePersonalInfo(dataframe.merge(SC2016['nibrs_victim_injury'], how = 'left', on = 'victim_id')
                                  .merge(SC2016['nibrs_injury'], how = 'left', on = 'injury_id')\
                                  .drop(['injury_id', 'injury_code'], axis=1)\
                                  .merge(SC2016['nibrs_offender'], how = 'left',\
                                         on = ['incident_id', 'offender_id'])\
                                  .drop(['ff_line_number', 'age_range_low_num', 'age_range_high_num'], axis = 1)\
                                  .merge(SC2016['nibrs_offense'], how = 'left', on = 'incident_id')\
                                  .drop(['attempt_complete_flag', 'location_name', 'num_premises_entered',\
                                         'method_entry_code', 'ff_line_number'], axis=1)\
                                  .merge(SC2016['nibrs_bias_motivation'], how = 'left', on = 'offense_id')\
                                  .merge(SC2016['nibrs_bias_list'], how = 'left', on = 'bias_id')\
                                  .drop(['victim_id', 'offender_id', 'incident_id', 'offense_id', 'bias_id',\
                                         'bias_code'], axis=1), 'offender')

dataframe.tail()",0.5082092881,
1914,shortcut pca in scikit learn,"def get_shot_xG(shot):
    return lr.predict_proba(shot.reshape(1,-1))[:,1][0]",0.4441987872,
1914,shortcut pca in scikit learn,"from pycausal import search as s
Pc = s.pc(df, dataType = 0, depth = -1, alpha = 0.05, verbose = True)",0.4440609515,
1914,shortcut pca in scikit learn,"from pycausal import search as s
cpc = s.cpc(df,dataType = 0, depth = -1, alpha = 0.05, verbose = True)",0.4430762231,
1914,shortcut pca in scikit learn,"class ThetaPosterior(stats.rv_continuous):
    def _pdf(self, x):
        return posterior_fn(x)
    
    def _cdf(self, r):
        # This function is needed for .rvs to converge.
        n = 10000
        x = np.linspace(0, r, n)
        return np.sum(posterior_fn(x) / ((n + 1) / r))

theta_posterior = ThetaPosterior()
draws = theta_posterior.rvs(size=1000)
plt.hist(draws, 40);
import pandas as pd
pd.Series(draws).describe()",0.4426835775,
1914,shortcut pca in scikit learn,"# From https://github.com/bckenstler/CLR for Cyclical Learning Rate
from keras.callbacks import *

class CyclicLR(Callback):
    """"""This callback implements a cyclical learning rate policy (CLR).
    The method cycles the learning rate between two boundaries with
    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).
    The amplitude of the cycle can be scaled on a per-iteration or 
    per-cycle basis.
    This class has three built-in policies, as put forth in the paper.
    ""triangular"":
        A basic triangular cycle w/ no amplitude scaling.
    ""triangular2"":
        A basic triangular cycle that scales initial amplitude by half each cycle.
    ""exp_range"":
        A cycle that scales initial amplitude by gamma**(cycle iterations) at each 
        cycle iteration.
    For more detail, please see paper.
    
    # Example
        ```python
            clr = CyclicLR(base_lr=0.001, max_lr=0.006,
                                step_size=2000., mode='triangular')
            model.fit(X_train, Y_train, callbacks=[clr])
        ```
    
    Class also supports custom scaling functions:
        ```python
            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))
            clr = CyclicLR(base_lr=0.001, max_lr=0.006,
                                step_size=2000., scale_fn=clr_fn,
                                scale_mode='cycle')
            model.fit(X_train, Y_train, callbacks=[clr])
        ```    
    # Arguments
        base_lr: initial learning rate which is the
            lower boundary in the cycle.
        max_lr: upper boundary in the cycle. Functionally,
            it defines the cycle amplitude (max_lr - base_lr).
            The lr at any cycle is the sum of base_lr
            and some scaling of the amplitude; therefore 
            max_lr may not actually be reached depending on
            scaling function.
        step_size: number of training iterations per
            half cycle. Authors suggest setting step_size
            2-8 x training iterations in epoch.
        mode: one of {triangular, triangular2, exp_range}.
            Default 'triangular'.
            Values correspond to policies detailed above.
            If scale_fn is not None, this argument is ignored.
        gamma: constant in 'exp_range' scaling function:
            gamma**(cycle iterations)
        scale_fn: Custom scaling policy defined by a single
            argument lambda function, where 
            0 <= scale_fn(x) <= 1 for all x >= 0.
            mode paramater is ignored 
        scale_mode: {'cycle', 'iterations'}.
            Defines whether scale_fn is evaluated on 
            cycle number or cycle iterations (training
            iterations since start of cycle). Default is 'cycle'.
    """"""

    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',
                 gamma=1., scale_fn=None, scale_mode='cycle'):
        super(CyclicLR, self).__init__()

        self.base_lr = base_lr
        self.max_lr = max_lr
        self.step_size = step_size
        self.mode = mode
        self.gamma = gamma
        if scale_fn == None:
            if self.mode == 'triangular':
                self.scale_fn = lambda x: 1.
                self.scale_mode = 'cycle'
            elif self.mode == 'triangular2':
                self.scale_fn = lambda x: 1/(2.**(x-1))
                self.scale_mode = 'cycle'
            elif self.mode == 'exp_range':
                self.scale_fn = lambda x: gamma**(x)
                self.scale_mode = 'iterations'
        else:
            self.scale_fn = scale_fn
            self.scale_mode = scale_mode
        self.clr_iterations = 0.
        self.trn_iterations = 0.
        self.history = {}

        self._reset()

    def _reset(self, new_base_lr=None, new_max_lr=None,
               new_step_size=None):
        """"""Resets cycle iterations.
        Optional boundary/step size adjustment.
        """"""
        if new_base_lr != None:
            self.base_lr = new_base_lr
        if new_max_lr != None:
            self.max_lr = new_max_lr
        if new_step_size != None:
            self.step_size = new_step_size
        self.clr_iterations = 0.
        
    def clr(self):
        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))
        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)
        if self.scale_mode == 'cycle':
            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)
        else:
            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)
        
    def on_train_begin(self, logs={}):
        logs = logs or {}

        if self.clr_iterations == 0:
            K.set_value(self.model.optimizer.lr, self.base_lr)
        else:
            K.set_value(self.model.optimizer.lr, self.clr())        
            
    def on_batch_end(self, epoch, logs=None):
        
        logs = logs or {}
        self.trn_iterations += 1
        self.clr_iterations += 1

        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))
        self.history.setdefault('iterations', []).append(self.trn_iterations)

        for k, v in logs.items():
            self.history.setdefault(k, []).append(v)
        
        K.set_value(self.model.optimizer.lr, self.clr())",0.442181319,
1914,shortcut pca in scikit learn,"tsne_en_embedding = get_tsne(en_embedding, pca_dim = 10, n_items=5000)
plot_tsne(en_embedding, tsne_en_embedding, n = 20)",0.4414926171,
1914,shortcut pca in scikit learn,"# Approximate PSD for one alpha sub band TRIAL
f_min, f_max = 1, 20
raw_temp = raw_alpha_sub_list[0].copy()
psds_trial, freqs_trial = psd_multitaper(raw_temp, low_bias=True, 
                                         tmin=t_croped_start + rest_len, tmax=t_croped_end, 
                                         fmin=f_min, fmax=f_max, proj=True)",0.4396131933,
1914,shortcut pca in scikit learn,"ad_plot(x = df_raw.feature_1, y = df_raw.feature_2 , mask = df_raw.prob < epsilon )",0.4387712181,
1914,shortcut pca in scikit learn,"bo = BayesianOptimization(f, {'x': (-2, 10)})
bo.maximize(init_points=1, n_iter=0, acq='ucb', kappa=5)

def posterior(bo, x, xmin=-2, xmax=10):
    xmin, xmax = -2, 10
    bo.gp.fit(bo.X, bo.Y)
    mu, sigma = bo.gp.predict(x, return_std=True)
    return mu, sigma

def plot_gp(bo, x, y):
    
    fig = plt.figure(figsize=(8, 5))
    fig.suptitle('t = {} Steps'.format(len(bo.X)), fontdict={'size':30})
    
    gs = gridspec.GridSpec(2, 1, height_ratios=[3, 1]) 
    axis = plt.subplot(gs[0])
    acq = plt.subplot(gs[1])
    
    mu, sigma = posterior(bo, x)
    axis.plot(x, y, linewidth=3, label='Target')
    axis.plot(bo.X.flatten(), bo.Y, 'D', markersize=8, label=u'Observations', color='r')
    axis.plot(x, mu, '--', color='k', label='Prediction')

    axis.fill(np.concatenate([x, x[::-1]]), 
              np.concatenate([mu - 1.9600 * sigma, (mu + 1.9600 * sigma)[::-1]]),
        alpha=.6, fc='c', ec='None', label='95% confidence interval')
    
    axis.set_xlim((-2, 10))
    axis.set_ylim((None, None))
    axis.set_ylabel('f(x)', fontdict={'size':20})
    axis.set_xlabel('x', fontdict={'size':20})
    
    utility = bo.util.utility(x, bo.gp, 0)
    acq.plot(x, utility, label='Utility Function', color='purple')
    acq.plot(x[np.argmax(utility)], np.max(utility), '.', markersize=15, 
             label=u'Next Best Guess', markerfacecolor='gold', markeredgecolor='k', markeredgewidth=1)
    acq.set_xlim((-2, 10))
    acq.set_ylim((0, np.max(utility) + 0.5))
    acq.set_ylabel('Utility', fontdict={'size':20})
    acq.set_xlabel('x', fontdict={'size':20})
    
    axis.legend(loc=2, bbox_to_anchor=(1.01, 1), borderaxespad=0.)
    acq.legend(loc=2, bbox_to_anchor=(1.01, 1), borderaxespad=0.)",0.4351478517,
1914,shortcut pca in scikit learn,"coords4, model4 = allel.pca(gnus, n_components=10, scaler='patterson')
fig_pca(coords4, model4, 'Figure 7. Conventional PCA, CMS downsampled to 50.', 
        sample_population=df_samples.take(sidx).population.values)",0.4350046515,
829,how did we do?,"inst = ExampleClass('Bob', 3)
inst.say_name()
print(inst.test_exponent(3))
print(inst.tally)",0.2952786684,
829,how did we do?,"#=======================================================================================
# Define the line class for video processing 
#
#  From class notes, but way simplified
#=====================================================================================

# Define a class to receive the characteristics of each line detection
class lane():
    def __init__(self):
        self.last_fits = None",0.295163691,
829,how did we do?,"better_inst = MoreSpecificClass(""Alice"", 2)
better_inst.say_name()
print(better_inst.test_exponent(5))
print(better_inst.tally)",0.2926728725,
829,how did we do?,"def hello():
    return ""Hello, how you are?""

def morning():
    return ""Good morning.""

def sup():
    return ""What's up?""

def suh():
    return ""Suh dude.""

def weezy():
    return ""Hello mothaf, hey hi how ya durrn'? It's Weezy F Baby.""",0.2892828584,
829,how did we do?,"class Dog():
    
    def bark():
        return ""I'm an instance method! Oh and... ruff ruff!""",0.287040621,
829,how did we do?,"# Some usefull functions for the code of next sections ...

def _1():
    return 'You entered one'

def _2():
    return 'You entered two'

def different():
    return 'You dit not enter one or two'",0.2867448032,
829,how did we do?,"# this next line overloads the addition operator for our MyTuple objects, or in other words adds the 'add' function to our MyTuple class definition on the fly
MyTuple.__add__ = add

# overload the reverse direction so that a + b = b + a
MyTuple.__radd__ = add",0.2826849818,
829,how did we do?,"get_ipython().magic('matplotlib inline')
get_ipython().magic('reload_ext autoreload')
get_ipython().magic('autoreload 2')
from anchor_check import *
from cloth_analysis import *
# import sys
# default_stdout = sys.stdout
# sys.stdout = open('../log/log_prepare_cloth.txt', 'w')",0.2825420499,
829,how did we do?,"def roll_die_unfair():
    return random.choice([1,2,3,4,5,6,6])",0.2824046016,
829,how did we do?,"import numpy as np

class Virus():
    """"""
    This class produces objects which are single agents of a influenza virus.
    seg_n = number of segments
    k = number of deleterious mutation
    s = fitness decrease from deleterious mutation
    L = sequence length for a virus
    cost = cost of having multisegments
    w = fitness
    progeny_n = number of progenies a virus agent will have during reproduction. Default is 0.
    seq = allele sequence vector
    one_index = index of allele that is 1 (mutant)
    """"""
    def __init__(self,one_index,k,seg_n):
        self.seg_n = seg_n
        self.k = k
        self.s = s
        self.L = L
        self.cost = cost
        self.progeny_n = 0
        if self.seg_n == 1:
            self.w = (1 - self.s)**self.k
        else:
            self.w = (1 - self.s)**self.k - self.cost
        self.seq = np.repeat(0,self.L)
        for i in one_index:
            self.seq[i] = 1
    
    def mutate(self,mu):
        """"""
        Mutation in sequence before reproduction
        mu = mutation rate
        """"""
        self.mutation_num = np.random.binomial(self.L, mu) # number of mutation
        mut_seq_index = np.random.randint(self.L, size = self.mutation_num) # pick which allele goes thru mutation
        for i in mut_seq_index:
            if self.seq[i] == 1: # back mutation allowed.
                self.seq[i] = 0
                self.k -= 1
            else:
                self.seq[i] = 1
                self.k += 1",0.282304287,
237,combined lab + discussion pandas overview,"# examine how often tokens of our dictionary appear in each rating category
tokens = pd.DataFrame(
    {'token': vect.get_feature_names(),
     '1 star'  : nb.feature_count_[0, :] / nb.class_count_[0],
     '2 stars' : nb.feature_count_[1, :] / nb.class_count_[1],
     '3 stars' : nb.feature_count_[2, :] / nb.class_count_[2] ,
     '4 stars' : nb.feature_count_[3, :] / nb.class_count_[3] ,
     '5 stars' : nb.feature_count_[4, :] / nb.class_count_[4]}
).set_index('token')
tokens.sample(10, random_state=3)",0.4746137261,
237,combined lab + discussion pandas overview,"bcm = BinaryClassificationMetrics(predictions, scoreCol='probability', labelCol='Survived')",0.4677164257,
237,combined lab + discussion pandas overview,"# nodes_tags value: Concentrated on street-related public utilities
print pandas.read_sql_query(""\
    SELECT value, COUNT(*) as num \
    FROM nodes_tags \
    GROUP BY value \
    ORDER BY num DESC \
    LIMIT 15 \
    ;"", conn)",0.4674435258,
237,combined lab + discussion pandas overview,"data.groupby(['gender','Class']).Discussion.agg(['mean'])",0.4663226008,
237,combined lab + discussion pandas overview,data.groupby('gender').Discussion.agg(['mean']),0.4662142396,
237,combined lab + discussion pandas overview,"# among the publications without abstract information,
# which article subject do they belong to and how many (only showing the top 20)
df[df.abstract.isnull()].article_subject.value_counts().head(20)",0.4582899511,
237,combined lab + discussion pandas overview,"column_names_2011 = [""Tract"", ""Estimated Median Family Income"", ""Less Than High School"", ""High School"", ""Some College"",
                ""College Graduate"", ""Graduate School"", ""Educational Attainment Score"", ""% of Single Parent-Households""
                , ""% of Owner Occupied Homes"", ""% of Population Speaking a Language Other than English"", 
                     ""Weighted Average ISAT Performance at Attendance Area Schools"", ""Number of School-Age Children""]

census2011_numbers = read_pdf(file_names[1], pages = ""all"", area = area2011_numbers, 
                      pandas_options = {'header': None, 'names': column_names_2011, 'index_col': 'Tract'})",0.4575080872,
237,combined lab + discussion pandas overview,print altDF['Score'].dropna('index')   # Uncomment to view the DataFrame inline,0.4572384953,
237,combined lab + discussion pandas overview,"scores = viterbivars
for k,v in tag_to_ix.items():
    print ('tag: ',k, ' score: ',scores[v].data.numpy(), ' back-pointer-tag: ',
           ix_to_tag[bptrs[(all_tags).index(k)]])",0.4552204609,
237,combined lab + discussion pandas overview,"# print sample terms from overview
feature_names = np.array(vectorizer.get_feature_names()).reshape(-1, 1)
print ""Total number of overviews and terms: "" + str(n) + "" overviews and "" \
    + str(p) + "" terms""
print
terms_df = pd.DataFrame(feature_names[1000:1010, 0], 
                        columns = ['Sample_Stemmed_Terms'])
display(terms_df)",0.4547816515,
884,implementing logistic regression using mle by gradient ascent,"class GradientDescent:
    
    def fit(self,X,y,learning_rate,epochs,sample_p = 0.5):
        data = np.concatenate((X,y),1)      
        x1_mat = X[:,0]
        x2_mat = X[:,1]
        self.x1 = x1_mat
        self.x2 = x2_mat
        self.y = y
        w0=0
        w1=0
        w2=0
        w3=0
        for itter in range(epochs):
            data = np.concatenate((sample_matrix[:,0:2],sample_matrix[:,2:]),1)
            sample = pd.DataFrame(data)[pd.Series(np.random.rand(40))<sample_p]
            sample.columns=[""x1"",""x2"",""y""]
            samplex1 = sample[""x1""].as_matrix()
            samplex2 = sample[""x2""].as_matrix()
            sampley = sample[""y""].as_matrix()
            for point in range(len(samplex1)):
                x1 = samplex1[point]
                x2 = samplex2[point]
                y_act = sampley[point]
                w0 = w0 - learning_rate*loss_function_base(w0,w1,w2,w3,x1,x2,y_act)
                w1 = w1 - learning_rate*loss_function_base(w0,w1,w2,w3,x1,x2,y_act)*x1
                w2 = w2 - learning_rate*loss_function_base(w0,w1,w2,w3,x1,x2,y_act)*2*x1
                w3 = w3 - learning_rate*loss_function_base(w0,w1,w2,w3,x1,x2,y_act)*x2

        self.w0=w0
        self.w1=w1
        self.w2=w2
        self.w3=w3
    
    def predict(self,X):
        x1_mat = X[:,0]
        x2_mat = X[:,1]
        y_pred = []
        w0 = self.w0
        w1 = self.w1
        w2 = self.w2
        w3 = self.w3
        for point in range(len(x1_mat)):
            x1 = x1_mat[point]
            x2 = x2_mat[point]
            y = self.w0 + ((w1)*(x1)) + w2*((x1)**2) + w3*x2
            y_pred.append(y)
        self.y_pred = y_pred
        return y_pred
    
    def get_ws(self):
        return [self.w0, self.w1, self.w2,self.w3]
    
    def RMSE(self):
        SE = 0
        for i in range(len(self.y_pred)):
            SE = SE + (self.y_pred[i]-self.y[:,0][i])**2
        MSE = float(SE)/float(len(self.y_pred))
        RMSE = np.sqrt(MSE)
        return RMSE",0.5193988085,
884,implementing logistic regression using mle by gradient ascent,"class Grad_Descent():
    
    def minimize(self, f, f_grad, x, step=1e-3, iterations=1e3, precision=1e-3):
        count, self.fs = 0, np.zeros((int(iterations), len(x)))
        """"""Find local minimum using gradient descent method""""""
        while True:
            last_x = x
            x = x - step * f_grad(x)
            self.fs[count, :] = x
            count += 1
            if count >= iterations:
                self.msg = ""Optimization Stop Reason: Exceeded maximum number of iterations""
                break
            if np.linalg.norm(x - last_x) < precision:
                self.msg = ""Optimization Stop Reason: Precision is reached""
                break
        self.fs = np.apply_along_axis(f, 1, self.fs[:count,:])
        return x

    def converge_plot(self):
        """"""Shows if function values decline as number of iteration increases;
           Used to debug gradient descent method
        """"""
        plt.plot(self.fs, 'o')
        plt.xlabel('Number of Iterations')
        plt.ylabel('Function Values')
        plt.show()",0.5142269731,
884,implementing logistic regression using mle by gradient ascent,"class GradientRegressor():
    """"""
    Calculates the best fit line for a regression task by using gradient descent
    """"""

    def __init__(self, x, y, learning_rate=.01):
        self.x = x
        self.y = y
        self.N = len(x)  # getting the length of the dataset
        self.m = 0
        self.b = 0
        self.learning_rate = learning_rate
        self.learning_rate_list = []
        self.simple_speedchange_rate = 5  # hyperparameter tuned through trial and error
        self.m_avg_grad = []
        self.b_avg_grad = []
        self.training_step = 0
        self.epsilon = 1e-10  # stopping condition
        self.training = True


    def calculate_error(self):
        """"""
        Calculates the MSE on all datapoints
        """"""
        squared_error = (self.y - (self.m * self.x + self.b)) ** 2
        mse = np.mean(squared_error)
        return mse


    def gradient_descent(self):
        """"""
        Calculates the gradients for m and b and then changes m and b in relation to those gradients
        :return:
        """"""
        m_gradient = (2 / self.N) * np.sum(-self.x * (self.y - (self.m * self.x + self.b)))
        b_gradient = (2 / self.N) * np.sum(-(self.y - (self.m * self.x + self.b)))
        self.m_avg_grad.append(m_gradient)
        self.b_avg_grad.append(b_gradient)
        learning_rate = self.optimize_learningrate(m_gradient, b_gradient)
        self.m -= m_gradient * learning_rate
        self.b -= b_gradient * learning_rate


    def optimize_learningrate(self, m_grad, b_grad):
        """"""
        Increases learning rate if gradient is shallower than average;
        decreases learning rate if gradient is steeper than average
        :param m_grad:
        :param b_grad:
        :return learning_rate:
        """"""
        learning_rate = self.learning_rate
        if np.abs(m_grad) > np.abs(np.mean(self.m_avg_grad[-10:]) * 1.2) or np.abs(b_grad) > np.abs(
                        np.mean(self.b_avg_grad[-10:]) * 1.2):
            learning_rate *= 1 / self.simple_speedchange_rate
        elif np.abs(m_grad) < np.abs(np.mean(self.m_avg_grad[-10:]) * .8) or np.abs(b_grad) < np.abs(
                        np.mean(self.b_avg_grad[-10:]) * .8):
            learning_rate *= self.simple_speedchange_rate
        self.learning_rate_list.append(learning_rate)
        return learning_rate


    def regress(self):
        """"""
        Continually runs gradient descent until the mean of the previous MSEs
        are no different than epsilon from the current mean
        :return slope, intercept, mses, learning_rates:
        """"""
        mse_list = [100, 100]
        mse_list.append(self.calculate_error())
        while np.abs(mse_list[-1] - np.mean(mse_list[-3:])) > self.epsilon:
            self.gradient_descent()
            mse_list.append(self.calculate_error())
            self.training_step += 1
            self.plot_results()

        print(""Training complete"")
        print(""Training steps"",self.training_step)
        print(""Error:"", mse_list[-1])
        self.training = False
        return mse_list


    def plot_results(self, mses=None):
        """"""
        Plots the results in 3 plots--
        1. Best fit line through data
        2. MSE
        3. Learning rate
        :param mses:
        """"""
        if self.training == True:
            y_out = lambda points: self.m * points + self.b
            plt.plot(self.x, y_out(self.x), 'g', alpha=.1)
        else:
            y_out = lambda points: self.m * points + self.b
            plt.plot(self.x, y_out(self.x), 'r', alpha=1)
            plt.scatter(self.x, self.y)
            plt.title(""Data ({} Training Steps)"".format(self.training_step))
            plt.show()

            plt.figure(1)
            plt.subplot(211)
            plt.plot(np.arange(len(mses) - 3), mses[3:])
            plt.title(""Mean Squared Error"")
            plt.ylabel(""MSE"")

            plt.subplot(212)
            plt.plot(np.arange(len(self.learning_rate_list)), self.learning_rate_list)
            plt.title(""Learning Rate"")
            plt.ylabel(""Rate"")

            plt.tight_layout()
            plt.show()",0.5134869814,
884,implementing logistic regression using mle by gradient ascent,"def grad_descent_step(learning_rate, theta_0, theta_1, x, y):
    ''''''
    # Initialize variables
    m = size(x)
    dtheta_0 = 0.0
    dtheta_1 = 0.0
    
    # Do m gradient descent steps 
    for i in range(m):
        h = x[i] * theta_1 + theta_0
        dtheta_0 = dtheta_0 + # Insert code here!
        dtheta_1 = dtheta_1 + # Insert code here!
        
    # Insert code here!
    return theta_0, theta_1

# Insert code here!",0.5125570297,
884,implementing logistic regression using mle by gradient ascent,"def train(total_loss,learning_rate=0.01):
    #learning_rate = 0.01
    return tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)",0.5120884776,
884,implementing logistic regression using mle by gradient ascent,"class GradientDescent:   
    def __init__(self, fit_intercept=True, normalize=True, copy_X=True, n_jobs=1):
        self.fit_intercept = fit_intercept
        self.normalize = normalize
        self.copy_X = copy_X
        self.n_jobs = n_jobs
        
    def fit(self, X, y, alpha, n_iters):
        m, n = np.shape(X)
        theta = np.ones(n)
        xTrans = X.transpose()
        for i in range(0, n_iters):
            hypothesis = np.dot(X, theta)
            loss = hypothesis - y
            # avg cost per example (the 2 in 2*m doesn't really matter here.
            # But to be consistent with the gradient, I include it)
            cost = np.sum(loss ** 2) / (2 * samples)
            #print(""Iteration %d | Cost: %f"" % (i, cost))
            # avg gradient per example
            gradient = np.dot(X.T, loss) / samples
            # update
            theta = theta - alpha * gradient
        return theta
    
#    def predict(self, X)
#        scores = self.decision_function(X)
#        if len(scores.shape) == 1:
#            indices = (scores > 0).astype(np.int)
#        else:
#            indices = scores.argmax(axis=1)
#        return self.classes_[indices]

sgd = GradientDescent()
row_sums = X_train.sum(axis=1)
new_matrix = X_train / row_sums[:, np.newaxis]
alpha = 0.1
n_iters=1500
coefs = sgd.fit(new_matrix, y_train, alpha, n_iters)
print ""Coefficients using the stochastic gradient descent: ""
for coef in coefs:
    print coef
#error = metrics.mean_squared_error(y_test, sgd.predict(X_test))",0.510866344,
884,implementing logistic regression using mle by gradient ascent,"fit(model, data, 4, opt, F.mse_loss)",0.5100681782,
884,implementing logistic regression using mle by gradient ascent,"class SGAdaGradClassifier:
    def fit(self, X, y, n_iter='default'):
        n = len(X)
        b = np.zeros(len(X[0]))
        G = 1
        
        if n_iter == 'default':
            size = 10*len(X) # adjust sample size for comparability per default
        else: 
            size = n_iter
    
        # stochastic gradient ascent with AdaGrad stepsize
        for j in range(size):
            i = random.randint(0,n-1)
            grad = (y[i]-(1 + np.exp(min(50,-b.dot(X[i]))))**-1)*X[i]
            G += grad**2
            b += grad/G**0.5
            
        self.b = b
        
    def score(self, X, y):
        est_correct = 0
        
        for i in range(len(X)):
            isSpam = y[i]
            isSpamEstimation = (1 + math.exp(min(50,-self.b.dot(X[i]))))**-1
            
            # isSpamEstimation mostly is either close to 0 or close to 1, 
            # for the values in between we only identify it as spam if the estimation is really close to 1
            if isSpamEstimation <= 0.99:
                isSpamEstimation = 0
            else:
                isSpamEstimation = 1
                
            if isSpamEstimation == isSpam:
                est_correct += 1
                
        return float(est_correct) / len(X)",0.5086864829,
884,implementing logistic regression using mle by gradient ascent,"eta = 0.000001
delta = 0.10000
num_epochs = 300

updates,loss,theta = lrnmod.run_gradient_descent(X_train, y_train, 
                                                 eta, delta, num_epochs, 
                                                 print_loss = False, 
                                                 inv_eta = False)

getplt.plot_losses(config, [loss], ['b'], [eta], 'eta')",0.5073670149,
884,implementing logistic regression using mle by gradient ascent,"from sklearn import svm,preprocessing

def make_meshgrid(x, y, h=.02):
    """"""Create a mesh of points to plot in

    Parameters
    ----------
    x: data to base x-axis meshgrid on
    y: data to base y-axis meshgrid on
    h: stepsize for meshgrid, optional

    Returns
    -------
    xx, yy : ndarray
    """"""
    x_min, x_max = x.min() - 1, x.max() + 1
    y_min, y_max = y.min() - 1, y.max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    return xx, yy


def plot_contours(ax, clf, xx, yy, **params):
    """"""Plot the decision boundaries for a classifier.

    Parameters
    ----------
    ax: matplotlib axes object
    clf: a classifier
    xx: meshgrid ndarray
    yy: meshgrid ndarray
    params: dictionary of params to pass to contourf, optional
    """"""
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    out = ax.contourf(xx, yy, Z, levels = np.arange(10), **params)
    return out

#LSVC_model = svm.SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,decision_function_shape='ovr',
#                     degree=3, gamma='auto', kernel='rbf',max_iter=-1, probability=False, random_state=None,
#                     shrinking=True,tol=0.001, verbose=False)

LSVC_model = svm.LinearSVC(C=1.0)

X = T
Y = sample_target

model_fit  = LSVC_model.fit(X,Y)

# Plotting result
fig,ax = plt.subplots(1,1,figsize=(8,8))

X0, X1 = X[:, 0], X[:, 1]
xx, yy = make_meshgrid(X0, X1)

c_dict = {0 : '#FF33B2',1 : 'k',2 : 'r',3 : 'b',4 : 'g',5 : 'c',6 : 'y',7 : 'm',8 : '#8EA9A8',9 : '#FFA302'}

im = plot_contours(ax, model_fit, xx, yy, colors=c_dict.values() , alpha=0.8)#
ax.scatter(X0, X1, c=[c_dict[int(c)] for c in Y], s=40, edgecolors='k')
plt.colorbar(im)
print(LSVC_model.score(X,Y))",0.506746769,
603,extra challenge,"def get_base_price(self):
    """"""Calculate base price using splurge pricing and rush hour fee.""""""

    # Splurge rate
    base_price = random.randrange(5, 10)

    now = datetime.datetime.now()

    # Is it rush hour?
    if now.hour >= 8 and now.hour <= 11 and now.weekday() < 5:
        base_price += 4

    return base_price",0.4797193408,
603,extra challenge,"def tsmaker(m, s, j):
    '''
    Helper function: randomly generates a time series for testing.

    Parameters
    ----------
    m : float
        Mean value for generating time series data
    s : float
        Standard deviation value for generating time series data
    j : float
        Quantifies the ""jitter"" to add to the time series data

    Returns
    -------
    A time series and associated meta data.
    '''

    # generate metadata
    meta = {}
    meta['order'] = int(np.random.choice(
        [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]))
    meta['blarg'] = int(np.random.choice([1, 2]))

    # generate time series data
    t = np.arange(0.0, 1.0, 0.01)
    v = norm.pdf(t, m, s) + j * np.random.randn(ts_length)

    # return time series and metadata
    return meta, TimeSeries(t, v)",0.4636356235,
603,extra challenge,"def select_action_using_simulator(selected_action, other_actions):
    simulator_chance = (random.randint(1, 100))

    if 71 <= simulator_chance <= 80 and len(other_actions) >= 1:
        selected_action = other_actions[0]

    elif 81 <= simulator_chance <= 90 and len(other_actions) >= 2:
        selected_action = other_actions[1]

    elif 91 <= simulator_chance <= 100 and len(other_actions) >= 3:
        selected_action = other_actions[2]

    return selected_action",0.4621171355,
603,extra challenge,"def take_bet(chip):
    '''
    Input:
    chip - a Chip() object
    '''
    while True:   
        try:
            chip.bet = int(input(""Please provide your bet: ""))
        except:
            print(""It looks that you have not provided a number. Please, try again by providing an integer value!"")
        else:
            if chip.bet > chip.total:
                print(""You exceeded your total amount of chips! You have: {} chips"".format(chip.total))
            else:
                print(""Thank you! Your bet is equal to {}"".format(chip.bet))
                break",0.4607647359,
603,extra challenge,"def get_clothes(weather):
    print('--------------------------------------------------------------------------------------')
    print(' {}'.format(weather['description']))
    # determine by cloudiness
    if weather['cloudiness'] < 10:
        print(', ')
    # determine by rain
    if weather['rain'] == 0 or weather['rain'] == 0.0 :
        print("", "")
    elif weather['rain']/3 < 2.5:
        print("", T"")
    elif weather['rain']/3 < 7.6:
        print("", "")
    elif weather['rain']/3 < 50:
        print("", "")
    elif weather['rain']/3 > 50:
        print("", "")
    # determine by temperature
    if weather['temperature'] < 273:
        print("", "")
    elif weather['temperature'] < 283:
        print("", "")
    elif weather['temperature'] < 293:
        print("", "")
    elif weather['temperature'] < 303:
        print("", "")
    # determine by wind
    if weather['wind'] > 30:
        print("", "")
    elif weather['wind'] > 10:
        print("", "")
    else:
        print("", "")
    
    print('-----------------------------------------------------------------------------------------------')",0.458553493,
603,extra challenge,"# define the objective function that the fmin module can later optimize on
def test_fm(params):
    
    # convert certain hyperparameter values to integers
    print('==========TESTING FM==========')
    params['n_iter'] = int(params['n_iter'])
    params['rank'] = int(params['rank'])
    print(params)
    
    fm = als.FMRegression(n_iter=params['n_iter'],
                          rank=params['rank'],
                          init_stdev=params['init_stdev'],
                          l2_reg_w=params['l2_reg_w'],
                          l2_reg_V=params['l2_reg_V'])
    
    X, y = build_data(use_actors=params['use_actors'],
                      use_country=params['use_country'],
                      use_directors=params['use_directors'],
                      use_genres=params['use_genres'],
                      use_language=params['use_language'],
                      use_mpaa=params['use_mpaa'],
                      use_studios=params['use_studios'],
                      use_type=params['use_type'],
                      use_scores=params['use_scores'],
                      use_popularity=params['use_popularity'],
                      use_year=params['use_year'],
                      use_model_results=params['use_model_results'])

    # train model and evaluate MAE on cross-validation set
    fm.fit(X['train'], y['train'])
    y_cv_pred = fm.predict(X['cv'])
    print('HERE')
    print(y_cv_pred)
    mae = mean_absolute_error(y_cv_pred, y['cv'])
    print('MAE:', mae)
    return mae",0.4578949213,
603,extra challenge,"def lets_guess(true_class = 'micro'):
    rand = random.randint(0, 6)
    path = 'other_assets/validation_set/{}/{}_{}.png'.format(true_class, true_class, rand)
    display(Image_(path))
    test_image = image.load_img(path, target_size = (64, 64))
    test_image = image.img_to_array(test_image)
    test_image = np.expand_dims(test_image, axis = 0)
    result = classifier.predict(test_image)
    print('Model says...')
    if true_class == 'micro':        
        if result[0][0] == 1:
            print('Sorry, it\'s a chest X-ray.') ## false positive
        else:
            print('Good guess!  It\'s a microscope photo!') ## true negative
    elif true_class == 'chest':
        if result[0][0] == 1:
            print('Good guess!  It\'s a chest X-ray!') ## true positive
        else:
            print('Sorry, it\'s a microscope photo :(') ## false negative
    return",0.4574922919,
603,extra challenge,"def print_doc(doc):
    if doc[""doc_id""] <= 5:  # print only the first 5 documents
        print(""docID:"", doc[""doc_id""])
        print(""date:"", doc[""date""])
        print(""title:"", doc[""title""])
        print(""body:"", doc[""body""])
        print(""--"")",0.4560576081,
603,extra challenge,"def welcome(name):
    print('helo')
    print('welcome to india')
    print('Welcome to Python, '+name)",0.4526643455,
603,extra challenge,"def create_pokemon(poke_id, poke_name, poke_type):
    return {
        'poke_id': poke_id,
        'poke_name': poke_name,
        'poke_type': poke_type,
        'hp': np.random.randint(400,500), 
        'attack': np.random.randint(50,100),
        'defense': np.random.randint(50,100),
        'special_attack': np.random.randint(100,150),
        'special_defense': np.random.randint(100,150),
        'speed': np.random.randint(0,100)
    }
create_pokemon(poke_id=1, poke_name= 'charmander', poke_type = 'fire')",0.4504481256,
1835,run this cell reading in ta feng,"def update(self):
        """"""
        Interface: 
        After trabeculectomy is done, nothing is done besides set number of visit
        until IOP > IOP target
        """"""
        if self.medicalRecords['OnTrabeculectomy'] == False:
            self.SurgeryTE()
        else:
            self.SetNumberofVisits()
        #Check whether IOP > IOP target yet. Then evaluate whether to leave the block    
        self.DeterminetoExitTEblock()",0.3952909708,
1835,run this cell reading in ta feng,"# groups by m_predicate and lists the lexicalizations
def m_predicates_with_generated_lexes(ds):
    
    # merges modified triple dataset and lexicalization dataset
    df = pd.merge(ds.mdf, ds.ldf)
    
    # groups by triple and for each triple generates a list of lexicalizations
    ddf = df.groupby('predicate').ltext.apply(list).to_frame()
    # adds a column with the number of lexicalizations
    ddf['size'] = ddf.ltext.apply(len)
    
    return ddf

# prints triple and its lexicalizations
def print_m_predicate_with_generated_lexes(mtriple):
    
    print(""predicate: "", mtriple.name, '\n')
    for ltext in sorted(mtriple.ltext):
        print(""lexicalization: "", ltext, '\n')",0.3947250247,
1835,run this cell reading in ta feng,"# groups by mtext and lists the lexicalizations
def mtriples_with_generated_lexes(ds):
    
    # merges modified triple dataset and lexicalization dataset
    df = pd.merge(ds.mdf, ds.ldf)
    
    # groups by triple and for each triple generates a list of lexicalizations
    ddf = df.groupby('mtext').ltext.apply(list).to_frame()
    # adds a column with the number of lexicalizations
    ddf['size'] = ddf.ltext.apply(len)
    
    return ddf

# prints triple and its lexicalizations
def print_mtriple_with_generated_lexes(mtriple):
    
    print(""triple: "", mtriple.name, '\n')
    for ltext in mtriple.ltext:
        print(""lexicalization: "", ltext, '\n')",0.3944197297,
1835,run this cell reading in ta feng,"def read_and_decode(filename_queue):
    
    reader = tf.TFRecordReader()
    
    _, serialized_example = reader.read(filename_queue)
    
    # Unpack the features you created in the method to save to TFRecords
    # This is for images, but the features will differ depending on your problem
    features = tf.parse_single_example(
        serialized_example,
        features={
            'height': tf.FixedLenFeature([], tf.int64),
            'width': tf.FixedLenFeature([], tf.int64),
            # 'depth': tf.FixedLenFeature([], tf.int64),      # if you encoded layers, e.g. IR cameras
            'image_raw': tf.FixedLenFeature([], tf.string),
            'label': tf.FixedLenFeature([], tf.int64), 
            # Other features you encoded during encoding process
        })

    # Grab the image data that has a fixed length storage
    image = tf.decode_raw(features['image_raw'], tf.uint8)
    
    # Get image shape to reshape it
    height = tf.cast(features['height'], tf.int32)
    width = tf.cast(features['width'], tf.int32)
    
    ##### RESHAPE - if needed
    
    # To reshape the image as an image with 3 channels:
    # image_shape = tf.pack([height, width, 3])
    # image = tf.reshape(image, image_shape)

    # To reshape the image as a vector - here we use all three channels
    image.set_shape([IMAGE_HEIGHT*IMAGE_WIDTH*3]) # creates a vector from the images
    
    # Unclear what this is for at the moment
    # image_size_const = tf.constant((IMAGE_HEIGHT, IMAGE_WIDTH, 3), dtype=tf.int32)
    
    
    ##### TRANSFORMATIONS
    
    # If the image needs to be cropped or padded for the network
    # resized_image = tf.image.resize_image_with_crop_or_pad(image=image,
    #                                        target_height=IMAGE_HEIGHT,
    #                                        target_width=IMAGE_WIDTH)

    # My example images in TFRecords are in [0, 255] in all channels, 
    # so I transformed to [-0.5, 0.5]
    image = tf.cast(image, tf.float32) * (1. / 255) - 0.5

    # Convert label from a scalar uint8 tensor to an int32 scalar.
    label = tf.cast(features['label'], tf.int32)
    
    ##### MODIFY LABELS if needed
    
    
    return image, label",0.3872963786,
1835,run this cell reading in ta feng,"def read_and_decode(filename_queue):
    
    reader = tf.TFRecordReader()

    _, serialized_example = reader.read(filename_queue)

    features = tf.parse_single_example(
      serialized_example,
      # Defaults are not specified since both keys are required.
      features={
        'height': tf.FixedLenFeature([], tf.int64),
        'width': tf.FixedLenFeature([], tf.int64),
        'image_raw': tf.FixedLenFeature([], tf.string),
#         'mask_raw': tf.FixedLenFeature([], tf.string),
        'label': tf.FixedLenFeature([], tf.int64)
        })

    # Convert from a scalar string tensor (whose single string has
    # length mnist.IMAGE_PIXELS) to a uint8 tensor with shape
    # [mnist.IMAGE_PIXELS].
    image = tf.decode_raw(features['image_raw'], tf.uint8)
#     annotation = tf.decode_raw(features['mask_raw'], tf.uint8)
    label =  tf.cast(features['label'], tf.int32)
    height = tf.cast(features['height'], tf.int32)
    width = tf.cast(features['width'], tf.int32)
    
    #tf.stack equal to tf.stack([x, y, z]) = np.asarray([x, y, z])
    image_shape = tf.stack([height, width, 3])
    
    image = tf.reshape(image, image_shape)
    
    image_size_const = tf.constant((IMAGE_HEIGHT, IMAGE_WIDTH, 3), dtype=tf.int32)
    
    # Random transformations can be put here: right before you crop images
    # to predefined size. To get more information look at the stackoverflow
    # question linked above.
    
    resized_image = tf.image.resize_image_with_crop_or_pad(image=image,
                                           target_height=IMAGE_HEIGHT,
                                           target_width=IMAGE_WIDTH)
    
        
    images,label = tf.train.shuffle_batch( [resized_image,label],
                                                 batch_size=BATCH_SIZE,
                                                 capacity=30000,
                                                 num_threads=3,
                                                 min_after_dequeue=10)
    
    return images, label",0.3828802407,
1835,run this cell reading in ta feng,"def read_and_decode_TFRecords(filename_queue):    
    reader = tf.TFRecordReader()
    _, serialized_example = reader.read(filename_queue)

    features = tf.parse_single_example(
      serialized_example,
      # Defaults are not specified since both keys are required.
      features={
        'image': tf.FixedLenFeature([], tf.string),
        'masks': tf.FixedLenFeature([], tf.string)
        })
    image = tf.decode_raw(features['image'], tf.float64)
    masks = tf.decode_raw(features['masks'], tf.float64)
    image = tf.reshape(image, [1,512,512,1])
    masks = tf.reshape(masks, [1,512,512,2])
    
    """"""image, masks = tf.train.shuffle_batch([image, masks],
                                          batch_size=1,
                                          capacity=30,
                                          num_threads=2,
                                          min_after_dequeue=10)""""""    
    return image, masks",0.3778774142,
1835,run this cell reading in ta feng,"# check the output fits file information using astropy fits
with fits.open(flat_field_output_file) as hdulist:
    
    # check the calibration step information
    print'Check flat_field step information status:'
    step_check = hdulist[0].header['S_FLAT']
    print step_check + '\n'
    
    # print the fits extension table and properties
    hdulist.info()",0.377749145,
1835,run this cell reading in ta feng,"def read_and_decode(filename_queue):
    reader = tf.TFRecordReader()
    _, serialized_example = reader.read(filename_queue)
    features = tf.parse_single_example(
      serialized_example,
      # Defaults are not specified since both keys are required.
      features={
          'label': tf.FixedLenFeature([], tf.int64),
          'image_raw': tf.FixedLenFeature([], tf.string)
            
      })

    # Convert from a scalar string tensor (whose single string has
    # length mnist.IMAGE_PIXELS) to a uint8 tensor with shape
    # [mnist.IMAGE_PIXELS].
    image = tf.decode_raw(features['image_raw'], tf.float32)
    image.set_shape([mnist.IMAGE_PIXELS])

    # OPTIONAL: Could reshape into a 28x28 image and apply distortions
    # here.  Since we are not applying any distortions in this
    # example, and the next step expects the image to be flattened
    # into a vector, we don't bother.

    # Convert from [0, 255] -> [-0.5, 0.5] floats.
    image = image - 0.5

    # Convert label from a scalar uint8 tensor to an int32 scalar.
    label = tf.cast(features['label'], tf.int32)

    return image, label


def inputs(filename, batch_size, num_epochs):
    """"""Reads input data num_epochs times.
    Args:
      filename: filename of TFRecords
      batch_size: Number of examples per returned batch.
      num_epochs: Number of times to read the input data, or 0/None to
      train forever.
    Returns:
        A tuple (images, labels), where:
        * images is a float tensor with shape [batch_size, mnist.IMAGE_PIXELS]
        in the range [-0.5, 0.5].
        * labels is an int32 tensor with shape [batch_size] with the true label,
        a number in the range [0, mnist.NUM_CLASSES).
    Note that an tf.train.QueueRunner is added to the graph, which
    must be run using e.g. tf.train.start_queue_runners().
    """"""
    if not num_epochs: num_epochs = None
     

    with tf.name_scope('input'):
        filename_queue = tf.train.string_input_producer(
            [filename], num_epochs=num_epochs)

        # Even when reading in multiple threads, share the filename
        # queue.
        image, label = read_and_decode(filename_queue)

        # Shuffle the examples and collect them into batch_size batches.
        # (Internally uses a RandomShuffleQueue.)
        # We run this in two threads to avoid being a bottleneck.
        images, sparse_labels = tf.train.shuffle_batch(
            [image, label], batch_size=batch_size, num_threads=2,
            capacity=1000 + 3 * batch_size,
            # Ensures a minimum amount of shuffling of examples.
            min_after_dequeue=1000)

    return images, sparse_labels",0.3759343028,
1835,run this cell reading in ta feng,"def convert_state (instant, state):
    
    lla = LLA.Cartesian(state.getPosition().inFrame(Frame.ITRF(), state.getInstant()).getCoordinates(), Earth.EquatorialRadius, Earth.Flattening)
    
    return [
                repr(instant),
                float(instant.getModifiedJulianDate(Time.Scale.UTC)),
                *state.getPosition().getCoordinates().transpose()[0].tolist(),
                *state.getVelocity().getCoordinates().transpose()[0].tolist(),
                float(lla.getLatitude().inDegrees()),
                float(lla.getLongitude().inDegrees()),
                float(lla.getAltitude().inMeters())
            ]",0.373840332,
1835,run this cell reading in ta feng,"def translation(seq, t_table):
    """""" DNA  ( 6  Frames)""""""
    codon = str()
    all = dict()
    
    for frame in range(3):
      protein = list()
      for item in range(frame, int(len(seq)/3)*3, 3):
        protein.append(t_table[codon.join(seq[item:item+3])]['one'])
      all['positive strain frame-'+str(frame+1)] = protein
    
    c_seq = complementary_seq(seq)
    r_seq = reverse_seq(c_seq)

    for frame in range(3):
      protein = list()
      for item in range(frame, int(len(r_seq)/3)*3, 3):
        protein.append(t_table[codon.join(seq[item:item+3])]['one'])
      all['negative strain frame-'+str(frame+1)] = protein
    return all

frames = translation(seq, t_table)
frames?",0.3732286096,
517,eda,"def predictions(df_test):
    
    df_test['enconding'] = df_test.infinitivo.apply(encode_sequence).apply(decode_sequence)
    return df_test",0.4488912225,
517,eda,"def plot_residuals_ladloss(model):
    
    resids = model.resid
    
    resid_lim = np.max([abs(np.min(resids)), abs(np.max(resids))]) + 1
    
    resid_points = np.linspace(-1*resid_lim, resid_lim, 200)
    
    plt.figure(figsize=(10,8))
    
    for r in resids:
        
        plt.plot((r, r), (0, abs(r)), 'k-', ls='dashed', lw=1)
        
    plt.plot(resid_points, np.abs(resid_points), c='gold', alpha=0.7)",0.4389736652,
517,eda,"cajas_ejercicio es:

    imprimir las cajas que tengan un area menor que max_area.
    
input:
    una lista de 2 puntos 
    [
    (x1,y1, x2,y2 ),
    (x1,y1, x2,y2 ),
    (x1,y1, x2,y2 )
    ]

y max_area, que es la  maxima area deseada.  
    
output es:
    una lista que tenga las cajas chicas.",0.4348128438,
517,eda,"def dl_files(x):
    df = pd.DataFrame(x.groupby('Nombre').Nombre.count())
    files = x.file_name
    for i in df.index:
        c_dir = 'Pancitas Ultrasound {}'.format(i.lower().title())
        os.mkdir(c_dir)
        os.chdir('.\\' + c_dir)
        dl_df = []
        for k in files:
            if i.upper() in k:
                dl_df.append(k)
        for j in dl_df:
            f, metadata = client.get_file_and_metadata('/'+j)
            out = open(j, 'wb')
            out.write(f.read())
            out.close()
        watermark(dl_df)
        for l in os.listdir(c_dir):
            if 'Pancitas' in l:
                pass
            else:
                os.remove(l)
        os.chdir('..\\')",0.4333033562,
517,eda,"def fill_titanicAge(row):
    global titanicAge_dict
    age = row['age']
    title = row['title']
    if math.isnan(age):
        if title in titanicAge_dict.keys():
            return titanicAge_dict[title]
    else:
        return age",0.433005631,
517,eda,"def predictions(df_test):
    
    df_test['enconding'] = df_test.infinitivo.apply(encode_sequence).apply(decode_sequence)
    return test",0.4254885018,
517,eda,"def get_cage_id(row):
    if row.oligo_type != ""RANDOM"":
        cage_id = row.seq_name.split(""__"")[1].split("","")[0]
    else:
        cage_id = ""none""
    return cage_id",0.4227903783,
517,eda,"def build_family(data):
    result = data.copy()
    result['FamilySize'] = result.Parch + result.SibSp + 1
    result = result.drop(['Parch', 'SibSp'], axis=1)
    
    return result

data = build_family(data)",0.4189068675,
517,eda,"if parametro.gera_pos_fonte: 
    aux.posicao_fonte(parametro.Nz,parametro.Nx,parametro.N_shot,parametro.Fx0,parametro.Fz0,parametro.SpaFonte)",0.4117991328,
517,eda,"%%bq query -n placedispo_158
SELECT  
    timestamp(record_timestamp) AS record_timestamp
    ,sum(nbVelosDispo) AS VelosDispo
FROM (
  SELECT  
    record_timestamp
    ,nbVelosDispo
    ,borneid
    FROM `gcp-data-cirruseo.aus_velill_demo.vlilleRealtime`
    WHERE DATE(TIMESTAMP(record_timestamp))>= DATE_ADD(CURRENT_DATE,  INTERVAL -14 DAY )
    GROUP BY 1,2,3
  )
WHERE borneid=""158""
GROUP BY 1
ORDER BY 1 asc",0.4106238484,
1632,problem missing values inspection point,"def get_column_info(df,column):
    # print general info about column
    print df[column].describe()
    
    # show histogram amd box plot
    plt.figure()
    df[df.TARGET==0][column].plot(kind=""hist"", label=""satisfied customers"")
    df[df.TARGET==1][column].plot(kind=""hist"", label=""unsatisfied customers"")
    plt.legend();
    
    plt.figure()
    df[df.TARGET==0][column].plot(kind=""kde"", label=""satisfied customers"")
    df[df.TARGET==1][column].plot(kind=""kde"", label=""unsatisfied customers"")
    plt.legend();
    
    plt.figure()
    df[column].plot(kind='box')",0.3598043919,
1632,problem missing values inspection point,"# Simple Euler method integration for area as a function of area
def Euler(f, T, A_0):
    dt = T[1] - T[0] # assume it's the same...
    A = [A_0]
    for t in enumerate(T[1:]):
        A.append(A[-1] + f(A[-1]) * dt)
    return np.array(A)",0.3449275494,
1632,problem missing values inspection point,"p_2y = 1 - (- z_2y)


def report_result(p,a):
    print ('is the p value ' + 
           '{0:.2f} smaller than the critical value {1:.2f}?'.format(p,a))
    if p < a:
        print (""YES!"")
    else: 
        print (""NO!"")
    
    print ('the Null hypothesis is {}'.format(\
                            'rejected' if p < a  else 'not rejected') )

    
report_result(p_2y, alpha)",0.3423449993,
1632,problem missing values inspection point,"p_2y = 1 - 0.9984


def report_result(p,a):
    print ('is the p value ' + 
           '{0:.2f} smaller than the critical value {1:.2f}?'.format(p,a))
    if p < a:
        print (""YES!"")
    else: 
        print (""NO!"")
    
    print ('the Null hypothesis is {}'.format(\
                            'rejected' if p < a  else 'not rejected') )

    
report_result(p_2y, alpha)",0.3420493603,
1632,problem missing values inspection point,"p_2y = 1 - 0.9998


def report_result(p,a):
    print ('is the p value ' + 
           '{0:.2f} smaller than the critical value {1:.2f}?'.format(p,a))
    if p < a:
        print (""YES!"")
    else: 
        print (""NO!"")
    
    print ('the Null hypothesis is {}'.format(\
                            'rejected' if p < a  else 'not rejected') )

    
report_result(p_2y, alpha)",0.3416341543,
1632,problem missing values inspection point,"# Class to store camera calibration data
class calibration():
    def __init__(self, obj_pts, img_pts, shape):
        self.ret, self.M, self.dist, self.rvecs, self.tvecs =\
        cv2.calibrateCamera(obj_pts, img_pts, shape, None, None)

# Class to store perspective transform matrices
class transform():
    def __init__(self, src, dst):       
        self.M = cv2.getPerspectiveTransform(src, dst)
        self.Minv = cv2.getPerspectiveTransform(dst, src)
        
# Class to store lane detection data
class lane():
    def __init__(self):
        self.detected = False
        self.x = None
        self.y = None
        self.fit = None
        
# Class to store color and gradient feature extraction parameters
class parameters():
    def __init__(self):
        self.spat_size = (32, 32)
        self.hist_bins = 32
        self.orient = 8
        self.pxs_cell = (8, 8)
        self.cells_block = (2, 2)

# Class to store vehicle detection data
class vehicle():
    def __init__(self):
        self.scaler = None
        self.clf = None
        self.heatmaps = None",0.341329962,
1632,problem missing values inspection point,"p_3y=1-0.7995

#report functino from prof's code
def report_result(p,a):
    print ('is the p value ' + 
           '{0:.2f} smaller than the critical value {1:.2f}?'.format(p,a))
    if p < a:
        print (""YES!"")
    else: 
        print (""NO!"")
    
    print ('the Null hypothesis is {}'.format(\
                            'rejected' if p < a  else 'not rejected') )

    
report_result(p_3y, alpha)",0.3392409086,
1632,problem missing values inspection point,"def nullPercentage(x):
    print(""Number of nulls: "", train_set[x.isnull()].shape[0])
    print(train_set[x.isnull()].shape[0] / train_set.shape[0])",0.3376819491,
1632,problem missing values inspection point,"p_2y = 1 - 0.8023


def report_result(p,a):
    print ('is the p value ' + 
           '{0:.2f} smaller than the critical value {1:.2f}?'.format(p,a))
    if p < a:
        print (""YES!"")
    else: 
        print (""NO!"")
    
    print ('the Null hypothesis is {}'.format(\
                            'rejected' if p < a  else 'not rejected') )

    
report_result(p_2y, alpha)",0.3374968171,
1632,problem missing values inspection point,"p_3y=1-0.8023
def report_result(p,a):
    print ('is the p value ' + 
           '{0:.2f} smaller than the critical value {1:.2f}?'.format(p,a))
    if p < a:
        print (""YES!"")
    else: 
        print (""NO!"")
    
    print ('the Null hypothesis is {}'.format(\
                            'rejected' if p < a  else 'not rejected') )

    
report_result(p_3y, alpha)",0.33704108,
1591,problem a simple problem,"from __future__ import division

def solution2():
    n_uplets=[[0],[0,1],[0,1,1]]
    i=3
    while len(n_uplets) < 150:
        n_uplets.append([0 for j in range(i+1)])
        n_uplets[i][i]=1
        for j in range(1,i):
            if j > i/2:
                n_uplets[i][j]=sum(n_uplets[i-j])
            else:
                n_uplets[i][j]=sum(n_uplets[i-j][:j+1])
        i+=1
    return n_uplets",0.4324709475,
1591,problem a simple problem,"def p3_7():
    
    c =  [3,2]
    A = [[1,-1], 
         [3,1],
         [4,3]]
    b =  [2,  ,7]

    problem = Simplex(c,A,b)
    
    print(""c:\n"",c,""\nA:\n"",A,""\nb:\n"",b)
    print(""\n--> SOLUTION:"")
    print(problem.solve())

    dat = np.load(""productMix.npz"")

    a,p,m,d = dat[""A""],dat[""p""],dat[""m""],dat[""d""]
    A = np.row_stack([a, np.eye((4))])
    b = np.concatenate([m, d])

    print(""\n\n--> SOLUTION for productMix data:"")
    print(Simplex(p,A,b).solve())",0.4288669825,
1591,problem a simple problem,"# Failed 
# Time: O(m*n)
# Space: O(m*n)
# Longest Palindromic Substring -> longest commonest substring = failed (abcdjklbcda)

class Solution(object):
    def longestPalindrome(self, s):
        """"""
        :type s: str
        :rtype: str
        """"""
        t= s[::-1]
        result = [ [0 for i in range(len(s) + 1)] for j in range(len(t) + 1) ]
        longest = 0
        
        if not s:
            return None
                
        for i in range(1,len(s)+1):
            for j in range(1,len(t)+1):
                if s[i-1] == t[j-1]:
                    result[i][j] = result[i-1][j-1] + 1
                    if result[i][j] > longest:
                        longest = result[i][j]
                        end = i 
        return s[end-longest:end]",0.425160557,
1591,problem a simple problem,"class Solution:
    def findDuplicate(self, nums):
        # Find the intersection point of the two runners.
        tortoise = nums[0]
        hare = nums[0]
        while True:
            tortoise = nums[tortoise]
            hare = nums[nums[hare]]
            if tortoise == hare:
                break
        
        # Find the ""entrance"" to the cycle.
        ptr1 = nums[0]
        ptr2 = tortoise
        while ptr1 != ptr2:
            ptr1 = nums[ptr1]
            ptr2 = nums[ptr2]
        
        return ptr1",0.4238507748,
1591,problem a simple problem,"def factorit(n):
    display(Eq(x**n-1, factor(x**n-1)))",0.4236249328,
1591,problem a simple problem,"def setupMinuit(params):
    print 'Setup Minuit...'
    PRINT_LEVEL = 1  # -1 => quiet, 1 => loud
    UP = 0.5 # 1: appropriate for 68% CL using chisq (use 0.5 for log-likelihood)
    npar = len(params)
    minuit = rt.TMinuit(npar)
    minuit.SetFCN(nnl)
    minuit.SetErrorDef(UP)
    minuit.SetPrintLevel(PRINT_LEVEL)

    status = rt.Long() # needed for integers passed by refence (int& ii)
    print ""%-20s %10s %10s %10s %10s"" % \
    ('param', 'guess', 'step', 'min', 'max')
    
    for ii, t in enumerate(params):
        print ""%-20s %10.2e %10.3e %10.3e %10.3e"" % t
        name, guess, step, pmin, pmax = t
        minuit.mnparm(ii, name, guess, step, pmin, pmax, status)
        if status != 0:
            sys.exit(""** mnparm(%s) status = %d"" % (name, status))
    return minuit",0.4234426618,
1591,problem a simple problem,"def build_model(layers):
        d = 0.2
        model = Sequential()
        model.add(LSTM(128, input_shape=(layers[1], layers[0]), return_sequences=True))
        model.add(Dropout(d))
        model.add(LSTM(64, input_shape=(layers[1], layers[0]), return_sequences=False))
        model.add(Dropout(d))
        model.add(Dense(16,kernel_initializer='uniform',activation='tanh'))        
        model.add(Dense(1,kernel_initializer='uniform',activation='tanh'))
        model.compile(loss='mse',optimizer='adam',metrics=['accuracy'])
        return model",0.4228951335,
1591,problem a simple problem,"def build_model(layers):
    d = 0.2
    model = Sequential()
    
    # now model.output_shape == (None, 128)
    model.add(LSTM(128, input_shape=(layers[1], layers[0]), return_sequences=True))
    model.add(Dropout(d))
    
    # for subsequent layers, no need to specify the input size:
    model.add(LSTM(64, return_sequences=False))
    model.add(Dropout(d))
    
    # fully connected layer
    model.add(Dense(16,kernel_initializer='uniform',activation='relu'))        
    model.add(Dense(1,kernel_initializer='uniform',activation='linear'))
    model.compile(loss='mse',optimizer='adam',metrics=['accuracy'])
    return model",0.4228951335,
1591,problem a simple problem,"def my_pipeline(test_size = 0.3, random_state = 0):
    """"""
    The function has the following major steps:
    1. Divide the data into training and testing
    2. Standardize the features and target
    3. Train the model on the training data
    4. Test the model on the testing data
    5. Plot the model on the testing data
    6. Get the accuracy of the model
    7. Print the winning model
    
    Parameters
    ----------
    test_size : the proportion for testing, 0.3 by default
    random_state : the seed used by the random number generator, 0 by default
    """"""
    
    # Divide the data into training and testing
    # Randomly choose 30% of the data for testing
    # Implement me
    X_train, X_test, y_train, y_test = 
    
    # Standardize the features and the target
    # Declare the standard scaler
    std_scaler = StandardScaler()

    # Standardize the training and testing data
    X_train = std_scaler.fit_transform(X_train)
    y_train = std_scaler.fit_transform(y_train.reshape(-1, 1)).reshape(-1)
    X_test = std_scaler.transform(X_test)
    y_test = std_scaler.transform(y_test.reshape(-1, 1)).reshape(-1)
    
    # Declare the linear regression model
    slr = LinearRegression()
    
    # The accuracy of the two models
    scores = []
    
    for feature in [alpha, beta]:
        # Get the index of the feature
        idx = features.index(feature)

        # Train the model on the training set
        slr.fit(X_train[:, idx].reshape(-1, 1), y_train)

        # Test the model on the testing set
        # Implement me
        y_pred =     

        # Plot the model on the testing set
        plt.scatter(X_test[:, idx], y_test, c='steelblue', edgecolor='white', s=70)
        plt.plot(X_test[:, idx], y_pred, color='black', lw=2)  
        plt.xlabel(feature)
        plt.ylabel(target)
        plt.show()

        # Add the accuracy of the model
        scores.append(slr.score(X_test[:, idx].reshape(-1, 1), y_test))
        
    # Get the accuracy of the model
    score_alpha, score_beta = scores
        
    # Model comparsion
    # Print empty line
    print()

    # Print the accuracy
    print('Accuracy for feature ' + alpha + ': %.3f' % score_alpha)
    print('Accuracy for feature ' + beta + ': %.3f' % score_beta)

    # Print empty line
    print()

    # Print the winning model
    if score_alpha > score_beta:
        print('The winning model is the one based on feature: ' + alpha)
    elif score_alpha < score_beta:
        print('The winning model is the one based on feature: ' + beta)
    else:
        print('The two models are equally accurate')",0.4226917028,
1591,problem a simple problem,"def insert_sort2(items):
    """"""
    This version uses Python's list slicing so that we can shift all items at once
    when inserting an item to the left, rather than swapping them one at a time
    """"""
    items = items[:] # this makes a copy of the list because I didn't want to change the original one
    # in Python 3, you can do items.copy() instead (or list(items) in Python 2)
    n_items = len(items)
    for next_idx_to_sort in xrange(1, n_items):
        next_item = items[next_idx_to_sort]
        insert_idx = next_idx_to_sort # make a separate variable, because we'll be moving it
                
        while next_item < items[insert_idx - 1]: # keep swapping left if next_item is still smaller
            insert_idx -= 1
            if insert_idx == -1: # this means next_item was less than the smallest item
                insert_idx = 0 # so we insert it at the very start of the list
                break
        items[insert_idx + 1 : next_idx_to_sort + 1] = items[insert_idx:next_idx_to_sort] # shift over
        items[insert_idx] = next_item
    return items",0.4224259853,
1651,problem set the iris dataset,"def visuvalize_petal():
    iris = datasets.load_iris()
    X = iris.data[:, 2:]  # we only take the last two features.
    y = iris.target
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)
    plt.xlabel('Petal length')
    plt.ylabel('Petal width')
    plt.title('Petal Width & Length')
    plt.show()
 
visuvalize_petal()",0.4510939121,
1651,problem set the iris dataset,"def visuvalize_petal_data():
    iris = datasets.load_iris()
    X = iris.data[:, 2:]  # we only take the last two features.
    y = iris.target
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)
    plt.xlabel('Petal length')
    plt.ylabel('Petal width')
    plt.title('Petal Width & Length')
    plt.show()
 
visuvalize_petal_data()",0.4510939121,
1651,problem set the iris dataset,"def visuvalization_petal_data():
    iris = datasets.load_iris()
    X = iris.data[:, 2:]
    y = iris.target
    plt.scatter(X[:, 0],X[:, 1],c=y,cmap=plt.cm.BrBG)
    plt.xlabel('Petal length')
    plt.ylabel('Petal width')
    plt.title('Petal Width & Length')
    plt.show()",0.4510939121,
1651,problem set the iris dataset,"def get_iris_df():
    ds = sklearn.datasets.load_iris()
    df = pd.DataFrame(ds['data'],
    columns = ds['feature_names'])
    code_species_map = dict(zip(
        range(3), ds['target_names']))
    df['species'] = [code_species_map[c]
    for c in ds['target']]
    return df",0.4509056211,
1651,problem set the iris dataset,"def que2():
    iris = datasets.load_iris() 
    X = iris.data
    target = iris.target 
    names = iris.target_names
    acc_rf = cross_val(RandomForestClassifier(n_estimators = 20), X, target, 5)
    acc_svm = cross_val(SVC(), X, target, 5)
    # skLearn function from cross validation accuracy score
    sk_learn_cross_val_rf = cross_val_score(RandomForestClassifier(n_estimators = 20), X, target, cv = 10, scoring = 'accuracy')
    sk_learn_cross_val_svc = cross_val_score(SVC(), X, target, cv = 10, scoring = 'accuracy')
    # Answer 2-b -START
    print(""Non aggregated accuracy of user defined cross validation function v/s sk learn cross valtion function"")
    print(""Accuraccy score of user defined cross val function"")
    print(acc_rf)
    
    print(""Accuraccy score of sk learn cross val function"")
    print(sk_learn_cross_val_rf)
    # Answer 2-b -FINISH
    
    # Answer 2-c - START
    print(""Comparing Accuracy score between Random Forest and SVM Classification :"")
    print(""Random Forest accuracy (Mean) :"")
    print(np.mean(sk_learn_cross_val_rf))
    print(""SVM Classification accuracy (Mean) :"")
    print(np.mean(sk_learn_cross_val_svc))
    # Answer 2-c FINISH

que2()",0.4461807609,
1651,problem set the iris dataset,"def load_dataset(split):
    """"""Load and split the dataset into training and test parts.
    
    Parameters
    ----------
    split : float in range (0, 1)
        Fraction of the data used for training.
    
    Returns
    -------
    X_train : array, shape (N_train, 4)
        Training features.
    y_train : array, shape (N_train)
        Training labels.
    X_test : array, shape (N_test, 4)
        Test features.
    y_test : array, shape (N_test)
        Test labels.
    """"""
    dataset = datasets.load_iris()
    X, y = dataset['data'], dataset['target']
    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, random_state=123, test_size=(1 - split))
    return X_train, X_test, y_train, y_test",0.4456174374,
1651,problem set the iris dataset,"def load_dataset(split):
    """"""Load and split the dataset into training and test parts.
    
    Parameters
    ----------
    split : float in range (0, 1)
        Fraction of the data used for training.
    
    Returns
    -------
    X_train : array, shape (N_train, 4)
        Training features.
    y_train : array, shape (N_train)
        Training labels.
    X_test : array, shape (N_test, 4)
        Test features.
    y_test : array, shape (N_test)
        Test labels.
    """"""
    dataset = datasets.load_iris()
    X, y = dataset['data'], dataset['target']
    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, random_state=123)
    return X_train, X_test, y_train, y_test",0.4456174374,
1651,problem set the iris dataset,"def visuvalize_sepal_data():
    iris = datasets.load_iris()
    X = iris.data[:, :2]
    y = iris.target
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)
    plt.xlabel('Sepal length')
    plt.ylabel('Sepal width')
    plt.title('Sepal Width & Length')
    plt.show()",0.4433168769,
1651,problem set the iris dataset,"def visuvalize_sepal_data():
    iris = datasets.load_iris()
    X = iris.data[:, :2]  # we only take the first two features.
    y = iris.target
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)
    plt.xlabel('Sepal length')
    plt.ylabel('Sepal width')
    plt.title('Sepal Width & Length')
    plt.show()
 
visuvalize_sepal_data()",0.4433168769,
1651,problem set the iris dataset,"def load_dataset():
    iris = datasets.load_iris()
    X = iris.data
    y = np_utils.to_categorical(iris.target) #i transformed the target (different integers for each flower class) into a one-hot vector 


    xy = np.hstack((X,y)) #stacking the features and classes vectors horizontally, so during the shuffle phase i'll not get lost
    np.random.shuffle(xy) #shuffling the lines

    #there are four iris features and 150 samples. I'm spliting the now suffled vector into 4. Training vectors with 105 samples and 45 samples to model validation.
    X_train, X_test = xy[:105,:4], xy[105:,:4] 
    y_train, y_test = xy[:105,4:], xy[105:,4:]

    return { 'data':{'train': X_train, 'test': X_test},
             'classes':{'train': y_train, 'test': y_test} }",0.4310859442,
1387,part c numpy point,"# conver lab to rgb (range: [0,1])
def lab2rgb(inputColor):
    color_lab = LabColor(inputColor[0],inputColor[1],inputColor[2])
    # convert lab to sRGB
    color_rgb = convert_color(color_lab, sRGBColor)
    # convert the format to ndarray
    [a,b,c] = np.array(col_tuple2list(color_rgb))    
    return [a,b,c]",0.4856053591,
1387,part c numpy point,"def f(u):
    
    h = u[0]
    v = u[1]
    return np.array([v, (-(ms+mp)*g + mp_dot*ve - 0.5*rho*v*np.abs(v)*A*CD)/(ms+mp)])",0.4776699543,
1387,part c numpy point,"def concat_params(input):
    a = input[0]
    b = input[1]
    return concatenate([a,b], axis=1)",0.476411283,
1387,part c numpy point,"def foo(x):  # x will be a list of values, describing the dimensions of a trapezoidal prism
    box = IsoscelesTrapezoidalPrism(x[0], x[1], x[2], x[3])
    return cost(box.surfaceArea()) / box.volume()",0.4747165143,
1387,part c numpy point,"def f_drag_spin(r):
    dfdt = zeros(4)
    
    v = sqrt(r[2]**2 + r[3]**2)
    dfdt[0] = r[2]
    dfdt[1] = r[3]
    dfdt[2] = -(C(v)*rho*A*r[2]*v + Somega*r[3])/m
    dfdt[3] = -g - (C(v)*rho*A*r[3]*v - Somega*r[2])/m
    
    return dfdt",0.4746280611,
1387,part c numpy point,"def f_drag(r):
    dfdt = zeros(4)
    
    v = sqrt(r[2]**2 + r[3]**2)
    dfdt[0] = r[2]
    dfdt[1] = r[3]
    dfdt[2] = -C(v)*rho*A*r[2]*v/m
    dfdt[3] = -g - C(v)*rho*A*r[3]*v/m
    
    return dfdt",0.4746280611,
1387,part c numpy point,"def raster2Zigzag(b):
    return np.array([b[0,0], b[0,1], b[1,0], b[2,0], 
            b[1,1], b[0,2], b[0,3], b[1,2], 
            b[2,1], b[3,0], b[3,1], b[2,2],
            b[1,3], b[2,3], b[3,2], b[3,3]])

zz = raster2Zigzag(transform)
print(""Transformed Coeffecients(zigzag ordering): %s"" % zz)",0.4738568962,
1387,part c numpy point,"def create_hist(rdd_histogram_data):
  """"""Given an RDD.histogram, plot a pyplot histogram""""""
  heights = np.array(rdd_histogram_data[1])
  full_bins = rdd_histogram_data[0]
  mid_point_bins = full_bins[:-1]
  widths = [abs(i - j) for i, j in zip(full_bins[:-1], full_bins[1:])]
  bar = plt.bar(mid_point_bins, heights, width=widths, color='b')
  return bar",0.4672883153,
1387,part c numpy point,"def wrapper_func(theta):
    return(CostFunction(df.X, df.Y, theta[0], theta[1]))",0.46707654,
1387,part c numpy point,"def circle_constraint(s):
    x = s[0]
    y = s[1]
    r = sqrt(x**2 + y**2)
    theta = atan(y / x)
    return 1 + 0.3 * np.sin(7 * (theta + 4)) - r
print(circle_constraint(np.array([1,1])))",0.4646838903,
754,get the sum of all the values in mat,"def fitness(p, chromosome):
    return np.sum(np.multiply(p.v, chromosome))",0.5159114599,
754,get the sum of all the values in mat,"def dft_spectrum(x):
    return np.array([dft(x, k).sum() for k in range(len(x))])",0.5108078122,
754,get the sum of all the values in mat,"##function for generating object counts
##get true object counts for each window
def get_object_counts(object_maps, windows):
    '''
    object_maps   =  M x K x D      newaxis
                     M x K x 1 x D
        windows   =          N x D  mult
                     M x K x N x D  sum(axis=-1)
                     M x K x N      clip(0,1).sum(axis=1)
    object_counts =  M x   x N      astype(int)
    
    '''
    return np.sum(object_maps[:, :,np.newaxis,:]*windows,axis=-1).clip(0,1).sum(axis=1).astype('int')",0.5078006387,
754,get the sum of all the values in mat,"def sum_sqz(a,axis): return a.sum(axis).squeeze(axis)",0.5058541894,
754,get the sum of all the values in mat,"def compute_soft_counts(resp):
    # Compute the total responsibility assigned to each cluster, which will be useful when 
    # implementing M-steps below. In the lectures this is called N^{soft}
    counts = np.sum(resp, axis=0)
    return counts",0.5033507347,
754,get the sum of all the values in mat,"def compute_soft_counts(resp):
    # Computing the total responsibility assigned to each cluster, which will be useful when 
    # implementing M-steps below. 
    counts = np.sum(resp, axis=0)
    return counts",0.5033507347,
754,get the sum of all the values in mat,"def compute_soft_counts(resp):
    # Compute the total responsibility assigned to each cluster, which will be useful when 
    # implementing M-steps below. Here this is called N^{soft}
    counts = np.sum(resp, axis = 0)
    return counts",0.5033507347,
754,get the sum of all the values in mat,"def compute_soft_counts(resp):
    # Compute the total responsibility assigned to each cluster, which will be useful when 
    # implementing M-steps below. In the lectures this is called N^{soft}
    counts = np.sum(resp, axis=0) # -- sum going down the column (k) of each cluster k.
    return counts",0.5033507347,
754,get the sum of all the values in mat,"def predict_classification(thetas: np.array, xs: np.matrix) -> np.array:
    return sigmoid(thetas[0] + np.dot(xs, thetas[1:]))",0.502821207,
754,get the sum of all the values in mat,"def get_path_length(path):
    return np.sum(geocalc(path[1:,0], path[1:,1],
                          path[:-1,0], path[:-1,1]))",0.5013618469,
495,doing some basic comparisons of results and forecasting efficaciousness,"# the percentage filled after manual map
print(""Percentage Filled:"",100*(df_map.Canton.shape[0]-df_map[pd.isnull(df_map['Canton'])].shape[0])/df_map.Canton.shape[0])",0.4465283751,
495,doing some basic comparisons of results and forecasting efficaciousness,"# Hypothesis test 
print 'Sucess rates of two groups:',p7_p,p8_p
# Assuming null hypothesis - h0 - No significance of school type
# Combined p
p9 = float(len(p7[p7.funding_status == 'completed']) + len(p8[p8.funding_status == 'completed'])) / (len(p7)+len(p8))
mean_diff = 0 #(Null hypothesis)
var_p7 = (p9*(1-p9))/len(p7)
var_p8 = (p9*(1-p9))/len(p8)
var_diff = var_p7 + var_p8
std_diff = np.sqrt(var_diff)
print 'Mean and Std of sampling dist p8-p7',mean_diff,std_diff

# Diff b/w sample means
mean_sample_diff = p8_p - p7_p

#z-score
z_score = (mean_diff-mean_sample_diff)/std_diff
print'z-score',z_score

# p-Value
p_value = stats.norm.cdf(z_score)
print 'P-Value =',p_value",0.4450371861,
495,doing some basic comparisons of results and forecasting efficaciousness,"# We need to clean up the following categories:
#  1 - we standardize every numerical input to floats
#  2 - weight, standardize [father,mother]_profession

# 2 (pre-processing) - let us check how many unique entries we have on each column
print (""before standardization:\tfather_profession uniques: %d, mother_profession uniques: %d""
       %(qoflDF.father_profession.nunique() , qoflDF.mother_profession.nunique()))

# helper function to check if string ends in suffix list
def checkEnds(string, ends):
    words = string.split()
    
    for word in words:
        if any(word.endswith(end) for end in ends):
            return True, word
    
    return False, string

# 2 - standardize profession
def standardize_profession (string):
    
    string = string.lower()
    string = string.strip()
    
    endsw, word = checkEnds(string, [""er"", ""or"", ""ic"", ""st"", ""ct"", ""nt"", ""an"", ""ch""])
    
    if any(x for x in [""own"", ""self"", ""business""] if x in string):
        output = ""BUSINESSMAN""
    elif any(x for x in [""teacher"", ""prof""] if x in string):
        output = ""TEACHER""
    elif ""home"" in string:
        output = ""HOMEMAKER""
    elif ""construct"" in string:
        output = ""CONSTRUCTION""
    elif ""police"" in string:
        output = ""POLICEMAN""
    elif endsw == True:
        output = word.upper()
    elif ""sales"" in string:
        output = ""SALESMAN""
    elif ""retired"" in string:
        output = ""RETIRED""
    elif any(x for x in [""ceo"", ""cfo"", ""vp"", ""president"", ""cto""] if x in string):
        output = ""EXECUTIVE""
    else:
        output = ""OTHER""
        
    return output

# finally drop nan by replacing with empty strings
qoflDF[""father_profession""] = qoflDF[""father_profession""].fillna("""")
qoflDF[""mother_profession""] = qoflDF[""mother_profession""].fillna("""")

# apply our filter to standardize the profession columns
qoflDF[""father_profession""] = qoflDF[""father_profession""].apply(standardize_profession)
qoflDF[""mother_profession""] = qoflDF[""mother_profession""].apply(standardize_profession)

# print new unique counts
print (""after standardization:\tfather_profession uniques: %d, mother_profession uniques: %d""
       %(qoflDF.father_profession.nunique() , qoflDF.mother_profession.nunique()))",0.4410995543,
495,doing some basic comparisons of results and forecasting efficaciousness,"total_outcomes = 5 + 3 + 4 + 2 + 5  # all of the sandwich possibilities
favorable_outcomes = 2              # all of the ham handwich possibilities   
ham_prob = favorable_outcomes / total_outcomes
print ""Probability of getting a ham sandwich is:"", ""%.2f"" %ham_prob  # using string formatting to get 2 decimal place float",0.4409004152,
495,doing some basic comparisons of results and forecasting efficaciousness,"# The values are 1 or 0, so a sum is equivalent to a count. 
sum(bw_env.recent_precip > 0.0), sum(bw_env.recent_precip > 0.0)/len(bw_env)",0.4403838813,
495,doing some basic comparisons of results and forecasting efficaciousness,"enrichments = {
    ""degree"": nx.degree_centrality,
    ""eigenvector"": nx.eigenvector_centrality,
    ""closeness"": nx.closeness_centrality,
    ""harmonic"": nx.harmonic_centrality,
    ""betweenness"": nx.betweenness_centrality,
    ""katz"": nx.katz_centrality
}",0.4402452707,
495,doing some basic comparisons of results and forecasting efficaciousness,inner = ((w_mean * (1-w_mean))/data[data.race=='w'].race.count()) + ((b_mean * (1-b_mean))/data[data.race=='b'].race.count()),0.4400160313,
495,doing some basic comparisons of results and forecasting efficaciousness,How do we calculate the winning party,0.437143147,
495,doing some basic comparisons of results and forecasting efficaciousness,"flavorWts = { 0 : 0.33 * (jf_df['jet_LabDr_HadF'] != 15).sum() / (jf_df['jet_LabDr_HadF'] == 0).sum(),
              4 : 0.33 * (jf_df['jet_LabDr_HadF'] != 15).sum() / (jf_df['jet_LabDr_HadF'] == 4).sum(),
              5 : 0.33 * (jf_df['jet_LabDr_HadF'] != 15).sum() / (jf_df['jet_LabDr_HadF'] == 5).sum()
            }

weights = np.array([flavorWts[pdg] for pdg in jf_df['jet_LabDr_HadF'] if pdg != 15])

weights_bs = np.array([1 if pdg==5 else 0 for pdg in jf_df['jet_LabDr_HadF'] if pdg != 15])


allBsCs = (jf_df['jet_LabDr_HadF'] == 4) | (jf_df['jet_LabDr_HadF'] == 5)
heavyFlavorWts = { 0 : 0,
                   4 : 0.33 * allBsCs.sum() / (jf_df['jet_LabDr_HadF'] == 4).sum(),
                   5 : 0.33 * allBsCs.sum() / (jf_df['jet_LabDr_HadF'] == 5).sum()
                 }

weights_bcs = np.array([heavyFlavorWts[pdg] for pdg in jf_df['jet_LabDr_HadF'] if pdg != 15])",0.436965704,
495,doing some basic comparisons of results and forecasting efficaciousness,"# Let's do a Hypothesis test on the significance of difference of sucesses of Highest poverty schoold vs remaining schools
p1 = p[p.poverty_level == 'highest poverty']
p2 = p[p.poverty_level != 'highest poverty']
p1_p = float(len(p1[p1.funding_status == 'completed'])) / len(p1)
p2_p = float(len(p2[p2.funding_status == 'completed'])) / len(p2)
print'Sucess rates of both the groups:',p1_p,p2_p
# Assuming null hypothesis - h0 - No significance of poverty level ""highest""
# Combined p
p3 = float(len(p1[p1.funding_status == 'completed']) + len(p2[p2.funding_status == 'completed'])) / (len(p1)+len(p2))
mean_diff = 0 #(Null hypothesis)
var_p1 = (p3*(1-p3))/len(p1)
var_p2 = (p3*(1-p3))/len(p2)
var_diff = var_p1 + var_p2
std_diff = np.sqrt(var_diff)
print 'Mean and Std of sampling dist p1-p2',mean_diff,std_diff

# Diff b/w sample means
mean_sample_diff = p1_p - p2_p

#z-score
z_score = (mean_diff-mean_sample_diff)/std_diff
print'z-score',z_score

# p-Value
p_value = stats.norm.cdf(z_score)
print 'P-Value =',p_value",0.4366384745,
1293,numpy and tables,"import numpy as np

# Read dataset_HW0.txt into birth_data var without column headers (skiprows =1)
birth_data = np.loadtxt('dataset_HW0.txt',delimiter=',',skiprows=1)


# printing out the dimensions
print 'Bitrh Data dimensions   : ', birth_data.shape

print 'First three rows        : ' 
print  birth_data[0:3,:]

print 'Range of birth weight   : ', np.min(birth_data[:,0]) , np.max(birth_data[:,0])
print 'Range of femure length  : ', np.min(birth_data[:,1]) , np.max(birth_data[:,1])
print 'Range of mother age     : ', np.min(birth_data[:,2]) , np.max(birth_data[:,2])",0.4582203627,
1293,numpy and tables,"data = np.loadtxt('pca-data-2d.dat', delimiter=' ', skiprows=0, usecols=range(0, 2))
data.shape",0.4571982026,
1293,numpy and tables,"rN_LZ_ER=loadtxt('data_yields/LZ_rootNEST_flat_ER.txt',skiprows=1).T
rN_LZ_NR=loadtxt('data_yields/LZ_rootNEST_flat_NR.txt',skiprows=1).T",0.453553766,
1293,numpy and tables,"# 2 dimensional matrix
components = np.loadtxt('melodic_mix', ndmin=2)
print(str(components.shape))
# these are integers referring to the columns of the components matrix
# each column listed is a noise component
# I subtract 1 from the integers since the list starts the count at
# 1, but python lists start at index 0.
noise_components_indices = np.loadtxt('classified_motion_ICs.txt', 
                                     dtype=int, delimiter=',') - 1
print(str(noise_components_indices.shape))",0.4501711726,
1293,numpy and tables,"from astropy.table import Table

# Model Accuracy Table
#
arr = np.array([(mod_cor, mod_inc, total, cor_pct)], 
               dtype = [('Correctly Classified', 'i4'), ('Incorrectly Classified', 'i4'), ('Total', 'i4'),
                        ('Accuracy', 'S8')])


accu_tab = Table(arr)
print(accu_tab)",0.4486317039,
1293,numpy and tables,"data = numpy.array([[""Tim"", 45],[""Fred"", 32], [""Jane"", 43]])
toyplot.data.Table(data)",0.4472347498,
1293,numpy and tables,"np.loadtxt('test.dat',dtype=np.float32, skiprows=2, delimiter=',' ,usecols=[1,2,3,4,5])",0.4467639327,
1293,numpy and tables,"plastic = pd.read_table('plasticHardness.txt', names=['brinell', 'hour'], skipinitialspace=True, sep=' ')
print plastic.shape",0.4460735917,
1293,numpy and tables,"data = np.loadtxt('expDat.txt', delimiter=',', skiprows=1, usecols=range(1, 21))
data.shape, data",0.445091784,
1293,numpy and tables,"f = np.loadtxt(open('1528929673010O-result.csv',""rb""),
                delimiter="","",dtype=str)
print f.shape
print f[0,:]",0.4447209239,
599,exponential distribution,"# return a gaussian
def gauss(x, amplitude=1, mu=0, sig=1):
    return amplitude * np.exp(-np.power(x - mu, 2.) / (2 * np.power(sig, 2.)))",0.4920733869,
599,exponential distribution,"def gaussian(x,*args):
    mu = args[0]
    sigma = args[1]
    return np.exp(-np.power(x-mu,2.)/(2.*sigma*sigma))/(np.sqrt(2.*np.pi)*sigma)",0.4887936413,
599,exponential distribution,"def exponential(x, a, b):
    y = a * np.exp(b*x) 
    return y",0.4872458577,
599,exponential distribution,"def exponential_growth(x,a1,a2):
    f = a1*np.exp(a2*x)
    return f",0.4872458577,
599,exponential distribution,"def EqwModel(z,a,b):
    return np.exp(a*np.log(z))+b",0.4870359004,
599,exponential distribution,"# redifine gauss using *args
def gauss(x, *args):

    A, mu, sigma = args
    return A*np.exp(-(x-mu)**2/(2.*sigma**2))",0.4852361083,
599,exponential distribution,"def gaussian(x, *p):
    A, mu, sigma = p
    return(A*np.exp(-(x-mu)**2/(2.*sigma**2)))",0.4852361083,
599,exponential distribution,"# let's define that curve in python code
def gauss(x, *args):
    a, mu, sigma = args
    return a*np.exp(-((x-mu)**2)/(2.*sigma**2))",0.4852361083,
599,exponential distribution,"# Define model function to be used to fit to the data above:
def gaussian(x, *p):
    A, mu, sigma = p
    return A*np.exp(-(x-mu)**2/(2.*sigma**2))",0.4852361083,
599,exponential distribution,"def neal_funnel (x, y):
    return np.exp(norm(0, 3).logpdf(y) + np.sum(norm(0, np.exp(y/2.)).logpdf(x)))",0.4834054112,
758,getting a sense of the data,"BT.root.data, BT.root.left.data, BT.root.right.data",0.4325752258,
758,getting a sense of the data,"print ('start vectorizing...')              # Showing message at the begining of vectorization.
dummies = pd.get_dummies(test_data.animal_colors)
for dum in dummies:
    test_data[dum] = dummies[dum]          # Making featurs from dummies
print ('Done!...')                         # Showing message at the end.",0.4243612289,
758,getting a sense of the data,In order to get a measurement of how good the data is,0.4228460193,
758,getting a sense of the data,"print ('start vectorizing...')              # Showing message at the begining of vectorization.
dummies = pd.get_dummies(train_data.animal_colors)
for dum in dummies:
    train_data[dum] = dummies[dum]          # Making featurs from dummies
print ('Done!...')                          # Showing message at the end.",0.421482265,
758,getting a sense of the data,"{""is_male"": ""True"", ""mother_age"": 26.0, ""plurality"": ""Single(1)"", ""gestation_weeks"": 39}",0.4211151004,
758,getting a sense of the data,Take a moment to look through the plots and notice how NMF has expressed the digit as a sum of the components!,0.4193974733,
758,getting a sense of the data,"aust_popul.get_pop_change_list()
aust_popul.get_averages()",0.4173177481,
758,getting a sense of the data,"houst_popul.get_pop_change_list()
houst_popul.get_averages()",0.4173177481,
758,getting a sense of the data,"fig2.data=[fign.data[1], fign.data[0]]",0.4144173265,
758,getting a sense of the data,"fill_dict = {
    'Lot Frontage' : test_df['Lot Frontage'].mean(),
    'Garage Yr Blt' : test_df['Garage Yr Blt'].mean(),
    'Garage Cond' : 'NA',
    'Garage Finish' : 'NA',
    'Garage Qual' : 'NA',
    'Garage Type' : 'NA',
    'BsmtFin Type 2' : 'NA',
    'BsmtFin Type 1' : 'NA',
    'Bsmt Exposure' : 'NA',
    'Bsmt Cond' : 'NA',
    'Bsmt Qual' : 'NA',
    'Mas Vnr Area' : test_df['Mas Vnr Area'].mean(),
    'Mas Vnr Type' : 'None',
    'Electrical' : test_df['Electrical'].mode()[0],
}

for key, value in fill_dict.items():
    test_df[[key]] = test_df[[key]].fillna(value=value)",0.4141994715,
2386,train,"def test_model_on_breast_cancer_data(model):
    X, y = get_breast_cancer_data()
    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=0)
    for C in [0.1, 1, 10, 100, 250]:
        clf = model(C=C).fit(X_train, y_train)
        print(f'Breast cancer Logistic regresion with C = {C}')
        print(""Train Accurary: {}"".format(clf.score(X_train, y_train)))
        print(""Test Accurary: {}"".format(clf.score(X_test, y_test)))

test_model_on_breast_cancer_data(sklearn.linear_model.LogisticRegression)",0.5608472824,
2386,train,"def main():
    args = docopt(__doc__)
    try:
        model = DenseGGNNChemModel(args)
        model.train()
    except:
        typ, value, tb = sys.exc_info()
        traceback.print_exc()
        pdb.post_mortem(tb)


if __name__ == ""__main__"":
    main()",0.5551000237,
2386,train,"def train_input_fn():
    ds_tr = dataset.training_dataset(hparams.data_dir, DATA_SET)
    ds_tr_tr, _ = split_datasource(ds_tr, 60000, 0.95)
    ds1 = ds_tr_tr.cache().shuffle(buffer_size=57000).\
        repeat(hparams.train_epochs).\
        batch(hparams.batch_size)
    return ds1

def eval_input_fn():
    ds_tr = dataset.training_dataset(hparams.data_dir, DATA_SET)
    _, ds_tr_ev = split_datasource(ds_tr, 60000, 0.95)
    ds2 = ds_tr_ev.batch(hparams.batch_size)
    return ds2",0.5522696376,
2386,train,"def main():
    lines = load_lines(data_path)[1:]
    training_set_lines, validation_set_lines = train_test_split(lines, test_size=0.2)
    
    nb_training = len(training_set_lines)*6
    nb_validation = len(validation_set_lines)*6

    training_images, steering_angles = get_data_without_generator(data_path, lines[0:500])
    return (training_images, steering_angles)
data_path = ""data-from-udacity""
#main()",0.548815906,
2386,train,"def EvaluatePerformance(SplitRation):
    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=SplitRation)
    clf = KNeighborsClassifier(n_neighbors=5)
    clf.fit(X_train, y_train)
    TrainingScore = clf.score(X_train, y_train)
    TestingScore  = clf.score(X_test, y_test)
    Result = [TrainingScore, TestingScore]
    return Result",0.5471435189,
2386,train,"def prepro(config):
    print('Preprocessing ... ')
    train_dir = config.train_dir
    dev_dir = config.dev_dir
    test_dir = config.test_dir
    
    # prepro   

    input_tf = make_tf_record(train_dir,""Training data"")
    produce_tfrecord(input_tf)
    '''
    make_tf_record(dev_dir,""Dev data"")
    make_tf_record(test_dir,""Test data"")
    '''",0.5471019745,
2386,train,"def train(FLAGS):
    """"""
    Train a previous or new model.
    """"""
    # Data paths
    vocab_path = os.path.join(
        basedir, 'data/processed_reviews/vocab.txt')
    train_data_path = os.path.join(
        basedir, 'data/processed_reviews/train.p')
    validation_data_path = os.path.join(
        basedir, 'data/processed_reviews/validation.p')
    vocab = Vocab(vocab_path)
    FLAGS.num_classes = 2

    # Load embeddings (if using GloVe)
    if FLAGS.embedding == 'glove':
        with open(os.path.join(
            basedir, 'data/processed_reviews/embeddings.p'), 'rb') as f:
            embeddings = pickle.load(f)
        FLAGS.vocab_size = len(embeddings)

    # Start tensorflow session
    with tf.Session() as sess:

        # Create|reload model
        imdb_model = create_model(sess, FLAGS, len(vocab))

        # Metrics
        metrics = {
            ""train_loss"": [],
            ""valid_loss"": [],
            ""train_acc"": [],
            ""valid_acc"": [],
        }

        # Store attention score history for few samples
        attn_history = {
            ""sample_0"":
            {""review"": None, ""label"": None, ""review_len"": None, ""attn_scores"": []},
            ""sample_1"":
            {""review"": None, ""label"": None, ""review_len"": None, ""attn_scores"": []},
            ""sample_2"":
            {""review"": None, ""label"": None, ""review_len"": None, ""attn_scores"": []},
            ""sample_3"":
            {""review"": None, ""label"": None, ""review_len"": None, ""attn_scores"": []},
            ""sample_4"":
            {""review"": None, ""label"": None, ""review_len"": None, ""attn_scores"": []},
        }

        # Start training
        for train_epoch_num, train_epoch in \
            enumerate(generate_epoch(
                train_data_path, FLAGS.num_epochs, FLAGS.batch_size)):

            print (""==> EPOCH:"", train_epoch_num)

            for train_batch_num, (batch_features, batch_seq_lens) in \
                enumerate(train_epoch):

                batch_reviews, batch_labels = batch_features
                batch_review_lens, = batch_seq_lens

                # Display shapes once
                if (train_epoch_num == 0 and train_batch_num == 0):
                    print (""Reviews: "", np.shape(batch_reviews))
                    print (""Labels: "", np.shape(batch_labels))
                    print (""Review lens: "", np.shape(batch_review_lens))

                _, train_logits, train_loss, train_acc, lr, attn_scores = \
                    imdb_model.train(
                        sess=sess,
                        batch_reviews=batch_reviews,
                        batch_labels=batch_labels,
                        batch_review_lens=batch_review_lens,
                        embeddings=embeddings,
                        keep_prob=FLAGS.keep_prob,
                        )

            for valid_epoch_num, valid_epoch in \
                enumerate(generate_epoch(
                    data_path=validation_data_path,
                    num_epochs=1,
                    batch_size=FLAGS.batch_size,
                    )):

                for valid_batch_num, (valid_batch_features, valid_batch_seq_lens) in \
                    enumerate(valid_epoch):

                    valid_batch_reviews, valid_batch_labels = valid_batch_features
                    valid_batch_review_lens, = valid_batch_seq_lens

                    valid_logits, valid_loss, valid_acc = imdb_model.eval(
                        sess=sess,
                        batch_reviews=valid_batch_reviews,
                        batch_labels=valid_batch_labels,
                        batch_review_lens=valid_batch_review_lens,
                        embeddings=embeddings,
                        keep_prob=1.0, # no dropout for val|test
                        )

            print (""[EPOCH]: %i, [LR]: %.6e, [TRAIN ACC]: %.3f, [VALID ACC]: %.3f "" \
                   ""[TRAIN LOSS]: %.6f, [VALID LOSS]: %.6f"" % (
                train_epoch_num, lr, train_acc, valid_acc, train_loss, valid_loss))

            # Store the metrics
            metrics[""train_loss""].append(train_loss)
            metrics[""valid_loss""].append(valid_loss)
            metrics[""train_acc""].append(train_acc)
            metrics[""valid_acc""].append(valid_acc)

            # Store attn history
            for i in range(5):
                sample = ""sample_%i""%i
                attn_history[sample][""review""] = batch_reviews[i]
                attn_history[sample][""label""] = batch_labels[i]
                attn_history[sample][""review_len""] = batch_review_lens[i]
                attn_history[sample][""attn_scores""].append(attn_scores[i])

            # Save the model (maybe)
            if ((train_epoch_num == (FLAGS.num_epochs-1)) or
            ((train_epoch_num%FLAGS.save_every == 0) and (train_epoch_num>0))):

                # Make parents ckpt dir if it does not exist
                if not os.path.isdir(os.path.join(basedir, FLAGS.data_dir, 'ckpt')):
                    os.makedirs(os.path.join(basedir, FLAGS.data_dir, 'ckpt'))

                # Make child ckpt dir for this specific model
                if not os.path.isdir(os.path.join(basedir, FLAGS.ckpt_dir)):
                    os.makedirs(os.path.join(basedir, FLAGS.ckpt_dir))

                checkpoint_path = \
                    os.path.join(
                        basedir, FLAGS.ckpt_dir, ""%s.ckpt"" % FLAGS.model_name)

                print (""==> Saving the model."")
                imdb_model.saver.save(sess, checkpoint_path,
                                 global_step=imdb_model.global_step)

    # Save the metrics
    metrics_file = os.path.join(basedir, FLAGS.ckpt_dir, 'metrics.p')
    with open(metrics_file, 'wb') as f:
        pickle.dump(metrics, f)

    # Save the attention scores
    attn_history_file = os.path.join(basedir, FLAGS.ckpt_dir, 'attn_history.p')
    with open(attn_history_file, 'wb') as f:
        pickle.dump(attn_history, f)",0.5451112986,
2386,train,"## TODO: Create train and evaluate function using tf.estimator
def train_and_evaluate(output_dir, num_train_steps):
  #ADD CODE HERE
  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)",0.5448133945,
2386,train,"def encode_feature(feature_name):
    # globals()[df_name]
    global X_train, X_test
    
    # integer encode
    label_encoder = LabelEncoder()
    integer_encoded = label_encoder.fit_transform(X_train[feature_name])
    orig_levels = list(X_train[feature_name].unique())
    # dictionary: encoded level: original level
    col_name = dict(zip(label_encoder.transform(orig_levels),\
                    label_encoder.inverse_transform(label_encoder.transform(orig_levels))))
    # binary encode
    onehot_encoder = OneHotEncoder(sparse=False)
    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)
    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)

    X_train = pd.concat([X_train.reset_index(), pd.DataFrame(onehot_encoded)], axis=1).set_index('id')
    X_train.rename(columns=col_name, inplace=True)

    del integer_encoded, onehot_encoded 

    integer_encoded = label_encoder.transform(X_test[feature_name])
    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)
    onehot_encoded = onehot_encoder.transform(integer_encoded)

    X_test = pd.concat([X_test.reset_index(), pd.DataFrame(onehot_encoded)], axis=1).set_index('id')
    X_test.rename(columns=col_name, inplace=True)

    # merge_rename(X_test)

    del integer_encoded, onehot_encoded 

encode_feature('domain_type')
encode_feature('day')",0.5412896872,
2386,train,"def accuracy(X, Y):
    accuracy = model.evaluate(X.reshape(shape(X)), Y)[1]
    print (""Accuracy:"", accuracy)

for i in range(len(inputs)):
    accuracy(inputs[i], targets[i])",0.5406933427,
41,algorithm,"def h(n):
    if n <= 3:
        return n
    else:
        return h(n-1) + 2*h(n-2) + 3*h(n-3)


assert h(1) == 1
assert h(2) == 2
assert h(3) == 3
assert h(4) == 10
assert h(5) == 22

def h_iter(n):",0.4093361497,
41,algorithm,"def calculate_io(k, x):
    return 4 * (21 + k + 3 * (x + (1 - x) * k))

for k, x in [(3, 0.75), (2, 0.5), (2, 0.75), (3, 0.5)]:
    print(calculate_io(k, x))",0.3954842091,
41,algorithm,"# average of scores to compare the different models

print crossValidate(features, target, KNeighborsClassifier(3),10, 0)
print crossValidate(features, target, KNeighborsClassifier(4),10, 0)
print crossValidate(features, target, KNeighborsClassifier(5),10, 0)",0.394551456,
41,algorithm,"def question4(T, r, n1, n2):
    builtTree = Tree()
    
question4(tree,4,8,10)


print [val for val in tree]",0.3900641501,
41,algorithm,"subplot(121)
imshow(maximum_filter(cc1_w, (20,20)))
colorbar()
subplot(122)
imshow(minimum_filter(cc1_w, (20,20)))
colorbar()",0.3879497349,
41,algorithm,"subplot(121)
imshow(maximum_filter(cc2_w, (20,20)))
colorbar()
subplot(122)
imshow(minimum_filter(cc2_w, (20,20)))
colorbar()",0.3879497349,
41,algorithm,"num_samples_in_active_learning_batch = [50] + [5]*200 + [20]*200
al_history = run_active_learner(df, model, num_samples_in_active_learning_batch, select_next_batch_func)",0.3872045577,
41,algorithm,"class MasterPedagogicPeriod(Enum):
    Master1 = 2230106
    Master2 = 942192
    Master3 = 2230128
    #These next three are commented out because there is no data for them.
    #Master4 = 2230140
    #Mineur1 = 2335667
    #Mineur2 = 2335676
    MasterThesisFall = 249127
    MasterThesisSpring = 3781783",0.3864262104,
41,algorithm,"def replace_rating(col):
    if col > 3:
        return 2
    if col == 3:
        return 1
    if col < 3:
        return 0
            
df_new['pos_neg'] = df_new['overall'].apply(replace_rating)",0.3863862753,
41,algorithm,"exp = Exponential(-3)
pol = Polynomial( [1, -1, 1, -1, 1] )
sin = Sin(30)
a = 4
(a * exp * pol * sin + 10).plot()",0.3855256736,
1074,linear regression predicting miles per gallon usage in cars,"fluxes = jl.results.get_point_source_flux(100 * u.keV, 1 * u.MeV)

# Same results would be obtained with
# fluxes = results_reloaded.get_point_source_flux(100 * u.keV, 1 * u.MeV)",0.5082815289,
1074,linear regression predicting miles per gallon usage in cars,"fluxes_bs = bs.results.get_point_source_flux(100 * u.keV, 1 * u.MeV)",0.5082815289,
1074,linear regression predicting miles per gallon usage in cars,"unit=set_units(muH=1.4271)
print unit['density'].cgs/1.4271/c.m_p.cgs
print unit['velocity']
print unit['length']
print unit['mass']
print unit['time']
print unit['magnetic_field']

# other units can be easily obtained
print unit.keys()",0.5055721402,
1074,linear regression predicting miles per gallon usage in cars,(65-55 - 2) * b2.mV/(20.0 * b2.Mohm),0.5015828609,
1074,linear regression predicting miles per gallon usage in cars,"ha_hist_y, mname = comp.readMetricData(all_dict['HA Histogram y band OneDSlicer']['metricName'],
                                       all_dict['HA Histogram y band OneDSlicer']['metricMetadata'],
                                       all_dict['HA Histogram y band OneDSlicer']['slicerName'])",0.5001510978,
1074,linear regression predicting miles per gallon usage in cars,"ha_hist_all, mname = comp.readMetricData(all_dict['HA Histogram all bands OneDSlicer']['metricName'],
                                         all_dict['HA Histogram all bands OneDSlicer']['metricMetadata'],
                                         all_dict['HA Histogram all bands OneDSlicer']['slicerName'])",0.5001510978,
1074,linear regression predicting miles per gallon usage in cars,"df_criteria = liquor_2015_q1[liquor_2015_q1['City <lambda>'] == 'DES MOINES']
df_target = liquor_2016_q1[liquor_2016_q1['City <lambda>'] == 'DES MOINES']  
x_train = df_criteria['Volume Sold (Liters) sum']
x_train = x_train.reshape(-1,1)
y_train = df_criteria['Sale (Dollars) sum']
x_test = df_target['Volume Sold (Liters) sum']
x_test = x_test.reshape(-1,1)
y_test = df_target['Sale (Dollars) sum']
scores = cross_val_score(reg, x_train,y_train, cv=8)
model = reg.fit(x_train,y_train)
pred = model.predict(x_test)
print 'Des Moines'
print ''
print 'r2 score: ', r2_score(y_test, pred)
print 'mse: ', mean_squared_error(y_test, pred)
print 'coefficients: ', model.coef_
print 'cv scores: ', scores
print ''
print '---------------------'
print''
fig = plt.figure(figsize=(8,6))
ax = fig.add_subplot(111)
ax.scatter(x_test,y_test)
ax.plot(x_test,pred)
ax.set(xlabel='Total volume sold', ylabel='sales sum')
plt.show()",0.4996179044,
1074,linear regression predicting miles per gallon usage in cars,"stats.mstats.kruskalwallis(pivoted.Timely_Response_Ratio['No Tag'], pivoted.Timely_Response_Ratio['Older American'], 
                          pivoted.Timely_Response_Ratio['Older American, Servicemember'], pivoted.Timely_Response_Ratio['Servicemember'])",0.4987068474,
1074,linear regression predicting miles per gallon usage in cars,"print('The max clad temperature is ',(obj.max_T_clad()[0]).to(U.degF), 
      ' at ',obj.max_T_clad()[1])
print('The max fuel temperature is ',(obj.max_T_fuel()[0]).to(U.degF), 
      ' at ',obj.max_T_fuel()[1])",0.4975265265,
1074,linear regression predicting miles per gallon usage in cars,"pm.visualize_voltages(model_name=""PC2015Masoli"", region_of_interest=""vm_soma"")",0.4971374273,
2622,write a python function that takes a list and returns a new list with unique elements of the first list,"# using a list-comprehension
def list_length_words(list_words):
    return [len(word) for word in list_words]

print(list_length_words(['great', 'job', 'so', 'far']))",0.5213526487,
2622,write a python function that takes a list and returns a new list with unique elements of the first list,"from collections import Counter


## number of unique states
label_set = np.unique(sorted_states)
nlabels = len(label_set)


b_all = []
for s in sorted_states_new:
    b = np.zeros((nlabels,nlabels))

    for (x,y), c in Counter(zip(s, s[1:])).iteritems():
        b[x-1,y-1] = c

    b_all.append(b)",0.5155652165,
2622,write a python function that takes a list and returns a new list with unique elements of the first list,"def uniques(x):
    return train_enhanced[x].unique()

def num_uniques(x):
    return len(train_enhanced[x].unique())

def perc_nulls(x):
    return float(nulls_details[nulls_details['Item']==x]['%'])

def data_type(x):
    return train_enhanced[x].dtype

def dataset_summary(dataset):
    
    df=pd.DataFrame({'Item':dataset.columns})
    df['Uniques']=df['Item'].apply(uniques)
    df['Num_Uniques']=df['Item'].apply(num_uniques)
    df['Data Type']=df['Item'].apply(data_type)
    #df['%_nulls']=df['Item'].apply(perc_nulls)
    df=df.sort_values(by=['Num_Uniques'],ascending=True)
    return df",0.5140504837,
2622,write a python function that takes a list and returns a new list with unique elements of the first list,"IncGroup = sorted(dfMCats.IncidentGroup2.unique().tolist())
IncNum = dict(zip(IncGroup, range(len(IncGroup))))
IncNum",0.5116978884,
2622,write a python function that takes a list and returns a new list with unique elements of the first list,"# list all possible single events
# list of events
event_list = all_data.Events.unique()

# tokenize and flat the event list
event_list_tokenized = [str(x).split('-') for x in event_list]
flatten = lambda l: [item for sublist in l for item in sublist]
single_event_list = list(set(flatten(event_list_tokenized)))
single_event_list.remove('nan')",0.5086289644,
2622,write a python function that takes a list and returns a new list with unique elements of the first list,"from collections import Counter
token_counter = Counter(token.lower() for sentences in tokenized for token in sentences)
top10 = token_counter.most_common()[:10]
for index, tok in enumerate(top10):
    print('{:>2}.{:>5}  freq: {:>7}'.format(index+1, tok[0], tok[1]))",0.50610286,
2622,write a python function that takes a list and returns a new list with unique elements of the first list,"from collections import Counter

#Note that we convert all tokens to lower case, otherwise words like *The* and *the* are different tokens.
token_counter = Counter(token.lower() for sentence in tokenized for token in sentence)
top10 = token_counter.most_common()[:10]
for i, t in enumerate(top10):
    print('{:>2}.{:>5}  freq: {:>7}'.format(i+1, t[0], t[1]))",0.50610286,
2622,write a python function that takes a list and returns a new list with unique elements of the first list,"from collections import Counter
in_several_lists = Counter(all_phil)

# Print most popular philosopher
print ""Philosopher in most lists: ""
print in_several_lists.most_common(1)

## The list of all unique philosophers
unique_all_phil = np.unique(all_phil)

## Printing the values
#for philosopher in unique_all_phil.flat:
#    print(philosopher)",0.5058258176,
2622,write a python function that takes a list and returns a new list with unique elements of the first list,"equivalence_classes = [constrain(f) for f in fractions]
equivalence_classes = list(set(equivalence_classes))
equivalence_classes.sort(key=hd)",0.5033724308,
2622,write a python function that takes a list and returns a new list with unique elements of the first list,"# Import Counter to generate BOW
from collections import Counter

# Tokenize the article: tokens
tokens = word_tokenize(article)

# Convert the tokens into lowercase: lower_tokens
lower_tokens = [t.lower() for t in tokens]

# Create a Counter with the lowercase tokens: bow_simple
bow_simple = Counter(lower_tokens)

# Print the 10 most common tokens
print(bow_simple.most_common(10))",0.5029486418,
1419,"part implement the affine transformation, w x + b","def warp_im(im, M, dshape):
    output_im = numpy.zeros(dshape, dtype=im.dtype)
    cv2.warpAffine(im,
                   M[:2],
                   (dshape[1], dshape[0]),
                   dst=output_im,
                   borderMode=cv2.BORDER_TRANSPARENT,
                   flags=cv2.WARP_INVERSE_MAP)
    return output_im",0.4827848375,
1419,"part implement the affine transformation, w x + b","def affine_transform(input_image, input_triangle, output_triangle, size):
    warp_matrix = cv2.getAffineTransform(
        np.float32(input_triangle), np.float32(output_triangle))
    output_image = cv2.warpAffine(input_image, warp_matrix, (size[0], size[1]), None,
                                  flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT_101)
    return output_image",0.4558809102,
1419,"part implement the affine transformation, w x + b","def new_algorithm(image):
    # Just mess up the affine
    bad_affine = image.affine.copy()
    bad_affine[:, :2] = mask.affine[:, 1::-1]
    return image.__class__(image.get_data(), bad_affine, mask.header)

mask = nl.image.math_img(""img > 0"", img='/data/ds000114/derivatives/freesurfer/sub-01/mri/brainmask.auto.mgz')
new_mask = new_algorithm(mask)",0.4535021782,
1419,"part implement the affine transformation, w x + b","# functions to generate new images

def apply_affine_transform(img, T):
    rows, cols, ch = img.shape
    return cv2.warpAffine(img, T, (cols, rows))

def random_translate(img, max_displacement = 2):
    h, w =  img.shape[0:2]
    
    tx = np.random.uniform(-max_displacement, max_displacement)
    ty = np.random.uniform(-max_displacement, max_displacement)
    
    T = np.array([[1.0, 0.0, tx], [0.0, 1.0, ty]], dtype = np.float32)

    return apply_affine_transform(img, T)

def random_rotate(img, max_angle_degrees = 15.0):
    theta = np.random.uniform(-max_angle_degrees, max_angle_degrees) * np.pi/180.0
    
    c = np.cos(theta)
    s = np.sin(theta)
    
    T = np.array([[c, -s, 0.0], [s, c, 0.0]], dtype = np.float32)
    return apply_affine_transform(img, T)

def random_scale(img, max_scale_factor = 0.1):    
    sx = 1.0 + np.random.uniform(-max_scale_factor, max_scale_factor)
    sy = 1.0 + np.random.uniform(-max_scale_factor, max_scale_factor)
    
    T = np.array([[sx, 0.0, 0.0], [0.0, sy, 0.0]], dtype = np.float32)
    return apply_affine_transform(img, T)",0.4455005229,
1419,"part implement the affine transformation, w x + b","def register(inImg, baseImg, iterations, MI=False):
    return ndreg.imgAffineComposite(inImg,
                              baseImg,
                              iterations=iterations,
                              useMI=MI,
                              verbose=False)",0.4423239827,
1419,"part implement the affine transformation, w x + b","def cen_rot2 (im, rot, dim_out, offset1=(0,0), offset2=(0,0), order=2):
    """"""cen_rot2 - takes a cube of images im, and a set of rotation angles in rot,
                and translates the middle of the frame with a size dim_out to the middle of
                a new output frame with an additional rotation of rot.
                """"""
    from scipy.ndimage import affine_transform
    a = rot * np.pi / 180.
    
    # make a rotation matrix
    transform=np.array([[np.cos(a),-np.sin(a)],[np.sin(a),np.cos(a)]])
    
    # calculate total offset for image output

    # determine centre of input image

    # -0.5 is there for fencepost counting error
    c_in = np.array(offset1) - 0.5
    c_out = 0.5 * np.array(dim_out) - 0.5

    # c_out has to be pre-rotated to make offset correct
    offset = c_in - c_out.dot(transform) - np.array(offset2).dot(transform)
    
    # perform the transformation
    dst=affine_transform( \
        im,transform.T, order=order,offset=offset, \
        output_shape=dim_out, cval=0.0)
    return(dst)",0.4418680072,
1419,"part implement the affine transformation, w x + b","### Pre Process Steps for randomized approaches !!
def apply_affine_transform(img, T):
    rows, cols, ch = img.shape
    return cv2.warpAffine(img, T, (cols, rows))

def random_translate(img, max_relative_displacement = 0.1):
    h, w =  img.shape[0:2]
    
    tx = np.random.uniform(-max_relative_displacement * w, max_relative_displacement * w)
    ty = np.random.uniform(-max_relative_displacement * h, max_relative_displacement * h)
    
    T = np.array([[1.0, 0.0, tx], [0.0, 1.0, ty]], dtype = np.float32)

    return apply_affine_transform(img, T)

def random_rotate(img, max_angle_degrees = 5.0):
    theta = np.random.uniform(-max_angle_degrees, max_angle_degrees) * np.pi/180.0
    
    c = np.cos(theta)
    s = np.sin(theta)
    
    T = np.array([[c, -s, 0.0], [s, c, 0.0]], dtype = np.float32)
    return apply_affine_transform(img, T)

def random_scale(img, max_scale_factor = 0.05):    
    sx = 1.0 + np.random.uniform(-max_scale_factor, max_scale_factor)
    sy = 1.0 + np.random.uniform(-max_scale_factor, max_scale_factor)
    
    T = np.array([[sx, 0.0, 0.0], [0.0, sy, 0.0]], dtype = np.float32)
    return apply_affine_transform(img, T)

def random_shear(img, max_sheer_factor = 0.05):
    kx = np.random.uniform(-max_sheer_factor, max_sheer_factor)
    ky = np.random.uniform(-max_sheer_factor, max_sheer_factor)
    
    T = np.array([[1.0, kx, 0.0], [ky, 1.0, 0]], dtype = np.float32)
    return apply_affine_transform(img, T)

def jitter_image(img):
    fnc = np.random.randint(0, 4)
    
    if fnc == 0:
        return random_translate(img)    
    elif fnc == 1:
        return random_rotate(img)    
    elif fnc == 2:
        return random_scale(img)
    else:
        return random_shear(img)        
    
def generate_fake_data(X, y, old_class_distribution, extend_factor = 10):
    X_fake = []
    y_fake = []
    
    # Get minimum final size of dataset, as well as nr of images per class
    final_size = extend_factor * X.shape[0]
    n_classes = len(old_class_distribution)
    n_images_per_class = int(np.ceil(final_size / n_classes))
    
    for i in range(X.shape[0]):
        # Compute number of required images
        class_idx = y[i]
        current_distribution_i = old_class_distribution[class_idx]        
        n_fake_images = int(np.ceil((n_images_per_class - current_distribution_i) / current_distribution_i))        
        
        # Generate fake images
        for j in range(n_fake_images):
            X_fake.append(jitter_image(X[i]))
            y_fake.append(y[i])
        
    return (np.array(X_fake), np.array(y_fake))

def extend_training_data(X, y, old_class_distribution):
    X_fake, y_fake = generate_fake_data(X, y, old_class_distribution)
    
    X = np.concatenate((X, X_fake), axis = 0)
    y = np.concatenate((y, y_fake), axis = 0)
    
    return (X, y)",0.4410244823,
1419,"part implement the affine transformation, w x + b","def strided_image(img, kernel_size, pad_val=0):
    padw = kernel_size[0] // 2
    padh = kernel_size[1] // 2
    padded = np.pad(img, mode='constant', 
                    pad_width=((padw,padw),(padh,padh),(0,0)), 
                    constant_values=pad_val)
    shape = [*padded.shape[:-1], *kernel_size, padded.shape[-1]]
    strides = [*padded.strides[:-1], *padded.strides]
    shape[0] -= padw * 2 # apparent shape hides pad
    shape[1] -= padh * 2
    return as_strided(padded, shape=shape, strides=strides)",0.4396868348,
1419,"part implement the affine transformation, w x + b","def transform_triangle(a, b, verts):
    t = cv2.getAffineTransform(np.float32(a[:,:-1]), np.float32(b[:,:-1]))
    verts2d = np.copy(verts)
    verts2d[:,2] = 1
    result = np.copy(verts)
    result[:,:-1] = np.dot(verts2d, t.T)
    return result",0.4333221912,
1419,"part implement the affine transformation, w x + b","def fancy_x_trans_slice(img_slice, x_vox_trans):
    """""" Make a new copy of `img_slice` translated by `x_vox_trans` voxels
    
    `x_vox_trans` can be positive or negative, and can be a float.
    """"""
    # Resample image using bilinear interpolation (order=1)
    trans_slice = snd.affine_transform(img_slice, [1, 1], [-x_vox_trans, 0], order=1)
    return trans_slice",0.4266956449,
1891,separate features and labels,"def split_train_test():
    # TODO: complete this function
        
    X_train = data[:,:] # currently, this selects all rows and all columns
    X_test  =
    
    y_train = target[:] # currently, this selects all rows
    y_test  = 
    
    return X_train, y_train, X_test, y_test",0.4861306846,
1891,separate features and labels,"def load_data():
    from sklearn.model_selection import train_test_split
    
    X = np.genfromtxt('mush_features.csv')
    Y = np.genfromtxt('mush_labels.csv')
    
    train_set_x, test_set_x, train_set_y, test_set_y = train_test_split(X, Y, test_size=0.33, random_state=42)
    
    train_set_x = train_set_x[:300].astype(float)
    train_set_y = train_set_y[:300].astype(float)
    
    test_set_x = test_set_x[:100].astype(float)
    test_set_y = test_set_y[:100].astype(float)
    
    x_test = train_set_x[:5]
    y_test = train_set_y[:5]   
    
    train_set_x = train_set_x.reshape(train_set_x.shape[0], -1).T
    test_set_x = test_set_x.reshape(test_set_x.shape[0], -1).T
    
    train_set_y = train_set_y.reshape((1, train_set_y.shape[0]))
    test_set_y = test_set_y.reshape((1, test_set_y.shape[0]))
    
    x_test = x_test.reshape(x_test.shape[0], -1).T
    y_test = y_test.reshape((1, y_test.shape[0]))
    
    return train_set_x, test_set_x, train_set_y, test_set_y, x_test, y_test",0.4691094458,
1891,separate features and labels,"# Loading the data

def load_data():
    from sklearn.model_selection import train_test_split

    data = np.genfromtxt('kangaroo.csv', delimiter=',')
    
    x = data[:, 0]
    y = data[:, 1]
    
    train_set_x, test_set_x, train_set_y, test_set_y = train_test_split(x, y, test_size=0.33, random_state=42)
        
    return train_set_x, test_set_x, train_set_y, test_set_y

train_set_x, test_set_x, train_set_y, test_set_y = load_data()",0.4655672908,
1891,separate features and labels,"def process_embarked():
    global combined
    
    # 2 missing values in embarked column
    combined.Embarked.fillna('S',inplace=True)
    
    # encoding in dummy variables
    embarked_dummies=pd.get_dummies(combined['Embarked'],prefix='Embarked')
    combined=pd.concat([combined, embarked_dummies],axis=1)
    
    # removing embarked variable
    combined.drop('Embarked',axis=1,inplace=True)
    
    status('Embarked')",0.4654252231,
1891,separate features and labels,"def process_embarked():
    global combined
    # two missing embarked values - filling them with the most frequent one in the train  set(S)
    combined.Embarked.fillna('S', inplace=True)
    # dummy encoding 
    embarked_dummies = pd.get_dummies(combined['Embarked'], prefix='Embarked')
    combined = pd.concat([combined, embarked_dummies], axis=1)
    combined.drop('Embarked', axis=1, inplace=True)
    status('embarked')
    return combined",0.4654252231,
1891,separate features and labels,"def process_embarked():
    
    global combined
    # There are only two missing embarked values - let's fill them with the most frequent one (S)
    combined.Embarked.fillna('S',inplace=True)
    
    # dummy encoding 
    embarked_dummies = pd.get_dummies(combined['Embarked'],prefix='Embarked')
    combined = pd.concat([combined,embarked_dummies],axis=1)
    combined.drop('Embarked',axis=1,inplace=True)
    
    status('embarked')",0.4654252231,
1891,separate features and labels,"def process_embarked():
    global combined
    combined.Embarked.fillna('S', inplace=True)
    # dummy encoding
    embarked_dummies = pd.get_dummies(combined['Embarked'], prefix='Embarked')
    combined = pd.concat([combined, embarked_dummies], axis=1)
    combined.drop('Embarked', axis=1, inplace=True)
    status('Embarked')
    
# execute the function    
process_embarked()",0.4654252231,
1891,separate features and labels,"def process_embarked():
    global combined
    # two missing embarked values - filling them with the most frequent one in the train set(S)
    combined.Embarked.fillna('S', inplace=True)
    # dummy encoding
    embarked_dummies = pd.get_dummies(combined['Embarked'], prefix='Embarked')
    combined = pd.concat([combined, embarked_dummies], axis=1)
    combined.drop('Embarked', axis=1, inplace=True)
    status('embarked')
    return combined",0.4654252231,
1891,separate features and labels,"def process_embarked():
    global combined
    # there are two missing embarked value, lets fill them with the most frequent one (S)
    combined.Embarked.fillna('S', inplace=True)
    
    # dummy encoding
    # we will extract the embarked locations and make them as separate fetaures 
    embarked_dummies = pd.get_dummies(combined['Embarked'],prefix='Embarked')
    combined = pd.concat([combined,embarked_dummies],axis=1)
    combined.drop('Embarked',axis=1,inplace=True)
    
    status('Embarked')",0.4654252231,
1891,separate features and labels,"def json_description(crawl_json_records):
    """"""Produces information about a JSON object containing crawl data.
    
    Args:
      crawl_json_records (list): A list of JSON objects such as the
                                 crawl_json variable above.
    
    Returns:
      2-tuple: An (int, set) pair.  The integer is the number of records
               in the given list.  The set is a set (constructed with
               Python's set() function) of strings.  It contains all the
               field top names that appear at the top level of any record
               in crawl_json_records.
    """"""
    num_records = len(crawl_json_records)
    possible_fields = list(crawl_json_records[0].keys())
    # Feel free to erase the next line.  It just demonstrates how
    # to return a tuple.  You'll have to replace the variable names
    # with whatever you define in this function.
    return (num_records, possible_fields)",0.464412719,
218,cluster analysis,"def split(X):
    cluster = int(input())
    model = KMeans(n_clusters=cluster)
    minsos = []
    l = []
    closepts = []
    m = []
    n = []
    closest = []
    closeindex = []
    smallest = 9999
    for i in range(50):

        model.fit(X)
        y_pred = model.predict(X)
        l = []
        m = []
        n = []
        for i in {i: np.where(model.labels_ == i)[0] for i in range(model.n_clusters)}:
            for j in {i: np.where(model.labels_ == i)[0] for i in range(model.n_clusters)}[i]:
                if math.sqrt((dataset[j][0]-model.cluster_centers_[i][0])**2+(dataset[j][1]-model.cluster_centers_[i][1])**2+(dataset[j][2]-model.cluster_centers_[i][2])**2) < smallest:
                    smallest = math.sqrt((dataset[j][0]-model.cluster_centers_[i][0])**2+(dataset[j][1]-model.cluster_centers_[i][1])**2+(dataset[j][2]-model.cluster_centers_[i][2])**2)
                    closest = dataset[j]
                    closestlabel = j
            l.append(smallest)
            m.append(closest)
            n.append(closestlabel)
            smallest = 9999
        minsos.append(l)
        closepts.append(m)
        closeindex.append(n)
    
    
    smallestsos = 9999
    sos = 0
    mark = 0
    for i in range(len(minsos)):
        sos = 0
        for j in minsos[i]:
            sos = sos+j**2
        if sos < smallestsos:
            smallestsos = sos
            mark = i
    
    trainingset = closepts[mark]
    totalindex = closeindex[mark]
    

    testset = []
    testen = []
    for k in dataset:
        testset.append(k)
    for k in energy:
        testen.append(k)
    for i in range(len(dataset)):
        for j in range(len(trainingset)):
            if trainingset[j] == dataset[i]:
                testset.remove(trainingset[j])
                testen.remove(energy[i])
    test = np.array(testset)
    testenergy = np.array(testen)
    

    A = trainingset
    B = []
    for i in totalindex:
        B.append(energy[i])
    training = np.array(A)
    trainingenergy = np.array(B)
    return training, trainingenergy, test, testenergy, model, cluster",0.52651757,
218,cluster analysis,"# remove noise - clusters with n of observations < 5

def clean_clusters(AP):
    MIN_NO_OF_POINTS = 5
    good_clusters = []

    for cluster_id in pd.unique(AP.cluster):
        if len(AP[AP.cluster == cluster_id]) >= MIN_NO_OF_POINTS:
            good_clusters.append(cluster_id)

    return AP[AP.cluster.isin(good_clusters)]
        
# condition 2 - clusters not overlapping in time
def find_mobile_APs(AP):
    cluster_timeranges = []
    for cluster_id in good_clusters:
        cluster = AP[AP['cluster'] == cluster_id]

        timeranges_columns = ('cluster', 'min_time', 'max_time')
        cluster_timeranges.append((cluster_id, cluster.index.min(), cluster.index.max()))
    #identify overlap
    time_overlap = set()
    
    for cluster, min_time, max_time in cluster_timeranges:
        for this_cluster, this_min_time, this_max_time in cluster_timeranges:
            if cluster == this_cluster: continue
        
            if this_min_time > min_time < this_max_time:
                time_overlap.add((cluster, this_cluster))
            if this_min_time > max_time < this_max_time:
                time_overlap.add((cluster, this_cluster))
    
    BSSID = AP.BSSID.iloc[0]
    if time_overlap:
        print (""AP {} is mobile with cluster pairs {} overlapping in time"".format(BSSID, time_overlap))
        return True
    else:
        print (""AP {} does not seem to be mobile as there is no overlap in it's clusters"".format(BSSID))
        return False",0.5193817616,
218,cluster analysis,"def plot_store_transactions_cluster(clust):
    clust_stores=stores.loc[stores.cluster==clust,'store_nbr']
    n=len(clust_stores)
    m=1
    for x in range(1,6):
        if (n-1) in range((x-1)**2,x**2):
            m=x
    plt.figure(figsize=(15,15))
    for x in range(n):
        nbr=clust_stores.iloc[x]
        ax1 = plt.subplot(m,m,x+1)
        ax1.scatter(sales.loc[(~sales.dayoff)&(sales.cluster==clust)&(sales.store_nbr==nbr),'date'].values,
                sales.loc[(~sales.dayoff)&(sales.cluster==clust)&(sales.store_nbr==nbr),'transactions'])
        ax1.scatter(sales.loc[(sales.dayoff)&(sales.cluster==clust)&(sales.store_nbr==nbr),'date'].values,
                sales.loc[(sales.dayoff)&(sales.cluster==clust)&(sales.store_nbr==nbr),'transactions'])
        plt.title('Store {}, Type {}, Cluster {}'.format(nbr,
                                                         list(stores.loc[stores.store_nbr==nbr,'type'])[0],
                                                         list(stores.loc[stores.store_nbr==nbr,'cluster'])[0])
             )
        plt.suptitle(' Cluster {} stores'.format(clust),fontsize=25)
    plt.show()",0.5153737664,
218,cluster analysis,"def split(X):
    
    model = KMeans(n_clusters=cluster)
    minsos = []
    l = []
    closepts = []
    m = []
    n = []
    closest = []
    closeindex = []
    smallest = 9999
    for i in range(50):

        model.fit(X)
        
        l = []
        m = []
        n = []
        for i in {i: np.where(model.labels_ == i)[0] for i in range(model.n_clusters)}:
            for j in {i: np.where(model.labels_ == i)[0] for i in range(model.n_clusters)}[i]:
                if distance(model.cluster_centers_[i][0], model.cluster_centers_[i][1], model.cluster_centers_[i][2], dataset[j][0], dataset[j][1], dataset[j][2]) < smallest:
                    smallest = distance(model.cluster_centers_[i][0], model.cluster_centers_[i][1], model.cluster_centers_[i][2], dataset[j][0], dataset[j][1], dataset[j][2])
                    closest = dataset[j]
                    closestlabel = j
            l.append(smallest)
            m.append(closest)
            n.append(closestlabel)
            smallest = 9999
        minsos.append(l)
        closepts.append(m)
        closeindex.append(n)
    
    
    smallestsos = 9999
    sos = 0
    mark = 0
    for i in range(len(minsos)):
        sos = 0
        for j in minsos[i]:
            sos = sos+j**2
        if sos < smallestsos:
            smallestsos = sos
            mark = i
    
    trainingset = closepts[mark]
    totalindex = closeindex[mark]
    

    testset = []
    testen = []
    for k in dataset:
        testset.append(k)
    for k in energy:
        testen.append(k)
    for i in range(len(dataset)):
        for j in range(len(trainingset)):
            if trainingset[j] == dataset[i]:
                testset.remove(trainingset[j])
                testen.remove(energy[i])
    test = np.array(testset)
    testenergy = np.array(testen)
    

    A = trainingset
    B = []
    for i in totalindex:
        B.append(energy[i])
    training = np.array(A)
    trainingenergy = np.array(B)
    return training, trainingenergy, test, testenergy, model, cluster",0.5066717267,
218,cluster analysis,"def run_visualization(L,p):
    plt.imshow(PercolationAnalysis(L,p).clusterIDs,cmap='OrRd',interpolation=""none"")
    plt.title('Cluster ID')
    plt.colorbar()
    plt.show()
    return",0.5019888878,
218,cluster analysis,"def print_clusters(labels, names, ret=None):
    clusters=[[] for i in range(clf2.n_clusters)]
    for i,label in enumerate(labels):
        clusters[label].append(names[i][0])
    for i, c in enumerate(clusters):
        print ""Cluster"",i,"":"", "", "".join(c)
        print
    if ret:
        return clusters
print_clusters(clf2.labels_, names_train)",0.4973343015,
218,cluster analysis,"def plot_cluster(dataframe, subreddit):
    # Build a color map to match the Bokeh plot
    colormap = dict(zip(
        np.unique(dataframe.cluster),
        ['#777777'] + sns.color_palette('plasma', dataframe.cluster.max() + 1).as_hex()
    ))
    subregion_defined = True
    
    # Figure and gridspec to layout axes
    fig = plt.figure()
    gs = GridSpec(3, 3)
    
    # First axes, spanning most of the figure
    # Contains just the points in a region 
    # around the points in the cluster
    ax1 = plt.subplot(gs[:,:2])
    try:
        bounds = cluster_bounds(dataframe, subreddit)
    except IndexError:
        ax1.text(0.5, 0.5, 'Subreddit {} not found!'.format(subreddit), 
                 horizontalalignment='center', verticalalignment='center',
                 transform=ax1.transAxes, fontsize=18)
        subregion_defined = False
    
    if subregion_defined:
        to_plot = data_in_bounds(dataframe, bounds)
        ax1.scatter(to_plot.x, to_plot.y, c=to_plot.cluster.map(colormap), s=30, alpha=0.5)
    
        # We want to add text labels. We subsample up to 50 labels
        # And then use adjustText to get them non-overlapping
        text_elements = []
        for row in to_plot.sample(n=min(len(to_plot),50), random_state=0).values:
            if row[2] != subreddit:
                text_elements.append(ax1.text(row[0], row[1], row[2], alpha=0.5, fontsize=9))
        row = to_plot[to_plot.subreddit == subreddit].values[0]
        text_elements.append(ax1.text(row[0], row[1], row[2], 
                                      color='g',
                                      alpha=0.5, fontsize=11))
        adjustText.adjust_text(text_elements, ax=ax1, lim=100,
                               force_text=0.1, force_points=0.1,
                               arrowprops=dict(arrowstyle=""-"", color='k', lw=0.5))
    
    ax1.xaxis.set_ticklabels([])
    ax1.yaxis.set_ticklabels([])

    # Second axes, center right of the figure
    # Plots all the data and a rectangle
    # Showing the area selected out
    ax2 = plt.subplot(gs[1,2])
    ax2.scatter(dataframe.x, dataframe.y, s=20,
                c=dataframe.cluster.map(colormap), alpha=0.05)
    
    if subregion_defined:
        ax2.add_patch(Rectangle(xy=(bounds[0], bounds[2]),
                                    width=(bounds[1] - bounds[0]),
                                    height=(bounds[3] - bounds[2]),
                                    edgecolor='k', facecolor='none', lw=1))
    ax2.xaxis.set_ticklabels([])
    ax2.yaxis.set_ticklabels([])
    plt.tight_layout()

    if subregion_defined:
        # Now we make use of the power of matplotlib transforms
        # to draw line from the subselected rectangle in axes2
        # all the way to the bounds of axes1
        trans_figure = fig.transFigure.inverted()

        ax1_coord = trans_figure.transform(ax1.transAxes.transform((1,0)))
        ax2_coord = trans_figure.transform(ax2.transData.transform((bounds[1],bounds[2])))
        connector1 = Line2D((ax1_coord[0],ax2_coord[0]),(ax1_coord[1],ax2_coord[1]),
                              transform=fig.transFigure, lw=1, color='k')
        ax1_coord = trans_figure.transform(ax1.transAxes.transform((1,1)))
        ax2_coord = trans_figure.transform(ax2.transData.transform((bounds[1],bounds[3])))
        connector2 = Line2D((ax1_coord[0],ax2_coord[0]),(ax1_coord[1],ax2_coord[1]),
                              transform=fig.transFigure, lw=1, color='k')

        fig.lines = [connector1, connector2]",0.4966534376,
218,cluster analysis,"def run_clustering(n):
    '''
    This function runs k-mean and agglorative clustering using a user specified cluster number.
    Input: number of clusters
    Return: None; it creates km and agc lables (km_group, agc_group) 
            respectively in the model dataset
    '''
    
    # Use K-Mean
    kmeans = KMeans(n_clusters=n, random_state=seed, n_init=init)
    whiten_df[""km_group""] = kmeans.fit_predict(whiten_df)
    
    # Use Agglomerative Clustering
    agc = AgglomerativeClustering(n_clusters=n,  compute_full_tree=True)
    whiten_df[""agc_group""] = agc.fit_predict(whiten_df)
    
    print(""--- K Mean Cluster Size ---"")
    print(whiten_df.groupby(""km_group"").size())
    
    print(""--- Agglomerative Cluster Size ---"")
    print(whiten_df.groupby(""agc_group"").size())    

def plot_clusters(cluster_name, n):
    
    '''
    This function plots the time series clusters.
    Input: the label name (i.e. 'km_group' or 'agc_group')
    Return: None; it shows the plots.
    '''
    
    fig = plt.figure(figsize = (16, 16));
    fig.subplots_adjust(hspace = 0.15*n)
    try:
        # Plot Time Series in Each Sample 
        for k in range(0, n):
            data = whiten_df[whiten_df[cluster_name] ==k][features];
            position = str(n)+""1""+str(k+1)
            fig.add_subplot(int(position));

            for row in range(0, data.shape[0]):
                plt.title(""{}: {}"".format(cluster_name, k))
                plt.plot([year for year in range(1994, 2015)], data.iloc[row, :], alpha = 0.6, c = ""grey"")

            cluster_mean = [data.iloc[:, col].mean() for col in range(0, data.shape[1])]
            plt.plot([year for year in range(1994, 2015)], cluster_mean, c = ""black"")
    except Exception as ex:
        
        print(ex)
        print('--- NOT COMPLETED ---')",0.492186904,
218,cluster analysis,"def doKMeans(data, num_clusters=0):
    # TODO: Do the KMeans clustering here, passing in the # of clusters parameter
    # and fit it against your data. Then, return a tuple containing the cluster
    # centers and the labels.
    #
    # Hint: Just like with doPCA above, you will have to create a variable called
    # `model`, which will be a SKLearn K-Means model for this to work.
    
    
    # .. your code here ..
    model = KMeans(n_clusters=num_clusters)
    model.fit(data)
    return model.cluster_centers_, model.labels_",0.49085325,
218,cluster analysis,"def graph_identification():
    
    # PA model: networks with a power law degree distribution / small shortest paths but small clustering coefficient 
    # Small World: high clustering coefficient but long shortest paths (with high p)
    
    methods = []
    for G in P1_Graphs:
        # to measure clustering coeffieient
        clustering_coef = nx.average_clustering(G)
        # to measure average shorest paths
        avg_paths = nx.average_shortest_path_length(G)
        
        degree_hist = degree_distribution(G)
        if len(degree_hist)>10:
            methods.append('PA')
        elif clustering_coef < 0.1:
            methods.append('SW_H')
        else:
            methods.append('SW_L')
    
    return methods

def degree_distribution(G):
    degrees = G.degree()
    degree_values = sorted(set(degrees.values()))
    histogram = [list(degrees.values()).count(i)/float(nx.number_of_nodes( G)) for i in degree_values]
    return histogram",0.4896581173,
185,chaos game,"# Note: Game will ignore the 0 index
def reset_board():
    global board,game_state
    board = [' '] * 10
    game_state = True",0.4479669929,
185,chaos game,"class game:
    #------------------
    # attributes
    #------------------
    def __init__(self, w, h):
        self.board = np.zeros((h,w), dtype=np.int)-1  # board
        self.player = 0                               # current player's turn
        self.col_count = np.zeros((w,), dtype=np.int) # number of pieces in each column
        self.row_count = np.zeros((h,), dtype=np.int) # number of pieces in each row
        
    #------------------
    # methods
    #------------------
    # display the status of the board
    def print_board(self):
        print(""Player "" + str(self.player+1) + ""'s turn:"")
        for k in range(h):
            row = self.board[k,:]
            row = ' '.join(str(e) for e in row)
            row = row.replace('-1','.').replace('1','+')
            print('| ' + row + ' |')

        print('-'*17)
        print('  ' + ' '.join(str(e) for e in range(w)))        
        
    #------------------
    # enter a piece into the board
    def enter_col(self):
        
        # validate input
        def parse_input():               
            # check to see that the input in an integer
            def is_int(value):
                try: 
                    int(value)
                    return True
                except: 
                    clear_output()
                    print('Input not integral.\nPlease enter an integer between 0 and ' + str(w-1))
                    return False

            print('')
            while True:
                self.print_board()
                
                # ask for input
                col = input('Enter the column to place your piece: ')
                if col.lower()=='q': print('Thanks for playing!');   exit(0)
                    
                # check if input is an integer
                if is_int(col):
                    col = int(col)
                    
                    # check to see that input is between 0 and w
                    if col in range(w):
                    
                        # check to see column is not full
                        if self.col_count[col]<w: return col
                        else: 
                            clear_output()
                            print('Column ' + str(col) + ' is full.  Please enter a different column.')
                        
                    else:
                        clear_output()
                        print('Input not an integer between 0 and ' + str(w-1) + 
                              '.\nPlease enter an integer between 0 and ' + str(w-1))

        # update the board
        col = parse_input()
        row = np.where(self.board[:,col]==-1)[0][-1]
        self.board[row, col] = self.player

        # update counts
        self.col_count[col] += 1
        self.row_count[h-row-1] += 1

        # update the player
        if self.player==0: self.player = 1
        else: self.player = 0
        

    #------------------
    # check to see if the game has been won
    def is_won(self):
        if self.player==0: player = 1
        else: player = 0
        
        def check7(vec):
            if vec[3]!=player: return False
            if vec[2]!=player: return all(vec[4:]==player) 
            if vec[1]!=player: return all(vec[4:6]==player)
            return vec[0]==player or vec[4]==player
                        
        def check6(vec):
            if any(vec[2:4]!=player): return False
            if vec[1]!=player: return all(vec[4:6]==player)
            return vec[0]==player or vec[4]==player
                    
        def check5(vec):
            if any(vec[1:4]!=player): return False
            return vec[0]==player or vec[4]==player

        def check4(vec): return all(vec==player)
                    
                    
        def player_won():
            # check rows with at least 4 pieces
            rows = np.where(self.row_count[::-1]>=4)[0].tolist()
            if rows: 
                for i in rows: 
                    if check7(self.board[i,:]): return True

            # check cols with at least 4 pieces
            cols = np.where(self.col_count>=4)[0].tolist()
            if cols: 
                for j in cols: 
                    if check6(self.board[:,j]): return True
                
            # check main diagonals
            if check6(np.diag(self.board, k=1)): return True
            if check6(np.diag(self.board, k=0)): return True
            
            # check 1st sub diagonals
            if check5(np.diag(self.board, k= 2)): return True
            if check5(np.diag(self.board, k=-1)): return True
            
            # check 2nd sub diagonals
            if check4(np.diag(self.board, k= 3)): return True
            if check4(np.diag(self.board, k=-2)): return True            
                        
            # flip board and check the opposite diagonals
            oboard = np.fliplr(self.board.T)
             
            # check main diagonals
            if check6(np.diag(oboard, k=1)): return True
            if check6(np.diag(oboard, k=0)): return True
            
            # check 1st sub diagonals
            if check5(np.diag(oboard, k= 2)): return True
            if check5(np.diag(oboard, k=-1)): return True
            
            # check 2nd sub diagonals
            if check4(np.diag(oboard, k= 3)): return True
            if check4(np.diag(oboard, k=-2)): return True 
            
            return False

        if player_won(): 
            clear_output();   
            self.print_board()
            print('Player ' + str(player+1) + ' won') 
            return True
        return False

    #------------------
    def play_game(self):
        while True:
            clear_output()
            self.enter_col()
            if self.is_won(): return",0.4461504817,
185,chaos game,"class Game:
    def __init__(self,playerOne,playerTwo):
        self.playerOneWin=False
        self.playerOneLoose=False
        self.tie=False
        self.playerOne=playerOne
        self.playerTwo=playerTwo
        self.board=Board()
        self.history=[]                               
        self.addBoardToHistory()
        
    def addBoardToHistory(self):        
        self.history.append(copy.deepcopy(self.board))
    def play(self):        
        if isinstance(self.playerOne,HumanPlayer):
            print self.board
        while(True):
            self.playerOne.makeMove(self)            
            if isinstance(self.playerOne,HumanPlayer) or isinstance(self.playerTwo,HumanPlayer):
                print self.board
            self.addBoardToHistory()
            if self.finished(): break
            
            self.playerTwo.makeMove(self)
            if isinstance(self.playerOne,HumanPlayer) or isinstance(self.playerTwo,HumanPlayer):
                print self.board
            self.addBoardToHistory()
            if self.finished(): break
                    
    def finished(self):
        win,loose=self.playerOne.winLoose(self.board)
        self.win=win
        self.loose=loose
        if not(win or loose):
            empty=self.board.emptyCells()
            if len(empty)==0:
                self.tie=True
        if (self.win or self.loose or self.tie):
            return True
        return False",0.4414823949,
185,chaos game,"class TTTGame:
    def __init__(self, players, size=3):
        self.size = size
        self.board = np.zeros((self.size, self.size))
        self.curPlayer = -1
        self.players = players
        for i in range(len(self.players)):
            #NN players need to know which symbol represents their plays in 
            #order to evaluate them
            self.players[i].symbol = 2*i - 1
        
    def isValidMove(self, coords):
        if self.board[coords] == 0:
            return True
        else:
            return False
        
    def play(self):
        curPlayerIdx = (self.curPlayer + 1) // 2
        coords = self.players[curPlayerIdx].play(self.board)
        if self.isValidMove(coords):
            self.board[coords] = self.curPlayer
            self.curPlayer *= -1
            return 1
        return 0
    
    #This is ugly, I know
    #Returns the number corresponding to a player if he's the winner,
    #two if there's no winner yet, or zero if there's neither a winner
    #nor free slots to play in, ie. a tie(it can't tell in advance)
    def checkWinner(self):
        for i in self.board.sum(axis=0):
            if int(np.abs(i) // self.size) != 0:
                return i // self.size
        for i in self.board.sum(axis=1):
            if int(np.abs(i) // self.size) != 0:
                return i // self.size
        i = self.board.trace()
        if int(np.abs(i) // self.size) != 0:
            return i // self.size
        i = self.board[::-1].trace()
        if int(np.abs(i) // self.size) != 0:
            return i // self.size
        if self.board[self.board == 0].size == 0:
            return 0
        return 2
    
    def printBoard(self):
        boardStr = [*map(lambda row:
            [*map(lambda elem:
                ({-1: ""O "", 0: ""_ "", 1:""X ""})[elem]
            , row)]
        , self.board)]
        boardStr = reduce(lambda row, res:
            """".join(res) + ""\n"" + """".join(row)
        , boardStr, """")
        print(boardStr)
        
    def reset(self):
        self.__init__(self.players, self.size)",0.4397512972,
185,chaos game,"from tic_tac_toe.Player import Player


def play_game(board: Board, player1: Player, player2: Player):
    player1.new_game(CROSS)
    player2.new_game(NAUGHT)
    board.reset()
    
    finished = False
    while not finished:
        result, finished = player1.move(board)
        if finished:
            if result == GameResult.DRAW:
                final_result = GameResult.DRAW
            else:
                final_result =  GameResult.CROSS_WIN
        else:
            result, finished = player2.move(board)
            if finished:
                if result == GameResult.DRAW:
                    final_result =  GameResult.DRAW
                else:
                    final_result =  GameResult.NAUGHT_WIN
        
    player1.final_result(final_result)
    player2.final_result(final_result)
    return final_result",0.4319895506,
185,chaos game,"def reset():
    global board,state #importing global variables in function
    board =[' ']*10
    state =True",0.4308089316,
185,chaos game,"b = Board(4)
b.shuffle_board()
print(b.board) # test that shuffle works
print(b) # test that __str__ works
b # test that __repr__ works",0.4211890101,
185,chaos game,"def disp_game_state(xo):
    
    ''' This function will be the one to call when the script wishes to display
    the current state of the game.
    
    Input: a LIST of nine elements, of either nothing (''), an 'O', or an 'X'
    Output: the tic-tac-toe board displayed using the nine elements specified
    '''
    
    print ('     |     |     '
           '\n{a[0]:^5}|{a[1]:^5}|{a[2]:^5}'
           '\n     |     |     '
           '\n{pad:->17}'
           '\n     |     |     '
           '\n{a[3]:^5}|{a[4]:^5}|{a[5]:^5}'
           '\n     |     |     '
           '\n{pad:->17}'
           '\n     |     |     '
           '\n{a[6]:^5}|{a[7]:^5}|{a[8]:^5}'
           '\n     |     |     '
           .format(pad='-',a=xo))",0.4207668304,
185,chaos game,"class Game:
    def __init__(self):
        self.size = (500,500)
        self.running = True
        self.scene = list()
        self.event_handlers = dict()
        self.event_handlers[(('type',pygame.QUIT),)] = self.on_quit
        self.event_handlers[(('type',pygame.KEYDOWN), ('key',pygame.K_q))] = self.on_quit
        self.event_handlers[(('type',pygame.KEYDOWN), ('key',pygame.K_ESCAPE))] = self.on_quit
        self.flipdelay=16
        self.tickcounter=0
        
    def render(self):
        self.disp.fill((0,0,0))
        for obj in self.scene:
            try:
                obj.render(self.disp)
            except Exception:
                traceback.print_exc()
                self.scene.remove(obj)
                print(""Exception during render: Object ""+str(obj)+"" removed from the scene"")
        pygame.display.flip()

    def update(self):
            dt=pygame.time.get_ticks()- self.tickcounter
            for obj in self.scene:
                try:
                    obj.update(dt)
                except Exception:
                    traceback.print_exc()
                    self.scene.remove(obj)
                    print(""Exception during update: Object ""+str(obj)+"" removed from the scene"")
            self.tickcounter=pygame.time.get_ticks()
            pygame.time.delay(self.flipdelay)
        

        
    def on_quit(self, event):
        self.running = False
        
    def process_events(self):
        for event in pygame.event.get():
            dire = dir(event)
            for eh in self.event_handlers.keys():
                callit=True
                for (attrname,attrvalue) in eh:
                    if (not attrname in dire) or (event.__getattribute__(attrname)!=attrvalue):
                        callit=False
                        break
                if callit:
                    self.event_handlers[eh](event)
        
    def mainloop(self):
        pygame.init()
        self.disp=pygame.display.set_mode(self.size, pygame.HWSURFACE | pygame.DOUBLEBUF)
        self.tickcounter=pygame.time.get_ticks()
        while( self.running ):
            try:
                self.render()
                self.process_events()
                self.update()
            except Exception:
                traceback.print_exc()
                pygame.time.delay(10000)
        pygame.quit()",0.4195095301,
185,chaos game,"def play_background():
    """""" Play the background song. """"""
    
    # Choose background music
    
    if random.randint(0,1) == 0:
        audio_file = 'sound-files/Stable_A_16_NZD.wav'
        
    else:
        audio_file = 'sound-files/Stable_B_16_NZD.wav'
    
    # Instantiate player; remove any preexisting audio from queue

    global bplayer
    
    try:
        bplayer
    except NameError: bplayer = None

    if bplayer is None:
        bplayer = pyglet.media.Player()
        
    else:
        bplayer.delete()
    
    # Add new audio file to queue

    bsource = pyglet.media.load(audio_file, streaming=False)
    
    looper = pyglet.media.SourceGroup(bsource.audio_format, None)
    looper.loop = True
    looper.queue(bsource)
    
    bplayer.queue(looper)
    
    bplayer.play()",0.419092834,
2315,testing correlation,"def print_tests(series1, series2): #this function just presents normality and t-tests. 
    import scipy, numpy as np #the function does these tests with arcsin(sqrt(acc)) to help with normality
    normTest2 = scipy.stats.shapiro(np.arcsin(np.sqrt(series1))-np.arcsin(np.sqrt(series2)))
    Test2 = scipy.stats.ttest_rel(np.arcsin(np.sqrt(series1)), np.arcsin(np.sqrt(series2)))
    Test3 = scipy.stats.wilcoxon(np.arcsin(np.sqrt(series1)), y=np.arcsin(np.sqrt(series2)))
    print '\t normality test adj. Test value: %s P-value: %s' % (str(np.round(normTest2[0],2)), 
                                                                 str(np.round(normTest2[1],4)))
    if normTest2[1] > 0.1: print '\t T-test adj. Test value: %s P-value: %s' % (str(np.round(Test2[0],2)), 
                                                                                str(np.round(Test2[1],4)))
    if normTest2[1] <= 0.1: print '\t Wilcoxon. Test value: %s P-value: %s' % (str(np.round(Test3[0],2)),
                                                                               str(np.round(Test3[1],2)))
    
def print_tests_ChiSq(series): #this function just presents normality and t-tests. 
    import scipy, numpy as np #the function does these tests with arcsin(sqrt(acc)) to help with normality
    Test1 = scipy.stats.chisquare(series[1], f_exp = 0.25)
    Test2 = scipy.stats.chisquare(series[2], f_exp = 0.25)
    print '\t Surprise Test. Comparison to Chance: %s P-value: %s' % (str(np.round(Test1[0],4)),
                                                                      str(np.round(Test1[1],4)))
    print '\t After Surprise Test. Comparison to Chance: %s P-value: %s' % (str(np.round(Test2[0],4)),
                                                                            str(np.round(Test2[1],4)))
    x = scipy.stats.chi2_contingency([[sum(series[1]==1),sum(series[2]==1)], [sum(series[1]==0),sum(series[2]==0)]],
                                     correction=False) 
    print '\t Chi-Square Comparison: %s P-value: %s' % (str(np.round(x[0],4)),str(np.round(x[1],4)))
    
def Analysis_and_Plot(tableRT2, CIs): #this function plots the data and writes the results, including stats tests
    PlotFrame = pd.DataFrame([tableRT2.mean()])
    PlotFrame.columns = ['Presurprise', 'Surprise', 'Post surprise']
    PlotFrame2 = pd.DataFrame([CIs])
    PlotFrame2.columns = ['Presurprise', 'Surprise', 'Post surprise']
    PlotFrame.plot(ylim = [0, 1], yerr=PlotFrame2, kind='bar')
    plt.xticks(range(1), ['Trial Type'], rotation=0);

    print '---------------------------------'
    print 'Mean Presurprise: %s' % (str(round(np.mean(tableRT2[0]),2)))
    print 'Mean Surprise: %s' % (str(round(np.mean(tableRT2[1]),2)))
    print 'Mean Postsurprise: %s' % (str(round(np.mean(tableRT2[2]),2)))
    print 'Presurprise - Surprise: %s' % (str(round(np.mean(tableRT2[0])-np.mean(tableRT2[1]),2)))
    print 'Postsurprise - Surprise: %s' % (str(round(np.mean(tableRT2[2])-np.mean(tableRT2[1]),2)))
    print 'Postsurprise - Presurprise: %s' % (str(round(np.mean(tableRT2[0])-np.mean(tableRT2[1]),2)))
    print '---------------------------------'
    print 'Presurprise vs Surprise'
    print_tests(tableRT2[1],tableRT2[0])
    print 'Postsuprise vs Surprise'
    print_tests(tableRT2[2],tableRT2[1])
    print 'Presurprise vs Postsurprise'
    print_tests(tableRT2[0],tableRT2[2])
    
def Analysis_and_Plot_2(tableRT2, CIs): #this function also plots the data and prints results.
    PlotFrame = pd.DataFrame([tableRT2.mean()]) #I could probably consolidate these functions, but whatever. This works.
    PlotFrame.columns = ['Surprise', 'Postsurprise']
    PlotFrame.plot(ylim = [0, 1], kind='bar')#yerr=PlotFrame2, kind='bar')
    plt.xticks(range(1), ['Trial Type'], rotation=0);

    print '---------------------------------'
    print 'Mean Surprise: %s' % (str(round(np.mean(tableRT2[1]),2)))
    print 'Mean Postsurprise: %s' % (str(round(np.mean(tableRT2[2]),2)))
    print 'Postsurprise - Surprise: %s' % (str(round(np.mean(tableRT2[2])-np.mean(tableRT2[1]),2)))
    print '---------------------------------'
    print 'Postsurprise vs Surprise'
    print_tests_ChiSq(tableRT2)
    
def Analysis_and_Plot_3(tableRT2, CIs): #another plotting function...i should really consolidate these.
    PlotFrame = pd.DataFrame([tableRT2.mean()])
    PlotFrame.columns = ['Match', 'Mismatch']
    PlotFrame2 = pd.DataFrame()
    PlotFrame2['Match'] = pd.DataFrame([CIs])
    PlotFrame2['Mismatch'] = pd.DataFrame([CIs])
    PlotFrame.plot(ylim = [0, 1], yerr=PlotFrame2, kind='bar')
    plt.xticks(range(1), ['Trial Type'], rotation=0);

    #disp_tab = np.round(tableRT2,2)
    #disp_tab['Match-Mismatch'] = disp_tab[1] - disp_tab[2]
    #print disp_tab

    print '---------------------------------'
    print 'Mean match: %s' % (str(round(np.mean(tableRT2[1]),4)))
    print 'Mean mismatch: %s' % (str(round(np.mean(tableRT2[2]),4)))
    print 'Match - Mismatch: %s' % (str(round(np.mean(tableRT2[1])-np.mean(tableRT2[2]),4)))
    print '---------------------------------'
    print 'Match vs Mismatch'
    print_tests(tableRT2[1],tableRT2[2])
    
def OneWayConfInterval(table): #Calculates confidence intervals for a one way anova, this is Cousineau, Morey ect
    import scipy.stats, numpy
    ParticipantsMeans = []
    STEs = []
    CIs = []
    for participant in table.index:
        mean = []
        for condition in xrange(numpy.shape(table)[1]): #there's definitely a better way to do this, but this works...
            mean.append(np.array(table[table.index==participant][condition]))
        ParticipantsMeans.append(sum(mean)/len(mean))
    ConfMeans = numpy.zeros(shape=numpy.shape(table))
    for counter, participant in enumerate(table.index):
        for condition in xrange(numpy.shape(table)[1]):
            ConfMeans[counter][condition] = table[table.index==participant][condition]-\
            ParticipantsMeans[counter]+numpy.array(ParticipantsMeans).mean()
    for counter, column in enumerate(ConfMeans.T):
        STEs.append(numpy.std(column, ddof=1)/numpy.sqrt(len(column)))
        CIs.append(STEs[counter]*scipy.stats.t.isf(0.025, len(ConfMeans)-1))
    return CIs

def SimpleComparisonCI(table): #confidence interval for pairwise comparisons - masson & loftus, Baguley (2012)
    import scipy.stats, math
    ttest = scipy.stats.ttest_rel(table[1], table[2])
    MeanDiff_byT = abs((table[1].mean()-table[2].mean())/ttest[0])
    CI = MeanDiff_byT*scipy.stats.t.isf(0.025, len(table)-1)*math.pow(2,0.05)/2
    return CI",0.4756652415,
2315,testing correlation,"# The null hypothesis is that the two distributions are uncorrelated. 

def report_pearson( p, alpha, r):
    
    print('NULL HYPOTHESIS: Two distributions of age for Female and Male are uncorrelated. '
          + 'Significance level p = 0.05.')
    print('Result:')
    if p < alpha:
        print ( 'We can reject the null hypothesis since the pvalue: %.1f is less than the alpha: %.2f, '%(p, alpha) +
               'which means that two distributions are correleated with correlation of %0.3f'%(r)) 
    else:
        print ( 'We can not reject the null hypothesis since the pvalue: %.1f is less than the alpha: %.2f, '%(p, alpha)
               + 'which means that two distribution are uncorrelated.')
                                                                                    

report_pearson(pvalue_p, alpha, r_p)",0.4564655423,
2315,testing correlation,"def compare_ndimensional_distributions(test_data, features, reweighters=None, iterations=30):
    result = """"
    aucs_all = {}
    plt.figure(figsize=(30, 3))
    for label, particle_name in labels_names_correspondence.items():
        subplot(1, 6, label + 1)
        aucs = []
        for i in range(iterations):
            dt_p = DecisionTrainClassifier(n_estimators=1000, depth=6, learning_rate=0.1, n_threads=12)
            dt_p = FoldingClassifier(dt_p, features=features, n_folds=2)

            particle_data = test_data[test_data.Signal == label]
            weights = numpy.ones(len(particle_data)) 
            if reweighters is not None: 
                weights = reweighters[label].predict_weights(particle_data[features])
                weights[particle_data['file_id'].values == 1] = 1.
            
            dt_p.fit(particle_data, particle_data['file_id'], sample_weight=weights)
            report = dt_p.test_on(particle_data, particle_data['file_id'], sample_weight=weights)
            aucs.append(report.compute_metric(RocAuc())['clf'])
        report.learning_curve(RocAuc(), steps=1).plot()
        legend(loc='lower right')
        legend([particle_name])
        result += ""{:10}"".format(particle_name) + ' discrimination : %1.5f +/- %1.5f \n' \
            %(numpy.mean(aucs), numpy.std(aucs))
        aucs_all[particle_name] = (numpy.mean(aucs), numpy.std(aucs))
    print '-----------------------------------'
    print result
    return aucs_all",0.4371408224,
2315,testing correlation,"# don't think we will go over this. 
def is_independent(X, Y, Zs=[], significance_level=0.05):
    return est.test_conditional_independence(X, Y, Zs)[1] >= significance_level",0.4331546426,
2315,testing correlation,"def is_independent(X, Y, Zs=[], significance_level=0.05):
    return est.test_conditional_independence(X, Y, Zs)[1] >= significance_level

print(is_independent('OPI', 'INJ'))
print(is_independent('OPI', 'LBS'))
print(is_independent('LBS', 'INJ'))",0.4326481819,
2315,testing correlation,"def is_independent(X, Y, Zs=[], significance_level=0.05):
    return est.test_conditional_independence(X, Y, Zs)[1] >= significance_level

print(is_independent('B', 'H'))
print(is_independent('B', 'E'))
print(is_independent('B', 'H', ['A']))
print(is_independent('A', 'G'))
print(is_independent('A', 'G', ['H']))",0.4326481819,
2315,testing correlation,"def is_independent(X, Y, Zs=[], significance_level=0.05):
    return est.test_conditional_independence(X, Y, Zs)[1] >= significance_level
print(is_independent('B', 'C'))
print(is_independent('B', 'H'))
print(is_independent('B', 'E'))
print(is_independent('B', 'H', ['A']))
print(is_independent('A', 'G'))
print(is_independent('A', 'G', ['H']))",0.4326481819,
2315,testing correlation,"def getCorr(dataframeParam, xParam, yParam):
        
    calcPears_A = dataframeParam.corr(method = 'pearson')
    print(""Corr Method A:"")    
    print(calcPears_A)
    print()

    print(""Corr Method B:"")
    calcCorr = stats.pearsonr(dataframeParam[xParam], dataframeParam[yParam])
    calcPears_B = round(calcCorr[0],2)
    calcP_B = round(calcCorr[1],2)
    print(""  Pearson: {}"".format(calcPears_B))
    print(""  P: {}"".format(calcP_B))
    print()    
    
#     calcPears_C = dataframeParam[[xParam, yParam]].corr(method='pearson')[yParam][xParam]
#     print(""Corr Method C:"")
#     print(""  {}"".format(calcPears_C))
#     print()
    
#     calcPears_D = np.corrcoef(x = dataframeParam[xParam], y = dataframeParam[yParam])[0][1]
#     print(""Corr Method D:"")  
#     print(""  {}"".format(calcPears_D))
#     print()",0.4278627038,
2315,testing correlation,"def print_ind_ttest(df, column, group_a, group_b, y_col):
    """"""Print results of two-sample ind. t-test between two groups on y_col.""""""
    from scipy.stats import ttest_ind

    series_a = df.loc[df[column]==group_a, y_col]
    series_b = df.loc[df[column]==group_b, y_col]
    # y_col makes sure the rows that are returned only contain the data from the column you want (not the whole row)
    t_test = ttest_ind(series_a, series_b, nan_policy='omit')

    asterisk = "" *"" if t_test.pvalue < 0.05 else """" # an astrisk will indicate significant difference
    out = ""{}{}\n"".format(y_col.upper(), asterisk)
    out += (""p = {:.3f}, T = {:.2f}"".format(t_test.pvalue, t_test.statistic))
    out += (""\n\t{}\t{}\nN\t{}\t{}\nMean\t{:.2f}\t{:.2f}""
            """".format(group_a, group_b, len(series_a), len(series_b),
                      series_a.mean(), series_b.mean()))
for assessment, start, end in assessment_content:
    print(start, end)    print(out)",0.4278084934,
2315,testing correlation,"def calculate_accuracy(clf, t_features, t_labels):
    """"""
    This function takes a trained model as well as the test features and its
    corresponding labels, and reports the accuracy of the model.
    
    @input clf: The trained classifier.
    @type model: sklearn.neural_network.multilayer_perceptron.MLPClassifier
    @input t_features: The features from the test set.
    @type f_features: numpy.ndarray of float
    @input t_labels: The labels of the test set features.
    @type t_labels: numpy.ndarray of int
    
    @return: The accuracy.
    @rtype: float
    """"""
    count = 0
    predictions = clf.predict(t_features)
    t_labels_hot = one_hot(t_labels)
    for i in range(len(t_features)):
        if (type(clf) == SVC):
            if t_labels[i] == predictions[i]:
                count += 1
        else:
            if np.array_equal(t_labels_hot[i], predictions[i]):
                count += 1
    return count / len(t_features)

# Print the Test Accuracy
print(calculate_accuracy(classifier, test_features, test_labels))",0.4268296957,
410,data modeling,"num_pixels = 784
num_classes = 10
def classifier():
    """"""
    Creates an model for MNIST image classification with input dimension 784
    returns: Keras model
    """"""
    
    model = Sequential()
#     model.add(Dense(num_pixels,input_dim=num_pixels,kernel_initializer='normal',activation='relu'))
#     model.add(Dense(int(num_pixels*2),activation='relu'))
#     model.add(Dense(int(num_pixels*2),activation='relu'))
#     model.add(Dense(int(num_pixels*2),activation='relu'))
    
    model.add(Dense(num_classes,activation='softmax'))
    
    

    
    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
    return model


# Create and compile model

model = classifier()
model.fit(x_train,y_train,validation_data=(x_test, y_test), epochs=12, batch_size=200,verbose = 1)",0.4393618703,
410,data modeling,"def P5():
    
### STUDENT START ###
    
    #setting up, fitting, and predicting KNeighbors model
    KNNC = KNeighborsClassifier(n_neighbors=1, weights=""uniform"")
    KNNC.fit(mini_train_data, mini_train_labels)
    pred_labels = KNNC.predict(dev_data)
    
    #confusion matrix print out
    print(confusion_matrix(dev_labels, pred_labels))
    
### STUDENT END ###

P5()",0.4305785298,
410,data modeling,"def bad_MNIST_net():
    '''Build simple neural network ~94% accuracy'''
    net = tflearn.input_data(shape=[None, 784])
    net = tflearn.fully_connected(net, 16, activation='relu', bias=True)
    net = tflearn.fully_connected(net, 10, activation='softmax')
    net = tflearn.regression(net)
    return net",0.4283608794,
410,data modeling,"def create_model():
    # create model
    model = Sequential()
    model.add(Dense(35, input_shape=(X_train.shape[1],), activation='relu'))
    model.add(Dense(35, activation='relu'))
    model.add(Dense(10, activation='softmax'))
    sgd = SGD(lr=0.001, momentum=0.25, decay=0.0)
    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
    print(model.summary())
    return model

#history = LossHistory()
history = History()
filepath = '../data/external/weights_best.hdf5'
model = KerasClassifier(build_fn=create_model, epochs=200, batch_size=1, verbose=2, )#, validation_data=(X_test, y_test))
earlystop = EarlyStopping(monitor='val_acc', min_delta=0.0001, patience=5, \
                          verbose=1, mode='auto')
model.fit(X_train.values, y_train, callbacks = [history, earlystop], validation_data=(X_val.values, y_val))# model__validation_split=0.3)
# plot history
plt.plot(history.history['acc'], label='train')
##plt.plot(history.losses, label='train', alpha=.5)
plt.plot(history.history['val_acc'], label='test')
#plt.plot(history.val_losses, label='test', alpha=.5)
plt.legend()
plt.show()",0.4265657067,
410,data modeling,"def plot_lat_lon():
    global training_data
    data = training_data[(training_data.longitude < 0) & (training_data.latitude > 0)]
    plt.figure(figsize=(7,6))
    for klass in ['low', 'medium', 'high']:
        subdata = data[data.interest_level == klass]
        plt.scatter(subdata['longitude'], subdata['latitude'], alpha=0.4)
    plt.legend(['low', 'medium', 'high'])
    plt.show()
plot_lat_lon()",0.4256329238,
410,data modeling,"model = VGG16(weights='imagenet', include_top=False)
def extractFeatureFromImage(filepath):
    global model
    img = image.load_img(filepath, target_size=(224, 224))
    x = image.img_to_array(img)
    x = np.expand_dims(x, axis=0)
    x = preprocess_input(x)
    features = model.predict(x)
    return features.reshape(1,-1)",0.4241084754,
410,data modeling,"def do_train():
    global model
    model = create_model()
    reader = create_reader(""atis.train.ctf"", is_training=True)
    train(reader, model)
do_train()",0.4239488542,
410,data modeling,"def P5():
### STUDENT START ###
    ### train the classifier 
    knn_1 = KNeighborsClassifier(n_neighbors =1)
    knn_1.fit(mini_train_data, mini_train_labels)
    dev_labels_pred = knn_1.predict(dev_data)
    confmat = confusion_matrix(dev_labels, dev_labels_pred, labels =[0,1,2,3,4,5,6,7,8,9])
    print(confmat)
    
    ### find the most confused pair in the matrix (which has largest value in the non-diagonal path) and store the index 
    
    
    n,n1_max,n2_max=0,0,0
    
    for n1 in np.arange(0,len(confmat[0])):
        for n2 in np.arange(0,len(confmat[1])):
            if((confmat[n1,n2]>n)&(n1!=n2)):
                n=confmat[n1,n2]
                n1_max=n1
                n2_max=n2
    print('The index of the most confused pair in the matrix is: ({:1.0f},{:1.0f}) '.format(n1_max, n2_max))
    print('The classifier think this value is{:2.0f} but in fact, the true label is{:2.0f} and below are a few of the images '.format(n2_max, n1_max))


    ### now get all sample indices of the most confused pairs
    most_confused_items = []
     
    for i in np.arange(0, len(dev_labels_pred)):
        if ((dev_labels_pred[i]==n2_max)&(dev_labels[i]==n1_max)):
            most_confused_items.append(i)
    
    ### now sample and print a few (40%) of them
    sample_size = int(len(most_confused_items)*0.45)
    sampled =np.random.choice(most_confused_items, size =sample_size, replace =False)
    t=0
    for k in sampled:
        t=t+1
        plt.subplot(1, len(sampled),t)
        plt.imshow(dev_data[k].reshape(28,28), cmap='Reds')
        plt.axis('off')




### STUDENT END ###

P5()",0.4229444861,
410,data modeling,"NN=True #GridSearchCV is time consumen when increasing the number of hyperparameters.
if NN==True:
    clf_final=MLPClassifier(learning_rate='adaptive')
    print clf_final
    parameters = {'activation': [""relu""],
              'solver':['lbfgs', 'sgd'],
              'hidden_layer_sizes': [(20,20), (40,40), (10,10), (5,5), 20,30,40,50,60], #Neural net architecture.
              'learning_rate_init' :[0.001], #Learning rate
              'alpha':[0.1, 0.03, 0.01, 0.003, 0.001, 0.0003, 0.0001, 0.00003] # Study regularization, higher regularization with higher alpha
                }
    cv = KFold(n_splits=10)
    grid_obj = GridSearchCV(clf_final, parameters, cv=cv) # Cross validation is important to avoid overfitting and chose the hyperparameters in an unbiased way
    grid_fit = grid_obj.fit(features, training_data.Survived)
    clf_final = grid_fit.best_estimator_ 
    print clf_final

    clf_final.fit(features,training_data.Survived)

    accu = cross_val_score(clf_final, features, training_data.Survived, cv=cv)
    print accu
    print accu.mean()
    print clf_final.score(features, training_data.Survived)",0.4228291512,
410,data modeling,"def testRandomforest():
    etclf = ExtraTreesClassifier(n_estimators=20)
    etclf.fit(x_train, y_train)
    scores = cross_val_score(etclf, x, y)

testRandomforest()",0.4224692285,
619,fictitious names,"def surname(X):
    return (
        X
        .name
        .str.lower()
        .str.extract(r'([a-z]+),', expand=False)
    )

def title(X):
    return (
        X
        .name
        .str.lower()
        .str.extract(r', (\w+)', expand=False)
        .rename('title')
    )

def title_fill(X):
    def rare(row):
        if row.title in ['miss', 'mrs', 'master', 'mr']:
            return row.title
        elif row.title in d:
            return d[row.title]
        elif row.sex == 'male':
            return 'mr'
        elif row.sex == 'female':
            return 'mrs'
        else:
            raise ValueError('row.sex is missing / not in [`male`, `female`]')

    miss = ['ms', 'mlle']
    mrs = ['mme', 'dona', 'lady', 'the']
    mr = [
        'capt',
        'col',
        'don',
        'jonkheer',
        'major',
        'rev',
        'sir',
    ]

    d = {
        **{k: 'mr' for k in mr},
        **{k: 'mrs' for k in mrs},
        **{k: 'miss' for k in miss}
    }

    return (
        X
        .assign(title=title)
        .apply(rare, axis=1)
        .rename('title')
    )",0.3939490914,
619,fictitious names,"def get_key_word(r): 
    for x in key_word:
        if x in r.lemmas:
            return x
    return ''",0.3886944056,
619,fictitious names,"# Now we'll group by year and calculate the fraction of births represented by the top 1000 names in Python
def top1000_fraction(namesAndBirths):
    # namesAndBirths will be an unsorted list of the form [('Mary', 10), ('John', 500), ...]
    sortedNamesAndBirths = sorted(namesAndBirths, key=lambda (name, births): -births)
    
    # Add up all births first
    allBirths = sum([births for (_, births) in sortedNamesAndBirths])
    
    # Add up just the first 1000
    top1000Births = sum([births for (_, births) in sortedNamesAndBirths[:1000]])
    
    return float(top1000Births) / float(allBirths)

top1000FractionRDD = (
    yearNameBirthsRDD
    .groupByKey()
    .mapValues(top1000_fraction)
)",0.3879006505,
619,fictitious names,"class MyName:
    def __init__(self, name, city):
        self.name = name
        self.city = city
    def prt_screen(self):
        return self.name
        
nam1 = MyName('Manavalan', 'Chennai')
nam2 = nam1
nam3 = MyName('Xavier', 'Chennai')
print(nam1.__dict__ == nam2.__dict__ )
print(nam1.__dict__ == nam3.__dict__ )",0.3864252567,
619,fictitious names,"import pprint
from collections import namedtuple
from operator import attrgetter

def main():
    ProgrammingLang = namedtuple('ProgrammingLang', 'name ranking')

    stats = ( ('Ruby', 14), ('Javascript', 8), ('Python', 7),
              ('Scala', 31), ('Swift', 18), ('Lisp', 23) )

    lang_stats = [ProgrammingLang(n, r) for n, r in stats]
    pp = pprint.PrettyPrinter(indent=5)
    pp.pprint(sorted(lang_stats, key=attrgetter('name')))
    print("""")
    pp.pprint(sorted(lang_stats, key=attrgetter('ranking')))

if __name__ == ""__main__"":
    main()",0.3737244904,
619,fictitious names,"from collections import namedtuple
import hashlib 

def filename_to_uniform_number(filepath, MAX_ITEMS_PER_CLASS=2 ** 27 - 1 ):
    """"""Associate the `filepath` to a number [0.0 - 1.0] based on file name.
        
    The numbers are generated based on sha1 of the file name, so should be uniformly distributed 
    and can be used to separate a validation set from training set. It let's you later add more training examples
    and keep the training and validation set separate.

    Parameters
    ----------
          filepath - pathlib.Path object with a path to file name
          MAX_ITEMS_PER_CLASS - helper constant defining a maximum number elements in class
    Returns
    -------
          a number random number from 0.0-1.0 to upper_bound, that depends only on a file name
    """"""
    hash_name = filepath.name.split('_nohash_')[0]
    hash_name_hashed = hashlib.sha1(hash_name.encode(""utf-8"")).hexdigest()
    return ((int(hash_name_hashed, 16) % (MAX_ITEMS_PER_CLASS + 1)) *
           (1 / MAX_ITEMS_PER_CLASS))

def which_set(fn, validation_size=0.10):
    if filename_to_uniform_number(fn) < validation_size:
         return tf.estimator.ModeKeys.EVAL
    return tf.estimator.ModeKeys.TRAIN
# based on: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/speech_commands/input_data.py#L61",0.3729863763,
619,fictitious names,"class FrenchDeck():
    """""" Get a full French deck of cards. """"""
    
    # ['2','3','4','5','6','7','8','9', 10','J','Q','K','A']
    ranks = [str(n) for n in range(2, 11)] + list('JQKA')
    
    # ['spades', 'diamonds', 'clubs', 'hearts']
    suits = 'spades diamonds clubs hearts'.split()
    
    def __init__(self):
        """""" Creates a full deck of cards. """"""
        self._cards = [Card(rank, suit) 
                       for suit in self.suits 
                       for rank in self.ranks]
        
    def __len__(self):
        return len(self._cards)
    
    def __getitem__(self, position):
        return self._cards[position]",0.3727903962,
619,fictitious names,"class Suit:
    def __init__(self, name, symbol):
        self.name = name
        self.symbol = symbol",0.3698710799,
619,fictitious names,"from pyne import nucname
print(""Rb86 decayse to two isotopes,"", ""\n"")
print(""The first daughter is"", nucname.name(360860000), ""\n"")
print(""The second daughter is"", nucname.name(380860000), ""\n"")",0.3694489002,
619,fictitious names,"for num in range(2,7):
        home = 'Home'
        away = 'Away'
        HomeGameForm=""%s%sGameForm"" % (home,num)
        HomeGameGoalsFor=""%s%sGameGoalsFor"" % (home,num)
        HomeGameGoalDiff=""%s%sGameGoalDiff"" % (home,num)
        AwayGameForm=""%s%sGameForm"" % (away,num)
        AwayGameGoalsFor=""%s%sGameGoalsFor"" % (away,num)
        AwayGameGoalDiff=""%s%sGameGoalDiff"" % (away,num)
        GameFormDiff = ""%sGameFormDiff"" % (num)
        GameGoalsForDiff = ""%sGameGoalsForDiff"" % (num)
        GameGoalDiffDiff = ""%sGameGoalDiffDiff"" % (num)
        full_results_combo[GameFormDiff] = full_results_combo[HomeGameForm] - full_results_combo[AwayGameForm]
        full_results_combo[GameGoalsForDiff] = full_results_combo[HomeGameGoalsFor] - full_results_combo[AwayGameGoalsFor]
        full_results_combo[GameGoalDiffDiff] = full_results_combo[HomeGameGoalDiff] - full_results_combo[AwayGameGoalDiff]",0.369125545,
73,arrays and matrices,"kg.tonic, amixy.tonic",0.3908200264,
73,arrays and matrices,"A = sp.MatrixSymbol('A', 3, 3)
L1 = sp.MatrixSymbol('L_1', 3, 3)
L2 = sp.MatrixSymbol('L_2', 3, 3)
l1 = sp.eye(3)
l1[1, 0] = -a[1, 0]/a[0, 0]
l1[2, 0] = -a[2, 0]/a[0, 0]

fmt.displayMath(L1*a, fmt.joinMath('=', x, L1*y), ""\;,\;\;"", l1*a, fmt.joinMath('=', x, l1*y), sep="""", 
                pre=""\\scriptsize "")",0.3828112483,
73,arrays and matrices,"img = np.array(imread('images/peppers256.png', flatten=1))
img = np.array(imread('images/AN148_C3.png', flatten=1))
print(img.shape[0], img.shape[1], img.shape[0]*img.shape[1])",0.3779883981,
73,arrays and matrices,"v = np.matrix(zip(range(5), range(5,10)))
v",0.3755164742,
73,arrays and matrices,"from numpy import matrix
from numpy import linalg
A = matrix([[1,2,3],[11,12,13],[21,22,23]])  # Creates a matrix
x = matrix([[1],[2],[3]])                    # Creates a matrix (like a column vector)
y = matrix([[1,2,3]])                        # Creates a matrix (like a row vector)
print (A.T)                                  # Transpose of A
print (A*x)                                  # Matrix multiplication of A and x
print (A.I)                                  # Inverse of A
print (linalg.solve(A, x))                   # Solve the linear equation system",0.3746456802,
73,arrays and matrices,"from numpy import sin, cos, pi
rotation_matrix=np.matrix([[cos(pi/4),-sin(pi/4)],
                           [sin(pi/4),cos(pi/4)]])
xr=(rotation_matrix*(x.T)).T
xr=np.array(xr)

fig,ax=subplots()
ax.set_aspect(1)
_=ax.axis(xmin=-2,xmax=7,ymin=-4,ymax=4)

_=ax.plot(ma.masked_array(xr[:,1],y==1),ma.masked_array(xr[:,0],y==1),'ow',mec='k',ms=15)
_=ax.plot(ma.masked_array(xr[:,1],y==0),ma.masked_array(xr[:,0],y==0),'o',color='gray',ms=15)

_=clf.fit(xr,y)

for i,j in zip(clf.tree_.feature,clf.tree_.threshold):
    if i==1:
        _=ax.vlines(j,-1,6,lw=3.)
    elif i==0:
        _=ax.hlines(j,-1,6,lw=3.)",0.3737469316,
73,arrays and matrices,from scipy.sparse import lil_matrix,0.3727423549,
73,arrays and matrices,"input_sequence = T.matrix('token sequence','int32')
target_target_letters = T.matrix('target target_letters','int32')",0.3718583286,
73,arrays and matrices,"input_sequence = T.matrix('token sequencea','int32')
target_values = T.matrix('actual next token','int32')",0.3718583286,
73,arrays and matrices,"input_sequence = T.matrix('token sequence', 'int32')
target_target_letters = T.matrix('target target_letters', 'int32')",0.3718583286,
559,example of gini index,"# Simulates the increases and decreases we'd see under the simple
# story.  Produces the net number of increases in 1 simulation.

def simulate_net_increases():
    # First we make a table that gives the chances of each outcome:
    # 1/2 for an increase, and 1/2 for a decrease.
    changes_distribution = Table().with_column(""change"",      [""increase"", ""decrease""])\
                                  .with_column(""probability"", [1/2,        1/2])
    
    # Now we sample 2150 times from that distribution, using the
    # built-in method sample_from_distribution.  This results in
    # a table that tells us how many times (out of the 2150 simulated
    # periods) we saw an increase, and how many times we saw a
    # decrease.
    num_periods = 50*43
    simulated_changes = changes_distribution.sample_from_distribution(""probability"", num_periods)
    
    # You can uncomment the next line (remove the # sign) and run
    # this cell to see what the simulated_changes table looks like.
    #simulated_changes.show()
    
    # We extract the number of increases and the number of decreases
    # and subtract one from the other.
    num_increases_in_simulation = simulated_changes.column(""probability sample"").item(0)
    num_decreases_in_simulation = simulated_changes.column(""probability sample"").item(1)
    return num_increases_in_simulation - num_decreases_in_simulation

# This is a little magic to make sure that you see the same results
# we did, just for pedagogical purposes.
np.random.seed(1234567)

# Simulate once:
one_simulation_result = simulate_net_increases()

print(""In one simulation, there were"", one_simulation_result, ""net increases (versus"", total_net_increases, ""in the real data)."")",0.4378984571,
559,example of gini index,"def get_cols():
    feats = get_importance__features(train, limit=50)
    return [feat for feat in feats[ feats.name > 0 ].index if feat  not in ['is_test']]",0.436070472,
559,example of gini index,"def train_input():
    ds = tf.contrib.data.Dataset.from_tensor_slices(({'x':x_train},y_train))
    ds = ds.repeat().shuffle(100000).batch(16)
    
    iterator = ds.make_one_shot_iterator()
    images, labels = iterator.get_next()
    
    return images, labels
    
    
def test_input():
    ds = tf.contrib.data.Dataset.from_tensor_slices(({'x':x_test},y_test))
    ds = ds.repeat().shuffle(1000).batch(16)
    
    iterator = ds.make_one_shot_iterator()
    images, labels = iterator.get_next()
    
    return images, labels",0.4349039197,
559,example of gini index,"hide_code
# https://github.com/udacity/deep-learning/blob/master/tv-script-generation/problem_unittests.py
def _print_success_message():
    print('Tests Passed')

def test_create_lookup_tables(create_lookup_tables):
    with tf.Graph().as_default():
        test_text = '''
        Moe_Szyslak Moe's Tavern Where the elite meet to drink         
        Bart_Simpson Eh yeah hello is Mike there Last name Rotch        
        Moe_Szyslak Hold on I'll check Mike Rotch Mike Rotch Hey has anybody seen Mike Rotch lately       
        Moe_Szyslak Listen you little puke One of these days 
                    I'm gonna catch you and I'm gonna carve my name on your back with an ice pick                   
        Moe_Szyslak Whats the matter Homer You're not your normal effervescent self        
        Homer_Simpson I got my problems Moe Give me another one       
        Moe_Szyslak Homer hey you should not drink to forget your problems       
        Barney_Gumble Yeah you should only drink to enhance your social skills'''

        test_text = test_text.lower()
        test_text = test_text.split()

        vocab_to_int, int_to_vocab = create_lookup_tables(test_text)

        # Check types
        assert isinstance(vocab_to_int, dict),\
            'vocab_to_int is not a dictionary.'
        assert isinstance(int_to_vocab, dict),\
            'int_to_vocab is not a dictionary.'

        # Compare lengths of dicts
        assert len(vocab_to_int) == len(int_to_vocab),\
            'Length of vocab_to_int and int_to_vocab don\'t match. ' \
            'vocab_to_int is length {}. int_to_vocab is length {}'.format(len(vocab_to_int), len(int_to_vocab))

        # Make sure the dicts have the same words
        vocab_to_int_word_set = set(vocab_to_int.keys())
        int_to_vocab_word_set = set(int_to_vocab.values())

        assert not (vocab_to_int_word_set - int_to_vocab_word_set),\
            'vocab_to_int and int_to_vocab don\'t have the same words.' \
            '{} found in vocab_to_int, but not in int_to_vocab'\
            .format(vocab_to_int_word_set - int_to_vocab_word_set)
        assert not (int_to_vocab_word_set - vocab_to_int_word_set),\
            'vocab_to_int and int_to_vocab don\'t have the same words.' \
            '{} found in int_to_vocab, but not in vocab_to_int'\
            .format(int_to_vocab_word_set - vocab_to_int_word_set)

        # Make sure the dicts have the same word ids
        vocab_to_int_word_id_set = set(vocab_to_int.values())
        int_to_vocab_word_id_set = set(int_to_vocab.keys())

        assert not (vocab_to_int_word_id_set - int_to_vocab_word_id_set),\
            'vocab_to_int and int_to_vocab don\'t contain the same word ids.' \
            '{} found in vocab_to_int, but not in int_to_vocab'\
            .format(vocab_to_int_word_id_set - int_to_vocab_word_id_set)
        assert not (int_to_vocab_word_id_set - vocab_to_int_word_id_set),\
            'vocab_to_int and int_to_vocab don\'t contain the same word ids.' \
            '{} found in int_to_vocab, but not in vocab_to_int'\
            .format(int_to_vocab_word_id_set - vocab_to_int_word_id_set)

        # Make sure the dicts make the same lookup
        missmatches = [(word, id, id, int_to_vocab[id]) \
                       for word, id in vocab_to_int.items() if int_to_vocab[id] != word]

        assert not missmatches,\
            'Found {} missmatche(s). First missmatch: vocab_to_int[{}] = {} and int_to_vocab[{}] = {}'\
            .format(len(missmatches),*missmatches[0])

        assert len(vocab_to_int) > len(set(test_text))/2,\
            'The length of vocab seems too small.  Found a length of {}'.format(len(vocab_to_int))

    _print_success_message()

def test_get_batches(get_batches):
    with tf.Graph().as_default():
        test_batch_size = 128
        test_seq_length = 5
        test_int_text = list(range(1000*test_seq_length))
        batches = get_batches(test_int_text, test_batch_size, test_seq_length)

        # Check type
        assert isinstance(batches, np.ndarray),\
            'Batches is not a Numpy array'

        # Check shape
        assert batches.shape == (7, 2, 128, 5),\
            'Batches returned wrong shape.  Found {}'.format(batches.shape)

        for x in range(batches.shape[2]):
            assert np.array_equal(batches[0,0,x], np.array(range(x * 35, x * 35 + batches.shape[3]))),\
                'Batches returned wrong contents. For example, input sequence {} in the first batch was {}'\
                .format(x, batches[0,0,x])
            assert np.array_equal(batches[0,1,x], np.array(range(x * 35 + 1, x * 35 + 1 + batches.shape[3]))),\
                'Batches returned wrong contents. For example, target sequence {} in the first batch was {}'\
                .format(x, batches[0,1,x])


        last_seq_target = (test_batch_size-1) * 35 + 31
        last_seq = np.array(range(last_seq_target, last_seq_target+ batches.shape[3]))
        last_seq[-1] = batches[0,0,0,0]

        assert np.array_equal(batches[-1,1,-1], last_seq),\
            'The last target of the last batch should be the first input of the first batch. \
            Found {} but expected {}'.format(batches[-1,1,-1], last_seq)

    _print_success_message()

def test_tokenize(token_lookup):
    with tf.Graph().as_default():
        symbols = set(['.', ',', '""', ';', '!', '?', '(', ')', '--', '\n'])
        token_dict = token_lookup()

        # Check type
        assert isinstance(token_dict, dict), \
            'Returned type is {}.'.format(type(token_dict))

        # Check symbols
        missing_symbols = symbols - set(token_dict.keys())
        unknown_symbols = set(token_dict.keys()) - symbols

        assert not missing_symbols, \
            'Missing symbols: {}'.format(missing_symbols)
        assert not unknown_symbols, \
            'Unknown symbols: {}'.format(unknown_symbols)

        # Check values type
        bad_value_type = [type(val) for val in token_dict.values() if not isinstance(val, str)]

        assert not bad_value_type,\
            'Found token as {} type.'.format(bad_value_type[0])

        # Check for spaces
        key_has_spaces = [k for k in token_dict.keys() if ' ' in k]
        val_has_spaces = [val for val in token_dict.values() if ' ' in val]

        assert not key_has_spaces,\
            'The key ""{}"" includes spaces. Remove spaces from keys and values'.format(key_has_spaces[0])
        assert not val_has_spaces,\
            'The value ""{}"" includes spaces. Remove spaces from keys and values'.format(val_has_spaces[0])

        # Check for symbols in values
        symbol_val = ()
        for symbol in symbols:
            for val in token_dict.values():
                if symbol in val:
                    symbol_val = (symbol, val)

        assert not symbol_val,\
            'Don\'t use a symbol that will be replaced in your tokens. Found the symbol {} in value {}'\
            .format(*symbol_val)

    _print_success_message()

def test_get_inputs(get_inputs):
    with tf.Graph().as_default():
        input_data, targets, lr = get_inputs()

        # Check type
        assert input_data.op.type == 'Placeholder',\
            'Input not a Placeholder.'
        assert targets.op.type == 'Placeholder',\
            'Targets not a Placeholder.'
        assert lr.op.type == 'Placeholder',\
            'Learning Rate not a Placeholder.'

        # Check name
        assert input_data.name == 'input:0',\
            'Input has bad name.  Found name {}'.format(input_data.name)

        # Check rank
        input_rank = 0 if input_data.get_shape() == None else len(input_data.get_shape())
        targets_rank = 0 if targets.get_shape() == None else len(targets.get_shape())
        lr_rank = 0 if lr.get_shape() == None else len(lr.get_shape())

        assert input_rank == 2,\
            'Input has wrong rank.  Rank {} found.'.format(input_rank)
        assert targets_rank == 2,\
            'Targets has wrong rank. Rank {} found.'.format(targets_rank)
        assert lr_rank == 0,\
            'Learning Rate has wrong rank. Rank {} found'.format(lr_rank)

    _print_success_message()

def test_get_init_cell(get_init_cell):
    with tf.Graph().as_default():
        test_batch_size_ph = tf.placeholder(tf.int32, [])
        test_rnn_size = 256

        cell, init_state = get_init_cell(test_batch_size_ph, test_rnn_size)

        # Check type
        assert isinstance(cell, tf.contrib.rnn.MultiRNNCell),\
            'Cell is wrong type.  Found {} type'.format(type(cell))

        # Check for name attribute
        assert hasattr(init_state, 'name'),\
            'Initial state doesn\'t have the ""name"" attribute.  Try using `tf.identity` to set the name.'

        # Check name
        assert init_state.name == 'initial_state:0',\
            'Initial state doesn\'t have the correct name. Found the name {}'.format(init_state.name)

    _print_success_message()

def test_get_embed(get_embed):
    with tf.Graph().as_default():
        embed_shape = [50, 5, 256]
        test_input_data = tf.placeholder(tf.int32, embed_shape[:2])
        test_vocab_size = 27
        test_embed_dim = embed_shape[2]

        embed = get_embed(test_input_data, test_vocab_size, test_embed_dim)

        # Check shape
        assert embed.shape == embed_shape,\
            'Wrong shape.  Found shape {}'.format(embed.shape)

    _print_success_message()

def test_build_rnn(build_rnn):
    with tf.Graph().as_default():
        test_rnn_size = 256
        test_rnn_layer_size = 2
        test_cell = rnn.MultiRNNCell([rnn.BasicLSTMCell(test_rnn_size) for _ in range(test_rnn_layer_size)])

        test_inputs = tf.placeholder(tf.float32, [None, None, test_rnn_size])
        outputs, final_state = build_rnn(test_cell, test_inputs)

        # Check name
        assert hasattr(final_state, 'name'),\
            'Final state doesn\'t have the ""name"" attribute.  Try using `tf.identity` to set the name.'
        assert final_state.name == 'final_state:0',\
            'Final state doesn\'t have the correct name. Found the name {}'.format(final_state.name)

        # Check shape
        assert outputs.get_shape().as_list() == [None, None, test_rnn_size],\
            'Outputs has wrong shape.  Found shape {}'.format(outputs.get_shape())
        assert final_state.get_shape().as_list() == [test_rnn_layer_size, 2, None, test_rnn_size],\
            'Final state wrong shape.  Found shape {}'.format(final_state.get_shape())

    _print_success_message()


def test_build_nn(build_nn):
    with tf.Graph().as_default():
        test_input_data_shape = [128, 5]
        test_input_data = tf.placeholder(tf.int32, test_input_data_shape)
        test_rnn_size = 256
        test_embed_dim = 300
        test_rnn_layer_size = 2
        test_vocab_size = 27
        test_cell = rnn.MultiRNNCell([rnn.BasicLSTMCell(test_rnn_size) for _ in range(test_rnn_layer_size)])

        logits, final_state = build_nn(test_cell, test_rnn_size, test_input_data, test_vocab_size, test_embed_dim)

        # Check name
        assert hasattr(final_state, 'name'), \
            'Final state doesn\'t have the ""name"" attribute.  Are you using build_rnn?'
        assert final_state.name == 'final_state:0', \
            'Final state doesn\'t have the correct name. Found the name {}. Are you using build_rnn?'\
            .format(final_state.name)

        # Check Shape
        assert logits.get_shape().as_list() == test_input_data_shape + [test_vocab_size], \
            'Outputs has wrong shape.  Found shape {}'.format(logits.get_shape())
        assert final_state.get_shape().as_list() == [test_rnn_layer_size, 2, None, test_rnn_size], \
            'Final state wrong shape.  Found shape {}'.format(final_state.get_shape())

    _print_success_message()

def test_get_tensors(get_tensors):
    test_graph = tf.Graph()
    with test_graph.as_default():
        test_input = tf.placeholder(tf.int32, name='input')
        test_initial_state = tf.placeholder(tf.int32, name='initial_state')
        test_final_state = tf.placeholder(tf.int32, name='final_state')
        test_probs = tf.placeholder(tf.float32, name='probs')

    input_text, initial_state, final_state, probs = get_tensors(test_graph)

    # Check correct tensor
    assert input_text == test_input,\
        'Test input is wrong tensor'
    assert initial_state == test_initial_state, \
        'Initial state is wrong tensor'
    assert final_state == test_final_state, \
        'Final state is wrong tensor'
    assert probs == test_probs, \
        'Probabilities is wrong tensor'

    _print_success_message()

def test_pick_word(pick_word):
    with tf.Graph().as_default():
        test_probabilities = np.array([0.1, 0.8, 0.05, 0.05])
        test_int_to_vocab = {word_i: word for word_i, word in enumerate(['this', 'is', 'a', 'test'])}

        pred_word = pick_word(test_probabilities, test_int_to_vocab)

        # Check type
        assert isinstance(pred_word, str),\
            'Predicted word is wrong type. Found {} type.'.format(type(pred_word))

        # Check word is from vocab
        assert pred_word in test_int_to_vocab.values(),\
            'Predicted word not found in int_to_vocab.'

    _print_success_message()",0.4322478175,
559,example of gini index,"def load_df_planes(): 
    planes_data = sc.textFile(plane_data_filename)
    header_plane_data = planes_data.first()
    planes_clean_data = planes_data.filter(lambda row: row!=header_plane_data).map(lambda row: deleteNA(row))

    plane_data_schema = StructType([
            StructField(""n_number"", StringType(), True),
            StructField(""serial_number"", StringType(), True),
            StructField(""mfr_mdl_code"", StringType(), True),
            StructField(""eng_mfr_mdl"", StringType(), True),
            StructField(""year_mfr"", IntegerType(), True),
            StructField(""type_registrant"", StringType(), True),
            StructField(""type_aircraft"", IntegerType(), True),
            StructField(""type_engine"", IntegerType(), True),
            StructField(""expiration_date"", StringType(), True)
        ])

    planes_clean_data_to_columns = planes_clean_data.map(lambda l: list(csv.reader([l]))[0])\
        .map(lambda cols: (""N""+cols[0].strip() if cols[0].strip() else None, # n_number
                           cols[1].strip() if cols[1].strip() else None, # serial_number
                           cols[2].strip() if cols[2].strip() else None, # mfr_mdl_code
                           cols[3].strip() if cols[3].strip() else None, #eng_mfr_mdl
                           int(cols[4].strip()) if cols[4].strip() else None, #year_mfr
                           cols[5].strip() if cols[5].strip() else None, #type_registrant
                           int(cols[6].strip()) if cols[6].strip() else None, #type_aircraft
                           int(cols[7].strip()) if cols[7].strip() else None, #type_engine
                           int(cols[8].strip()[0:4]) if cols[8].strip() else None, # expiration_date

            ))

    df_planes = sqlContext.createDataFrame(planes_clean_data_to_columns, schema=plane_data_schema)
    
    return df_planes",0.4308279157,
559,example of gini index,"def load_df_states_codes():
    states_codes_data = sc.textFile(states_codes_filename)
    header_states_codes = states_codes_data.first()
    states_codes_clean_data = states_codes_data.filter(lambda row: row!=header_states_codes)

    states_codes_data_schema = StructType([
            StructField(""state_name"", StringType(), True),
            StructField(""state"", StringType(), True)
        ])

    states_codes_clean_data_to_columns = states_codes_clean_data.map(lambda l: list(csv.reader([l]))[0])\
        .map(lambda cols: ( cols[0].upper() if cols[0] else None,
                            cols[1] if cols[1] else None,
                           ))

    df_states_codes = sqlContext.createDataFrame(states_codes_clean_data_to_columns, schema=states_codes_data_schema)
    return df_states_codes",0.4308279157,
559,example of gini index,"def parseLogs():
    """""" Read and parse log file """"""
    parsed_logs = (sc
                   .textFile(logFile)
                   .map(parseApacheLogLine)
                   .cache())

    access_logs = (parsed_logs
                   .filter(lambda s: s[1] == 1)
                   .map(lambda s: s[0])
                   .cache())

    failed_logs = (parsed_logs
                   .filter(lambda s: s[1] == 0)
                   .map(lambda s: s[0]))
    failed_logs_count = failed_logs.count()
    if failed_logs_count > 0:
        print 'Number of invalid logline: %d' % failed_logs.count()
        for line in failed_logs.take(20):
            print 'Invalid logline: %s' % line

    print 'Read %d lines, successfully parsed %d lines, failed to parse %d lines' % (parsed_logs.count(), access_logs.count(), failed_logs.count())
    return parsed_logs, access_logs, failed_logs


parsed_logs, access_logs, failed_logs = parseLogs()",0.4305730462,
559,example of gini index,"def model_train_and_eval():
    """"""Build the full graph for feeding inputs, training, and
    saving checkpoints.  Run the training. Then, load the saved graph and
    run some predictions.""""""

    # Get input data: get the sets of images and labels for training,
    # validation, and test on MNIST.
    data_sets = read_data_sets(DATA_DIR, False)

    mnist_graph = tf.Graph()
    with mnist_graph.as_default():
        # Generate placeholders for the images and labels.
        images_placeholder = tf.placeholder(tf.float32)
        labels_placeholder = tf.placeholder(tf.int32)
        tf.add_to_collection(""images"", images_placeholder)  # Remember this Op.
        tf.add_to_collection(""labels"", labels_placeholder)  # Remember this Op.

        # Build a Graph that computes predictions from the inference model.
        logits = mnist_inference(images_placeholder,
                                 HIDDEN1_UNITS)
        tf.add_to_collection(""logits"", logits)  # Remember this Op.

        # Add to the Graph the Ops that calculate and apply gradients.
        train_op, loss = mnist_training(
            logits, labels_placeholder, 0.01)

        # prediction accuracy
        _, indices_op = tf.nn.top_k(logits)
        flattened = tf.reshape(indices_op, [-1])
        correct_prediction = tf.cast(
            tf.equal(labels_placeholder, flattened), tf.float32)
        accuracy = tf.reduce_mean(correct_prediction)

        # Define info to be used by the SummaryWriter. This will let
        # TensorBoard plot values during the training process.
        loss_summary = tf.scalar_summary(""loss"", loss)
        train_summary_op = tf.merge_summary([loss_summary])

        # Add the variable initializer Op.
        init = tf.initialize_all_variables()

        # Create a saver for writing training checkpoints.
        saver = tf.train.Saver()

        # Create a summary writer.
        print(""Writing Summaries to %s"" % MODEL_DIR)
        train_summary_writer = tf.train.SummaryWriter(MODEL_DIR)

    # Run training for MAX_STEPS and save checkpoint at the end.
    with tf.Session(graph=mnist_graph) as sess:
        # Run the Op to initialize the variables.
        sess.run(init)

        # Start the training loop.
        print(""Starting training..."")
        for step in xrange(NUM_STEPS):
            # Read a batch of images and labels.
            images_feed, labels_feed = data_sets.train.next_batch(BATCH_SIZE)

            # Run one step of the model.  The return values are the activations
            # from the `train_op` (which is discarded) and the `loss` Op.  To
            # inspect the values of your Ops or variables, you may include them
            # in the list passed to sess.run() and the value tensors will be
            # returned in the tuple from the call.
            _, loss_value, tsummary, acc = sess.run(
                [train_op, loss, train_summary_op, accuracy],
                feed_dict={images_placeholder: images_feed,
                           labels_placeholder: labels_feed})
            if step % 100 == 0:
                # Write summary info
                train_summary_writer.add_summary(tsummary, step)
            if step % 1000 == 0:
                # Print loss/accuracy info
                print('----Step %d: loss = %.4f' % (step, loss_value))
                print(""accuracy: %s"" % acc)

        print(""\nFinished training. Writing checkpoint file."")
        checkpoint_file = os.path.join(MODEL_DIR, 'checkpoint')
        saver.save(sess, checkpoint_file, global_step=step)
        _, loss_value = sess.run(
            [train_op, loss],
            feed_dict={images_placeholder: data_sets.test.images,
                       labels_placeholder: data_sets.test.labels})
        print(""Test set loss: %s"" % loss_value)

    # Run evaluation based on the saved checkpoint.
    with tf.Session(graph=tf.Graph()) as sess:
        checkpoint_file = tf.train.latest_checkpoint(MODEL_DIR)
        print(""\nRunning predictions based on saved checkpoint."")
        print(""checkpoint file: {}"".format(checkpoint_file))
        # Load the saved meta graph and restore variables
        saver = tf.train.import_meta_graph(""{}.meta"".format(checkpoint_file))
        saver.restore(sess, checkpoint_file)

        # Retrieve the Ops we 'remembered'.
        logits = tf.get_collection(""logits"")[0]
        images_placeholder = tf.get_collection(""images"")[0]
        labels_placeholder = tf.get_collection(""labels"")[0]

        # Add an Op that chooses the top k predictions.
        eval_op = tf.nn.top_k(logits)

        # Run evaluation.
        images_feed, labels_feed = data_sets.validation.next_batch(
            EVAL_BATCH_SIZE)
        prediction = sess.run(eval_op,
                              feed_dict={images_placeholder: images_feed,
                                         labels_placeholder: labels_feed})
        for i in range(len(labels_feed)):
            print(""Ground truth: %d\nPrediction: %d"" %
                  (labels_feed[i], prediction.indices[i][0]))",0.4291257858,
559,example of gini index,"def get_data():
    data_files = glob(os.path.join(DATA_PATH, ""*.jpg""))
    
    rdd_train_images = sc.parallelize(data_files[:100000]) \
                              .map(lambda path: get_image(path, image_size).transpose(2, 0, 1))

    rdd_train_sample = rdd_train_images.map(lambda img: Sample.from_ndarray(img, [np.array(0.0), img]))
    return rdd_train_sample",0.4289322495,
559,example of gini index,"def get_data():
    data_files = glob(os.path.join(DATA_PATH, ""*.jpg""))

    rdd_train_images = sc.parallelize(data_files[:100000]) \
        .map(lambda path: inverse_transform(get_image(path, IMAGE_SIZE)).transpose(2, 0, 1))
    rdd_train_sample = rdd_train_images.map(lambda img: Sample.from_ndarray(img, [np.array(0.0), img]))
    return rdd_train_sample",0.4289322495,
788,guided eda,"def generate_image(frame, netG):
    fixed_noise_128 = torch.randn(128, 128)
    if use_cuda:
        fixed_noise_128 = fixed_noise_128.cuda(gpu)
    noisev = autograd.Variable(fixed_noise_128, volatile=True)
    samples = netG(noisev)
    samples = samples.view(-1, 3, 128, 128)
    samples = samples.mul(0.5).add(0.5)
    samples = samples.cpu().data.numpy()

    lib.save_images.save_images(samples, './tmp/celebA/samples_{}.jpg'.format(frame))",0.3602626324,
788,guided eda,"def forward(x):
    yhat = torch.mm(x, w) + b
    return yhat",0.3590899706,
788,guided eda,"%%writefile /lustre/storeB/project/fou/kl/admaeolus/catalog.yaml
plugins:
  source:
    - module: intake_xarray

sources:

  adm_full:
    description: ""ADM aeolus SCA/MCA retrieval, full orbit""
    driver: netcdf
    direct_access: allow
    parameters:
      retrieval:
        description: ""aeolus retrieval""
        type: str
        default: ""mca""
        allowed: [ ""mca"", ""sca"" ]
      date:
        description: ""date ('YYYYMMDD' format)""
        type: str
        default: ""*""
      orbit:
        description: ""absolute orbit ('06d' format)""
        type: str
        default: ""*""
      version:
        description: ""file version ('04d' format)""
        type: str
        default: ""*""
        allowed: [ ""*"", ""0001"", ""0002"" ]
    args:
      concat_dim: point
      urlpath: '{{ CATALOG_DIR}}/data.rev.TD01/netcdf_{{ retrieval }}/AE_TD01_ALD_U_N_2A_{{ date }}T*_{{ orbit }}_{{ version }}.DBL.nc'
      path_as_pattern: 'AE_TD01_ALD_U_N_2A_{date:%Y%m%dT%H%M%S%f}_{time_len:09d}_{orbit:06d}_{version:04d}.DBL.nc'

  adm_cast:
    description: ""ADM aeolus SCA/MCA retrieval over the MACC14 domain""
    driver: netcdf
    direct_access: allow
    parameters:
      retrieval:
        description: ""aeolus retrieval""
        type: str
        default: ""mca""
        allowed: [ ""mca"", ""sca"" ]
      date:
        description: ""date ('YYYYMMDD' format)""
        type: str
        default: ""*""
      orbit:
        description: ""absolute orbit ('06d' format)""
        type: str
        default: ""*""
      version:
        description: ""file version ('04d' format)""
        type: str
        default: ""*""
        allowed: [ ""*"", ""0001"", ""0002"" ]
    args:
      concat_dim: point
      urlpath: '{{ CATALOG_DIR}}/data.rev.TD01/netcdf_emep_domain_{{ retrieval }}/AE_TD01_ALD_U_N_2A_{{ date }}T*_{{ orbit }}_{{ version }}.DBL.nc'
      path_as_pattern: 'AE_TD01_ALD_U_N_2A_{date:%Y%m%dT%H%M%S%f}_{time_len:09d}_{orbit:06d}_{version:04d}.DBL.nc'
        
  cwf_cast:
    description: ""EMEP 12ST forecast colocated""
    driver: netcdf
    direct_access: allow
    parameters:
      date:
        description: ""date ('YYYYMMDD' format)""
        type: str
        default: ""*""
      orbit:
        description: ""absolute orbit ('06d' format)""
        type: str
        default: ""*""
    args:
      concat_dim: point
      urlpath: '{{ CATALOG_DIR}}/EMEPmodel/cast-{{ date }}_{{ orbit }}.nc'
      path_as_pattern: 'cast-{date:%Y%m%d}_{orbit}.nc'

  cwf_full:
    description: ""EMEP 12ST forecast, full MACC14 domain""
    driver: netcdf
    direct_access: allow
    parameters:
      date:
        description: ""date ('YYYYMMDD' format)""
        type: str
        default: ""201812*""
    args:
      concat_dim: time
      urlpath: '{{ CATALOG_DIR}}/EMEPmodel/CWF_12ST-{{ date}}_hourInst.nc'
      path_as_pattern: 'CWF_12ST-{date:%Y%m%d}_hourInst.nc'

  topo:
    description: ""MACC14 domain topography""
    driver: netcdf
    direct_access: allow
    args:
      urlpath: '{{ CATALOG_DIR}}/EMEP.topo/MACC14_topo_ecdis.nc'",0.3561552167,
788,guided eda,"def f(displacement):
    y_pert = np.exp(-((x-displacement)/width)**2)*relative_amplitude
    fit_result = lmfit.minimize(fit_function, params, method='least_squares', kws={'data': y+y_pert, 'x': x})
    return fit_result.params['x_offset']
    

perturbation_displacement = np.linspace(0,4,100)
apparent_displacement = np.array([f(x) for x in perturbation_displacement])",0.3548777401,
788,guided eda,"import emcee

# log likelihood for the model, given the data
def lnprob(pars, obs):
    v = 0.15
    return -np.sum((obs[:,1]-func(obs[:,0],*pars))**2)/v
    
ndim = 2                # parameter space dimensionality
nwalkers=10             # number of walkers

# create the emcee object (set threads>1 for multiprocessing)
data = np.array([xo,yo]).T
sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob, threads=1, args=[data,])

# set the initial location of the walkers
pars = [2.5, 2.5]       # initial guess
p0 = np.array([pars + 1e-3*np.random.randn(ndim) for i in range(nwalkers)])  # add some noise

# set the emcee sampler to start at the initial guess and run 100 burn-in jumps
pos,prob,state=sampler.run_mcmc(p0,100)
sampler.reset()

# run 1000 jumps and save the results 
pos,prob,state=sampler.run_mcmc(pos,1000)
f = open(""chain.dat"", ""w"")
nk,nit,ndim=sampler.chain.shape
for k in range(nk):
    for i in range(nit):
        f.write(""{:d} {:d} "".format(k, i))
        for j in range(ndim):
            f.write(""{:15.7f} "".format(sampler.chain[k,i,j]))
        f.write(""{:15.7f}\n"".format(sampler.lnprobability[k,i]))
f.close()",0.3539187312,
788,guided eda,"def calculate_fitness(individual, minimization):
    phenotype = individual[""phenotype""]
    shifted_sphere_value = sphere(0.5, phenotype)
    individual[""shifted_sphere_value""] = shifted_sphere_value
    
    if minimization:
        fitness = minimization_fitness(shifted_sphere_value)
    
    individual[""fitness""] = fitness",0.3536126614,
788,guided eda,"def match_score(r_a, r_b):
    return (jellyfish.jaro_winkler(r_a[0], r_b[0]) + 
            (1.0 if r_a[1] == r_b[1] else 0) + 
            (1.0 if r_a[2] == r_b[2] else 0))

_=df_m[['nname_x','snum_x', 'nzip', 
        'nname_y', 'snum_y', 'nzip']].apply(
       lambda r: match_score(r[0:3], r[3:]), axis=1).hist(bins=20)
_=df_u[['nname_x','snum_x', 'nzip', 
        'nname_y', 'snum_y', 'nzip']].apply(
       lambda r: match_score(r[0:3], r[3:]), axis=1).hist(bins=20,color='r')
_=plt.xlim([2.5,3.0])",0.3527216911,
788,guided eda,"from audioop import add

def delay(audio_bytes,params,offset_ms):
    """"""version 1: delay after 'offset_ms' milliseconds""""""
    #calculate the number of bytes which corresponds to the offset in milliseconds
    offset= params.sampwidth*offset_ms*int(params.framerate/1000)
    #create some silence
    beginning= b'\0'*offset
    #remove space from the end
    end= audio_bytes[:-offset]
    return add(audio_bytes, beginning+end, params.sampwidth)",0.3479929566,
788,guided eda,"def add_data(net, ppc, pm):
    pm[""gen""][""1""][""bar""] = ""foo""
    pm[""f_hz""] = net.f_hz    

pp.runpm(net, julia_file=""run_powermodels_custom.jl"", pp_to_pm_callback=add_data)
print(net._pm[""gen""][""1""][""bar""])
print(net._pm[""f_hz""])",0.3460451365,
788,guided eda,"def step_forward(T):
    return T + dt / C * ( ASR(alpha) - OLR(T, tau=0.57) )",0.3449372053,
448,default arguments,"class Parameters:
    def __init__(self):
        self.colorspace = 'YUV'  # RGB, HSV, LUV, HLS, YUV, YCrCb
        self.orient = 11
        self.pix_per_cell = 16
        self.cell_per_block = 2
        self.hog_channel = 'ALL'  # 0, 1, 2, 'ALL'
        
        self.ystart = 350
        self.ystop = 656
        self.scale = 2.0
        self.spatial_size = (32,32)
        self.hist_bins = 32",0.4005631208,
448,default arguments,"class Parameters():
    
    def __init__(self):
        '''
        Initialize with a default set of parameters. We will try to keep
        this class method-free for possible later conversion to Julia.
        Perhaps use a dictionary in which the keys are string
        versions of the parameter names. However, we don't take this 
        stricture very seriously. We like Python. Anyway the policy is to
        maintain all of the parameter values here. If they need to be changed
        during a run, fine. Normally do a deepcopy and change the new copy.
        '''
        self.runCount = 0 # This is used in sweep() to identify each parameter setting
        # The number of runs (distinct parameter settings exeuctions, not replications)
        # completed.
        # Should this parameter be stored and held outside of Parameters()? For now, it's in.
        self.numReplications = 10 # The number of replications to be done of each run.
        # Number of ticks, i.e., paired encounters, in a single run:
        self.runLengthInTicks =  1000000 # is the default for real runs. 50000 for testing.
        self.replicationCount = 0 # This is used in replicate() to keep a running tab on
        # the replication count. Counter of replications.
        self.tick = 0 # Each run goes for a number of ticks, set by runLengthInTicks. 
        # We put the outputs in a standard place, with a timestamp for uniqueness:
        self.outputRoot = '../outputs/Run2016-09-12' + str(datetime.datetime.now())
        self.bdlogfile = 'birthanddeathlog.csv'
        self.bdlogging = False
        self.runDiagnosticsFile='rundiagnostics.csv'
        self.recordRunDiagnostics = True
        self.runParametersLog = 'runparameterslog.txt' #formerly runlog.txt
        self.recordRunParameters = True
        
        # These are the default sweep parameters, to be
        # changed as needed for any given sweep.
        self.aggressionResList=[1.0, 0.95, 0.9, 0.85]
        self.metCostFactorsList=[1.0, 0.95, 0.9, 0.85]
        self.resourceHonestAppropriationsList=[1.0, 0.95, 0.9, 0.85]
        self.resourceDisHonestAppropriationsList=[1.0, 0.95, 0.9, 0.85]
        
        # aggressionResources measures the initial endowments of resources
        # (against metabolic costs, etc.) of the wasps, by aggression
        # level, 0 (low aggerssion), 1, and 2 (high aggression).
        self.aggressionResources = {} # dictionary of resources at init for 
        # various levels of aggression.
        self.aggressionResources[0] = 50
        self.aggressionResources[1] = 75
        self.aggressionResources[2] = 100
            
        # Metabolic costs by level of aggression.
        self.metCostFactors = {}
        self.metCostFactors[0] = 0.05
        self.metCostFactors[1] = 0.075
        self.metCostFactors[2] = 0.1
                
        # Proportion of resources taken from defeated honest agents.
        self.resourceHonestAppropriations = {}
        self.resourceHonestAppropriations[0] = 0.25
        self.resourceHonestAppropriations[1] = 0.5
        self.resourceHonestAppropriations[2] = 0.75
        
        # Proportion of resources taken from defeated dishonest agents.
        self.resourceDisHonestAppropriations = {}
        self.resourceDisHonestAppropriations[0] = 0.5
        self.resourceDisHonestAppropriations[1] = 0.75
        self.resourceDisHonestAppropriations[2] = 1.0
        
        # Number of wasps in the total population.
        self.popSize = 3000
        
        self.outputFolder = self.outputRoot + os.sep + 'RunDataMulti'
        self.recordEvery = 1000",0.3977259099,
448,default arguments,"class Arguments:
    def __init__(self):
        self.annDir = 'COCO annotation path'
        self.imageDir = 'COCO image path'
        self.logDir = 'trained model path'

        self.imageSize = 256
        self.cropSize = 224
        self.batchSize = 64
        self.startEpoch = 1
        
args = Arguments()",0.3936864734,
448,default arguments,"#class to create windows for the search
class Windows():
    
    def __init__(self):
        self.windowssmall = []
        self.windowsmed = []
        self.windowslarge = []
        self.windows = []
        
    def slide_window(self, img, x_start_stop=[None, None], y_start_stop=[None, None], 
                    xy_window=(64, 64), xy_overlap=(0.5, 0.5)):
    # If x and/or y start/stop positions not defined, set to image size
        if x_start_stop[0] == None:
            x_start_stop[0] = 0
        if x_start_stop[1] == None:
            x_start_stop[1] = img.shape[1]
        if y_start_stop[0] == None:
            y_start_stop[0] = 0
        if y_start_stop[1] == None:
            y_start_stop[1] = img.shape[0]
        # Compute the span of the region to be searched    
        xspan = x_start_stop[1] - x_start_stop[0]
        yspan = y_start_stop[1] - y_start_stop[0]
    # Compute the number of pixels per step in x/y
        nx_pix_per_step = np.int(xy_window[0]*(1 - xy_overlap[0]))
        ny_pix_per_step = np.int(xy_window[1]*(1 - xy_overlap[1]))
        # Compute the number of windows in x/y
        nx_buffer = np.int(xy_window[0]*(xy_overlap[0]))
        ny_buffer = np.int(xy_window[1]*(xy_overlap[1]))
        nx_windows = np.int((xspan-nx_buffer)/nx_pix_per_step) 
        ny_windows = np.int((yspan-ny_buffer)/ny_pix_per_step) 
        # Initialize a list to append window positions to
        window_list = []
        # Loop through finding x and y window positions
        # Note: you could vectorize this step, but in practice
        # you'll be considering windows one by one with your
        # classifier, so looping makes sense
        for ys in range(ny_windows):
            for xs in range(nx_windows):
                # Calculate window position
                startx = xs*nx_pix_per_step + x_start_stop[0]
                endx = startx + xy_window[0]
                starty = ys*ny_pix_per_step + y_start_stop[0]
                endy = starty + xy_window[1]
                # Append window position to list
                window_list.append(((startx, starty), (endx, endy)))
        # Return the list of windows
        return window_list

    def createWindows(self, image):
    
        self.windowssmall = slide_window(image, x_start_stop=[None, None], y_start_stop=[400, 500], 
                        xy_window=(72, 72), xy_overlap=(.8, .8))

        self.windowsmed = slide_window(image, x_start_stop=[None, None], y_start_stop=[400, 600], 
                        xy_window=(128, 96), xy_overlap=(.75, .75))

        self.windowslarge = slide_window(image, x_start_stop=[None, None], y_start_stop=[400, 600], 
                        xy_window=(160, 128), xy_overlap=(.65, .65))

        return  self.windowssmall + self.windowsmed + self.windowslarge",0.3875136077,
448,default arguments,"class Args():
    def __init__(self):
        '''directory to store checkpointed models'''
        self.save_dir = 'save_face_training_v0.3'
        
        '''Picture size'''
        self.picture_size = 32
    
        '''size of RNN hidden state'''
        self.rnn_size = self.picture_size*3 
        '''minibatch size'''
        self.batch_size = 1
        '''RNN sequence length'''
        self.seq_length = self.picture_size
        '''number of epochs'''
        self.num_epochs = 10 # was 5
        '''save frequency'''
        self.save_every = 100 # was 500
        '''Print frequency'''
        self.print_every = 10
        '''clip gradients at this value'''
        self.grad_clip = 5.
        '''learning rate'''
        self.learning_rate = 0.002 # was 0.002
        '''decay rate for rmsprop'''
        self.decay_rate = 0.98
        """"""continue training from saved model at this path.
        Path must contain files saved by previous training process: """"""
        #self.init_from = 'save_face_training'
        self.init_from = None
        
        '''number of ligne to sample'''
        self.n = 250",0.386700362,
448,default arguments,"class Adsorption:
    """"""Describe a cadmium/ferrihydrite adsorption system.
    """"""
    
    def __init__(self, Nsite=1):
        self.nsite=Nsite        # number of adsorption sites
        datain=np.loadtxt('CadmiumFerriHydrite.txt',skiprows=1)   # read from file
        self.data_aqu_log=datain[:,0]
        self.data_ads_log=datain[:,1]
        self.kin=10000*np.ones((self.nsite))   # Kinetic constants
        self.sto=0.0005*np.ones((self.nsite))   # Conc of adsorption sites
        self.b_weight = 0.01               # weight for the background term of the cost function.
        
    def compute_ads(self, aqu_in, kin_in, sto_in):
        """"""Compute adsorbed concentrations at each site from other aqu_in, kin_in, sto_in .""""""
        dimaqu = np.shape(aqu_in)[0]
        ads_out = np.empty((dimaqu, self.nsite))
        for iv in range(dimaqu):
            for isi in range(self.nsite):
                num = sto_in[isi] * kin_in[isi] * aqu_in[iv]
                denom = 1. + kin_in[isi] * aqu_in[iv]
                ads_out[iv,isi] = num/denom
        return ads_out
                      
    def plot_ads(self, sites=None):
        """"""Plot -log(ads) vs -log10(aqu) based on the model parameters (class attribute).
        sites: set different from None to plot Langmuir isotherms for all adsorption sites.""""""
        xxaqu_log = np.arange(-5.5,-1.,0.1)
        xxaqu = 10**xxaqu_log
        xxads = self.compute_ads(xxaqu, self.kin, self.sto)
        xxads_tot = np.sum(xxads, axis=1)
        if sites != None:
            for isi in range(self.nsite):
                plt.plot(np.log10(xxaqu),np.log10(xxads[:,isi]), 'b--')
        plt.plot(np.log10(xxaqu),np.log10(xxads_tot[:]))
        plt.plot(self.data_aqu_log, self.data_ads_log, 'ro')
        
    def CostFunction(self, params):
        """"""Compute cost function to minimize and find kinetic constants and site concentrations. """"""
        cost=0.
        ## Jb
        params_ref = np.log10( np.hstack((self.kin, self.sto)) )
        incr = params - params_ref
        costb = 0.5 * self.b_weight * np.dot(incr, incr)
        ## Jo
        kin = 10**params[:self.nsite]
        sto = 10**params[self.nsite:]
        data_aqu = 10**self.data_aqu_log
        xxads = self.compute_ads(data_aqu, kin, sto)
        xxads_tot = np.sum(xxads, axis=1)
        innov = self.data_ads_log - np.log10(xxads_tot)
        costo = 0.5 * np.dot(innov, innov)
        cost = costb + costo
        return cost
        
    def optim(self):
        """"""Compute optimal parameters by minimizing CostFunction. Return the final cost function value. """"""
        params = np.log10( np.hstack((self.kin, self.sto)) )
        result = so.minimize(self.CostFunction, params, method = ""Powell"")
        params_out = 10**result['x']
        self.kin = params_out[:self.nsite]
        self.sto = params_out[self.nsite:]
        return result['fun']",0.3853009641,
448,default arguments,"def update_descriptions():
    global predictions
    p0_button.description = predictions[0]
    p1_button.description = predictions[1]
    p2_button.description = predictions[2]
    p3_button.description = predictions[3]
    p4_button.description = predictions[4]
    p5_button.description = predictions[5]
    p6_button.description = predictions[6]
    p7_button.description = predictions[7]",0.3847108483,
448,default arguments,"def fib_func():
    if 'a' not in fib_func.__dict__:
        fib_func.a = 0
    if 'b' not in fib_func.__dict__:
        fib_func.b = 1
    temp =fib_func.a
    fib_func.a, fib_func.b = fib_func.b, fib_func.a + fib_func.b
    return temp

# print n fibbonacci numbers
n=6
for i in range(n):
    print(fib_func())
    
# This is same as generater or class technique just a different way of implementation",0.3834097683,
448,default arguments,"class BlackJack(object):
    def __init__(self):
        self.upcard = None

        self.dealer = []
        self.player = []

        self.sum_of_dealer = 0
        self.sum_of_player = 0

        self.having_ace_flag_of_dealer = False
        self.having_ace_flag_of_player = False

        self.usable_ace_flag_of_dealer = False
        self.usable_ace_flag_of_player = False

        self.change_flag = False

        self.HIT  = 0
        self.STAY = 1
        self.action = None

        self.DEALER = -1
        self.PLAYER = 1
        self.DRAW   = 0
        self.winner = 0

        self.state_info = []
        self.end_flag = False

    def drow(self):
        card = randint(1,14)
        return card if card < 10 else 10

    def set(self):
        upcard = self.drow()
        self.upcard = upcard
        self.dealer.append(upcard)
        self.dealer.append(self.drow())
        self.player.append(self.drow())
        self.player.append(self.drow())

        self.sum_of_dealer = np.sum(self.dealer)
        self.sum_of_player = np.sum(self.player)
        self._check_have_one()

        self._judge_usable_ace(who=""dealer"")
        self._judge_usable_ace(who=""player"")

        if self.having_ace_flag_of_dealer: self.sum_of_dealer += 10
        if self.having_ace_flag_of_player: self.sum_of_player += 10


    def hit(self, who=""player""):
        if who == ""player"":
            self.player.append(self.drow())
            self.sum_of_player = np.sum(self.player)
            self._check_have_one()
            self._judge_usable_ace(who=""player"")
            if self.usable_ace_flag_of_player:
                self.sum_of_player += 10
            if self.change_flag:
                self.change_flag = False
                self.sum_of_player -= 10

        elif who == ""dealer"":
            self.dealer.append(self.drow())
            self.sum_of_dealer = np.sum(self.dealer)
            self._check_have_one()
            self._judge_usable_ace(who=""dealer"")
            if self.usable_ace_flag_of_dealer:
                self.sum_of_dealer += 10
            if self.change_flag:
                self.change_flag = False
                self.sum_of_dealer -= 10

    def update(self):
        upcard = copy(self.upcard)
        sum_of_player = copy(self.sum_of_player)
        usable = copy(self.usable_ace_flag_of_player)
        action = copy(self.action)
        self.state_info.append([upcard, sum_of_player, usable, action])

    def _dealer_drowing(self):
        while True:
            if self.sum_of_dealer > self.sum_of_player:
                self.winner = self.DEALER
                self.end_flag = True
                break
            elif self.sum_of_dealer > 16:
                if self.sum_of_dealer > self.sum_of_player:
                    self.winner = self.DEALER
                    self.end_flag = True
                    break
                elif self.sum_of_dealer == self.sum_of_player:
                    self.winner = self.DRAW
                    self.end_flag = True
                    break
                else:
                    self.winner = self.PLAYER
                    self.end_flag = True
                    break
            else:
                self.hit(who=""dealer"")
                self._judge_bust(who=""dealer"")
                if self.end_flag: break

    def _check_have_one(self):
        self.having_ace_flag_of_dealer = True \
        if 1 in self.dealer else False
        self.having_ace_flag_of_player = True \
        if 1 in self.player else False

    def _judge_usable_ace(self, who):
        #check whether the player can use the ace as ten.
        if who == ""dealer"":
            if self.usable_ace_flag_of_dealer:
                if (1 in self.dealer) and self.sum_of_dealer < 11:
                    self.usable_ace_flag_of_dealer = True
                else:
                    self.usable_ace_flag_of_dealer = False
                    self.change_flag = True
            else:
                self.usable_ace_flag_of_dealer = True \
                if (1 in self.dealer) and self.sum_of_dealer < 11 else False

        if who == ""player"":
            if self.usable_ace_flag_of_player:
                if (1 in self.player) and self.sum_of_player < 11:
                    self.usable_ace_flag_of_player = True
                else:
                    self.usable_ace_flag_of_player = False
                    self.change_flag = True
            else:
                self.usable_ace_flag_of_player = True \
                if (1 in self.player) and self.sum_of_player < 12 else False

    def _judge_bust(self, who=""player""):
        if who == ""player"":
            if self.usable_ace_flag_of_player is False \
            and self.sum_of_player > 21:
                self.winner = self.DEALER
                self.end_flag = True
        elif who == ""dealer"":
            if self.usable_ace_flag_of_dealer is False \
            and self.sum_of_dealer > 21:
                self.winner = self.PLAYER
                self.end_flag = True

    def play_with_policy1(self):
        '''policy1
        stay :  sum = 20, 21
        hit  :  others
        '''
        while not self.end_flag:
            if self.sum_of_player < 20:
                self.action = self.HIT
                self.update()
                self.hit(who=""player"")
                self._judge_bust()
            else:
                self.action = self.STAY
                self.update()
                self._dealer_drowing()

    def get_info(self):
        upcard = self.upcard
        print(""upcard:"", upcard)
        print(""Sum of player cards:"", self.sum_of_player)
        print(""Sum of dealer cards:"", self.sum_of_dealer)
        print(""Cards of player:"", self.player)
        print(""Cards of dealer:"", self.dealer)
        print(""Winner:"", self.winner)
        print(""Have usable ace?:"", self.usable_ace_flag_of_player)
        print(""S_t-1 info:"", self.state_info)

    def test(self):
        self.set()
        self.play_with_policy1()
        self.get_info()",0.3825802803,
448,default arguments,"class Args():
    def __init__(self):
        '''directory to store checkpointed models'''
        self.save_dir = 'save_face_training_0.5_128'
        
        '''Picture size'''
        self.picture_size = 64
    
        self.vocab_size = 16
    
        '''size of RNN hidden state'''
        self.rnn_size = 300

        '''RNN sequence length'''
        self.seq_length = self.picture_size*4
        '''number of epochs'''
        self.num_epochs = 5
        '''save frequency'''
        self.save_every = 50 # was 500
        '''Print frequency'''
        self.print_every = 10
        '''clip gradients at this value'''
        self.grad_clip = 5.
        '''learning rate'''
        self.learning_rate = 0.002 # was 0.002
        '''decay rate for rmsprop'''
        self.decay_rate = 0.98
        """"""continue training from saved model at this path.
        Path must contain files saved by previous training process: """"""
        self.init_from = 'save_face_training_0.5_128'
        #self.init_from = None",0.3822440207,
2309,test your model point,"def play_one(env, pmodel, gamma):
    observation = env.reset()
    done = False
    totalreward = 0
    iters = 0

    while not done and iters < 2000:
        # if we reach 2000, just quit, don't want this going forever
        # the 200 limit seems a bit early
        action = pmodel.sample_action(observation)
        # oddly, the mountain car environment requires the action to be in
        # an object where the actual action is stored in object[0]
        observation, reward, done, info = env.step([action])

        totalreward += reward
        iters += 1
    return totalreward


def play_multiple_episodes(env, T, pmodel, gamma, print_iters=False, status=False):
    totalrewards = np.empty(T)
    r = range(T)
    if status:
        r = tqdm_notebook(range(T), desc='Episodes'):
    for i in range(T):
        totalrewards[i] = play_one(env, pmodel, gamma)
        if print_iters:
            print(i, ""avg so far:"", totalrewards[:(i+1)].mean())

    avg_totalrewards = totalrewards.mean()
    print(""avg totalrewards:"", avg_totalrewards)
    return avg_totalrewards",0.4663324952,
2309,test your model point,"#### 2194

# Module: control_guided.pde
# static void guided_posvel_control_run()
# {
    ...
    ...
    # // calculate dt
    # float dt = pos_control.time_since_last_xy_update();

    # // update at poscontrol update rate
    # if (dt >= pos_control.get_dt_xy()) {    
    #     ....    
    #     pos_control.update_z_controller(); <== BUG
    # }

# pos_control.update_z_controller(); <== FIX
# }",0.4645743966,
2309,test your model point,"def simulator(vi, track): #vi is initial velocity, track is a dictionary of form {1:['s', lenght], 2:['c', length, raduis]}

    """"""
    simulates the track calling the three main functions(straight line, corner, brake) appropriately
    
    """"""
    i = 0 
    xx = {} #will handle data during solution

    a = straight_line(vi, track[0][1]) #first part of the track is known to be straight line so the straight road function is called
    
    xx[i] = a #a contains t, v, list_t, list_v, list_s of the function called (the solution of the road is added to dic xx)
    i += 1
    s_simulation = [] #distance travelled during the whole simulation
    v_simulation = [] #corresponding velocity
    t_simulation = [] #corresponding time
    
    while i < len(track): #will end itsedlf when track end reached
        if track[i][0] == 'c': # 'c' for corner, so call corner function
            a = corner(xx[i-1][1], track[i][1], track[i][2])
            if len(a) == 1: #it means the function returned on the max velociy of the corner as the car velocity at the entrance was larger than the corner max velocity so it will call the brake function as explained in the corner function
                corrective = brake(xx[i-1][3], xx[i-1][2], xx[i-1][4], a[0]) #solve for new solution(list_v, list_t, list_s, v_max)
                del(xx[i-1]) #deletes old rejected solution of the straight road
                a = corner(a[0], track[i][1], track[i][2]) #call the corner function again but with the accepted entrance velocity; functions arguments (corner(vi, length, raduis))
                xx[i-1] = corrective #new straight road solution
                xx[i] = a #new corner solution
                i += 1
            else:
                xx[i] = a
                i += 1
                

        else:
            a = straight_line(xx[i-1][1], track[i][1]) #call straigh road function; arguments taken (straight_line(vi, length))
            xx[i] = a #add straight road solution to dic xx
            print (round(i/len(track) *100,1), ""% of the simulation done"") #shows the progress of the solution
            i += 1            
    
    i = 0
    lateral_acc = []
    while i < len(xx):

        if xx[i][-1] == 's':
            lateral_acc += [0]*len(xx[i][3])
        elif xx[i][-1] == 'c':
            for e in xx[i][3]:
                lateral_acc += [(e**2/(xx[i][5]*9.81))]
                
        if i == 0:
            s_simulation += xx[i][4] #as s_list is the 5th element of the dic xx and are added up to form the list-distance of the whole simulation
            v_simulation += xx[i][3] #as v_list is the 4th element of the dic xx and are added up to form the list-velocity of the whole simulation
            t_simulation += xx[i][2] #as t_list is the 3rd element of the dic xx and are added up to form the list-time of the whole simulation
            i += 1
        else:            
            s_simulation_temp = [x+s_simulation[-1] for x in xx[i][4]] #to accumelate distance it adds the last element of the prevoius list to all the elements of the current one
            s_simulation += s_simulation_temp #concatenate the lists (the distances)
            v_simulation += xx[i][3]
            t_simulation_temp = [x+t_simulation[-1] for x in xx[i][2]] # to accumelate time
            t_simulation += t_simulation_temp
            i += 1
    
    print('Simulation completed')
    print('lap time', round(t_simulation[-1],3), 'sec')
    #print('velocity at end of the track', round(v_simulation[-1],2), 'm/s')   
    return [s_simulation, v_simulation,round(t_simulation[-1],3), lateral_acc] #list of the whole track for distance and corresponding velocity and lateral acceleration, also returns time of the lap",0.4632540941,
2309,test your model point,"def make_print_confusion_matrix(clf, clf_name):
    x_train, x_test, y_train, y_test = train_test_split(aggr_rf_input_data, player_colors_2, test_size=0.25)
    clf.fit(x_train, y_train)
    prediction = clf.predict(x_test)
    accuracy = np.mean(cross_val_score(clf, aggr_rf_input_data, player_colors_2, cv=5, n_jobs=3, pre_dispatch='n_jobs+1', verbose=1))
    print(clf_name + ' Accuracy: ',accuracy)
    cm = confusion_matrix(y_test, prediction)
    class_names = ['WWW', 'BBB']
    plot_confusion_matrix(cm, classes=class_names, title='Confusion matrix of '+clf_name)
    plt.show()",0.4571566582,
2309,test your model point,"def fit_predict_model():

    # Get the features and labels from the Boston housing data
    
    X, y = boston.data, boston.target

    # Setup a Decision Tree Regressor
    
    regressor = DecisionTreeRegressor()
    
    parameters = {'max_depth':(1,2,3,4,5,6,7,8,9,10)}
    
    mse_scoring = make_scorer(mean_squared_error, greater_is_better=False)
    
    #using grid search to fine tune the Decision Tree Regressor and
    #obtain the parameters that generate the best training performance. 

    reg = GridSearchCV(regressor, parameters, scoring = mse_scoring)
    reg.fit(X,y)
    
    # Fit the learner to the training data to obtain the best parameter set
    print ""Final Model: ""
    print (reg.fit(X, y))    

    # Using the model to predict the output of a particular sample
    x = [11.95, 0.00, 18.100, 0, 0.6590, 5.6090, 90.00, 1.385, 24, 680.0, 20.20, 332.09, 12.13]
    x = np.array(x)
    x = x.reshape(1, -1)
    y = reg.predict(x)

    #Predict Housing Price:
    print ""\nHouse: "" + str(x)
    print ""\nPredicted: "" + str(y)
    print ""\nBest Score %s:"" % (reg.best_score_)
    
    #DataFrame of Client_Features
    #x = [11.95, 0.00, 18.100, 0, 0.6590, 5.6090, 90.00, 1.385, 24, 680.0, 20.20, 332.09, 12.13]
    #pd.DataFrame(zip(boston.feature_names, x), columns = ['Features', 'Client_Features'])",0.4551064372,
2309,test your model point,"for i in range(0,21):
  
    # fit the dqn algo for 100,000 steps of gameplay
    dqn.fit(env, nb_steps=100000,verbose=0)

    # save the weights
    dqn.save_weights(""TESTpoint6eps2mil_weights_""+str(i*100000)+""_.h5f"", overwrite=True)

    # printing progress
    print(""Completed Steps: ""+str(i*100000)+""/2000000"")

    # running one test to see how effective it seems
    dqn.test(env, nb_episodes=1, visualize=False)

    try: # handling errors in file download from Colaboratory, files can be downloaded to local computer at a later time
    # download to local for testing
    files.download(""TESTpoint6eps2mil_weights_""+str(i*100000)+""_.h5f"")
    except:
    pass",0.4516341686,
2309,test your model point,"p=0
def randomtestprobability():
    global p,a,predict,features
    a = random.choice(test)
    test1=sorted(glob.glob(a))

    test2=empty(len(features)+1)
    test2[0]=1
    test2[1:]=dothew(test1)
    predict=''
    if sign(dot(W09,test2))==1:
        predict='09'
    else:
            if sign(dot(W1,test2))==1:
                predict='_1'
            else:
                    if sign(dot(W8,test2))==1:
                        predict='_8'
                    else:
                            if sign(dot(Wm,test2))==1:
                                predict='_m'
                            else:
                                    if sign(dot(W0,test2))==1:
                                        predict='_0'
                                    else:
                                            if sign(dot(W2,test2))==1:
                                                predict='_2'
                                            else:
                                                    if sign(dot(W7,test2))==1:
                                                        predict='_7'
                                                    else:
                                                            if sign(dot(Wr,test2))==1:
                                                                predict='_r'
                                                            else:
                                                                    if sign(dot(W9,test2))==1:
                                                                        predict='_9'
                                                                    else:
                                                                            predict='_0'


    if predict == a[-6:-4]:
        p+=1
def printit():
    global a,predict
    print(""This is a new trial, one of the source we randomly choose is"",a,""\n"", )

    if predict == a[-6:-4]:
        print(""So the orignal class is"",a[-6:-4], ""\nThe predict class is"",predict,""\nCONG!"")
    else:
        print(""So the orignal class is"",a[-6:-4], ""\nThe predict class is"",predict,""\nWrong Classification!"")",0.4513630271,
2309,test your model point,"def full_eval(network, baseline_mean, baseline_std):
    # Check performance at the point in training with the best dev performance
    network.restore(network.best_dev_tup)
    print 'At best Dev performance...'
    print '\nTrain accuracy (appoximate): '
    network.check_accuracy(data_dict['train'], 2000);
    print '\nDev accuracy: '
    network.check_accuracy(data_dict['dev']);
    print '\nTest accuracy: '
    network.check_accuracy(data_dict['test']);
    
    # Check the performance at the termination of training
    network.restore(network.end_of_train_tup)
    print '\n\nAt end of training...'
    print '\nTrain accuracy (appoximate): '
    network.check_accuracy(data_dict['train'], 2000);
    print '\nDev accuracy: '
    network.check_accuracy(data_dict['dev']);
    print '\nTest accuracy: '
    a = 100.*network.check_accuracy(data_dict['test']);

    # Compare to the goal
    print '\n\nProbability this score beats Sheng Tai baseline:'
    print '{}%'.format(
        100.*np.round(1000.*np.mean(np.random.normal(baseline_mean, baseline_std, 10000) < a))/1000.)",0.4465625286,
2309,test your model point,"###############################
# load model                  #
###############################
def get_feature_extractor(prefix, epoch):
    
    sym, arg_params, aux_params = mx.model.load_checkpoint(prefix, epoch)

    all_layers = sym.get_internals()
    flatten_output_sym = all_layers['flatten0_output']
    mod_output = mx.mod.Module(symbol=flatten_output_sym, label_names=None, context=mx.cpu())
    mod_output.bind(for_training=False, data_shapes=[('data', (1,3,224,224))])
    mod_output.set_params(arg_params, aux_params)
    
    feature_extractor = mod_output
    return feature_extractor",0.4450274408,
2309,test your model point,"def predict(model, arduinoSerial, frame, check_predictions_only = False):
    # The current frame of gesture
    gesture_code = ' '
    gesture_label = ' '
    labels = ['nothing', 'left', 'right', 'grip', 'loose', 'foward', 'back', 'up', 'down']
    rlabels = ['n', 'l', 'r', 'g', 'o', 'f', 'b', 'u', 'd']
    try:
        frame = utils.preprocess(frame) # apply the preprocessing
        frame = np.array([frame])       # the model expects 4D array
        # predict the gesture
        #gesture = float(model.predict(frame, batch_size=1))
        gesture = np.argmax(model.predict(frame, batch_size=1))
        #if(gesture <= 0.8):
        #    gesture_code = 'n';
        #elif(gesture <= 1.8):
        #    gesture_code = 'l';
        #elif(gesture <= 2.8):
        #    gesture_code = 'r';
        #elif(gesture <= 3.8):
        #    gesture_code = 'g';
        #elif(gesture <= 4.8):
        #    gesture_code = 'o';
        #elif(gesture <= 5.8):
        #    gesture_code = 'f';
        #elif(gesture <= 6.8):
        #    gesture_code = 'b';
        #elif(gesture <= 7.8):
        #    gesture_code = 'u';
        #elif(gesture <= 8.8):
        #    gesture_code = 'd';
        #if(gesture != ' '):
        if(0>= gesture <=8)
            gesture_code = rlables[gesture]
            gesture_label = lables[gesture]
            if not check_predictions_only:
                arduinoSerial.write(bytes(gesture_code, 'utf-8'))
                time.sleep(.02)
    except Exception as e:
        print(e)
    return gesture_code",0.4431405962,
1919,show the top actors/actresses that have starred in the most movies released in the usa in the christmas season since and including the year,"# get movie, genre pairs from TMDb
latest_id = tmdb.Movies().latest()['id']

def worker(i):
    try:
        movie = tmdb.Movies(i).info()
    except:
        movie = """"
    return [i, movie]

ids = list(range(1,latest_id))
random.shuffle(ids)
ids = ids[1:25000] #take random sample of 25,000 IDs 

p = multiprocessing.Pool()
ans = p.map(worker,ids)

global movie_dictionary 
movie_dictionary = {}
for i in ans:
    if i[1]: #only add the non-deleted movies to movie dictionary
        movie_dictionary[i[0]] = i[1]",0.5291318297,
1919,show the top actors/actresses that have starred in the most movies released in the usa in the christmas season since and including the year,"def get_user_preferences(user_movies, verbose=False):
    user_features = movies_df[movies_df.movie_title.isin(user_movies)].ix[:,3:].T
    user_features = user_features.mean(axis=1).reset_index()
    if verbose:
        print(user_features)
    return user_features.ix[:,1].tolist()

user_preferences_list = get_user_preferences(user_movies, verbose=True)",0.5094230175,
1919,show the top actors/actresses that have starred in the most movies released in the usa in the christmas season since and including the year,"#Functions that take in a movie and return ID, Genre, info and Posters
def get_poster(movie):
    response = search.movie(query=movie)
    id=response['results'][0]['id']
    movie = tmdb.Movies(id)
    posterpath=movie.info()['poster_path']
    title=movie.info()['original_title']
    url='http://image.tmdb.org/t/p/original'+posterpath
    title='_'.join(title.split(' '))
    r = requests.get(url)
    completeName = os.path.join(poster_folder, title) 
    with open(completeName,'wb') as w:
        w.write(r.content)

def getmovie_id(movie):
    resp= search.movie(query=movie)
    movie_id=resp['results'][0]['id']
    return movie_id

def getmovie_info(movie):
    resp= search.movie(query=movie)
    id=resp['results'][0]['id']
    movie = tmdb.Movies(id)
    info=movie.info()
    return info

def getmovie_genre(movie):
    resp = search.movie(query=movie)
    id=resp['results'][0]['id']
    movie = tmdb.Movies(id)
    genres=movie.info()['genres']
    return genres",0.50893116,
1919,show the top actors/actresses that have starred in the most movies released in the usa in the christmas season since and including the year,"def get_page(discover, year, page):
    results = discover.movie(page=page, sort_by='popularity.desc', primary_release_year=year)['results']
    return [movie['id'] for movie in results]

# Merge lists of lists into a single list
def flatten(lst):
    return list(chain.from_iterable(lst))

def get_yearly_ids(discover, year):
    # Ensure that we do not make more than 40 requests per 10 seconds
    sleep(10)
    return flatten([get_page(discover, year, page) for page in range(1, 26)])

def get_all_ids():
    discover = t_simple.Discover()
    results = flatten([get_yearly_ids(discover, year) for year in range(2000, 2017)])
    
    # Remove Super Gals (2001) from the list since it can no longer be found with the API
    super_gals = 440656
    if super_gals in results:
        results.remove(super_gals)
        
    return results

def list_to_str(lst):
    return ', '.join([x.name for x in lst])

def get_role(crew, role):
    return next((person.name for person in crew if person.job == role), None)

def get_actor(cast, main):
    return cast[0 if main else (1 if len(cast) > 1 else 0)].name if len(cast) > 0 else None

def get_row(m):
    return {'id': m.id, 'title': m.title, 'tagline': m.tagline, 'overview': m.overview, 'runtime': m.runtime, 
            'budget': m.budget, 'revenue': m.revenue, 'releasedate': m.releasedate, 'popularity': m.popularity, 
            'userrating': m.userrating, 'votes': m.votes, 'adult': m.adult, 'genres': list_to_str(m.genres), 
            'studios': list_to_str(m.studios), 'countries': list_to_str(m.countries), 
            'keywords': list_to_str(m.keywords), 'director': get_role(m.crew, 'Director'), 
            'producer': get_role(m.crew, 'Producer'), 'editor': get_role(m.crew, 'Editor'), 
            'main_actor': get_actor(m.cast, True), 'supporting_actor': get_actor(m.cast, False), 
            'poster': m.poster.geturl('w500') if m.poster else None}

def get_all_movies(movie_ids):
    return pd.DataFrame([get_row(tmdb.Movie(m_id)) for m_id in movie_ids])

def genre_to_matrix(genre_str):
    genres = genre_str.split(', ')
    return [(1 if g in genres else 0) for g in genre_lst]

def get_genre_lst():
    genres = json.loads(tmdb.request.Request('/genre/movie/list').read())['genres']
    return [genre['name'] for genre in genres]

def get_posters(links, indices):
    posters = [(Image.open(StringIO(urllib.urlopen(img).read())), sleep(0.25))[0] for img in links]
    return pd.DataFrame({'poster': posters}, index=indices)",0.5079963207,
1919,show the top actors/actresses that have starred in the most movies released in the usa in the christmas season since and including the year,"# Functions goes here
def grab_poster_tmdb(movie):
    response = search.movie(query=movie)
    id=response['results'][0]['id']
    movie = tmdb.Movies(id)
    posterp=movie.info()['poster_path']
    title=movie.info()['original_title']
    url='image.tmdb.org/t/p/original'+posterp
    title='_'.join(title.split(' '))
    strcmd='wget -O '+poster_folder+title+'.jpg '+url
    os.system(strcmd)
    #print(response['results'])
    
def get_movie_id_tmdb(movie):
    response = search.movie(query=movie)
    movie_id=response['results'][0]['id']
    return movie_id

def get_movie_info_tmdb(movie):
    response = search.movie(query=movie)
    movie_id=response['results'][0]['id']
    movie=tmdb.Movies(movie_id)
    info = movie.info()
    return info

def get_movie_genres_tmdb(movie):
    response=search.movie(query=movie)
    id=response['results'][0]['id']
    movie=tmdb.Movies(id)
    genres=movie.info()['genres']
    return genres",0.5077271461,
1919,show the top actors/actresses that have starred in the most movies released in the usa in the christmas season since and including the year,"from IPython.display import HTML
from IPython.display import display

def display_top_k_movies(similarity, mapper, movie_idx, base_url, api_key, k=5):
    movie_indices = np.argsort(similarity[movie_idx,:])[::-1]
    images = ''
    k_ctr = 0
    # Start i at 1 to not grab the input movie
    i = 1
    while k_ctr < 5:
        movie = mapper[movie_indices[i]]
        poster = get_poster(movie, base_url, api_key)
        if poster != '':
            images += ""<img style='width: 120px; margin: 0px; \
                      float: left; border: 1px solid black;' src='%s' />""\
                      % poster
            k_ctr += 1
        i += 1
    display(HTML(images))

def compare_recs(als_similarity, sgd_similarity, mapper,\
                 movie_idx, base_url, api_key, k=5):
    # Display input
    display(HTML('<font size=5>'+'Input'+'</font>'))
    input_poster = get_poster(mapper[movie_idx], base_url, api_key)
    input_image = ""<img style='width: 120px; margin: 0px; \
                      float: left; border: 1px solid black;' src='%s' />"" \
                  % input_poster
    display(HTML(input_image))
    # Display ALS Recs
    display(HTML('<font size=5>'+'ALS Recs'+'</font>'))
    display_top_k_movies(als_similarity, idx_to_movie,\
                         movie_idx, base_url, api_key)
    # Display SGD Recs
    display(HTML('<font size=5>'+'SGD Recs'+'</font>'))
    display_top_k_movies(sgd_similarity, idx_to_movie,\
                         movie_idx, base_url, api_key)",0.506962657,
1919,show the top actors/actresses that have starred in the most movies released in the usa in the christmas season since and including the year,"def get_recommendations(user_id,n,U,V_df,all_movies_genre):
        #Get all the movies rated by user ID 
        user_rated = Utility.loc[user_id][Utility.loc[user_id].notnull()]    
        #Get all the ratings of the user having more than 4    
        user_high_rated_df = all_movies_genre[all_movies_genre[""item_id""].isin( \
        user_rated[user_rated.values >= 4].index)]
        user_V_df = V_df[V_df[""item_id""].isin(user_rated.index)]
        
        user_watched= pd.merge(user_V_df,user_high_rated_df,on=""item_id"")
        user_watched[""watched""] = ""Yes""

        
        user_UV = UV.loc[user_id].copy()
        user_UV[user_rated.index] = -1
        recommended_movie_ids =  user_UV.sort_values(ascending=False).index[0:n]
        recommended_movies_df = V_df[V_df[""item_id""].isin(recommended_movie_ids)]

        recommended_movies_df=pd.merge(recommended_movies_df,
         all_movies_genre[all_movies_genre[""item_id""].isin(recommended_movie_ids)],
         on=""item_id"")
        recommended_movies_df[""watched""] = ""No""

        print ""The following movies were liked by user ID:{}, since s/he gave a rating of 4 or more\
         for these movies:"".format(user_id)
        display(user_watched)
        print ""The following movies are recommended to user ID:{}"".format(user_id)
        display(recommended_movies_df)
        plot_df = pd.concat([user_watched,recommended_movies_df])
        #display(plot_df)
        #fig, ax = plt.subplots()
        #for dd,daata in plot_df.groupby('watched'):
        #     ax.plot(daata['Factor-1']*100,daata['Factor-2']*100,'o',alpha=0.3)
        import seaborn as sns
        sns.set(style=""ticks"", context=""talk"")
        # Make a custom sequential palette using the cubehelix system
        #pal = sns.cubehelix_palette(20, 1, 1, light=.4, dark=.6)
        pal= sns.color_palette(""husl"", 2)
         
        sns.lmplot(x=""Factor-1"",y=""Factor-2"",fit_reg=False,hue=""watched"",
           palette=pal,data=plot_df,size=10,scatter_kws={'alpha':0.3})
         
        plt.show()
        return plot_df",0.5057072043,
1919,show the top actors/actresses that have starred in the most movies released in the usa in the christmas season since and including the year,"def get_movies_info(offset, limit):
    url = ""http://rt-client-facade-v2-6-1.aws.prod.flixster.com/\
    movie?expand=true&filter=%7B%22country%22%3A%22UA%22%2C%22search\
    %22%3A%7B%22mpaa-rating-min%22%3A%22G%22%2C%22release-year-max%22%3A2017\
    %2C%22filter-release-date%22%3A%22false%22%2C%22offset%22%3A{}%2C%22\
    services%22%3A%5B%22amazon%22%2C%22hbo_go%22%2C%22itunes%22%2C%22\
    netflix_iw%22%2C%22vudu%22%2C%22amazon_prime%22%2C%22fandango_now%22%5D%2C%22\
    sort%22%3A%22DVD_RELEASE_DATE_DESC%22%2C%22distribution%22%3A%22ANY%22%2C%22\
    tomatometer-min%22%3A0%2C%22mpaa-rating-max%22%3A%22UN%22%2C%22release-day-of-month-max\
    %22%3A14%2C%22limit%22%3A{}%2C%22release-month-max%22%3A4%2C%22tomatometer-max\
    %22%3A100%2C%22use-dvd-date%22%3Atrue%2C%22status%22%3A%22LIVE%22%7D%7D"".replace("" "", """").format(offset, limit)
    return requests.get(url).json()['data']",0.5056442022,
1919,show the top actors/actresses that have starred in the most movies released in the usa in the christmas season since and including the year,"a2a = ""Select * from Movies m where  m.title like '%""+str(year)+ ""%' order by last,first""
    q2m = cursor.execute(a2a).fetchall()
    #titles = list()
    #for row in q2m:
     #   titles.append(row[0])",0.5043748021,
1919,show the top actors/actresses that have starred in the most movies released in the usa in the christmas season since and including the year,"def get_movies_meta(df):
    max_movie_id_plus_one = df.movie_id.max() + 1

    unique_genres = sorted(df.genre.apply(lambda x: set_plus(x)).sum())
    genres_map = dict(zip(unique_genres, range(len(unique_genres))))
    
    genres = sp.lil_matrix((max_movie_id_plus_one, len(unique_genres)), dtype=np.float32)
    identity = sp.identity(max_movie_id_plus_one, format='csr', dtype=np.float32)
    
    for row in df.itertuples():
        movie_id = row.movie_id
        genre = row.genre
        
        for g in genre:
            genre_index = genres_map[g]
            genres[movie_id, genre_index] = 1.0
    
    return sp.hstack([genres, identity]).tocsr(), {""genre"": genres_map}",0.5043339729,
1195,midterm part instructions,"# Define function for lemmatization and apply to dataframe

def lem(row):
    lemma = WordNetLemmatizer()
    if(row[2] == None):
        return(lemma.lemmatize(row[0]))
    return(lemma.lemmatize(row[0],row[2]))

job_desc_lem_df[""lem_word""] = job_desc_lem_df.apply(lem,axis=1)",0.4364813566,
1195,midterm part instructions,"def decisiontree(x):
    decision = ''
    if x[0] == 'yes': 
        if x[1] < 29.5: 
            desicion = 'less risk'
        elif x[1] > 29.5:
            desicion = 'more risk'
        else: desicion ='other'
            
    elif x[0] == 'no':
        if x[2] =='good':
            desicion = 'less risk'
        elif x[2] == 'poor':
            desicion = 'more risk'
        else: desicion ='other'
            
    else: desicion = 'other'
        
    return desicion 

decisiontree(('yes',31,'good'))",0.427064091,
1195,midterm part instructions,"def enrich_record(record):
    if record[4] == 'A':
        record.append(4)
    elif record[4] == 'A-':
        record.append(3.7)
    elif record[4] == 'B+':
        record.append(3.3)
    elif record[4] == 'B':
        record.append(3)
    elif record[4] == 'B-':
        record.append(2.7)
    elif record[4] == 'C+':
        record.append(2.3)
    elif record[4] == 'C':
        record.append(2)
    elif record[4] == 'C-':
        record.append(1.7)
    elif record[4] == 'D+':
        record.append(1.3)
    elif record[4] == 'D':
        record.append(1)
    elif record[4] == 'F':
        record.append(0)
    modified_record = record
    return modified_record",0.4221890569,
1195,midterm part instructions,"# TODO: Replace <FILL IN> with appropriate code

# First, implement a helper function `getCountsAndAverages` using only Python
def getCountsAndAverages(IDandRatingsTuple):
    """""" Calculate average rating
    Args:
        IDandRatingsTuple: a single tuple of (MovieID, (Rating1, Rating2, Rating3, ...))
    Returns:
        tuple: a tuple of (MovieID, (number of ratings, averageRating))
    """"""
    ratings = IDandRatingsTuple[1]
    count = len(ratings) 
    avg = sum(ratings) / float(count)
    return (IDandRatingsTuple[0],(count,avg))",0.4179282784,
1195,midterm part instructions,"def teamMember(individual):
    member = individual[0]
    age = individual[1]
    if member == ""player"" :
        if ((age < 19) or (age > 25)):
            return (member, False)
        else:
            return (member, True)
    elif member == ""coach"":
        if (age > 25):
            return (member, True)
        else:
            return (member, False)
    else:
        return (""Invalid input"")",0.41786623,
1195,midterm part instructions,"def child_define(cols):
    Sex = cols[0]
    Age = cols[1]

    if Sex == 'male': 
        if Age > 18:
            return 'Father'
        else:
            return 'Child'
    elif Sex == 'female':
        if Age > 18:
            return 'Mother'
        else:
            return 'Child'",0.4171301126,
1195,midterm part instructions,"def test_inverse_captcha(f):
    assert(f(""1122"") == 3)
    assert(f(""1111"") == 4)
    assert(f(""1234"") == 0)
    assert(f(""91212129"") == 9)
    assert(f("""") == 0)
    assert(f(""3"") == 3)

def find_inverse_captcha(digits):
    total = 0
    for i in range(len(digits)):
        if digits[i] == digits[i-1]:
            total += int(digits[i])
    return total

test_inverse_captcha(find_inverse_captcha)",0.4138221741,
1195,midterm part instructions,"# Add a new column 'Who', value: man, woman, childage < 18
def get_who(age_sex_df):
    if age_sex_df[0] < 18:
        return 'child'
    elif age_sex_df[1] == 'female':
        return 'woman'
    return 'man'",0.4123133421,
1195,midterm part instructions,"def p_expression_1(t):
    'expression : expression PLUS term'
    #
    t[0] = (t[1][0] + t[3][0],
            attrDyadicInfix(""+"", t[1][1], t[3][1]))   
    
def p_expression_2(t):
    'expression : expression MINUS term'
    #
    t[0] = (t[1][0] - t[3][0],
            attrDyadicInfix(""-"", t[1][1], t[3][1]))   
    
def p_expression_3(t):
    'expression : term'
    #
    t[0] = t[1]  
    
# Consult this excellent reference for info on precedences
# https://www.cs.utah.edu/~zachary/isp/worksheets/operprec/operprec.html
    
    
def p_term_1(t):
    'term :  term TIMES factor'
    #
    t[0] = (t[1][0] * t[3][0],
            attrDyadicInfix(""*"", t[1][1], t[3][1])) 

    
def p_term_2(t):
    'term :  term DIVIDE factor'
    #
    if (t[3][0] == 0):
        print(""Error, divide by zero!"")
        t[3][0] = 1 # fix it
    t[0] = (t[1][0] / t[3][0],
            attrDyadicInfix(""/"", t[1][1], t[3][1]))


def p_term_3(t):
    'term :  factor'
    #
    t[0] = t[1]  

def p_factor_1(t):
    'factor : innerfactor EXP factor'
    #
    t[0] = (t[1][0] ** t[3][0],
            attrDyadicInfix(""^"", t[1][1], t[3][1])) 

def p_factor_2(t):
    'factor : innerfactor'
    #
    t[0] = t[1]
    
def p_innerfactor_1(t):
    'innerfactor : UMINUS innerfactor'
    #
    ast  = ('~', t[2][1]['ast'])
    
    nlin = t[2][1]['dig']['nl']
    elin = t[2][1]['dig']['el']
    
    rootin = nlin[0]

    root = NxtStateStr(""~E_"") 
    left = NxtStateStr(""~_"")

    t[0] =(-t[2][0], 
           {'ast' : ast,
            'dig' : {'nl' : [ root, left ] + nlin, # this order important for proper layout!
                     'el' : elin + [ (root, left),
                                     (root, rootin) ]
                    }})

    
def p_innerfactor_2(t):
    'innerfactor : LPAREN expression RPAREN'
    #
    ast  = t[2][1]['ast']
    
    nlin = t[2][1]['dig']['nl']
    elin = t[2][1]['dig']['el']
    
    rootin = nlin[0]
    
    root = NxtStateStr(""(E)_"")
    left = NxtStateStr(""(_"")
    right= NxtStateStr("")_"")
    
    t[0] =(t[2][0],
           {'ast' : ast,
            'dig' : {'nl' : [root, left] + nlin + [right], #order important f. proper layout!
                     'el' : elin + [ (root, left),
                                     (root, rootin),
                                     (root, right) ]
                    }})

def p_innerfactor_3(t):
    'innerfactor : NUMBER'
    #
    strn = str(t[1])
    ast  = ('NUMBER', strn)           
    t[0] =(t[1],
           { 'ast' : ast,
             'dig' : {'nl' : [ strn + NxtStateStr(""_"") ],
                      'el' : []
                     }})


def p_error(t):
    print(""Syntax error at '%s'"" % t.value)

#--
    
def attrDyadicInfix(op, attr1, attr3):
    ast  = (op, (attr1['ast'], attr3['ast']))
    
    nlin1 = attr1['dig']['nl']
    nlin3 = attr3['dig']['nl']
    nlin  = nlin1 + nlin3
    
    elin1 = attr1['dig']['el']
    elin3 = attr3['dig']['el']
    elin  = elin1 + elin3
    
    rootin1 = nlin1[0]
    rootin3 = nlin3[0]    
    
    root   = NxtStateStr(""E1""+op+""E2""+""_"") # NxtStateStr(""$_"")
    left   = rootin1
    middle = NxtStateStr(op+""_"")
    right  = rootin3
    
    return {'ast' : ast,
            'dig' : {'nl' : [ root, left, middle, right ] + nlin,
                     'el' : elin + [ (root, left),
                                     (root, middle),
                                     (root, right) ]
                     }}

#===
# This is the main function in this Jove file.  
#===

def parseExp(s):
    """"""In: a string s containing a regular expression.
       Out: An attribute triple consisting of
            1) An abstract syntax tree suitable for processing in the derivative-based scanner
            2) A node-list for the parse-tree digraph generated. Good for drawing a parse tree 
               using the drawPT function below
            3) An edge list for the parse-tree generated (again good for drawing using the
               drawPT function below)
    """"""
    mylexer  = lex()
    myparser = yacc()
    pt = myparser.parse(s, lexer = mylexer)
    
    # print('parsed result  is ', pt)
    # (result, ast, nodes, edges)
    return (pt[0], pt[1]['ast'], pt[1]['dig']['nl'], pt[1]['dig']['el'])

def drawPT(ast_rslt_nl_el, comment=""PT""):
    """"""Given an (ast, nl, el) triple where nl is the node and el the edge-list,
       draw the Parse Tree by returning a dot object.
    """"""
    (rslt, ast, nl, el) = ast_rslt_nl_el
    
    print(""Result calculated = "", rslt)
    print(""Drawing AST for "", ast)
    
    dotObj_pt = Digraph(comment)
    dotObj_pt.graph_attr['rankdir'] = 'TB'
    for n in nl:
        prNam = n.split('_')[0]
        dotObj_pt.node(n, prNam, shape=""oval"", peripheries=""1"")
    for e in el:
        dotObj_pt.edge(e[0], e[1])
    return dotObj_pt",0.4107792377,
1195,midterm part instructions,"def getTransferTime(x):
    if x[1] == str(0):
        return 1
    elif x[1] == str(1):
        return 10
    elif x[1] == str(2):
        return 100",0.4103190005,
939,installing new packages,"FROM ubuntu:xenial

WORKDIR /app

RUN apt-get update && \
    apt-get install -y --no-install-recommends libcurl4-openssl-dev python-pip libboost-python-dev && \
    rm -rf /var/lib/apt/lists/* 

COPY requirements.txt ./
RUN pip install -r requirements.txt

COPY . .
RUN mkdir /myvol
RUN touch /myvol/data.json
RUN chmod 777 /myvol/data.json
VOLUME /myvol
RUN useradd -ms /bin/bash moduleuser
USER moduleuser

CMD [ ""python"", ""-u"", ""./main.py"" ]",0.5003251433,
939,installing new packages,"%%bash
pip install --upgrade requests",0.4920807183,
939,installing new packages,"%%bash
cd .. 
pip install . --upgrade",0.4910984039,
939,installing new packages,"RUN apt-get update && apt-get install -y --no-install-recommends \
        build-essential \
        curl \
        libfreetype6-dev \
        libpng12-dev \
        libzmq3-dev \
        pkg-config \
        python \
        python-dev \
        rsync \
        software-properties-common \
        unzip \
        && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*",0.4904007018,
939,installing new packages,"# With a raw URL
ct.install_package(url='https://github.com/choldgraf/connectortools.git')",0.4889755249,
939,installing new packages,"## LISTING 1
import pip
    installed_packages = pip.get_installed_distributions()
    flat_installed_packages = [package.project_name for package in installed_packages]
    if ""psutil"" not in flat_installed_packages:
        print(""Package 'psutil' must be installed to correctly run this program."")
        exit()",0.4862549305,
939,installing new packages,"package_harmonie.download_package()
package_fmi_hirlam.download_package()",0.4847054482,
939,installing new packages,"package_swan.download_package()
package_ww3.download_package()",0.4847054482,
939,installing new packages,"%%writefile scripts/install_java.sh
#!/bin/bash

# Add webupd8team to apt sources

if [ ! -f /etc/apt/sources.list.d/webupd8team-java-trusty.list ]; then
    sudo add-apt-repository -y ppa:webupd8team/java
    sudo apt-get update
fi

# Install Java 8

if  [ """" == ""$(which java)"" ]; then
    echo debconf shared/accepted-oracle-license-v1-1 select true | \
        sudo debconf-set-selections
    echo debconf shared/accepted-oracle-license-v1-1 seen true | \
        sudo debconf-set-selections
    sudo apt-get -y install oracle-java8-installer

    echo ""export JAVA_HOME=/usr/lib/jvm/java-8-oracle"" >> $HOME/.profile
fi",0.4843428731,
939,installing new packages,"utils = importr('utils')
utils.install_packages('bnlearn');",0.4805309772,
2302,tensorflow,"with tf.Session():
    input = tf.placeholder(tf.float32)
    classifier = ...
    print(classifier.eval(feed_dict={input: my_python_preprocessing_fn()}))",0.617898345,
2302,tensorflow,"import tensorflow as tf
import numpy as np

with tf.Session():
    input1 = tf.constant(1.0, shape=[2, 3])
    #input2 = tf.constant(np.reshape(np.arange(1.0, 7.0, dtype=np.float32), (2, 3)))
    input2 = tf.reshape(
        tf.range(start=1.0, 
                 limit=7.0, 
                 delta=1.0, 
                 dtype=tf.float32, 
                 name='input2_range'), 
        shape=[2,3])
    output = tf.add(input1, input2)
    print(output.eval())",0.612473011,
2302,tensorflow,"import tensorflow as tf
tf.reset_default_graph()

with tf.Session():
  with tf.device(""/cpu:0""):    
    matrix1 = tf.constant([[3., 3.]])
    matrix2 = tf.constant([[2.],[2.]])
    product = tf.matmul(matrix1, matrix2)
    print product.eval()",0.6098496914,
2302,tensorflow,"W = tf.Variable(1)
#assign_op = W.assign(2)
with tf.Session() as session:
    session.run(W.initializer)
    session.run(W.assign(2))
    print(W.eval())",0.6083083153,
2302,tensorflow,"# Creates a tensor of shape and all elements will be ones
with tf.Session() as sess:
    print(sess.run(tf.ones_like(tensor=input_tensor)))",0.6082130075,
2302,tensorflow,"with tf.Session() as sess:
  constant = tf.constant([1, 2, 3])
  tensor = constant * constant
  print(tensor.eval())",0.6068713665,
2302,tensorflow,"with tf.Session() as sess:
    constant = tf.constant([1, 2, 3])
    tensor = constant * constant
    print(tensor.eval())
    
    p = tf.placeholder(tf.float32)
    t = p + 1.0
    # t.eval()  # This will fail, since the placeholder did not get a value.
    print(t.eval(feed_dict={p:2.0}))  # This will succeed because we're feeding a value
                               # to the placeholder.",0.6057355404,
2302,tensorflow,"with tf.Session() as sess_vgg:
    imgs = tf.placeholder(tf.float32, [None, 200, 200, 3])
    vgg = vgg16(imgs, 'vgg16_weights.npz', sess_vgg)
    img_files = ['./data/img/cropped/' + i for i in os.listdir('./data/img/cropped')]
    imgs = [imread(file, mode='RGB') for file in img_files]
    temps = [sess_vgg.run(vgg.fc1, feed_dict={vgg.imgs: [imgs[i]]})[0] for i in range(50)]
    reimgs= np.reshape(a=temps, newshape=[50,-1])
    sess_vgg.close()",0.6039416194,
2302,tensorflow,"import tensorflow as tf
tf.reset_default_graph()

a = tf.Variable(3,name=""my_var"")

with tf.Session() as sess:
    sess.run(tf.initialize_all_variables())
    print(sess.run(a.assign_add(1)))
    print(sess.run(a.assign_sub(1)))
    print(sess.run(a.assign_sub(1)))
    sess.run(tf.initialize_all_variables())
    print(sess.run(a))",0.6038050652,
2302,tensorflow,"#@test {""output"": ""ignore""}
import tensorflow as tf
import numpy as np

with tf.Session():
    input_features = tf.constant(np.reshape([1, 0, 0, 1], (1, 4)).astype(np.float32))
    weights = tf.constant(np.random.randn(4, 2).astype(np.float32))
    output = tf.matmul(input_features, weights)
    print(""Input:"")
    print(input_features.eval())
    print(""Weights:"")
    print(weights.eval())
    print(""Output:"")
    print(output.eval())",0.6037895679,
1768,recommendation engine,"def gen_recs(model, data):
    recs = model.recommend(data['user_id'])
    return recs",0.5211628675,
1768,recommendation engine,"# This function calculates the catelog coverage 
# The inputs are: the model we trained, the test dataset and number of recommendations we make for each user
# The output is a result of catelog coverage 
def cal_coverage(model,test,num_k):
    recs= float(model.recommend(users=test['user_id'],k=num_k,items=test[""item_id""],verbose=False).to_dataframe().item_id.nunique())
    all = float(test.to_dataframe().item_id.nunique())
    return recs/all",0.5075184107,
1768,recommendation engine,"def test_recommenders(client):
    return tuple([recommender.can_recommend(client) for recommender in recommenders.values()])",0.5069375038,
1768,recommendation engine,"class Recommender(object):
    '''
    Main recommender engine, that decides how to proceed with a particular customer ID.
    '''
    def __init__(self, data, mba_analyzer):
        self.cust_id = None
        self.shop_id = None
        self.cust_type = None
        self.suggested_items = None
        self.basket = []
        self.basket_analyzer = mba_analyzer
        print(""Initialized Recommender.."")
        
    def add_to_basket(self, barcode):
        self.basket.append(barcode)
        
    def remove_from_basket(self, barcode):
        self.basket.remove(barcode)

    def clear_data(self):
        self.cust_id = None
        self.shop_id = None
        self.basket.clear() # or del self.basket[:]
        self.cust_type = None
        print()
        
    def calc_retention(self, _from, _until):
        # train_[train_['shopId'] == self.shop_id]['orderProcessingTime'].min()
        df_earlier = train_[train_['orderProcessingTime'] < _from]
        df_during = train_[(train_['orderProcessingTime'] >= _from) & (train_['orderProcessingTime'] <= _until)]
        S = df_earlier['customerId']
        E = train_[train_['orderProcessingTime'] <= _until]['customerId']
        N = set(E) - set(S)
        CR = set(df_during['customerId']) - N
#         print(CR)
        
#         print(S.unique().tolist())
#         _from = _from.strftime(""%d, %b %Y"")
#         _until = _until.strftime(""%d, %b %Y"")
        _from = custom_strftime('%B {S}, %Y', _from)
        _until = custom_strftime('%B {S}, %Y', _until)

        
        print(""\nFor the period:- from %s, until %s :-"" % (_from, _until))
        print(""\tCustomer Retention Rate = (CR/S)*100 \nCRR = %s"" 
              % (len(CR)/len(set(S))*100))        
        
        print(""""""\t# of customer at the end [{}] (E): {}
        # of customers before [{}] (S): {}\n\
        # of new customers acquired (N): {}\n\
        # of customers retained [from before {}] (CR): {}""""""\
              .format(_until, len(E.unique()),
                      _from, len(S.unique()), 
                      len(N),
                      _from, len(CR)))
    
#     def load_basket_pairs(self):
#         self.basket_analyzer.load()
    def details(self):
        print(""--------------------------------------"")
        print(""Customer ID: "", self.cust_id)
        print(""Shop ID: "", self.shop_id)
        basket = df_products[df_products['barcodeId'].isin(self.basket)].productName.tolist()
        print(""Basket: "", basket)
#         if not self.suggested_items.empty:
#             print(""======================================"")
#             print(""\nTop Recommendations: \n"", self.suggested_items)
#             print(""======================================"")
#         print(""--------------------------------------"")
        print()
        
    def market_basket_analyzer(self, basket=[]):
        """"""
        @cust_type: kind of customer (if retained or not). 
               Values stand for:
                - 0: Retained (and went to same store)
                - 1: Retained (but goes to a new store this time)
                - 2: Acquired (doesn't exist in the database yet)        
        """"""
        # TODO: add cohort analysis and make basket analysis a conditional on whether customer data exists
        if bool(basket):
            if self.cust_type==2:
                self.suggested_items = self.basket_analyzer.get_closest_pairs(self.shop_id, basket)
            elif self.cust_type==1:
                # TODO: add cohort analysis
                self.suggested_items = self.basket_analyzer.get_closest_pairs(self.shop_id, basket)            
            else:
                # TODO: add cohort analysis
                pair_1 = self.basket_analyzer.get_closest_pairs(self.shop_id, basket)
                if pair_1.empty:
                    pair_1 = self.basket_analyzer.top_items(self.shop_id)
                pair_2 = self.basket_analyzer.analyze_historical_data(self.shop_id, basket)
                self.suggested_items = set(pair_1) & pair_2
        else:
            self.suggested_items = self.basket_analyzer.top_items(self.shop_id)
           
        if self.suggested_items.empty:
#         if not bool(self.suggested_items):
            self.suggested_items = self.basket_analyzer.top_items(self.shop_id)['productName'].values   
        print(""Top Recommendations: "")
        print(self.suggested_items['productName'].values)
        
    def populate(self, cust_id=None, shop_id=None, cust_type=None, basket=[]):
        """"""
        @cust_id: Customer's ID (if new, should be serialized in sync with the Database)
        
        @shopId: Current shop in which the customer is looking for recommendations
                
        @basket: barcoded list of items that are already present in this customer's cart
        """"""
        if not cust_id:
            print(""ERROR: need a customer ID to suggest items with.."")
            self.invalid_data = 1
            return
        
        self.cust_id = cust_id
        self.shop_id = shop_id
        if basket:
            self.basket = basket   
        if shop_id and (not any(train_.shopId == shop_id)):
            print(""ERROR: Customer ID %s supplied with Invalid Shop ID %s. "" 
                  % (cust_id, shop_id))
            self.invalid_data = 1
            return
                
        print(""INFO: Customer ID %s with shop # %s -- "" 
              % (cust_id, shop_id), end="""")
        self.invalid_data = 0
        
    def suggest(self):
        """"""
        @cust_type: kind of customer (if retained or not). 
               Values stand for:
                - 0: Retained (and went to same store)
                - 1: Retained (but goes to a new store this time)
                - 2: Acquired (doesn't exist in the database yet)        
        """"""
        if self.invalid_data == 1:
            return
        
        if not self.cust_type:
            if ((train_['customerId'] == self.cust_id) & (train_['shopId'] == self.shop_id)).any():
                print(""[Retained][went to same store]"")
                self.cust_type=0
            elif ((train_['customerId'] == self.cust_id) & (train_['shopId'] != self.shop_id)).any():
                print(""[Retained][goes to a new store]"")
                self.cust_type=1
            else:
                # not any(train_.customerId == cust_id)
                print(""[Acquired]"")
                self.cust_type=2
                
        self.market_basket_analyzer()
            
#         return self.suggested_items",0.4749213457,
1768,recommendation engine,"def evaluate_mrr(model):
    '''Function to calculate MRR score.'''
    is_holdout = model.recommendations==0 # holdout items are always in the first column before sorting
    pos = np.where(is_holdout)[1] + 1.0 # position of holdout items (indexing starts from 0, so adding 1) 
    mrr = np.reciprocal(pos).mean() # mean reciprocal rank
    return mrr",0.471283257,
1768,recommendation engine,"# feature is zero if not a reply and number of upvotes of original comment if it is a reply
def replycounter(x):
    if x == 0:
        return 0
    elif x != 0:
        return df.recommendations.at[int(np.where(df.commentID == x)[0])]
    else:
        return np.nan

# run only on the dev set for speed
X['reply'] = multip(replycounter, df.inReplyTo[dev_set_index])

# uncomment this for full set
# X_full['reply'] = multip(replycounter, df.inReplyTo)

print(X.reply.describe())",0.4503213465,
1768,recommendation engine,"sr1 = SimpleRecommender()
res1 = sr1.recommend(None,None,None,""test"",2)
print res1",0.4500140548,
1768,recommendation engine,"def train_model(data):
  import graphlab as gl
  model = gl.recommender.create(data, user_id='user_id', item_id='movie_id', target='rating')
  return model",0.4486292899,
1768,recommendation engine,"recommender = UserBasedRecommender(model, similarity, neighborhood, with_preference=True)
results = recommender.recommend(target_user)",0.4478643537,
1768,recommendation engine,"def items_cross_val_scores(item_data, n_splits=5):
    return cross_val_scores(lambda train_sf:\
                            gl.factorization_recommender.create(train_sf, user_id='UserId', item_id='ItemId', target='Rating', verbose=False,
                                                                item_data=item_data,
                                                                num_factors=3,
                                                                max_iterations=70,
                                                                solver='sgd',
                                                                sgd_step_size=0,
                                                                side_data_factorization=True,
                                                                regularization_type='weighted',
                                                                linear_regularization=0.758620689655,
                                                                regularization=0.413793103449,
                                                                ), train_provided_sf, n_splits, verbose=False)",0.4471703768,
1234,multidimensional arrays,"print(len(test_predicts_list))
test_predicts_am = np.zeros(test_predicts_list[0].shape)

for fold_predict in test_predicts_list:
    test_predicts_am += fold_predict

test_predicts_am = (test_predicts_am / len(test_predicts_list))

test_ids = test_df[""id""].values
test_ids = test_ids.reshape((len(test_ids), 1))

test_predicts_am = pd.DataFrame(data=test_predicts_am, columns=CLASSES)
test_predicts_am[""id""] = test_ids
test_predicts_am = test_predicts_am[[""id""] + CLASSES]
test_predicts_am.to_csv(""10fold_lstmpp_am.csv"", index=False)",0.4914472699,
1234,multidimensional arrays,"np.prod(np.arange(1, a_MP-1))",0.4879585505,
1234,multidimensional arrays,"# The long way
final_image = np.zeros(shape=image_concat[0].shape)

for image in image_concat:
    final_image += image

# The short way
#final_image = np.sum(image_concat, axis=0)",0.4845418334,
1234,multidimensional arrays,"# The long way
final_image = np.zeros(shape=image_concat[0].shape)

for image in image_concat:
    final_image += image

# The short way
# final_image = np.sum(image_concat, axis=0)",0.4845418334,
1234,multidimensional arrays,np.zeros([3]),0.4816759229,
1234,multidimensional arrays,"allPaths=Path(np.empty((1,2)))
currBurr=0
for currBurr in range(0,34):
    currBurr_array=[]
    currBurr_array=np.asarray(burrShapes['features'][currBurr]['geometry']['coordinates'][0][0])
    currBurrPath = Path(currBurr_array)
    allPaths = Path.make_compound_path(allPaths,currBurrPath)",0.4799262285,
1234,multidimensional arrays,"# Create video with function

# Make a numpy array of HxWx3 values, each of which is an 8-bit integer 
screen_size = 100
grid = [[[0,0,0] for c in range(screen_size)] for r in range(screen_size)]
grid = np.array(grid)

# Must return a HxWx3 numpy array (of 8-bits integers) representing the frame at time t
def make_frame(t):
    return grid

new_video = mpy.VideoClip(make_frame, duration=2) # 2 seconds
new_video.fps = 15
new_video.ipython_display()",0.4787402153,
1234,multidimensional arrays,"# make the fovs to the same dimensions
base = np.zeros(np.max([im.shape for im in im_means],0))
im_means_resc = []
for im in im_means:
    base_=base.copy()+np.median(im)
    base_[:im.shape[0],:im.shape[1],:im.shape[2]]=im
    im_means_resc.append(base_)
# Select and curate the chromosomes
chroms = mv.imshow_mark_3d(im_means_resc,image_names=fovs,save_file =analysis_folder+os.sep+'Selected_Spot.pkl')
chroms.fast=False
chroms.th_seed=400
chroms.gfilt_size=1
""Press A or D to move between fovs and press T to seed (maybe Y to fit) right click to select, Shift + right click to delete""",0.4786732197,
1234,multidimensional arrays,"# Create 3x5 matrix with a parameter(s)
x = 5
np.full((3,5), x)",0.4779586196,
1234,multidimensional arrays,"print(w0[[0,2]].imag)",0.477591604,
527,encoding the labels,"def label_encode(feature):
    """""" this function encodes the labels of a given feature """"""
    le = preprocessing.LabelEncoder()
    le.fit(feature.values)
    return le.transform(feature.values)",0.5208970308,
527,encoding the labels,"def oneHotEncoded_y(y):
    
    encoder = preprocessing.LabelEncoder()
    encoder.fit(y)
    encoded_y = encoder.transform(y)
    encoded_y = encoded_y.reshape(-1,1)
    categorical_y = np_utils.to_categorical(encoded_y)
    
    return categorical_y",0.5095600486,
527,encoding the labels,"# Method for applying label encoding to column
def ModifyingColumnToLabel(df,colname):
    lab_enc = preprocessing.LabelEncoder()
    _=lab_enc.fit(df[colname])
    new_val = lab_enc.transform(df[colname])
    _=df.loc[:, '{}_val'.format(colname)] = new_val",0.5034614801,
527,encoding the labels,"def label_encode(df,feature):
    """"""Encodes categorical data from dataframe""""""
    enc = skpre.LabelEncoder()
    idx = enc.fit_transform(df[feature])
    n = enc.classes_.shape[0]
    return idx, n, enc",0.4996482134,
527,encoding the labels,"def one_hot_encode(labels):
    """"""
        Turn single digit numerical value classes into length 10 vectors...
        1 for the positive class and 0's for the other 9 negative classes.
    """"""
    
    min_l, max_l = min(labels), max(labels)
    one_hot_map = {k: [0 if i < k else 1 if i == k else 0 \
                              for i in range(min_l, max_l+1)] \
                      for k in range(min_l, max_l+1)}
    
    return np.array([one_hot_map[label[0]] for label in labels]).reshape(-1, 10)
    
def sigmoid(z):
    """"""
        Squishes the output of our hypothesis function into the range (0, 1).
    """"""
       
    exp = np.exp(z)
    return exp/(exp+1)

def softmax(z):
    """"""
        Produce confidence probabilities for the output classes.
    """"""

    exp = np.exp(z)
    return exp/exp.sum(axis=1).reshape(-1, 1)
    
def eval_softmax(weights, data, labels):
    """"""
        Convert softmax activations into well-defined outputs; evaluate accuracy using labels.
    """"""
    
    net = {'weights': weights}
    s_max = softmax(forward(net, data))

    # Apply mask to accentuate softmax output
    test_mask = np.where(s_max == s_max.max(axis=1).reshape(-1, 1), 1, 0)

    return sum([1 if (test_mask[i]==l).all() else 0 for i, l in enumerate(labels)])/float(len(labels))*100
    
def log_loss(output, labels):
    """"""
        Computes the log loss when provided with the output from the final activation layer
        and the corresponding labeled data.
    """"""
    
    output = output.clip(min=1e-10, max=0.9999999999)
            
    return (-1./labels.shape[0]) * np.sum(np.multiply(labels, np.log(output)) + np.multiply((1-labels), np.log(1-output)))
    
def forward(net, X):
    """"""
        Perform forward propagation through our neural network. Apply sigmoid to 
        pre-activation signals.
    """"""
    
    # Retrieve weights from the network and reinitialize the cached activations
    W1 = net['weights'][0]
    W2 = net['weights'][1]
    net['activations'] = []
           
    # Perform forward pass while appyling bias terms to the input and a1 layers
    X_bias = np.concatenate((X, np.ones((X.shape[0], 1))), axis=1)   
    a1 = sigmoid(np.dot(W1, X_bias.T))                              
    a1_bias = np.concatenate((a1, np.ones((1, X.shape[0]))), axis=0) 
    a2 = sigmoid(np.dot(W2, a1_bias)).T    
    
    # Cache input data and activations
    net['input'] = X  
    net['activations'].append(a1)
    net['activations'].append(a2)
                
    return a2
    
def backward(net, labels):
    """"""
        Perform back propagation through our neural network. Compute error derivatives
        w.r.t. parameter and bias weights.
    """"""
    
    # Retrieve input data, cached activations, and weights from the network
    X = net['input']
    a1 = net['activations'][0]
    a2 = net['activations'][1]
    W1 = net['weights'][0][:, :-1]
    W2 = net['weights'][1][:, :-1]

    # Compute loss function derivatives w.r.t. parameters and biases
    dL_dW2 = (1./X.shape[0]) * np.dot(a1, (a2-labels)).T
    dL_db2 = (1./X.shape[0]) * np.sum((a2-labels), axis=0, keepdims=True).T
    dL_dW1 = (1./X.shape[0]) * np.dot(np.multiply(np.dot((a2-labels), W2).T, (a1 * (1-a1))), X)
    dL_db1 = (1./X.shape[0]) * np.sum(np.multiply(np.dot((a2-labels), W2).T, (a1 * (1-a1))), axis=1, keepdims=True)   
        
    return dL_dW1, dL_db1, dL_dW2, dL_db2
        
def gradient_descent(net, X, labels, alpha, iters, suppress_output=False):
    """"""
        Use loss function derivatives w.r.t weights to continually improve the
        performace of our network.
    """"""
      
    def update_weights(weights, gradients):
        W1, b1 = weights[0][:, :-1], weights[0][:, -1].reshape(-1, 1)
        W2, b2 = weights[1][:, :-1], weights[1][:, -1].reshape(-1, 1)
        
        W1 -= alpha * gradients[0]
        b1 -= alpha * gradients[1]
        W2 -= alpha * gradients[2]
        b2 -= alpha * gradients[3]  
        
    new_cost, old_cost = log_loss(forward(net, X), labels), float('inf')    
    beginning = start = time()
    count = 0
    
    if not suppress_output:
        print 'Initial cost:', new_cost

    while count < iters:
        update_weights(net['weights'], backward(net, labels))
        old_cost = new_cost
        new_cost = log_loss(forward(net, X), labels)
        if not suppress_output:
            if time() - start > 10:
                print '\t', new_cost
            start = time()
        count += 1
     
    elapsed = time() - beginning
    train_loss = old_cost
    test_loss = log_loss(forward(net, x_test), y_test)
        
    if not suppress_output:
        print 'Model has a log loss of {}\n\tachieved in {} iterations in {} seconds with a learning rate of {}'.format(
                train_loss, iters, elapsed, alpha)
    
    return (train_loss, test_loss, elapsed)",0.4922119975,
527,encoding the labels,"def one_hot_encode(labels):
    """"""
        Turn single digit numerical value classes into length 10 vectors...
        1 for the positive class and 0's for the other 9 negative classes.
    """"""
    
    min_l, max_l = min(labels), max(labels)
    one_hot_map = {k: [0 if i < k else 1 if i == k else 0 \
                              for i in range(min_l, max_l+1)] \
                      for k in range(min_l, max_l+1)}
    
    return np.array([one_hot_map[label[0]] for label in labels]).reshape(-1, 10)
    
def sigmoid(z):
    """"""
        Squishes the output of our hypothesis function into the range (0, 1).
    """"""
    
    z = z.clip(min=-500, max=500)
    exp = np.exp(z)
    return exp/(exp+1)

def tanh(z):
    """"""
        Squishes the output of our hypothesis function into the range(-1, 1).
    """"""
    
    return 2*sigmoid(2*z) - 1

def relu(z):
    """"""
        Rectified Linear Unit.
    """"""
    
    return np.where(z>0, z, 0)

def lrelu(z):
    """"""
        Leaky Rectified Linear Unit.
    """"""

    return np.where(z>0, z, 0.01*z)
    
def elu(z):
    """"""
        Exponential Linear Unit.
    """"""
    
    z = z.clip(min=-500, max=500)
    alpha = 1
    return np.where(z>0, z, alpha*(np.exp(z) - 1)) 
    
def softmax(z):
    """"""
        Produce confidence probabilities for the output classes.
    """"""

    z = z.clip(min=-500, max=500)
    exp = np.exp(z)
    return exp/exp.sum(axis=1).reshape(-1, 1)
    
def eval_softmax(weights, data, labels, activation):
    """"""
        Convert softmax activations into well-defined outputs; evaluate accuracy using labels.
    """"""
    
    net = {'weights': weights}
    s_max = softmax(forward(net, data, activation))

    # Apply mask to accentuate softmax output
    test_mask = np.where(s_max == s_max.max(axis=1).reshape(-1, 1), 1, 0)

    return sum([1 if (test_mask[i]==l).all() else 0 for i, l in enumerate(labels)])/float(len(labels))*100
    
def log_loss(output, labels, regularization=None, reg_lambda=None):
    """"""
        Computes the log loss when provided with the output from the final activation layer
        and the corresponding labeled data.
    """"""
    
    output = output.clip(min=1e-10, max=0.9999999999)
    
    cost = (-1./labels.shape[0]) * np.sum(np.multiply(labels, np.log(output)) + np.multiply((1-labels), np.log(1-output)))
    if regularization == 'l2':
        cost += (reg_lambda/2.) * (np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3)))
    elif regularization == 'l1':
        cost += (reg_lambda/2.) * (np.sum(np.absolute(W1)) + np.sum(np.absolute(W2)) + np.sum(np.absolute(W3)))
    elif regularization == 'elastic':
        pass
    elif regularization != None:
        print 'Unrecognized regularization term'
    
    return cost
    
def forward(net, X, activation):
    """"""
        Perform forward propagation through our neural network. Apply sigmoid to 
        pre-activation signals.
    """"""
    
    # Retrieve weights from the network and reinitialize the cached activations
    W1 = net['weights'][0]
    W2 = net['weights'][1]
    W3 = net['weights'][2]
    net['activations'] = []
                   
    # Perform forward pass while appyling bias terms to the input and a1 layers
    X_bias = np.concatenate((X, np.ones((X.shape[0], 1))), axis=1)  
    a1 = activation(np.dot(W1, X_bias.T))
    a1_bias = np.concatenate((a1, np.ones((1, X.shape[0]))), axis=0) 
    a2 = activation(np.dot(W2, a1_bias))
    a2_bias = np.concatenate((a2, np.ones((1, X.shape[0]))), axis=0)
    a3 = sigmoid(np.dot(W3, a2_bias)).T
    
    # Cache input data and activations
    net['input'] = X  
    net['activations'].append(a1)
    net['activations'].append(a2)
    net['activations'].append(a3)
                    
    return a3
    
def backward(net, labels, activation, reg_lambda=0):
    """"""
        Perform back propagation through our neural network. Compute error derivatives
        w.r.t. parameter and bias weights.
    """"""
    
    # Retrieve input data, cached activations, and weights from the network
    X = net['input']
    a1 = net['activations'][0]
    a2 = net['activations'][1]
    a3 = net['activations'][2]
    W1 = net['weights'][0][:, :-1]
    W2 = net['weights'][1][:, :-1]
    W3 = net['weights'][2][:, :-1]

    # Compute loss function derivatives w.r.t. parameters and biases
    m = (1./X.shape[0])
    dL_da3 = (a3-labels)
    dL_dW3 = m * np.dot(a2, dL_da3).T + m * reg_lambda * W3
    dL_db3 = m * np.sum(dL_da3, axis=0, keepdims=True).T
    dL_da2 = np.dot(dL_da3, W3).T

    activation_func = str(activation).split()[1]    
    if activation_func == 'sigmoid':
        dL_dz2 = np.multiply(dL_da2, (a2 * (1-a2)))
        dL_dz1 = np.multiply(np.dot(dL_dz2.T, W2).T, (a1 *(1-a1)))
    elif activation_func == 'tanh':
        dL_dz2 = np.multiply(dL_da2, (1 - np.square(a2))) 
        dL_dz1 = np.multiply(np.dot(dL_dz2.T, W2).T, (1 - np.square(a1)))
    elif activation_func == 'relu':
        dL_dz2 = np.multiply(dL_da2, np.where(a2>0, 1, 0))
        dL_dz1 = np.multiply(np.dot(dL_dz2.T, W2).T, np.where(a1>0, 1, 0))
    elif activation_func == 'lrelu':
        dL_dz2 = np.multiply(dL_da2, np.where(a2>0, 1, 0.01)) 
        dL_dz1 = np.multiply(np.dot(dL_dz2.T, W2).T, np.where(a1>0, 1, 0.01))
    elif activation_func == 'elu':
        alpha = 1
        dL_dz2 = np.multiply(dL_da2, np.where(a2>0, 1, alpha*np.exp(a2)))
        dL_dz1 = np.multiply(np.dot(dL_dz2.T, W2).T, np.where(a1>0, 1, alpha*np.exp(a1)))
    else:
        raise 'Unrecognized activation function'
        
    dL_dW2 = m * np.dot(dL_dz2, a1.T) + m * reg_lambda * W2
    dL_db2 = m * np.sum(dL_dz2, axis=1, keepdims=True)
    dL_dW1 = m * np.dot(dL_dz1, X) + m * reg_lambda * W1
    dL_db1 = m * np.sum(dL_dz1, axis=1, keepdims=True)  
    
    return dL_dW1, dL_db1, dL_dW2, dL_db2, dL_dW3, dL_db3
        
###
last_gradients, mew = None, 0.6
###
def gradient_descent(net, X, labels, alpha, iters, suppress_output=False, activation=sigmoid, regularization=None, reg_lambda=0.1):
    """"""
        Use loss function derivatives w.r.t weights to continually improve the
        performace of our network.
    """"""
      
    def update_weights(weights, gradients):
        W1, b1 = weights[0][:, :-1], weights[0][:, -1].reshape(-1, 1)
        W2, b2 = weights[1][:, :-1], weights[1][:, -1].reshape(-1, 1)
        W3, b3 = weights[2][:, :-1], weights[2][:, -1].reshape(-1, 1)
     
        global last_gradients, mew
        last_gradients = [np.copy(g) for g in gradients]
        
        W1 -= alpha * (gradients[0] + reg_lambda * W1) + mew * last_gradients[0]
        W2 -= alpha * (gradients[2] + reg_lambda * W2) + mew * last_gradients[2]
        W3 -= alpha * (gradients[4] + reg_lambda * W3) + mew * last_gradients[4]
        b1 -= alpha * gradients[1] + mew * last_gradients[1]
        b2 -= alpha * gradients[3] + mew * last_gradients[3]
        b3 -= alpha * gradients[5] + mew * last_gradients[5]
        
    new_cost, old_cost = log_loss(forward(net, X, activation), labels, regularization, reg_lambda), float('inf')    
    beginning = start = time()
    count = 0
    
    if not suppress_output:
        print 'Initial cost:', new_cost

    results = []
    while count < max(iters):
        update_weights(net['weights'], backward(net, labels, activation, reg_lambda))
        old_cost = new_cost
        new_cost = log_loss(forward(net, X, activation), labels, regularization, reg_lambda)
        if count in iters:
            elapsed = time() - beginning
            print str(count) + ': ' + str(round(elapsed, 2)) + ' ',
            train_loss = new_cost
            net_copy = {'weights': [np.copy(w) for w in net['weights']], 'activations': []}
            test_loss = log_loss(forward(net_copy, x_test, activation), y_test, regularization, reg_lambda)
            results.append((train_loss, test_loss, elapsed, net_copy['weights']))
        if not suppress_output:
            if time() - start > 10:
                print '\t', new_cost
            start = time()
        count += 1
    print max(iters),
    
    elapsed = time() - beginning
    train_loss = old_cost
    test_loss = log_loss(forward(net, x_test, activation), y_test, regularization, reg_lambda)
    results.append((train_loss, test_loss, elapsed, [np.copy(w) for w in net['weights']]))
        
    if not suppress_output:
        print 'Model has a log loss of {}\n\tachieved in {} iterations in {} seconds with a learning rate of {}'.format(
                train_loss, max(iters), elapsed, alpha)
    
    return results",0.4922119975,
527,encoding the labels,"# Recast port of departure as numerical feature. 
def simplify_embark(data):
    # Two missing values, assign Cherbourg as port of departure.
    data.Embarked = data.Embarked.fillna('C')
    
    le = preprocessing.LabelEncoder().fit(data.Embarked)
    data.Embarked = le.transform(data.Embarked)
    
    return data",0.4908128083,
527,encoding the labels,"for f in [""X0"", ""X1"", ""X2"", ""X3"", ""X4"", ""X5"", ""X6"", ""X8""]:
        lbl = preprocessing.LabelEncoder()
        lbl.fit(list(test_df[f].values)) 
        test_df[f] = lbl.transform(list(test_df[f].values))

test_X = test_df.drop([""ID"", ""eval_set""], axis=1)
dtest = xgb.DMatrix(test_X, feature_names=test_X.columns.values)",0.489503175,
527,encoding the labels,"def toNumeric(data,to):
    if mercariframe[data].dtype == type(object):
        le = preprocessing.LabelEncoder()
        mercariframe[to] = le.fit_transform(mercariframe[data].astype(str))   
toNumeric('name','n_name')
toNumeric('category_name','n_category_name')
toNumeric('brand_name','n_brand_name')
toNumeric('main_category','n_main_category')
toNumeric('subcat_1','n_subcat_1')
toNumeric('subcat_2','n_subcat_2')
mercariframe.head()",0.4886958003,
527,encoding the labels,"def encode_labels(labels):
    
    from sklearn import preprocessing 
    
    le = preprocessing.LabelEncoder()
    
    return le.fit_transform(labels)",0.4868309498,
460,demo pipelines,"def cnn_and_rnn():
    embedding_dimension = 100
    sentence_size = 120
    
    model_cnn = Sequential()

    model_cnn.add(Embedding(input_dim = vocab_size, output_dim = embedding_dimension, input_length = sentence_size))
    model_cnn.add(Reshape((sentence_size, embedding_dimension, 1), input_shape = (sentence_size, embedding_dimension)))
    model_cnn.add(Conv2D(filters = 50, kernel_size = (5, embedding_dimension), strides = (1,1), padding = 'valid'))
    model_cnn.add(GlobalMaxPooling2D())
    
    model_cnn.add(Dense(20))
    model_cnn.add(Activation('relu'))
    print('CNN Branch Architecture ------------------------------------------------------')
    print('------------------------------------------------------------------------------')
    print(model_cnn.summary())
    
    model_rnn = Sequential()
    model_rnn.add(Embedding(vocab_size, embedding_vecor_length, input_length=len(X_train[0])))
    model_rnn.add(LSTM(100))
    model_rnn.add(Dense(20))
    model_rnn.add(Activation('relu'))
    print('RNN Branch Architecture ------------------------------------------------------ ')
    print('------------------------------------------------------------------------------ ')
    print(model_rnn.summary())
    
    model = Sequential()
    model.add(keras.layers.Merge([model_cnn, model_rnn], mode='concat'))
    model.add(Dropout(0.3))
    model.add(Dense(10, activation = 'relu'))
    model.add(Dense(1, activation='sigmoid'))

    adam = optimizers.Adam(lr = 0.001)
    model.compile(loss='binary_crossentropy', optimizer=adam , metrics=['accuracy'])
    print('Merged Model Architecture ---------------------------------------------------- ')
    print('------------------------------------------------------------------------------ ')   
    print(model.summary())
    return model
merged_model = cnn_and_rnn()
merged_model.fit([X_train, X_train], y_train, validation_data=([X_test, X_test], y_test), epochs=5, batch_size=256)",0.4531521797,
460,demo pipelines,"def test_knows_more_than_existence():
    input_geometry = 'geometry.gro'
    topology = 'parameters/topology.top'

    sim = Simulation(name='test_simulation',
                     gro=input_geometry,
                     top=topology)
    assert sim.name == 'test_simulation', 'The name is wrong'
    assert sim.gro == input_geometry, 'The geometry is wrong'
    assert sim.top == topology, 'The topology is wrong'",0.4514454603,
460,demo pipelines,"def RNN():
    top_words=5000
    embedding_vecor_length = 32
    maxlen=100
    
    model = Sequential()
    model.add(Embedding(top_words,embedding_vecor_length, input_length=maxlen))
    model.add(Dropout(0.2))
    model.add(Bidirectional(LSTM(100)))
    model.add(Dropout(0.5))
    model.add(Dense(1, activation = 'sigmoid'))
    
    print(model.summary())
    adam = optimizers.Adam(lr=0.0001)
    model.compile(loss='binary_crossentropy',
                  optimizer=adam,
                  metrics=['accuracy'])

    return model",0.4512605667,
460,demo pipelines,"def my_pipeline(test_size = 0.3, random_state = 0):
    """"""
    The function has the following major steps:
    1. Divide the data into training and testing
    2. Standardize the features and target
    3. Train the model on the training data
    4. Test the model on the testing data
    5. Plot the model on the testing data
    6. Get the accuracy of the model
    7. Print the winning model
    
    Parameters
    ----------
    test_size : the proportion for testing, 0.3 by default
    random_state : the seed used by the random number generator, 0 by default
    """"""
    
    # Divide the data into training and testing
    # Randomly choose 30% of the data for testing
    # Implement me
    X_train, X_test, y_train, y_test = 
    
    # Standardize the features and the target
    # Declare the standard scaler
    std_scaler = StandardScaler()

    # Standardize the training and testing data
    X_train = std_scaler.fit_transform(X_train)
    y_train = std_scaler.fit_transform(y_train.reshape(-1, 1)).reshape(-1)
    X_test = std_scaler.transform(X_test)
    y_test = std_scaler.transform(y_test.reshape(-1, 1)).reshape(-1)
    
    # Declare the linear regression model
    slr = LinearRegression()
    
    # The accuracy of the two models
    scores = []
    
    for feature in [alpha, beta]:
        # Get the index of the feature
        idx = features.index(feature)

        # Train the model on the training set
        slr.fit(X_train[:, idx].reshape(-1, 1), y_train)

        # Test the model on the testing set
        # Implement me
        y_pred =     

        # Plot the model on the testing set
        plt.scatter(X_test[:, idx], y_test, c='steelblue', edgecolor='white', s=70)
        plt.plot(X_test[:, idx], y_pred, color='black', lw=2)  
        plt.xlabel(feature)
        plt.ylabel(target)
        plt.show()

        # Add the accuracy of the model
        scores.append(slr.score(X_test[:, idx].reshape(-1, 1), y_test))
        
    # Get the accuracy of the model
    score_alpha, score_beta = scores
        
    # Model comparsion
    # Print empty line
    print()

    # Print the accuracy
    print('Accuracy for feature ' + alpha + ': %.3f' % score_alpha)
    print('Accuracy for feature ' + beta + ': %.3f' % score_beta)

    # Print empty line
    print()

    # Print the winning model
    if score_alpha > score_beta:
        print('The winning model is the one based on feature: ' + alpha)
    elif score_alpha < score_beta:
        print('The winning model is the one based on feature: ' + beta)
    else:
        print('The two models are equally accurate')",0.4509179592,
460,demo pipelines,"def random_choices():
    randoms = random.choices(list('python'), k=500)
    
    return timeit(stmt='random.choice(randoms)', 
                  setup='import random', 
                  globals=globals())",0.450265646,
460,demo pipelines,"def random_choices():
    randoms = random.choices(list('python'), k=500)
    
    return timeit(stmt='random.choice(randoms)', 
                  setup='import random', 
                  globals=locals())",0.450265646,
460,demo pipelines,"def build_model():
    tensor_input = keras.engine.topology.Input(shape=(101,101,3))
    hidden = keras.layers.Conv2D(64, 3, strides=(1, 1), padding='valid',activation=""relu"",kernel_initializer = 'random_normal')(tensor_input)
    hidden = keras.layers.Conv2D(128, 3, strides=(1, 1), padding='valid',activation=""relu"",kernel_initializer = 'random_normal')(hidden)
    hidden = keras.layers.Conv2D(256, 3, strides=(1, 1), padding='valid',activation=""relu"",kernel_initializer = 'random_normal')(hidden)
    hidden = keras.layers.Conv2D(128, 3, strides=(1, 1), padding='valid',activation=""relu"",kernel_initializer = 'random_normal')(hidden)
    hidden = keras.layers.Conv2D(64, 3, strides=(1, 1), padding='valid',activation=""relu"",kernel_initializer = 'random_normal')(hidden)

    flat = keras.layers.Flatten()(hidden)
    
    tensor = keras.layers.Dense(2,activation=""softmax"",kernel_initializer = 'random_normal')(flat)

    model = keras.models.Model(inputs=tensor_input, outputs=tensor)
    model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
    return model

model = build_model()
model.summary()",0.4495802224,
460,demo pipelines,"def build_simple_model():
    model = Sequential()
    model.add(Conv2D(32, (3, 3), input_shape = (150, 150, 3)))
    model.add(ELU())
    model.add(MaxPooling2D(pool_size=(2, 2)))

    
    model.add(Conv2D(32, (3, 3)))
    model.add(ELU())
    model.add(MaxPooling2D(pool_size=(2, 2)))

    model.add(Conv2D(64, (3, 3)))
    model.add(ELU())
    model.add(MaxPooling2D(pool_size=(2, 2)))

    model.add(Flatten())  
    model.add(Dense(64))
    model.add(ELU())
    model.add(Dropout(0.5))
    model.add(Dense(1, activation='sigmoid'))


    model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])
    return model",0.4484875798,
460,demo pipelines,"def make_discriminator():
    model = Sequential()
    model.add(Convolution2D(64, (5, 5), padding='same', input_shape=(32, 32, 1)))
    model.add(LeakyReLU())
    model.add(Convolution2D(128, (5, 5), kernel_initializer='he_normal', strides=[2, 2]))
    model.add(LeakyReLU())
    model.add(Convolution2D(128, (5, 5), kernel_initializer='he_normal', padding='same', strides=[2, 2]))
    model.add(LeakyReLU())
    model.add(Flatten())
    model.add(Dense(1024, kernel_initializer='he_normal'))
    model.add(LeakyReLU())
    model.add(Dense(1, kernel_initializer='he_normal'))
    return model

discriminator = make_discriminator()
discriminator.summary()",0.4466308355,
460,demo pipelines,"def create_model():
    ch, row, col = 3, 160, 320  # camera format

    model = Sequential()
    model.add(Lambda(lambda x: x/127.5 - 1.,
                     input_shape=(row, col, ch),
                     output_shape=(row, col, ch)))
    model.add(Cropping2D(cropping=((70, 25), (0, 0))))
    model.add(Conv2D(24, (5, 5), strides=(2, 2), padding=""same""))
    model.add(ELU())
    model.add(MaxPooling2D(pool_size=(2,2)))
    model.add(Conv2D(36, (5, 5), strides=(2, 2), padding=""same""))
    model.add(ELU())
    model.add(MaxPooling2D(pool_size=(2,2)))
    model.add(Conv2D(48, (5, 5), strides=(2, 2), padding=""same""))
    model.add(ELU())
    #model.add(MaxPooling2D(pool_size=(2,2)))

    model.add(Conv2D(64, (3, 3), strides=(1, 1), padding=""same""))
    model.add(ELU())
    model.add(MaxPooling2D(pool_size=(2,2)))
    model.add(Conv2D(64, (3, 3), strides=(1, 1), padding=""same""))
    model.add(ELU())
    #model.add(MaxPooling2D(pool_size=(2,2)))

    model.add(Flatten())
    model.add(Dropout(.2))
    model.add(Dense(512))
    model.add(Dropout(.5))
    model.add(ELU())
    model.add(Dense(128))
    model.add(Dropout(.5))
    model.add(ELU())
    model.add(Dense(1))

    model.compile(optimizer=""adam"", loss=""mse"")

    return model",0.4462347031,
1406,part expectation maximization,"def apply_boundary(T,nx,ny):
    T[:,0] = 100 # Left boundary
    T[:,-1] = 100*np.sin(np.arange(0,nx)/nx) # Right boundary
    T[0,:] = 0 # Top boundary
    T[-1,:] = 0 # Bottom boundary
    
    
# We modify T in place. If you hate side effects, 
# make a copy of T at the beginning, 
# and change the return statements to return T
def solveLaplace(T,nx,ny,epsilon=1e-1,maxiter=1000):
    apply_boundary(T,nx,ny)
    TN = T.copy()
    for i in range(maxiter):
        # We'll be a bit clever with numpy indexing here
        T[1:-1,1:-1] = (T[0:-2,1:-1]+T[2:,1:-1]+T[1:-1,0:-2]+T[1:-1,2:])/4.
        apply_boundary(T,nx,ny)
        err = np.abs(TN - T)
        
        if np.max(err) < epsilon:
            print (""Reached convergence after"",i+1,""iterations"")
            print (""(max error: %g)""%(np.max(err)))
            return
        TN = T.copy()
    print (""No convergence after"",i+1,""iterations"")
    print (""(max error: %g)""%(np.max(err)))
    return

def plot_and_solve(nx=20,ny=20):
    T = np.zeros((nx,ny))

    
    fig = plt.figure()
    plt.imshow(T,interpolation='none',origin='lower')
    apply_boundary(T,nx,ny)
    fig = plt.figure()
    plt.imshow(T,interpolation='none',origin='lower')
    fig = plt.figure()
    solveLaplace(T,maxiter=1000,nx=nx,ny=ny)
    plt.imshow(T,interpolation='None',origin='lower')",0.4569172859,
1406,part expectation maximization,"def apply_boundary(T,nx,ny):
    T[:,0] = 100 # Left boundary
    T[:,-1] = 100*np.sin(np.arange(0,nx)/nx) # Right boundary
    T[0,:] = 0 # Top boundary
    T[-1,:] = 0 # Bottom boundary
    T[10,10]=100
plot_and_solve()",0.4565885067,
1406,part expectation maximization,"#----------------------------------------------------------------------------------
# spread_spectrum(img):                applies a histogram equalization transformation
#
# img:                                 a single scan
#
# returns:                             a transformed scan
#----------------------------------------------------------------------------------

def spread_spectrum(img):
    img = stats.threshold(img, threshmin=12, newval=0)
    
    # see http://docs.opencv.org/3.1.0/d5/daf/tutorial_py_histogram_equalization.html
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
    img= clahe.apply(img)
    
    return img",0.4555805922,
1406,part expectation maximization,"def optimizeTheta(theta, X, y):
    result = optimize.fmin(computeCost, x0=theta, args=(X, y), maxiter=400, full_output=True)
    return (result[0].reshape(-1,1))",0.4545201659,
1406,part expectation maximization,"# e-greedy policy
class eGreedyPolicy:
    
    # initializing
    def __init__(self, epsilon, buffer_size):
        
        # saving parameters
        self.epsilon = epsilon
        self.buffer_size = buffer_size
    
    # choice of bandit
    def choose_bandit(self, k_array, reward_array, n_bandits):
        
        # number of plays
        n_plays = int(k_array.sum())
        
        # limiting the size of the buffer
        reward_array = reward_array[:,:n_plays][:,-self.buffer_size:]
        k_array = k_array[:,:n_plays][:,-self.buffer_size:]
       
        # sucesses and failures
        success_count = reward_array.sum(axis=1)
        total_count = k_array.sum(axis=1)
         
        # ratio of sucesses vs total
        success_ratio = success_count/total_count
        
        # choosing best greedy action or random depending with epsilon probability
        if np.random.random() < self.epsilon:
            
            # returning random action, excluding best
            return np.random.choice(np.delete(list(range(N_BANDITS)), np.argmax(success_ratio)))
        
        # else return best
        else:
            
            # returning best greedy action
            return np.argmax(success_ratio)",0.4515570402,
1406,part expectation maximization,"class Perceptron(object):
    
    def __init__(self, n, lr = 0.01, epochs = 100):
        self.lr = lr
        self.epochs = epochs
        self.n = n
    
    def train(self):
        X, y = self.prepData()
        #self.w = np.random.rand(1, len(X[1])+1)
        #self.w  = [(x-0.5) for x in self.w]
        #print len(self.w)
        #self.w = self.w[0]
        self.w = np.random.uniform(-0.5, 0.5, len(X[1])+1)
        self.errors = []
        for _ in range(0, self.epochs):
            error = 0
            for x, target in zip(X, y):
                #print x
                coefUpd = self.lr *(target - self.pred(x))
                self.w[1:] += x*coefUpd
                self.w[0] +=coefUpd
                
                error +=int(coefUpd != 0.0) 
            self.errors.append(error)
            if(_%25 == 0):
                print (""Epoch: "" + str(_) +"", Error: ""+str(error))
            
        return self
    
    def test(self):
        mat = [[0,0],[0,0]]
        dt = pd.read_csv(""mnist_test.csv"")
        y = dt.iloc[:,0].values
        y = np.where(y!=self.n, 0, 1)
        X = dt.iloc[:,1:].values
        
        for x, i in zip(X,y):
            if int(self.pred(x)) == i and i == 1: 
                mat[0][0]+=1
            elif int(self.pred(x)) == i and i == 0:
                mat[1][1]+=1
            elif int(self.pred(x)) != i and i == 1:
                mat[0][1]+=1
            elif int(self.pred(x)) != i and i == 0:
                mat[1][0]+=1
        
        return mat
        
                
    def step(self, dp):
        return np.where(dp >= 0, 1, 0)
    
    def dp(self, X):
        return np.dot(X,self.w[1:])+self.w[0]
    
    def pred(self, xi):
        return self.step(self.dp(xi))
    def predN(self, xi):
        return self.dp(xi)
    
    def prepData(self):
        tr = pd.read_csv(""mnist_train.csv"")
        #for some reason pandas give me the number 5 as the index to first
        #column my guess is 5 appears more in the data
        y = tr.iloc[:,0].values
        labels = y
        X = tr.iloc[:,1:].values
        y = np.where(y!=self.n, 0, 1)
        ls = [[]]
        ly = []
        print ""Preparing it""
        for i,l in zip(y,X):
            if(i==1):
                ls.append(list(l))
                ly.append(int(i))
        ls.remove([])
      
        print ""Extending it""
        for i in range(0,8):
            X = np.append(X, ls,axis = 0)
            y = np.append(y, ly, axis = 0)
            c = list(zip(X, y))
            random.shuffle(c)
            X, y = zip(*c)
        
        return X, y",0.4502897859,
1406,part expectation maximization,"def guide(data):
    
    # register the two variational parameters with Pyro
    # - mu parameter will have initial value 0
    # - sigma parameter will have initial value 1
    # - because we invoke constraints.positive, the optimizer
    # will take gradients on the unconstrained parameters
    # (which are related to the constrained parameters by a log)
    
    mu_q = pyro.param(""mu_q"", torch.tensor(0.))
    
    sigma_q = pyro.param(""sigma_q"", torch.tensor(1.),
                        constraint=constraints.positive)
    
    # sample latent_fairness from the distribution Beta(alpha_q, beta_q)
    pyro.sample(""latent_variable"", dist.Normal(mu_q, sigma_q))",0.4492458701,
1406,part expectation maximization,"for i in (20,40,100):
    m = GPy.models.SparseGPRegression(X_train, y_train, num_inducing=i)
    m.optimize('bfgs')
    posteriorYtestS = m.posterior_samples_f(X_test)
    mean_y_test_sparse,_ = m.predict(X_test)
    print ('For {} inducing points R2 score is {}'.format(i, r2_score(y_test, mean_y_test_sparse)))",0.4486726522,
1406,part expectation maximization,"# this is an example as a reduced version of the pytorch internal RMSprop optimizer
class RMSprop(torch.optim.Optimizer):
    def __init__(self, params, lr=1e-4, alpha=0.9, eps=1e-8):
        defaults = dict(lr=lr, alpha=alpha, eps=eps)
        super(RMSprop, self).__init__(params, defaults)

    def step(self):
        for group in self.param_groups:
            for p in group['params']:
                grad = p.grad.data
                state = self.state[p]

                # State initialization
                if len(state) == 0:
                    state['square_avg'] = torch.zeros_like(p.data)

                square_avg = state['square_avg']
                alpha = group['alpha']

                # update running averages
                square_avg.mul_(alpha).addcmul_(1 - alpha, grad, grad)
                avg = square_avg.sqrt().add_(group['eps'])

                # gradient update
                p.data.addcdiv_(-group['lr'], grad, avg)",0.4456580877,
1406,part expectation maximization,"for r in np.arange(0,6.5,0.5):
    with mcaps:
        mcaps.reactions.EX_nh4_e.bounds = -1000, 0
        mcaps.reactions.PMMOipp.bounds = 0, 1000
        mcaps.reactions.PMMODCipp.bounds = 0, 1000
        mcaps.reactions.GLNS.bounds = 0, 0
        mcaps.reactions.ALAD_L.bounds = 0, 1000
        c3 = mcaps.problem.Constraint(
                mcaps.reactions.PMMOipp.flux_expression - 
                r* mcaps.reactions.PMMODCipp.flux_expression, lb=0, ub=0, name='c3')
        mcaps.add_cons_vars(c3)
        solution = pfba(mcaps)
        print(solution.fluxes.MEOHDHipp,solution.fluxes.PMMODCipp, solution.fluxes.PMMOipp)

    df = df_record(solution, df, 'NH4, pMMODC, RedEff {}'.format(r))",0.445402801,
375,create_interests_graph,"def most_similar_interests_to(interest_id):
    similarities = interest_similarities[interest_id]
    pairs = [(unique_interests[other_interest_id], similarity)
             for other_interest_id, similarity in enumerate(similarities)
             if interest_id != other_interest_id and similarity > 0]
    return sorted(pairs,
                  key=lambda (_, similarity): similarity,
                  reverse=True)",0.4954507351,
375,create_interests_graph,"def simple_graph():

    G = nx.DiGraph()

    G.add_edges_from(
        [('A', 'B'), ('A', 'C'), ('H', 'A'), ('G', 'A'), ('F', 'A')])

    val_map = {'A': 1.0,
               'D': 0.5714285714285714,
               'H': 0.0}

    values = [val_map.get(node, 0.25) for node in G.nodes()]

    pl.axis('off')
    nx.draw_networkx(G, node_color = 'white')

    return G",0.4783402085,
375,create_interests_graph,"def createGraph(p, num_children_min, num_children_max):
    while True:
        print '.',
        structure = buildHierarchicalStructure(p)
        graph = FSBGraph()
        graph.start_node, graph.end_node = structure.generate_graph(graph)
        if num_children_min <= graph.num_children <= num_children_max:
            print
            break
    pprint (structure)
    return graph",0.4720183611,
375,create_interests_graph,"def pull_data():
    symblist=['AAPL','GOOG','NFLX']
    price=[]
    date= datetime.datetime.utcnow()
    for sym in symblist:
    
        url = ""http://finance.yahoo.com/q;_ylt=AqAeVPItbXuQ1azBg40Rso3FgfME?uhb=uhb2&fr=uh3_finance_vert_gs_ctrl2_e&type=2button&s=""+sym
    
        htmlfile=urllib.urlopen(url)
        htmltext=htmlfile.read()
                        )    
        regex='<span id=""yfs_l84_'+sym.lower()+'"">(.+?)</span>'
        pattern=re.compile(regex)
        price.append(re.findall(pattern,htmltext))
        
    return date, price",0.471983254,
375,create_interests_graph,"def findMinimumSpanningTree(partition):
    
    # The graph (Vertices plus Edges) for storing minimum spanning tree
    minimumSpanningGraph = {
        'vertices': partition.graph['vertices'],
        'edges': []
    }
    
    # Sorting 
    partition.graph[""edges""] = sorted(partition.graph['edges'], key=lambda z:z[0])
    
    # Creating minimum spanning tree 
    for i in partition.graph['edges']:
        if partition.find(i[1]) != partition.find(i[2]):
            partition.union(i[1], i[2])
            minimumSpanningGraph['edges'].append(i)
            # Visualising this step
            visualize(minimumSpanningGraph)
    return minimumSpanningGraph",0.4662895203,
375,create_interests_graph,"def delete_note_in_graph():
    G = nx.Graph()
    
    # add 3 egdes
    G.add_edge('A','B')
    G.add_edge('B','C')
    G.add_edge('C','A')
    
    # draw the graph
    nx.draw(G,with_labels=True,node_color='y',node_size=800)
    
    #remove note B from the graph
    G.remove_node('B')
    
    nx.draw(G,with_labels=True,node_color='y',node_size=800)
    
delete_note_in_graph()",0.4649927616,
375,create_interests_graph,"# define our function called simple-graph

def simple_graph():
    #create our empty graph
    G = nx.Graph()
    
    #add edges 
    G.add_edge('A','B')
    G.add_edge('B','C')
    G.add_edge('C','A')
    
    # draw the graph
    nx.draw(G,with_labels=True,node_color='y',node_size=800)",0.4649927616,
375,create_interests_graph,"# hv Dataframe buffer for plotting eigenvalues

def eigenvalue_plot():
    eigenvalue_estimates = pd.DataFrame({'iteration': [], 'eigenvalue': [], 'diff': []}, columns=['iteration', 'eigenvalue', 'diff'])
    eigenvalue_stream    = Buffer(eigenvalue_estimates, length=100, index=False)

    def plot(data):
        h = hv.Curve(data, 'iteration', 'eigenvalue', label='Evolution of the Eigenvalue')+\
            hv.Curve(data, 'iteration', 'diff',       label = 'Change in the Eigenvector')
        return h

    h =\
    hv.DynamicMap( plot, streams=[eigenvalue_stream] )\
      .relabel('Evolution of the Eigenvalue')

    return eigenvalue_stream, h",0.4629475176,
375,create_interests_graph,"def DFS(G, i):

    # mark i as explored
    d[i]['explored'] = 1
    # set leader of i as node S
    #global S
    d[i]['leader'] = S
    for j in G[i]:
        if j == 'x': # We have reached a ghost node; meaning 'i' is a sink node. No further recursion to be done.
            continue
        else:
            if d[j]['explored'] <>1:
                DFS(G,j)

    global t
    t = t + 1
    d[i]['t'] = t
    #print 'Node: ', i, 't: ', t, 'S: ', S",0.4625173211,
375,create_interests_graph,"def DFS_Loop(G):
    # define the global variables

    # Create a dictionary for book keeping. This dictionary will keep track of: 
    # - whether a node has been explored or not
    # - the finishing times of each node in the first Pass of DFS on reversed graph G_rev
    # - the leader node for each node in second pass of DFS on the forward graph G
    global t
    t = 0
    global S 
    S = ''
    if is_first_pass == 1:
        #create the dict for book keeping
        for key in G.keys():
            d[key] = {'explored':0, 't':0, 'leader':''}
        
        # In the first pass the order of picking the nodes doesn't matter
        sort_key = None
    else:
        # mark all nodes as unexplored
        for key in G.keys():
            d[key]['explored']= 0
        # in the second pass, the nodes must be picked in the reverse order of their finishing times 't' 
        sort_key = lambda x: d[x]['t']
        
    for i in sorted(G.keys(), key = sort_key, reverse = True):
        if i == 'x':
            continue
        else:
            if d[i]['explored'] <> 1:
                S = i
                DFS(G,i)",0.462115258,
2126,step present general statistics by company,"def summed_mutation_overview(period):
    """"""Sums all mutations per account_to and adds name to each account.
    
    df: Entire history in a DataFrame.
    start: DateTime for start (inclusive) of the period.
    end: DateTime for end (inclusive) of the period.
    Returns: DataFrame with columns 'account_to, name, amount'. Amounts
        are summed by account_to. The first name found for an account_to
        in the df is assigned as a value in the respective name column. 
        The only exception to this is the account_to with value '-', this
        gets the name '-'.
    """"""
    # Combine amounts with respect to the account_to and add a name.
    accountto_amount = period[['account_to', 'amount']].groupby('account_to', as_index=False).sum()
    accountto_name = period[['account_to', 'name']].groupby('account_to', as_index=False).first()
    overview = pd.merge(accountto_name, accountto_amount, how='left', on='account_to')
    # Assigning name '-' to account_to == '-'.
    overview.loc[overview.account_to == '-', 'name'] = '-'
    # Prettify and sort.
    overview = overview.sort_values(by=['amount'])
    overview.amount = overview.amount.apply(amount_to_str)
    return overview",0.5176491141,
2126,step present general statistics by company,"def grp_plot(df):
    grp = df[['yearID','playerID']].groupby('yearID', as_index=False).count()
    plt.scatter(grp['yearID'],grp['playerID'])
    plt.title('Eligible rookies by year ({} total rookies)'.format(df.shape[0]))",0.5169004798,
2126,step present general statistics by company,"def SurgeryTE(self):
    #reassign the IOP based on the normal distribution reported by van Gestel
    self.Attribute['IOP'] = random.normal(12.5,0.3)
    self.medicalRecords['ContinueTreatment'] = False
    self.medicalRecords['OnTrabeculectomy'] = True
    self.medicalRecords['MedicationIntake'] = 0
    self.medicalRecords['NumberTrabeculectomy'] +=1
    self.medicalRecords['CurrentMedicationType'] = 30
    self.medicalRecords['TreatmentOverallStatus'] = 0
    #nullify any side effect left as patients no longer on medication
    self.params['SideEffect'] = 0",0.5139104128,
2126,step present general statistics by company,"# for s in sorted(list(df['TypeOfEstablishment'].unique())):
#     print s
# print

def collapse_types(df):
    df['school_type'] = df['TypeOfEstablishment']
    df.ix[df.TypeOfEstablishment.astype(str).str[:7]=='Academy', 'school_type'] ='Academy'
    df.ix[df.TypeOfEstablishment.astype(str).str[:9]=='Community', 'school_type'] ='Community'
    df.ix[df.TypeOfEstablishment.astype(str).str[:10]=='Foundation', 'school_type'] ='Foundation'
    df.ix[df.TypeOfEstablishment.astype(str).str[:4]=='Free', 'school_type'] ='Free'
    df.ix[df.TypeOfEstablishment.astype(str).str[:5]=='Other', 'school_type'] ='Independent'
    df.ix[df.TypeOfEstablishment.astype(str).str[:9]=='Voluntary', 'school_type'] ='Voluntary'
    return df
df = collapse_types(df)

## Create table
sch_type = []
edubase_tot = []
url_tot = []
scrape_tot = []
url_perc = []
blurb_tot = []
blurb_perc = []
blurb_perc_of_urls = []
med_tot = []
med_perc = []
for s in sorted(list(df['school_type'].unique())):
    sub = df[df.school_type==s]
    sch_type.append(s)
    edubase_tot.append(len(sub))
    url_tot.append(sum(sub.SchoolWebsite.notnull()))
    scrape_tot.append(sum(sub['scraped_dataset']==True))
    url_perc.append(float(sum(sub['scraped_dataset']==True))/float(len(sub)))
    blurb_tot.append(sum(sub['flag']=='Found_blurb'))
    blurb_perc.append(float(sum(sub['flag']=='Found_blurb'))/float(len(sub)))
    blurb_perc_of_urls.append(float(sum(sub['flag']=='Found_blurb'))/float(sum(sub['scraped_dataset']==True)))
    sub_blurb = sub[sub['flag']=='Found_blurb']
    med_tot.append(len(sub_blurb[(sub_blurb['length']>=250) & (sub_blurb['length']<=4000)]))
    med_perc.append(float(len(sub_blurb[(sub_blurb['length']>=250) & (sub_blurb['length']<=4000)]))/float(len(sub)))
df_sch_type = pd.DataFrame({'sch_type': sch_type})
df_sch_type['edubase_tot'] = edubase_tot
df_sch_type['url_tot'] = url_tot
df_sch_type['scrape_tot'] = scrape_tot
df_sch_type['url_perc'] = url_perc
df_sch_type['blurb_tot'] = blurb_tot
df_sch_type['blurb_perc'] = blurb_perc
df_sch_type['blurb_perc_of_urls'] = blurb_perc_of_urls
df_sch_type['med_tot'] = med_tot
df_sch_type['med_perc'] = med_perc
df_sch_type",0.5112928748,
2126,step present general statistics by company,"def P_label_log(df):
    '''Function split our data and create an article performance column'''
    
    df[""Performance""] = 0
    
    df.loc[(df['articleId_totalViews_7days_log'] > 0) & (df['articleId_totalViews_7days_log']\
        <= 4.76), ['Performance']] = 1
    df.loc[(df['articleId_totalViews_7days_log'] > 4.76) & (df['articleId_totalViews_7days_log']\
        <= 8), ['Performance']] = 2
    df.loc[(df['articleId_totalViews_7days_log'] > 8) & (df['articleId_totalViews_7days_log']\
        <= 10.57), ['Performance']] = 3
    df.loc[(df['articleId_totalViews_7days_log'] > 10.57), ['Performance']] = 4
    
    return df",0.5090557933,
2126,step present general statistics by company,"def default_missing(results):
    red_states = [""Alabama"", ""Alaska"", ""Arkansas"", ""Idaho"", ""Wyoming""]
    blue_states = [""Delaware"", ""District of Columbia"", ""Hawaii""]
    results.loc[red_states, [""poll_mean""]] = -100.0
    results.loc[red_states, [""poll_std""]] = 0.1
    results.loc[blue_states, [""poll_mean""]] = 100.0
    results.loc[blue_states, [""poll_std""]] = 0.1
default_missing(avg)
avg.head()",0.5041394234,
2126,step present general statistics by company,"def prepare_matrix(m):
    ''' just copy-pasted from above '''
    m['facility_type'] = m.facility_type.fillna('missing')
    m = m.fillna(0)
    m = pd.get_dummies(m)
    return m",0.5036194921,
2126,step present general statistics by company,"# generate dataframe of station info by user type 
def get_trip_counts_by_user_type(user_type):
    locations = trip_data_half.groupby(""End Station ID"").first()
    locations = locations.loc[:, [""End Station Latitude"",
                                 ""End Station Longitude"",
                                 ""End Station Name""]]
    
    subset = trip_data_half[trip_data_half['User Type']==user_type]
    
    # count trips for each origin
    departure_counts = subset.groupby('Start Station ID').count().iloc[:, [0]]
    departure_counts.columns = ['Departure Count']
    
    # count trips for each destination
    arrival_counts = subset.groupby('End Station ID').count().iloc[:, [0]]
    arrival_counts.columns= [""Arrival Count""]
    
    # join dataframes 
    trip_counts = arrival_counts.join(departure_counts).join(locations)
    trip_counts.columns = ['Arrival Count',  'Departure Count',
                           'Latitude', 'Longitude', 
                         'Station Name']
    
    # fill NaN with 0 where 
    trip_counts['Departure Count'].fillna(0, inplace=True)
    return trip_counts.sort_values(by='Arrival Count', ascending=False)",0.5013452768,
2126,step present general statistics by company,"%%time

def plot_traffic(df):
    fig = plt.figure(figsize=[12, 12])

    df_traffic = df.loc[df.traffic == 1]
    df_clear = df.loc[df.traffic == 0]

    plot_centre = [df.lng.mean(), df.lat.mean()]
    plot_range = [df.lng.var()**0.4, df.lat.var()**0.4]

    plt.scatter(df_clear.lng, df_clear.lat, alpha=0.1, s=0.1, color='b')
    plt.scatter(df_traffic.lng, df_traffic.lat, alpha=0.9, s=0.5, color='r')
    plt.title('Scatter plot showing traffic observed in the dataset')
    plt.axis('off')
    plt.xlim([plot_centre[0] - plot_range[0], plot_centre[0] + plot_range[0]])
    plt.ylim([plot_centre[1] - plot_range[1], plot_centre[1] + plot_range[1]])
    plt.show()
    
plot_traffic(df)",0.5009480119,
2126,step present general statistics by company,"def extract_udp_features(df):
    forward_data = df.groupby(""type"").get_group(""client"").groupby(""ip.dst"")
    backward_data = df.groupby(""type"").get_group(""server"").groupby(""ip.src"")
    index = range(0,len(backward_data))
    
    fpackets = forward_data[""ip.dst""].count()
    fbytes = forward_data[""udp.length""].sum() # Bytes in Forward Direction 
    min_fiat = forward_data[""frame.time_delta""].min() # Min Forward Inter-Arrival Time
    max_fiat = forward_data[""frame.time_delta""].max() # Max Forward Inter-Arrival Time
    std_fiat = forward_data[""frame.time_delta""].std()  # Standard Deviation of Forward Inter-Arrival Time 
    mean_fiat = forward_data[""frame.time_delta""].mean()  # Mean Forward Inter-Arrival Time
    min_fpkt = forward_data[""udp.length""].min() # Min Forward Packet Length
    max_fpkt = forward_data[""udp.length""].max() # Max Forward Packet Length
    mean_fpkt = forward_data[""udp.length""].mean() # Mean Forward Packet Length
    std_fpkt = forward_data[""udp.length""].std() # # Std. Forward Packet Length
    bpackets = backward_data[""ip.src""].count() # Packets in Backward Direction
    bbytes = backward_data[""udp.length""].sum() # Bytes in Backward Direction
    min_biat = backward_data[""frame.time_delta""].min() # Min Backward Inter-Arrival Time
    max_biat = backward_data[""frame.time_delta""].max() # Max Backward Inter-Arrival Time
    std_biat = backward_data[""frame.time_delta""].std()  # Std. Backward Inter-Arrival Time 
    mean_biat = backward_data[""frame.time_delta""].mean()  # MeanBackward Inter-Arrival Time
    min_bpkt = backward_data[""udp.length""].min() # Min Backward Packet Length
    max_bpkt = backward_data[""udp.length""].max() # Max Backward Packet Length
    mean_bpkt = backward_data[""udp.length""].mean() # Mean Backward Packet Length
    std_bpkt = backward_data[""udp.length""].std() # Std. Backward Packet Length
        
    d = {""fpackets"":fpackets,""fbytes"":fbytes,
         ""min_fiat"":min_fiat,""max_fiat"":max_fiat,
         ""std_fiat"":std_fiat,""mean_fiat"":mean_fiat,
         ""min_fpkt"":min_fiat,""max_fpkt"":max_fpkt,
        ""mean_fpkt"":mean_fpkt,""std_fpkt"":std_fpkt,
         ""bpackets"":bpackets,""bbytes"":bbytes,
        ""min_biat"":min_biat,""max_biat"":max_biat,
         ""std_biat"":std_biat,""mean_biat"":mean_biat,
        ""min_bpkt"":min_bpkt,""max_bpkt"":max_bpkt,
         ""mean_bpkt"":mean_bpkt,""std_bpkt"":std_bpkt,""protocol"":""udp""}
    
    df = pd.DataFrame(d)
    return df.dropna()",0.5006633997,
899,importing data with pandas,"import pandas as pd
def convert_df(datatlist):
    df = pd.DataFrame(datatlist)
    df.columns = ['index', 'actual', 'generated']
    return df",0.5028969049,
899,importing data with pandas,"import dataiku
from dataiku import pandasutils as pdu
import pandas as pd
import numpy as np",0.4996708035,
899,importing data with pandas,"import pandas as pd
features = pd.DataFrame(irisdata['data'])
features.columns = irisdata['feature_names']
targets = pd.DataFrame(irisdata['target'])
targets = targets.replace([0,1,2],irisdata['target_names'])",0.4985865355,
899,importing data with pandas,"data = load_diabetes()

df_data = DataFrame(data=data['data'], columns = data['feature_names']).dropna()",0.4948666096,
899,importing data with pandas,"####Check The Data Set
import dataiku
from dataiku import pandasutils as pdu
import pandas as pd
# Example: load a DSS dataset as a Pandas dataframe
train_dataset = dataiku.Dataset(""train"")
train_df = train_dataset.get_dataframe()",0.4896866083,
899,importing data with pandas,"data = [('ABC', 'DEF'), ('DEF', 'XYZ')]

df = pd.DataFrame.from_records(data)

niceCx_df_2_column = NiceCXNetwork(pandas_df=df)

print(niceCx_df_2_column.getSummary())

df_2_col_from_niceCx = niceCx_df_2_column.to_pandas()
print(df_2_col_from_niceCx)",0.488281399,
899,importing data with pandas,"data = load_breast_cancer()

df_data = DataFrame(data=data['data'], columns = data['feature_names']).dropna()
df_target = DataFrame(data=data['target'])",0.4878604114,
899,importing data with pandas,"data = load_diabetes()

df_data = DataFrame(data=data['data'], columns = data['feature_names']).dropna()
df_target = DataFrame(data=data['target'])",0.4878604114,
899,importing data with pandas,"import pandas as pd

#Observations
X = pd.DataFrame(iris.data)
#Targets
y= pd.DataFrame(iris.target)",0.4865905643,
899,importing data with pandas,"import pandas as pd
import numpy as np

data['shooting_victims_df'] = pd.DataFrame(data['shooting_victims'])
data['homicides_df'] = pd.DataFrame(data['homicides'])",0.4862297177,
2304,tensorflow project exercise,"with tf.Session() as sess:
     saver.restore(sess, '/tmp/data/final.ckpt')
     codings_train = codings.eval(feed_dict={X:X_train})
     codings_test = codings.eval(feed_dict={X:X_test})
     outputs_test = outputs.eval(feed_dict={X:X_test})
     
     plt.subplot(131)
     plt.imshow(X_test[0].reshape(28, 28), cmap='binary'); plt.axis('off')
     plt.subplot(132)
     plt.imshow(codings_test[0].reshape(14, 14), cmap='binary'); plt.axis('off')
     plt.subplot(133)
     plt.imshow(outputs_test[0].reshape(28, 28), cmap='binary'); plt.axis('off')",0.5933618546,
2304,tensorflow project exercise,"#make external predictions on the test_dat
with tf.Session() as sess:
    saver.restore(sess, ""./cams_model_final.ckpt"") # or better, use save_path
    Z = logits.eval(feed_dict={X: test_dat}) #switched from outputs to logits
    y_pred = Z[:,1]",0.5886415839,
2304,tensorflow project exercise,"with tf.Session() as sess:
    softmax_tensor = sess.graph.get_tensor_by_name('softmax:0')
    predictions = sess.run(
        softmax_tensor,
        {'DecodeJpeg/contents:0': panda_data}
    )
    predictions = np.squeeze(predictions)",0.5870559216,
2304,tensorflow project exercise,"with tf.Session() as sess:
    saver.restore(sess, ""./LSTM-Shreyas"")
    pr_1 = sess.run(prediction, {X: np.reshape(train_s[1],(-1,65,513)), Y: np.reshape(IBM_Targets[1],(-1,65,513)), keep_prob: 0.9})
    recons1_matrix = np.transpose(np.reshape(np.multiply(pr_1, complex_s[1]), (65,513)))",0.5862370729,
2304,tensorflow project exercise,"with tf.Session() as sess:
    saver.restore(sess, ""./my_model_final.ckpt"") # or better, use save_path
    X_new_scaled = test_mnist
    Z = logits.eval(feed_dict={X: X_new_scaled})
    y_pred = np.argmax(Z, axis=1)",0.5857842565,
2304,tensorflow project exercise,"num_top_predictions = 3

with tf.Session() as sess:
    softmax_tensor = sess.graph.get_tensor_by_name('softmax:0')
    predictions = sess.run(
        softmax_tensor,
        {'DecodeJpeg/contents:0': image_data}
    )
    predictions = np.squeeze(predictions)

    node_lookup = NodeLookup()

    top_k = predictions.argsort()[-num_top_predictions:][::-1]
    for node_id in top_k:
        human_string = node_lookup.id_to_string(node_id)
        score = predictions[node_id]
        print('{} (score = {:.5f})'.format(human_string, score))",0.5824516416,
2304,tensorflow project exercise,"### Visualize the softmax probabilities here.
### Feel free to use as many code cells as needed.
with tf.Session() as sess:
#     sess.run(tf.global_variables_initializer())
    saver.restore(sess,os.getcwd()+'/tmp/model.ckpt')
    prob = sess.run([softmax_prob],feed_dict={y:real_set , keep_prob:1})
    print(prob)",0.5805158615,
2304,tensorflow project exercise,"with tf.Session() as sess:
    # Restore variables from disk.
    saver.restore(sess, ""models/model.ckpt"")
    print(""Model restored."")

    acc = sess.run(accuracy, feed_dict={x:test_corpus_embed, y:test_label})
    print(""Test acc: %g"" % (acc))",0.5795121193,
2304,tensorflow project exercise,"with tf.Session() as sess:
    saver.restore(sess, './model.chkpt')
    print(""Model restored."")
    values = sess.run(y_pred, feed_dict={x: X_test[1000:1005],
                y: y_test[1000:1005]})
    print(values[0])",0.5795121193,
2304,tensorflow project exercise,"content_layer = 'vgg/conv3_2/conv3_2:0'
with tf.Session(graph = g) as sess:
    content_features = g.get_tensor_by_name(content_layer).eval(
        feed_dict = {x: content_img,
            'vgg/dropout_1/random_uniform:0': [[1.0]],
            'vgg/dropout/random_uniform:0': [[1.0]]})
print(content_features.shape)",0.5793021917,
1098,load in data sets for visualization examples,"exposures = [15, 30]
master_darks = {}
combiners = {}
for exposure in exposures:
    # make list of darks with this exposure time
    a_list = []
    for dark, fname in images.hdus(imagetyp='dark', exptime=exposure, return_fname=True):
        meta = dark.header
        meta['filename'] = fname
        a_list.append(ccdproc.CCDData(data=dark.data, meta=meta, unit=""adu""))

    # get the exposure time as it appears in the fits file for use as a dictionary key
    exposure_time_in_fits_file = a_list[0].header['exptime']
    
    # make a combiner for sigma clipping and median combine
    a_combiner = overscan_trim_and_sigma_clip_median(a_list)
    combiners[exposure_time_in_fits_file] = a_combiner
    master_darks[exposure_time_in_fits_file] = a_combiner.median_combine(median_func=bn_median)

    # set the exposure time in the master -- right now combiner doesn't attempt to combine meta
    master_darks[exposure_time_in_fits_file].header['exptime'] = exposure_time_in_fits_file
    print ""For exposure {} seconds there are {} bad pixels in the master."".format(exposure_time_in_fits_file,
                                                                                  master_darks[exposure_time_in_fits_file].mask.sum())",0.4750058651,
1098,load in data sets for visualization examples,"def gmx_benzene_coul_dHdl():
    dataset = alchemtest.gmx.load_benzene()

    dHdl = pd.concat([gmx.extract_dHdl(filename, T=300)
                      for filename in dataset['data']['Coulomb']])

    return dHdl",0.4731363654,
1098,load in data sets for visualization examples,"def generate_embeddings():
    # Import data
    mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True, fake_data=FLAGS.fake_data)
    sess = tf.InteractiveSession()
    # Input set for Embedded TensorBoard visualization
    # Performed with cpu to conserve memory and processing power
    with tf.device(""/cpu:0""):
        embedding = tf.Variable(tf.stack(mnist.test.images[:FLAGS.max_steps], axis=0), 
                                trainable=False, name='embedding')
    tf.global_variables_initializer().run()
    saver = tf.train.Saver()
    writer = tf.summary.FileWriter(FLAGS.log_dir + '/projector', sess.graph)
    # Add embedding tensorboard visualization. 
    config = projector.ProjectorConfig()
    embed= config.embeddings.add()
    embed.tensor_name = 'embedding:0'
    embed.metadata_path = os.path.join(FLAGS.log_dir + '/projector/metadata.tsv')
    embed.sprite.image_path = os.path.join(FLAGS.data_dir + '/mnist_10k_sprite.png')
    # Specify the width and height of a single thumbnail.
    embed.sprite.single_image_dim.extend([28, 28])
    projector.visualize_embeddings(writer, config)
    saver.save(sess, os.path.join(
        FLAGS.log_dir, 'projector/a_model.ckpt'), global_step=FLAGS.max_steps)

def generate_metadata_file():
    # Import data
    mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True, fake_data=FLAGS.fake_data)
    def save_metadata(file):
        with open(file, 'w') as f:
            for i in range(FLAGS.max_steps):
                c = np.nonzero(mnist.test.labels[::1])[1:][0][i]
                f.write('{}\n'.format(c))
    save_metadata(FLAGS.log_dir + '/projector/metadata.tsv')",0.4670450091,
1098,load in data sets for visualization examples,"for dataset in chosen_sets:
    dataset['X'], dataset['Y'] = dbh.load_data_matrices(
        cnx, set_id=dataset['set_id'], filenumbers=dataset['filenumbers']
    )
    dataset['A'], dataset['T'], dataset['V'] = analytics.compute_hard_limits(dataset['Y'])
    plt.title('Set {}'.format(dataset['set_id']))
    for key in dataset['A']:
        plt.plot(dataset['X'][0], dataset['A'][key], label=key)
    plt.legend(loc='best')
    plt.show()",0.464358449,
1098,load in data sets for visualization examples,"def main(unused_argv):
  # Load training and eval data
  mnist = tf.contrib.learn.datasets.load_dataset(""mnist"")
  train_data = mnist.train.images  # Returns np.array
  train_labels = np.asarray(mnist.train.labels, dtype=np.int32)
  eval_data = mnist.test.images  # Returns np.array
  eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)

  # Create the Estimator
  mnist_classifier = tf.estimator.Estimator(
      model_fn=cnn_model_fn, model_dir=""/tmp/mnist_convnet_modelf"")

  # Set up logging for predictions
 #Log the values in the ""Softmax"" tensor with label ""probabilities""
  tensors_to_log = {""probabilities"": ""softmax_tensor""}
  logging_hook = tf.train.LoggingTensorHook(
    tensors=tensors_to_log, every_n_iter=100)
 
  # Train the model
  train_input_fn = tf.estimator.inputs.numpy_input_fn(
      x={""x"": train_data},
      y=train_labels,
      batch_size=200,
      num_epochs=None,
      shuffle=True)
  
  mnist_classifier.train(
      input_fn=train_input_fn,
      steps=2000,
      hooks=[logging_hook])

  # Evaluate the model and print results
  predict_input_fn = tf.estimator.inputs.numpy_input_fn(
    x={""x"": eval_data},
    num_epochs = 1,
    shuffle=False)

  results = list(mnist_classifier.predict(input_fn=predict_input_fn))
  predictions = [i['classes'] for i in results]
  get_metrics(eval_labels, predictions)",0.4584208131,
1098,load in data sets for visualization examples,"def merge_models(df):
    import io
    import pandas
    import sklearn.ensemble
    import sklearn.externals
    estimators = [sklearn.externals.joblib.load(io.BytesIO(byt))
                  for byt in df['model']]
    if not estimators:
        return None
    labelencoder = sklearn.preprocessing.LabelEncoder()
    labelencoder.fit([0,1,2])
    model = sklearn.ensemble.VotingClassifier(())
    model.estimators_ = estimators
    model.le_ = labelencoder
    buf = io.BytesIO()
    sklearn.externals.joblib.dump(model, buf)
    return pandas.DataFrame({'count': df.sum()['count'],
                             'model': [buf.getvalue()]})

ar_fun = db.input(upload_data=scidbstrm.pack_func(merge_models),
                  upload_schema=scidbpy.Schema.fromstring('<x:binary not null>[i]')).store()

que = db.unpack(
    #The unpack puts all the models into a single chunk (assuming there aren't more than 1M instances)
    db.arrays.IHI_PARTIAL_MODEL,
    ""i"", 
    ""10000000""
).stream(
    scidbstrm.python_map,
    ""'format=feather'"",
    ""'types=int64,binary'"",
    ""'names=count,model'"",
    '_sg({}, 0)'.format(ar_fun.name)
).store(
db.arrays.IHI_FINAL_MODEL)",0.4578425288,
1098,load in data sets for visualization examples,"datasets=sio.loadmat(datasets) # load datasets
    
    ############# lhrhadvs_sample_data.mat #############
    # train_x  = 240 volumes x 74484 voxels  
    # train_x  = 240 volumes x 1 [0:left-hand clenching task, 1:right-hand clenching task, 2:auditory task, 3:visual task]
    # test_x  = 120 volumes x 74484 voxels
    # test_y  = 120 volumes x 1 [0:left-hand clenching task, 1:right-hand clenching task, 2:auditory task, 3:visual task]
    ############################################################

    train_x = datasets['train_x'];     train_y = datasets['train_y'];
    test_x  = datasets['test_x'];    test_y  = datasets['test_y'];
    
    train_set_x = theano.shared(numpy.asarray(train_x, dtype=theano.config.floatX))
    train_set_y = T.cast(theano.shared(train_y.flatten(),borrow=True),'int32')
    
    test_set_x = theano.shared(numpy.asarray(test_x, dtype=theano.config.floatX))
    test_set_y = T.cast(theano.shared(test_y.flatten(),borrow=True),'int32')

    # compute number of minibatches for training, validation and testing
    n_train_batches = int(train_set_x.get_value(borrow=True).shape[0] / batch_size)
    n_test_batches = int(test_set_x.get_value(borrow=True).shape[0] / batch_size)",0.4550575018,
1098,load in data sets for visualization examples,"def main(unused_argv):
    # Load training and eval data
    mnist = tf.contrib.learn.datasets.load_dataset(""mnist"")
    train_data = mnist.train.images # Returns np.array
    train_labels = np.asarray(mnist.train.labels, dtype=np.int32)
    eval_data = mnist.test.images # Returns np.array
    eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)
    # Set up logging for predictions
    tensors_to_log = {""probabilities"": ""softmax_tensor""}
    logging_hook = tf.train.LoggingTensorHook(
    tensors=tensors_to_log, every_n_iter=50)
    # Train the model
    train_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={""x"": train_data},
        y=train_labels,
        batch_size=100,
        num_epochs=None,
        shuffle=True)
    mnist_classifier.train(
        input_fn=train_input_fn,
        steps=20000,
        hooks=[logging_hook])",0.4548444152,
1098,load in data sets for visualization examples,"# Load the punkt tokenizer
tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')

# Define a function to split a review into parsed sentences
def createSentences(review, tokenizer, remove_stopwords = False):
    # Init container to hold results
    sentences = []
    
    # Split review string into sentences
    tokenSentences = tokenizer.tokenize(review.strip())

    # Clean the sentences via cleanReview() function
    for s in tokenSentences:
        # If a sentence is empty, skip it
        if len(s) > 0:
            # Clean sentence
            sentences.append( cleanReview( s, remove_stopwords ))
    
    # Return list of clean sentences
    return sentences",0.4537830949,
1098,load in data sets for visualization examples,"from sklearn import datasets

def get_digits_dataset():
    '''
    Use this function to collect your training and testing data. This function
    uses the first 80% of the dataset as training data and the last 20% as test data.
    
    Returns
    ----------
    X_train : (number train samples, number features)
    y_train : (number train samples, )
    X_test : (number test samples, number features)
    y_test : (number test samples,)
    '''
    per_train_sz = 0.8
    digits = datasets.load_digits()
    X = digits.images.reshape([digits.images.shape[0], -1])
    y = digits.target
    train_id = int(X.shape[0] * per_train_sz)
    X_train, y_train, X_test, y_test = X[:train_id], y[:train_id], X[train_id:], y[train_id:]
    return X_train, y_train, X_test, y_test",0.4529677927,
2382,tokenize song lyrics,"def get_song_data(artist_name, song_title):
    """"""Get track features data from 
    the Echo Nest database.
    """"""
    from pyechonest import song
    try: 
        result = song.search(artist=artist_name, 
                             title=song_title)
        song_result = result[0]
        song_data = song_result.audio_summary
        
        # returns a dictionary of song features
        return song_data
    
    except IndexError as e:
        return None
         
def add_features_to_db(db, features):
    """"""Request track features from
    the Echo Nest database and add to the DB.
    """"""
    from time import sleep
    
    for key, value in db.iteritems():
        # check if attributes have been already added
        if value.get('tempo'):
            pass
        else:
            song_data = get_song_data(value['artist'],
                                     value['song'])
            # if the song is in the Echo Nest DB, 
            # I add data to the DB
            if song_data:
                for f in features:
                    value.update({f : song_data[f]})
                db[key] = value
            
            # if not, I delete the track
            else:
                del db[key]
        # the Echo Nest limits number of requests to 20 per minute
        sleep(7)",0.4636769891,
2382,tokenize song lyrics,"class Song(object):
    def __init__(self, artist, title):
        self.artist = self.__format_str(artist)
        self.title = self.__format_str(title)
        self.url = None
        self.lyric = None

    def __format_str(self, s):
        # remove paranthesis and contents
        s = s.strip()
        try:
            # strip accent
            s = ''.join(c for c in unicodedata.normalize('NFD', s)
                         if unicodedata.category(c) != 'Mn')
        except:
            pass
        s = s.title()
        return s

    def __quote(self, s):
         return urllib.parse.quote(s.replace(' ', '_'))

    def __make_url(self):
        artist = self.__quote(self.artist)
        title = self.__quote(self.title)
        artist_title = '%s:%s' %(artist, title)
        url = 'http://lyrics.wikia.com/' + artist_title
        self.url = url

    def update(self, artist=None, title=None):
        if artist:
            self.artist = self.__format_str(artist)
        if title:
            self.title = self.__format_str(title)

    def lyricwikia(self):
        self.__make_url()
        try:
            doc = lxml.html.parse(self.url)
            lyricbox = doc.getroot().cssselect('.lyricbox')[0]
        except IOError:
            self.lyric = ''
            return
        lyrics = []

        for node in lyricbox:
            if node.tag == 'br':
                lyrics.append('\n')
            if node.tail is not None:
                lyrics.append(node.tail)
        self.lyric =  """".join(lyrics).strip()    
        return self.lyric",0.4499291778,
2382,tokenize song lyrics,"class Song():

    def __init__(self, title, artist, lyrics):
        """"""
        simple song class to store basic info of a song
        Args:
            title: string, name of the song
            artist: string, name(s) of the singer
            lyrics: string, lyrics words seperated by spaces
        """"""
        self.song_info = {}
        self.song_info['title'] = title
        self.song_info['artist'] = artist
        self.song_info['lyrics'] = lyrics",0.443290472,
2382,tokenize song lyrics,"def tip_artists(user):
    
    total_music_to_tip = 0
    for artist in user.listening_to_artists:
        # Find the total songs
        total_music_to_tip = total_music_to_tip + artist.amount_of_songs_tm
        
    # Tip every artist according to the number of songs they have
    # Determine how much the user tips in total
    total_tip = np.random.normal(user.mean_amount_of_tip, user.SD_amount_of_tip)
    for artist in user.listening_to_artists:
        artist.financial_capital = artist.financial_capital + totaltip * (artist.amount_of_songs_tm/total_music_to_tip)
    
    # We leave out the chance to tip for now, but we can introduce it if the results are not realistic
    
def find_new_artists(user):
    # Replace the 10 artists with 10 new ones, which are connected to the old ones
    # Maybe give a chance to keep listening
    for artist in user.listening_to_artists:
        # Does he stay?
        # Who replaces him?
        # Replace him
    listening_to_artists_nt = []
        while len(listening_to_artists) < number_listening_to_artists:
            potential_artist = random.choice(musicians)
            if (set(favorite_genres).isdisjoint(potential_artist.genres)):
                listening_to_artists.append(potential_artist)        
        
    
def live_or_die(musician, musicians):
    # Determine if they made enough to live
    # Change number of songs next month permanently to zero
    
    return # True or False",0.4370362759,
2382,tokenize song lyrics,"def get_lyrics(song, artist):
    response = api.search_song(song, artist)
    if response:
        return response.lyrics
    else:
        return None",0.4363308251,
2382,tokenize song lyrics,"for word in ['salt', 'pepper', 'restaurant', 'italian', 'indian', 'chinese', 'direction', 'pittsburgh', 'burgh', 'city', 'location', 'cmu', 'pizza']:
    syms = word2vec_model.findSynonyms(word, 5)
    print('Words most similar to ', word, ' : ', [s[0] for s in syms])",0.4346884489,
2382,tokenize song lyrics,"def play_next_song():
    global songs
    # move current song to the back of the list 
    songs = songs[1:] + [songs[0]] 
    # load song now at the front of the queue 
    mix.music.load(songs[0])
    mix.music.play()",0.423713088,
2382,tokenize song lyrics,"# Reference: https://blog.ouseful.info/2016/09/13/making-music-and-embedding-sounds-in-jupyter-notebooks/

def play_midi(filename):
    """"""Plays a midi file
    Args:
        filename - path to the midi file
    """"""
    from music21 import midi
    
    mf = midi.MidiFile()
    mf.open(filename)
    mf.read()
    mf.close()
    stream = midi.translate.midiFileToStream(mf)
    stream.show('midi')

play_midi('Classical-Piano-Composer/test_output.mid')",0.4234354496,
2382,tokenize song lyrics,"class Song:
    def __init__(self, title=None, artist=None, time='00:00:00'):
        self.title = title
        self.artist = artist
        self.time = time
        self.__total_seconds = 0
        self.total_seconds = time

    def __int__(self):
        return self.total_seconds

    def __add__(self, other):
        return self.total_seconds + other

    def __radd__(self, other):
        return self.total_seconds + other

    @property
    def total_seconds(self):
        return self.__total_seconds

    @total_seconds.setter
    def total_seconds(self, time):
        if time.count(':') != 2:
            raise ValueError(
                ""'time' argument should only be hours:minutes:seconds""
            )
        seconds, minutes, hours = map(int, time.split(':')[::-1])
        seconds += minutes * 60
        seconds += hours * 3600
        self.__total_seconds = seconds",0.4229030609,
2382,tokenize song lyrics,"#! SETUP 2
_snlp_train_dir = _snlp_book_dir + ""/data/ohhla/train""
_snlp_dev_dir = _snlp_book_dir + ""/data/ohhla/dev""
_snlp_train_song_words = ohhla.words(ohhla.load_all_songs(_snlp_train_dir))
_snlp_dev_song_words = ohhla.words(ohhla.load_all_songs(_snlp_dev_dir))
assert(len(_snlp_train_song_words)==1041496)",0.4228155017,
1739,random numbers and fair coin,"def random_step(size=10):
    """"""
    Args: 
        size:array size
    Return
        Random array
    """"""
    return random.randint(a=-size,b=size) * np.ones(random.randint(a=1,b=size))",0.5605216026,
1739,random numbers and fair coin,"from random import randrange
# randomly create a quantum state of a qubit
def random_quantum_state():
    first_entry = randrange(100)
    first_entry = first_entry/100
    first_entry = first_entry**0.5
    if randrange(2) == 0: # determine the sign of the first entry
        first_entry = -1 * first_entry
    second_entry = 1 - (first_entry**2)
    second_entry = second_entry**0.5 # the second entry cannot be nonnegative
    if randrange(2) == 0: # determine the sign
       second_entry = -1 * second_entry
    return [first_entry,second_entry]

print(""function 'random_quantum_state' is defined now, and so it can be used in the following part"")",0.5455574393,
1739,random numbers and fair coin,"import random
def sum_imperative():
    res = 0
    for n in [random.random() for _ in range(100)]:
        res += n
    return res

def sum_functional():
    return sum(random.random() for _ in range(100))",0.5431616306,
1739,random numbers and fair coin,"def get_spending_of_attendee():
    if random.random() < 0.03:  # Let's say 3% doesn't even care about the secret shop
        return 0

    return int((random.paretovariate(2) - 0.5) * 100)

print([get_spending_of_attendee() for _ in range(100)])",0.5424296856,
1739,random numbers and fair coin,"def randomGenerator():
    num = random.randint(0,100)
    num/=100
    return num",0.5421120524,
1739,random numbers and fair coin,"def rand(): 
    return random.randint(0,100000)/100000",0.5415699482,
1739,random numbers and fair coin,"# put your code here.  

import random

# feed it a random seed so we get the same numbers each time
random.seed(8675309)

# function step1d takes no arguments and returns +1 or -1
def step1d():
    a = random.randint(0,1)
    if a == 0:
        return -1
    else:
        return 1
    
# loop several times to give me some outputs
for i in range(10):
    mystep = step1d()
    print(mystep)",0.5409629345,
1739,random numbers and fair coin,"import random

def roll_die(n_sides, weights=""same""):
    """"""This function will take an n_sides die and will return the 
    results from a single roll. If fair=True, probability of each
    side will be 1/n_sides. Else, bias will be reflected by list
    of values introduced. """"""
    
    if weights==""same"":
        return random.randint(1, n_sides)
    
    else:
        return random.choices(range(1,n_sides+1), weights)[0]",0.5405766964,
1739,random numbers and fair coin,"def generate_secret():
    numbers = list(map(int,digits))   # seed it with 0-9
    return(random.sample(numbers,3))  # return a list of 3 random selections",0.5402599573,
1739,random numbers and fair coin,"import random

def choose_first():
    
    player = random.randint(1,2)
    
    if player == 1:
        return('player 1')
    else:
        return('player 2')",0.536375165,