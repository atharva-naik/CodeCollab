{
    "Examples->Release Highlights->Release Highlights for scikit-learn 1.2": [
        [
            "We are pleased to announce the release of scikit-learn 1.2! Many bug fixes\nand improvements were added, as well as some new key features. We detail\nbelow a few of the major features of this release. <strong>For an exhaustive list of\nall the changes</strong>, please refer to the .",
            "markdown"
        ],
        [
            "To install the latest version (with pip):",
            "markdown"
        ],
        [
            "pip install --upgrade scikit-learn",
            "code"
        ],
        [
            "or with conda:",
            "markdown"
        ],
        [
            "conda install -c conda-forge scikit-learn",
            "code"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 1.2->Pandas output with set_output API": [
        [
            "scikit-learn\u2019s transformers now support pandas output with the set_output API.\nTo learn more about the set_output API see the example:\n and\n# this .",
            "markdown"
        ],
        [
            "import numpy as np\nfrom sklearn.datasets import \nfrom sklearn.preprocessing import , \nfrom sklearn.compose import \n\nX, y = (as_frame=True, return_X_y=True)\nsepal_cols = [\"sepal length (cm)\", \"sepal width (cm)\"]\npetal_cols = [\"petal length (cm)\", \"petal width (cm)\"]\n\npreprocessor = (\n    [\n        (\"scaler\", (), sepal_cols),\n        (\"kbin\", (encode=\"ordinal\"), petal_cols),\n    ],\n    verbose_feature_names_out=False,\n).set_output(transform=\"pandas\")\n\nX_out = preprocessor.fit_transform(X)\nX_out.sample(n=5, random_state=0)\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "code"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 1.2->Interaction constraints in Histogram-based Gradient Boosting Trees": [
        [
            "and\n now supports interaction constraints\nwith the interaction_cst parameter. For details, see the\n. In the following example, features are not\nallowed to interact.",
            "markdown"
        ],
        [
            "from sklearn.datasets import \nfrom sklearn.ensemble import \n\nX, y = (return_X_y=True, as_frame=True)\n\nhist_no_interact = (\n    interaction_cst=[[i] for i in range(X.shape[1])], random_state=0\n)\nhist_no_interact.fit(X, y)",
            "code"
        ],
        [
            "HistGradientBoostingRegressor(interaction_cst=[[0], [1], [2], [3], [4], [5],\n                                               [6], [7], [8], [9]],\n                              random_state=0)<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input checked=\"\" class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-1\">HistGradientBoostingRegressor</label>",
            "code"
        ],
        [
            "HistGradientBoostingRegressor(interaction_cst=[[0], [1], [2], [3], [4], [5],\n                                               [6], [7], [8], [9]],\n                              random_state=0)\n\n<br/>\n<br/>",
            "code"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 1.2->New and enhanced displays": [
        [
            "provides a way to analyze regression\nmodels in a qualitative manner.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\nfrom sklearn.metrics import PredictionErrorDisplay\n\nfig, axs = (nrows=1, ncols=2, figsize=(12, 5))\n_ = (\n    hist_no_interact, X, y, kind=\"actual_vs_predicted\", ax=axs[0]\n)\n_ = (\n    hist_no_interact, X, y, kind=\"residual_vs_predicted\", ax=axs[1]\n)\n\n\n<img alt=\"plot release highlights 1 2 0\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_release_highlights_1_2_0_001.png\" srcset=\"../../_images/sphx_glr_plot_release_highlights_1_2_0_001.png\"/>",
            "code"
        ],
        [
            "is now available to plot\nresults from .",
            "markdown"
        ],
        [
            "from sklearn.model_selection import LearningCurveDisplay\n\n_ = (\n    hist_no_interact, X, y, cv=5, n_jobs=2, train_sizes=(0.1, 1, 5)\n)\n\n\n<img alt=\"plot release highlights 1 2 0\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_release_highlights_1_2_0_002.png\" srcset=\"../../_images/sphx_glr_plot_release_highlights_1_2_0_002.png\"/>",
            "code"
        ],
        [
            "exposes a new parameter\ncategorical_features to display partial dependence for categorical features\nusing bar plots and heatmaps.",
            "markdown"
        ],
        [
            "from sklearn.datasets import \n\nX, y = (\n    \"titanic\", version=1, as_frame=True, return_X_y=True, parser=\"pandas\"\n)\nX = X.select_dtypes([\"number\", \"category\"]).drop(columns=[\"body\"])",
            "code"
        ],
        [
            "from sklearn.preprocessing import \nfrom sklearn.pipeline import \n\ncategorical_features = [\"pclass\", \"sex\", \"embarked\"]\nmodel = (\n    (\n        transformers=[(\"cat\", (), categorical_features)],\n        remainder=\"passthrough\",\n    ),\n    (random_state=0),\n).fit(X, y)",
            "code"
        ],
        [
            "from sklearn.inspection import PartialDependenceDisplay\n\nfig, ax = (figsize=(14, 4), constrained_layout=True)\n_ = (\n    model,\n    X,\n    features=[\"age\", \"sex\", (\"pclass\", \"sex\")],\n    categorical_features=categorical_features,\n    ax=ax,\n)\n\n\n<img alt=\"plot release highlights 1 2 0\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_release_highlights_1_2_0_003.png\" srcset=\"../../_images/sphx_glr_plot_release_highlights_1_2_0_003.png\"/>",
            "code"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 1.2->Faster parser in": [
        [
            "now supports a new \"pandas\" parser that is\nmore memory and CPU efficient. In v1.4, the default will change to\nparser=\"auto\" which will automatically use the \"pandas\" parser for dense\ndata and \"liac-arff\" for sparse data.",
            "markdown"
        ],
        [
            "X, y = (\n    \"titanic\", version=1, as_frame=True, return_X_y=True, parser=\"pandas\"\n)\nX.head()\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "code"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 1.2->Experimental Array API support in": [
        [
            "Experimental support for the \nspecification was added to .\nThe estimator can now run on any Array API compliant libraries such as\n, a GPU-accelerated array\nlibrary. For details, see the .",
            "markdown"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 1.2->Improved efficiency of many estimators": [
        [
            "In version 1.1 the efficiency of many estimators relying on the computation of\npairwise distances (essentially estimators related to clustering, manifold\nlearning and neighbors search algorithms) was greatly improved for float64\ndense input. Efficiency improvement especially were a reduced memory footprint\nand a much better scalability on multi-core machines.\nIn version 1.2, the efficiency of these estimators was further improved for all\ncombinations of dense and sparse inputs on float32 and float64 datasets, except\nthe sparse-dense and dense-sparse combinations for the Euclidean and Squared\nEuclidean Distance metrics.\nA detailed list of the impacted estimators can be found in the\n.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  10.646 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 1.1": [
        [
            "We are pleased to announce the release of scikit-learn 1.1! Many bug fixes\nand improvements were added, as well as some new key features. We detail\nbelow a few of the major features of this release. <strong>For an exhaustive list of\nall the changes</strong>, please refer to the .",
            "markdown"
        ],
        [
            "To install the latest version (with pip):",
            "markdown"
        ],
        [
            "pip install --upgrade scikit-learn",
            "code"
        ],
        [
            "or with conda:",
            "markdown"
        ],
        [
            "conda install -c conda-forge scikit-learn",
            "code"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 1.1->Quantile loss in": [
        [
            "can model quantiles with\nloss=\"quantile\" and the new parameter quantile.",
            "markdown"
        ],
        [
            "from sklearn.ensemble import \nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simple regression function for X * cos(X)\nrng = (42)\nX_1d = (0, 10, num=2000)\nX = X_1d.reshape(-1, 1)\ny = X_1d * (X_1d) + rng.normal(scale=X_1d / 3)\n\nquantiles = [0.95, 0.5, 0.05]\nparameters = dict(loss=\"quantile\", max_bins=32, max_iter=50)\nhist_quantiles = {\n    f\"quantile={quantile:.2f}\": (\n        **parameters, quantile=quantile\n    ).fit(X, y)\n    for quantile in quantiles\n}\n\nfig, ax = ()\nax.plot(X_1d, y, \"o\", alpha=0.5, markersize=1)\nfor quantile, hist in hist_quantiles.items():\n    ax.plot(X_1d, hist.predict(X), label=quantile)\n_ = ax.legend(loc=\"lower left\")\n\n\n<img alt=\"plot release highlights 1 1 0\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_release_highlights_1_1_0_001.png\" srcset=\"../../_images/sphx_glr_plot_release_highlights_1_1_0_001.png\"/>",
            "code"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 1.1->get_feature_names_out Available in all Transformers": [
        [
            "is now available in all Transformers. This enables\n to construct the output feature names for more complex\npipelines:",
            "markdown"
        ],
        [
            "from sklearn.compose import \nfrom sklearn.preprocessing import , \nfrom sklearn.pipeline import \nfrom sklearn.impute import \nfrom sklearn.feature_selection import \nfrom sklearn.datasets import \nfrom sklearn.linear_model import \n\nX, y = (\n    \"titanic\", version=1, as_frame=True, return_X_y=True, parser=\"pandas\"\n)\nnumeric_features = [\"age\", \"fare\"]\nnumeric_transformer = ((strategy=\"median\"), ())\ncategorical_features = [\"embarked\", \"pclass\"]\n\npreprocessor = (\n    [\n        (\"num\", numeric_transformer, numeric_features),\n        (\n            \"cat\",\n            (handle_unknown=\"ignore\", sparse_output=False),\n            categorical_features,\n        ),\n    ],\n    verbose_feature_names_out=False,\n)\nlog_reg = (preprocessor, (k=7), ())\nlog_reg.fit(X, y)",
            "code"
        ],
        [
            "Pipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('standardscaler',\n                                                                   StandardScaler())]),\n                                                  ['age', 'fare']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse_output=False),\n                                                  ['embarked', 'pclass'])],\n                                   verbose_feature_names_out=False)),\n                ('selectkbest', SelectKBest(k=7)),\n                ('logisticregression', LogisticRegression())])<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-2\">Pipeline</label>",
            "code"
        ],
        [
            "Pipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('standardscaler',\n                                                                   StandardScaler())]),\n                                                  ['age', 'fare']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse_output=False),\n                                                  ['embarked', 'pclass'])],\n                                   verbose_feature_names_out=False)),\n                ('selectkbest', SelectKBest(k=7)),\n                ('logisticregression', LogisticRegression())])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-3\">columntransformer: ColumnTransformer</label>",
            "code"
        ],
        [
            "ColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('standardscaler',\n                                                  StandardScaler())]),\n                                 ['age', 'fare']),\n                                ('cat',\n                                 OneHotEncoder(handle_unknown='ignore',\n                                               sparse_output=False),\n                                 ['embarked', 'pclass'])],\n                  verbose_feature_names_out=False)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-4\">num</label>",
            "code"
        ],
        [
            "['age', 'fare']<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-5\">SimpleImputer</label>",
            "code"
        ],
        [
            "SimpleImputer(strategy='median')<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-6\">StandardScaler</label>",
            "code"
        ],
        [
            "StandardScaler()<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-7\">cat</label>",
            "code"
        ],
        [
            "['embarked', 'pclass']<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-8\">OneHotEncoder</label>",
            "code"
        ],
        [
            "OneHotEncoder(handle_unknown='ignore', sparse_output=False)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-9\">SelectKBest</label>",
            "code"
        ],
        [
            "SelectKBest(k=7)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-10\">LogisticRegression</label>",
            "code"
        ],
        [
            "LogisticRegression()\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "Here we slice the pipeline to include all the steps but the last one. The output\nfeature names of this pipeline slice are the features put into logistic\nregression. These names correspond directly to the coefficients in the logistic\nregression:",
            "markdown"
        ],
        [
            "import pandas as pd\n\nlog_reg_input_features = log_reg[:-1].get_feature_names_out()\n(log_reg[-1].coef_.ravel(), index=log_reg_input_features).plot.bar()\n()\n\n\n<img alt=\"plot release highlights 1 1 0\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_release_highlights_1_1_0_002.png\" srcset=\"../../_images/sphx_glr_plot_release_highlights_1_1_0_002.png\"/>",
            "code"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 1.1->Grouping infrequent categories in OneHotEncoder": [
        [
            "OneHotEncoder supports aggregating infrequent categories into a single\noutput for each feature. The parameters to enable the gathering of infrequent\ncategories are min_frequency and max_categories. See the\n for more details.",
            "markdown"
        ],
        [
            "from sklearn.preprocessing import \nimport numpy as np\n\nX = (\n    [[\"dog\"] * 5 + [\"cat\"] * 20 + [\"rabbit\"] * 10 + [\"snake\"] * 3], dtype=object\n).T\nenc = (min_frequency=6, sparse_output=False).fit(X)\nenc.infrequent_categories_",
            "code"
        ],
        [
            "[array(['dog', 'snake'], dtype=object)]",
            "code"
        ],
        [
            "Since dog and snake are infrequent categories, they are grouped together when\ntransformed:",
            "markdown"
        ],
        [
            "encoded = enc.transform(([[\"dog\"], [\"snake\"], [\"cat\"], [\"rabbit\"]]))\n(encoded, columns=enc.get_feature_names_out())\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "code"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 1.1->Performance improvements": [
        [
            "Reductions on pairwise distances for dense float64 datasets has been refactored\nto better take advantage of non-blocking thread parallelism. For example,\n and\n can respectively be up to \u00d720 and\n\u00d75 faster than previously. In summary, the following functions and estimators\nnow benefit from improved performance:",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "To know more about the technical details of this work, you can read\n.",
            "markdown"
        ],
        [
            "Moreover, the computation of loss functions has been refactored using\nCython resulting in performance improvements for the following estimators:",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 1.1->MiniBatchNMF: an online version of NMF": [
        [
            "The new class  implements a faster but less\naccurate version of non-negative matrix factorization ().\nMiniBatchNMF divides the data into mini-batches and optimizes the NMF model\nin an online manner by cycling over the mini-batches, making it better suited for\nlarge datasets. In particular, it implements partial_fit, which can be used for\nonline learning when the data is not readily available from the start, or when the\ndata does not fit into memory.",
            "markdown"
        ],
        [
            "import numpy as np\nfrom sklearn.decomposition import \n\nrng = (0)\nn_samples, n_features, n_components = 10, 10, 5\ntrue_W = rng.uniform(size=(n_samples, n_components))\ntrue_H = rng.uniform(size=(n_components, n_features))\nX = true_W @ true_H\n\nnmf = (n_components=n_components, random_state=0)\n\nfor _ in range(10):\n    nmf.partial_fit(X)\n\nW = nmf.transform(X)\nH = nmf.components_\nX_reconstructed = W @ H\n\nprint(\n    f\"relative reconstruction error: \",\n    f\"{((X - X_reconstructed) ** 2) / (X**2):.5f}\",\n)",
            "code"
        ],
        [
            "relative reconstruction error:  0.00364",
            "code"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 1.1->BisectingKMeans: divide and cluster": [
        [
            "The new class  is a variant of KMeans, using\ndivisive hierarchical clustering. Instead of creating all centroids at once, centroids\nare picked progressively based on a previous clustering: a cluster is split into two\nnew clusters repeatedly until the target number of clusters is reached, giving a\nhierarchical structure to the clustering.",
            "markdown"
        ],
        [
            "from sklearn.datasets import \nfrom sklearn.cluster import , \nimport matplotlib.pyplot as plt\n\nX, _ = (n_samples=1000, centers=2, random_state=0)\n\nkm = (n_clusters=5, random_state=0, n_init=\"auto\").fit(X)\nbisect_km = (n_clusters=5, random_state=0).fit(X)\n\nfig, ax = (1, 2, figsize=(10, 5))\nax[0].scatter(X[:, 0], X[:, 1], s=10, c=km.labels_)\nax[0].scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], s=20, c=\"r\")\nax[0].set_title(\"KMeans\")\n\nax[1].scatter(X[:, 0], X[:, 1], s=10, c=bisect_km.labels_)\nax[1].scatter(\n    bisect_km.cluster_centers_[:, 0], bisect_km.cluster_centers_[:, 1], s=20, c=\"r\"\n)\n_ = ax[1].set_title(\"BisectingKMeans\")\n\n\n<img alt=\"KMeans, BisectingKMeans\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_release_highlights_1_1_0_003.png\" srcset=\"../../_images/sphx_glr_plot_release_highlights_1_1_0_003.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  1.264 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 1.0": [
        [
            "We are very pleased to announce the release of scikit-learn 1.0! The library\nhas been stable for quite some time, releasing version 1.0 is recognizing that\nand signalling it to our users. This release does not include any breaking\nchanges apart from the usual two-release deprecation cycle. For the future, we\ndo our best to keep this pattern.",
            "markdown"
        ],
        [
            "This release includes some new key features as well as many improvements and\nbug fixes. We detail below a few of the major features of this release. <strong>For\nan exhaustive list of all the changes</strong>, please refer to the .",
            "markdown"
        ],
        [
            "To install the latest version (with pip):",
            "markdown"
        ],
        [
            "pip install --upgrade scikit-learn",
            "code"
        ],
        [
            "or with conda:",
            "markdown"
        ],
        [
            "conda install -c conda-forge scikit-learn",
            "code"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 1.0->Keyword and positional arguments": [
        [
            "The scikit-learn API exposes many functions and methods which have many input\nparameters. For example, before this release, one could instantiate a\n as:",
            "markdown"
        ],
        [
            "HistGradientBoostingRegressor(\"squared_error\", 0.1, 100, 31, None,\n    20, 0.0, 255, None, None, False, \"auto\", \"loss\", 0.1, 10, 1e-7,\n    0, None)",
            "code"
        ],
        [
            "Understanding the above code requires the reader to go to the API\ndocumentation and to check each and every parameter for its position and\nits meaning. To improve the readability of code written based on scikit-learn,\nnow users have to provide most parameters with their names, as keyword\narguments, instead of positional arguments. For example, the above code would\nbe:",
            "markdown"
        ],
        [
            "HistGradientBoostingRegressor(\n    loss=\"squared_error\",\n    learning_rate=0.1,\n    max_iter=100,\n    max_leaf_nodes=31,\n    max_depth=None,\n    min_samples_leaf=20,\n    l2_regularization=0.0,\n    max_bins=255,\n    categorical_features=None,\n    monotonic_cst=None,\n    warm_start=False,\n    early_stopping=\"auto\",\n    scoring=\"loss\",\n    validation_fraction=0.1,\n    n_iter_no_change=10,\n    tol=1e-7,\n    verbose=0,\n    random_state=None,\n)",
            "code"
        ],
        [
            "which is much more readable. Positional arguments have been deprecated since\nversion 0.23 and will now raise a TypeError. A limited number of\npositional arguments are still allowed in some cases, for example in\n, where PCA(10) is still allowed, but PCA(10,\nFalse) is not allowed.",
            "markdown"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 1.0->Spline Transformers": [
        [
            "One way to add nonlinear terms to a dataset\u2019s feature set is to generate\nspline basis functions for continuous/numerical features with the new\n. Splines are piecewise polynomials,\nparametrized by their polynomial degree and the positions of the knots. The\n implements a B-spline basis.\n<figure class=\"align-center\">\n\n</figure>",
            "markdown"
        ],
        [
            "The following code shows splines in action, for more information, please\nrefer to the .",
            "markdown"
        ],
        [
            "import numpy as np\nfrom sklearn.preprocessing import \n\nX = (5).reshape(5, 1)\nspline = (degree=2, n_knots=3)\nspline.fit_transform(X)",
            "code"
        ],
        [
            "array([[0.5  , 0.5  , 0.   , 0.   ],\n       [0.125, 0.75 , 0.125, 0.   ],\n       [0.   , 0.5  , 0.5  , 0.   ],\n       [0.   , 0.125, 0.75 , 0.125],\n       [0.   , 0.   , 0.5  , 0.5  ]])",
            "code"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 1.0->Quantile Regressor": [
        [
            "Quantile regression estimates the median or other quantiles of \\(y\\)\nconditional on \\(X\\), while ordinary least squares (OLS) estimates the\nconditional mean.",
            "markdown"
        ],
        [
            "As a linear model, the new  gives\nlinear predictions \\(\\hat{y}(w, X) = Xw\\) for the \\(q\\)-th quantile,\n\\(q \\in (0, 1)\\). The weights or coefficients \\(w\\) are then found by\nthe following minimization problem:\n\n\\[\\min_{w} {\\frac{1}{n_{\\text{samples}}}\n\\sum_i PB_q(y_i - X_i w) + \\alpha ||w||_1}.\\]",
            "markdown"
        ],
        [
            "This consists of the pinball loss (also known as linear loss),\nsee also ,\n\n\\[\\begin{split}PB_q(t) = q \\max(t, 0) + (1 - q) \\max(-t, 0) =\n\\begin{cases}\n    q t, & t > 0, \\\\\n    0,    & t = 0, \\\\\n    (1-q) t, & t &lt; 0\n\\end{cases}\\end{split}\\]",
            "markdown"
        ],
        [
            "and the L1 penalty controlled by parameter alpha, similar to\n.",
            "markdown"
        ],
        [
            "Please check the following example to see how it works, and the  for more details.\n<figure class=\"align-center\">\n\n</figure>",
            "markdown"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 1.0->Feature Names Support": [
        [
            "When an estimator is passed a  during\n, the estimator will set a feature_names_in_ attribute\ncontaining the feature names. Note that feature names support is only enabled\nwhen the column names in the dataframe are all strings. feature_names_in_\nis used to check that the column names of the dataframe passed in\nnon-, such as , are consistent with features in\n:",
            "markdown"
        ],
        [
            "from sklearn.preprocessing import \nimport pandas as pd\n\nX = ([[1, 2, 3], [4, 5, 6]], columns=[\"a\", \"b\", \"c\"])\nscalar = ().fit(X)\nscalar.feature_names_in_",
            "code"
        ],
        [
            "array(['a', 'b', 'c'], dtype=object)",
            "code"
        ],
        [
            "The support of  is available for transformers\nthat already had get_feature_names and transformers with a one-to-one\ncorrespondence between input and output such as\n.  support\nwill be added to all other transformers in future releases. Additionally,\n is available to\ncombine feature names of its transformers:",
            "markdown"
        ],
        [
            "from sklearn.compose import \nfrom sklearn.preprocessing import \nimport pandas as pd\n\nX = ({\"pet\": [\"dog\", \"cat\", \"fish\"], \"age\": [3, 7, 1]})\npreprocessor = (\n    [\n        (\"numerical\", (), [\"age\"]),\n        (\"categorical\", (), [\"pet\"]),\n    ],\n    verbose_feature_names_out=False,\n).fit(X)\n\npreprocessor.get_feature_names_out()",
            "code"
        ],
        [
            "array(['age', 'pet_cat', 'pet_dog', 'pet_fish'], dtype=object)",
            "code"
        ],
        [
            "When this preprocessor is used with a pipeline, the feature names used\nby the classifier are obtained by slicing and calling\n:",
            "markdown"
        ],
        [
            "from sklearn.linear_model import \nfrom sklearn.pipeline import \n\ny = [1, 0, 1]\npipe = (preprocessor, ())\npipe.fit(X, y)\npipe[:-1].get_feature_names_out()",
            "code"
        ],
        [
            "array(['age', 'pet_cat', 'pet_dog', 'pet_fish'], dtype=object)",
            "code"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 1.0->A more flexible plotting API": [
        [
            ",\n, ,\nand  now expose two class\nmethods: from_estimator and from_predictions which allow users to create\na plot given the predictions or an estimator. This means the corresponding\nplot_* functions are deprecated. Please check  and\n for\nhow to use the new plotting functionalities.",
            "markdown"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 1.0->Online One-Class SVM": [
        [
            "The new class  implements an online\nlinear version of the One-Class SVM using a stochastic gradient descent.\nCombined with kernel approximation techniques,\n can be used to approximate the solution\nof a kernelized One-Class SVM, implemented in , with\na fit time complexity linear in the number of samples. Note that the\ncomplexity of a kernelized One-Class SVM is at best quadratic in the number\nof samples.  is thus well suited for\ndatasets with a large number of training samples (> 10,000) for which the SGD\nvariant can be several orders of magnitude faster. Please check this\n to see how\nit\u2019s used, and the  for more\ndetails.\n<figure class=\"align-center\">\n\n</figure>",
            "markdown"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 1.0->Histogram-based Gradient Boosting Models are now stable": [
        [
            "and\n are no longer experimental\nand can simply be imported and used as:",
            "markdown"
        ],
        [
            "from sklearn.ensemble import HistGradientBoostingClassifier",
            "code"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 1.0->New documentation improvements": [
        [
            "This release includes many documentation improvements. Out of over 2100\nmerged pull requests, about 800 of them are improvements to our\ndocumentation.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.015 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 0.24": [
        [
            "We are pleased to announce the release of scikit-learn 0.24! Many bug fixes\nand improvements were added, as well as some new key features. We detail\nbelow a few of the major features of this release. <strong>For an exhaustive list of\nall the changes</strong>, please refer to the .",
            "markdown"
        ],
        [
            "To install the latest version (with pip):",
            "markdown"
        ],
        [
            "pip install --upgrade scikit-learn",
            "code"
        ],
        [
            "or with conda:",
            "markdown"
        ],
        [
            "conda install -c conda-forge scikit-learn",
            "code"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 0.24->Successive Halving estimators for tuning hyper-parameters": [
        [
            "Successive Halving, a state of the art method, is now available to\nexplore the space of the parameters and identify their best combination.\n and\n can be\nused as drop-in replacement for\n and\n.\nSuccessive Halving is an iterative selection process illustrated in the\nfigure below. The first iteration is run with a small amount of resources,\nwhere the resource typically corresponds to the number of training samples,\nbut can also be an arbitrary integer parameter such as n_estimators in a\nrandom forest. Only a subset of the parameter candidates are selected for the\nnext iteration, which will be run with an increasing amount of allocated\nresources. Only a subset of candidates will last until the end of the\niteration process, and the best parameter candidate is the one that has the\nhighest score on the last iteration.",
            "markdown"
        ],
        [
            "Read more in the  (note:\nthe Successive Halving estimators are still ).\n<figure class=\"align-center\">\n\n</figure>",
            "markdown"
        ],
        [
            "import numpy as np\nfrom scipy.stats import \nfrom sklearn.experimental import enable_halving_search_cv  # noqa\nfrom sklearn.model_selection import \nfrom sklearn.ensemble import \nfrom sklearn.datasets import \n\nrng = (0)\n\nX, y = (n_samples=700, random_state=rng)\n\nclf = (n_estimators=10, random_state=rng)\n\nparam_dist = {\n    \"max_depth\": [3, None],\n    \"max_features\": (1, 11),\n    \"min_samples_split\": (2, 11),\n    \"bootstrap\": [True, False],\n    \"criterion\": [\"gini\", \"entropy\"],\n}\n\nrsh = (\n    estimator=clf, param_distributions=param_dist, factor=2, random_state=rng\n)\nrsh.fit(X, y)\nrsh.best_params_",
            "code"
        ],
        [
            "{'bootstrap': True, 'criterion': 'gini', 'max_depth': None, 'max_features': 10, 'min_samples_split': 10}",
            "code"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 0.24->Native support for categorical features in HistGradientBoosting estimators": [
        [
            "and\n now have native\nsupport for categorical features: they can consider splits on non-ordered,\ncategorical data. Read more in the .\n<figure class=\"align-center\">\n\n</figure>",
            "markdown"
        ],
        [
            "The plot shows that the new native support for categorical features leads to\nfitting times that are comparable to models where the categories are treated\nas ordered quantities, i.e. simply ordinal-encoded. Native support is also\nmore expressive than both one-hot encoding and ordinal encoding. However, to\nuse the new categorical_features parameter, it is still required to\npreprocess the data within a pipeline as demonstrated in this .",
            "markdown"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 0.24->Improved performances of HistGradientBoosting estimators": [
        [
            "The memory footprint of  and\n has been significantly\nimproved during calls to fit. In addition, histogram initialization is now\ndone in parallel which results in slight speed improvements.\nSee more in the .",
            "markdown"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 0.24->New self-training meta-estimator": [
        [
            "A new self-training implementation, based on  can now be used with any\nclassifier that implements . The sub-classifier\nwill behave as a\nsemi-supervised classifier, allowing it to learn from unlabeled data.\nRead more in the .",
            "markdown"
        ],
        [
            "import numpy as np\nfrom sklearn import datasets\nfrom sklearn.semi_supervised import \nfrom sklearn.svm import \n\nrng = (42)\niris = ()\nrandom_unlabeled_points = rng.rand(iris.target.shape[0]) &lt; 0.3\niris.target[random_unlabeled_points] = -1\nsvc = (probability=True, gamma=\"auto\")\nself_training_model = (svc)\nself_training_model.fit(iris.data, iris.target)",
            "code"
        ],
        [
            "SelfTrainingClassifier(base_estimator=SVC(gamma='auto', probability=True))<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-11\">SelfTrainingClassifier</label>",
            "code"
        ],
        [
            "SelfTrainingClassifier(base_estimator=SVC(gamma='auto', probability=True))<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-12\">base_estimator: SVC</label>",
            "code"
        ],
        [
            "SVC(gamma='auto', probability=True)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-13\">SVC</label>",
            "code"
        ],
        [
            "SVC(gamma='auto', probability=True)\n\n<br/>\n<br/>",
            "code"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 0.24->New SequentialFeatureSelector transformer": [
        [
            "A new iterative transformer to select features is available:\n.\nSequential Feature Selection can add features one at a time (forward\nselection) or remove features from the list of the available features\n(backward selection), based on a cross-validated score maximization.\nSee the .",
            "markdown"
        ],
        [
            "from sklearn.feature_selection import \nfrom sklearn.neighbors import \nfrom sklearn.datasets import \n\nX, y = (return_X_y=True, as_frame=True)\nfeature_names = X.columns\nknn = (n_neighbors=3)\nsfs = (knn, n_features_to_select=2)\nsfs.fit(X, y)\nprint(\n    \"Features selected by forward sequential selection: \"\n    f\"{feature_names[sfs.get_support()].tolist()}\"\n)",
            "code"
        ],
        [
            "Features selected by forward sequential selection: ['sepal length (cm)', 'petal width (cm)']",
            "code"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 0.24->New PolynomialCountSketch kernel approximation function": [
        [
            "The new \napproximates a polynomial expansion of a feature space when used with linear\nmodels, but uses much less memory than\n.",
            "markdown"
        ],
        [
            "from sklearn.datasets import \nfrom sklearn.pipeline import \nfrom sklearn.model_selection import \nfrom sklearn.preprocessing import \nfrom sklearn.kernel_approximation import \nfrom sklearn.linear_model import \n\nX, y = (return_X_y=True)\npipe = (\n    (),\n    (degree=2, n_components=300),\n    (max_iter=1000),\n)\nX_train, X_test, y_train, y_test = (\n    X, y, train_size=5000, test_size=10000, random_state=42\n)\npipe.fit(X_train, y_train).score(X_test, y_test)",
            "code"
        ],
        [
            "0.7344",
            "code"
        ],
        [
            "For comparison, here is the score of a linear baseline for the same data:",
            "markdown"
        ],
        [
            "linear_baseline = ((), (max_iter=1000))\nlinear_baseline.fit(X_train, y_train).score(X_test, y_test)",
            "code"
        ],
        [
            "0.7137",
            "code"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 0.24->Individual Conditional Expectation plots": [
        [
            "A new kind of partial dependence plot is available: the Individual\nConditional Expectation (ICE) plot. ICE plots visualize the dependence of the\nprediction on a feature for each sample separately, with one line per sample.\nSee the",
            "markdown"
        ],
        [
            "from sklearn.ensemble import \nfrom sklearn.datasets import \n\n# from sklearn.inspection import plot_partial_dependence\nfrom sklearn.inspection import PartialDependenceDisplay\n\nX, y = (return_X_y=True, as_frame=True)\nfeatures = [\"MedInc\", \"AveOccup\", \"HouseAge\", \"AveRooms\"]\nest = (n_estimators=10)\nest.fit(X, y)\n\n# plot_partial_dependence has been removed in version 1.2. From 1.2, use\n# PartialDependenceDisplay instead.\n# display = plot_partial_dependence(\ndisplay = (\n    est,\n    X,\n    features,\n    kind=\"individual\",\n    subsample=50,\n    n_jobs=3,\n    grid_resolution=20,\n    random_state=0,\n)\ndisplay.figure_.suptitle(\n    \"Partial dependence of house value on non-location features\\n\"\n    \"for the California housing dataset, with BayesianRidge\"\n)\ndisplay.figure_.subplots_adjust(hspace=0.3)\n\n\n<img alt=\"Partial dependence of house value on non-location features for the California housing dataset, with BayesianRidge\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_release_highlights_0_24_0_001.png\" srcset=\"../../_images/sphx_glr_plot_release_highlights_0_24_0_001.png\"/>",
            "code"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 0.24->New Poisson splitting criterion for DecisionTreeRegressor": [
        [
            "The integration of Poisson regression estimation continues from version 0.23.\n now supports a new 'poisson'\nsplitting criterion. Setting criterion=\"poisson\" might be a good choice\nif your target is a count or a frequency.",
            "markdown"
        ],
        [
            "from sklearn.tree import \nfrom sklearn.model_selection import \nimport numpy as np\n\nn_samples, n_features = 1000, 20\nrng = (0)\nX = rng.randn(n_samples, n_features)\n# positive integer target correlated with X[:, 5] with many zeros:\ny = rng.poisson(lam=(X[:, 5]) / 2)\nX_train, X_test, y_train, y_test = (X, y, random_state=rng)\nregressor = (criterion=\"poisson\", random_state=0)\nregressor.fit(X_train, y_train)",
            "code"
        ],
        [
            "DecisionTreeRegressor(criterion='poisson', random_state=0)<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input checked=\"\" class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-14\">DecisionTreeRegressor</label>",
            "code"
        ],
        [
            "DecisionTreeRegressor(criterion='poisson', random_state=0)\n\n<br/>\n<br/>",
            "code"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 0.24->New documentation improvements": [
        [
            "New examples and documentation pages have been added, in a continuous effort\nto improve the understanding of machine learning practices:",
            "markdown"
        ],
        [
            "a new section about ,",
            "markdown"
        ],
        [
            "an example illustrating how to \nevaluated using ,",
            "markdown"
        ],
        [
            "an example on how to ,",
            "markdown"
        ],
        [
            "an \ncomparing Principal Component Regression and Partial Least Squares.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 1 minutes  21.226 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 0.23": [
        [
            "We are pleased to announce the release of scikit-learn 0.23! Many bug fixes\nand improvements were added, as well as some new key features. We detail\nbelow a few of the major features of this release. <strong>For an exhaustive list of\nall the changes</strong>, please refer to the .",
            "markdown"
        ],
        [
            "To install the latest version (with pip):",
            "markdown"
        ],
        [
            "pip install --upgrade scikit-learn",
            "code"
        ],
        [
            "or with conda:",
            "markdown"
        ],
        [
            "conda install -c conda-forge scikit-learn",
            "code"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 0.23->Generalized Linear Models, and Poisson loss for gradient boosting": [
        [
            "Long-awaited Generalized Linear Models with non-normal loss functions are now\navailable. In particular, three new regressors were implemented:\n,\n, and\n. The Poisson regressor can be\nused to model positive integer counts, or relative frequencies. Read more in\nthe . Additionally,\n supports a new\n\u2018poisson\u2019 loss as well.",
            "markdown"
        ],
        [
            "import numpy as np\nfrom sklearn.model_selection import \nfrom sklearn.linear_model import \nfrom sklearn.ensemble import \n\nn_samples, n_features = 1000, 20\nrng = (0)\nX = rng.randn(n_samples, n_features)\n# positive integer target correlated with X[:, 5] with many zeros:\ny = rng.poisson(lam=(X[:, 5]) / 2)\nX_train, X_test, y_train, y_test = (X, y, random_state=rng)\nglm = ()\ngbdt = (loss=\"poisson\", learning_rate=0.01)\nglm.fit(X_train, y_train)\ngbdt.fit(X_train, y_train)\nprint(glm.score(X_test, y_test))\nprint(gbdt.score(X_test, y_test))",
            "code"
        ],
        [
            "0.35776189065725783\n0.42425183539869404",
            "code"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 0.23->Rich visual representation of estimators": [
        [
            "Estimators can now be visualized in notebooks by enabling the\ndisplay='diagram' option. This is particularly useful to summarise the\nstructure of pipelines and other composite estimators, with interactivity to\nprovide detail.  Click on the example image below to expand Pipeline\nelements.  See  for how you can use\nthis feature.",
            "markdown"
        ],
        [
            "from sklearn import \nfrom sklearn.pipeline import \nfrom sklearn.preprocessing import , \nfrom sklearn.impute import \nfrom sklearn.compose import \nfrom sklearn.linear_model import \n\n(display=\"diagram\")\n\nnum_proc = ((strategy=\"median\"), ())\n\ncat_proc = (\n    (strategy=\"constant\", fill_value=\"missing\"),\n    (handle_unknown=\"ignore\"),\n)\n\npreprocessor = (\n    (num_proc, (\"feat1\", \"feat3\")), (cat_proc, (\"feat0\", \"feat2\"))\n)\n\nclf = (preprocessor, ())\nclf",
            "code"
        ],
        [
            "Pipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('pipeline-1',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('standardscaler',\n                                                                   StandardScaler())]),\n                                                  ('feat1', 'feat3')),\n                                                 ('pipeline-2',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('onehotencoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ('feat0', 'feat2'))])),\n                ('logisticregression', LogisticRegression())])<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-15\">Pipeline</label>",
            "code"
        ],
        [
            "Pipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('pipeline-1',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('standardscaler',\n                                                                   StandardScaler())]),\n                                                  ('feat1', 'feat3')),\n                                                 ('pipeline-2',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('onehotencoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ('feat0', 'feat2'))])),\n                ('logisticregression', LogisticRegression())])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-16\">columntransformer: ColumnTransformer</label>",
            "code"
        ],
        [
            "ColumnTransformer(transformers=[('pipeline-1',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('standardscaler',\n                                                  StandardScaler())]),\n                                 ('feat1', 'feat3')),\n                                ('pipeline-2',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 ('feat0', 'feat2'))])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-17\">pipeline-1</label>",
            "code"
        ],
        [
            "('feat1', 'feat3')<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-18\">SimpleImputer</label>",
            "code"
        ],
        [
            "SimpleImputer(strategy='median')<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-19\">StandardScaler</label>",
            "code"
        ],
        [
            "StandardScaler()<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-20\">pipeline-2</label>",
            "code"
        ],
        [
            "('feat0', 'feat2')<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-21\">SimpleImputer</label>",
            "code"
        ],
        [
            "SimpleImputer(fill_value='missing', strategy='constant')<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-22\">OneHotEncoder</label>",
            "code"
        ],
        [
            "OneHotEncoder(handle_unknown='ignore')<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-23\">LogisticRegression</label>",
            "code"
        ],
        [
            "LogisticRegression()\n\n<br/>\n<br/>",
            "code"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 0.23->Scalability and stability improvements to KMeans": [
        [
            "The  estimator was entirely re-worked, and it\nis now significantly faster and more stable. In addition, the Elkan algorithm\nis now compatible with sparse matrices. The estimator uses OpenMP based\nparallelism instead of relying on joblib, so the n_jobs parameter has no\neffect anymore. For more details on how to control the number of threads,\nplease refer to our  notes.",
            "markdown"
        ],
        [
            "import scipy\nimport numpy as np\nfrom sklearn.model_selection import \nfrom sklearn.cluster import \nfrom sklearn.datasets import \nfrom sklearn.metrics import \n\nrng = (0)\nX, y = (random_state=rng)\nX = (X)\nX_train, X_test, _, y_test = (X, y, random_state=rng)\nkmeans = (n_init=\"auto\").fit(X_train)\nprint((kmeans.predict(X_test), y_test))",
            "code"
        ],
        [
            "0.5673318426584951",
            "code"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 0.23->Improvements to the histogram-based Gradient Boosting estimators": [
        [
            "Various improvements were made to\n and\n. On top of the\nPoisson loss mentioned above, these estimators now support . Also, an automatic early-stopping criterion was added:\nearly-stopping is enabled by default when the number of samples exceeds 10k.\nFinally, users can now define  to constrain the predictions based on the variations of\nspecific features. In the following example, we construct a target that is\ngenerally positively correlated with the first feature, with some noise.\nApplying monotoinc constraints allows the prediction to capture the global\neffect of the first feature, instead of fitting the noise.",
            "markdown"
        ],
        [
            "import numpy as np\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import \n\n# from sklearn.inspection import plot_partial_dependence\nfrom sklearn.inspection import PartialDependenceDisplay\nfrom sklearn.ensemble import \n\nn_samples = 500\nrng = (0)\nX = rng.randn(n_samples, 2)\nnoise = rng.normal(loc=0.0, scale=0.01, size=n_samples)\ny = 5 * X[:, 0] + (10 *  * X[:, 0]) - noise\n\ngbdt_no_cst = ().fit(X, y)\ngbdt_cst = (monotonic_cst=[1, 0]).fit(X, y)\n\n# plot_partial_dependence has been removed in version 1.2. From 1.2, use\n# PartialDependenceDisplay instead.\n# disp = plot_partial_dependence(\ndisp = (\n    gbdt_no_cst,\n    X,\n    features=[0],\n    feature_names=[\"feature 0\"],\n    line_kw={\"linewidth\": 4, \"label\": \"unconstrained\", \"color\": \"tab:blue\"},\n)\n# plot_partial_dependence(\n(\n    gbdt_cst,\n    X,\n    features=[0],\n    line_kw={\"linewidth\": 4, \"label\": \"constrained\", \"color\": \"tab:orange\"},\n    ax=disp.axes_,\n)\ndisp.axes_[0, 0].plot(\n    X[:, 0], y, \"o\", alpha=0.5, zorder=-1, label=\"samples\", color=\"tab:green\"\n)\ndisp.axes_[0, 0].set_ylim(-3, 3)\ndisp.axes_[0, 0].set_xlim(-1, 1)\n()\n()\n\n\n<img alt=\"plot release highlights 0 23 0\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_release_highlights_0_23_0_001.png\" srcset=\"../../_images/sphx_glr_plot_release_highlights_0_23_0_001.png\"/>",
            "code"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 0.23->Sample-weight support for Lasso and ElasticNet": [
        [
            "The two linear regressors  and\n now support sample weights.",
            "markdown"
        ],
        [
            "from sklearn.model_selection import \nfrom sklearn.datasets import \nfrom sklearn.linear_model import \nimport numpy as np\n\nn_samples, n_features = 1000, 20\nrng = (0)\nX, y = (n_samples, n_features, random_state=rng)\nsample_weight = rng.rand(n_samples)\nX_train, X_test, y_train, y_test, sw_train, sw_test = (\n    X, y, sample_weight, random_state=rng\n)\nreg = ()\nreg.fit(X_train, y_train, sample_weight=sw_train)\nprint(reg.score(X_test, y_test, sw_test))",
            "code"
        ],
        [
            "0.999791942438998",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.777 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 0.22": [
        [
            "We are pleased to announce the release of scikit-learn 0.22, which comes\nwith many bug fixes and new features! We detail below a few of the major\nfeatures of this release. For an exhaustive list of all the changes, please\nrefer to the .",
            "markdown"
        ],
        [
            "To install the latest version (with pip):",
            "markdown"
        ],
        [
            "pip install --upgrade scikit-learn",
            "code"
        ],
        [
            "or with conda:",
            "markdown"
        ],
        [
            "conda install -c conda-forge scikit-learn",
            "code"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 0.22->New plotting API": [
        [
            "A new plotting API is available for creating visualizations. This new API\nallows for quickly adjusting the visuals of a plot without involving any\nrecomputation. It is also possible to add different plots to the same\nfigure. The following example illustrates plot_roc_curve,\nbut other plots utilities are supported like\nplot_partial_dependence,\nplot_precision_recall_curve, and\nplot_confusion_matrix. Read more about this new API in the\n.",
            "markdown"
        ],
        [
            "from sklearn.model_selection import \nfrom sklearn.svm import \n\n# from sklearn.metrics import plot_roc_curve\nfrom sklearn.metrics import RocCurveDisplay\n\nfrom sklearn.ensemble import \nfrom sklearn.datasets import \nimport matplotlib.pyplot as plt\n\nX, y = (random_state=0)\nX_train, X_test, y_train, y_test = (X, y, random_state=42)\n\nsvc = (random_state=42)\nsvc.fit(X_train, y_train)\nrfc = (random_state=42)\nrfc.fit(X_train, y_train)\n\n# plot_roc_curve has been removed in version 1.2. From 1.2, use RocCurveDisplay instead.\n# svc_disp = plot_roc_curve(svc, X_test, y_test)\n# rfc_disp = plot_roc_curve(rfc, X_test, y_test, ax=svc_disp.ax_)\nsvc_disp = (svc, X_test, y_test)\nrfc_disp = (rfc, X_test, y_test, ax=svc_disp.ax_)\nrfc_disp.figure_.suptitle(\"ROC curve comparison\")\n\n()\n\n\n<img alt=\"ROC curve comparison\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_release_highlights_0_22_0_001.png\" srcset=\"../../_images/sphx_glr_plot_release_highlights_0_22_0_001.png\"/>",
            "code"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 0.22->Stacking Classifier and Regressor": [
        [
            "and\n\nallow you to have a stack of estimators with a final classifier or\na regressor.\nStacked generalization consists in stacking the output of individual\nestimators and use a classifier to compute the final prediction. Stacking\nallows to use the strength of each individual estimator by using their output\nas input of a final estimator.\nBase estimators are fitted on the full X while\nthe final estimator is trained using cross-validated predictions of the\nbase estimators using cross_val_predict.",
            "markdown"
        ],
        [
            "Read more in the .",
            "markdown"
        ],
        [
            "from sklearn.datasets import \nfrom sklearn.svm import \nfrom sklearn.linear_model import \nfrom sklearn.preprocessing import \nfrom sklearn.pipeline import \nfrom sklearn.ensemble import \nfrom sklearn.model_selection import \n\nX, y = (return_X_y=True)\nestimators = [\n    (\"rf\", (n_estimators=10, random_state=42)),\n    (\"svr\", ((), (random_state=42))),\n]\nclf = (estimators=estimators, final_estimator=())\nX_train, X_test, y_train, y_test = (X, y, stratify=y, random_state=42)\nclf.fit(X_train, y_train).score(X_test, y_test)",
            "code"
        ],
        [
            "0.9473684210526315",
            "code"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 0.22->Permutation-based feature importance": [
        [
            "The  can be used to get an\nestimate of the importance of each feature, for any fitted estimator:",
            "markdown"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import \nfrom sklearn.ensemble import \nfrom sklearn.inspection import \n\nX, y = (random_state=0, n_features=5, n_informative=3)\nfeature_names = ([f\"x_{i}\" for i in range(X.shape[1])])\n\nrf = (random_state=0).fit(X, y)\nresult = (rf, X, y, n_repeats=10, random_state=0, n_jobs=2)\n\nfig, ax = ()\nsorted_idx = result.importances_mean.argsort()\nax.boxplot(\n    result.importances[sorted_idx].T, vert=False, labels=feature_names[sorted_idx]\n)\nax.set_title(\"Permutation Importance of each feature\")\nax.set_ylabel(\"Features\")\nfig.tight_layout()\n()\n\n\n<img alt=\"Permutation Importance of each feature\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_release_highlights_0_22_0_002.png\" srcset=\"../../_images/sphx_glr_plot_release_highlights_0_22_0_002.png\"/>",
            "code"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 0.22->Native support for missing values for gradient boosting": [
        [
            "The \nand  now have native\nsupport for missing values (NaNs). This means that there is no need for\nimputing data when training or predicting.",
            "markdown"
        ],
        [
            "from sklearn.ensemble import \n\nX = ([0, 1, 2, ]).reshape(-1, 1)\ny = [0, 0, 1, 1]\n\ngbdt = (min_samples_leaf=1).fit(X, y)\nprint(gbdt.predict(X))",
            "code"
        ],
        [
            "[0 0 1 1]",
            "code"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 0.22->Precomputed sparse nearest neighbors graph": [
        [
            "Most estimators based on nearest neighbors graphs now accept precomputed\nsparse graphs as input, to reuse the same graph for multiple estimator fits.\nTo use this feature in a pipeline, one can use the memory parameter, along\nwith one of the two new transformers,\n and\n. The precomputation\ncan also be performed by custom estimators to use alternative\nimplementations, such as approximate nearest neighbors methods.\nSee more details in the .",
            "markdown"
        ],
        [
            "from tempfile import \nfrom sklearn.neighbors import \nfrom sklearn.manifold import \nfrom sklearn.pipeline import \n\nX, y = (random_state=0)\n\nwith (prefix=\"sklearn_cache_\") as tmpdir:\n    estimator = (\n        (n_neighbors=10, mode=\"distance\"),\n        (n_neighbors=10, metric=\"precomputed\"),\n        memory=tmpdir,\n    )\n    estimator.fit(X)\n\n    # We can decrease the number of neighbors and the graph will not be\n    # recomputed.\n    estimator.set_params(isomap__n_neighbors=5)\n    estimator.fit(X)",
            "code"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 0.22->KNN Based Imputation": [
        [
            "We now support imputation for completing missing values using k-Nearest\nNeighbors.",
            "markdown"
        ],
        [
            "Each sample\u2019s missing values are imputed using the mean value from\nn_neighbors nearest neighbors found in the training set. Two samples are\nclose if the features that neither is missing are close.\nBy default, a euclidean distance metric\nthat supports missing values,\nnan_euclidean_distances, is used to find the nearest\nneighbors.",
            "markdown"
        ],
        [
            "Read more in the .",
            "markdown"
        ],
        [
            "from sklearn.impute import \n\nX = [[1, 2, ], [3, 4, 3], [, 6, 5], [8, 8, 7]]\nimputer = (n_neighbors=2)\nprint(imputer.fit_transform(X))",
            "code"
        ],
        [
            "[[1.  2.  4. ]\n [3.  4.  3. ]\n [5.5 6.  5. ]\n [8.  8.  7. ]]",
            "code"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 0.22->Tree pruning": [
        [
            "It is now possible to prune most tree-based estimators once the trees are\nbuilt. The pruning is based on minimal cost-complexity. Read more in the\n for details.",
            "markdown"
        ],
        [
            "X, y = (random_state=0)\n\nrf = (random_state=0, ccp_alpha=0).fit(X, y)\nprint(\n    \"Average number of nodes without pruning {:.1f}\".format(\n        ([e.tree_.node_count for e in rf.estimators_])\n    )\n)\n\nrf = (random_state=0, ccp_alpha=0.05).fit(X, y)\nprint(\n    \"Average number of nodes with pruning {:.1f}\".format(\n        ([e.tree_.node_count for e in rf.estimators_])\n    )\n)",
            "code"
        ],
        [
            "Average number of nodes without pruning 22.3\nAverage number of nodes with pruning 6.4",
            "code"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 0.22->Retrieve dataframes from OpenML": [
        [
            "can now return pandas dataframe and thus\nproperly handle datasets with heterogeneous data:",
            "markdown"
        ],
        [
            "from sklearn.datasets import \n\ntitanic = (\"titanic\", version=1, as_frame=True, parser=\"pandas\")\nprint(titanic.data.head()[[\"pclass\", \"embarked\"]])",
            "code"
        ],
        [
            "pclass embarked\n0       1        S\n1       1        S\n2       1        S\n3       1        S\n4       1        S",
            "code"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 0.22->Checking scikit-learn compatibility of an estimator": [
        [
            "Developers can check the compatibility of their scikit-learn compatible\nestimators using . For\ninstance, the check_estimator(LinearSVC()) passes.",
            "markdown"
        ],
        [
            "We now provide a pytest specific decorator which allows pytest\nto run all checks independently and report the checks that are failing.\n\n..note::",
            "markdown"
        ],
        [
            "This entry was slightly updated in version 0.24, where passing classes\nisn\u2019t supported anymore: pass instances instead.\n\n</dl>",
            "markdown"
        ],
        [
            "from sklearn.linear_model import \nfrom sklearn.tree import \nfrom sklearn.utils.estimator_checks import \n\n\n@parametrize_with_checks([(), ()])\ndef test_sklearn_compatible_estimator(estimator, check):\n    check(estimator)",
            "code"
        ]
    ],
    "Examples->Release Highlights->Release Highlights for scikit-learn 0.22->ROC AUC now supports multiclass classification": [
        [
            "The roc_auc_score function can also be used in multi-class\nclassification. Two averaging strategies are currently supported: the\none-vs-one algorithm computes the average of the pairwise ROC AUC scores, and\nthe one-vs-rest algorithm computes the average of the ROC AUC scores for each\nclass against all other classes. In both cases, the multiclass ROC AUC scores\nare computed from the probability estimates that a sample belongs to a\nparticular class according to the model. The OvO and OvR algorithms support\nweighting uniformly (average='macro') and weighting by the prevalence\n(average='weighted').",
            "markdown"
        ],
        [
            "Read more in the .",
            "markdown"
        ],
        [
            "from sklearn.datasets import \nfrom sklearn.svm import \nfrom sklearn.metrics import \n\nX, y = (n_classes=4, n_informative=16)\nclf = (decision_function_shape=\"ovo\", probability=True).fit(X, y)\nprint((y, clf.predict_proba(X), multi_class=\"ovo\"))",
            "code"
        ],
        [
            "0.9863999999999998",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  1.545 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Biclustering->A demo of the Spectral Biclustering algorithm": [
        [
            "This example demonstrates how to generate a checkerboard dataset and\nbicluster it using the Spectral Biclustering algorithm.",
            "markdown"
        ],
        [
            "The data is generated with the make_checkerboard function, then\nshuffled and passed to the Spectral Biclustering algorithm. The rows\nand columns of the shuffled matrix are rearranged to show the\nbiclusters found by the algorithm.",
            "markdown"
        ],
        [
            "The outer product of the row and column label vectors shows a\nrepresentation of the checkerboard structure.\n\n<img alt=\"Original dataset\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_spectral_biclustering_001.png\" srcset=\"../../_images/sphx_glr_plot_spectral_biclustering_001.png\"/>\n<img alt=\"Shuffled dataset\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_spectral_biclustering_002.png\" srcset=\"../../_images/sphx_glr_plot_spectral_biclustering_002.png\"/>\n<img alt=\"After biclustering; rearranged to show biclusters\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_spectral_biclustering_003.png\" srcset=\"../../_images/sphx_glr_plot_spectral_biclustering_003.png\"/>\n<img alt=\"Checkerboard structure of rearranged data\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_spectral_biclustering_004.png\" srcset=\"../../_images/sphx_glr_plot_spectral_biclustering_004.png\"/>",
            "markdown"
        ],
        [
            "consensus score: 1.0\n\n\n\n<br/>",
            "code"
        ],
        [
            "# Author: Kemal Eren &lt;kemal@kemaleren.com\n# License: BSD 3 clause\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import \nfrom sklearn.cluster import \nfrom sklearn.metrics import \n\n\nn_clusters = (4, 3)\ndata, rows, columns = (\n    shape=(300, 300), n_clusters=n_clusters, noise=10, shuffle=False, random_state=0\n)\n\n(data, cmap=plt.cm.Blues)\n(\"Original dataset\")\n\n# shuffle clusters\nrng = (0)\nrow_idx = rng.permutation(data.shape[0])\ncol_idx = rng.permutation(data.shape[1])\ndata = data[row_idx][:, col_idx]\n\n(data, cmap=plt.cm.Blues)\n(\"Shuffled dataset\")\n\nmodel = (n_clusters=n_clusters, method=\"log\", random_state=0)\nmodel.fit(data)\nscore = (model.biclusters_, (rows[:, row_idx], columns[:, col_idx]))\n\nprint(\"consensus score: {:.1f}\".format(score))\n\nfit_data = data[(model.row_labels_)]\nfit_data = fit_data[:, (model.column_labels_)]\n\n(fit_data, cmap=plt.cm.Blues)\n(\"After biclustering; rearranged to show biclusters\")\n\n(\n    ((model.row_labels_) + 1, (model.column_labels_) + 1),\n    cmap=plt.cm.Blues,\n)\n(\"Checkerboard structure of rearranged data\")\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.573 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Biclustering->A demo of the Spectral Co-Clustering algorithm": [
        [
            "This example demonstrates how to generate a dataset and bicluster it\nusing the Spectral Co-Clustering algorithm.",
            "markdown"
        ],
        [
            "The dataset is generated using the make_biclusters function, which\ncreates a matrix of small values and implants bicluster with large\nvalues. The rows and columns are then shuffled and passed to the\nSpectral Co-Clustering algorithm. Rearranging the shuffled matrix to\nmake biclusters contiguous shows how accurately the algorithm found\nthe biclusters.\n\n<img alt=\"Original dataset\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_spectral_coclustering_001.png\" srcset=\"../../_images/sphx_glr_plot_spectral_coclustering_001.png\"/>\n<img alt=\"Shuffled dataset\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_spectral_coclustering_002.png\" srcset=\"../../_images/sphx_glr_plot_spectral_coclustering_002.png\"/>\n<img alt=\"After biclustering; rearranged to show biclusters\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_spectral_coclustering_003.png\" srcset=\"../../_images/sphx_glr_plot_spectral_coclustering_003.png\"/>",
            "markdown"
        ],
        [
            "consensus score: 1.000\n\n\n\n<br/>",
            "code"
        ],
        [
            "# Author: Kemal Eren &lt;kemal@kemaleren.com\n# License: BSD 3 clause\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import \nfrom sklearn.cluster import \nfrom sklearn.metrics import \n\ndata, rows, columns = (\n    shape=(300, 300), n_clusters=5, noise=5, shuffle=False, random_state=0\n)\n\n(data, cmap=plt.cm.Blues)\n(\"Original dataset\")\n\n# shuffle clusters\nrng = (0)\nrow_idx = rng.permutation(data.shape[0])\ncol_idx = rng.permutation(data.shape[1])\ndata = data[row_idx][:, col_idx]\n\n(data, cmap=plt.cm.Blues)\n(\"Shuffled dataset\")\n\nmodel = (n_clusters=5, random_state=0)\nmodel.fit(data)\nscore = (model.biclusters_, (rows[:, row_idx], columns[:, col_idx]))\n\nprint(\"consensus score: {:.3f}\".format(score))\n\nfit_data = data[(model.row_labels_)]\nfit_data = fit_data[:, (model.column_labels_)]\n\n(fit_data, cmap=plt.cm.Blues)\n(\"After biclustering; rearranged to show biclusters\")\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.374 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Biclustering->Biclustering documents with the Spectral Co-clustering algorithm": [
        [
            "This example demonstrates the Spectral Co-clustering algorithm on the\ntwenty newsgroups dataset. The \u2018comp.os.ms-windows.misc\u2019 category is\nexcluded because it contains many posts containing nothing but data.",
            "markdown"
        ],
        [
            "The TF-IDF vectorized posts form a word frequency matrix, which is\nthen biclustered using Dhillon\u2019s Spectral Co-Clustering algorithm. The\nresulting document-word biclusters indicate subsets words used more\noften in those subsets documents.",
            "markdown"
        ],
        [
            "For a few of the best biclusters, its most common document categories\nand its ten most important words get printed. The best biclusters are\ndetermined by their normalized cut. The best words are determined by\ncomparing their sums inside and outside the bicluster.",
            "markdown"
        ],
        [
            "For comparison, the documents are also clustered using\nMiniBatchKMeans. The document clusters derived from the biclusters\nachieve a better V-measure than clusters found by MiniBatchKMeans.",
            "markdown"
        ],
        [
            "Vectorizing...\nCoclustering...\nDone in 1.63s. V-measure: 0.4431\nMiniBatchKMeans...\nDone in 4.59s. V-measure: 0.3177\n\nBest biclusters:\n----------------\nbicluster 0 : 1961 documents, 4388 words\ncategories   : 23% talk.politics.guns, 18% talk.politics.misc, 17% sci.med\nwords        : gun, geb, guns, banks, gordon, clinton, pitt, cdt, surrender, veal\n\nbicluster 1 : 1269 documents, 3558 words\ncategories   : 27% soc.religion.christian, 25% talk.politics.mideast, 24% alt.atheism\nwords        : god, jesus, christians, sin, objective, kent, belief, christ, faith, moral\n\nbicluster 2 : 2201 documents, 2747 words\ncategories   : 18% comp.sys.mac.hardware, 17% comp.sys.ibm.pc.hardware, 16% comp.graphics\nwords        : voltage, board, dsp, packages, receiver, stereo, shipping, package, compression, image\n\nbicluster 3 : 1773 documents, 2620 words\ncategories   : 27% rec.motorcycles, 23% rec.autos, 13% misc.forsale\nwords        : bike, car, dod, engine, motorcycle, ride, honda, bikes, helmet, bmw\n\nbicluster 4 : 201 documents, 1175 words\ncategories   : 81% talk.politics.mideast, 10% alt.atheism, 7% soc.religion.christian\nwords        : turkish, armenia, armenian, armenians, turks, petch, sera, zuma, argic, gvg47\n\n\n\n<br/>",
            "code"
        ],
        [
            "from collections import \nimport operator\nfrom time import \n\nimport numpy as np\n\nfrom sklearn.cluster import \nfrom sklearn.cluster import \nfrom sklearn.datasets import \nfrom sklearn.feature_extraction.text import \nfrom sklearn.metrics.cluster import \n\n\ndef number_normalizer(tokens):\n    \"\"\"Map all numeric tokens to a placeholder.\n\n    For many applications, tokens that begin with a number are not directly\n    useful, but the fact that such a token exists can be relevant.  By applying\n    this form of dimensionality reduction, some methods may perform better.\n    \"\"\"\n    return (\"#NUMBER\" if token[0].isdigit() else token for token in tokens)\n\n\nclass NumberNormalizingVectorizer():\n    def build_tokenizer(self):\n        tokenize = super().build_tokenizer()\n        return lambda doc: list(number_normalizer(tokenize(doc)))\n\n\n# exclude 'comp.os.ms-windows.misc'\ncategories = [\n    \"alt.atheism\",\n    \"comp.graphics\",\n    \"comp.sys.ibm.pc.hardware\",\n    \"comp.sys.mac.hardware\",\n    \"comp.windows.x\",\n    \"misc.forsale\",\n    \"rec.autos\",\n    \"rec.motorcycles\",\n    \"rec.sport.baseball\",\n    \"rec.sport.hockey\",\n    \"sci.crypt\",\n    \"sci.electronics\",\n    \"sci.med\",\n    \"sci.space\",\n    \"soc.religion.christian\",\n    \"talk.politics.guns\",\n    \"talk.politics.mideast\",\n    \"talk.politics.misc\",\n    \"talk.religion.misc\",\n]\nnewsgroups = (categories=categories)\ny_true = newsgroups.target\n\nvectorizer = NumberNormalizingVectorizer(stop_words=\"english\", min_df=5)\ncocluster = (\n    n_clusters=len(categories), svd_method=\"arpack\", random_state=0\n)\nkmeans = (\n    n_clusters=len(categories), batch_size=20000, random_state=0, n_init=3\n)\n\nprint(\"Vectorizing...\")\nX = vectorizer.fit_transform(newsgroups.data)\n\nprint(\"Coclustering...\")\nstart_time = ()\ncocluster.fit(X)\ny_cocluster = cocluster.row_labels_\nprint(\n    \"Done in {:.2f}s. V-measure: {:.4f}\".format(\n        () - start_time, (y_cocluster, y_true)\n    )\n)\n\nprint(\"MiniBatchKMeans...\")\nstart_time = ()\ny_kmeans = kmeans.fit_predict(X)\nprint(\n    \"Done in {:.2f}s. V-measure: {:.4f}\".format(\n        () - start_time, (y_kmeans, y_true)\n    )\n)\n\nfeature_names = vectorizer.get_feature_names_out()\ndocument_names = list(newsgroups.target_names[i] for i in newsgroups.target)\n\n\ndef bicluster_ncut(i):\n    rows, cols = cocluster.get_indices(i)\n    if not ((rows) and (cols)):\n        import sys\n\n        return sys.float_info.max\n    row_complement = ((cocluster.rows_[i]))[0]\n    col_complement = ((cocluster.columns_[i]))[0]\n    # Note: the following is identical to X[rows[:, np.newaxis],\n    # cols].sum() but much faster in scipy &lt;= 0.16\n    weight = X[rows][:, cols].sum()\n    cut = X[row_complement][:, cols].sum() + X[rows][:, col_complement].sum()\n    return cut / weight\n\n\ndef most_common(d):\n    \"\"\"Items of a defaultdict(int) with the highest values.\n\n    Like Counter.most_common in Python =2.7.\n    \"\"\"\n    return sorted(d.items(), key=(1), reverse=True)\n\n\nbicluster_ncuts = list(bicluster_ncut(i) for i in range(len(newsgroups.target_names)))\nbest_idx = (bicluster_ncuts)[:5]\n\nprint()\nprint(\"Best biclusters:\")\nprint(\"----------------\")\nfor idx, cluster in enumerate(best_idx):\n    n_rows, n_cols = cocluster.get_shape(cluster)\n    cluster_docs, cluster_words = cocluster.get_indices(cluster)\n    if not len(cluster_docs) or not len(cluster_words):\n        continue\n\n    # categories\n    counter = (int)\n    for i in cluster_docs:\n        counter[document_names[i]] += 1\n    cat_string = \", \".join(\n        \"{:.0f}% {}\".format(float(c) / n_rows * 100, name)\n        for name, c in most_common(counter)[:3]\n    )\n\n    # words\n    out_of_cluster_docs = cocluster.row_labels_ != cluster\n    out_of_cluster_docs = (out_of_cluster_docs)[0]\n    word_col = X[:, cluster_words]\n    word_scores = (\n        word_col[cluster_docs, :].sum(axis=0)\n        - word_col[out_of_cluster_docs, :].sum(axis=0)\n    )\n    word_scores = word_scores.ravel()\n    important_words = list(\n        feature_names[cluster_words[i]] for i in word_scores.argsort()[:-11:-1]\n    )\n\n    print(\"bicluster {} : {} documents, {} words\".format(idx, n_rows, n_cols))\n    print(\"categories   : {}\".format(cat_string))\n    print(\"words        : {}\\n\".format(\", \".join(important_words)))",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  18.989 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Calibration->Comparison of Calibration of Classifiers": [
        [
            "Well calibrated classifiers are probabilistic classifiers for which the output\nof  can be directly interpreted as a confidence level.\nFor instance, a well calibrated (binary) classifier should classify the samples\nsuch that for the samples to which it gave a  value close\nto 0.8, approximately 80% actually belong to the positive class.",
            "markdown"
        ],
        [
            "In this example we will compare the calibration of four different\nmodels: , ,\n and .",
            "markdown"
        ],
        [
            "Author: Jan Hendrik Metzen &lt;>\nLicense: BSD 3 clause.",
            "markdown"
        ]
    ],
    "Examples->Calibration->Comparison of Calibration of Classifiers->Dataset": [
        [
            "We will use a synthetic binary classification dataset with 100,000 samples\nand 20 features. Of the 20 features, only 2 are informative, 2 are\nredundant (random combinations of the informative features) and the\nremaining 16 are uninformative (random numbers). Of the 100,000 samples,\n100 will be used for model fitting and the remaining for testing.",
            "markdown"
        ],
        [
            "from sklearn.datasets import \nfrom sklearn.model_selection import \n\nX, y = (\n    n_samples=100_000, n_features=20, n_informative=2, n_redundant=2, random_state=42\n)\n\ntrain_samples = 100  # Samples used for training the models\nX_train, X_test, y_train, y_test = (\n    X,\n    y,\n    shuffle=False,\n    test_size=100_000 - train_samples,\n)",
            "code"
        ]
    ],
    "Examples->Calibration->Comparison of Calibration of Classifiers->Calibration curves": [
        [
            "Below, we train each of the four models with the small training dataset, then\nplot calibration curves (also known as reliability diagrams) using\npredicted probabilities of the test dataset. Calibration curves are created\nby binning predicted probabilities, then plotting the mean predicted\nprobability in each bin against the observed frequency (\u2018fraction of\npositives\u2019). Below the calibration curve, we plot a histogram showing\nthe distribution of the predicted probabilities or more specifically,\nthe number of samples in each predicted probability bin.",
            "markdown"
        ],
        [
            "import numpy as np\n\nfrom sklearn.svm import \n\n\nclass NaivelyCalibratedLinearSVC():\n    \"\"\"LinearSVC with `predict_proba` method that naively scales\n    `decision_function` output.\"\"\"\n\n    def fit(self, X, y):\n        super().fit(X, y)\n        df = self.decision_function(X)\n        self.df_min_ = df.min()\n        self.df_max_ = df.max()\n\n    def predict_proba(self, X):\n        \"\"\"Min-max scale output of `decision_function` to [0,1].\"\"\"\n        df = self.decision_function(X)\n        calibrated_df = (df - self.df_min_) / (self.df_max_ - self.df_min_)\n        proba_pos_class = (calibrated_df, 0, 1)\n        proba_neg_class = 1 - proba_pos_class\n        proba = [proba_neg_class, proba_pos_class]\n        return proba",
            "code"
        ],
        [
            "from sklearn.calibration import CalibrationDisplay\nfrom sklearn.ensemble import \nfrom sklearn.linear_model import \nfrom sklearn.naive_bayes import \n\n# Create classifiers\nlr = ()\ngnb = ()\nsvc = NaivelyCalibratedLinearSVC(C=1.0)\nrfc = ()\n\nclf_list = [\n    (lr, \"Logistic\"),\n    (gnb, \"Naive Bayes\"),\n    (svc, \"SVC\"),\n    (rfc, \"Random forest\"),\n]",
            "code"
        ],
        [
            "import matplotlib.pyplot as plt\nfrom matplotlib.gridspec import \n\nfig = (figsize=(10, 10))\ngs = (4, 2)\ncolors = plt.cm.get_cmap(\"Dark2\")\n\nax_calibration_curve = fig.add_subplot(gs[:2, :2])\ncalibration_displays = {}\nmarkers = [\"^\", \"v\", \"s\", \"o\"]\nfor i, (clf, name) in enumerate(clf_list):\n    clf.fit(X_train, y_train)\n    display = (\n        clf,\n        X_test,\n        y_test,\n        n_bins=10,\n        name=name,\n        ax=ax_calibration_curve,\n        color=colors(i),\n        marker=markers[i],\n    )\n    calibration_displays[name] = display\n\nax_calibration_curve.grid()\nax_calibration_curve.set_title(\"Calibration plots\")\n\n# Add histogram\ngrid_positions = [(2, 0), (2, 1), (3, 0), (3, 1)]\nfor i, (_, name) in enumerate(clf_list):\n    row, col = grid_positions[i]\n    ax = fig.add_subplot(gs[row, col])\n\n    ax.hist(\n        calibration_displays[name].y_prob,\n        range=(0, 1),\n        bins=10,\n        label=name,\n        color=colors(i),\n    )\n    ax.set(title=name, xlabel=\"Mean predicted probability\", ylabel=\"Count\")\n\n()\n()\n\n\n<img alt=\"Calibration plots, Logistic, Naive Bayes, SVC, Random forest\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_compare_calibration_001.png\" srcset=\"../../_images/sphx_glr_plot_compare_calibration_001.png\"/>",
            "code"
        ],
        [
            "/home/circleci/project/sklearn/calibration.py:1176: UserWarning:\n\nmarker is redundantly defined by the 'marker' keyword argument and the fmt string \"s-\" (- marker='s'). The keyword argument will take precedence.\n\n/home/circleci/project/sklearn/calibration.py:1176: UserWarning:\n\nmarker is redundantly defined by the 'marker' keyword argument and the fmt string \"s-\" (- marker='s'). The keyword argument will take precedence.\n\n/home/circleci/project/sklearn/calibration.py:1176: UserWarning:\n\nmarker is redundantly defined by the 'marker' keyword argument and the fmt string \"s-\" (- marker='s'). The keyword argument will take precedence.\n\n/home/circleci/project/sklearn/calibration.py:1176: UserWarning:\n\nmarker is redundantly defined by the 'marker' keyword argument and the fmt string \"s-\" (- marker='s'). The keyword argument will take precedence.",
            "code"
        ],
        [
            "returns well calibrated\npredictions as it directly optimizes log-loss. In contrast, the other methods\nreturn biased probabilities, with different biases for each method:",
            "markdown"
        ],
        [
            "tends to push\nprobabilities to 0 or 1 (see histogram). This is mainly\nbecause the naive Bayes equation only provides correct estimate of\nprobabilities when the assumption that features are conditionally\nindependent holds . However, features tend to be positively correlated\nand is the case with this dataset, which contains 2 features\ngenerated as random linear combinations of the informative features. These\ncorrelated features are effectively being \u2018counted twice\u2019, resulting in\npushing the predicted probabilities towards 0 and 1 .",
            "markdown"
        ],
        [
            "shows the opposite\nbehavior: the histograms show peaks at approx. 0.2 and 0.9 probability,\nwhile probabilities close to 0 or 1 are very rare. An explanation for this\nis given by Niculescu-Mizil and Caruana : \u201cMethods such as bagging and\nrandom forests that average predictions from a base set of models can have\ndifficulty making predictions near 0 and 1 because variance in the\nunderlying base models will bias predictions that should be near zero or\none away from these values. Because predictions are restricted to the\ninterval [0,1], errors caused by variance tend to be one- sided near zero\nand one. For example, if a model should predict p = 0 for a case, the only\nway bagging can achieve this is if all bagged trees predict zero. If we add\nnoise to the trees that bagging is averaging over, this noise will cause\nsome trees to predict values larger than 0 for this case, thus moving the\naverage prediction of the bagged ensemble away from 0. We observe this\neffect most strongly with random forests because the base-level trees\ntrained with random forests have relatively high variance due to feature\nsubsetting.\u201d As a result, the calibration curve shows a characteristic\nsigmoid shape, indicating that the classifier is under-confident\nand could return probabilities closer to 0 or 1.",
            "markdown"
        ],
        [
            "To show the performance of , we naively\nscale the output of the  into [0, 1] by applying\nmin-max scaling, since SVC does not output probabilities by default.\n shows an\neven more sigmoid curve than the\n, which is typical for\nmaximum-margin methods  as they focus on difficult to classify samples\nthat are close to the decision boundary (the support vectors).",
            "markdown"
        ]
    ],
    "Examples->Calibration->Comparison of Calibration of Classifiers->References": [
        [
            ",\nA. Niculescu-Mizil & R. Caruana, ICML 2005\n\n\n[]",
            "markdown"
        ],
        [
            "Domingos, P., & Pazzani, M., Proc. 13th Intl. Conf. Machine Learning.\n1996.\n\n\n[]",
            "markdown"
        ],
        [
            "Zadrozny, Bianca, and Charles Elkan. Icml. Vol. 1. 2001.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  1.295 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Calibration->Probability Calibration curves": [
        [
            "When performing classification one often wants to predict not only the class\nlabel, but also the associated probability. This probability gives some\nkind of confidence on the prediction. This example demonstrates how to\nvisualize how well calibrated the predicted probabilities are using calibration\ncurves, also known as reliability diagrams. Calibration of an uncalibrated\nclassifier will also be demonstrated.",
            "markdown"
        ],
        [
            "# Author: Alexandre Gramfort &lt;alexandre.gramfort@telecom-paristech.fr\n#         Jan Hendrik Metzen &lt;jhm@informatik.uni-bremen.de\n# License: BSD 3 clause.",
            "code"
        ]
    ],
    "Examples->Calibration->Probability Calibration curves->Dataset": [
        [
            "We will use a synthetic binary classification dataset with 100,000 samples\nand 20 features. Of the 20 features, only 2 are informative, 10 are\nredundant (random combinations of the informative features) and the\nremaining 8 are uninformative (random numbers). Of the 100,000 samples, 1,000\nwill be used for model fitting and the rest for testing.",
            "markdown"
        ],
        [
            "from sklearn.datasets import \nfrom sklearn.model_selection import \n\nX, y = (\n    n_samples=100_000, n_features=20, n_informative=2, n_redundant=10, random_state=42\n)\n\nX_train, X_test, y_train, y_test = (\n    X, y, test_size=0.99, random_state=42\n)",
            "code"
        ]
    ],
    "Examples->Calibration->Probability Calibration curves->Calibration curves->Gaussian Naive Bayes": [
        [
            "First, we will compare:",
            "markdown"
        ],
        [
            "(used as baseline\nsince very often, properly regularized logistic regression is well\ncalibrated by default thanks to the use of the log-loss)",
            "markdown"
        ],
        [
            "Uncalibrated",
            "markdown"
        ],
        [
            "with isotonic and sigmoid\ncalibration (see )",
            "markdown"
        ],
        [
            "Calibration curves for all 4 conditions are plotted below, with the average\npredicted probability for each bin on the x-axis and the fraction of positive\nclasses in each bin on the y-axis.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\nfrom matplotlib.gridspec import \n\nfrom sklearn.calibration import , CalibrationDisplay\nfrom sklearn.linear_model import \nfrom sklearn.naive_bayes import \n\nlr = (C=1.0)\ngnb = ()\ngnb_isotonic = (gnb, cv=2, method=\"isotonic\")\ngnb_sigmoid = (gnb, cv=2, method=\"sigmoid\")\n\nclf_list = [\n    (lr, \"Logistic\"),\n    (gnb, \"Naive Bayes\"),\n    (gnb_isotonic, \"Naive Bayes + Isotonic\"),\n    (gnb_sigmoid, \"Naive Bayes + Sigmoid\"),\n]",
            "code"
        ],
        [
            "fig = (figsize=(10, 10))\ngs = (4, 2)\ncolors = plt.cm.get_cmap(\"Dark2\")\n\nax_calibration_curve = fig.add_subplot(gs[:2, :2])\ncalibration_displays = {}\nfor i, (clf, name) in enumerate(clf_list):\n    clf.fit(X_train, y_train)\n    display = (\n        clf,\n        X_test,\n        y_test,\n        n_bins=10,\n        name=name,\n        ax=ax_calibration_curve,\n        color=colors(i),\n    )\n    calibration_displays[name] = display\n\nax_calibration_curve.grid()\nax_calibration_curve.set_title(\"Calibration plots (Naive Bayes)\")\n\n# Add histogram\ngrid_positions = [(2, 0), (2, 1), (3, 0), (3, 1)]\nfor i, (_, name) in enumerate(clf_list):\n    row, col = grid_positions[i]\n    ax = fig.add_subplot(gs[row, col])\n\n    ax.hist(\n        calibration_displays[name].y_prob,\n        range=(0, 1),\n        bins=10,\n        label=name,\n        color=colors(i),\n    )\n    ax.set(title=name, xlabel=\"Mean predicted probability\", ylabel=\"Count\")\n\n()\n()\n\n\n<img alt=\"Calibration plots (Naive Bayes), Logistic, Naive Bayes, Naive Bayes + Isotonic, Naive Bayes + Sigmoid\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_calibration_curve_001.png\" srcset=\"../../_images/sphx_glr_plot_calibration_curve_001.png\"/>",
            "code"
        ],
        [
            "Uncalibrated  is poorly calibrated\nbecause of\nthe redundant features which violate the assumption of feature-independence\nand result in an overly confident classifier, which is indicated by the\ntypical transposed-sigmoid curve. Calibration of the probabilities of\n with  can fix\nthis issue as can be seen from the nearly diagonal calibration curve.\n also improves calibration\nslightly,\nalbeit not as strongly as the non-parametric isotonic regression. This can be\nattributed to the fact that we have plenty of calibration data such that the\ngreater flexibility of the non-parametric model can be exploited.",
            "markdown"
        ],
        [
            "Below we will make a quantitative analysis considering several classification\nmetrics: , ,\n and\n.",
            "markdown"
        ],
        [
            "from collections import \n\nimport pandas as pd\n\nfrom sklearn.metrics import (\n    ,\n    ,\n    ,\n    ,\n    ,\n    ,\n)\n\nscores = (list)\nfor i, (clf, name) in enumerate(clf_list):\n    clf.fit(X_train, y_train)\n    y_prob = clf.predict_proba(X_test)\n    y_pred = clf.predict(X_test)\n    scores[\"Classifier\"].append(name)\n\n    for metric in [, ]:\n        score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n        scores[score_name].append(metric(y_test, y_prob[:, 1]))\n\n    for metric in [, , , ]:\n        score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n        scores[score_name].append(metric(y_test, y_pred))\n\n    score_df = (scores).set_index(\"Classifier\")\n    score_df.round(decimals=3)\n\nscore_df\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "Notice that although calibration improves the  (a\nmetric composed\nof calibration term and refinement term) and , it does not\nsignificantly alter the prediction accuracy measures (precision, recall and\nF1 score).\nThis is because calibration should not significantly change prediction\nprobabilities at the location of the decision threshold (at x = 0.5 on the\ngraph). Calibration should however, make the predicted probabilities more\naccurate and thus more useful for making allocation decisions under\nuncertainty.\nFurther, ROC AUC, should not change at all because calibration is a\nmonotonic transformation. Indeed, no rank metrics are affected by\ncalibration.",
            "markdown"
        ]
    ],
    "Examples->Calibration->Probability Calibration curves->Calibration curves->Linear support vector classifier": [
        [
            "Next, we will compare:",
            "markdown"
        ],
        [
            "(baseline)",
            "markdown"
        ],
        [
            "Uncalibrated . Since SVC does not output\nprobabilities by default, we naively scale the output of the\n into [0, 1] by applying min-max scaling.",
            "markdown"
        ],
        [
            "with isotonic and sigmoid\ncalibration (see )",
            "markdown"
        ],
        [
            "import numpy as np\n\nfrom sklearn.svm import \n\n\nclass NaivelyCalibratedLinearSVC():\n    \"\"\"LinearSVC with `predict_proba` method that naively scales\n    `decision_function` output for binary classification.\"\"\"\n\n    def fit(self, X, y):\n        super().fit(X, y)\n        df = self.decision_function(X)\n        self.df_min_ = df.min()\n        self.df_max_ = df.max()\n\n    def predict_proba(self, X):\n        \"\"\"Min-max scale output of `decision_function` to [0, 1].\"\"\"\n        df = self.decision_function(X)\n        calibrated_df = (df - self.df_min_) / (self.df_max_ - self.df_min_)\n        proba_pos_class = (calibrated_df, 0, 1)\n        proba_neg_class = 1 - proba_pos_class\n        proba = [proba_neg_class, proba_pos_class]\n        return proba",
            "code"
        ],
        [
            "lr = (C=1.0)\nsvc = NaivelyCalibratedLinearSVC(max_iter=10_000)\nsvc_isotonic = (svc, cv=2, method=\"isotonic\")\nsvc_sigmoid = (svc, cv=2, method=\"sigmoid\")\n\nclf_list = [\n    (lr, \"Logistic\"),\n    (svc, \"SVC\"),\n    (svc_isotonic, \"SVC + Isotonic\"),\n    (svc_sigmoid, \"SVC + Sigmoid\"),\n]",
            "code"
        ],
        [
            "fig = (figsize=(10, 10))\ngs = (4, 2)\n\nax_calibration_curve = fig.add_subplot(gs[:2, :2])\ncalibration_displays = {}\nfor i, (clf, name) in enumerate(clf_list):\n    clf.fit(X_train, y_train)\n    display = (\n        clf,\n        X_test,\n        y_test,\n        n_bins=10,\n        name=name,\n        ax=ax_calibration_curve,\n        color=colors(i),\n    )\n    calibration_displays[name] = display\n\nax_calibration_curve.grid()\nax_calibration_curve.set_title(\"Calibration plots (SVC)\")\n\n# Add histogram\ngrid_positions = [(2, 0), (2, 1), (3, 0), (3, 1)]\nfor i, (_, name) in enumerate(clf_list):\n    row, col = grid_positions[i]\n    ax = fig.add_subplot(gs[row, col])\n\n    ax.hist(\n        calibration_displays[name].y_prob,\n        range=(0, 1),\n        bins=10,\n        label=name,\n        color=colors(i),\n    )\n    ax.set(title=name, xlabel=\"Mean predicted probability\", ylabel=\"Count\")\n\n()\n()\n\n\n<img alt=\"Calibration plots (SVC), Logistic, SVC, SVC + Isotonic, SVC + Sigmoid\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_calibration_curve_002.png\" srcset=\"../../_images/sphx_glr_plot_calibration_curve_002.png\"/>",
            "code"
        ],
        [
            "shows the opposite\nbehavior to ; the calibration\ncurve has a sigmoid shape, which is typical for an under-confident\nclassifier. In the case of , this is caused\nby the margin property of the hinge loss, which focuses on samples that are\nclose to the decision boundary (support vectors). Samples that are far\naway from the decision boundary do not impact the hinge loss. It thus makes\nsense that  does not try to separate samples\nin the high confidence region regions. This leads to flatter calibration\ncurves near 0 and 1 and is empirically shown with a variety of datasets\nin Niculescu-Mizil & Caruana .",
            "markdown"
        ],
        [
            "Both kinds of calibration (sigmoid and isotonic) can fix this issue and\nyield similar results.",
            "markdown"
        ],
        [
            "As before, we show the , ,\n and\n.",
            "markdown"
        ],
        [
            "scores = (list)\nfor i, (clf, name) in enumerate(clf_list):\n    clf.fit(X_train, y_train)\n    y_prob = clf.predict_proba(X_test)\n    y_pred = clf.predict(X_test)\n    scores[\"Classifier\"].append(name)\n\n    for metric in [, ]:\n        score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n        scores[score_name].append(metric(y_test, y_prob[:, 1]))\n\n    for metric in [, , , ]:\n        score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n        scores[score_name].append(metric(y_test, y_pred))\n\n    score_df = (scores).set_index(\"Classifier\")\n    score_df.round(decimals=3)\n\nscore_df\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "As with  above, calibration improves\nboth  and  but does not alter the\nprediction accuracy measures (precision, recall and F1 score) much.",
            "markdown"
        ]
    ],
    "Examples->Calibration->Probability Calibration curves->Summary": [
        [
            "Parametric sigmoid calibration can deal with situations where the calibration\ncurve of the base classifier is sigmoid (e.g., for\n) but not where it is transposed-sigmoid\n(e.g., ). Non-parametric\nisotonic calibration can deal with both situations but may require more\ndata to produce good results.",
            "markdown"
        ]
    ],
    "Examples->Calibration->Probability Calibration curves->References": [
        [
            ",\nA. Niculescu-Mizil & R. Caruana, ICML 2005",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  3.427 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Calibration->Probability Calibration for 3-class classification": [
        [
            "This example illustrates how sigmoid  changes\npredicted probabilities for a 3-class classification problem. Illustrated is\nthe standard 2-simplex, where the three corners correspond to the three\nclasses. Arrows point from the probability vectors predicted by an uncalibrated\nclassifier to the probability vectors predicted by the same classifier after\nsigmoid calibration on a hold-out validation set. Colors indicate the true\nclass of an instance (red: class 1, green: class 2, blue: class 3).",
            "markdown"
        ]
    ],
    "Examples->Calibration->Probability Calibration for 3-class classification->Data": [
        [
            "Below, we generate a classification dataset with 2000 samples, 2 features\nand 3 target classes. We then split the data as follows:",
            "markdown"
        ],
        [
            "train: 600 samples (for training the classifier)",
            "markdown"
        ],
        [
            "valid: 400 samples (for calibrating predicted probabilities)",
            "markdown"
        ],
        [
            "test: 1000 samples",
            "markdown"
        ],
        [
            "Note that we also create X_train_valid and y_train_valid, which consists\nof both the train and valid subsets. This is used when we only want to train\nthe classifier but not calibrate the predicted probabilities.",
            "markdown"
        ],
        [
            "# Author: Jan Hendrik Metzen &lt;jhm@informatik.uni-bremen.de\n# License: BSD Style.\n\nimport numpy as np\nfrom sklearn.datasets import \n\n(0)\n\nX, y = (\n    n_samples=2000, n_features=2, centers=3, random_state=42, cluster_std=5.0\n)\nX_train, y_train = X[:600], y[:600]\nX_valid, y_valid = X[600:1000], y[600:1000]\nX_train_valid, y_train_valid = X[:1000], y[:1000]\nX_test, y_test = X[1000:], y[1000:]",
            "code"
        ]
    ],
    "Examples->Calibration->Probability Calibration for 3-class classification->Fitting and calibration": [
        [
            "First, we will train a \nwith 25 base estimators (trees) on the concatenated train and validation\ndata (1000 samples). This is the uncalibrated classifier.",
            "markdown"
        ],
        [
            "from sklearn.ensemble import \n\nclf = (n_estimators=25)\nclf.fit(X_train_valid, y_train_valid)",
            "code"
        ],
        [
            "RandomForestClassifier(n_estimators=25)<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input checked=\"\" class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-24\">RandomForestClassifier</label>",
            "code"
        ],
        [
            "RandomForestClassifier(n_estimators=25)\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "To train the calibrated classifier, we start with the same\n but train it using only\nthe train data subset (600 samples) then calibrate, with method='sigmoid',\nusing the valid data subset (400 samples) in a 2-stage process.",
            "markdown"
        ],
        [
            "from sklearn.calibration import \n\nclf = (n_estimators=25)\nclf.fit(X_train, y_train)\ncal_clf = (clf, method=\"sigmoid\", cv=\"prefit\")\ncal_clf.fit(X_valid, y_valid)",
            "code"
        ],
        [
            "CalibratedClassifierCV(cv='prefit',\n                       estimator=RandomForestClassifier(n_estimators=25))<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-25\">CalibratedClassifierCV</label>",
            "code"
        ],
        [
            "CalibratedClassifierCV(cv='prefit',\n                       estimator=RandomForestClassifier(n_estimators=25))<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-26\">estimator: RandomForestClassifier</label>",
            "code"
        ],
        [
            "RandomForestClassifier(n_estimators=25)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-27\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-27\">RandomForestClassifier</label>",
            "code"
        ],
        [
            "RandomForestClassifier(n_estimators=25)\n\n<br/>\n<br/>",
            "code"
        ]
    ],
    "Examples->Calibration->Probability Calibration for 3-class classification->Compare probabilities": [
        [
            "Below we plot a 2-simplex with arrows showing the change in predicted\nprobabilities of the test samples.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n\n(figsize=(10, 10))\ncolors = [\"r\", \"g\", \"b\"]\n\nclf_probs = clf.predict_proba(X_test)\ncal_clf_probs = cal_clf.predict_proba(X_test)\n# Plot arrows\nfor i in range(clf_probs.shape[0]):\n    (\n        clf_probs[i, 0],\n        clf_probs[i, 1],\n        cal_clf_probs[i, 0] - clf_probs[i, 0],\n        cal_clf_probs[i, 1] - clf_probs[i, 1],\n        color=colors[y_test[i]],\n        head_width=1e-2,\n    )\n\n# Plot perfect predictions, at each vertex\n([1.0], [0.0], \"ro\", ms=20, label=\"Class 1\")\n([0.0], [1.0], \"go\", ms=20, label=\"Class 2\")\n([0.0], [0.0], \"bo\", ms=20, label=\"Class 3\")\n\n# Plot boundaries of unit simplex\n([0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], \"k\", label=\"Simplex\")\n\n# Annotate points 6 points around the simplex, and mid point inside simplex\n(\n    r\"($\\frac{1}{3}$, $\\frac{1}{3}$, $\\frac{1}{3}$)\",\n    xy=(1.0 / 3, 1.0 / 3),\n    xytext=(1.0 / 3, 0.23),\n    xycoords=\"data\",\n    arrowprops=dict(facecolor=\"black\", shrink=0.05),\n    horizontalalignment=\"center\",\n    verticalalignment=\"center\",\n)\n([1.0 / 3], [1.0 / 3], \"ko\", ms=5)\n(\n    r\"($\\frac{1}{2}$, $0$, $\\frac{1}{2}$)\",\n    xy=(0.5, 0.0),\n    xytext=(0.5, 0.1),\n    xycoords=\"data\",\n    arrowprops=dict(facecolor=\"black\", shrink=0.05),\n    horizontalalignment=\"center\",\n    verticalalignment=\"center\",\n)\n(\n    r\"($0$, $\\frac{1}{2}$, $\\frac{1}{2}$)\",\n    xy=(0.0, 0.5),\n    xytext=(0.1, 0.5),\n    xycoords=\"data\",\n    arrowprops=dict(facecolor=\"black\", shrink=0.05),\n    horizontalalignment=\"center\",\n    verticalalignment=\"center\",\n)\n(\n    r\"($\\frac{1}{2}$, $\\frac{1}{2}$, $0$)\",\n    xy=(0.5, 0.5),\n    xytext=(0.6, 0.6),\n    xycoords=\"data\",\n    arrowprops=dict(facecolor=\"black\", shrink=0.05),\n    horizontalalignment=\"center\",\n    verticalalignment=\"center\",\n)\n(\n    r\"($0$, $0$, $1$)\",\n    xy=(0, 0),\n    xytext=(0.1, 0.1),\n    xycoords=\"data\",\n    arrowprops=dict(facecolor=\"black\", shrink=0.05),\n    horizontalalignment=\"center\",\n    verticalalignment=\"center\",\n)\n(\n    r\"($1$, $0$, $0$)\",\n    xy=(1, 0),\n    xytext=(1, 0.1),\n    xycoords=\"data\",\n    arrowprops=dict(facecolor=\"black\", shrink=0.05),\n    horizontalalignment=\"center\",\n    verticalalignment=\"center\",\n)\n(\n    r\"($0$, $1$, $0$)\",\n    xy=(0, 1),\n    xytext=(0.1, 1),\n    xycoords=\"data\",\n    arrowprops=dict(facecolor=\"black\", shrink=0.05),\n    horizontalalignment=\"center\",\n    verticalalignment=\"center\",\n)\n# Add grid\n(False)\nfor x in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n    ([0, x], [x, 0], \"k\", alpha=0.2)\n    ([0, 0 + (1 - x) / 2], [x, x + (1 - x) / 2], \"k\", alpha=0.2)\n    ([x, x + (1 - x) / 2], [0, 0 + (1 - x) / 2], \"k\", alpha=0.2)\n\n(\"Change of predicted probabilities on test samples after sigmoid calibration\")\n(\"Probability class 1\")\n(\"Probability class 2\")\n(-0.05, 1.05)\n(-0.05, 1.05)\n_ = (loc=\"best\")\n\n\n<img alt=\"Change of predicted probabilities on test samples after sigmoid calibration\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_calibration_multiclass_001.png\" srcset=\"../../_images/sphx_glr_plot_calibration_multiclass_001.png\"/>",
            "code"
        ],
        [
            "In the figure above, each vertex of the simplex represents\na perfectly predicted class (e.g., 1, 0, 0). The mid point\ninside the simplex represents predicting the three classes with equal\nprobability (i.e., 1/3, 1/3, 1/3). Each arrow starts at the\nuncalibrated probabilities and end with the arrow head at the calibrated\nprobability. The color of the arrow represents the true class of that test\nsample.",
            "markdown"
        ],
        [
            "The uncalibrated classifier is overly confident in its predictions and\nincurs a large . The calibrated classifier incurs\na lower  due to two factors. First, notice in the\nfigure above that the arrows generally point away from the edges of the\nsimplex, where the probability of one class is 0. Second, a large proportion\nof the arrows point towards the true class, e.g., green arrows (samples where\nthe true class is \u2018green\u2019) generally point towards the green vertex. This\nresults in fewer over-confident, 0 predicted probabilities and at the same\ntime an increase in the predicted probabilities of the correct class.\nThus, the calibrated classifier produces more accurate predicted probabilities\nthat incur a lower",
            "markdown"
        ],
        [
            "We can show this objectively by comparing the  of\nthe uncalibrated and calibrated classifiers on the predictions of the 1000\ntest samples. Note that an alternative would have been to increase the number\nof base estimators (trees) of the\n which would have resulted\nin a similar decrease in .",
            "markdown"
        ],
        [
            "from sklearn.metrics import \n\nscore = (y_test, clf_probs)\ncal_score = (y_test, cal_clf_probs)\n\nprint(\"Log-loss of\")\nprint(f\" * uncalibrated classifier: {score:.3f}\")\nprint(f\" * calibrated classifier: {cal_score:.3f}\")",
            "code"
        ],
        [
            "Log-loss of\n * uncalibrated classifier: 1.327\n * calibrated classifier: 0.549",
            "code"
        ],
        [
            "Finally we generate a grid of possible uncalibrated probabilities over\nthe 2-simplex, compute the corresponding calibrated probabilities and\nplot arrows for each. The arrows are colored according the highest\nuncalibrated probability. This illustrates the learned calibration map:",
            "markdown"
        ],
        [
            "(figsize=(10, 10))\n# Generate grid of probability values\np1d = (0, 1, 20)\np0, p1 = (p1d, p1d)\np2 = 1 - p0 - p1\np = [p0.ravel(), p1.ravel(), p2.ravel()]\np = p[p[:, 2] = 0]\n\n# Use the three class-wise calibrators to compute calibrated probabilities\ncalibrated_classifier = cal_clf.calibrated_classifiers_[0]\nprediction = (\n    [\n        calibrator.predict(this_p)\n        for calibrator, this_p in zip(calibrated_classifier.calibrators, p.T)\n    ]\n).T\n\n# Re-normalize the calibrated predictions to make sure they stay inside the\n# simplex. This same renormalization step is performed internally by the\n# predict method of CalibratedClassifierCV on multiclass problems.\nprediction /= prediction.sum(axis=1)[:, None]\n\n# Plot changes in predicted probabilities induced by the calibrators\nfor i in range(prediction.shape[0]):\n    (\n        p[i, 0],\n        p[i, 1],\n        prediction[i, 0] - p[i, 0],\n        prediction[i, 1] - p[i, 1],\n        head_width=1e-2,\n        color=colors[(p[i])],\n    )\n\n# Plot the boundaries of the unit simplex\n([0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], \"k\", label=\"Simplex\")\n\n(False)\nfor x in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n    ([0, x], [x, 0], \"k\", alpha=0.2)\n    ([0, 0 + (1 - x) / 2], [x, x + (1 - x) / 2], \"k\", alpha=0.2)\n    ([x, x + (1 - x) / 2], [0, 0 + (1 - x) / 2], \"k\", alpha=0.2)\n\n(\"Learned sigmoid calibration map\")\n(\"Probability class 1\")\n(\"Probability class 2\")\n(-0.05, 1.05)\n(-0.05, 1.05)\n\n()\n\n\n<img alt=\"Learned sigmoid calibration map\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_calibration_multiclass_002.png\" srcset=\"../../_images/sphx_glr_plot_calibration_multiclass_002.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  2.080 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Calibration->Probability calibration of classifiers": [
        [
            "When performing classification you often want to predict not only\nthe class label, but also the associated probability. This probability\ngives you some kind of confidence on the prediction. However, not all\nclassifiers provide well-calibrated probabilities, some being over-confident\nwhile others being under-confident. Thus, a separate calibration of predicted\nprobabilities is often desirable as a postprocessing. This example illustrates\ntwo different methods for this calibration and evaluates the quality of the\nreturned probabilities using Brier\u2019s score\n(see ).",
            "markdown"
        ],
        [
            "Compared are the estimated probability using a Gaussian naive Bayes classifier\nwithout calibration, with a sigmoid calibration, and with a non-parametric\nisotonic calibration. One can observe that only the non-parametric model is\nable to provide a probability calibration that returns probabilities close\nto the expected 0.5 for most of the samples belonging to the middle\ncluster with heterogeneous labels. This results in a significantly improved\nBrier score.",
            "markdown"
        ],
        [
            "# Authors:\n# Mathieu Blondel &lt;mathieu@mblondel.org\n# Alexandre Gramfort &lt;alexandre.gramfort@telecom-paristech.fr\n# Balazs Kegl &lt;balazs.kegl@gmail.com\n# Jan Hendrik Metzen &lt;jhm@informatik.uni-bremen.de\n# License: BSD Style.",
            "code"
        ]
    ],
    "Examples->Calibration->Probability calibration of classifiers->Generate synthetic dataset": [
        [
            "import numpy as np\n\nfrom sklearn.datasets import \nfrom sklearn.model_selection import \n\nn_samples = 50000\nn_bins = 3  # use 3 bins for calibration_curve as we have 3 clusters here\n\n# Generate 3 blobs with 2 classes where the second blob contains\n# half positive samples and half negative samples. Probability in this\n# blob is therefore 0.5.\ncenters = [(-5, -5), (0, 0), (5, 5)]\nX, y = (n_samples=n_samples, centers=centers, shuffle=False, random_state=42)\n\ny[: n_samples // 2] = 0\ny[n_samples // 2 :] = 1\nsample_weight = (42).rand(y.shape[0])\n\n# split train, test for calibration\nX_train, X_test, y_train, y_test, sw_train, sw_test = (\n    X, y, sample_weight, test_size=0.9, random_state=42\n)",
            "code"
        ]
    ],
    "Examples->Calibration->Probability calibration of classifiers->Gaussian Naive-Bayes": [
        [
            "from sklearn.calibration import \nfrom sklearn.metrics import \nfrom sklearn.naive_bayes import \n\n# With no calibration\nclf = ()\nclf.fit(X_train, y_train)  # GaussianNB itself does not support sample-weights\nprob_pos_clf = clf.predict_proba(X_test)[:, 1]\n\n# With isotonic calibration\nclf_isotonic = (clf, cv=2, method=\"isotonic\")\nclf_isotonic.fit(X_train, y_train, sample_weight=sw_train)\nprob_pos_isotonic = clf_isotonic.predict_proba(X_test)[:, 1]\n\n# With sigmoid calibration\nclf_sigmoid = (clf, cv=2, method=\"sigmoid\")\nclf_sigmoid.fit(X_train, y_train, sample_weight=sw_train)\nprob_pos_sigmoid = clf_sigmoid.predict_proba(X_test)[:, 1]\n\nprint(\"Brier score losses: (the smaller the better)\")\n\nclf_score = (y_test, prob_pos_clf, sample_weight=sw_test)\nprint(\"No calibration: %1.3f\" % clf_score)\n\nclf_isotonic_score = (y_test, prob_pos_isotonic, sample_weight=sw_test)\nprint(\"With isotonic calibration: %1.3f\" % clf_isotonic_score)\n\nclf_sigmoid_score = (y_test, prob_pos_sigmoid, sample_weight=sw_test)\nprint(\"With sigmoid calibration: %1.3f\" % clf_sigmoid_score)",
            "code"
        ],
        [
            "Brier score losses: (the smaller the better)\nNo calibration: 0.104\nWith isotonic calibration: 0.084\nWith sigmoid calibration: 0.109",
            "code"
        ]
    ],
    "Examples->Calibration->Probability calibration of classifiers->Plot data and the predicted probabilities": [
        [
            "from matplotlib import cm\nimport matplotlib.pyplot as plt\n\n()\ny_unique = (y)\ncolors = cm.rainbow((0.0, 1.0, y_unique.size))\nfor this_y, color in zip(y_unique, colors):\n    this_X = X_train[y_train == this_y]\n    this_sw = sw_train[y_train == this_y]\n    (\n        this_X[:, 0],\n        this_X[:, 1],\n        s=this_sw * 50,\n        c=color[, :],\n        alpha=0.5,\n        edgecolor=\"k\",\n        label=\"Class %s\" % this_y,\n    )\n(loc=\"best\")\n(\"Data\")\n\n()\n\norder = ((prob_pos_clf,))\n(prob_pos_clf[order], \"r\", label=\"No calibration (%1.3f)\" % clf_score)\n(\n    prob_pos_isotonic[order],\n    \"g\",\n    linewidth=3,\n    label=\"Isotonic calibration (%1.3f)\" % clf_isotonic_score,\n)\n(\n    prob_pos_sigmoid[order],\n    \"b\",\n    linewidth=3,\n    label=\"Sigmoid calibration (%1.3f)\" % clf_sigmoid_score,\n)\n(\n    (0, y_test.size, 51)[1::2],\n    y_test[order].reshape(25, -1).mean(1),\n    \"k\",\n    linewidth=3,\n    label=r\"Empirical\",\n)\n([-0.05, 1.05])\n(\"Instances sorted according to predicted probability (uncalibrated GNB)\")\n(\"P(y=1)\")\n(loc=\"upper left\")\n(\"Gaussian naive Bayes probabilities\")\n\n()\n\n\n\n<img alt=\"Data\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_calibration_001.png\" srcset=\"../../_images/sphx_glr_plot_calibration_001.png\"/>\n<img alt=\"Gaussian naive Bayes probabilities\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_calibration_002.png\" srcset=\"../../_images/sphx_glr_plot_calibration_002.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.397 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Classification->Classifier comparison": [
        [
            "A comparison of a several classifiers in scikit-learn on synthetic datasets.\nThe point of this example is to illustrate the nature of decision boundaries\nof different classifiers.\nThis should be taken with a grain of salt, as the intuition conveyed by\nthese examples does not necessarily carry over to real datasets.",
            "markdown"
        ],
        [
            "Particularly in high-dimensional spaces, data can more easily be separated\nlinearly and the simplicity of classifiers such as naive Bayes and linear SVMs\nmight lead to better generalization than is achieved by other classifiers.",
            "markdown"
        ],
        [
            "The plots show training points in solid colors and testing points\nsemi-transparent. The lower right shows the classification accuracy on the test\nset.\n<img alt=\"Input data, Nearest Neighbors, Linear SVM, RBF SVM, Gaussian Process, Decision Tree, Random Forest, Neural Net, AdaBoost, Naive Bayes, QDA\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_classifier_comparison_001.png\" srcset=\"../../_images/sphx_glr_plot_classifier_comparison_001.png\"/>",
            "markdown"
        ],
        [
            "# Code source: Ga\u00ebl Varoquaux\n#              Andreas M\u00fcller\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import \nfrom sklearn.model_selection import \nfrom sklearn.preprocessing import \nfrom sklearn.pipeline import \nfrom sklearn.datasets import , , \nfrom sklearn.neural_network import \nfrom sklearn.neighbors import \nfrom sklearn.svm import \nfrom sklearn.gaussian_process import \nfrom sklearn.gaussian_process.kernels import \nfrom sklearn.tree import \nfrom sklearn.ensemble import , \nfrom sklearn.naive_bayes import \nfrom sklearn.discriminant_analysis import \nfrom sklearn.inspection import DecisionBoundaryDisplay\n\nnames = [\n    \"Nearest Neighbors\",\n    \"Linear SVM\",\n    \"RBF SVM\",\n    \"Gaussian Process\",\n    \"Decision Tree\",\n    \"Random Forest\",\n    \"Neural Net\",\n    \"AdaBoost\",\n    \"Naive Bayes\",\n    \"QDA\",\n]\n\nclassifiers = [\n    (3),\n    (kernel=\"linear\", C=0.025),\n    (gamma=2, C=1),\n    (1.0 * (1.0)),\n    (max_depth=5),\n    (max_depth=5, n_estimators=10, max_features=1),\n    (alpha=1, max_iter=1000),\n    (),\n    (),\n    (),\n]\n\nX, y = (\n    n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1\n)\nrng = (2)\nX += 2 * rng.uniform(size=X.shape)\nlinearly_separable = (X, y)\n\ndatasets = [\n    (noise=0.3, random_state=0),\n    (noise=0.2, factor=0.5, random_state=1),\n    linearly_separable,\n]\n\nfigure = (figsize=(27, 9))\ni = 1\n# iterate over datasets\nfor ds_cnt, ds in enumerate(datasets):\n    # preprocess dataset, split into training and test part\n    X, y = ds\n    X_train, X_test, y_train, y_test = (\n        X, y, test_size=0.4, random_state=42\n    )\n\n    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n\n    # just plot the dataset first\n    cm = plt.cm.RdBu\n    cm_bright = ([\"#FF0000\", \"#0000FF\"])\n    ax = (len(datasets), len(classifiers) + 1, i)\n    if ds_cnt == 0:\n        ax.set_title(\"Input data\")\n    # Plot the training points\n    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\")\n    # Plot the testing points\n    ax.scatter(\n        X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6, edgecolors=\"k\"\n    )\n    ax.set_xlim(x_min, x_max)\n    ax.set_ylim(y_min, y_max)\n    ax.set_xticks(())\n    ax.set_yticks(())\n    i += 1\n\n    # iterate over classifiers\n    for name, clf in zip(names, classifiers):\n        ax = (len(datasets), len(classifiers) + 1, i)\n\n        clf = ((), clf)\n        clf.fit(X_train, y_train)\n        score = clf.score(X_test, y_test)\n        (\n            clf, X, cmap=cm, alpha=0.8, ax=ax, eps=0.5\n        )\n\n        # Plot the training points\n        ax.scatter(\n            X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\"\n        )\n        # Plot the testing points\n        ax.scatter(\n            X_test[:, 0],\n            X_test[:, 1],\n            c=y_test,\n            cmap=cm_bright,\n            edgecolors=\"k\",\n            alpha=0.6,\n        )\n\n        ax.set_xlim(x_min, x_max)\n        ax.set_ylim(y_min, y_max)\n        ax.set_xticks(())\n        ax.set_yticks(())\n        if ds_cnt == 0:\n            ax.set_title(name)\n        ax.text(\n            x_max - 0.3,\n            y_min + 0.3,\n            (\"%.2f\" % score).lstrip(\"0\"),\n            size=15,\n            horizontalalignment=\"right\",\n        )\n        i += 1\n\n()\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  3.167 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Classification->Linear and Quadratic Discriminant Analysis with covariance ellipsoid": [
        [
            "This example plots the covariance ellipsoids of each class and\ndecision boundary learned by LDA and QDA. The ellipsoids display\nthe double standard deviation for each class. With LDA, the\nstandard deviation is the same for all the classes, while each\nclass has its own standard deviation with QDA.",
            "markdown"
        ]
    ],
    "Examples->Classification->Linear and Quadratic Discriminant Analysis with covariance ellipsoid->Colormap": [
        [
            "import matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom matplotlib import colors\n\ncmap = (\n    \"red_blue_classes\",\n    {\n        \"red\": [(0, 1, 1), (1, 0.7, 0.7)],\n        \"green\": [(0, 0.7, 0.7), (1, 0.7, 0.7)],\n        \"blue\": [(0, 0.7, 0.7), (1, 1, 1)],\n    },\n)\nplt.cm.register_cmap(cmap=cmap)",
            "code"
        ]
    ],
    "Examples->Classification->Linear and Quadratic Discriminant Analysis with covariance ellipsoid->Datasets generation functions": [
        [
            "import numpy as np\n\n\ndef dataset_fixed_cov():\n    \"\"\"Generate 2 Gaussians samples with the same covariance matrix\"\"\"\n    n, dim = 300, 2\n    (0)\n    C = ([[0.0, -0.23], [0.83, 0.23]])\n    X = [\n        ((n, dim), C),\n        ((n, dim), C) + ([1, 1]),\n    ]\n    y = (((n), (n)))\n    return X, y\n\n\ndef dataset_cov():\n    \"\"\"Generate 2 Gaussians samples with different covariance matrices\"\"\"\n    n, dim = 300, 2\n    (0)\n    C = ([[0.0, -1.0], [2.5, 0.7]]) * 2.0\n    X = [\n        ((n, dim), C),\n        ((n, dim), C.T) + ([1, 4]),\n    ]\n    y = (((n), (n)))\n    return X, y",
            "code"
        ]
    ],
    "Examples->Classification->Linear and Quadratic Discriminant Analysis with covariance ellipsoid->Plot functions": [
        [
            "from scipy import linalg\n\n\ndef plot_data(lda, X, y, y_pred, fig_index):\n    splot = (2, 2, fig_index)\n    if fig_index == 1:\n        (\"Linear Discriminant Analysis\")\n        (\"Data with\\n fixed covariance\")\n    elif fig_index == 2:\n        (\"Quadratic Discriminant Analysis\")\n    elif fig_index == 3:\n        (\"Data with\\n varying covariances\")\n\n    tp = y == y_pred  # True Positive\n    tp0, tp1 = tp[y == 0], tp[y == 1]\n    X0, X1 = X[y == 0], X[y == 1]\n    X0_tp, X0_fp = X0[tp0], X0[~tp0]\n    X1_tp, X1_fp = X1[tp1], X1[~tp1]\n\n    # class 0: dots\n    (X0_tp[:, 0], X0_tp[:, 1], marker=\".\", color=\"red\")\n    (X0_fp[:, 0], X0_fp[:, 1], marker=\"x\", s=20, color=\"#990000\")  # dark red\n\n    # class 1: dots\n    (X1_tp[:, 0], X1_tp[:, 1], marker=\".\", color=\"blue\")\n    (\n        X1_fp[:, 0], X1_fp[:, 1], marker=\"x\", s=20, color=\"#000099\"\n    )  # dark blue\n\n    # class 0 and 1 : areas\n    nx, ny = 200, 100\n    x_min, x_max = ()\n    y_min, y_max = ()\n    xx, yy = ((x_min, x_max, nx), (y_min, y_max, ny))\n    Z = lda.predict_proba([xx.ravel(), yy.ravel()])\n    Z = Z[:, 1].reshape(xx.shape)\n    (\n        xx, yy, Z, cmap=\"red_blue_classes\", norm=(0.0, 1.0), zorder=0\n    )\n    (xx, yy, Z, [0.5], linewidths=2.0, colors=\"white\")\n\n    # means\n    (\n        lda.means_[0][0],\n        lda.means_[0][1],\n        \"*\",\n        color=\"yellow\",\n        markersize=15,\n        markeredgecolor=\"grey\",\n    )\n    (\n        lda.means_[1][0],\n        lda.means_[1][1],\n        \"*\",\n        color=\"yellow\",\n        markersize=15,\n        markeredgecolor=\"grey\",\n    )\n\n    return splot\n\n\ndef plot_ellipse(splot, mean, cov, color):\n    v, w = (cov)\n    u = w[0] / (w[0])\n    angle = (u[1] / u[0])\n    angle = 180 * angle /   # convert to degrees\n    # filled Gaussian at 2 standard deviation\n    ell = (\n        mean,\n        2 * v[0] ** 0.5,\n        2 * v[1] ** 0.5,\n        angle=180 + angle,\n        facecolor=color,\n        edgecolor=\"black\",\n        linewidth=2,\n    )\n    ell.set_clip_box(splot.bbox)\n    ell.set_alpha(0.2)\n    splot.add_artist(ell)\n    splot.set_xticks(())\n    splot.set_yticks(())\n\n\ndef plot_lda_cov(lda, splot):\n    plot_ellipse(splot, lda.means_[0], lda.covariance_, \"red\")\n    plot_ellipse(splot, lda.means_[1], lda.covariance_, \"blue\")\n\n\ndef plot_qda_cov(qda, splot):\n    plot_ellipse(splot, qda.means_[0], qda.covariance_[0], \"red\")\n    plot_ellipse(splot, qda.means_[1], qda.covariance_[1], \"blue\")",
            "code"
        ]
    ],
    "Examples->Classification->Linear and Quadratic Discriminant Analysis with covariance ellipsoid->Plot": [
        [
            "(figsize=(10, 8), facecolor=\"white\")\n(\n    \"Linear Discriminant Analysis vs Quadratic Discriminant Analysis\",\n    y=0.98,\n    fontsize=15,\n)\n\nfrom sklearn.discriminant_analysis import \nfrom sklearn.discriminant_analysis import \n\nfor i, (X, y) in enumerate([dataset_fixed_cov(), dataset_cov()]):\n    # Linear Discriminant Analysis\n    lda = (solver=\"svd\", store_covariance=True)\n    y_pred = lda.fit(X, y).predict(X)\n    splot = plot_data(lda, X, y, y_pred, fig_index=2 * i + 1)\n    plot_lda_cov(lda, splot)\n    (\"tight\")\n\n    # Quadratic Discriminant Analysis\n    qda = (store_covariance=True)\n    y_pred = qda.fit(X, y).predict(X)\n    splot = plot_data(qda, X, y, y_pred, fig_index=2 * i + 2)\n    plot_qda_cov(qda, splot)\n    (\"tight\")\n\n()\n(top=0.92)\n()\n\n\n<img alt=\"Linear Discriminant Analysis vs Quadratic Discriminant Analysis, Linear Discriminant Analysis, Quadratic Discriminant Analysis\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_lda_qda_001.png\" srcset=\"../../_images/sphx_glr_plot_lda_qda_001.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.409 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Classification->Normal, Ledoit-Wolf and OAS Linear Discriminant Analysis for classification": [
        [
            "This example illustrates how the Ledoit-Wolf and Oracle Shrinkage\nApproximating (OAS) estimators of covariance can improve classification.\n<img alt=\"LDA (Linear Discriminant Analysis) vs.  LDA with Ledoit Wolf vs.  LDA with OAS (1 discriminative feature)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_lda_001.png\" srcset=\"../../_images/sphx_glr_plot_lda_001.png\"/>",
            "markdown"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import \nfrom sklearn.discriminant_analysis import \nfrom sklearn.covariance import \n\n\nn_train = 20  # samples for training\nn_test = 200  # samples for testing\nn_averages = 50  # how often to repeat classification\nn_features_max = 75  # maximum number of features\nstep = 4  # step size for the calculation\n\n\ndef generate_data(n_samples, n_features):\n    \"\"\"Generate random blob-ish data with noisy features.\n\n    This returns an array of input data with shape `(n_samples, n_features)`\n    and an array of `n_samples` target labels.\n\n    Only one feature contains discriminative information, the other features\n    contain only noise.\n    \"\"\"\n    X, y = (n_samples=n_samples, n_features=1, centers=[[-2], [2]])\n\n    # add non-discriminative features\n    if n_features  1:\n        X = ([X, (n_samples, n_features - 1)])\n    return X, y\n\n\nacc_clf1, acc_clf2, acc_clf3 = [], [], []\nn_features_range = range(1, n_features_max + 1, step)\nfor n_features in n_features_range:\n    score_clf1, score_clf2, score_clf3 = 0, 0, 0\n    for _ in range(n_averages):\n        X, y = generate_data(n_train, n_features)\n\n        clf1 = (solver=\"lsqr\", shrinkage=None).fit(X, y)\n        clf2 = (solver=\"lsqr\", shrinkage=\"auto\").fit(X, y)\n        oa = (store_precision=False, assume_centered=False)\n        clf3 = (solver=\"lsqr\", covariance_estimator=oa).fit(\n            X, y\n        )\n\n        X, y = generate_data(n_test, n_features)\n        score_clf1 += clf1.score(X, y)\n        score_clf2 += clf2.score(X, y)\n        score_clf3 += clf3.score(X, y)\n\n    acc_clf1.append(score_clf1 / n_averages)\n    acc_clf2.append(score_clf2 / n_averages)\n    acc_clf3.append(score_clf3 / n_averages)\n\nfeatures_samples_ratio = (n_features_range) / n_train\n\n(\n    features_samples_ratio,\n    acc_clf1,\n    linewidth=2,\n    label=\"LDA\",\n    color=\"gold\",\n    linestyle=\"solid\",\n)\n(\n    features_samples_ratio,\n    acc_clf2,\n    linewidth=2,\n    label=\"LDA with Ledoit Wolf\",\n    color=\"navy\",\n    linestyle=\"dashed\",\n)\n(\n    features_samples_ratio,\n    acc_clf3,\n    linewidth=2,\n    label=\"LDA with OAS\",\n    color=\"red\",\n    linestyle=\"dotted\",\n)\n\n(\"n_features / n_samples\")\n(\"Classification accuracy\")\n\n(loc=\"lower left\")\n((0.65, 1.0))\n(\n    \"LDA (Linear Discriminant Analysis) vs. \"\n    + \"\\n\"\n    + \"LDA with Ledoit Wolf vs. \"\n    + \"\\n\"\n    + \"LDA with OAS (1 discriminative feature)\"\n)\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  8.002 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Classification->Plot classification probability": [
        [
            "Plot the classification probability for different classifiers. We use a 3 class\ndataset, and we classify it with a Support Vector classifier, L1 and L2\npenalized logistic regression with either a One-Vs-Rest or multinomial setting,\nand Gaussian process classification.",
            "markdown"
        ],
        [
            "Linear SVC is not a probabilistic classifier by default but it has a built-in\ncalibration option enabled in this example (probability=True).",
            "markdown"
        ],
        [
            "The logistic regression with One-Vs-Rest is not a multiclass classifier out of\nthe box. As a result it has more trouble in separating class 2 and 3 than the\nother estimators.\n<img alt=\"Class 0, Class 1, Class 2, Class 0, Class 1, Class 2, Class 0, Class 1, Class 2, Class 0, Class 1, Class 2, Class 0, Class 1, Class 2, Probability\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_classification_probability_001.png\" srcset=\"../../_images/sphx_glr_plot_classification_probability_001.png\"/>",
            "markdown"
        ],
        [
            "Accuracy (train) for L1 logistic: 83.3%\nAccuracy (train) for L2 logistic (Multinomial): 82.7%\nAccuracy (train) for L2 logistic (OvR): 79.3%\nAccuracy (train) for Linear SVC: 82.0%\nAccuracy (train) for GPC: 82.7%\n\n\n\n<br/>",
            "code"
        ],
        [
            "# Author: Alexandre Gramfort &lt;alexandre.gramfort@inria.fr\n# License: BSD 3 clause\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.metrics import \nfrom sklearn.linear_model import \nfrom sklearn.svm import \nfrom sklearn.gaussian_process import \nfrom sklearn.gaussian_process.kernels import \nfrom sklearn import datasets\n\niris = ()\nX = iris.data[:, 0:2]  # we only take the first two features for visualization\ny = iris.target\n\nn_features = X.shape[1]\n\nC = 10\nkernel = 1.0 * ([1.0, 1.0])  # for GPC\n\n# Create different classifiers.\nclassifiers = {\n    \"L1 logistic\": (\n        C=C, penalty=\"l1\", solver=\"saga\", multi_class=\"multinomial\", max_iter=10000\n    ),\n    \"L2 logistic (Multinomial)\": (\n        C=C, penalty=\"l2\", solver=\"saga\", multi_class=\"multinomial\", max_iter=10000\n    ),\n    \"L2 logistic (OvR)\": (\n        C=C, penalty=\"l2\", solver=\"saga\", multi_class=\"ovr\", max_iter=10000\n    ),\n    \"Linear SVC\": (kernel=\"linear\", C=C, probability=True, random_state=0),\n    \"GPC\": (kernel),\n}\n\nn_classifiers = len(classifiers)\n\n(figsize=(3 * 2, n_classifiers * 2))\n(bottom=0.2, top=0.95)\n\nxx = (3, 9, 100)\nyy = (1, 5, 100).T\nxx, yy = (xx, yy)\nXfull = [xx.ravel(), yy.ravel()]\n\nfor index, (name, classifier) in enumerate(classifiers.items()):\n    classifier.fit(X, y)\n\n    y_pred = classifier.predict(X)\n    accuracy = (y, y_pred)\n    print(\"Accuracy (train) for %s: %0.1f%% \" % (name, accuracy * 100))\n\n    # View probabilities:\n    probas = classifier.predict_proba(Xfull)\n    n_classes = (y_pred).size\n    for k in range(n_classes):\n        (n_classifiers, n_classes, index * n_classes + k + 1)\n        (\"Class %d\" % k)\n        if k == 0:\n            (name)\n        imshow_handle = (\n            probas[:, k].reshape((100, 100)), extent=(3, 9, 1, 5), origin=\"lower\"\n        )\n        (())\n        (())\n        idx = y_pred == k\n        if idx.any():\n            (X[idx, 0], X[idx, 1], marker=\"o\", c=\"w\", edgecolor=\"k\")\n\nax = ([0.15, 0.04, 0.7, 0.05])\n(\"Probability\")\n(imshow_handle, cax=ax, orientation=\"horizontal\")\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  1.575 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Classification->Recognizing hand-written digits": [
        [
            "This example shows how scikit-learn can be used to recognize images of\nhand-written digits, from 0-9.",
            "markdown"
        ],
        [
            "# Author: Gael Varoquaux &lt;gael dot varoquaux at normalesup dot org\n# License: BSD 3 clause\n\n# Standard scientific Python imports\nimport matplotlib.pyplot as plt\n\n# Import datasets, classifiers and performance metrics\nfrom sklearn import datasets, svm, metrics\nfrom sklearn.model_selection import",
            "code"
        ]
    ],
    "Examples->Classification->Recognizing hand-written digits->Digits dataset": [
        [
            "The digits dataset consists of 8x8\npixel images of digits. The images attribute of the dataset stores\n8x8 arrays of grayscale values for each image. We will use these arrays to\nvisualize the first 4 images. The target attribute of the dataset stores\nthe digit each image represents and this is included in the title of the 4\nplots below.",
            "markdown"
        ],
        [
            "Note: if we were working from image files (e.g., \u2018png\u2019 files), we would load\nthem using .",
            "markdown"
        ],
        [
            "digits = ()\n\n_, axes = (nrows=1, ncols=4, figsize=(10, 3))\nfor ax, image, label in zip(axes, digits.images, digits.target):\n    ax.set_axis_off()\n    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n    ax.set_title(\"Training: %i\" % label)\n\n\n<img alt=\"Training: 0, Training: 1, Training: 2, Training: 3\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_digits_classification_001.png\" srcset=\"../../_images/sphx_glr_plot_digits_classification_001.png\"/>",
            "code"
        ]
    ],
    "Examples->Classification->Recognizing hand-written digits->Classification": [
        [
            "To apply a classifier on this data, we need to flatten the images, turning\neach 2-D array of grayscale values from shape (8, 8) into shape\n(64,). Subsequently, the entire dataset will be of shape\n(n_samples, n_features), where n_samples is the number of images and\nn_features is the total number of pixels in each image.",
            "markdown"
        ],
        [
            "We can then split the data into train and test subsets and fit a support\nvector classifier on the train samples. The fitted classifier can\nsubsequently be used to predict the value of the digit for the samples\nin the test subset.",
            "markdown"
        ],
        [
            "# flatten the images\nn_samples = len(digits.images)\ndata = digits.images.reshape((n_samples, -1))\n\n# Create a classifier: a support vector classifier\nclf = (gamma=0.001)\n\n# Split data into 50% train and 50% test subsets\nX_train, X_test, y_train, y_test = (\n    data, digits.target, test_size=0.5, shuffle=False\n)\n\n# Learn the digits on the train subset\nclf.fit(X_train, y_train)\n\n# Predict the value of the digit on the test subset\npredicted = clf.predict(X_test)",
            "code"
        ],
        [
            "Below we visualize the first 4 test samples and show their predicted\ndigit value in the title.",
            "markdown"
        ],
        [
            "_, axes = (nrows=1, ncols=4, figsize=(10, 3))\nfor ax, image, prediction in zip(axes, X_test, predicted):\n    ax.set_axis_off()\n    image = image.reshape(8, 8)\n    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n    ax.set_title(f\"Prediction: {prediction}\")\n\n\n<img alt=\"Prediction: 8, Prediction: 8, Prediction: 4, Prediction: 9\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_digits_classification_002.png\" srcset=\"../../_images/sphx_glr_plot_digits_classification_002.png\"/>",
            "code"
        ],
        [
            "builds a text report showing\nthe main classification metrics.",
            "markdown"
        ],
        [
            "print(\n    f\"Classification report for classifier {clf}:\\n\"\n    f\"{(y_test, predicted)}\\n\"\n)",
            "code"
        ],
        [
            "Classification report for classifier SVC(gamma=0.001):\n              precision    recall  f1-score   support\n\n           0       1.00      0.99      0.99        88\n           1       0.99      0.97      0.98        91\n           2       0.99      0.99      0.99        86\n           3       0.98      0.87      0.92        91\n           4       0.99      0.96      0.97        92\n           5       0.95      0.97      0.96        91\n           6       0.99      0.99      0.99        91\n           7       0.96      0.99      0.97        89\n           8       0.94      1.00      0.97        88\n           9       0.93      0.98      0.95        92\n\n    accuracy                           0.97       899\n   macro avg       0.97      0.97      0.97       899\nweighted avg       0.97      0.97      0.97       899",
            "code"
        ],
        [
            "We can also plot a  of the\ntrue digit values and the predicted digit values.",
            "markdown"
        ],
        [
            "disp = (y_test, predicted)\ndisp.figure_.suptitle(\"Confusion Matrix\")\nprint(f\"Confusion matrix:\\n{disp.confusion_matrix}\")\n\n()\n\n\n<img alt=\"Confusion Matrix\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_digits_classification_003.png\" srcset=\"../../_images/sphx_glr_plot_digits_classification_003.png\"/>",
            "code"
        ],
        [
            "Confusion matrix:\n[[87  0  0  0  1  0  0  0  0  0]\n [ 0 88  1  0  0  0  0  0  1  1]\n [ 0  0 85  1  0  0  0  0  0  0]\n [ 0  0  0 79  0  3  0  4  5  0]\n [ 0  0  0  0 88  0  0  0  0  4]\n [ 0  0  0  0  0 88  1  0  0  2]\n [ 0  1  0  0  0  0 90  0  0  0]\n [ 0  0  0  0  0  1  0 88  0  0]\n [ 0  0  0  0  0  0  0  0 88  0]\n [ 0  0  0  1  0  1  0  0  0 90]]",
            "code"
        ],
        [
            "If the results from evaluating a classifier are stored in the form of a\n and not in terms of y_true and\ny_pred, one can still build a \nas follows:",
            "markdown"
        ],
        [
            "# The ground truth and predicted lists\ny_true = []\ny_pred = []\ncm = disp.confusion_matrix\n\n# For each cell in the confusion matrix, add the corresponding ground truths\n# and predictions to the lists\nfor gt in range(len(cm)):\n    for pred in range(len(cm)):\n        y_true += [gt] * cm[gt][pred]\n        y_pred += [pred] * cm[gt][pred]\n\nprint(\n    \"Classification report rebuilt from confusion matrix:\\n\"\n    f\"{(y_true, y_pred)}\\n\"\n)",
            "code"
        ],
        [
            "Classification report rebuilt from confusion matrix:\n              precision    recall  f1-score   support\n\n           0       1.00      0.99      0.99        88\n           1       0.99      0.97      0.98        91\n           2       0.99      0.99      0.99        86\n           3       0.98      0.87      0.92        91\n           4       0.99      0.96      0.97        92\n           5       0.95      0.97      0.96        91\n           6       0.99      0.99      0.99        91\n           7       0.96      0.99      0.97        89\n           8       0.94      1.00      0.97        88\n           9       0.93      0.98      0.95        92\n\n    accuracy                           0.97       899\n   macro avg       0.97      0.97      0.97       899\nweighted avg       0.97      0.97      0.97       899",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.525 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Clustering->A demo of K-Means clustering on the handwritten digits data": [
        [
            "In this example we compare the various initialization strategies for K-means in\nterms of runtime and quality of the results.",
            "markdown"
        ],
        [
            "As the ground truth is known here, we also apply different cluster quality\nmetrics to judge the goodness of fit of the cluster labels to the ground truth.",
            "markdown"
        ],
        [
            "Cluster quality metrics evaluated (see  for\ndefinitions and discussions of the metrics):",
            "markdown"
        ]
    ],
    "Examples->Clustering->A demo of K-Means clustering on the handwritten digits data->Load the dataset": [
        [
            "We will start by loading the digits dataset. This dataset contains\nhandwritten digits from 0 to 9. In the context of clustering, one would like\nto group images such that the handwritten digits on the image are the same.",
            "markdown"
        ],
        [
            "import numpy as np\nfrom sklearn.datasets import \n\ndata, labels = (return_X_y=True)\n(n_samples, n_features), n_digits = data.shape, (labels).size\n\nprint(f\"# digits: {n_digits}; # samples: {n_samples}; # features {n_features}\")",
            "code"
        ],
        [
            "# digits: 10; # samples: 1797; # features 64",
            "code"
        ]
    ],
    "Examples->Clustering->A demo of K-Means clustering on the handwritten digits data->Define our evaluation benchmark": [
        [
            "We will first our evaluation benchmark. During this benchmark, we intend to\ncompare different initialization methods for KMeans. Our benchmark will:",
            "markdown"
        ],
        [
            "create a pipeline which will scale the data using a\n;",
            "markdown"
        ],
        [
            "train and time the pipeline fitting;",
            "markdown"
        ],
        [
            "measure the performance of the clustering obtained via different metrics.",
            "markdown"
        ],
        [
            "from time import \nfrom sklearn import metrics\nfrom sklearn.pipeline import \nfrom sklearn.preprocessing import \n\n\ndef bench_k_means(kmeans, name, data, labels):\n    \"\"\"Benchmark to evaluate the KMeans initialization methods.\n\n    Parameters\n    ----------\n    kmeans : KMeans instance\n        A :class:`~sklearn.cluster.KMeans` instance with the initialization\n        already set.\n    name : str\n        Name given to the strategy. It will be used to show the results in a\n        table.\n    data : ndarray of shape (n_samples, n_features)\n        The data to cluster.\n    labels : ndarray of shape (n_samples,)\n        The labels used to compute the clustering metrics which requires some\n        supervision.\n    \"\"\"\n    t0 = ()\n    estimator = ((), kmeans).fit(data)\n    fit_time = () - t0\n    results = [name, fit_time, estimator[-1].inertia_]\n\n    # Define the metrics which require only the true labels and estimator\n    # labels\n    clustering_metrics = [\n        ,\n        ,\n        ,\n        ,\n        ,\n    ]\n    results += [m(labels, estimator[-1].labels_) for m in clustering_metrics]\n\n    # The silhouette score requires the full dataset\n    results += [\n        (\n            data,\n            estimator[-1].labels_,\n            metric=\"euclidean\",\n            sample_size=300,\n        )\n    ]\n\n    # Show the results\n    formatter_result = (\n        \"{:9s}\\t{:.3f}s\\t{:.0f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\"\n    )\n    print(formatter_result.format(*results))",
            "code"
        ]
    ],
    "Examples->Clustering->A demo of K-Means clustering on the handwritten digits data->Run the benchmark": [
        [
            "We will compare three approaches:",
            "markdown"
        ],
        [
            "an initialization using k-means++. This method is stochastic and we will\nrun the initialization 4 times;",
            "markdown"
        ],
        [
            "a random initialization. This method is stochastic as well and we will run\nthe initialization 4 times;",
            "markdown"
        ],
        [
            "an initialization based on a \nprojection. Indeed, we will use the components of the\n to initialize KMeans. This method is\ndeterministic and a single initialization suffice.",
            "markdown"
        ],
        [
            "from sklearn.cluster import \nfrom sklearn.decomposition import \n\nprint(82 * \"_\")\nprint(\"init\\t\\ttime\\tinertia\\thomo\\tcompl\\tv-meas\\tARI\\tAMI\\tsilhouette\")\n\nkmeans = (init=\"k-means++\", n_clusters=n_digits, n_init=4, random_state=0)\nbench_k_means(kmeans=kmeans, name=\"k-means++\", data=data, labels=labels)\n\nkmeans = (init=\"random\", n_clusters=n_digits, n_init=4, random_state=0)\nbench_k_means(kmeans=kmeans, name=\"random\", data=data, labels=labels)\n\npca = (n_components=n_digits).fit(data)\nkmeans = (init=pca.components_, n_clusters=n_digits, n_init=1)\nbench_k_means(kmeans=kmeans, name=\"PCA-based\", data=data, labels=labels)\n\nprint(82 * \"_\")",
            "code"
        ],
        [
            "__________________________________________________________________________________\ninit            time    inertia homo    compl   v-meas  ARI     AMI     silhouette\nk-means++       0.054s  69662   0.680   0.719   0.699   0.570   0.695   0.181\nrandom          0.031s  69707   0.675   0.716   0.694   0.560   0.691   0.174\nPCA-based       0.018s  72686   0.636   0.658   0.647   0.521   0.643   0.142\n__________________________________________________________________________________",
            "code"
        ]
    ],
    "Examples->Clustering->A demo of K-Means clustering on the handwritten digits data->Visualize the results on PCA-reduced data": [
        [
            "allows to project the data from the\noriginal 64-dimensional space into a lower dimensional space. Subsequently,\nwe can use  to project into a\n2-dimensional space and plot the data and the clusters in this new space.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n\nreduced_data = (n_components=2).fit_transform(data)\nkmeans = (init=\"k-means++\", n_clusters=n_digits, n_init=4)\nkmeans.fit(reduced_data)\n\n# Step size of the mesh. Decrease to increase the quality of the VQ.\nh = 0.02  # point in the mesh [x_min, x_max]x[y_min, y_max].\n\n# Plot the decision boundary. For that, we will assign a color to each\nx_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\ny_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\nxx, yy = ((x_min, x_max, h), (y_min, y_max, h))\n\n# Obtain labels for each point in mesh. Use last trained model.\nZ = kmeans.predict([xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\n(1)\n()\n(\n    Z,\n    interpolation=\"nearest\",\n    extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n    cmap=plt.cm.Paired,\n    aspect=\"auto\",\n    origin=\"lower\",\n)\n\n(reduced_data[:, 0], reduced_data[:, 1], \"k.\", markersize=2)\n# Plot the centroids as a white X\ncentroids = kmeans.cluster_centers_\n(\n    centroids[:, 0],\n    centroids[:, 1],\n    marker=\"x\",\n    s=169,\n    linewidths=3,\n    color=\"w\",\n    zorder=10,\n)\n(\n    \"K-means clustering on the digits dataset (PCA-reduced data)\\n\"\n    \"Centroids are marked with white cross\"\n)\n(x_min, x_max)\n(y_min, y_max)\n(())\n(())\n()\n\n\n<img alt=\"K-means clustering on the digits dataset (PCA-reduced data) Centroids are marked with white cross\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_kmeans_digits_001.png\" srcset=\"../../_images/sphx_glr_plot_kmeans_digits_001.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  1.060 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Clustering->A demo of structured Ward hierarchical clustering on an image of coins": [
        [
            "Compute the segmentation of a 2D image with Ward hierarchical\nclustering. The clustering is spatially constrained in order\nfor each segmented region to be in one piece.",
            "markdown"
        ],
        [
            "# Author : Vincent Michel, 2010\n#          Alexandre Gramfort, 2011\n# License: BSD 3 clause",
            "code"
        ]
    ],
    "Examples->Clustering->A demo of structured Ward hierarchical clustering on an image of coins->Generate data": [
        [
            "from skimage.data import coins\n\norig_coins = coins()",
            "code"
        ],
        [
            "Resize it to 20% of the original size to speed up the processing\nApplying a Gaussian filter for smoothing prior to down-scaling\nreduces aliasing artifacts.",
            "markdown"
        ],
        [
            "import numpy as np\nfrom scipy.ndimage import \nfrom skimage.transform import rescale\n\nsmoothened_coins = (orig_coins, sigma=2)\nrescaled_coins = rescale(\n    smoothened_coins,\n    0.2,\n    mode=\"reflect\",\n    anti_aliasing=False,\n)\n\nX = (rescaled_coins, (-1, 1))",
            "code"
        ]
    ],
    "Examples->Clustering->A demo of structured Ward hierarchical clustering on an image of coins->Define structure of the data": [
        [
            "Pixels are connected to their neighbors.",
            "markdown"
        ],
        [
            "from sklearn.feature_extraction.image import grid_to_graph\n\nconnectivity = grid_to_graph(*rescaled_coins.shape)",
            "code"
        ]
    ],
    "Examples->Clustering->A demo of structured Ward hierarchical clustering on an image of coins->Compute clustering": [
        [
            "import time as time\n\nfrom sklearn.cluster import \n\nprint(\"Compute structured hierarchical clustering...\")\nst = ()\nn_clusters = 27  # number of regions\nward = (\n    n_clusters=n_clusters, linkage=\"ward\", connectivity=connectivity\n)\nward.fit(X)\nlabel = (ward.labels_, rescaled_coins.shape)\nprint(f\"Elapsed time: {() - st:.3f}s\")\nprint(f\"Number of pixels: {label.size}\")\nprint(f\"Number of clusters: {(label).size}\")",
            "code"
        ],
        [
            "Compute structured hierarchical clustering...\nElapsed time: 0.169s\nNumber of pixels: 4697\nNumber of clusters: 27",
            "code"
        ]
    ],
    "Examples->Clustering->A demo of structured Ward hierarchical clustering on an image of coins->Plot the results on an image": [
        [
            "Agglomerative clustering is able to segment each coin however, we have had to\nuse a n_cluster larger than the number of coins because the segmentation\nis finding a large in the background.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n\n(figsize=(5, 5))\n(rescaled_coins, cmap=plt.cm.gray)\nfor l in range(n_clusters):\n    (\n        label == l,\n        colors=[\n            plt.cm.nipy_spectral(l / float(n_clusters)),\n        ],\n    )\n(\"off\")\n()\n\n\n<img alt=\"plot coin ward segmentation\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_coin_ward_segmentation_001.png\" srcset=\"../../_images/sphx_glr_plot_coin_ward_segmentation_001.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.661 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Clustering->A demo of the mean-shift clustering algorithm": [
        [
            "Reference:",
            "markdown"
        ],
        [
            "Dorin Comaniciu and Peter Meer, \u201cMean Shift: A robust approach toward\nfeature space analysis\u201d. IEEE Transactions on Pattern Analysis and\nMachine Intelligence. 2002. pp. 603-619.",
            "markdown"
        ],
        [
            "import numpy as np\nfrom sklearn.cluster import , \nfrom sklearn.datasets import",
            "code"
        ]
    ],
    "Examples->Clustering->A demo of the mean-shift clustering algorithm->Generate sample data": [
        [
            "centers = [[1, 1], [-1, -1], [1, -1]]\nX, _ = (n_samples=10000, centers=centers, cluster_std=0.6)",
            "code"
        ]
    ],
    "Examples->Clustering->A demo of the mean-shift clustering algorithm->Compute clustering with MeanShift": [
        [
            "# The following bandwidth can be automatically detected using\nbandwidth = (X, quantile=0.2, n_samples=500)\n\nms = (bandwidth=bandwidth, bin_seeding=True)\nms.fit(X)\nlabels = ms.labels_\ncluster_centers = ms.cluster_centers_\n\nlabels_unique = (labels)\nn_clusters_ = len(labels_unique)\n\nprint(\"number of estimated clusters : %d\" % n_clusters_)",
            "code"
        ],
        [
            "number of estimated clusters : 3",
            "code"
        ]
    ],
    "Examples->Clustering->A demo of the mean-shift clustering algorithm->Plot result": [
        [
            "import matplotlib.pyplot as plt\n\n(1)\n()\n\ncolors = [\"#dede00\", \"#377eb8\", \"#f781bf\"]\nmarkers = [\"x\", \"o\", \"^\"]\n\nfor k, col in zip(range(n_clusters_), colors):\n    my_members = labels == k\n    cluster_center = cluster_centers[k]\n    (X[my_members, 0], X[my_members, 1], markers[k], color=col)\n    (\n        cluster_center[0],\n        cluster_center[1],\n        markers[k],\n        markerfacecolor=col,\n        markeredgecolor=\"k\",\n        markersize=14,\n    )\n(\"Estimated number of clusters: %d\" % n_clusters_)\n()\n\n\n<img alt=\"Estimated number of clusters: 3\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_mean_shift_001.png\" srcset=\"../../_images/sphx_glr_plot_mean_shift_001.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.523 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Clustering->Adjustment for chance in clustering performance evaluation": [
        [
            "This notebook explores the impact of uniformly-distributed random labeling on\nthe behavior of some clustering evaluation metrics. For such purpose, the\nmetrics are computed with a fixed number of samples and as a function of the number\nof clusters assigned by the estimator. The example is divided into two\nexperiments:",
            "markdown"
        ],
        [
            "a first experiment with fixed \u201cground truth labels\u201d (and therefore fixed\nnumber of classes) and randomly \u201cpredicted labels\u201d;",
            "markdown"
        ],
        [
            "a second experiment with varying \u201cground truth labels\u201d, randomly \u201cpredicted\nlabels\u201d. The \u201cpredicted labels\u201d have the same number of classes and clusters\nas the \u201cground truth labels\u201d.",
            "markdown"
        ],
        [
            "# Author: Olivier Grisel &lt;olivier.grisel@ensta.org\n#         Arturo Amor &lt;david-arturo.amor-quiroz@inria.fr\n# License: BSD 3 clause",
            "code"
        ]
    ],
    "Examples->Clustering->Adjustment for chance in clustering performance evaluation->Defining the list of metrics to evaluate": [
        [
            "Clustering algorithms are fundamentally unsupervised learning methods.\nHowever, since we assign class labels for the synthetic clusters in this\nexample, it is possible to use evaluation metrics that leverage this\n\u201csupervised\u201d ground truth information to quantify the quality of the resulting\nclusters. Examples of such metrics are the following:",
            "markdown"
        ],
        [
            "V-measure, the harmonic mean of completeness and homogeneity;",
            "markdown"
        ],
        [
            "Rand index, which measures how frequently pairs of data points are grouped\nconsistently according to the result of the clustering algorithm and the\nground truth class assignment;",
            "markdown"
        ],
        [
            "Adjusted Rand index (ARI), a chance-adjusted Rand index such that a random\ncluster assignment has an ARI of 0.0 in expectation;",
            "markdown"
        ],
        [
            "Mutual Information (MI) is an information theoretic measure that quantifies\nhow dependent are the two labelings. Note that the maximum value of MI for\nperfect labelings depends on the number of clusters and samples;",
            "markdown"
        ],
        [
            "Normalized Mutual Information (NMI), a Mutual Information defined between 0\n(no mutual information) in the limit of large number of data points and 1\n(perfectly matching label assignments, up to a permutation of the labels).\nIt is not adjusted for chance: then the number of clustered data points is\nnot large enough, the expected values of MI or NMI for random labelings can\nbe significantly non-zero;",
            "markdown"
        ],
        [
            "Adjusted Mutual Information (AMI), a chance-adjusted Mutual Information.\nSimilarly to ARI, random cluster assignment has an AMI of 0.0 in\nexpectation.",
            "markdown"
        ],
        [
            "For more information, see the  module.",
            "markdown"
        ],
        [
            "from sklearn import metrics\n\nscore_funcs = [\n    (\"V-measure\", ),\n    (\"Rand index\", ),\n    (\"ARI\", ),\n    (\"MI\", ),\n    (\"NMI\", ),\n    (\"AMI\", ),\n]",
            "code"
        ]
    ],
    "Examples->Clustering->Adjustment for chance in clustering performance evaluation->First experiment: fixed ground truth labels and growing number of clusters": [
        [
            "We first define a function that creates uniformly-distributed random labeling.",
            "markdown"
        ],
        [
            "import numpy as np\n\nrng = (0)\n\n\ndef random_labels(n_samples, n_classes):\n    return rng.randint(low=0, high=n_classes, size=n_samples)",
            "code"
        ],
        [
            "Another function will use the random_labels function to create a fixed set\nof ground truth labels (labels_a) distributed in n_classes and then score\nseveral sets of randomly \u201cpredicted\u201d labels (labels_b) to assess the\nvariability of a given metric at a given n_clusters.",
            "markdown"
        ],
        [
            "def fixed_classes_uniform_labelings_scores(\n    score_func, n_samples, n_clusters_range, n_classes, n_runs=5\n):\n    scores = ((len(n_clusters_range), n_runs))\n    labels_a = random_labels(n_samples=n_samples, n_classes=n_classes)\n\n    for i, n_clusters in enumerate(n_clusters_range):\n        for j in range(n_runs):\n            labels_b = random_labels(n_samples=n_samples, n_classes=n_clusters)\n            scores[i, j] = score_func(labels_a, labels_b)\n    return scores",
            "code"
        ],
        [
            "In this first example we set the number of clases (true number of clusters) to\nn_classes=10. The number of clusters varies over the values provided by\nn_clusters_range.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\nimport seaborn as sns\n\nn_samples = 1000\nn_classes = 10\nn_clusters_range = (2, 100, 10).astype(int)\nplots = []\nnames = []\n\n(\"colorblind\")\n(1)\n\nfor marker, (score_name, score_func) in zip(\"d^vx.,\", score_funcs):\n    scores = fixed_classes_uniform_labelings_scores(\n        score_func, n_samples, n_clusters_range, n_classes=n_classes\n    )\n    plots.append(\n        (\n            n_clusters_range,\n            scores.mean(axis=1),\n            scores.std(axis=1),\n            alpha=0.8,\n            linewidth=1,\n            marker=marker,\n        )[0]\n    )\n    names.append(score_name)\n\n(\n    \"Clustering measures for random uniform labeling\\n\"\n    f\"against reference assignment with {n_classes} classes\"\n)\n(f\"Number of clusters (Number of samples is fixed to {n_samples})\")\n(\"Score value\")\n(bottom=-0.05, top=1.05)\n(plots, names, bbox_to_anchor=(0.5, 0.5))\n()\n\n\n<img alt=\"Clustering measures for random uniform labeling against reference assignment with 10 classes\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_adjusted_for_chance_measures_001.png\" srcset=\"../../_images/sphx_glr_plot_adjusted_for_chance_measures_001.png\"/>",
            "code"
        ],
        [
            "The Rand index saturates for n_clusters > n_classes. Other non-adjusted\nmeasures such as the V-Measure show a linear dependency between the number of\nclusters and the number of samples.",
            "markdown"
        ],
        [
            "Adjusted for chance measure, such as ARI and AMI, display some random\nvariations centered around a mean score of 0.0, independently of the number of\nsamples and clusters.",
            "markdown"
        ]
    ],
    "Examples->Clustering->Adjustment for chance in clustering performance evaluation->Second experiment: varying number of classes and clusters": [
        [
            "In this section we define a similar function that uses several metrics to\nscore 2 uniformly-distributed random labelings. In this case, the number of\nclasses and assigned number of clusters are matched for each possible value in\nn_clusters_range.",
            "markdown"
        ],
        [
            "def uniform_labelings_scores(score_func, n_samples, n_clusters_range, n_runs=5):\n    scores = ((len(n_clusters_range), n_runs))\n\n    for i, n_clusters in enumerate(n_clusters_range):\n        for j in range(n_runs):\n            labels_a = random_labels(n_samples=n_samples, n_classes=n_clusters)\n            labels_b = random_labels(n_samples=n_samples, n_classes=n_clusters)\n            scores[i, j] = score_func(labels_a, labels_b)\n    return scores",
            "code"
        ],
        [
            "In this case, we use n_samples=100 to show the effect of having a number of\nclusters similar or equal to the number of samples.",
            "markdown"
        ],
        [
            "n_samples = 100\nn_clusters_range = (2, n_samples, 10).astype(int)\n\n(2)\n\nplots = []\nnames = []\n\nfor marker, (score_name, score_func) in zip(\"d^vx.,\", score_funcs):\n    scores = uniform_labelings_scores(score_func, n_samples, n_clusters_range)\n    plots.append(\n        (\n            n_clusters_range,\n            (scores, axis=1),\n            scores.std(axis=1),\n            alpha=0.8,\n            linewidth=2,\n            marker=marker,\n        )[0]\n    )\n    names.append(score_name)\n\n(\n    \"Clustering measures for 2 random uniform labelings\\nwith equal number of clusters\"\n)\n(f\"Number of clusters (Number of samples is fixed to {n_samples})\")\n(\"Score value\")\n(plots, names)\n(bottom=-0.05, top=1.05)\n()\n\n\n<img alt=\"Clustering measures for 2 random uniform labelings with equal number of clusters\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_adjusted_for_chance_measures_002.png\" srcset=\"../../_images/sphx_glr_plot_adjusted_for_chance_measures_002.png\"/>",
            "code"
        ],
        [
            "We observe similar results as for the first experiment: adjusted for chance\nmetrics stay constantly near zero while other metrics tend to get larger with\nfiner-grained labelings. The mean V-measure of random labeling increases\nsignificantly as the number of clusters is closer to the total number of\nsamples used to compute the measure. Furthermore, raw Mutual Information is\nunbounded from above and its scale depends on the dimensions of the clustering\nproblem and the cardinality of the ground truth classes. This is why the\ncurve goes off the chart.",
            "markdown"
        ],
        [
            "Only adjusted measures can hence be safely used as a consensus index to\nevaluate the average stability of clustering algorithms for a given value of k\non various overlapping sub-samples of the dataset.",
            "markdown"
        ],
        [
            "Non-adjusted clustering evaluation metric can therefore be misleading as they\noutput large values for fine-grained labelings, one could be lead to think\nthat the labeling has captured meaningful groups while they can be totally\nrandom. In particular, such non-adjusted metrics should not be used to compare\nthe results of different clustering algorithms that output a different number\nof clusters.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  1.304 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Clustering->Agglomerative clustering with and without structure": [
        [
            "This example shows the effect of imposing a connectivity graph to capture\nlocal structure in the data. The graph is simply the graph of 20 nearest\nneighbors.",
            "markdown"
        ],
        [
            "There are two advantages of imposing a connectivity. First, clustering\nwithout a connectivity matrix is much faster.",
            "markdown"
        ],
        [
            "Second, when using a connectivity matrix, single, average and complete\nlinkage are unstable and tend to create a few clusters that grow very\nquickly. Indeed, average and complete linkage fight this percolation behavior\nby considering all the distances between two clusters when merging them (\nwhile single linkage exaggerates the behaviour by considering only the\nshortest distance between clusters). The connectivity graph breaks this\nmechanism for average and complete linkage, making them resemble the more\nbrittle single linkage. This effect is more pronounced for very sparse graphs\n(try decreasing the number of neighbors in kneighbors_graph) and with\ncomplete linkage. In particular, having a very small number of neighbors in\nthe graph, imposes a geometry that is close to that of single linkage,\nwhich is well known to have this percolation instability.\n\n<img alt=\"n_cluster=30, connectivity=False, linkage=average (time 0.04s), linkage=complete (time 0.05s), linkage=ward (time 0.04s), linkage=single (time 0.02s)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_agglomerative_clustering_001.png\" srcset=\"../../_images/sphx_glr_plot_agglomerative_clustering_001.png\"/>\n<img alt=\"n_cluster=3, connectivity=False, linkage=average (time 0.03s), linkage=complete (time 0.05s), linkage=ward (time 0.04s), linkage=single (time 0.02s)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_agglomerative_clustering_002.png\" srcset=\"../../_images/sphx_glr_plot_agglomerative_clustering_002.png\"/>\n<img alt=\"n_cluster=30, connectivity=True, linkage=average (time 0.13s), linkage=complete (time 0.14s), linkage=ward (time 0.16s), linkage=single (time 0.02s)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_agglomerative_clustering_003.png\" srcset=\"../../_images/sphx_glr_plot_agglomerative_clustering_003.png\"/>\n<img alt=\"n_cluster=3, connectivity=True, linkage=average (time 0.12s), linkage=complete (time 0.13s), linkage=ward (time 0.15s), linkage=single (time 0.03s)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_agglomerative_clustering_004.png\" srcset=\"../../_images/sphx_glr_plot_agglomerative_clustering_004.png\"/>",
            "markdown"
        ],
        [
            "# Authors: Gael Varoquaux, Nelle Varoquaux\n# License: BSD 3 clause\n\nimport time\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.cluster import \nfrom sklearn.neighbors import \n\n# Generate sample data\nn_samples = 1500\n(0)\nt = 1.5 *  * (1 + 3 * (1, n_samples))\nx = t * (t)\ny = t * (t)\n\n\nX = ((x, y))\nX += 0.7 * (2, n_samples)\nX = X.T\n\n# Create a graph capturing local connectivity. Larger number of neighbors\n# will give more homogeneous clusters to the cost of computation\n# time. A very large number of neighbors gives more evenly distributed\n# cluster sizes, but may not impose the local manifold structure of\n# the data\nknn_graph = (X, 30, include_self=False)\n\nfor connectivity in (None, knn_graph):\n    for n_clusters in (30, 3):\n        (figsize=(10, 4))\n        for index, linkage in enumerate((\"average\", \"complete\", \"ward\", \"single\")):\n            (1, 4, index + 1)\n            model = (\n                linkage=linkage, connectivity=connectivity, n_clusters=n_clusters\n            )\n            t0 = ()\n            model.fit(X)\n            elapsed_time = () - t0\n            (X[:, 0], X[:, 1], c=model.labels_, cmap=plt.cm.nipy_spectral)\n            (\n                \"linkage=%s\\n(time %.2fs)\" % (linkage, elapsed_time),\n                fontdict=dict(verticalalignment=\"top\"),\n            )\n            (\"equal\")\n            (\"off\")\n\n            (bottom=0, top=0.83, wspace=0, left=0, right=1)\n            (\n                \"n_cluster=%i, connectivity=%r\"\n                % (n_clusters, connectivity is not None),\n                size=17,\n            )\n\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  2.238 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Clustering->Agglomerative clustering with different metrics": [
        [
            "Demonstrates the effect of different metrics on the hierarchical clustering.",
            "markdown"
        ],
        [
            "The example is engineered to show the effect of the choice of different\nmetrics. It is applied to waveforms, which can be seen as\nhigh-dimensional vector. Indeed, the difference between metrics is\nusually more pronounced in high dimension (in particular for euclidean\nand cityblock).",
            "markdown"
        ],
        [
            "We generate data from three groups of waveforms. Two of the waveforms\n(waveform 1 and waveform 2) are proportional one to the other. The cosine\ndistance is invariant to a scaling of the data, as a result, it cannot\ndistinguish these two waveforms. Thus even with no noise, clustering\nusing this distance will not separate out waveform 1 and 2.",
            "markdown"
        ],
        [
            "We add observation noise to these waveforms. We generate very sparse\nnoise: only 6% of the time points contain noise. As a result, the\nl1 norm of this noise (ie \u201ccityblock\u201d distance) is much smaller than it\u2019s\nl2 norm (\u201ceuclidean\u201d distance). This can be seen on the inter-class\ndistance matrices: the values on the diagonal, that characterize the\nspread of the class, are much bigger for the Euclidean distance than for\nthe cityblock distance.",
            "markdown"
        ],
        [
            "When we apply clustering to the data, we find that the clustering\nreflects what was in the distance matrices. Indeed, for the Euclidean\ndistance, the classes are ill-separated because of the noise, and thus\nthe clustering does not separate the waveforms. For the cityblock\ndistance, the separation is good and the waveform classes are recovered.\nFinally, the cosine distance does not separate at all waveform 1 and 2,\nthus the clustering puts them in the same cluster.\n\n<img alt=\"Ground truth\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_agglomerative_clustering_metrics_001.png\" srcset=\"../../_images/sphx_glr_plot_agglomerative_clustering_metrics_001.png\"/>\n<img alt=\"Interclass cosine distances\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_agglomerative_clustering_metrics_002.png\" srcset=\"../../_images/sphx_glr_plot_agglomerative_clustering_metrics_002.png\"/>\n<img alt=\"Interclass euclidean distances\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_agglomerative_clustering_metrics_003.png\" srcset=\"../../_images/sphx_glr_plot_agglomerative_clustering_metrics_003.png\"/>\n<img alt=\"Interclass cityblock distances\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_agglomerative_clustering_metrics_004.png\" srcset=\"../../_images/sphx_glr_plot_agglomerative_clustering_metrics_004.png\"/>\n<img alt=\"AgglomerativeClustering(metric=cosine)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_agglomerative_clustering_metrics_005.png\" srcset=\"../../_images/sphx_glr_plot_agglomerative_clustering_metrics_005.png\"/>\n<img alt=\"AgglomerativeClustering(metric=euclidean)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_agglomerative_clustering_metrics_006.png\" srcset=\"../../_images/sphx_glr_plot_agglomerative_clustering_metrics_006.png\"/>\n<img alt=\"AgglomerativeClustering(metric=cityblock)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_agglomerative_clustering_metrics_007.png\" srcset=\"../../_images/sphx_glr_plot_agglomerative_clustering_metrics_007.png\"/>",
            "markdown"
        ],
        [
            "# Author: Gael Varoquaux\n# License: BSD 3-Clause or CC-0\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patheffects as PathEffects\nimport numpy as np\n\nfrom sklearn.cluster import \nfrom sklearn.metrics import \n\n(0)\n\n# Generate waveform data\nn_features = 2000\nt =  * (0, 1, n_features)\n\n\ndef sqr(x):\n    return ((x))\n\n\nX = list()\ny = list()\nfor i, (phi, a) in enumerate([(0.5, 0.15), (0.5, 0.6), (0.3, 0.2)]):\n    for _ in range(30):\n        phase_noise = 0.01 * ()\n        amplitude_noise = 0.04 * ()\n        additional_noise = 1 - 2 * (n_features)\n        # Make the noise sparse\n        additional_noise[np.abs(additional_noise) &lt; 0.997] = 0\n\n        X.append(\n            12\n            * (\n                (a + amplitude_noise) * (sqr(6 * (t + phi + phase_noise)))\n                + additional_noise\n            )\n        )\n        y.append(i)\n\nX = (X)\ny = (y)\n\nn_clusters = 3\n\nlabels = (\"Waveform 1\", \"Waveform 2\", \"Waveform 3\")\n\ncolors = [\"#f7bd01\", \"#377eb8\", \"#f781bf\"]\n\n# Plot the ground-truth labelling\n()\n([0, 0, 1, 1])\nfor l, color, n in zip(range(n_clusters), colors, labels):\n    lines = (X[y == l].T, c=color, alpha=0.5)\n    lines[0].set_label(n)\n\n(loc=\"best\")\n\n(\"tight\")\n(\"off\")\n(\"Ground truth\", size=20, y=1)\n\n\n# Plot the distances\nfor index, metric in enumerate([\"cosine\", \"euclidean\", \"cityblock\"]):\n    avg_dist = ((n_clusters, n_clusters))\n    (figsize=(5, 4.5))\n    for i in range(n_clusters):\n        for j in range(n_clusters):\n            avg_dist[i, j] = (\n                X[y == i], X[y == j], metric=metric\n            ).mean()\n    avg_dist /= avg_dist.max()\n    for i in range(n_clusters):\n        for j in range(n_clusters):\n            t = (\n                i,\n                j,\n                \"%5.3f\" % avg_dist[i, j],\n                verticalalignment=\"center\",\n                horizontalalignment=\"center\",\n            )\n            t.set_path_effects(\n                [(linewidth=5, foreground=\"w\", alpha=0.5)]\n            )\n\n    (avg_dist, interpolation=\"nearest\", cmap=\"cividis\", vmin=0)\n    (range(n_clusters), labels, rotation=45)\n    (range(n_clusters), labels)\n    ()\n    (\"Interclass %s distances\" % metric, size=18, y=1)\n    ()\n\n\n# Plot clustering results\nfor index, metric in enumerate([\"cosine\", \"euclidean\", \"cityblock\"]):\n    model = (\n        n_clusters=n_clusters, linkage=\"average\", metric=metric\n    )\n    model.fit(X)\n    ()\n    ([0, 0, 1, 1])\n    for l, color in zip((model.n_clusters), colors):\n        (X[model.labels_ == l].T, c=color, alpha=0.5)\n    (\"tight\")\n    (\"off\")\n    (\"AgglomerativeClustering(metric=%s)\" % metric, size=20, y=1)\n\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  1.266 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Clustering->An example of K-Means++ initialization": [
        [
            "An example to show the output of the \nfunction for generating initial seeds for clustering.",
            "markdown"
        ],
        [
            "K-Means++ is used as the default initialization for .\n<img alt=\"K-Means++ Initialization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_kmeans_plusplus_001.png\" srcset=\"../../_images/sphx_glr_plot_kmeans_plusplus_001.png\"/>",
            "markdown"
        ],
        [
            "from sklearn.cluster import \nfrom sklearn.datasets import \nimport matplotlib.pyplot as plt\n\n# Generate sample data\nn_samples = 4000\nn_components = 4\n\nX, y_true = (\n    n_samples=n_samples, centers=n_components, cluster_std=0.60, random_state=0\n)\nX = X[:, ::-1]\n\n# Calculate seeds from k-means++\ncenters_init, indices = (X, n_clusters=4, random_state=0)\n\n# Plot init seeds along side sample data\n(1)\ncolors = [\"#4EACC5\", \"#FF9C34\", \"#4E9A06\", \"m\"]\n\nfor k, col in enumerate(colors):\n    cluster_data = y_true == k\n    (X[cluster_data, 0], X[cluster_data, 1], c=col, marker=\".\", s=10)\n\n(centers_init[:, 0], centers_init[:, 1], c=\"b\", s=50)\n(\"K-Means++ Initialization\")\n([])\n([])\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.083 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Clustering->Bisecting K-Means and Regular K-Means Performance Comparison": [
        [
            "This example shows differences between Regular K-Means algorithm and Bisecting K-Means.",
            "markdown"
        ],
        [
            "While K-Means clusterings are different when increasing n_clusters,\nBisecting K-Means clustering builds on top of the previous ones. As a result, it\ntends to create clusters that have a more regular large-scale structure. This\ndifference can be visually observed: for all numbers of clusters, there is a\ndividing line cutting the overall data cloud in two for BisectingKMeans, which is not\npresent for regular K-Means.\n<img alt=\"Bisecting K-Means : 4 clusters, Bisecting K-Means : 8 clusters, Bisecting K-Means : 16 clusters, K-Means : 4 clusters, K-Means : 8 clusters, K-Means : 16 clusters\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_bisect_kmeans_001.png\" srcset=\"../../_images/sphx_glr_plot_bisect_kmeans_001.png\"/>",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n\nfrom sklearn.datasets import \nfrom sklearn.cluster import , \n\n\nprint(__doc__)\n\n\n# Generate sample data\nn_samples = 10000\nrandom_state = 0\n\nX, _ = (n_samples=n_samples, centers=2, random_state=random_state)\n\n# Number of cluster centers for KMeans and BisectingKMeans\nn_clusters_list = [4, 8, 16]\n\n# Algorithms to compare\nclustering_algorithms = {\n    \"Bisecting K-Means\": ,\n    \"K-Means\": ,\n}\n\n# Make subplots for each variant\nfig, axs = (\n    len(clustering_algorithms), len(n_clusters_list), figsize=(12, 5)\n)\n\naxs = axs.T\n\nfor i, (algorithm_name, Algorithm) in enumerate(clustering_algorithms.items()):\n    for j, n_clusters in enumerate(n_clusters_list):\n        algo = Algorithm(n_clusters=n_clusters, random_state=random_state, n_init=3)\n        algo.fit(X)\n        centers = algo.cluster_centers_\n\n        axs[j, i].scatter(X[:, 0], X[:, 1], s=10, c=algo.labels_)\n        axs[j, i].scatter(centers[:, 0], centers[:, 1], c=\"r\", s=20)\n\n        axs[j, i].set_title(f\"{algorithm_name} : {n_clusters} clusters\")\n\n\n# Hide x labels and tick labels for top plots and y ticks for right plots.\nfor ax in axs.flat:\n    ax.label_outer()\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  1.247 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Clustering->Color Quantization using K-Means": [
        [
            "Performs a pixel-wise Vector Quantization (VQ) of an image of the summer palace\n(China), reducing the number of colors required to show the image from 96,615\nunique colors to 64, while preserving the overall appearance quality.",
            "markdown"
        ],
        [
            "In this example, pixels are represented in a 3D-space and K-means is used to\nfind 64 color clusters. In the image processing literature, the codebook\nobtained from K-means (the cluster centers) is called the color palette. Using\na single byte, up to 256 colors can be addressed, whereas an RGB encoding\nrequires 3 bytes per pixel. The GIF file format, for example, uses such a\npalette.",
            "markdown"
        ],
        [
            "For comparison, a quantized image using a random codebook (colors picked up\nrandomly) is also shown.\n\n<img alt=\"Original image (96,615 colors)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_color_quantization_001.png\" srcset=\"../../_images/sphx_glr_plot_color_quantization_001.png\"/>\n<img alt=\"Quantized image (64 colors, K-Means)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_color_quantization_002.png\" srcset=\"../../_images/sphx_glr_plot_color_quantization_002.png\"/>\n<img alt=\"Quantized image (64 colors, Random)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_color_quantization_003.png\" srcset=\"../../_images/sphx_glr_plot_color_quantization_003.png\"/>",
            "markdown"
        ],
        [
            "Fitting model on a small sub-sample of the data\ndone in 0.019s.\nPredicting color indices on the full image (k-means)\ndone in 0.053s.\nPredicting color indices on the full image (random)\ndone in 0.087s.\n\n\n\n<br/>",
            "code"
        ],
        [
            "# Authors: Robert Layton &lt;robertlayton@gmail.com\n#          Olivier Grisel &lt;olivier.grisel@ensta.org\n#          Mathieu Blondel &lt;mathieu@mblondel.org\n#\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import \nfrom sklearn.metrics import \nfrom sklearn.datasets import \nfrom sklearn.utils import \nfrom time import \n\nn_colors = 64\n\n# Load the Summer Palace photo\nchina = (\"china.jpg\")\n\n# Convert to floats instead of the default 8 bits integer coding. Dividing by\n# 255 is important so that plt.imshow behaves works well on float data (need to\n# be in the range [0-1])\nchina = (china, dtype=) / 255\n\n# Load Image and transform to a 2D numpy array.\nw, h, d = original_shape = tuple(china.shape)\nassert d == 3\nimage_array = (china, (w * h, d))\n\nprint(\"Fitting model on a small sub-sample of the data\")\nt0 = ()\nimage_array_sample = (image_array, random_state=0, n_samples=1_000)\nkmeans = (n_clusters=n_colors, n_init=\"auto\", random_state=0).fit(\n    image_array_sample\n)\nprint(f\"done in {() - t0:0.3f}s.\")\n\n# Get labels for all points\nprint(\"Predicting color indices on the full image (k-means)\")\nt0 = ()\nlabels = kmeans.predict(image_array)\nprint(f\"done in {() - t0:0.3f}s.\")\n\n\ncodebook_random = (image_array, random_state=0, n_samples=n_colors)\nprint(\"Predicting color indices on the full image (random)\")\nt0 = ()\nlabels_random = (codebook_random, image_array, axis=0)\nprint(f\"done in {() - t0:0.3f}s.\")\n\n\ndef recreate_image(codebook, labels, w, h):\n    \"\"\"Recreate the (compressed) image from the code book & labels\"\"\"\n    return codebook[labels].reshape(w, h, -1)\n\n\n# Display all results, alongside original image\n(1)\n()\n(\"off\")\n(\"Original image (96,615 colors)\")\n(china)\n\n(2)\n()\n(\"off\")\n(f\"Quantized image ({n_colors} colors, K-Means)\")\n(recreate_image(kmeans.cluster_centers_, labels, w, h))\n\n(3)\n()\n(\"off\")\n(f\"Quantized image ({n_colors} colors, Random)\")\n(recreate_image(codebook_random, labels_random, w, h))\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.597 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Clustering->Compare BIRCH and MiniBatchKMeans": [
        [
            "This example compares the timing of BIRCH (with and without the global\nclustering step) and MiniBatchKMeans on a synthetic dataset having\n25,000 samples and 2 features generated using make_blobs.",
            "markdown"
        ],
        [
            "Both MiniBatchKMeans and BIRCH are very scalable algorithms and could\nrun efficiently on hundreds of thousands or even millions of datapoints. We\nchose to limit the dataset size of this example in the interest of keeping\nour Continuous Integration resource usage reasonable but the interested\nreader might enjoy editing this script to rerun it with a larger value for\nn_samples.",
            "markdown"
        ],
        [
            "If n_clusters is set to None, the data is reduced from 25,000\nsamples to a set of 158 clusters. This can be viewed as a preprocessing\nstep before the final (global) clustering step that further reduces these\n158 clusters to 100 clusters.\n<img alt=\"BIRCH without global clustering, BIRCH with global clustering, MiniBatchKMeans\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_birch_vs_minibatchkmeans_001.png\" srcset=\"../../_images/sphx_glr_plot_birch_vs_minibatchkmeans_001.png\"/>",
            "markdown"
        ],
        [
            "BIRCH without global clustering as the final step took 0.75 seconds\nn_clusters : 158\nBIRCH with global clustering as the final step took 0.66 seconds\nn_clusters : 100\nTime taken to run MiniBatchKMeans 0.26 seconds\n\n\n\n<br/>",
            "code"
        ],
        [
            "# Authors: Manoj Kumar &lt;manojkumarsivaraj334@gmail.com\n#          Alexandre Gramfort &lt;alexandre.gramfort@telecom-paristech.fr\n# License: BSD 3 clause\n\nfrom joblib import cpu_count\nfrom itertools import \nfrom time import \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as colors\n\nfrom sklearn.cluster import , \nfrom sklearn.datasets import \n\n\n# Generate centers for the blobs so that it forms a 10 X 10 grid.\nxx = (-22, 22, 10)\nyy = (-22, 22, 10)\nxx, yy = (xx, yy)\nn_centers = (((xx)[:, ], (yy)[:, ]))\n\n# Generate blobs to do a comparison between MiniBatchKMeans and BIRCH.\nX, y = (n_samples=25000, centers=n_centers, random_state=0)\n\n# Use all colors that matplotlib provides by default.\ncolors_ = (colors.cnames.keys())\n\nfig = (figsize=(12, 4))\nfig.subplots_adjust(left=0.04, right=0.98, bottom=0.1, top=0.9)\n\n# Compute clustering with BIRCH with and without the final clustering step\n# and plot.\nbirch_models = [\n    (threshold=1.7, n_clusters=None),\n    (threshold=1.7, n_clusters=100),\n]\nfinal_step = [\"without global clustering\", \"with global clustering\"]\n\nfor ind, (birch_model, info) in enumerate(zip(birch_models, final_step)):\n    t = ()\n    birch_model.fit(X)\n    print(\"BIRCH %s as the final step took %0.2f seconds\" % (info, (() - t)))\n\n    # Plot result\n    labels = birch_model.labels_\n    centroids = birch_model.subcluster_centers_\n    n_clusters = (labels).size\n    print(\"n_clusters : %d\" % n_clusters)\n\n    ax = fig.add_subplot(1, 3, ind + 1)\n    for this_centroid, k, col in zip(centroids, range(n_clusters), colors_):\n        mask = labels == k\n        ax.scatter(X[mask, 0], X[mask, 1], c=\"w\", edgecolor=col, marker=\".\", alpha=0.5)\n        if birch_model.n_clusters is None:\n            ax.scatter(this_centroid[0], this_centroid[1], marker=\"+\", c=\"k\", s=25)\n    ax.set_ylim([-25, 25])\n    ax.set_xlim([-25, 25])\n    ax.set_autoscaley_on(False)\n    ax.set_title(\"BIRCH %s\" % info)\n\n# Compute clustering with MiniBatchKMeans.\nmbk = (\n    init=\"k-means++\",\n    n_clusters=100,\n    batch_size=256 * cpu_count(),\n    n_init=10,\n    max_no_improvement=10,\n    verbose=0,\n    random_state=0,\n)\nt0 = ()\nmbk.fit(X)\nt_mini_batch = () - t0\nprint(\"Time taken to run MiniBatchKMeans %0.2f seconds\" % t_mini_batch)\nmbk_means_labels_unique = (mbk.labels_)\n\nax = fig.add_subplot(1, 3, 3)\nfor this_centroid, k, col in zip(mbk.cluster_centers_, range(n_clusters), colors_):\n    mask = mbk.labels_ == k\n    ax.scatter(X[mask, 0], X[mask, 1], marker=\".\", c=\"w\", edgecolor=col, alpha=0.5)\n    ax.scatter(this_centroid[0], this_centroid[1], marker=\"+\", c=\"k\", s=25)\nax.set_xlim([-25, 25])\nax.set_ylim([-25, 25])\nax.set_title(\"MiniBatchKMeans\")\nax.set_autoscaley_on(False)\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  4.622 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Clustering->Comparing different clustering algorithms on toy datasets": [
        [
            "This example shows characteristics of different\nclustering algorithms on datasets that are \u201cinteresting\u201d\nbut still in 2D. With the exception of the last dataset,\nthe parameters of each of these dataset-algorithm pairs\nhas been tuned to produce good clustering results. Some\nalgorithms are more sensitive to parameter values than\nothers.",
            "markdown"
        ],
        [
            "The last dataset is an example of a \u2018null\u2019 situation for\nclustering: the data is homogeneous, and there is no good\nclustering. For this example, the null dataset uses the\nsame parameters as the dataset in the row above it, which\nrepresents a mismatch in the parameter values and the\ndata structure.",
            "markdown"
        ],
        [
            "While these examples give some intuition about the\nalgorithms, this intuition might not apply to very high\ndimensional data.\n<img alt=\"MiniBatch KMeans, Affinity Propagation, MeanShift, Spectral Clustering, Ward, Agglomerative Clustering, DBSCAN, OPTICS, BIRCH, Gaussian Mixture\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cluster_comparison_001.png\" srcset=\"../../_images/sphx_glr_plot_cluster_comparison_001.png\"/>",
            "markdown"
        ],
        [
            "import time\nimport warnings\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import cluster, , mixture\nfrom sklearn.neighbors import \nfrom sklearn.preprocessing import \nfrom itertools import , \n\n(0)\n\n# ============\n# Generate datasets. We choose the size big enough to see the scalability\n# of the algorithms, but not too big to avoid too long running times\n# ============\nn_samples = 500\nnoisy_circles = (n_samples=n_samples, factor=0.5, noise=0.05)\nnoisy_moons = (n_samples=n_samples, noise=0.05)\nblobs = (n_samples=n_samples, random_state=8)\nno_structure = (n_samples, 2), None\n\n# Anisotropicly distributed data\nrandom_state = 170\nX, y = (n_samples=n_samples, random_state=random_state)\ntransformation = [[0.6, -0.6], [-0.4, 0.8]]\nX_aniso = (X, transformation)\naniso = (X_aniso, y)\n\n# blobs with varied variances\nvaried = (\n    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state\n)\n\n# ============\n# Set up cluster parameters\n# ============\n(figsize=(9 * 2 + 3, 13))\n(\n    left=0.02, right=0.98, bottom=0.001, top=0.95, wspace=0.05, hspace=0.01\n)\n\nplot_num = 1\n\ndefault_base = {\n    \"quantile\": 0.3,\n    \"eps\": 0.3,\n    \"damping\": 0.9,\n    \"preference\": -200,\n    \"n_neighbors\": 3,\n    \"n_clusters\": 3,\n    \"min_samples\": 7,\n    \"xi\": 0.05,\n    \"min_cluster_size\": 0.1,\n}\n\n = [\n    (\n        noisy_circles,\n        {\n            \"damping\": 0.77,\n            \"preference\": -240,\n            \"quantile\": 0.2,\n            \"n_clusters\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.08,\n        },\n    ),\n    (\n        noisy_moons,\n        {\n            \"damping\": 0.75,\n            \"preference\": -220,\n            \"n_clusters\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.1,\n        },\n    ),\n    (\n        varied,\n        {\n            \"eps\": 0.18,\n            \"n_neighbors\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.01,\n            \"min_cluster_size\": 0.2,\n        },\n    ),\n    (\n        aniso,\n        {\n            \"eps\": 0.15,\n            \"n_neighbors\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.1,\n            \"min_cluster_size\": 0.2,\n        },\n    ),\n    (blobs, {\"min_samples\": 7, \"xi\": 0.1, \"min_cluster_size\": 0.2}),\n    (no_structure, {}),\n]\n\nfor i_dataset, (dataset, algo_params) in enumerate():\n    # update parameters with dataset-specific values\n    params = default_base.copy()\n    params.update(algo_params)\n\n    X, y = dataset\n\n    # normalize dataset for easier parameter selection\n    X = ().fit_transform(X)\n\n    # estimate bandwidth for mean shift\n    bandwidth = (X, quantile=params[\"quantile\"])\n\n    # connectivity matrix for structured Ward\n    connectivity = (\n        X, n_neighbors=params[\"n_neighbors\"], include_self=False\n    )\n    # make connectivity symmetric\n    connectivity = 0.5 * (connectivity + connectivity.T)\n\n    # ============\n    # Create cluster objects\n    # ============\n    ms = (bandwidth=bandwidth, bin_seeding=True)\n    two_means = (n_clusters=params[\"n_clusters\"], n_init=\"auto\")\n    ward = (\n        n_clusters=params[\"n_clusters\"], linkage=\"ward\", connectivity=connectivity\n    )\n    spectral = (\n        n_clusters=params[\"n_clusters\"],\n        eigen_solver=\"arpack\",\n        affinity=\"nearest_neighbors\",\n    )\n    dbscan = (eps=params[\"eps\"])\n    optics = (\n        min_samples=params[\"min_samples\"],\n        xi=params[\"xi\"],\n        min_cluster_size=params[\"min_cluster_size\"],\n    )\n    affinity_propagation = (\n        damping=params[\"damping\"], preference=params[\"preference\"], random_state=0\n    )\n    average_linkage = (\n        linkage=\"average\",\n        metric=\"cityblock\",\n        n_clusters=params[\"n_clusters\"],\n        connectivity=connectivity,\n    )\n    birch = (n_clusters=params[\"n_clusters\"])\n    gmm = (\n        n_components=params[\"n_clusters\"], covariance_type=\"full\"\n    )\n\n    clustering_algorithms = (\n        (\"MiniBatch\\nKMeans\", two_means),\n        (\"Affinity\\nPropagation\", affinity_propagation),\n        (\"MeanShift\", ms),\n        (\"Spectral\\nClustering\", spectral),\n        (\"Ward\", ward),\n        (\"Agglomerative\\nClustering\", average_linkage),\n        (\"DBSCAN\", dbscan),\n        (\"OPTICS\", optics),\n        (\"BIRCH\", birch),\n        (\"Gaussian\\nMixture\", gmm),\n    )\n\n    for name, algorithm in clustering_algorithms:\n        t0 = ()\n\n        # catch warnings related to kneighbors_graph\n        with ():\n            (\n                \"ignore\",\n                message=\"the number of connected components of the \"\n                + \"connectivity matrix is [0-9]{1,2}\"\n                + \"  1. Completing it to avoid stopping the tree early.\",\n                category=UserWarning,\n            )\n            (\n                \"ignore\",\n                message=\"Graph is not fully connected, spectral embedding\"\n                + \" may not work as expected.\",\n                category=UserWarning,\n            )\n            algorithm.fit(X)\n\n        t1 = ()\n        if hasattr(algorithm, \"labels_\"):\n            y_pred = algorithm.labels_.astype(int)\n        else:\n            y_pred = algorithm.predict(X)\n\n        (len(), len(clustering_algorithms), plot_num)\n        if i_dataset == 0:\n            (name, size=18)\n\n        colors = (\n            list(\n                (\n                    (\n                        [\n                            \"#377eb8\",\n                            \"#ff7f00\",\n                            \"#4daf4a\",\n                            \"#f781bf\",\n                            \"#a65628\",\n                            \"#984ea3\",\n                            \"#999999\",\n                            \"#e41a1c\",\n                            \"#dede00\",\n                        ]\n                    ),\n                    int(max(y_pred) + 1),\n                )\n            )\n        )\n        # add black color for outliers (if any)\n        colors = (colors, [\"#000000\"])\n        (X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n\n        (-2.5, 2.5)\n        (-2.5, 2.5)\n        (())\n        (())\n        (\n            0.99,\n            0.01,\n            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n            transform=().transAxes,\n            size=15,\n            horizontalalignment=\"right\",\n        )\n        plot_num += 1\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  6.094 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Clustering->Comparing different hierarchical linkage methods on toy datasets": [
        [
            "This example shows characteristics of different linkage\nmethods for hierarchical clustering on datasets that are\n\u201cinteresting\u201d but still in 2D.",
            "markdown"
        ],
        [
            "The main observations to make are:",
            "markdown"
        ],
        [
            "single linkage is fast, and can perform well on\nnon-globular data, but it performs poorly in the\npresence of noise.",
            "markdown"
        ],
        [
            "average and complete linkage perform well on\ncleanly separated globular clusters, but have mixed\nresults otherwise.",
            "markdown"
        ],
        [
            "Ward is the most effective method for noisy data.",
            "markdown"
        ],
        [
            "While these examples give some intuition about the\nalgorithms, this intuition might not apply to very high\ndimensional data.",
            "markdown"
        ],
        [
            "import time\nimport warnings\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import cluster, \nfrom sklearn.preprocessing import \nfrom itertools import , \n\n(0)",
            "code"
        ],
        [
            "Generate datasets. We choose the size big enough to see the scalability\nof the algorithms, but not too big to avoid too long running times",
            "markdown"
        ],
        [
            "n_samples = 1500\nnoisy_circles = (n_samples=n_samples, factor=0.5, noise=0.05)\nnoisy_moons = (n_samples=n_samples, noise=0.05)\nblobs = (n_samples=n_samples, random_state=8)\nno_structure = (n_samples, 2), None\n\n# Anisotropicly distributed data\nrandom_state = 170\nX, y = (n_samples=n_samples, random_state=random_state)\ntransformation = [[0.6, -0.6], [-0.4, 0.8]]\nX_aniso = (X, transformation)\naniso = (X_aniso, y)\n\n# blobs with varied variances\nvaried = (\n    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state\n)",
            "code"
        ],
        [
            "Run the clustering and plot",
            "markdown"
        ],
        [
            "# Set up cluster parameters\n(figsize=(9 * 1.3 + 2, 14.5))\n(\n    left=0.02, right=0.98, bottom=0.001, top=0.96, wspace=0.05, hspace=0.01\n)\n\nplot_num = 1\n\ndefault_base = {\"n_neighbors\": 10, \"n_clusters\": 3}\n\n = [\n    (noisy_circles, {\"n_clusters\": 2}),\n    (noisy_moons, {\"n_clusters\": 2}),\n    (varied, {\"n_neighbors\": 2}),\n    (aniso, {\"n_neighbors\": 2}),\n    (blobs, {}),\n    (no_structure, {}),\n]\n\nfor i_dataset, (dataset, algo_params) in enumerate():\n    # update parameters with dataset-specific values\n    params = default_base.copy()\n    params.update(algo_params)\n\n    X, y = dataset\n\n    # normalize dataset for easier parameter selection\n    X = ().fit_transform(X)\n\n    # ============\n    # Create cluster objects\n    # ============\n    ward = (\n        n_clusters=params[\"n_clusters\"], linkage=\"ward\"\n    )\n    complete = (\n        n_clusters=params[\"n_clusters\"], linkage=\"complete\"\n    )\n    average = (\n        n_clusters=params[\"n_clusters\"], linkage=\"average\"\n    )\n    single = (\n        n_clusters=params[\"n_clusters\"], linkage=\"single\"\n    )\n\n    clustering_algorithms = (\n        (\"Single Linkage\", single),\n        (\"Average Linkage\", average),\n        (\"Complete Linkage\", complete),\n        (\"Ward Linkage\", ward),\n    )\n\n    for name, algorithm in clustering_algorithms:\n        t0 = ()\n\n        # catch warnings related to kneighbors_graph\n        with ():\n            (\n                \"ignore\",\n                message=\"the number of connected components of the \"\n                + \"connectivity matrix is [0-9]{1,2}\"\n                + \"  1. Completing it to avoid stopping the tree early.\",\n                category=UserWarning,\n            )\n            algorithm.fit(X)\n\n        t1 = ()\n        if hasattr(algorithm, \"labels_\"):\n            y_pred = algorithm.labels_.astype(int)\n        else:\n            y_pred = algorithm.predict(X)\n\n        (len(), len(clustering_algorithms), plot_num)\n        if i_dataset == 0:\n            (name, size=18)\n\n        colors = (\n            list(\n                (\n                    (\n                        [\n                            \"#377eb8\",\n                            \"#ff7f00\",\n                            \"#4daf4a\",\n                            \"#f781bf\",\n                            \"#a65628\",\n                            \"#984ea3\",\n                            \"#999999\",\n                            \"#e41a1c\",\n                            \"#dede00\",\n                        ]\n                    ),\n                    int(max(y_pred) + 1),\n                )\n            )\n        )\n        (X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n\n        (-2.5, 2.5)\n        (-2.5, 2.5)\n        (())\n        (())\n        (\n            0.99,\n            0.01,\n            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n            transform=().transAxes,\n            size=15,\n            horizontalalignment=\"right\",\n        )\n        plot_num += 1\n\n()\n\n\n<img alt=\"Single Linkage, Average Linkage, Complete Linkage, Ward Linkage\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linkage_comparison_001.png\" srcset=\"../../_images/sphx_glr_plot_linkage_comparison_001.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  2.121 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Clustering->Comparison of the K-Means and MiniBatchKMeans clustering algorithms": [
        [
            "We want to compare the performance of the MiniBatchKMeans and KMeans:\nthe MiniBatchKMeans is faster, but gives slightly different results (see\n).",
            "markdown"
        ],
        [
            "We will cluster a set of data, first with KMeans and then with\nMiniBatchKMeans, and plot the results.\nWe will also plot the points that are labelled differently between the two\nalgorithms.",
            "markdown"
        ]
    ],
    "Examples->Clustering->Comparison of the K-Means and MiniBatchKMeans clustering algorithms->Generate the data": [
        [
            "We start by generating the blobs of data to be clustered.",
            "markdown"
        ],
        [
            "import numpy as np\nfrom sklearn.datasets import \n\n(0)\n\nbatch_size = 45\ncenters = [[1, 1], [-1, -1], [1, -1]]\nn_clusters = len(centers)\nX, labels_true = (n_samples=3000, centers=centers, cluster_std=0.7)",
            "code"
        ]
    ],
    "Examples->Clustering->Comparison of the K-Means and MiniBatchKMeans clustering algorithms->Compute clustering with KMeans": [
        [
            "import time\nfrom sklearn.cluster import \n\nk_means = (init=\"k-means++\", n_clusters=3, n_init=10)\nt0 = ()\nk_means.fit(X)\nt_batch = () - t0",
            "code"
        ]
    ],
    "Examples->Clustering->Comparison of the K-Means and MiniBatchKMeans clustering algorithms->Compute clustering with MiniBatchKMeans": [
        [
            "from sklearn.cluster import \n\nmbk = (\n    init=\"k-means++\",\n    n_clusters=3,\n    batch_size=batch_size,\n    n_init=10,\n    max_no_improvement=10,\n    verbose=0,\n)\nt0 = ()\nmbk.fit(X)\nt_mini_batch = () - t0",
            "code"
        ]
    ],
    "Examples->Clustering->Comparison of the K-Means and MiniBatchKMeans clustering algorithms->Establishing parity between clusters": [
        [
            "We want to have the same color for the same cluster from both the\nMiniBatchKMeans and the KMeans algorithm. Let\u2019s pair the cluster centers per\nclosest one.",
            "markdown"
        ],
        [
            "from sklearn.metrics.pairwise import \n\nk_means_cluster_centers = k_means.cluster_centers_\norder = (k_means.cluster_centers_, mbk.cluster_centers_)\nmbk_means_cluster_centers = mbk.cluster_centers_[order]\n\nk_means_labels = (X, k_means_cluster_centers)\nmbk_means_labels = (X, mbk_means_cluster_centers)",
            "code"
        ]
    ],
    "Examples->Clustering->Comparison of the K-Means and MiniBatchKMeans clustering algorithms->Plotting the results": [
        [
            "import matplotlib.pyplot as plt\n\nfig = (figsize=(8, 3))\nfig.subplots_adjust(left=0.02, right=0.98, bottom=0.05, top=0.9)\ncolors = [\"#4EACC5\", \"#FF9C34\", \"#4E9A06\"]\n\n# KMeans\nax = fig.add_subplot(1, 3, 1)\nfor k, col in zip(range(n_clusters), colors):\n    my_members = k_means_labels == k\n    cluster_center = k_means_cluster_centers[k]\n    ax.plot(X[my_members, 0], X[my_members, 1], \"w\", markerfacecolor=col, marker=\".\")\n    ax.plot(\n        cluster_center[0],\n        cluster_center[1],\n        \"o\",\n        markerfacecolor=col,\n        markeredgecolor=\"k\",\n        markersize=6,\n    )\nax.set_title(\"KMeans\")\nax.set_xticks(())\nax.set_yticks(())\n(-3.5, 1.8, \"train time: %.2fs\\ninertia: %f\" % (t_batch, k_means.inertia_))\n\n# MiniBatchKMeans\nax = fig.add_subplot(1, 3, 2)\nfor k, col in zip(range(n_clusters), colors):\n    my_members = mbk_means_labels == k\n    cluster_center = mbk_means_cluster_centers[k]\n    ax.plot(X[my_members, 0], X[my_members, 1], \"w\", markerfacecolor=col, marker=\".\")\n    ax.plot(\n        cluster_center[0],\n        cluster_center[1],\n        \"o\",\n        markerfacecolor=col,\n        markeredgecolor=\"k\",\n        markersize=6,\n    )\nax.set_title(\"MiniBatchKMeans\")\nax.set_xticks(())\nax.set_yticks(())\n(-3.5, 1.8, \"train time: %.2fs\\ninertia: %f\" % (t_mini_batch, mbk.inertia_))\n\n# Initialize the different array to all False\ndifferent = mbk_means_labels == 4\nax = fig.add_subplot(1, 3, 3)\n\nfor k in range(n_clusters):\n    different += (k_means_labels == k) != (mbk_means_labels == k)\n\nidentic = (different)\nax.plot(X[identic, 0], X[identic, 1], \"w\", markerfacecolor=\"#bbbbbb\", marker=\".\")\nax.plot(X[different, 0], X[different, 1], \"w\", markerfacecolor=\"m\", marker=\".\")\nax.set_title(\"Difference\")\nax.set_xticks(())\nax.set_yticks(())\n\n()\n\n\n<img alt=\"KMeans, MiniBatchKMeans, Difference\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_mini_batch_kmeans_001.png\" srcset=\"../../_images/sphx_glr_plot_mini_batch_kmeans_001.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.199 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Clustering->Demo of DBSCAN clustering algorithm": [
        [
            "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) finds core\nsamples in regions of high density and expands clusters from them. This\nalgorithm is good for data which contains clusters of similar density.",
            "markdown"
        ],
        [
            "See the  example\nfor a demo of different clustering algorithms on 2D datasets.",
            "markdown"
        ]
    ],
    "Examples->Clustering->Demo of DBSCAN clustering algorithm->Data generation": [
        [
            "We use  to create 3 synthetic clusters.",
            "markdown"
        ],
        [
            "from sklearn.datasets import \nfrom sklearn.preprocessing import \n\ncenters = [[1, 1], [-1, -1], [1, -1]]\nX, labels_true = (\n    n_samples=750, centers=centers, cluster_std=0.4, random_state=0\n)\n\nX = ().fit_transform(X)",
            "code"
        ],
        [
            "We can visualize the resulting data:",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n\n(X[:, 0], X[:, 1])\n()\n\n\n<img alt=\"plot dbscan\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_dbscan_001.png\" srcset=\"../../_images/sphx_glr_plot_dbscan_001.png\"/>",
            "code"
        ]
    ],
    "Examples->Clustering->Demo of DBSCAN clustering algorithm->Compute DBSCAN": [
        [
            "One can access the labels assigned by  using\nthe labels_ attribute. Noisy samples are given the label math:-1.",
            "markdown"
        ],
        [
            "import numpy as np\nfrom sklearn.cluster import \nfrom sklearn import metrics\n\ndb = (eps=0.3, min_samples=10).fit(X)\nlabels = db.labels_\n\n# Number of clusters in labels, ignoring noise if present.\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\nn_noise_ = list(labels).count(-1)\n\nprint(\"Estimated number of clusters: %d\" % n_clusters_)\nprint(\"Estimated number of noise points: %d\" % n_noise_)",
            "code"
        ],
        [
            "Estimated number of clusters: 3\nEstimated number of noise points: 18",
            "code"
        ],
        [
            "Clustering algorithms are fundamentally unsupervised learning methods.\nHowever, since  gives access to the true\nlabels of the synthetic clusters, it is possible to use evaluation metrics\nthat leverage this \u201csupervised\u201d ground truth information to quantify the\nquality of the resulting clusters. Examples of such metrics are the\nhomogeneity, completeness, V-measure, Rand-Index, Adjusted Rand-Index and\nAdjusted Mutual Information (AMI).",
            "markdown"
        ],
        [
            "If the ground truth labels are not known, evaluation can only be performed\nusing the model results itself. In that case, the Silhouette Coefficient comes\nin handy.",
            "markdown"
        ],
        [
            "For more information, see the\n\nexample or the  module.",
            "markdown"
        ],
        [
            "print(f\"Homogeneity: {(labels_true, labels):.3f}\")\nprint(f\"Completeness: {(labels_true, labels):.3f}\")\nprint(f\"V-measure: {(labels_true, labels):.3f}\")\nprint(f\"Adjusted Rand Index: {(labels_true, labels):.3f}\")\nprint(\n    \"Adjusted Mutual Information:\"\n    f\" {(labels_true, labels):.3f}\"\n)\nprint(f\"Silhouette Coefficient: {(X, labels):.3f}\")",
            "code"
        ],
        [
            "Homogeneity: 0.953\nCompleteness: 0.883\nV-measure: 0.917\nAdjusted Rand Index: 0.952\nAdjusted Mutual Information: 0.916\nSilhouette Coefficient: 0.626",
            "code"
        ]
    ],
    "Examples->Clustering->Demo of DBSCAN clustering algorithm->Plot results": [
        [
            "Core samples (large dots) and non-core samples (small dots) are color-coded\naccording to the asigned cluster. Samples tagged as noise are represented in\nblack.",
            "markdown"
        ],
        [
            "unique_labels = set(labels)\ncore_samples_mask = (labels, dtype=bool)\ncore_samples_mask[db.core_sample_indices_] = True\n\ncolors = [plt.cm.Spectral(each) for each in (0, 1, len(unique_labels))]\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        # Black used for noise.\n        col = [0, 0, 0, 1]\n\n    class_member_mask = labels == k\n\n    xy = X[class_member_mask & core_samples_mask]\n    (\n        xy[:, 0],\n        xy[:, 1],\n        \"o\",\n        markerfacecolor=tuple(col),\n        markeredgecolor=\"k\",\n        markersize=14,\n    )\n\n    xy = X[class_member_mask & ~core_samples_mask]\n    (\n        xy[:, 0],\n        xy[:, 1],\n        \"o\",\n        markerfacecolor=tuple(col),\n        markeredgecolor=\"k\",\n        markersize=6,\n    )\n\n(f\"Estimated number of clusters: {n_clusters_}\")\n()\n\n\n<img alt=\"Estimated number of clusters: 3\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_dbscan_002.png\" srcset=\"../../_images/sphx_glr_plot_dbscan_002.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.194 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Clustering->Demo of OPTICS clustering algorithm": [
        [
            "Finds core samples of high density and expands clusters from them.\nThis example uses data that is generated so that the clusters have\ndifferent densities.\nThe  is first used with its Xi cluster detection\nmethod, and then setting specific thresholds on the reachability, which\ncorresponds to . We can see that the different\nclusters of OPTICS\u2019s Xi method can be recovered with different choices of\nthresholds in DBSCAN.\n<img alt=\"Reachability Plot, Automatic Clustering OPTICS, Clustering at 0.5 epsilon cut DBSCAN, Clustering at 2.0 epsilon cut DBSCAN\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_optics_001.png\" srcset=\"../../_images/sphx_glr_plot_optics_001.png\"/>",
            "markdown"
        ],
        [
            "# Authors: Shane Grigsby &lt;refuge@rocktalus.com\n#          Adrin Jalali &lt;adrin.jalali@gmail.com\n# License: BSD 3 clause\n\nfrom sklearn.cluster import , \nimport matplotlib.gridspec as gridspec\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate sample data\n\n(0)\nn_points_per_cluster = 250\n\nC1 = [-5, -2] + 0.8 * (n_points_per_cluster, 2)\nC2 = [4, -1] + 0.1 * (n_points_per_cluster, 2)\nC3 = [1, -2] + 0.2 * (n_points_per_cluster, 2)\nC4 = [-2, 3] + 0.3 * (n_points_per_cluster, 2)\nC5 = [3, -2] + 1.6 * (n_points_per_cluster, 2)\nC6 = [5, 6] + 2 * (n_points_per_cluster, 2)\nX = ((C1, C2, C3, C4, C5, C6))\n\nclust = (min_samples=50, xi=0.05, min_cluster_size=0.05)\n\n# Run the fit\nclust.fit(X)\n\nlabels_050 = (\n    reachability=clust.reachability_,\n    core_distances=clust.core_distances_,\n    ordering=clust.ordering_,\n    eps=0.5,\n)\nlabels_200 = (\n    reachability=clust.reachability_,\n    core_distances=clust.core_distances_,\n    ordering=clust.ordering_,\n    eps=2,\n)\n\nspace = (len(X))\nreachability = clust.reachability_[clust.ordering_]\nlabels = clust.labels_[clust.ordering_]\n\n(figsize=(10, 7))\nG = (2, 3)\nax1 = (G[0, :])\nax2 = (G[1, 0])\nax3 = (G[1, 1])\nax4 = (G[1, 2])\n\n# Reachability plot\ncolors = [\"g.\", \"r.\", \"b.\", \"y.\", \"c.\"]\nfor klass, color in zip(range(0, 5), colors):\n    Xk = space[labels == klass]\n    Rk = reachability[labels == klass]\n    ax1.plot(Xk, Rk, color, alpha=0.3)\nax1.plot(space[labels == -1], reachability[labels == -1], \"k.\", alpha=0.3)\nax1.plot(space, (space, 2.0, dtype=float), \"k-\", alpha=0.5)\nax1.plot(space, (space, 0.5, dtype=float), \"k-.\", alpha=0.5)\nax1.set_ylabel(\"Reachability (epsilon distance)\")\nax1.set_title(\"Reachability Plot\")\n\n# OPTICS\ncolors = [\"g.\", \"r.\", \"b.\", \"y.\", \"c.\"]\nfor klass, color in zip(range(0, 5), colors):\n    Xk = X[clust.labels_ == klass]\n    ax2.plot(Xk[:, 0], Xk[:, 1], color, alpha=0.3)\nax2.plot(X[clust.labels_ == -1, 0], X[clust.labels_ == -1, 1], \"k+\", alpha=0.1)\nax2.set_title(\"Automatic Clustering\\nOPTICS\")\n\n# DBSCAN at 0.5\ncolors = [\"g.\", \"r.\", \"b.\", \"c.\"]\nfor klass, color in zip(range(0, 4), colors):\n    Xk = X[labels_050 == klass]\n    ax3.plot(Xk[:, 0], Xk[:, 1], color, alpha=0.3)\nax3.plot(X[labels_050 == -1, 0], X[labels_050 == -1, 1], \"k+\", alpha=0.1)\nax3.set_title(\"Clustering at 0.5 epsilon cut\\nDBSCAN\")\n\n# DBSCAN at 2.\ncolors = [\"g.\", \"m.\", \"y.\", \"c.\"]\nfor klass, color in zip(range(0, 4), colors):\n    Xk = X[labels_200 == klass]\n    ax4.plot(Xk[:, 0], Xk[:, 1], color, alpha=0.3)\nax4.plot(X[labels_200 == -1, 0], X[labels_200 == -1, 1], \"k+\", alpha=0.1)\nax4.set_title(\"Clustering at 2.0 epsilon cut\\nDBSCAN\")\n\n()\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  1.358 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Clustering->Demo of affinity propagation clustering algorithm": [
        [
            "Reference:\nBrendan J. Frey and Delbert Dueck, \u201cClustering by Passing Messages\nBetween Data Points\u201d, Science Feb. 2007",
            "markdown"
        ],
        [
            "import numpy as np\n\nfrom sklearn.cluster import \nfrom sklearn import metrics\nfrom sklearn.datasets import",
            "code"
        ]
    ],
    "Examples->Clustering->Demo of affinity propagation clustering algorithm->Generate sample data": [
        [
            "centers = [[1, 1], [-1, -1], [1, -1]]\nX, labels_true = (\n    n_samples=300, centers=centers, cluster_std=0.5, random_state=0\n)",
            "code"
        ]
    ],
    "Examples->Clustering->Demo of affinity propagation clustering algorithm->Compute Affinity Propagation": [
        [
            "af = (preference=-50, random_state=0).fit(X)\ncluster_centers_indices = af.cluster_centers_indices_\nlabels = af.labels_\n\nn_clusters_ = len(cluster_centers_indices)\n\nprint(\"Estimated number of clusters: %d\" % n_clusters_)\nprint(\"Homogeneity: %0.3f\" % (labels_true, labels))\nprint(\"Completeness: %0.3f\" % (labels_true, labels))\nprint(\"V-measure: %0.3f\" % (labels_true, labels))\nprint(\"Adjusted Rand Index: %0.3f\" % (labels_true, labels))\nprint(\n    \"Adjusted Mutual Information: %0.3f\"\n    % (labels_true, labels)\n)\nprint(\n    \"Silhouette Coefficient: %0.3f\"\n    % (X, labels, metric=\"sqeuclidean\")\n)",
            "code"
        ],
        [
            "Estimated number of clusters: 3\nHomogeneity: 0.872\nCompleteness: 0.872\nV-measure: 0.872\nAdjusted Rand Index: 0.912\nAdjusted Mutual Information: 0.871\nSilhouette Coefficient: 0.753",
            "code"
        ]
    ],
    "Examples->Clustering->Demo of affinity propagation clustering algorithm->Plot result": [
        [
            "import matplotlib.pyplot as plt\n\n(\"all\")\n(1)\n()\n\ncolors = plt.cycler(\"color\", plt.cm.viridis((0, 1, 4)))\n\nfor k, col in zip(range(n_clusters_), colors):\n    class_members = labels == k\n    cluster_center = X[cluster_centers_indices[k]]\n    (\n        X[class_members, 0], X[class_members, 1], color=col[\"color\"], marker=\".\"\n    )\n    (\n        cluster_center[0], cluster_center[1], s=14, color=col[\"color\"], marker=\"o\"\n    )\n    for x in X[class_members]:\n        (\n            [cluster_center[0], x[0]], [cluster_center[1], x[1]], color=col[\"color\"]\n        )\n\n(\"Estimated number of clusters: %d\" % n_clusters_)\n()\n\n\n<img alt=\"Estimated number of clusters: 3\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_affinity_propagation_001.png\" srcset=\"../../_images/sphx_glr_plot_affinity_propagation_001.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.366 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Clustering->Demonstration of k-means assumptions": [
        [
            "This example is meant to illustrate situations where k-means produces\nunintuitive and possibly undesirable clusters.",
            "markdown"
        ],
        [
            "# Author: Phil Roth &lt;mr.phil.roth@gmail.com\n#         Arturo Amor &lt;david-arturo.amor-quiroz@inria.fr\n# License: BSD 3 clause",
            "code"
        ]
    ],
    "Examples->Clustering->Demonstration of k-means assumptions->Data generation": [
        [
            "The function  generates isotropic\n(spherical) gaussian blobs. To obtain anisotropic (elliptical) gaussian blobs\none has to define a linear transformation.",
            "markdown"
        ],
        [
            "import numpy as np\nfrom sklearn.datasets import \n\nn_samples = 1500\nrandom_state = 170\ntransformation = [[0.60834549, -0.63667341], [-0.40887718, 0.85253229]]\n\nX, y = (n_samples=n_samples, random_state=random_state)\nX_aniso = (X, transformation)  # Anisotropic blobs\nX_varied, y_varied = (\n    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state\n)  # Unequal variance\nX_filtered = (\n    (X[y == 0][:500], X[y == 1][:100], X[y == 2][:10])\n)  # Unevenly sized blobs\ny_filtered = [0] * 500 + [1] * 100 + [2] * 10",
            "code"
        ],
        [
            "We can visualize the resulting data:",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n\nfig, axs = (nrows=2, ncols=2, figsize=(12, 12))\n\naxs[0, 0].scatter(X[:, 0], X[:, 1], c=y)\naxs[0, 0].set_title(\"Mixture of Gaussian Blobs\")\n\naxs[0, 1].scatter(X_aniso[:, 0], X_aniso[:, 1], c=y)\naxs[0, 1].set_title(\"Anisotropically Distributed Blobs\")\n\naxs[1, 0].scatter(X_varied[:, 0], X_varied[:, 1], c=y_varied)\naxs[1, 0].set_title(\"Unequal Variance\")\n\naxs[1, 1].scatter(X_filtered[:, 0], X_filtered[:, 1], c=y_filtered)\naxs[1, 1].set_title(\"Unevenly Sized Blobs\")\n\n(\"Ground truth clusters\").set_y(0.95)\n()\n\n\n<img alt=\"Ground truth clusters, Mixture of Gaussian Blobs, Anisotropically Distributed Blobs, Unequal Variance, Unevenly Sized Blobs\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_kmeans_assumptions_001.png\" srcset=\"../../_images/sphx_glr_plot_kmeans_assumptions_001.png\"/>",
            "code"
        ]
    ],
    "Examples->Clustering->Demonstration of k-means assumptions->Fit models and plot results": [
        [
            "The previously generated data is now used to show how\n behaves in the following scenarios:",
            "markdown"
        ],
        [
            "Non-optimal number of clusters: in a real setting there is no uniquely\ndefined <strong>true</strong> number of clusters. An appropriate number of clusters has\nto be decided from data-based criteria and knowledge of the intended goal.",
            "markdown"
        ],
        [
            "Anisotropically distributed blobs: k-means consists of minimizing sample\u2019s\neuclidean distances to the centroid of the cluster they are assigned to. As\na consequence, k-means is more appropriate for clusters that are isotropic\nand normally distributed (i.e. spherical gaussians).",
            "markdown"
        ],
        [
            "Unequal variance: k-means is equivalent to taking the maximum likelihood\nestimator for a \u201cmixture\u201d of k gaussian distributions with the same\nvariances but with possibly different means.",
            "markdown"
        ],
        [
            "Unevenly sized blobs: there is no theoretical result about k-means that\nstates that it requires similar cluster sizes to perform well, yet\nminimizing euclidean distances does mean that the more sparse and\nhigh-dimensional the problem is, the higher is the need to run the algorithm\nwith different centroid seeds to ensure a global minimal inertia.",
            "markdown"
        ],
        [
            "from sklearn.cluster import \n\ncommon_params = {\n    \"n_init\": \"auto\",\n    \"random_state\": random_state,\n}\n\nfig, axs = (nrows=2, ncols=2, figsize=(12, 12))\n\ny_pred = (n_clusters=2, **common_params).fit_predict(X)\naxs[0, 0].scatter(X[:, 0], X[:, 1], c=y_pred)\naxs[0, 0].set_title(\"Non-optimal Number of Clusters\")\n\ny_pred = (n_clusters=3, **common_params).fit_predict(X_aniso)\naxs[0, 1].scatter(X_aniso[:, 0], X_aniso[:, 1], c=y_pred)\naxs[0, 1].set_title(\"Anisotropically Distributed Blobs\")\n\ny_pred = (n_clusters=3, **common_params).fit_predict(X_varied)\naxs[1, 0].scatter(X_varied[:, 0], X_varied[:, 1], c=y_pred)\naxs[1, 0].set_title(\"Unequal Variance\")\n\ny_pred = (n_clusters=3, **common_params).fit_predict(X_filtered)\naxs[1, 1].scatter(X_filtered[:, 0], X_filtered[:, 1], c=y_pred)\naxs[1, 1].set_title(\"Unevenly Sized Blobs\")\n\n(\"Unexpected KMeans clusters\").set_y(0.95)\n()\n\n\n<img alt=\"Unexpected KMeans clusters, Non-optimal Number of Clusters, Anisotropically Distributed Blobs, Unequal Variance, Unevenly Sized Blobs\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_kmeans_assumptions_002.png\" srcset=\"../../_images/sphx_glr_plot_kmeans_assumptions_002.png\"/>",
            "code"
        ]
    ],
    "Examples->Clustering->Demonstration of k-means assumptions->Possible solutions": [
        [
            "For an example on how to find a correct number of blobs, see\n.\nIn this case it suffices to set n_clusters=3.",
            "markdown"
        ],
        [
            "y_pred = (n_clusters=3, **common_params).fit_predict(X)\n(X[:, 0], X[:, 1], c=y_pred)\n(\"Optimal Number of Clusters\")\n()\n\n\n<img alt=\"Optimal Number of Clusters\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_kmeans_assumptions_003.png\" srcset=\"../../_images/sphx_glr_plot_kmeans_assumptions_003.png\"/>",
            "code"
        ],
        [
            "To deal with unevenly sized blobs one can increase the number of random\ninitializations. In this case we set n_init=10 to avoid finding a\nsub-optimal local minimum. For more details see .",
            "markdown"
        ],
        [
            "y_pred = (n_clusters=3, n_init=10, random_state=random_state).fit_predict(\n    X_filtered\n)\n(X_filtered[:, 0], X_filtered[:, 1], c=y_pred)\n(\"Unevenly Sized Blobs \\nwith several initializations\")\n()\n\n\n<img alt=\"Unevenly Sized Blobs  with several initializations\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_kmeans_assumptions_004.png\" srcset=\"../../_images/sphx_glr_plot_kmeans_assumptions_004.png\"/>",
            "code"
        ],
        [
            "As anisotropic and unequal variances are real limitations of the k-means\nalgorithm, here we propose instead the use of\n, which also assumes gaussian\nclusters but does not impose any constraints on their variances. Notice that\none still has to find the correct number of blobs (see\n).",
            "markdown"
        ],
        [
            "For an example on how other clustering methods deal with anisotropic or\nunequal variance blobs, see the example\n.",
            "markdown"
        ],
        [
            "from sklearn.mixture import \n\nfig, (ax1, ax2) = (nrows=1, ncols=2, figsize=(12, 6))\n\ny_pred = (n_components=3).fit_predict(X_aniso)\nax1.scatter(X_aniso[:, 0], X_aniso[:, 1], c=y_pred)\nax1.set_title(\"Anisotropically Distributed Blobs\")\n\ny_pred = (n_components=3).fit_predict(X_varied)\nax2.scatter(X_varied[:, 0], X_varied[:, 1], c=y_pred)\nax2.set_title(\"Unequal Variance\")\n\n(\"Gaussian mixture clusters\").set_y(0.95)\n()\n\n\n<img alt=\"Gaussian mixture clusters, Anisotropically Distributed Blobs, Unequal Variance\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_kmeans_assumptions_005.png\" srcset=\"../../_images/sphx_glr_plot_kmeans_assumptions_005.png\"/>",
            "code"
        ]
    ],
    "Examples->Clustering->Demonstration of k-means assumptions->Final remarks": [
        [
            "In high-dimensional spaces, Euclidean distances tend to become inflated\n(not shown in this example). Running a dimensionality reduction algorithm\nprior to k-means clustering can alleviate this problem and speed up the\ncomputations (see the example\n).",
            "markdown"
        ],
        [
            "In the case where clusters are known to be isotropic, have similar variance\nand are not too sparse, the k-means algorithm is quite effective and is one of\nthe fastest clustering algorithms available. This advantage is lost if one has\nto restart it several times to avoid convergence to a local minimum.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  1.245 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Clustering->Empirical evaluation of the impact of k-means initialization": [
        [
            "Evaluate the ability of k-means initializations strategies to make\nthe algorithm convergence robust, as measured by the relative standard\ndeviation of the inertia of the clustering (i.e. the sum of squared\ndistances to the nearest cluster center).",
            "markdown"
        ],
        [
            "The first plot shows the best inertia reached for each combination\nof the model (KMeans or MiniBatchKMeans), and the init method\n(init=\"random\" or init=\"k-means++\") for increasing values of the\nn_init parameter that controls the number of initializations.",
            "markdown"
        ],
        [
            "The second plot demonstrates one single run of the MiniBatchKMeans\nestimator using a init=\"random\" and n_init=1. This run leads to\na bad convergence (local optimum), with estimated centers stuck\nbetween ground truth clusters.",
            "markdown"
        ],
        [
            "The dataset used for evaluation is a 2D grid of isotropic Gaussian\nclusters widely spaced.\n\n<img alt=\"Mean inertia for various k-means init across 5 runs\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_kmeans_stability_low_dim_dense_001.png\" srcset=\"../../_images/sphx_glr_plot_kmeans_stability_low_dim_dense_001.png\"/>\n<img alt=\"Example cluster allocation with a single random init with MiniBatchKMeans\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_kmeans_stability_low_dim_dense_002.png\" srcset=\"../../_images/sphx_glr_plot_kmeans_stability_low_dim_dense_002.png\"/>",
            "markdown"
        ],
        [
            "Evaluation of KMeans with k-means++ init\nEvaluation of KMeans with random init\nEvaluation of MiniBatchKMeans with k-means++ init\nEvaluation of MiniBatchKMeans with random init\n\n\n\n<br/>",
            "code"
        ],
        [
            "# Author: Olivier Grisel &lt;olivier.grisel@ensta.org\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\n\nfrom sklearn.utils import \nfrom sklearn.utils import \nfrom sklearn.cluster import \nfrom sklearn.cluster import \n\nrandom_state = (0)\n\n# Number of run (with randomly generated dataset) for each strategy so as\n# to be able to compute an estimate of the standard deviation\nn_runs = 5\n\n# k-means models can do several random inits so as to be able to trade\n# CPU time for convergence robustness\nn_init_range = ([1, 5, 10, 15, 20])\n\n# Datasets generation parameters\nn_samples_per_center = 100\ngrid_size = 3\nscale = 0.1\nn_clusters = grid_size**2\n\n\ndef make_data(random_state, n_samples_per_center, grid_size, scale):\n    random_state = (random_state)\n    centers = ([[i, j] for i in range(grid_size) for j in range(grid_size)])\n    n_clusters_true, n_features = centers.shape\n\n    noise = random_state.normal(\n        scale=scale, size=(n_samples_per_center, centers.shape[1])\n    )\n\n    X = ([c + noise for c in centers])\n    y = ([[i] * n_samples_per_center for i in range(n_clusters_true)])\n    return (X, y, random_state=random_state)\n\n\n# Part 1: Quantitative evaluation of various init methods\n\n\n()\nplots = []\nlegends = []\n\ncases = [\n    (, \"k-means++\", {}, \"^-\"),\n    (, \"random\", {}, \"o-\"),\n    (, \"k-means++\", {\"max_no_improvement\": 3}, \"x-\"),\n    (, \"random\", {\"max_no_improvement\": 3, \"init_size\": 500}, \"d-\"),\n]\n\nfor factory, init, params, format in cases:\n    print(\"Evaluation of %s with %s init\" % (factory.__name__, init))\n    inertia = ((len(n_init_range), n_runs))\n\n    for run_id in range(n_runs):\n        X, y = make_data(run_id, n_samples_per_center, grid_size, scale)\n        for i, n_init in enumerate(n_init_range):\n            km = factory(\n                n_clusters=n_clusters,\n                init=init,\n                random_state=run_id,\n                n_init=n_init,\n                **params,\n            ).fit(X)\n            inertia[i, run_id] = km.inertia_\n    p = (\n        n_init_range, inertia.mean(axis=1), inertia.std(axis=1), fmt=format\n    )\n    plots.append(p[0])\n    legends.append(\"%s with %s init\" % (factory.__name__, init))\n\n(\"n_init\")\n(\"inertia\")\n(plots, legends)\n(\"Mean inertia for various k-means init across %d runs\" % n_runs)\n\n# Part 2: Qualitative visual inspection of the convergence\n\nX, y = make_data(random_state, n_samples_per_center, grid_size, scale)\nkm = (\n    n_clusters=n_clusters, init=\"random\", n_init=1, random_state=random_state\n).fit(X)\n\n()\nfor k in range(n_clusters):\n    my_members = km.labels_ == k\n    color = cm.nipy_spectral(float(k) / n_clusters, 1)\n    (X[my_members, 0], X[my_members, 1], \".\", c=color)\n    cluster_center = km.cluster_centers_[k]\n    (\n        cluster_center[0],\n        cluster_center[1],\n        \"o\",\n        markerfacecolor=color,\n        markeredgecolor=\"k\",\n        markersize=6,\n    )\n    (\n        \"Example cluster allocation with a single random init\\nwith MiniBatchKMeans\"\n    )\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  1.514 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Clustering->Feature agglomeration": [
        [
            "These images how similar features are merged together using\nfeature agglomeration.\n<img alt=\"Original data, Agglomerated data, Labels\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_digits_agglomeration_001.png\" srcset=\"../../_images/sphx_glr_plot_digits_agglomeration_001.png\"/>",
            "markdown"
        ],
        [
            "# Code source: Ga\u00ebl Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets, cluster\nfrom sklearn.feature_extraction.image import grid_to_graph\n\ndigits = ()\nimages = digits.images\nX = (images, (len(images), -1))\nconnectivity = grid_to_graph(*images[0].shape)\n\nagglo = (connectivity=connectivity, n_clusters=32)\n\nagglo.fit(X)\nX_reduced = agglo.transform(X)\n\nX_restored = agglo.inverse_transform(X_reduced)\nimages_restored = (X_restored, images.shape)\n(1, figsize=(4, 3.5))\n()\n(left=0.01, right=0.99, bottom=0.01, top=0.91)\nfor i in range(4):\n    (3, 4, i + 1)\n    (images[i], cmap=plt.cm.gray, vmax=16, interpolation=\"nearest\")\n    (())\n    (())\n    if i == 1:\n        (\"Original data\")\n    (3, 4, 4 + i + 1)\n    (images_restored[i], cmap=plt.cm.gray, vmax=16, interpolation=\"nearest\")\n    if i == 1:\n        (\"Agglomerated data\")\n    (())\n    (())\n\n(3, 4, 10)\n(\n    (agglo.labels_, images[0].shape),\n    interpolation=\"nearest\",\n    cmap=plt.cm.nipy_spectral,\n)\n(())\n(())\n(\"Labels\")\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.200 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Clustering->Feature agglomeration vs. univariate selection": [
        [
            "This example compares 2 dimensionality reduction strategies:",
            "markdown"
        ],
        [
            "univariate feature selection with Anova",
            "markdown"
        ],
        [
            "feature agglomeration with Ward hierarchical clustering",
            "markdown"
        ],
        [
            "Both methods are compared in a regression problem using\na BayesianRidge as supervised estimator.",
            "markdown"
        ],
        [
            "# Author: Alexandre Gramfort &lt;alexandre.gramfort@inria.fr\n# License: BSD 3 clause",
            "code"
        ],
        [
            "import shutil\nimport tempfile\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import linalg, ndimage\nfrom joblib import \n\nfrom sklearn.feature_extraction.image import grid_to_graph\nfrom sklearn import feature_selection\nfrom sklearn.cluster import \nfrom sklearn.linear_model import \nfrom sklearn.pipeline import \nfrom sklearn.model_selection import \nfrom sklearn.model_selection import",
            "code"
        ],
        [
            "Set parameters",
            "markdown"
        ],
        [
            "n_samples = 200\nsize = 40  # image size\nroi_size = 15\nsnr = 5.0\n(0)",
            "code"
        ],
        [
            "Generate data",
            "markdown"
        ],
        [
            "coef = ((size, size))\ncoef[0:roi_size, 0:roi_size] = -1.0\ncoef[-roi_size:, -roi_size:] = 1.0\n\nX = (n_samples, size**2)\nfor x in X:  # smooth data\n    x[:] = (x.reshape(size, size), sigma=1.0).ravel()\nX -= X.mean(axis=0)\nX /= X.std(axis=0)\n\ny = (X, coef.ravel())",
            "code"
        ],
        [
            "add noise",
            "markdown"
        ],
        [
            "noise = (y.shape[0])\nnoise_coef = ((y, 2) / (snr / 20.0)) / (noise, 2)\ny += noise_coef * noise",
            "code"
        ],
        [
            "Compute the coefs of a Bayesian Ridge with GridSearch",
            "markdown"
        ],
        [
            "cv = (2)  # cross-validation generator for model selection\nridge = ()\ncachedir = ()\nmem = (location=cachedir, verbose=1)",
            "code"
        ],
        [
            "Ward agglomeration followed by BayesianRidge",
            "markdown"
        ],
        [
            "connectivity = grid_to_graph(n_x=size, n_y=size)\nward = (n_clusters=10, connectivity=connectivity, memory=mem)\nclf = ([(\"ward\", ward), (\"ridge\", ridge)])\n# Select the optimal number of parcels with grid search\nclf = (clf, {\"ward__n_clusters\": [10, 20, 30]}, n_jobs=1, cv=cv)\nclf.fit(X, y)  # set the best parameters\ncoef_ = clf.best_estimator_.steps[-1][1].coef_\ncoef_ = clf.best_estimator_.steps[0][1].inverse_transform(coef_)\ncoef_agglomeration_ = coef_.reshape(size, size)",
            "code"
        ],
        [
            "________________________________________________________________________________\n[Memory] Calling sklearn.cluster._agglomerative.ward_tree...\nward_tree(array([[-0.451933, ..., -0.675318],\n       ...,\n       [ 0.275706, ..., -1.085711]]), connectivity=&lt;1600x1600 sparse matrix of type '&lt;class 'numpy.int64''\n        with 7840 stored elements in COOrdinate format, n_clusters=None, return_distance=False)\n________________________________________________________ward_tree - 0.1s, 0.0min\n________________________________________________________________________________\n[Memory] Calling sklearn.cluster._agglomerative.ward_tree...\nward_tree(array([[ 0.905206, ...,  0.161245],\n       ...,\n       [-0.849835, ..., -1.091621]]), connectivity=&lt;1600x1600 sparse matrix of type '&lt;class 'numpy.int64''\n        with 7840 stored elements in COOrdinate format, n_clusters=None, return_distance=False)\n________________________________________________________ward_tree - 0.0s, 0.0min\n________________________________________________________________________________\n[Memory] Calling sklearn.cluster._agglomerative.ward_tree...\nward_tree(array([[ 0.905206, ..., -0.675318],\n       ...,\n       [-0.849835, ..., -1.085711]]), connectivity=&lt;1600x1600 sparse matrix of type '&lt;class 'numpy.int64''\n        with 7840 stored elements in COOrdinate format, n_clusters=None, return_distance=False)\n________________________________________________________ward_tree - 0.1s, 0.0min",
            "code"
        ],
        [
            "Anova univariate feature selection followed by BayesianRidge",
            "markdown"
        ],
        [
            "f_regression = mem.cache()  # caching function\nanova = (f_regression)\nclf = ([(\"anova\", anova), (\"ridge\", ridge)])\n# Select the optimal percentage of features with grid search\nclf = (clf, {\"anova__percentile\": [5, 10, 20]}, cv=cv)\nclf.fit(X, y)  # set the best parameters\ncoef_ = clf.best_estimator_.steps[-1][1].coef_\ncoef_ = clf.best_estimator_.steps[0][1].inverse_transform(coef_.reshape(1, -1))\ncoef_selection_ = coef_.reshape(size, size)",
            "code"
        ],
        [
            "________________________________________________________________________________\n[Memory] Calling sklearn.feature_selection._univariate_selection.f_regression...\nf_regression(array([[-0.451933, ...,  0.275706],\n       ...,\n       [-0.675318, ..., -1.085711]]),\narray([ 25.267703, ..., -25.026711]))\n_____________________________________________________f_regression - 0.0s, 0.0min\n________________________________________________________________________________\n[Memory] Calling sklearn.feature_selection._univariate_selection.f_regression...\nf_regression(array([[ 0.905206, ..., -0.849835],\n       ...,\n       [ 0.161245, ..., -1.091621]]),\narray([ -27.447268, ..., -112.638768]))\n_____________________________________________________f_regression - 0.0s, 0.0min\n________________________________________________________________________________\n[Memory] Calling sklearn.feature_selection._univariate_selection.f_regression...\nf_regression(array([[ 0.905206, ..., -0.849835],\n       ...,\n       [-0.675318, ..., -1.085711]]),\narray([-27.447268, ..., -25.026711]))\n_____________________________________________________f_regression - 0.0s, 0.0min",
            "code"
        ],
        [
            "Inverse the transformation to plot the results on an image",
            "markdown"
        ],
        [
            "(\"all\")\n(figsize=(7.3, 2.7))\n(1, 3, 1)\n(coef, interpolation=\"nearest\", cmap=plt.cm.RdBu_r)\n(\"True weights\")\n(1, 3, 2)\n(coef_selection_, interpolation=\"nearest\", cmap=plt.cm.RdBu_r)\n(\"Feature Selection\")\n(1, 3, 3)\n(coef_agglomeration_, interpolation=\"nearest\", cmap=plt.cm.RdBu_r)\n(\"Feature Agglomeration\")\n(0.04, 0.0, 0.98, 0.94, 0.16, 0.26)\n()\n\n\n<img alt=\"True weights, Feature Selection, Feature Agglomeration\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_feature_agglomeration_vs_univariate_selection_001.png\" srcset=\"../../_images/sphx_glr_plot_feature_agglomeration_vs_univariate_selection_001.png\"/>",
            "code"
        ],
        [
            "Attempt to remove the temporary cachedir, but don\u2019t worry if it fails",
            "markdown"
        ],
        [
            "(cachedir, ignore_errors=True)",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.684 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Clustering->Hierarchical clustering: structured vs unstructured ward": [
        [
            "Example builds a swiss roll dataset and runs\nhierarchical clustering on their position.",
            "markdown"
        ],
        [
            "For more information, see .",
            "markdown"
        ],
        [
            "In a first step, the hierarchical clustering is performed without connectivity\nconstraints on the structure and is solely based on distance, whereas in\na second step the clustering is restricted to the k-Nearest Neighbors\ngraph: it\u2019s a hierarchical clustering with structure prior.",
            "markdown"
        ],
        [
            "Some of the clusters learned without connectivity constraints do not\nrespect the structure of the swiss roll and extend across different folds of\nthe manifolds. On the opposite, when opposing connectivity constraints,\nthe clusters form a nice parcellation of the swiss roll.",
            "markdown"
        ],
        [
            "# Authors : Vincent Michel, 2010\n#           Alexandre Gramfort, 2010\n#           Gael Varoquaux, 2010\n# License: BSD 3 clause\n\nimport time as time\n\n# The following import is required\n# for 3D projection to work with matplotlib &lt; 3.2\n\nimport mpl_toolkits.mplot3d  # noqa: F401\n\nimport numpy as np",
            "code"
        ]
    ],
    "Examples->Clustering->Hierarchical clustering: structured vs unstructured ward->Generate data": [
        [
            "We start by generating the Swiss Roll dataset.",
            "markdown"
        ],
        [
            "from sklearn.datasets import \n\nn_samples = 1500\nnoise = 0.05\nX, _ = (n_samples, noise=noise)\n# Make it thinner\nX[:, 1] *= 0.5",
            "code"
        ]
    ],
    "Examples->Clustering->Hierarchical clustering: structured vs unstructured ward->Compute clustering": [
        [
            "We perform AgglomerativeClustering which comes under Hierarchical Clustering\nwithout any connectivity constraints.",
            "markdown"
        ],
        [
            "from sklearn.cluster import \n\nprint(\"Compute unstructured hierarchical clustering...\")\nst = ()\nward = (n_clusters=6, linkage=\"ward\").fit(X)\nelapsed_time = () - st\nlabel = ward.labels_\nprint(f\"Elapsed time: {elapsed_time:.2f}s\")\nprint(f\"Number of points: {label.size}\")",
            "code"
        ],
        [
            "Compute unstructured hierarchical clustering...\nElapsed time: 0.04s\nNumber of points: 1500",
            "code"
        ],
        [
            "We perform AgglomerativeClustering again with connectivity constraints.",
            "markdown"
        ],
        [
            "print(\"Compute structured hierarchical clustering...\")\nst = ()\nward = (\n    n_clusters=6, connectivity=connectivity, linkage=\"ward\"\n).fit(X)\nelapsed_time = () - st\nlabel = ward.labels_\nprint(f\"Elapsed time: {elapsed_time:.2f}s\")\nprint(f\"Number of points: {label.size}\")",
            "code"
        ],
        [
            "Compute structured hierarchical clustering...\nElapsed time: 0.07s\nNumber of points: 1500",
            "code"
        ]
    ],
    "Examples->Clustering->Hierarchical clustering: structured vs unstructured ward->Plot result": [
        [
            "Plotting the unstructured hierarchical clusters.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n\nfig1 = ()\nax1 = fig1.add_subplot(111, projection=\"3d\", elev=7, azim=-80)\nax1.set_position([0, 0, 0.95, 1])\nfor l in (label):\n    ax1.scatter(\n        X[label == l, 0],\n        X[label == l, 1],\n        X[label == l, 2],\n        color=plt.cm.jet(float(l) / np.max(label + 1)),\n        s=20,\n        edgecolor=\"k\",\n    )\n_ = fig1.suptitle(f\"Without connectivity constraints (time {elapsed_time:.2f}s)\")\n\n\n<img alt=\"Without connectivity constraints (time 0.04s)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_ward_structured_vs_unstructured_001.png\" srcset=\"../../_images/sphx_glr_plot_ward_structured_vs_unstructured_001.png\"/>",
            "code"
        ],
        [
            "Plotting the structured hierarchical clusters.",
            "markdown"
        ],
        [
            "fig2 = ()\nax2 = fig2.add_subplot(121, projection=\"3d\", elev=7, azim=-80)\nax2.set_position([0, 0, 0.95, 1])\nfor l in (label):\n    ax2.scatter(\n        X[label == l, 0],\n        X[label == l, 1],\n        X[label == l, 2],\n        color=plt.cm.jet(float(l) / np.max(label + 1)),\n        s=20,\n        edgecolor=\"k\",\n    )\nfig2.suptitle(f\"With connectivity constraints (time {elapsed_time:.2f}s)\")\n\n()\n\n\n<img alt=\"With connectivity constraints (time 0.07s)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_ward_structured_vs_unstructured_002.png\" srcset=\"../../_images/sphx_glr_plot_ward_structured_vs_unstructured_002.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.437 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Clustering->Hierarchical clustering: structured vs unstructured ward->We are defining k-Nearest Neighbors with 10 neighbors": [
        [
            "from sklearn.neighbors import \n\nconnectivity = (X, n_neighbors=10, include_self=False)",
            "code"
        ]
    ],
    "Examples->Clustering->Inductive Clustering": [
        [
            "Clustering can be expensive, especially when our dataset contains millions\nof datapoints. Many clustering algorithms are not  and so\ncannot be directly applied to new data samples without recomputing the\nclustering, which may be intractable. Instead, we can use clustering to then\nlearn an inductive model with a classifier, which has several benefits:",
            "markdown"
        ],
        [
            "it allows the clusters to scale and apply to new data",
            "markdown"
        ],
        [
            "unlike re-fitting the clusters to new samples, it makes sure the labelling\nprocedure is consistent over time",
            "markdown"
        ],
        [
            "it allows us to use the inferential capabilities of the classifier to\ndescribe or explain the clusters",
            "markdown"
        ],
        [
            "This example illustrates a generic implementation of a meta-estimator which\nextends clustering by inducing a classifier from the cluster labels.\n<img alt=\"Ward Linkage, Unknown instances, Classify unknown instances\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_inductive_clustering_001.png\" srcset=\"../../_images/sphx_glr_plot_inductive_clustering_001.png\"/>",
            "markdown"
        ],
        [
            "# Authors: Chirag Nagpal\n#          Christos Aridas\n\nimport matplotlib.pyplot as plt\nfrom sklearn.base import , clone\nfrom sklearn.cluster import \nfrom sklearn.datasets import \nfrom sklearn.ensemble import \nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.utils.metaestimators import \nfrom sklearn.utils.validation import \n\n\nN_SAMPLES = 5000\nRANDOM_STATE = 42\n\n\ndef _classifier_has(attr):\n    \"\"\"Check if we can delegate a method to the underlying classifier.\n\n    First, we check the first fitted classifier if available, otherwise we\n    check the unfitted classifier.\n    \"\"\"\n    return lambda estimator: (\n        hasattr(estimator.classifier_, attr)\n        if hasattr(estimator, \"classifier_\")\n        else hasattr(estimator.classifier, attr)\n    )\n\n\nclass InductiveClusterer():\n    def __init__(self, clusterer, classifier):\n        self.clusterer = clusterer\n        self.classifier = classifier\n\n    def fit(self, X, y=None):\n        self.clusterer_ = clone(self.clusterer)\n        self.classifier_ = clone(self.classifier)\n        y = self.clusterer_.fit_predict(X)\n        self.classifier_.fit(X, y)\n        return self\n\n    @available_if(_classifier_has(\"predict\"))\n    def predict(self, X):\n        (self)\n        return self.classifier_.predict(X)\n\n    @available_if(_classifier_has(\"decision_function\"))\n    def decision_function(self, X):\n        (self)\n        return self.classifier_.decision_function(X)\n\n\ndef plot_scatter(X, color, alpha=0.5):\n    return (X[:, 0], X[:, 1], c=color, alpha=alpha, edgecolor=\"k\")\n\n\n# Generate some training data from clustering\nX, y = (\n    n_samples=N_SAMPLES,\n    cluster_std=[1.0, 1.0, 0.5],\n    centers=[(-5, -5), (0, 0), (5, 5)],\n    random_state=RANDOM_STATE,\n)\n\n\n# Train a clustering algorithm on the training data and get the cluster labels\nclusterer = (n_clusters=3)\ncluster_labels = clusterer.fit_predict(X)\n\n(figsize=(12, 4))\n\n(131)\nplot_scatter(X, cluster_labels)\n(\"Ward Linkage\")\n\n\n# Generate new samples and plot them along with the original dataset\nX_new, y_new = (\n    n_samples=10, centers=[(-7, -1), (-2, 4), (3, 6)], random_state=RANDOM_STATE\n)\n\n(132)\nplot_scatter(X, cluster_labels)\nplot_scatter(X_new, \"black\", 1)\n(\"Unknown instances\")\n\n\n# Declare the inductive learning model that it will be used to\n# predict cluster membership for unknown instances\nclassifier = (random_state=RANDOM_STATE)\ninductive_learner = InductiveClusterer(clusterer, classifier).fit(X)\n\nprobable_clusters = inductive_learner.predict(X_new)\n\n\nax = (133)\nplot_scatter(X, cluster_labels)\nplot_scatter(X_new, probable_clusters)\n\n# Plotting decision regions\n(\n    inductive_learner, X, response_method=\"predict\", alpha=0.4, ax=ax\n)\n(\"Classify unknown instances\")\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  2.604 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Clustering->K-means Clustering": [
        [
            "The plot shows:",
            "markdown"
        ],
        [
            "top left: What a K-means algorithm would yield using 8 clusters.",
            "markdown"
        ],
        [
            "top right: What the effect of a bad initialization is\non the classification process: By setting n_init to only 1\n(default is 10), the amount of times that the algorithm will\nbe run with different centroid seeds is reduced.",
            "markdown"
        ],
        [
            "bottom left: What using eight clusters would deliver.",
            "markdown"
        ],
        [
            "bottom right: The ground truth.\n\n<img alt=\"8 clusters, 3 clusters, 3 clusters, bad initialization, Ground Truth\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cluster_iris_001.png\" srcset=\"../../_images/sphx_glr_plot_cluster_iris_001.png\"/>",
            "markdown"
        ],
        [
            "# Code source: Ga\u00ebl Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Though the following import is not directly being used, it is required\n# for 3D projection to work with matplotlib &lt; 3.2\nimport mpl_toolkits.mplot3d  # noqa: F401\n\nfrom sklearn.cluster import \nfrom sklearn import datasets\n\n(5)\n\niris = ()\nX = iris.data\ny = iris.target\n\nestimators = [\n    (\"k_means_iris_8\", (n_clusters=8, n_init=\"auto\")),\n    (\"k_means_iris_3\", (n_clusters=3, n_init=\"auto\")),\n    (\"k_means_iris_bad_init\", (n_clusters=3, n_init=1, init=\"random\")),\n]\n\nfig = (figsize=(10, 8))\ntitles = [\"8 clusters\", \"3 clusters\", \"3 clusters, bad initialization\"]\nfor idx, ((name, est), title) in enumerate(zip(estimators, titles)):\n    ax = fig.add_subplot(2, 2, idx + 1, projection=\"3d\", elev=48, azim=134)\n    est.fit(X)\n    labels = est.labels_\n\n    ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=labels.astype(float), edgecolor=\"k\")\n\n    ax.xaxis.set_ticklabels([])\n    ax.yaxis.set_ticklabels([])\n    ax.zaxis.set_ticklabels([])\n    ax.set_xlabel(\"Petal width\")\n    ax.set_ylabel(\"Sepal length\")\n    ax.set_zlabel(\"Petal length\")\n    ax.set_title(title)\n\n# Plot the ground truth\nax = fig.add_subplot(2, 2, 4, projection=\"3d\", elev=48, azim=134)\n\nfor name, label in [(\"Setosa\", 0), (\"Versicolour\", 1), (\"Virginica\", 2)]:\n    ax.text3D(\n        X[y == label, 3].mean(),\n        X[y == label, 0].mean(),\n        X[y == label, 2].mean() + 2,\n        name,\n        horizontalalignment=\"center\",\n        bbox=dict(alpha=0.2, edgecolor=\"w\", facecolor=\"w\"),\n    )\n# Reorder the labels to have colors matching the cluster results\ny = (y, [1, 2, 0]).astype(float)\nax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y, edgecolor=\"k\")\n\nax.xaxis.set_ticklabels([])\nax.yaxis.set_ticklabels([])\nax.zaxis.set_ticklabels([])\nax.set_xlabel(\"Petal width\")\nax.set_ylabel(\"Sepal length\")\nax.set_zlabel(\"Petal length\")\nax.set_title(\"Ground Truth\")\n\n(wspace=0.25, hspace=0.25)\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.354 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Clustering->Online learning of a dictionary of parts of faces": [
        [
            "This example uses a large dataset of faces to learn a set of 20 x 20\nimages patches that constitute faces.",
            "markdown"
        ],
        [
            "From the programming standpoint, it is interesting because it shows how\nto use the online API of the scikit-learn to process a very large\ndataset by chunks. The way we proceed is that we load an image at a time\nand extract randomly 50 patches from this image. Once we have accumulated\n500 of these patches (using 10 images), we run the\n method\nof the online KMeans object, MiniBatchKMeans.",
            "markdown"
        ],
        [
            "The verbose setting on the MiniBatchKMeans enables us to see that some\nclusters are reassigned during the successive calls to\npartial-fit. This is because the number of patches that they represent\nhas become too low, and it is better to choose a random new\ncluster.",
            "markdown"
        ]
    ],
    "Examples->Clustering->Online learning of a dictionary of parts of faces->Load the data": [
        [
            "from sklearn import datasets\n\nfaces = ()",
            "code"
        ],
        [
            "downloading Olivetti faces from https://ndownloader.figshare.com/files/5976027 to /home/circleci/scikit_learn_data",
            "code"
        ]
    ],
    "Examples->Clustering->Online learning of a dictionary of parts of faces->Learn the dictionary of images": [
        [
            "import time\n\nimport numpy as np\n\nfrom sklearn.cluster import \nfrom sklearn.feature_extraction.image import \n\nprint(\"Learning the dictionary... \")\nrng = (0)\nkmeans = (n_clusters=81, random_state=rng, verbose=True, n_init=3)\npatch_size = (20, 20)\n\nbuffer = []\nt0 = ()\n\n# The online learning part: cycle over the whole dataset 6 times\nindex = 0\nfor _ in range(6):\n    for img in faces.images:\n        data = (img, patch_size, max_patches=50, random_state=rng)\n        data = (data, (len(data), -1))\n        buffer.append(data)\n        index += 1\n        if index % 10 == 0:\n            data = (buffer, axis=0)\n            data -= (data, axis=0)\n            data /= (data, axis=0)\n            kmeans.partial_fit(data)\n            buffer = []\n        if index % 100 == 0:\n            print(\"Partial fit of %4i out of %i\" % (index, 6 * len(faces.images)))\n\ndt = () - t0\nprint(\"done in %.2fs.\" % dt)",
            "code"
        ],
        [
            "Learning the dictionary...\n[MiniBatchKMeans] Reassigning 6 cluster centers.\n[MiniBatchKMeans] Reassigning 3 cluster centers.\nPartial fit of  100 out of 2400\n[MiniBatchKMeans] Reassigning 3 cluster centers.\n[MiniBatchKMeans] Reassigning 2 cluster centers.\nPartial fit of  200 out of 2400\n[MiniBatchKMeans] Reassigning 1 cluster centers.\n[MiniBatchKMeans] Reassigning 1 cluster centers.\nPartial fit of  300 out of 2400\nPartial fit of  400 out of 2400\nPartial fit of  500 out of 2400\nPartial fit of  600 out of 2400\nPartial fit of  700 out of 2400\nPartial fit of  800 out of 2400\nPartial fit of  900 out of 2400\nPartial fit of 1000 out of 2400\nPartial fit of 1100 out of 2400\nPartial fit of 1200 out of 2400\nPartial fit of 1300 out of 2400\nPartial fit of 1400 out of 2400\nPartial fit of 1500 out of 2400\nPartial fit of 1600 out of 2400\nPartial fit of 1700 out of 2400\nPartial fit of 1800 out of 2400\nPartial fit of 1900 out of 2400\nPartial fit of 2000 out of 2400\nPartial fit of 2100 out of 2400\nPartial fit of 2200 out of 2400\nPartial fit of 2300 out of 2400\nPartial fit of 2400 out of 2400\ndone in 1.25s.",
            "code"
        ]
    ],
    "Examples->Clustering->Online learning of a dictionary of parts of faces->Plot the results": [
        [
            "import matplotlib.pyplot as plt\n\n(figsize=(4.2, 4))\nfor i, patch in enumerate(kmeans.cluster_centers_):\n    (9, 9, i + 1)\n    (patch.reshape(patch_size), cmap=plt.cm.gray, interpolation=\"nearest\")\n    (())\n    (())\n\n\n(\n    \"Patches of faces\\nTrain time %.1fs on %d patches\" % (dt, 8 * len(faces.images)),\n    fontsize=16,\n)\n(0.08, 0.02, 0.92, 0.85, 0.08, 0.23)\n\n()\n\n\n<img alt=\"Patches of faces Train time 1.3s on 3200 patches\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_dict_face_patches_001.png\" srcset=\"../../_images/sphx_glr_plot_dict_face_patches_001.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  3.987 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Clustering->Plot Hierarchical Clustering Dendrogram": [
        [
            "This example plots the corresponding dendrogram of a hierarchical clustering\nusing AgglomerativeClustering and the dendrogram method available in scipy.\n<img alt=\"Hierarchical Clustering Dendrogram\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_agglomerative_dendrogram_001.png\" srcset=\"../../_images/sphx_glr_plot_agglomerative_dendrogram_001.png\"/>",
            "markdown"
        ],
        [
            "import numpy as np\n\nfrom matplotlib import pyplot as plt\nfrom scipy.cluster.hierarchy import \nfrom sklearn.datasets import \nfrom sklearn.cluster import \n\n\ndef plot_dendrogram(model, **kwargs):\n    # Create linkage matrix and then plot the dendrogram\n\n    # create the counts of samples under each node\n    counts = (model.children_.shape[0])\n    n_samples = len(model.labels_)\n    for i, merge in enumerate(model.children_):\n        current_count = 0\n        for child_idx in merge:\n            if child_idx &lt; n_samples:\n                current_count += 1  # leaf node\n            else:\n                current_count += counts[child_idx - n_samples]\n        counts[i] = current_count\n\n    linkage_matrix = (\n        [model.children_, model.distances_, counts]\n    ).astype(float)\n\n    # Plot the corresponding dendrogram\n    (linkage_matrix, **kwargs)\n\n\niris = ()\nX = iris.data\n\n# setting distance_threshold=0 ensures we compute the full tree.\nmodel = (distance_threshold=0, n_clusters=None)\n\nmodel = model.fit(X)\n(\"Hierarchical Clustering Dendrogram\")\n# plot the top three levels of the dendrogram\nplot_dendrogram(model, truncate_mode=\"level\", p=3)\n(\"Number of points in node (or index of point if no parenthesis).\")\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.151 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Clustering->Segmenting the picture of greek coins in regions": [
        [
            "This example uses  on a graph created from\nvoxel-to-voxel difference on an image to break this image into multiple\npartly-homogeneous regions.",
            "markdown"
        ],
        [
            "This procedure (spectral clustering on an image) is an efficient\napproximate solution for finding normalized graph cuts.",
            "markdown"
        ],
        [
            "There are three options to assign labels:",
            "markdown"
        ],
        [
            "\u2018kmeans\u2019 spectral clustering clusters samples in the embedding space\nusing a kmeans algorithm",
            "markdown"
        ],
        [
            "\u2018discrete\u2019 iteratively searches for the closest partition\nspace to the embedding space of spectral clustering.",
            "markdown"
        ],
        [
            "\u2018cluster_qr\u2019 assigns labels using the QR factorization with pivoting\nthat directly determines the partition in the embedding space.",
            "markdown"
        ],
        [
            "# Author: Gael Varoquaux &lt;gael.varoquaux@normalesup.org\n#         Brian Cheung\n#         Andrew Knyazev &lt;Andrew.Knyazev@ucdenver.edu\n# License: BSD 3 clause\n\nimport time\n\nimport numpy as np\nfrom scipy.ndimage import \nimport matplotlib.pyplot as plt\nfrom skimage.data import coins\nfrom skimage.transform import rescale\n\nfrom sklearn.feature_extraction import image\nfrom sklearn.cluster import \n\n\n# load the coins as a numpy array\norig_coins = coins()\n\n# Resize it to 20% of the original size to speed up the processing\n# Applying a Gaussian filter for smoothing prior to down-scaling\n# reduces aliasing artifacts.\nsmoothened_coins = (orig_coins, sigma=2)\nrescaled_coins = rescale(smoothened_coins, 0.2, mode=\"reflect\", anti_aliasing=False)\n\n# Convert the image into a graph with the value of the gradient on the\n# edges.\ngraph = image.img_to_graph(rescaled_coins)\n\n# Take a decreasing function of the gradient: an exponential\n# The smaller beta is, the more independent the segmentation is of the\n# actual image. For beta=1, the segmentation is close to a voronoi\nbeta = 10\neps = 1e-6\ngraph.data = (-beta * graph.data / graph.data.std()) + eps\n\n# The number of segmented regions to display needs to be chosen manually.\n# The current version of 'spectral_clustering' does not support determining\n# the number of good quality clusters automatically.\nn_regions = 26",
            "code"
        ],
        [
            "Compute and visualize the resulting regions",
            "markdown"
        ],
        [
            "# Computing a few extra eigenvectors may speed up the eigen_solver.\n# The spectral clustering quality may also benetif from requesting\n# extra regions for segmentation.\nn_regions_plus = 3\n\n# Apply spectral clustering using the default eigen_solver='arpack'.\n# Any implemented solver can be used: eigen_solver='arpack', 'lobpcg', or 'amg'.\n# Choosing eigen_solver='amg' requires an extra package called 'pyamg'.\n# The quality of segmentation and the speed of calculations is mostly determined\n# by the choice of the solver and the value of the tolerance 'eigen_tol'.\n# TODO: varying eigen_tol seems to have no effect for 'lobpcg' and 'amg' #21243.\nfor assign_labels in (\"kmeans\", \"discretize\", \"cluster_qr\"):\n    t0 = ()\n    labels = (\n        graph,\n        n_clusters=(n_regions + n_regions_plus),\n        eigen_tol=1e-7,\n        assign_labels=assign_labels,\n        random_state=42,\n    )\n\n    t1 = ()\n    labels = labels.reshape(rescaled_coins.shape)\n    (figsize=(5, 5))\n    (rescaled_coins, cmap=plt.cm.gray)\n\n    (())\n    (())\n    title = \"Spectral clustering: %s, %.2fs\" % (assign_labels, (t1 - t0))\n    print(title)\n    (title)\n    for l in range(n_regions):\n        colors = [plt.cm.nipy_spectral((l + 4) / float(n_regions + 4))]\n        (labels == l, colors=colors)\n        # To view individual segments as appear comment in plt.pause(0.5)\n()\n\n# TODO: After #21194 is merged and #21243 is fixed, check which eigen_solver\n# is the best and set eigen_solver='arpack', 'lobpcg', or 'amg' and eigen_tol\n# explicitly in this example.\n\n\n\n<img alt=\"Spectral clustering: kmeans, 2.04s\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_coin_segmentation_001.png\" srcset=\"../../_images/sphx_glr_plot_coin_segmentation_001.png\"/>\n<img alt=\"Spectral clustering: discretize, 1.83s\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_coin_segmentation_002.png\" srcset=\"../../_images/sphx_glr_plot_coin_segmentation_002.png\"/>\n<img alt=\"Spectral clustering: cluster_qr, 1.82s\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_coin_segmentation_003.png\" srcset=\"../../_images/sphx_glr_plot_coin_segmentation_003.png\"/>",
            "code"
        ],
        [
            "Spectral clustering: kmeans, 2.04s\nSpectral clustering: discretize, 1.83s\nSpectral clustering: cluster_qr, 1.82s",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  6.433 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Clustering->Selecting the number of clusters with silhouette analysis on KMeans clustering": [
        [
            "Silhouette analysis can be used to study the separation distance between the\nresulting clusters. The silhouette plot displays a measure of how close each\npoint in one cluster is to points in the neighboring clusters and thus provides\na way to assess parameters like number of clusters visually. This measure has a\nrange of [-1, 1].",
            "markdown"
        ],
        [
            "Silhouette coefficients (as these values are referred to as) near +1 indicate\nthat the sample is far away from the neighboring clusters. A value of 0\nindicates that the sample is on or very close to the decision boundary between\ntwo neighboring clusters and negative values indicate that those samples might\nhave been assigned to the wrong cluster.",
            "markdown"
        ],
        [
            "In this example the silhouette analysis is used to choose an optimal value for\nn_clusters. The silhouette plot shows that the n_clusters value of 3, 5\nand 6 are a bad pick for the given data due to the presence of clusters with\nbelow average silhouette scores and also due to wide fluctuations in the size\nof the silhouette plots. Silhouette analysis is more ambivalent in deciding\nbetween 2 and 4.",
            "markdown"
        ],
        [
            "Also from the thickness of the silhouette plot the cluster size can be\nvisualized. The silhouette plot for cluster 0 when n_clusters is equal to\n2, is bigger in size owing to the grouping of the 3 sub clusters into one big\ncluster. However when the n_clusters is equal to 4, all the plots are more\nor less of similar thickness and hence are of similar sizes as can be also\nverified from the labelled scatter plot on the right.\n\n<img alt=\"Silhouette analysis for KMeans clustering on sample data with n_clusters = 2, The silhouette plot for the various clusters., The visualization of the clustered data.\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_kmeans_silhouette_analysis_001.png\" srcset=\"../../_images/sphx_glr_plot_kmeans_silhouette_analysis_001.png\"/>\n<img alt=\"Silhouette analysis for KMeans clustering on sample data with n_clusters = 3, The silhouette plot for the various clusters., The visualization of the clustered data.\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_kmeans_silhouette_analysis_002.png\" srcset=\"../../_images/sphx_glr_plot_kmeans_silhouette_analysis_002.png\"/>\n<img alt=\"Silhouette analysis for KMeans clustering on sample data with n_clusters = 4, The silhouette plot for the various clusters., The visualization of the clustered data.\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_kmeans_silhouette_analysis_003.png\" srcset=\"../../_images/sphx_glr_plot_kmeans_silhouette_analysis_003.png\"/>\n<img alt=\"Silhouette analysis for KMeans clustering on sample data with n_clusters = 5, The silhouette plot for the various clusters., The visualization of the clustered data.\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_kmeans_silhouette_analysis_004.png\" srcset=\"../../_images/sphx_glr_plot_kmeans_silhouette_analysis_004.png\"/>\n<img alt=\"Silhouette analysis for KMeans clustering on sample data with n_clusters = 6, The silhouette plot for the various clusters., The visualization of the clustered data.\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_kmeans_silhouette_analysis_005.png\" srcset=\"../../_images/sphx_glr_plot_kmeans_silhouette_analysis_005.png\"/>",
            "markdown"
        ],
        [
            "For n_clusters = 2 The average silhouette_score is : 0.7049787496083262\nFor n_clusters = 3 The average silhouette_score is : 0.5882004012129721\nFor n_clusters = 4 The average silhouette_score is : 0.6505186632729437\nFor n_clusters = 5 The average silhouette_score is : 0.5662344175321901\nFor n_clusters = 6 The average silhouette_score is : 0.4358297989156284\n\n\n\n<br/>",
            "code"
        ],
        [
            "from sklearn.datasets import \nfrom sklearn.cluster import \nfrom sklearn.metrics import , \n\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport numpy as np\n\n# Generating the sample data from make_blobs\n# This particular setting has one distinct cluster and 3 clusters placed close\n# together.\nX, y = (\n    n_samples=500,\n    n_features=2,\n    centers=4,\n    cluster_std=1,\n    center_box=(-10.0, 10.0),\n    shuffle=True,\n    random_state=1,\n)  # For reproducibility\n\nrange_n_clusters = [2, 3, 4, 5, 6]\n\nfor n_clusters in range_n_clusters:\n    # Create a subplot with 1 row and 2 columns\n    fig, (ax1, ax2) = (1, 2)\n    fig.set_size_inches(18, 7)\n\n    # The 1st subplot is the silhouette plot\n    # The silhouette coefficient can range from -1, 1 but in this example all\n    # lie within [-0.1, 1]\n    ax1.set_xlim([-0.1, 1])\n    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n    # plots of individual clusters, to demarcate them clearly.\n    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n\n    # Initialize the clusterer with n_clusters value and a random generator\n    # seed of 10 for reproducibility.\n    clusterer = (n_clusters=n_clusters, n_init=\"auto\", random_state=10)\n    cluster_labels = clusterer.fit_predict(X)\n\n    # The silhouette_score gives the average value for all the samples.\n    # This gives a perspective into the density and separation of the formed\n    # clusters\n    silhouette_avg = (X, cluster_labels)\n    print(\n        \"For n_clusters =\",\n        n_clusters,\n        \"The average silhouette_score is :\",\n        silhouette_avg,\n    )\n\n    # Compute the silhouette scores for each sample\n    sample_silhouette_values = (X, cluster_labels)\n\n    y_lower = 10\n    for i in range(n_clusters):\n        # Aggregate the silhouette scores for samples belonging to\n        # cluster i, and sort them\n        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        color = cm.nipy_spectral(float(i) / n_clusters)\n        ax1.fill_betweenx(\n            (y_lower, y_upper),\n            0,\n            ith_cluster_silhouette_values,\n            facecolor=color,\n            edgecolor=color,\n            alpha=0.7,\n        )\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title(\"The silhouette plot for the various clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\n    # 2nd Plot showing the actual clusters formed\n    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n    ax2.scatter(\n        X[:, 0], X[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n    )\n\n    # Labeling the clusters\n    centers = clusterer.cluster_centers_\n    # Draw white circles at cluster centers\n    ax2.scatter(\n        centers[:, 0],\n        centers[:, 1],\n        marker=\"o\",\n        c=\"white\",\n        alpha=1,\n        s=200,\n        edgecolor=\"k\",\n    )\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\n\n    ax2.set_title(\"The visualization of the clustered data.\")\n    ax2.set_xlabel(\"Feature space for the 1st feature\")\n    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\n    (\n        \"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\"\n        % n_clusters,\n        fontsize=14,\n        fontweight=\"bold\",\n    )\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  1.223 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Clustering->Spectral clustering for image segmentation": [
        [
            "In this example, an image with connected circles is generated and\nspectral clustering is used to separate the circles.",
            "markdown"
        ],
        [
            "In these settings, the  approach solves the problem\nknow as \u2018normalized graph cuts\u2019: the image is seen as a graph of\nconnected voxels, and the spectral clustering algorithm amounts to\nchoosing graph cuts defining regions while minimizing the ratio of the\ngradient along the cut, and the volume of the region.",
            "markdown"
        ],
        [
            "As the algorithm tries to balance the volume (ie balance the region\nsizes), if we take circles with different sizes, the segmentation fails.",
            "markdown"
        ],
        [
            "In addition, as there is no useful information in the intensity of the image,\nor its gradient, we choose to perform the spectral clustering on a graph\nthat is only weakly informed by the gradient. This is close to performing\na Voronoi partition of the graph.",
            "markdown"
        ],
        [
            "In addition, we use the mask of the objects to restrict the graph to the\noutline of the objects. In this example, we are interested in\nseparating the objects one from the other, and not from the background.",
            "markdown"
        ],
        [
            "# Authors:  Emmanuelle Gouillart &lt;emmanuelle.gouillart@normalesup.org\n#           Gael Varoquaux &lt;gael.varoquaux@normalesup.org\n# License: BSD 3 clause",
            "code"
        ]
    ],
    "Examples->Clustering->Spectral clustering for image segmentation->Generate the data": [
        [
            "import numpy as np\n\nl = 100\nx, y = ((l, l))\n\ncenter1 = (28, 24)\ncenter2 = (40, 50)\ncenter3 = (67, 58)\ncenter4 = (24, 70)\n\nradius1, radius2, radius3, radius4 = 16, 14, 15, 14\n\ncircle1 = (x - center1[0]) ** 2 + (y - center1[1]) ** 2 &lt; radius1**2\ncircle2 = (x - center2[0]) ** 2 + (y - center2[1]) ** 2 &lt; radius2**2\ncircle3 = (x - center3[0]) ** 2 + (y - center3[1]) ** 2 &lt; radius3**2\ncircle4 = (x - center4[0]) ** 2 + (y - center4[1]) ** 2 &lt; radius4**2",
            "code"
        ]
    ],
    "Examples->Clustering->Spectral clustering for image segmentation->Plotting four circles": [
        [
            "img = circle1 + circle2 + circle3 + circle4\n\n# We use a mask that limits to the foreground: the problem that we are\n# interested in here is not separating the objects from the background,\n# but separating them one from the other.\nmask = img.astype(bool)\n\nimg = img.astype(float)\nimg += 1 + 0.2 * (*img.shape)",
            "code"
        ],
        [
            "Convert the image into a graph with the value of the gradient on the\nedges.",
            "markdown"
        ],
        [
            "from sklearn.feature_extraction import image\n\ngraph = image.img_to_graph(img, mask=mask)",
            "code"
        ],
        [
            "Take a decreasing function of the gradient resulting in a segmentation\nthat is close to a Voronoi partition",
            "markdown"
        ],
        [
            "graph.data = (-graph.data / graph.data.std())",
            "code"
        ],
        [
            "Here we perform spectral clustering using the arpack solver since amg is\nnumerically unstable on this example. We then plot the results.",
            "markdown"
        ],
        [
            "from sklearn.cluster import \nimport matplotlib.pyplot as plt\n\nlabels = (graph, n_clusters=4, eigen_solver=\"arpack\")\nlabel_im = (mask.shape, -1.0)\nlabel_im[mask] = labels\n\nfig, axs = (nrows=1, ncols=2, figsize=(10, 5))\naxs[0].matshow(img)\naxs[1].matshow(label_im)\n\n()\n\n\n<img alt=\"plot segmentation toy\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_segmentation_toy_001.png\" srcset=\"../../_images/sphx_glr_plot_segmentation_toy_001.png\"/>",
            "code"
        ]
    ],
    "Examples->Clustering->Spectral clustering for image segmentation->Plotting two circles": [
        [
            "Here we repeat the above process but only consider the first two circles\nwe generated. Note that this results in a cleaner separation between the\ncircles as the region sizes are easier to balance in this case.",
            "markdown"
        ],
        [
            "img = circle1 + circle2\nmask = img.astype(bool)\nimg = img.astype(float)\n\nimg += 1 + 0.2 * (*img.shape)\n\ngraph = image.img_to_graph(img, mask=mask)\ngraph.data = (-graph.data / graph.data.std())\n\nlabels = (graph, n_clusters=2, eigen_solver=\"arpack\")\nlabel_im = (mask.shape, -1.0)\nlabel_im[mask] = labels\n\nfig, axs = (nrows=1, ncols=2, figsize=(10, 5))\naxs[0].matshow(img)\naxs[1].matshow(label_im)\n\n()\n\n\n<img alt=\"plot segmentation toy\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_segmentation_toy_002.png\" srcset=\"../../_images/sphx_glr_plot_segmentation_toy_002.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.560 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Clustering->Various Agglomerative Clustering on a 2D embedding of digits": [
        [
            "An illustration of various linkage option for agglomerative clustering on\na 2D embedding of the digits dataset.",
            "markdown"
        ],
        [
            "The goal of this example is to show intuitively how the metrics behave, and\nnot to find good clusters for the digits. This is why the example works on a\n2D embedding.",
            "markdown"
        ],
        [
            "What this example shows us is the behavior \u201crich getting richer\u201d of\nagglomerative clustering that tends to create uneven cluster sizes.",
            "markdown"
        ],
        [
            "This behavior is pronounced for the average linkage strategy,\nthat ends up with a couple of clusters with few datapoints.",
            "markdown"
        ],
        [
            "The case of single linkage is even more pathologic with a very\nlarge cluster covering most digits, an intermediate size (clean)\ncluster with most zero digits and all other clusters being drawn\nfrom noise points around the fringes.",
            "markdown"
        ],
        [
            "The other linkage strategies lead to more evenly distributed\nclusters that are therefore likely to be less sensible to a\nrandom resampling of the dataset.\n\n<img alt=\"ward linkage\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_digits_linkage_001.png\" srcset=\"../../_images/sphx_glr_plot_digits_linkage_001.png\"/>\n<img alt=\"average linkage\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_digits_linkage_002.png\" srcset=\"../../_images/sphx_glr_plot_digits_linkage_002.png\"/>\n<img alt=\"complete linkage\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_digits_linkage_003.png\" srcset=\"../../_images/sphx_glr_plot_digits_linkage_003.png\"/>\n<img alt=\"single linkage\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_digits_linkage_004.png\" srcset=\"../../_images/sphx_glr_plot_digits_linkage_004.png\"/>",
            "markdown"
        ],
        [
            "Computing embedding\nDone.\nward :  0.06s\naverage :       0.06s\ncomplete :      0.05s\nsingle :        0.02s\n\n\n\n<br/>",
            "code"
        ],
        [
            "# Authors: Gael Varoquaux\n# License: BSD 3 clause (C) INRIA 2014\n\nfrom time import \n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom sklearn import manifold, datasets\n\ndigits = ()\nX, y = digits.data, digits.target\nn_samples, n_features = X.shape\n\n(0)\n\n\n# ----------------------------------------------------------------------\n# Visualize the clustering\ndef plot_clustering(X_red, labels, title=None):\n    x_min, x_max = np.min(X_red, axis=0), np.max(X_red, axis=0)\n    X_red = (X_red - x_min) / (x_max - x_min)\n\n    (figsize=(6, 4))\n    for digit in digits.target_names:\n        (\n            *X_red[y == digit].T,\n            marker=f\"${digit}$\",\n            s=50,\n            c=plt.cm.nipy_spectral(labels[y == digit] / 10),\n            alpha=0.5,\n        )\n\n    ([])\n    ([])\n    if title is not None:\n        (title, size=17)\n    (\"off\")\n    (rect=[0, 0.03, 1, 0.95])\n\n\n# ----------------------------------------------------------------------\n# 2D embedding of the digits dataset\nprint(\"Computing embedding\")\nX_red = (n_components=2).fit_transform(X)\nprint(\"Done.\")\n\nfrom sklearn.cluster import \n\nfor linkage in (\"ward\", \"average\", \"complete\", \"single\"):\n    clustering = (linkage=linkage, n_clusters=10)\n    t0 = ()\n    clustering.fit(X_red)\n    print(\"%s :\\t%.2fs\" % (linkage, () - t0))\n\n    plot_clustering(X_red, clustering.labels_, \"%s linkage\" % linkage)\n\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  1.679 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Clustering->Vector Quantization Example": [
        [
            "This example shows how one can use \nto perform vector quantization on a set of toy image, the raccoon face.",
            "markdown"
        ],
        [
            "# Authors: Gael Varoquaux\n#          Jaques Grobler\n# License: BSD 3 clause",
            "code"
        ]
    ],
    "Examples->Clustering->Vector Quantization Example->Original image": [
        [
            "We start by loading the raccoon face image from SciPy. We will additionally check\na couple of information regarding the image, such as the shape and data type used\nto store the image.",
            "markdown"
        ],
        [
            "Note that depending of the SciPy version, we have to adapt the import since the\nfunction returning the image is not located in the same module. Also, SciPy >= 1.10\nrequires the package pooch to be installed.",
            "markdown"
        ],
        [
            "try:  # Scipy = 1.10\n    from scipy.datasets import \nexcept ImportError:\n    from scipy.misc import \n\nraccoon_face = (gray=True)\n\nprint(f\"The dimension of the image is {raccoon_face.shape}\")\nprint(f\"The data used to encode the image is of type {raccoon_face.dtype}\")\nprint(f\"The number of bytes taken in RAM is {raccoon_face.nbytes}\")",
            "code"
        ],
        [
            "The dimension of the image is (768, 1024)\nThe data used to encode the image is of type uint8\nThe number of bytes taken in RAM is 786432",
            "code"
        ],
        [
            "Thus the image is a 2D array of 768 pixels in height and 1024 pixels in width. Each\nvalue is a 8-bit unsigned integer, which means that the image is encoded using 8\nbits per pixel. The total memory usage of the image is 786 kilobytes (1 byte equals\n8 bits).",
            "markdown"
        ],
        [
            "Using 8-bit unsigned integer means that the image is encoded using 256 different\nshades of gray, at most. We can check the distribution of these values.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n\nfig, ax = (ncols=2, figsize=(12, 4))\n\nax[0].imshow(raccoon_face, cmap=plt.cm.gray)\nax[0].axis(\"off\")\nax[0].set_title(\"Rendering of the image\")\nax[1].hist(raccoon_face.ravel(), bins=256)\nax[1].set_xlabel(\"Pixel value\")\nax[1].set_ylabel(\"Count of pixels\")\nax[1].set_title(\"Distribution of the pixel values\")\n_ = fig.suptitle(\"Original image of a raccoon face\")\n\n\n<img alt=\"Original image of a raccoon face, Rendering of the image, Distribution of the pixel values\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_face_compress_001.png\" srcset=\"../../_images/sphx_glr_plot_face_compress_001.png\"/>",
            "code"
        ]
    ],
    "Examples->Clustering->Vector Quantization Example->Compression via vector quantization": [
        [
            "The idea behind compression via vector quantization is to reduce the number of\ngray levels to represent an image. For instance, we can use 8 values instead\nof 256 values. Therefore, it means that we could efficiently use 3 bits instead\nof 8 bits to encode a single pixel and therefore reduce the memory usage by a\nfactor of approximately 2.5. We will later discuss about this memory usage.",
            "markdown"
        ]
    ],
    "Examples->Clustering->Vector Quantization Example->Compression via vector quantization->Encoding strategy": [
        [
            "The compression can be done using a\n. We need to choose a strategy\nto define the 8 gray values to sub-sample. The simplest strategy is to define\nthem equally spaced, which correspond to setting strategy=\"uniform\". From\nthe previous histogram, we know that this strategy is certainly not optimal.",
            "markdown"
        ],
        [
            "from sklearn.preprocessing import \n\nn_bins = 8\nencoder = (\n    n_bins=n_bins, encode=\"ordinal\", strategy=\"uniform\", random_state=0\n)\ncompressed_raccoon_uniform = encoder.fit_transform(raccoon_face.reshape(-1, 1)).reshape(\n    raccoon_face.shape\n)\n\nfig, ax = (ncols=2, figsize=(12, 4))\nax[0].imshow(compressed_raccoon_uniform, cmap=plt.cm.gray)\nax[0].axis(\"off\")\nax[0].set_title(\"Rendering of the image\")\nax[1].hist(compressed_raccoon_uniform.ravel(), bins=256)\nax[1].set_xlabel(\"Pixel value\")\nax[1].set_ylabel(\"Count of pixels\")\nax[1].set_title(\"Sub-sampled distribution of the pixel values\")\n_ = fig.suptitle(\"Raccoon face compressed using 3 bits and a uniform strategy\")\n\n\n<img alt=\"Raccoon face compressed using 3 bits and a uniform strategy, Rendering of the image, Sub-sampled distribution of the pixel values\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_face_compress_002.png\" srcset=\"../../_images/sphx_glr_plot_face_compress_002.png\"/>",
            "code"
        ],
        [
            "Qualitatively, we can spot some small regions where we see the effect of the\ncompression (e.g. leaves on the bottom right corner). But after all, the resulting\nimage is still looking good.",
            "markdown"
        ],
        [
            "We observe that the distribution of pixels values have been mapped to 8\ndifferent values. We can check the correspondance between such values and the\noriginal pixel values.",
            "markdown"
        ],
        [
            "bin_edges = encoder.bin_edges_[0]\nbin_center = bin_edges[:-1] + (bin_edges[1:] - bin_edges[:-1]) / 2\nbin_center",
            "code"
        ],
        [
            "array([ 15.625,  46.875,  78.125, 109.375, 140.625, 171.875, 203.125,\n       234.375])",
            "code"
        ],
        [
            "_, ax = ()\nax.hist(raccoon_face.ravel(), bins=256)\ncolor = \"tab:orange\"\nfor center in bin_center:\n    ax.axvline(center, color=color)\n    ax.text(center - 10, ax.get_ybound()[1] + 100, f\"{center:.1f}\", color=color)\n\n\n<img alt=\"plot face compress\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_face_compress_003.png\" srcset=\"../../_images/sphx_glr_plot_face_compress_003.png\"/>",
            "code"
        ],
        [
            "As previously stated, the uniform sampling strategy is not optimal. Notice for\ninstance that the pixels mapped to the value 7 will encode a rather small\namount of information, whereas the mapped value 3 will represent a large\namount of counts. We can instead use a clustering strategy such as k-means to\nfind a more optimal mapping.",
            "markdown"
        ],
        [
            "encoder = (\n    n_bins=n_bins, encode=\"ordinal\", strategy=\"kmeans\", random_state=0\n)\ncompressed_raccoon_kmeans = encoder.fit_transform(raccoon_face.reshape(-1, 1)).reshape(\n    raccoon_face.shape\n)\n\nfig, ax = (ncols=2, figsize=(12, 4))\nax[0].imshow(compressed_raccoon_kmeans, cmap=plt.cm.gray)\nax[0].axis(\"off\")\nax[0].set_title(\"Rendering of the image\")\nax[1].hist(compressed_raccoon_kmeans.ravel(), bins=256)\nax[1].set_xlabel(\"Pixel value\")\nax[1].set_ylabel(\"Number of pixels\")\nax[1].set_title(\"Distribution of the pixel values\")\n_ = fig.suptitle(\"Raccoon face compressed using 3 bits and a K-means strategy\")\n\n\n<img alt=\"Raccoon face compressed using 3 bits and a K-means strategy, Rendering of the image, Distribution of the pixel values\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_face_compress_004.png\" srcset=\"../../_images/sphx_glr_plot_face_compress_004.png\"/>",
            "code"
        ],
        [
            "bin_edges = encoder.bin_edges_[0]\nbin_center = bin_edges[:-1] + (bin_edges[1:] - bin_edges[:-1]) / 2\nbin_center",
            "code"
        ],
        [
            "array([ 18.9141241 ,  53.3627656 ,  82.65000752, 109.26055499,\n       134.68738405, 159.79431128, 185.18557327, 224.01945707])",
            "code"
        ],
        [
            "_, ax = ()\nax.hist(raccoon_face.ravel(), bins=256)\ncolor = \"tab:orange\"\nfor center in bin_center:\n    ax.axvline(center, color=color)\n    ax.text(center - 10, ax.get_ybound()[1] + 100, f\"{center:.1f}\", color=color)\n\n\n<img alt=\"plot face compress\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_face_compress_005.png\" srcset=\"../../_images/sphx_glr_plot_face_compress_005.png\"/>",
            "code"
        ],
        [
            "The counts in the bins are now more balanced and their centers are no longer\nequally spaced. Note that we could enforce the same number of pixels per bin\nby using the strategy=\"quantile\" instead of strategy=\"kmeans\".",
            "markdown"
        ]
    ],
    "Examples->Clustering->Vector Quantization Example->Compression via vector quantization->Memory footprint": [
        [
            "We previously stated that we should save 8 times less memory. Let\u2019s verify it.",
            "markdown"
        ],
        [
            "print(f\"The number of bytes taken in RAM is {compressed_raccoon_kmeans.nbytes}\")\nprint(f\"Compression ratio: {compressed_raccoon_kmeans.nbytes / raccoon_face.nbytes}\")",
            "code"
        ],
        [
            "The number of bytes taken in RAM is 6291456\nCompression ratio: 8.0",
            "code"
        ],
        [
            "It is quite surprising to see that our compressed image is taking x8 more\nmemory than the original image. This is indeed the opposite of what we\nexpected. The reason is mainly due to the type of data used to encode the\nimage.",
            "markdown"
        ],
        [
            "print(f\"Type of the compressed image: {compressed_raccoon_kmeans.dtype}\")",
            "code"
        ],
        [
            "Type of the compressed image: float64",
            "code"
        ],
        [
            "Indeed, the output of the  is\nan array of 64-bit float. It means that it takes x8 more memory. However, we\nuse this 64-bit float representation to encode 8 values. Indeed, we will save\nmemory only if we cast the compressed image into an array of 3-bits integers. We\ncould use the method numpy.ndarray.astype. However, a 3-bits integer\nrepresentation does not exist and to encode the 8 values, we would need to use\nthe 8-bit unsigned integer representation as well.",
            "markdown"
        ],
        [
            "In practice, observing a memory gain would require the original image to be in\na 64-bit float representation.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  3.002 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Covariance estimation->Ledoit-Wolf vs OAS estimation": [
        [
            "The usual covariance maximum likelihood estimate can be regularized\nusing shrinkage. Ledoit and Wolf proposed a close formula to compute\nthe asymptotically optimal shrinkage parameter (minimizing a MSE\ncriterion), yielding the Ledoit-Wolf covariance estimate.",
            "markdown"
        ],
        [
            "Chen et al. proposed an improvement of the Ledoit-Wolf shrinkage\nparameter, the OAS coefficient, whose convergence is significantly\nbetter under the assumption that the data are Gaussian.",
            "markdown"
        ],
        [
            "This example, inspired from Chen\u2019s publication [1], shows a comparison\nof the estimated MSE of the LW and OAS methods, using Gaussian\ndistributed data.",
            "markdown"
        ],
        [
            "[1] \u201cShrinkage Algorithms for MMSE Covariance Estimation\u201d\nChen et al., IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010.",
            "markdown"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.linalg import , \n\nfrom sklearn.covariance import , \n\n(0)",
            "code"
        ],
        [
            "n_features = 100\n# simulation covariance matrix (AR(1) process)\nr = 0.1\nreal_cov = (r ** (n_features))\ncoloring_matrix = (real_cov)\n\nn_samples_range = (6, 31, 1)\nrepeat = 100\nlw_mse = ((n_samples_range.size, repeat))\noa_mse = ((n_samples_range.size, repeat))\nlw_shrinkage = ((n_samples_range.size, repeat))\noa_shrinkage = ((n_samples_range.size, repeat))\nfor i, n_samples in enumerate(n_samples_range):\n    for j in range(repeat):\n        X = ((size=(n_samples, n_features)), coloring_matrix.T)\n\n        lw = (store_precision=False, assume_centered=True)\n        lw.fit(X)\n        lw_mse[i, j] = lw.error_norm(real_cov, scaling=False)\n        lw_shrinkage[i, j] = lw.shrinkage_\n\n        oa = (store_precision=False, assume_centered=True)\n        oa.fit(X)\n        oa_mse[i, j] = oa.error_norm(real_cov, scaling=False)\n        oa_shrinkage[i, j] = oa.shrinkage_\n\n# plot MSE\n(2, 1, 1)\n(\n    n_samples_range,\n    lw_mse.mean(1),\n    yerr=lw_mse.std(1),\n    label=\"Ledoit-Wolf\",\n    color=\"navy\",\n    lw=2,\n)\n(\n    n_samples_range,\n    oa_mse.mean(1),\n    yerr=oa_mse.std(1),\n    label=\"OAS\",\n    color=\"darkorange\",\n    lw=2,\n)\n(\"Squared error\")\n(loc=\"upper right\")\n(\"Comparison of covariance estimators\")\n(5, 31)\n\n# plot shrinkage coefficient\n(2, 1, 2)\n(\n    n_samples_range,\n    lw_shrinkage.mean(1),\n    yerr=lw_shrinkage.std(1),\n    label=\"Ledoit-Wolf\",\n    color=\"navy\",\n    lw=2,\n)\n(\n    n_samples_range,\n    oa_shrinkage.mean(1),\n    yerr=oa_shrinkage.std(1),\n    label=\"OAS\",\n    color=\"darkorange\",\n    lw=2,\n)\n(\"n_samples\")\n(\"Shrinkage\")\n(loc=\"lower right\")\n(()[0], 1.0 + (()[1] - ()[0]) / 10.0)\n(5, 31)\n\n()\n\n\n<img alt=\"Comparison of covariance estimators\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_lw_vs_oas_001.png\" srcset=\"../../_images/sphx_glr_plot_lw_vs_oas_001.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  2.684 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Covariance estimation->Robust covariance estimation and Mahalanobis distances relevance": [
        [
            "This example shows covariance estimation with Mahalanobis\ndistances on Gaussian distributed data.",
            "markdown"
        ],
        [
            "For Gaussian distributed data, the distance of an observation\n\\(x_i\\) to the mode of the distribution can be computed using its\nMahalanobis distance:\n\n\\[d_{(\\mu,\\Sigma)}(x_i)^2 = (x_i - \\mu)^T\\Sigma^{-1}(x_i - \\mu)\\]",
            "markdown"
        ],
        [
            "where \\(\\mu\\) and \\(\\Sigma\\) are the location and the covariance of\nthe underlying Gaussian distributions.",
            "markdown"
        ],
        [
            "In practice, \\(\\mu\\) and \\(\\Sigma\\) are replaced by some\nestimates. The standard covariance maximum likelihood estimate (MLE) is very\nsensitive to the presence of outliers in the data set and therefore,\nthe downstream Mahalanobis distances also are. It would be better to\nuse a robust estimator of covariance to guarantee that the estimation is\nresistant to \u201cerroneous\u201d observations in the dataset and that the\ncalculated Mahalanobis distances accurately reflect the true\norganization of the observations.",
            "markdown"
        ],
        [
            "The Minimum Covariance Determinant estimator (MCD) is a robust,\nhigh-breakdown point (i.e. it can be used to estimate the covariance\nmatrix of highly contaminated datasets, up to\n\\(\\frac{n_\\text{samples}-n_\\text{features}-1}{2}\\) outliers)\nestimator of covariance. The idea behind the MCD is to find\n\\(\\frac{n_\\text{samples}+n_\\text{features}+1}{2}\\)\nobservations whose empirical covariance has the smallest determinant,\nyielding a \u201cpure\u201d subset of observations from which to compute\nstandards estimates of location and covariance. The MCD was introduced by\nP.J.Rousseuw in .",
            "markdown"
        ],
        [
            "This example illustrates how the Mahalanobis distances are affected by\noutlying data. Observations drawn from a contaminating distribution\nare not distinguishable from the observations coming from the real,\nGaussian distribution when using standard covariance MLE based Mahalanobis\ndistances. Using MCD-based\nMahalanobis distances, the two populations become\ndistinguishable. Associated applications include outlier detection,\nobservation ranking and clustering.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "See also",
            "markdown"
        ],
        [
            "References:\n\n\n[]",
            "markdown"
        ],
        [
            "P. J. Rousseeuw. . J. Am\nStat Ass, 79:871, 1984.\n\n\n[]",
            "markdown"
        ],
        [
            "Wilson, E. B., & Hilferty, M. M. (1931). \nProceedings of the National Academy of Sciences of the United States\nof America, 17, 684-688.",
            "markdown"
        ]
    ],
    "Examples->Covariance estimation->Robust covariance estimation and Mahalanobis distances relevance->Generate data": [
        [
            "First, we generate a dataset of 125 samples and 2 features. Both features\nare Gaussian distributed with mean of 0 but feature 1 has a standard\ndeviation equal to 2 and feature 2 has a standard deviation equal to 1. Next,\n25 samples are replaced with Gaussian outlier samples where feature 1 has\na standard deviation equal to 1 and feature 2 has a standard deviation equal\nto 7.",
            "markdown"
        ],
        [
            "import numpy as np\n\n# for consistent results\n(7)\n\nn_samples = 125\nn_outliers = 25\nn_features = 2\n\n# generate Gaussian data of shape (125, 2)\ngen_cov = (n_features)\ngen_cov[0, 0] = 2.0\nX = ((n_samples, n_features), gen_cov)\n# add some outliers\noutliers_cov = (n_features)\noutliers_cov[(1, n_features), (1, n_features)] = 7.0\nX[-n_outliers:] = ((n_outliers, n_features), outliers_cov)",
            "code"
        ]
    ],
    "Examples->Covariance estimation->Robust covariance estimation and Mahalanobis distances relevance->Comparison of results": [
        [
            "Below, we fit MCD and MLE based covariance estimators to our data and print\nthe estimated covariance matrices. Note that the estimated variance of\nfeature 2 is much higher with the MLE based estimator (7.5) than\nthat of the MCD robust estimator (1.2). This shows that the MCD based\nrobust estimator is much more resistant to the outlier samples, which were\ndesigned to have a much larger variance in feature 2.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\nfrom sklearn.covariance import , \n\n# fit a MCD robust estimator to data\nrobust_cov = ().fit(X)\n# fit a MLE estimator to data\nemp_cov = ().fit(X)\nprint(\n    \"Estimated covariance matrix:\\nMCD (Robust):\\n{}\\nMLE:\\n{}\".format(\n        robust_cov.covariance_, emp_cov.covariance_\n    )\n)",
            "code"
        ],
        [
            "Estimated covariance matrix:\nMCD (Robust):\n[[ 3.26253567e+00 -3.06695631e-03]\n [-3.06695631e-03  1.22747343e+00]]\nMLE:\n[[ 3.23773583 -0.24640578]\n [-0.24640578  7.51963999]]",
            "code"
        ],
        [
            "To better visualize the difference, we plot contours of the\nMahalanobis distances calculated by both methods. Notice that the robust\nMCD based Mahalanobis distances fit the inlier black points much better,\nwhereas the MLE based distances are more influenced by the outlier\nred points.",
            "markdown"
        ],
        [
            "fig, ax = (figsize=(10, 5))\n# Plot data set\ninlier_plot = ax.scatter(X[:, 0], X[:, 1], color=\"black\", label=\"inliers\")\noutlier_plot = ax.scatter(\n    X[:, 0][-n_outliers:], X[:, 1][-n_outliers:], color=\"red\", label=\"outliers\"\n)\nax.set_xlim(ax.get_xlim()[0], 10.0)\nax.set_title(\"Mahalanobis distances of a contaminated data set\")\n\n# Create meshgrid of feature 1 and feature 2 values\nxx, yy = (\n    (()[0], ()[1], 100),\n    (()[0], ()[1], 100),\n)\nzz = [xx.ravel(), yy.ravel()]\n# Calculate the MLE based Mahalanobis distances of the meshgrid\nmahal_emp_cov = emp_cov.mahalanobis(zz)\nmahal_emp_cov = mahal_emp_cov.reshape(xx.shape)\nemp_cov_contour = (\n    xx, yy, (mahal_emp_cov), cmap=plt.cm.PuBu_r, linestyles=\"dashed\"\n)\n# Calculate the MCD based Mahalanobis distances\nmahal_robust_cov = robust_cov.mahalanobis(zz)\nmahal_robust_cov = mahal_robust_cov.reshape(xx.shape)\nrobust_contour = ax.contour(\n    xx, yy, (mahal_robust_cov), cmap=plt.cm.YlOrBr_r, linestyles=\"dotted\"\n)\n\n# Add legend\nax.legend(\n    [\n        emp_cov_contour.collections[1],\n        robust_contour.collections[1],\n        inlier_plot,\n        outlier_plot,\n    ],\n    [\"MLE dist\", \"MCD dist\", \"inliers\", \"outliers\"],\n    loc=\"upper right\",\n    borderaxespad=0,\n)\n\n()\n\n\n<img alt=\"Mahalanobis distances of a contaminated data set\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_mahalanobis_distances_001.png\" srcset=\"../../_images/sphx_glr_plot_mahalanobis_distances_001.png\"/>",
            "code"
        ],
        [
            "Finally, we highlight the ability of MCD based Mahalanobis distances to\ndistinguish outliers. We take the cubic root of the Mahalanobis distances,\nyielding approximately normal distributions (as suggested by Wilson and\nHilferty ), then plot the values of inlier and outlier samples with\nboxplots. The distribution of outlier samples is more separated from the\ndistribution of inlier samples for robust MCD based Mahalanobis distances.",
            "markdown"
        ],
        [
            "fig, (ax1, ax2) = (1, 2)\n(wspace=0.6)\n\n# Calculate cubic root of MLE Mahalanobis distances for samples\nemp_mahal = emp_cov.mahalanobis(X - (X, 0)) ** (0.33)\n# Plot boxplots\nax1.boxplot([emp_mahal[:-n_outliers], emp_mahal[-n_outliers:]], widths=0.25)\n# Plot individual samples\nax1.plot(\n    (n_samples - n_outliers, 1.26),\n    emp_mahal[:-n_outliers],\n    \"+k\",\n    markeredgewidth=1,\n)\nax1.plot((n_outliers, 2.26), emp_mahal[-n_outliers:], \"+k\", markeredgewidth=1)\nax1.axes.set_xticklabels((\"inliers\", \"outliers\"), size=15)\nax1.set_ylabel(r\"$\\sqrt[3]{\\rm{(Mahal. dist.)}}$\", size=16)\nax1.set_title(\"Using non-robust estimates\\n(Maximum Likelihood)\")\n\n# Calculate cubic root of MCD Mahalanobis distances for samples\nrobust_mahal = robust_cov.mahalanobis(X - robust_cov.location_) ** (0.33)\n# Plot boxplots\nax2.boxplot([robust_mahal[:-n_outliers], robust_mahal[-n_outliers:]], widths=0.25)\n# Plot individual samples\nax2.plot(\n    (n_samples - n_outliers, 1.26),\n    robust_mahal[:-n_outliers],\n    \"+k\",\n    markeredgewidth=1,\n)\nax2.plot((n_outliers, 2.26), robust_mahal[-n_outliers:], \"+k\", markeredgewidth=1)\nax2.axes.set_xticklabels((\"inliers\", \"outliers\"), size=15)\nax2.set_ylabel(r\"$\\sqrt[3]{\\rm{(Mahal. dist.)}}$\", size=16)\nax2.set_title(\"Using robust estimates\\n(Minimum Covariance Determinant)\")\n\n()\n\n\n<img alt=\"Using non-robust estimates (Maximum Likelihood), Using robust estimates (Minimum Covariance Determinant)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_mahalanobis_distances_002.png\" srcset=\"../../_images/sphx_glr_plot_mahalanobis_distances_002.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.319 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Covariance estimation->Robust vs Empirical covariance estimate": [
        [
            "The usual covariance maximum likelihood estimate is very sensitive to the\npresence of outliers in the data set. In such a case, it would be better to\nuse a robust estimator of covariance to guarantee that the estimation is\nresistant to \u201cerroneous\u201d observations in the data set. ,",
            "markdown"
        ]
    ],
    "Examples->Covariance estimation->Robust vs Empirical covariance estimate->Minimum Covariance Determinant Estimator": [
        [
            "The Minimum Covariance Determinant estimator is a robust, high-breakdown point\n(i.e. it can be used to estimate the covariance matrix of highly contaminated\ndatasets, up to\n\\(\\frac{n_\\text{samples} - n_\\text{features}-1}{2}\\) outliers) estimator of\ncovariance. The idea is to find\n\\(\\frac{n_\\text{samples} + n_\\text{features}+1}{2}\\)\nobservations whose empirical covariance has the smallest determinant, yielding\na \u201cpure\u201d subset of observations from which to compute standards estimates of\nlocation and covariance. After a correction step aiming at compensating the\nfact that the estimates were learned from only a portion of the initial data,\nwe end up with robust estimates of the data set location and covariance.",
            "markdown"
        ],
        [
            "The Minimum Covariance Determinant estimator (MCD) has been introduced by\nP.J.Rousseuw in .",
            "markdown"
        ]
    ],
    "Examples->Covariance estimation->Robust vs Empirical covariance estimate->Evaluation": [
        [
            "In this example, we compare the estimation errors that are made when using\nvarious types of location and covariance estimates on contaminated Gaussian\ndistributed data sets:",
            "markdown"
        ],
        [
            "The mean and the empirical covariance of the full dataset, which break\ndown as soon as there are outliers in the data set",
            "markdown"
        ],
        [
            "The robust MCD, that has a low error provided\n\\(n_\\text{samples} > 5n_\\text{features}\\)",
            "markdown"
        ],
        [
            "The mean and the empirical covariance of the observations that are known\nto be good ones. This can be considered as a \u201cperfect\u201d MCD estimation,\nso one can trust our implementation by comparing to this case.",
            "markdown"
        ]
    ],
    "Examples->Covariance estimation->Robust vs Empirical covariance estimate->References": [
        [
            "Johanna Hardin, David M Rocke. The distribution of robust distances.\nJournal of Computational and Graphical Statistics. December 1, 2005,\n14(4): 928-946.\n\n\n[]",
            "markdown"
        ],
        [
            "Zoubir A., Koivunen V., Chakhchoukh Y. and Muma M. (2012). Robust\nestimation in signal processing: A tutorial-style treatment of\nfundamental concepts. IEEE Signal Processing Magazine 29(4), 61-80.\n\n\n[]",
            "markdown"
        ],
        [
            "P. J. Rousseeuw. Least median of squares regression. Journal of American\nStatistical Ass., 79:871, 1984.\n\n\n<img alt=\"Influence of outliers on the location estimation, Influence of outliers on the covariance estimation\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_robust_vs_empirical_covariance_001.png\" srcset=\"../../_images/sphx_glr_plot_robust_vs_empirical_covariance_001.png\"/>",
            "markdown"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager\n\nfrom sklearn.covariance import , \n\n# example settings\nn_samples = 80\nn_features = 5\nrepeat = 10\n\nrange_n_outliers = (\n    (\n        (0, n_samples / 8, 5),\n        (n_samples / 8, n_samples / 2, 5)[1:-1],\n    )\n).astype(int)\n\n# definition of arrays to store results\nerr_loc_mcd = ((range_n_outliers.size, repeat))\nerr_cov_mcd = ((range_n_outliers.size, repeat))\nerr_loc_emp_full = ((range_n_outliers.size, repeat))\nerr_cov_emp_full = ((range_n_outliers.size, repeat))\nerr_loc_emp_pure = ((range_n_outliers.size, repeat))\nerr_cov_emp_pure = ((range_n_outliers.size, repeat))\n\n# computation\nfor i, n_outliers in enumerate(range_n_outliers):\n    for j in range(repeat):\n\n        rng = (i * j)\n\n        # generate data\n        X = rng.randn(n_samples, n_features)\n        # add some outliers\n        outliers_index = rng.permutation(n_samples)[:n_outliers]\n        outliers_offset = 10.0 * (\n            (2, size=(n_outliers, n_features)) - 0.5\n        )\n        X[outliers_index] += outliers_offset\n        inliers_mask = (n_samples).astype(bool)\n        inliers_mask[outliers_index] = False\n\n        # fit a Minimum Covariance Determinant (MCD) robust estimator to data\n        mcd = ().fit(X)\n        # compare raw robust estimates with the true location and covariance\n        err_loc_mcd[i, j] = (mcd.location_**2)\n        err_cov_mcd[i, j] = mcd.error_norm((n_features))\n\n        # compare estimators learned from the full data set with true\n        # parameters\n        err_loc_emp_full[i, j] = (X.mean(0) ** 2)\n        err_cov_emp_full[i, j] = (\n            ().fit(X).error_norm((n_features))\n        )\n\n        # compare with an empirical covariance learned from a pure data set\n        # (i.e. \"perfect\" mcd)\n        pure_X = X[inliers_mask]\n        pure_location = pure_X.mean(0)\n        pure_emp_cov = ().fit(pure_X)\n        err_loc_emp_pure[i, j] = (pure_location**2)\n        err_cov_emp_pure[i, j] = pure_emp_cov.error_norm((n_features))\n\n# Display results\nfont_prop = (size=11)\n(2, 1, 1)\nlw = 2\n(\n    range_n_outliers,\n    err_loc_mcd.mean(1),\n    yerr=err_loc_mcd.std(1) / (repeat),\n    label=\"Robust location\",\n    lw=lw,\n    color=\"m\",\n)\n(\n    range_n_outliers,\n    err_loc_emp_full.mean(1),\n    yerr=err_loc_emp_full.std(1) / (repeat),\n    label=\"Full data set mean\",\n    lw=lw,\n    color=\"green\",\n)\n(\n    range_n_outliers,\n    err_loc_emp_pure.mean(1),\n    yerr=err_loc_emp_pure.std(1) / (repeat),\n    label=\"Pure data set mean\",\n    lw=lw,\n    color=\"black\",\n)\n(\"Influence of outliers on the location estimation\")\n(r\"Error ($||\\mu - \\hat{\\mu}||_2^2$)\")\n(loc=\"upper left\", prop=font_prop)\n\n(2, 1, 2)\nx_size = range_n_outliers.size\n(\n    range_n_outliers,\n    err_cov_mcd.mean(1),\n    yerr=err_cov_mcd.std(1),\n    label=\"Robust covariance (mcd)\",\n    color=\"m\",\n)\n(\n    range_n_outliers[: (x_size // 5 + 1)],\n    err_cov_emp_full.mean(1)[: (x_size // 5 + 1)],\n    yerr=err_cov_emp_full.std(1)[: (x_size // 5 + 1)],\n    label=\"Full data set empirical covariance\",\n    color=\"green\",\n)\n(\n    range_n_outliers[(x_size // 5) : (x_size // 2 - 1)],\n    err_cov_emp_full.mean(1)[(x_size // 5) : (x_size // 2 - 1)],\n    color=\"green\",\n    ls=\"--\",\n)\n(\n    range_n_outliers,\n    err_cov_emp_pure.mean(1),\n    yerr=err_cov_emp_pure.std(1),\n    label=\"Pure data set empirical covariance\",\n    color=\"black\",\n)\n(\"Influence of outliers on the covariance estimation\")\n(\"Amount of contamination (%)\")\n(\"RMSE\")\n(loc=\"upper center\", prop=font_prop)\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  2.931 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Covariance estimation->Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood": [
        [
            "When working with covariance estimation, the usual approach is to use\na maximum likelihood estimator, such as the\n. It is unbiased, i.e. it\nconverges to the true (population) covariance when given many\nobservations. However, it can also be beneficial to regularize it, in\norder to reduce its variance; this, in turn, introduces some bias. This\nexample illustrates the simple regularization used in\n estimators. In particular, it focuses on how to\nset the amount of regularization, i.e. how to choose the bias-variance\ntrade-off.",
            "markdown"
        ]
    ],
    "Examples->Covariance estimation->Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood->Generate sample data": [
        [
            "import numpy as np\n\nn_features, n_samples = 40, 20\n(42)\nbase_X_train = (size=(n_samples, n_features))\nbase_X_test = (size=(n_samples, n_features))\n\n# Color samples\ncoloring_matrix = (size=(n_features, n_features))\nX_train = (base_X_train, coloring_matrix)\nX_test = (base_X_test, coloring_matrix)",
            "code"
        ]
    ],
    "Examples->Covariance estimation->Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood->Compute the likelihood on test data": [
        [
            "from sklearn.covariance import , , log_likelihood\nfrom scipy import linalg\n\n# spanning a range of possible shrinkage coefficient values\nshrinkages = (-2, 0, 30)\nnegative_logliks = [\n    -(shrinkage=s).fit(X_train).score(X_test) for s in shrinkages\n]\n\n# under the ground-truth model, which we would not have access to in real\n# settings\nreal_cov = (coloring_matrix.T, coloring_matrix)\nemp_cov = (X_train)\nloglik_real = -log_likelihood(emp_cov, (real_cov))",
            "code"
        ]
    ],
    "Examples->Covariance estimation->Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood->Compare different approaches to setting the regularization parameter": [
        [
            "Here we compare 3 approaches:",
            "markdown"
        ],
        [
            "Setting the parameter by cross-validating the likelihood on three folds\naccording to a grid of potential shrinkage parameters.",
            "markdown"
        ],
        [
            "A close formula proposed by Ledoit and Wolf to compute\nthe asymptotically optimal regularization parameter (minimizing a MSE\ncriterion), yielding the \ncovariance estimate.",
            "markdown"
        ],
        [
            "An improvement of the Ledoit-Wolf shrinkage, the\n, proposed by Chen et al. Its\nconvergence is significantly better under the assumption that the data\nare Gaussian, in particular for small samples.",
            "markdown"
        ],
        [
            "from sklearn.model_selection import \nfrom sklearn.covariance import , \n\n# GridSearch for an optimal shrinkage coefficient\ntuned_parameters = [{\"shrinkage\": shrinkages}]\ncv = ((), tuned_parameters)\ncv.fit(X_train)\n\n# Ledoit-Wolf optimal shrinkage coefficient estimate\nlw = ()\nloglik_lw = lw.fit(X_train).score(X_test)\n\n# OAS coefficient estimate\noa = ()\nloglik_oa = oa.fit(X_train).score(X_test)",
            "code"
        ]
    ],
    "Examples->Covariance estimation->Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood->Plot results": [
        [
            "To quantify estimation error, we plot the likelihood of unseen data for\ndifferent values of the shrinkage parameter. We also show the choices by\ncross-validation, or with the LedoitWolf and OAS estimates.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n\nfig = ()\n(\"Regularized covariance: likelihood and shrinkage coefficient\")\n(\"Regularization parameter: shrinkage coefficient\")\n(\"Error: negative log-likelihood on test data\")\n# range shrinkage curve\n(shrinkages, negative_logliks, label=\"Negative log-likelihood\")\n\n((), 2 * [loglik_real], \"--r\", label=\"Real covariance likelihood\")\n\n# adjust view\nlik_max = (negative_logliks)\nlik_min = (negative_logliks)\nymin = lik_min - 6.0 * ((()[1] - ()[0]))\nymax = lik_max + 10.0 * (lik_max - lik_min)\nxmin = shrinkages[0]\nxmax = shrinkages[-1]\n# LW likelihood\n(\n    lw.shrinkage_,\n    ymin,\n    -loglik_lw,\n    color=\"magenta\",\n    linewidth=3,\n    label=\"Ledoit-Wolf estimate\",\n)\n# OAS likelihood\n(\n    oa.shrinkage_, ymin, -loglik_oa, color=\"purple\", linewidth=3, label=\"OAS estimate\"\n)\n# best CV estimator likelihood\n(\n    cv.best_estimator_.shrinkage,\n    ymin,\n    -cv.best_estimator_.score(X_test),\n    color=\"cyan\",\n    linewidth=3,\n    label=\"Cross-validation best estimate\",\n)\n\n(ymin, ymax)\n(xmin, xmax)\n()\n\n()\n\n\n<img alt=\"Regularized covariance: likelihood and shrinkage coefficient\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_covariance_estimation_001.png\" srcset=\"../../_images/sphx_glr_plot_covariance_estimation_001.png\"/>",
            "code"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "The maximum likelihood estimate corresponds to no shrinkage,\nand thus performs poorly. The Ledoit-Wolf estimate performs really well,\nas it is close to the optimal and is not computationally costly. In this\nexample, the OAS estimate is a bit further away. Interestingly, both\napproaches outperform cross-validation, which is significantly most\ncomputationally costly.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.495 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Covariance estimation->Sparse inverse covariance estimation": [
        [
            "Using the GraphicalLasso estimator to learn a covariance and sparse precision\nfrom a small number of samples.",
            "markdown"
        ],
        [
            "To estimate a probabilistic model (e.g. a Gaussian model), estimating the\nprecision matrix, that is the inverse covariance matrix, is as important\nas estimating the covariance matrix. Indeed a Gaussian model is\nparametrized by the precision matrix.",
            "markdown"
        ],
        [
            "To be in favorable recovery conditions, we sample the data from a model\nwith a sparse inverse covariance matrix. In addition, we ensure that the\ndata is not too much correlated (limiting the largest coefficient of the\nprecision matrix) and that there a no small coefficients in the\nprecision matrix that cannot be recovered. In addition, with a small\nnumber of observations, it is easier to recover a correlation matrix\nrather than a covariance, thus we scale the time series.",
            "markdown"
        ],
        [
            "Here, the number of samples is slightly larger than the number of\ndimensions, thus the empirical covariance is still invertible. However,\nas the observations are strongly correlated, the empirical covariance\nmatrix is ill-conditioned and as a result its inverse \u2013the empirical\nprecision matrix\u2013 is very far from the ground truth.",
            "markdown"
        ],
        [
            "If we use l2 shrinkage, as with the Ledoit-Wolf estimator, as the number\nof samples is small, we need to shrink a lot. As a result, the\nLedoit-Wolf precision is fairly close to the ground truth precision, that\nis not far from being diagonal, but the off-diagonal structure is lost.",
            "markdown"
        ],
        [
            "The l1-penalized estimator can recover part of this off-diagonal\nstructure. It learns a sparse precision. It is not able to\nrecover the exact sparsity pattern: it detects too many non-zero\ncoefficients. However, the highest non-zero coefficients of the l1\nestimated correspond to the non-zero coefficients in the ground truth.\nFinally, the coefficients of the l1 precision estimate are biased toward\nzero: because of the penalty, they are all smaller than the corresponding\nground truth value, as can be seen on the figure.",
            "markdown"
        ],
        [
            "Note that, the color range of the precision matrices is tweaked to\nimprove readability of the figure. The full range of values of the\nempirical precision is not displayed.",
            "markdown"
        ],
        [
            "The alpha parameter of the GraphicalLasso setting the sparsity of the model is\nset by internal cross-validation in the GraphicalLassoCV. As can be\nseen on figure 2, the grid to compute the cross-validation score is\niteratively refined in the neighborhood of the maximum.",
            "markdown"
        ],
        [
            "# author: Gael Varoquaux &lt;gael.varoquaux@inria.fr\n# License: BSD 3 clause\n# Copyright: INRIA",
            "code"
        ]
    ],
    "Examples->Covariance estimation->Sparse inverse covariance estimation->Generate the data": [
        [
            "import numpy as np\nfrom scipy import linalg\nfrom sklearn.datasets import \n\nn_samples = 60\nn_features = 20\n\nprng = (1)\nprec = (\n    n_features, alpha=0.98, smallest_coef=0.4, largest_coef=0.7, random_state=prng\n)\ncov = (prec)\nd = ((cov))\ncov /= d\ncov /= d[:, ]\nprec *= d\nprec *= d[:, ]\nX = prng.multivariate_normal((n_features), cov, size=n_samples)\nX -= X.mean(axis=0)\nX /= X.std(axis=0)",
            "code"
        ]
    ],
    "Examples->Covariance estimation->Sparse inverse covariance estimation->Estimate the covariance": [
        [
            "from sklearn.covariance import , \n\nemp_cov = (X.T, X) / n_samples\n\nmodel = ()\nmodel.fit(X)\ncov_ = model.covariance_\nprec_ = model.precision_\n\nlw_cov_, _ = (X)\nlw_prec_ = (lw_cov_)",
            "code"
        ]
    ],
    "Examples->Covariance estimation->Sparse inverse covariance estimation->Plot the results": [
        [
            "import matplotlib.pyplot as plt\n\n(figsize=(10, 6))\n(left=0.02, right=0.98)\n\n# plot the covariances\ncovs = [\n    (\"Empirical\", emp_cov),\n    (\"Ledoit-Wolf\", lw_cov_),\n    (\"GraphicalLassoCV\", cov_),\n    (\"True\", cov),\n]\nvmax = cov_.max()\nfor i, (name, this_cov) in enumerate(covs):\n    (2, 4, i + 1)\n    (\n        this_cov, interpolation=\"nearest\", vmin=-vmax, vmax=vmax, cmap=plt.cm.RdBu_r\n    )\n    (())\n    (())\n    (\"%s covariance\" % name)\n\n\n# plot the precisions\nprecs = [\n    (\"Empirical\", (emp_cov)),\n    (\"Ledoit-Wolf\", lw_prec_),\n    (\"GraphicalLasso\", prec_),\n    (\"True\", prec),\n]\nvmax = 0.9 * prec_.max()\nfor i, (name, this_prec) in enumerate(precs):\n    ax = (2, 4, i + 5)\n    (\n        (this_prec, 0),\n        interpolation=\"nearest\",\n        vmin=-vmax,\n        vmax=vmax,\n        cmap=plt.cm.RdBu_r,\n    )\n    (())\n    (())\n    (\"%s precision\" % name)\n    if hasattr(ax, \"set_facecolor\"):\n        ax.set_facecolor(\".7\")\n    else:\n        ax.set_axis_bgcolor(\".7\")\n\n\n<img alt=\"Empirical covariance, Ledoit-Wolf covariance, GraphicalLassoCV covariance, True covariance, Empirical precision, Ledoit-Wolf precision, GraphicalLasso precision, True precision\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_sparse_cov_001.png\" srcset=\"../../_images/sphx_glr_plot_sparse_cov_001.png\"/>",
            "code"
        ],
        [
            "# plot the model selection metric\n(figsize=(4, 3))\n([0.2, 0.15, 0.75, 0.7])\n(model.cv_results_[\"alphas\"], model.cv_results_[\"mean_test_score\"], \"o-\")\n(model.alpha_, color=\".5\")\n(\"Model selection\")\n(\"Cross-validation score\")\n(\"alpha\")\n\n()\n\n\n<img alt=\"Model selection\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_sparse_cov_002.png\" srcset=\"../../_images/sphx_glr_plot_sparse_cov_002.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.584 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Cross decomposition->Compare cross decomposition methods": [
        [
            "Simple usage of various cross decomposition algorithms:",
            "markdown"
        ],
        [
            "PLSCanonical",
            "markdown"
        ],
        [
            "PLSRegression, with multivariate response, a.k.a. PLS2",
            "markdown"
        ],
        [
            "PLSRegression, with univariate response, a.k.a. PLS1",
            "markdown"
        ],
        [
            "CCA",
            "markdown"
        ],
        [
            "Given 2 multivariate covarying two-dimensional datasets, X, and Y,\nPLS extracts the \u2018directions of covariance\u2019, i.e. the components of each\ndatasets that explain the most shared variance between both datasets.\nThis is apparent on the <strong>scatterplot matrix</strong> display: components 1 in\ndataset X and dataset Y are maximally correlated (points lie around the\nfirst diagonal). This is also true for components 2 in both dataset,\nhowever, the correlation across datasets for different components is\nweak: the point cloud is very spherical.",
            "markdown"
        ]
    ],
    "Examples->Cross decomposition->Compare cross decomposition methods->Dataset based latent variables model": [
        [
            "import numpy as np\n\nn = 500\n# 2 latents vars:\nl1 = (size=n)\nl2 = (size=n)\n\nlatents = ([l1, l1, l2, l2]).T\nX = latents + (size=4 * n).reshape((n, 4))\nY = latents + (size=4 * n).reshape((n, 4))\n\nX_train = X[: n // 2]\nY_train = Y[: n // 2]\nX_test = X[n // 2 :]\nY_test = Y[n // 2 :]\n\nprint(\"Corr(X)\")\nprint(np.round((X.T), 2))\nprint(\"Corr(Y)\")\nprint(np.round((Y.T), 2))",
            "code"
        ],
        [
            "Corr(X)\n[[ 1.    0.44 -0.06 -0.01]\n [ 0.44  1.   -0.01 -0.06]\n [-0.06 -0.01  1.    0.5 ]\n [-0.01 -0.06  0.5   1.  ]]\nCorr(Y)\n[[ 1.    0.47 -0.05  0.02]\n [ 0.47  1.   -0.01  0.03]\n [-0.05 -0.01  1.    0.47]\n [ 0.02  0.03  0.47  1.  ]]",
            "code"
        ]
    ],
    "Examples->Cross decomposition->Compare cross decomposition methods->Canonical (symmetric) PLS->Transform data": [
        [
            "from sklearn.cross_decomposition import \n\nplsca = (n_components=2)\nplsca.fit(X_train, Y_train)\nX_train_r, Y_train_r = plsca.transform(X_train, Y_train)\nX_test_r, Y_test_r = plsca.transform(X_test, Y_test)",
            "code"
        ]
    ],
    "Examples->Cross decomposition->Compare cross decomposition methods->Canonical (symmetric) PLS->Scatter plot of scores": [
        [
            "import matplotlib.pyplot as plt\n\n# On diagonal plot X vs Y scores on each components\n(figsize=(12, 8))\n(221)\n(X_train_r[:, 0], Y_train_r[:, 0], label=\"train\", marker=\"o\", s=25)\n(X_test_r[:, 0], Y_test_r[:, 0], label=\"test\", marker=\"o\", s=25)\n(\"x scores\")\n(\"y scores\")\n(\n    \"Comp. 1: X vs Y (test corr = %.2f)\"\n    % (X_test_r[:, 0], Y_test_r[:, 0])[0, 1]\n)\n(())\n(())\n(loc=\"best\")\n\n(224)\n(X_train_r[:, 1], Y_train_r[:, 1], label=\"train\", marker=\"o\", s=25)\n(X_test_r[:, 1], Y_test_r[:, 1], label=\"test\", marker=\"o\", s=25)\n(\"x scores\")\n(\"y scores\")\n(\n    \"Comp. 2: X vs Y (test corr = %.2f)\"\n    % (X_test_r[:, 1], Y_test_r[:, 1])[0, 1]\n)\n(())\n(())\n(loc=\"best\")\n\n# Off diagonal plot components 1 vs 2 for X and Y\n(222)\n(X_train_r[:, 0], X_train_r[:, 1], label=\"train\", marker=\"*\", s=50)\n(X_test_r[:, 0], X_test_r[:, 1], label=\"test\", marker=\"*\", s=50)\n(\"X comp. 1\")\n(\"X comp. 2\")\n(\n    \"X comp. 1 vs X comp. 2 (test corr = %.2f)\"\n    % (X_test_r[:, 0], X_test_r[:, 1])[0, 1]\n)\n(loc=\"best\")\n(())\n(())\n\n(223)\n(Y_train_r[:, 0], Y_train_r[:, 1], label=\"train\", marker=\"*\", s=50)\n(Y_test_r[:, 0], Y_test_r[:, 1], label=\"test\", marker=\"*\", s=50)\n(\"Y comp. 1\")\n(\"Y comp. 2\")\n(\n    \"Y comp. 1 vs Y comp. 2 , (test corr = %.2f)\"\n    % (Y_test_r[:, 0], Y_test_r[:, 1])[0, 1]\n)\n(loc=\"best\")\n(())\n(())\n()\n\n\n<img alt=\"Comp. 1: X vs Y (test corr = 0.60), Comp. 2: X vs Y (test corr = 0.67), X comp. 1 vs X comp. 2 (test corr = -0.17), Y comp. 1 vs Y comp. 2 , (test corr = -0.05)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_compare_cross_decomposition_001.png\" srcset=\"../../_images/sphx_glr_plot_compare_cross_decomposition_001.png\"/>",
            "code"
        ]
    ],
    "Examples->Cross decomposition->Compare cross decomposition methods->PLS regression, with multivariate response, a.k.a. PLS2": [
        [
            "from sklearn.cross_decomposition import \n\nn = 1000\nq = 3\np = 10\nX = (size=n * p).reshape((n, p))\nB = ([[1, 2] + [0] * (p - 2)] * q).T\n# each Yj = 1*X1 + 2*X2 + noize\nY = (X, B) + (size=n * q).reshape((n, q)) + 5\n\npls2 = (n_components=3)\npls2.fit(X, Y)\nprint(\"True B (such that: Y = XB + Err)\")\nprint(B)\n# compare pls2.coef_ with B\nprint(\"Estimated B\")\nprint(np.round(pls2.coef_, 1))\npls2.predict(X)",
            "code"
        ],
        [
            "True B (such that: Y = XB + Err)\n[[1 1 1]\n [2 2 2]\n [0 0 0]\n [0 0 0]\n [0 0 0]\n [0 0 0]\n [0 0 0]\n [0 0 0]\n [0 0 0]\n [0 0 0]]\nEstimated B\n/home/circleci/project/sklearn/cross_decomposition/_pls.py:503: FutureWarning:\n\nThe attribute `coef_` will be transposed in version 1.3 to be consistent with other linear models in scikit-learn. Currently, `coef_` has a shape of (n_features, n_targets) and in the future it will have a shape of (n_targets, n_features).\n\n[[ 1.   1.   1. ]\n [ 2.   1.9  1.9]\n [-0.1 -0.   0. ]\n [ 0.   0.  -0. ]\n [-0.  -0.   0. ]\n [ 0.   0.   0. ]\n [ 0.   0.   0. ]\n [ 0.  -0.  -0. ]\n [ 0.   0.   0. ]\n [ 0.   0.   0.1]]\n\narray([[ 3.50210309,  3.55301008,  3.72528805],\n       [10.03429511,  9.83576671,  9.74902647],\n       [ 8.03916339,  7.84652988,  7.78629756],\n       ...,\n       [ 2.11231897,  2.1905275 ,  2.33508757],\n       [ 5.35433161,  5.32686504,  5.39877158],\n       [ 5.47827435,  5.38004088,  5.35574845]])",
            "code"
        ]
    ],
    "Examples->Cross decomposition->Compare cross decomposition methods->PLS regression, with univariate response, a.k.a. PLS1": [
        [
            "n = 1000\np = 10\nX = (size=n * p).reshape((n, p))\ny = X[:, 0] + 2 * X[:, 1] + (size=n * 1) + 5\npls1 = (n_components=3)\npls1.fit(X, y)\n# note that the number of components exceeds 1 (the dimension of y)\nprint(\"Estimated betas\")\nprint(np.round(pls1.coef_, 1))",
            "code"
        ],
        [
            "Estimated betas\n/home/circleci/project/sklearn/cross_decomposition/_pls.py:503: FutureWarning:\n\nThe attribute `coef_` will be transposed in version 1.3 to be consistent with other linear models in scikit-learn. Currently, `coef_` has a shape of (n_features, n_targets) and in the future it will have a shape of (n_targets, n_features).\n\n[[ 1.]\n [ 2.]\n [-0.]\n [ 0.]\n [-0.]\n [ 0.]\n [-0.]\n [ 0.]\n [-0.]\n [ 0.]]",
            "code"
        ]
    ],
    "Examples->Cross decomposition->Compare cross decomposition methods->CCA (PLS mode B with symmetric deflation)": [
        [
            "from sklearn.cross_decomposition import \n\ncca = (n_components=2)\ncca.fit(X_train, Y_train)\nX_train_r, Y_train_r = cca.transform(X_train, Y_train)\nX_test_r, Y_test_r = cca.transform(X_test, Y_test)",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.242 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Cross decomposition->Principal Component Regression vs Partial Least Squares Regression": [
        [
            "This example compares  (PCR) and\n (PLS) on a\ntoy dataset. Our goal is to illustrate how PLS can outperform PCR when the\ntarget is strongly correlated with some directions in the data that have a\nlow variance.",
            "markdown"
        ],
        [
            "PCR is a regressor composed of two steps: first,\n is applied to the training data, possibly\nperforming dimensionality reduction; then, a regressor (e.g. a linear\nregressor) is trained on the transformed samples. In\n, the transformation is purely\nunsupervised, meaning that no information about the targets is used. As a\nresult, PCR may perform poorly in some datasets where the target is strongly\ncorrelated with directions that have low variance. Indeed, the\ndimensionality reduction of PCA projects the data into a lower dimensional\nspace where the variance of the projected data is greedily maximized along\neach axis. Despite them having the most predictive power on the target, the\ndirections with a lower variance will be dropped, and the final regressor\nwill not be able to leverage them.",
            "markdown"
        ],
        [
            "PLS is both a transformer and a regressor, and it is quite similar to PCR: it\nalso applies a dimensionality reduction to the samples before applying a\nlinear regressor to the transformed data. The main difference with PCR is\nthat the PLS transformation is supervised. Therefore, as we will see in this\nexample, it does not suffer from the issue we just mentioned.",
            "markdown"
        ]
    ],
    "Examples->Cross decomposition->Principal Component Regression vs Partial Least Squares Regression->The data": [
        [
            "We start by creating a simple dataset with two features. Before we even dive\ninto PCR and PLS, we fit a PCA estimator to display the two principal\ncomponents of this dataset, i.e. the two directions that explain the most\nvariance in the data.",
            "markdown"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import \n\nrng = (0)\nn_samples = 500\ncov = [[3, 3], [3, 4]]\nX = rng.multivariate_normal(mean=[0, 0], cov=cov, size=n_samples)\npca = (n_components=2).fit(X)\n\n\n(X[:, 0], X[:, 1], alpha=0.3, label=\"samples\")\nfor i, (comp, var) in enumerate(zip(pca.components_, pca.explained_variance_)):\n    comp = comp * var  # scale component by its variance explanation power\n    (\n        [0, comp[0]],\n        [0, comp[1]],\n        label=f\"Component {i}\",\n        linewidth=5,\n        color=f\"C{i + 2}\",\n    )\n().set(\n    aspect=\"equal\",\n    title=\"2-dimensional dataset with principal components\",\n    xlabel=\"first feature\",\n    ylabel=\"second feature\",\n)\n()\n()\n\n\n<img alt=\"2-dimensional dataset with principal components\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_pcr_vs_pls_001.png\" srcset=\"../../_images/sphx_glr_plot_pcr_vs_pls_001.png\"/>",
            "code"
        ],
        [
            "For the purpose of this example, we now define the target y such that it is\nstrongly correlated with a direction that has a small variance. To this end,\nwe will project X onto the second component, and add some noise to it.",
            "markdown"
        ],
        [
            "y = X.dot(pca.components_[1]) + rng.normal(size=n_samples) / 2\n\nfig, axes = (1, 2, figsize=(10, 3))\n\naxes[0].scatter(X.dot(pca.components_[0]), y, alpha=0.3)\naxes[0].set(xlabel=\"Projected data onto first PCA component\", ylabel=\"y\")\naxes[1].scatter(X.dot(pca.components_[1]), y, alpha=0.3)\naxes[1].set(xlabel=\"Projected data onto second PCA component\", ylabel=\"y\")\n()\n()\n\n\n<img alt=\"plot pcr vs pls\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_pcr_vs_pls_002.png\" srcset=\"../../_images/sphx_glr_plot_pcr_vs_pls_002.png\"/>",
            "code"
        ]
    ],
    "Examples->Cross decomposition->Principal Component Regression vs Partial Least Squares Regression->Projection on one component and predictive power": [
        [
            "We now create two regressors: PCR and PLS, and for our illustration purposes\nwe set the number of components to 1. Before feeding the data to the PCA step\nof PCR, we first standardize it, as recommended by good practice. The PLS\nestimator has built-in scaling capabilities.",
            "markdown"
        ],
        [
            "For both models, we plot the projected data onto the first component against\nthe target. In both cases, this projected data is what the regressors will\nuse as training data.",
            "markdown"
        ],
        [
            "from sklearn.model_selection import \nfrom sklearn.pipeline import \nfrom sklearn.linear_model import \nfrom sklearn.preprocessing import \nfrom sklearn.decomposition import \nfrom sklearn.cross_decomposition import \n\nX_train, X_test, y_train, y_test = (X, y, random_state=rng)\n\npcr = ((), (n_components=1), ())\npcr.fit(X_train, y_train)\npca = pcr.named_steps[\"pca\"]  # retrieve the PCA step of the pipeline\n\npls = (n_components=1)\npls.fit(X_train, y_train)\n\nfig, axes = (1, 2, figsize=(10, 3))\naxes[0].scatter(pca.transform(X_test), y_test, alpha=0.3, label=\"ground truth\")\naxes[0].scatter(\n    pca.transform(X_test), pcr.predict(X_test), alpha=0.3, label=\"predictions\"\n)\naxes[0].set(\n    xlabel=\"Projected data onto first PCA component\", ylabel=\"y\", title=\"PCR / PCA\"\n)\naxes[0].legend()\naxes[1].scatter(pls.transform(X_test), y_test, alpha=0.3, label=\"ground truth\")\naxes[1].scatter(\n    pls.transform(X_test), pls.predict(X_test), alpha=0.3, label=\"predictions\"\n)\naxes[1].set(xlabel=\"Projected data onto first PLS component\", ylabel=\"y\", title=\"PLS\")\naxes[1].legend()\n()\n()\n\n\n<img alt=\"PCR / PCA, PLS\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_pcr_vs_pls_003.png\" srcset=\"../../_images/sphx_glr_plot_pcr_vs_pls_003.png\"/>",
            "code"
        ],
        [
            "As expected, the unsupervised PCA transformation of PCR has dropped the\nsecond component, i.e. the direction with the lowest variance, despite\nit being the most predictive direction. This is because PCA is a completely\nunsupervised transformation, and results in the projected data having a low\npredictive power on the target.",
            "markdown"
        ],
        [
            "On the other hand, the PLS regressor manages to capture the effect of the\ndirection with the lowest variance, thanks to its use of target information\nduring the transformation: it can recognize that this direction is actually\nthe most predictive. We note that the first PLS component is negatively\ncorrelated with the target, which comes from the fact that the signs of\neigenvectors are arbitrary.",
            "markdown"
        ],
        [
            "We also print the R-squared scores of both estimators, which further confirms\nthat PLS is a better alternative than PCR in this case. A negative R-squared\nindicates that PCR performs worse than a regressor that would simply predict\nthe mean of the target.",
            "markdown"
        ],
        [
            "print(f\"PCR r-squared {pcr.score(X_test, y_test):.3f}\")\nprint(f\"PLS r-squared {pls.score(X_test, y_test):.3f}\")",
            "code"
        ],
        [
            "PCR r-squared -0.026\nPLS r-squared 0.658",
            "code"
        ],
        [
            "As a final remark, we note that PCR with 2 components performs as well as\nPLS: this is because in this case, PCR was able to leverage the second\ncomponent which has the most preditive power on the target.",
            "markdown"
        ],
        [
            "pca_2 = ((n_components=2), ())\npca_2.fit(X_train, y_train)\nprint(f\"PCR r-squared with 2 components {pca_2.score(X_test, y_test):.3f}\")",
            "code"
        ],
        [
            "PCR r-squared with 2 components 0.673",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.488 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Dataset examples->Plot randomly generated classification dataset": [
        [
            "This example plots several randomly generated classification datasets.\nFor easy visualization, all datasets have 2 features, plotted on the x and y\naxis. The color of each point represents its class label.",
            "markdown"
        ],
        [
            "The first 4 plots use the  with\ndifferent numbers of informative features, clusters per class and classes.\nThe final 2 plots use  and\n.\n<img alt=\"One informative feature, one cluster per class, Two informative features, one cluster per class, Two informative features, two clusters per class, Multi-class, two informative features, one cluster, Three blobs, Gaussian divided into three quantiles\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_random_dataset_001.png\" srcset=\"../../_images/sphx_glr_plot_random_dataset_001.png\"/>",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n\nfrom sklearn.datasets import \nfrom sklearn.datasets import \nfrom sklearn.datasets import \n\n(figsize=(8, 8))\n(bottom=0.05, top=0.9, left=0.05, right=0.95)\n\n(321)\n(\"One informative feature, one cluster per class\", fontsize=\"small\")\nX1, Y1 = (\n    n_features=2, n_redundant=0, n_informative=1, n_clusters_per_class=1\n)\n(X1[:, 0], X1[:, 1], marker=\"o\", c=Y1, s=25, edgecolor=\"k\")\n\n(322)\n(\"Two informative features, one cluster per class\", fontsize=\"small\")\nX1, Y1 = (\n    n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1\n)\n(X1[:, 0], X1[:, 1], marker=\"o\", c=Y1, s=25, edgecolor=\"k\")\n\n(323)\n(\"Two informative features, two clusters per class\", fontsize=\"small\")\nX2, Y2 = (n_features=2, n_redundant=0, n_informative=2)\n(X2[:, 0], X2[:, 1], marker=\"o\", c=Y2, s=25, edgecolor=\"k\")\n\n(324)\n(\"Multi-class, two informative features, one cluster\", fontsize=\"small\")\nX1, Y1 = (\n    n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1, n_classes=3\n)\n(X1[:, 0], X1[:, 1], marker=\"o\", c=Y1, s=25, edgecolor=\"k\")\n\n(325)\n(\"Three blobs\", fontsize=\"small\")\nX1, Y1 = (n_features=2, centers=3)\n(X1[:, 0], X1[:, 1], marker=\"o\", c=Y1, s=25, edgecolor=\"k\")\n\n(326)\n(\"Gaussian divided into three quantiles\", fontsize=\"small\")\nX1, Y1 = (n_features=2, n_classes=3)\n(X1[:, 0], X1[:, 1], marker=\"o\", c=Y1, s=25, edgecolor=\"k\")\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.383 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Dataset examples->Plot randomly generated multilabel dataset": [
        [
            "This illustrates the \ndataset generator. Each sample consists of counts of two features (up to 50 in\ntotal), which are differently distributed in each of two classes.",
            "markdown"
        ],
        [
            "Points are labeled as follows, where Y means the class is present:\n\n\n</blockquote>",
            "markdown"
        ],
        [
            "A star marks the expected sample for each class; its size reflects the\nprobability of selecting that class label.",
            "markdown"
        ],
        [
            "The left and right examples highlight the n_labels parameter:\nmore of the samples in the right plot have 2 or 3 labels.",
            "markdown"
        ],
        [
            "Note that this two-dimensional example is very degenerate:\ngenerally the number of features would be much greater than the\n\u201cdocument length\u201d, while here we have much larger documents than vocabulary.\nSimilarly, with n_classes > n_features, it is much less likely that a\nfeature distinguishes a particular class.\n<img alt=\"n_labels=1, length=50, n_labels=3, length=50\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_random_multilabel_dataset_001.png\" srcset=\"../../_images/sphx_glr_plot_random_multilabel_dataset_001.png\"/>",
            "markdown"
        ],
        [
            "The data was generated from (random_state=757):\nClass   P(C)    P(w0|C) P(w1|C)\nred     0.42    0.51    0.49\nblue    0.35    0.18    0.82\nyellow  0.23    0.34    0.66\n\n\n\n<br/>",
            "code"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_multilabel_classification as \n\nCOLORS = (\n    [\n        \"!\",\n        \"#FF3333\",  # red\n        \"#0198E1\",  # blue\n        \"#BF5FFF\",  # purple\n        \"#FCD116\",  # yellow\n        \"#FF7216\",  # orange\n        \"#4DBD33\",  # green\n        \"#87421F\",  # brown\n    ]\n)\n\n# Use same random seed for multiple calls to make_multilabel_classification to\n# ensure same distributions\nRANDOM_SEED = (2**10)\n\n\ndef plot_2d(ax, n_labels=1, n_classes=3, length=50):\n    X, Y, p_c, p_w_c = (\n        n_samples=150,\n        n_features=2,\n        n_classes=n_classes,\n        n_labels=n_labels,\n        length=length,\n        allow_unlabeled=False,\n        return_distributions=True,\n        random_state=RANDOM_SEED,\n    )\n\n    ax.scatter(\n        X[:, 0], X[:, 1], color=COLORS.take((Y * [1, 2, 4]).sum(axis=1)), marker=\".\"\n    )\n    ax.scatter(\n        p_w_c[0] * length,\n        p_w_c[1] * length,\n        marker=\"*\",\n        linewidth=0.5,\n        edgecolor=\"black\",\n        s=20 + 1500 * p_c**2,\n        color=COLORS.take([1, 2, 4]),\n    )\n    ax.set_xlabel(\"Feature 0 count\")\n    return p_c, p_w_c\n\n\n_, (ax1, ax2) = (1, 2, sharex=\"row\", sharey=\"row\", figsize=(8, 4))\n(bottom=0.15)\n\np_c, p_w_c = plot_2d(ax1, n_labels=1)\nax1.set_title(\"n_labels=1, length=50\")\nax1.set_ylabel(\"Feature 1 count\")\n\nplot_2d(ax2, n_labels=3)\nax2.set_title(\"n_labels=3, length=50\")\nax2.set_xlim(left=0, auto=True)\nax2.set_ylim(bottom=0, auto=True)\n\n()\n\nprint(\"The data was generated from (random_state=%d):\" % RANDOM_SEED)\nprint(\"Class\", \"P(C)\", \"P(w0|C)\", \"P(w1|C)\", sep=\"\\t\")\nfor k, p, p_w in zip([\"red\", \"blue\", \"yellow\"], p_c, p_w_c.T):\n    print(\"%s\\t%0.2f\\t%0.2f\\t%0.2f\" % (k, p, p_w[0], p_w[1]))",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.135 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Dataset examples->The Digit Dataset": [
        [
            "This dataset is made up of 1797 8x8 images. Each image,\nlike the one shown below, is of a hand-written digit.\nIn order to utilize an 8x8 figure like this, we\u2019d have to\nfirst transform it into a feature vector with length 64.",
            "markdown"
        ],
        [
            "See \nfor more information about this dataset.\n<img alt=\"plot digits last image\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_digits_last_image_001.png\" srcset=\"../../_images/sphx_glr_plot_digits_last_image_001.png\"/>",
            "markdown"
        ],
        [
            "# Code source: Ga\u00ebl Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nfrom sklearn import datasets\n\nimport matplotlib.pyplot as plt\n\n# Load the digits dataset\ndigits = ()\n\n# Display the last digit\n(1, figsize=(3, 3))\n(digits.images[-1], cmap=plt.cm.gray_r, interpolation=\"nearest\")\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.083 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Dataset examples->The Iris Dataset": [
        [
            "This data sets consists of 3 different types of irises\u2019\n(Setosa, Versicolour, and Virginica) petal and sepal\nlength, stored in a 150x4 numpy.ndarray",
            "markdown"
        ],
        [
            "The rows being the samples and the columns being:\nSepal Length, Sepal Width, Petal Length and Petal Width.",
            "markdown"
        ],
        [
            "The below plot uses the first two features.\nSee  for more\ninformation on this dataset.\n\n<img alt=\"First three PCA directions\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_iris_dataset_001.png\" srcset=\"../../_images/sphx_glr_plot_iris_dataset_001.png\"/>\n<img alt=\"plot iris dataset\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_iris_dataset_002.png\" srcset=\"../../_images/sphx_glr_plot_iris_dataset_002.png\"/>",
            "markdown"
        ],
        [
            "# Code source: Ga\u00ebl Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nimport matplotlib.pyplot as plt\n\n# unused but required import for doing 3d projections with matplotlib &lt; 3.2\nimport mpl_toolkits.mplot3d  # noqa: F401\n\nfrom sklearn import datasets\nfrom sklearn.decomposition import \n\n# import some data to play with\niris = ()\nX = iris.data[:, :2]  # we only take the first two features.\ny = iris.target\n\nx_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\ny_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n\n(2, figsize=(8, 6))\n()\n\n# Plot the training points\n(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1, edgecolor=\"k\")\n(\"Sepal length\")\n(\"Sepal width\")\n\n(x_min, x_max)\n(y_min, y_max)\n(())\n(())\n\n# To getter a better understanding of interaction of the dimensions\n# plot the first three PCA dimensions\nfig = (1, figsize=(8, 6))\nax = fig.add_subplot(111, projection=\"3d\", elev=-150, azim=110)\n\nX_reduced = (n_components=3).fit_transform(iris.data)\nax.scatter(\n    X_reduced[:, 0],\n    X_reduced[:, 1],\n    X_reduced[:, 2],\n    c=y,\n    cmap=plt.cm.Set1,\n    edgecolor=\"k\",\n    s=40,\n)\n\nax.set_title(\"First three PCA directions\")\nax.set_xlabel(\"1st eigenvector\")\nax.xaxis.set_ticklabels([])\nax.set_ylabel(\"2nd eigenvector\")\nax.yaxis.set_ticklabels([])\nax.set_zlabel(\"3rd eigenvector\")\nax.zaxis.set_ticklabels([])\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.173 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Decision Trees->Decision Tree Regression": [
        [
            "A 1D regression with decision tree.",
            "markdown"
        ],
        [
            "The  is\nused to fit a sine curve with addition noisy observation. As a result, it\nlearns local linear regressions approximating the sine curve.",
            "markdown"
        ],
        [
            "We can see that if the maximum depth of the tree (controlled by the\nmax_depth parameter) is set too high, the decision trees learn too fine\ndetails of the training data and learn from the noise, i.e. they overfit.\n<img alt=\"Decision Tree Regression\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_tree_regression_001.png\" srcset=\"../../_images/sphx_glr_plot_tree_regression_001.png\"/>",
            "markdown"
        ],
        [
            "# Import the necessary modules and libraries\nimport numpy as np\nfrom sklearn.tree import \nimport matplotlib.pyplot as plt\n\n# Create a random dataset\nrng = (1)\nX = (5 * rng.rand(80, 1), axis=0)\ny = (X).ravel()\ny[::5] += 3 * (0.5 - rng.rand(16))\n\n# Fit regression model\nregr_1 = (max_depth=2)\nregr_2 = (max_depth=5)\nregr_1.fit(X, y)\nregr_2.fit(X, y)\n\n# Predict\nX_test = (0.0, 5.0, 0.01)[:, ]\ny_1 = regr_1.predict(X_test)\ny_2 = regr_2.predict(X_test)\n\n# Plot the results\n()\n(X, y, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"data\")\n(X_test, y_1, color=\"cornflowerblue\", label=\"max_depth=2\", linewidth=2)\n(X_test, y_2, color=\"yellowgreen\", label=\"max_depth=5\", linewidth=2)\n(\"data\")\n(\"target\")\n(\"Decision Tree Regression\")\n()\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.100 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Decision Trees->Multi-output Decision Tree Regression": [
        [
            "An example to illustrate multi-output regression with decision tree.",
            "markdown"
        ],
        [
            "The \nis used to predict simultaneously the noisy x and y observations of a circle\ngiven a single underlying feature. As a result, it learns local linear\nregressions approximating the circle.",
            "markdown"
        ],
        [
            "We can see that if the maximum depth of the tree (controlled by the\nmax_depth parameter) is set too high, the decision trees learn too fine\ndetails of the training data and learn from the noise, i.e. they overfit.\n<img alt=\"Multi-output Decision Tree Regression\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_tree_regression_multioutput_001.png\" srcset=\"../../_images/sphx_glr_plot_tree_regression_multioutput_001.png\"/>",
            "markdown"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import \n\n# Create a random dataset\nrng = (1)\nX = (200 * rng.rand(100, 1) - 100, axis=0)\ny = ([ * (X).ravel(),  * (X).ravel()]).T\ny[::5, :] += 0.5 - rng.rand(20, 2)\n\n# Fit regression model\nregr_1 = (max_depth=2)\nregr_2 = (max_depth=5)\nregr_3 = (max_depth=8)\nregr_1.fit(X, y)\nregr_2.fit(X, y)\nregr_3.fit(X, y)\n\n# Predict\nX_test = (-100.0, 100.0, 0.01)[:, ]\ny_1 = regr_1.predict(X_test)\ny_2 = regr_2.predict(X_test)\ny_3 = regr_3.predict(X_test)\n\n# Plot the results\n()\ns = 25\n(y[:, 0], y[:, 1], c=\"navy\", s=s, edgecolor=\"black\", label=\"data\")\n(\n    y_1[:, 0],\n    y_1[:, 1],\n    c=\"cornflowerblue\",\n    s=s,\n    edgecolor=\"black\",\n    label=\"max_depth=2\",\n)\n(y_2[:, 0], y_2[:, 1], c=\"red\", s=s, edgecolor=\"black\", label=\"max_depth=5\")\n(\n    y_3[:, 0], y_3[:, 1], c=\"orange\", s=s, edgecolor=\"black\", label=\"max_depth=8\"\n)\n([-6, 6])\n([-6, 6])\n(\"target 1\")\n(\"target 2\")\n(\"Multi-output Decision Tree Regression\")\n(loc=\"best\")\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.256 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Decision Trees->Plot the decision surface of decision trees trained on the iris dataset": [
        [
            "Plot the decision surface of a decision tree trained on pairs\nof features of the iris dataset.",
            "markdown"
        ],
        [
            "See  for more information on the estimator.",
            "markdown"
        ],
        [
            "For each pair of iris features, the decision tree learns decision\nboundaries made of combinations of simple thresholding rules inferred from\nthe training samples.",
            "markdown"
        ],
        [
            "We also show the tree structure of a model built on all of the features.",
            "markdown"
        ],
        [
            "First load the copy of the Iris dataset shipped with scikit-learn:",
            "markdown"
        ],
        [
            "from sklearn.datasets import \n\niris = ()",
            "code"
        ],
        [
            "Display the decision functions of trees trained on all pairs of features.",
            "markdown"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import \nfrom sklearn.tree import \nfrom sklearn.inspection import DecisionBoundaryDisplay\n\n\n# Parameters\nn_classes = 3\nplot_colors = \"ryb\"\nplot_step = 0.02\n\n\nfor pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3], [1, 2], [1, 3], [2, 3]]):\n    # We only take the two corresponding features\n    X = iris.data[:, pair]\n    y = iris.target\n\n    # Train\n    clf = ().fit(X, y)\n\n    # Plot the decision boundary\n    ax = (2, 3, pairidx + 1)\n    (h_pad=0.5, w_pad=0.5, pad=2.5)\n    (\n        clf,\n        X,\n        cmap=plt.cm.RdYlBu,\n        response_method=\"predict\",\n        ax=ax,\n        xlabel=iris.feature_names[pair[0]],\n        ylabel=iris.feature_names[pair[1]],\n    )\n\n    # Plot the training points\n    for i, color in zip(range(n_classes), plot_colors):\n        idx = (y == i)\n        (\n            X[idx, 0],\n            X[idx, 1],\n            c=color,\n            label=iris.target_names[i],\n            cmap=plt.cm.RdYlBu,\n            edgecolor=\"black\",\n            s=15,\n        )\n\n(\"Decision surface of decision trees trained on pairs of features\")\n(loc=\"lower right\", borderpad=0, handletextpad=0)\n_ = (\"tight\")\n\n\n<img alt=\"Decision surface of decision trees trained on pairs of features\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_iris_dtc_001.png\" srcset=\"../../_images/sphx_glr_plot_iris_dtc_001.png\"/>",
            "code"
        ],
        [
            "/home/circleci/project/examples/tree/plot_iris_dtc.py:64: UserWarning:\n\nNo data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n\n/home/circleci/project/examples/tree/plot_iris_dtc.py:64: UserWarning:\n\nNo data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n\n/home/circleci/project/examples/tree/plot_iris_dtc.py:64: UserWarning:\n\nNo data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n\n/home/circleci/project/examples/tree/plot_iris_dtc.py:64: UserWarning:\n\nNo data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n\n/home/circleci/project/examples/tree/plot_iris_dtc.py:64: UserWarning:\n\nNo data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n\n/home/circleci/project/examples/tree/plot_iris_dtc.py:64: UserWarning:\n\nNo data for colormapping provided via 'c'. Parameters 'cmap' will be ignored",
            "code"
        ],
        [
            "Display the structure of a single decision tree trained on all the features\ntogether.",
            "markdown"
        ],
        [
            "from sklearn.tree import \n\n()\nclf = ().fit(iris.data, iris.target)\n(clf, filled=True)\n(\"Decision tree trained on all the iris features\")\n()\n\n\n<img alt=\"Decision tree trained on all the iris features\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_iris_dtc_002.png\" srcset=\"../../_images/sphx_glr_plot_iris_dtc_002.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.841 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Decision Trees->Post pruning decision trees with cost complexity pruning": [
        [
            "The  provides parameters such as\nmin_samples_leaf and max_depth to prevent a tree from overfiting. Cost\ncomplexity pruning provides another option to control the size of a tree. In\n, this pruning technique is parameterized by the\ncost complexity parameter, ccp_alpha. Greater values of ccp_alpha\nincrease the number of nodes pruned. Here we only show the effect of\nccp_alpha on regularizing the trees and how to choose a ccp_alpha\nbased on validation scores.",
            "markdown"
        ],
        [
            "See also  for details on pruning.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\nfrom sklearn.model_selection import \nfrom sklearn.datasets import \nfrom sklearn.tree import",
            "code"
        ]
    ],
    "Examples->Decision Trees->Post pruning decision trees with cost complexity pruning->Total impurity of leaves vs effective alphas of pruned tree": [
        [
            "Minimal cost complexity pruning recursively finds the node with the \u201cweakest\nlink\u201d. The weakest link is characterized by an effective alpha, where the\nnodes with the smallest effective alpha are pruned first. To get an idea of\nwhat values of ccp_alpha could be appropriate, scikit-learn provides\n that returns the\neffective alphas and the corresponding total leaf impurities at each step of\nthe pruning process. As alpha increases, more of the tree is pruned, which\nincreases the total impurity of its leaves.",
            "markdown"
        ],
        [
            "X, y = (return_X_y=True)\nX_train, X_test, y_train, y_test = (X, y, random_state=0)\n\nclf = (random_state=0)\npath = clf.cost_complexity_pruning_path(X_train, y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities",
            "code"
        ],
        [
            "In the following plot, the maximum effective alpha value is removed, because\nit is the trivial tree with only one node.",
            "markdown"
        ],
        [
            "fig, ax = ()\nax.plot(ccp_alphas[:-1], impurities[:-1], marker=\"o\", drawstyle=\"steps-post\")\nax.set_xlabel(\"effective alpha\")\nax.set_ylabel(\"total impurity of leaves\")\nax.set_title(\"Total Impurity vs effective alpha for training set\")\n\n\n<img alt=\"Total Impurity vs effective alpha for training set\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cost_complexity_pruning_001.png\" srcset=\"../../_images/sphx_glr_plot_cost_complexity_pruning_001.png\"/>",
            "code"
        ],
        [
            "Text(0.5, 1.0, 'Total Impurity vs effective alpha for training set')",
            "code"
        ],
        [
            "Next, we train a decision tree using the effective alphas. The last value\nin ccp_alphas is the alpha value that prunes the whole tree,\nleaving the tree, clfs[-1], with one node.",
            "markdown"
        ],
        [
            "clfs = []\nfor ccp_alpha in ccp_alphas:\n    clf = (random_state=0, ccp_alpha=ccp_alpha)\n    clf.fit(X_train, y_train)\n    clfs.append(clf)\nprint(\n    \"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n        clfs[-1].tree_.node_count, ccp_alphas[-1]\n    )\n)",
            "code"
        ],
        [
            "Number of nodes in the last tree is: 1 with ccp_alpha: 0.3272984419327777",
            "code"
        ],
        [
            "For the remainder of this example, we remove the last element in\nclfs and ccp_alphas, because it is the trivial tree with only one\nnode. Here we show that the number of nodes and tree depth decreases as alpha\nincreases.",
            "markdown"
        ],
        [
            "clfs = clfs[:-1]\nccp_alphas = ccp_alphas[:-1]\n\nnode_counts = [clf.tree_.node_count for clf in clfs]\ndepth = [clf.tree_.max_depth for clf in clfs]\nfig, ax = (2, 1)\nax[0].plot(ccp_alphas, node_counts, marker=\"o\", drawstyle=\"steps-post\")\nax[0].set_xlabel(\"alpha\")\nax[0].set_ylabel(\"number of nodes\")\nax[0].set_title(\"Number of nodes vs alpha\")\nax[1].plot(ccp_alphas, depth, marker=\"o\", drawstyle=\"steps-post\")\nax[1].set_xlabel(\"alpha\")\nax[1].set_ylabel(\"depth of tree\")\nax[1].set_title(\"Depth vs alpha\")\nfig.tight_layout()\n\n\n<img alt=\"Number of nodes vs alpha, Depth vs alpha\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cost_complexity_pruning_002.png\" srcset=\"../../_images/sphx_glr_plot_cost_complexity_pruning_002.png\"/>",
            "code"
        ]
    ],
    "Examples->Decision Trees->Post pruning decision trees with cost complexity pruning->Accuracy vs alpha for training and testing sets": [
        [
            "When ccp_alpha is set to zero and keeping the other default parameters\nof , the tree overfits, leading to\na 100% training accuracy and 88% testing accuracy. As alpha increases, more\nof the tree is pruned, thus creating a decision tree that generalizes better.\nIn this example, setting ccp_alpha=0.015 maximizes the testing accuracy.",
            "markdown"
        ],
        [
            "train_scores = [clf.score(X_train, y_train) for clf in clfs]\ntest_scores = [clf.score(X_test, y_test) for clf in clfs]\n\nfig, ax = ()\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker=\"o\", label=\"test\", drawstyle=\"steps-post\")\nax.legend()\n()\n\n\n<img alt=\"Accuracy vs alpha for training and testing sets\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cost_complexity_pruning_003.png\" srcset=\"../../_images/sphx_glr_plot_cost_complexity_pruning_003.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.434 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Decision Trees->Understanding the decision tree structure": [
        [
            "The decision tree structure can be analysed to gain further insight on the\nrelation between the features and the target to predict. In this example, we\nshow how to retrieve:",
            "markdown"
        ],
        [
            "the binary tree structure;",
            "markdown"
        ],
        [
            "the depth of each node and whether or not it\u2019s a leaf;",
            "markdown"
        ],
        [
            "the nodes that were reached by a sample using the decision_path method;",
            "markdown"
        ],
        [
            "the leaf that was reached by a sample using the apply method;",
            "markdown"
        ],
        [
            "the rules that were used to predict a sample;",
            "markdown"
        ],
        [
            "the decision path shared by a group of samples.",
            "markdown"
        ],
        [
            "import numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.model_selection import \nfrom sklearn.datasets import \nfrom sklearn.tree import \nfrom sklearn import tree",
            "code"
        ]
    ],
    "Examples->Decision Trees->Understanding the decision tree structure->Train tree classifier": [
        [
            "First, we fit a  using the\n dataset.",
            "markdown"
        ],
        [
            "iris = ()\nX = iris.data\ny = iris.target\nX_train, X_test, y_train, y_test = (X, y, random_state=0)\n\nclf = (max_leaf_nodes=3, random_state=0)\nclf.fit(X_train, y_train)",
            "code"
        ],
        [
            "DecisionTreeClassifier(max_leaf_nodes=3, random_state=0)<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input checked=\"\" class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-28\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-28\">DecisionTreeClassifier</label>",
            "code"
        ],
        [
            "DecisionTreeClassifier(max_leaf_nodes=3, random_state=0)\n\n<br/>\n<br/>",
            "code"
        ]
    ],
    "Examples->Decision Trees->Understanding the decision tree structure->Tree structure": [
        [
            "The decision classifier has an attribute called tree_ which allows access\nto low level attributes such as node_count, the total number of nodes,\nand max_depth, the maximal depth of the tree. It also stores the\nentire binary tree structure, represented as a number of parallel arrays. The\ni-th element of each array holds information about the node i. Node 0 is\nthe tree\u2019s root. Some of the arrays only apply to either leaves or split\nnodes. In this case the values of the nodes of the other type is arbitrary.\nFor example, the arrays feature and threshold only apply to split\nnodes. The values for leaf nodes in these arrays are therefore arbitrary.",
            "markdown"
        ],
        [
            "Among these arrays, we have:",
            "markdown"
        ],
        [
            "children_left[i]: id of the left child of node i or -1 if leaf\nnode",
            "markdown"
        ],
        [
            "children_right[i]: id of the right child of node i or -1 if leaf\nnode",
            "markdown"
        ],
        [
            "feature[i]: feature used for splitting node i",
            "markdown"
        ],
        [
            "threshold[i]: threshold value at node i",
            "markdown"
        ],
        [
            "n_node_samples[i]: the number of training samples reaching node\ni",
            "markdown"
        ],
        [
            "impurity[i]: the impurity at node i\n\n</blockquote>",
            "markdown"
        ],
        [
            "Using the arrays, we can traverse the tree structure to compute various\nproperties. Below, we will compute the depth of each node and whether or not\nit is a leaf.",
            "markdown"
        ],
        [
            "n_nodes = clf.tree_.node_count\nchildren_left = clf.tree_.children_left\nchildren_right = clf.tree_.children_right\nfeature = clf.tree_.feature\nthreshold = clf.tree_.threshold\n\nnode_depth = (shape=n_nodes, dtype=)\nis_leaves = (shape=n_nodes, dtype=bool)\nstack = [(0, 0)]  # start with the root node id (0) and its depth (0)\nwhile len(stack)  0:\n    # `pop` ensures each node is only visited once\n    node_id, depth = stack.pop()\n    node_depth[node_id] = depth\n\n    # If the left and right child of a node is not the same we have a split\n    # node\n    is_split_node = children_left[node_id] != children_right[node_id]\n    # If a split node, append left and right children and depth to `stack`\n    # so we can loop through them\n    if is_split_node:\n        stack.append((children_left[node_id], depth + 1))\n        stack.append((children_right[node_id], depth + 1))\n    else:\n        is_leaves[node_id] = True\n\nprint(\n    \"The binary tree structure has {n} nodes and has \"\n    \"the following tree structure:\\n\".format(n=n_nodes)\n)\nfor i in range(n_nodes):\n    if is_leaves[i]:\n        print(\n            \"{space}node={node} is a leaf node.\".format(\n                space=node_depth[i] * \"\\t\", node=i\n            )\n        )\n    else:\n        print(\n            \"{space}node={node} is a split node: \"\n            \"go to node {left} if X[:, {feature}] &lt;= {threshold} \"\n            \"else to node {right}.\".format(\n                space=node_depth[i] * \"\\t\",\n                node=i,\n                left=children_left[i],\n                feature=feature[i],\n                threshold=threshold[i],\n                right=children_right[i],\n            )\n        )",
            "code"
        ],
        [
            "The binary tree structure has 5 nodes and has the following tree structure:\n\nnode=0 is a split node: go to node 1 if X[:, 3] &lt;= 0.800000011920929 else to node 2.\n        node=1 is a leaf node.\n        node=2 is a split node: go to node 3 if X[:, 2] &lt;= 4.950000047683716 else to node 4.\n                node=3 is a leaf node.\n                node=4 is a leaf node.",
            "code"
        ],
        [
            "We can compare the above output to the plot of the decision tree.",
            "markdown"
        ],
        [
            "(clf)\n()\n\n\n<img alt=\"plot unveil tree structure\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_unveil_tree_structure_001.png\" srcset=\"../../_images/sphx_glr_plot_unveil_tree_structure_001.png\"/>",
            "code"
        ]
    ],
    "Examples->Decision Trees->Understanding the decision tree structure->Decision path": [
        [
            "We can also retrieve the decision path of samples of interest. The\ndecision_path method outputs an indicator matrix that allows us to\nretrieve the nodes the samples of interest traverse through. A non zero\nelement in the indicator matrix at position (i, j) indicates that\nthe sample i goes through the node j. Or, for one sample i, the\npositions of the non zero elements in row i of the indicator matrix\ndesignate the ids of the nodes that sample goes through.",
            "markdown"
        ],
        [
            "The leaf ids reached by samples of interest can be obtained with the\napply method. This returns an array of the node ids of the leaves\nreached by each sample of interest. Using the leaf ids and the\ndecision_path we can obtain the splitting conditions that were used to\npredict a sample or a group of samples. First, let\u2019s do it for one sample.\nNote that node_index is a sparse matrix.",
            "markdown"
        ],
        [
            "node_indicator = clf.decision_path(X_test)\nleaf_id = clf.apply(X_test)\n\nsample_id = 0\n# obtain ids of the nodes `sample_id` goes through, i.e., row `sample_id`\nnode_index = node_indicator.indices[\n    node_indicator.indptr[sample_id] : node_indicator.indptr[sample_id + 1]\n]\n\nprint(\"Rules used to predict sample {id}:\\n\".format(id=sample_id))\nfor node_id in node_index:\n    # continue to the next node if it is a leaf node\n    if leaf_id[sample_id] == node_id:\n        continue\n\n    # check if value of the split feature for sample 0 is below threshold\n    if X_test[sample_id, feature[node_id]] &lt;= threshold[node_id]:\n        threshold_sign = \"&lt;=\"\n    else:\n        threshold_sign = \"\"\n\n    print(\n        \"decision node {node} : (X_test[{sample}, {feature}] = {value}) \"\n        \"{inequality} {threshold})\".format(\n            node=node_id,\n            sample=sample_id,\n            feature=feature[node_id],\n            value=X_test[sample_id, feature[node_id]],\n            inequality=threshold_sign,\n            threshold=threshold[node_id],\n        )\n    )",
            "code"
        ],
        [
            "Rules used to predict sample 0:\n\ndecision node 0 : (X_test[0, 3] = 2.4)  0.800000011920929)\ndecision node 2 : (X_test[0, 2] = 5.1)  4.950000047683716)",
            "code"
        ],
        [
            "For a group of samples, we can determine the common nodes the samples go\nthrough.",
            "markdown"
        ],
        [
            "sample_ids = [0, 1]\n# boolean array indicating the nodes both samples go through\ncommon_nodes = node_indicator.toarray()[sample_ids].sum(axis=0) == len(sample_ids)\n# obtain node ids using position in array\ncommon_node_id = (n_nodes)[common_nodes]\n\nprint(\n    \"\\nThe following samples {samples} share the node(s) {nodes} in the tree.\".format(\n        samples=sample_ids, nodes=common_node_id\n    )\n)\nprint(\"This is {prop}% of all nodes.\".format(prop=100 * len(common_node_id) / n_nodes))",
            "code"
        ],
        [
            "The following samples [0, 1] share the node(s) [0 2] in the tree.\nThis is 40.0% of all nodes.",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.122 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Decomposition->Beta-divergence loss functions": [
        [
            "A plot that compares the various Beta-divergence loss functions supported by\nthe Multiplicative-Update (\u2018mu\u2019) solver in .\n<img alt=\"beta-divergence(1, x)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_beta_divergence_001.png\" srcset=\"../../_images/sphx_glr_plot_beta_divergence_001.png\"/>",
            "markdown"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition._nmf import _beta_divergence\n\nx = (0.001, 4, 1000)\ny = (x.shape)\n\ncolors = \"mbgyr\"\nfor j, beta in enumerate((0.0, 0.5, 1.0, 1.5, 2.0)):\n    for i, xi in enumerate(x):\n        y[i] = _beta_divergence(1, xi, 1, beta)\n    name = \"beta = %1.1f\" % beta\n    (x, y, label=name, color=colors[j])\n\n(\"x\")\n(\"beta-divergence(1, x)\")\n(loc=0)\n([0, 4, 0, 3])\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.254 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Decomposition->Blind source separation using FastICA": [
        [
            "An example of estimating sources from noisy data.",
            "markdown"
        ],
        [
            "is used to estimate sources given noisy measurements.\nImagine 3 instruments playing simultaneously and 3 microphones\nrecording the mixed signals. ICA is used to recover the sources\nie. what is played by each instrument. Importantly, PCA fails\nat recovering our instruments since the related signals reflect\nnon-Gaussian processes.",
            "markdown"
        ]
    ],
    "Examples->Decomposition->Blind source separation using FastICA->Generate sample data": [
        [
            "import numpy as np\nfrom scipy import signal\n\n(0)\nn_samples = 2000\ntime = (0, 8, n_samples)\n\ns1 = (2 * time)  # Signal 1 : sinusoidal signal\ns2 = ((3 * time))  # Signal 2 : square signal\ns3 = (2 *  * time)  # Signal 3: saw tooth signal\n\nS = [s1, s2, s3]\nS += 0.2 * (size=S.shape)  # Add noise\n\nS /= S.std(axis=0)  # Standardize data\n# Mix data\nA = ([[1, 1, 1], [0.5, 2, 1.0], [1.5, 1.0, 2.0]])  # Mixing matrix\nX = (S, A.T)  # Generate observations",
            "code"
        ]
    ],
    "Examples->Decomposition->Blind source separation using FastICA->Fit ICA and PCA models": [
        [
            "from sklearn.decomposition import , \n\n# Compute ICA\nica = (n_components=3, whiten=\"arbitrary-variance\")\nS_ = ica.fit_transform(X)  # Reconstruct signals\nA_ = ica.mixing_  # Get estimated mixing matrix\n\n# We can `prove` that the ICA model applies by reverting the unmixing.\nassert (X, (S_, A_.T) + ica.mean_)\n\n# For comparison, compute PCA\npca = (n_components=3)\nH = pca.fit_transform(X)  # Reconstruct signals based on orthogonal components",
            "code"
        ]
    ],
    "Examples->Decomposition->Blind source separation using FastICA->Plot results": [
        [
            "import matplotlib.pyplot as plt\n\n()\n\nmodels = [X, S, S_, H]\nnames = [\n    \"Observations (mixed signal)\",\n    \"True Sources\",\n    \"ICA recovered signals\",\n    \"PCA recovered signals\",\n]\ncolors = [\"red\", \"steelblue\", \"orange\"]\n\nfor ii, (model, name) in enumerate(zip(models, names), 1):\n    (4, 1, ii)\n    (name)\n    for sig, color in zip(model.T, colors):\n        (sig, color=color)\n\n()\n()\n\n\n<img alt=\"Observations (mixed signal), True Sources, ICA recovered signals, PCA recovered signals\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_ica_blind_source_separation_001.png\" srcset=\"../../_images/sphx_glr_plot_ica_blind_source_separation_001.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.308 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Decomposition->Comparison of LDA and PCA 2D projection of Iris dataset": [
        [
            "The Iris dataset represents 3 kind of Iris flowers (Setosa, Versicolour\nand Virginica) with 4 attributes: sepal length, sepal width, petal length\nand petal width.",
            "markdown"
        ],
        [
            "Principal Component Analysis (PCA) applied to this data identifies the\ncombination of attributes (principal components, or directions in the\nfeature space) that account for the most variance in the data. Here we\nplot the different samples on the 2 first principal components.",
            "markdown"
        ],
        [
            "Linear Discriminant Analysis (LDA) tries to identify attributes that\naccount for the most variance between classes. In particular,\nLDA, in contrast to PCA, is a supervised method, using known class labels.\n\n<img alt=\"PCA of IRIS dataset\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_pca_vs_lda_001.png\" srcset=\"../../_images/sphx_glr_plot_pca_vs_lda_001.png\"/>\n<img alt=\"LDA of IRIS dataset\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_pca_vs_lda_002.png\" srcset=\"../../_images/sphx_glr_plot_pca_vs_lda_002.png\"/>",
            "markdown"
        ],
        [
            "explained variance ratio (first two components): [0.92461872 0.05306648]\n\n\n\n<br/>",
            "code"
        ],
        [
            "import matplotlib.pyplot as plt\n\nfrom sklearn import datasets\nfrom sklearn.decomposition import \nfrom sklearn.discriminant_analysis import \n\niris = ()\n\nX = iris.data\ny = iris.target\ntarget_names = iris.target_names\n\npca = (n_components=2)\nX_r = pca.fit(X).transform(X)\n\nlda = (n_components=2)\nX_r2 = lda.fit(X, y).transform(X)\n\n# Percentage of variance explained for each components\nprint(\n    \"explained variance ratio (first two components): %s\"\n    % str(pca.explained_variance_ratio_)\n)\n\n()\ncolors = [\"navy\", \"turquoise\", \"darkorange\"]\nlw = 2\n\nfor color, i, target_name in zip(colors, [0, 1, 2], target_names):\n    (\n        X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=0.8, lw=lw, label=target_name\n    )\n(loc=\"best\", shadow=False, scatterpoints=1)\n(\"PCA of IRIS dataset\")\n\n()\nfor color, i, target_name in zip(colors, [0, 1, 2], target_names):\n    (\n        X_r2[y == i, 0], X_r2[y == i, 1], alpha=0.8, color=color, label=target_name\n    )\n(loc=\"best\", shadow=False, scatterpoints=1)\n(\"LDA of IRIS dataset\")\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.226 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Decomposition->Faces dataset decompositions": [
        [
            "This example applies to  different unsupervised\nmatrix decomposition (dimension reduction) methods from the module\n (see the documentation chapter\n).",
            "markdown"
        ],
        [
            "Authors: Vlad Niculae, Alexandre Gramfort",
            "markdown"
        ],
        [
            "License: BSD 3 clause",
            "markdown"
        ]
    ],
    "Examples->Decomposition->Faces dataset decompositions->Dataset preparation": [
        [
            "Loading and preprocessing the Olivetti faces dataset.",
            "markdown"
        ],
        [
            "import logging\n\nfrom numpy.random import \nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import \nfrom sklearn import cluster\nfrom sklearn import decomposition\n\nrng = (0)\n\n# Display progress logs on stdout\n(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n\nfaces, _ = (return_X_y=True, shuffle=True, random_state=rng)\nn_samples, n_features = faces.shape\n\n# Global centering (focus on one feature, centering all samples)\nfaces_centered = faces - faces.mean(axis=0)\n\n# Local centering (focus on one sample, centering all features)\nfaces_centered -= faces_centered.mean(axis=1).reshape(n_samples, -1)\n\nprint(\"Dataset consists of %d faces\" % n_samples)",
            "code"
        ],
        [
            "Dataset consists of 400 faces",
            "code"
        ],
        [
            "Define a base function to plot the gallery of faces.",
            "markdown"
        ],
        [
            "n_row, n_col = 2, 3\nn_components = n_row * n_col\nimage_shape = (64, 64)\n\n\ndef plot_gallery(title, images, n_col=n_col, n_row=n_row, cmap=plt.cm.gray):\n    fig, axs = (\n        nrows=n_row,\n        ncols=n_col,\n        figsize=(2.0 * n_col, 2.3 * n_row),\n        facecolor=\"white\",\n        constrained_layout=True,\n    )\n    fig.set_constrained_layout_pads(w_pad=0.01, h_pad=0.02, hspace=0, wspace=0)\n    fig.set_edgecolor(\"black\")\n    fig.suptitle(title, size=16)\n    for ax, vec in zip(axs.flat, images):\n        vmax = max(vec.max(), -vec.min())\n        im = ax.imshow(\n            vec.reshape(image_shape),\n            cmap=cmap,\n            interpolation=\"nearest\",\n            vmin=-vmax,\n            vmax=vmax,\n        )\n        ax.axis(\"off\")\n\n    fig.colorbar(im, ax=axs, orientation=\"horizontal\", shrink=0.99, aspect=40, pad=0.01)\n    ()",
            "code"
        ],
        [
            "Let\u2019s take a look at our data. Gray color indicates negative values,\nwhite indicates positive values.",
            "markdown"
        ],
        [
            "plot_gallery(\"Faces from dataset\", faces_centered[:n_components])\n\n\n<img alt=\"Faces from dataset\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_001.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_001.png\"/>",
            "code"
        ]
    ],
    "Examples->Decomposition->Faces dataset decompositions->Decomposition": [
        [
            "Initialise different estimators for decomposition and fit each\nof them on all images and plot some results. Each estimator extracts\n6 components as vectors \\(h \\in \\mathbb{R}^{4096}\\).\nWe just displayed these vectors in human-friendly visualisation as 64x64 pixel images.",
            "markdown"
        ],
        [
            "Read more in the .",
            "markdown"
        ]
    ],
    "Examples->Decomposition->Faces dataset decompositions->Decomposition->Eigenfaces - PCA using randomized SVD": [
        [
            "Linear dimensionality reduction using Singular Value Decomposition (SVD) of the data\nto project it to a lower dimensional space.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "The Eigenfaces estimator, via the ,\nalso provides a scalar noise_variance_ (the mean of pixelwise variance)\nthat cannot be displayed as an image.",
            "markdown"
        ],
        [
            "pca_estimator = (\n    n_components=n_components, svd_solver=\"randomized\", whiten=True\n)\npca_estimator.fit(faces_centered)\nplot_gallery(\n    \"Eigenfaces - PCA using randomized SVD\", pca_estimator.components_[:n_components]\n)\n\n\n<img alt=\"Eigenfaces - PCA using randomized SVD\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_002.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_002.png\"/>",
            "code"
        ]
    ],
    "Examples->Decomposition->Faces dataset decompositions->Decomposition->Non-negative components - NMF": [
        [
            "Estimate non-negative original data as production of two non-negative matrices.",
            "markdown"
        ],
        [
            "nmf_estimator = (n_components=n_components, tol=5e-3)\nnmf_estimator.fit(faces)  # original non- negative dataset\nplot_gallery(\"Non-negative components - NMF\", nmf_estimator.components_[:n_components])\n\n\n<img alt=\"Non-negative components - NMF\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_003.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_003.png\"/>",
            "code"
        ]
    ],
    "Examples->Decomposition->Faces dataset decompositions->Decomposition->Independent components - FastICA": [
        [
            "Independent component analysis separates a multivariate vectors into additive\nsubcomponents that are maximally independent.",
            "markdown"
        ],
        [
            "ica_estimator = (\n    n_components=n_components, max_iter=400, whiten=\"arbitrary-variance\", tol=15e-5\n)\nica_estimator.fit(faces_centered)\nplot_gallery(\n    \"Independent components - FastICA\", ica_estimator.components_[:n_components]\n)\n\n\n<img alt=\"Independent components - FastICA\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_004.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_004.png\"/>",
            "code"
        ]
    ],
    "Examples->Decomposition->Faces dataset decompositions->Decomposition->Sparse components - MiniBatchSparsePCA": [
        [
            "Mini-batch sparse PCA (MiniBatchSparsePCA) extracts the set of sparse\ncomponents that best reconstruct the data. This variant is faster but\nless accurate than the similar .",
            "markdown"
        ],
        [
            "batch_pca_estimator = (\n    n_components=n_components, alpha=0.1, max_iter=100, batch_size=3, random_state=rng\n)\nbatch_pca_estimator.fit(faces_centered)\nplot_gallery(\n    \"Sparse components - MiniBatchSparsePCA\",\n    batch_pca_estimator.components_[:n_components],\n)\n\n\n<img alt=\"Sparse components - MiniBatchSparsePCA\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_005.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_005.png\"/>",
            "code"
        ]
    ],
    "Examples->Decomposition->Faces dataset decompositions->Decomposition->Dictionary learning": [
        [
            "By default, MiniBatchDictionaryLearning divides the data into\nmini-batches and optimizes in an online manner by cycling over the\nmini-batches for the specified number of iterations.",
            "markdown"
        ],
        [
            "batch_dict_estimator = (\n    n_components=n_components, alpha=0.1, max_iter=50, batch_size=3, random_state=rng\n)\nbatch_dict_estimator.fit(faces_centered)\nplot_gallery(\"Dictionary learning\", batch_dict_estimator.components_[:n_components])\n\n\n<img alt=\"Dictionary learning\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_006.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_006.png\"/>",
            "code"
        ]
    ],
    "Examples->Decomposition->Faces dataset decompositions->Decomposition->Cluster centers - MiniBatchKMeans": [
        [
            "MiniBatchKMeans is computationally efficient and implements on-line\nlearning with a partial_fit method. That is why it could be beneficial\nto enhance some time-consuming algorithms with  MiniBatchKMeans.",
            "markdown"
        ],
        [
            "kmeans_estimator = (\n    n_clusters=n_components,\n    tol=1e-3,\n    batch_size=20,\n    max_iter=50,\n    random_state=rng,\n    n_init=\"auto\",\n)\nkmeans_estimator.fit(faces_centered)\nplot_gallery(\n    \"Cluster centers - MiniBatchKMeans\",\n    kmeans_estimator.cluster_centers_[:n_components],\n)\n\n\n<img alt=\"Cluster centers - MiniBatchKMeans\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_007.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_007.png\"/>",
            "code"
        ]
    ],
    "Examples->Decomposition->Faces dataset decompositions->Decomposition->Factor Analysis components - FA": [
        [
            "Factor Analysis is similar to PCA but has the advantage of modelling the\nvariance in every direction of the input space independently\n(heteroscedastic noise).\nRead more in the .",
            "markdown"
        ],
        [
            "fa_estimator = (n_components=n_components, max_iter=20)\nfa_estimator.fit(faces_centered)\nplot_gallery(\"Factor Analysis (FA)\", fa_estimator.components_[:n_components])\n\n# --- Pixelwise variance\n(figsize=(3.2, 3.6), facecolor=\"white\", tight_layout=True)\nvec = fa_estimator.noise_variance_\nvmax = max(vec.max(), -vec.min())\n(\n    vec.reshape(image_shape),\n    cmap=plt.cm.gray,\n    interpolation=\"nearest\",\n    vmin=-vmax,\n    vmax=vmax,\n)\n(\"off\")\n(\"Pixelwise variance from \\n Factor Analysis (FA)\", size=16, wrap=True)\n(orientation=\"horizontal\", shrink=0.8, pad=0.03)\n()\n\n\n\n<img alt=\"Factor Analysis (FA)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_008.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_008.png\"/>\n<img alt=\"Pixelwise variance from   Factor Analysis (FA)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_009.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_009.png\"/>",
            "code"
        ]
    ],
    "Examples->Decomposition->Faces dataset decompositions->Decomposition: Dictionary learning": [
        [
            "In the further section, let\u2019s consider  more precisely.\nDictionary learning is a problem that amounts to finding a sparse representation\nof the input data as a combination of simple elements. These simple elements form\na dictionary. It is possible to constrain the dictionary and/or coding coefficients\nto be positive to match constraints that may be present in the data.",
            "markdown"
        ],
        [
            "MiniBatchDictionaryLearning implements a faster, but less accurate\nversion of the dictionary learning algorithm that is better suited for large\ndatasets. Read more in the .",
            "markdown"
        ],
        [
            "Plot the same samples from our dataset but with another colormap.\nRed indicates negative values, blue indicates positive values,\nand white represents zeros.",
            "markdown"
        ],
        [
            "plot_gallery(\"Faces from dataset\", faces_centered[:n_components], cmap=plt.cm.RdBu)\n\n\n<img alt=\"Faces from dataset\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_010.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_010.png\"/>",
            "code"
        ],
        [
            "Similar to the previous examples, we change parameters and train\nMiniBatchDictionaryLearning estimator on all images. Generally,\nthe dictionary learning and sparse encoding decompose input data\ninto the dictionary and the coding coefficients matrices.\n\\(X \\approx UV\\), where \\(X = [x_1, . . . , x_n]\\),\n\\(X \\in \\mathbb{R}^{m\u00d7n}\\), dictionary \\(U \\in \\mathbb{R}^{m\u00d7k}\\), coding\ncoefficients \\(V \\in \\mathbb{R}^{k\u00d7n}\\).",
            "markdown"
        ],
        [
            "Also below are the results when the dictionary and coding\ncoefficients are positively constrained.",
            "markdown"
        ]
    ],
    "Examples->Decomposition->Faces dataset decompositions->Decomposition: Dictionary learning->Dictionary learning - positive dictionary": [
        [
            "In the following section we enforce positivity when finding the dictionary.",
            "markdown"
        ],
        [
            "dict_pos_dict_estimator = (\n    n_components=n_components,\n    alpha=0.1,\n    max_iter=50,\n    batch_size=3,\n    random_state=rng,\n    positive_dict=True,\n)\ndict_pos_dict_estimator.fit(faces_centered)\nplot_gallery(\n    \"Dictionary learning - positive dictionary\",\n    dict_pos_dict_estimator.components_[:n_components],\n    cmap=plt.cm.RdBu,\n)\n\n\n<img alt=\"Dictionary learning - positive dictionary\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_011.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_011.png\"/>",
            "code"
        ]
    ],
    "Examples->Decomposition->Faces dataset decompositions->Decomposition: Dictionary learning->Dictionary learning - positive code": [
        [
            "Below we constrain the coding coefficients as a positive matrix.",
            "markdown"
        ],
        [
            "dict_pos_code_estimator = (\n    n_components=n_components,\n    alpha=0.1,\n    max_iter=50,\n    batch_size=3,\n    fit_algorithm=\"cd\",\n    random_state=rng,\n    positive_code=True,\n)\ndict_pos_code_estimator.fit(faces_centered)\nplot_gallery(\n    \"Dictionary learning - positive code\",\n    dict_pos_code_estimator.components_[:n_components],\n    cmap=plt.cm.RdBu,\n)\n\n\n<img alt=\"Dictionary learning - positive code\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_012.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_012.png\"/>",
            "code"
        ]
    ],
    "Examples->Decomposition->Faces dataset decompositions->Decomposition: Dictionary learning->Dictionary learning - positive dictionary & code": [
        [
            "Also below are the results if the dictionary values and coding\ncoefficients are positively constrained.",
            "markdown"
        ],
        [
            "dict_pos_estimator = (\n    n_components=n_components,\n    alpha=0.1,\n    max_iter=50,\n    batch_size=3,\n    fit_algorithm=\"cd\",\n    random_state=rng,\n    positive_dict=True,\n    positive_code=True,\n)\ndict_pos_estimator.fit(faces_centered)\nplot_gallery(\n    \"Dictionary learning - positive dictionary & code\",\n    dict_pos_estimator.components_[:n_components],\n    cmap=plt.cm.RdBu,\n)\n\n\n<img alt=\"Dictionary learning - positive dictionary & code\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_013.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_013.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  10.198 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Decomposition->Factor Analysis (with rotation) to visualize patterns": [
        [
            "Investigating the Iris dataset, we see that sepal length, petal\nlength and petal width are highly correlated. Sepal width is\nless redundant. Matrix decomposition techniques can uncover\nthese latent patterns. Applying rotations to the resulting\ncomponents does not inherently improve the predictive value\nof the derived latent space, but can help visualise their\nstructure; here, for example, the varimax rotation, which\nis found by maximizing the squared variances of the weights,\nfinds a structure where the second component only loads\npositively on sepal width.",
            "markdown"
        ],
        [
            "# Authors: Jona Sassenhagen\n# License: BSD 3 clause\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.decomposition import , \nfrom sklearn.preprocessing import \nfrom sklearn.datasets import",
            "code"
        ],
        [
            "Load Iris data",
            "markdown"
        ],
        [
            "data = ()\nX = ().fit_transform(data[\"data\"])\nfeature_names = data[\"feature_names\"]",
            "code"
        ],
        [
            "Plot covariance of Iris features",
            "markdown"
        ],
        [
            "ax = ()\n\nim = ax.imshow((X.T), cmap=\"RdBu_r\", vmin=-1, vmax=1)\n\nax.set_xticks([0, 1, 2, 3])\nax.set_xticklabels(list(feature_names), rotation=90)\nax.set_yticks([0, 1, 2, 3])\nax.set_yticklabels(list(feature_names))\n\n(im).ax.set_ylabel(\"$r$\", rotation=0)\nax.set_title(\"Iris feature correlation matrix\")\n()\n\n\n<img alt=\"Iris feature correlation matrix\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_varimax_fa_001.png\" srcset=\"../../_images/sphx_glr_plot_varimax_fa_001.png\"/>",
            "code"
        ],
        [
            "Run factor analysis with Varimax rotation",
            "markdown"
        ],
        [
            "n_comps = 2\n\nmethods = [\n    (\"PCA\", ()),\n    (\"Unrotated FA\", ()),\n    (\"Varimax FA\", (rotation=\"varimax\")),\n]\nfig, axes = (ncols=len(methods), figsize=(10, 8), sharey=True)\n\nfor ax, (method, fa) in zip(axes, methods):\n    fa.set_params(n_components=n_comps)\n    fa.fit(X)\n\n    components = fa.components_.T\n    print(\"\\n\\n %s :\\n\" % method)\n    print(components)\n\n    vmax = np.abs(components).max()\n    ax.imshow(components, cmap=\"RdBu_r\", vmax=vmax, vmin=-vmax)\n    ax.set_yticks((len(feature_names)))\n    ax.set_yticklabels(feature_names)\n    ax.set_title(str(method))\n    ax.set_xticks([0, 1])\n    ax.set_xticklabels([\"Comp. 1\", \"Comp. 2\"])\nfig.suptitle(\"Factors\")\n()\n()\n\n\n<img alt=\"Factors, PCA, Unrotated FA, Varimax FA\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_varimax_fa_002.png\" srcset=\"../../_images/sphx_glr_plot_varimax_fa_002.png\"/>",
            "code"
        ],
        [
            "PCA :\n\n[[ 0.52106591  0.37741762]\n [-0.26934744  0.92329566]\n [ 0.5804131   0.02449161]\n [ 0.56485654  0.06694199]]\n\n\n Unrotated FA :\n\n[[ 0.88096009 -0.4472869 ]\n [-0.41691605 -0.55390036]\n [ 0.99918858  0.01915283]\n [ 0.96228895  0.05840206]]\n\n\n Varimax FA :\n\n[[ 0.98633022 -0.05752333]\n [-0.16052385 -0.67443065]\n [ 0.90809432  0.41726413]\n [ 0.85857475  0.43847489]]",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.398 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Decomposition->FastICA on 2D point clouds": [
        [
            "This example illustrates visually in the feature space a comparison by\nresults using two different component analysis techniques.",
            "markdown"
        ],
        [
            "vs .",
            "markdown"
        ],
        [
            "Representing ICA in the feature space gives the view of \u2018geometric ICA\u2019:\nICA is an algorithm that finds directions in the feature space\ncorresponding to projections with high non-Gaussianity. These directions\nneed not be orthogonal in the original feature space, but they are\northogonal in the whitened feature space, in which all directions\ncorrespond to the same variance.",
            "markdown"
        ],
        [
            "PCA, on the other hand, finds orthogonal directions in the raw feature\nspace that correspond to directions accounting for maximum variance.",
            "markdown"
        ],
        [
            "Here we simulate independent sources using a highly non-Gaussian\nprocess, 2 student T with a low number of degrees of freedom (top left\nfigure). We mix them to create observations (top right figure).\nIn this raw observation space, directions identified by PCA are\nrepresented by orange vectors. We represent the signal in the PCA space,\nafter whitening by the variance corresponding to the PCA vectors (lower\nleft). Running ICA corresponds to finding a rotation in this space to\nidentify the directions of largest non-Gaussianity (lower right).",
            "markdown"
        ],
        [
            "# Authors: Alexandre Gramfort, Gael Varoquaux\n# License: BSD 3 clause",
            "code"
        ]
    ],
    "Examples->Decomposition->FastICA on 2D point clouds->Generate sample data": [
        [
            "import numpy as np\n\nfrom sklearn.decomposition import , \n\nrng = (42)\nS = rng.standard_t(1.5, size=(20000, 2))\nS[:, 0] *= 2.0\n\n# Mix data\nA = ([[1, 1], [0, 2]])  # Mixing matrix\n\nX = (S, A.T)  # Generate observations\n\npca = ()\nS_pca_ = pca.fit(X).transform(X)\n\nica = (random_state=rng, whiten=\"arbitrary-variance\")\nS_ica_ = ica.fit(X).transform(X)  # Estimate the sources\n\nS_ica_ /= S_ica_.std(axis=0)",
            "code"
        ]
    ],
    "Examples->Decomposition->FastICA on 2D point clouds->Plot results": [
        [
            "import matplotlib.pyplot as plt\n\n\ndef plot_samples(S, axis_list=None):\n    (\n        S[:, 0], S[:, 1], s=2, marker=\"o\", zorder=10, color=\"steelblue\", alpha=0.5\n    )\n    if axis_list is not None:\n        for axis, color, label in axis_list:\n            axis /= axis.std()\n            x_axis, y_axis = axis\n            (\n                (0, 0),\n                (0, 0),\n                x_axis,\n                y_axis,\n                zorder=11,\n                width=0.01,\n                scale=6,\n                color=color,\n                label=label,\n            )\n\n    (0, -3, 3)\n    (0, -3, 3)\n    (-3, 3)\n    (-3, 3)\n    (\"x\")\n    (\"y\")\n\n\n()\n(2, 2, 1)\nplot_samples(S / S.std())\n(\"True Independent Sources\")\n\naxis_list = [(pca.components_.T, \"orange\", \"PCA\"), (ica.mixing_, \"red\", \"ICA\")]\n(2, 2, 2)\nplot_samples(X / (X), axis_list=axis_list)\nlegend = (loc=\"lower right\")\nlegend.set_zorder(100)\n\n(\"Observations\")\n\n(2, 2, 3)\nplot_samples(S_pca_ / (S_pca_, axis=0))\n(\"PCA recovered signals\")\n\n(2, 2, 4)\nplot_samples(S_ica_ / (S_ica_))\n(\"ICA recovered signals\")\n\n(0.09, 0.04, 0.94, 0.94, 0.26, 0.36)\n()\n\n\n<img alt=\"True Independent Sources, Observations, PCA recovered signals, ICA recovered signals\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_ica_vs_pca_001.png\" srcset=\"../../_images/sphx_glr_plot_ica_vs_pca_001.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.359 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Decomposition->Image denoising using dictionary learning": [
        [
            "An example comparing the effect of reconstructing noisy fragments\nof a raccoon face image using firstly online  and\nvarious transform methods.",
            "markdown"
        ],
        [
            "The dictionary is fitted on the distorted left half of the image, and\nsubsequently used to reconstruct the right half. Note that even better\nperformance could be achieved by fitting to an undistorted (i.e.\nnoiseless) image, but here we start from the assumption that it is not\navailable.",
            "markdown"
        ],
        [
            "A common practice for evaluating the results of image denoising is by looking\nat the difference between the reconstruction and the original image. If the\nreconstruction is perfect this will look like Gaussian noise.",
            "markdown"
        ],
        [
            "It can be seen from the plots that the results of  with two\nnon-zero coefficients is a bit less biased than when keeping only one\n(the edges look less prominent). It is in addition closer from the ground\ntruth in Frobenius norm.",
            "markdown"
        ],
        [
            "The result of  is much more strongly biased: the\ndifference is reminiscent of the local intensity value of the original image.",
            "markdown"
        ],
        [
            "Thresholding is clearly not useful for denoising, but it is here to show that\nit can produce a suggestive output with very high speed, and thus be useful\nfor other tasks such as object classification, where performance is not\nnecessarily related to visualisation.",
            "markdown"
        ]
    ],
    "Examples->Decomposition->Image denoising using dictionary learning->Generate distorted image": [
        [
            "import numpy as np\n\n\ntry:  # Scipy = 1.10\n    from scipy.datasets import \nexcept ImportError:\n    from scipy.misc import \n\nraccoon_face = (gray=True)\n\n# Convert from uint8 representation with values between 0 and 255 to\n# a floating point representation with values between 0 and 1.\nraccoon_face = raccoon_face / 255.0\n\n# downsample for higher speed\nraccoon_face = (\n    raccoon_face[::4, ::4]\n    + raccoon_face[1::4, ::4]\n    + raccoon_face[::4, 1::4]\n    + raccoon_face[1::4, 1::4]\n)\nraccoon_face /= 4.0\nheight, width = raccoon_face.shape\n\n# Distort the right half of the image\nprint(\"Distorting image...\")\ndistorted = raccoon_face.copy()\ndistorted[:, width // 2 :] += 0.075 * (height, width // 2)",
            "code"
        ],
        [
            "Distorting image...",
            "code"
        ]
    ],
    "Examples->Decomposition->Image denoising using dictionary learning->Display the distorted image": [
        [
            "import matplotlib.pyplot as plt\n\n\ndef show_with_diff(image, reference, title):\n    \"\"\"Helper function to display denoising\"\"\"\n    (figsize=(5, 3.3))\n    (1, 2, 1)\n    (\"Image\")\n    (image, vmin=0, vmax=1, cmap=plt.cm.gray, interpolation=\"nearest\")\n    (())\n    (())\n    (1, 2, 2)\n    difference = image - reference\n\n    (\"Difference (norm: %.2f)\" % ((difference**2)))\n    (\n        difference, vmin=-0.5, vmax=0.5, cmap=plt.cm.PuOr, interpolation=\"nearest\"\n    )\n    (())\n    (())\n    (title, size=16)\n    (0.02, 0.02, 0.98, 0.79, 0.02, 0.2)\n\n\nshow_with_diff(distorted, raccoon_face, \"Distorted image\")\n\n\n<img alt=\"Distorted image, Image, Difference (norm: 11.80)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_image_denoising_001.png\" srcset=\"../../_images/sphx_glr_plot_image_denoising_001.png\"/>",
            "code"
        ]
    ],
    "Examples->Decomposition->Image denoising using dictionary learning->Extract reference patches": [
        [
            "from time import \n\nfrom sklearn.feature_extraction.image import \n\n# Extract all reference patches from the left half of the image\nprint(\"Extracting reference patches...\")\nt0 = ()\npatch_size = (7, 7)\ndata = (distorted[:, : width // 2], patch_size)\ndata = data.reshape(data.shape[0], -1)\ndata -= (data, axis=0)\ndata /= (data, axis=0)\nprint(f\"{data.shape[0]} patches extracted in %.2fs.\" % (() - t0))",
            "code"
        ],
        [
            "Extracting reference patches...\n22692 patches extracted in 0.01s.",
            "code"
        ]
    ],
    "Examples->Decomposition->Image denoising using dictionary learning->Learn the dictionary from reference patches": [
        [
            "from sklearn.decomposition import \n\nprint(\"Learning the dictionary...\")\nt0 = ()\ndico = (\n    # increase to 300 for higher quality results at the cost of slower\n    # training times.\n    n_components=50,\n    batch_size=200,\n    alpha=1.0,\n    max_iter=10,\n)\nV = dico.fit(data).components_\ndt = () - t0\nprint(f\"{dico.n_iter_} iterations / {dico.n_steps_} steps in {dt:.2f}.\")\n\n(figsize=(4.2, 4))\nfor i, comp in enumerate(V[:100]):\n    (10, 10, i + 1)\n    (comp.reshape(patch_size), cmap=plt.cm.gray_r, interpolation=\"nearest\")\n    (())\n    (())\n(\n    \"Dictionary learned from face patches\\n\"\n    + \"Train time %.1fs on %d patches\" % (dt, len(data)),\n    fontsize=16,\n)\n(0.08, 0.02, 0.92, 0.85, 0.08, 0.23)\n\n\n<img alt=\"Dictionary learned from face patches Train time 16.0s on 22692 patches\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_image_denoising_002.png\" srcset=\"../../_images/sphx_glr_plot_image_denoising_002.png\"/>",
            "code"
        ],
        [
            "Learning the dictionary...\n1.0 iterations / 109 steps in 16.00.",
            "code"
        ]
    ],
    "Examples->Decomposition->Image denoising using dictionary learning->Extract noisy patches and reconstruct them using the dictionary": [
        [
            "from sklearn.feature_extraction.image import \n\nprint(\"Extracting noisy patches... \")\nt0 = ()\ndata = (distorted[:, width // 2 :], patch_size)\ndata = data.reshape(data.shape[0], -1)\nintercept = (data, axis=0)\ndata -= intercept\nprint(\"done in %.2fs.\" % (() - t0))\n\ntransform_algorithms = [\n    (\"Orthogonal Matching Pursuit\\n1 atom\", \"omp\", {\"transform_n_nonzero_coefs\": 1}),\n    (\"Orthogonal Matching Pursuit\\n2 atoms\", \"omp\", {\"transform_n_nonzero_coefs\": 2}),\n    (\"Least-angle regression\\n4 atoms\", \"lars\", {\"transform_n_nonzero_coefs\": 4}),\n    (\"Thresholding\\n alpha=0.1\", \"threshold\", {\"transform_alpha\": 0.1}),\n]\n\nreconstructions = {}\nfor title, transform_algorithm, kwargs in transform_algorithms:\n    print(title + \"...\")\n    reconstructions[title] = raccoon_face.copy()\n    t0 = ()\n    dico.set_params(transform_algorithm=transform_algorithm, **kwargs)\n    code = dico.transform(data)\n    patches = (code, V)\n\n    patches += intercept\n    patches = patches.reshape(len(data), *patch_size)\n    if transform_algorithm == \"threshold\":\n        patches -= patches.min()\n        patches /= patches.max()\n    reconstructions[title][:, width // 2 :] = (\n        patches, (height, width // 2)\n    )\n    dt = () - t0\n    print(\"done in %.2fs.\" % dt)\n    show_with_diff(reconstructions[title], raccoon_face, title + \" (time: %.1fs)\" % dt)\n\n()\n\n\n\n<img alt=\"Orthogonal Matching Pursuit 1 atom (time: 0.7s), Image, Difference (norm: 10.69)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_image_denoising_003.png\" srcset=\"../../_images/sphx_glr_plot_image_denoising_003.png\"/>\n<img alt=\"Orthogonal Matching Pursuit 2 atoms (time: 1.5s), Image, Difference (norm: 9.29)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_image_denoising_004.png\" srcset=\"../../_images/sphx_glr_plot_image_denoising_004.png\"/>\n<img alt=\"Least-angle regression 4 atoms (time: 10.3s), Image, Difference (norm: 13.52)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_image_denoising_005.png\" srcset=\"../../_images/sphx_glr_plot_image_denoising_005.png\"/>\n<img alt=\"Thresholding  alpha=0.1 (time: 0.1s), Image, Difference (norm: 14.15)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_image_denoising_006.png\" srcset=\"../../_images/sphx_glr_plot_image_denoising_006.png\"/>",
            "code"
        ],
        [
            "Extracting noisy patches...\ndone in 0.00s.\nOrthogonal Matching Pursuit\n1 atom...\ndone in 0.74s.\nOrthogonal Matching Pursuit\n2 atoms...\ndone in 1.51s.\nLeast-angle regression\n4 atoms...\ndone in 10.35s.\nThresholding\n alpha=0.1...\ndone in 0.10s.",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  30.243 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Decomposition->Incremental PCA": [
        [
            "Incremental principal component analysis (IPCA) is typically used as a\nreplacement for principal component analysis (PCA) when the dataset to be\ndecomposed is too large to fit in memory. IPCA builds a low-rank approximation\nfor the input data using an amount of memory which is independent of the\nnumber of input data samples. It is still dependent on the input data features,\nbut changing the batch size allows for control of memory usage.",
            "markdown"
        ],
        [
            "This example serves as a visual check that IPCA is able to find a similar\nprojection of the data to PCA (to a sign flip), while only processing a\nfew samples at a time. This can be considered a \u201ctoy example\u201d, as IPCA is\nintended for large datasets which do not fit in main memory, requiring\nincremental approaches.\n\n<img alt=\"Incremental PCA of iris dataset Mean absolute unsigned error 0.002201\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_incremental_pca_001.png\" srcset=\"../../_images/sphx_glr_plot_incremental_pca_001.png\"/>\n<img alt=\"PCA of iris dataset\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_incremental_pca_002.png\" srcset=\"../../_images/sphx_glr_plot_incremental_pca_002.png\"/>",
            "markdown"
        ],
        [
            "# Authors: Kyle Kastner\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import \nfrom sklearn.decomposition import , \n\niris = ()\nX = iris.data\ny = iris.target\n\nn_components = 2\nipca = (n_components=n_components, batch_size=10)\nX_ipca = ipca.fit_transform(X)\n\npca = (n_components=n_components)\nX_pca = pca.fit_transform(X)\n\ncolors = [\"navy\", \"turquoise\", \"darkorange\"]\n\nfor X_transformed, title in [(X_ipca, \"Incremental PCA\"), (X_pca, \"PCA\")]:\n    (figsize=(8, 8))\n    for color, i, target_name in zip(colors, [0, 1, 2], iris.target_names):\n        (\n            X_transformed[y == i, 0],\n            X_transformed[y == i, 1],\n            color=color,\n            lw=2,\n            label=target_name,\n        )\n\n    if \"Incremental\" in title:\n        err = np.abs(np.abs(X_pca) - np.abs(X_ipca)).mean()\n        (title + \" of iris dataset\\nMean absolute unsigned error %.6f\" % err)\n    else:\n        (title + \" of iris dataset\")\n    (loc=\"best\", shadow=False, scatterpoints=1)\n    ([-4, 4, -1.5, 1.5])\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.286 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Decomposition->Kernel PCA": [
        [
            "This example shows the difference between the Principal Components Analysis\n() and its kernalized version\n().",
            "markdown"
        ],
        [
            "On the one hand, we show that  is able\nto find a projection of the data which linearly separates them while it is not the case\nwith .",
            "markdown"
        ],
        [
            "Finally, we show that inverting this projection is an approximation with\n, while it is exact with\n.",
            "markdown"
        ],
        [
            "# Authors: Mathieu Blondel\n#          Andreas Mueller\n#          Guillaume Lemaitre\n# License: BSD 3 clause",
            "code"
        ]
    ],
    "Examples->Decomposition->Kernel PCA->Projecting data: PCA vs. KernelPCA": [
        [
            "In this section, we show the advantages of using a kernel when\nprojecting data using a Principal Component Analysis (PCA). We create a\ndataset made of two nested circles.",
            "markdown"
        ],
        [
            "from sklearn.datasets import \nfrom sklearn.model_selection import \n\nX, y = (n_samples=1_000, factor=0.3, noise=0.05, random_state=0)\nX_train, X_test, y_train, y_test = (X, y, stratify=y, random_state=0)",
            "code"
        ],
        [
            "Let\u2019s have a quick first look at the generated dataset.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n\n_, (train_ax, test_ax) = (ncols=2, sharex=True, sharey=True, figsize=(8, 4))\n\ntrain_ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train)\ntrain_ax.set_ylabel(\"Feature #1\")\ntrain_ax.set_xlabel(\"Feature #0\")\ntrain_ax.set_title(\"Training data\")\n\ntest_ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test)\ntest_ax.set_xlabel(\"Feature #0\")\n_ = test_ax.set_title(\"Testing data\")\n\n\n<img alt=\"Training data, Testing data\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_kernel_pca_001.png\" srcset=\"../../_images/sphx_glr_plot_kernel_pca_001.png\"/>",
            "code"
        ],
        [
            "The samples from each class cannot be linearly separated: there is no\nstraight line that can split the samples of the inner set from the outer\nset.",
            "markdown"
        ],
        [
            "Now, we will use PCA with and without a kernel to see what is the effect of\nusing such a kernel. The kernel used here is a radial basis function (RBF)\nkernel.",
            "markdown"
        ],
        [
            "from sklearn.decomposition import , \n\npca = (n_components=2)\nkernel_pca = (\n    n_components=None, kernel=\"rbf\", gamma=10, fit_inverse_transform=True, alpha=0.1\n)\n\nX_test_pca = pca.fit(X_train).transform(X_test)\nX_test_kernel_pca = kernel_pca.fit(X_train).transform(X_test)",
            "code"
        ],
        [
            "fig, (orig_data_ax, pca_proj_ax, kernel_pca_proj_ax) = (\n    ncols=3, figsize=(14, 4)\n)\n\norig_data_ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test)\norig_data_ax.set_ylabel(\"Feature #1\")\norig_data_ax.set_xlabel(\"Feature #0\")\norig_data_ax.set_title(\"Testing data\")\n\npca_proj_ax.scatter(X_test_pca[:, 0], X_test_pca[:, 1], c=y_test)\npca_proj_ax.set_ylabel(\"Principal component #1\")\npca_proj_ax.set_xlabel(\"Principal component #0\")\npca_proj_ax.set_title(\"Projection of testing data\\n using PCA\")\n\nkernel_pca_proj_ax.scatter(X_test_kernel_pca[:, 0], X_test_kernel_pca[:, 1], c=y_test)\nkernel_pca_proj_ax.set_ylabel(\"Principal component #1\")\nkernel_pca_proj_ax.set_xlabel(\"Principal component #0\")\n_ = kernel_pca_proj_ax.set_title(\"Projection of testing data\\n using KernelPCA\")\n\n\n<img alt=\"Testing data, Projection of testing data  using PCA, Projection of testing data  using KernelPCA\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_kernel_pca_002.png\" srcset=\"../../_images/sphx_glr_plot_kernel_pca_002.png\"/>",
            "code"
        ],
        [
            "We recall that PCA transforms the data linearly. Intuitively, it means that\nthe coordinate system will be centered, rescaled on each component\nwith respected to its variance and finally be rotated.\nThe obtained data from this transformation is isotropic and can now be\nprojected on its principal components.",
            "markdown"
        ],
        [
            "Thus, looking at the projection made using PCA (i.e. the middle figure), we\nsee that there is no change regarding the scaling; indeed the data being two\nconcentric circles centered in zero, the original data is already isotropic.\nHowever, we can see that the data have been rotated. As a\nconclusion, we see that such a projection would not help if define a linear\nclassifier to distinguish samples from both classes.",
            "markdown"
        ],
        [
            "Using a kernel allows to make a non-linear projection. Here, by using an RBF\nkernel, we expect that the projection will unfold the dataset while keeping\napproximately preserving the relative distances of pairs of data points that\nare close to one another in the original space.",
            "markdown"
        ],
        [
            "We observe such behaviour in the figure on the right: the samples of a given\nclass are closer to each other than the samples from the opposite class,\nuntangling both sample sets. Now, we can use a linear classifier to separate\nthe samples from the two classes.",
            "markdown"
        ]
    ],
    "Examples->Decomposition->Kernel PCA->Projecting into the original feature space": [
        [
            "One particularity to have in mind when using\n is related to the reconstruction\n(i.e. the back projection in the original feature space). With\n, the reconstruction will be exact if\nn_components is the same than the number of original features.\nThis is the case in this example.",
            "markdown"
        ],
        [
            "We can investigate if we get the original dataset when back projecting with\n.",
            "markdown"
        ],
        [
            "X_reconstructed_pca = pca.inverse_transform(pca.transform(X_test))\nX_reconstructed_kernel_pca = kernel_pca.inverse_transform(kernel_pca.transform(X_test))",
            "code"
        ],
        [
            "fig, (orig_data_ax, pca_back_proj_ax, kernel_pca_back_proj_ax) = (\n    ncols=3, sharex=True, sharey=True, figsize=(13, 4)\n)\n\norig_data_ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test)\norig_data_ax.set_ylabel(\"Feature #1\")\norig_data_ax.set_xlabel(\"Feature #0\")\norig_data_ax.set_title(\"Original test data\")\n\npca_back_proj_ax.scatter(X_reconstructed_pca[:, 0], X_reconstructed_pca[:, 1], c=y_test)\npca_back_proj_ax.set_xlabel(\"Feature #0\")\npca_back_proj_ax.set_title(\"Reconstruction via PCA\")\n\nkernel_pca_back_proj_ax.scatter(\n    X_reconstructed_kernel_pca[:, 0], X_reconstructed_kernel_pca[:, 1], c=y_test\n)\nkernel_pca_back_proj_ax.set_xlabel(\"Feature #0\")\n_ = kernel_pca_back_proj_ax.set_title(\"Reconstruction via KernelPCA\")\n\n\n<img alt=\"Original test data, Reconstruction via PCA, Reconstruction via KernelPCA\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_kernel_pca_003.png\" srcset=\"../../_images/sphx_glr_plot_kernel_pca_003.png\"/>",
            "code"
        ],
        [
            "While we see a perfect reconstruction with\n we observe a different result for\n.",
            "markdown"
        ],
        [
            "Indeed,  cannot\nrely on an analytical back-projection and thus an exact reconstruction.\nInstead, a  is internally trained\nto learn a mapping from the kernalized PCA basis to the original feature\nspace. This method therefore comes with an approximation introducing small\ndifferences when back projecting in the original feature space.",
            "markdown"
        ],
        [
            "To improve the reconstruction using\n, one can tune\nalpha in , the regularization term\nwhich controls the reliance on the training data during the training of\nthe mapping.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.672 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Decomposition->Model selection with Probabilistic PCA and Factor Analysis (FA)": [
        [
            "Probabilistic PCA and Factor Analysis are probabilistic models.\nThe consequence is that the likelihood of new data can be used\nfor model selection and covariance estimation.\nHere we compare PCA and FA with cross-validation on low rank data corrupted\nwith homoscedastic noise (noise variance\nis the same for each feature) or heteroscedastic noise (noise variance\nis the different for each feature). In a second step we compare the model\nlikelihood to the likelihoods obtained from shrinkage covariance estimators.",
            "markdown"
        ],
        [
            "One can observe that with homoscedastic noise both FA and PCA succeed\nin recovering the size of the low rank subspace. The likelihood with PCA\nis higher than FA in this case. However PCA fails and overestimates\nthe rank when heteroscedastic noise is present. Under appropriate\ncircumstances (choice of the number of components), the held-out\ndata is more likely for low rank models than for shrinkage models.",
            "markdown"
        ],
        [
            "The automatic estimation from\nAutomatic Choice of Dimensionality for PCA. NIPS 2000: 598-604\nby Thomas P. Minka is also compared.",
            "markdown"
        ],
        [
            "# Authors: Alexandre Gramfort\n#          Denis A. Engemann\n# License: BSD 3 clause",
            "code"
        ]
    ],
    "Examples->Decomposition->Model selection with Probabilistic PCA and Factor Analysis (FA)->Create the data": [
        [
            "import numpy as np\n\nfrom scipy import linalg\n\nn_samples, n_features, rank = 500, 25, 5\nsigma = 1.0\nrng = (42)\nU, _, _ = (rng.randn(n_features, n_features))\nX = (rng.randn(n_samples, rank), U[:, :rank].T)\n\n# Adding homoscedastic noise\nX_homo = X + sigma * rng.randn(n_samples, n_features)\n\n# Adding heteroscedastic noise\nsigmas = sigma * rng.rand(n_features) + sigma / 2.0\nX_hetero = X + rng.randn(n_samples, n_features) * sigmas",
            "code"
        ]
    ],
    "Examples->Decomposition->Model selection with Probabilistic PCA and Factor Analysis (FA)->Fit the models": [
        [
            "import matplotlib.pyplot as plt\n\nfrom sklearn.decomposition import , \nfrom sklearn.covariance import , \nfrom sklearn.model_selection import \nfrom sklearn.model_selection import \n\nn_components = (0, n_features, 5)  # options for n_components\n\n\ndef compute_scores(X):\n    pca = (svd_solver=\"full\")\n    fa = ()\n\n    pca_scores, fa_scores = [], []\n    for n in n_components:\n        pca.n_components = n\n        fa.n_components = n\n        pca_scores.append(((pca, X)))\n        fa_scores.append(((fa, X)))\n\n    return pca_scores, fa_scores\n\n\ndef shrunk_cov_score(X):\n    shrinkages = (-2, 0, 30)\n    cv = ((), {\"shrinkage\": shrinkages})\n    return ((cv.fit(X).best_estimator_, X))\n\n\ndef lw_score(X):\n    return (((), X))\n\n\nfor X, title in [(X_homo, \"Homoscedastic Noise\"), (X_hetero, \"Heteroscedastic Noise\")]:\n    pca_scores, fa_scores = compute_scores(X)\n    n_components_pca = n_components[(pca_scores)]\n    n_components_fa = n_components[(fa_scores)]\n\n    pca = (svd_solver=\"full\", n_components=\"mle\")\n    pca.fit(X)\n    n_components_pca_mle = pca.n_components_\n\n    print(\"best n_components by PCA CV = %d\" % n_components_pca)\n    print(\"best n_components by FactorAnalysis CV = %d\" % n_components_fa)\n    print(\"best n_components by PCA MLE = %d\" % n_components_pca_mle)\n\n    ()\n    (n_components, pca_scores, \"b\", label=\"PCA scores\")\n    (n_components, fa_scores, \"r\", label=\"FA scores\")\n    (rank, color=\"g\", label=\"TRUTH: %d\" % rank, linestyle=\"-\")\n    (\n        n_components_pca,\n        color=\"b\",\n        label=\"PCA CV: %d\" % n_components_pca,\n        linestyle=\"--\",\n    )\n    (\n        n_components_fa,\n        color=\"r\",\n        label=\"FactorAnalysis CV: %d\" % n_components_fa,\n        linestyle=\"--\",\n    )\n    (\n        n_components_pca_mle,\n        color=\"k\",\n        label=\"PCA MLE: %d\" % n_components_pca_mle,\n        linestyle=\"--\",\n    )\n\n    # compare with other covariance estimators\n    (\n        shrunk_cov_score(X),\n        color=\"violet\",\n        label=\"Shrunk Covariance MLE\",\n        linestyle=\"-.\",\n    )\n    (\n        lw_score(X),\n        color=\"orange\",\n        label=\"LedoitWolf MLE\" % n_components_pca_mle,\n        linestyle=\"-.\",\n    )\n\n    (\"nb of components\")\n    (\"CV scores\")\n    (loc=\"lower right\")\n    (title)\n\n()\n\n\n\n<img alt=\"Homoscedastic Noise\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_pca_vs_fa_model_selection_001.png\" srcset=\"../../_images/sphx_glr_plot_pca_vs_fa_model_selection_001.png\"/>\n<img alt=\"Heteroscedastic Noise\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_pca_vs_fa_model_selection_002.png\" srcset=\"../../_images/sphx_glr_plot_pca_vs_fa_model_selection_002.png\"/>",
            "code"
        ],
        [
            "best n_components by PCA CV = 5\nbest n_components by FactorAnalysis CV = 5\nbest n_components by PCA MLE = 5\nbest n_components by PCA CV = 20\nbest n_components by FactorAnalysis CV = 5\nbest n_components by PCA MLE = 18",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  3.116 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Decomposition->PCA example with Iris Data-set": [
        [
            "Principal Component Analysis applied to the Iris dataset.",
            "markdown"
        ],
        [
            "See  for more\ninformation on this dataset.\n<img alt=\"plot pca iris\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_pca_iris_001.png\" srcset=\"../../_images/sphx_glr_plot_pca_iris_001.png\"/>",
            "markdown"
        ],
        [
            "# Code source: Ga\u00ebl Varoquaux\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn import decomposition\nfrom sklearn import datasets\n\n# unused but required import for doing 3d projections with matplotlib &lt; 3.2\nimport mpl_toolkits.mplot3d  # noqa: F401\n\n(5)\n\niris = ()\nX = iris.data\ny = iris.target\n\nfig = (1, figsize=(4, 3))\n()\n\nax = fig.add_subplot(111, projection=\"3d\", elev=48, azim=134)\nax.set_position([0, 0, 0.95, 1])\n\n\n()\npca = (n_components=3)\npca.fit(X)\nX = pca.transform(X)\n\nfor name, label in [(\"Setosa\", 0), (\"Versicolour\", 1), (\"Virginica\", 2)]:\n    ax.text3D(\n        X[y == label, 0].mean(),\n        X[y == label, 1].mean() + 1.5,\n        X[y == label, 2].mean(),\n        name,\n        horizontalalignment=\"center\",\n        bbox=dict(alpha=0.5, edgecolor=\"w\", facecolor=\"w\"),\n    )\n# Reorder the labels to have colors matching the cluster results\ny = (y, [1, 2, 0]).astype(float)\nax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=plt.cm.nipy_spectral, edgecolor=\"k\")\n\nax.xaxis.set_ticklabels([])\nax.yaxis.set_ticklabels([])\nax.zaxis.set_ticklabels([])\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.086 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Decomposition->Principal components analysis (PCA)": [
        [
            "These figures aid in illustrating how a point cloud\ncan be very flat in one direction\u2013which is where PCA\ncomes in to choose a direction that is not flat.",
            "markdown"
        ],
        [
            "# Authors: Gael Varoquaux\n#          Jaques Grobler\n#          Kevin Hughes\n# License: BSD 3 clause",
            "code"
        ]
    ],
    "Examples->Decomposition->Principal components analysis (PCA)->Create the data": [
        [
            "import numpy as np\n\nfrom scipy import stats\n\ne = (1)\n(4)\n\n\ndef pdf(x):\n    return 0.5 * ((scale=0.25 / e).pdf(x) + (scale=4 / e).pdf(x))\n\n\ny = (scale=0.5, size=(30000))\nx = (scale=0.5, size=(30000))\nz = (scale=0.1, size=len(x))\n\ndensity = pdf(x) * pdf(y)\npdf_z = pdf(5 * z)\n\ndensity *= pdf_z\n\na = x + y\nb = 2 * y\nc = a - b + z\n\nnorm = (a.var() + b.var())\na /= norm\nb /= norm",
            "code"
        ]
    ],
    "Examples->Decomposition->Principal components analysis (PCA)->Plot the figures": [
        [
            "from sklearn.decomposition import \n\nimport matplotlib.pyplot as plt\n\n# unused but required import for doing 3d projections with matplotlib &lt; 3.2\nimport mpl_toolkits.mplot3d  # noqa: F401\n\n\ndef plot_figs(fig_num, elev, azim):\n    fig = (fig_num, figsize=(4, 3))\n    ()\n    ax = fig.add_subplot(111, projection=\"3d\", elev=elev, azim=azim)\n    ax.set_position([0, 0, 0.95, 1])\n\n    ax.scatter(a[::10], b[::10], c[::10], c=density[::10], marker=\"+\", alpha=0.4)\n    Y = [a, b, c]\n\n    # Using SciPy's SVD, this would be:\n    # _, pca_score, Vt = scipy.linalg.svd(Y, full_matrices=False)\n\n    pca = (n_components=3)\n    pca.fit(Y)\n    V = pca.components_.T\n\n    x_pca_axis, y_pca_axis, z_pca_axis = 3 * V\n    x_pca_plane = [x_pca_axis[:2], -x_pca_axis[1::-1]]\n    y_pca_plane = [y_pca_axis[:2], -y_pca_axis[1::-1]]\n    z_pca_plane = [z_pca_axis[:2], -z_pca_axis[1::-1]]\n    x_pca_plane.shape = (2, 2)\n    y_pca_plane.shape = (2, 2)\n    z_pca_plane.shape = (2, 2)\n    ax.plot_surface(x_pca_plane, y_pca_plane, z_pca_plane)\n    ax.xaxis.set_ticklabels([])\n    ax.yaxis.set_ticklabels([])\n    ax.zaxis.set_ticklabels([])\n\n\nelev = -40\nazim = -80\nplot_figs(1, elev, azim)\n\nelev = 30\nazim = 20\nplot_figs(2, elev, azim)\n\n()\n\n\n\n<img alt=\"plot pca 3d\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_pca_3d_001.png\" srcset=\"../../_images/sphx_glr_plot_pca_3d_001.png\"/>\n<img alt=\"plot pca 3d\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_pca_3d_002.png\" srcset=\"../../_images/sphx_glr_plot_pca_3d_002.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.176 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Decomposition->Sparse coding with a precomputed dictionary": [
        [
            "Transform a signal as a sparse combination of Ricker wavelets. This example\nvisually compares different sparse coding methods using the\n estimator. The Ricker (also known\nas Mexican hat or the second derivative of a Gaussian) is not a particularly\ngood kernel to represent piecewise constant signals like this one. It can\ntherefore be seen how much adding different widths of atoms matters and it\ntherefore motivates learning the dictionary to best fit your type of signals.",
            "markdown"
        ],
        [
            "The richer dictionary on the right is not larger in size, heavier subsampling\nis performed in order to stay on the same order of magnitude.\n<img alt=\"Sparse coding against fixed width dictionary, Sparse coding against multiple widths dictionary\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_sparse_coding_001.png\" srcset=\"../../_images/sphx_glr_plot_sparse_coding_001.png\"/>",
            "markdown"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.decomposition import \n\n\ndef ricker_function(resolution, center, width):\n    \"\"\"Discrete sub-sampled Ricker (Mexican hat) wavelet\"\"\"\n    x = (0, resolution - 1, resolution)\n    x = (\n        (2 / ((3 * width) * **0.25))\n        * (1 - (x - center) ** 2 / width**2)\n        * (-((x - center) ** 2) / (2 * width**2))\n    )\n    return x\n\n\ndef ricker_matrix(width, resolution, n_components):\n    \"\"\"Dictionary of Ricker (Mexican hat) wavelets\"\"\"\n    centers = (0, resolution - 1, n_components)\n    D = ((n_components, resolution))\n    for i, center in enumerate(centers):\n        D[i] = ricker_function(resolution, center, width)\n    D /= ((D**2, axis=1))[:, ]\n    return D\n\n\nresolution = 1024\nsubsampling = 3  # subsampling factor\nwidth = 100\nn_components = resolution // subsampling\n\n# Compute a wavelet dictionary\nD_fixed = ricker_matrix(width=width, resolution=resolution, n_components=n_components)\nD_multi = [\n    tuple(\n        ricker_matrix(width=w, resolution=resolution, n_components=n_components // 5)\n        for w in (10, 50, 100, 500, 1000)\n    )\n]\n\n# Generate a signal\ny = (0, resolution - 1, resolution)\nfirst_quarter = y &lt; resolution / 4\ny[first_quarter] = 3.0\ny[(first_quarter)] = -1.0\n\n# List the different sparse coding methods in the following format:\n# (title, transform_algorithm, transform_alpha,\n#  transform_n_nozero_coefs, color)\nestimators = [\n    (\"OMP\", \"omp\", None, 15, \"navy\"),\n    (\"Lasso\", \"lasso_lars\", 2, None, \"turquoise\"),\n]\nlw = 2\n\n(figsize=(13, 6))\nfor subplot, (D, title) in enumerate(\n    zip((D_fixed, D_multi), (\"fixed width\", \"multiple widths\"))\n):\n    (1, 2, subplot + 1)\n    (\"Sparse coding against %s dictionary\" % title)\n    (y, lw=lw, linestyle=\"--\", label=\"Original signal\")\n    # Do a wavelet approximation\n    for title, algo, alpha, n_nonzero, color in estimators:\n        coder = (\n            dictionary=D,\n            transform_n_nonzero_coefs=n_nonzero,\n            transform_alpha=alpha,\n            transform_algorithm=algo,\n        )\n        x = coder.transform(y.reshape(1, -1))\n        density = len((x))\n        x = ((x, D))\n        squared_error = ((y - x) ** 2)\n        (\n            x,\n            color=color,\n            lw=lw,\n            label=\"%s: %s nonzero coefs,\\n%.2f error\" % (title, density, squared_error),\n        )\n\n    # Soft thresholding debiasing\n    coder = (\n        dictionary=D, transform_algorithm=\"threshold\", transform_alpha=20\n    )\n    x = coder.transform(y.reshape(1, -1))\n    _, idx = (x != 0)\n    x[0, idx], _, _, _ = (D[idx, :].T, y, rcond=None)\n    x = ((x, D))\n    squared_error = ((y - x) ** 2)\n    (\n        x,\n        color=\"darkorange\",\n        lw=lw,\n        label=\"Thresholding w/ debiasing:\\n%d nonzero coefs, %.2f error\"\n        % (len(idx), squared_error),\n    )\n    (\"tight\")\n    (shadow=False, loc=\"best\")\n(0.04, 0.07, 0.97, 0.90, 0.09, 0.2)\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.309 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Ensemble methods->Categorical Feature Support in Gradient Boosting": [
        [
            "In this example, we will compare the training times and prediction\nperformances of  with\ndifferent encoding strategies for categorical features. In\nparticular, we will evaluate:",
            "markdown"
        ],
        [
            "dropping the categorical features",
            "markdown"
        ],
        [
            "using a",
            "markdown"
        ],
        [
            "using an  and treat categories as\nordered, equidistant quantities",
            "markdown"
        ],
        [
            "using an  and rely on the  of the\n estimator.",
            "markdown"
        ],
        [
            "We will work with the Ames Lowa Housing dataset which consists of numerical\nand categorical features, where the houses\u2019 sales prices is the target.",
            "markdown"
        ]
    ],
    "Examples->Ensemble methods->Categorical Feature Support in Gradient Boosting->Load Ames Housing dataset": [
        [
            "First, we load the Ames Housing data as a pandas dataframe. The features\nare either categorical or numerical:",
            "markdown"
        ],
        [
            "from sklearn.datasets import \n\nX, y = (data_id=42165, as_frame=True, return_X_y=True, parser=\"pandas\")\n\n# Select only a subset of features of X to make the example faster to run\ncategorical_columns_subset = [\n    \"BldgType\",\n    \"GarageFinish\",\n    \"LotConfig\",\n    \"Functional\",\n    \"MasVnrType\",\n    \"HouseStyle\",\n    \"FireplaceQu\",\n    \"ExterCond\",\n    \"ExterQual\",\n    \"PoolQC\",\n]\n\nnumerical_columns_subset = [\n    \"3SsnPorch\",\n    \"Fireplaces\",\n    \"BsmtHalfBath\",\n    \"HalfBath\",\n    \"GarageCars\",\n    \"TotRmsAbvGrd\",\n    \"BsmtFinSF1\",\n    \"BsmtFinSF2\",\n    \"GrLivArea\",\n    \"ScreenPorch\",\n]\n\nX = X[categorical_columns_subset + numerical_columns_subset]\nX[categorical_columns_subset] = X[categorical_columns_subset].astype(\"category\")\n\ncategorical_columns = X.select_dtypes(include=\"category\").columns\nn_categorical_features = len(categorical_columns)\nn_numerical_features = X.select_dtypes(include=\"number\").shape[1]\n\nprint(f\"Number of samples: {X.shape[0]}\")\nprint(f\"Number of features: {X.shape[1]}\")\nprint(f\"Number of categorical features: {n_categorical_features}\")\nprint(f\"Number of numerical features: {n_numerical_features}\")",
            "code"
        ],
        [
            "Number of samples: 1460\nNumber of features: 20\nNumber of categorical features: 10\nNumber of numerical features: 10",
            "code"
        ]
    ],
    "Examples->Ensemble methods->Categorical Feature Support in Gradient Boosting->Gradient boosting estimator with dropped categorical features": [
        [
            "As a baseline, we create an estimator where the categorical features are\ndropped:",
            "markdown"
        ],
        [
            "from sklearn.ensemble import \nfrom sklearn.pipeline import \nfrom sklearn.compose import \nfrom sklearn.compose import \n\ndropper = (\n    (\"drop\", (dtype_include=\"category\")), remainder=\"passthrough\"\n)\nhist_dropped = (dropper, (random_state=42))",
            "code"
        ]
    ],
    "Examples->Ensemble methods->Categorical Feature Support in Gradient Boosting->Gradient boosting estimator with one-hot encoding": [
        [
            "Next, we create a pipeline that will one-hot encode the categorical features\nand let the rest of the numerical data to passthrough:",
            "markdown"
        ],
        [
            "from sklearn.preprocessing import \n\none_hot_encoder = (\n    (\n        (sparse_output=False, handle_unknown=\"ignore\"),\n        (dtype_include=\"category\"),\n    ),\n    remainder=\"passthrough\",\n)\n\nhist_one_hot = (\n    one_hot_encoder, (random_state=42)\n)",
            "code"
        ]
    ],
    "Examples->Ensemble methods->Categorical Feature Support in Gradient Boosting->Gradient boosting estimator with ordinal encoding": [
        [
            "Next, we create a pipeline that will treat categorical features as if they\nwere ordered quantities, i.e. the categories will be encoded as 0, 1, 2,\netc., and treated as continuous features.",
            "markdown"
        ],
        [
            "from sklearn.preprocessing import \nimport numpy as np\n\nordinal_encoder = (\n    (\n        (handle_unknown=\"use_encoded_value\", unknown_value=),\n        (dtype_include=\"category\"),\n    ),\n    remainder=\"passthrough\",\n    # Use short feature names to make it easier to specify the categorical\n    # variables in the HistGradientBoostingRegressor in the next step\n    # of the pipeline.\n    verbose_feature_names_out=False,\n)\n\nhist_ordinal = (\n    ordinal_encoder, (random_state=42)\n)",
            "code"
        ]
    ],
    "Examples->Ensemble methods->Categorical Feature Support in Gradient Boosting->Gradient boosting estimator with native categorical support": [
        [
            "We now create a  estimator\nthat will natively handle categorical features. This estimator will not treat\ncategorical features as ordered quantities.",
            "markdown"
        ],
        [
            "Since the  requires category\nvalues to be encoded in [0, n_unique_categories - 1], we still rely on an\n to pre-process the data.",
            "markdown"
        ],
        [
            "The main difference between this pipeline and the previous one is that in\nthis one, we let the  know\nwhich features are categorical.",
            "markdown"
        ],
        [
            "# The ordinal encoder will first output the categorical features, and then the\n# continuous (passed-through) features\n\nhist_native = (\n    ordinal_encoder,\n    (\n        random_state=42,\n        categorical_features=categorical_columns,\n    ),\n).set_output(transform=\"pandas\")",
            "code"
        ]
    ],
    "Examples->Ensemble methods->Categorical Feature Support in Gradient Boosting->Model comparison": [
        [
            "Finally, we evaluate the models using cross validation. Here we compare the\nmodels performance in terms of\n and fit times.",
            "markdown"
        ],
        [
            "from sklearn.model_selection import \nimport matplotlib.pyplot as plt\n\nscoring = \"neg_mean_absolute_percentage_error\"\nn_cv_folds = 3\n\ndropped_result = (hist_dropped, X, y, cv=n_cv_folds, scoring=scoring)\none_hot_result = (hist_one_hot, X, y, cv=n_cv_folds, scoring=scoring)\nordinal_result = (hist_ordinal, X, y, cv=n_cv_folds, scoring=scoring)\nnative_result = (hist_native, X, y, cv=n_cv_folds, scoring=scoring)\n\n\ndef plot_results(figure_title):\n    fig, (ax1, ax2) = (1, 2, figsize=(12, 8))\n\n    plot_info = [\n        (\"fit_time\", \"Fit times (s)\", ax1, None),\n        (\"test_score\", \"Mean Absolute Percentage Error\", ax2, None),\n    ]\n\n    x, width = (4), 0.9\n    for key, title, ax, y_limit in plot_info:\n        items = [\n            dropped_result[key],\n            one_hot_result[key],\n            ordinal_result[key],\n            native_result[key],\n        ]\n\n        mape_cv_mean = [(np.abs(item)) for item in items]\n        mape_cv_std = [(item) for item in items]\n\n        ax.bar(\n            x=x,\n            height=mape_cv_mean,\n            width=width,\n            yerr=mape_cv_std,\n            color=[\"C0\", \"C1\", \"C2\", \"C3\"],\n        )\n        ax.set(\n            xlabel=\"Model\",\n            title=title,\n            xticks=x,\n            xticklabels=[\"Dropped\", \"One Hot\", \"Ordinal\", \"Native\"],\n            ylim=y_limit,\n        )\n    fig.suptitle(figure_title)\n\n\nplot_results(\"Gradient Boosting on Ames Housing\")\n\n\n<img alt=\"Gradient Boosting on Ames Housing, Fit times (s), Mean Absolute Percentage Error\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_001.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_001.png\"/>",
            "code"
        ],
        [
            "We see that the model with one-hot-encoded data is by far the slowest. This\nis to be expected, since one-hot-encoding creates one additional feature per\ncategory value (for each categorical feature), and thus more split points\nneed to be considered during fitting. In theory, we expect the native\nhandling of categorical features to be slightly slower than treating\ncategories as ordered quantities (\u2018Ordinal\u2019), since native handling requires\n. Fitting times should\nhowever be close when the number of categories is small, and this may not\nalways be reflected in practice.",
            "markdown"
        ],
        [
            "In terms of prediction performance, dropping the categorical features leads\nto poorer performance. The three models that use categorical features have\ncomparable error rates, with a slight edge for the native handling.",
            "markdown"
        ]
    ],
    "Examples->Ensemble methods->Categorical Feature Support in Gradient Boosting->Limiting the number of splits": [
        [
            "In general, one can expect poorer predictions from one-hot-encoded data,\nespecially when the tree depths or the number of nodes are limited: with\none-hot-encoded data, one needs more split points, i.e. more depth, in order\nto recover an equivalent split that could be obtained in one single split\npoint with native handling.",
            "markdown"
        ],
        [
            "This is also true when categories are treated as ordinal quantities: if\ncategories are A..F and the best split is ACF - BDE the one-hot-encoder\nmodel will need 3 split points (one per category in the left node), and the\nordinal non-native model will need 4 splits: 1 split to isolate A, 1 split\nto isolate F, and 2 splits to isolate C from BCDE.",
            "markdown"
        ],
        [
            "How strongly the models\u2019 performances differ in practice will depend on the\ndataset and on the flexibility of the trees.",
            "markdown"
        ],
        [
            "To see this, let us re-run the same analysis with under-fitting models where\nwe artificially limit the total number of splits by both limiting the number\nof trees and the depth of each tree.",
            "markdown"
        ],
        [
            "for pipe in (hist_dropped, hist_one_hot, hist_ordinal, hist_native):\n    pipe.set_params(\n        histgradientboostingregressor__max_depth=3,\n        histgradientboostingregressor__max_iter=15,\n    )\n\ndropped_result = (hist_dropped, X, y, cv=n_cv_folds, scoring=scoring)\none_hot_result = (hist_one_hot, X, y, cv=n_cv_folds, scoring=scoring)\nordinal_result = (hist_ordinal, X, y, cv=n_cv_folds, scoring=scoring)\nnative_result = (hist_native, X, y, cv=n_cv_folds, scoring=scoring)\n\nplot_results(\"Gradient Boosting on Ames Housing (few and small trees)\")\n\n()\n\n\n<img alt=\"Gradient Boosting on Ames Housing (few and small trees), Fit times (s), Mean Absolute Percentage Error\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_002.png\"/>",
            "code"
        ],
        [
            "The results for these under-fitting models confirm our previous intuition:\nthe native category handling strategy performs the best when the splitting\nbudget is constrained. The two other strategies (one-hot encoding and\ntreating categories as ordinal values) lead to error values comparable\nto the baseline model that just dropped the categorical features altogether.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  8.826 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Ensemble methods->Combine predictors using stacking": [
        [
            "Stacking refers to a method to blend estimators. In this strategy, some\nestimators are individually fitted on some training data while a final\nestimator is trained using the stacked predictions of these base estimators.",
            "markdown"
        ],
        [
            "In this example, we illustrate the use case in which different regressors are\nstacked together and a final linear penalized regressor is used to output the\nprediction. We compare the performance of each individual regressor with the\nstacking strategy. Stacking slightly improves the overall performance.",
            "markdown"
        ],
        [
            "# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com\n#          Maria Telenczuk    &lt;https://github.com/maikia\n# License: BSD 3 clause",
            "code"
        ]
    ],
    "Examples->Ensemble methods->Combine predictors using stacking->Download the dataset": [
        [
            "We will use the  dataset which was first compiled by Dean De Cock\nand became better known after it was used in Kaggle challenge. It is a set\nof 1460 residential homes in Ames, Iowa, each described by 80 features. We\nwill use it to predict the final logarithmic price of the houses. In this\nexample we will use only 20 most interesting features chosen using\nGradientBoostingRegressor() and limit number of entries (here we won\u2019t go\ninto the details on how to select the most interesting features).",
            "markdown"
        ],
        [
            "The Ames housing dataset is not shipped with scikit-learn and therefore we\nwill fetch it from .\n</blockquote>",
            "markdown"
        ],
        [
            "import numpy as np\n\nfrom sklearn.datasets import \nfrom sklearn.utils import \n\n\ndef load_ames_housing():\n    df = (name=\"house_prices\", as_frame=True, parser=\"pandas\")\n    X = df.data\n    y = df.target\n\n    features = [\n        \"YrSold\",\n        \"HeatingQC\",\n        \"Street\",\n        \"YearRemodAdd\",\n        \"Heating\",\n        \"MasVnrType\",\n        \"BsmtUnfSF\",\n        \"Foundation\",\n        \"MasVnrArea\",\n        \"MSSubClass\",\n        \"ExterQual\",\n        \"Condition2\",\n        \"GarageCars\",\n        \"GarageType\",\n        \"OverallQual\",\n        \"TotalBsmtSF\",\n        \"BsmtFinSF1\",\n        \"HouseStyle\",\n        \"MiscFeature\",\n        \"MoSold\",\n    ]\n\n    X = X.loc[:, features]\n    X, y = (X, y, random_state=0)\n\n    X = X.iloc[:600]\n    y = y.iloc[:600]\n    return X, (y)\n\n\nX, y = load_ames_housing()",
            "code"
        ]
    ],
    "Examples->Ensemble methods->Combine predictors using stacking->Make pipeline to preprocess the data": [
        [
            "Before we can use Ames dataset we still need to do some preprocessing.\nFirst, we will select the categorical and numerical columns of the dataset to\nconstruct the first step of the pipeline.\n</blockquote>",
            "markdown"
        ],
        [
            "from sklearn.compose import \n\ncat_selector = (dtype_include=object)\nnum_selector = (dtype_include=)\ncat_selector(X)",
            "code"
        ],
        [
            "['HeatingQC', 'Street', 'Heating', 'MasVnrType', 'Foundation', 'ExterQual', 'Condition2', 'GarageType', 'HouseStyle', 'MiscFeature']",
            "code"
        ],
        [
            "num_selector(X)",
            "code"
        ],
        [
            "['YrSold', 'YearRemodAdd', 'BsmtUnfSF', 'MasVnrArea', 'MSSubClass', 'GarageCars', 'OverallQual', 'TotalBsmtSF', 'BsmtFinSF1', 'MoSold']",
            "code"
        ],
        [
            "Then, we will need to design preprocessing pipelines which depends on the\nending regressor. If the ending regressor is a linear model, one needs to\none-hot encode the categories. If the ending regressor is a tree-based model\nan ordinal encoder will be sufficient. Besides, numerical values need to be\nstandardized for a linear model while the raw numerical data can be treated\nas is by a tree-based model. However, both models need an imputer to\nhandle missing values.",
            "markdown"
        ],
        [
            "We will first design the pipeline required for the tree-based models.",
            "markdown"
        ],
        [
            "from sklearn.compose import \nfrom sklearn.impute import \nfrom sklearn.pipeline import \nfrom sklearn.preprocessing import \n\ncat_tree_processor = (\n    handle_unknown=\"use_encoded_value\",\n    unknown_value=-1,\n    encoded_missing_value=-2,\n)\nnum_tree_processor = (strategy=\"mean\", add_indicator=True)\n\ntree_preprocessor = (\n    (num_tree_processor, num_selector), (cat_tree_processor, cat_selector)\n)\ntree_preprocessor",
            "code"
        ],
        [
            "ColumnTransformer(transformers=[('simpleimputer',\n                                 SimpleImputer(add_indicator=True),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d3f890af0),\n                                ('ordinalencoder',\n                                 OrdinalEncoder(encoded_missing_value=-2,\n                                                handle_unknown='use_encoded_value',\n                                                unknown_value=-1),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d547a2a00)])<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-29\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-29\">ColumnTransformer</label>",
            "code"
        ],
        [
            "ColumnTransformer(transformers=[('simpleimputer',\n                                 SimpleImputer(add_indicator=True),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d3f890af0),\n                                ('ordinalencoder',\n                                 OrdinalEncoder(encoded_missing_value=-2,\n                                                handle_unknown='use_encoded_value',\n                                                unknown_value=-1),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d547a2a00)])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-30\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-30\">simpleimputer</label>",
            "code"
        ],
        [
            "&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d3f890af0<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-31\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-31\">SimpleImputer</label>",
            "code"
        ],
        [
            "SimpleImputer(add_indicator=True)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-32\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-32\">ordinalencoder</label>",
            "code"
        ],
        [
            "&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d547a2a00<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-33\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-33\">OrdinalEncoder</label>",
            "code"
        ],
        [
            "OrdinalEncoder(encoded_missing_value=-2, handle_unknown='use_encoded_value',\n               unknown_value=-1)\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "Then, we will now define the preprocessor used when the ending regressor\nis a linear model.",
            "markdown"
        ],
        [
            "from sklearn.preprocessing import \nfrom sklearn.preprocessing import \n\ncat_linear_processor = (handle_unknown=\"ignore\")\nnum_linear_processor = (\n    (), (strategy=\"mean\", add_indicator=True)\n)\n\nlinear_preprocessor = (\n    (num_linear_processor, num_selector), (cat_linear_processor, cat_selector)\n)\nlinear_preprocessor",
            "code"
        ],
        [
            "ColumnTransformer(transformers=[('pipeline',\n                                 Pipeline(steps=[('standardscaler',\n                                                  StandardScaler()),\n                                                 ('simpleimputer',\n                                                  SimpleImputer(add_indicator=True))]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d3f890af0),\n                                ('onehotencoder',\n                                 OneHotEncoder(handle_unknown='ignore'),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d547a2a00)])<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-34\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-34\">ColumnTransformer</label>",
            "code"
        ],
        [
            "ColumnTransformer(transformers=[('pipeline',\n                                 Pipeline(steps=[('standardscaler',\n                                                  StandardScaler()),\n                                                 ('simpleimputer',\n                                                  SimpleImputer(add_indicator=True))]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d3f890af0),\n                                ('onehotencoder',\n                                 OneHotEncoder(handle_unknown='ignore'),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d547a2a00)])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-35\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-35\">pipeline</label>",
            "code"
        ],
        [
            "&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d3f890af0<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-36\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-36\">StandardScaler</label>",
            "code"
        ],
        [
            "StandardScaler()<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-37\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-37\">SimpleImputer</label>",
            "code"
        ],
        [
            "SimpleImputer(add_indicator=True)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-38\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-38\">onehotencoder</label>",
            "code"
        ],
        [
            "&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d547a2a00<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-39\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-39\">OneHotEncoder</label>",
            "code"
        ],
        [
            "OneHotEncoder(handle_unknown='ignore')\n\n<br/>\n<br/>",
            "code"
        ]
    ],
    "Examples->Ensemble methods->Combine predictors using stacking->Stack of predictors on a single data set": [
        [
            "It is sometimes tedious to find the model which will best perform on a given\ndataset. Stacking provide an alternative by combining the outputs of several\nlearners, without the need to choose a model specifically. The performance of\nstacking is usually close to the best model and sometimes it can outperform\nthe prediction performance of each individual model.",
            "markdown"
        ],
        [
            "Here, we combine 3 learners (linear and non-linear) and use a ridge regressor\nto combine their outputs together.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "Although we will make new pipelines with the processors which we wrote in\nthe previous section for the 3 learners, the final estimator\n does not need preprocessing of\nthe data as it will be fed with the already preprocessed output from the 3\nlearners.\n\n</blockquote>",
            "markdown"
        ],
        [
            "from sklearn.linear_model import \n\nlasso_pipeline = (linear_preprocessor, ())\nlasso_pipeline",
            "code"
        ],
        [
            "Pipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('pipeline',\n                                                  Pipeline(steps=[('standardscaler',\n                                                                   StandardScaler()),\n                                                                  ('simpleimputer',\n                                                                   SimpleImputer(add_indicator=True))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d3f890af0),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d547a2a00)])),\n                ('lassocv', LassoCV())])<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-40\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-40\">Pipeline</label>",
            "code"
        ],
        [
            "Pipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('pipeline',\n                                                  Pipeline(steps=[('standardscaler',\n                                                                   StandardScaler()),\n                                                                  ('simpleimputer',\n                                                                   SimpleImputer(add_indicator=True))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d3f890af0),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d547a2a00)])),\n                ('lassocv', LassoCV())])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-41\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-41\">columntransformer: ColumnTransformer</label>",
            "code"
        ],
        [
            "ColumnTransformer(transformers=[('pipeline',\n                                 Pipeline(steps=[('standardscaler',\n                                                  StandardScaler()),\n                                                 ('simpleimputer',\n                                                  SimpleImputer(add_indicator=True))]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d3f890af0),\n                                ('onehotencoder',\n                                 OneHotEncoder(handle_unknown='ignore'),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d547a2a00)])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-42\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-42\">pipeline</label>",
            "code"
        ],
        [
            "&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d3f890af0<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-43\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-43\">StandardScaler</label>",
            "code"
        ],
        [
            "StandardScaler()<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-44\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-44\">SimpleImputer</label>",
            "code"
        ],
        [
            "SimpleImputer(add_indicator=True)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-45\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-45\">onehotencoder</label>",
            "code"
        ],
        [
            "&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d547a2a00<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-46\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-46\">OneHotEncoder</label>",
            "code"
        ],
        [
            "OneHotEncoder(handle_unknown='ignore')<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-47\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-47\">LassoCV</label>",
            "code"
        ],
        [
            "LassoCV()\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "from sklearn.ensemble import \n\nrf_pipeline = (tree_preprocessor, (random_state=42))\nrf_pipeline",
            "code"
        ],
        [
            "Pipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('simpleimputer',\n                                                  SimpleImputer(add_indicator=True),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d3f890af0),\n                                                 ('ordinalencoder',\n                                                  OrdinalEncoder(encoded_missing_value=-2,\n                                                                 handle_unknown='use_encoded_value',\n                                                                 unknown_value=-1),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d547a2a00)])),\n                ('randomforestregressor',\n                 RandomForestRegressor(random_state=42))])<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-48\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-48\">Pipeline</label>",
            "code"
        ],
        [
            "Pipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('simpleimputer',\n                                                  SimpleImputer(add_indicator=True),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d3f890af0),\n                                                 ('ordinalencoder',\n                                                  OrdinalEncoder(encoded_missing_value=-2,\n                                                                 handle_unknown='use_encoded_value',\n                                                                 unknown_value=-1),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d547a2a00)])),\n                ('randomforestregressor',\n                 RandomForestRegressor(random_state=42))])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-49\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-49\">columntransformer: ColumnTransformer</label>",
            "code"
        ],
        [
            "ColumnTransformer(transformers=[('simpleimputer',\n                                 SimpleImputer(add_indicator=True),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d3f890af0),\n                                ('ordinalencoder',\n                                 OrdinalEncoder(encoded_missing_value=-2,\n                                                handle_unknown='use_encoded_value',\n                                                unknown_value=-1),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d547a2a00)])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-50\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-50\">simpleimputer</label>",
            "code"
        ],
        [
            "&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d3f890af0<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-51\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-51\">SimpleImputer</label>",
            "code"
        ],
        [
            "SimpleImputer(add_indicator=True)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-52\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-52\">ordinalencoder</label>",
            "code"
        ],
        [
            "&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d547a2a00<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-53\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-53\">OrdinalEncoder</label>",
            "code"
        ],
        [
            "OrdinalEncoder(encoded_missing_value=-2, handle_unknown='use_encoded_value',\n               unknown_value=-1)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-54\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-54\">RandomForestRegressor</label>",
            "code"
        ],
        [
            "RandomForestRegressor(random_state=42)\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "from sklearn.ensemble import \n\ngbdt_pipeline = (\n    tree_preprocessor, (random_state=0)\n)\ngbdt_pipeline",
            "code"
        ],
        [
            "Pipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('simpleimputer',\n                                                  SimpleImputer(add_indicator=True),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d3f890af0),\n                                                 ('ordinalencoder',\n                                                  OrdinalEncoder(encoded_missing_value=-2,\n                                                                 handle_unknown='use_encoded_value',\n                                                                 unknown_value=-1),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d547a2a00)])),\n                ('histgradientboostingregressor',\n                 HistGradientBoostingRegressor(random_state=0))])<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-55\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-55\">Pipeline</label>",
            "code"
        ],
        [
            "Pipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('simpleimputer',\n                                                  SimpleImputer(add_indicator=True),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d3f890af0),\n                                                 ('ordinalencoder',\n                                                  OrdinalEncoder(encoded_missing_value=-2,\n                                                                 handle_unknown='use_encoded_value',\n                                                                 unknown_value=-1),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d547a2a00)])),\n                ('histgradientboostingregressor',\n                 HistGradientBoostingRegressor(random_state=0))])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-56\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-56\">columntransformer: ColumnTransformer</label>",
            "code"
        ],
        [
            "ColumnTransformer(transformers=[('simpleimputer',\n                                 SimpleImputer(add_indicator=True),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d3f890af0),\n                                ('ordinalencoder',\n                                 OrdinalEncoder(encoded_missing_value=-2,\n                                                handle_unknown='use_encoded_value',\n                                                unknown_value=-1),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d547a2a00)])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-57\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-57\">simpleimputer</label>",
            "code"
        ],
        [
            "&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d3f890af0<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-58\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-58\">SimpleImputer</label>",
            "code"
        ],
        [
            "SimpleImputer(add_indicator=True)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-59\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-59\">ordinalencoder</label>",
            "code"
        ],
        [
            "&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d547a2a00<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-60\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-60\">OrdinalEncoder</label>",
            "code"
        ],
        [
            "OrdinalEncoder(encoded_missing_value=-2, handle_unknown='use_encoded_value',\n               unknown_value=-1)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-61\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-61\">HistGradientBoostingRegressor</label>",
            "code"
        ],
        [
            "HistGradientBoostingRegressor(random_state=0)\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "from sklearn.ensemble import \nfrom sklearn.linear_model import \n\nestimators = [\n    (\"Random Forest\", rf_pipeline),\n    (\"Lasso\", lasso_pipeline),\n    (\"Gradient Boosting\", gbdt_pipeline),\n]\n\nstacking_regressor = (estimators=estimators, final_estimator=())\nstacking_regressor",
            "code"
        ],
        [
            "StackingRegressor(estimators=[('Random Forest',\n                               Pipeline(steps=[('columntransformer',\n                                                ColumnTransformer(transformers=[('simpleimputer',\n                                                                                 SimpleImputer(add_indicator=True),\n                                                                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d3f890af0),\n                                                                                ('ordinalencoder',\n                                                                                 OrdinalEncoder(encoded_missing_value=-2,\n                                                                                                handle_unknown='use_encoded_value',\n                                                                                                unknown_v...\n                                                                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d3f890af0),\n                                                                                ('ordinalencoder',\n                                                                                 OrdinalEncoder(encoded_missing_value=-2,\n                                                                                                handle_unknown='use_encoded_value',\n                                                                                                unknown_value=-1),\n                                                                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d547a2a00)])),\n                                               ('histgradientboostingregressor',\n                                                HistGradientBoostingRegressor(random_state=0))]))],\n                  final_estimator=RidgeCV())<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-62\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-62\">StackingRegressor</label>",
            "code"
        ],
        [
            "StackingRegressor(estimators=[('Random Forest',\n                               Pipeline(steps=[('columntransformer',\n                                                ColumnTransformer(transformers=[('simpleimputer',\n                                                                                 SimpleImputer(add_indicator=True),\n                                                                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d3f890af0),\n                                                                                ('ordinalencoder',\n                                                                                 OrdinalEncoder(encoded_missing_value=-2,\n                                                                                                handle_unknown='use_encoded_value',\n                                                                                                unknown_v...\n                                                                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d3f890af0),\n                                                                                ('ordinalencoder',\n                                                                                 OrdinalEncoder(encoded_missing_value=-2,\n                                                                                                handle_unknown='use_encoded_value',\n                                                                                                unknown_value=-1),\n                                                                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d547a2a00)])),\n                                               ('histgradientboostingregressor',\n                                                HistGradientBoostingRegressor(random_state=0))]))],\n                  final_estimator=RidgeCV())<label>Random Forest</label><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-63\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-63\">columntransformer: ColumnTransformer</label>",
            "code"
        ],
        [
            "ColumnTransformer(transformers=[('simpleimputer',\n                                 SimpleImputer(add_indicator=True),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d3f890af0),\n                                ('ordinalencoder',\n                                 OrdinalEncoder(encoded_missing_value=-2,\n                                                handle_unknown='use_encoded_value',\n                                                unknown_value=-1),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d547a2a00)])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-64\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-64\">simpleimputer</label>",
            "code"
        ],
        [
            "&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d3f890af0<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-65\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-65\">SimpleImputer</label>",
            "code"
        ],
        [
            "SimpleImputer(add_indicator=True)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-66\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-66\">ordinalencoder</label>",
            "code"
        ],
        [
            "&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d547a2a00<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-67\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-67\">OrdinalEncoder</label>",
            "code"
        ],
        [
            "OrdinalEncoder(encoded_missing_value=-2, handle_unknown='use_encoded_value',\n               unknown_value=-1)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-68\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-68\">RandomForestRegressor</label>",
            "code"
        ],
        [
            "RandomForestRegressor(random_state=42)<label>Lasso</label><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-69\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-69\">columntransformer: ColumnTransformer</label>",
            "code"
        ],
        [
            "ColumnTransformer(transformers=[('pipeline',\n                                 Pipeline(steps=[('standardscaler',\n                                                  StandardScaler()),\n                                                 ('simpleimputer',\n                                                  SimpleImputer(add_indicator=True))]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d3f890af0),\n                                ('onehotencoder',\n                                 OneHotEncoder(handle_unknown='ignore'),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d547a2a00)])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-70\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-70\">pipeline</label>",
            "code"
        ],
        [
            "&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d3f890af0<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-71\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-71\">StandardScaler</label>",
            "code"
        ],
        [
            "StandardScaler()<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-72\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-72\">SimpleImputer</label>",
            "code"
        ],
        [
            "SimpleImputer(add_indicator=True)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-73\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-73\">onehotencoder</label>",
            "code"
        ],
        [
            "&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d547a2a00<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-74\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-74\">OneHotEncoder</label>",
            "code"
        ],
        [
            "OneHotEncoder(handle_unknown='ignore')<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-75\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-75\">LassoCV</label>",
            "code"
        ],
        [
            "LassoCV()<label>Gradient Boosting</label><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-76\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-76\">columntransformer: ColumnTransformer</label>",
            "code"
        ],
        [
            "ColumnTransformer(transformers=[('simpleimputer',\n                                 SimpleImputer(add_indicator=True),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d3f890af0),\n                                ('ordinalencoder',\n                                 OrdinalEncoder(encoded_missing_value=-2,\n                                                handle_unknown='use_encoded_value',\n                                                unknown_value=-1),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d547a2a00)])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-77\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-77\">simpleimputer</label>",
            "code"
        ],
        [
            "&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d3f890af0<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-78\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-78\">SimpleImputer</label>",
            "code"
        ],
        [
            "SimpleImputer(add_indicator=True)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-79\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-79\">ordinalencoder</label>",
            "code"
        ],
        [
            "&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d547a2a00<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-80\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-80\">OrdinalEncoder</label>",
            "code"
        ],
        [
            "OrdinalEncoder(encoded_missing_value=-2, handle_unknown='use_encoded_value',\n               unknown_value=-1)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-81\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-81\">HistGradientBoostingRegressor</label>",
            "code"
        ],
        [
            "HistGradientBoostingRegressor(random_state=0)<label>final_estimator</label><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-82\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-82\">RidgeCV</label>",
            "code"
        ],
        [
            "RidgeCV()\n\n<br/>\n<br/>",
            "code"
        ]
    ],
    "Examples->Ensemble methods->Combine predictors using stacking->Measure and plot the results": [
        [
            "Now we can use Ames Housing dataset to make the predictions. We check the\nperformance of each individual predictor as well as of the stack of the\nregressors.\n</blockquote>",
            "markdown"
        ],
        [
            "import time\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import PredictionErrorDisplay\nfrom sklearn.model_selection import , \n\nfig, axs = (2, 2, figsize=(9, 7))\naxs = (axs)\n\nfor ax, (name, est) in zip(\n    axs, estimators + [(\"Stacking Regressor\", stacking_regressor)]\n):\n    scorers = {\"R2\": \"r2\", \"MAE\": \"neg_mean_absolute_error\"}\n\n    start_time = ()\n    scores = (\n        est, X, y, scoring=list(scorers.values()), n_jobs=-1, verbose=0\n    )\n    elapsed_time = () - start_time\n\n    y_pred = (est, X, y, n_jobs=-1, verbose=0)\n    scores = {\n        key: (\n            f\"{np.abs((scores[f'test_{value}'])):.2f} +- \"\n            f\"{(scores[f'test_{value}']):.2f}\"\n        )\n        for key, value in scorers.items()\n    }\n\n    display = (\n        y_true=y,\n        y_pred=y_pred,\n        kind=\"actual_vs_predicted\",\n        ax=ax,\n        scatter_kwargs={\"alpha\": 0.2, \"color\": \"tab:blue\"},\n        line_kwargs={\"color\": \"tab:red\"},\n    )\n    ax.set_title(f\"{name}\\nEvaluation in {elapsed_time:.2f} seconds\")\n\n    for name, score in scores.items():\n        ax.plot([], [], \" \", label=f\"{name}: {score}\")\n    ax.legend(loc=\"upper left\")\n\n(\"Single predictors versus stacked predictors\")\n()\n(top=0.9)\n()\n\n\n<img alt=\"Single predictors versus stacked predictors, Random Forest Evaluation in 0.98 seconds, Lasso Evaluation in 0.49 seconds, Gradient Boosting Evaluation in 0.56 seconds, Stacking Regressor Evaluation in 11.70 seconds\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_stack_predictors_001.png\" srcset=\"../../_images/sphx_glr_plot_stack_predictors_001.png\"/>",
            "code"
        ],
        [
            "The stacked regressor will combine the strengths of the different regressors.\nHowever, we also see that training the stacked regressor is much more\ncomputationally expensive.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  28.239 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Ensemble methods->Comparing random forests and the multi-output meta estimator": [
        [
            "An example to compare multi-output regression with random forest and\nthe  meta-estimator.",
            "markdown"
        ],
        [
            "This example illustrates the use of the\n meta-estimator\nto perform multi-output regression. A random forest regressor is used,\nwhich supports multi-output regression natively, so the results can be\ncompared.",
            "markdown"
        ],
        [
            "The random forest regressor will only ever predict values within the\nrange of observations or closer to zero for each of the targets. As a\nresult the predictions are biased towards the centre of the circle.",
            "markdown"
        ],
        [
            "Using a single underlying feature the model learns both the\nx and y coordinate as output.\n<img alt=\"Comparing random forests and the multi-output meta estimator\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_random_forest_regression_multioutput_001.png\" srcset=\"../../_images/sphx_glr_plot_random_forest_regression_multioutput_001.png\"/>",
            "markdown"
        ],
        [
            "# Author: Tim Head &lt;betatim@gmail.com\n#\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import \nfrom sklearn.model_selection import \nfrom sklearn.multioutput import \n\n\n# Create a random dataset\nrng = (1)\nX = (200 * rng.rand(600, 1) - 100, axis=0)\ny = ([ * (X).ravel(),  * (X).ravel()]).T\ny += 0.5 - rng.rand(*y.shape)\n\nX_train, X_test, y_train, y_test = (\n    X, y, train_size=400, test_size=200, random_state=4\n)\n\nmax_depth = 30\nregr_multirf = (\n    (n_estimators=100, max_depth=max_depth, random_state=0)\n)\nregr_multirf.fit(X_train, y_train)\n\nregr_rf = (n_estimators=100, max_depth=max_depth, random_state=2)\nregr_rf.fit(X_train, y_train)\n\n# Predict on new data\ny_multirf = regr_multirf.predict(X_test)\ny_rf = regr_rf.predict(X_test)\n\n# Plot the results\n()\ns = 50\na = 0.4\n(\n    y_test[:, 0],\n    y_test[:, 1],\n    edgecolor=\"k\",\n    c=\"navy\",\n    s=s,\n    marker=\"s\",\n    alpha=a,\n    label=\"Data\",\n)\n(\n    y_multirf[:, 0],\n    y_multirf[:, 1],\n    edgecolor=\"k\",\n    c=\"cornflowerblue\",\n    s=s,\n    alpha=a,\n    label=\"Multi RF score=%.2f\" % regr_multirf.score(X_test, y_test),\n)\n(\n    y_rf[:, 0],\n    y_rf[:, 1],\n    edgecolor=\"k\",\n    c=\"c\",\n    s=s,\n    marker=\"^\",\n    alpha=a,\n    label=\"RF score=%.2f\" % regr_rf.score(X_test, y_test),\n)\n([-6, 6])\n([-6, 6])\n(\"target 1\")\n(\"target 2\")\n(\"Comparing random forests and the multi-output meta estimator\")\n()\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.539 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Ensemble methods->Decision Tree Regression with AdaBoost": [
        [
            "A decision tree is boosted using the AdaBoost.R2  algorithm on a 1D\nsinusoidal dataset with a small amount of Gaussian noise.\n299 boosts (300 decision trees) is compared with a single decision tree\nregressor. As the number of boosts is increased the regressor can fit more\ndetail.\n\n\n[]",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Ensemble methods->Decision Tree Regression with AdaBoost->Preparing the data": [
        [
            "First, we prepare dummy data with a sinusoidal relationship and some gaussian noise.",
            "markdown"
        ],
        [
            "# Author: Noel Dawe &lt;noel.dawe@gmail.com\n#\n# License: BSD 3 clause\n\nimport numpy as np\n\nrng = (1)\nX = (0, 6, 100)[:, ]\ny = (X).ravel() + (6 * X).ravel() + rng.normal(0, 0.1, X.shape[0])",
            "code"
        ]
    ],
    "Examples->Ensemble methods->Decision Tree Regression with AdaBoost->Training and prediction with DecisionTree and AdaBoost Regressors": [
        [
            "Now, we define the classifiers and fit them to the data.\nThen we predict on that same data to see how well they could fit it.\nThe first regressor is a DecisionTreeRegressor with max_depth=4.\nThe second regressor is an AdaBoostRegressor with a DecisionTreeRegressor\nof max_depth=4 as base learner and will be built with n_estimators=300\nof those base learners.",
            "markdown"
        ],
        [
            "from sklearn.ensemble import \nfrom sklearn.tree import \n\nregr_1 = (max_depth=4)\n\nregr_2 = (\n    (max_depth=4), n_estimators=300, random_state=rng\n)\n\nregr_1.fit(X, y)\nregr_2.fit(X, y)\n\ny_1 = regr_1.predict(X)\ny_2 = regr_2.predict(X)",
            "code"
        ]
    ],
    "Examples->Ensemble methods->Decision Tree Regression with AdaBoost->Plotting the results": [
        [
            "Finally, we plot how well our two regressors,\nsingle decision tree regressor and AdaBoost regressor, could fit the data.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\nimport seaborn as sns\n\ncolors = (\"colorblind\")\n\n()\n(X, y, color=colors[0], label=\"training samples\")\n(X, y_1, color=colors[1], label=\"n_estimators=1\", linewidth=2)\n(X, y_2, color=colors[2], label=\"n_estimators=300\", linewidth=2)\n(\"data\")\n(\"target\")\n(\"Boosted Decision Tree Regression\")\n()\n()\n\n\n<img alt=\"Boosted Decision Tree Regression\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_adaboost_regression_001.png\" srcset=\"../../_images/sphx_glr_plot_adaboost_regression_001.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.485 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Ensemble methods->Discrete versus Real AdaBoost": [
        [
            "This notebook is based on Figure 10.2 from Hastie et al 2009  and\nillustrates the difference in performance between the discrete SAMME \nboosting algorithm and real SAMME.R boosting algorithm. Both algorithms are\nevaluated on a binary classification task where the target Y is a non-linear\nfunction of 10 input features.",
            "markdown"
        ],
        [
            "Discrete SAMME AdaBoost adapts based on errors in predicted class labels\nwhereas real SAMME.R uses the predicted class probabilities.\n\n\n[]",
            "markdown"
        ],
        [
            "T. Hastie, R. Tibshirani and J. Friedman, \u201cElements of Statistical\nLearning Ed. 2\u201d, Springer, 2009.\n\n\n[]",
            "markdown"
        ],
        [
            "J Zhu, H. Zou, S. Rosset, T. Hastie, \u201cMulti-class AdaBoost\u201d,\nStatistics and Its Interface, 2009.",
            "markdown"
        ]
    ],
    "Examples->Ensemble methods->Discrete versus Real AdaBoost->Preparing the data and baseline models": [
        [
            "We start by generating the binary classification dataset\nused in Hastie et al. 2009, Example 10.2.",
            "markdown"
        ],
        [
            "# Authors: Peter Prettenhofer &lt;peter.prettenhofer@gmail.com,\n#          Noel Dawe &lt;noel.dawe@gmail.com\n#\n# License: BSD 3 clause\n\nfrom sklearn import datasets\n\nX, y = (n_samples=12_000, random_state=1)",
            "code"
        ],
        [
            "Now, we set the hyperparameters for our AdaBoost classifiers.\nBe aware, a learning rate of 1.0 may not be optimal for both SAMME and SAMME.R",
            "markdown"
        ],
        [
            "n_estimators = 400\nlearning_rate = 1.0",
            "code"
        ],
        [
            "We split the data into a training and a test set.\nThen, we train our baseline classifiers, a DecisionTreeClassifier with depth=9\nand a \u201cstump\u201d DecisionTreeClassifier with depth=1 and compute the test error.",
            "markdown"
        ],
        [
            "from sklearn.model_selection import \nfrom sklearn.tree import \n\nX_train, X_test, y_train, y_test = (\n    X, y, test_size=2_000, shuffle=False\n)\n\ndt_stump = (max_depth=1, min_samples_leaf=1)\ndt_stump.fit(X_train, y_train)\ndt_stump_err = 1.0 - dt_stump.score(X_test, y_test)\n\ndt = (max_depth=9, min_samples_leaf=1)\ndt.fit(X_train, y_train)\ndt_err = 1.0 - dt.score(X_test, y_test)",
            "code"
        ]
    ],
    "Examples->Ensemble methods->Discrete versus Real AdaBoost->Adaboost with discrete SAMME and real SAMME.R": [
        [
            "We now define the discrete and real AdaBoost classifiers\nand fit them to the training set.",
            "markdown"
        ],
        [
            "from sklearn.ensemble import \n\nada_discrete = (\n    estimator=dt_stump,\n    learning_rate=learning_rate,\n    n_estimators=n_estimators,\n    algorithm=\"SAMME\",\n)\nada_discrete.fit(X_train, y_train)",
            "code"
        ],
        [
            "AdaBoostClassifier(algorithm='SAMME',\n                   estimator=DecisionTreeClassifier(max_depth=1),\n                   n_estimators=400)<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-83\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-83\">AdaBoostClassifier</label>",
            "code"
        ],
        [
            "AdaBoostClassifier(algorithm='SAMME',\n                   estimator=DecisionTreeClassifier(max_depth=1),\n                   n_estimators=400)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-84\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-84\">estimator: DecisionTreeClassifier</label>",
            "code"
        ],
        [
            "DecisionTreeClassifier(max_depth=1)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-85\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-85\">DecisionTreeClassifier</label>",
            "code"
        ],
        [
            "DecisionTreeClassifier(max_depth=1)\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "ada_real = (\n    estimator=dt_stump,\n    learning_rate=learning_rate,\n    n_estimators=n_estimators,\n    algorithm=\"SAMME.R\",\n)\nada_real.fit(X_train, y_train)",
            "code"
        ],
        [
            "AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1),\n                   n_estimators=400)<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-86\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-86\">AdaBoostClassifier</label>",
            "code"
        ],
        [
            "AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1),\n                   n_estimators=400)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-87\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-87\">estimator: DecisionTreeClassifier</label>",
            "code"
        ],
        [
            "DecisionTreeClassifier(max_depth=1)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-88\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-88\">DecisionTreeClassifier</label>",
            "code"
        ],
        [
            "DecisionTreeClassifier(max_depth=1)\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "Now, let\u2019s compute the test error of the discrete and\nreal AdaBoost classifiers for each new stump in n_estimators\nadded to the ensemble.",
            "markdown"
        ],
        [
            "import numpy as np\nfrom sklearn.metrics import \n\nada_discrete_err = ((n_estimators,))\nfor i, y_pred in enumerate(ada_discrete.staged_predict(X_test)):\n    ada_discrete_err[i] = (y_pred, y_test)\n\nada_discrete_err_train = ((n_estimators,))\nfor i, y_pred in enumerate(ada_discrete.staged_predict(X_train)):\n    ada_discrete_err_train[i] = (y_pred, y_train)\n\nada_real_err = ((n_estimators,))\nfor i, y_pred in enumerate(ada_real.staged_predict(X_test)):\n    ada_real_err[i] = (y_pred, y_test)\n\nada_real_err_train = ((n_estimators,))\nfor i, y_pred in enumerate(ada_real.staged_predict(X_train)):\n    ada_real_err_train[i] = (y_pred, y_train)",
            "code"
        ]
    ],
    "Examples->Ensemble methods->Discrete versus Real AdaBoost->Plotting the results": [
        [
            "Finally, we plot the train and test errors of our baselines\nand of the discrete and real AdaBoost classifiers",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig = ()\nax = fig.add_subplot(111)\n\nax.plot([1, n_estimators], [dt_stump_err] * 2, \"k-\", label=\"Decision Stump Error\")\nax.plot([1, n_estimators], [dt_err] * 2, \"k--\", label=\"Decision Tree Error\")\n\ncolors = (\"colorblind\")\n\nax.plot(\n    (n_estimators) + 1,\n    ada_discrete_err,\n    label=\"Discrete AdaBoost Test Error\",\n    color=colors[0],\n)\nax.plot(\n    (n_estimators) + 1,\n    ada_discrete_err_train,\n    label=\"Discrete AdaBoost Train Error\",\n    color=colors[1],\n)\nax.plot(\n    (n_estimators) + 1,\n    ada_real_err,\n    label=\"Real AdaBoost Test Error\",\n    color=colors[2],\n)\nax.plot(\n    (n_estimators) + 1,\n    ada_real_err_train,\n    label=\"Real AdaBoost Train Error\",\n    color=colors[4],\n)\n\nax.set_ylim((0.0, 0.5))\nax.set_xlabel(\"Number of weak learners\")\nax.set_ylabel(\"error rate\")\n\nleg = ax.legend(loc=\"upper right\", fancybox=True)\nleg.get_frame().set_alpha(0.7)\n\n()\n\n\n<img alt=\"plot adaboost hastie 10 2\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_adaboost_hastie_10_2_001.png\" srcset=\"../../_images/sphx_glr_plot_adaboost_hastie_10_2_001.png\"/>",
            "code"
        ]
    ],
    "Examples->Ensemble methods->Discrete versus Real AdaBoost->Concluding remarks": [
        [
            "We observe that the error rate for both train and test sets of real AdaBoost\nis lower than that of discrete AdaBoost.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  15.557 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Ensemble methods->Early stopping of Gradient Boosting": [
        [
            "Gradient boosting is an ensembling technique where several weak learners\n(regression trees) are combined to yield a powerful single model, in an\niterative fashion.",
            "markdown"
        ],
        [
            "Early stopping support in Gradient Boosting enables us to find the least number\nof iterations which is sufficient to build a model that generalizes well to\nunseen data.",
            "markdown"
        ],
        [
            "The concept of early stopping is simple. We specify a validation_fraction\nwhich denotes the fraction of the whole dataset that will be kept aside from\ntraining to assess the validation loss of the model. The gradient boosting\nmodel is trained using the training set and evaluated using the validation set.\nWhen each additional stage of regression tree is added, the validation set is\nused to score the model.  This is continued until the scores of the model in\nthe last n_iter_no_change stages do not improve by at least tol. After\nthat the model is considered to have converged and further addition of stages\nis \u201cstopped early\u201d.",
            "markdown"
        ],
        [
            "The number of stages of the final model is available at the attribute\nn_estimators_.",
            "markdown"
        ],
        [
            "This example illustrates how the early stopping can used in the\n model to achieve\nalmost the same accuracy as compared to a model built without early stopping\nusing many fewer estimators. This can significantly reduce training time,\nmemory usage and prediction latency.",
            "markdown"
        ],
        [
            "# Authors: Vighnesh Birodkar &lt;vighneshbirodkar@nyu.edu\n#          Raghav RV &lt;rvraghav93@gmail.com\n# License: BSD 3 clause\n\nimport time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import ensemble\nfrom sklearn import datasets\nfrom sklearn.model_selection import \n\ndata_list = [\n    (return_X_y=True),\n    (n_samples=800, random_state=0),\n    (n_samples=2000, random_state=0),\n]\nnames = [\"Iris Data\", \"Classification Data\", \"Hastie Data\"]\n\nn_gb = []\nscore_gb = []\ntime_gb = []\nn_gbes = []\nscore_gbes = []\ntime_gbes = []\n\nn_estimators = 200\n\nfor X, y in data_list:\n    X_train, X_test, y_train, y_test = (\n        X, y, test_size=0.2, random_state=0\n    )\n\n    # We specify that if the scores don't improve by at least 0.01 for the last\n    # 10 stages, stop fitting additional stages\n    gbes = (\n        n_estimators=n_estimators,\n        validation_fraction=0.2,\n        n_iter_no_change=5,\n        tol=0.01,\n        random_state=0,\n    )\n    gb = (n_estimators=n_estimators, random_state=0)\n    start = ()\n    gb.fit(X_train, y_train)\n    time_gb.append(() - start)\n\n    start = ()\n    gbes.fit(X_train, y_train)\n    time_gbes.append(() - start)\n\n    score_gb.append(gb.score(X_test, y_test))\n    score_gbes.append(gbes.score(X_test, y_test))\n\n    n_gb.append(gb.n_estimators_)\n    n_gbes.append(gbes.n_estimators_)\n\nbar_width = 0.2\nn = len(data_list)\nindex = (0, n * bar_width, bar_width) * 2.5\nindex = index[0:n]",
            "code"
        ]
    ],
    "Examples->Ensemble methods->Early stopping of Gradient Boosting->Compare scores with and without early stopping": [
        [
            "(figsize=(9, 5))\n\nbar1 = (\n    index, score_gb, bar_width, label=\"Without early stopping\", color=\"crimson\"\n)\nbar2 = (\n    index + bar_width, score_gbes, bar_width, label=\"With early stopping\", color=\"coral\"\n)\n\n(index + bar_width, names)\n((0, 1.3, 0.1))\n\n\ndef autolabel(rects, n_estimators):\n    \"\"\"\n    Attach a text label above each bar displaying n_estimators of each model\n    \"\"\"\n    for i, rect in enumerate(rects):\n        (\n            rect.get_x() + rect.get_width() / 2.0,\n            1.05 * rect.get_height(),\n            \"n_est=%d\" % n_estimators[i],\n            ha=\"center\",\n            va=\"bottom\",\n        )\n\n\nautolabel(bar1, n_gb)\nautolabel(bar2, n_gbes)\n\n([0, 1.3])\n(loc=\"best\")\n(True)\n\n(\"Datasets\")\n(\"Test score\")\n\n()\n\n\n<img alt=\"plot gradient boosting early stopping\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_early_stopping_001.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_early_stopping_001.png\"/>",
            "code"
        ]
    ],
    "Examples->Ensemble methods->Early stopping of Gradient Boosting->Compare fit times with and without early stopping": [
        [
            "(figsize=(9, 5))\n\nbar1 = (\n    index, time_gb, bar_width, label=\"Without early stopping\", color=\"crimson\"\n)\nbar2 = (\n    index + bar_width, time_gbes, bar_width, label=\"With early stopping\", color=\"coral\"\n)\n\nmax_y = ((time_gb, time_gbes))\n\n(index + bar_width, names)\n((0, 1.3 * max_y, 13))\n\nautolabel(bar1, n_gb)\nautolabel(bar2, n_gbes)\n\n([0, 1.3 * max_y])\n(loc=\"best\")\n(True)\n\n(\"Datasets\")\n(\"Fit Time\")\n\n()\n\n\n<img alt=\"plot gradient boosting early stopping\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_early_stopping_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_early_stopping_002.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  3.107 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Ensemble methods->Feature importances with a forest of trees": [
        [
            "This example shows the use of a forest of trees to evaluate the importance of\nfeatures on an artificial classification task. The blue bars are the feature\nimportances of the forest, along with their inter-trees variability represented\nby the error bars.",
            "markdown"
        ],
        [
            "As expected, the plot suggests that 3 features are informative, while the\nremaining are not.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt",
            "code"
        ]
    ],
    "Examples->Ensemble methods->Feature importances with a forest of trees->Data generation and model fitting": [
        [
            "We generate a synthetic dataset with only 3 informative features. We will\nexplicitly not shuffle the dataset to ensure that the informative features\nwill correspond to the three first columns of X. In addition, we will split\nour dataset into training and testing subsets.",
            "markdown"
        ],
        [
            "from sklearn.datasets import \nfrom sklearn.model_selection import \n\nX, y = (\n    n_samples=1000,\n    n_features=10,\n    n_informative=3,\n    n_redundant=0,\n    n_repeated=0,\n    n_classes=2,\n    random_state=0,\n    shuffle=False,\n)\nX_train, X_test, y_train, y_test = (X, y, stratify=y, random_state=42)",
            "code"
        ],
        [
            "A random forest classifier will be fitted to compute the feature importances.",
            "markdown"
        ],
        [
            "from sklearn.ensemble import \n\nfeature_names = [f\"feature {i}\" for i in range(X.shape[1])]\nforest = (random_state=0)\nforest.fit(X_train, y_train)",
            "code"
        ],
        [
            "RandomForestClassifier(random_state=0)<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input checked=\"\" class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-89\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-89\">RandomForestClassifier</label>",
            "code"
        ],
        [
            "RandomForestClassifier(random_state=0)\n\n<br/>\n<br/>",
            "code"
        ]
    ],
    "Examples->Ensemble methods->Feature importances with a forest of trees->Feature importance based on mean decrease in impurity": [
        [
            "Feature importances are provided by the fitted attribute\nfeature_importances_ and they are computed as the mean and standard\ndeviation of accumulation of the impurity decrease within each tree.",
            "markdown"
        ],
        [
            "Warning",
            "markdown"
        ],
        [
            "Impurity-based feature importances can be misleading for <strong>high\ncardinality</strong> features (many unique values). See\n as an alternative below.",
            "markdown"
        ],
        [
            "import time\nimport numpy as np\n\nstart_time = ()\nimportances = forest.feature_importances_\nstd = ([tree.feature_importances_ for tree in forest.estimators_], axis=0)\nelapsed_time = () - start_time\n\nprint(f\"Elapsed time to compute the importances: {elapsed_time:.3f} seconds\")",
            "code"
        ],
        [
            "Elapsed time to compute the importances: 0.007 seconds",
            "code"
        ],
        [
            "Let\u2019s plot the impurity-based importance.",
            "markdown"
        ],
        [
            "import pandas as pd\n\nforest_importances = (importances, index=feature_names)\n\nfig, ax = ()\nforest_importances.plot.bar(yerr=std, ax=ax)\nax.set_title(\"Feature importances using MDI\")\nax.set_ylabel(\"Mean decrease in impurity\")\nfig.tight_layout()\n\n\n<img alt=\"Feature importances using MDI\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_forest_importances_001.png\" srcset=\"../../_images/sphx_glr_plot_forest_importances_001.png\"/>",
            "code"
        ],
        [
            "We observe that, as expected, the three first features are found important.",
            "markdown"
        ]
    ],
    "Examples->Ensemble methods->Feature importances with a forest of trees->Feature importance based on feature permutation": [
        [
            "Permutation feature importance overcomes limitations of the impurity-based\nfeature importance: they do not have a bias toward high-cardinality features\nand can be computed on a left-out test set.",
            "markdown"
        ],
        [
            "from sklearn.inspection import \n\nstart_time = ()\nresult = (\n    forest, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nelapsed_time = () - start_time\nprint(f\"Elapsed time to compute the importances: {elapsed_time:.3f} seconds\")\n\nforest_importances = (result.importances_mean, index=feature_names)",
            "code"
        ],
        [
            "Elapsed time to compute the importances: 0.713 seconds",
            "code"
        ],
        [
            "The computation for full permutation importance is more costly. Features are\nshuffled n times and the model refitted to estimate the importance of it.\nPlease see  for more details. We can now plot\nthe importance ranking.",
            "markdown"
        ],
        [
            "fig, ax = ()\nforest_importances.plot.bar(yerr=result.importances_std, ax=ax)\nax.set_title(\"Feature importances using permutation on full model\")\nax.set_ylabel(\"Mean accuracy decrease\")\nfig.tight_layout()\n()\n\n\n<img alt=\"Feature importances using permutation on full model\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_forest_importances_002.png\" srcset=\"../../_images/sphx_glr_plot_forest_importances_002.png\"/>",
            "code"
        ],
        [
            "The same features are detected as most important using both methods. Although\nthe relative importances vary. As seen on the plots, MDI is less likely than\npermutation importance to fully omit a feature.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  1.178 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Ensemble methods->Feature transformations with ensembles of trees": [
        [
            "Transform your features into a higher dimensional, sparse space. Then train a\nlinear model on these features.",
            "markdown"
        ],
        [
            "First fit an ensemble of trees (totally random trees, a random forest, or\ngradient boosted trees) on the training set. Then each leaf of each tree in the\nensemble is assigned a fixed arbitrary feature index in a new feature space.\nThese leaf indices are then encoded in a one-hot fashion.",
            "markdown"
        ],
        [
            "Each sample goes through the decisions of each tree of the ensemble and ends up\nin one leaf per tree. The sample is encoded by setting feature values for these\nleaves to 1 and the other feature values to 0.",
            "markdown"
        ],
        [
            "The resulting transformer has then learned a supervised, sparse,\nhigh-dimensional categorical embedding of the data.",
            "markdown"
        ],
        [
            "# Author: Tim Head &lt;betatim@gmail.com\n#\n# License: BSD 3 clause",
            "code"
        ],
        [
            "First, we will create a large dataset and split it into three sets:",
            "markdown"
        ],
        [
            "a set to train the ensemble methods which are later used to as a feature\nengineering transformer;",
            "markdown"
        ],
        [
            "a set to train the linear model;",
            "markdown"
        ],
        [
            "a set to test the linear model.",
            "markdown"
        ],
        [
            "It is important to split the data in such way to avoid overfitting by leaking\ndata.",
            "markdown"
        ],
        [
            "from sklearn.datasets import \nfrom sklearn.model_selection import \n\nX, y = (n_samples=80_000, random_state=10)\n\nX_full_train, X_test, y_full_train, y_test = (\n    X, y, test_size=0.5, random_state=10\n)\nX_train_ensemble, X_train_linear, y_train_ensemble, y_train_linear = (\n    X_full_train, y_full_train, test_size=0.5, random_state=10\n)",
            "code"
        ],
        [
            "For each of the ensemble methods, we will use 10 estimators and a maximum\ndepth of 3 levels.",
            "markdown"
        ],
        [
            "n_estimators = 10\nmax_depth = 3",
            "code"
        ],
        [
            "First, we will start by training the random forest and gradient boosting on\nthe separated training set",
            "markdown"
        ],
        [
            "from sklearn.ensemble import , \n\nrandom_forest = (\n    n_estimators=n_estimators, max_depth=max_depth, random_state=10\n)\nrandom_forest.fit(X_train_ensemble, y_train_ensemble)\n\ngradient_boosting = (\n    n_estimators=n_estimators, max_depth=max_depth, random_state=10\n)\n_ = gradient_boosting.fit(X_train_ensemble, y_train_ensemble)",
            "code"
        ],
        [
            "Notice that  is much\nfaster than  starting\nwith intermediate datasets (n_samples >= 10_000), which is not the case of\nthe present example.",
            "markdown"
        ],
        [
            "The  is an unsupervised method\nand thus does not required to be trained independently.",
            "markdown"
        ],
        [
            "from sklearn.ensemble import \n\nrandom_tree_embedding = (\n    n_estimators=n_estimators, max_depth=max_depth, random_state=0\n)",
            "code"
        ],
        [
            "Now, we will create three pipelines that will use the above embedding as\na preprocessing stage.",
            "markdown"
        ],
        [
            "The random trees embedding can be directly pipelined with the logistic\nregression because it is a standard scikit-learn transformer.",
            "markdown"
        ],
        [
            "from sklearn.linear_model import \nfrom sklearn.pipeline import \n\nrt_model = (random_tree_embedding, (max_iter=1000))\nrt_model.fit(X_train_linear, y_train_linear)",
            "code"
        ],
        [
            "Pipeline(steps=[('randomtreesembedding',\n                 RandomTreesEmbedding(max_depth=3, n_estimators=10,\n                                      random_state=0)),\n                ('logisticregression', LogisticRegression(max_iter=1000))])<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-90\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-90\">Pipeline</label>",
            "code"
        ],
        [
            "Pipeline(steps=[('randomtreesembedding',\n                 RandomTreesEmbedding(max_depth=3, n_estimators=10,\n                                      random_state=0)),\n                ('logisticregression', LogisticRegression(max_iter=1000))])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-91\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-91\">RandomTreesEmbedding</label>",
            "code"
        ],
        [
            "RandomTreesEmbedding(max_depth=3, n_estimators=10, random_state=0)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-92\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-92\">LogisticRegression</label>",
            "code"
        ],
        [
            "LogisticRegression(max_iter=1000)\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "Then, we can pipeline random forest or gradient boosting with a logistic\nregression. However, the feature transformation will happen by calling the\nmethod apply. The pipeline in scikit-learn expects a call to transform.\nTherefore, we wrapped the call to apply within a FunctionTransformer.",
            "markdown"
        ],
        [
            "from sklearn.preprocessing import \nfrom sklearn.preprocessing import \n\n\ndef rf_apply(X, model):\n    return model.apply(X)\n\n\nrf_leaves_yielder = (rf_apply, kw_args={\"model\": random_forest})\n\nrf_model = (\n    rf_leaves_yielder,\n    (handle_unknown=\"ignore\"),\n    (max_iter=1000),\n)\nrf_model.fit(X_train_linear, y_train_linear)",
            "code"
        ],
        [
            "Pipeline(steps=[('functiontransformer',\n                 FunctionTransformer(func=&lt;function rf_apply at 0x7f0d54bd1f70,\n                                     kw_args={'model': RandomForestClassifier(max_depth=3,\n                                                                              n_estimators=10,\n                                                                              random_state=10)})),\n                ('onehotencoder', OneHotEncoder(handle_unknown='ignore')),\n                ('logisticregression', LogisticRegression(max_iter=1000))])<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-93\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-93\">Pipeline</label>",
            "code"
        ],
        [
            "Pipeline(steps=[('functiontransformer',\n                 FunctionTransformer(func=&lt;function rf_apply at 0x7f0d54bd1f70,\n                                     kw_args={'model': RandomForestClassifier(max_depth=3,\n                                                                              n_estimators=10,\n                                                                              random_state=10)})),\n                ('onehotencoder', OneHotEncoder(handle_unknown='ignore')),\n                ('logisticregression', LogisticRegression(max_iter=1000))])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-94\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-94\">FunctionTransformer</label>",
            "code"
        ],
        [
            "FunctionTransformer(func=&lt;function rf_apply at 0x7f0d54bd1f70,\n                    kw_args={'model': RandomForestClassifier(max_depth=3,\n                                                             n_estimators=10,\n                                                             random_state=10)})<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-95\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-95\">OneHotEncoder</label>",
            "code"
        ],
        [
            "OneHotEncoder(handle_unknown='ignore')<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-96\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-96\">LogisticRegression</label>",
            "code"
        ],
        [
            "LogisticRegression(max_iter=1000)\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "def gbdt_apply(X, model):\n    return model.apply(X)[:, :, 0]\n\n\ngbdt_leaves_yielder = (\n    gbdt_apply, kw_args={\"model\": gradient_boosting}\n)\n\ngbdt_model = (\n    gbdt_leaves_yielder,\n    (handle_unknown=\"ignore\"),\n    (max_iter=1000),\n)\ngbdt_model.fit(X_train_linear, y_train_linear)",
            "code"
        ],
        [
            "Pipeline(steps=[('functiontransformer',\n                 FunctionTransformer(func=&lt;function gbdt_apply at 0x7f0d549dbe50,\n                                     kw_args={'model': GradientBoostingClassifier(n_estimators=10,\n                                                                                  random_state=10)})),\n                ('onehotencoder', OneHotEncoder(handle_unknown='ignore')),\n                ('logisticregression', LogisticRegression(max_iter=1000))])<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-97\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-97\">Pipeline</label>",
            "code"
        ],
        [
            "Pipeline(steps=[('functiontransformer',\n                 FunctionTransformer(func=&lt;function gbdt_apply at 0x7f0d549dbe50,\n                                     kw_args={'model': GradientBoostingClassifier(n_estimators=10,\n                                                                                  random_state=10)})),\n                ('onehotencoder', OneHotEncoder(handle_unknown='ignore')),\n                ('logisticregression', LogisticRegression(max_iter=1000))])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-98\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-98\">FunctionTransformer</label>",
            "code"
        ],
        [
            "FunctionTransformer(func=&lt;function gbdt_apply at 0x7f0d549dbe50,\n                    kw_args={'model': GradientBoostingClassifier(n_estimators=10,\n                                                                 random_state=10)})<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-99\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-99\">OneHotEncoder</label>",
            "code"
        ],
        [
            "OneHotEncoder(handle_unknown='ignore')<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-100\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-100\">LogisticRegression</label>",
            "code"
        ],
        [
            "LogisticRegression(max_iter=1000)\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "We can finally show the different ROC curves for all the models.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\nfrom sklearn.metrics import RocCurveDisplay\n\nfig, ax = ()\n\nmodels = [\n    (\"RT embedding - LR\", rt_model),\n    (\"RF\", random_forest),\n    (\"RF embedding - LR\", rf_model),\n    (\"GBDT\", gradient_boosting),\n    (\"GBDT embedding - LR\", gbdt_model),\n]\n\nmodel_displays = {}\nfor name, pipeline in models:\n    model_displays[name] = (\n        pipeline, X_test, y_test, ax=ax, name=name\n    )\n_ = ax.set_title(\"ROC curve\")\n\n\n<img alt=\"ROC curve\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_feature_transformation_001.png\" srcset=\"../../_images/sphx_glr_plot_feature_transformation_001.png\"/>",
            "code"
        ],
        [
            "fig, ax = ()\nfor name, pipeline in models:\n    model_displays[name].plot(ax=ax)\n\nax.set_xlim(0, 0.2)\nax.set_ylim(0.8, 1)\n_ = ax.set_title(\"ROC curve (zoomed in at top left)\")\n\n\n<img alt=\"ROC curve (zoomed in at top left)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_feature_transformation_002.png\" srcset=\"../../_images/sphx_glr_plot_feature_transformation_002.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  2.788 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Ensemble methods->Gradient Boosting Out-of-Bag estimates": [
        [
            "Out-of-bag (OOB) estimates can be a useful heuristic to estimate\nthe \u201coptimal\u201d number of boosting iterations.\nOOB estimates are almost identical to cross-validation estimates but\nthey can be computed on-the-fly without the need for repeated model\nfitting.\nOOB estimates are only available for Stochastic Gradient Boosting\n(i.e. subsample &lt; 1.0), the estimates are derived from the improvement\nin loss based on the examples not included in the bootstrap sample\n(the so-called out-of-bag examples).\nThe OOB estimator is a pessimistic estimator of the true\ntest loss, but remains a fairly good approximation for a small number of trees.\nThe figure shows the cumulative sum of the negative OOB improvements\nas a function of the boosting iteration. As you can see, it tracks the test\nloss for the first hundred iterations but then diverges in a\npessimistic way.\nThe figure also shows the performance of 3-fold cross validation which\nusually gives a better estimate of the test loss\nbut is computationally more demanding.\n<img alt=\"plot gradient boosting oob\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_oob_001.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_oob_001.png\"/>",
            "markdown"
        ],
        [
            "Accuracy: 0.6820\n\n\n\n<br/>",
            "code"
        ],
        [
            "# Author: Peter Prettenhofer &lt;peter.prettenhofer@gmail.com\n#\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import ensemble\nfrom sklearn.model_selection import \nfrom sklearn.model_selection import \nfrom sklearn.metrics import \n\nfrom scipy.special import \n\n# Generate data (adapted from G. Ridgeway's gbm example)\nn_samples = 1000\nrandom_state = (13)\nx1 = random_state.uniform(size=n_samples)\nx2 = random_state.uniform(size=n_samples)\nx3 = random_state.randint(0, 4, size=n_samples)\n\np = ((3 * x1) - 4 * x2 + x3)\ny = random_state.binomial(1, p, size=n_samples)\n\nX = [x1, x2, x3]\n\nX = X.astype()\nX_train, X_test, y_train, y_test = (X, y, test_size=0.5, random_state=9)\n\n# Fit classifier with out-of-bag estimates\nparams = {\n    \"n_estimators\": 1200,\n    \"max_depth\": 3,\n    \"subsample\": 0.5,\n    \"learning_rate\": 0.01,\n    \"min_samples_leaf\": 1,\n    \"random_state\": 3,\n}\nclf = (**params)\n\nclf.fit(X_train, y_train)\nacc = clf.score(X_test, y_test)\nprint(\"Accuracy: {:.4f}\".format(acc))\n\nn_estimators = params[\"n_estimators\"]\nx = (n_estimators) + 1\n\n\ndef heldout_score(clf, X_test, y_test):\n    \"\"\"compute deviance scores on ``X_test`` and ``y_test``.\"\"\"\n    score = ((n_estimators,), dtype=)\n    for i, y_proba in enumerate(clf.staged_predict_proba(X_test)):\n        score[i] = 2 * (y_test, y_proba[:, 1])\n    return score\n\n\ndef cv_estimate(n_splits=None):\n    cv = (n_splits=n_splits)\n    cv_clf = (**params)\n    val_scores = ((n_estimators,), dtype=)\n    for train, test in cv.split(X_train, y_train):\n        cv_clf.fit(X_train[train], y_train[train])\n        val_scores += heldout_score(cv_clf, X_train[test], y_train[test])\n    val_scores /= n_splits\n    return val_scores\n\n\n# Estimate best n_estimator using cross-validation\ncv_score = cv_estimate(3)\n\n# Compute best n_estimator for test data\ntest_score = heldout_score(clf, X_test, y_test)\n\n# negative cumulative sum of oob improvements\ncumsum = -(clf.oob_improvement_)\n\n# min loss according to OOB\noob_best_iter = x[(cumsum)]\n\n# min loss according to test (normalize such that first loss is 0)\ntest_score -= test_score[0]\ntest_best_iter = x[(test_score)]\n\n# min loss according to cv (normalize such that first loss is 0)\ncv_score -= cv_score[0]\ncv_best_iter = x[(cv_score)]\n\n# color brew for the three curves\noob_color = list(map(lambda x: x / 256.0, (190, 174, 212)))\ntest_color = list(map(lambda x: x / 256.0, (127, 201, 127)))\ncv_color = list(map(lambda x: x / 256.0, (253, 192, 134)))\n\n# line type for the three curves\noob_line = \"dashed\"\ntest_line = \"solid\"\ncv_line = \"dashdot\"\n\n# plot curves and vertical lines for best iterations\n(figsize=(8, 4.8))\n(x, cumsum, label=\"OOB loss\", color=oob_color, linestyle=oob_line)\n(x, test_score, label=\"Test loss\", color=test_color, linestyle=test_line)\n(x, cv_score, label=\"CV loss\", color=cv_color, linestyle=cv_line)\n(x=oob_best_iter, color=oob_color, linestyle=oob_line)\n(x=test_best_iter, color=test_color, linestyle=test_line)\n(x=cv_best_iter, color=cv_color, linestyle=cv_line)\n\n# add three vertical lines to xticks\nxticks = ()\nxticks_pos = (\n    xticks[0].tolist() + [oob_best_iter, cv_best_iter, test_best_iter]\n)\nxticks_label = (list(map(lambda t: int(t), xticks[0])) + [\"OOB\", \"CV\", \"Test\"])\nind = (xticks_pos)\nxticks_pos = xticks_pos[ind]\nxticks_label = xticks_label[ind]\n(xticks_pos, xticks_label, rotation=90)\n\n(loc=\"upper center\")\n(\"normalized loss\")\n(\"number of iterations\")\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  8.293 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Ensemble methods->Gradient Boosting regression": [
        [
            "This example demonstrates Gradient Boosting to produce a predictive\nmodel from an ensemble of weak predictive models. Gradient boosting can be used\nfor regression and classification problems. Here, we will train a model to\ntackle a diabetes regression task. We will obtain the results from\n with least squares loss\nand 500 regression trees of depth 4.",
            "markdown"
        ],
        [
            "Note: For larger datasets (n_samples >= 10000), please refer to\n.",
            "markdown"
        ],
        [
            "# Author: Peter Prettenhofer &lt;peter.prettenhofer@gmail.com\n#         Maria Telenczuk &lt;https://github.com/maikia\n#         Katrina Ni &lt;https://github.com/nilichen\n#\n# License: BSD 3 clause\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import datasets, ensemble\nfrom sklearn.inspection import \nfrom sklearn.metrics import \nfrom sklearn.model_selection import",
            "code"
        ]
    ],
    "Examples->Ensemble methods->Gradient Boosting regression->Load the data": [
        [
            "First we need to load the data.",
            "markdown"
        ],
        [
            "diabetes = ()\nX, y = diabetes.data, diabetes.target",
            "code"
        ]
    ],
    "Examples->Ensemble methods->Gradient Boosting regression->Data preprocessing": [
        [
            "Next, we will split our dataset to use 90% for training and leave the rest\nfor testing. We will also set the regression model parameters. You can play\nwith these parameters to see how the results change.",
            "markdown"
        ],
        [
            "n_estimators : the number of boosting stages that will be performed.\nLater, we will plot deviance against boosting iterations.",
            "markdown"
        ],
        [
            "max_depth : limits the number of nodes in the tree.\nThe best value depends on the interaction of the input variables.",
            "markdown"
        ],
        [
            "min_samples_split : the minimum number of samples required to split an\ninternal node.",
            "markdown"
        ],
        [
            "learning_rate : how much the contribution of each tree will shrink.",
            "markdown"
        ],
        [
            "loss : loss function to optimize. The least squares function is  used in\nthis case however, there are many other options (see\n ).",
            "markdown"
        ],
        [
            "X_train, X_test, y_train, y_test = (\n    X, y, test_size=0.1, random_state=13\n)\n\nparams = {\n    \"n_estimators\": 500,\n    \"max_depth\": 4,\n    \"min_samples_split\": 5,\n    \"learning_rate\": 0.01,\n    \"loss\": \"squared_error\",\n}",
            "code"
        ]
    ],
    "Examples->Ensemble methods->Gradient Boosting regression->Fit regression model": [
        [
            "Now we will initiate the gradient boosting regressors and fit it with our\ntraining data. Let\u2019s also look and the mean squared error on the test data.",
            "markdown"
        ],
        [
            "reg = (**params)\nreg.fit(X_train, y_train)\n\nmse = (y_test, reg.predict(X_test))\nprint(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))",
            "code"
        ],
        [
            "The mean squared error (MSE) on test set: 3025.7877",
            "code"
        ]
    ],
    "Examples->Ensemble methods->Gradient Boosting regression->Plot training deviance": [
        [
            "Finally, we will visualize the results. To do that we will first compute the\ntest set deviance and then plot it against boosting iterations.",
            "markdown"
        ],
        [
            "test_score = ((params[\"n_estimators\"],), dtype=)\nfor i, y_pred in enumerate(reg.staged_predict(X_test)):\n    test_score[i] = (y_test, y_pred)\n\nfig = (figsize=(6, 6))\n(1, 1, 1)\n(\"Deviance\")\n(\n    (params[\"n_estimators\"]) + 1,\n    reg.train_score_,\n    \"b-\",\n    label=\"Training Set Deviance\",\n)\n(\n    (params[\"n_estimators\"]) + 1, test_score, \"r-\", label=\"Test Set Deviance\"\n)\n(loc=\"upper right\")\n(\"Boosting Iterations\")\n(\"Deviance\")\nfig.tight_layout()\n()\n\n\n<img alt=\"Deviance\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_regression_001.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_regression_001.png\"/>",
            "code"
        ]
    ],
    "Examples->Ensemble methods->Gradient Boosting regression->Plot feature importance": [
        [
            "Warning",
            "markdown"
        ],
        [
            "Careful, impurity-based feature importances can be misleading for\n<strong>high cardinality</strong> features (many unique values). As an alternative,\nthe permutation importances of reg can be computed on a\nheld out test set. See  for more details.",
            "markdown"
        ],
        [
            "For this example, the impurity-based and permutation methods identify the\nsame 2 strongly predictive features but not in the same order. The third most\npredictive feature, \u201cbp\u201d, is also the same for the 2 methods. The remaining\nfeatures are less predictive and the error bars of the permutation plot\nshow that they overlap with 0.",
            "markdown"
        ],
        [
            "feature_importance = reg.feature_importances_\nsorted_idx = (feature_importance)\npos = (sorted_idx.shape[0]) + 0.5\nfig = (figsize=(12, 6))\n(1, 2, 1)\n(pos, feature_importance[sorted_idx], align=\"center\")\n(pos, (diabetes.feature_names)[sorted_idx])\n(\"Feature Importance (MDI)\")\n\nresult = (\n    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_idx = result.importances_mean.argsort()\n(1, 2, 2)\n(\n    result.importances[sorted_idx].T,\n    vert=False,\n    labels=(diabetes.feature_names)[sorted_idx],\n)\n(\"Permutation Importance (test set)\")\nfig.tight_layout()\n()\n\n\n<img alt=\"Feature Importance (MDI), Permutation Importance (test set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  1.214 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Ensemble methods->Gradient Boosting regularization": [
        [
            "Illustration of the effect of different regularization strategies\nfor Gradient Boosting. The example is taken from Hastie et al 2009 .",
            "markdown"
        ],
        [
            "The loss function used is binomial deviance. Regularization via\nshrinkage (learning_rate &lt; 1.0) improves performance considerably.\nIn combination with shrinkage, stochastic gradient boosting\n(subsample &lt; 1.0) can produce more accurate models by reducing the\nvariance via bagging.\nSubsampling without shrinkage usually does poorly.\nAnother strategy to reduce the variance is by subsampling the features\nanalogous to the random splits in Random Forests\n(via the max_features parameter).\n\n\n[]",
            "markdown"
        ],
        [
            "T. Hastie, R. Tibshirani and J. Friedman, \u201cElements of Statistical\nLearning Ed. 2\u201d, Springer, 2009.\n\n\n<img alt=\"plot gradient boosting regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_regularization_001.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_regularization_001.png\"/>",
            "markdown"
        ],
        [
            "# Author: Peter Prettenhofer &lt;peter.prettenhofer@gmail.com\n#\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import ensemble\nfrom sklearn import datasets\nfrom sklearn.metrics import \nfrom sklearn.model_selection import \n\nX, y = (n_samples=4000, random_state=1)\n\n# map labels from {-1, 1} to {0, 1}\nlabels, y = (y, return_inverse=True)\n\nX_train, X_test, y_train, y_test = (X, y, test_size=0.8, random_state=0)\n\noriginal_params = {\n    \"n_estimators\": 400,\n    \"max_leaf_nodes\": 4,\n    \"max_depth\": None,\n    \"random_state\": 2,\n    \"min_samples_split\": 5,\n}\n\n()\n\nfor label, color, setting in [\n    (\"No shrinkage\", \"orange\", {\"learning_rate\": 1.0, \"subsample\": 1.0}),\n    (\"learning_rate=0.2\", \"turquoise\", {\"learning_rate\": 0.2, \"subsample\": 1.0}),\n    (\"subsample=0.5\", \"blue\", {\"learning_rate\": 1.0, \"subsample\": 0.5}),\n    (\n        \"learning_rate=0.2, subsample=0.5\",\n        \"gray\",\n        {\"learning_rate\": 0.2, \"subsample\": 0.5},\n    ),\n    (\n        \"learning_rate=0.2, max_features=2\",\n        \"magenta\",\n        {\"learning_rate\": 0.2, \"max_features\": 2},\n    ),\n]:\n    params = dict(original_params)\n    params.update(setting)\n\n    clf = (**params)\n    clf.fit(X_train, y_train)\n\n    # compute test set deviance\n    test_deviance = ((params[\"n_estimators\"],), dtype=)\n\n    for i, y_proba in enumerate(clf.staged_predict_proba(X_test)):\n        test_deviance[i] = 2 * (y_test, y_proba[:, 1])\n\n    (\n        ((test_deviance.shape[0]) + 1)[::5],\n        test_deviance[::5],\n        \"-\",\n        color=color,\n        label=label,\n    )\n\n(loc=\"upper right\")\n(\"Boosting Iterations\")\n(\"Test Set Deviance\")\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  7.315 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Ensemble methods->Hashing feature transformation using Totally Random Trees": [
        [
            "RandomTreesEmbedding provides a way to map data to a\nvery high-dimensional, sparse representation, which might\nbe beneficial for classification.\nThe mapping is completely unsupervised and very efficient.",
            "markdown"
        ],
        [
            "This example visualizes the partitions given by several\ntrees and shows how the transformation can also be used for\nnon-linear dimensionality reduction or non-linear classification.",
            "markdown"
        ],
        [
            "Points that are neighboring often share the same leaf of a tree and therefore\nshare large parts of their hashed representation. This allows to\nseparate two concentric circles simply based on the principal components\nof the transformed data with truncated SVD.",
            "markdown"
        ],
        [
            "In high-dimensional spaces, linear classifiers often achieve\nexcellent accuracy. For sparse binary data, BernoulliNB\nis particularly well-suited. The bottom row compares the\ndecision boundary obtained by BernoulliNB in the transformed\nspace with an ExtraTreesClassifier forests learned on the\noriginal data.\n<img alt=\"Original Data (2d), Truncated SVD reduction (2d) of transformed data (74d), Naive Bayes on Transformed data, ExtraTrees predictions\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_random_forest_embedding_001.png\" srcset=\"../../_images/sphx_glr_plot_random_forest_embedding_001.png\"/>",
            "markdown"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import \nfrom sklearn.ensemble import , \nfrom sklearn.decomposition import \nfrom sklearn.naive_bayes import \n\n# make a synthetic dataset\nX, y = (factor=0.5, random_state=0, noise=0.05)\n\n# use RandomTreesEmbedding to transform data\nhasher = (n_estimators=10, random_state=0, max_depth=3)\nX_transformed = hasher.fit_transform(X)\n\n# Visualize result after dimensionality reduction using truncated SVD\nsvd = (n_components=2)\nX_reduced = svd.fit_transform(X_transformed)\n\n# Learn a Naive Bayes classifier on the transformed data\nnb = ()\nnb.fit(X_transformed, y)\n\n\n# Learn an ExtraTreesClassifier for comparison\ntrees = (max_depth=3, n_estimators=10, random_state=0)\ntrees.fit(X, y)\n\n\n# scatter plot of original and reduced data\nfig = (figsize=(9, 8))\n\nax = (221)\nax.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolor=\"k\")\nax.set_title(\"Original Data (2d)\")\nax.set_xticks(())\nax.set_yticks(())\n\nax = (222)\nax.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, s=50, edgecolor=\"k\")\nax.set_title(\n    \"Truncated SVD reduction (2d) of transformed data (%dd)\" % X_transformed.shape[1]\n)\nax.set_xticks(())\nax.set_yticks(())\n\n# Plot the decision in original space. For that, we will assign a color\n# to each point in the mesh [x_min, x_max]x[y_min, y_max].\nh = 0.01\nx_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\ny_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\nxx, yy = ((x_min, x_max, h), (y_min, y_max, h))\n\n# transform grid using RandomTreesEmbedding\ntransformed_grid = hasher.transform([xx.ravel(), yy.ravel()])\ny_grid_pred = nb.predict_proba(transformed_grid)[:, 1]\n\nax = (223)\nax.set_title(\"Naive Bayes on Transformed data\")\nax.pcolormesh(xx, yy, y_grid_pred.reshape(xx.shape))\nax.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolor=\"k\")\nax.set_ylim(-1.4, 1.4)\nax.set_xlim(-1.4, 1.4)\nax.set_xticks(())\nax.set_yticks(())\n\n# transform grid using ExtraTreesClassifier\ny_grid_pred = trees.predict_proba([xx.ravel(), yy.ravel()])[:, 1]\n\nax = (224)\nax.set_title(\"ExtraTrees predictions\")\nax.pcolormesh(xx, yy, y_grid_pred.reshape(xx.shape))\nax.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolor=\"k\")\nax.set_ylim(-1.4, 1.4)\nax.set_xlim(-1.4, 1.4)\nax.set_xticks(())\nax.set_yticks(())\n\n()\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.378 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Ensemble methods->IsolationForest example": [
        [
            "An example using  for anomaly\ndetection.",
            "markdown"
        ],
        [
            "The  is an ensemble of \u201cIsolation Trees\u201d that \u201cisolate\u201d\nobservations by recursive random partitioning, which can be represented by a\ntree structure. The number of splittings required to isolate a sample is lower\nfor outliers and higher for inliers.",
            "markdown"
        ],
        [
            "In the present example we demo two ways to visualize the decision boundary of an\nIsolation Forest trained on a toy dataset.",
            "markdown"
        ]
    ],
    "Examples->Ensemble methods->IsolationForest example->Data generation": [
        [
            "We generate two clusters (each one containing n_samples) by randomly\nsampling the standard normal distribution as returned by\n. One of them is spherical and the other one is\nslightly deformed.",
            "markdown"
        ],
        [
            "For consistency with the  notation,\nthe inliers (i.e. the gaussian clusters) are assigned a ground truth label 1\nwhereas the outliers (created with ) are assigned\nthe label -1.",
            "markdown"
        ],
        [
            "import numpy as np\nfrom sklearn.model_selection import \n\nn_samples, n_outliers = 120, 40\nrng = (0)\ncovariance = ([[0.5, -0.1], [0.7, 0.4]])\ncluster_1 = 0.4 * rng.randn(n_samples, 2) @ covariance + ([2, 2])  # general\ncluster_2 = 0.3 * rng.randn(n_samples, 2) + ([-2, -2])  # spherical\noutliers = rng.uniform(low=-4, high=4, size=(n_outliers, 2))\n\nX = ([cluster_1, cluster_2, outliers])\ny = (\n    [((2 * n_samples), dtype=int), -((n_outliers), dtype=int)]\n)\n\nX_train, X_test, y_train, y_test = (X, y, stratify=y, random_state=42)",
            "code"
        ],
        [
            "We can visualize the resulting clusters:",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n\nscatter = (X[:, 0], X[:, 1], c=y, s=20, edgecolor=\"k\")\nhandles, labels = scatter.legend_elements()\n(\"square\")\n(handles=handles, labels=[\"outliers\", \"inliers\"], title=\"true class\")\n(\"Gaussian inliers with \\nuniformly distributed outliers\")\n()\n\n\n<img alt=\"Gaussian inliers with  uniformly distributed outliers\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_isolation_forest_001.png\" srcset=\"../../_images/sphx_glr_plot_isolation_forest_001.png\"/>",
            "code"
        ]
    ],
    "Examples->Ensemble methods->IsolationForest example->Training of the model": [
        [
            "from sklearn.ensemble import \n\nclf = (max_samples=100, random_state=0)\nclf.fit(X_train)",
            "code"
        ],
        [
            "IsolationForest(max_samples=100, random_state=0)<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input checked=\"\" class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-101\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-101\">IsolationForest</label>",
            "code"
        ],
        [
            "IsolationForest(max_samples=100, random_state=0)\n\n<br/>\n<br/>",
            "code"
        ]
    ],
    "Examples->Ensemble methods->IsolationForest example->Plot discrete decision boundary": [
        [
            "We use the class  to\nvisualize a discrete decision boundary. The background color represents\nwhether a sample in that given area is predicted to be an outlier\nor not. The scatter plot displays the true labels.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\ndisp = (\n    clf,\n    X,\n    response_method=\"predict\",\n    alpha=0.5,\n)\ndisp.ax_.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor=\"k\")\ndisp.ax_.set_title(\"Binary decision boundary \\nof IsolationForest\")\n(\"square\")\n(handles=handles, labels=[\"outliers\", \"inliers\"], title=\"true class\")\n()\n\n\n<img alt=\"Binary decision boundary  of IsolationForest\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_isolation_forest_002.png\" srcset=\"../../_images/sphx_glr_plot_isolation_forest_002.png\"/>",
            "code"
        ]
    ],
    "Examples->Ensemble methods->IsolationForest example->Plot path length decision boundary": [
        [
            "By setting the response_method=\"decision_function\", the background of the\n represents the measure of\nnormality of an observation. Such score is given by the path length averaged\nover a forest of random trees, which itself is given by the depth of the leaf\n(or equivalently the number of splits) required to isolate a given sample.",
            "markdown"
        ],
        [
            "When a forest of random trees collectively produce short path lengths for\nisolating some particular samples, they are highly likely to be anomalies and\nthe measure of normality is close to 0. Similarly, large paths correspond to\nvalues close to 1 and are more likely to be inliers.",
            "markdown"
        ],
        [
            "disp = (\n    clf,\n    X,\n    response_method=\"decision_function\",\n    alpha=0.5,\n)\ndisp.ax_.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor=\"k\")\ndisp.ax_.set_title(\"Path length decision boundary \\nof IsolationForest\")\n(\"square\")\n(handles=handles, labels=[\"outliers\", \"inliers\"], title=\"true class\")\n(disp.ax_.collections[1])\n()\n\n\n<img alt=\"Path length decision boundary  of IsolationForest\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_isolation_forest_003.png\" srcset=\"../../_images/sphx_glr_plot_isolation_forest_003.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.669 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Ensemble methods->Monotonic Constraints": [
        [
            "This example illustrates the effect of monotonic constraints on a gradient\nboosting estimator.",
            "markdown"
        ],
        [
            "We build an artificial dataset where the target value is in general\npositively correlated with the first feature (with some random and\nnon-random variations), and in general negatively correlated with the second\nfeature.",
            "markdown"
        ],
        [
            "By imposing a monotonic increase or a monotonic decrease constraint, respectively,\non the features during the learning process, the estimator is able to properly follow\nthe general trend instead of being subject to the variations.",
            "markdown"
        ],
        [
            "This example was inspired by the .",
            "markdown"
        ],
        [
            "from sklearn.ensemble import \nfrom sklearn.inspection import PartialDependenceDisplay\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nrng = (0)\n\nn_samples = 1000\nf_0 = rng.rand(n_samples)\nf_1 = rng.rand(n_samples)\nX = [f_0, f_1]\nnoise = rng.normal(loc=0.0, scale=0.01, size=n_samples)\n\n# y is positively correlated with f_0, and negatively correlated with f_1\ny = 5 * f_0 + (10 *  * f_0) - 5 * f_1 - (10 *  * f_1) + noise",
            "code"
        ],
        [
            "Fit a first model on this dataset without any constraints.",
            "markdown"
        ],
        [
            "gbdt_no_cst = ()\ngbdt_no_cst.fit(X, y)",
            "code"
        ],
        [
            "HistGradientBoostingRegressor()<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input checked=\"\" class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-102\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-102\">HistGradientBoostingRegressor</label>",
            "code"
        ],
        [
            "HistGradientBoostingRegressor()\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "Fit a second model on this dataset with monotonic increase (1)\nand a monotonic decrease (-1) constraints, respectively.",
            "markdown"
        ],
        [
            "gbdt_with_monotonic_cst = (monotonic_cst=[1, -1])\ngbdt_with_monotonic_cst.fit(X, y)",
            "code"
        ],
        [
            "HistGradientBoostingRegressor(monotonic_cst=[1, -1])<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input checked=\"\" class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-103\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-103\">HistGradientBoostingRegressor</label>",
            "code"
        ],
        [
            "HistGradientBoostingRegressor(monotonic_cst=[1, -1])\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "Let\u2019s display the partial dependence of the predictions on the two features.",
            "markdown"
        ],
        [
            "fig, ax = ()\ndisp = (\n    gbdt_no_cst,\n    X,\n    features=[0, 1],\n    feature_names=(\n        \"First feature\",\n        \"Second feature\",\n    ),\n    line_kw={\"linewidth\": 4, \"label\": \"unconstrained\", \"color\": \"tab:blue\"},\n    ax=ax,\n)\n(\n    gbdt_with_monotonic_cst,\n    X,\n    features=[0, 1],\n    line_kw={\"linewidth\": 4, \"label\": \"constrained\", \"color\": \"tab:orange\"},\n    ax=disp.axes_,\n)\n\nfor f_idx in (0, 1):\n    disp.axes_[0, f_idx].plot(\n        X[:, f_idx], y, \"o\", alpha=0.3, zorder=-1, color=\"tab:green\"\n    )\n    disp.axes_[0, f_idx].set_ylim(-6, 6)\n\n()\nfig.suptitle(\"Monotonic constraints effect on partial dependences\")\n()\n\n\n<img alt=\"Monotonic constraints effect on partial dependences\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_monotonic_constraints_001.png\" srcset=\"../../_images/sphx_glr_plot_monotonic_constraints_001.png\"/>",
            "code"
        ],
        [
            "We can see that the predictions of the unconstrained model capture the\noscillations of the data while the constrained model follows the general\ntrend and ignores the local variations.",
            "markdown"
        ]
    ],
    "Examples->Ensemble methods->Monotonic Constraints->Using feature names to specify monotonic constraints": [
        [
            "Note that if the training data has feature names, it\u2019s possible to specifiy the\nmonotonic constraints by passing a dictionary:",
            "markdown"
        ],
        [
            "import pandas as pd\n\nX_df = (X, columns=[\"f_0\", \"f_1\"])\n\ngbdt_with_monotonic_cst_df = (\n    monotonic_cst={\"f_0\": 1, \"f_1\": -1}\n).fit(X_df, y)\n\n(\n    gbdt_with_monotonic_cst_df.predict(X_df), gbdt_with_monotonic_cst.predict(X)\n)",
            "code"
        ],
        [
            "True",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.653 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Ensemble methods->Multi-class AdaBoosted Decision Trees": [
        [
            "This example reproduces Figure 1 of Zhu et al  and shows how boosting can\nimprove prediction accuracy on a multi-class problem. The classification\ndataset is constructed by taking a ten-dimensional standard normal distribution\nand defining three classes separated by nested concentric ten-dimensional\nspheres such that roughly equal numbers of samples are in each class (quantiles\nof the \\(\\chi^2\\) distribution).",
            "markdown"
        ],
        [
            "The performance of the SAMME and SAMME.R  algorithms are compared. SAMME.R\nuses the probability estimates to update the additive model, while SAMME  uses\nthe classifications only. As the example illustrates, the SAMME.R algorithm\ntypically converges faster than SAMME, achieving a lower test error with fewer\nboosting iterations. The error of each algorithm on the test set after each\nboosting iteration is shown on the left, the classification error on the test\nset of each tree is shown in the middle, and the boost weight of each tree is\nshown on the right. All trees have a weight of one in the SAMME.R algorithm and\ntherefore are not shown.\n\n\n[1]\n(,)",
            "markdown"
        ],
        [
            "Zhu, H. Zou, S. Rosset, T. Hastie, \u201cMulti-class AdaBoost\u201d, 2009.\n\n\n\n<img alt=\"plot adaboost multiclass\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_adaboost_multiclass_001.png\" srcset=\"../../_images/sphx_glr_plot_adaboost_multiclass_001.png\"/>",
            "markdown"
        ],
        [
            "# Author: Noel Dawe &lt;noel.dawe@gmail.com\n#\n# License: BSD 3 clause\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import \nfrom sklearn.ensemble import \nfrom sklearn.metrics import \nfrom sklearn.tree import \n\n\nX, y = (\n    n_samples=13000, n_features=10, n_classes=3, random_state=1\n)\n\nn_split = 3000\n\nX_train, X_test = X[:n_split], X[n_split:]\ny_train, y_test = y[:n_split], y[n_split:]\n\nbdt_real = (\n    (max_depth=2), n_estimators=300, learning_rate=1\n)\n\nbdt_discrete = (\n    (max_depth=2),\n    n_estimators=300,\n    learning_rate=1.5,\n    algorithm=\"SAMME\",\n)\n\nbdt_real.fit(X_train, y_train)\nbdt_discrete.fit(X_train, y_train)\n\nreal_test_errors = []\ndiscrete_test_errors = []\n\nfor real_test_predict, discrete_test_predict in zip(\n    bdt_real.staged_predict(X_test), bdt_discrete.staged_predict(X_test)\n):\n    real_test_errors.append(1.0 - (real_test_predict, y_test))\n    discrete_test_errors.append(1.0 - (discrete_test_predict, y_test))\n\nn_trees_discrete = len(bdt_discrete)\nn_trees_real = len(bdt_real)\n\n# Boosting might terminate early, but the following arrays are always\n# n_estimators long. We crop them to the actual number of trees here:\ndiscrete_estimator_errors = bdt_discrete.estimator_errors_[:n_trees_discrete]\nreal_estimator_errors = bdt_real.estimator_errors_[:n_trees_real]\ndiscrete_estimator_weights = bdt_discrete.estimator_weights_[:n_trees_discrete]\n\n(figsize=(15, 5))\n\n(131)\n(range(1, n_trees_discrete + 1), discrete_test_errors, c=\"black\", label=\"SAMME\")\n(\n    range(1, n_trees_real + 1),\n    real_test_errors,\n    c=\"black\",\n    linestyle=\"dashed\",\n    label=\"SAMME.R\",\n)\n()\n(0.18, 0.62)\n(\"Test Error\")\n(\"Number of Trees\")\n\n(132)\n(\n    range(1, n_trees_discrete + 1),\n    discrete_estimator_errors,\n    \"b\",\n    label=\"SAMME\",\n    alpha=0.5,\n)\n(\n    range(1, n_trees_real + 1), real_estimator_errors, \"r\", label=\"SAMME.R\", alpha=0.5\n)\n()\n(\"Error\")\n(\"Number of Trees\")\n((0.2, max(real_estimator_errors.max(), discrete_estimator_errors.max()) * 1.2))\n((-20, len(bdt_discrete) + 20))\n\n(133)\n(range(1, n_trees_discrete + 1), discrete_estimator_weights, \"b\", label=\"SAMME\")\n()\n(\"Weight\")\n(\"Number of Trees\")\n((0, discrete_estimator_weights.max() * 1.2))\n((-20, n_trees_discrete + 20))\n\n# prevent overlapping y-axis labels\n(wspace=0.25)\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  6.861 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Ensemble methods->OOB Errors for Random Forests": [
        [
            "The RandomForestClassifier is trained using bootstrap aggregation, where\neach new tree is fit from a bootstrap sample of the training observations\n\\(z_i = (x_i, y_i)\\). The out-of-bag (OOB) error is the average error for\neach \\(z_i\\) calculated using predictions from the trees that do not\ncontain \\(z_i\\) in their respective bootstrap sample. This allows the\nRandomForestClassifier to be fit and validated whilst being trained .",
            "markdown"
        ],
        [
            "The example below demonstrates how the OOB error can be measured at the\naddition of each new tree during training. The resulting plot allows a\npractitioner to approximate a suitable value of n_estimators at which the\nerror stabilizes.\n\n\n[]",
            "markdown"
        ],
        [
            "T. Hastie, R. Tibshirani and J. Friedman, \u201cElements of Statistical\nLearning Ed. 2\u201d, p592-593, Springer, 2009.\n\n\n<img alt=\"plot ensemble oob\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_ensemble_oob_001.png\" srcset=\"../../_images/sphx_glr_plot_ensemble_oob_001.png\"/>",
            "markdown"
        ],
        [
            "# Author: Kian Ho &lt;hui.kian.ho@gmail.com\n#         Gilles Louppe &lt;g.louppe@gmail.com\n#         Andreas Mueller &lt;amueller@ais.uni-bonn.de\n#\n# License: BSD 3 Clause\n\nimport matplotlib.pyplot as plt\n\nfrom collections import \nfrom sklearn.datasets import \nfrom sklearn.ensemble import \n\nRANDOM_STATE = 123\n\n# Generate a binary classification dataset.\nX, y = (\n    n_samples=500,\n    n_features=25,\n    n_clusters_per_class=1,\n    n_informative=15,\n    random_state=RANDOM_STATE,\n)\n\n# NOTE: Setting the `warm_start` construction parameter to `True` disables\n# support for parallelized ensembles but is necessary for tracking the OOB\n# error trajectory during training.\nensemble_clfs = [\n    (\n        \"RandomForestClassifier, max_features='sqrt'\",\n        (\n            warm_start=True,\n            oob_score=True,\n            max_features=\"sqrt\",\n            random_state=RANDOM_STATE,\n        ),\n    ),\n    (\n        \"RandomForestClassifier, max_features='log2'\",\n        (\n            warm_start=True,\n            max_features=\"log2\",\n            oob_score=True,\n            random_state=RANDOM_STATE,\n        ),\n    ),\n    (\n        \"RandomForestClassifier, max_features=None\",\n        (\n            warm_start=True,\n            max_features=None,\n            oob_score=True,\n            random_state=RANDOM_STATE,\n        ),\n    ),\n]\n\n# Map a classifier name to a list of (&lt;n_estimators, &lt;error rate) pairs.\nerror_rate = ((label, []) for label, _ in ensemble_clfs)\n\n# Range of `n_estimators` values to explore.\nmin_estimators = 15\nmax_estimators = 150\n\nfor label, clf in ensemble_clfs:\n    for i in range(min_estimators, max_estimators + 1, 5):\n        clf.set_params(n_estimators=i)\n        clf.fit(X, y)\n\n        # Record the OOB error for each `n_estimators=i` setting.\n        oob_error = 1 - clf.oob_score_\n        error_rate[label].append((i, oob_error))\n\n# Generate the \"OOB error rate\" vs. \"n_estimators\" plot.\nfor label, clf_err in error_rate.items():\n    xs, ys = zip(*clf_err)\n    (xs, ys, label=label)\n\n(min_estimators, max_estimators)\n(\"n_estimators\")\n(\"OOB error rate\")\n(loc=\"upper right\")\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  4.001 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Ensemble methods->Pixel importances with a parallel forest of trees": [
        [
            "This example shows the use of a forest of trees to evaluate the impurity\nbased importance of the pixels in an image classification task on the faces\ndataset. The hotter the pixel, the more important it is.",
            "markdown"
        ],
        [
            "The code below also illustrates how the construction and the computation\nof the predictions can be parallelized within multiple jobs.",
            "markdown"
        ]
    ],
    "Examples->Ensemble methods->Pixel importances with a parallel forest of trees->Loading the data and model fitting": [
        [
            "First, we load the olivetti faces dataset and limit the dataset to contain\nonly the first five classes. Then we train a random forest on the dataset\nand evaluate the impurity-based feature importance. One drawback of this\nmethod is that it cannot be evaluated on a separate test set. For this\nexample, we are interested in representing the information learned from\nthe full dataset. Also, we\u2019ll set the number of cores to use for the tasks.",
            "markdown"
        ],
        [
            "from sklearn.datasets import",
            "code"
        ],
        [
            "We select the number of cores to use to perform parallel fitting of\nthe forest model. -1 means use all available cores.",
            "markdown"
        ],
        [
            "n_jobs = -1",
            "code"
        ],
        [
            "Load the faces dataset",
            "markdown"
        ],
        [
            "data = ()\nX, y = data.data, data.target",
            "code"
        ],
        [
            "Limit the dataset to 5 classes.",
            "markdown"
        ],
        [
            "mask = y &lt; 5\nX = X[mask]\ny = y[mask]",
            "code"
        ],
        [
            "A random forest classifier will be fitted to compute the feature importances.",
            "markdown"
        ],
        [
            "from sklearn.ensemble import \n\nforest = (n_estimators=750, n_jobs=n_jobs, random_state=42)\n\nforest.fit(X, y)",
            "code"
        ],
        [
            "RandomForestClassifier(n_estimators=750, n_jobs=-1, random_state=42)<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input checked=\"\" class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-104\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-104\">RandomForestClassifier</label>",
            "code"
        ],
        [
            "RandomForestClassifier(n_estimators=750, n_jobs=-1, random_state=42)\n\n<br/>\n<br/>",
            "code"
        ]
    ],
    "Examples->Ensemble methods->Pixel importances with a parallel forest of trees->Feature importance based on mean decrease in impurity (MDI)": [
        [
            "Feature importances are provided by the fitted attribute\nfeature_importances_ and they are computed as the mean and standard\ndeviation of accumulation of the impurity decrease within each tree.",
            "markdown"
        ],
        [
            "Warning",
            "markdown"
        ],
        [
            "Impurity-based feature importances can be misleading for <strong>high\ncardinality</strong> features (many unique values). See\n as an alternative.",
            "markdown"
        ],
        [
            "import time\nimport matplotlib.pyplot as plt\n\nstart_time = ()\nimg_shape = data.images[0].shape\nimportances = forest.feature_importances_\nelapsed_time = () - start_time\n\nprint(f\"Elapsed time to compute the importances: {elapsed_time:.3f} seconds\")\nimp_reshaped = importances.reshape(img_shape)\n(imp_reshaped, cmap=plt.cm.hot)\n(\"Pixel importances using impurity values\")\n()\n()\n\n\n<img alt=\"Pixel importances using impurity values\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_forest_importances_faces_001.png\" srcset=\"../../_images/sphx_glr_plot_forest_importances_faces_001.png\"/>",
            "code"
        ],
        [
            "Elapsed time to compute the importances: 0.137 seconds",
            "code"
        ],
        [
            "Can you still recognize a face?",
            "markdown"
        ],
        [
            "The limitations of MDI is not a problem for this dataset because:",
            "markdown"
        ],
        [
            "All features are (ordered) numeric and will thus not suffer the\ncardinality bias",
            "markdown"
        ],
        [
            "We are only interested to represent knowledge of the forest acquired\non the training set.\n\n</blockquote>",
            "markdown"
        ],
        [
            "If these two conditions are not met, it is recommended to instead use\nthe .",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  1.601 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Ensemble methods->Plot class probabilities calculated by the VotingClassifier": [
        [
            "Plot the class probabilities of the first sample in a toy dataset predicted by\nthree different classifiers and averaged by the\n.",
            "markdown"
        ],
        [
            "First, three examplary classifiers are initialized\n(, ,\nand ) and used to initialize a\nsoft-voting  with weights [1, 1, 5], which\nmeans that the predicted probabilities of the\n count 5 times as much as the weights\nof the other classifiers when the averaged probability is calculated.",
            "markdown"
        ],
        [
            "To visualize the probability weighting, we fit each classifier on the training\nset and plot the predicted class probabilities for the first sample in this\nexample dataset.\n<img alt=\"Class probabilities for sample 1 by different classifiers\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_voting_probas_001.png\" srcset=\"../../_images/sphx_glr_plot_voting_probas_001.png\"/>",
            "markdown"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import \nfrom sklearn.naive_bayes import \nfrom sklearn.ensemble import \nfrom sklearn.ensemble import \n\nclf1 = (max_iter=1000, random_state=123)\nclf2 = (n_estimators=100, random_state=123)\nclf3 = ()\nX = ([[-1.0, -1.0], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])\ny = ([1, 1, 2, 2])\n\neclf = (\n    estimators=[(\"lr\", clf1), (\"rf\", clf2), (\"gnb\", clf3)],\n    voting=\"soft\",\n    weights=[1, 1, 5],\n)\n\n# predict class probabilities for all classifiers\nprobas = [c.fit(X, y).predict_proba(X) for c in (clf1, clf2, clf3, eclf)]\n\n# get class probabilities for the first sample in the dataset\nclass1_1 = [pr[0, 0] for pr in probas]\nclass2_1 = [pr[0, 1] for pr in probas]\n\n\n# plotting\n\nN = 4  # number of groups\nind = (N)  # group positions\nwidth = 0.35  # bar width\n\nfig, ax = ()\n\n# bars for classifier 1-3\np1 = ax.bar(ind, (([class1_1[:-1], [0]])), width, color=\"green\", edgecolor=\"k\")\np2 = ax.bar(\n    ind + width,\n    (([class2_1[:-1], [0]])),\n    width,\n    color=\"lightgreen\",\n    edgecolor=\"k\",\n)\n\n# bars for VotingClassifier\np3 = ax.bar(ind, [0, 0, 0, class1_1[-1]], width, color=\"blue\", edgecolor=\"k\")\np4 = ax.bar(\n    ind + width, [0, 0, 0, class2_1[-1]], width, color=\"steelblue\", edgecolor=\"k\"\n)\n\n# plot annotations\n(2.8, color=\"k\", linestyle=\"dashed\")\nax.set_xticks(ind + width)\nax.set_xticklabels(\n    [\n        \"LogisticRegression\\nweight 1\",\n        \"GaussianNB\\nweight 1\",\n        \"RandomForestClassifier\\nweight 5\",\n        \"VotingClassifier\\n(average probabilities)\",\n    ],\n    rotation=40,\n    ha=\"right\",\n)\n([0, 1])\n(\"Class probabilities for sample 1 by different classifiers\")\n([p1[0], p2[0]], [\"class 1\", \"class 2\"], loc=\"upper left\")\n()\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.356 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Ensemble methods->Plot individual and voting regression predictions": [
        [
            "A voting regressor is an ensemble meta-estimator that fits several base\nregressors, each on the whole dataset. Then it averages the individual\npredictions to form a final prediction.\nWe will use three different regressors to predict the data:\n,\n, and\n).\nThen the above 3 regressors will be used for the\n.",
            "markdown"
        ],
        [
            "Finally, we will plot the predictions made by all models for comparison.",
            "markdown"
        ],
        [
            "We will work with the diabetes dataset which consists of 10 features\ncollected from a cohort of diabetes patients. The target is a quantitative\nmeasure of disease progression one year after baseline.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n\nfrom sklearn.datasets import \nfrom sklearn.ensemble import \nfrom sklearn.ensemble import \nfrom sklearn.linear_model import \nfrom sklearn.ensemble import",
            "code"
        ]
    ],
    "Examples->Ensemble methods->Plot individual and voting regression predictions->Training classifiers": [
        [
            "First, we will load the diabetes dataset and initiate a gradient boosting\nregressor, a random forest regressor and a linear regression. Next, we will\nuse the 3 regressors to build the voting regressor:",
            "markdown"
        ],
        [
            "X, y = (return_X_y=True)\n\n# Train classifiers\nreg1 = (random_state=1)\nreg2 = (random_state=1)\nreg3 = ()\n\nreg1.fit(X, y)\nreg2.fit(X, y)\nreg3.fit(X, y)\n\nereg = ([(\"gb\", reg1), (\"rf\", reg2), (\"lr\", reg3)])\nereg.fit(X, y)",
            "code"
        ],
        [
            "VotingRegressor(estimators=[('gb', GradientBoostingRegressor(random_state=1)),\n                            ('rf', RandomForestRegressor(random_state=1)),\n                            ('lr', LinearRegression())])<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-105\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-105\">VotingRegressor</label>",
            "code"
        ],
        [
            "VotingRegressor(estimators=[('gb', GradientBoostingRegressor(random_state=1)),\n                            ('rf', RandomForestRegressor(random_state=1)),\n                            ('lr', LinearRegression())])<label>gb</label><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-106\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-106\">GradientBoostingRegressor</label>",
            "code"
        ],
        [
            "GradientBoostingRegressor(random_state=1)<label>rf</label><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-107\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-107\">RandomForestRegressor</label>",
            "code"
        ],
        [
            "RandomForestRegressor(random_state=1)<label>lr</label><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-108\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-108\">LinearRegression</label>",
            "code"
        ],
        [
            "LinearRegression()\n\n<br/>\n<br/>",
            "code"
        ]
    ],
    "Examples->Ensemble methods->Plot individual and voting regression predictions->Making predictions": [
        [
            "Now we will use each of the regressors to make the 20 first predictions.",
            "markdown"
        ],
        [
            "xt = X[:20]\n\npred1 = reg1.predict(xt)\npred2 = reg2.predict(xt)\npred3 = reg3.predict(xt)\npred4 = ereg.predict(xt)",
            "code"
        ]
    ],
    "Examples->Ensemble methods->Plot individual and voting regression predictions->Plot the results": [
        [
            "Finally, we will visualize the 20 predictions. The red stars show the average\nprediction made by .",
            "markdown"
        ],
        [
            "()\n(pred1, \"gd\", label=\"GradientBoostingRegressor\")\n(pred2, \"b^\", label=\"RandomForestRegressor\")\n(pred3, \"ys\", label=\"LinearRegression\")\n(pred4, \"r*\", ms=10, label=\"VotingRegressor\")\n\n(axis=\"x\", which=\"both\", bottom=False, top=False, labelbottom=False)\n(\"predicted\")\n(\"training samples\")\n(loc=\"best\")\n(\"Regressor predictions and their average\")\n\n()\n\n\n<img alt=\"Regressor predictions and their average\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_voting_regressor_001.png\" srcset=\"../../_images/sphx_glr_plot_voting_regressor_001.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.881 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Ensemble methods->Plot the decision boundaries of a VotingClassifier": [
        [
            "Plot the decision boundaries of a  for two\nfeatures of the Iris dataset.",
            "markdown"
        ],
        [
            "Plot the class probabilities of the first sample in a toy dataset predicted by\nthree different classifiers and averaged by the\n.",
            "markdown"
        ],
        [
            "First, three exemplary classifiers are initialized\n(,\n, and ) and used to\ninitialize a soft-voting  with weights [2,\n1, 2], which means that the predicted probabilities of the\n and  each count 2 times\nas much as the weights of the \nclassifier when the averaged probability is calculated.\n<img alt=\"Decision Tree (depth=4), KNN (k=7), Kernel SVM, Soft Voting\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_voting_decision_regions_001.png\" srcset=\"../../_images/sphx_glr_plot_voting_decision_regions_001.png\"/>",
            "markdown"
        ],
        [
            "from itertools import \n\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets\nfrom sklearn.tree import \nfrom sklearn.neighbors import \nfrom sklearn.svm import \nfrom sklearn.ensemble import \nfrom sklearn.inspection import DecisionBoundaryDisplay\n\n# Loading some example data\niris = ()\nX = iris.data[:, [0, 2]]\ny = iris.target\n\n# Training classifiers\nclf1 = (max_depth=4)\nclf2 = (n_neighbors=7)\nclf3 = (gamma=0.1, kernel=\"rbf\", probability=True)\neclf = (\n    estimators=[(\"dt\", clf1), (\"knn\", clf2), (\"svc\", clf3)],\n    voting=\"soft\",\n    weights=[2, 1, 2],\n)\n\nclf1.fit(X, y)\nclf2.fit(X, y)\nclf3.fit(X, y)\neclf.fit(X, y)\n\n# Plotting decision regions\nf, axarr = (2, 2, sharex=\"col\", sharey=\"row\", figsize=(10, 8))\nfor idx, clf, tt in zip(\n    ([0, 1], [0, 1]),\n    [clf1, clf2, clf3, eclf],\n    [\"Decision Tree (depth=4)\", \"KNN (k=7)\", \"Kernel SVM\", \"Soft Voting\"],\n):\n    (\n        clf, X, alpha=0.4, ax=axarr[idx[0], idx[1]], response_method=\"predict\"\n    )\n    axarr[idx[0], idx[1]].scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor=\"k\")\n    axarr[idx[0], idx[1]].set_title(tt)\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.593 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Ensemble methods->Plot the decision surfaces of ensembles of trees on the iris dataset": [
        [
            "Plot the decision surfaces of forests of randomized trees trained on pairs of\nfeatures of the iris dataset.",
            "markdown"
        ],
        [
            "This plot compares the decision surfaces learned by a decision tree classifier\n(first column), by a random forest classifier (second column), by an extra-\ntrees classifier (third column) and by an AdaBoost classifier (fourth column).",
            "markdown"
        ],
        [
            "In the first row, the classifiers are built using the sepal width and\nthe sepal length features only, on the second row using the petal length and\nsepal length only, and on the third row using the petal width and the\npetal length only.",
            "markdown"
        ],
        [
            "In descending order of quality, when trained (outside of this example) on all\n4 features using 30 estimators and scored using 10 fold cross validation,\nwe see:",
            "markdown"
        ],
        [
            "()  # 0.95 score\n()  # 0.94 score\nAdaBoost(DecisionTree(max_depth=3))  # 0.94 score\nDecisionTree(max_depth=None)  # 0.94 score",
            "code"
        ],
        [
            "Increasing max_depth for AdaBoost lowers the standard deviation of\nthe scores (but the average score does not improve).",
            "markdown"
        ],
        [
            "See the console\u2019s output for further details about each model.",
            "markdown"
        ],
        [
            "In this example you might try to:",
            "markdown"
        ],
        [
            "vary the max_depth for the DecisionTreeClassifier and\nAdaBoostClassifier, perhaps try max_depth=3 for the\nDecisionTreeClassifier or max_depth=None for AdaBoostClassifier",
            "markdown"
        ],
        [
            "vary n_estimators",
            "markdown"
        ],
        [
            "It is worth noting that RandomForests and ExtraTrees can be fitted in parallel\non many cores as each tree is built independently of the others. AdaBoost\u2019s\nsamples are built sequentially and so do not use multiple cores.\n<img alt=\"Classifiers on feature subsets of the Iris dataset, DecisionTree, RandomForest, ExtraTrees, AdaBoost\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_forest_iris_001.png\" srcset=\"../../_images/sphx_glr_plot_forest_iris_001.png\"/>",
            "markdown"
        ],
        [
            "DecisionTree with features [0, 1] has a score of 0.9266666666666666\nRandomForest with 30 estimators with features [0, 1] has a score of 0.9266666666666666\nExtraTrees with 30 estimators with features [0, 1] has a score of 0.9266666666666666\nAdaBoost with 30 estimators with features [0, 1] has a score of 0.8666666666666667\nDecisionTree with features [0, 2] has a score of 0.9933333333333333\nRandomForest with 30 estimators with features [0, 2] has a score of 0.9933333333333333\nExtraTrees with 30 estimators with features [0, 2] has a score of 0.9933333333333333\nAdaBoost with 30 estimators with features [0, 2] has a score of 0.9933333333333333\nDecisionTree with features [2, 3] has a score of 0.9933333333333333\nRandomForest with 30 estimators with features [2, 3] has a score of 0.9933333333333333\nExtraTrees with 30 estimators with features [2, 3] has a score of 0.9933333333333333\nAdaBoost with 30 estimators with features [2, 3] has a score of 0.9933333333333333\n\n\n\n<br/>",
            "code"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import \n\nfrom sklearn.datasets import \nfrom sklearn.ensemble import (\n    ,\n    ,\n    ,\n)\nfrom sklearn.tree import \n\n# Parameters\nn_classes = 3\nn_estimators = 30\ncmap = plt.cm.RdYlBu\nplot_step = 0.02  # fine step width for decision surface contours\nplot_step_coarser = 0.5  # step widths for coarse classifier guesses\nRANDOM_SEED = 13  # fix the seed on each iteration\n\n# Load data\niris = ()\n\nplot_idx = 1\n\nmodels = [\n    (max_depth=None),\n    (n_estimators=n_estimators),\n    (n_estimators=n_estimators),\n    ((max_depth=3), n_estimators=n_estimators),\n]\n\nfor pair in ([0, 1], [0, 2], [2, 3]):\n    for model in models:\n        # We only take the two corresponding features\n        X = iris.data[:, pair]\n        y = iris.target\n\n        # Shuffle\n        idx = (X.shape[0])\n        (RANDOM_SEED)\n        (idx)\n        X = X[idx]\n        y = y[idx]\n\n        # Standardize\n        mean = X.mean(axis=0)\n        std = X.std(axis=0)\n        X = (X - mean) / std\n\n        # Train\n        model.fit(X, y)\n\n        scores = model.score(X, y)\n        # Create a title for each column and the console by using str() and\n        # slicing away useless parts of the string\n        model_title = str(type(model)).split(\".\")[-1][:-2][: -len(\"Classifier\")]\n\n        model_details = model_title\n        if hasattr(model, \"estimators_\"):\n            model_details += \" with {} estimators\".format(len(model.estimators_))\n        print(model_details + \" with features\", pair, \"has a score of\", scores)\n\n        (3, 4, plot_idx)\n        if plot_idx &lt;= len(models):\n            # Add a title at the top of each column\n            (model_title, fontsize=9)\n\n        # Now plot the decision boundary using a fine mesh as input to a\n        # filled contour plot\n        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n        xx, yy = (\n            (x_min, x_max, plot_step), (y_min, y_max, plot_step)\n        )\n\n        # Plot either a single DecisionTreeClassifier or alpha blend the\n        # decision surfaces of the ensemble of classifiers\n        if isinstance(model, ):\n            Z = model.predict([xx.ravel(), yy.ravel()])\n            Z = Z.reshape(xx.shape)\n            cs = (xx, yy, Z, cmap=cmap)\n        else:\n            # Choose alpha blend level with respect to the number\n            # of estimators\n            # that are in use (noting that AdaBoost can use fewer estimators\n            # than its maximum if it achieves a good enough fit early on)\n            estimator_alpha = 1.0 / len(model.estimators_)\n            for tree in model.estimators_:\n                Z = tree.predict([xx.ravel(), yy.ravel()])\n                Z = Z.reshape(xx.shape)\n                cs = (xx, yy, Z, alpha=estimator_alpha, cmap=cmap)\n\n        # Build a coarser grid to plot a set of ensemble classifications\n        # to show how these are different to what we see in the decision\n        # surfaces. These points are regularly space and do not have a\n        # black outline\n        xx_coarser, yy_coarser = (\n            (x_min, x_max, plot_step_coarser),\n            (y_min, y_max, plot_step_coarser),\n        )\n        Z_points_coarser = model.predict(\n            [xx_coarser.ravel(), yy_coarser.ravel()]\n        ).reshape(xx_coarser.shape)\n        cs_points = (\n            xx_coarser,\n            yy_coarser,\n            s=15,\n            c=Z_points_coarser,\n            cmap=cmap,\n            edgecolors=\"none\",\n        )\n\n        # Plot the training points, these are clustered together and have a\n        # black outline\n        (\n            X[:, 0],\n            X[:, 1],\n            c=y,\n            cmap=([\"r\", \"y\", \"b\"]),\n            edgecolor=\"k\",\n            s=20,\n        )\n        plot_idx += 1  # move on to the next plot in sequence\n\n(\"Classifiers on feature subsets of the Iris dataset\", fontsize=12)\n(\"tight\")\n(h_pad=0.2, w_pad=0.2, pad=2.5)\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  7.700 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Ensemble methods->Prediction Intervals for Gradient Boosting Regression": [
        [
            "This example shows how quantile regression can be used to create prediction\nintervals.",
            "markdown"
        ],
        [
            "Generate some data for a synthetic regression problem by applying the\nfunction f to uniformly sampled random inputs.",
            "markdown"
        ],
        [
            "import numpy as np\nfrom sklearn.model_selection import \n\n\ndef f(x):\n    \"\"\"The function to predict.\"\"\"\n    return x * (x)\n\n\nrng = (42)\nX = (rng.uniform(0, 10.0, size=1000)).T\nexpected_y = f(X).ravel()",
            "code"
        ],
        [
            "To make the problem interesting, we generate observations of the target y as\nthe sum of a deterministic term computed by the function f and a random noise\nterm that follows a centered . To make this even\nmore interesting we consider the case where the amplitude of the noise\ndepends on the input variable x (heteroscedastic noise).",
            "markdown"
        ],
        [
            "The lognormal distribution is non-symmetric and long tailed: observing large\noutliers is likely but it is impossible to observe small outliers.",
            "markdown"
        ],
        [
            "sigma = 0.5 + X.ravel() / 10\nnoise = rng.lognormal(sigma=sigma) - (sigma**2 / 2)\ny = expected_y + noise",
            "code"
        ],
        [
            "Split into train, test datasets:",
            "markdown"
        ],
        [
            "X_train, X_test, y_train, y_test = (X, y, random_state=0)",
            "code"
        ]
    ],
    "Examples->Ensemble methods->Prediction Intervals for Gradient Boosting Regression->Fitting non-linear quantile and least squares regressors": [
        [
            "Fit gradient boosting models trained with the quantile loss and\nalpha=0.05, 0.5, 0.95.",
            "markdown"
        ],
        [
            "The models obtained for alpha=0.05 and alpha=0.95 produce a 90% confidence\ninterval (95% - 5% = 90%).",
            "markdown"
        ],
        [
            "The model trained with alpha=0.5 produces a regression of the median: on\naverage, there should be the same number of target observations above and\nbelow the predicted values.",
            "markdown"
        ],
        [
            "from sklearn.ensemble import \nfrom sklearn.metrics import , \n\n\nall_models = {}\ncommon_params = dict(\n    learning_rate=0.05,\n    n_estimators=200,\n    max_depth=2,\n    min_samples_leaf=9,\n    min_samples_split=9,\n)\nfor alpha in [0.05, 0.5, 0.95]:\n    gbr = (loss=\"quantile\", alpha=alpha, **common_params)\n    all_models[\"q %1.2f\" % alpha] = gbr.fit(X_train, y_train)",
            "code"
        ],
        [
            "Notice that  is much\nfaster than  starting with\nintermediate datasets (n_samples >= 10_000), which is not the case of the\npresent example.",
            "markdown"
        ],
        [
            "For the sake of comparison, we also fit a baseline model trained with the\nusual (mean) squared error (MSE).",
            "markdown"
        ],
        [
            "gbr_ls = (loss=\"squared_error\", **common_params)\nall_models[\"mse\"] = gbr_ls.fit(X_train, y_train)",
            "code"
        ],
        [
            "Create an evenly spaced evaluation set of input values spanning the [0, 10]\nrange.",
            "markdown"
        ],
        [
            "xx = ((0, 10, 1000)).T",
            "code"
        ],
        [
            "Plot the true conditional mean function f, the predictions of the conditional\nmean (loss equals squared error), the conditional median and the conditional\n90% interval (from 5th to 95th conditional percentiles).",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n\n\ny_pred = all_models[\"mse\"].predict(xx)\ny_lower = all_models[\"q 0.05\"].predict(xx)\ny_upper = all_models[\"q 0.95\"].predict(xx)\ny_med = all_models[\"q 0.50\"].predict(xx)\n\nfig = (figsize=(10, 10))\n(xx, f(xx), \"g:\", linewidth=3, label=r\"$f(x) = x\\,\\sin(x)$\")\n(X_test, y_test, \"b.\", markersize=10, label=\"Test observations\")\n(xx, y_med, \"r-\", label=\"Predicted median\")\n(xx, y_pred, \"r-\", label=\"Predicted mean\")\n(xx, y_upper, \"k-\")\n(xx, y_lower, \"k-\")\n(\n    xx.ravel(), y_lower, y_upper, alpha=0.4, label=\"Predicted 90% interval\"\n)\n(\"$x$\")\n(\"$f(x)$\")\n(-10, 25)\n(loc=\"upper left\")\n()\n\n\n<img alt=\"plot gradient boosting quantile\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_quantile_001.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_quantile_001.png\"/>",
            "code"
        ],
        [
            "Comparing the predicted median with the predicted mean, we note that the\nmedian is on average below the mean as the noise is skewed towards high\nvalues (large outliers). The median estimate also seems to be smoother\nbecause of its natural robustness to outliers.",
            "markdown"
        ],
        [
            "Also observe that the inductive bias of gradient boosting trees is\nunfortunately preventing our 0.05 quantile to fully capture the sinoisoidal\nshape of the signal, in particular around x=8. Tuning hyper-parameters can\nreduce this effect as shown in the last part of this notebook.",
            "markdown"
        ]
    ],
    "Examples->Ensemble methods->Prediction Intervals for Gradient Boosting Regression->Analysis of the error metrics": [
        [
            "Measure the models with mean_squared_error and\nmean_pinball_loss metrics on the training dataset.",
            "markdown"
        ],
        [
            "import pandas as pd\n\n\ndef highlight_min(x):\n    x_min = x.min()\n    return [\"font-weight: bold\" if v == x_min else \"\" for v in x]\n\n\nresults = []\nfor name, gbr in sorted(all_models.items()):\n    metrics = {\"model\": name}\n    y_pred = gbr.predict(X_train)\n    for alpha in [0.05, 0.5, 0.95]:\n        metrics[\"pbl=%1.2f\" % alpha] = (y_train, y_pred, alpha=alpha)\n    metrics[\"MSE\"] = (y_train, y_pred)\n    results.append(metrics)\n\n(results).set_index(\"model\").style.apply(highlight_min)\n\n\n\n\n\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "One column shows all models evaluated by the same metric. The minimum number\non a column should be obtained when the model is trained and measured with\nthe same metric. This should be always the case on the training set if the\ntraining converged.",
            "markdown"
        ],
        [
            "Note that because the target distribution is asymmetric, the expected\nconditional mean and conditional median are signficiantly different and\ntherefore one could not use the squared error model get a good estimation of\nthe conditional median nor the converse.",
            "markdown"
        ],
        [
            "If the target distribution were symmetric and had no outliers (e.g. with a\nGaussian noise), then median estimator and the least squares estimator would\nhave yielded similar predictions.",
            "markdown"
        ],
        [
            "We then do the same on the test set.",
            "markdown"
        ],
        [
            "results = []\nfor name, gbr in sorted(all_models.items()):\n    metrics = {\"model\": name}\n    y_pred = gbr.predict(X_test)\n    for alpha in [0.05, 0.5, 0.95]:\n        metrics[\"pbl=%1.2f\" % alpha] = (y_test, y_pred, alpha=alpha)\n    metrics[\"MSE\"] = (y_test, y_pred)\n    results.append(metrics)\n\n(results).set_index(\"model\").style.apply(highlight_min)\n\n\n\n\n\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "Errors are higher meaning the models slightly overfitted the data. It still\nshows that the best test metric is obtained when the model is trained by\nminimizing this same metric.",
            "markdown"
        ],
        [
            "Note that the conditional median estimator is competitive with the squared\nerror estimator in terms of MSE on the test set: this can be explained by\nthe fact the squared error estimator is very sensitive to large outliers\nwhich can cause significant overfitting. This can be seen on the right hand\nside of the previous plot. The conditional median estimator is biased\n(underestimation for this asymmetric noise) but is also naturally robust to\noutliers and overfits less.",
            "markdown"
        ]
    ],
    "Examples->Ensemble methods->Prediction Intervals for Gradient Boosting Regression->Calibration of the confidence interval": [
        [
            "We can also evaluate the ability of the two extreme quantile estimators at\nproducing a well-calibrated conditational 90%-confidence interval.",
            "markdown"
        ],
        [
            "To do this we can compute the fraction of observations that fall between the\npredictions:",
            "markdown"
        ],
        [
            "def coverage_fraction(y, y_low, y_high):\n    return ((y = y_low, y &lt;= y_high))\n\n\ncoverage_fraction(\n    y_train,\n    all_models[\"q 0.05\"].predict(X_train),\n    all_models[\"q 0.95\"].predict(X_train),\n)",
            "code"
        ],
        [
            "0.9",
            "code"
        ],
        [
            "On the training set the calibration is very close to the expected coverage\nvalue for a 90% confidence interval.",
            "markdown"
        ],
        [
            "coverage_fraction(\n    y_test, all_models[\"q 0.05\"].predict(X_test), all_models[\"q 0.95\"].predict(X_test)\n)",
            "code"
        ],
        [
            "0.868",
            "code"
        ],
        [
            "On the test set, the estimated confidence interval is slightly too narrow.\nNote, however, that we would need to wrap those metrics in a cross-validation\nloop to assess their variability under data resampling.",
            "markdown"
        ]
    ],
    "Examples->Ensemble methods->Prediction Intervals for Gradient Boosting Regression->Tuning the hyper-parameters of the quantile regressors": [
        [
            "In the plot above, we observed that the 5th percentile regressor seems to\nunderfit and could not adapt to sinusoidal shape of the signal.",
            "markdown"
        ],
        [
            "The hyper-parameters of the model were approximately hand-tuned for the\nmedian regressor and there is no reason that the same hyper-parameters are\nsuitable for the 5th percentile regressor.",
            "markdown"
        ],
        [
            "To confirm this hypothesis, we tune the hyper-parameters of a new regressor\nof the 5th percentile by selecting the best model parameters by\ncross-validation on the pinball loss with alpha=0.05:",
            "markdown"
        ],
        [
            "from sklearn.experimental import enable_halving_search_cv  # noqa\nfrom sklearn.model_selection import \nfrom sklearn.metrics import \nfrom pprint import \n\nparam_grid = dict(\n    learning_rate=[0.05, 0.1, 0.2],\n    max_depth=[2, 5, 10],\n    min_samples_leaf=[1, 5, 10, 20],\n    min_samples_split=[5, 10, 20, 30, 50],\n)\nalpha = 0.05\nneg_mean_pinball_loss_05p_scorer = (\n    ,\n    alpha=alpha,\n    greater_is_better=False,  # maximize the negative loss\n)\ngbr = (loss=\"quantile\", alpha=alpha, random_state=0)\nsearch_05p = (\n    gbr,\n    param_grid,\n    resource=\"n_estimators\",\n    max_resources=250,\n    min_resources=50,\n    scoring=neg_mean_pinball_loss_05p_scorer,\n    n_jobs=2,\n    random_state=0,\n).fit(X_train, y_train)\n(search_05p.best_params_)",
            "code"
        ],
        [
            "{'learning_rate': 0.2,\n 'max_depth': 2,\n 'min_samples_leaf': 20,\n 'min_samples_split': 10,\n 'n_estimators': 150}",
            "code"
        ],
        [
            "We observe that the hyper-parameters that were hand-tuned for the median\nregressor are in the same range as the hyper-parameters suitable for the 5th\npercentile regressor.",
            "markdown"
        ],
        [
            "Let\u2019s now tune the hyper-parameters for the 95th percentile regressor. We\nneed to redefine the scoring metric used to select the best model, along\nwith adjusting the alpha parameter of the inner gradient boosting estimator\nitself:",
            "markdown"
        ],
        [
            "from sklearn.base import clone\n\nalpha = 0.95\nneg_mean_pinball_loss_95p_scorer = (\n    ,\n    alpha=alpha,\n    greater_is_better=False,  # maximize the negative loss\n)\nsearch_95p = clone(search_05p).set_params(\n    estimator__alpha=alpha,\n    scoring=neg_mean_pinball_loss_95p_scorer,\n)\nsearch_95p.fit(X_train, y_train)\n(search_95p.best_params_)",
            "code"
        ],
        [
            "{'learning_rate': 0.05,\n 'max_depth': 2,\n 'min_samples_leaf': 5,\n 'min_samples_split': 20,\n 'n_estimators': 150}",
            "code"
        ],
        [
            "The result shows that the hyper-parameters for the 95th percentile regressor\nidentified by the search procedure are roughly in the same range as the hand-\ntuned hyper-parameters for the median regressor and the hyper-parameters\nidentified by the search procedure for the 5th percentile regressor. However,\nthe hyper-parameter searches did lead to an improved 90% confidence interval\nthat is comprised by the predictions of those two tuned quantile regressors.\nNote that the prediction of the upper 95th percentile has a much coarser shape\nthan the prediction of the lower 5th percentile because of the outliers:",
            "markdown"
        ],
        [
            "y_lower = search_05p.predict(xx)\ny_upper = search_95p.predict(xx)\n\nfig = (figsize=(10, 10))\n(xx, f(xx), \"g:\", linewidth=3, label=r\"$f(x) = x\\,\\sin(x)$\")\n(X_test, y_test, \"b.\", markersize=10, label=\"Test observations\")\n(xx, y_upper, \"k-\")\n(xx, y_lower, \"k-\")\n(\n    xx.ravel(), y_lower, y_upper, alpha=0.4, label=\"Predicted 90% interval\"\n)\n(\"$x$\")\n(\"$f(x)$\")\n(-10, 25)\n(loc=\"upper left\")\n(\"Prediction with tuned hyper-parameters\")\n()\n\n\n<img alt=\"Prediction with tuned hyper-parameters\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_quantile_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_quantile_002.png\"/>",
            "code"
        ],
        [
            "The plot looks qualitatively better than for the untuned models, especially\nfor the shape of the of lower quantile.",
            "markdown"
        ],
        [
            "We now quantitatively evaluate the joint-calibration of the pair of\nestimators:",
            "markdown"
        ],
        [
            "coverage_fraction(y_train, search_05p.predict(X_train), search_95p.predict(X_train))",
            "code"
        ],
        [
            "0.9026666666666666",
            "code"
        ],
        [
            "coverage_fraction(y_test, search_05p.predict(X_test), search_95p.predict(X_test))",
            "code"
        ],
        [
            "0.796",
            "code"
        ],
        [
            "The calibration of the tuned pair is sadly not better on the test set: the\nwidth of the estimated confidence interval is still too narrow.",
            "markdown"
        ],
        [
            "Again, we would need to wrap this study in a cross-validation loop to\nbetter assess the variability of those estimates.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  12.081 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Ensemble methods->Single estimator versus bagging: bias-variance decomposition": [
        [
            "This example illustrates and compares the bias-variance decomposition of the\nexpected mean squared error of a single estimator against a bagging ensemble.",
            "markdown"
        ],
        [
            "In regression, the expected mean squared error of an estimator can be\ndecomposed in terms of bias, variance and noise. On average over datasets of\nthe regression problem, the bias term measures the average amount by which the\npredictions of the estimator differ from the predictions of the best possible\nestimator for the problem (i.e., the Bayes model). The variance term measures\nthe variability of the predictions of the estimator when fit over different\nrandom instances of the same problem. Each problem instance is noted \u201cLS\u201d, for\n\u201cLearning Sample\u201d, in the following. Finally, the noise measures the irreducible part\nof the error which is due the variability in the data.",
            "markdown"
        ],
        [
            "The upper left figure illustrates the predictions (in dark red) of a single\ndecision tree trained over a random dataset LS (the blue dots) of a toy 1d\nregression problem. It also illustrates the predictions (in light red) of other\nsingle decision trees trained over other (and different) randomly drawn\ninstances LS of the problem. Intuitively, the variance term here corresponds to\nthe width of the beam of predictions (in light red) of the individual\nestimators. The larger the variance, the more sensitive are the predictions for\nx to small changes in the training set. The bias term corresponds to the\ndifference between the average prediction of the estimator (in cyan) and the\nbest possible model (in dark blue). On this problem, we can thus observe that\nthe bias is quite low (both the cyan and the blue curves are close to each\nother) while the variance is large (the red beam is rather wide).",
            "markdown"
        ],
        [
            "The lower left figure plots the pointwise decomposition of the expected mean\nsquared error of a single decision tree. It confirms that the bias term (in\nblue) is low while the variance is large (in green). It also illustrates the\nnoise part of the error which, as expected, appears to be constant and around\n0.01.",
            "markdown"
        ],
        [
            "The right figures correspond to the same plots but using instead a bagging\nensemble of decision trees. In both figures, we can observe that the bias term\nis larger than in the previous case. In the upper right figure, the difference\nbetween the average prediction (in cyan) and the best possible model is larger\n(e.g., notice the offset around x=2). In the lower right figure, the bias\ncurve is also slightly higher than in the lower left figure. In terms of\nvariance however, the beam of predictions is narrower, which suggests that the\nvariance is lower. Indeed, as the lower right figure confirms, the variance\nterm (in green) is lower than for single decision trees. Overall, the bias-\nvariance decomposition is therefore no longer the same. The tradeoff is better\nfor bagging: averaging several decision trees fit on bootstrap copies of the\ndataset slightly increases the bias term but allows for a larger reduction of\nthe variance, which results in a lower overall mean squared error (compare the\nred curves int the lower figures). The script output also confirms this\nintuition. The total error of the bagging ensemble is lower than the total\nerror of a single decision tree, and this difference indeed mainly stems from a\nreduced variance.",
            "markdown"
        ],
        [
            "For further details on bias-variance decomposition, see section 7.3 of .",
            "markdown"
        ]
    ],
    "Examples->Ensemble methods->Single estimator versus bagging: bias-variance decomposition->References": [
        [
            "T. Hastie, R. Tibshirani and J. Friedman,\n\u201cElements of Statistical Learning\u201d, Springer, 2009.\n\n\n<img alt=\"Tree, Bagging(Tree)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_bias_variance_001.png\" srcset=\"../../_images/sphx_glr_plot_bias_variance_001.png\"/>",
            "markdown"
        ],
        [
            "Tree: 0.0255 (error) = 0.0003 (bias^2)  + 0.0152 (var) + 0.0098 (noise)\nBagging(Tree): 0.0196 (error) = 0.0004 (bias^2)  + 0.0092 (var) + 0.0098 (noise)\n\n\n\n<br/>",
            "code"
        ],
        [
            "# Author: Gilles Louppe &lt;g.louppe@gmail.com\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.ensemble import \nfrom sklearn.tree import \n\n# Settings\nn_repeat = 50  # Number of iterations for computing expectations\nn_train = 50  # Size of the training set\nn_test = 1000  # Size of the test set\nnoise = 0.1  # Standard deviation of the noise\n(0)\n\n# Change this for exploring the bias-variance decomposition of other\n# estimators. This should work well for estimators with high variance (e.g.,\n# decision trees or KNN), but poorly for estimators with low variance (e.g.,\n# linear models).\nestimators = [\n    (\"Tree\", ()),\n    (\"Bagging(Tree)\", (())),\n]\n\nn_estimators = len(estimators)\n\n\n# Generate data\ndef f(x):\n    x = x.ravel()\n\n    return (-(x**2)) + 1.5 * (-((x - 2) ** 2))\n\n\ndef generate(n_samples, noise, n_repeat=1):\n    X = (n_samples) * 10 - 5\n    X = (X)\n\n    if n_repeat == 1:\n        y = f(X) + (0.0, noise, n_samples)\n    else:\n        y = ((n_samples, n_repeat))\n\n        for i in range(n_repeat):\n            y[:, i] = f(X) + (0.0, noise, n_samples)\n\n    X = X.reshape((n_samples, 1))\n\n    return X, y\n\n\nX_train = []\ny_train = []\n\nfor i in range(n_repeat):\n    X, y = generate(n_samples=n_train, noise=noise)\n    X_train.append(X)\n    y_train.append(y)\n\nX_test, y_test = generate(n_samples=n_test, noise=noise, n_repeat=n_repeat)\n\n(figsize=(10, 8))\n\n# Loop over estimators to compare\nfor n, (name, estimator) in enumerate(estimators):\n    # Compute predictions\n    y_predict = ((n_test, n_repeat))\n\n    for i in range(n_repeat):\n        estimator.fit(X_train[i], y_train[i])\n        y_predict[:, i] = estimator.predict(X_test)\n\n    # Bias^2 + Variance + Noise decomposition of the mean squared error\n    y_error = (n_test)\n\n    for i in range(n_repeat):\n        for j in range(n_repeat):\n            y_error += (y_test[:, j] - y_predict[:, i]) ** 2\n\n    y_error /= n_repeat * n_repeat\n\n    y_noise = (y_test, axis=1)\n    y_bias = (f(X_test) - (y_predict, axis=1)) ** 2\n    y_var = (y_predict, axis=1)\n\n    print(\n        \"{0}: {1:.4f} (error) = {2:.4f} (bias^2) \"\n        \" + {3:.4f} (var) + {4:.4f} (noise)\".format(\n            name, (y_error), (y_bias), (y_var), (y_noise)\n        )\n    )\n\n    # Plot figures\n    (2, n_estimators, n + 1)\n    (X_test, f(X_test), \"b\", label=\"$f(x)$\")\n    (X_train[0], y_train[0], \".b\", label=\"LS ~ $y = f(x)+noise$\")\n\n    for i in range(n_repeat):\n        if i == 0:\n            (X_test, y_predict[:, i], \"r\", label=r\"$\\^y(x)$\")\n        else:\n            (X_test, y_predict[:, i], \"r\", alpha=0.05)\n\n    (X_test, (y_predict, axis=1), \"c\", label=r\"$\\mathbb{E}_{LS} \\^y(x)$\")\n\n    ([-5, 5])\n    (name)\n\n    if n == n_estimators - 1:\n        (loc=(1.1, 0.5))\n\n    (2, n_estimators, n_estimators + n + 1)\n    (X_test, y_error, \"r\", label=\"$error(x)$\")\n    (X_test, y_bias, \"b\", label=\"$bias^2(x)$\"),\n    (X_test, y_var, \"g\", label=\"$variance(x)$\"),\n    (X_test, y_noise, \"c\", label=\"$noise(x)$\")\n\n    ([-5, 5])\n    ([0, 0.1])\n\n    if n == n_estimators - 1:\n\n        (loc=(1.1, 0.5))\n\n(right=0.75)\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  1.262 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Ensemble methods->Two-class AdaBoost": [
        [
            "This example fits an AdaBoosted decision stump on a non-linearly separable\nclassification dataset composed of two \u201cGaussian quantiles\u201d clusters\n(see ) and plots the decision\nboundary and decision scores. The distributions of decision scores are shown\nseparately for samples of class A and B. The predicted class label for each\nsample is determined by the sign of the decision score. Samples with decision\nscores greater than zero are classified as B, and are otherwise classified\nas A. The magnitude of a decision score determines the degree of likeness with\nthe predicted class label. Additionally, a new dataset could be constructed\ncontaining a desired purity of class B, for example, by only selecting samples\nwith a decision score above some value.\n<img alt=\"Decision Boundary, Decision Scores\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_adaboost_twoclass_001.png\" srcset=\"../../_images/sphx_glr_plot_adaboost_twoclass_001.png\"/>",
            "markdown"
        ],
        [
            "/home/circleci/project/examples/ensemble/plot_adaboost_twoclass.py:74: UserWarning:\n\nNo data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n\n\n\n<br/>",
            "code"
        ],
        [
            "# Author: Noel Dawe &lt;noel.dawe@gmail.com\n#\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.ensemble import \nfrom sklearn.tree import \nfrom sklearn.datasets import \nfrom sklearn.inspection import DecisionBoundaryDisplay\n\n\n# Construct dataset\nX1, y1 = (\n    cov=2.0, n_samples=200, n_features=2, n_classes=2, random_state=1\n)\nX2, y2 = (\n    mean=(3, 3), cov=1.5, n_samples=300, n_features=2, n_classes=2, random_state=1\n)\nX = ((X1, X2))\ny = ((y1, -y2 + 1))\n\n# Create and fit an AdaBoosted decision tree\nbdt = (\n    (max_depth=1), algorithm=\"SAMME\", n_estimators=200\n)\n\nbdt.fit(X, y)\n\nplot_colors = \"br\"\nplot_step = 0.02\nclass_names = \"AB\"\n\n(figsize=(10, 5))\n\n# Plot the decision boundaries\nax = (121)\ndisp = (\n    bdt,\n    X,\n    cmap=plt.cm.Paired,\n    response_method=\"predict\",\n    ax=ax,\n    xlabel=\"x\",\n    ylabel=\"y\",\n)\nx_min, x_max = disp.xx0.min(), disp.xx0.max()\ny_min, y_max = disp.xx1.min(), disp.xx1.max()\n(\"tight\")\n\n# Plot the training points\nfor i, n, c in zip(range(2), class_names, plot_colors):\n    idx = (y == i)\n    (\n        X[idx, 0],\n        X[idx, 1],\n        c=c,\n        cmap=plt.cm.Paired,\n        s=20,\n        edgecolor=\"k\",\n        label=\"Class %s\" % n,\n    )\n(x_min, x_max)\n(y_min, y_max)\n(loc=\"upper right\")\n\n(\"Decision Boundary\")\n\n# Plot the two-class decision scores\ntwoclass_output = bdt.decision_function(X)\nplot_range = (twoclass_output.min(), twoclass_output.max())\n(122)\nfor i, n, c in zip(range(2), class_names, plot_colors):\n    (\n        twoclass_output[y == i],\n        bins=10,\n        range=plot_range,\n        facecolor=c,\n        label=\"Class %s\" % n,\n        alpha=0.5,\n        edgecolor=\"k\",\n    )\nx1, x2, y1, y2 = ()\n((x1, x2, y1, y2 * 1.2))\n(loc=\"upper right\")\n(\"Samples\")\n(\"Score\")\n(\"Decision Scores\")\n\n()\n(wspace=0.35)\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.623 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Examples based on real world datasets->Compressive sensing: tomography reconstruction with L1 prior (Lasso)": [
        [
            "This example shows the reconstruction of an image from a set of parallel\nprojections, acquired along different angles. Such a dataset is acquired in\n<strong>computed tomography</strong> (CT).",
            "markdown"
        ],
        [
            "Without any prior information on the sample, the number of projections\nrequired to reconstruct the image is of the order of the linear size\nl of the image (in pixels). For simplicity we consider here a sparse\nimage, where only pixels on the boundary of objects have a non-zero\nvalue. Such data could correspond for example to a cellular material.\nNote however that most images are sparse in a different basis, such as\nthe Haar wavelets. Only l/7 projections are acquired, therefore it is\nnecessary to use prior information available on the sample (its\nsparsity): this is an example of <strong>compressive sensing</strong>.",
            "markdown"
        ],
        [
            "The tomography projection operation is a linear transformation. In\naddition to the data-fidelity term corresponding to a linear regression,\nwe penalize the L1 norm of the image to account for its sparsity. The\nresulting optimization problem is called the . We use the\nclass , that uses the coordinate descent\nalgorithm. Importantly, this implementation is more computationally efficient\non a sparse matrix, than the projection operator used here.",
            "markdown"
        ],
        [
            "The reconstruction with L1 penalization gives a result with zero error\n(all pixels are successfully labeled with 0 or 1), even if noise was\nadded to the projections. In comparison, an L2 penalization\n() produces a large number of labeling\nerrors for the pixels. Important artifacts are observed on the\nreconstructed image, contrary to the L1 penalization. Note in particular\nthe circular artifact separating the pixels in the corners, that have\ncontributed to fewer projections than the central disk.\n<img alt=\"original image, L2 penalization, L1 penalization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_tomography_l1_reconstruction_001.png\" srcset=\"../../_images/sphx_glr_plot_tomography_l1_reconstruction_001.png\"/>",
            "markdown"
        ],
        [
            "# Author: Emmanuelle Gouillart &lt;emmanuelle.gouillart@nsup.org\n# License: BSD 3 clause\n\nimport numpy as np\nfrom scipy import sparse\nfrom scipy import ndimage\nfrom sklearn.linear_model import \nfrom sklearn.linear_model import \nimport matplotlib.pyplot as plt\n\n\ndef _weights(x, dx=1, orig=0):\n    x = (x)\n    floor_x = ((x - orig) / dx).astype()\n    alpha = (x - orig - floor_x * dx) / dx\n    return ((floor_x, floor_x + 1)), ((1 - alpha, alpha))\n\n\ndef _generate_center_coordinates(l_x):\n    X, Y = [:l_x, :l_x].astype()\n    center = l_x / 2.0\n    X += 0.5 - center\n    Y += 0.5 - center\n    return X, Y\n\n\ndef build_projection_operator(l_x, n_dir):\n    \"\"\"Compute the tomography design matrix.\n\n    Parameters\n    ----------\n\n    l_x : int\n        linear size of image array\n\n    n_dir : int\n        number of angles at which projections are acquired.\n\n    Returns\n    -------\n    p : sparse matrix of shape (n_dir l_x, l_x**2)\n    \"\"\"\n    X, Y = _generate_center_coordinates(l_x)\n    angles = (0, , n_dir, endpoint=False)\n    data_inds, weights, camera_inds = [], [], []\n    data_unravel_indices = (l_x**2)\n    data_unravel_indices = ((data_unravel_indices, data_unravel_indices))\n    for i, angle in enumerate(angles):\n        Xrot = (angle) * X - (angle) * Y\n        inds, w = _weights(Xrot, dx=1, orig=X.min())\n        mask = (inds = 0, inds &lt; l_x)\n        weights += list(w[mask])\n        camera_inds += list(inds[mask] + i * l_x)\n        data_inds += list(data_unravel_indices[mask])\n    proj_operator = ((weights, (camera_inds, data_inds)))\n    return proj_operator\n\n\ndef generate_synthetic_data():\n    \"\"\"Synthetic binary data\"\"\"\n    rs = (0)\n    n_pts = 36\n    x, y = [0:l, 0:l]\n    mask_outer = (x - l / 2.0) ** 2 + (y - l / 2.0) ** 2 &lt; (l / 2.0) ** 2\n    mask = ((l, l))\n    points = l * rs.rand(2, n_pts)\n    mask[(points[0]).astype(int), (points[1]).astype(int)] = 1\n    mask = (mask, sigma=l / n_pts)\n    res = (mask  mask.mean(), mask_outer)\n    return (res, (res))\n\n\n# Generate synthetic images, and projections\nl = 128\nproj_operator = build_projection_operator(l, l // 7)\ndata = generate_synthetic_data()\nproj = proj_operator @ data.ravel()[:, ]\nproj += 0.15 * (*proj.shape)\n\n# Reconstruction with L2 (Ridge) penalization\nrgr_ridge = (alpha=0.2)\nrgr_ridge.fit(proj_operator, proj.ravel())\nrec_l2 = rgr_ridge.coef_.reshape(l, l)\n\n# Reconstruction with L1 (Lasso) penalization\n# the best value of alpha was determined using cross validation\n# with LassoCV\nrgr_lasso = (alpha=0.001)\nrgr_lasso.fit(proj_operator, proj.ravel())\nrec_l1 = rgr_lasso.coef_.reshape(l, l)\n\n(figsize=(8, 3.3))\n(131)\n(data, cmap=plt.cm.gray, interpolation=\"nearest\")\n(\"off\")\n(\"original image\")\n(132)\n(rec_l2, cmap=plt.cm.gray, interpolation=\"nearest\")\n(\"L2 penalization\")\n(\"off\")\n(133)\n(rec_l1, cmap=plt.cm.gray, interpolation=\"nearest\")\n(\"L1 penalization\")\n(\"off\")\n\n(hspace=0.01, wspace=0.01, top=1, bottom=0, left=0, right=1)\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  9.849 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Examples based on real world datasets->Faces recognition example using eigenfaces and SVMs": [
        [
            "The dataset used in this example is a preprocessed excerpt of the\n\u201cLabeled Faces in the Wild\u201d, aka :",
            "markdown"
        ],
        [
            "(233MB)\n</blockquote>",
            "markdown"
        ],
        [
            "from time import \nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import \nfrom sklearn.model_selection import \nfrom sklearn.datasets import \nfrom sklearn.metrics import \nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.preprocessing import \nfrom sklearn.decomposition import \nfrom sklearn.svm import \nfrom sklearn.utils.fixes import loguniform",
            "code"
        ],
        [
            "Download the data, if not already on disk and load it as numpy arrays",
            "markdown"
        ],
        [
            "lfw_people = (min_faces_per_person=70, resize=0.4)\n\n# introspect the images arrays to find the shapes (for plotting)\nn_samples, h, w = lfw_people.images.shape\n\n# for machine learning we use the 2 data directly (as relative pixel\n# positions info is ignored by this model)\nX = lfw_people.data\nn_features = X.shape[1]\n\n# the label to predict is the id of the person\ny = lfw_people.target\ntarget_names = lfw_people.target_names\nn_classes = target_names.shape[0]\n\nprint(\"Total dataset size:\")\nprint(\"n_samples: %d\" % n_samples)\nprint(\"n_features: %d\" % n_features)\nprint(\"n_classes: %d\" % n_classes)",
            "code"
        ],
        [
            "Total dataset size:\nn_samples: 1288\nn_features: 1850\nn_classes: 7",
            "code"
        ],
        [
            "Split into a training set and a test and keep 25% of the data for testing.",
            "markdown"
        ],
        [
            "X_train, X_test, y_train, y_test = (\n    X, y, test_size=0.25, random_state=42\n)\n\nscaler = ()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)",
            "code"
        ],
        [
            "Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled\ndataset): unsupervised feature extraction / dimensionality reduction",
            "markdown"
        ],
        [
            "n_components = 150\n\nprint(\n    \"Extracting the top %d eigenfaces from %d faces\" % (n_components, X_train.shape[0])\n)\nt0 = ()\npca = (n_components=n_components, svd_solver=\"randomized\", whiten=True).fit(X_train)\nprint(\"done in %0.3fs\" % (() - t0))\n\neigenfaces = pca.components_.reshape((n_components, h, w))\n\nprint(\"Projecting the input data on the eigenfaces orthonormal basis\")\nt0 = ()\nX_train_pca = pca.transform(X_train)\nX_test_pca = pca.transform(X_test)\nprint(\"done in %0.3fs\" % (() - t0))",
            "code"
        ],
        [
            "Extracting the top 150 eigenfaces from 966 faces\ndone in 0.095s\nProjecting the input data on the eigenfaces orthonormal basis\ndone in 0.010s",
            "code"
        ],
        [
            "Train a SVM classification model",
            "markdown"
        ],
        [
            "print(\"Fitting the classifier to the training set\")\nt0 = ()\nparam_grid = {\n    \"C\": loguniform(1e3, 1e5),\n    \"gamma\": loguniform(1e-4, 1e-1),\n}\nclf = (\n    (kernel=\"rbf\", class_weight=\"balanced\"), param_grid, n_iter=10\n)\nclf = clf.fit(X_train_pca, y_train)\nprint(\"done in %0.3fs\" % (() - t0))\nprint(\"Best estimator found by grid search:\")\nprint(clf.best_estimator_)",
            "code"
        ],
        [
            "Fitting the classifier to the training set\ndone in 6.272s\nBest estimator found by grid search:\nSVC(C=76823.03433306453, class_weight='balanced', gamma=0.003418945823095797)",
            "code"
        ],
        [
            "Quantitative evaluation of the model quality on the test set",
            "markdown"
        ],
        [
            "print(\"Predicting people's names on the test set\")\nt0 = ()\ny_pred = clf.predict(X_test_pca)\nprint(\"done in %0.3fs\" % (() - t0))\n\nprint((y_test, y_pred, target_names=target_names))\n(\n    clf, X_test_pca, y_test, display_labels=target_names, xticks_rotation=\"vertical\"\n)\n()\n()\n\n\n<img alt=\"plot face recognition\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_face_recognition_001.png\" srcset=\"../../_images/sphx_glr_plot_face_recognition_001.png\"/>",
            "code"
        ],
        [
            "Predicting people's names on the test set\ndone in 0.047s\n                   precision    recall  f1-score   support\n\n     Ariel Sharon       0.75      0.69      0.72        13\n     Colin Powell       0.72      0.87      0.79        60\n  Donald Rumsfeld       0.77      0.63      0.69        27\n    George W Bush       0.88      0.95      0.91       146\nGerhard Schroeder       0.95      0.80      0.87        25\n      Hugo Chavez       0.90      0.60      0.72        15\n       Tony Blair       0.93      0.75      0.83        36\n\n         accuracy                           0.84       322\n        macro avg       0.84      0.75      0.79       322\n     weighted avg       0.85      0.84      0.84       322",
            "code"
        ],
        [
            "Qualitative evaluation of the predictions using matplotlib",
            "markdown"
        ],
        [
            "def plot_gallery(images, titles, h, w, n_row=3, n_col=4):\n    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n    (figsize=(1.8 * n_col, 2.4 * n_row))\n    (bottom=0, left=0.01, right=0.99, top=0.90, hspace=0.35)\n    for i in range(n_row * n_col):\n        (n_row, n_col, i + 1)\n        (images[i].reshape((h, w)), cmap=plt.cm.gray)\n        (titles[i], size=12)\n        (())\n        (())",
            "code"
        ],
        [
            "plot the result of the prediction on a portion of the test set",
            "markdown"
        ],
        [
            "def title(y_pred, y_test, target_names, i):\n    pred_name = target_names[y_pred[i]].rsplit(\" \", 1)[-1]\n    true_name = target_names[y_test[i]].rsplit(\" \", 1)[-1]\n    return \"predicted: %s\\ntrue:      %s\" % (pred_name, true_name)\n\n\nprediction_titles = [\n    title(y_pred, y_test, target_names, i) for i in range(y_pred.shape[0])\n]\n\nplot_gallery(X_test, prediction_titles, h, w)\n\n\n<img alt=\"predicted: Bush true:      Bush, predicted: Bush true:      Bush, predicted: Blair true:      Blair, predicted: Bush true:      Bush, predicted: Bush true:      Bush, predicted: Bush true:      Bush, predicted: Schroeder true:      Schroeder, predicted: Powell true:      Powell, predicted: Bush true:      Bush, predicted: Bush true:      Bush, predicted: Bush true:      Bush, predicted: Bush true:      Bush\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_face_recognition_002.png\" srcset=\"../../_images/sphx_glr_plot_face_recognition_002.png\"/>",
            "code"
        ],
        [
            "plot the gallery of the most significative eigenfaces",
            "markdown"
        ],
        [
            "eigenface_titles = [\"eigenface %d\" % i for i in range(eigenfaces.shape[0])]\nplot_gallery(eigenfaces, eigenface_titles, h, w)\n\n()\n\n\n<img alt=\"eigenface 0, eigenface 1, eigenface 2, eigenface 3, eigenface 4, eigenface 5, eigenface 6, eigenface 7, eigenface 8, eigenface 9, eigenface 10, eigenface 11\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_face_recognition_003.png\" srcset=\"../../_images/sphx_glr_plot_face_recognition_003.png\"/>",
            "code"
        ],
        [
            "Face recognition problem would be much more effectively solved by training\nconvolutional neural networks but this family of models is outside of the scope of\nthe scikit-learn library. Interested readers should instead try to use pytorch or\ntensorflow to implement such models.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  24.675 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Examples based on real world datasets->Image denoising using kernel PCA": [
        [
            "This example shows how to use  to\ndenoise images. In short, we take advantage of the approximation function\nlearned during fit to reconstruct the original image.",
            "markdown"
        ],
        [
            "We will compare the results with an exact reconstruction using\n.",
            "markdown"
        ],
        [
            "We will use USPS digits dataset to reproduce presented in Sect. 4 of .",
            "markdown"
        ],
        [
            "References\n\n\n[]",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "# Authors: Guillaume Lemaitre &lt;guillaume.lemaitre@inria.fr\n# Licence: BSD 3 clause",
            "code"
        ]
    ],
    "Examples->Examples based on real world datasets->Image denoising using kernel PCA->Load the dataset via OpenML": [
        [
            "The USPS digits datasets is available in OpenML. We use\n to get this dataset. In addition, we\nnormalize the dataset such that all pixel values are in the range (0, 1).",
            "markdown"
        ],
        [
            "import numpy as np\nfrom sklearn.datasets import \nfrom sklearn.preprocessing import \nfrom sklearn.model_selection import \n\nX, y = (data_id=41082, as_frame=False, return_X_y=True, parser=\"pandas\")\nX = ().fit_transform(X)",
            "code"
        ],
        [
            "The idea will be to learn a PCA basis (with and without a kernel) on\nnoisy images and then use these models to reconstruct and denoise these\nimages.",
            "markdown"
        ],
        [
            "Thus, we split our dataset into a training and testing set composed of 1,000\nsamples for the training and 100 samples for testing. These images are\nnoise-free and we will use them to evaluate the efficiency of the denoising\napproaches. In addition, we create a copy of the original dataset and add a\nGaussian noise.",
            "markdown"
        ],
        [
            "The idea of this application, is to show that we can denoise corrupted images\nby learning a PCA basis on some uncorrupted images. We will use both a PCA\nand a kernel-based PCA to solve this problem.",
            "markdown"
        ],
        [
            "X_train, X_test, y_train, y_test = (\n    X, y, stratify=y, random_state=0, train_size=1_000, test_size=100\n)\n\nrng = (0)\nnoise = rng.normal(scale=0.25, size=X_test.shape)\nX_test_noisy = X_test + noise\n\nnoise = rng.normal(scale=0.25, size=X_train.shape)\nX_train_noisy = X_train + noise",
            "code"
        ],
        [
            "In addition, we will create a helper function to qualitatively assess the\nimage reconstruction by plotting the test images.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n\n\ndef plot_digits(X, title):\n    \"\"\"Small helper function to plot 100 digits.\"\"\"\n    fig, axs = (nrows=10, ncols=10, figsize=(8, 8))\n    for img, ax in zip(X, axs.ravel()):\n        ax.imshow(img.reshape((16, 16)), cmap=\"Greys\")\n        ax.axis(\"off\")\n    fig.suptitle(title, fontsize=24)",
            "code"
        ],
        [
            "In addition, we will use the mean squared error (MSE) to quantitatively\nassess the image reconstruction.",
            "markdown"
        ],
        [
            "Let\u2019s first have a look to see the difference between noise-free and noisy\nimages. We will check the test set in this regard.",
            "markdown"
        ],
        [
            "plot_digits(X_test, \"Uncorrupted test images\")\nplot_digits(\n    X_test_noisy, f\"Noisy test images\\nMSE: {((X_test - X_test_noisy) ** 2):.2f}\"\n)\n\n\n\n<img alt=\"Uncorrupted test images\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_digits_denoising_001.png\" srcset=\"../../_images/sphx_glr_plot_digits_denoising_001.png\"/>\n<img alt=\"Noisy test images MSE: 0.06\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_digits_denoising_002.png\" srcset=\"../../_images/sphx_glr_plot_digits_denoising_002.png\"/>",
            "code"
        ]
    ],
    "Examples->Examples based on real world datasets->Image denoising using kernel PCA->Learn the PCA basis": [
        [
            "We can now learn our PCA basis using both a linear PCA and a kernel PCA that\nuses a radial basis function (RBF) kernel.",
            "markdown"
        ],
        [
            "from sklearn.decomposition import , \n\npca = (n_components=32)\nkernel_pca = (\n    n_components=400, kernel=\"rbf\", gamma=1e-3, fit_inverse_transform=True, alpha=5e-3\n)\n\npca.fit(X_train_noisy)\n_ = kernel_pca.fit(X_train_noisy)",
            "code"
        ]
    ],
    "Examples->Examples based on real world datasets->Image denoising using kernel PCA->Reconstruct and denoise test images": [
        [
            "Now, we can transform and reconstruct the noisy test set. Since we used less\ncomponents than the number of original features, we will get an approximation\nof the original set. Indeed, by dropping the components explaining variance\nin PCA the least, we hope to remove noise. Similar thinking happens in kernel\nPCA; however, we expect a better reconstruction because we use a non-linear\nkernel to learn the PCA basis and a kernel ridge to learn the mapping\nfunction.",
            "markdown"
        ],
        [
            "X_reconstructed_kernel_pca = kernel_pca.inverse_transform(\n    kernel_pca.transform(X_test_noisy)\n)\nX_reconstructed_pca = pca.inverse_transform(pca.transform(X_test_noisy))",
            "code"
        ],
        [
            "plot_digits(X_test, \"Uncorrupted test images\")\nplot_digits(\n    X_reconstructed_pca,\n    f\"PCA reconstruction\\nMSE: {((X_test - X_reconstructed_pca) ** 2):.2f}\",\n)\nplot_digits(\n    X_reconstructed_kernel_pca,\n    \"Kernel PCA reconstruction\\n\"\n    f\"MSE: {((X_test - X_reconstructed_kernel_pca) ** 2):.2f}\",\n)\n\n\n\n<img alt=\"Uncorrupted test images\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_digits_denoising_003.png\" srcset=\"../../_images/sphx_glr_plot_digits_denoising_003.png\"/>\n<img alt=\"PCA reconstruction MSE: 0.01\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_digits_denoising_004.png\" srcset=\"../../_images/sphx_glr_plot_digits_denoising_004.png\"/>\n<img alt=\"Kernel PCA reconstruction MSE: 0.03\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_digits_denoising_005.png\" srcset=\"../../_images/sphx_glr_plot_digits_denoising_005.png\"/>",
            "code"
        ],
        [
            "PCA has a lower MSE than kernel PCA. However, the qualitative analysis might\nnot favor PCA instead of kernel PCA. We observe that kernel PCA is able to\nremove background noise and provide a smoother image.",
            "markdown"
        ],
        [
            "However, it should be noted that the results of the denoising with kernel PCA\nwill depend of the parameters n_components, gamma, and alpha.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  16.096 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Examples based on real world datasets->Libsvm GUI": [
        [
            "A simple graphical frontend for Libsvm mainly intended for didactic\npurposes. You can create data points by point and click and visualize\nthe decision region induced by different kernels and parameter settings.",
            "markdown"
        ],
        [
            "To create positive examples click the left mouse button; to create\nnegative examples click the right button.",
            "markdown"
        ],
        [
            "If all examples are from the same class, it uses a one-class SVM.",
            "markdown"
        ],
        [
            "# Author: Peter Prettenhoer &lt;peter.prettenhofer@gmail.com\n#\n# License: BSD 3 clause\n\nimport matplotlib\n\n(\"TkAgg\")\nfrom matplotlib.backends.backend_tkagg import \n\ntry:\n    from matplotlib.backends.backend_tkagg import NavigationToolbar2Tk\nexcept ImportError:\n    # NavigationToolbar2TkAgg was deprecated in matplotlib 2.2\n    from matplotlib.backends.backend_tkagg import (\n        NavigationToolbar2TkAgg as NavigationToolbar2Tk,\n    )\nfrom matplotlib.figure import \nfrom matplotlib.contour import \n\nimport sys\nimport numpy as np\nimport tkinter as Tk\n\nfrom sklearn import svm\nfrom sklearn.datasets import \n\ny_min, y_max = -50, 50\nx_min, x_max = -50, 50\n\n\nclass Model:\n    \"\"\"The Model which hold the data. It implements the\n    observable in the observer pattern and notifies the\n    registered observers on change event.\n    \"\"\"\n\n    def __init__(self):\n        self.observers = []\n        self.surface = None\n        self.data = []\n        self.cls = None\n        self.surface_type = 0\n\n    def changed(self, event):\n        \"\"\"Notify the observers.\"\"\"\n        for observer in self.observers:\n            observer.update(event, self)\n\n    def add_observer(self, observer):\n        \"\"\"Register an observer.\"\"\"\n        self.observers.append(observer)\n\n    def set_surface(self, surface):\n        self.surface = surface\n\n    def dump_svmlight_file(self, file):\n        data = (self.data)\n        X = data[:, 0:2]\n        y = data[:, 2]\n        (X, y, file)\n\n\nclass Controller:\n    def __init__(self, model):\n        self.model = model\n        self.kernel = Tk.IntVar()\n        self.surface_type = Tk.IntVar()\n        # Whether or not a model has been fitted\n        self.fitted = False\n\n    def fit(self):\n        print(\"fit the model\")\n        train = (self.model.data)\n        X = train[:, 0:2]\n        y = train[:, 2]\n\n        C = float(self.complexity.get())\n        gamma = float(self.gamma.get())\n        coef0 = float(self.coef0.get())\n        degree = int(self.degree.get())\n        kernel_map = {0: \"linear\", 1: \"rbf\", 2: \"poly\"}\n        if len((y)) == 1:\n            clf = (\n                kernel=kernel_map[self.kernel.get()],\n                gamma=gamma,\n                coef0=coef0,\n                degree=degree,\n            )\n            clf.fit(X)\n        else:\n            clf = (\n                kernel=kernel_map[self.kernel.get()],\n                C=C,\n                gamma=gamma,\n                coef0=coef0,\n                degree=degree,\n            )\n            clf.fit(X, y)\n        if hasattr(clf, \"score\"):\n            print(\"Accuracy:\", clf.score(X, y) * 100)\n        X1, X2, Z = self.decision_surface(clf)\n        self.model.clf = clf\n        self.model.set_surface((X1, X2, Z))\n        self.model.surface_type = self.surface_type.get()\n        self.fitted = True\n        self.model.changed(\"surface\")\n\n    def decision_surface(self, cls):\n        delta = 1\n        x = (x_min, x_max + delta, delta)\n        y = (y_min, y_max + delta, delta)\n        X1, X2 = (x, y)\n        Z = cls.decision_function([X1.ravel(), X2.ravel()])\n        Z = Z.reshape(X1.shape)\n        return X1, X2, Z\n\n    def clear_data(self):\n        self.model.data = []\n        self.fitted = False\n        self.model.changed(\"clear\")\n\n    def add_example(self, x, y, label):\n        self.model.data.append((x, y, label))\n        self.model.changed(\"example_added\")\n\n        # update decision surface if already fitted.\n        self.refit()\n\n    def refit(self):\n        \"\"\"Refit the model if already fitted.\"\"\"\n        if self.fitted:\n            self.fit()\n\n\nclass View:\n    \"\"\"Test docstring.\"\"\"\n\n    def __init__(self, root, controller):\n        f = ()\n        ax = f.add_subplot(111)\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_xlim((x_min, x_max))\n        ax.set_ylim((y_min, y_max))\n        canvas = (f, master=root)\n        try:\n            canvas.draw()\n        except AttributeError:\n            # support for matplotlib (1.*)\n            canvas.show()\n        canvas.get_tk_widget().pack(side=Tk.TOP, fill=Tk.BOTH, expand=1)\n        canvas._tkcanvas.pack(side=Tk.TOP, fill=Tk.BOTH, expand=1)\n        canvas.mpl_connect(\"button_press_event\", self.onclick)\n        toolbar = NavigationToolbar2Tk(canvas, root)\n        toolbar.update()\n        self.controllbar = ControllBar(root, controller)\n        self.f = f\n        self.ax = ax\n        self.canvas = canvas\n        self.controller = controller\n        self.contours = []\n        self.c_labels = None\n        self.plot_kernels()\n\n    def plot_kernels(self):\n        self.ax.text(-50, -60, \"Linear: $u^T v$\")\n        self.ax.text(-20, -60, r\"RBF: $\\exp (-\\gamma \\| u-v \\|^2)$\")\n        self.ax.text(10, -60, r\"Poly: $(\\gamma \\, u^T v + r)^d$\")\n\n    def onclick(self, event):\n        if event.xdata and event.ydata:\n            if event.button == 1:\n                self.controller.add_example(event.xdata, event.ydata, 1)\n            elif event.button == 3:\n                self.controller.add_example(event.xdata, event.ydata, -1)\n\n    def update_example(self, model, idx):\n        x, y, l = model.data[idx]\n        if l == 1:\n            color = \"w\"\n        elif l == -1:\n            color = \"k\"\n        self.ax.plot([x], [y], \"%so\" % color, scalex=0.0, scaley=0.0)\n\n    def update(self, event, model):\n        if event == \"examples_loaded\":\n            for i in range(len(model.data)):\n                self.update_example(model, i)\n\n        if event == \"example_added\":\n            self.update_example(model, -1)\n\n        if event == \"clear\":\n            self.ax.clear()\n            self.ax.set_xticks([])\n            self.ax.set_yticks([])\n            self.contours = []\n            self.c_labels = None\n            self.plot_kernels()\n\n        if event == \"surface\":\n            self.remove_surface()\n            self.plot_support_vectors(model.clf.support_vectors_)\n            self.plot_decision_surface(model.surface, model.surface_type)\n\n        self.canvas.draw()\n\n    def remove_surface(self):\n        \"\"\"Remove old decision surface.\"\"\"\n        if len(self.contours)  0:\n            for contour in self.contours:\n                if isinstance(contour, ):\n                    for lineset in contour.collections:\n                        lineset.remove()\n                else:\n                    contour.remove()\n            self.contours = []\n\n    def plot_support_vectors(self, support_vectors):\n        \"\"\"Plot the support vectors by placing circles over the\n        corresponding data points and adds the circle collection\n        to the contours list.\"\"\"\n        cs = self.ax.scatter(\n            support_vectors[:, 0],\n            support_vectors[:, 1],\n            s=80,\n            edgecolors=\"k\",\n            facecolors=\"none\",\n        )\n        self.contours.append(cs)\n\n    def plot_decision_surface(self, surface, type):\n        X1, X2, Z = surface\n        if type == 0:\n            levels = [-1.0, 0.0, 1.0]\n            linestyles = [\"dashed\", \"solid\", \"dashed\"]\n            colors = \"k\"\n            self.contours.append(\n                self.ax.contour(X1, X2, Z, levels, colors=colors, linestyles=linestyles)\n            )\n        elif type == 1:\n            self.contours.append(\n                self.ax.contourf(\n                    X1, X2, Z, 10, cmap=matplotlib.cm.bone, origin=\"lower\", alpha=0.85\n                )\n            )\n            self.contours.append(\n                self.ax.contour(X1, X2, Z, [0.0], colors=\"k\", linestyles=[\"solid\"])\n            )\n        else:\n            raise ValueError(\"surface type unknown\")\n\n\nclass ControllBar:\n    def __init__(self, root, controller):\n        fm = Tk.Frame(root)\n        kernel_group = Tk.Frame(fm)\n        Tk.Radiobutton(\n            kernel_group,\n            text=\"Linear\",\n            variable=controller.kernel,\n            value=0,\n            command=controller.refit,\n        ).pack(anchor=Tk.W)\n        Tk.Radiobutton(\n            kernel_group,\n            text=\"RBF\",\n            variable=controller.kernel,\n            value=1,\n            command=controller.refit,\n        ).pack(anchor=Tk.W)\n        Tk.Radiobutton(\n            kernel_group,\n            text=\"Poly\",\n            variable=controller.kernel,\n            value=2,\n            command=controller.refit,\n        ).pack(anchor=Tk.W)\n        kernel_group.pack(side=Tk.LEFT)\n\n        valbox = Tk.Frame(fm)\n        controller.complexity = Tk.StringVar()\n        controller.complexity.set(\"1.0\")\n        c = Tk.Frame(valbox)\n        Tk.Label(c, text=\"C:\", anchor=\"e\", width=7).pack(side=Tk.LEFT)\n        Tk.Entry(c, width=6, textvariable=controller.complexity).pack(side=Tk.LEFT)\n        c.pack()\n\n        controller.gamma = Tk.StringVar()\n        controller.gamma.set(\"0.01\")\n        g = Tk.Frame(valbox)\n        Tk.Label(g, text=\"gamma:\", anchor=\"e\", width=7).pack(side=Tk.LEFT)\n        Tk.Entry(g, width=6, textvariable=controller.gamma).pack(side=Tk.LEFT)\n        g.pack()\n\n        controller.degree = Tk.StringVar()\n        controller.degree.set(\"3\")\n        d = Tk.Frame(valbox)\n        Tk.Label(d, text=\"degree:\", anchor=\"e\", width=7).pack(side=Tk.LEFT)\n        Tk.Entry(d, width=6, textvariable=controller.degree).pack(side=Tk.LEFT)\n        d.pack()\n\n        controller.coef0 = Tk.StringVar()\n        controller.coef0.set(\"0\")\n        r = Tk.Frame(valbox)\n        Tk.Label(r, text=\"coef0:\", anchor=\"e\", width=7).pack(side=Tk.LEFT)\n        Tk.Entry(r, width=6, textvariable=controller.coef0).pack(side=Tk.LEFT)\n        r.pack()\n        valbox.pack(side=Tk.LEFT)\n\n        cmap_group = Tk.Frame(fm)\n        Tk.Radiobutton(\n            cmap_group,\n            text=\"Hyperplanes\",\n            variable=controller.surface_type,\n            value=0,\n            command=controller.refit,\n        ).pack(anchor=Tk.W)\n        Tk.Radiobutton(\n            cmap_group,\n            text=\"Surface\",\n            variable=controller.surface_type,\n            value=1,\n            command=controller.refit,\n        ).pack(anchor=Tk.W)\n\n        cmap_group.pack(side=Tk.LEFT)\n\n        train_button = Tk.Button(fm, text=\"Fit\", width=5, command=controller.fit)\n        train_button.pack()\n        fm.pack(side=Tk.LEFT)\n        Tk.Button(fm, text=\"Clear\", width=5, command=controller.clear_data).pack(\n            side=Tk.LEFT\n        )\n\n\ndef get_parser():\n    from optparse import \n\n    op = ()\n    op.add_option(\n        \"--output\",\n        action=\"store\",\n        type=\"str\",\n        dest=\"output\",\n        help=\"Path where to dump data.\",\n    )\n    return op\n\n\ndef main(argv):\n    op = get_parser()\n    opts, args = op.parse_args(argv[1:])\n    root = ()\n    model = Model()\n    controller = Controller(model)\n    root.wm_title(\"Scikit-learn Libsvm GUI\")\n    view = View(root, controller)\n    model.add_observer(view)\n    Tk.mainloop()\n\n    if opts.output:\n        model.(opts.output)\n\n\nif __name__ == \"__main__\":\n    main()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Examples based on real world datasets->Model Complexity Influence": [
        [
            "Demonstrate how model complexity influences both prediction accuracy and\ncomputational performance.\n\nWe will be using two datasets:",
            "markdown"
        ],
        [
            "for regression.\nThis dataset consists of 10 measurements taken from diabetes patients.\nThe task is to predict disease progression;",
            "markdown"
        ],
        [
            "for classification. This dataset consists of\nnewsgroup posts. The task is to predict on which topic (out of 20 topics)\nthe post is written about.\n\n\nWe will model the complexity influence on three different estimators:",
            "markdown"
        ],
        [
            "(for classification data)\nwhich implements stochastic gradient descent learning;",
            "markdown"
        ],
        [
            "(for regression data) which implements\nNu support vector regression;",
            "markdown"
        ],
        [
            "builds an additive\nmodel in a forward stage-wise fashion. Notice that\n is much faster\nthan  starting with\nintermediate datasets (n_samples >= 10_000), which is not the case for\nthis example.\n\n\n</dl>",
            "markdown"
        ],
        [
            "We make the model complexity vary through the choice of relevant model\nparameters in each of our selected models. Next, we will measure the influence\non both computational performance (latency) and predictive power (MSE or\nHamming Loss).",
            "markdown"
        ],
        [
            "# Authors: Eustache Diemert &lt;eustache@diemert.fr\n#          Maria Telenczuk &lt;https://github.com/maikia\n#          Guillaume Lemaitre &lt;g.lemaitre58@gmail.com\n# License: BSD 3 clause\n\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets\nfrom sklearn.model_selection import \nfrom sklearn.metrics import \nfrom sklearn.svm import \nfrom sklearn.ensemble import \nfrom sklearn.linear_model import \nfrom sklearn.metrics import \n\n# Initialize random generator\n(0)",
            "code"
        ]
    ],
    "Examples->Examples based on real world datasets->Model Complexity Influence->Load the data": [
        [
            "First we load both datasets.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "We are using\n to download 20\nnewsgroups dataset. It returns ready-to-use features.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "X of the 20 newsgroups dataset is a sparse matrix while X\nof diabetes dataset is a numpy array.",
            "markdown"
        ],
        [
            "def generate_data(case):\n    \"\"\"Generate regression/classification data.\"\"\"\n    if case == \"regression\":\n        X, y = (return_X_y=True)\n        train_size = 0.8\n    elif case == \"classification\":\n        X, y = (subset=\"all\", return_X_y=True)\n        train_size = 0.4  # to make the example run faster\n\n    X_train, X_test, y_train, y_test = (\n        X, y, train_size=train_size, random_state=0\n    )\n\n    data = {\"X_train\": X_train, \"X_test\": X_test, \"y_train\": y_train, \"y_test\": y_test}\n    return data\n\n\nregression_data = generate_data(\"regression\")\nclassification_data = generate_data(\"classification\")",
            "code"
        ]
    ],
    "Examples->Examples based on real world datasets->Model Complexity Influence->Benchmark influence": [
        [
            "Next, we can calculate the influence of the parameters on the given\nestimator. In each round, we will set the estimator with the new value of\nchanging_param and we will be collecting the prediction times, prediction\nperformance and complexities to see how those changes affect the estimator.\nWe will calculate the complexity using complexity_computer passed as a\nparameter.",
            "markdown"
        ],
        [
            "def benchmark_influence(conf):\n    \"\"\"\n    Benchmark influence of `changing_param` on both MSE and latency.\n    \"\"\"\n    prediction_times = []\n    prediction_powers = []\n    complexities = []\n    for param_value in conf[\"changing_param_values\"]:\n        conf[\"tuned_params\"][conf[\"changing_param\"]] = param_value\n        estimator = conf[\"estimator\"](**conf[\"tuned_params\"])\n\n        print(\"Benchmarking %s\" % estimator)\n        estimator.fit(conf[\"data\"][\"X_train\"], conf[\"data\"][\"y_train\"])\n        conf[\"postfit_hook\"](estimator)\n        complexity = conf[\"complexity_computer\"](estimator)\n        complexities.append(complexity)\n        start_time = ()\n        for _ in range(conf[\"n_samples\"]):\n            y_pred = estimator.predict(conf[\"data\"][\"X_test\"])\n        elapsed_time = (() - start_time) / float(conf[\"n_samples\"])\n        prediction_times.append(elapsed_time)\n        pred_score = conf[\"prediction_performance_computer\"](\n            conf[\"data\"][\"y_test\"], y_pred\n        )\n        prediction_powers.append(pred_score)\n        print(\n            \"Complexity: %d | %s: %.4f | Pred. Time: %fs\\n\"\n            % (\n                complexity,\n                conf[\"prediction_performance_label\"],\n                pred_score,\n                elapsed_time,\n            )\n        )\n    return prediction_powers, prediction_times, complexities",
            "code"
        ]
    ],
    "Examples->Examples based on real world datasets->Model Complexity Influence->Choose parameters": [
        [
            "We choose the parameters for each of our estimators by making\na dictionary with all the necessary values.\nchanging_param is the name of the parameter which will vary in each\nestimator.\nComplexity will be defined by the complexity_label and calculated using\ncomplexity_computer.\nAlso note that depending on the estimator type we are passing\ndifferent data.",
            "markdown"
        ],
        [
            "def _count_nonzero_coefficients(estimator):\n    a = estimator.coef_.toarray()\n    return (a)\n\n\nconfigurations = [\n    {\n        \"estimator\": ,\n        \"tuned_params\": {\n            \"penalty\": \"elasticnet\",\n            \"alpha\": 0.001,\n            \"loss\": \"modified_huber\",\n            \"fit_intercept\": True,\n            \"tol\": 1e-1,\n            \"n_iter_no_change\": 2,\n        },\n        \"changing_param\": \"l1_ratio\",\n        \"changing_param_values\": [0.25, 0.5, 0.75, 0.9],\n        \"complexity_label\": \"non_zero coefficients\",\n        \"complexity_computer\": _count_nonzero_coefficients,\n        \"prediction_performance_computer\": ,\n        \"prediction_performance_label\": \"Hamming Loss (Misclassification Ratio)\",\n        \"postfit_hook\": lambda x: x.sparsify(),\n        \"data\": classification_data,\n        \"n_samples\": 5,\n    },\n    {\n        \"estimator\": ,\n        \"tuned_params\": {\"C\": 1e3, \"gamma\": 2**-15},\n        \"changing_param\": \"nu\",\n        \"changing_param_values\": [0.05, 0.1, 0.2, 0.35, 0.5],\n        \"complexity_label\": \"n_support_vectors\",\n        \"complexity_computer\": lambda x: len(x.support_vectors_),\n        \"data\": regression_data,\n        \"postfit_hook\": lambda x: x,\n        \"prediction_performance_computer\": ,\n        \"prediction_performance_label\": \"MSE\",\n        \"n_samples\": 15,\n    },\n    {\n        \"estimator\": ,\n        \"tuned_params\": {\n            \"loss\": \"squared_error\",\n            \"learning_rate\": 0.05,\n            \"max_depth\": 2,\n        },\n        \"changing_param\": \"n_estimators\",\n        \"changing_param_values\": [10, 25, 50, 75, 100],\n        \"complexity_label\": \"n_trees\",\n        \"complexity_computer\": lambda x: x.n_estimators,\n        \"data\": regression_data,\n        \"postfit_hook\": lambda x: x,\n        \"prediction_performance_computer\": ,\n        \"prediction_performance_label\": \"MSE\",\n        \"n_samples\": 15,\n    },\n]",
            "code"
        ]
    ],
    "Examples->Examples based on real world datasets->Model Complexity Influence->Run the code and plot the results": [
        [
            "We defined all the functions required to run our benchmark. Now, we will loop\nover the different configurations that we defined previously. Subsequently,\nwe can analyze the plots obtained from the benchmark:\nRelaxing the L1 penalty in the SGD classifier reduces the prediction error\nbut leads to an increase in the training time.\nWe can draw a similar analysis regarding the training time which increases\nwith the number of support vectors with a Nu-SVR. However, we observed that\nthere is an optimal number of support vectors which reduces the prediction\nerror. Indeed, too few support vectors lead to an under-fitted model while\ntoo many support vectors lead to an over-fitted model.\nThe exact same conclusion can be drawn for the gradient-boosting model. The\nonly the difference with the Nu-SVR is that having too many trees in the\nensemble is not as detrimental.",
            "markdown"
        ],
        [
            "def plot_influence(conf, mse_values, prediction_times, complexities):\n    \"\"\"\n    Plot influence of model complexity on both accuracy and latency.\n    \"\"\"\n\n    fig = ()\n    fig.subplots_adjust(right=0.75)\n\n    # first axes (prediction error)\n    ax1 = fig.add_subplot(111)\n    line1 = ax1.plot(complexities, mse_values, c=\"tab:blue\", ls=\"-\")[0]\n    ax1.set_xlabel(\"Model Complexity (%s)\" % conf[\"complexity_label\"])\n    y1_label = conf[\"prediction_performance_label\"]\n    ax1.set_ylabel(y1_label)\n\n    ax1.spines[\"left\"].set_color(line1.get_color())\n    ax1.yaxis.label.set_color(line1.get_color())\n    ax1.tick_params(axis=\"y\", colors=line1.get_color())\n\n    # second axes (latency)\n    ax2 = fig.add_subplot(111, sharex=ax1, frameon=False)\n    line2 = ax2.plot(complexities, prediction_times, c=\"tab:orange\", ls=\"-\")[0]\n    ax2.yaxis.tick_right()\n    ax2.yaxis.set_label_position(\"right\")\n    y2_label = \"Time (s)\"\n    ax2.set_ylabel(y2_label)\n    ax1.spines[\"right\"].set_color(line2.get_color())\n    ax2.yaxis.label.set_color(line2.get_color())\n    ax2.tick_params(axis=\"y\", colors=line2.get_color())\n\n    (\n        (line1, line2), (\"prediction error\", \"prediction latency\"), loc=\"upper center\"\n    )\n\n    (\n        \"Influence of varying '%s' on %s\"\n        % (conf[\"changing_param\"], conf[\"estimator\"].__name__)\n    )\n\n\nfor conf in configurations:\n    prediction_performances, prediction_times, complexities = benchmark_influence(conf)\n    plot_influence(conf, prediction_performances, prediction_times, complexities)\n()\n\n\n\n<img alt=\"Influence of varying 'l1_ratio' on SGDClassifier\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_model_complexity_influence_001.png\" srcset=\"../../_images/sphx_glr_plot_model_complexity_influence_001.png\"/>\n<img alt=\"Influence of varying 'nu' on NuSVR\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_model_complexity_influence_002.png\" srcset=\"../../_images/sphx_glr_plot_model_complexity_influence_002.png\"/>\n<img alt=\"Influence of varying 'n_estimators' on GradientBoostingRegressor\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_model_complexity_influence_003.png\" srcset=\"../../_images/sphx_glr_plot_model_complexity_influence_003.png\"/>",
            "code"
        ],
        [
            "Benchmarking SGDClassifier(alpha=0.001, l1_ratio=0.25, loss='modified_huber',\n              n_iter_no_change=2, penalty='elasticnet', tol=0.1)\nComplexity: 4948 | Hamming Loss (Misclassification Ratio): 0.2675 | Pred. Time: 0.060509s\n\nBenchmarking SGDClassifier(alpha=0.001, l1_ratio=0.5, loss='modified_huber',\n              n_iter_no_change=2, penalty='elasticnet', tol=0.1)\nComplexity: 1847 | Hamming Loss (Misclassification Ratio): 0.3264 | Pred. Time: 0.046364s\n\nBenchmarking SGDClassifier(alpha=0.001, l1_ratio=0.75, loss='modified_huber',\n              n_iter_no_change=2, penalty='elasticnet', tol=0.1)\nComplexity: 997 | Hamming Loss (Misclassification Ratio): 0.3383 | Pred. Time: 0.039613s\n\nBenchmarking SGDClassifier(alpha=0.001, l1_ratio=0.9, loss='modified_huber',\n              n_iter_no_change=2, penalty='elasticnet', tol=0.1)\nComplexity: 802 | Hamming Loss (Misclassification Ratio): 0.3582 | Pred. Time: 0.034851s\n\nBenchmarking NuSVR(C=1000.0, gamma=3.0517578125e-05, nu=0.05)\nComplexity: 18 | MSE: 5558.7313 | Pred. Time: 0.000159s\n\nBenchmarking NuSVR(C=1000.0, gamma=3.0517578125e-05, nu=0.1)\nComplexity: 36 | MSE: 5289.8022 | Pred. Time: 0.000243s\n\nBenchmarking NuSVR(C=1000.0, gamma=3.0517578125e-05, nu=0.2)\nComplexity: 72 | MSE: 5193.8353 | Pred. Time: 0.000403s\n\nBenchmarking NuSVR(C=1000.0, gamma=3.0517578125e-05, nu=0.35)\nComplexity: 124 | MSE: 5131.3279 | Pred. Time: 0.000630s\n\nBenchmarking NuSVR(C=1000.0, gamma=3.0517578125e-05)\nComplexity: 178 | MSE: 5149.0779 | Pred. Time: 0.000876s\n\nBenchmarking GradientBoostingRegressor(learning_rate=0.05, max_depth=2, n_estimators=10)\nComplexity: 10 | MSE: 4066.4812 | Pred. Time: 0.000159s\n\nBenchmarking GradientBoostingRegressor(learning_rate=0.05, max_depth=2, n_estimators=25)\nComplexity: 25 | MSE: 3551.1723 | Pred. Time: 0.000143s\n\nBenchmarking GradientBoostingRegressor(learning_rate=0.05, max_depth=2, n_estimators=50)\nComplexity: 50 | MSE: 3445.2171 | Pred. Time: 0.000166s\n\nBenchmarking GradientBoostingRegressor(learning_rate=0.05, max_depth=2, n_estimators=75)\nComplexity: 75 | MSE: 3433.0358 | Pred. Time: 0.000275s\n\nBenchmarking GradientBoostingRegressor(learning_rate=0.05, max_depth=2)\nComplexity: 100 | MSE: 3456.0602 | Pred. Time: 0.000231s",
            "code"
        ]
    ],
    "Examples->Examples based on real world datasets->Model Complexity Influence->Conclusion": [
        [
            "As a conclusion, we can deduce the following insights:",
            "markdown"
        ],
        [
            "a model which is more complex (or expressive) will require a larger\ntraining time;",
            "markdown"
        ],
        [
            "a more complex model does not guarantee to reduce the prediction error.",
            "markdown"
        ],
        [
            "These aspects are related to model generalization and avoiding model\nunder-fitting or over-fitting.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  19.500 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Examples based on real world datasets->Out-of-core classification of text documents": [
        [
            "This is an example showing how scikit-learn can be used for classification\nusing an out-of-core approach: learning from data that doesn\u2019t fit into main\nmemory. We make use of an online classifier, i.e., one that supports the\npartial_fit method, that will be fed with batches of examples. To guarantee\nthat the features space remains the same over time we leverage a\nHashingVectorizer that will project each example into the same feature space.\nThis is especially useful in the case of text classification where new\nfeatures (words) may appear in each batch.",
            "markdown"
        ],
        [
            "# Authors: Eustache Diemert &lt;eustache@diemert.fr\n#          @FedericoV &lt;https://github.com/FedericoV/\n# License: BSD 3 clause\n\nimport itertools\nfrom pathlib import \nfrom hashlib import sha256\nimport re\nimport tarfile\nimport time\nimport sys\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import \n\nfrom html.parser import \nfrom urllib.request import \nfrom sklearn.datasets import \nfrom sklearn.feature_extraction.text import \nfrom sklearn.linear_model import \nfrom sklearn.linear_model import \nfrom sklearn.linear_model import \nfrom sklearn.naive_bayes import \n\n\ndef _not_in_sphinx():\n    # Hack to detect whether we are running by the sphinx builder\n    return \"__file__\" in globals()",
            "code"
        ]
    ],
    "Examples->Examples based on real world datasets->Out-of-core classification of text documents->Reuters Dataset related routines": [
        [
            "The dataset used in this example is Reuters-21578 as provided by the UCI ML\nrepository. It will be automatically downloaded and uncompressed on first\nrun.",
            "markdown"
        ],
        [
            "class ReutersParser():\n    \"\"\"Utility class to parse a SGML file and yield documents one at a time.\"\"\"\n\n    def __init__(self, encoding=\"latin-1\"):\n        .__init__(self)\n        self._reset()\n        self.encoding = encoding\n\n    def handle_starttag(self, tag, attrs):\n        method = \"start_\" + tag\n        getattr(self, method, lambda x: None)(attrs)\n\n    def handle_endtag(self, tag):\n        method = \"end_\" + tag\n        getattr(self, method, lambda: None)()\n\n    def _reset(self):\n        self.in_title = 0\n        self.in_body = 0\n        self.in_topics = 0\n        self.in_topic_d = 0\n        self.title = \"\"\n        self.body = \"\"\n        self.topics = []\n        self.topic_d = \"\"\n\n    def parse(self, fd):\n        self.docs = []\n        for chunk in fd:\n            self.feed(chunk.decode(self.encoding))\n            for doc in self.docs:\n                yield doc\n            self.docs = []\n        self.close()\n\n    def handle_data(self, data):\n        if self.in_body:\n            self.body += data\n        elif self.in_title:\n            self.title += data\n        elif self.in_topic_d:\n            self.topic_d += data\n\n    def start_reuters(self, attributes):\n        pass\n\n    def end_reuters(self):\n        self.body = (r\"\\s+\", r\" \", self.body)\n        self.docs.append(\n            {\"title\": self.title, \"body\": self.body, \"topics\": self.topics}\n        )\n        self._reset()\n\n    def start_title(self, attributes):\n        self.in_title = 1\n\n    def end_title(self):\n        self.in_title = 0\n\n    def start_body(self, attributes):\n        self.in_body = 1\n\n    def end_body(self):\n        self.in_body = 0\n\n    def start_topics(self, attributes):\n        self.in_topics = 1\n\n    def end_topics(self):\n        self.in_topics = 0\n\n    def start_d(self, attributes):\n        self.in_topic_d = 1\n\n    def end_d(self):\n        self.in_topic_d = 0\n        self.topics.append(self.topic_d)\n        self.topic_d = \"\"\n\n\ndef stream_reuters_documents(data_path=None):\n    \"\"\"Iterate over documents of the Reuters dataset.\n\n    The Reuters archive will automatically be downloaded and uncompressed if\n    the `data_path` directory does not exist.\n\n    Documents are represented as dictionaries with 'body' (str),\n    'title' (str), 'topics' (list(str)) keys.\n\n    \"\"\"\n\n    DOWNLOAD_URL = (\n        \"http://archive.ics.uci.edu/ml/machine-learning-databases/\"\n        \"reuters21578-mld/reuters21578.tar.gz\"\n    )\n    ARCHIVE_SHA256 = \"3bae43c9b14e387f76a61b6d82bf98a4fb5d3ef99ef7e7075ff2ccbcf59f9d30\"\n    ARCHIVE_FILENAME = \"reuters21578.tar.gz\"\n\n    if data_path is None:\n        data_path = (()) / \"reuters\"\n    else:\n        data_path = (data_path)\n    if not data_path.exists():\n        \"\"\"Download the dataset.\"\"\"\n        print(\"downloading dataset (once and for all) into %s\" % data_path)\n        data_path.mkdir(parents=True, exist_ok=True)\n\n        def progress(blocknum, bs, size):\n            total_sz_mb = \"%.2f MB\" % (size / 1e6)\n            current_sz_mb = \"%.2f MB\" % ((blocknum * bs) / 1e6)\n            if _not_in_sphinx():\n                sys.stdout.write(\"\\rdownloaded %s / %s\" % (current_sz_mb, total_sz_mb))\n\n        archive_path = data_path / ARCHIVE_FILENAME\n\n        (DOWNLOAD_URL, filename=archive_path, reporthook=progress)\n        if _not_in_sphinx():\n            sys.stdout.write(\"\\r\")\n\n        # Check that the archive was not tampered:\n        assert sha256(archive_path.read_bytes()).hexdigest() == ARCHIVE_SHA256\n\n        print(\"untarring Reuters dataset...\")\n        (archive_path, \"r:gz\").extractall(data_path)\n        print(\"done.\")\n\n    parser = ReutersParser()\n    for filename in data_path.glob(\"*.sgm\"):\n        for doc in parser.parse(open(filename, \"rb\")):\n            yield doc",
            "code"
        ]
    ],
    "Examples->Examples based on real world datasets->Out-of-core classification of text documents->Main": [
        [
            "Create the vectorizer and limit the number of features to a reasonable\nmaximum",
            "markdown"
        ],
        [
            "vectorizer = (\n    decode_error=\"ignore\", n_features=2**18, alternate_sign=False\n)\n\n\n# Iterator over parsed Reuters SGML files.\ndata_stream = stream_reuters_documents()\n\n# We learn a binary classification between the \"acq\" class and all the others.\n# \"acq\" was chosen as it is more or less evenly distributed in the Reuters\n# files. For other datasets, one should take care of creating a test set with\n# a realistic portion of positive instances.\nall_classes = ([0, 1])\npositive_class = \"acq\"\n\n# Here are some classifiers that support the `partial_fit` method\npartial_fit_classifiers = {\n    \"SGD\": (max_iter=5),\n    \"Perceptron\": (),\n    \"NB Multinomial\": (alpha=0.01),\n    \"Passive-Aggressive\": (),\n}\n\n\ndef get_minibatch(doc_iter, size, pos_class=positive_class):\n    \"\"\"Extract a minibatch of examples, return a tuple X_text, y.\n\n    Note: size is before excluding invalid docs with no topics assigned.\n\n    \"\"\"\n    data = [\n        (\"{title}\\n\\n{body}\".format(**doc), pos_class in doc[\"topics\"])\n        for doc in (doc_iter, size)\n        if doc[\"topics\"]\n    ]\n    if not len(data):\n        return ([], dtype=int), ([], dtype=int)\n    X_text, y = zip(*data)\n    return X_text, (y, dtype=int)\n\n\ndef iter_minibatches(doc_iter, minibatch_size):\n    \"\"\"Generator of minibatches.\"\"\"\n    X_text, y = get_minibatch(doc_iter, minibatch_size)\n    while len(X_text):\n        yield X_text, y\n        X_text, y = get_minibatch(doc_iter, minibatch_size)\n\n\n# test data statistics\ntest_stats = {\"n_test\": 0, \"n_test_pos\": 0}\n\n# First we hold out a number of examples to estimate accuracy\nn_test_documents = 1000\ntick = ()\nX_test_text, y_test = get_minibatch(data_stream, 1000)\nparsing_time = () - tick\ntick = ()\nX_test = vectorizer.transform(X_test_text)\nvectorizing_time = () - tick\ntest_stats[\"n_test\"] += len(y_test)\ntest_stats[\"n_test_pos\"] += sum(y_test)\nprint(\"Test set is %d documents (%d positive)\" % (len(y_test), sum(y_test)))\n\n\ndef progress(cls_name, stats):\n    \"\"\"Report progress information, return a string.\"\"\"\n    duration = () - stats[\"t0\"]\n    s = \"%20s classifier : \\t\" % cls_name\n    s += \"%(n_train)6d train docs (%(n_train_pos)6d positive) \" % stats\n    s += \"%(n_test)6d test docs (%(n_test_pos)6d positive) \" % test_stats\n    s += \"accuracy: %(accuracy).3f \" % stats\n    s += \"in %.2fs (%5d docs/s)\" % (duration, stats[\"n_train\"] / duration)\n    return s\n\n\ncls_stats = {}\n\nfor cls_name in partial_fit_classifiers:\n    stats = {\n        \"n_train\": 0,\n        \"n_train_pos\": 0,\n        \"accuracy\": 0.0,\n        \"accuracy_history\": [(0, 0)],\n        \"t0\": (),\n        \"runtime_history\": [(0, 0)],\n        \"total_fit_time\": 0.0,\n    }\n    cls_stats[cls_name] = stats\n\nget_minibatch(data_stream, n_test_documents)\n# Discard test set\n\n# We will feed the classifier with mini-batches of 1000 documents; this means\n# we have at most 1000 docs in memory at any time.  The smaller the document\n# batch, the bigger the relative overhead of the partial fit methods.\nminibatch_size = 1000\n\n# Create the data_stream that parses Reuters SGML files and iterates on\n# documents as a stream.\nminibatch_iterators = iter_minibatches(data_stream, minibatch_size)\ntotal_vect_time = 0.0\n\n# Main loop : iterate on mini-batches of examples\nfor i, (X_train_text, y_train) in enumerate(minibatch_iterators):\n\n    tick = ()\n    X_train = vectorizer.transform(X_train_text)\n    total_vect_time += () - tick\n\n    for cls_name, cls in partial_fit_classifiers.items():\n        tick = ()\n        # update estimator with examples in the current mini-batch\n        cls.partial_fit(X_train, y_train, classes=all_classes)\n\n        # accumulate test accuracy stats\n        cls_stats[cls_name][\"total_fit_time\"] += () - tick\n        cls_stats[cls_name][\"n_train\"] += X_train.shape[0]\n        cls_stats[cls_name][\"n_train_pos\"] += sum(y_train)\n        tick = ()\n        cls_stats[cls_name][\"accuracy\"] = cls.score(X_test, y_test)\n        cls_stats[cls_name][\"prediction_time\"] = () - tick\n        acc_history = (cls_stats[cls_name][\"accuracy\"], cls_stats[cls_name][\"n_train\"])\n        cls_stats[cls_name][\"accuracy_history\"].append(acc_history)\n        run_history = (\n            cls_stats[cls_name][\"accuracy\"],\n            total_vect_time + cls_stats[cls_name][\"total_fit_time\"],\n        )\n        cls_stats[cls_name][\"runtime_history\"].append(run_history)\n\n        if i % 3 == 0:\n            print(progress(cls_name, cls_stats[cls_name]))\n    if i % 3 == 0:\n        print(\"\\n\")",
            "code"
        ],
        [
            "downloading dataset (once and for all) into /home/circleci/scikit_learn_data/reuters\nuntarring Reuters dataset...\ndone.\nTest set is 878 documents (108 positive)\n                 SGD classifier :          962 train docs (   132 positive)    878 test docs (   108 positive) accuracy: 0.915 in 0.67s ( 1438 docs/s)\n          Perceptron classifier :          962 train docs (   132 positive)    878 test docs (   108 positive) accuracy: 0.855 in 0.67s ( 1428 docs/s)\n      NB Multinomial classifier :          962 train docs (   132 positive)    878 test docs (   108 positive) accuracy: 0.877 in 0.69s ( 1402 docs/s)\n  Passive-Aggressive classifier :          962 train docs (   132 positive)    878 test docs (   108 positive) accuracy: 0.933 in 0.69s ( 1392 docs/s)\n\n\n                 SGD classifier :         3911 train docs (   517 positive)    878 test docs (   108 positive) accuracy: 0.938 in 1.98s ( 1972 docs/s)\n          Perceptron classifier :         3911 train docs (   517 positive)    878 test docs (   108 positive) accuracy: 0.936 in 1.99s ( 1970 docs/s)\n      NB Multinomial classifier :         3911 train docs (   517 positive)    878 test docs (   108 positive) accuracy: 0.885 in 1.99s ( 1961 docs/s)\n  Passive-Aggressive classifier :         3911 train docs (   517 positive)    878 test docs (   108 positive) accuracy: 0.941 in 2.00s ( 1958 docs/s)\n\n\n                 SGD classifier :         6821 train docs (   891 positive)    878 test docs (   108 positive) accuracy: 0.952 in 3.14s ( 2171 docs/s)\n          Perceptron classifier :         6821 train docs (   891 positive)    878 test docs (   108 positive) accuracy: 0.952 in 3.14s ( 2169 docs/s)\n      NB Multinomial classifier :         6821 train docs (   891 positive)    878 test docs (   108 positive) accuracy: 0.900 in 3.15s ( 2161 docs/s)\n  Passive-Aggressive classifier :         6821 train docs (   891 positive)    878 test docs (   108 positive) accuracy: 0.953 in 3.16s ( 2159 docs/s)\n\n\n                 SGD classifier :         9759 train docs (  1276 positive)    878 test docs (   108 positive) accuracy: 0.949 in 4.32s ( 2256 docs/s)\n          Perceptron classifier :         9759 train docs (  1276 positive)    878 test docs (   108 positive) accuracy: 0.953 in 4.33s ( 2255 docs/s)\n      NB Multinomial classifier :         9759 train docs (  1276 positive)    878 test docs (   108 positive) accuracy: 0.909 in 4.34s ( 2250 docs/s)\n  Passive-Aggressive classifier :         9759 train docs (  1276 positive)    878 test docs (   108 positive) accuracy: 0.958 in 4.34s ( 2249 docs/s)\n\n\n                 SGD classifier :        11680 train docs (  1499 positive)    878 test docs (   108 positive) accuracy: 0.944 in 5.34s ( 2187 docs/s)\n          Perceptron classifier :        11680 train docs (  1499 positive)    878 test docs (   108 positive) accuracy: 0.956 in 5.34s ( 2185 docs/s)\n      NB Multinomial classifier :        11680 train docs (  1499 positive)    878 test docs (   108 positive) accuracy: 0.915 in 5.36s ( 2180 docs/s)\n  Passive-Aggressive classifier :        11680 train docs (  1499 positive)    878 test docs (   108 positive) accuracy: 0.950 in 5.36s ( 2179 docs/s)\n\n\n                 SGD classifier :        14625 train docs (  1865 positive)    878 test docs (   108 positive) accuracy: 0.965 in 6.53s ( 2240 docs/s)\n          Perceptron classifier :        14625 train docs (  1865 positive)    878 test docs (   108 positive) accuracy: 0.903 in 6.53s ( 2239 docs/s)\n      NB Multinomial classifier :        14625 train docs (  1865 positive)    878 test docs (   108 positive) accuracy: 0.924 in 6.54s ( 2236 docs/s)\n  Passive-Aggressive classifier :        14625 train docs (  1865 positive)    878 test docs (   108 positive) accuracy: 0.957 in 6.54s ( 2235 docs/s)\n\n\n                 SGD classifier :        17360 train docs (  2179 positive)    878 test docs (   108 positive) accuracy: 0.957 in 7.64s ( 2272 docs/s)\n          Perceptron classifier :        17360 train docs (  2179 positive)    878 test docs (   108 positive) accuracy: 0.933 in 7.64s ( 2271 docs/s)\n      NB Multinomial classifier :        17360 train docs (  2179 positive)    878 test docs (   108 positive) accuracy: 0.932 in 7.65s ( 2268 docs/s)\n  Passive-Aggressive classifier :        17360 train docs (  2179 positive)    878 test docs (   108 positive) accuracy: 0.952 in 7.66s ( 2267 docs/s)",
            "code"
        ]
    ],
    "Examples->Examples based on real world datasets->Out-of-core classification of text documents->Plot results": [
        [
            "The plot represents the learning curve of the classifier: the evolution\nof classification accuracy over the course of the mini-batches. Accuracy is\nmeasured on the first 1000 samples, held out as a validation set.",
            "markdown"
        ],
        [
            "To limit the memory consumption, we queue examples up to a fixed amount\nbefore feeding them to the learner.",
            "markdown"
        ],
        [
            "def plot_accuracy(x, y, x_legend):\n    \"\"\"Plot accuracy as a function of x.\"\"\"\n    x = (x)\n    y = (y)\n    (\"Classification accuracy as a function of %s\" % x_legend)\n    (\"%s\" % x_legend)\n    (\"Accuracy\")\n    (True)\n    (x, y)\n\n\n[\"legend.fontsize\"] = 10\ncls_names = list(sorted(cls_stats.keys()))\n\n# Plot accuracy evolution\n()\nfor _, stats in sorted(cls_stats.items()):\n    # Plot accuracy evolution with #examples\n    accuracy, n_examples = zip(*stats[\"accuracy_history\"])\n    plot_accuracy(n_examples, accuracy, \"training examples (#)\")\n    ax = ()\n    ax.set_ylim((0.8, 1))\n(cls_names, loc=\"best\")\n\n()\nfor _, stats in sorted(cls_stats.items()):\n    # Plot accuracy evolution with runtime\n    accuracy, runtime = zip(*stats[\"runtime_history\"])\n    plot_accuracy(runtime, accuracy, \"runtime (s)\")\n    ax = ()\n    ax.set_ylim((0.8, 1))\n(cls_names, loc=\"best\")\n\n# Plot fitting times\n()\nfig = ()\ncls_runtime = [stats[\"total_fit_time\"] for cls_name, stats in sorted(cls_stats.items())]\n\ncls_runtime.append(total_vect_time)\ncls_names.append(\"Vectorization\")\nbar_colors = [\"b\", \"g\", \"r\", \"c\", \"m\", \"y\"]\n\nax = (111)\nrectangles = (range(len(cls_names)), cls_runtime, width=0.5, color=bar_colors)\n\nax.set_xticks((0, len(cls_names) - 1, len(cls_names)))\nax.set_xticklabels(cls_names, fontsize=10)\nymax = max(cls_runtime) * 1.2\nax.set_ylim((0, ymax))\nax.set_ylabel(\"runtime (s)\")\nax.set_title(\"Training Times\")\n\n\ndef autolabel(rectangles):\n    \"\"\"attach some text vi autolabel on rectangles.\"\"\"\n    for rect in rectangles:\n        height = rect.get_height()\n        ax.text(\n            rect.get_x() + rect.get_width() / 2.0,\n            1.05 * height,\n            \"%.4f\" % height,\n            ha=\"center\",\n            va=\"bottom\",\n        )\n        (()[1], rotation=30)\n\n\nautolabel(rectangles)\n()\n()\n\n# Plot prediction times\n()\ncls_runtime = []\ncls_names = list(sorted(cls_stats.keys()))\nfor cls_name, stats in sorted(cls_stats.items()):\n    cls_runtime.append(stats[\"prediction_time\"])\ncls_runtime.append(parsing_time)\ncls_names.append(\"Read/Parse\\n+Feat.Extr.\")\ncls_runtime.append(vectorizing_time)\ncls_names.append(\"Hashing\\n+Vect.\")\n\nax = (111)\nrectangles = (range(len(cls_names)), cls_runtime, width=0.5, color=bar_colors)\n\nax.set_xticks((0, len(cls_names) - 1, len(cls_names)))\nax.set_xticklabels(cls_names, fontsize=8)\n(()[1], rotation=30)\nymax = max(cls_runtime) * 1.2\nax.set_ylim((0, ymax))\nax.set_ylabel(\"runtime (s)\")\nax.set_title(\"Prediction Times (%d instances)\" % n_test_documents)\nautolabel(rectangles)\n()\n()\n\n\n\n<img alt=\"Classification accuracy as a function of training examples (#)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_out_of_core_classification_001.png\" srcset=\"../../_images/sphx_glr_plot_out_of_core_classification_001.png\"/>\n<img alt=\"Classification accuracy as a function of runtime (s)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_out_of_core_classification_002.png\" srcset=\"../../_images/sphx_glr_plot_out_of_core_classification_002.png\"/>\n<img alt=\"Training Times\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_out_of_core_classification_003.png\" srcset=\"../../_images/sphx_glr_plot_out_of_core_classification_003.png\"/>\n<img alt=\"Prediction Times (1000 instances)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_out_of_core_classification_004.png\" srcset=\"../../_images/sphx_glr_plot_out_of_core_classification_004.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  9.907 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Examples based on real world datasets->Outlier detection on a real data set": [
        [
            "This example illustrates the need for robust covariance estimation\non a real data set. It is useful both for outlier detection and for\na better understanding of the data structure.",
            "markdown"
        ],
        [
            "We selected two sets of two variables from the Wine data set\nas an illustration of what kind of analysis can be done with several\noutlier detection tools. For the purpose of visualization, we are working\nwith two-dimensional examples, but one should be aware that things are\nnot so trivial in high-dimension, as it will be pointed out.",
            "markdown"
        ],
        [
            "In both examples below, the main result is that the empirical covariance\nestimate, as a non-robust one, is highly influenced by the heterogeneous\nstructure of the observations. Although the robust covariance estimate is\nable to focus on the main mode of the data distribution, it sticks to the\nassumption that the data should be Gaussian distributed, yielding some biased\nestimation of the data structure, but yet accurate to some extent.\nThe One-Class SVM does not assume any parametric form of the data distribution\nand can therefore model the complex shape of the data much better.",
            "markdown"
        ]
    ],
    "Examples->Examples based on real world datasets->Outlier detection on a real data set->First example": [
        [
            "The first example illustrates how the Minimum Covariance Determinant\nrobust estimator can help concentrate on a relevant cluster when outlying\npoints exist. Here the empirical covariance estimation is skewed by points\noutside of the main cluster. Of course, some screening tools would have pointed\nout the presence of two clusters (Support Vector Machines, Gaussian Mixture\nModels, univariate outlier detection, \u2026). But had it been a high-dimensional\nexample, none of these could be applied that easily.",
            "markdown"
        ],
        [
            "# Author: Virgile Fritsch &lt;virgile.fritsch@inria.fr\n# License: BSD 3 clause\n\nimport numpy as np\nfrom sklearn.covariance import \nfrom sklearn.svm import \nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager\nfrom sklearn.datasets import \n\n# Define \"classifiers\" to be used\nclassifiers = {\n    \"Empirical Covariance\": (support_fraction=1.0, contamination=0.25),\n    \"Robust Covariance (Minimum Covariance Determinant)\": (\n        contamination=0.25\n    ),\n    \"OCSVM\": (nu=0.25, gamma=0.35),\n}\ncolors = [\"m\", \"g\", \"b\"]\nlegend1 = {}\nlegend2 = {}\n\n# Get data\nX1 = ()[\"data\"][:, [1, 2]]  # two clusters\n\n# Learn a frontier for outlier detection with several classifiers\nxx1, yy1 = ((0, 6, 500), (1, 4.5, 500))\nfor i, (clf_name, clf) in enumerate(classifiers.items()):\n    (1)\n    clf.fit(X1)\n    Z1 = clf.decision_function([xx1.ravel(), yy1.ravel()])\n    Z1 = Z1.reshape(xx1.shape)\n    legend1[clf_name] = (\n        xx1, yy1, Z1, levels=[0], linewidths=2, colors=colors[i]\n    )\n\nlegend1_values_list = list(legend1.values())\nlegend1_keys_list = list(legend1.keys())\n\n# Plot the results (= shape of the data points cloud)\n(1)  # two clusters\n(\"Outlier detection on a real data set (wine recognition)\")\n(X1[:, 0], X1[:, 1], color=\"black\")\nbbox_args = dict(boxstyle=\"round\", fc=\"0.8\")\narrow_args = dict(arrowstyle=\"-\")\n(\n    \"outlying points\",\n    xy=(4, 2),\n    xycoords=\"data\",\n    textcoords=\"data\",\n    xytext=(3, 1.25),\n    bbox=bbox_args,\n    arrowprops=arrow_args,\n)\n((xx1.min(), xx1.max()))\n((yy1.min(), yy1.max()))\n(\n    (\n        legend1_values_list[0].collections[0],\n        legend1_values_list[1].collections[0],\n        legend1_values_list[2].collections[0],\n    ),\n    (legend1_keys_list[0], legend1_keys_list[1], legend1_keys_list[2]),\n    loc=\"upper center\",\n    prop=(size=11),\n)\n(\"ash\")\n(\"malic_acid\")\n\n()\n\n\n<img alt=\"Outlier detection on a real data set (wine recognition)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_outlier_detection_wine_001.png\" srcset=\"../../_images/sphx_glr_plot_outlier_detection_wine_001.png\"/>",
            "code"
        ]
    ],
    "Examples->Examples based on real world datasets->Outlier detection on a real data set->Second example": [
        [
            "The second example shows the ability of the Minimum Covariance Determinant\nrobust estimator of covariance to concentrate on the main mode of the data\ndistribution: the location seems to be well estimated, although the\ncovariance is hard to estimate due to the banana-shaped distribution. Anyway,\nwe can get rid of some outlying observations. The One-Class SVM is able to\ncapture the real data structure, but the difficulty is to adjust its kernel\nbandwidth parameter so as to obtain a good compromise between the shape of\nthe data scatter matrix and the risk of over-fitting the data.",
            "markdown"
        ],
        [
            "# Get data\nX2 = ()[\"data\"][:, [6, 9]]  # \"banana\"-shaped\n\n# Learn a frontier for outlier detection with several classifiers\nxx2, yy2 = ((-1, 5.5, 500), (-2.5, 19, 500))\nfor i, (clf_name, clf) in enumerate(classifiers.items()):\n    (2)\n    clf.fit(X2)\n    Z2 = clf.decision_function([xx2.ravel(), yy2.ravel()])\n    Z2 = Z2.reshape(xx2.shape)\n    legend2[clf_name] = (\n        xx2, yy2, Z2, levels=[0], linewidths=2, colors=colors[i]\n    )\n\nlegend2_values_list = list(legend2.values())\nlegend2_keys_list = list(legend2.keys())\n\n# Plot the results (= shape of the data points cloud)\n(2)  # \"banana\" shape\n(\"Outlier detection on a real data set (wine recognition)\")\n(X2[:, 0], X2[:, 1], color=\"black\")\n((xx2.min(), xx2.max()))\n((yy2.min(), yy2.max()))\n(\n    (\n        legend2_values_list[0].collections[0],\n        legend2_values_list[1].collections[0],\n        legend2_values_list[2].collections[0],\n    ),\n    (legend2_keys_list[0], legend2_keys_list[1], legend2_keys_list[2]),\n    loc=\"upper center\",\n    prop=(size=11),\n)\n(\"color_intensity\")\n(\"flavanoids\")\n\n()\n\n\n<img alt=\"Outlier detection on a real data set (wine recognition)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_outlier_detection_wine_002.png\" srcset=\"../../_images/sphx_glr_plot_outlier_detection_wine_002.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  1.426 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Examples based on real world datasets->Prediction Latency": [
        [
            "This is an example showing the prediction latency of various scikit-learn\nestimators.",
            "markdown"
        ],
        [
            "The goal is to measure the latency one can expect when doing predictions\neither in bulk or atomic (i.e. one by one) mode.",
            "markdown"
        ],
        [
            "The plots represent the distribution of the prediction latency as a boxplot.",
            "markdown"
        ],
        [
            "# Authors: Eustache Diemert &lt;eustache@diemert.fr\n# License: BSD 3 clause\n\nfrom collections import \n\nimport time\nimport gc\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import \nfrom sklearn.model_selection import \nfrom sklearn.datasets import \nfrom sklearn.ensemble import \nfrom sklearn.linear_model import \nfrom sklearn.linear_model import \nfrom sklearn.svm import \nfrom sklearn.utils import \n\n\ndef _not_in_sphinx():\n    # Hack to detect whether we are running by the sphinx builder\n    return \"__file__\" in globals()",
            "code"
        ]
    ],
    "Examples->Examples based on real world datasets->Prediction Latency->Benchmark and plot helper functions": [
        [
            "def atomic_benchmark_estimator(estimator, X_test, verbose=False):\n    \"\"\"Measure runtime prediction of each instance.\"\"\"\n    n_instances = X_test.shape[0]\n    runtimes = (n_instances, dtype=float)\n    for i in range(n_instances):\n        instance = X_test[[i], :]\n        start = ()\n        estimator.predict(instance)\n        runtimes[i] = () - start\n    if verbose:\n        print(\n            \"atomic_benchmark runtimes:\",\n            min(runtimes),\n            (runtimes, 50),\n            max(runtimes),\n        )\n    return runtimes\n\n\ndef bulk_benchmark_estimator(estimator, X_test, n_bulk_repeats, verbose):\n    \"\"\"Measure runtime prediction of the whole input.\"\"\"\n    n_instances = X_test.shape[0]\n    runtimes = (n_bulk_repeats, dtype=float)\n    for i in range(n_bulk_repeats):\n        start = ()\n        estimator.predict(X_test)\n        runtimes[i] = () - start\n    runtimes = (list(map(lambda x: x / float(n_instances), runtimes)))\n    if verbose:\n        print(\n            \"bulk_benchmark runtimes:\",\n            min(runtimes),\n            (runtimes, 50),\n            max(runtimes),\n        )\n    return runtimes\n\n\ndef benchmark_estimator(estimator, X_test, n_bulk_repeats=30, verbose=False):\n    \"\"\"\n    Measure runtimes of prediction in both atomic and bulk mode.\n\n    Parameters\n    ----------\n    estimator : already trained estimator supporting `predict()`\n    X_test : test input\n    n_bulk_repeats : how many times to repeat when evaluating bulk mode\n\n    Returns\n    -------\n    atomic_runtimes, bulk_runtimes : a pair of `np.array` which contain the\n    runtimes in seconds.\n\n    \"\"\"\n    atomic_runtimes = atomic_benchmark_estimator(estimator, X_test, verbose)\n    bulk_runtimes = bulk_benchmark_estimator(estimator, X_test, n_bulk_repeats, verbose)\n    return atomic_runtimes, bulk_runtimes\n\n\ndef generate_dataset(n_train, n_test, n_features, noise=0.1, verbose=False):\n    \"\"\"Generate a regression dataset with the given parameters.\"\"\"\n    if verbose:\n        print(\"generating dataset...\")\n\n    X, y, coef = (\n        n_samples=n_train + n_test, n_features=n_features, noise=noise, coef=True\n    )\n\n    random_seed = 13\n    X_train, X_test, y_train, y_test = (\n        X, y, train_size=n_train, test_size=n_test, random_state=random_seed\n    )\n    X_train, y_train = (X_train, y_train, random_state=random_seed)\n\n    X_scaler = ()\n    X_train = X_scaler.fit_transform(X_train)\n    X_test = X_scaler.transform(X_test)\n\n    y_scaler = ()\n    y_train = y_scaler.fit_transform(y_train[:, None])[:, 0]\n    y_test = y_scaler.transform(y_test[:, None])[:, 0]\n\n    ()\n    if verbose:\n        print(\"ok\")\n    return X_train, y_train, X_test, y_test\n\n\ndef boxplot_runtimes(runtimes, pred_type, configuration):\n    \"\"\"\n    Plot a new `Figure` with boxplots of prediction runtimes.\n\n    Parameters\n    ----------\n    runtimes : list of `np.array` of latencies in micro-seconds\n    cls_names : list of estimator class names that generated the runtimes\n    pred_type : 'bulk' or 'atomic'\n\n    \"\"\"\n\n    fig, ax1 = (figsize=(10, 6))\n    bp = (\n        runtimes,\n    )\n\n    cls_infos = [\n        \"%s\\n(%d %s)\"\n        % (\n            estimator_conf[\"name\"],\n            estimator_conf[\"complexity_computer\"](estimator_conf[\"instance\"]),\n            estimator_conf[\"complexity_label\"],\n        )\n        for estimator_conf in configuration[\"estimators\"]\n    ]\n    (ax1, xticklabels=cls_infos)\n    (bp[\"boxes\"], color=\"black\")\n    (bp[\"whiskers\"], color=\"black\")\n    (bp[\"fliers\"], color=\"red\", marker=\"+\")\n\n    ax1.yaxis.grid(True, linestyle=\"-\", which=\"major\", color=\"lightgrey\", alpha=0.5)\n\n    ax1.set_axisbelow(True)\n    ax1.set_title(\n        \"Prediction Time per Instance - %s, %d feats.\"\n        % (pred_type.capitalize(), configuration[\"n_features\"])\n    )\n    ax1.set_ylabel(\"Prediction Time (us)\")\n\n    ()\n\n\ndef benchmark(configuration):\n    \"\"\"Run the whole benchmark.\"\"\"\n    X_train, y_train, X_test, y_test = generate_dataset(\n        configuration[\"n_train\"], configuration[\"n_test\"], configuration[\"n_features\"]\n    )\n\n    stats = {}\n    for estimator_conf in configuration[\"estimators\"]:\n        print(\"Benchmarking\", estimator_conf[\"instance\"])\n        estimator_conf[\"instance\"].fit(X_train, y_train)\n        ()\n        a, b = benchmark_estimator(estimator_conf[\"instance\"], X_test)\n        stats[estimator_conf[\"name\"]] = {\"atomic\": a, \"bulk\": b}\n\n    cls_names = [\n        estimator_conf[\"name\"] for estimator_conf in configuration[\"estimators\"]\n    ]\n    runtimes = [1e6 * stats[clf_name][\"atomic\"] for clf_name in cls_names]\n    boxplot_runtimes(runtimes, \"atomic\", configuration)\n    runtimes = [1e6 * stats[clf_name][\"bulk\"] for clf_name in cls_names]\n    boxplot_runtimes(runtimes, \"bulk (%d)\" % configuration[\"n_test\"], configuration)\n\n\ndef n_feature_influence(estimators, n_train, n_test, n_features, percentile):\n    \"\"\"\n    Estimate influence of the number of features on prediction time.\n\n    Parameters\n    ----------\n\n    estimators : dict of (name (str), estimator) to benchmark\n    n_train : nber of training instances (int)\n    n_test : nber of testing instances (int)\n    n_features : list of feature-space dimensionality to test (int)\n    percentile : percentile at which to measure the speed (int [0-100])\n\n    Returns:\n    --------\n\n    percentiles : dict(estimator_name,\n                       dict(n_features, percentile_perf_in_us))\n\n    \"\"\"\n    percentiles = ()\n    for n in n_features:\n        print(\"benchmarking with %d features\" % n)\n        X_train, y_train, X_test, y_test = generate_dataset(n_train, n_test, n)\n        for cls_name, estimator in estimators.items():\n            estimator.fit(X_train, y_train)\n            ()\n            runtimes = bulk_benchmark_estimator(estimator, X_test, 30, False)\n            percentiles[cls_name][n] = 1e6 * (runtimes, percentile)\n    return percentiles\n\n\ndef plot_n_features_influence(percentiles, percentile):\n    fig, ax1 = (figsize=(10, 6))\n    colors = [\"r\", \"g\", \"b\"]\n    for i, cls_name in enumerate(percentiles.keys()):\n        x = (sorted([n for n in percentiles[cls_name].keys()]))\n        y = ([percentiles[cls_name][n] for n in x])\n        (\n            x,\n            y,\n            color=colors[i],\n        )\n    ax1.yaxis.grid(True, linestyle=\"-\", which=\"major\", color=\"lightgrey\", alpha=0.5)\n    ax1.set_axisbelow(True)\n    ax1.set_title(\"Evolution of Prediction Time with #Features\")\n    ax1.set_xlabel(\"#Features\")\n    ax1.set_ylabel(\"Prediction Time at %d%%-ile (us)\" % percentile)\n    ()\n\n\ndef benchmark_throughputs(configuration, duration_secs=0.1):\n    \"\"\"benchmark throughput for different estimators.\"\"\"\n    X_train, y_train, X_test, y_test = generate_dataset(\n        configuration[\"n_train\"], configuration[\"n_test\"], configuration[\"n_features\"]\n    )\n    throughputs = dict()\n    for estimator_config in configuration[\"estimators\"]:\n        estimator_config[\"instance\"].fit(X_train, y_train)\n        start_time = ()\n        n_predictions = 0\n        while (() - start_time) &lt; duration_secs:\n            estimator_config[\"instance\"].predict(X_test[[0]])\n            n_predictions += 1\n        throughputs[estimator_config[\"name\"]] = n_predictions / duration_secs\n    return throughputs\n\n\ndef plot_benchmark_throughput(throughputs, configuration):\n    fig, ax = (figsize=(10, 6))\n    colors = [\"r\", \"g\", \"b\"]\n    cls_infos = [\n        \"%s\\n(%d %s)\"\n        % (\n            estimator_conf[\"name\"],\n            estimator_conf[\"complexity_computer\"](estimator_conf[\"instance\"]),\n            estimator_conf[\"complexity_label\"],\n        )\n        for estimator_conf in configuration[\"estimators\"]\n    ]\n    cls_values = [\n        throughputs[estimator_conf[\"name\"]]\n        for estimator_conf in configuration[\"estimators\"]\n    ]\n    (range(len(throughputs)), cls_values, width=0.5, color=colors)\n    ax.set_xticks((0.25, len(throughputs) - 0.75, len(throughputs)))\n    ax.set_xticklabels(cls_infos, fontsize=10)\n    ymax = max(cls_values) * 1.2\n    ax.set_ylim((0, ymax))\n    ax.set_ylabel(\"Throughput (predictions/sec)\")\n    ax.set_title(\n        \"Prediction Throughput for different estimators (%d features)\"\n        % configuration[\"n_features\"]\n    )\n    ()",
            "code"
        ]
    ],
    "Examples->Examples based on real world datasets->Prediction Latency->Benchmark bulk/atomic prediction speed for various regressors": [
        [
            "configuration = {\n    \"n_train\": int(1e3),\n    \"n_test\": int(1e2),\n    \"n_features\": int(1e2),\n    \"estimators\": [\n        {\n            \"name\": \"Linear Model\",\n            \"instance\": (\n                penalty=\"elasticnet\", alpha=0.01, l1_ratio=0.25, tol=1e-4\n            ),\n            \"complexity_label\": \"non-zero coefficients\",\n            \"complexity_computer\": lambda clf: (clf.coef_),\n        },\n        {\n            \"name\": \"RandomForest\",\n            \"instance\": (),\n            \"complexity_label\": \"estimators\",\n            \"complexity_computer\": lambda clf: clf.n_estimators,\n        },\n        {\n            \"name\": \"SVR\",\n            \"instance\": (kernel=\"rbf\"),\n            \"complexity_label\": \"support vectors\",\n            \"complexity_computer\": lambda clf: len(clf.support_vectors_),\n        },\n    ],\n}\nbenchmark(configuration)\n\n\n\n<img alt=\"Prediction Time per Instance - Atomic, 100 feats.\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_prediction_latency_001.png\" srcset=\"../../_images/sphx_glr_plot_prediction_latency_001.png\"/>\n<img alt=\"Prediction Time per Instance - Bulk (100), 100 feats.\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_prediction_latency_002.png\" srcset=\"../../_images/sphx_glr_plot_prediction_latency_002.png\"/>",
            "code"
        ],
        [
            "Benchmarking SGDRegressor(alpha=0.01, l1_ratio=0.25, penalty='elasticnet', tol=0.0001)\nBenchmarking RandomForestRegressor()\nBenchmarking SVR()",
            "code"
        ]
    ],
    "Examples->Examples based on real world datasets->Prediction Latency->Benchmark n_features influence on prediction speed": [
        [
            "percentile = 90\npercentiles = n_feature_influence(\n    {\"ridge\": ()},\n    configuration[\"n_train\"],\n    configuration[\"n_test\"],\n    [100, 250, 500],\n    percentile,\n)\nplot_n_features_influence(percentiles, percentile)\n\n\n<img alt=\"Evolution of Prediction Time with #Features\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_prediction_latency_003.png\" srcset=\"../../_images/sphx_glr_plot_prediction_latency_003.png\"/>",
            "code"
        ],
        [
            "benchmarking with 100 features\nbenchmarking with 250 features\nbenchmarking with 500 features",
            "code"
        ]
    ],
    "Examples->Examples based on real world datasets->Prediction Latency->Benchmark throughput": [
        [
            "throughputs = benchmark_throughputs(configuration)\nplot_benchmark_throughput(throughputs, configuration)\n\n\n<img alt=\"Prediction Throughput for different estimators (100 features)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_prediction_latency_004.png\" srcset=\"../../_images/sphx_glr_plot_prediction_latency_004.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  12.131 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Examples based on real world datasets->Species distribution modeling": [
        [
            "Modeling species\u2019 geographic distributions is an important\nproblem in conservation biology. In this example, we\nmodel the geographic distribution of two South American\nmammals given past observations and 14 environmental\nvariables. Since we have only positive examples (there are\nno unsuccessful observations), we cast this problem as a\ndensity estimation problem and use the \nas our modeling tool. The dataset is provided by Phillips et. al. (2006).\nIf available, the example uses\n\nto plot the coast lines and national boundaries of South America.",
            "markdown"
        ],
        [
            "The two species are:",
            "markdown"
        ],
        [
            ",\nthe Brown-throated Sloth.",
            "markdown"
        ],
        [
            ",\nalso known as the Forest Small Rice Rat, a rodent that lives in Peru,\nColombia, Ecuador, Peru, and Venezuela.\n\n</blockquote>",
            "markdown"
        ]
    ],
    "Examples->Examples based on real world datasets->Species distribution modeling->References": [
        [
            "S. J. Phillips, R. P. Anderson, R. E. Schapire - Ecological Modelling,\n190:231-259, 2006.\n\n</blockquote>\n<img alt=\"bradypus variegatus, microryzomys minutus\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_species_distribution_modeling_001.png\" srcset=\"../../_images/sphx_glr_plot_species_distribution_modeling_001.png\"/>",
            "markdown"
        ],
        [
            "________________________________________________________________________________\nModeling distribution of species 'bradypus variegatus'\n - fit OneClassSVM ... done.\n - plot coastlines from coverage\n - predict species distribution\n\n Area under the ROC curve : 0.868443\n________________________________________________________________________________\nModeling distribution of species 'microryzomys minutus'\n - fit OneClassSVM ... done.\n - plot coastlines from coverage\n - predict species distribution\n\n Area under the ROC curve : 0.993919\n\ntime elapsed: 11.34s\n\n\n\n<br/>",
            "code"
        ],
        [
            "# Authors: Peter Prettenhofer &lt;peter.prettenhofer@gmail.com\n#          Jake Vanderplas &lt;vanderplas@astro.washington.edu\n#\n# License: BSD 3 clause\n\nfrom time import \n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.utils import \nfrom sklearn.datasets import \nfrom sklearn import svm, metrics\n\n# if basemap is available, we'll use it.\n# otherwise, we'll improvise later...\ntry:\n    from mpl_toolkits.basemap import Basemap\n\n    basemap = True\nexcept ImportError:\n    basemap = False\n\n\ndef construct_grids(batch):\n    \"\"\"Construct the map grid from the batch object\n\n    Parameters\n    ----------\n    batch : Batch object\n        The object returned by :func:`fetch_species_distributions`\n\n    Returns\n    -------\n    (xgrid, ygrid) : 1-D arrays\n        The grid corresponding to the values in batch.coverages\n    \"\"\"\n    # x,y coordinates for corner cells\n    xmin = batch.x_left_lower_corner + batch.grid_size\n    xmax = xmin + (batch.Nx * batch.grid_size)\n    ymin = batch.y_left_lower_corner + batch.grid_size\n    ymax = ymin + (batch.Ny * batch.grid_size)\n\n    # x coordinates of the grid cells\n    xgrid = (xmin, xmax, batch.grid_size)\n    # y coordinates of the grid cells\n    ygrid = (ymin, ymax, batch.grid_size)\n\n    return (xgrid, ygrid)\n\n\ndef create_species_bunch(species_name, train, test, coverages, xgrid, ygrid):\n    \"\"\"Create a bunch with information about a particular organism\n\n    This will use the test/train record arrays to extract the\n    data specific to the given species name.\n    \"\"\"\n    bunch = (name=\" \".join(species_name.split(\"_\")[:2]))\n    species_name = species_name.encode(\"ascii\")\n    points = dict(test=test, train=train)\n\n    for label, pts in points.items():\n        # choose points associated with the desired species\n        pts = pts[pts[\"species\"] == species_name]\n        bunch[\"pts_%s\" % label] = pts\n\n        # determine coverage values for each of the training & testing points\n        ix = (xgrid, pts[\"dd long\"])\n        iy = (ygrid, pts[\"dd lat\"])\n        bunch[\"cov_%s\" % label] = coverages[:, -iy, ix].T\n\n    return bunch\n\n\ndef plot_species_distribution(\n    species=(\"bradypus_variegatus_0\", \"microryzomys_minutus_0\")\n):\n    \"\"\"\n    Plot the species distribution.\n    \"\"\"\n    if len(species)  2:\n        print(\n            \"Note: when more than two species are provided,\"\n            \" only the first two will be used\"\n        )\n\n    t0 = ()\n\n    # Load the compressed data\n    data = ()\n\n    # Set up the data grid\n    xgrid, ygrid = construct_grids(data)\n\n    # The grid in x,y coordinates\n    X, Y = (xgrid, ygrid[::-1])\n\n    # create a bunch for each species\n    BV_bunch = create_species_bunch(\n        species[0], data.train, data.test, data.coverages, xgrid, ygrid\n    )\n    MM_bunch = create_species_bunch(\n        species[1], data.train, data.test, data.coverages, xgrid, ygrid\n    )\n\n    # background points (grid coordinates) for evaluation\n    (13)\n    background_points = [\n        (low=0, high=data.Ny, size=10000),\n        (low=0, high=data.Nx, size=10000),\n    ].T\n\n    # We'll make use of the fact that coverages[6] has measurements at all\n    # land points.  This will help us decide between land and water.\n    land_reference = data.coverages[6]\n\n    # Fit, predict, and plot for each species.\n    for i, species in enumerate([BV_bunch, MM_bunch]):\n        print(\"_\" * 80)\n        print(\"Modeling distribution of species '%s'\" % species.name)\n\n        # Standardize features\n        mean = species.cov_train.mean(axis=0)\n        std = species.cov_train.std(axis=0)\n        train_cover_std = (species.cov_train - mean) / std\n\n        # Fit OneClassSVM\n        print(\" - fit OneClassSVM ... \", end=\"\")\n        clf = (nu=0.1, kernel=\"rbf\", gamma=0.5)\n        clf.fit(train_cover_std)\n        print(\"done.\")\n\n        # Plot map of South America\n        (1, 2, i + 1)\n        if basemap:\n            print(\" - plot coastlines using basemap\")\n            m = Basemap(\n                projection=\"cyl\",\n                llcrnrlat=Y.min(),\n                urcrnrlat=Y.max(),\n                llcrnrlon=X.min(),\n                urcrnrlon=X.max(),\n                resolution=\"c\",\n            )\n            m.drawcoastlines()\n            m.drawcountries()\n        else:\n            print(\" - plot coastlines from coverage\")\n            (\n                X, Y, land_reference, levels=[-9998], colors=\"k\", linestyles=\"solid\"\n            )\n            ([])\n            ([])\n\n        print(\" - predict species distribution\")\n\n        # Predict species distribution using the training data\n        Z = ((data.Ny, data.Nx), dtype=)\n\n        # We'll predict only for the land points.\n        idx = (land_reference  -9999)\n        coverages_land = data.coverages[:, idx[0], idx[1]].T\n\n        pred = clf.decision_function((coverages_land - mean) / std)\n        Z *= pred.min()\n        Z[idx[0], idx[1]] = pred\n\n        levels = (Z.min(), Z.max(), 25)\n        Z[land_reference == -9999] = -9999\n\n        # plot contours of the prediction\n        (X, Y, Z, levels=levels, cmap=plt.cm.Reds)\n        (format=\"%.2f\")\n\n        # scatter training/testing points\n        (\n            species.pts_train[\"dd long\"],\n            species.pts_train[\"dd lat\"],\n            s=2**2,\n            c=\"black\",\n            marker=\"^\",\n            label=\"train\",\n        )\n        (\n            species.pts_test[\"dd long\"],\n            species.pts_test[\"dd lat\"],\n            s=2**2,\n            c=\"black\",\n            marker=\"x\",\n            label=\"test\",\n        )\n        ()\n        (species.name)\n        (\"equal\")\n\n        # Compute AUC with regards to background points\n        pred_background = Z[background_points[0], background_points[1]]\n        pred_test = clf.decision_function((species.cov_test - mean) / std)\n        scores = [pred_test, pred_background]\n        y = [(pred_test.shape), (pred_background.shape)]\n        fpr, tpr, thresholds = (y, scores)\n        roc_auc = (fpr, tpr)\n        (-35, -70, \"AUC: %.3f\" % roc_auc, ha=\"right\")\n        print(\"\\n Area under the ROC curve : %f\" % roc_auc)\n\n    print(\"\\ntime elapsed: %.2fs\" % (() - t0))\n\n\nplot_species_distribution()\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  11.583 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Examples based on real world datasets->Time-related feature engineering": [
        [
            "This notebook introduces different strategies to leverage time-related features\nfor a bike sharing demand regression task that is highly dependent on business\ncycles (days, weeks, months) and yearly season cycles.",
            "markdown"
        ],
        [
            "In the process, we introduce how to perform periodic feature engineering using\nthe  class and its\nextrapolation=\"periodic\" option.",
            "markdown"
        ]
    ],
    "Examples->Examples based on real world datasets->Time-related feature engineering->Data exploration on the Bike Sharing Demand dataset": [
        [
            "We start by loading the data from the OpenML repository.",
            "markdown"
        ],
        [
            "from sklearn.datasets import \n\nbike_sharing = (\n    \"Bike_Sharing_Demand\", version=2, as_frame=True, parser=\"pandas\"\n)\ndf = bike_sharing.frame",
            "code"
        ],
        [
            "To get a quick understanding of the periodic patterns of the data, let us\nhave a look at the average demand per hour during a week.",
            "markdown"
        ],
        [
            "Note that the week starts on a Sunday, during the weekend. We can clearly\ndistinguish the commute patterns in the morning and evenings of the work days\nand the leisure use of the bikes on the weekends with a more spread peak\ndemand around the middle of the days:",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n\n\nfig, ax = (figsize=(12, 4))\naverage_week_demand = df.groupby([\"weekday\", \"hour\"])[\"count\"].mean()\naverage_week_demand.plot(ax=ax)\n_ = ax.set(\n    title=\"Average hourly bike demand during the week\",\n    xticks=[i * 24 for i in range(7)],\n    xticklabels=[\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\"],\n    xlabel=\"Time of the week\",\n    ylabel=\"Number of bike rentals\",\n)\n\n\n<img alt=\"Average hourly bike demand during the week\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_001.png\" srcset=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_001.png\"/>",
            "code"
        ],
        [
            "The target of the prediction problem is the absolute count of bike rentals on\na hourly basis:",
            "markdown"
        ],
        [
            "df[\"count\"].max()",
            "code"
        ],
        [
            "977",
            "code"
        ],
        [
            "Let us rescale the target variable (number of hourly bike rentals) to predict\na relative demand so that the mean absolute error is more easily interpreted\nas a fraction of the maximum demand.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "The fit method of the models used in this notebook all minimize the\nmean squared error to estimate the conditional mean instead of the mean\nabsolute error that would fit an estimator of the conditional median.",
            "markdown"
        ],
        [
            "When reporting performance measure on the test set in the discussion, we\ninstead choose to focus on the mean absolute error that is more\nintuitive than the (root) mean squared error. Note, however, that the\nbest models for one metric are also the best for the other in this\nstudy.",
            "markdown"
        ],
        [
            "y = df[\"count\"] / df[\"count\"].max()",
            "code"
        ],
        [
            "fig, ax = (figsize=(12, 4))\ny.hist(bins=30, ax=ax)\n_ = ax.set(\n    xlabel=\"Fraction of rented fleet demand\",\n    ylabel=\"Number of hours\",\n)\n\n\n<img alt=\"plot cyclical feature engineering\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_002.png\" srcset=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_002.png\"/>",
            "code"
        ],
        [
            "The input feature data frame is a time annotated hourly log of variables\ndescribing the weather conditions. It includes both numerical and categorical\nvariables. Note that the time information has already been expanded into\nseveral complementary columns.",
            "markdown"
        ],
        [
            "X = df.drop(\"count\", axis=\"columns\")\nX",
            "code"
        ],
        [
            "17379 rows \u00d7 12 columns\n\n\n<br/>\n<br/>",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "If the time information was only present as a date or datetime column, we\ncould have expanded it into hour-in-the-day, day-in-the-week,\nday-in-the-month, month-in-the-year using pandas:",
            "markdown"
        ],
        [
            "We now introspect the distribution of the categorical variables, starting\nwith \"weather\":",
            "markdown"
        ],
        [
            "X[\"weather\"].value_counts()",
            "code"
        ],
        [
            "clear         11413\nmisty          4544\nrain           1419\nheavy_rain        3\nName: weather, dtype: int64",
            "code"
        ],
        [
            "Since there are only 3 \"heavy_rain\" events, we cannot use this category to\ntrain machine learning models with cross validation. Instead, we simplify the\nrepresentation by collapsing those into the \"rain\" category.",
            "markdown"
        ],
        [
            "X[\"weather\"].replace(to_replace=\"heavy_rain\", value=\"rain\", inplace=True)",
            "code"
        ],
        [
            "X[\"weather\"].value_counts()",
            "code"
        ],
        [
            "clear    11413\nmisty     4544\nrain      1422\nName: weather, dtype: int64",
            "code"
        ],
        [
            "As expected, the \"season\" variable is well balanced:",
            "markdown"
        ],
        [
            "X[\"season\"].value_counts()",
            "code"
        ],
        [
            "fall      4496\nsummer    4409\nspring    4242\nwinter    4232\nName: season, dtype: int64",
            "code"
        ]
    ],
    "Examples->Examples based on real world datasets->Time-related feature engineering->Time-based cross-validation": [
        [
            "Since the dataset is a time-ordered event log (hourly demand), we will use a\ntime-sensitive cross-validation splitter to evaluate our demand forecasting\nmodel as realistically as possible. We use a gap of 2 days between the train\nand test side of the splits. We also limit the training set size to make the\nperformance of the CV folds more stable.",
            "markdown"
        ],
        [
            "1000 test datapoints should be enough to quantify the performance of the\nmodel. This represents a bit less than a month and a half of contiguous test\ndata:",
            "markdown"
        ],
        [
            "from sklearn.model_selection import \n\nts_cv = (\n    n_splits=5,\n    gap=48,\n    max_train_size=10000,\n    test_size=1000,\n)",
            "code"
        ],
        [
            "Let us manually inspect the various splits to check that the\nTimeSeriesSplit works as we expect, starting with the first split:",
            "markdown"
        ],
        [
            "all_splits = list(ts_cv.split(X, y))\ntrain_0, test_0 = all_splits[0]",
            "code"
        ],
        [
            "X.iloc[test_0]",
            "code"
        ],
        [
            "1000 rows \u00d7 12 columns\n\n\n<br/>\n<br/>",
            "markdown"
        ],
        [
            "X.iloc[train_0]",
            "code"
        ],
        [
            "10000 rows \u00d7 12 columns\n\n\n<br/>\n<br/>",
            "markdown"
        ],
        [
            "We now inspect the last split:",
            "markdown"
        ],
        [
            "train_4, test_4 = all_splits[4]",
            "code"
        ],
        [
            "X.iloc[test_4]",
            "code"
        ],
        [
            "1000 rows \u00d7 12 columns\n\n\n<br/>\n<br/>",
            "markdown"
        ],
        [
            "X.iloc[train_4]",
            "code"
        ],
        [
            "10000 rows \u00d7 12 columns\n\n\n<br/>\n<br/>",
            "markdown"
        ],
        [
            "All is well. We are now ready to do some predictive modeling!",
            "markdown"
        ]
    ],
    "Examples->Examples based on real world datasets->Time-related feature engineering->Gradient Boosting": [
        [
            "Gradient Boosting Regression with decision trees is often flexible enough to\nefficiently handle heteorogenous tabular data with a mix of categorical and\nnumerical features as long as the number of samples is large enough.",
            "markdown"
        ],
        [
            "Here, we do minimal ordinal encoding for the categorical variables and then\nlet the model know that it should treat those as categorical variables by\nusing a dedicated tree splitting rule. Since we use an ordinal encoder, we\npass the list of categorical values explicitly to use a logical order when\nencoding the categories as integers instead of the lexicographical order.\nThis also has the added benefit of preventing any issue with unknown\ncategories when using cross-validation.",
            "markdown"
        ],
        [
            "The numerical variables need no preprocessing and, for the sake of simplicity,\nwe only try the default hyper-parameters for this model:",
            "markdown"
        ],
        [
            "from sklearn.pipeline import \nfrom sklearn.preprocessing import \nfrom sklearn.compose import \nfrom sklearn.ensemble import \nfrom sklearn.model_selection import \n\n\ncategorical_columns = [\n    \"weather\",\n    \"season\",\n    \"holiday\",\n    \"workingday\",\n]\ncategories = [\n    [\"clear\", \"misty\", \"rain\"],\n    [\"spring\", \"summer\", \"fall\", \"winter\"],\n    [\"False\", \"True\"],\n    [\"False\", \"True\"],\n]\nordinal_encoder = (categories=categories)\n\n\ngbrt_pipeline = (\n    (\n        transformers=[\n            (\"categorical\", ordinal_encoder, categorical_columns),\n        ],\n        remainder=\"passthrough\",\n        # Use short feature names to make it easier to specify the categorical\n        # variables in the HistGradientBoostingRegressor in the next\n        # step of the pipeline.\n        verbose_feature_names_out=False,\n    ),\n    (\n        categorical_features=categorical_columns,\n    ),\n).set_output(transform=\"pandas\")",
            "code"
        ],
        [
            "Lets evaluate our gradient boosting model with the mean absolute error of the\nrelative demand averaged across our 5 time-based cross-validation splits:",
            "markdown"
        ],
        [
            "def evaluate(model, X, y, cv):\n    cv_results = (\n        model,\n        X,\n        y,\n        cv=cv,\n        scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"],\n    )\n    mae = -cv_results[\"test_neg_mean_absolute_error\"]\n    rmse = -cv_results[\"test_neg_root_mean_squared_error\"]\n    print(\n        f\"Mean Absolute Error:     {mae.mean():.3f} +/- {mae.std():.3f}\\n\"\n        f\"Root Mean Squared Error: {rmse.mean():.3f} +/- {rmse.std():.3f}\"\n    )\n\n\nevaluate(gbrt_pipeline, X, y, cv=ts_cv)",
            "code"
        ],
        [
            "Mean Absolute Error:     0.044 +/- 0.003\nRoot Mean Squared Error: 0.068 +/- 0.005",
            "code"
        ],
        [
            "This model has an average error around 4 to 5% of the maximum demand. This is\nquite good for a first trial without any hyper-parameter tuning! We just had\nto make the categorical variables explicit. Note that the time related\nfeatures are passed as is, i.e. without processing them. But this is not much\nof a problem for tree-based models as they can learn a non-monotonic\nrelationship between ordinal input features and the target.",
            "markdown"
        ],
        [
            "This is not the case for linear regression models as we will see in the\nfollowing.",
            "markdown"
        ]
    ],
    "Examples->Examples based on real world datasets->Time-related feature engineering->Naive linear regression": [
        [
            "As usual for linear models, categorical variables need to be one-hot encoded.\nFor consistency, we scale the numerical features to the same 0-1 range using\nclass:sklearn.preprocessing.MinMaxScaler, although in this case it does not\nimpact the results much because they are already on comparable scales:",
            "markdown"
        ],
        [
            "from sklearn.preprocessing import \nfrom sklearn.preprocessing import \nfrom sklearn.linear_model import \nimport numpy as np\n\n\none_hot_encoder = (handle_unknown=\"ignore\", sparse_output=False)\nalphas = (-6, 6, 25)\nnaive_linear_pipeline = (\n    (\n        transformers=[\n            (\"categorical\", one_hot_encoder, categorical_columns),\n        ],\n        remainder=(),\n    ),\n    (alphas=alphas),\n)\n\n\nevaluate(naive_linear_pipeline, X, y, cv=ts_cv)",
            "code"
        ],
        [
            "Mean Absolute Error:     0.142 +/- 0.014\nRoot Mean Squared Error: 0.184 +/- 0.020",
            "code"
        ],
        [
            "The performance is not good: the average error is around 14% of the maximum\ndemand. This is more than three times higher than the average error of the\ngradient boosting model. We can suspect that the naive original encoding\n(merely min-max scaled) of the periodic time-related features might prevent\nthe linear regression model to properly leverage the time information: linear\nregression does not automatically model non-monotonic relationships between\nthe input features and the target. Non-linear terms have to be engineered in\nthe input.",
            "markdown"
        ],
        [
            "For example, the raw numerical encoding of the \"hour\" feature prevents the\nlinear model from recognizing that an increase of hour in the morning from 6\nto 8 should have a strong positive impact on the number of bike rentals while\nan increase of similar magnitude in the evening from 18 to 20 should have a\nstrong negative impact on the predicted number of bike rentals.",
            "markdown"
        ]
    ],
    "Examples->Examples based on real world datasets->Time-related feature engineering->Time-steps as categories": [
        [
            "Since the time features are encoded in a discrete manner using integers (24\nunique values in the \u201chours\u201d feature), we could decide to treat those as\ncategorical variables using a one-hot encoding and thereby ignore any\nassumption implied by the ordering of the hour values.",
            "markdown"
        ],
        [
            "Using one-hot encoding for the time features gives the linear model a lot\nmore flexibility as we introduce one additional feature per discrete time\nlevel.",
            "markdown"
        ],
        [
            "one_hot_linear_pipeline = (\n    (\n        transformers=[\n            (\"categorical\", one_hot_encoder, categorical_columns),\n            (\"one_hot_time\", one_hot_encoder, [\"hour\", \"weekday\", \"month\"]),\n        ],\n        remainder=(),\n    ),\n    (alphas=alphas),\n)\n\nevaluate(one_hot_linear_pipeline, X, y, cv=ts_cv)",
            "code"
        ],
        [
            "Mean Absolute Error:     0.099 +/- 0.011\nRoot Mean Squared Error: 0.131 +/- 0.011",
            "code"
        ],
        [
            "The average error rate of this model is 10% which is much better than using\nthe original (ordinal) encoding of the time feature, confirming our intuition\nthat the linear regression model benefits from the added flexibility to not\ntreat time progression in a monotonic manner.",
            "markdown"
        ],
        [
            "However, this introduces a very large number of new features. If the time of\nthe day was represented in minutes since the start of the day instead of\nhours, one-hot encoding would have introduced 1440 features instead of 24.\nThis could cause some significant overfitting. To avoid this we could use\n instead to re-bin the number\nof levels of fine-grained ordinal or numerical variables while still\nbenefitting from the non-monotonic expressivity advantages of one-hot\nencoding.",
            "markdown"
        ],
        [
            "Finally, we also observe that one-hot encoding completely ignores the\nordering of the hour levels while this could be an interesting inductive bias\nto preserve to some level. In the following we try to explore smooth,\nnon-monotonic encoding that locally preserves the relative ordering of time\nfeatures.",
            "markdown"
        ]
    ],
    "Examples->Examples based on real world datasets->Time-related feature engineering->Trigonometric features": [
        [
            "As a first attempt, we can try to encode each of those periodic features\nusing a sine and cosine transformation with the matching period.",
            "markdown"
        ],
        [
            "Each ordinal time feature is transformed into 2 features that together encode\nequivalent information in a non-monotonic way, and more importantly without\nany jump between the first and the last value of the periodic range.",
            "markdown"
        ],
        [
            "from sklearn.preprocessing import \n\n\ndef sin_transformer(period):\n    return (lambda x: (x / period * 2 * ))\n\n\ndef cos_transformer(period):\n    return (lambda x: (x / period * 2 * ))",
            "code"
        ],
        [
            "Let us visualize the effect of this feature expansion on some synthetic hour\ndata with a bit of extrapolation beyond hour=23:",
            "markdown"
        ],
        [
            "import pandas as pd\n\nhour_df = (\n    (26).reshape(-1, 1),\n    columns=[\"hour\"],\n)\nhour_df[\"hour_sin\"] = sin_transformer(24).fit_transform(hour_df)[\"hour\"]\nhour_df[\"hour_cos\"] = cos_transformer(24).fit_transform(hour_df)[\"hour\"]\nhour_df.plot(x=\"hour\")\n_ = (\"Trigonometric encoding for the 'hour' feature\")\n\n\n<img alt=\"Trigonometric encoding for the 'hour' feature\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_003.png\" srcset=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_003.png\"/>",
            "code"
        ],
        [
            "Let\u2019s use a 2D scatter plot with the hours encoded as colors to better see\nhow this representation maps the 24 hours of the day to a 2D space, akin to\nsome sort of a 24 hour version of an analog clock. Note that the \u201c25th\u201d hour\nis mapped back to the 1st hour because of the periodic nature of the\nsine/cosine representation.",
            "markdown"
        ],
        [
            "fig, ax = (figsize=(7, 5))\nsp = ax.scatter(hour_df[\"hour_sin\"], hour_df[\"hour_cos\"], c=hour_df[\"hour\"])\nax.set(\n    xlabel=\"sin(hour)\",\n    ylabel=\"cos(hour)\",\n)\n_ = fig.colorbar(sp)\n\n\n<img alt=\"plot cyclical feature engineering\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_004.png\" srcset=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_004.png\"/>",
            "code"
        ],
        [
            "We can now build a feature extraction pipeline using this strategy:",
            "markdown"
        ],
        [
            "cyclic_cossin_transformer = (\n    transformers=[\n        (\"categorical\", one_hot_encoder, categorical_columns),\n        (\"month_sin\", sin_transformer(12), [\"month\"]),\n        (\"month_cos\", cos_transformer(12), [\"month\"]),\n        (\"weekday_sin\", sin_transformer(7), [\"weekday\"]),\n        (\"weekday_cos\", cos_transformer(7), [\"weekday\"]),\n        (\"hour_sin\", sin_transformer(24), [\"hour\"]),\n        (\"hour_cos\", cos_transformer(24), [\"hour\"]),\n    ],\n    remainder=(),\n)\ncyclic_cossin_linear_pipeline = (\n    cyclic_cossin_transformer,\n    (alphas=alphas),\n)\nevaluate(cyclic_cossin_linear_pipeline, X, y, cv=ts_cv)",
            "code"
        ],
        [
            "Mean Absolute Error:     0.125 +/- 0.014\nRoot Mean Squared Error: 0.166 +/- 0.020",
            "code"
        ],
        [
            "The performance of our linear regression model with this simple feature\nengineering is a bit better than using the original ordinal time features but\nworse than using the one-hot encoded time features. We will further analyze\npossible reasons for this disappointing outcome at the end of this notebook.",
            "markdown"
        ]
    ],
    "Examples->Examples based on real world datasets->Time-related feature engineering->Periodic spline features": [
        [
            "We can try an alternative encoding of the periodic time-related features\nusing spline transformations with a large enough number of splines, and as a\nresult a larger number of expanded features compared to the sine/cosine\ntransformation:",
            "markdown"
        ],
        [
            "from sklearn.preprocessing import \n\n\ndef periodic_spline_transformer(period, n_splines=None, degree=3):\n    if n_splines is None:\n        n_splines = period\n    n_knots = n_splines + 1  # periodic and include_bias is True\n    return (\n        degree=degree,\n        n_knots=n_knots,\n        knots=(0, period, n_knots).reshape(n_knots, 1),\n        extrapolation=\"periodic\",\n        include_bias=True,\n    )",
            "code"
        ],
        [
            "Again, let us visualize the effect of this feature expansion on some\nsynthetic hour data with a bit of extrapolation beyond hour=23:",
            "markdown"
        ],
        [
            "hour_df = (\n    (0, 26, 1000).reshape(-1, 1),\n    columns=[\"hour\"],\n)\nsplines = periodic_spline_transformer(24, n_splines=12).fit_transform(hour_df)\nsplines_df = (\n    splines,\n    columns=[f\"spline_{i}\" for i in range(splines.shape[1])],\n)\n([hour_df, splines_df], axis=\"columns\").plot(x=\"hour\", cmap=plt.cm.tab20b)\n_ = (\"Periodic spline-based encoding for the 'hour' feature\")\n\n\n<img alt=\"Periodic spline-based encoding for the 'hour' feature\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_005.png\" srcset=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_005.png\"/>",
            "code"
        ],
        [
            "Thanks to the use of the extrapolation=\"periodic\" parameter, we observe\nthat the feature encoding stays smooth when extrapolating beyond midnight.",
            "markdown"
        ],
        [
            "We can now build a predictive pipeline using this alternative periodic\nfeature engineering strategy.",
            "markdown"
        ],
        [
            "It is possible to use fewer splines than discrete levels for those ordinal\nvalues. This makes spline-based encoding more efficient than one-hot encoding\nwhile preserving most of the expressivity:",
            "markdown"
        ],
        [
            "cyclic_spline_transformer = (\n    transformers=[\n        (\"categorical\", one_hot_encoder, categorical_columns),\n        (\"cyclic_month\", periodic_spline_transformer(12, n_splines=6), [\"month\"]),\n        (\"cyclic_weekday\", periodic_spline_transformer(7, n_splines=3), [\"weekday\"]),\n        (\"cyclic_hour\", periodic_spline_transformer(24, n_splines=12), [\"hour\"]),\n    ],\n    remainder=(),\n)\ncyclic_spline_linear_pipeline = (\n    cyclic_spline_transformer,\n    (alphas=alphas),\n)\nevaluate(cyclic_spline_linear_pipeline, X, y, cv=ts_cv)",
            "code"
        ],
        [
            "Mean Absolute Error:     0.097 +/- 0.011\nRoot Mean Squared Error: 0.132 +/- 0.013",
            "code"
        ],
        [
            "Spline features make it possible for the linear model to successfully\nleverage the periodic time-related features and reduce the error from ~14% to\n~10% of the maximum demand, which is similar to what we observed with the\none-hot encoded features.",
            "markdown"
        ]
    ],
    "Examples->Examples based on real world datasets->Time-related feature engineering->Qualitative analysis of the impact of features on linear model predictions": [
        [
            "Here, we want to visualize the impact of the feature engineering choices on\nthe time related shape of the predictions.",
            "markdown"
        ],
        [
            "To do so we consider an arbitrary time-based split to compare the predictions\non a range of held out data points.",
            "markdown"
        ],
        [
            "naive_linear_pipeline.fit(X.iloc[train_0], y.iloc[train_0])\nnaive_linear_predictions = naive_linear_pipeline.predict(X.iloc[test_0])\n\none_hot_linear_pipeline.fit(X.iloc[train_0], y.iloc[train_0])\none_hot_linear_predictions = one_hot_linear_pipeline.predict(X.iloc[test_0])\n\ncyclic_cossin_linear_pipeline.fit(X.iloc[train_0], y.iloc[train_0])\ncyclic_cossin_linear_predictions = cyclic_cossin_linear_pipeline.predict(X.iloc[test_0])\n\ncyclic_spline_linear_pipeline.fit(X.iloc[train_0], y.iloc[train_0])\ncyclic_spline_linear_predictions = cyclic_spline_linear_pipeline.predict(X.iloc[test_0])",
            "code"
        ],
        [
            "We visualize those predictions by zooming on the last 96 hours (4 days) of\nthe test set to get some qualitative insights:",
            "markdown"
        ],
        [
            "last_hours = slice(-96, None)\nfig, ax = (figsize=(12, 4))\nfig.suptitle(\"Predictions by linear models\")\nax.plot(\n    y.iloc[test_0].values[last_hours],\n    \"x-\",\n    alpha=0.2,\n    label=\"Actual demand\",\n    color=\"black\",\n)\nax.plot(naive_linear_predictions[last_hours], \"x-\", label=\"Ordinal time features\")\nax.plot(\n    cyclic_cossin_linear_predictions[last_hours],\n    \"x-\",\n    label=\"Trigonometric time features\",\n)\nax.plot(\n    cyclic_spline_linear_predictions[last_hours],\n    \"x-\",\n    label=\"Spline-based time features\",\n)\nax.plot(\n    one_hot_linear_predictions[last_hours],\n    \"x-\",\n    label=\"One-hot time features\",\n)\n_ = ax.legend()\n\n\n<img alt=\"Predictions by linear models\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_006.png\" srcset=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_006.png\"/>",
            "code"
        ],
        [
            "We can draw the following conclusions from the above plot:",
            "markdown"
        ],
        [
            "The <strong>raw ordinal time-related features</strong> are problematic because they do\nnot capture the natural periodicity: we observe a big jump in the\npredictions at the end of each day when the hour features goes from 23 back\nto 0. We can expect similar artifacts at the end of each week or each year.",
            "markdown"
        ],
        [
            "As expected, the <strong>trigonometric features</strong> (sine and cosine) do not have\nthese discontinuities at midnight, but the linear regression model fails to\nleverage those features to properly model intra-day variations.\nUsing trigonometric features for higher harmonics or additional\ntrigonometric features for the natural period with different phases could\npotentially fix this problem.",
            "markdown"
        ],
        [
            "the <strong>periodic spline-based features</strong> fix those two problems at once: they\ngive more expressivity to the linear model by making it possible to focus\non specific hours thanks to the use of 12 splines. Furthermore the\nextrapolation=\"periodic\" option enforces a smooth representation between\nhour=23 and hour=0.",
            "markdown"
        ],
        [
            "The <strong>one-hot encoded features</strong> behave similarly to the periodic\nspline-based features but are more spiky: for instance they can better\nmodel the morning peak during the week days since this peak lasts shorter\nthan an hour. However, we will see in the following that what can be an\nadvantage for linear models is not necessarily one for more expressive\nmodels.",
            "markdown"
        ],
        [
            "We can also compare the number of features extracted by each feature\nengineering pipeline:",
            "markdown"
        ],
        [
            "naive_linear_pipeline[:-1].transform(X).shape",
            "code"
        ],
        [
            "(17379, 19)",
            "code"
        ],
        [
            "one_hot_linear_pipeline[:-1].transform(X).shape",
            "code"
        ],
        [
            "(17379, 59)",
            "code"
        ],
        [
            "cyclic_cossin_linear_pipeline[:-1].transform(X).shape",
            "code"
        ],
        [
            "(17379, 22)",
            "code"
        ],
        [
            "cyclic_spline_linear_pipeline[:-1].transform(X).shape",
            "code"
        ],
        [
            "(17379, 37)",
            "code"
        ],
        [
            "This confirms that the one-hot encoding and the spline encoding strategies\ncreate a lot more features for the time representation than the alternatives,\nwhich in turn gives the downstream linear model more flexibility (degrees of\nfreedom) to avoid underfitting.",
            "markdown"
        ],
        [
            "Finally, we observe that none of the linear models can approximate the true\nbike rentals demand, especially for the peaks that can be very sharp at rush\nhours during the working days but much flatter during the week-ends: the most\naccurate linear models based on splines or one-hot encoding tend to forecast\npeaks of commuting-related bike rentals even on the week-ends and\nunder-estimate the commuting-related events during the working days.",
            "markdown"
        ],
        [
            "These systematic prediction errors reveal a form of under-fitting and can be\nexplained by the lack of interactions terms between features, e.g.\n\u201cworkingday\u201d and features derived from \u201chours\u201d. This issue will be addressed\nin the following section.",
            "markdown"
        ]
    ],
    "Examples->Examples based on real world datasets->Time-related feature engineering->Modeling pairwise interactions with splines and polynomial features": [
        [
            "Linear models do not automatically capture interaction effects between input\nfeatures. It does not help that some features are marginally non-linear as is\nthe case with features constructed by SplineTransformer (or one-hot\nencoding or binning).",
            "markdown"
        ],
        [
            "However, it is possible to use the PolynomialFeatures class on coarse\ngrained spline encoded hours to model the \u201cworkingday\u201d/\u201dhours\u201d interaction\nexplicitly without introducing too many new variables:",
            "markdown"
        ],
        [
            "from sklearn.preprocessing import \nfrom sklearn.pipeline import \n\n\nhour_workday_interaction = (\n    (\n        [\n            (\"cyclic_hour\", periodic_spline_transformer(24, n_splines=8), [\"hour\"]),\n            (\"workingday\", (lambda x: x == \"True\"), [\"workingday\"]),\n        ]\n    ),\n    (degree=2, interaction_only=True, include_bias=False),\n)",
            "code"
        ],
        [
            "Those features are then combined with the ones already computed in the\nprevious spline-base pipeline. We can observe a nice performance improvemnt\nby modeling this pairwise interaction explicitly:",
            "markdown"
        ],
        [
            "cyclic_spline_interactions_pipeline = (\n    (\n        [\n            (\"marginal\", cyclic_spline_transformer),\n            (\"interactions\", hour_workday_interaction),\n        ]\n    ),\n    (alphas=alphas),\n)\nevaluate(cyclic_spline_interactions_pipeline, X, y, cv=ts_cv)",
            "code"
        ],
        [
            "Mean Absolute Error:     0.078 +/- 0.009\nRoot Mean Squared Error: 0.104 +/- 0.009",
            "code"
        ]
    ],
    "Examples->Examples based on real world datasets->Time-related feature engineering->Modeling non-linear feature interactions with kernels": [
        [
            "The previous analysis highlighted the need to model the interactions between\n\"workingday\" and \"hours\". Another example of a such a non-linear\ninteraction that we would like to model could be the impact of the rain that\nmight not be the same during the working days and the week-ends and holidays\nfor instance.",
            "markdown"
        ],
        [
            "To model all such interactions, we could either use a polynomial expansion on\nall marginal features at once, after their spline-based expansion. However,\nthis would create a quadratic number of features which can cause overfitting\nand computational tractability issues.",
            "markdown"
        ],
        [
            "Alternatively, we can use the Nystr\u00f6m method to compute an approximate\npolynomial kernel expansion. Let us try the latter:",
            "markdown"
        ],
        [
            "from sklearn.kernel_approximation import \n\n\ncyclic_spline_poly_pipeline = (\n    cyclic_spline_transformer,\n    (kernel=\"poly\", degree=2, n_components=300, random_state=0),\n    (alphas=alphas),\n)\nevaluate(cyclic_spline_poly_pipeline, X, y, cv=ts_cv)",
            "code"
        ],
        [
            "Mean Absolute Error:     0.053 +/- 0.002\nRoot Mean Squared Error: 0.076 +/- 0.004",
            "code"
        ],
        [
            "We observe that this model can almost rival the performance of the gradient\nboosted trees with an average error around 5% of the maximum demand.",
            "markdown"
        ],
        [
            "Note that while the final step of this pipeline is a linear regression model,\nthe intermediate steps such as the spline feature extraction and the Nystr\u00f6m\nkernel approximation are highly non-linear. As a result the compound pipeline\nis much more expressive than a simple linear regression model with raw features.",
            "markdown"
        ],
        [
            "For the sake of completeness, we also evaluate the combination of one-hot\nencoding and kernel approximation:",
            "markdown"
        ],
        [
            "one_hot_poly_pipeline = (\n    (\n        transformers=[\n            (\"categorical\", one_hot_encoder, categorical_columns),\n            (\"one_hot_time\", one_hot_encoder, [\"hour\", \"weekday\", \"month\"]),\n        ],\n        remainder=\"passthrough\",\n    ),\n    (kernel=\"poly\", degree=2, n_components=300, random_state=0),\n    (alphas=alphas),\n)\nevaluate(one_hot_poly_pipeline, X, y, cv=ts_cv)",
            "code"
        ],
        [
            "Mean Absolute Error:     0.082 +/- 0.006\nRoot Mean Squared Error: 0.111 +/- 0.011",
            "code"
        ],
        [
            "While one-hot encoded features were competitive with spline-based features\nwhen using linear models, this is no longer the case when using a low-rank\napproximation of a non-linear kernel: this can be explained by the fact that\nspline features are smoother and allow the kernel approximation to find a\nmore expressive decision function.",
            "markdown"
        ],
        [
            "Let us now have a qualitative look at the predictions of the kernel models\nand of the gradient boosted trees that should be able to better model\nnon-linear interactions between features:",
            "markdown"
        ],
        [
            "gbrt_pipeline.fit(X.iloc[train_0], y.iloc[train_0])\ngbrt_predictions = gbrt_pipeline.predict(X.iloc[test_0])\n\none_hot_poly_pipeline.fit(X.iloc[train_0], y.iloc[train_0])\none_hot_poly_predictions = one_hot_poly_pipeline.predict(X.iloc[test_0])\n\ncyclic_spline_poly_pipeline.fit(X.iloc[train_0], y.iloc[train_0])\ncyclic_spline_poly_predictions = cyclic_spline_poly_pipeline.predict(X.iloc[test_0])",
            "code"
        ],
        [
            "Again we zoom on the last 4 days of the test set:",
            "markdown"
        ],
        [
            "last_hours = slice(-96, None)\nfig, ax = (figsize=(12, 4))\nfig.suptitle(\"Predictions by non-linear regression models\")\nax.plot(\n    y.iloc[test_0].values[last_hours],\n    \"x-\",\n    alpha=0.2,\n    label=\"Actual demand\",\n    color=\"black\",\n)\nax.plot(\n    gbrt_predictions[last_hours],\n    \"x-\",\n    label=\"Gradient Boosted Trees\",\n)\nax.plot(\n    one_hot_poly_predictions[last_hours],\n    \"x-\",\n    label=\"One-hot + polynomial kernel\",\n)\nax.plot(\n    cyclic_spline_poly_predictions[last_hours],\n    \"x-\",\n    label=\"Splines + polynomial kernel\",\n)\n_ = ax.legend()\n\n\n<img alt=\"Predictions by non-linear regression models\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_007.png\" srcset=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_007.png\"/>",
            "code"
        ],
        [
            "First, note that trees can naturally model non-linear feature interactions\nsince, by default, decision trees are allowed to grow beyond a depth of 2\nlevels.",
            "markdown"
        ],
        [
            "Here, we can observe that the combinations of spline features and non-linear\nkernels works quite well and can almost rival the accuracy of the gradient\nboosting regression trees.",
            "markdown"
        ],
        [
            "On the contrary, one-hot encoded time features do not perform that well with\nthe low rank kernel model. In particular, they significantly over-estimate\nthe low demand hours more than the competing models.",
            "markdown"
        ],
        [
            "We also observe that none of the models can successfully predict some of the\npeak rentals at the rush hours during the working days. It is possible that\naccess to additional features would be required to further improve the\naccuracy of the predictions. For instance, it could be useful to have access\nto the geographical repartition of the fleet at any point in time or the\nfraction of bikes that are immobilized because they need servicing.",
            "markdown"
        ],
        [
            "Let us finally get a more quantative look at the prediction errors of those\nthree models using the true vs predicted demand scatter plots:",
            "markdown"
        ],
        [
            "fig, axes = (ncols=3, figsize=(12, 4), sharey=True)\nfig.suptitle(\"Non-linear regression models\")\npredictions = [\n    one_hot_poly_predictions,\n    cyclic_spline_poly_predictions,\n    gbrt_predictions,\n]\nlabels = [\n    \"One hot + polynomial kernel\",\n    \"Splines + polynomial kernel\",\n    \"Gradient Boosted Trees\",\n]\nfor ax, pred, label in zip(axes, predictions, labels):\n    ax.scatter(y.iloc[test_0].values, pred, alpha=0.3, label=label)\n    ax.plot([0, 1], [0, 1], \"--\", label=\"Perfect model\")\n    ax.set(\n        xlim=(0, 1),\n        ylim=(0, 1),\n        xlabel=\"True demand\",\n        ylabel=\"Predicted demand\",\n    )\n    ax.legend()\n\n()\n\n\n<img alt=\"Non-linear regression models\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_008.png\" srcset=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_008.png\"/>",
            "code"
        ],
        [
            "This visualization confirms the conclusions we draw on the previous plot.",
            "markdown"
        ],
        [
            "All models under-estimate the high demand events (working day rush hours),\nbut gradient boosting a bit less so. The low demand events are well predicted\non average by gradient boosting while the one-hot polynomial regression\npipeline seems to systematically over-estimate demand in that regime. Overall\nthe predictions of the gradient boosted trees are closer to the diagonal than\nfor the kernel models.",
            "markdown"
        ]
    ],
    "Examples->Examples based on real world datasets->Time-related feature engineering->Concluding remarks": [
        [
            "We note that we could have obtained slightly better results for kernel models\nby using more components (higher rank kernel approximation) at the cost of\nlonger fit and prediction durations. For large values of n_components, the\nperformance of the one-hot encoded features would even match the spline\nfeatures.",
            "markdown"
        ],
        [
            "The Nystroem + RidgeCV regressor could also have been replaced by\n with one or two hidden layers\nand we would have obtained quite similar results.",
            "markdown"
        ],
        [
            "The dataset we used in this case study is sampled on a hourly basis. However\ncyclic spline-based features could model time-within-day or time-within-week\nvery efficiently with finer-grained time resolutions (for instance with\nmeasurements taken every minute instead of every hours) without introducing\nmore features. One-hot encoding time representations would not offer this\nflexibility.",
            "markdown"
        ],
        [
            "Finally, in this notebook we used RidgeCV because it is very efficient from\na computational point of view. However, it models the target variable as a\nGaussian random variable with constant variance. For positive regression\nproblems, it is likely that using a Poisson or Gamma distribution would make\nmore sense. This could be achieved by using\nGridSearchCV(TweedieRegressor(power=2), param_grid({\"alpha\": alphas}))\ninstead of RidgeCV.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  19.443 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Examples based on real world datasets->Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation": [
        [
            "This is an example of applying  and\n on a corpus\nof documents and extract additive models of the topic structure of the\ncorpus.  The output is a plot of topics, each represented as bar plot\nusing top few words based on weights.",
            "markdown"
        ],
        [
            "Non-negative Matrix Factorization is applied with two different objective\nfunctions: the Frobenius norm, and the generalized Kullback-Leibler divergence.\nThe latter is equivalent to Probabilistic Latent Semantic Indexing.",
            "markdown"
        ],
        [
            "The default parameters (n_samples / n_features / n_components) should make\nthe example runnable in a couple of tens of seconds. You can try to\nincrease the dimensions of the problem, but be aware that the time\ncomplexity is polynomial in NMF. In LDA, the time complexity is\nproportional to (n_samples * iterations).\n\n<img alt=\"Topics in NMF model (Frobenius norm), Topic 1, Topic 2, Topic 3, Topic 4, Topic 5, Topic 6, Topic 7, Topic 8, Topic 9, Topic 10\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_topics_extraction_with_nmf_lda_001.png\" srcset=\"../../_images/sphx_glr_plot_topics_extraction_with_nmf_lda_001.png\"/>\n<img alt=\"Topics in NMF model (generalized Kullback-Leibler divergence), Topic 1, Topic 2, Topic 3, Topic 4, Topic 5, Topic 6, Topic 7, Topic 8, Topic 9, Topic 10\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_topics_extraction_with_nmf_lda_002.png\" srcset=\"../../_images/sphx_glr_plot_topics_extraction_with_nmf_lda_002.png\"/>\n<img alt=\"Topics in MiniBatchNMF model (Frobenius norm), Topic 1, Topic 2, Topic 3, Topic 4, Topic 5, Topic 6, Topic 7, Topic 8, Topic 9, Topic 10\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_topics_extraction_with_nmf_lda_003.png\" srcset=\"../../_images/sphx_glr_plot_topics_extraction_with_nmf_lda_003.png\"/>\n<img alt=\"Topics in MiniBatchNMF model (generalized Kullback-Leibler divergence), Topic 1, Topic 2, Topic 3, Topic 4, Topic 5, Topic 6, Topic 7, Topic 8, Topic 9, Topic 10\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_topics_extraction_with_nmf_lda_004.png\" srcset=\"../../_images/sphx_glr_plot_topics_extraction_with_nmf_lda_004.png\"/>\n<img alt=\"Topics in LDA model, Topic 1, Topic 2, Topic 3, Topic 4, Topic 5, Topic 6, Topic 7, Topic 8, Topic 9, Topic 10\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_topics_extraction_with_nmf_lda_005.png\" srcset=\"../../_images/sphx_glr_plot_topics_extraction_with_nmf_lda_005.png\"/>",
            "markdown"
        ],
        [
            "Loading dataset...\ndone in 1.252s.\nExtracting tf-idf features for NMF...\ndone in 0.306s.\nExtracting tf features for LDA...\ndone in 0.290s.\n\nFitting the NMF model (Frobenius norm) with tf-idf features, n_samples=2000 and n_features=1000...\ndone in 0.083s.\n\n\n Fitting the NMF model (generalized Kullback-Leibler divergence) with tf-idf features, n_samples=2000 and n_features=1000...\ndone in 1.214s.\n\n\n Fitting the MiniBatchNMF model (Frobenius norm) with tf-idf features, n_samples=2000 and n_features=1000, batch_size=128...\ndone in 0.115s.\n\n\n Fitting the MiniBatchNMF model (generalized Kullback-Leibler divergence) with tf-idf features, n_samples=2000 and n_features=1000, batch_size=128...\ndone in 0.255s.\n\n\n Fitting LDA models with tf features, n_samples=2000 and n_features=1000...\ndone in 2.768s.\n\n\n\n<br/>",
            "code"
        ],
        [
            "# Author: Olivier Grisel &lt;olivier.grisel@ensta.org\n#         Lars Buitinck\n#         Chyi-Kwei Yau &lt;chyikwei.yau@gmail.com\n# License: BSD 3 clause\n\nfrom time import \nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction.text import , \nfrom sklearn.decomposition import , , \nfrom sklearn.datasets import \n\nn_samples = 2000\nn_features = 1000\nn_components = 10\nn_top_words = 20\nbatch_size = 128\ninit = \"nndsvda\"\n\n\ndef plot_top_words(model, feature_names, n_top_words, title):\n    fig, axes = (2, 5, figsize=(30, 15), sharex=True)\n    axes = axes.flatten()\n    for topic_idx, topic in enumerate(model.components_):\n        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n        top_features = [feature_names[i] for i in top_features_ind]\n        weights = topic[top_features_ind]\n\n        ax = axes[topic_idx]\n        ax.barh(top_features, weights, height=0.7)\n        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n        ax.invert_yaxis()\n        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n        for i in \"top right left\".split():\n            ax.spines[i].set_visible(False)\n        fig.suptitle(title, fontsize=40)\n\n    (top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n    ()\n\n\n# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n# to filter out useless terms early on: the posts are stripped of headers,\n# footers and quoted replies, and common English words, words occurring in\n# only one document or in at least 95% of the documents are removed.\n\nprint(\"Loading dataset...\")\nt0 = ()\ndata, _ = (\n    shuffle=True,\n    random_state=1,\n    remove=(\"headers\", \"footers\", \"quotes\"),\n    return_X_y=True,\n)\ndata_samples = data[:n_samples]\nprint(\"done in %0.3fs.\" % (() - t0))\n\n# Use tf-idf features for NMF.\nprint(\"Extracting tf-idf features for NMF...\")\ntfidf_vectorizer = (\n    max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\"\n)\nt0 = ()\ntfidf = tfidf_vectorizer.fit_transform(data_samples)\nprint(\"done in %0.3fs.\" % (() - t0))\n\n# Use tf (raw term count) features for LDA.\nprint(\"Extracting tf features for LDA...\")\ntf_vectorizer = (\n    max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\"\n)\nt0 = ()\ntf = tf_vectorizer.fit_transform(data_samples)\nprint(\"done in %0.3fs.\" % (() - t0))\nprint()\n\n# Fit the NMF model\nprint(\n    \"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n    \"n_samples=%d and n_features=%d...\" % (n_samples, n_features)\n)\nt0 = ()\nnmf = (\n    n_components=n_components,\n    random_state=1,\n    init=init,\n    beta_loss=\"frobenius\",\n    alpha_W=0.00005,\n    alpha_H=0.00005,\n    l1_ratio=1,\n).fit(tfidf)\nprint(\"done in %0.3fs.\" % (() - t0))\n\n\ntfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\nplot_top_words(\n    nmf, tfidf_feature_names, n_top_words, \"Topics in NMF model (Frobenius norm)\"\n)\n\n# Fit the NMF model\nprint(\n    \"\\n\" * 2,\n    \"Fitting the NMF model (generalized Kullback-Leibler \"\n    \"divergence) with tf-idf features, n_samples=%d and n_features=%d...\"\n    % (n_samples, n_features),\n)\nt0 = ()\nnmf = (\n    n_components=n_components,\n    random_state=1,\n    init=init,\n    beta_loss=\"kullback-leibler\",\n    solver=\"mu\",\n    max_iter=1000,\n    alpha_W=0.00005,\n    alpha_H=0.00005,\n    l1_ratio=0.5,\n).fit(tfidf)\nprint(\"done in %0.3fs.\" % (() - t0))\n\ntfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\nplot_top_words(\n    nmf,\n    tfidf_feature_names,\n    n_top_words,\n    \"Topics in NMF model (generalized Kullback-Leibler divergence)\",\n)\n\n# Fit the MiniBatchNMF model\nprint(\n    \"\\n\" * 2,\n    \"Fitting the MiniBatchNMF model (Frobenius norm) with tf-idf \"\n    \"features, n_samples=%d and n_features=%d, batch_size=%d...\"\n    % (n_samples, n_features, batch_size),\n)\nt0 = ()\nmbnmf = (\n    n_components=n_components,\n    random_state=1,\n    batch_size=batch_size,\n    init=init,\n    beta_loss=\"frobenius\",\n    alpha_W=0.00005,\n    alpha_H=0.00005,\n    l1_ratio=0.5,\n).fit(tfidf)\nprint(\"done in %0.3fs.\" % (() - t0))\n\n\ntfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\nplot_top_words(\n    mbnmf,\n    tfidf_feature_names,\n    n_top_words,\n    \"Topics in MiniBatchNMF model (Frobenius norm)\",\n)\n\n# Fit the MiniBatchNMF model\nprint(\n    \"\\n\" * 2,\n    \"Fitting the MiniBatchNMF model (generalized Kullback-Leibler \"\n    \"divergence) with tf-idf features, n_samples=%d and n_features=%d, \"\n    \"batch_size=%d...\" % (n_samples, n_features, batch_size),\n)\nt0 = ()\nmbnmf = (\n    n_components=n_components,\n    random_state=1,\n    batch_size=batch_size,\n    init=init,\n    beta_loss=\"kullback-leibler\",\n    alpha_W=0.00005,\n    alpha_H=0.00005,\n    l1_ratio=0.5,\n).fit(tfidf)\nprint(\"done in %0.3fs.\" % (() - t0))\n\ntfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\nplot_top_words(\n    mbnmf,\n    tfidf_feature_names,\n    n_top_words,\n    \"Topics in MiniBatchNMF model (generalized Kullback-Leibler divergence)\",\n)\n\nprint(\n    \"\\n\" * 2,\n    \"Fitting LDA models with tf features, n_samples=%d and n_features=%d...\"\n    % (n_samples, n_features),\n)\nlda = (\n    n_components=n_components,\n    max_iter=5,\n    learning_method=\"online\",\n    learning_offset=50.0,\n    random_state=0,\n)\nt0 = ()\nlda.fit(tf)\nprint(\"done in %0.3fs.\" % (() - t0))\n\ntf_feature_names = tf_vectorizer.get_feature_names_out()\nplot_top_words(lda, tf_feature_names, n_top_words, \"Topics in LDA model\")",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  12.240 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Examples based on real world datasets->Visualizing the stock market structure": [
        [
            "This example employs several unsupervised learning techniques to extract\nthe stock market structure from variations in historical quotes.",
            "markdown"
        ],
        [
            "The quantity that we use is the daily variation in quote price: quotes\nthat are linked tend to fluctuate in relation to each other during a day.",
            "markdown"
        ],
        [
            "# Author: Gael Varoquaux gael.varoquaux@normalesup.org\n# License: BSD 3 clause",
            "code"
        ]
    ],
    "Examples->Examples based on real world datasets->Visualizing the stock market structure->Retrieve the data from Internet": [
        [
            "The data is from 2003 - 2008. This is reasonably calm: (not too long ago so\nthat we get high-tech firms, and before the 2008 crash). This kind of\nhistorical data can be obtained from APIs like the\n and\n.",
            "markdown"
        ],
        [
            "import sys\nimport numpy as np\nimport pandas as pd\n\nsymbol_dict = {\n    \"TOT\": \"Total\",\n    \"XOM\": \"Exxon\",\n    \"CVX\": \"Chevron\",\n    \"COP\": \"ConocoPhillips\",\n    \"VLO\": \"Valero Energy\",\n    \"MSFT\": \"Microsoft\",\n    \"IBM\": \"IBM\",\n    \"TWX\": \"Time Warner\",\n    \"CMCSA\": \"Comcast\",\n    \"CVC\": \"Cablevision\",\n    \"YHOO\": \"Yahoo\",\n    \"DELL\": \"Dell\",\n    \"HPQ\": \"HP\",\n    \"AMZN\": \"Amazon\",\n    \"TM\": \"Toyota\",\n    \"CAJ\": \"Canon\",\n    \"SNE\": \"Sony\",\n    \"F\": \"Ford\",\n    \"HMC\": \"Honda\",\n    \"NAV\": \"Navistar\",\n    \"NOC\": \"Northrop Grumman\",\n    \"BA\": \"Boeing\",\n    \"KO\": \"Coca Cola\",\n    \"MMM\": \"3M\",\n    \"MCD\": \"McDonald's\",\n    \"PEP\": \"Pepsi\",\n    \"K\": \"Kellogg\",\n    \"UN\": \"Unilever\",\n    \"MAR\": \"Marriott\",\n    \"PG\": \"Procter Gamble\",\n    \"CL\": \"Colgate-Palmolive\",\n    \"GE\": \"General Electrics\",\n    \"WFC\": \"Wells Fargo\",\n    \"JPM\": \"JPMorgan Chase\",\n    \"AIG\": \"AIG\",\n    \"AXP\": \"American express\",\n    \"BAC\": \"Bank of America\",\n    \"GS\": \"Goldman Sachs\",\n    \"AAPL\": \"Apple\",\n    \"SAP\": \"SAP\",\n    \"CSCO\": \"Cisco\",\n    \"TXN\": \"Texas Instruments\",\n    \"XRX\": \"Xerox\",\n    \"WMT\": \"Wal-Mart\",\n    \"HD\": \"Home Depot\",\n    \"GSK\": \"GlaxoSmithKline\",\n    \"PFE\": \"Pfizer\",\n    \"SNY\": \"Sanofi-Aventis\",\n    \"NVS\": \"Novartis\",\n    \"KMB\": \"Kimberly-Clark\",\n    \"R\": \"Ryder\",\n    \"GD\": \"General Dynamics\",\n    \"RTN\": \"Raytheon\",\n    \"CVS\": \"CVS\",\n    \"CAT\": \"Caterpillar\",\n    \"DD\": \"DuPont de Nemours\",\n}\n\n\nsymbols, names = (sorted(symbol_dict.items())).T\n\nquotes = []\n\nfor symbol in symbols:\n    print(\"Fetching quote history for %r\" % symbol, file=)\n    url = (\n        \"https://raw.githubusercontent.com/scikit-learn/examples-data/\"\n        \"master/financial-data/{}.csv\"\n    )\n    quotes.append((url.format(symbol)))\n\nclose_prices = ([q[\"close\"] for q in quotes])\nopen_prices = ([q[\"open\"] for q in quotes])\n\n# The daily variations of the quotes are what carry the most information\nvariation = close_prices - open_prices",
            "code"
        ],
        [
            "Fetching quote history for 'AAPL'\nFetching quote history for 'AIG'\nFetching quote history for 'AMZN'\nFetching quote history for 'AXP'\nFetching quote history for 'BA'\nFetching quote history for 'BAC'\nFetching quote history for 'CAJ'\nFetching quote history for 'CAT'\nFetching quote history for 'CL'\nFetching quote history for 'CMCSA'\nFetching quote history for 'COP'\nFetching quote history for 'CSCO'\nFetching quote history for 'CVC'\nFetching quote history for 'CVS'\nFetching quote history for 'CVX'\nFetching quote history for 'DD'\nFetching quote history for 'DELL'\nFetching quote history for 'F'\nFetching quote history for 'GD'\nFetching quote history for 'GE'\nFetching quote history for 'GS'\nFetching quote history for 'GSK'\nFetching quote history for 'HD'\nFetching quote history for 'HMC'\nFetching quote history for 'HPQ'\nFetching quote history for 'IBM'\nFetching quote history for 'JPM'\nFetching quote history for 'K'\nFetching quote history for 'KMB'\nFetching quote history for 'KO'\nFetching quote history for 'MAR'\nFetching quote history for 'MCD'\nFetching quote history for 'MMM'\nFetching quote history for 'MSFT'\nFetching quote history for 'NAV'\nFetching quote history for 'NOC'\nFetching quote history for 'NVS'\nFetching quote history for 'PEP'\nFetching quote history for 'PFE'\nFetching quote history for 'PG'\nFetching quote history for 'R'\nFetching quote history for 'RTN'\nFetching quote history for 'SAP'\nFetching quote history for 'SNE'\nFetching quote history for 'SNY'\nFetching quote history for 'TM'\nFetching quote history for 'TOT'\nFetching quote history for 'TWX'\nFetching quote history for 'TXN'\nFetching quote history for 'UN'\nFetching quote history for 'VLO'\nFetching quote history for 'WFC'\nFetching quote history for 'WMT'\nFetching quote history for 'XOM'\nFetching quote history for 'XRX'\nFetching quote history for 'YHOO'",
            "code"
        ]
    ],
    "Examples->Examples based on real world datasets->Visualizing the stock market structure->Learning a graph structure": [
        [
            "We use sparse inverse covariance estimation to find which quotes are\ncorrelated conditionally on the others. Specifically, sparse inverse\ncovariance gives us a graph, that is a list of connections. For each\nsymbol, the symbols that it is connected to are those useful to explain\nits fluctuations.",
            "markdown"
        ],
        [
            "from sklearn import covariance\n\nalphas = (-1.5, 1, num=10)\nedge_model = (alphas=alphas)\n\n# standardize the time series: using correlations rather than covariance\n# former is more efficient for structure recovery\nX = variation.copy().T\nX /= X.std(axis=0)\nedge_model.fit(X)",
            "code"
        ],
        [
            "GraphicalLassoCV(alphas=array([ 0.03162278,  0.05994843,  0.11364637,  0.21544347,  0.40842387,\n        0.77426368,  1.46779927,  2.7825594 ,  5.27499706, 10.        ]))<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input checked=\"\" class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-109\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-109\">GraphicalLassoCV</label>",
            "code"
        ],
        [
            "GraphicalLassoCV(alphas=array([ 0.03162278,  0.05994843,  0.11364637,  0.21544347,  0.40842387,\n        0.77426368,  1.46779927,  2.7825594 ,  5.27499706, 10.        ]))\n\n<br/>\n<br/>",
            "code"
        ]
    ],
    "Examples->Examples based on real world datasets->Visualizing the stock market structure->Clustering using affinity propagation": [
        [
            "We use clustering to group together quotes that behave similarly. Here,\namongst the  available\nin the scikit-learn, we use  as it does\nnot enforce equal-size clusters, and it can choose automatically the\nnumber of clusters from the data.",
            "markdown"
        ],
        [
            "Note that this gives us a different indication than the graph, as the\ngraph reflects conditional relations between variables, while the\nclustering reflects marginal properties: variables clustered together can\nbe considered as having a similar impact at the level of the full stock\nmarket.",
            "markdown"
        ],
        [
            "from sklearn import cluster\n\n_, labels = (edge_model.covariance_, random_state=0)\nn_labels = labels.max()\n\nfor i in range(n_labels + 1):\n    print(f\"Cluster {i + 1}: {', '.join(names[labels == i])}\")",
            "code"
        ],
        [
            "Cluster 1: Apple, Amazon, Yahoo\nCluster 2: Comcast, Cablevision, Time Warner\nCluster 3: ConocoPhillips, Chevron, Total, Valero Energy, Exxon\nCluster 4: Cisco, Dell, HP, IBM, Microsoft, SAP, Texas Instruments\nCluster 5: Boeing, General Dynamics, Northrop Grumman, Raytheon\nCluster 6: AIG, American express, Bank of America, Caterpillar, CVS, DuPont de Nemours, Ford, General Electrics, Goldman Sachs, Home Depot, JPMorgan Chase, Marriott, McDonald's, 3M, Ryder, Wells Fargo, Wal-Mart\nCluster 7: GlaxoSmithKline, Novartis, Pfizer, Sanofi-Aventis, Unilever\nCluster 8: Kellogg, Coca Cola, Pepsi\nCluster 9: Colgate-Palmolive, Kimberly-Clark, Procter Gamble\nCluster 10: Canon, Honda, Navistar, Sony, Toyota, Xerox",
            "code"
        ]
    ],
    "Examples->Examples based on real world datasets->Visualizing the stock market structure->Embedding in 2D space": [
        [
            "For visualization purposes, we need to lay out the different symbols on a\n2D canvas. For this we use  techniques to retrieve 2D\nembedding.\nWe use a dense eigen_solver to achieve reproducibility (arpack is initiated\nwith the random vectors that we don\u2019t control). In addition, we use a large\nnumber of neighbors to capture the large-scale structure.",
            "markdown"
        ],
        [
            "# Finding a low-dimension embedding for visualization: find the best position of\n# the nodes (the stocks) on a 2D plane\n\nfrom sklearn import manifold\n\nnode_position_model = (\n    n_components=2, eigen_solver=\"dense\", n_neighbors=6\n)\n\nembedding = node_position_model.fit_transform(X.T).T",
            "code"
        ]
    ],
    "Examples->Examples based on real world datasets->Visualizing the stock market structure->Visualization": [
        [
            "The output of the 3 models are combined in a 2D graph where nodes\nrepresents the stocks and edges the:",
            "markdown"
        ],
        [
            "cluster labels are used to define the color of the nodes",
            "markdown"
        ],
        [
            "the sparse covariance model is used to display the strength of the edges",
            "markdown"
        ],
        [
            "the 2D embedding is used to position the nodes in the plan",
            "markdown"
        ],
        [
            "This example has a fair amount of visualization-related code, as\nvisualization is crucial here to display the graph. One of the challenge\nis to position the labels minimizing overlap. For this we use an\nheuristic based on the direction of the nearest neighbor along each\naxis.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\nfrom matplotlib.collections import \n\n(1, facecolor=\"w\", figsize=(10, 8))\n()\nax = ([0.0, 0.0, 1.0, 1.0])\n(\"off\")\n\n# Plot the graph of partial correlations\npartial_correlations = edge_model.precision_.copy()\nd = 1 / ((partial_correlations))\npartial_correlations *= d\npartial_correlations *= d[:, ]\nnon_zero = np.abs((partial_correlations, k=1))  0.02\n\n# Plot the nodes using the coordinates of our embedding\n(\n    embedding[0], embedding[1], s=100 * d**2, c=labels, cmap=plt.cm.nipy_spectral\n)\n\n# Plot the edges\nstart_idx, end_idx = (non_zero)\n# a sequence of (*line0*, *line1*, *line2*), where::\n#            linen = (x0, y0), (x1, y1), ... (xm, ym)\nsegments = [\n    [embedding[:, start], embedding[:, stop]] for start, stop in zip(start_idx, end_idx)\n]\nvalues = np.abs(partial_correlations[non_zero])\nlc = (\n    segments, zorder=0, cmap=plt.cm.hot_r, norm=plt.Normalize(0, 0.7 * values.max())\n)\nlc.set_array(values)\nlc.set_linewidths(15 * values)\nax.add_collection(lc)\n\n# Add a label to each node. The challenge here is that we want to\n# position the labels to avoid overlap with other labels\nfor index, (name, label, (x, y)) in enumerate(zip(names, labels, embedding.T)):\n\n    dx = x - embedding[0]\n    dx[index] = 1\n    dy = y - embedding[1]\n    dy[index] = 1\n    this_dx = dx[(np.abs(dy))]\n    this_dy = dy[(np.abs(dx))]\n    if this_dx  0:\n        horizontalalignment = \"left\"\n        x = x + 0.002\n    else:\n        horizontalalignment = \"right\"\n        x = x - 0.002\n    if this_dy  0:\n        verticalalignment = \"bottom\"\n        y = y + 0.002\n    else:\n        verticalalignment = \"top\"\n        y = y - 0.002\n    (\n        x,\n        y,\n        name,\n        size=10,\n        horizontalalignment=horizontalalignment,\n        verticalalignment=verticalalignment,\n        bbox=dict(\n            facecolor=\"w\",\n            edgecolor=plt.cm.nipy_spectral(label / float(n_labels)),\n            alpha=0.6,\n        ),\n    )\n\n(\n    embedding[0].min() - 0.15 * embedding[0].ptp(),\n    embedding[0].max() + 0.10 * embedding[0].ptp(),\n)\n(\n    embedding[1].min() - 0.03 * embedding[1].ptp(),\n    embedding[1].max() + 0.03 * embedding[1].ptp(),\n)\n\n()\n\n\n<img alt=\"plot stock market\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_stock_market_001.png\" srcset=\"../../_images/sphx_glr_plot_stock_market_001.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  4.049 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Examples based on real world datasets->Wikipedia principal eigenvector": [
        [
            "A classical way to assert the relative importance of vertices in a\ngraph is to compute the principal eigenvector of the adjacency matrix\nso as to assign to each vertex the values of the components of the first\neigenvector as a centrality score:",
            "markdown"
        ],
        [
            "</blockquote>",
            "markdown"
        ],
        [
            "On the graph of webpages and links those values are called the PageRank\nscores by Google.",
            "markdown"
        ],
        [
            "The goal of this example is to analyze the graph of links inside\nwikipedia articles to rank articles by relative importance according to\nthis eigenvector centrality.",
            "markdown"
        ],
        [
            "The traditional way to compute the principal eigenvector is to use the\npower iteration method:",
            "markdown"
        ],
        [
            "</blockquote>",
            "markdown"
        ],
        [
            "Here the computation is achieved thanks to Martinsson\u2019s Randomized SVD\nalgorithm implemented in scikit-learn.",
            "markdown"
        ],
        [
            "The graph data is fetched from the DBpedia dumps. DBpedia is an extraction\nof the latent structured data of the Wikipedia content.",
            "markdown"
        ],
        [
            "# Author: Olivier Grisel &lt;olivier.grisel@ensta.org\n# License: BSD 3 clause\n\nfrom bz2 import \nimport os\nfrom datetime import datetime\nfrom pprint import \nfrom time import \n\nimport numpy as np\n\nfrom scipy import sparse\n\nfrom sklearn.decomposition import randomized_svd\nfrom urllib.request import",
            "code"
        ]
    ],
    "Examples->Examples based on real world datasets->Wikipedia principal eigenvector->Download data, if not already on disk": [
        [
            "redirects_url = \"http://downloads.dbpedia.org/3.5.1/en/redirects_en.nt.bz2\"\nredirects_filename = redirects_url.rsplit(\"/\", 1)[1]\n\npage_links_url = \"http://downloads.dbpedia.org/3.5.1/en/page_links_en.nt.bz2\"\npage_links_filename = page_links_url.rsplit(\"/\", 1)[1]\n\nresources = [\n    (redirects_url, redirects_filename),\n    (page_links_url, page_links_filename),\n]\n\nfor url, filename in resources:\n    if not (filename):\n        print(\"Downloading data from '%s', please wait...\" % url)\n        opener = (url)\n        with open(filename, \"wb\") as f:\n            f.write(opener.read())\n        print()",
            "code"
        ]
    ],
    "Examples->Examples based on real world datasets->Wikipedia principal eigenvector->Loading the redirect files": [
        [
            "def index(redirects, index_map, k):\n    \"\"\"Find the index of an article name after redirect resolution\"\"\"\n    k = redirects.get(k, k)\n    return index_map.setdefault(k, len(index_map))\n\n\nDBPEDIA_RESOURCE_PREFIX_LEN = len(\"http://dbpedia.org/resource/\")\nSHORTNAME_SLICE = slice(DBPEDIA_RESOURCE_PREFIX_LEN + 1, -1)\n\n\ndef short_name(nt_uri):\n    \"\"\"Remove the &lt; and  URI markers and the common URI prefix\"\"\"\n    return nt_uri[SHORTNAME_SLICE]\n\n\ndef get_redirects(redirects_filename):\n    \"\"\"Parse the redirections and build a transitively closed map out of it\"\"\"\n    redirects = {}\n    print(\"Parsing the NT redirect file\")\n    for l, line in enumerate((redirects_filename)):\n        split = line.split()\n        if len(split) != 4:\n            print(\"ignoring malformed line: \" + line)\n            continue\n        redirects[short_name(split[0])] = short_name(split[2])\n        if l % 1000000 == 0:\n            print(\"[%s] line: %08d\" % (datetime.now().isoformat(), l))\n\n    # compute the transitive closure\n    print(\"Computing the transitive closure of the redirect relation\")\n    for l, source in enumerate(redirects.keys()):\n        transitive_target = None\n        target = redirects[source]\n        seen = {source}\n        while True:\n            transitive_target = target\n            target = redirects.get(target)\n            if target is None or target in seen:\n                break\n            seen.add(target)\n        redirects[source] = transitive_target\n        if l % 1000000 == 0:\n            print(\"[%s] line: %08d\" % (datetime.now().isoformat(), l))\n\n    return redirects",
            "code"
        ]
    ],
    "Examples->Examples based on real world datasets->Wikipedia principal eigenvector->Computing the Adjacency matrix": [
        [
            "def get_adjacency_matrix(redirects_filename, page_links_filename, limit=None):\n    \"\"\"Extract the adjacency graph as a scipy sparse matrix\n\n    Redirects are resolved first.\n\n    Returns X, the scipy sparse adjacency matrix, redirects as python\n    dict from article names to article names and index_map a python dict\n    from article names to python int (article indexes).\n    \"\"\"\n\n    print(\"Computing the redirect map\")\n    redirects = get_redirects(redirects_filename)\n\n    print(\"Computing the integer index map\")\n    index_map = dict()\n    links = list()\n    for l, line in enumerate((page_links_filename)):\n        split = line.split()\n        if len(split) != 4:\n            print(\"ignoring malformed line: \" + line)\n            continue\n        i = index(redirects, index_map, short_name(split[0]))\n        j = index(redirects, index_map, short_name(split[2]))\n        links.append((i, j))\n        if l % 1000000 == 0:\n            print(\"[%s] line: %08d\" % (datetime.now().isoformat(), l))\n\n        if limit is not None and l = limit - 1:\n            break\n\n    print(\"Computing the adjacency matrix\")\n    X = ((len(index_map), len(index_map)), dtype=)\n    for i, j in links:\n        X[i, j] = 1.0\n    del links\n    print(\"Converting to CSR representation\")\n    X = X.tocsr()\n    print(\"CSR conversion done\")\n    return X, redirects, index_map\n\n\n# stop after 5M links to make it possible to work in RAM\nX, redirects, index_map = get_adjacency_matrix(\n    redirects_filename, page_links_filename, limit=5000000\n)\nnames = {i: name for name, i in index_map.items()}",
            "code"
        ]
    ],
    "Examples->Examples based on real world datasets->Wikipedia principal eigenvector->Computing Principal Singular Vector using Randomized SVD": [
        [
            "print(\"Computing the principal singular vectors using randomized_svd\")\nt0 = ()\nU, s, V = randomized_svd(X, 5, n_iter=3)\nprint(\"done in %0.3fs\" % (() - t0))\n\n# print the names of the wikipedia related strongest components of the\n# principal singular vector which should be similar to the highest eigenvector\nprint(\"Top wikipedia pages according to principal singular vectors\")\n([names[i] for i in np.abs(U.T[0]).argsort()[-10:]])\n([names[i] for i in np.abs(V[0]).argsort()[-10:]])",
            "code"
        ]
    ],
    "Examples->Examples based on real world datasets->Wikipedia principal eigenvector->Computing Centrality scores": [
        [
            "def centrality_scores(X, alpha=0.85, max_iter=100, tol=1e-10):\n    \"\"\"Power iteration computation of the principal eigenvector\n\n    This method is also known as Google PageRank and the implementation\n    is based on the one from the NetworkX project (BSD licensed too)\n    with copyrights by:\n\n      Aric Hagberg &lt;hagberg@lanl.gov\n      Dan Schult &lt;dschult@colgate.edu\n      Pieter Swart &lt;swart@lanl.gov\n    \"\"\"\n    n = X.shape[0]\n    X = X.copy()\n    incoming_counts = (X.sum(axis=1)).ravel()\n\n    print(\"Normalizing the graph\")\n    for i in incoming_counts.nonzero()[0]:\n        X.data[X.indptr[i] : X.indptr[i + 1]] *= 1.0 / incoming_counts[i]\n    dangle = (((X.sum(axis=1), 0), 1.0 / n, 0)).ravel()\n\n    scores = (n, 1.0 / n, dtype=)  # initial guess\n    for i in range(max_iter):\n        print(\"power iteration #%d\" % i)\n        prev_scores = scores\n        scores = (\n            alpha * (scores * X + (dangle, prev_scores))\n            + (1 - alpha) * prev_scores.sum() / n\n        )\n        # check convergence: normalized l_inf norm\n        scores_max = np.abs(scores).max()\n        if scores_max == 0.0:\n            scores_max = 1.0\n        err = np.abs(scores - prev_scores).max() / scores_max\n        print(\"error: %0.6f\" % err)\n        if err &lt; n * tol:\n            return scores\n\n    return scores\n\n\nprint(\"Computing principal eigenvector score using a power iteration method\")\nt0 = ()\nscores = centrality_scores(X, max_iter=100)\nprint(\"done in %0.3fs\" % (() - t0))\n([names[i] for i in np.abs(scores).argsort()[-10:]])",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Feature Selection->Comparison of F-test and mutual information": [
        [
            "This example illustrates the differences between univariate F-test statistics\nand mutual information.",
            "markdown"
        ],
        [
            "We consider 3 features x_1, x_2, x_3 distributed uniformly over [0, 1], the\ntarget depends on them as follows:",
            "markdown"
        ],
        [
            "y = x_1 + sin(6 * pi * x_2) + 0.1 * N(0, 1), that is the third features is\ncompletely irrelevant.",
            "markdown"
        ],
        [
            "The code below plots the dependency of y against individual x_i and normalized\nvalues of univariate F-tests statistics and mutual information.",
            "markdown"
        ],
        [
            "As F-test captures only linear dependency, it rates x_1 as the most\ndiscriminative feature. On the other hand, mutual information can capture any\nkind of dependency between variables and it rates x_2 as the most\ndiscriminative feature, which probably agrees better with our intuitive\nperception for this example. Both methods correctly marks x_3 as irrelevant.\n<img alt=\"F-test=1.00, MI=0.36, F-test=0.28, MI=1.00, F-test=0.00, MI=0.00\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_f_test_vs_mi_001.png\" srcset=\"../../_images/sphx_glr_plot_f_test_vs_mi_001.png\"/>",
            "markdown"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_selection import , \n\n(0)\nX = (1000, 3)\ny = X[:, 0] + (6 *  * X[:, 1]) + 0.1 * (1000)\n\nf_test, _ = (X, y)\nf_test /= np.max(f_test)\n\nmi = (X, y)\nmi /= np.max(mi)\n\n(figsize=(15, 5))\nfor i in range(3):\n    (1, 3, i + 1)\n    (X[:, i], y, edgecolor=\"black\", s=20)\n    (\"$x_{}$\".format(i + 1), fontsize=14)\n    if i == 0:\n        (\"$y$\", fontsize=14)\n    (\"F-test={:.2f}, MI={:.2f}\".format(f_test[i], mi[i]), fontsize=16)\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.242 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Feature Selection->Model-based and sequential feature selection": [
        [
            "This example illustrates and compares two approaches for feature selection:\n which is based on feature\nimportance, and\nSequentialFeatureSelection which relies\non a greedy approach.",
            "markdown"
        ],
        [
            "We use the Diabetes dataset, which consists of 10 features collected from 442\ndiabetes patients.",
            "markdown"
        ],
        [
            "Authors: ,\n, Nicolas Hug.",
            "markdown"
        ],
        [
            "License: BSD 3 clause",
            "markdown"
        ]
    ],
    "Examples->Feature Selection->Model-based and sequential feature selection->Loading the data": [
        [
            "We first load the diabetes dataset which is available from within\nscikit-learn, and print its description:",
            "markdown"
        ],
        [
            "from sklearn.datasets import \n\ndiabetes = ()\nX, y = diabetes.data, diabetes.target\nprint(diabetes.DESCR)",
            "code"
        ],
        [
            ".. _diabetes_dataset:\n\nDiabetes dataset\n----------------\n\nTen baseline variables, age, sex, body mass index, average blood\npressure, and six blood serum measurements were obtained for each of n =\n442 diabetes patients, as well as the response of interest, a\nquantitative measure of disease progression one year after baseline.\n\n**Data Set Characteristics:**\n\n  :Number of Instances: 442\n\n  :Number of Attributes: First 10 columns are numeric predictive values\n\n  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n\n  :Attribute Information:\n      - age     age in years\n      - sex\n      - bmi     body mass index\n      - bp      average blood pressure\n      - s1      tc, total serum cholesterol\n      - s2      ldl, low-density lipoproteins\n      - s3      hdl, high-density lipoproteins\n      - s4      tch, total cholesterol / HDL\n      - s5      ltg, possibly log of serum triglycerides level\n      - s6      glu, blood sugar level\n\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of `n_samples` (i.e. the sum of squares of each column totals 1).\n\nSource URL:\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n\nFor more information see:\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)",
            "code"
        ]
    ],
    "Examples->Feature Selection->Model-based and sequential feature selection->Feature importance from coefficients": [
        [
            "To get an idea of the importance of the features, we are going to use the\n estimator. The features with the\nhighest absolute coef_ value are considered the most important.\nWe can observe the coefficients directly without needing to scale them (or\nscale the data) because from the description above, we know that the features\nwere already standardized.\nFor a more complete example on the interpretations of the coefficients of\nlinear models, you may refer to\n.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.linear_model import \n\nridge = (alphas=(-6, 6, num=5)).fit(X, y)\nimportance = np.abs(ridge.coef_)\nfeature_names = (diabetes.feature_names)\n(height=importance, x=feature_names)\n(\"Feature importances via coefficients\")\n()\n\n\n<img alt=\"Feature importances via coefficients\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_select_from_model_diabetes_001.png\" srcset=\"../../_images/sphx_glr_plot_select_from_model_diabetes_001.png\"/>",
            "code"
        ]
    ],
    "Examples->Feature Selection->Model-based and sequential feature selection->Selecting features based on importance": [
        [
            "Now we want to select the two features which are the most important according\nto the coefficients. The \nis meant just for that. \naccepts a threshold parameter and will select the features whose importance\n(defined by the coefficients) are above this threshold.",
            "markdown"
        ],
        [
            "Since we want to select only 2 features, we will set this threshold slightly\nabove the coefficient of third most important feature.",
            "markdown"
        ],
        [
            "from sklearn.feature_selection import \nfrom time import \n\nthreshold = (importance)[-3] + 0.01\n\ntic = ()\nsfm = (ridge, threshold=threshold).fit(X, y)\ntoc = ()\nprint(f\"Features selected by SelectFromModel: {feature_names[sfm.get_support()]}\")\nprint(f\"Done in {toc - tic:.3f}s\")",
            "code"
        ],
        [
            "Features selected by SelectFromModel: ['s1' 's5']\nDone in 0.002s",
            "code"
        ]
    ],
    "Examples->Feature Selection->Model-based and sequential feature selection->Selecting features with Sequential Feature Selection": [
        [
            "Another way of selecting features is to use\n\n(SFS). SFS is a greedy procedure where, at each iteration, we choose the best\nnew feature to add to our selected features based a cross-validation score.\nThat is, we start with 0 features and choose the best single feature with the\nhighest score. The procedure is repeated until we reach the desired number of\nselected features.",
            "markdown"
        ],
        [
            "We can also go in the reverse direction (backward SFS), i.e. start with all\nthe features and greedily choose features to remove one by one. We illustrate\nboth approaches here.",
            "markdown"
        ],
        [
            "from sklearn.feature_selection import \n\ntic_fwd = ()\nsfs_forward = (\n    ridge, n_features_to_select=2, direction=\"forward\"\n).fit(X, y)\ntoc_fwd = ()\n\ntic_bwd = ()\nsfs_backward = (\n    ridge, n_features_to_select=2, direction=\"backward\"\n).fit(X, y)\ntoc_bwd = ()\n\nprint(\n    \"Features selected by forward sequential selection: \"\n    f\"{feature_names[sfs_forward.get_support()]}\"\n)\nprint(f\"Done in {toc_fwd - tic_fwd:.3f}s\")\nprint(\n    \"Features selected by backward sequential selection: \"\n    f\"{feature_names[sfs_backward.get_support()]}\"\n)\nprint(f\"Done in {toc_bwd - tic_bwd:.3f}s\")",
            "code"
        ],
        [
            "Features selected by forward sequential selection: ['bmi' 's5']\nDone in 0.144s\nFeatures selected by backward sequential selection: ['bmi' 's5']\nDone in 0.453s",
            "code"
        ]
    ],
    "Examples->Feature Selection->Model-based and sequential feature selection->Discussion": [
        [
            "Interestingly, forward and backward selection have selected the same set of\nfeatures. In general, this isn\u2019t the case and the two methods would lead to\ndifferent results.",
            "markdown"
        ],
        [
            "We also note that the features selected by SFS differ from those selected by\nfeature importance: SFS selects bmi instead of s1. This does sound\nreasonable though, since bmi corresponds to the third most important\nfeature according to the coefficients. It is quite remarkable considering\nthat SFS makes no use of the coefficients at all.",
            "markdown"
        ],
        [
            "To finish with, we should note that\n is significantly faster\nthan SFS. Indeed,  only\nneeds to fit a model once, while SFS needs to cross-validate many different\nmodels for each of the iterations. SFS however works with any model, while\n requires the underlying\nestimator to expose a coef_ attribute or a feature_importances_\nattribute. The forward SFS is faster than the backward SFS because it only\nneeds to perform n_features_to_select = 2 iterations, while the backward\nSFS needs to perform n_features - n_features_to_select = 8 iterations.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.685 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Feature Selection->Pipeline ANOVA SVM": [
        [
            "This example shows how a feature selection can be easily integrated within\na machine learning pipeline.",
            "markdown"
        ],
        [
            "We also show that you can easily inspect part of the pipeline.",
            "markdown"
        ],
        [
            "We will start by generating a binary classification dataset. Subsequently, we\nwill divide the dataset into two subsets.",
            "markdown"
        ],
        [
            "from sklearn.datasets import \nfrom sklearn.model_selection import \n\nX, y = (\n    n_features=20,\n    n_informative=3,\n    n_redundant=0,\n    n_classes=2,\n    n_clusters_per_class=2,\n    random_state=42,\n)\nX_train, X_test, y_train, y_test = (X, y, random_state=42)",
            "code"
        ],
        [
            "A common mistake done with feature selection is to search a subset of\ndiscriminative features on the full dataset, instead of only using the\ntraining set. The usage of scikit-learn \nprevents to make such mistake.",
            "markdown"
        ],
        [
            "Here, we will demonstrate how to build a pipeline where the first step will\nbe the feature selection.",
            "markdown"
        ],
        [
            "When calling fit on the training data, a subset of feature will be selected\nand the index of these selected features will be stored. The feature selector\nwill subsequently reduce the number of features, and pass this subset to the\nclassifier which will be trained.",
            "markdown"
        ],
        [
            "from sklearn.feature_selection import , \nfrom sklearn.pipeline import \nfrom sklearn.svm import \n\nanova_filter = (, k=3)\nclf = ()\nanova_svm = (anova_filter, clf)\nanova_svm.fit(X_train, y_train)",
            "code"
        ],
        [
            "Pipeline(steps=[('selectkbest', SelectKBest(k=3)), ('linearsvc', LinearSVC())])<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-110\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-110\">Pipeline</label>",
            "code"
        ],
        [
            "Pipeline(steps=[('selectkbest', SelectKBest(k=3)), ('linearsvc', LinearSVC())])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-111\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-111\">SelectKBest</label>",
            "code"
        ],
        [
            "SelectKBest(k=3)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-112\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-112\">LinearSVC</label>",
            "code"
        ],
        [
            "LinearSVC()\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "Once the training is complete, we can predict on new unseen samples. In this\ncase, the feature selector will only select the most discriminative features\nbased on the information stored during training. Then, the data will be\npassed to the classifier which will make the prediction.",
            "markdown"
        ],
        [
            "Here, we show the final metrics via a classification report.",
            "markdown"
        ],
        [
            "from sklearn.metrics import \n\ny_pred = anova_svm.predict(X_test)\nprint((y_test, y_pred))",
            "code"
        ],
        [
            "precision    recall  f1-score   support\n\n           0       0.92      0.80      0.86        15\n           1       0.75      0.90      0.82        10\n\n    accuracy                           0.84        25\n   macro avg       0.84      0.85      0.84        25\nweighted avg       0.85      0.84      0.84        25",
            "code"
        ],
        [
            "Be aware that you can inspect a step in the pipeline. For instance, we might\nbe interested about the parameters of the classifier. Since we selected\nthree features, we expect to have three coefficients.",
            "markdown"
        ],
        [
            "anova_svm[-1].coef_",
            "code"
        ],
        [
            "array([[0.75790919, 0.27158706, 0.26109741]])",
            "code"
        ],
        [
            "However, we do not know which features were selected from the original\ndataset. We could proceed by several manners. Here, we will invert the\ntransformation of these coefficients to get information about the original\nspace.",
            "markdown"
        ],
        [
            "anova_svm[:-1].inverse_transform(anova_svm[-1].coef_)",
            "code"
        ],
        [
            "array([[0.        , 0.        , 0.75790919, 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.27158706,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.26109741]])",
            "code"
        ],
        [
            "We can see that the features with non-zero coefficients are the selected\nfeatures by the first step.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.015 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Feature Selection->Recursive feature elimination": [
        [
            "A recursive feature elimination example showing the relevance of pixels in\na digit classification task.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "See also \n\n<img alt=\"Ranking of pixels with RFE\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_rfe_digits_001.png\" srcset=\"../../_images/sphx_glr_plot_rfe_digits_001.png\"/>",
            "markdown"
        ],
        [
            "from sklearn.svm import \nfrom sklearn.datasets import \nfrom sklearn.feature_selection import \nimport matplotlib.pyplot as plt\n\n# Load the digits dataset\ndigits = ()\nX = digits.images.reshape((len(digits.images), -1))\ny = digits.target\n\n# Create the RFE object and rank each pixel\nsvc = (kernel=\"linear\", C=1)\nrfe = (estimator=svc, n_features_to_select=1, step=1)\nrfe.fit(X, y)\nranking = rfe.ranking_.reshape(digits.images[0].shape)\n\n# Plot pixel ranking\n(ranking, cmap=plt.cm.Blues)\n()\n(\"Ranking of pixels with RFE\")\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  3.993 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Feature Selection->Recursive feature elimination with cross-validation": [
        [
            "A Recursive Feature Elimination (RFE) example with automatic tuning of the\nnumber of features selected with cross-validation.",
            "markdown"
        ]
    ],
    "Examples->Feature Selection->Recursive feature elimination with cross-validation->Data generation": [
        [
            "We build a classification task using 3 informative features. The introduction\nof 2 additional redundant (i.e. correlated) features has the effect that the\nselected features vary depending on the cross-validation fold. The remaining\nfeatures are non-informative as they are drawn at random.",
            "markdown"
        ],
        [
            "from sklearn.datasets import \n\nX, y = (\n    n_samples=500,\n    n_features=15,\n    n_informative=3,\n    n_redundant=2,\n    n_repeated=0,\n    n_classes=8,\n    n_clusters_per_class=1,\n    class_sep=0.8,\n    random_state=0,\n)",
            "code"
        ]
    ],
    "Examples->Feature Selection->Recursive feature elimination with cross-validation->Model training and selection": [
        [
            "We create the RFE object and compute the cross-validated scores. The scoring\nstrategy \u201caccuracy\u201d optimizes the proportion of correctly classified samples.",
            "markdown"
        ],
        [
            "from sklearn.feature_selection import \nfrom sklearn.model_selection import \nfrom sklearn.linear_model import \n\nmin_features_to_select = 1  # Minimum number of features to consider\nclf = ()\ncv = (5)\n\nrfecv = (\n    estimator=clf,\n    step=1,\n    cv=cv,\n    scoring=\"accuracy\",\n    min_features_to_select=min_features_to_select,\n    n_jobs=2,\n)\nrfecv.fit(X, y)\n\nprint(f\"Optimal number of features: {rfecv.n_features_}\")",
            "code"
        ],
        [
            "Optimal number of features: 3",
            "code"
        ],
        [
            "In the present case, the model with 3 features (which corresponds to the true\ngenerative model) is found to be the most optimal.",
            "markdown"
        ]
    ],
    "Examples->Feature Selection->Recursive feature elimination with cross-validation->Plot number of features VS. cross-validation scores": [
        [
            "import matplotlib.pyplot as plt\n\nn_scores = len(rfecv.cv_results_[\"mean_test_score\"])\n()\n(\"Number of features selected\")\n(\"Mean test accuracy\")\n(\n    range(min_features_to_select, n_scores + min_features_to_select),\n    rfecv.cv_results_[\"mean_test_score\"],\n    yerr=rfecv.cv_results_[\"std_test_score\"],\n)\n(\"Recursive Feature Elimination \\nwith correlated features\")\n()\n\n\n<img alt=\"Recursive Feature Elimination  with correlated features\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_rfe_with_cross_validation_001.png\" srcset=\"../../_images/sphx_glr_plot_rfe_with_cross_validation_001.png\"/>",
            "code"
        ],
        [
            "From the plot above one can further notice a plateau of equivalent scores\n(similar mean value and overlapping errorbars) for 3 to 5 selected features.\nThis is the result of introducing correlated features. Indeed, the optimal\nmodel selected by the RFE can lie within this range, depending on the\ncross-validation technique. The test accuracy decreases above 5 selected\nfeatures, this is, keeping non-informative features leads to over-fitting and\nis therefore detrimental for the statistical performance of the models.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.640 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Feature Selection->Univariate Feature Selection": [
        [
            "This notebook is an example of using univariate feature selection\nto improve classification accuracy on a noisy dataset.",
            "markdown"
        ],
        [
            "In this example, some noisy (non informative) features are added to\nthe iris dataset. Support vector machine (SVM) is used to classify the\ndataset both before and after applying univariate feature selection.\nFor each feature, we plot the p-values for the univariate feature selection\nand the corresponding weights of SVMs. With this, we will compare model\naccuracy and examine the impact of univariate feature selection on model\nweights.",
            "markdown"
        ]
    ],
    "Examples->Feature Selection->Univariate Feature Selection->Generate sample data": [
        [
            "import numpy as np\nfrom sklearn.datasets import \nfrom sklearn.model_selection import \n\n# The iris dataset\nX, y = (return_X_y=True)\n\n# Some noisy data not correlated\nE = (42).uniform(0, 0.1, size=(X.shape[0], 20))\n\n# Add the noisy data to the informative features\nX = ((X, E))\n\n# Split dataset to select feature and evaluate the classifier\nX_train, X_test, y_train, y_test = (X, y, stratify=y, random_state=0)",
            "code"
        ]
    ],
    "Examples->Feature Selection->Univariate Feature Selection->Univariate feature selection": [
        [
            "Univariate feature selection with F-test for feature scoring.\nWe use the default selection function to select\nthe four most significant features.",
            "markdown"
        ],
        [
            "from sklearn.feature_selection import , \n\nselector = (, k=4)\nselector.fit(X_train, y_train)\nscores = -(selector.pvalues_)\nscores /= scores.max()",
            "code"
        ],
        [
            "import matplotlib.pyplot as plt\n\nX_indices = (X.shape[-1])\n(1)\n()\n(X_indices - 0.05, scores, width=0.2)\n(\"Feature univariate score\")\n(\"Feature number\")\n(r\"Univariate score ($-Log(p_{value})$)\")\n()\n\n\n<img alt=\"Feature univariate score\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_feature_selection_001.png\" srcset=\"../../_images/sphx_glr_plot_feature_selection_001.png\"/>",
            "code"
        ],
        [
            "In the total set of features, only the 4 of the original features are significant.\nWe can see that they have the highest score with univariate feature\nselection.",
            "markdown"
        ]
    ],
    "Examples->Feature Selection->Univariate Feature Selection->Compare with SVMs": [
        [
            "Without univariate feature selection",
            "markdown"
        ],
        [
            "from sklearn.pipeline import \nfrom sklearn.preprocessing import \nfrom sklearn.svm import \n\nclf = ((), ())\nclf.fit(X_train, y_train)\nprint(\n    \"Classification accuracy without selecting features: {:.3f}\".format(\n        clf.score(X_test, y_test)\n    )\n)\n\nsvm_weights = np.abs(clf[-1].coef_).sum(axis=0)\nsvm_weights /= svm_weights.sum()",
            "code"
        ],
        [
            "Classification accuracy without selecting features: 0.789",
            "code"
        ],
        [
            "After univariate feature selection",
            "markdown"
        ],
        [
            "clf_selected = ((, k=4), (), ())\nclf_selected.fit(X_train, y_train)\nprint(\n    \"Classification accuracy after univariate feature selection: {:.3f}\".format(\n        clf_selected.score(X_test, y_test)\n    )\n)\n\nsvm_weights_selected = np.abs(clf_selected[-1].coef_).sum(axis=0)\nsvm_weights_selected /= svm_weights_selected.sum()",
            "code"
        ],
        [
            "Classification accuracy after univariate feature selection: 0.868",
            "code"
        ],
        [
            "(\n    X_indices - 0.45, scores, width=0.2, label=r\"Univariate score ($-Log(p_{value})$)\"\n)\n\n(X_indices - 0.25, svm_weights, width=0.2, label=\"SVM weight\")\n\n(\n    X_indices[selector.get_support()] - 0.05,\n    svm_weights_selected,\n    width=0.2,\n    label=\"SVM weights after selection\",\n)\n\n(\"Comparing feature selection\")\n(\"Feature number\")\n(())\n(\"tight\")\n(loc=\"upper right\")\n()\n\n\n<img alt=\"Comparing feature selection\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_feature_selection_002.png\" srcset=\"../../_images/sphx_glr_plot_feature_selection_002.png\"/>",
            "code"
        ],
        [
            "Without univariate feature selection, the SVM assigns a large weight\nto the first 4 original significant features, but also selects many of the\nnon-informative features. Applying univariate feature selection before\nthe SVM increases the SVM weight attributed to the significant features,\nand will thus improve classification.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.220 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Gaussian Mixture Models->Concentration Prior Type Analysis of Variation Bayesian Gaussian Mixture": [
        [
            "This example plots the ellipsoids obtained from a toy dataset (mixture of three\nGaussians) fitted by the BayesianGaussianMixture class models with a\nDirichlet distribution prior\n(weight_concentration_prior_type='dirichlet_distribution') and a Dirichlet\nprocess prior (weight_concentration_prior_type='dirichlet_process'). On\neach figure, we plot the results for three different values of the weight\nconcentration prior.",
            "markdown"
        ],
        [
            "The BayesianGaussianMixture class can adapt its number of mixture\ncomponents automatically. The parameter weight_concentration_prior has a\ndirect link with the resulting number of components with non-zero weights.\nSpecifying a low value for the concentration prior will make the model put most\nof the weight on few components set the remaining components weights very close\nto zero. High values of the concentration prior will allow a larger number of\ncomponents to be active in the mixture.",
            "markdown"
        ],
        [
            "The Dirichlet process prior allows to define an infinite number of components\nand automatically selects the correct number of components: it activates a\ncomponent only if it is necessary.",
            "markdown"
        ],
        [
            "On the contrary the classical finite mixture model with a Dirichlet\ndistribution prior will favor more uniformly weighted components and therefore\ntends to divide natural clusters into unnecessary sub-components.\n\n<img alt=\"Finite mixture with a Dirichlet distribution prior and $\\gamma_0=$$1.0e-03$, Finite mixture with a Dirichlet distribution prior and $\\gamma_0=$$1.0e+00$, Finite mixture with a Dirichlet distribution prior and $\\gamma_0=$$1.0e+03$\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_concentration_prior_001.png\" srcset=\"../../_images/sphx_glr_plot_concentration_prior_001.png\"/>\n<img alt=\"Infinite mixture with a Dirichlet process  prior and$\\gamma_0=$$1.0e+00$, Infinite mixture with a Dirichlet process  prior and$\\gamma_0=$$1.0e+03$, Infinite mixture with a Dirichlet process  prior and$\\gamma_0=$$1.0e+05$\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_concentration_prior_002.png\" srcset=\"../../_images/sphx_glr_plot_concentration_prior_002.png\"/>",
            "markdown"
        ],
        [
            "# Author: Thierry Guillemot &lt;thierry.guillemot.work@gmail.com\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nfrom sklearn.mixture import \n\n\ndef plot_ellipses(ax, weights, means, covars):\n    for n in range(means.shape[0]):\n        eig_vals, eig_vecs = (covars[n])\n        unit_eig_vec = eig_vecs[0] / (eig_vecs[0])\n        angle = (unit_eig_vec[1], unit_eig_vec[0])\n        # Ellipse needs degrees\n        angle = 180 * angle / \n        # eigenvector normalization\n        eig_vals = 2 * (2) * (eig_vals)\n        ell = (\n            means[n], eig_vals[0], eig_vals[1], angle=180 + angle, edgecolor=\"black\"\n        )\n        ell.set_clip_box(ax.bbox)\n        ell.set_alpha(weights[n])\n        ell.set_facecolor(\"#56B4E9\")\n        ax.add_artist(ell)\n\n\ndef plot_results(ax1, ax2, estimator, X, y, title, plot_title=False):\n    ax1.set_title(title)\n    ax1.scatter(X[:, 0], X[:, 1], s=5, marker=\"o\", color=colors[y], alpha=0.8)\n    ax1.set_xlim(-2.0, 2.0)\n    ax1.set_ylim(-3.0, 3.0)\n    ax1.set_xticks(())\n    ax1.set_yticks(())\n    plot_ellipses(ax1, estimator.weights_, estimator.means_, estimator.covariances_)\n\n    ax2.get_xaxis().set_tick_params(direction=\"out\")\n    ax2.yaxis.grid(True, alpha=0.7)\n    for k, w in enumerate(estimator.weights_):\n        ax2.bar(\n            k,\n            w,\n            width=0.9,\n            color=\"#56B4E9\",\n            zorder=3,\n            align=\"center\",\n            edgecolor=\"black\",\n        )\n        ax2.text(k, w + 0.007, \"%.1f%%\" % (w * 100.0), horizontalalignment=\"center\")\n    ax2.set_xlim(-0.6, 2 * n_components - 0.4)\n    ax2.set_ylim(0.0, 1.1)\n    ax2.tick_params(axis=\"y\", which=\"both\", left=False, right=False, labelleft=False)\n    ax2.tick_params(axis=\"x\", which=\"both\", top=False)\n\n    if plot_title:\n        ax1.set_ylabel(\"Estimated Mixtures\")\n        ax2.set_ylabel(\"Weight of each component\")\n\n\n# Parameters of the dataset\nrandom_state, n_components, n_features = 2, 3, 2\ncolors = ([\"#0072B2\", \"#F0E442\", \"#D55E00\"])\n\ncovars = (\n    [[[0.7, 0.0], [0.0, 0.1]], [[0.5, 0.0], [0.0, 0.1]], [[0.5, 0.0], [0.0, 0.1]]]\n)\nsamples = ([200, 500, 200])\nmeans = ([[0.0, -0.70], [0.0, 0.0], [0.0, 0.70]])\n\n# mean_precision_prior= 0.8 to minimize the influence of the prior\nestimators = [\n    (\n        \"Finite mixture with a Dirichlet distribution\\nprior and \" r\"$\\gamma_0=$\",\n        (\n            weight_concentration_prior_type=\"dirichlet_distribution\",\n            n_components=2 * n_components,\n            reg_covar=0,\n            init_params=\"random\",\n            max_iter=1500,\n            mean_precision_prior=0.8,\n            random_state=random_state,\n        ),\n        [0.001, 1, 1000],\n    ),\n    (\n        \"Infinite mixture with a Dirichlet process\\n prior and\" r\"$\\gamma_0=$\",\n        (\n            weight_concentration_prior_type=\"dirichlet_process\",\n            n_components=2 * n_components,\n            reg_covar=0,\n            init_params=\"random\",\n            max_iter=1500,\n            mean_precision_prior=0.8,\n            random_state=random_state,\n        ),\n        [1, 1000, 100000],\n    ),\n]\n\n# Generate data\nrng = (random_state)\nX = (\n    [\n        rng.multivariate_normal(means[j], covars[j], samples[j])\n        for j in range(n_components)\n    ]\n)\ny = ([(samples[j], j, dtype=int) for j in range(n_components)])\n\n# Plot results in two different figures\nfor title, estimator, concentrations_prior in estimators:\n    (figsize=(4.7 * 3, 8))\n    (\n        bottom=0.04, top=0.90, hspace=0.05, wspace=0.05, left=0.03, right=0.99\n    )\n\n    gs = (3, len(concentrations_prior))\n    for k, concentration in enumerate(concentrations_prior):\n        estimator.weight_concentration_prior = concentration\n        estimator.fit(X)\n        plot_results(\n            (gs[0:2, k]),\n            (gs[2, k]),\n            estimator,\n            X,\n            y,\n            r\"%s$%.1e$\" % (title, concentration),\n            plot_title=k == 0,\n        )\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  6.002 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Gaussian Mixture Models->Density Estimation for a Gaussian mixture": [
        [
            "Plot the density estimation of a mixture of two Gaussians. Data is\ngenerated from two Gaussians with different centers and covariance\nmatrices.\n<img alt=\"Negative log-likelihood predicted by a GMM\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gmm_pdf_001.png\" srcset=\"../../_images/sphx_glr_plot_gmm_pdf_001.png\"/>",
            "markdown"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import \nfrom sklearn import mixture\n\nn_samples = 300\n\n# generate random sample, two components\n(0)\n\n# generate spherical data centered on (20, 20)\nshifted_gaussian = (n_samples, 2) + ([20, 20])\n\n# generate zero centered stretched Gaussian data\nC = ([[0.0, -0.7], [3.5, 0.7]])\nstretched_gaussian = ((n_samples, 2), C)\n\n# concatenate the two datasets into the final training set\nX_train = ([shifted_gaussian, stretched_gaussian])\n\n# fit a Gaussian Mixture Model with two components\nclf = (n_components=2, covariance_type=\"full\")\nclf.fit(X_train)\n\n# display predicted scores by the model as a contour plot\nx = (-20.0, 30.0)\ny = (-20.0, 40.0)\nX, Y = (x, y)\nXX = ([X.ravel(), Y.ravel()]).T\nZ = -clf.score_samples(XX)\nZ = Z.reshape(X.shape)\n\nCS = (\n    X, Y, Z, norm=(vmin=1.0, vmax=1000.0), levels=(0, 3, 10)\n)\nCB = (CS, shrink=0.8, extend=\"both\")\n(X_train[:, 0], X_train[:, 1], 0.8)\n\n(\"Negative log-likelihood predicted by a GMM\")\n(\"tight\")\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.149 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Gaussian Mixture Models->GMM Initialization Methods": [
        [
            "Examples of the different methods of initialization in Gaussian Mixture Models",
            "markdown"
        ],
        [
            "See  for more information on the estimator.",
            "markdown"
        ],
        [
            "Here we generate some sample data with four easy to identify clusters. The\npurpose of this example is to show the four different methods for the\ninitialization parameter init_param.",
            "markdown"
        ],
        [
            "The four initializations are kmeans (default), random, random_from_data and\nk-means++.",
            "markdown"
        ],
        [
            "Orange diamonds represent the initialization centers for the gmm generated by\nthe init_param. The rest of the data is represented as crosses and the\ncolouring represents the eventual associated classification after the GMM has\nfinished.",
            "markdown"
        ],
        [
            "The numbers in the top right of each subplot represent the number of\niterations taken for the GaussianMixture to converge and the relative time\ntaken for the initialization part of the algorithm to run. The shorter\ninitialization times tend to have a greater number of iterations to converge.",
            "markdown"
        ],
        [
            "The initialization time is the ratio of the time taken for that method versus\nthe time taken for the default kmeans method. As you can see all three\nalternative methods take less time to initialize when compared to kmeans.",
            "markdown"
        ],
        [
            "In this example, when initialized with random_from_data or random the model takes\nmore iterations to converge. Here k-means++ does a good job of both low\ntime to initialize and low number of GaussianMixture iterations to converge.\n<img alt=\"GMM iterations and relative time taken to initialize, kmeans, Iter 9 | Init Time 1.00x, random_from_data, Iter 132 | Init Time 0.52x, k-means++, Iter 12 | Init Time 0.88x, random, Iter 56 | Init Time 0.67x\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gmm_init_001.png\" srcset=\"../../_images/sphx_glr_plot_gmm_init_001.png\"/>",
            "markdown"
        ],
        [
            "# Author: Gordon Walsh &lt;gordon.p.walsh@gmail.com\n# Data generation code from Jake Vanderplas &lt;vanderplas@astro.washington.edu\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.mixture import \nfrom sklearn.utils.extmath import row_norms\nfrom sklearn.datasets._samples_generator import \nfrom timeit import default_timer as \n\nprint(__doc__)\n\n# Generate some data\n\nX, y_true = (n_samples=4000, centers=4, cluster_std=0.60, random_state=0)\nX = X[:, ::-1]\n\nn_samples = 4000\nn_components = 4\nx_squared_norms = row_norms(X, squared=True)\n\n\ndef get_initial_means(X, init_params, r):\n    # Run a GaussianMixture with max_iter=0 to output the initalization means\n    gmm = (\n        n_components=4, init_params=init_params, tol=1e-9, max_iter=0, random_state=r\n    ).fit(X)\n    return gmm.means_\n\n\nmethods = [\"kmeans\", \"random_from_data\", \"k-means++\", \"random\"]\ncolors = [\"navy\", \"turquoise\", \"cornflowerblue\", \"darkorange\"]\ntimes_init = {}\nrelative_times = {}\n\n(figsize=(4 * len(methods) // 2, 6))\n(\n    bottom=0.1, top=0.9, hspace=0.15, wspace=0.05, left=0.05, right=0.95\n)\n\nfor n, method in enumerate(methods):\n    r = (seed=1234)\n    (2, len(methods) // 2, n + 1)\n\n    start = ()\n    ini = get_initial_means(X, method, r)\n    end = ()\n    init_time = end - start\n\n    gmm = (\n        n_components=4, means_init=ini, tol=1e-9, max_iter=2000, random_state=r\n    ).fit(X)\n\n    times_init[method] = init_time\n    for i, color in enumerate(colors):\n        data = X[gmm.predict(X) == i]\n        (data[:, 0], data[:, 1], color=color, marker=\"x\")\n\n    (\n        ini[:, 0], ini[:, 1], s=75, marker=\"D\", c=\"orange\", lw=1.5, edgecolors=\"black\"\n    )\n    relative_times[method] = times_init[method] / times_init[methods[0]]\n\n    (())\n    (())\n    (method, loc=\"left\", fontsize=12)\n    (\n        \"Iter %i | Init Time %.2fx\" % (gmm.n_iter_, relative_times[method]),\n        loc=\"right\",\n        fontsize=10,\n    )\n(\"GMM iterations and relative time taken to initialize\")\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.630 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Gaussian Mixture Models->GMM covariances": [
        [
            "Demonstration of several covariances types for Gaussian mixture models.",
            "markdown"
        ],
        [
            "See  for more information on the estimator.",
            "markdown"
        ],
        [
            "Although GMM are often used for clustering, we can compare the obtained\nclusters with the actual classes from the dataset. We initialize the means\nof the Gaussians with the means of the classes from the training set to make\nthis comparison valid.",
            "markdown"
        ],
        [
            "We plot predicted labels on both training and held out test data using a\nvariety of GMM covariance types on the iris dataset.\nWe compare GMMs with spherical, diagonal, full, and tied covariance\nmatrices in increasing order of performance. Although one would\nexpect full covariance to perform best in general, it is prone to\noverfitting on small datasets and does not generalize well to held out\ntest data.",
            "markdown"
        ],
        [
            "On the plots, train data is shown as dots, while test data is shown as\ncrosses. The iris dataset is four-dimensional. Only the first two\ndimensions are shown here, and thus some points are separated in other\ndimensions.\n<img alt=\"spherical, diag, tied, full\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gmm_covariances_001.png\" srcset=\"../../_images/sphx_glr_plot_gmm_covariances_001.png\"/>",
            "markdown"
        ],
        [
            "# Author: Ron Weiss &lt;ronweiss@gmail.com, Gael Varoquaux\n# Modified by Thierry Guillemot &lt;thierry.guillemot.work@gmail.com\n# License: BSD 3 clause\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nimport numpy as np\n\nfrom sklearn import datasets\nfrom sklearn.mixture import \nfrom sklearn.model_selection import \n\ncolors = [\"navy\", \"turquoise\", \"darkorange\"]\n\n\ndef make_ellipses(gmm, ax):\n    for n, color in enumerate(colors):\n        if gmm.covariance_type == \"full\":\n            covariances = gmm.covariances_[n][:2, :2]\n        elif gmm.covariance_type == \"tied\":\n            covariances = gmm.covariances_[:2, :2]\n        elif gmm.covariance_type == \"diag\":\n            covariances = (gmm.covariances_[n][:2])\n        elif gmm.covariance_type == \"spherical\":\n            covariances = (gmm.means_.shape[1]) * gmm.covariances_[n]\n        v, w = (covariances)\n        u = w[0] / (w[0])\n        angle = (u[1], u[0])\n        angle = 180 * angle /   # convert to degrees\n        v = 2.0 * (2.0) * (v)\n        ell = (\n            gmm.means_[n, :2], v[0], v[1], angle=180 + angle, color=color\n        )\n        ell.set_clip_box(ax.bbox)\n        ell.set_alpha(0.5)\n        ax.add_artist(ell)\n        ax.set_aspect(\"equal\", \"datalim\")\n\n\niris = ()\n\n# Break up the dataset into non-overlapping training (75%) and testing\n# (25%) sets.\nskf = (n_splits=4)\n# Only take the first fold.\ntrain_index, test_index = next(iter(skf.split(iris.data, iris.target)))\n\n\nX_train = iris.data[train_index]\ny_train = iris.target[train_index]\nX_test = iris.data[test_index]\ny_test = iris.target[test_index]\n\nn_classes = len((y_train))\n\n# Try GMMs using different types of covariances.\nestimators = {\n    cov_type: (\n        n_components=n_classes, covariance_type=cov_type, max_iter=20, random_state=0\n    )\n    for cov_type in [\"spherical\", \"diag\", \"tied\", \"full\"]\n}\n\nn_estimators = len(estimators)\n\n(figsize=(3 * n_estimators // 2, 6))\n(\n    bottom=0.01, top=0.95, hspace=0.15, wspace=0.05, left=0.01, right=0.99\n)\n\n\nfor index, (name, estimator) in enumerate(estimators.items()):\n    # Since we have class labels for the training data, we can\n    # initialize the GMM parameters in a supervised manner.\n    estimator.means_init = (\n        [X_train[y_train == i].mean(axis=0) for i in range(n_classes)]\n    )\n\n    # Train the other parameters using the EM algorithm.\n    estimator.fit(X_train)\n\n    h = (2, n_estimators // 2, index + 1)\n    make_ellipses(estimator, h)\n\n    for n, color in enumerate(colors):\n        data = iris.data[iris.target == n]\n        (\n            data[:, 0], data[:, 1], s=0.8, color=color, label=iris.target_names[n]\n        )\n    # Plot the test data with crosses\n    for n, color in enumerate(colors):\n        data = X_test[y_test == n]\n        (data[:, 0], data[:, 1], marker=\"x\", color=color)\n\n    y_train_pred = estimator.predict(X_train)\n    train_accuracy = (y_train_pred.ravel() == y_train.ravel()) * 100\n    (0.05, 0.9, \"Train accuracy: %.1f\" % train_accuracy, transform=h.transAxes)\n\n    y_test_pred = estimator.predict(X_test)\n    test_accuracy = (y_test_pred.ravel() == y_test.ravel()) * 100\n    (0.05, 0.8, \"Test accuracy: %.1f\" % test_accuracy, transform=h.transAxes)\n\n    (())\n    (())\n    (name)\n\n(scatterpoints=1, loc=\"lower right\", prop=dict(size=12))\n\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.213 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Gaussian Mixture Models->Gaussian Mixture Model Ellipsoids": [
        [
            "Plot the confidence ellipsoids of a mixture of two Gaussians\nobtained with Expectation Maximisation (GaussianMixture class) and\nVariational Inference (BayesianGaussianMixture class models with\na Dirichlet process prior).",
            "markdown"
        ],
        [
            "Both models have access to five components with which to fit the data. Note\nthat the Expectation Maximisation model will necessarily use all five\ncomponents while the Variational Inference model will effectively only use as\nmany as are needed for a good fit. Here we can see that the Expectation\nMaximisation model splits some components arbitrarily, because it is trying to\nfit too many components, while the Dirichlet Process model adapts it number of\nstate automatically.",
            "markdown"
        ],
        [
            "This example doesn\u2019t show it, as we\u2019re in a low-dimensional space, but\nanother advantage of the Dirichlet process model is that it can fit\nfull covariance matrices effectively even when there are less examples\nper cluster than there are dimensions in the data, due to\nregularization properties of the inference algorithm.\n<img alt=\"Gaussian Mixture, Bayesian Gaussian Mixture with a Dirichlet process prior\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gmm_001.png\" srcset=\"../../_images/sphx_glr_plot_gmm_001.png\"/>",
            "markdown"
        ],
        [
            "import itertools\n\nimport numpy as np\nfrom scipy import linalg\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nfrom sklearn import mixture\n\ncolor_iter = ([\"navy\", \"c\", \"cornflowerblue\", \"gold\", \"darkorange\"])\n\n\ndef plot_results(X, Y_, means, covariances, index, title):\n    splot = (2, 1, 1 + index)\n    for i, (mean, covar, color) in enumerate(zip(means, covariances, color_iter)):\n        v, w = (covar)\n        v = 2.0 * (2.0) * (v)\n        u = w[0] / (w[0])\n        # as the DP will not use every component it has access to\n        # unless it needs it, we shouldn't plot the redundant\n        # components.\n        if not (Y_ == i):\n            continue\n        (X[Y_ == i, 0], X[Y_ == i, 1], 0.8, color=color)\n\n        # Plot an ellipse to show the Gaussian component\n        angle = (u[1] / u[0])\n        angle = 180.0 * angle /   # convert to degrees\n        ell = (mean, v[0], v[1], angle=180.0 + angle, color=color)\n        ell.set_clip_box(splot.bbox)\n        ell.set_alpha(0.5)\n        splot.add_artist(ell)\n\n    (-9.0, 5.0)\n    (-3.0, 6.0)\n    (())\n    (())\n    (title)\n\n\n# Number of samples per component\nn_samples = 500\n\n# Generate random sample, two components\n(0)\nC = ([[0.0, -0.1], [1.7, 0.4]])\nX = [\n    ((n_samples, 2), C),\n    0.7 * (n_samples, 2) + ([-6, 3]),\n]\n\n# Fit a Gaussian mixture with EM using five components\ngmm = (n_components=5, covariance_type=\"full\").fit(X)\nplot_results(X, gmm.predict(X), gmm.means_, gmm.covariances_, 0, \"Gaussian Mixture\")\n\n# Fit a Dirichlet process Gaussian mixture using five components\ndpgmm = (n_components=5, covariance_type=\"full\").fit(X)\nplot_results(\n    X,\n    dpgmm.predict(X),\n    dpgmm.means_,\n    dpgmm.covariances_,\n    1,\n    \"Bayesian Gaussian Mixture with a Dirichlet process prior\",\n)\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.216 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Gaussian Mixture Models->Gaussian Mixture Model Selection": [
        [
            "This example shows that model selection can be performed with Gaussian Mixture\nModels (GMM) using . Model selection\nconcerns both the covariance type and the number of components in the model.",
            "markdown"
        ],
        [
            "In this case, both the Akaike Information Criterion (AIC) and the Bayes\nInformation Criterion (BIC) provide the right result, but we only demo the\nlatter as BIC is better suited to identify the true model among a set of\ncandidates. Unlike Bayesian procedures, such inferences are prior-free.",
            "markdown"
        ]
    ],
    "Examples->Gaussian Mixture Models->Gaussian Mixture Model Selection->Data generation": [
        [
            "We generate two components (each one containing n_samples) by randomly\nsampling the standard normal distribution as returned by numpy.random.randn.\nOne component is kept spherical yet shifted and re-scaled. The other one is\ndeformed to have a more general covariance matrix.",
            "markdown"
        ],
        [
            "import numpy as np\n\nn_samples = 500\n(0)\nC = ([[0.0, -0.1], [1.7, 0.4]])\ncomponent_1 = ((n_samples, 2), C)  # general\ncomponent_2 = 0.7 * (n_samples, 2) + ([-4, 1])  # spherical\n\nX = ([component_1, component_2])",
            "code"
        ],
        [
            "We can visualize the different components:",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n\n(component_1[:, 0], component_1[:, 1], s=0.8)\n(component_2[:, 0], component_2[:, 1], s=0.8)\n(\"Gaussian Mixture components\")\n(\"equal\")\n()\n\n\n<img alt=\"Gaussian Mixture components\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gmm_selection_001.png\" srcset=\"../../_images/sphx_glr_plot_gmm_selection_001.png\"/>",
            "code"
        ]
    ],
    "Examples->Gaussian Mixture Models->Gaussian Mixture Model Selection->Model training and selection": [
        [
            "We vary the number of components from 1 to 6 and the type of covariance\nparameters to use:",
            "markdown"
        ],
        [
            "\"full\": each component has its own general covariance matrix.",
            "markdown"
        ],
        [
            "\"tied\": all components share the same general covariance matrix.",
            "markdown"
        ],
        [
            "\"diag\": each component has its own diagonal covariance matrix.",
            "markdown"
        ],
        [
            "\"spherical\": each component has its own single variance.",
            "markdown"
        ],
        [
            "We score the different models and keep the best model (the lowest BIC). This\nis done by using  and a\nuser-defined score function which returns the negative BIC score, as\n is designed to <strong>maximize</strong> a\nscore (maximizing the negative BIC is equivalent to minimizing the BIC).",
            "markdown"
        ],
        [
            "The best set of parameters and estimator are stored in best_parameters_ and\nbest_estimator_, respectively.",
            "markdown"
        ],
        [
            "from sklearn.mixture import \nfrom sklearn.model_selection import \n\n\ndef gmm_bic_score(estimator, X):\n    \"\"\"Callable to pass to GridSearchCV that will use the BIC score.\"\"\"\n    # Make it negative since GridSearchCV expects a score to maximize\n    return -estimator.bic(X)\n\n\nparam_grid = {\n    \"n_components\": range(1, 7),\n    \"covariance_type\": [\"spherical\", \"tied\", \"diag\", \"full\"],\n}\ngrid_search = (\n    (), param_grid=param_grid, scoring=gmm_bic_score\n)\ngrid_search.fit(X)",
            "code"
        ],
        [
            "GridSearchCV(estimator=GaussianMixture(),\n             param_grid={'covariance_type': ['spherical', 'tied', 'diag',\n                                             'full'],\n                         'n_components': range(1, 7)},\n             scoring=&lt;function gmm_bic_score at 0x7f0d3a589670)<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-113\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-113\">GridSearchCV</label>",
            "code"
        ],
        [
            "GridSearchCV(estimator=GaussianMixture(),\n             param_grid={'covariance_type': ['spherical', 'tied', 'diag',\n                                             'full'],\n                         'n_components': range(1, 7)},\n             scoring=&lt;function gmm_bic_score at 0x7f0d3a589670)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-114\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-114\">estimator: GaussianMixture</label>",
            "code"
        ],
        [
            "GaussianMixture()<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-115\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-115\">GaussianMixture</label>",
            "code"
        ],
        [
            "GaussianMixture()\n\n<br/>\n<br/>",
            "code"
        ]
    ],
    "Examples->Gaussian Mixture Models->Gaussian Mixture Model Selection->Plot the BIC scores": [
        [
            "To ease the plotting we can create a pandas.DataFrame from the results of\nthe cross-validation done by the grid search. We re-inverse the sign of the\nBIC score to show the effect of minimizing it.",
            "markdown"
        ],
        [
            "import pandas as pd\n\ndf = (grid_search.cv_results_)[\n    [\"param_n_components\", \"param_covariance_type\", \"mean_test_score\"]\n]\ndf[\"mean_test_score\"] = -df[\"mean_test_score\"]\ndf = df.rename(\n    columns={\n        \"param_n_components\": \"Number of components\",\n        \"param_covariance_type\": \"Type of covariance\",\n        \"mean_test_score\": \"BIC score\",\n    }\n)\ndf.sort_values(by=\"BIC score\").head()\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "import seaborn as sns\n\n(\n    data=df,\n    kind=\"bar\",\n    x=\"Number of components\",\n    y=\"BIC score\",\n    hue=\"Type of covariance\",\n)\n()\n\n\n<img alt=\"plot gmm selection\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gmm_selection_002.png\" srcset=\"../../_images/sphx_glr_plot_gmm_selection_002.png\"/>",
            "code"
        ],
        [
            "In the present case, the model with 2 components and full covariance (which\ncorresponds to the true generative model) has the lowest BIC score and is\ntherefore selected by the grid search.",
            "markdown"
        ]
    ],
    "Examples->Gaussian Mixture Models->Gaussian Mixture Model Selection->Plot the best model": [
        [
            "We plot an ellipse to show each Gaussian component of the selected model. For\nsuch purpose, one needs to find the eigenvalues of the covariance matrices as\nreturned by the covariances_ attribute. The shape of such matrices depends\non the covariance_type:",
            "markdown"
        ],
        [
            "\"full\": (n_components, n_features, n_features)",
            "markdown"
        ],
        [
            "\"tied\": (n_features, n_features)",
            "markdown"
        ],
        [
            "\"diag\": (n_components, n_features)",
            "markdown"
        ],
        [
            "\"spherical\": (n_components,)",
            "markdown"
        ],
        [
            "from matplotlib.patches import \nfrom scipy import linalg\n\ncolor_iter = (\"tab10\", 2)[::-1]\nY_ = grid_search.predict(X)\n\nfig, ax = ()\n\nfor i, (mean, cov, color) in enumerate(\n    zip(\n        grid_search.best_estimator_.means_,\n        grid_search.best_estimator_.covariances_,\n        color_iter,\n    )\n):\n    v, w = (cov)\n    if not (Y_ == i):\n        continue\n    (X[Y_ == i, 0], X[Y_ == i, 1], 0.8, color=color)\n\n    angle = (w[0][1], w[0][0])\n    angle = 180.0 * angle /   # convert to degrees\n    v = 2.0 * (2.0) * (v)\n    ellipse = (mean, v[0], v[1], angle=180.0 + angle, color=color)\n    ellipse.set_clip_box(fig.bbox)\n    ellipse.set_alpha(0.5)\n    ax.add_artist(ellipse)\n\n(\n    f\"Selected GMM: {grid_search.best_params_['covariance_type']} model, \"\n    f\"{grid_search.best_params_['n_components']} components\"\n)\n(\"equal\")\n()\n\n\n<img alt=\"Selected GMM: full model, 2 components\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gmm_selection_003.png\" srcset=\"../../_images/sphx_glr_plot_gmm_selection_003.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  1.355 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Gaussian Mixture Models->Gaussian Mixture Model Sine Curve": [
        [
            "This example demonstrates the behavior of Gaussian mixture models fit on data\nthat was not sampled from a mixture of Gaussian random variables. The dataset\nis formed by 100 points loosely spaced following a noisy sine curve. There is\ntherefore no ground truth value for the number of Gaussian components.",
            "markdown"
        ],
        [
            "The first model is a classical Gaussian Mixture Model with 10 components fit\nwith the Expectation-Maximization algorithm.",
            "markdown"
        ],
        [
            "The second model is a Bayesian Gaussian Mixture Model with a Dirichlet process\nprior fit with variational inference. The low value of the concentration prior\nmakes the model favor a lower number of active components. This models\n\u201cdecides\u201d to focus its modeling power on the big picture of the structure of\nthe dataset: groups of points with alternating directions modeled by\nnon-diagonal covariance matrices. Those alternating directions roughly capture\nthe alternating nature of the original sine signal.",
            "markdown"
        ],
        [
            "The third model is also a Bayesian Gaussian mixture model with a Dirichlet\nprocess prior but this time the value of the concentration prior is higher\ngiving the model more liberty to model the fine-grained structure of the data.\nThe result is a mixture with a larger number of active components that is\nsimilar to the first model where we arbitrarily decided to fix the number of\ncomponents to 10.",
            "markdown"
        ],
        [
            "Which model is the best is a matter of subjective judgment: do we want to\nfavor models that only capture the big picture to summarize and explain most of\nthe structure of the data while ignoring the details or do we prefer models\nthat closely follow the high density regions of the signal?",
            "markdown"
        ],
        [
            "The last two panels show how we can sample from the last two models. The\nresulting samples distributions do not look exactly like the original data\ndistribution. The difference primarily stems from the approximation error we\nmade by using a model that assumes that the data was generated by a finite\nnumber of Gaussian components instead of a continuous noisy sine curve.\n<img alt=\"Expectation-maximization, Bayesian Gaussian mixture models with a Dirichlet process prior for $\\gamma_0=0.01$., Gaussian mixture with a Dirichlet process prior for $\\gamma_0=0.01$ sampled with $2000$ samples., Bayesian Gaussian mixture models with a Dirichlet process prior for $\\gamma_0=100$, Gaussian mixture with a Dirichlet process prior for $\\gamma_0=100$ sampled with $2000$ samples.\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gmm_sin_001.png\" srcset=\"../../_images/sphx_glr_plot_gmm_sin_001.png\"/>",
            "markdown"
        ],
        [
            "import itertools\n\nimport numpy as np\nfrom scipy import linalg\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nfrom sklearn import mixture\n\ncolor_iter = ([\"navy\", \"c\", \"cornflowerblue\", \"gold\", \"darkorange\"])\n\n\ndef plot_results(X, Y, means, covariances, index, title):\n    splot = (5, 1, 1 + index)\n    for i, (mean, covar, color) in enumerate(zip(means, covariances, color_iter)):\n        v, w = (covar)\n        v = 2.0 * (2.0) * (v)\n        u = w[0] / (w[0])\n        # as the DP will not use every component it has access to\n        # unless it needs it, we shouldn't plot the redundant\n        # components.\n        if not (Y == i):\n            continue\n        (X[Y == i, 0], X[Y == i, 1], 0.8, color=color)\n\n        # Plot an ellipse to show the Gaussian component\n        angle = (u[1] / u[0])\n        angle = 180.0 * angle /   # convert to degrees\n        ell = (mean, v[0], v[1], angle=180.0 + angle, color=color)\n        ell.set_clip_box(splot.bbox)\n        ell.set_alpha(0.5)\n        splot.add_artist(ell)\n\n    (-6.0, 4.0 *  - 6.0)\n    (-5.0, 5.0)\n    (title)\n    (())\n    (())\n\n\ndef plot_samples(X, Y, n_components, index, title):\n    (5, 1, 4 + index)\n    for i, color in zip(range(n_components), color_iter):\n        # as the DP will not use every component it has access to\n        # unless it needs it, we shouldn't plot the redundant\n        # components.\n        if not (Y == i):\n            continue\n        (X[Y == i, 0], X[Y == i, 1], 0.8, color=color)\n\n    (-6.0, 4.0 *  - 6.0)\n    (-5.0, 5.0)\n    (title)\n    (())\n    (())\n\n\n# Parameters\nn_samples = 100\n\n# Generate random sample following a sine curve\n(0)\nX = ((n_samples, 2))\nstep = 4.0 *  / n_samples\n\nfor i in range(X.shape[0]):\n    x = i * step - 6.0\n    X[i, 0] = x + (0, 0.1)\n    X[i, 1] = 3.0 * ((x) + (0, 0.2))\n\n(figsize=(10, 10))\n(\n    bottom=0.04, top=0.95, hspace=0.2, wspace=0.05, left=0.03, right=0.97\n)\n\n# Fit a Gaussian mixture with EM using ten components\ngmm = (\n    n_components=10, covariance_type=\"full\", max_iter=100\n).fit(X)\nplot_results(\n    X, gmm.predict(X), gmm.means_, gmm.covariances_, 0, \"Expectation-maximization\"\n)\n\ndpgmm = (\n    n_components=10,\n    covariance_type=\"full\",\n    weight_concentration_prior=1e-2,\n    weight_concentration_prior_type=\"dirichlet_process\",\n    mean_precision_prior=1e-2,\n    covariance_prior=1e0 * (2),\n    init_params=\"random\",\n    max_iter=100,\n    random_state=2,\n).fit(X)\nplot_results(\n    X,\n    dpgmm.predict(X),\n    dpgmm.means_,\n    dpgmm.covariances_,\n    1,\n    \"Bayesian Gaussian mixture models with a Dirichlet process prior \"\n    r\"for $\\gamma_0=0.01$.\",\n)\n\nX_s, y_s = dpgmm.sample(n_samples=2000)\nplot_samples(\n    X_s,\n    y_s,\n    dpgmm.n_components,\n    0,\n    \"Gaussian mixture with a Dirichlet process prior \"\n    r\"for $\\gamma_0=0.01$ sampled with $2000$ samples.\",\n)\n\ndpgmm = (\n    n_components=10,\n    covariance_type=\"full\",\n    weight_concentration_prior=1e2,\n    weight_concentration_prior_type=\"dirichlet_process\",\n    mean_precision_prior=1e-2,\n    covariance_prior=1e0 * (2),\n    init_params=\"kmeans\",\n    max_iter=100,\n    random_state=2,\n).fit(X)\nplot_results(\n    X,\n    dpgmm.predict(X),\n    dpgmm.means_,\n    dpgmm.covariances_,\n    2,\n    \"Bayesian Gaussian mixture models with a Dirichlet process prior \"\n    r\"for $\\gamma_0=100$\",\n)\n\nX_s, y_s = dpgmm.sample(n_samples=2000)\nplot_samples(\n    X_s,\n    y_s,\n    dpgmm.n_components,\n    1,\n    \"Gaussian mixture with a Dirichlet process prior \"\n    r\"for $\\gamma_0=100$ sampled with $2000$ samples.\",\n)\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.496 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Gaussian Process for Machine Learning->Comparison of kernel ridge and Gaussian process regression": [
        [
            "This example illustrates differences between a kernel ridge regression and a\nGaussian process regression.",
            "markdown"
        ],
        [
            "Both kernel ridge regression and Gaussian process regression are using a\nso-called \u201ckernel trick\u201d to make their models expressive enough to fit\nthe training data. However, the machine learning problems solved by the two\nmethods are drastically different.",
            "markdown"
        ],
        [
            "Kernel ridge regression will find the target function that minimizes a loss\nfunction (the mean squared error).",
            "markdown"
        ],
        [
            "Instead of finding a single target function, the Gaussian process regression\nemploys a probabilistic approach : a Gaussian posterior distribution over\ntarget functions is defined based on the Bayes\u2019 theorem, Thus prior\nprobabilities on target functions are being combined with a likelihood function\ndefined by the observed training data to provide estimates of the posterior\ndistributions.",
            "markdown"
        ],
        [
            "We will illustrate these differences with an example and we will also focus on\ntuning the kernel hyperparameters.",
            "markdown"
        ],
        [
            "# Authors: Jan Hendrik Metzen &lt;jhm@informatik.uni-bremen.de\n#          Guillaume Lemaitre &lt;g.lemaitre58@gmail.com\n# License: BSD 3 clause",
            "code"
        ]
    ],
    "Examples->Gaussian Process for Machine Learning->Comparison of kernel ridge and Gaussian process regression->Generating a dataset": [
        [
            "We create a synthetic dataset. The true generative process will take a 1-D\nvector and compute its sine. Note that the period of this sine is thus\n\\(2 \\pi\\). We will reuse this information later in this example.",
            "markdown"
        ],
        [
            "import numpy as np\n\nrng = (0)\ndata = (0, 30, num=1_000).reshape(-1, 1)\ntarget = (data).ravel()",
            "code"
        ],
        [
            "Now, we can imagine a scenario where we get observations from this true\nprocess. However, we will add some challenges:",
            "markdown"
        ],
        [
            "the measurements will be noisy;",
            "markdown"
        ],
        [
            "only samples from the beginning of the signal will be available.",
            "markdown"
        ],
        [
            "training_sample_indices = rng.choice((0, 400), size=40, replace=False)\ntraining_data = data[training_sample_indices]\ntraining_noisy_target = target[training_sample_indices] + 0.5 * rng.randn(\n    len(training_sample_indices)\n)",
            "code"
        ],
        [
            "Let\u2019s plot the true signal and the noisy measurements available for training.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n\n(data, target, label=\"True signal\", linewidth=2)\n(\n    training_data,\n    training_noisy_target,\n    color=\"black\",\n    label=\"Noisy measurements\",\n)\n()\n(\"data\")\n(\"target\")\n_ = (\n    \"Illustration of the true generative process and \\n\"\n    \"noisy measurements available during training\"\n)\n\n\n<img alt=\"Illustration of the true generative process and  noisy measurements available during training\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_compare_gpr_krr_001.png\" srcset=\"../../_images/sphx_glr_plot_compare_gpr_krr_001.png\"/>",
            "code"
        ]
    ],
    "Examples->Gaussian Process for Machine Learning->Comparison of kernel ridge and Gaussian process regression->Limitations of a simple linear model": [
        [
            "First, we would like to highlight the limitations of a linear model given\nour dataset. We fit a  and check the\npredictions of this model on our dataset.",
            "markdown"
        ],
        [
            "from sklearn.linear_model import \n\nridge = ().fit(training_data, training_noisy_target)\n\n(data, target, label=\"True signal\", linewidth=2)\n(\n    training_data,\n    training_noisy_target,\n    color=\"black\",\n    label=\"Noisy measurements\",\n)\n(data, ridge.predict(data), label=\"Ridge regression\")\n()\n(\"data\")\n(\"target\")\n_ = (\"Limitation of a linear model such as ridge\")\n\n\n<img alt=\"Limitation of a linear model such as ridge\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_compare_gpr_krr_002.png\" srcset=\"../../_images/sphx_glr_plot_compare_gpr_krr_002.png\"/>",
            "code"
        ],
        [
            "Such a ridge regressor underfits data since it is not expressive enough.",
            "markdown"
        ]
    ],
    "Examples->Gaussian Process for Machine Learning->Comparison of kernel ridge and Gaussian process regression->Kernel methods: kernel ridge and Gaussian process->Kernel ridge": [
        [
            "We can make the previous linear model more expressive by using a so-called\nkernel. A kernel is an embedding from the original feature space to another\none. Simply put, it is used to map our original data into a newer and more\ncomplex feature space. This new space is explicitly defined by the choice of\nkernel.",
            "markdown"
        ],
        [
            "In our case, we know that the true generative process is a periodic function.\nWe can use a  kernel\nwhich allows recovering the periodicity. The class\n will accept such a kernel.",
            "markdown"
        ],
        [
            "Using this model together with a kernel is equivalent to embed the data\nusing the mapping function of the kernel and then apply a ridge regression.\nIn practice, the data are not mapped explicitly; instead the dot product\nbetween samples in the higher dimensional feature space is computed using the\n\u201ckernel trick\u201d.",
            "markdown"
        ],
        [
            "Thus, let\u2019s use such a .",
            "markdown"
        ],
        [
            "import time\nfrom sklearn.gaussian_process.kernels import \nfrom sklearn.kernel_ridge import \n\nkernel_ridge = (kernel=())\n\nstart_time = ()\nkernel_ridge.fit(training_data, training_noisy_target)\nprint(\n    f\"Fitting KernelRidge with default kernel: {() - start_time:.3f} seconds\"\n)",
            "code"
        ],
        [
            "Fitting KernelRidge with default kernel: 0.001 seconds",
            "code"
        ],
        [
            "(data, target, label=\"True signal\", linewidth=2, linestyle=\"dashed\")\n(\n    training_data,\n    training_noisy_target,\n    color=\"black\",\n    label=\"Noisy measurements\",\n)\n(\n    data,\n    kernel_ridge.predict(data),\n    label=\"Kernel ridge\",\n    linewidth=2,\n    linestyle=\"dashdot\",\n)\n(loc=\"lower right\")\n(\"data\")\n(\"target\")\n_ = (\n    \"Kernel ridge regression with an exponential sine squared\\n \"\n    \"kernel using default hyperparameters\"\n)\n\n\n<img alt=\"Kernel ridge regression with an exponential sine squared  kernel using default hyperparameters\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_compare_gpr_krr_003.png\" srcset=\"../../_images/sphx_glr_plot_compare_gpr_krr_003.png\"/>",
            "code"
        ],
        [
            "This fitted model is not accurate. Indeed, we did not set the parameters of\nthe kernel and instead used the default ones. We can inspect them.",
            "markdown"
        ],
        [
            "kernel_ridge.kernel",
            "code"
        ],
        [
            "ExpSineSquared(length_scale=1, periodicity=1)",
            "code"
        ],
        [
            "Our kernel has two parameters: the length-scale and the periodicity. For our\ndataset, we use sin as the generative process, implying a\n\\(2 \\pi\\)-periodicity for the signal. The default value of the parameter\nbeing \\(1\\), it explains the high frequency observed in the predictions of\nour model.\nSimilar conclusions could be drawn with the length-scale parameter. Thus, it\ntell us that the kernel parameters need to be tuned. We will use a randomized\nsearch to tune the different parameters the kernel ridge model: the alpha\nparameter and the kernel parameters.",
            "markdown"
        ],
        [
            "from sklearn.model_selection import \nfrom sklearn.utils.fixes import loguniform\n\nparam_distributions = {\n    \"alpha\": loguniform(1e0, 1e3),\n    \"kernel__length_scale\": loguniform(1e-2, 1e2),\n    \"kernel__periodicity\": loguniform(1e0, 1e1),\n}\nkernel_ridge_tuned = (\n    kernel_ridge,\n    param_distributions=param_distributions,\n    n_iter=500,\n    random_state=0,\n)\nstart_time = ()\nkernel_ridge_tuned.fit(training_data, training_noisy_target)\nprint(f\"Time for KernelRidge fitting: {() - start_time:.3f} seconds\")",
            "code"
        ],
        [
            "Time for KernelRidge fitting: 3.141 seconds",
            "code"
        ],
        [
            "Fitting the model is now more computationally expensive since we have to try\nseveral combinations of hyperparameters. We can have a look at the\nhyperparameters found to get some intuitions.",
            "markdown"
        ],
        [
            "kernel_ridge_tuned.best_params_",
            "code"
        ],
        [
            "{'alpha': 1.9915849773450223, 'kernel__length_scale': 0.7986499491396728, 'kernel__periodicity': 6.607275806426108}",
            "code"
        ],
        [
            "Looking at the best parameters, we see that they are different from the\ndefaults. We also see that the periodicity is closer to the expected value:\n\\(2 \\pi\\). We can now inspect the predictions of our tuned kernel ridge.",
            "markdown"
        ],
        [
            "start_time = ()\npredictions_kr = kernel_ridge_tuned.predict(data)\nprint(f\"Time for KernelRidge predict: {() - start_time:.3f} seconds\")",
            "code"
        ],
        [
            "Time for KernelRidge predict: 0.001 seconds",
            "code"
        ],
        [
            "(data, target, label=\"True signal\", linewidth=2, linestyle=\"dashed\")\n(\n    training_data,\n    training_noisy_target,\n    color=\"black\",\n    label=\"Noisy measurements\",\n)\n(\n    data,\n    predictions_kr,\n    label=\"Kernel ridge\",\n    linewidth=2,\n    linestyle=\"dashdot\",\n)\n(loc=\"lower right\")\n(\"data\")\n(\"target\")\n_ = (\n    \"Kernel ridge regression with an exponential sine squared\\n \"\n    \"kernel using tuned hyperparameters\"\n)\n\n\n<img alt=\"Kernel ridge regression with an exponential sine squared  kernel using tuned hyperparameters\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_compare_gpr_krr_004.png\" srcset=\"../../_images/sphx_glr_plot_compare_gpr_krr_004.png\"/>",
            "code"
        ],
        [
            "We get a much more accurate model. We still observe some errors mainly due to\nthe noise added to the dataset.",
            "markdown"
        ]
    ],
    "Examples->Gaussian Process for Machine Learning->Comparison of kernel ridge and Gaussian process regression->Kernel methods: kernel ridge and Gaussian process->Gaussian process regression": [
        [
            "Now, we will use a\n to fit the same\ndataset. When training a Gaussian process, the hyperparameters of the kernel\nare optimized during the fitting process. There is no need for an external\nhyperparameter search. Here, we create a slightly more complex kernel than\nfor the kernel ridge regressor: we add a\n that is used to\nestimate the noise in the dataset.",
            "markdown"
        ],
        [
            "from sklearn.gaussian_process import \nfrom sklearn.gaussian_process.kernels import \n\nkernel = 1.0 * (1.0, 5.0, periodicity_bounds=(1e-2, 1e1)) + (\n    1e-1\n)\ngaussian_process = (kernel=kernel)\nstart_time = ()\ngaussian_process.fit(training_data, training_noisy_target)\nprint(\n    f\"Time for GaussianProcessRegressor fitting: {() - start_time:.3f} seconds\"\n)",
            "code"
        ],
        [
            "Time for GaussianProcessRegressor fitting: 0.030 seconds",
            "code"
        ],
        [
            "The computation cost of training a Gaussian process is much less than the\nkernel ridge that uses a randomized search. We can check the parameters of\nthe kernels that we computed.",
            "markdown"
        ],
        [
            "gaussian_process.kernel_",
            "code"
        ],
        [
            "0.675**2 * ExpSineSquared(length_scale=1.34, periodicity=6.57) + WhiteKernel(noise_level=0.182)",
            "code"
        ],
        [
            "Indeed, we see that the parameters have been optimized. Looking at the\nperiodicity parameter, we see that we found a period close to the\ntheoretical value \\(2 \\pi\\). We can have a look now at the predictions of\nour model.",
            "markdown"
        ],
        [
            "start_time = ()\nmean_predictions_gpr, std_predictions_gpr = gaussian_process.predict(\n    data,\n    return_std=True,\n)\nprint(\n    f\"Time for GaussianProcessRegressor predict: {() - start_time:.3f} seconds\"\n)",
            "code"
        ],
        [
            "Time for GaussianProcessRegressor predict: 0.001 seconds",
            "code"
        ],
        [
            "(data, target, label=\"True signal\", linewidth=2, linestyle=\"dashed\")\n(\n    training_data,\n    training_noisy_target,\n    color=\"black\",\n    label=\"Noisy measurements\",\n)\n# Plot the predictions of the kernel ridge\n(\n    data,\n    predictions_kr,\n    label=\"Kernel ridge\",\n    linewidth=2,\n    linestyle=\"dashdot\",\n)\n# Plot the predictions of the gaussian process regressor\n(\n    data,\n    mean_predictions_gpr,\n    label=\"Gaussian process regressor\",\n    linewidth=2,\n    linestyle=\"dotted\",\n)\n(\n    data.ravel(),\n    mean_predictions_gpr - std_predictions_gpr,\n    mean_predictions_gpr + std_predictions_gpr,\n    color=\"tab:green\",\n    alpha=0.2,\n)\n(loc=\"lower right\")\n(\"data\")\n(\"target\")\n_ = (\"Comparison between kernel ridge and gaussian process regressor\")\n\n\n<img alt=\"Comparison between kernel ridge and gaussian process regressor\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_compare_gpr_krr_005.png\" srcset=\"../../_images/sphx_glr_plot_compare_gpr_krr_005.png\"/>",
            "code"
        ],
        [
            "We observe that the results of the kernel ridge and the Gaussian process\nregressor are close. However, the Gaussian process regressor also provide\nan uncertainty information that is not available with a kernel ridge.\nDue to the probabilistic formulation of the target functions, the\nGaussian process can output the standard deviation (or the covariance)\ntogether with the mean predictions of the target functions.",
            "markdown"
        ],
        [
            "However, it comes at a cost: the time to compute the predictions is higher\nwith a Gaussian process.",
            "markdown"
        ]
    ],
    "Examples->Gaussian Process for Machine Learning->Comparison of kernel ridge and Gaussian process regression->Final conclusion": [
        [
            "We can give a final word regarding the possibility of the two models to\nextrapolate. Indeed, we only provided the beginning of the signal as a\ntraining set. Using a periodic kernel forces our model to repeat the pattern\nfound on the training set. Using this kernel information together with the\ncapacity of the both models to extrapolate, we observe that the models will\ncontinue to predict the sine pattern.",
            "markdown"
        ],
        [
            "Gaussian process allows to combine kernels together. Thus, we could associate\nthe exponential sine squared kernel together with a radial basis function\nkernel.",
            "markdown"
        ],
        [
            "from sklearn.gaussian_process.kernels import \n\nkernel = 1.0 * (1.0, 5.0, periodicity_bounds=(1e-2, 1e1)) * (\n    length_scale=15, length_scale_bounds=\"fixed\"\n) + (1e-1)\ngaussian_process = (kernel=kernel)\ngaussian_process.fit(training_data, training_noisy_target)\nmean_predictions_gpr, std_predictions_gpr = gaussian_process.predict(\n    data,\n    return_std=True,\n)",
            "code"
        ],
        [
            "(data, target, label=\"True signal\", linewidth=2, linestyle=\"dashed\")\n(\n    training_data,\n    training_noisy_target,\n    color=\"black\",\n    label=\"Noisy measurements\",\n)\n# Plot the predictions of the kernel ridge\n(\n    data,\n    predictions_kr,\n    label=\"Kernel ridge\",\n    linewidth=2,\n    linestyle=\"dashdot\",\n)\n# Plot the predictions of the gaussian process regressor\n(\n    data,\n    mean_predictions_gpr,\n    label=\"Gaussian process regressor\",\n    linewidth=2,\n    linestyle=\"dotted\",\n)\n(\n    data.ravel(),\n    mean_predictions_gpr - std_predictions_gpr,\n    mean_predictions_gpr + std_predictions_gpr,\n    color=\"tab:green\",\n    alpha=0.2,\n)\n(loc=\"lower right\")\n(\"data\")\n(\"target\")\n_ = (\"Effect of using a radial basis function kernel\")\n\n\n<img alt=\"Effect of using a radial basis function kernel\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_compare_gpr_krr_006.png\" srcset=\"../../_images/sphx_glr_plot_compare_gpr_krr_006.png\"/>",
            "code"
        ],
        [
            "The effect of using a radial basis function kernel will attenuate the\nperiodicity effect once that no sample are available in the training.\nAs testing samples get further away from the training ones, predictions\nare converging towards their mean and their standard deviation\nalso increases.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  3.791 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Gaussian Process for Machine Learning->Gaussian Processes regression: basic introductory example": [
        [
            "A simple one-dimensional regression example computed in two different ways:",
            "markdown"
        ],
        [
            "A noise-free case",
            "markdown"
        ],
        [
            "A noisy case with known noise-level per datapoint",
            "markdown"
        ],
        [
            "In both cases, the kernel\u2019s parameters are estimated using the maximum\nlikelihood principle.",
            "markdown"
        ],
        [
            "The figures illustrate the interpolating property of the Gaussian Process model\nas well as its probabilistic nature in the form of a pointwise 95% confidence\ninterval.",
            "markdown"
        ],
        [
            "Note that alpha is a parameter to control the strength of the Tikhonov\nregularization on the assumed training points\u2019 covariance matrix.",
            "markdown"
        ],
        [
            "# Author: Vincent Dubourg &lt;vincent.dubourg@gmail.com\n#         Jake Vanderplas &lt;vanderplas@astro.washington.edu\n#         Jan Hendrik Metzen &lt;jhm@informatik.uni-bremen.de\n#         Guillaume Lemaitre &lt;g.lemaitre58@gmail.com\n# License: BSD 3 clause",
            "code"
        ]
    ],
    "Examples->Gaussian Process for Machine Learning->Gaussian Processes regression: basic introductory example->Dataset generation": [
        [
            "We will start by generating a synthetic dataset. The true generative process\nis defined as \\(f(x) = x \\sin(x)\\).",
            "markdown"
        ],
        [
            "import numpy as np\n\nX = (start=0, stop=10, num=1_000).reshape(-1, 1)\ny = (X * (X))",
            "code"
        ],
        [
            "import matplotlib.pyplot as plt\n\n(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\n()\n(\"$x$\")\n(\"$f(x)$\")\n_ = (\"True generative process\")\n\n\n<img alt=\"True generative process\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_noisy_targets_001.png\" srcset=\"../../_images/sphx_glr_plot_gpr_noisy_targets_001.png\"/>",
            "code"
        ],
        [
            "We will use this dataset in the next experiment to illustrate how Gaussian\nProcess regression is working.",
            "markdown"
        ]
    ],
    "Examples->Gaussian Process for Machine Learning->Gaussian Processes regression: basic introductory example->Example with noise-free target": [
        [
            "In this first example, we will use the true generative process without\nadding any noise. For training the Gaussian Process regression, we will only\nselect few samples.",
            "markdown"
        ],
        [
            "rng = (1)\ntraining_indices = rng.choice((y.size), size=6, replace=False)\nX_train, y_train = X[training_indices], y[training_indices]",
            "code"
        ],
        [
            "Now, we fit a Gaussian process on these few training data samples. We will\nuse a radial basis function (RBF) kernel and a constant parameter to fit the\namplitude.",
            "markdown"
        ],
        [
            "from sklearn.gaussian_process import \nfrom sklearn.gaussian_process.kernels import \n\nkernel = 1 * (length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\ngaussian_process = (kernel=kernel, n_restarts_optimizer=9)\ngaussian_process.fit(X_train, y_train)\ngaussian_process.kernel_",
            "code"
        ],
        [
            "5.02**2 * RBF(length_scale=1.43)",
            "code"
        ],
        [
            "After fitting our model, we see that the hyperparameters of the kernel have\nbeen optimized. Now, we will use our kernel to compute the mean prediction\nof the full dataset and plot the 95% confidence interval.",
            "markdown"
        ],
        [
            "mean_prediction, std_prediction = gaussian_process.predict(X, return_std=True)\n\n(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\n(X_train, y_train, label=\"Observations\")\n(X, mean_prediction, label=\"Mean prediction\")\n(\n    X.ravel(),\n    mean_prediction - 1.96 * std_prediction,\n    mean_prediction + 1.96 * std_prediction,\n    alpha=0.5,\n    label=r\"95% confidence interval\",\n)\n()\n(\"$x$\")\n(\"$f(x)$\")\n_ = (\"Gaussian process regression on noise-free dataset\")\n\n\n<img alt=\"Gaussian process regression on noise-free dataset\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_noisy_targets_002.png\" srcset=\"../../_images/sphx_glr_plot_gpr_noisy_targets_002.png\"/>",
            "code"
        ],
        [
            "We see that for a prediction made on a data point close to the one from the\ntraining set, the 95% confidence has a small amplitude. Whenever a sample\nfalls far from training data, our model\u2019s prediction is less accurate and the\nmodel prediction is less precise (higher uncertainty).",
            "markdown"
        ]
    ],
    "Examples->Gaussian Process for Machine Learning->Gaussian Processes regression: basic introductory example->Example with noisy targets": [
        [
            "We can repeat a similar experiment adding an additional noise to the target\nthis time. It will allow seeing the effect of the noise on the fitted model.",
            "markdown"
        ],
        [
            "We add some random Gaussian noise to the target with an arbitrary\nstandard deviation.",
            "markdown"
        ],
        [
            "noise_std = 0.75\ny_train_noisy = y_train + rng.normal(loc=0.0, scale=noise_std, size=y_train.shape)",
            "code"
        ],
        [
            "We create a similar Gaussian process model. In addition to the kernel, this\ntime, we specify the parameter alpha which can be interpreted as the\nvariance of a Gaussian noise.",
            "markdown"
        ],
        [
            "gaussian_process = (\n    kernel=kernel, alpha=noise_std**2, n_restarts_optimizer=9\n)\ngaussian_process.fit(X_train, y_train_noisy)\nmean_prediction, std_prediction = gaussian_process.predict(X, return_std=True)",
            "code"
        ],
        [
            "Let\u2019s plot the mean prediction and the uncertainty region as before.",
            "markdown"
        ],
        [
            "(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\n(\n    X_train,\n    y_train_noisy,\n    noise_std,\n    linestyle=\"None\",\n    color=\"tab:blue\",\n    marker=\".\",\n    markersize=10,\n    label=\"Observations\",\n)\n(X, mean_prediction, label=\"Mean prediction\")\n(\n    X.ravel(),\n    mean_prediction - 1.96 * std_prediction,\n    mean_prediction + 1.96 * std_prediction,\n    color=\"tab:orange\",\n    alpha=0.5,\n    label=r\"95% confidence interval\",\n)\n()\n(\"$x$\")\n(\"$f(x)$\")\n_ = (\"Gaussian process regression on a noisy dataset\")\n\n\n<img alt=\"Gaussian process regression on a noisy dataset\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_noisy_targets_003.png\" srcset=\"../../_images/sphx_glr_plot_gpr_noisy_targets_003.png\"/>",
            "code"
        ],
        [
            "The noise affects the predictions close to the training samples: the\npredictive uncertainty near to the training samples is larger because we\nexplicitly model a given level target noise independent of the input\nvariable.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.484 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Gaussian Process for Machine Learning->Gaussian process classification (GPC) on iris dataset": [
        [
            "This example illustrates the predicted probability of GPC for an isotropic\nand anisotropic RBF kernel on a two-dimensional version for the iris-dataset.\nThe anisotropic RBF kernel obtains slightly higher log-marginal-likelihood by\nassigning different length-scales to the two feature dimensions.\n<img alt=\"Isotropic RBF, LML: -48.316, Anisotropic RBF, LML: -47.888\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpc_iris_001.png\" srcset=\"../../_images/sphx_glr_plot_gpc_iris_001.png\"/>",
            "markdown"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.gaussian_process import \nfrom sklearn.gaussian_process.kernels import \n\n# import some data to play with\niris = ()\nX = iris.data[:, :2]  # we only take the first two features.\ny = (iris.target, dtype=int)\n\nh = 0.02  # step size in the mesh\n\nkernel = 1.0 * ([1.0])\ngpc_rbf_isotropic = (kernel=kernel).fit(X, y)\nkernel = 1.0 * ([1.0, 1.0])\ngpc_rbf_anisotropic = (kernel=kernel).fit(X, y)\n\n# create a mesh to plot in\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = ((x_min, x_max, h), (y_min, y_max, h))\n\ntitles = [\"Isotropic RBF\", \"Anisotropic RBF\"]\n(figsize=(10, 5))\nfor i, clf in enumerate((gpc_rbf_isotropic, gpc_rbf_anisotropic)):\n    # Plot the predicted probabilities. For that, we will assign a color to\n    # each point in the mesh [x_min, m_max]x[y_min, y_max].\n    (1, 2, i + 1)\n\n    Z = clf.predict_proba([xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape((xx.shape[0], xx.shape[1], 3))\n    (Z, extent=(x_min, x_max, y_min, y_max), origin=\"lower\")\n\n    # Plot also the training points\n    (X[:, 0], X[:, 1], c=([\"r\", \"g\", \"b\"])[y], edgecolors=(0, 0, 0))\n    (\"Sepal length\")\n    (\"Sepal width\")\n    (xx.min(), xx.max())\n    (yy.min(), yy.max())\n    (())\n    (())\n    (\n        \"%s, LML: %.3f\" % (titles[i], clf.log_marginal_likelihood(clf.kernel_.theta))\n    )\n\n()\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  3.249 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) on Mauna Loa CO2 data": [
        [
            "This example is based on Section 5.4.3 of \u201cGaussian Processes for Machine\nLearning\u201d . It illustrates an example of complex kernel engineering\nand hyperparameter optimization using gradient ascent on the\nlog-marginal-likelihood. The data consists of the monthly average atmospheric\nCO2 concentrations (in parts per million by volume (ppm)) collected at the\nMauna Loa Observatory in Hawaii, between 1958 and 2001. The objective is to\nmodel the CO2 concentration as a function of the time \\(t\\) and extrapolate\nfor years after 2001.",
            "markdown"
        ],
        [
            "print(__doc__)\n\n# Authors: Jan Hendrik Metzen &lt;jhm@informatik.uni-bremen.de\n#          Guillaume Lemaitre &lt;g.lemaitre58@gmail.com\n# License: BSD 3 clause",
            "code"
        ]
    ],
    "Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) on Mauna Loa CO2 data->Build the dataset": [
        [
            "We will derive a dataset from the Mauna Loa Observatory that collected air\nsamples. We are interested in estimating the concentration of CO2 and\nextrapolate it for further year. First, we load the original dataset available\nin OpenML.",
            "markdown"
        ],
        [
            "from sklearn.datasets import \n\nco2 = (data_id=41187, as_frame=True, parser=\"pandas\")\nco2.frame.head()\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "First, we process the original dataframe to create a date index and select\nonly the CO2 column.",
            "markdown"
        ],
        [
            "import pandas as pd\n\nco2_data = co2.frame\nco2_data[\"date\"] = (co2_data[[\"year\", \"month\", \"day\"]])\nco2_data = co2_data[[\"date\", \"co2\"]].set_index(\"date\")\nco2_data.head()\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "co2_data.index.min(), co2_data.index.max()",
            "code"
        ],
        [
            "(Timestamp('1958-03-29 00:00:00'), Timestamp('2001-12-29 00:00:00'))",
            "code"
        ],
        [
            "We see that we get CO2 concentration for some days from March, 1958 to\nDecember, 2001. We can plot these raw information to have a better\nunderstanding.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n\nco2_data.plot()\n(\"CO$_2$ concentration (ppm)\")\n_ = (\"Raw air samples measurements from the Mauna Loa Observatory\")\n\n\n<img alt=\"Raw air samples measurements from the Mauna Loa Observatory\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_co2_001.png\" srcset=\"../../_images/sphx_glr_plot_gpr_co2_001.png\"/>",
            "code"
        ],
        [
            "We will preprocess the dataset by taking a monthly average and drop month\nfor which no measurements were collected. Such a processing will have an\nsmoothing effect on the data.",
            "markdown"
        ],
        [
            "co2_data = co2_data.resample(\"M\").mean().dropna(axis=\"index\", how=\"any\")\nco2_data.plot()\n(\"Monthly average of CO$_2$ concentration (ppm)\")\n_ = (\n    \"Monthly average of air samples measurements\\nfrom the Mauna Loa Observatory\"\n)\n\n\n<img alt=\"Monthly average of air samples measurements from the Mauna Loa Observatory\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_co2_002.png\" srcset=\"../../_images/sphx_glr_plot_gpr_co2_002.png\"/>",
            "code"
        ],
        [
            "The idea in this example will be to predict the CO2 concentration in function\nof the date. We are as well interested in extrapolating for upcoming year\nafter 2001.",
            "markdown"
        ],
        [
            "As a first step, we will divide the data and the target to estimate. The data\nbeing a date, we will convert it into a numeric.",
            "markdown"
        ],
        [
            "X = (co2_data.index.year + co2_data.index.month / 12).to_numpy().reshape(-1, 1)\ny = co2_data[\"co2\"].to_numpy()",
            "code"
        ]
    ],
    "Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) on Mauna Loa CO2 data->Design the proper kernel": [
        [
            "To design the kernel to use with our Gaussian process, we can make some\nassumption regarding the data at hand. We observe that they have several\ncharacteristics: we see a long term rising trend, a pronounced seasonal\nvariation and some smaller irregularities. We can use different appropriate\nkernel that would capture these features.",
            "markdown"
        ],
        [
            "First, the long term rising trend could be fitted using a radial basis\nfunction (RBF) kernel with a large length-scale parameter. The RBF kernel\nwith a large length-scale enforces this component to be smooth. An trending\nincrease is not enforced as to give a degree of freedom to our model. The\nspecific length-scale and the amplitude are free hyperparameters.",
            "markdown"
        ],
        [
            "from sklearn.gaussian_process.kernels import \n\nlong_term_trend_kernel = 50.0**2 * (length_scale=50.0)",
            "code"
        ],
        [
            "The seasonal variation is explained by the periodic exponential sine squared\nkernel with a fixed periodicity of 1 year. The length-scale of this periodic\ncomponent, controlling its smoothness, is a free parameter. In order to allow\ndecaying away from exact periodicity, the product with an RBF kernel is\ntaken. The length-scale of this RBF component controls the decay time and is\na further free parameter. This type of kernel is also known as locally\nperiodic kernel.",
            "markdown"
        ],
        [
            "from sklearn.gaussian_process.kernels import \n\nseasonal_kernel = (\n    2.0**2\n    * (length_scale=100.0)\n    * (length_scale=1.0, periodicity=1.0, periodicity_bounds=\"fixed\")\n)",
            "code"
        ],
        [
            "The small irregularities are to be explained by a rational quadratic kernel\ncomponent, whose length-scale and alpha parameter, which quantifies the\ndiffuseness of the length-scales, are to be determined. A rational quadratic\nkernel is equivalent to an RBF kernel with several length-scale and will\nbetter accommodate the different irregularities.",
            "markdown"
        ],
        [
            "from sklearn.gaussian_process.kernels import \n\nirregularities_kernel = 0.5**2 * (length_scale=1.0, alpha=1.0)",
            "code"
        ],
        [
            "Finally, the noise in the dataset can be accounted with a kernel consisting\nof an RBF kernel contribution, which shall explain the correlated noise\ncomponents such as local weather phenomena, and a white kernel contribution\nfor the white noise. The relative amplitudes and the RBF\u2019s length scale are\nfurther free parameters.",
            "markdown"
        ],
        [
            "from sklearn.gaussian_process.kernels import \n\nnoise_kernel = 0.1**2 * (length_scale=0.1) + (\n    noise_level=0.1**2, noise_level_bounds=(1e-5, 1e5)\n)",
            "code"
        ],
        [
            "Thus, our final kernel is an addition of all previous kernel.",
            "markdown"
        ],
        [
            "co2_kernel = (\n    long_term_trend_kernel + seasonal_kernel + irregularities_kernel + noise_kernel\n)\nco2_kernel",
            "code"
        ],
        [
            "50**2 * RBF(length_scale=50) + 2**2 * RBF(length_scale=100) * ExpSineSquared(length_scale=1, periodicity=1) + 0.5**2 * RationalQuadratic(alpha=1, length_scale=1) + 0.1**2 * RBF(length_scale=0.1) + WhiteKernel(noise_level=0.01)",
            "code"
        ]
    ],
    "Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) on Mauna Loa CO2 data->Model fitting and extrapolation": [
        [
            "Now, we are ready to use a Gaussian process regressor and fit the available\ndata. To follow the example from the literature, we will subtract the mean\nfrom the target. We could have used normalize_y=True. However, doing so\nwould have also scaled the target (dividing y by its standard deviation).\nThus, the hyperparameters of the different kernel would have had different\nmeaning since they would not have been expressed in ppm.",
            "markdown"
        ],
        [
            "from sklearn.gaussian_process import \n\ny_mean = y.mean()\ngaussian_process = (kernel=co2_kernel, normalize_y=False)\ngaussian_process.fit(X, y - y_mean)",
            "code"
        ],
        [
            "GaussianProcessRegressor(kernel=50**2 * RBF(length_scale=50) + 2**2 * RBF(length_scale=100) * ExpSineSquared(length_scale=1, periodicity=1) + 0.5**2 * RationalQuadratic(alpha=1, length_scale=1) + 0.1**2 * RBF(length_scale=0.1) + WhiteKernel(noise_level=0.01))<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input checked=\"\" class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-116\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-116\">GaussianProcessRegressor</label>",
            "code"
        ],
        [
            "GaussianProcessRegressor(kernel=50**2 * RBF(length_scale=50) + 2**2 * RBF(length_scale=100) * ExpSineSquared(length_scale=1, periodicity=1) + 0.5**2 * RationalQuadratic(alpha=1, length_scale=1) + 0.1**2 * RBF(length_scale=0.1) + WhiteKernel(noise_level=0.01))\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "Now, we will use the Gaussian process to predict on:",
            "markdown"
        ],
        [
            "training data to inspect the goodness of fit;",
            "markdown"
        ],
        [
            "future data to see the extrapolation done by the model.",
            "markdown"
        ],
        [
            "Thus, we create synthetic data from 1958 to the current month. In addition,\nwe need to add the subtracted mean computed during training.",
            "markdown"
        ],
        [
            "import datetime\nimport numpy as np\n\ntoday = datetime.datetime.now()\ncurrent_month = today.year + today.month / 12\nX_test = (start=1958, stop=current_month, num=1_000).reshape(-1, 1)\nmean_y_pred, std_y_pred = gaussian_process.predict(X_test, return_std=True)\nmean_y_pred += y_mean",
            "code"
        ],
        [
            "(X, y, color=\"black\", linestyle=\"dashed\", label=\"Measurements\")\n(X_test, mean_y_pred, color=\"tab:blue\", alpha=0.4, label=\"Gaussian process\")\n(\n    X_test.ravel(),\n    mean_y_pred - std_y_pred,\n    mean_y_pred + std_y_pred,\n    color=\"tab:blue\",\n    alpha=0.2,\n)\n()\n(\"Year\")\n(\"Monthly average of CO$_2$ concentration (ppm)\")\n_ = (\n    \"Monthly average of air samples measurements\\nfrom the Mauna Loa Observatory\"\n)\n\n\n<img alt=\"Monthly average of air samples measurements from the Mauna Loa Observatory\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_co2_003.png\" srcset=\"../../_images/sphx_glr_plot_gpr_co2_003.png\"/>",
            "code"
        ],
        [
            "Our fitted model is capable to fit previous data properly and extrapolate to\nfuture year with confidence.",
            "markdown"
        ]
    ],
    "Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) on Mauna Loa CO2 data->Interpretation of kernel hyperparameters": [
        [
            "Now, we can have a look at the hyperparameters of the kernel.",
            "markdown"
        ],
        [
            "gaussian_process.kernel_",
            "code"
        ],
        [
            "44.8**2 * RBF(length_scale=51.6) + 2.64**2 * RBF(length_scale=91.5) * ExpSineSquared(length_scale=1.48, periodicity=1) + 0.536**2 * RationalQuadratic(alpha=2.89, length_scale=0.968) + 0.188**2 * RBF(length_scale=0.122) + WhiteKernel(noise_level=0.0367)",
            "code"
        ],
        [
            "Thus, most of the target signal, with the mean subtracted, is explained by a\nlong-term rising trend for ~45 ppm and a length-scale of ~52 years. The\nperiodic component has an amplitude of ~2.6ppm, a decay time of ~90 years and\na length-scale of ~1.5. The long decay time indicates that we have a\ncomponent very close to a seasonal periodicity. The correlated noise has an\namplitude of ~0.2 ppm with a length scale of ~0.12 years and a white-noise\ncontribution of ~0.04 ppm. Thus, the overall noise level is very small,\nindicating that the data can be very well explained by the model.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  8.254 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) with noise-level estimation": [
        [
            "This example shows the ability of the\n to estimate the noise\nlevel in the data. Moreover, we show the importance of kernel hyperparameters\ninitialization.",
            "markdown"
        ],
        [
            "# Authors: Jan Hendrik Metzen &lt;jhm@informatik.uni-bremen.de\n#          Guillaume Lemaitre &lt;guillaume.lemaitre@inria.fr\n# License: BSD 3 clause",
            "code"
        ]
    ],
    "Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) with noise-level estimation->Data generation": [
        [
            "We will work in a setting where X will contain a single feature. We create a\nfunction that will generate the target to be predicted. We will add an\noption to add some noise to the generated target.",
            "markdown"
        ],
        [
            "import numpy as np\n\n\ndef target_generator(X, add_noise=False):\n    target = 0.5 + (3 * X)\n    if add_noise:\n        rng = (1)\n        target += rng.normal(0, 0.3, size=target.shape)\n    return target.squeeze()",
            "code"
        ],
        [
            "Let\u2019s have a look to the target generator where we will not add any noise to\nobserve the signal that we would like to predict.",
            "markdown"
        ],
        [
            "X = (0, 5, num=30).reshape(-1, 1)\ny = target_generator(X, add_noise=False)",
            "code"
        ],
        [
            "import matplotlib.pyplot as plt\n\n(X, y, label=\"Expected signal\")\n()\n(\"X\")\n_ = (\"y\")\n\n\n<img alt=\"plot gpr noisy\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_noisy_001.png\" srcset=\"../../_images/sphx_glr_plot_gpr_noisy_001.png\"/>",
            "code"
        ],
        [
            "The target is transforming the input X using a sine function. Now, we will\ngenerate few noisy training samples. To illustrate the noise level, we will\nplot the true signal together with the noisy training samples.",
            "markdown"
        ],
        [
            "rng = (0)\nX_train = rng.uniform(0, 5, size=20).reshape(-1, 1)\ny_train = target_generator(X_train, add_noise=True)",
            "code"
        ],
        [
            "(X, y, label=\"Expected signal\")\n(\n    x=X_train[:, 0],\n    y=y_train,\n    color=\"black\",\n    alpha=0.4,\n    label=\"Observations\",\n)\n()\n(\"X\")\n_ = (\"y\")\n\n\n<img alt=\"plot gpr noisy\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_noisy_002.png\" srcset=\"../../_images/sphx_glr_plot_gpr_noisy_002.png\"/>",
            "code"
        ]
    ],
    "Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) with noise-level estimation->Optimisation of kernel hyperparameters in GPR": [
        [
            "Now, we will create a\n\nusing an additive kernel adding a\n and\n kernels.\nThe  is a kernel that\nwill able to estimate the amount of noise present in the data while the\n will serve at fitting the\nnon-linearity between the data and the target.",
            "markdown"
        ],
        [
            "However, we will show that the hyperparameter space contains several local\nminima. It will highlights the importance of initial hyperparameter values.",
            "markdown"
        ],
        [
            "We will create a model using a kernel with a high noise level and a large\nlength scale, which will explain all variations in the data by noise.",
            "markdown"
        ],
        [
            "from sklearn.gaussian_process import \nfrom sklearn.gaussian_process.kernels import , \n\nkernel = 1.0 * (length_scale=1e1, length_scale_bounds=(1e-2, 1e3)) + (\n    noise_level=1, noise_level_bounds=(1e-5, 1e1)\n)\ngpr = (kernel=kernel, alpha=0.0)\ngpr.fit(X_train, y_train)\ny_mean, y_std = gpr.predict(X, return_std=True)",
            "code"
        ],
        [
            "/home/circleci/project/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning:\n\nThe optimal value found for dimension 0 of parameter k1__k2__length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.",
            "code"
        ],
        [
            "(X, y, label=\"Expected signal\")\n(x=X_train[:, 0], y=y_train, color=\"black\", alpha=0.4, label=\"Observations\")\n(X, y_mean, y_std)\n()\n(\"X\")\n(\"y\")\n_ = (\n    f\"Initial: {kernel}\\nOptimum: {gpr.kernel_}\\nLog-Marginal-Likelihood: \"\n    f\"{gpr.log_marginal_likelihood(gpr.kernel_.theta)}\",\n    fontsize=8,\n)\n\n\n<img alt=\"Initial: 1**2 * RBF(length_scale=10) + WhiteKernel(noise_level=1) Optimum: 0.763**2 * RBF(length_scale=1e+03) + WhiteKernel(noise_level=0.525) Log-Marginal-Likelihood: -23.499266455424188\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_noisy_003.png\" srcset=\"../../_images/sphx_glr_plot_gpr_noisy_003.png\"/>",
            "code"
        ],
        [
            "We see that the optimum kernel found still have a high noise level and\nan even larger length scale. Furthermore, we observe that the\nmodel does not provide faithful predictions.",
            "markdown"
        ],
        [
            "Now, we will initialize the\n with a\nlarger length_scale and the\n\nwith a smaller noise level lower bound.",
            "markdown"
        ],
        [
            "kernel = 1.0 * (length_scale=1e-1, length_scale_bounds=(1e-2, 1e3)) + (\n    noise_level=1e-2, noise_level_bounds=(1e-10, 1e1)\n)\ngpr = (kernel=kernel, alpha=0.0)\ngpr.fit(X_train, y_train)\ny_mean, y_std = gpr.predict(X, return_std=True)",
            "code"
        ],
        [
            "(X, y, label=\"Expected signal\")\n(x=X_train[:, 0], y=y_train, color=\"black\", alpha=0.4, label=\"Observations\")\n(X, y_mean, y_std)\n()\n(\"X\")\n(\"y\")\n_ = (\n    f\"Initial: {kernel}\\nOptimum: {gpr.kernel_}\\nLog-Marginal-Likelihood: \"\n    f\"{gpr.log_marginal_likelihood(gpr.kernel_.theta)}\",\n    fontsize=8,\n)\n\n\n<img alt=\"Initial: 1**2 * RBF(length_scale=0.1) + WhiteKernel(noise_level=0.01) Optimum: 1.05**2 * RBF(length_scale=0.569) + WhiteKernel(noise_level=0.134) Log-Marginal-Likelihood: -18.429732528984058\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_noisy_004.png\" srcset=\"../../_images/sphx_glr_plot_gpr_noisy_004.png\"/>",
            "code"
        ],
        [
            "First, we see that the model\u2019s predictions are more precise than the\nprevious model\u2019s: this new model is able to estimate the noise-free\nfunctional relationship.",
            "markdown"
        ],
        [
            "Looking at the kernel hyperparameters, we see that the best combination found\nhas a smaller noise level and shorter length scale than the first model.",
            "markdown"
        ],
        [
            "We can inspect the Log-Marginal-Likelihood (LML) of\n\nfor different hyperparameters to get a sense of the local minima.",
            "markdown"
        ],
        [
            "from matplotlib.colors import \n\nlength_scale = (-2, 4, num=50)\nnoise_level = (-2, 1, num=50)\nlength_scale_grid, noise_level_grid = (length_scale, noise_level)\n\nlog_marginal_likelihood = [\n    gpr.log_marginal_likelihood(theta=([0.36, scale, noise]))\n    for scale, noise in zip(length_scale_grid.ravel(), noise_level_grid.ravel())\n]\nlog_marginal_likelihood = (\n    log_marginal_likelihood, newshape=noise_level_grid.shape\n)",
            "code"
        ],
        [
            "vmin, vmax = (-log_marginal_likelihood).min(), 50\nlevel = (((vmin), (vmax), num=50), decimals=1)\n(\n    length_scale_grid,\n    noise_level_grid,\n    -log_marginal_likelihood,\n    levels=level,\n    norm=(vmin=vmin, vmax=vmax),\n)\n()\n(\"log\")\n(\"log\")\n(\"Length-scale\")\n(\"Noise-level\")\n(\"Log-marginal-likelihood\")\n()\n\n\n<img alt=\"Log-marginal-likelihood\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_noisy_005.png\" srcset=\"../../_images/sphx_glr_plot_gpr_noisy_005.png\"/>",
            "code"
        ],
        [
            "We see that there are two local minima that correspond to the combination\nof hyperparameters previously found. Depending on the initial values for the\nhyperparameters, the gradient-based optimization might converge whether or\nnot to the best model. It is thus important to repeat the optimization\nseveral times for different initializations.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  2.915 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Gaussian Process for Machine Learning->Gaussian processes on discrete data structures": [
        [
            "This example illustrates the use of Gaussian processes for regression and\nclassification tasks on data that are not in fixed-length feature vector form.\nThis is achieved through the use of kernel functions that operates directly\non discrete structures such as variable-length sequences, trees, and graphs.",
            "markdown"
        ],
        [
            "Specifically, here the input variables are some gene sequences stored as\nvariable-length strings consisting of letters \u2018A\u2019, \u2018T\u2019, \u2018C\u2019, and \u2018G\u2019,\nwhile the output variables are floating point numbers and True/False labels\nin the regression and classification tasks, respectively.",
            "markdown"
        ],
        [
            "A kernel between the gene sequences is defined using R-convolution  by\nintegrating a binary letter-wise kernel over all pairs of letters among a pair\nof strings.",
            "markdown"
        ],
        [
            "This example will generate three figures.",
            "markdown"
        ],
        [
            "In the first figure, we visualize the value of the kernel, i.e. the similarity\nof the sequences, using a colormap. Brighter color here indicates higher\nsimilarity.",
            "markdown"
        ],
        [
            "In the second figure, we show some regression result on a dataset of 6\nsequences. Here we use the 1st, 2nd, 4th, and 5th sequences as the training set\nto make predictions on the 3rd and 6th sequences.",
            "markdown"
        ],
        [
            "In the third figure, we demonstrate a classification model by training on 6\nsequences and make predictions on another 5 sequences. The ground truth here is\nsimply  whether there is at least one \u2018A\u2019 in the sequence. Here the model makes\nfour correct classifications and fails on one.\n\n\n[]",
            "markdown"
        ],
        [
            "Haussler, D. (1999). Convolution kernels on discrete structures\n(Vol. 646). Technical report, Department of Computer Science, University\nof California at Santa Cruz.",
            "markdown"
        ],
        [
            "import numpy as np\nfrom sklearn.gaussian_process.kernels import , \nfrom sklearn.gaussian_process.kernels import GenericKernelMixin\nfrom sklearn.gaussian_process import \nfrom sklearn.gaussian_process import \nfrom sklearn.base import clone\n\n\nclass SequenceKernel(GenericKernelMixin, ):\n    \"\"\"\n    A minimal (but valid) convolutional kernel for sequences of variable\n    lengths.\"\"\"\n\n    def __init__(self, baseline_similarity=0.5, baseline_similarity_bounds=(1e-5, 1)):\n        self.baseline_similarity = baseline_similarity\n        self.baseline_similarity_bounds = baseline_similarity_bounds\n\n    @property\n    def hyperparameter_baseline_similarity(self):\n        return (\n            \"baseline_similarity\", \"numeric\", self.baseline_similarity_bounds\n        )\n\n    def _f(self, s1, s2):\n        \"\"\"\n        kernel value between a pair of sequences\n        \"\"\"\n        return sum(\n            [1.0 if c1 == c2 else self.baseline_similarity for c1 in s1 for c2 in s2]\n        )\n\n    def _g(self, s1, s2):\n        \"\"\"\n        kernel derivative between a pair of sequences\n        \"\"\"\n        return sum([0.0 if c1 == c2 else 1.0 for c1 in s1 for c2 in s2])\n\n    def __call__(self, X, Y=None, eval_gradient=False):\n        if Y is None:\n            Y = X\n\n        if eval_gradient:\n            return (\n                ([[self._f(x, y) for y in Y] for x in X]),\n                ([[[self._g(x, y)] for y in Y] for x in X]),\n            )\n        else:\n            return ([[self._f(x, y) for y in Y] for x in X])\n\n    def diag(self, X):\n        return ([self._f(x, x) for x in X])\n\n    def is_stationary(self):\n        return False\n\n    def clone_with_theta(self, theta):\n        cloned = clone(self)\n        cloned.theta = theta\n        return cloned\n\n\nkernel = SequenceKernel()",
            "code"
        ]
    ],
    "Examples->Gaussian Process for Machine Learning->Gaussian processes on discrete data structures->Sequence similarity matrix under the kernel": [
        [
            "import matplotlib.pyplot as plt\n\nX = ([\"AGCT\", \"AGC\", \"AACT\", \"TAA\", \"AAA\", \"GAACA\"])\n\nK = kernel(X)\nD = kernel.diag(X)\n\n(figsize=(8, 5))\n((D**-0.5).dot(K).dot((D**-0.5)))\n((len(X)), X)\n((len(X)), X)\n(\"Sequence similarity under the kernel\")\n()\n\n\n<img alt=\"Sequence similarity under the kernel\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_on_structured_data_001.png\" srcset=\"../../_images/sphx_glr_plot_gpr_on_structured_data_001.png\"/>",
            "code"
        ]
    ],
    "Examples->Gaussian Process for Machine Learning->Gaussian processes on discrete data structures->Regression": [
        [
            "X = ([\"AGCT\", \"AGC\", \"AACT\", \"TAA\", \"AAA\", \"GAACA\"])\nY = ([1.0, 1.0, 2.0, 2.0, 3.0, 3.0])\n\ntraining_idx = [0, 1, 3, 4]\ngp = (kernel=kernel)\ngp.fit(X[training_idx], Y[training_idx])\n\n(figsize=(8, 5))\n((len(X)), gp.predict(X), color=\"b\", label=\"prediction\")\n(training_idx, Y[training_idx], width=0.2, color=\"r\", alpha=1, label=\"training\")\n((len(X)), X)\n(\"Regression on sequences\")\n()\n()\n\n\n<img alt=\"Regression on sequences\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_on_structured_data_002.png\" srcset=\"../../_images/sphx_glr_plot_gpr_on_structured_data_002.png\"/>",
            "code"
        ]
    ],
    "Examples->Gaussian Process for Machine Learning->Gaussian processes on discrete data structures->Classification": [
        [
            "X_train = ([\"AGCT\", \"CGA\", \"TAAC\", \"TCG\", \"CTTT\", \"TGCT\"])\n# whether there are 'A's in the sequence\nY_train = ([True, True, True, False, False, False])\n\ngp = (kernel)\ngp.fit(X_train, Y_train)\n\nX_test = [\"AAA\", \"ATAG\", \"CTC\", \"CT\", \"C\"]\nY_test = [True, True, False, False, False]\n\n(figsize=(8, 5))\n(\n    (len(X_train)),\n    [1.0 if c else -1.0 for c in Y_train],\n    s=100,\n    marker=\"o\",\n    edgecolor=\"none\",\n    facecolor=(1, 0.75, 0),\n    label=\"training\",\n)\n(\n    len(X_train) + (len(X_test)),\n    [1.0 if c else -1.0 for c in Y_test],\n    s=100,\n    marker=\"o\",\n    edgecolor=\"none\",\n    facecolor=\"r\",\n    label=\"truth\",\n)\n(\n    len(X_train) + (len(X_test)),\n    [1.0 if c else -1.0 for c in gp.predict(X_test)],\n    s=100,\n    marker=\"x\",\n    facecolor=\"b\",\n    linewidth=2,\n    label=\"prediction\",\n)\n((len(X_train) + len(X_test)), ((X_train, X_test)))\n([-1, 1], [False, True])\n(\"Classification on sequences\")\n()\n()\n\n\n<img alt=\"Classification on sequences\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_on_structured_data_003.png\" srcset=\"../../_images/sphx_glr_plot_gpr_on_structured_data_003.png\"/>",
            "code"
        ],
        [
            "/home/circleci/project/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning:\n\nThe optimal value found for dimension 0 of parameter baseline_similarity is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.236 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Gaussian Process for Machine Learning->Illustration of Gaussian process classification (GPC) on the XOR dataset": [
        [
            "This example illustrates GPC on XOR data. Compared are a stationary, isotropic\nkernel (RBF) and a non-stationary kernel (DotProduct). On this particular\ndataset, the DotProduct kernel obtains considerably better results because the\nclass-boundaries are linear and coincide with the coordinate axes. In general,\nstationary kernels often obtain better results.\n<img alt=\"302**2 * RBF(length_scale=1.55)  Log-Marginal-Likelihood:-24.237, 316**2 * DotProduct(sigma_0=0.0104) ** 2  Log-Marginal-Likelihood:-9.284\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpc_xor_001.png\" srcset=\"../../_images/sphx_glr_plot_gpc_xor_001.png\"/>",
            "markdown"
        ],
        [
            "/home/circleci/project/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning:\n\nThe optimal value found for dimension 0 of parameter k1__constant_value is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n\n\n\n<br/>",
            "code"
        ],
        [
            "# Authors: Jan Hendrik Metzen &lt;jhm@informatik.uni-bremen.de\n#\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.gaussian_process import \nfrom sklearn.gaussian_process.kernels import , \n\n\nxx, yy = ((-3, 3, 50), (-3, 3, 50))\nrng = (0)\nX = rng.randn(200, 2)\nY = (X[:, 0]  0, X[:, 1]  0)\n\n# fit the model\n(figsize=(10, 5))\nkernels = [1.0 * (length_scale=1.15), 1.0 * (sigma_0=1.0) ** 2]\nfor i, kernel in enumerate(kernels):\n    clf = (kernel=kernel, warm_start=True).fit(X, Y)\n\n    # plot the decision function for each datapoint on the grid\n    Z = clf.predict_proba(((xx.ravel(), yy.ravel())).T)[:, 1]\n    Z = Z.reshape(xx.shape)\n\n    (1, 2, i + 1)\n    image = (\n        Z,\n        interpolation=\"nearest\",\n        extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n        aspect=\"auto\",\n        origin=\"lower\",\n        cmap=plt.cm.PuOr_r,\n    )\n    contours = (xx, yy, Z, levels=[0.5], linewidths=2, colors=[\"k\"])\n    (X[:, 0], X[:, 1], s=30, c=Y, cmap=plt.cm.Paired, edgecolors=(0, 0, 0))\n    (())\n    (())\n    ([-3, 3, -3, 3])\n    (image)\n    (\n        \"%s\\n Log-Marginal-Likelihood:%.3f\"\n        % (clf.kernel_, clf.log_marginal_likelihood(clf.kernel_.theta)),\n        fontsize=12,\n    )\n\n()\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.439 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Gaussian Process for Machine Learning->Illustration of prior and posterior Gaussian process for different kernels": [
        [
            "This example illustrates the prior and posterior of a\n with different\nkernels. Mean, standard deviation, and 5 samples are shown for both prior\nand posterior distributions.",
            "markdown"
        ],
        [
            "Here, we only give some illustration. To know more about kernels\u2019 formulation,\nrefer to the .",
            "markdown"
        ],
        [
            "# Authors: Jan Hendrik Metzen &lt;jhm@informatik.uni-bremen.de\n#          Guillaume Lemaitre &lt;g.lemaitre58@gmail.com\n# License: BSD 3 clause",
            "code"
        ]
    ],
    "Examples->Gaussian Process for Machine Learning->Illustration of prior and posterior Gaussian process for different kernels->Helper function": [
        [
            "Before presenting each individual kernel available for Gaussian processes,\nwe will define an helper function allowing us plotting samples drawn from\nthe Gaussian process.",
            "markdown"
        ],
        [
            "This function will take a\n model and will\ndrawn sample from the Gaussian process. If the model was not fit, the samples\nare drawn from the prior distribution while after model fitting, the samples are\ndrawn from the posterior distribution.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef plot_gpr_samples(gpr_model, n_samples, ax):\n    \"\"\"Plot samples drawn from the Gaussian process model.\n\n    If the Gaussian process model is not trained then the drawn samples are\n    drawn from the prior distribution. Otherwise, the samples are drawn from\n    the posterior distribution. Be aware that a sample here corresponds to a\n    function.\n\n    Parameters\n    ----------\n    gpr_model : `GaussianProcessRegressor`\n        A :class:`~sklearn.gaussian_process.GaussianProcessRegressor` model.\n    n_samples : int\n        The number of samples to draw from the Gaussian process distribution.\n    ax : matplotlib axis\n        The matplotlib axis where to plot the samples.\n    \"\"\"\n    x = (0, 5, 100)\n    X = x.reshape(-1, 1)\n\n    y_mean, y_std = gpr_model.predict(X, return_std=True)\n    y_samples = gpr_model.sample_y(X, n_samples)\n\n    for idx, single_prior in enumerate(y_samples.T):\n        ax.plot(\n            x,\n            single_prior,\n            linestyle=\"--\",\n            alpha=0.7,\n            label=f\"Sampled function #{idx + 1}\",\n        )\n    ax.plot(x, y_mean, color=\"black\", label=\"Mean\")\n    ax.fill_between(\n        x,\n        y_mean - y_std,\n        y_mean + y_std,\n        alpha=0.1,\n        color=\"black\",\n        label=r\"$\\pm$ 1 std. dev.\",\n    )\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"y\")\n    ax.set_ylim([-3, 3])",
            "code"
        ]
    ],
    "Examples->Gaussian Process for Machine Learning->Illustration of prior and posterior Gaussian process for different kernels->Dataset and Gaussian process generation": [
        [
            "We will create a training dataset that we will use in the different sections.",
            "markdown"
        ],
        [
            "rng = (4)\nX_train = rng.uniform(0, 5, 10).reshape(-1, 1)\ny_train = ((X_train[:, 0] - 2.5) ** 2)\nn_samples = 5",
            "code"
        ]
    ],
    "Examples->Gaussian Process for Machine Learning->Illustration of prior and posterior Gaussian process for different kernels->Kernel cookbook": [
        [
            "In this section, we illustrate some samples drawn from the prior and posterior\ndistributions of the Gaussian process with different kernels.",
            "markdown"
        ]
    ],
    "Examples->Gaussian Process for Machine Learning->Illustration of prior and posterior Gaussian process for different kernels->Kernel cookbook->Radial Basis Function kernel": [
        [
            "from sklearn.gaussian_process import \nfrom sklearn.gaussian_process.kernels import \n\nkernel = 1.0 * (length_scale=1.0, length_scale_bounds=(1e-1, 10.0))\ngpr = (kernel=kernel, random_state=0)\n\nfig, axs = (nrows=2, sharex=True, sharey=True, figsize=(10, 8))\n\n# plot prior\nplot_gpr_samples(gpr, n_samples=n_samples, ax=axs[0])\naxs[0].set_title(\"Samples from prior distribution\")\n\n# plot posterior\ngpr.fit(X_train, y_train)\nplot_gpr_samples(gpr, n_samples=n_samples, ax=axs[1])\naxs[1].scatter(X_train[:, 0], y_train, color=\"red\", zorder=10, label=\"Observations\")\naxs[1].legend(bbox_to_anchor=(1.05, 1.5), loc=\"upper left\")\naxs[1].set_title(\"Samples from posterior distribution\")\n\nfig.suptitle(\"Radial Basis Function kernel\", fontsize=18)\n()\n\n\n<img alt=\"Radial Basis Function kernel, Samples from prior distribution, Samples from posterior distribution\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_prior_posterior_001.png\" srcset=\"../../_images/sphx_glr_plot_gpr_prior_posterior_001.png\"/>",
            "code"
        ],
        [
            "print(f\"Kernel parameters before fit:\\n{kernel})\")\nprint(\n    f\"Kernel parameters after fit: \\n{gpr.kernel_} \\n\"\n    f\"Log-likelihood: {gpr.log_marginal_likelihood(gpr.kernel_.theta):.3f}\"\n)",
            "code"
        ],
        [
            "Kernel parameters before fit:\n1**2 * RBF(length_scale=1))\nKernel parameters after fit:\n0.594**2 * RBF(length_scale=0.279)\nLog-likelihood: -0.067",
            "code"
        ]
    ],
    "Examples->Gaussian Process for Machine Learning->Illustration of prior and posterior Gaussian process for different kernels->Kernel cookbook->Rational Quadradtic kernel": [
        [
            "from sklearn.gaussian_process.kernels import \n\nkernel = 1.0 * (length_scale=1.0, alpha=0.1, alpha_bounds=(1e-5, 1e15))\ngpr = (kernel=kernel, random_state=0)\n\nfig, axs = (nrows=2, sharex=True, sharey=True, figsize=(10, 8))\n\n# plot prior\nplot_gpr_samples(gpr, n_samples=n_samples, ax=axs[0])\naxs[0].set_title(\"Samples from prior distribution\")\n\n# plot posterior\ngpr.fit(X_train, y_train)\nplot_gpr_samples(gpr, n_samples=n_samples, ax=axs[1])\naxs[1].scatter(X_train[:, 0], y_train, color=\"red\", zorder=10, label=\"Observations\")\naxs[1].legend(bbox_to_anchor=(1.05, 1.5), loc=\"upper left\")\naxs[1].set_title(\"Samples from posterior distribution\")\n\nfig.suptitle(\"Rational Quadratic kernel\", fontsize=18)\n()\n\n\n<img alt=\"Rational Quadratic kernel, Samples from prior distribution, Samples from posterior distribution\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_prior_posterior_002.png\" srcset=\"../../_images/sphx_glr_plot_gpr_prior_posterior_002.png\"/>",
            "code"
        ],
        [
            "/home/circleci/project/sklearn/gaussian_process/_gpr.py:445: UserWarning:\n\nPredicted variances smaller than 0. Setting those variances to 0.\n\n/home/circleci/project/sklearn/gaussian_process/_gpr.py:492: RuntimeWarning:\n\ncovariance is not symmetric positive-semidefinite.",
            "code"
        ],
        [
            "print(f\"Kernel parameters before fit:\\n{kernel})\")\nprint(\n    f\"Kernel parameters after fit: \\n{gpr.kernel_} \\n\"\n    f\"Log-likelihood: {gpr.log_marginal_likelihood(gpr.kernel_.theta):.3f}\"\n)",
            "code"
        ],
        [
            "Kernel parameters before fit:\n1**2 * RationalQuadratic(alpha=0.1, length_scale=1))\nKernel parameters after fit:\n0.594**2 * RationalQuadratic(alpha=6.21e+10, length_scale=0.279)\nLog-likelihood: -0.008",
            "code"
        ]
    ],
    "Examples->Gaussian Process for Machine Learning->Illustration of prior and posterior Gaussian process for different kernels->Kernel cookbook->Exp-Sine-Squared kernel": [
        [
            "from sklearn.gaussian_process.kernels import \n\nkernel = 1.0 * (\n    length_scale=1.0,\n    periodicity=3.0,\n    length_scale_bounds=(0.1, 10.0),\n    periodicity_bounds=(1.0, 10.0),\n)\ngpr = (kernel=kernel, random_state=0)\n\nfig, axs = (nrows=2, sharex=True, sharey=True, figsize=(10, 8))\n\n# plot prior\nplot_gpr_samples(gpr, n_samples=n_samples, ax=axs[0])\naxs[0].set_title(\"Samples from prior distribution\")\n\n# plot posterior\ngpr.fit(X_train, y_train)\nplot_gpr_samples(gpr, n_samples=n_samples, ax=axs[1])\naxs[1].scatter(X_train[:, 0], y_train, color=\"red\", zorder=10, label=\"Observations\")\naxs[1].legend(bbox_to_anchor=(1.05, 1.5), loc=\"upper left\")\naxs[1].set_title(\"Samples from posterior distribution\")\n\nfig.suptitle(\"Exp-Sine-Squared kernel\", fontsize=18)\n()\n\n\n<img alt=\"Exp-Sine-Squared kernel, Samples from prior distribution, Samples from posterior distribution\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_prior_posterior_003.png\" srcset=\"../../_images/sphx_glr_plot_gpr_prior_posterior_003.png\"/>",
            "code"
        ],
        [
            "print(f\"Kernel parameters before fit:\\n{kernel})\")\nprint(\n    f\"Kernel parameters after fit: \\n{gpr.kernel_} \\n\"\n    f\"Log-likelihood: {gpr.log_marginal_likelihood(gpr.kernel_.theta):.3f}\"\n)",
            "code"
        ],
        [
            "Kernel parameters before fit:\n1**2 * ExpSineSquared(length_scale=1, periodicity=3))\nKernel parameters after fit:\n0.799**2 * ExpSineSquared(length_scale=0.791, periodicity=2.87)\nLog-likelihood: 3.394",
            "code"
        ]
    ],
    "Examples->Gaussian Process for Machine Learning->Illustration of prior and posterior Gaussian process for different kernels->Kernel cookbook->Dot-product kernel": [
        [
            "from sklearn.gaussian_process.kernels import , \n\nkernel = (0.1, (0.01, 10.0)) * (\n    (sigma_0=1.0, sigma_0_bounds=(0.1, 10.0)) ** 2\n)\ngpr = (kernel=kernel, random_state=0)\n\nfig, axs = (nrows=2, sharex=True, sharey=True, figsize=(10, 8))\n\n# plot prior\nplot_gpr_samples(gpr, n_samples=n_samples, ax=axs[0])\naxs[0].set_title(\"Samples from prior distribution\")\n\n# plot posterior\ngpr.fit(X_train, y_train)\nplot_gpr_samples(gpr, n_samples=n_samples, ax=axs[1])\naxs[1].scatter(X_train[:, 0], y_train, color=\"red\", zorder=10, label=\"Observations\")\naxs[1].legend(bbox_to_anchor=(1.05, 1.5), loc=\"upper left\")\naxs[1].set_title(\"Samples from posterior distribution\")\n\nfig.suptitle(\"Dot-product kernel\", fontsize=18)\n()\n\n\n<img alt=\"Dot-product kernel, Samples from prior distribution, Samples from posterior distribution\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_prior_posterior_004.png\" srcset=\"../../_images/sphx_glr_plot_gpr_prior_posterior_004.png\"/>",
            "code"
        ],
        [
            "/home/circleci/project/sklearn/gaussian_process/_gpr.py:629: ConvergenceWarning:\n\nlbfgs failed to converge (status=2):\nABNORMAL_TERMINATION_IN_LNSRCH.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n\n/home/circleci/project/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning:\n\nThe optimal value found for dimension 0 of parameter k1__constant_value is close to the specified upper bound 10.0. Increasing the bound and calling fit again may find a better value.\n\n/home/circleci/project/sklearn/gaussian_process/_gpr.py:445: UserWarning:\n\nPredicted variances smaller than 0. Setting those variances to 0.",
            "code"
        ],
        [
            "print(f\"Kernel parameters before fit:\\n{kernel})\")\nprint(\n    f\"Kernel parameters after fit: \\n{gpr.kernel_} \\n\"\n    f\"Log-likelihood: {gpr.log_marginal_likelihood(gpr.kernel_.theta):.3f}\"\n)",
            "code"
        ],
        [
            "Kernel parameters before fit:\n0.316**2 * DotProduct(sigma_0=1) ** 2)\nKernel parameters after fit:\n3.16**2 * DotProduct(sigma_0=9.61) ** 2\nLog-likelihood: -6576930005.798",
            "code"
        ]
    ],
    "Examples->Gaussian Process for Machine Learning->Illustration of prior and posterior Gaussian process for different kernels->Kernel cookbook->Mat\u00e9rn kernel": [
        [
            "from sklearn.gaussian_process.kernels import \n\nkernel = 1.0 * (length_scale=1.0, length_scale_bounds=(1e-1, 10.0), nu=1.5)\ngpr = (kernel=kernel, random_state=0)\n\nfig, axs = (nrows=2, sharex=True, sharey=True, figsize=(10, 8))\n\n# plot prior\nplot_gpr_samples(gpr, n_samples=n_samples, ax=axs[0])\naxs[0].set_title(\"Samples from prior distribution\")\n\n# plot posterior\ngpr.fit(X_train, y_train)\nplot_gpr_samples(gpr, n_samples=n_samples, ax=axs[1])\naxs[1].scatter(X_train[:, 0], y_train, color=\"red\", zorder=10, label=\"Observations\")\naxs[1].legend(bbox_to_anchor=(1.05, 1.5), loc=\"upper left\")\naxs[1].set_title(\"Samples from posterior distribution\")\n\nfig.suptitle(\"Mat\u00e9rn kernel\", fontsize=18)\n()\n\n\n<img alt=\"Mat\u00e9rn kernel, Samples from prior distribution, Samples from posterior distribution\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_prior_posterior_005.png\" srcset=\"../../_images/sphx_glr_plot_gpr_prior_posterior_005.png\"/>",
            "code"
        ],
        [
            "print(f\"Kernel parameters before fit:\\n{kernel})\")\nprint(\n    f\"Kernel parameters after fit: \\n{gpr.kernel_} \\n\"\n    f\"Log-likelihood: {gpr.log_marginal_likelihood(gpr.kernel_.theta):.3f}\"\n)",
            "code"
        ],
        [
            "Kernel parameters before fit:\n1**2 * Matern(length_scale=1, nu=1.5))\nKernel parameters after fit:\n0.609**2 * Matern(length_scale=0.484, nu=1.5)\nLog-likelihood: -1.185",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  1.274 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Gaussian Process for Machine Learning->Iso-probability lines for Gaussian Processes classification (GPC)": [
        [
            "A two-dimensional classification example showing iso-probability lines for\nthe predicted probabilities.\n<img alt=\"plot gpc isoprobability\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpc_isoprobability_001.png\" srcset=\"../../_images/sphx_glr_plot_gpc_isoprobability_001.png\"/>",
            "markdown"
        ],
        [
            "Learned kernel: 0.0256**2 * DotProduct(sigma_0=5.72) ** 2\n\n\n\n<br/>",
            "code"
        ],
        [
            "# Author: Vincent Dubourg &lt;vincent.dubourg@gmail.com\n# Adapted to GaussianProcessClassifier:\n#         Jan Hendrik Metzen &lt;jhm@informatik.uni-bremen.de\n# License: BSD 3 clause\n\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib import cm\n\nfrom sklearn.gaussian_process import \nfrom sklearn.gaussian_process.kernels import , ConstantKernel as \n\n# A few constants\nlim = 8\n\n\ndef g(x):\n    \"\"\"The function to predict (classification will then consist in predicting\n    whether g(x) &lt;= 0 or not)\"\"\"\n    return 5.0 - x[:, 1] - 0.5 * x[:, 0] ** 2.0\n\n\n# Design of experiments\nX = (\n    [\n        [-4.61611719, -6.00099547],\n        [4.10469096, 5.32782448],\n        [0.00000000, -0.50000000],\n        [-6.17289014, -4.6984743],\n        [1.3109306, -6.93271427],\n        [-5.03823144, 3.10584743],\n        [-2.87600388, 6.74310541],\n        [5.21301203, 4.26386883],\n    ]\n)\n\n# Observations\ny = (g(X)  0, dtype=int)\n\n# Instantiate and fit Gaussian Process Model\nkernel = (0.1, (1e-5, )) * (sigma_0=0.1) ** 2\ngp = (kernel=kernel)\ngp.fit(X, y)\nprint(\"Learned kernel: %s \" % gp.kernel_)\n\n# Evaluate real function and the predicted probability\nres = 50\nx1, x2 = ((-lim, lim, res), (-lim, lim, res))\nxx = ([x1.reshape(x1.size), x2.reshape(x2.size)]).T\n\ny_true = g(xx)\ny_prob = gp.predict_proba(xx)[:, 1]\ny_true = y_true.reshape((res, res))\ny_prob = y_prob.reshape((res, res))\n\n# Plot the probabilistic classification iso-values\nfig = (1)\nax = fig.gca()\nax.axes.set_aspect(\"equal\")\n([])\n([])\nax.set_xticklabels([])\nax.set_yticklabels([])\n(\"$x_1$\")\n(\"$x_2$\")\n\ncax = (y_prob, cmap=cm.gray_r, alpha=0.8, extent=(-lim, lim, -lim, lim))\nnorm = plt.matplotlib.colors.Normalize(vmin=0.0, vmax=0.9)\ncb = (cax, ticks=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0], norm=norm)\ncb.set_label(r\"${\\rm \\mathbb{P}}\\left[\\widehat{G}(\\mathbf{x}) \\leq 0\\right]$\")\n(0, 1)\n\n(X[y &lt;= 0, 0], X[y &lt;= 0, 1], \"r.\", markersize=12)\n\n(X[y  0, 0], X[y  0, 1], \"b.\", markersize=12)\n\n(x1, x2, y_true, [0.0], colors=\"k\", linestyles=\"dashdot\")\n\ncs = (x1, x2, y_prob, [0.666], colors=\"b\", linestyles=\"solid\")\n(cs, fontsize=11)\n\ncs = (x1, x2, y_prob, [0.5], colors=\"k\", linestyles=\"dashed\")\n(cs, fontsize=11)\n\ncs = (x1, x2, y_prob, [0.334], colors=\"r\", linestyles=\"solid\")\n(cs, fontsize=11)\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.146 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Gaussian Process for Machine Learning->Probabilistic predictions with Gaussian process classification (GPC)": [
        [
            "This example illustrates the predicted probability of GPC for an RBF kernel\nwith different choices of the hyperparameters. The first figure shows the\npredicted probability of GPC with arbitrarily chosen hyperparameters and with\nthe hyperparameters corresponding to the maximum log-marginal-likelihood (LML).",
            "markdown"
        ],
        [
            "While the hyperparameters chosen by optimizing LML have a considerable larger\nLML, they perform slightly worse according to the log-loss on test data. The\nfigure shows that this is because they exhibit a steep change of the class\nprobabilities at the class boundaries (which is good) but have predicted\nprobabilities close to 0.5 far away from the class boundaries (which is bad)\nThis undesirable effect is caused by the Laplace approximation used\ninternally by GPC.",
            "markdown"
        ],
        [
            "The second figure shows the log-marginal-likelihood for different choices of\nthe kernel\u2019s hyperparameters, highlighting the two choices of the\nhyperparameters used in the first figure by black dots.\n\n<img alt=\"plot gpc\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_gpc_001.png\" srcset=\"../../_images/sphx_glr_plot_gpc_001.png\"/>\n<img alt=\"Log-marginal-likelihood\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_gpc_002.png\" srcset=\"../../_images/sphx_glr_plot_gpc_002.png\"/>",
            "markdown"
        ],
        [
            "Log Marginal Likelihood (initial): -17.598\nLog Marginal Likelihood (optimized): -3.875\nAccuracy: 1.000 (initial) 1.000 (optimized)\nLog-loss: 0.214 (initial) 0.319 (optimized)\n\n\n\n<br/>",
            "code"
        ],
        [
            "# Authors: Jan Hendrik Metzen &lt;jhm@informatik.uni-bremen.de\n#\n# License: BSD 3 clause\n\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.metrics import , \nfrom sklearn.gaussian_process import \nfrom sklearn.gaussian_process.kernels import \n\n\n# Generate data\ntrain_size = 50\nrng = (0)\nX = rng.uniform(0, 5, 100)[:, ]\ny = (X[:, 0]  2.5, dtype=int)\n\n# Specify Gaussian Processes with fixed and optimized hyperparameters\ngp_fix = (kernel=1.0 * (length_scale=1.0), optimizer=None)\ngp_fix.fit(X[:train_size], y[:train_size])\n\ngp_opt = (kernel=1.0 * (length_scale=1.0))\ngp_opt.fit(X[:train_size], y[:train_size])\n\nprint(\n    \"Log Marginal Likelihood (initial): %.3f\"\n    % gp_fix.log_marginal_likelihood(gp_fix.kernel_.theta)\n)\nprint(\n    \"Log Marginal Likelihood (optimized): %.3f\"\n    % gp_opt.log_marginal_likelihood(gp_opt.kernel_.theta)\n)\n\nprint(\n    \"Accuracy: %.3f (initial) %.3f (optimized)\"\n    % (\n        (y[:train_size], gp_fix.predict(X[:train_size])),\n        (y[:train_size], gp_opt.predict(X[:train_size])),\n    )\n)\nprint(\n    \"Log-loss: %.3f (initial) %.3f (optimized)\"\n    % (\n        (y[:train_size], gp_fix.predict_proba(X[:train_size])[:, 1]),\n        (y[:train_size], gp_opt.predict_proba(X[:train_size])[:, 1]),\n    )\n)\n\n\n# Plot posteriors\n()\n(\n    X[:train_size, 0], y[:train_size], c=\"k\", label=\"Train data\", edgecolors=(0, 0, 0)\n)\n(\n    X[train_size:, 0], y[train_size:], c=\"g\", label=\"Test data\", edgecolors=(0, 0, 0)\n)\nX_ = (0, 5, 100)\n(\n    X_,\n    gp_fix.predict_proba(X_[:, ])[:, 1],\n    \"r\",\n    label=\"Initial kernel: %s\" % gp_fix.kernel_,\n)\n(\n    X_,\n    gp_opt.predict_proba(X_[:, ])[:, 1],\n    \"b\",\n    label=\"Optimized kernel: %s\" % gp_opt.kernel_,\n)\n(\"Feature\")\n(\"Class 1 probability\")\n(0, 5)\n(-0.25, 1.5)\n(loc=\"best\")\n\n# Plot LML landscape\n()\ntheta0 = (0, 8, 30)\ntheta1 = (-1, 1, 29)\nTheta0, Theta1 = (theta0, theta1)\nLML = [\n    [\n        gp_opt.log_marginal_likelihood(([Theta0[i, j], Theta1[i, j]]))\n        for i in range(Theta0.shape[0])\n    ]\n    for j in range(Theta0.shape[1])\n]\nLML = (LML).T\n(\n    (gp_fix.kernel_.theta)[0], (gp_fix.kernel_.theta)[1], \"ko\", zorder=10\n)\n(\n    (gp_opt.kernel_.theta)[0], (gp_opt.kernel_.theta)[1], \"ko\", zorder=10\n)\n(Theta0, Theta1, LML)\n(\"log\")\n(\"log\")\n()\n(\"Magnitude\")\n(\"Length-scale\")\n(\"Log-marginal-likelihood\")\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  2.138 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Comparing Linear Bayesian Regressors": [
        [
            "This example compares two different bayesian regressors:",
            "markdown"
        ],
        [
            "a",
            "markdown"
        ],
        [
            "a \n\n</blockquote>",
            "markdown"
        ],
        [
            "In the first part, we use an  (OLS) model as a\nbaseline for comparing the models\u2019 coefficients with respect to the true\ncoefficients. Thereafter, we show that the estimation of such models is done by\niteratively maximizing the marginal log-likelihood of the observations.",
            "markdown"
        ],
        [
            "In the last section we plot predictions and uncertainties for the ARD and the\nBayesian Ridge regressions using a polynomial feature expansion to fit a\nnon-linear relationship between X and y.",
            "markdown"
        ],
        [
            "# Author: Arturo Amor &lt;david-arturo.amor-quiroz@inria.fr",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Comparing Linear Bayesian Regressors->Models robustness to recover the ground truth weights->Generate synthetic dataset": [
        [
            "We generate a dataset where X and y are linearly linked: 10 of the\nfeatures of X will be used to generate y. The other features are not\nuseful at predicting y. In addition, we generate a dataset where n_samples\n== n_features. Such a setting is challenging for an OLS model and leads\npotentially to arbitrary large weights. Having a prior on the weights and a\npenalty alleviates the problem. Finally, gaussian noise is added.",
            "markdown"
        ],
        [
            "from sklearn.datasets import \n\nX, y, true_weights = (\n    n_samples=100,\n    n_features=100,\n    n_informative=10,\n    noise=8,\n    coef=True,\n    random_state=42,\n)",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Comparing Linear Bayesian Regressors->Models robustness to recover the ground truth weights->Fit the regressors": [
        [
            "We now fit both Bayesian models and the OLS to later compare the models\u2019\ncoefficients.",
            "markdown"
        ],
        [
            "import pandas as pd\nfrom sklearn.linear_model import , , \n\nolr = ().fit(X, y)\nbrr = (compute_score=True, n_iter=30).fit(X, y)\nard = (compute_score=True, n_iter=30).fit(X, y)\ndf = (\n    {\n        \"Weights of true generative process\": true_weights,\n        \"ARDRegression\": ard.coef_,\n        \"BayesianRidge\": brr.coef_,\n        \"LinearRegression\": olr.coef_,\n    }\n)",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Comparing Linear Bayesian Regressors->Models robustness to recover the ground truth weights->Plot the true and estimated coefficients": [
        [
            "Now we compare the coefficients of each model with the weights of\nthe true generative model.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.colors import \n\n(figsize=(10, 6))\nax = (\n    df.T,\n    norm=(linthresh=10e-4, vmin=-80, vmax=80),\n    cbar_kws={\"label\": \"coefficients' values\"},\n    cmap=\"seismic_r\",\n)\n(\"linear model\")\n(\"coefficients\")\n(rect=(0, 0, 1, 0.95))\n_ = (\"Models' coefficients\")\n\n\n<img alt=\"Models' coefficients\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_ard_001.png\" srcset=\"../../_images/sphx_glr_plot_ard_001.png\"/>",
            "code"
        ],
        [
            "Due to the added noise, none of the models recover the true weights. Indeed,\nall models always have more than 10 non-zero coefficients. Compared to the OLS\nestimator, the coefficients using a Bayesian Ridge regression are slightly\nshifted toward zero, which stabilises them. The ARD regression provides a\nsparser solution: some of the non-informative coefficients are set exactly to\nzero, while shifting others closer to zero. Some non-informative coefficients\nare still present and retain large values.",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Comparing Linear Bayesian Regressors->Models robustness to recover the ground truth weights->Plot the marginal log-likelihood": [
        [
            "import numpy as np\n\nard_scores = -(ard.scores_)\nbrr_scores = -(brr.scores_)\n(ard_scores, color=\"navy\", label=\"ARD\")\n(brr_scores, color=\"red\", label=\"BayesianRidge\")\n(\"Log-likelihood\")\n(\"Iterations\")\n(1, 30)\n()\n_ = (\"Models log-likelihood\")\n\n\n<img alt=\"Models log-likelihood\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_ard_002.png\" srcset=\"../../_images/sphx_glr_plot_ard_002.png\"/>",
            "code"
        ],
        [
            "Indeed, both models minimize the log-likelihood up to an arbitrary cutoff\ndefined by the n_iter parameter.",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Comparing Linear Bayesian Regressors->Bayesian regressions with polynomial feature expansion->Generate synthetic dataset": [
        [
            "We create a target that is a non-linear function of the input feature.\nNoise following a standard uniform distribution is added.",
            "markdown"
        ],
        [
            "from sklearn.pipeline import \nfrom sklearn.preprocessing import , \n\nrng = (0)\nn_samples = 110\n\n# sort the data to make plotting easier later\nX = (-10 * rng.rand(n_samples) + 10)\nnoise = rng.normal(0, 1, n_samples) * 1.35\ny = (X) * (X) + noise\nfull_data = ({\"input_feature\": X, \"target\": y})\nX = X.reshape((-1, 1))\n\n# extrapolation\nX_plot = (10, 10.4, 10)\ny_plot = (X_plot) * (X_plot)\nX_plot = ((X, X_plot.reshape((-1, 1))))\ny_plot = ((y - noise, y_plot))",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Comparing Linear Bayesian Regressors->Bayesian regressions with polynomial feature expansion->Fit the regressors": [
        [
            "Here we try a degree 10 polynomial to potentially overfit, though the bayesian\nlinear models regularize the size of the polynomial coefficients. As\nfit_intercept=True by default for\n and\n, then\n should not introduce an\nadditional bias feature. By setting return_std=True, the bayesian regressors\nreturn the standard deviation of the posterior distribution for the model\nparameters.",
            "markdown"
        ],
        [
            "ard_poly = (\n    (degree=10, include_bias=False),\n    (),\n    (),\n).fit(X, y)\nbrr_poly = (\n    (degree=10, include_bias=False),\n    (),\n    (),\n).fit(X, y)\n\ny_ard, y_ard_std = ard_poly.predict(X_plot, return_std=True)\ny_brr, y_brr_std = brr_poly.predict(X_plot, return_std=True)",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Comparing Linear Bayesian Regressors->Bayesian regressions with polynomial feature expansion->Plotting polynomial regressions with std errors of the scores": [
        [
            "ax = (\n    data=full_data, x=\"input_feature\", y=\"target\", color=\"black\", alpha=0.75\n)\nax.plot(X_plot, y_plot, color=\"black\", label=\"Ground Truth\")\nax.plot(X_plot, y_brr, color=\"red\", label=\"BayesianRidge with polynomial features\")\nax.plot(X_plot, y_ard, color=\"navy\", label=\"ARD with polynomial features\")\nax.fill_between(\n    X_plot.ravel(),\n    y_ard - y_ard_std,\n    y_ard + y_ard_std,\n    color=\"navy\",\n    alpha=0.3,\n)\nax.fill_between(\n    X_plot.ravel(),\n    y_brr - y_brr_std,\n    y_brr + y_brr_std,\n    color=\"red\",\n    alpha=0.3,\n)\nax.legend()\n_ = ax.set_title(\"Polynomial fit of a non-linear feature\")\n\n\n<img alt=\"Polynomial fit of a non-linear feature\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_ard_003.png\" srcset=\"../../_images/sphx_glr_plot_ard_003.png\"/>",
            "code"
        ],
        [
            "The error bars represent one standard deviation of the predicted gaussian\ndistribution of the query points. Notice that the ARD regression captures the\nground truth the best when using the default parameters in both models, but\nfurther reducing the lambda_init hyperparameter of the Bayesian Ridge can\nreduce its bias (see example\n).\nFinally, due to the intrinsic limitations of a polynomial regression, both\nmodels fail when extrapolating.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.621 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Comparing various online solvers": [
        [
            "An example showing how different online solvers perform\non the hand-written digits dataset.\n<img alt=\"plot sgd comparison\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_sgd_comparison_001.png\" srcset=\"../../_images/sphx_glr_plot_sgd_comparison_001.png\"/>",
            "markdown"
        ],
        [
            "training SGD\ntraining ASGD\ntraining Perceptron\ntraining Passive-Aggressive I\ntraining Passive-Aggressive II\ntraining SAG\n\n\n\n<br/>",
            "code"
        ],
        [
            "# Author: Rob Zinkov &lt;rob at zinkov dot com\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\n\nfrom sklearn.model_selection import \nfrom sklearn.linear_model import , \nfrom sklearn.linear_model import \nfrom sklearn.linear_model import \n\nheldout = [0.95, 0.90, 0.75, 0.50, 0.01]\n# Number of rounds to fit and evaluate an estimator.\nrounds = 10\nX, y = (return_X_y=True)\n\nclassifiers = [\n    (\"SGD\", (max_iter=110)),\n    (\"ASGD\", (max_iter=110, average=True)),\n    (\"Perceptron\", (max_iter=110)),\n    (\n        \"Passive-Aggressive I\",\n        (max_iter=110, loss=\"hinge\", C=1.0, tol=1e-4),\n    ),\n    (\n        \"Passive-Aggressive II\",\n        (\n            max_iter=110, loss=\"squared_hinge\", C=1.0, tol=1e-4\n        ),\n    ),\n    (\n        \"SAG\",\n        (max_iter=110, solver=\"sag\", tol=1e-1, C=1.0e4 / X.shape[0]),\n    ),\n]\n\nxx = 1.0 - (heldout)\n\nfor name, clf in classifiers:\n    print(\"training %s\" % name)\n    rng = (42)\n    yy = []\n    for i in heldout:\n        yy_ = []\n        for r in range(rounds):\n            X_train, X_test, y_train, y_test = (\n                X, y, test_size=i, random_state=rng\n            )\n            clf.fit(X_train, y_train)\n            y_pred = clf.predict(X_test)\n            yy_.append(1 - (y_pred == y_test))\n        yy.append((yy_))\n    (xx, yy, label=name)\n\n(loc=\"upper right\")\n(\"Proportion train\")\n(\"Test Error Rate\")\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  8.130 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Curve Fitting with Bayesian Ridge Regression": [
        [
            "Computes a Bayesian Ridge Regression of Sinusoids.",
            "markdown"
        ],
        [
            "See  for more information on the regressor.",
            "markdown"
        ],
        [
            "In general, when fitting a curve with a polynomial by Bayesian ridge\nregression, the selection of initial values of\nthe regularization parameters (alpha, lambda) may be important.\nThis is because the regularization parameters are determined by an iterative\nprocedure that depends on initial values.",
            "markdown"
        ],
        [
            "In this example, the sinusoid is approximated by a polynomial using different\npairs of initial values.",
            "markdown"
        ],
        [
            "When starting from the default values (alpha_init = 1.90, lambda_init = 1.),\nthe bias of the resulting curve is large, and the variance is small.\nSo, lambda_init should be relatively small (1.e-3) so as to reduce the bias.",
            "markdown"
        ],
        [
            "Also, by evaluating log marginal likelihood (L) of\nthese models, we can determine which one is better.\nIt can be concluded that the model with larger L is more likely.",
            "markdown"
        ],
        [
            "# Author: Yoshihiro Uchida &lt;nimbus1after2a1sun7shower@gmail.com",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Curve Fitting with Bayesian Ridge Regression->Generate sinusoidal data with noise": [
        [
            "import numpy as np\n\n\ndef func(x):\n    return (2 *  * x)\n\n\nsize = 25\nrng = (1234)\nx_train = rng.uniform(0.0, 1.0, size)\ny_train = func(x_train) + rng.normal(scale=0.1, size=size)\nx_test = (0.0, 1.0, 100)",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Curve Fitting with Bayesian Ridge Regression->Fit by cubic polynomial": [
        [
            "from sklearn.linear_model import \n\nn_order = 3\nX_train = (x_train, n_order + 1, increasing=True)\nX_test = (x_test, n_order + 1, increasing=True)\nreg = (tol=1e-6, fit_intercept=False, compute_score=True)",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Curve Fitting with Bayesian Ridge Regression->Plot the true and predicted curves with log marginal likelihood (L)": [
        [
            "import matplotlib.pyplot as plt\n\nfig, axes = (1, 2, figsize=(8, 4))\nfor i, ax in enumerate(axes):\n    # Bayesian ridge regression with different initial value pairs\n    if i == 0:\n        init = [1 / (y_train), 1.0]  # Default values\n    elif i == 1:\n        init = [1.0, 1e-3]\n        reg.set_params(alpha_init=init[0], lambda_init=init[1])\n    reg.fit(X_train, y_train)\n    ymean, ystd = reg.predict(X_test, return_std=True)\n\n    ax.plot(x_test, func(x_test), color=\"blue\", label=\"sin($2\\\\pi x$)\")\n    ax.scatter(x_train, y_train, s=50, alpha=0.5, label=\"observation\")\n    ax.plot(x_test, ymean, color=\"red\", label=\"predict mean\")\n    ax.fill_between(\n        x_test, ymean - ystd, ymean + ystd, color=\"pink\", alpha=0.5, label=\"predict std\"\n    )\n    ax.set_ylim(-1.3, 1.3)\n    ax.legend()\n    title = \"$\\\\alpha$_init$={:.2f},\\\\ \\\\lambda$_init$={}$\".format(init[0], init[1])\n    if i == 0:\n        title += \" (Default)\"\n    ax.set_title(title, fontsize=12)\n    text = \"$\\\\alpha={:.1f}$\\n$\\\\lambda={:.3f}$\\n$L={:.1f}$\".format(\n        reg.alpha_, reg.lambda_, reg.scores_[-1]\n    )\n    ax.text(0.05, -1.0, text, fontsize=12)\n\n()\n()\n\n\n<img alt=\"$\\alpha$_init$=1.90,\\ \\lambda$_init$=1.0$ (Default), $\\alpha$_init$=1.00,\\ \\lambda$_init$=0.001$\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_bayesian_ridge_curvefit_001.png\" srcset=\"../../_images/sphx_glr_plot_bayesian_ridge_curvefit_001.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.212 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Early stopping of Stochastic Gradient Descent": [
        [
            "Stochastic Gradient Descent is an optimization technique which minimizes a loss\nfunction in a stochastic fashion, performing a gradient descent step sample by\nsample. In particular, it is a very efficient method to fit linear models.",
            "markdown"
        ],
        [
            "As a stochastic method, the loss function is not necessarily decreasing at each\niteration, and convergence is only guaranteed in expectation. For this reason,\nmonitoring the convergence on the loss function can be difficult.",
            "markdown"
        ],
        [
            "Another approach is to monitor convergence on a validation score. In this case,\nthe input data is split into a training set and a validation set. The model is\nthen fitted on the training set and the stopping criterion is based on the\nprediction score computed on the validation set. This enables us to find the\nleast number of iterations which is sufficient to build a model that\ngeneralizes well to unseen data and reduces the chance of over-fitting the\ntraining data.",
            "markdown"
        ],
        [
            "This early stopping strategy is activated if early_stopping=True; otherwise\nthe stopping criterion only uses the training loss on the entire input data. To\nbetter control the early stopping strategy, we can specify a parameter\nvalidation_fraction which set the fraction of the input dataset that we\nkeep aside to compute the validation score. The optimization will continue\nuntil the validation score did not improve by at least tol during the last\nn_iter_no_change iterations. The actual number of iterations is available\nat the attribute n_iter_.",
            "markdown"
        ],
        [
            "This example illustrates how the early stopping can used in the\n model to achieve almost the same\naccuracy as compared to a model built without early stopping. This can\nsignificantly reduce training time. Note that scores differ between the\nstopping criteria even from early iterations because some of the training data\nis held out with the validation stopping criterion.\n\n<img alt=\"Train score, Test score\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_sgd_early_stopping_001.png\" srcset=\"../../_images/sphx_glr_plot_sgd_early_stopping_001.png\"/>\n<img alt=\"n_iter_, Fit time (sec)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_sgd_early_stopping_002.png\" srcset=\"../../_images/sphx_glr_plot_sgd_early_stopping_002.png\"/>",
            "markdown"
        ],
        [
            "No stopping criterion: .................................................\nTraining loss: .................................................\nValidation score: .................................................\n\n\n\n<br/>",
            "code"
        ],
        [
            "# Authors: Tom Dupre la Tour\n#\n# License: BSD 3 clause\n\nimport time\nimport sys\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import linear_model\nfrom sklearn.datasets import \nfrom sklearn.model_selection import \nfrom sklearn.utils._testing import ignore_warnings\nfrom sklearn.exceptions import \nfrom sklearn.utils import \n\n\ndef load_mnist(n_samples=None, class_0=\"0\", class_1=\"8\"):\n    \"\"\"Load MNIST, select two classes, shuffle and return only n_samples.\"\"\"\n    # Load data from http://openml.org/d/554\n    mnist = (\"mnist_784\", version=1, as_frame=False, parser=\"pandas\")\n\n    # take only two classes for binary classification\n    mask = (mnist.target == class_0, mnist.target == class_1)\n\n    X, y = (mnist.data[mask], mnist.target[mask], random_state=42)\n    if n_samples is not None:\n        X, y = X[:n_samples], y[:n_samples]\n    return X, y\n\n\n@ignore_warnings(category=)\ndef fit_and_score(estimator, max_iter, X_train, X_test, y_train, y_test):\n    \"\"\"Fit the estimator on the train set and score it on both sets\"\"\"\n    estimator.set_params(max_iter=max_iter)\n    estimator.set_params(random_state=0)\n\n    start = ()\n    estimator.fit(X_train, y_train)\n\n    fit_time = () - start\n    n_iter = estimator.n_iter_\n    train_score = estimator.score(X_train, y_train)\n    test_score = estimator.score(X_test, y_test)\n\n    return fit_time, n_iter, train_score, test_score\n\n\n# Define the estimators to compare\nestimator_dict = {\n    \"No stopping criterion\": (n_iter_no_change=3),\n    \"Training loss\": (\n        early_stopping=False, n_iter_no_change=3, tol=0.1\n    ),\n    \"Validation score\": (\n        early_stopping=True, n_iter_no_change=3, tol=0.0001, validation_fraction=0.2\n    ),\n}\n\n# Load the dataset\nX, y = load_mnist(n_samples=10000)\nX_train, X_test, y_train, y_test = (X, y, test_size=0.5, random_state=0)\n\nresults = []\nfor estimator_name, estimator in estimator_dict.items():\n    print(estimator_name + \": \", end=\"\")\n    for max_iter in range(1, 50):\n        print(\".\", end=\"\")\n        sys.stdout.flush()\n\n        fit_time, n_iter, train_score, test_score = fit_and_score(\n            estimator, max_iter, X_train, X_test, y_train, y_test\n        )\n\n        results.append(\n            (estimator_name, max_iter, fit_time, n_iter, train_score, test_score)\n        )\n    print(\"\")\n\n# Transform the results in a pandas dataframe for easy plotting\ncolumns = [\n    \"Stopping criterion\",\n    \"max_iter\",\n    \"Fit time (sec)\",\n    \"n_iter_\",\n    \"Train score\",\n    \"Test score\",\n]\nresults_df = (results, columns=columns)\n\n# Define what to plot\nlines = \"Stopping criterion\"\nx_axis = \"max_iter\"\nstyles = [\"-.\", \"--\", \"-\"]\n\n# First plot: train and test scores\nfig, axes = (nrows=1, ncols=2, sharey=True, figsize=(12, 4))\nfor ax, y_axis in zip(axes, [\"Train score\", \"Test score\"]):\n    for style, (criterion, group_df) in zip(styles, results_df.groupby(lines)):\n        group_df.plot(x=x_axis, y=y_axis, label=criterion, ax=ax, style=style)\n    ax.set_title(y_axis)\n    ax.legend(title=lines)\nfig.tight_layout()\n\n# Second plot: n_iter and fit time\nfig, axes = (nrows=1, ncols=2, figsize=(12, 4))\nfor ax, y_axis in zip(axes, [\"n_iter_\", \"Fit time (sec)\"]):\n    for style, (criterion, group_df) in zip(styles, results_df.groupby(lines)):\n        group_df.plot(x=x_axis, y=y_axis, label=criterion, ax=ax, style=style)\n    ax.set_title(y_axis)\n    ax.legend(title=lines)\nfig.tight_layout()\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  32.661 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Fitting an Elastic Net with a precomputed Gram Matrix and Weighted Samples": [
        [
            "The following example shows how to precompute the gram matrix\nwhile using weighted samples with an ElasticNet.",
            "markdown"
        ],
        [
            "If weighted samples are used, the design matrix must be centered and then\nrescaled by the square root of the weight vector before the gram matrix\nis computed.",
            "markdown"
        ],
        [
            "Note\n\nsample_weight vector is also rescaled to sum to n_samples, see the",
            "markdown"
        ],
        [
            "documentation for the sample_weight parameter to\nlinear_model.ElasticNet.fit.\n\n</dl>",
            "markdown"
        ],
        [
            "Let\u2019s start by loading the dataset and creating some sample weights.",
            "markdown"
        ],
        [
            "import numpy as np\nfrom sklearn.datasets import \n\nrng = (0)\n\nn_samples = int(1e5)\nX, y = (n_samples=n_samples, noise=0.5, random_state=rng)\n\nsample_weight = rng.lognormal(size=n_samples)\n# normalize the sample weights\nnormalized_weights = sample_weight * (n_samples / (sample_weight.sum()))",
            "code"
        ],
        [
            "To fit the elastic net using the precompute option together with the sample\nweights, we must first center the design matrix,  and rescale it by the\nnormalized weights prior to computing the gram matrix.",
            "markdown"
        ],
        [
            "X_offset = (X, axis=0, weights=normalized_weights)\nX_centered = X - (X, axis=0, weights=normalized_weights)\nX_scaled = X_centered * (normalized_weights)[:, ]\ngram = (X_scaled.T, X_scaled)",
            "code"
        ],
        [
            "We can now proceed with fitting. We must passed the centered design matrix to\nfit otherwise the elastic net estimator will detect that it is uncentered\nand discard the gram matrix we passed. However, if we pass the scaled design\nmatrix, the preprocessing code will incorrectly rescale it a second time.",
            "markdown"
        ],
        [
            "from sklearn.linear_model import \n\nlm = (alpha=0.01, precompute=gram)\nlm.fit(X_centered, y, sample_weight=normalized_weights)",
            "code"
        ],
        [
            "ElasticNet(alpha=0.01,\n           precompute=array([[ 9.98809919e+04, -4.48938813e+02, -1.03237920e+03, ...,\n        -2.25349312e+02, -3.53959628e+02, -1.67451144e+02],\n       [-4.48938813e+02,  1.00768662e+05,  1.19112072e+02, ...,\n        -1.07963978e+03,  7.47987268e+01, -5.76195467e+02],\n       [-1.03237920e+03,  1.19112072e+02,  1.00393284e+05, ...,\n        -3.07582983e+02,  6.66670169e+02,  2.65799352e+02],\n       ...,\n       [-2.25349312e+02, -1.07963978e+03, -3.07582983e+02, ...,\n         9.99891212e+04, -4.58195950e+02, -1.58667835e+02],\n       [-3.53959628e+02,  7.47987268e+01,  6.66670169e+02, ...,\n        -4.58195950e+02,  9.98350372e+04,  5.60836363e+02],\n       [-1.67451144e+02, -5.76195467e+02,  2.65799352e+02, ...,\n        -1.58667835e+02,  5.60836363e+02,  1.00911944e+05]]))<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input checked=\"\" class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-117\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-117\">ElasticNet</label>",
            "code"
        ],
        [
            "ElasticNet(alpha=0.01,\n           precompute=array([[ 9.98809919e+04, -4.48938813e+02, -1.03237920e+03, ...,\n        -2.25349312e+02, -3.53959628e+02, -1.67451144e+02],\n       [-4.48938813e+02,  1.00768662e+05,  1.19112072e+02, ...,\n        -1.07963978e+03,  7.47987268e+01, -5.76195467e+02],\n       [-1.03237920e+03,  1.19112072e+02,  1.00393284e+05, ...,\n        -3.07582983e+02,  6.66670169e+02,  2.65799352e+02],\n       ...,\n       [-2.25349312e+02, -1.07963978e+03, -3.07582983e+02, ...,\n         9.99891212e+04, -4.58195950e+02, -1.58667835e+02],\n       [-3.53959628e+02,  7.47987268e+01,  6.66670169e+02, ...,\n        -4.58195950e+02,  9.98350372e+04,  5.60836363e+02],\n       [-1.67451144e+02, -5.76195467e+02,  2.65799352e+02, ...,\n        -1.58667835e+02,  5.60836363e+02,  1.00911944e+05]]))\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  1.017 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->HuberRegressor vs Ridge on dataset with strong outliers": [
        [
            "Fit Ridge and HuberRegressor on a dataset with outliers.",
            "markdown"
        ],
        [
            "The example shows that the predictions in ridge are strongly influenced\nby the outliers present in the dataset. The Huber regressor is less\ninfluenced by the outliers since the model uses the linear loss for these.\nAs the parameter epsilon is increased for the Huber regressor, the decision\nfunction approaches that of the ridge.\n<img alt=\"Comparison of HuberRegressor vs Ridge\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_huber_vs_ridge_001.png\" srcset=\"../../_images/sphx_glr_plot_huber_vs_ridge_001.png\"/>",
            "markdown"
        ],
        [
            "# Authors: Manoj Kumar mks542@nyu.edu\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import \nfrom sklearn.linear_model import , \n\n# Generate toy data.\nrng = (0)\nX, y = (\n    n_samples=20, n_features=1, random_state=0, noise=4.0, bias=100.0\n)\n\n# Add four strong outliers to the dataset.\nX_outliers = rng.normal(0, 0.5, size=(4, 1))\ny_outliers = rng.normal(0, 2.0, size=4)\nX_outliers[:2, :] += X.max() + X.mean() / 4.0\nX_outliers[2:, :] += X.min() - X.mean() / 4.0\ny_outliers[:2] += y.min() - y.mean() / 4.0\ny_outliers[2:] += y.max() + y.mean() / 4.0\nX = ((X, X_outliers))\ny = ((y, y_outliers))\n(X, y, \"b.\")\n\n# Fit the huber regressor over a series of epsilon values.\ncolors = [\"r-\", \"b-\", \"y-\", \"m-\"]\n\nx = (X.min(), X.max(), 7)\nepsilon_values = [1, 1.5, 1.75, 1.9]\nfor k, epsilon in enumerate(epsilon_values):\n    huber = (alpha=0.0, epsilon=epsilon)\n    huber.fit(X, y)\n    coef_ = huber.coef_ * x + huber.intercept_\n    (x, coef_, colors[k], label=\"huber loss, %s\" % epsilon)\n\n# Fit a ridge regressor to compare it to huber regressor.\nridge = (alpha=0.0, random_state=0)\nridge.fit(X, y)\ncoef_ridge = ridge.coef_\ncoef_ = ridge.coef_ * x + ridge.intercept_\n(x, coef_, \"g-\", label=\"ridge regression\")\n\n(\"Comparison of HuberRegressor vs Ridge\")\n(\"X\")\n(\"y\")\n(loc=0)\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.099 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Joint feature selection with multi-task Lasso": [
        [
            "The multi-task lasso allows to fit multiple regression problems\njointly enforcing the selected features to be the same across\ntasks. This example simulates sequential measurements, each task\nis a time instant, and the relevant features vary in amplitude\nover time while being the same. The multi-task lasso imposes that\nfeatures that are selected at one time point are select for all time\npoint. This makes feature selection by the Lasso more stable.",
            "markdown"
        ],
        [
            "# Author: Alexandre Gramfort &lt;alexandre.gramfort@inria.fr\n# License: BSD 3 clause",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Joint feature selection with multi-task Lasso->Generate data": [
        [
            "import numpy as np\n\nrng = (42)\n\n# Generate some 2D coefficients with sine waves with random frequency and phase\nn_samples, n_features, n_tasks = 100, 30, 40\nn_relevant_features = 5\ncoef = ((n_tasks, n_features))\ntimes = (0, 2 * , n_tasks)\nfor k in range(n_relevant_features):\n    coef[:, k] = ((1.0 + rng.randn(1)) * times + 3 * rng.randn(1))\n\nX = rng.randn(n_samples, n_features)\nY = (X, coef.T) + rng.randn(n_samples, n_tasks)",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Joint feature selection with multi-task Lasso->Fit models": [
        [
            "from sklearn.linear_model import , \n\ncoef_lasso_ = ([(alpha=0.5).fit(X, y).coef_ for y in Y.T])\ncoef_multi_task_lasso_ = (alpha=1.0).fit(X, Y).coef_",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Joint feature selection with multi-task Lasso->Plot support and time series": [
        [
            "import matplotlib.pyplot as plt\n\nfig = (figsize=(8, 5))\n(1, 2, 1)\n(coef_lasso_)\n(\"Feature\")\n(\"Time (or Task)\")\n(10, 5, \"Lasso\")\n(1, 2, 2)\n(coef_multi_task_lasso_)\n(\"Feature\")\n(\"Time (or Task)\")\n(10, 5, \"MultiTaskLasso\")\nfig.suptitle(\"Coefficient non-zero location\")\n\nfeature_to_plot = 0\n()\nlw = 2\n(coef[:, feature_to_plot], color=\"seagreen\", linewidth=lw, label=\"Ground truth\")\n(\n    coef_lasso_[:, feature_to_plot], color=\"cornflowerblue\", linewidth=lw, label=\"Lasso\"\n)\n(\n    coef_multi_task_lasso_[:, feature_to_plot],\n    color=\"gold\",\n    linewidth=lw,\n    label=\"MultiTaskLasso\",\n)\n(loc=\"upper center\")\n(\"tight\")\n([-1.1, 1.1])\n()\n\n\n\n<img alt=\"Coefficient non-zero location\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_multi_task_lasso_support_001.png\" srcset=\"../../_images/sphx_glr_plot_multi_task_lasso_support_001.png\"/>\n<img alt=\"plot multi task lasso support\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_multi_task_lasso_support_002.png\" srcset=\"../../_images/sphx_glr_plot_multi_task_lasso_support_002.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.230 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->L1 Penalty and Sparsity in Logistic Regression": [
        [
            "Comparison of the sparsity (percentage of zero coefficients) of solutions when\nL1, L2 and Elastic-Net penalty are used for different values of C. We can see\nthat large values of C give more freedom to the model.  Conversely, smaller\nvalues of C constrain the model more. In the L1 penalty case, this leads to\nsparser solutions. As expected, the Elastic-Net penalty sparsity is between\nthat of L1 and L2.",
            "markdown"
        ],
        [
            "We classify 8x8 images of digits into two classes: 0-4 against 5-9.\nThe visualization shows coefficients of the models for varying C.\n<img alt=\"L1 penalty, Elastic-Net l1_ratio = 0.5, L2 penalty\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_logistic_l1_l2_sparsity_001.png\" srcset=\"../../_images/sphx_glr_plot_logistic_l1_l2_sparsity_001.png\"/>",
            "markdown"
        ],
        [
            "C=1.00\nSparsity with L1 penalty:                4.69%\nSparsity with Elastic-Net penalty:       4.69%\nSparsity with L2 penalty:                4.69%\nScore with L1 penalty:                   0.90\nScore with Elastic-Net penalty:          0.90\nScore with L2 penalty:                   0.90\nC=0.10\nSparsity with L1 penalty:                29.69%\nSparsity with Elastic-Net penalty:       14.06%\nSparsity with L2 penalty:                4.69%\nScore with L1 penalty:                   0.90\nScore with Elastic-Net penalty:          0.90\nScore with L2 penalty:                   0.90\nC=0.01\nSparsity with L1 penalty:                84.38%\nSparsity with Elastic-Net penalty:       68.75%\nSparsity with L2 penalty:                4.69%\nScore with L1 penalty:                   0.86\nScore with Elastic-Net penalty:          0.88\nScore with L2 penalty:                   0.89\n\n\n\n<br/>",
            "code"
        ],
        [
            "# Authors: Alexandre Gramfort &lt;alexandre.gramfort@inria.fr\n#          Mathieu Blondel &lt;mathieu@mblondel.org\n#          Andreas Mueller &lt;amueller@ais.uni-bonn.de\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import \nfrom sklearn import datasets\nfrom sklearn.preprocessing import \n\nX, y = (return_X_y=True)\n\nX = ().fit_transform(X)\n\n# classify small against large digits\ny = (y  4).astype(int)\n\nl1_ratio = 0.5  # L1 weight in the Elastic-Net regularization\n\nfig, axes = (3, 3)\n\n# Set regularization parameter\nfor i, (C, axes_row) in enumerate(zip((1, 0.1, 0.01), axes)):\n    # Increase tolerance for short training time\n    clf_l1_LR = (C=C, penalty=\"l1\", tol=0.01, solver=\"saga\")\n    clf_l2_LR = (C=C, penalty=\"l2\", tol=0.01, solver=\"saga\")\n    clf_en_LR = (\n        C=C, penalty=\"elasticnet\", solver=\"saga\", l1_ratio=l1_ratio, tol=0.01\n    )\n    clf_l1_LR.fit(X, y)\n    clf_l2_LR.fit(X, y)\n    clf_en_LR.fit(X, y)\n\n    coef_l1_LR = clf_l1_LR.coef_.ravel()\n    coef_l2_LR = clf_l2_LR.coef_.ravel()\n    coef_en_LR = clf_en_LR.coef_.ravel()\n\n    # coef_l1_LR contains zeros due to the\n    # L1 sparsity inducing norm\n\n    sparsity_l1_LR = (coef_l1_LR == 0) * 100\n    sparsity_l2_LR = (coef_l2_LR == 0) * 100\n    sparsity_en_LR = (coef_en_LR == 0) * 100\n\n    print(\"C=%.2f\" % C)\n    print(\"{:&lt;40} {:.2f}%\".format(\"Sparsity with L1 penalty:\", sparsity_l1_LR))\n    print(\"{:&lt;40} {:.2f}%\".format(\"Sparsity with Elastic-Net penalty:\", sparsity_en_LR))\n    print(\"{:&lt;40} {:.2f}%\".format(\"Sparsity with L2 penalty:\", sparsity_l2_LR))\n    print(\"{:&lt;40} {:.2f}\".format(\"Score with L1 penalty:\", clf_l1_LR.score(X, y)))\n    print(\n        \"{:&lt;40} {:.2f}\".format(\"Score with Elastic-Net penalty:\", clf_en_LR.score(X, y))\n    )\n    print(\"{:&lt;40} {:.2f}\".format(\"Score with L2 penalty:\", clf_l2_LR.score(X, y)))\n\n    if i == 0:\n        axes_row[0].set_title(\"L1 penalty\")\n        axes_row[1].set_title(\"Elastic-Net\\nl1_ratio = %s\" % l1_ratio)\n        axes_row[2].set_title(\"L2 penalty\")\n\n    for ax, coefs in zip(axes_row, [coef_l1_LR, coef_en_LR, coef_l2_LR]):\n        ax.imshow(\n            np.abs(coefs.reshape(8, 8)),\n            interpolation=\"nearest\",\n            cmap=\"binary\",\n            vmax=1,\n            vmin=0,\n        )\n        ax.set_xticks(())\n        ax.set_yticks(())\n\n    axes_row[0].set_ylabel(\"C = %s\" % C)\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.458 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Lasso and Elastic Net": [
        [
            "Lasso and elastic net (L1 and L2 penalisation) implemented using a\ncoordinate descent.",
            "markdown"
        ],
        [
            "The coefficients can be forced to be positive.\n\n<img alt=\"Lasso and Elastic-Net Paths\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_lasso_coordinate_descent_path_001.png\" srcset=\"../../_images/sphx_glr_plot_lasso_coordinate_descent_path_001.png\"/>\n<img alt=\"Lasso and positive Lasso\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_lasso_coordinate_descent_path_002.png\" srcset=\"../../_images/sphx_glr_plot_lasso_coordinate_descent_path_002.png\"/>\n<img alt=\"Elastic-Net and positive Elastic-Net\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_lasso_coordinate_descent_path_003.png\" srcset=\"../../_images/sphx_glr_plot_lasso_coordinate_descent_path_003.png\"/>",
            "markdown"
        ],
        [
            "Computing regularization path using the lasso...\nComputing regularization path using the positive lasso...\nComputing regularization path using the elastic net...\nComputing regularization path using the positive elastic net...\n\n\n\n<br/>",
            "code"
        ],
        [
            "# Author: Alexandre Gramfort &lt;alexandre.gramfort@inria.fr\n# License: BSD 3 clause\n\nfrom itertools import \nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import , \nfrom sklearn import datasets\n\n\nX, y = (return_X_y=True)\n\n\nX /= X.std(axis=0)  # Standardize data (easier to set the l1_ratio parameter)\n\n# Compute paths\n\neps = 5e-3  # the smaller it is the longer is the path\n\nprint(\"Computing regularization path using the lasso...\")\nalphas_lasso, coefs_lasso, _ = (X, y, eps=eps)\n\nprint(\"Computing regularization path using the positive lasso...\")\nalphas_positive_lasso, coefs_positive_lasso, _ = (\n    X, y, eps=eps, positive=True\n)\nprint(\"Computing regularization path using the elastic net...\")\nalphas_enet, coefs_enet, _ = (X, y, eps=eps, l1_ratio=0.8)\n\nprint(\"Computing regularization path using the positive elastic net...\")\nalphas_positive_enet, coefs_positive_enet, _ = (\n    X, y, eps=eps, l1_ratio=0.8, positive=True\n)\n\n# Display results\n\n(1)\ncolors = ([\"b\", \"r\", \"g\", \"c\", \"k\"])\nneg_log_alphas_lasso = -(alphas_lasso)\nneg_log_alphas_enet = -(alphas_enet)\nfor coef_l, coef_e, c in zip(coefs_lasso, coefs_enet, colors):\n    l1 = (neg_log_alphas_lasso, coef_l, c=c)\n    l2 = (neg_log_alphas_enet, coef_e, linestyle=\"--\", c=c)\n\n(\"-Log(alpha)\")\n(\"coefficients\")\n(\"Lasso and Elastic-Net Paths\")\n((l1[-1], l2[-1]), (\"Lasso\", \"Elastic-Net\"), loc=\"lower left\")\n(\"tight\")\n\n\n(2)\nneg_log_alphas_positive_lasso = -(alphas_positive_lasso)\nfor coef_l, coef_pl, c in zip(coefs_lasso, coefs_positive_lasso, colors):\n    l1 = (neg_log_alphas_lasso, coef_l, c=c)\n    l2 = (neg_log_alphas_positive_lasso, coef_pl, linestyle=\"--\", c=c)\n\n(\"-Log(alpha)\")\n(\"coefficients\")\n(\"Lasso and positive Lasso\")\n((l1[-1], l2[-1]), (\"Lasso\", \"positive Lasso\"), loc=\"lower left\")\n(\"tight\")\n\n\n(3)\nneg_log_alphas_positive_enet = -(alphas_positive_enet)\nfor coef_e, coef_pe, c in zip(coefs_enet, coefs_positive_enet, colors):\n    l1 = (neg_log_alphas_enet, coef_e, c=c)\n    l2 = (neg_log_alphas_positive_enet, coef_pe, linestyle=\"--\", c=c)\n\n(\"-Log(alpha)\")\n(\"coefficients\")\n(\"Elastic-Net and positive Elastic-Net\")\n((l1[-1], l2[-1]), (\"Elastic-Net\", \"positive Elastic-Net\"), loc=\"lower left\")\n(\"tight\")\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.321 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Lasso and Elastic Net for Sparse Signals": [
        [
            "Estimates Lasso and Elastic-Net regression models on a manually generated\nsparse signal corrupted with an additive noise. Estimated coefficients are\ncompared with the ground-truth.",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Lasso and Elastic Net for Sparse Signals->Data Generation": [
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import \n\n(42)\n\nn_samples, n_features = 50, 100\nX = (n_samples, n_features)\n\n# Decreasing coef w. alternated signs for visualization\nidx = (n_features)\ncoef = (-1) ** idx * (-idx / 10)\ncoef[10:] = 0  # sparsify coef\ny = (X, coef)\n\n# Add noise\ny += 0.01 * (size=n_samples)\n\n# Split data in train set and test set\nn_samples = X.shape[0]\nX_train, y_train = X[: n_samples // 2], y[: n_samples // 2]\nX_test, y_test = X[n_samples // 2 :], y[n_samples // 2 :]",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Lasso and Elastic Net for Sparse Signals->Lasso": [
        [
            "from sklearn.linear_model import \n\nalpha = 0.1\nlasso = (alpha=alpha)\n\ny_pred_lasso = lasso.fit(X_train, y_train).predict(X_test)\nr2_score_lasso = (y_test, y_pred_lasso)\nprint(lasso)\nprint(\"r^2 on test data : %f\" % r2_score_lasso)",
            "code"
        ],
        [
            "Lasso(alpha=0.1)\nr^2 on test data : 0.658064",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Lasso and Elastic Net for Sparse Signals->ElasticNet": [
        [
            "from sklearn.linear_model import \n\nenet = (alpha=alpha, l1_ratio=0.7)\n\ny_pred_enet = enet.fit(X_train, y_train).predict(X_test)\nr2_score_enet = (y_test, y_pred_enet)\nprint(enet)\nprint(\"r^2 on test data : %f\" % r2_score_enet)",
            "code"
        ],
        [
            "ElasticNet(alpha=0.1, l1_ratio=0.7)\nr^2 on test data : 0.642515",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Lasso and Elastic Net for Sparse Signals->Plot": [
        [
            "m, s, _ = (\n    (enet.coef_)[0],\n    enet.coef_[enet.coef_ != 0],\n    markerfmt=\"x\",\n    label=\"Elastic net coefficients\",\n)\n([m, s], color=\"#2ca02c\")\nm, s, _ = (\n    (lasso.coef_)[0],\n    lasso.coef_[lasso.coef_ != 0],\n    markerfmt=\"x\",\n    label=\"Lasso coefficients\",\n)\n([m, s], color=\"#ff7f0e\")\n(\n    (coef)[0],\n    coef[coef != 0],\n    label=\"true coefficients\",\n    markerfmt=\"bx\",\n)\n\n(loc=\"best\")\n(\n    \"Lasso $R^2$: %.3f, Elastic Net $R^2$: %.3f\" % (r2_score_lasso, r2_score_enet)\n)\n()\n\n\n<img alt=\"Lasso $R^2$: 0.658, Elastic Net $R^2$: 0.643\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_lasso_and_elasticnet_001.png\" srcset=\"../../_images/sphx_glr_plot_lasso_and_elasticnet_001.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.105 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Lasso model selection via information criteria": [
        [
            "This example reproduces the example of Fig. 2 of . A\n estimator is fit on a\ndiabetes dataset and the AIC and the BIC criteria are used to select\nthe best model.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "It is important to note that the optimization to find alpha with\n relies on the AIC or BIC\ncriteria that are computed in-sample, thus on the training set directly.\nThis approach differs from the cross-validation procedure. For a comparison\nof the two approaches, you can refer to the following example:\n.",
            "markdown"
        ],
        [
            "References\n\n\n[ZHT2007]\n(,)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "# Author: Alexandre Gramfort\n#         Guillaume Lemaitre\n# License: BSD 3 clause",
            "code"
        ],
        [
            "We will use the diabetes dataset.",
            "markdown"
        ],
        [
            "from sklearn.datasets import \n\nX, y = (return_X_y=True, as_frame=True)\nn_samples = X.shape[0]\nX.head()\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "Scikit-learn provides an estimator called\nLinearLarsIC that uses either Akaike\u2019s\ninformation criterion (AIC) or the Bayesian information criterion (BIC) to\nselect the best model. Before fitting\nthis model, we will scale the dataset.",
            "markdown"
        ],
        [
            "In the following, we are going to fit two models to compare the values\nreported by AIC and BIC.",
            "markdown"
        ],
        [
            "from sklearn.preprocessing import \nfrom sklearn.linear_model import \nfrom sklearn.pipeline import \n\nlasso_lars_ic = ((), (criterion=\"aic\")).fit(X, y)",
            "code"
        ],
        [
            "To be in line with the definition in , we need to rescale the\nAIC and the BIC. Indeed, Zou et al. are ignoring some constant terms\ncompared to the original definition of AIC derived from the maximum\nlog-likelihood of a linear model. You can refer to\n.",
            "markdown"
        ],
        [
            "def zou_et_al_criterion_rescaling(criterion, n_samples, noise_variance):\n    \"\"\"Rescale the information criterion to follow the definition of Zou et al.\"\"\"\n    return criterion - n_samples * (2 *  * noise_variance) - n_samples",
            "code"
        ],
        [
            "import numpy as np\n\naic_criterion = zou_et_al_criterion_rescaling(\n    lasso_lars_ic[-1].criterion_,\n    n_samples,\n    lasso_lars_ic[-1].noise_variance_,\n)\n\nindex_alpha_path_aic = (\n    lasso_lars_ic[-1].alphas_ == lasso_lars_ic[-1].alpha_\n)[0]",
            "code"
        ],
        [
            "lasso_lars_ic.set_params(lassolarsic__criterion=\"bic\").fit(X, y)\n\nbic_criterion = zou_et_al_criterion_rescaling(\n    lasso_lars_ic[-1].criterion_,\n    n_samples,\n    lasso_lars_ic[-1].noise_variance_,\n)\n\nindex_alpha_path_bic = (\n    lasso_lars_ic[-1].alphas_ == lasso_lars_ic[-1].alpha_\n)[0]",
            "code"
        ],
        [
            "Now that we collected the AIC and BIC, we can as well check that the minima\nof both criteria happen at the same alpha. Then, we can simplify the\nfollowing plot.",
            "markdown"
        ],
        [
            "index_alpha_path_aic == index_alpha_path_bic",
            "code"
        ],
        [
            "True",
            "code"
        ],
        [
            "Finally, we can plot the AIC and BIC criterion and the subsequent selected\nregularization parameter.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n\n(aic_criterion, color=\"tab:blue\", marker=\"o\", label=\"AIC criterion\")\n(bic_criterion, color=\"tab:orange\", marker=\"o\", label=\"BIC criterion\")\n(\n    index_alpha_path_bic,\n    aic_criterion.min(),\n    aic_criterion.max(),\n    color=\"black\",\n    linestyle=\"--\",\n    label=\"Selected alpha\",\n)\n()\n(\"Information criterion\")\n(\"Lasso model sequence\")\n_ = (\"Lasso model selection via AIC and BIC\")\n\n\n<img alt=\"Lasso model selection via AIC and BIC\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_lasso_lars_ic_001.png\" srcset=\"../../_images/sphx_glr_plot_lasso_lars_ic_001.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.112 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Lasso model selection: AIC-BIC / cross-validation": [
        [
            "This example focuses on model selection for Lasso models that are\nlinear models with an L1 penalty for regression problems.",
            "markdown"
        ],
        [
            "Indeed, several strategies can be used to select the value of the\nregularization parameter: via cross-validation or using an information\ncriterion, namely AIC or BIC.",
            "markdown"
        ],
        [
            "In what follows, we will discuss in details the different strategies.",
            "markdown"
        ],
        [
            "# Author: Olivier Grisel\n#         Gael Varoquaux\n#         Alexandre Gramfort\n#         Guillaume Lemaitre\n# License: BSD 3 clause",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Lasso model selection: AIC-BIC / cross-validation->Dataset": [
        [
            "In this example, we will use the diabetes dataset.",
            "markdown"
        ],
        [
            "from sklearn.datasets import \n\nX, y = (return_X_y=True, as_frame=True)\nX.head()\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "In addition, we add some random features to the original data to\nbetter illustrate the feature selection performed by the Lasso model.",
            "markdown"
        ],
        [
            "import numpy as np\nimport pandas as pd\n\nrng = (42)\nn_random_features = 14\nX_random = (\n    rng.randn(X.shape[0], n_random_features),\n    columns=[f\"random_{i:02d}\" for i in range(n_random_features)],\n)\nX = ([X, X_random], axis=1)\n# Show only a subset of the columns\nX[X.columns[::3]].head()\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Lasso model selection: AIC-BIC / cross-validation->Selecting Lasso via an information criterion": [
        [
            "provides a Lasso estimator that\nuses the Akaike information criterion (AIC) or the Bayes information\ncriterion (BIC) to select the optimal value of the regularization\nparameter alpha.",
            "markdown"
        ],
        [
            "Before fitting the model, we will standardize the data with a\n. In addition, we will\nmeasure the time to fit and tune the hyperparameter alpha in order to\ncompare with the cross-validation strategy.",
            "markdown"
        ],
        [
            "We will first fit a Lasso model with the AIC criterion.",
            "markdown"
        ],
        [
            "import time\nfrom sklearn.preprocessing import \nfrom sklearn.linear_model import \nfrom sklearn.pipeline import \n\nstart_time = ()\nlasso_lars_ic = ((), (criterion=\"aic\")).fit(X, y)\nfit_time = () - start_time",
            "code"
        ],
        [
            "We store the AIC metric for each value of alpha used during fit.",
            "markdown"
        ],
        [
            "results = (\n    {\n        \"alphas\": lasso_lars_ic[-1].alphas_,\n        \"AIC criterion\": lasso_lars_ic[-1].criterion_,\n    }\n).set_index(\"alphas\")\nalpha_aic = lasso_lars_ic[-1].alpha_",
            "code"
        ],
        [
            "Now, we perform the same analysis using the BIC criterion.",
            "markdown"
        ],
        [
            "lasso_lars_ic.set_params(lassolarsic__criterion=\"bic\").fit(X, y)\nresults[\"BIC criterion\"] = lasso_lars_ic[-1].criterion_\nalpha_bic = lasso_lars_ic[-1].alpha_",
            "code"
        ],
        [
            "We can check which value of alpha leads to the minimum AIC and BIC.",
            "markdown"
        ],
        [
            "def highlight_min(x):\n    x_min = x.min()\n    return [\"font-weight: bold\" if v == x_min else \"\" for v in x]\n\n\nresults.style.apply(highlight_min)\n\n\n\n\n\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "Finally, we can plot the AIC and BIC values for the different alpha values.\nThe vertical lines in the plot correspond to the alpha chosen for each\ncriterion. The selected alpha corresponds to the minimum of the AIC or BIC\ncriterion.",
            "markdown"
        ],
        [
            "ax = results.plot()\nax.vlines(\n    alpha_aic,\n    results[\"AIC criterion\"].min(),\n    results[\"AIC criterion\"].max(),\n    label=\"alpha: AIC estimate\",\n    linestyles=\"--\",\n    color=\"tab:blue\",\n)\nax.vlines(\n    alpha_bic,\n    results[\"BIC criterion\"].min(),\n    results[\"BIC criterion\"].max(),\n    label=\"alpha: BIC estimate\",\n    linestyle=\"--\",\n    color=\"tab:orange\",\n)\nax.set_xlabel(r\"$\\alpha$\")\nax.set_ylabel(\"criterion\")\nax.set_xscale(\"log\")\nax.legend()\n_ = ax.set_title(\n    f\"Information-criterion for model selection (training time {fit_time:.2f}s)\"\n)\n\n\n<img alt=\"Information-criterion for model selection (training time 0.01s)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_lasso_model_selection_001.png\" srcset=\"../../_images/sphx_glr_plot_lasso_model_selection_001.png\"/>",
            "code"
        ],
        [
            "Model selection with an information-criterion is very fast. It relies on\ncomputing the criterion on the in-sample set provided to fit. Both criteria\nestimate the model generalization error based on the training set error and\npenalize this overly optimistic error. However, this penalty relies on a\nproper estimation of the degrees of freedom and the noise variance. Both are\nderived for large samples (asymptotic results) and assume the model is\ncorrect, i.e. that the data are actually generated by this model.",
            "markdown"
        ],
        [
            "These models also tend to break when the problem is badly conditioned (more\nfeatures than samples). It is then required to provide an estimate of the\nnoise variance.",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Lasso model selection: AIC-BIC / cross-validation->Selecting Lasso via cross-validation": [
        [
            "The Lasso estimator can be implemented with different solvers: coordinate\ndescent and least angle regression. They differ with regards to their\nexecution speed and sources of numerical errors.",
            "markdown"
        ],
        [
            "In scikit-learn, two different estimators are available with integrated\ncross-validation:  and\n that respectively solve the\nproblem with coordinate descent and least angle regression.",
            "markdown"
        ],
        [
            "In the remainder of this section, we will present both approaches. For both\nalgorithms, we will use a 20-fold cross-validation strategy.",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Lasso model selection: AIC-BIC / cross-validation->Selecting Lasso via cross-validation->Lasso via coordinate descent": [
        [
            "Let\u2019s start by making the hyperparameter tuning using\n.",
            "markdown"
        ],
        [
            "from sklearn.linear_model import \n\nstart_time = ()\nmodel = ((), (cv=20)).fit(X, y)\nfit_time = () - start_time",
            "code"
        ],
        [
            "import matplotlib.pyplot as plt\n\nymin, ymax = 2300, 3800\nlasso = model[-1]\n(lasso.alphas_, lasso.mse_path_, linestyle=\":\")\n(\n    lasso.alphas_,\n    lasso.mse_path_.mean(axis=-1),\n    color=\"black\",\n    label=\"Average across the folds\",\n    linewidth=2,\n)\n(lasso.alpha_, linestyle=\"--\", color=\"black\", label=\"alpha: CV estimate\")\n\n(ymin, ymax)\n(r\"$\\alpha$\")\n(\"Mean square error\")\n()\n_ = (\n    f\"Mean square error on each fold: coordinate descent (train time: {fit_time:.2f}s)\"\n)\n\n\n<img alt=\"Mean square error on each fold: coordinate descent (train time: 0.22s)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_lasso_model_selection_002.png\" srcset=\"../../_images/sphx_glr_plot_lasso_model_selection_002.png\"/>",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Lasso model selection: AIC-BIC / cross-validation->Selecting Lasso via cross-validation->Lasso via least angle regression": [
        [
            "Let\u2019s start by making the hyperparameter tuning using\n.",
            "markdown"
        ],
        [
            "from sklearn.linear_model import \n\nstart_time = ()\nmodel = ((), (cv=20)).fit(X, y)\nfit_time = () - start_time",
            "code"
        ],
        [
            "lasso = model[-1]\n(lasso.cv_alphas_, lasso.mse_path_, \":\")\n(\n    lasso.cv_alphas_,\n    lasso.mse_path_.mean(axis=-1),\n    color=\"black\",\n    label=\"Average across the folds\",\n    linewidth=2,\n)\n(lasso.alpha_, linestyle=\"--\", color=\"black\", label=\"alpha CV\")\n\n(ymin, ymax)\n(r\"$\\alpha$\")\n(\"Mean square error\")\n()\n_ = (f\"Mean square error on each fold: Lars (train time: {fit_time:.2f}s)\")\n\n\n<img alt=\"Mean square error on each fold: Lars (train time: 0.08s)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_lasso_model_selection_003.png\" srcset=\"../../_images/sphx_glr_plot_lasso_model_selection_003.png\"/>",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Lasso model selection: AIC-BIC / cross-validation->Selecting Lasso via cross-validation->Summary of cross-validation approach": [
        [
            "Both algorithms give roughly the same results.",
            "markdown"
        ],
        [
            "Lars computes a solution path only for each kink in the path. As a result, it\nis very efficient when there are only of few kinks, which is the case if\nthere are few features or samples. Also, it is able to compute the full path\nwithout setting any hyperparameter. On the opposite, coordinate descent\ncomputes the path points on a pre-specified grid (here we use the default).\nThus it is more efficient if the number of grid points is smaller than the\nnumber of kinks in the path. Such a strategy can be interesting if the number\nof features is really large and there are enough samples to be selected in\neach of the cross-validation fold. In terms of numerical errors, for heavily\ncorrelated variables, Lars will accumulate more errors, while the coordinate\ndescent algorithm will only sample the path on a grid.",
            "markdown"
        ],
        [
            "Note how the optimal value of alpha varies for each fold. This illustrates\nwhy nested-cross validation is a good strategy when trying to evaluate the\nperformance of a method for which a parameter is chosen by cross-validation:\nthis choice of parameter may not be optimal for a final evaluation on\nunseen test set only.",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Lasso model selection: AIC-BIC / cross-validation->Conclusion": [
        [
            "In this tutorial, we presented two approaches for selecting the best\nhyperparameter alpha: one strategy finds the optimal value of alpha\nby only using the training set and some information criterion, and another\nstrategy is based on cross-validation.",
            "markdown"
        ],
        [
            "In this example, both approaches are working similarly. The in-sample\nhyperparameter selection even shows its efficacy in terms of computational\nperformance. However, it can only be used when the number of samples is large\nenough compared to the number of features.",
            "markdown"
        ],
        [
            "That\u2019s why hyperparameter optimization via cross-validation is a safe\nstrategy: it works in different settings.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.925 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Lasso on dense and sparse data": [
        [
            "We show that linear_model.Lasso provides the same results for dense and sparse\ndata and that in the case of sparse data the speed is improved.",
            "markdown"
        ],
        [
            "from time import \nfrom scipy import sparse\nfrom scipy import linalg\n\nfrom sklearn.datasets import \nfrom sklearn.linear_model import",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Lasso on dense and sparse data->Comparing the two Lasso implementations on Dense data": [
        [
            "We create a linear regression problem that is suitable for the Lasso,\nthat is to say, with more features than samples. We then store the data\nmatrix in both dense (the usual) and sparse format, and train a Lasso on\neach. We compute the runtime of both and check that they learned the\nsame model by computing the Euclidean norm of the difference between the\ncoefficients they learned. Because the data is dense, we expect better\nruntime with a dense data format.",
            "markdown"
        ],
        [
            "X, y = (n_samples=200, n_features=5000, random_state=0)\n# create a copy of X in sparse format\nX_sp = (X)\n\nalpha = 1\nsparse_lasso = (alpha=alpha, fit_intercept=False, max_iter=1000)\ndense_lasso = (alpha=alpha, fit_intercept=False, max_iter=1000)\n\nt0 = ()\nsparse_lasso.fit(X_sp, y)\nprint(f\"Sparse Lasso done in {(() - t0):.3f}s\")\n\nt0 = ()\ndense_lasso.fit(X, y)\nprint(f\"Dense Lasso done in {(() - t0):.3f}s\")\n\n# compare the regression coefficients\ncoeff_diff = (sparse_lasso.coef_ - dense_lasso.coef_)\nprint(f\"Distance between coefficients : {coeff_diff:.2e}\")\n\n#",
            "code"
        ],
        [
            "Sparse Lasso done in 0.108s\nDense Lasso done in 0.036s\nDistance between coefficients : 1.01e-13",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Lasso on dense and sparse data->Comparing the two Lasso implementations on Sparse data": [
        [
            "We make the previous problem sparse by replacing all small values with 0\nand run the same comparisons as above. Because the data is now sparse, we\nexpect the implementation that uses the sparse data format to be faster.",
            "markdown"
        ],
        [
            "# make a copy of the previous data\nXs = X.copy()\n# make Xs sparse by replacing the values lower than 2.5 with 0s\nXs[Xs &lt; 2.5] = 0.0\n# create a copy of Xs in sparse format\nXs_sp = (Xs)\nXs_sp = Xs_sp.tocsc()\n\n# compute the proportion of non-zero coefficient in the data matrix\nprint(f\"Matrix density : {(Xs_sp.nnz / float(X.size) * 100):.3f}%\")\n\nalpha = 0.1\nsparse_lasso = (alpha=alpha, fit_intercept=False, max_iter=10000)\ndense_lasso = (alpha=alpha, fit_intercept=False, max_iter=10000)\n\nt0 = ()\nsparse_lasso.fit(Xs_sp, y)\nprint(f\"Sparse Lasso done in {(() - t0):.3f}s\")\n\nt0 = ()\ndense_lasso.fit(Xs, y)\nprint(f\"Dense Lasso done in  {(() - t0):.3f}s\")\n\n# compare the regression coefficients\ncoeff_diff = (sparse_lasso.coef_ - dense_lasso.coef_)\nprint(f\"Distance between coefficients : {coeff_diff:.2e}\")",
            "code"
        ],
        [
            "Matrix density : 0.626%\nSparse Lasso done in 0.117s\nDense Lasso done in  0.805s\nDistance between coefficients : 8.65e-12",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  1.137 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Lasso path using LARS": [
        [
            "Computes Lasso Path along the regularization parameter using the LARS\nalgorithm on the diabetes dataset. Each color represents a different\nfeature of the coefficient vector, and this is displayed as a function\nof the regularization parameter.\n<img alt=\"LASSO Path\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_lasso_lars_001.png\" srcset=\"../../_images/sphx_glr_plot_lasso_lars_001.png\"/>",
            "markdown"
        ],
        [
            "Computing regularization path using the LARS ...\n.\n\n\n\n<br/>",
            "code"
        ],
        [
            "# Author: Fabian Pedregosa &lt;fabian.pedregosa@inria.fr\n#         Alexandre Gramfort &lt;alexandre.gramfort@inria.fr\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import linear_model\nfrom sklearn import datasets\n\nX, y = (return_X_y=True)\n\nprint(\"Computing regularization path using the LARS ...\")\n_, _, coefs = (X, y, method=\"lasso\", verbose=True)\n\nxx = (np.abs(coefs.T), axis=1)\nxx /= xx[-1]\n\n(xx, coefs.T)\nymin, ymax = ()\n(xx, ymin, ymax, linestyle=\"dashed\")\n(\"|coef| / max|coef|\")\n(\"Coefficients\")\n(\"LASSO Path\")\n(\"tight\")\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.082 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Linear Regression Example": [
        [
            "The example below uses only the first feature of the diabetes dataset,\nin order to illustrate the data points within the two-dimensional plot.\nThe straight line can be seen in the plot, showing how linear regression\nattempts to draw a straight line that will best minimize the\nresidual sum of squares between the observed responses in the dataset,\nand the responses predicted by the linear approximation.",
            "markdown"
        ],
        [
            "The coefficients, residual sum of squares and the coefficient of\ndetermination are also calculated.\n<img alt=\"plot ols\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_ols_001.png\" srcset=\"../../_images/sphx_glr_plot_ols_001.png\"/>",
            "markdown"
        ],
        [
            "Coefficients:\n [938.23786125]\nMean squared error: 2548.07\nCoefficient of determination: 0.47\n\n\n\n<br/>",
            "code"
        ],
        [
            "# Code source: Jaques Grobler\n# License: BSD 3 clause\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import datasets, linear_model\nfrom sklearn.metrics import , \n\n# Load the diabetes dataset\ndiabetes_X, diabetes_y = (return_X_y=True)\n\n# Use only one feature\ndiabetes_X = diabetes_X[:, , 2]\n\n# Split the data into training/testing sets\ndiabetes_X_train = diabetes_X[:-20]\ndiabetes_X_test = diabetes_X[-20:]\n\n# Split the targets into training/testing sets\ndiabetes_y_train = diabetes_y[:-20]\ndiabetes_y_test = diabetes_y[-20:]\n\n# Create linear regression object\nregr = ()\n\n# Train the model using the training sets\nregr.fit(diabetes_X_train, diabetes_y_train)\n\n# Make predictions using the testing set\ndiabetes_y_pred = regr.predict(diabetes_X_test)\n\n# The coefficients\nprint(\"Coefficients: \\n\", regr.coef_)\n# The mean squared error\nprint(\"Mean squared error: %.2f\" % (diabetes_y_test, diabetes_y_pred))\n# The coefficient of determination: 1 is perfect prediction\nprint(\"Coefficient of determination: %.2f\" % (diabetes_y_test, diabetes_y_pred))\n\n# Plot outputs\n(diabetes_X_test, diabetes_y_test, color=\"black\")\n(diabetes_X_test, diabetes_y_pred, color=\"blue\", linewidth=3)\n\n(())\n(())\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.039 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Logistic Regression 3-class Classifier": [
        [
            "Show below is a logistic-regression classifiers decision boundaries on the\nfirst two dimensions (sepal length and width) of the  dataset. The datapoints\nare colored according to their labels.\n<img alt=\"plot iris logistic\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_iris_logistic_001.png\" srcset=\"../../_images/sphx_glr_plot_iris_logistic_001.png\"/>",
            "markdown"
        ],
        [
            "# Code source: Ga\u00ebl Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import \nfrom sklearn import datasets\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\n# import some data to play with\niris = ()\nX = iris.data[:, :2]  # we only take the first two features.\nY = iris.target\n\n# Create an instance of Logistic Regression Classifier and fit the data.\nlogreg = (C=1e5)\nlogreg.fit(X, Y)\n\n_, ax = (figsize=(4, 3))\n(\n    logreg,\n    X,\n    cmap=plt.cm.Paired,\n    ax=ax,\n    response_method=\"predict\",\n    plot_method=\"pcolormesh\",\n    shading=\"auto\",\n    xlabel=\"Sepal length\",\n    ylabel=\"Sepal width\",\n    eps=0.5,\n)\n\n# Plot also the training points\n(X[:, 0], X[:, 1], c=Y, edgecolors=\"k\", cmap=plt.cm.Paired)\n\n\n(())\n(())\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.050 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Logistic function": [
        [
            "Shown in the plot is how the logistic regression would, in this\nsynthetic dataset, classify values as either 0 or 1,\ni.e. class one or two, using the logistic curve.\n<img alt=\"plot logistic\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_logistic_001.png\" srcset=\"../../_images/sphx_glr_plot_logistic_001.png\"/>",
            "markdown"
        ],
        [
            "# Code source: Gael Varoquaux\n# License: BSD 3 clause\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.special import \nfrom sklearn.linear_model import , \n\n# Generate a toy dataset, it's just a straight line with some Gaussian noise:\nxmin, xmax = -5, 5\nn_samples = 100\n(0)\nX = (size=n_samples)\ny = (X  0).astype(float)\nX[X  0] *= 4\nX += 0.3 * (size=n_samples)\n\nX = X[:, ]\n\n# Fit the classifier\nclf = (C=1e5)\nclf.fit(X, y)\n\n# and plot the result\n(1, figsize=(4, 3))\n()\n(X.ravel(), y, label=\"example data\", color=\"black\", zorder=20)\nX_test = (-5, 10, 300)\n\nloss = (X_test * clf.coef_ + clf.intercept_).ravel()\n(X_test, loss, label=\"Logistic Regression Model\", color=\"red\", linewidth=3)\n\nols = ()\nols.fit(X, y)\n(\n    X_test,\n    ols.coef_ * X_test + ols.intercept_,\n    label=\"Linear Regression Model\",\n    linewidth=1,\n)\n(0.5, color=\".5\")\n\n(\"y\")\n(\"X\")\n(range(-5, 10))\n([0, 0.5, 1])\n(-0.25, 1.25)\n(-4, 10)\n(\n    loc=\"lower right\",\n    fontsize=\"small\",\n)\n()\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.090 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->MNIST classification using multinomial logistic + L1": [
        [
            "Here we fit a multinomial logistic regression with L1 penalty on a subset of\nthe MNIST digits classification task. We use the SAGA algorithm for this\npurpose: this a solver that is fast when the number of samples is significantly\nlarger than the number of features and is able to finely optimize non-smooth\nobjective functions which is the case with the l1-penalty. Test accuracy\nreaches > 0.8, while weight vectors remains sparse and therefore more easily\ninterpretable.",
            "markdown"
        ],
        [
            "Note that this accuracy of this l1-penalized linear model is significantly\nbelow what can be reached by an l2-penalized linear model or a non-linear\nmulti-layer perceptron model on this dataset.\n<img alt=\"Classification vector for...\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_sparse_logistic_regression_mnist_001.png\" srcset=\"../../_images/sphx_glr_plot_sparse_logistic_regression_mnist_001.png\"/>",
            "markdown"
        ],
        [
            "Sparsity with L1 penalty: 74.57%\nTest score with L1 penalty: 0.8253\nExample run in 7.308 s\n\n\n\n<br/>",
            "code"
        ],
        [
            "# Author: Arthur Mensch &lt;arthur.mensch@m4x.org\n# License: BSD 3 clause\n\nimport time\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import \nfrom sklearn.linear_model import \nfrom sklearn.model_selection import \nfrom sklearn.preprocessing import \nfrom sklearn.utils import \n\n# Turn down for faster convergence\nt0 = ()\ntrain_samples = 5000\n\n# Load data from https://www.openml.org/d/554\nX, y = (\n    \"mnist_784\", version=1, return_X_y=True, as_frame=False, parser=\"pandas\"\n)\n\nrandom_state = (0)\npermutation = random_state.permutation(X.shape[0])\nX = X[permutation]\ny = y[permutation]\nX = X.reshape((X.shape[0], -1))\n\nX_train, X_test, y_train, y_test = (\n    X, y, train_size=train_samples, test_size=10000\n)\n\nscaler = ()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Turn up tolerance for faster convergence\nclf = (C=50.0 / train_samples, penalty=\"l1\", solver=\"saga\", tol=0.1)\nclf.fit(X_train, y_train)\nsparsity = (clf.coef_ == 0) * 100\nscore = clf.score(X_test, y_test)\n# print('Best C % .4f' % clf.C_)\nprint(\"Sparsity with L1 penalty: %.2f%%\" % sparsity)\nprint(\"Test score with L1 penalty: %.4f\" % score)\n\ncoef = clf.coef_.copy()\n(figsize=(10, 5))\nscale = np.abs(coef).max()\nfor i in range(10):\n    l1_plot = (2, 5, i + 1)\n    l1_plot.imshow(\n        coef[i].reshape(28, 28),\n        interpolation=\"nearest\",\n        cmap=plt.cm.RdBu,\n        vmin=-scale,\n        vmax=scale,\n    )\n    l1_plot.set_xticks(())\n    l1_plot.set_yticks(())\n    l1_plot.set_xlabel(\"Class %i\" % i)\n(\"Classification vector for...\")\n\nrun_time = () - t0\nprint(\"Example run in %.3f s\" % run_time)\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  7.378 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Multiclass sparse logistic regression on 20newgroups": [
        [
            "Comparison of multinomial logistic L1 vs one-versus-rest L1 logistic regression\nto classify documents from the newgroups20 dataset. Multinomial logistic\nregression yields more accurate results and is faster to train on the larger\nscale dataset.",
            "markdown"
        ],
        [
            "Here we use the l1 sparsity that trims the weights of not informative\nfeatures to zero. This is good if the goal is to extract the strongly\ndiscriminative vocabulary of each class. If the goal is to get the best\npredictive accuracy, it is better to use the non sparsity-inducing l2 penalty\ninstead.",
            "markdown"
        ],
        [
            "A more traditional (and possibly better) way to predict on a sparse subset of\ninput features would be to use univariate feature selection followed by a\ntraditional (l2-penalised) logistic regression model.\n<img alt=\"Multinomial vs One-vs-Rest Logistic L1 Dataset 20newsgroups\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_sparse_logistic_regression_20newsgroups_001.png\" srcset=\"../../_images/sphx_glr_plot_sparse_logistic_regression_20newsgroups_001.png\"/>",
            "markdown"
        ],
        [
            "Dataset 20newsgroup, train_samples=4500, n_features=130107, n_classes=20\n[model=One versus Rest, solver=saga] Number of epochs: 1\n[model=One versus Rest, solver=saga] Number of epochs: 2\n[model=One versus Rest, solver=saga] Number of epochs: 3\nTest accuracy for model ovr: 0.5960\n% non-zero coefficients for model ovr, per class:\n [0.26593496 0.43348936 0.26362917 0.31973683 0.37815029 0.2928359\n 0.27054655 0.62717609 0.19522393 0.30897646 0.34586917 0.28207552\n 0.34125758 0.29898468 0.34279478 0.59489497 0.38353048 0.35278655\n 0.19829832 0.14603365]\nRun time (3 epochs) for model ovr:1.28\n[model=Multinomial, solver=saga] Number of epochs: 1\n[model=Multinomial, solver=saga] Number of epochs: 2\n[model=Multinomial, solver=saga] Number of epochs: 5\nTest accuracy for model multinomial: 0.6440\n% non-zero coefficients for model multinomial, per class:\n [0.36047253 0.1268187  0.10606655 0.17985197 0.5395559  0.07993421\n 0.06686804 0.21443888 0.11528972 0.2075215  0.10914094 0.11144673\n 0.13988486 0.09684337 0.26286057 0.11682692 0.55800226 0.17370318\n 0.11452112 0.14603365]\nRun time (5 epochs) for model multinomial:1.09\nExample run in 5.421 s\n\n\n\n<br/>",
            "code"
        ],
        [
            "# Author: Arthur Mensch\n\nimport timeit\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import \nfrom sklearn.linear_model import \nfrom sklearn.model_selection import \nfrom sklearn.exceptions import \n\n(\"ignore\", category=, module=\"sklearn\")\nt0 = ()\n\n# We use SAGA solver\nsolver = \"saga\"\n\n# Turn down for faster run time\nn_samples = 5000\n\nX, y = (subset=\"all\", return_X_y=True)\nX = X[:n_samples]\ny = y[:n_samples]\n\nX_train, X_test, y_train, y_test = (\n    X, y, random_state=42, stratify=y, test_size=0.1\n)\ntrain_samples, n_features = X_train.shape\nn_classes = (y).shape[0]\n\nprint(\n    \"Dataset 20newsgroup, train_samples=%i, n_features=%i, n_classes=%i\"\n    % (train_samples, n_features, n_classes)\n)\n\nmodels = {\n    \"ovr\": {\"name\": \"One versus Rest\", \"iters\": [1, 2, 3]},\n    \"multinomial\": {\"name\": \"Multinomial\", \"iters\": [1, 2, 5]},\n}\n\nfor model in models:\n    # Add initial chance-level values for plotting purpose\n    accuracies = [1 / n_classes]\n    times = [0]\n    densities = [1]\n\n    model_params = models[model]\n\n    # Small number of epochs for fast runtime\n    for this_max_iter in model_params[\"iters\"]:\n        print(\n            \"[model=%s, solver=%s] Number of epochs: %s\"\n            % (model_params[\"name\"], solver, this_max_iter)\n        )\n        lr = (\n            solver=solver,\n            multi_class=model,\n            penalty=\"l1\",\n            max_iter=this_max_iter,\n            random_state=42,\n        )\n        t1 = ()\n        lr.fit(X_train, y_train)\n        train_time = () - t1\n\n        y_pred = lr.predict(X_test)\n        accuracy = (y_pred == y_test) / y_test.shape[0]\n        density = (lr.coef_ != 0, axis=1) * 100\n        accuracies.append(accuracy)\n        densities.append(density)\n        times.append(train_time)\n    models[model][\"times\"] = times\n    models[model][\"densities\"] = densities\n    models[model][\"accuracies\"] = accuracies\n    print(\"Test accuracy for model %s: %.4f\" % (model, accuracies[-1]))\n    print(\n        \"%% non-zero coefficients for model %s, per class:\\n %s\"\n        % (model, densities[-1])\n    )\n    print(\n        \"Run time (%i epochs) for model %s:%.2f\"\n        % (model_params[\"iters\"][-1], model, times[-1])\n    )\n\nfig = ()\nax = fig.add_subplot(111)\n\nfor model in models:\n    name = models[model][\"name\"]\n    times = models[model][\"times\"]\n    accuracies = models[model][\"accuracies\"]\n    ax.plot(times, accuracies, marker=\"o\", label=\"Model: %s\" % name)\n    ax.set_xlabel(\"Train time (s)\")\n    ax.set_ylabel(\"Test accuracy\")\nax.legend()\nfig.suptitle(\"Multinomial vs One-vs-Rest Logistic L1\\nDataset %s\" % \"20newsgroups\")\nfig.tight_layout()\nfig.subplots_adjust(top=0.85)\nrun_time = () - t0\nprint(\"Example run in %.3f s\" % run_time)\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  5.470 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Non-negative least squares": [
        [
            "In this example, we fit a linear model with positive constraints on the\nregression coefficients and compare the estimated coefficients to a classic\nlinear regression.",
            "markdown"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import",
            "code"
        ],
        [
            "Generate some random data",
            "markdown"
        ],
        [
            "(42)\n\nn_samples, n_features = 200, 50\nX = (n_samples, n_features)\ntrue_coef = 3 * (n_features)\n# Threshold coefficients to render them non-negative\ntrue_coef[true_coef &lt; 0] = 0\ny = (X, true_coef)\n\n# Add some noise\ny += 5 * (size=(n_samples,))",
            "code"
        ],
        [
            "Split the data in train set and test set",
            "markdown"
        ],
        [
            "from sklearn.model_selection import \n\nX_train, X_test, y_train, y_test = (X, y, test_size=0.5)",
            "code"
        ],
        [
            "Fit the Non-Negative least squares.",
            "markdown"
        ],
        [
            "from sklearn.linear_model import \n\nreg_nnls = (positive=True)\ny_pred_nnls = reg_nnls.fit(X_train, y_train).predict(X_test)\nr2_score_nnls = (y_test, y_pred_nnls)\nprint(\"NNLS R2 score\", r2_score_nnls)",
            "code"
        ],
        [
            "NNLS R2 score 0.8225220806196526",
            "code"
        ],
        [
            "Fit an OLS.",
            "markdown"
        ],
        [
            "reg_ols = ()\ny_pred_ols = reg_ols.fit(X_train, y_train).predict(X_test)\nr2_score_ols = (y_test, y_pred_ols)\nprint(\"OLS R2 score\", r2_score_ols)",
            "code"
        ],
        [
            "OLS R2 score 0.7436926291700356",
            "code"
        ],
        [
            "Comparing the regression coefficients between OLS and NNLS, we can observe\nthey are highly correlated (the dashed line is the identity relation),\nbut the non-negative constraint shrinks some to 0.\nThe Non-Negative Least squares inherently yield sparse results.",
            "markdown"
        ],
        [
            "fig, ax = ()\nax.plot(reg_ols.coef_, reg_nnls.coef_, linewidth=0, marker=\".\")\n\nlow_x, high_x = ax.get_xlim()\nlow_y, high_y = ax.get_ylim()\nlow = max(low_x, low_y)\nhigh = min(high_x, high_y)\nax.plot([low, high], [low, high], ls=\"--\", c=\".3\", alpha=0.5)\nax.set_xlabel(\"OLS regression coefficients\", fontweight=\"bold\")\nax.set_ylabel(\"NNLS regression coefficients\", fontweight=\"bold\")\n\n\n<img alt=\"plot nnls\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_nnls_001.png\" srcset=\"../../_images/sphx_glr_plot_nnls_001.png\"/>",
            "code"
        ],
        [
            "Text(55.847222222222214, 0.5, 'NNLS regression coefficients')",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.064 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->One-Class SVM versus One-Class SVM using Stochastic Gradient Descent": [
        [
            "This example shows how to approximate the solution of\n in the case of an RBF kernel with\n, a Stochastic Gradient Descent\n(SGD) version of the One-Class SVM. A kernel approximation is first used in\norder to apply  which implements a\nlinear One-Class SVM using SGD.",
            "markdown"
        ],
        [
            "Note that  scales linearly with\nthe number of samples whereas the complexity of a kernelized\n is at best quadratic with respect to the\nnumber of samples. It is not the purpose of this example to illustrate the\nbenefits of such an approximation in terms of computation time but rather to\nshow that we obtain similar results on a toy dataset.\n\n<img alt=\"One Class SVM\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_sgdocsvm_vs_ocsvm_001.png\" srcset=\"../../_images/sphx_glr_plot_sgdocsvm_vs_ocsvm_001.png\"/>\n<img alt=\"Online One-Class SVM\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_sgdocsvm_vs_ocsvm_002.png\" srcset=\"../../_images/sphx_glr_plot_sgdocsvm_vs_ocsvm_002.png\"/>",
            "markdown"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nfrom sklearn.svm import \nfrom sklearn.linear_model import \nfrom sklearn.kernel_approximation import \nfrom sklearn.pipeline import \n\nfont = {\"weight\": \"normal\", \"size\": 15}\n\n(\"font\", **font)\n\nrandom_state = 42\nrng = (random_state)\n\n# Generate train data\nX = 0.3 * rng.randn(500, 2)\nX_train = [X + 2, X - 2]\n# Generate some regular novel observations\nX = 0.3 * rng.randn(20, 2)\nX_test = [X + 2, X - 2]\n# Generate some abnormal novel observations\nX_outliers = rng.uniform(low=-4, high=4, size=(20, 2))\n\nxx, yy = ((-4.5, 4.5, 50), (-4.5, 4.5, 50))\n\n# OCSVM hyperparameters\nnu = 0.05\ngamma = 2.0\n\n# Fit the One-Class SVM\nclf = (gamma=gamma, kernel=\"rbf\", nu=nu)\nclf.fit(X_train)\ny_pred_train = clf.predict(X_train)\ny_pred_test = clf.predict(X_test)\ny_pred_outliers = clf.predict(X_outliers)\nn_error_train = y_pred_train[y_pred_train == -1].size\nn_error_test = y_pred_test[y_pred_test == -1].size\nn_error_outliers = y_pred_outliers[y_pred_outliers == 1].size\n\nZ = clf.decision_function([xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\n\n# Fit the One-Class SVM using a kernel approximation and SGD\ntransform = (gamma=gamma, random_state=random_state)\nclf_sgd = (\n    nu=nu, shuffle=True, fit_intercept=True, random_state=random_state, tol=1e-4\n)\npipe_sgd = (transform, clf_sgd)\npipe_sgd.fit(X_train)\ny_pred_train_sgd = pipe_sgd.predict(X_train)\ny_pred_test_sgd = pipe_sgd.predict(X_test)\ny_pred_outliers_sgd = pipe_sgd.predict(X_outliers)\nn_error_train_sgd = y_pred_train_sgd[y_pred_train_sgd == -1].size\nn_error_test_sgd = y_pred_test_sgd[y_pred_test_sgd == -1].size\nn_error_outliers_sgd = y_pred_outliers_sgd[y_pred_outliers_sgd == 1].size\n\nZ_sgd = pipe_sgd.decision_function([xx.ravel(), yy.ravel()])\nZ_sgd = Z_sgd.reshape(xx.shape)\n\n# plot the level sets of the decision function\n(figsize=(9, 6))\n(\"One Class SVM\")\n(xx, yy, Z, levels=(Z.min(), 0, 7), cmap=plt.cm.PuBu)\na = (xx, yy, Z, levels=[0], linewidths=2, colors=\"darkred\")\n(xx, yy, Z, levels=[0, Z.max()], colors=\"palevioletred\")\n\ns = 20\nb1 = (X_train[:, 0], X_train[:, 1], c=\"white\", s=s, edgecolors=\"k\")\nb2 = (X_test[:, 0], X_test[:, 1], c=\"blueviolet\", s=s, edgecolors=\"k\")\nc = (X_outliers[:, 0], X_outliers[:, 1], c=\"gold\", s=s, edgecolors=\"k\")\n(\"tight\")\n((-4.5, 4.5))\n((-4.5, 4.5))\n(\n    [a.collections[0], b1, b2, c],\n    [\n        \"learned frontier\",\n        \"training observations\",\n        \"new regular observations\",\n        \"new abnormal observations\",\n    ],\n    loc=\"upper left\",\n)\n(\n    \"error train: %d/%d; errors novel regular: %d/%d; errors novel abnormal: %d/%d\"\n    % (\n        n_error_train,\n        X_train.shape[0],\n        n_error_test,\n        X_test.shape[0],\n        n_error_outliers,\n        X_outliers.shape[0],\n    )\n)\n()\n\n(figsize=(9, 6))\n(\"Online One-Class SVM\")\n(xx, yy, Z_sgd, levels=(Z_sgd.min(), 0, 7), cmap=plt.cm.PuBu)\na = (xx, yy, Z_sgd, levels=[0], linewidths=2, colors=\"darkred\")\n(xx, yy, Z_sgd, levels=[0, Z_sgd.max()], colors=\"palevioletred\")\n\ns = 20\nb1 = (X_train[:, 0], X_train[:, 1], c=\"white\", s=s, edgecolors=\"k\")\nb2 = (X_test[:, 0], X_test[:, 1], c=\"blueviolet\", s=s, edgecolors=\"k\")\nc = (X_outliers[:, 0], X_outliers[:, 1], c=\"gold\", s=s, edgecolors=\"k\")\n(\"tight\")\n((-4.5, 4.5))\n((-4.5, 4.5))\n(\n    [a.collections[0], b1, b2, c],\n    [\n        \"learned frontier\",\n        \"training observations\",\n        \"new regular observations\",\n        \"new abnormal observations\",\n    ],\n    loc=\"upper left\",\n)\n(\n    \"error train: %d/%d; errors novel regular: %d/%d; errors novel abnormal: %d/%d\"\n    % (\n        n_error_train_sgd,\n        X_train.shape[0],\n        n_error_test_sgd,\n        X_test.shape[0],\n        n_error_outliers_sgd,\n        X_outliers.shape[0],\n    )\n)\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.304 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Ordinary Least Squares and Ridge Regression Variance": [
        [
            "Due to the few points in each dimension and the straight\nline that linear regression uses to follow these points\nas well as it can, noise on the observations will cause\ngreat variance as shown in the first plot. Every line\u2019s slope\ncan vary quite a bit for each prediction due to the noise\ninduced in the observations.",
            "markdown"
        ],
        [
            "Ridge regression is basically minimizing a penalised version\nof the least-squared function. The penalising shrinks the\nvalue of the regression coefficients.\nDespite the few data points in each dimension, the slope\nof the prediction is much more stable and the variance\nin the line itself is greatly reduced, in comparison to that\nof the standard linear regression\n\n<img alt=\"ols\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_ols_ridge_variance_001.png\" srcset=\"../../_images/sphx_glr_plot_ols_ridge_variance_001.png\"/>\n<img alt=\"ridge\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_ols_ridge_variance_002.png\" srcset=\"../../_images/sphx_glr_plot_ols_ridge_variance_002.png\"/>",
            "markdown"
        ],
        [
            "# Code source: Ga\u00ebl Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import linear_model\n\nX_train = [0.5, 1].T\ny_train = [0.5, 1]\nX_test = [0, 2].T\n\n(0)\n\nclassifiers = dict(\n    ols=(), ridge=(alpha=0.1)\n)\n\nfor name, clf in classifiers.items():\n    fig, ax = (figsize=(4, 3))\n\n    for _ in range(6):\n        this_X = 0.1 * (size=(2, 1)) + X_train\n        clf.fit(this_X, y_train)\n\n        ax.plot(X_test, clf.predict(X_test), color=\"gray\")\n        ax.scatter(this_X, y_train, s=3, c=\"gray\", marker=\"o\", zorder=10)\n\n    clf.fit(X_train, y_train)\n    ax.plot(X_test, clf.predict(X_test), linewidth=2, color=\"blue\")\n    ax.scatter(X_train, y_train, s=30, c=\"red\", marker=\"+\", zorder=10)\n\n    ax.set_title(name)\n    ax.set_xlim(0, 2)\n    ax.set_ylim((0, 1.6))\n    ax.set_xlabel(\"X\")\n    ax.set_ylabel(\"y\")\n\n    fig.tight_layout()\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.203 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Orthogonal Matching Pursuit": [
        [
            "Using orthogonal matching pursuit for recovering a sparse signal from a noisy\nmeasurement encoded with a dictionary\n<img alt=\"Sparse signal recovery with Orthogonal Matching Pursuit, Sparse signal, Recovered signal from noise-free measurements, Recovered signal from noisy measurements, Recovered signal from noisy measurements with CV\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_omp_001.png\" srcset=\"../../_images/sphx_glr_plot_omp_001.png\"/>",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.linear_model import \nfrom sklearn.linear_model import \nfrom sklearn.datasets import \n\nn_components, n_features = 512, 100\nn_nonzero_coefs = 17\n\n# generate the data\n\n# y = Xw\n# |x|_0 = n_nonzero_coefs\n\ny, X, w = (\n    n_samples=1,\n    n_components=n_components,\n    n_features=n_features,\n    n_nonzero_coefs=n_nonzero_coefs,\n    random_state=0,\n    data_transposed=True,\n)\n\n(idx,) = w.nonzero()\n\n# distort the clean signal\ny_noisy = y + 0.05 * (len(y))\n\n# plot the sparse signal\n(figsize=(7, 7))\n(4, 1, 1)\n(0, 512)\n(\"Sparse signal\")\n(idx, w[idx])\n\n# plot the noise-free reconstruction\nomp = (n_nonzero_coefs=n_nonzero_coefs)\nomp.fit(X, y)\ncoef = omp.coef_\n(idx_r,) = coef.nonzero()\n(4, 1, 2)\n(0, 512)\n(\"Recovered signal from noise-free measurements\")\n(idx_r, coef[idx_r])\n\n# plot the noisy reconstruction\nomp.fit(X, y_noisy)\ncoef = omp.coef_\n(idx_r,) = coef.nonzero()\n(4, 1, 3)\n(0, 512)\n(\"Recovered signal from noisy measurements\")\n(idx_r, coef[idx_r])\n\n# plot the noisy reconstruction with number of non-zeros set by CV\nomp_cv = ()\nomp_cv.fit(X, y_noisy)\ncoef = omp_cv.coef_\n(idx_r,) = coef.nonzero()\n(4, 1, 4)\n(0, 512)\n(\"Recovered signal from noisy measurements with CV\")\n(idx_r, coef[idx_r])\n\n(0.06, 0.04, 0.94, 0.90, 0.20, 0.38)\n(\"Sparse signal recovery with Orthogonal Matching Pursuit\", fontsize=16)\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.203 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Plot Ridge coefficients as a function of the L2 regularization": [
        [
            "Regression is the estimator used in this example.\nEach color in the left plot represents one different dimension of the\ncoefficient vector, and this is displayed as a function of the\nregularization parameter. The right plot shows how exact the solution\nis. This example illustrates how a well defined solution is\nfound by Ridge regression and how regularization affects the\ncoefficients and their values. The plot on the right shows how\nthe difference of the coefficients from the estimator changes\nas a function of regularization.",
            "markdown"
        ],
        [
            "In this example the dependent variable Y is set as a function\nof the input features: y = X*w + c. The coefficient vector w is\nrandomly sampled from a normal distribution, whereas the bias term c is\nset to a constant.",
            "markdown"
        ],
        [
            "As alpha tends toward zero the coefficients found by Ridge\nregression stabilize towards the randomly sampled vector w.\nFor big alpha (strong regularisation) the coefficients\nare smaller (eventually converging at 0) leading to a\nsimpler and biased solution.\nThese dependencies can be observed on the left plot.",
            "markdown"
        ],
        [
            "The right plot shows the mean squared error between the\ncoefficients found by the model and the chosen vector w.\nLess regularised models retrieve the exact\ncoefficients (error is equal to 0), stronger regularised\nmodels increase the error.",
            "markdown"
        ],
        [
            "Please note that in this example the data is non-noisy, hence\nit is possible to extract the exact coefficients.\n<img alt=\"Ridge coefficients as a function of the regularization, Coefficient error as a function of the regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_ridge_coeffs_001.png\" srcset=\"../../_images/sphx_glr_plot_ridge_coeffs_001.png\"/>",
            "markdown"
        ],
        [
            "# Author: Kornel Kielczewski -- &lt;kornel.k@plusnet.pl\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import \nfrom sklearn.linear_model import \nfrom sklearn.metrics import \n\nclf = ()\n\nX, y, w = (\n    n_samples=10, n_features=10, coef=True, random_state=1, bias=3.5\n)\n\ncoefs = []\nerrors = []\n\nalphas = (-6, 6, 200)\n\n# Train the model with different regularisation strengths\nfor a in alphas:\n    clf.set_params(alpha=a)\n    clf.fit(X, y)\n    coefs.append(clf.coef_)\n    errors.append((clf.coef_, w))\n\n# Display results\n(figsize=(20, 6))\n\n(121)\nax = ()\nax.plot(alphas, coefs)\nax.set_xscale(\"log\")\n(\"alpha\")\n(\"weights\")\n(\"Ridge coefficients as a function of the regularization\")\n(\"tight\")\n\n(122)\nax = ()\nax.plot(alphas, errors)\nax.set_xscale(\"log\")\n(\"alpha\")\n(\"error\")\n(\"Coefficient error as a function of the regularization\")\n(\"tight\")\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.335 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Plot Ridge coefficients as a function of the regularization": [
        [
            "Shows the effect of collinearity in the coefficients of an estimator.",
            "markdown"
        ],
        [
            "Regression is the estimator used in this example.\nEach color represents a different feature of the\ncoefficient vector, and this is displayed as a function of the\nregularization parameter.",
            "markdown"
        ],
        [
            "This example also shows the usefulness of applying Ridge regression\nto highly ill-conditioned matrices. For such matrices, a slight\nchange in the target variable can cause huge variances in the\ncalculated weights. In such cases, it is useful to set a certain\nregularization (alpha) to reduce this variation (noise).",
            "markdown"
        ],
        [
            "When alpha is very large, the regularization effect dominates the\nsquared loss function and the coefficients tend to zero.\nAt the end of the path, as alpha tends toward zero\nand the solution tends towards the ordinary least squares, coefficients\nexhibit big oscillations. In practise it is necessary to tune alpha\nin such a way that a balance is maintained between both.",
            "markdown"
        ],
        [
            "# Author: Fabian Pedregosa -- &lt;fabian.pedregosa@inria.fr\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model\n\n# X is the 10x10 Hilbert matrix\nX = 1.0 / ((1, 11) + (0, 10)[:, ])\ny = (10)",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Plot Ridge coefficients as a function of the regularization->Compute paths": [
        [
            "n_alphas = 200\nalphas = (-10, -2, n_alphas)\n\ncoefs = []\nfor a in alphas:\n    ridge = (alpha=a, fit_intercept=False)\n    ridge.fit(X, y)\n    coefs.append(ridge.coef_)",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Plot Ridge coefficients as a function of the regularization->Display results": [
        [
            "ax = ()\n\nax.plot(alphas, coefs)\nax.set_xscale(\"log\")\nax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\n(\"alpha\")\n(\"weights\")\n(\"Ridge coefficients as a function of the regularization\")\n(\"tight\")\n()\n\n\n<img alt=\"Ridge coefficients as a function of the regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_ridge_path_001.png\" srcset=\"../../_images/sphx_glr_plot_ridge_path_001.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.167 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Plot multi-class SGD on the iris dataset": [
        [
            "Plot decision surface of multi-class SGD on iris dataset.\nThe hyperplanes corresponding to the three one-versus-all (OVA) classifiers\nare represented by the dashed lines.\n<img alt=\"Decision surface of multi-class SGD\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_sgd_iris_001.png\" srcset=\"../../_images/sphx_glr_plot_sgd_iris_001.png\"/>",
            "markdown"
        ],
        [
            "/home/circleci/project/examples/linear_model/plot_sgd_iris.py:55: UserWarning:\n\nNo data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n\n\n\n<br/>",
            "code"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.linear_model import \nfrom sklearn.inspection import DecisionBoundaryDisplay\n\n# import some data to play with\niris = ()\n\n# we only take the first two features. We could\n# avoid this ugly slicing by using a two-dim dataset\nX = iris.data[:, :2]\ny = iris.target\ncolors = \"bry\"\n\n# shuffle\nidx = (X.shape[0])\n(13)\n(idx)\nX = X[idx]\ny = y[idx]\n\n# standardize\nmean = X.mean(axis=0)\nstd = X.std(axis=0)\nX = (X - mean) / std\n\nclf = (alpha=0.001, max_iter=100).fit(X, y)\nax = ()\n(\n    clf,\n    X,\n    cmap=plt.cm.Paired,\n    ax=ax,\n    response_method=\"predict\",\n    xlabel=iris.feature_names[0],\n    ylabel=iris.feature_names[1],\n)\n(\"tight\")\n\n# Plot also the training points\nfor i, color in zip(clf.classes_, colors):\n    idx = (y == i)\n    (\n        X[idx, 0],\n        X[idx, 1],\n        c=color,\n        label=iris.target_names[i],\n        cmap=plt.cm.Paired,\n        edgecolor=\"black\",\n        s=20,\n    )\n(\"Decision surface of multi-class SGD\")\n(\"tight\")\n\n# Plot the three one-against-all classifiers\nxmin, xmax = ()\nymin, ymax = ()\ncoef = clf.coef_\nintercept = clf.intercept_\n\n\ndef plot_hyperplane(c, color):\n    def line(x0):\n        return (-(x0 * coef[c, 0]) - intercept[c]) / coef[c, 1]\n\n    ([xmin, xmax], [line(xmin), line(xmax)], ls=\"--\", color=color)\n\n\nfor i, color in zip(clf.classes_, colors):\n    plot_hyperplane(i, color)\n()\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.117 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Plot multinomial and One-vs-Rest Logistic Regression": [
        [
            "Plot decision surface of multinomial and One-vs-Rest Logistic Regression.\nThe hyperplanes corresponding to the three One-vs-Rest (OVR) classifiers\nare represented by the dashed lines.\n\n<img alt=\"Decision surface of LogisticRegression (multinomial)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_logistic_multinomial_001.png\" srcset=\"../../_images/sphx_glr_plot_logistic_multinomial_001.png\"/>\n<img alt=\"Decision surface of LogisticRegression (ovr)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_logistic_multinomial_002.png\" srcset=\"../../_images/sphx_glr_plot_logistic_multinomial_002.png\"/>",
            "markdown"
        ],
        [
            "training score : 0.995 (multinomial)\n/home/circleci/project/examples/linear_model/plot_logistic_multinomial.py:46: UserWarning:\n\nNo data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n\ntraining score : 0.976 (ovr)\n/home/circleci/project/examples/linear_model/plot_logistic_multinomial.py:46: UserWarning:\n\nNo data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n\n\n\n<br/>",
            "code"
        ],
        [
            "# Authors: Tom Dupre la Tour &lt;tom.dupre-la-tour@m4x.org\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import \nfrom sklearn.linear_model import \nfrom sklearn.inspection import DecisionBoundaryDisplay\n\n# make 3-class dataset for classification\ncenters = [[-5, 0], [0, 1.5], [5, -1]]\nX, y = (n_samples=1000, centers=centers, random_state=40)\ntransformation = [[0.4, 0.2], [-0.4, 1.2]]\nX = (X, transformation)\n\nfor multi_class in (\"multinomial\", \"ovr\"):\n    clf = (\n        solver=\"sag\", max_iter=100, random_state=42, multi_class=multi_class\n    ).fit(X, y)\n\n    # print the training scores\n    print(\"training score : %.3f (%s)\" % (clf.score(X, y), multi_class))\n\n    _, ax = ()\n    (\n        clf, X, response_method=\"predict\", cmap=plt.cm.Paired, ax=ax\n    )\n    (\"Decision surface of LogisticRegression (%s)\" % multi_class)\n    (\"tight\")\n\n    # Plot also the training points\n    colors = \"bry\"\n    for i, color in zip(clf.classes_, colors):\n        idx = (y == i)\n        (\n            X[idx, 0], X[idx, 1], c=color, cmap=plt.cm.Paired, edgecolor=\"black\", s=20\n        )\n\n    # Plot the three one-against-all classifiers\n    xmin, xmax = ()\n    ymin, ymax = ()\n    coef = clf.coef_\n    intercept = clf.intercept_\n\n    def plot_hyperplane(c, color):\n        def line(x0):\n            return (-(x0 * coef[c, 0]) - intercept[c]) / coef[c, 1]\n\n        ([xmin, xmax], [line(xmin), line(xmax)], ls=\"--\", color=color)\n\n    for i, color in zip(clf.classes_, colors):\n        plot_hyperplane(i, color)\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.196 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Poisson regression and non-normal loss": [
        [
            "This example illustrates the use of log-linear Poisson regression on the\n from  and compares it with a linear\nmodel fitted with the usual least squared error and a non-linear GBRT model\nfitted with the Poisson loss (and a log-link).",
            "markdown"
        ],
        [
            "A few definitions:",
            "markdown"
        ],
        [
            "A <strong>policy</strong> is a contract between an insurance company and an individual:\nthe <strong>policyholder</strong>, that is, the vehicle driver in this case.",
            "markdown"
        ],
        [
            "A <strong>claim</strong> is the request made by a policyholder to the insurer to\ncompensate for a loss covered by the insurance.",
            "markdown"
        ],
        [
            "The <strong>exposure</strong> is the duration of the insurance coverage of a given policy,\nin years.",
            "markdown"
        ],
        [
            "The claim <strong>frequency</strong> is the number of claims divided by the exposure,\ntypically measured in number of claims per year.",
            "markdown"
        ],
        [
            "In this dataset, each sample corresponds to an insurance policy. Available\nfeatures include driver age, vehicle age, vehicle power, etc.",
            "markdown"
        ],
        [
            "Our goal is to predict the expected frequency of claims following car accidents\nfor a new policyholder given the historical data over a population of\npolicyholders.\n\n\n[]",
            "markdown"
        ],
        [
            "A. Noll, R. Salzmann and M.V. Wuthrich, Case Study: French Motor\nThird-Party Liability Claims (November 8, 2018).",
            "markdown"
        ],
        [
            "# Authors: Christian Lorentzen &lt;lorentzen.ch@gmail.com\n#          Roman Yurchak &lt;rth.yurchak@gmail.com\n#          Olivier Grisel &lt;olivier.grisel@ensta.org\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Poisson regression and non-normal loss->The French Motor Third-Party Liability Claims dataset": [
        [
            "Let\u2019s load the motor claim dataset from OpenML:",
            "markdown"
        ],
        [
            "from sklearn.datasets import \n\n\ndf = (data_id=41214, as_frame=True, parser=\"pandas\").frame\ndf",
            "code"
        ],
        [
            "678013 rows \u00d7 12 columns\n\n\n<br/>\n<br/>",
            "markdown"
        ],
        [
            "The number of claims (ClaimNb) is a positive integer that can be modeled\nas a Poisson distribution. It is then assumed to be the number of discrete\nevents occurring with a constant rate in a given time interval (Exposure,\nin units of years).",
            "markdown"
        ],
        [
            "Here we want to model the frequency y = ClaimNb / Exposure conditionally\non X via a (scaled) Poisson distribution, and use Exposure as\nsample_weight.",
            "markdown"
        ],
        [
            "df[\"Frequency\"] = df[\"ClaimNb\"] / df[\"Exposure\"]\n\nprint(\n    \"Average Frequency = {}\".format((df[\"Frequency\"], weights=df[\"Exposure\"]))\n)\n\nprint(\n    \"Fraction of exposure with zero claims = {0:.1%}\".format(\n        df.loc[df[\"ClaimNb\"] == 0, \"Exposure\"].sum() / df[\"Exposure\"].sum()\n    )\n)\n\nfig, (ax0, ax1, ax2) = (ncols=3, figsize=(16, 4))\nax0.set_title(\"Number of claims\")\n_ = df[\"ClaimNb\"].hist(bins=30, log=True, ax=ax0)\nax1.set_title(\"Exposure in years\")\n_ = df[\"Exposure\"].hist(bins=30, log=True, ax=ax1)\nax2.set_title(\"Frequency (number of claims per year)\")\n_ = df[\"Frequency\"].hist(bins=30, log=True, ax=ax2)\n\n\n<img alt=\"Number of claims, Exposure in years, Frequency (number of claims per year)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_poisson_regression_non_normal_loss_001.png\" srcset=\"../../_images/sphx_glr_plot_poisson_regression_non_normal_loss_001.png\"/>",
            "code"
        ],
        [
            "Average Frequency = 0.10070308464041304\nFraction of exposure with zero claims = 93.9%",
            "code"
        ],
        [
            "The remaining columns can be used to predict the frequency of claim events.\nThose columns are very heterogeneous with a mix of categorical and numeric\nvariables with different scales, possibly very unevenly distributed.",
            "markdown"
        ],
        [
            "In order to fit linear models with those predictors it is therefore\nnecessary to perform standard feature transformations as follows:",
            "markdown"
        ],
        [
            "from sklearn.pipeline import \nfrom sklearn.preprocessing import , \nfrom sklearn.preprocessing import , \nfrom sklearn.compose import \n\n\nlog_scale_transformer = (\n    (, validate=False), ()\n)\n\nlinear_model_preprocessor = (\n    [\n        (\"passthrough_numeric\", \"passthrough\", [\"BonusMalus\"]),\n        (\n            \"binned_numeric\",\n            (n_bins=10, subsample=int(2e5), random_state=0),\n            [\"VehAge\", \"DrivAge\"],\n        ),\n        (\"log_scaled_numeric\", log_scale_transformer, [\"Density\"]),\n        (\n            \"onehot_categorical\",\n            (),\n            [\"VehBrand\", \"VehPower\", \"VehGas\", \"Region\", \"Area\"],\n        ),\n    ],\n    remainder=\"drop\",\n)",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Poisson regression and non-normal loss->A constant prediction baseline": [
        [
            "It is worth noting that more than 93% of policyholders have zero claims. If\nwe were to convert this problem into a binary classification task, it would\nbe significantly imbalanced, and even a simplistic model that would only\npredict mean can achieve an accuracy of 93%.",
            "markdown"
        ],
        [
            "To evaluate the pertinence of the used metrics, we will consider as a\nbaseline a \u201cdummy\u201d estimator that constantly predicts the mean frequency of\nthe training sample.",
            "markdown"
        ],
        [
            "from sklearn.dummy import \nfrom sklearn.pipeline import \nfrom sklearn.model_selection import \n\ndf_train, df_test = (df, test_size=0.33, random_state=0)\n\ndummy = (\n    [\n        (\"preprocessor\", linear_model_preprocessor),\n        (\"regressor\", (strategy=\"mean\")),\n    ]\n).fit(df_train, df_train[\"Frequency\"], regressor__sample_weight=df_train[\"Exposure\"])",
            "code"
        ],
        [
            "Let\u2019s compute the performance of this constant prediction baseline with 3\ndifferent regression metrics:",
            "markdown"
        ],
        [
            "from sklearn.metrics import \nfrom sklearn.metrics import \nfrom sklearn.metrics import \n\n\ndef score_estimator(estimator, df_test):\n    \"\"\"Score an estimator on the test set.\"\"\"\n    y_pred = estimator.predict(df_test)\n\n    print(\n        \"MSE: %.3f\"\n        % (\n            df_test[\"Frequency\"], y_pred, sample_weight=df_test[\"Exposure\"]\n        )\n    )\n    print(\n        \"MAE: %.3f\"\n        % (\n            df_test[\"Frequency\"], y_pred, sample_weight=df_test[\"Exposure\"]\n        )\n    )\n\n    # Ignore non-positive predictions, as they are invalid for\n    # the Poisson deviance.\n    mask = y_pred  0\n    if (~mask).any():\n        n_masked, n_samples = (~mask).sum(), mask.shape[0]\n        print(\n            \"WARNING: Estimator yields invalid, non-positive predictions \"\n            f\" for {n_masked} samples out of {n_samples}. These predictions \"\n            \"are ignored when computing the Poisson deviance.\"\n        )\n\n    print(\n        \"mean Poisson deviance: %.3f\"\n        % (\n            df_test[\"Frequency\"][mask],\n            y_pred[mask],\n            sample_weight=df_test[\"Exposure\"][mask],\n        )\n    )\n\n\nprint(\"Constant mean frequency evaluation:\")\nscore_estimator(dummy, df_test)",
            "code"
        ],
        [
            "Constant mean frequency evaluation:\nMSE: 0.564\nMAE: 0.189\nmean Poisson deviance: 0.625",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Poisson regression and non-normal loss->(Generalized) linear models": [
        [
            "We start by modeling the target variable with the (l2 penalized) least\nsquares linear regression model, more comonly known as Ridge regression. We\nuse a low penalization alpha, as we expect such a linear model to under-fit\non such a large dataset.",
            "markdown"
        ],
        [
            "from sklearn.linear_model import \n\n\nridge_glm = (\n    [\n        (\"preprocessor\", linear_model_preprocessor),\n        (\"regressor\", (alpha=1e-6)),\n    ]\n).fit(df_train, df_train[\"Frequency\"], regressor__sample_weight=df_train[\"Exposure\"])",
            "code"
        ],
        [
            "The Poisson deviance cannot be computed on non-positive values predicted by\nthe model. For models that do return a few non-positive predictions (e.g.\n) we ignore the corresponding samples,\nmeaning that the obtained Poisson deviance is approximate. An alternative\napproach could be to use \nmeta-estimator to map y_pred to a strictly positive domain.",
            "markdown"
        ],
        [
            "print(\"Ridge evaluation:\")\nscore_estimator(ridge_glm, df_test)",
            "code"
        ],
        [
            "Ridge evaluation:\nMSE: 0.560\nMAE: 0.186\nWARNING: Estimator yields invalid, non-positive predictions  for 595 samples out of 223745. These predictions are ignored when computing the Poisson deviance.\nmean Poisson deviance: 0.597",
            "code"
        ],
        [
            "Next we fit the Poisson regressor on the target variable. We set the\nregularization strength alpha to approximately 1e-6 over number of\nsamples (i.e. 1e-12) in order to mimic the Ridge regressor whose L2 penalty\nterm scales differently with the number of samples.",
            "markdown"
        ],
        [
            "Since the Poisson regressor internally models the log of the expected target\nvalue instead of the expected value directly (log vs identity link function),\nthe relationship between X and y is not exactly linear anymore. Therefore the\nPoisson regressor is called a Generalized Linear Model (GLM) rather than a\nvanilla linear model as is the case for Ridge regression.",
            "markdown"
        ],
        [
            "from sklearn.linear_model import \n\nn_samples = df_train.shape[0]\n\npoisson_glm = (\n    [\n        (\"preprocessor\", linear_model_preprocessor),\n        (\"regressor\", (alpha=1e-12, solver=\"newton-cholesky\")),\n    ]\n)\npoisson_glm.fit(\n    df_train, df_train[\"Frequency\"], regressor__sample_weight=df_train[\"Exposure\"]\n)\n\nprint(\"PoissonRegressor evaluation:\")\nscore_estimator(poisson_glm, df_test)",
            "code"
        ],
        [
            "PoissonRegressor evaluation:\nMSE: 0.560\nMAE: 0.186\nmean Poisson deviance: 0.594",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Poisson regression and non-normal loss->Gradient Boosting Regression Trees for Poisson regression": [
        [
            "Finally, we will consider a non-linear model, namely Gradient Boosting\nRegression Trees. Tree-based models do not require the categorical data to be\none-hot encoded: instead, we can encode each category label with an arbitrary\ninteger using . With this\nencoding, the trees will treat the categorical features as ordered features,\nwhich might not be always a desired behavior. However this effect is limited\nfor deep enough trees which are able to recover the categorical nature of the\nfeatures. The main advantage of the\n over the\n is that it will make training\nfaster.",
            "markdown"
        ],
        [
            "Gradient Boosting also gives the possibility to fit the trees with a Poisson\nloss (with an implicit log-link function) instead of the default\nleast-squares loss. Here we only fit trees with the Poisson loss to keep this\nexample concise.",
            "markdown"
        ],
        [
            "from sklearn.ensemble import \nfrom sklearn.preprocessing import \n\n\ntree_preprocessor = (\n    [\n        (\n            \"categorical\",\n            (),\n            [\"VehBrand\", \"VehPower\", \"VehGas\", \"Region\", \"Area\"],\n        ),\n        (\"numeric\", \"passthrough\", [\"VehAge\", \"DrivAge\", \"BonusMalus\", \"Density\"]),\n    ],\n    remainder=\"drop\",\n)\npoisson_gbrt = (\n    [\n        (\"preprocessor\", tree_preprocessor),\n        (\n            \"regressor\",\n            (loss=\"poisson\", max_leaf_nodes=128),\n        ),\n    ]\n)\npoisson_gbrt.fit(\n    df_train, df_train[\"Frequency\"], regressor__sample_weight=df_train[\"Exposure\"]\n)\n\nprint(\"Poisson Gradient Boosted Trees evaluation:\")\nscore_estimator(poisson_gbrt, df_test)",
            "code"
        ],
        [
            "Poisson Gradient Boosted Trees evaluation:\nMSE: 0.566\nMAE: 0.184\nmean Poisson deviance: 0.575",
            "code"
        ],
        [
            "Like the Poisson GLM above, the gradient boosted trees model minimizes\nthe Poisson deviance. However, because of a higher predictive power,\nit reaches lower values of Poisson deviance.",
            "markdown"
        ],
        [
            "Evaluating models with a single train / test split is prone to random\nfluctuations. If computing resources allow, it should be verified that\ncross-validated performance metrics would lead to similar conclusions.",
            "markdown"
        ],
        [
            "The qualitative difference between these models can also be visualized by\ncomparing the histogram of observed target values with that of predicted\nvalues:",
            "markdown"
        ],
        [
            "fig, axes = (nrows=2, ncols=4, figsize=(16, 6), sharey=True)\nfig.subplots_adjust(bottom=0.2)\nn_bins = 20\nfor row_idx, label, df in zip(range(2), [\"train\", \"test\"], [df_train, df_test]):\n    df[\"Frequency\"].hist(bins=(-1, 30, n_bins), ax=axes[row_idx, 0])\n\n    axes[row_idx, 0].set_title(\"Data\")\n    axes[row_idx, 0].set_yscale(\"log\")\n    axes[row_idx, 0].set_xlabel(\"y (observed Frequency)\")\n    axes[row_idx, 0].set_ylim([1e1, 5e5])\n    axes[row_idx, 0].set_ylabel(label + \" samples\")\n\n    for idx, model in enumerate([ridge_glm, poisson_glm, poisson_gbrt]):\n        y_pred = model.predict(df)\n\n        (y_pred).hist(\n            bins=(-1, 4, n_bins), ax=axes[row_idx, idx + 1]\n        )\n        axes[row_idx, idx + 1].set(\n            title=model[-1].__class__.__name__,\n            yscale=\"log\",\n            xlabel=\"y_pred (predicted expected Frequency)\",\n        )\n()\n\n\n<img alt=\"Data, Ridge, PoissonRegressor, HistGradientBoostingRegressor, Data, Ridge, PoissonRegressor, HistGradientBoostingRegressor\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_poisson_regression_non_normal_loss_002.png\" srcset=\"../../_images/sphx_glr_plot_poisson_regression_non_normal_loss_002.png\"/>",
            "code"
        ],
        [
            "The experimental data presents a long tail distribution for y. In all\nmodels, we predict the expected frequency of a random variable, so we will\nhave necessarily fewer extreme values than for the observed realizations of\nthat random variable. This explains that the mode of the histograms of model\npredictions doesn\u2019t necessarily correspond to the smallest value.\nAdditionally, the normal distribution used in Ridge has a constant\nvariance, while for the Poisson distribution used in PoissonRegressor and\nHistGradientBoostingRegressor, the variance is proportional to the\npredicted expected value.",
            "markdown"
        ],
        [
            "Thus, among the considered estimators, PoissonRegressor and\nHistGradientBoostingRegressor are a-priori better suited for modeling the\nlong tail distribution of the non-negative data as compared to the Ridge\nmodel which makes a wrong assumption on the distribution of the target\nvariable.",
            "markdown"
        ],
        [
            "The HistGradientBoostingRegressor estimator has the most flexibility and\nis able to predict higher expected values.",
            "markdown"
        ],
        [
            "Note that we could have used the least squares loss for the\nHistGradientBoostingRegressor model. This would wrongly assume a normal\ndistributed response variable as does the Ridge model, and possibly\nalso lead to slightly negative predictions. However the gradient boosted\ntrees would still perform relatively well and in particular better than\nPoissonRegressor thanks to the flexibility of the trees combined with the\nlarge number of training samples.",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Poisson regression and non-normal loss->Evaluation of the calibration of predictions": [
        [
            "To ensure that estimators yield reasonable predictions for different\npolicyholder types, we can bin test samples according to y_pred returned\nby each model. Then for each bin, we compare the mean predicted y_pred,\nwith the mean observed target:",
            "markdown"
        ],
        [
            "from sklearn.utils import \n\n\ndef _mean_frequency_by_risk_group(y_true, y_pred, sample_weight=None, n_bins=100):\n    \"\"\"Compare predictions and observations for bins ordered by y_pred.\n\n    We order the samples by ``y_pred`` and split it in bins.\n    In each bin the observed mean is compared with the predicted mean.\n\n    Parameters\n    ----------\n    y_true: array-like of shape (n_samples,)\n        Ground truth (correct) target values.\n    y_pred: array-like of shape (n_samples,)\n        Estimated target values.\n    sample_weight : array-like of shape (n_samples,)\n        Sample weights.\n    n_bins: int\n        Number of bins to use.\n\n    Returns\n    -------\n    bin_centers: ndarray of shape (n_bins,)\n        bin centers\n    y_true_bin: ndarray of shape (n_bins,)\n        average y_pred for each bin\n    y_pred_bin: ndarray of shape (n_bins,)\n        average y_pred for each bin\n    \"\"\"\n    idx_sort = (y_pred)\n    bin_centers = (0, 1, 1 / n_bins) + 0.5 / n_bins\n    y_pred_bin = (n_bins)\n    y_true_bin = (n_bins)\n\n    for n, sl in enumerate((len(y_true), n_bins)):\n        weights = sample_weight[idx_sort][sl]\n        y_pred_bin[n] = (y_pred[idx_sort][sl], weights=weights)\n        y_true_bin[n] = (y_true[idx_sort][sl], weights=weights)\n    return bin_centers, y_true_bin, y_pred_bin\n\n\nprint(f\"Actual number of claims: {df_test['ClaimNb'].sum()}\")\nfig, ax = (nrows=2, ncols=2, figsize=(12, 8))\n(wspace=0.3)\n\nfor axi, model in zip(ax.ravel(), [ridge_glm, poisson_glm, poisson_gbrt, dummy]):\n    y_pred = model.predict(df_test)\n    y_true = df_test[\"Frequency\"].values\n    exposure = df_test[\"Exposure\"].values\n    q, y_true_seg, y_pred_seg = _mean_frequency_by_risk_group(\n        y_true, y_pred, sample_weight=exposure, n_bins=10\n    )\n\n    # Name of the model after the estimator used in the last step of the\n    # pipeline.\n    print(f\"Predicted number of claims by {model[-1]}: {(y_pred * exposure):.1f}\")\n\n    axi.plot(q, y_pred_seg, marker=\"x\", linestyle=\"--\", label=\"predictions\")\n    axi.plot(q, y_true_seg, marker=\"o\", linestyle=\"--\", label=\"observations\")\n    axi.set_xlim(0, 1.0)\n    axi.set_ylim(0, 0.5)\n    axi.set(\n        title=model[-1],\n        xlabel=\"Fraction of samples sorted by y_pred\",\n        ylabel=\"Mean Frequency (y_pred)\",\n    )\n    axi.legend()\n()\n\n\n<img alt=\"Ridge(alpha=1e-06), PoissonRegressor(alpha=1e-12, solver='newton-cholesky'), HistGradientBoostingRegressor(loss='poisson', max_leaf_nodes=128), DummyRegressor()\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_poisson_regression_non_normal_loss_003.png\" srcset=\"../../_images/sphx_glr_plot_poisson_regression_non_normal_loss_003.png\"/>",
            "code"
        ],
        [
            "Actual number of claims: 11935\nPredicted number of claims by Ridge(alpha=1e-06): 11933.4\nPredicted number of claims by PoissonRegressor(alpha=1e-12, solver='newton-cholesky'): 11932.0\nPredicted number of claims by HistGradientBoostingRegressor(loss='poisson', max_leaf_nodes=128): 12196.1\nPredicted number of claims by DummyRegressor(): 11931.2",
            "code"
        ],
        [
            "The dummy regression model predicts a constant frequency. This model does not\nattribute the same tied rank to all samples but is none-the-less globally\nwell calibrated (to estimate the mean frequency of the entire population).",
            "markdown"
        ],
        [
            "The Ridge regression model can predict very low expected frequencies that\ndo not match the data. It can therefore severely under-estimate the risk for\nsome policyholders.",
            "markdown"
        ],
        [
            "PoissonRegressor and HistGradientBoostingRegressor show better\nconsistency between predicted and observed targets, especially for low\npredicted target values.",
            "markdown"
        ],
        [
            "The sum of all predictions also confirms the calibration issue of the\nRidge model: it under-estimates by more than 3% the total number of\nclaims in the test set while the other three models can approximately recover\nthe total number of claims of the test portfolio.",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Poisson regression and non-normal loss->Evaluation of the ranking power": [
        [
            "For some business applications, we are interested in the ability of the model\nto rank the riskiest from the safest policyholders, irrespective of the\nabsolute value of the prediction. In this case, the model evaluation would\ncast the problem as a ranking problem rather than a regression problem.",
            "markdown"
        ],
        [
            "To compare the 3 models from this perspective, one can plot the cumulative\nproportion of claims vs the cumulative proportion of exposure for the test\nsamples order by the model predictions, from safest to riskiest according to\neach model.",
            "markdown"
        ],
        [
            "This plot is called a Lorenz curve and can be summarized by the Gini index:",
            "markdown"
        ],
        [
            "from sklearn.metrics import \n\n\ndef lorenz_curve(y_true, y_pred, exposure):\n    y_true, y_pred = (y_true), (y_pred)\n    exposure = (exposure)\n\n    # order samples by increasing predicted risk:\n    ranking = (y_pred)\n    ranked_frequencies = y_true[ranking]\n    ranked_exposure = exposure[ranking]\n    cumulated_claims = (ranked_frequencies * ranked_exposure)\n    cumulated_claims /= cumulated_claims[-1]\n    cumulated_exposure = (ranked_exposure)\n    cumulated_exposure /= cumulated_exposure[-1]\n    return cumulated_exposure, cumulated_claims\n\n\nfig, ax = (figsize=(8, 8))\n\nfor model in [dummy, ridge_glm, poisson_glm, poisson_gbrt]:\n    y_pred = model.predict(df_test)\n    cum_exposure, cum_claims = lorenz_curve(\n        df_test[\"Frequency\"], y_pred, df_test[\"Exposure\"]\n    )\n    gini = 1 - 2 * (cum_exposure, cum_claims)\n    label = \"{} (Gini: {:.2f})\".format(model[-1], gini)\n    ax.plot(cum_exposure, cum_claims, linestyle=\"-\", label=label)\n\n# Oracle model: y_pred == y_test\ncum_exposure, cum_claims = lorenz_curve(\n    df_test[\"Frequency\"], df_test[\"Frequency\"], df_test[\"Exposure\"]\n)\ngini = 1 - 2 * (cum_exposure, cum_claims)\nlabel = \"Oracle (Gini: {:.2f})\".format(gini)\nax.plot(cum_exposure, cum_claims, linestyle=\"-.\", color=\"gray\", label=label)\n\n# Random Baseline\nax.plot([0, 1], [0, 1], linestyle=\"--\", color=\"black\", label=\"Random baseline\")\nax.set(\n    title=\"Lorenz curves by model\",\n    xlabel=\"Cumulative proportion of exposure (from safest to riskiest)\",\n    ylabel=\"Cumulative proportion of claims\",\n)\nax.legend(loc=\"upper left\")\n\n\n<img alt=\"Lorenz curves by model\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_poisson_regression_non_normal_loss_004.png\" srcset=\"../../_images/sphx_glr_plot_poisson_regression_non_normal_loss_004.png\"/>",
            "code"
        ],
        [
            "&lt;matplotlib.legend.Legend object at 0x7f0d6d9ced00",
            "code"
        ],
        [
            "As expected, the dummy regressor is unable to correctly rank the samples and\ntherefore performs the worst on this plot.",
            "markdown"
        ],
        [
            "The tree-based model is significantly better at ranking policyholders by risk\nwhile the two linear models perform similarly.",
            "markdown"
        ],
        [
            "All three models are significantly better than chance but also very far from\nmaking perfect predictions.",
            "markdown"
        ],
        [
            "This last point is expected due to the nature of the problem: the occurrence\nof accidents is mostly dominated by circumstantial causes that are not\ncaptured in the columns of the dataset and can indeed be considered as purely\nrandom.",
            "markdown"
        ],
        [
            "The linear models assume no interactions between the input variables which\nlikely causes under-fitting. Inserting a polynomial feature extractor\n() indeed increases their\ndiscrimative power by 2 points of Gini index. In particular it improves the\nability of the models to identify the top 5% riskiest profiles.",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Poisson regression and non-normal loss->Main takeaways": [
        [
            "The performance of the models can be evaluated by their ability to yield\nwell-calibrated predictions and a good ranking.",
            "markdown"
        ],
        [
            "The calibration of the model can be assessed by plotting the mean observed\nvalue vs the mean predicted value on groups of test samples binned by\npredicted risk.",
            "markdown"
        ],
        [
            "The least squares loss (along with the implicit use of the identity link\nfunction) of the Ridge regression model seems to cause this model to be\nbadly calibrated. In particular, it tends to underestimate the risk and can\neven predict invalid negative frequencies.",
            "markdown"
        ],
        [
            "Using the Poisson loss with a log-link can correct these problems and lead\nto a well-calibrated linear model.",
            "markdown"
        ],
        [
            "The Gini index reflects the ability of a model to rank predictions\nirrespective of their absolute values, and therefore only assess their\nranking power.",
            "markdown"
        ],
        [
            "Despite the improvement in calibration, the ranking power of both linear\nmodels are comparable and well below the ranking power of the Gradient\nBoosting Regression Trees.",
            "markdown"
        ],
        [
            "The Poisson deviance computed as an evaluation metric reflects both the\ncalibration and the ranking power of the model. It also makes a linear\nassumption on the ideal relationship between the expected value and the\nvariance of the response variable. For the sake of conciseness we did not\ncheck whether this assumption holds.",
            "markdown"
        ],
        [
            "Traditional regression metrics such as Mean Squared Error and Mean Absolute\nError are hard to meaningfully interpret on count values with many zeros.",
            "markdown"
        ],
        [
            "()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  27.697 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Polynomial and Spline interpolation": [
        [
            "This example demonstrates how to approximate a function with polynomials up to\ndegree degree by using ridge regression. We show two different ways given\nn_samples of 1d points x_i:",
            "markdown"
        ],
        [
            "generates all monomials\nup to degree. This gives us the so called Vandermonde matrix with\nn_samples rows and degree + 1 columns:",
            "markdown"
        ],
        [
            "[[1, x_0, x_0 ** 2, x_0 ** 3, ..., x_0 ** degree],\n [1, x_1, x_1 ** 2, x_1 ** 3, ..., x_1 ** degree],\n ...]",
            "code"
        ],
        [
            "Intuitively, this matrix can be interpreted as a matrix of pseudo features\n(the points raised to some power). The matrix is akin to (but different from)\nthe matrix induced by a polynomial kernel.",
            "markdown"
        ],
        [
            "generates B-spline basis\nfunctions. A basis function of a B-spline is a piece-wise polynomial function\nof degree degree that is non-zero only between degree+1 consecutive\nknots. Given n_knots number of knots, this results in matrix of\nn_samples rows and n_knots + degree - 1 columns:",
            "markdown"
        ],
        [
            "[[basis_1(x_0), basis_2(x_0), ...],\n [basis_1(x_1), basis_2(x_1), ...],\n ...]",
            "code"
        ],
        [
            "This example shows that these two transformers are well suited to model\nnon-linear effects with a linear model, using a pipeline to add non-linear\nfeatures. Kernel methods extend this idea and can induce very high (even\ninfinite) dimensional feature spaces.",
            "markdown"
        ],
        [
            "# Author: Mathieu Blondel\n#         Jake Vanderplas\n#         Christian Lorentzen\n#         Malte Londschien\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import \nfrom sklearn.preprocessing import , \nfrom sklearn.pipeline import",
            "code"
        ],
        [
            "We start by defining a function that we intend to approximate and prepare\nplotting it.",
            "markdown"
        ],
        [
            "def f(x):\n    \"\"\"Function to be approximated by polynomial interpolation.\"\"\"\n    return x * (x)\n\n\n# whole range we want to plot\nx_plot = (-1, 11, 100)",
            "code"
        ],
        [
            "To make it interesting, we only give a small subset of points to train on.",
            "markdown"
        ],
        [
            "x_train = (0, 10, 100)\nrng = (0)\nx_train = (rng.choice(x_train, size=20, replace=False))\ny_train = f(x_train)\n\n# create 2D-array versions of these arrays to feed to transformers\nX_train = x_train[:, ]\nX_plot = x_plot[:, ]",
            "code"
        ],
        [
            "Now we are ready to create polynomial features and splines, fit on the\ntraining points and show how well they interpolate.",
            "markdown"
        ],
        [
            "# plot function\nlw = 2\nfig, ax = ()\nax.set_prop_cycle(\n    color=[\"black\", \"teal\", \"yellowgreen\", \"gold\", \"darkorange\", \"tomato\"]\n)\nax.plot(x_plot, f(x_plot), linewidth=lw, label=\"ground truth\")\n\n# plot training points\nax.scatter(x_train, y_train, label=\"training points\")\n\n# polynomial features\nfor degree in [3, 4, 5]:\n    model = ((degree), (alpha=1e-3))\n    model.fit(X_train, y_train)\n    y_plot = model.predict(X_plot)\n    ax.plot(x_plot, y_plot, label=f\"degree {degree}\")\n\n# B-spline with 4 + 3 - 1 = 6 basis functions\nmodel = ((n_knots=4, degree=3), (alpha=1e-3))\nmodel.fit(X_train, y_train)\n\ny_plot = model.predict(X_plot)\nax.plot(x_plot, y_plot, label=\"B-spline\")\nax.legend(loc=\"lower center\")\nax.set_ylim(-20, 10)\n()\n\n\n<img alt=\"plot polynomial interpolation\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_polynomial_interpolation_001.png\" srcset=\"../../_images/sphx_glr_plot_polynomial_interpolation_001.png\"/>",
            "code"
        ],
        [
            "This shows nicely that higher degree polynomials can fit the data better. But\nat the same time, too high powers can show unwanted oscillatory behaviour\nand are particularly dangerous for extrapolation beyond the range of fitted\ndata. This is an advantage of B-splines. They usually fit the data as well as\npolynomials and show very nice and smooth behaviour. They have also good\noptions to control the extrapolation, which defaults to continue with a\nconstant. Note that most often, you would rather increase the number of knots\nbut keep degree=3.",
            "markdown"
        ],
        [
            "In order to give more insights into the generated feature bases, we plot all\ncolumns of both transformers separately.",
            "markdown"
        ],
        [
            "fig, axes = (ncols=2, figsize=(16, 5))\npft = (degree=3).fit(X_train)\naxes[0].plot(x_plot, pft.transform(X_plot))\naxes[0].legend(axes[0].lines, [f\"degree {n}\" for n in range(4)])\naxes[0].set_title(\"PolynomialFeatures\")\n\nsplt = (n_knots=4, degree=3).fit(X_train)\naxes[1].plot(x_plot, splt.transform(X_plot))\naxes[1].legend(axes[1].lines, [f\"spline {n}\" for n in range(6)])\naxes[1].set_title(\"SplineTransformer\")\n\n# plot knots of spline\nknots = splt.bsplines_[0].t\naxes[1].vlines(knots[3:-3], ymin=0, ymax=0.8, linestyles=\"dashed\")\n()\n\n\n<img alt=\"PolynomialFeatures, SplineTransformer\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_polynomial_interpolation_002.png\" srcset=\"../../_images/sphx_glr_plot_polynomial_interpolation_002.png\"/>",
            "code"
        ],
        [
            "In the left plot, we recognize the lines corresponding to simple monomials\nfrom x**0 to x**3. In the right figure, we see the six B-spline\nbasis functions of degree=3 and also the four knot positions that were\nchosen during fit. Note that there are degree number of additional\nknots each to the left and to the right of the fitted interval. These are\nthere for technical reasons, so we refrain from showing them. Every basis\nfunction has local support and is continued as a constant beyond the fitted\nrange. This extrapolating behaviour could be changed by the argument\nextrapolation.",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Polynomial and Spline interpolation->Periodic Splines": [
        [
            "In the previous example we saw the limitations of polynomials and splines for\nextrapolation beyond the range of the training observations. In some\nsettings, e.g. with seasonal effects, we expect a periodic continuation of\nthe underlying signal. Such effects can be modelled using periodic splines,\nwhich have equal function value and equal derivatives at the first and last\nknot. In the following case we show how periodic splines provide a better fit\nboth within and outside of the range of training data given the additional\ninformation of periodicity. The splines period is the distance between\nthe first and last knot, which we specify manually.",
            "markdown"
        ],
        [
            "Periodic splines can also be useful for naturally periodic features (such as\nday of the year), as the smoothness at the boundary knots prevents a jump in\nthe transformed values (e.g. from Dec 31st to Jan 1st). For such naturally\nperiodic features or more generally features where the period is known, it is\nadvised to explicitly pass this information to the SplineTransformer by\nsetting the knots manually.",
            "markdown"
        ],
        [
            "def g(x):\n    \"\"\"Function to be approximated by periodic spline interpolation.\"\"\"\n    return (x) - 0.7 * (x * 3)\n\n\ny_train = g(x_train)\n\n# Extend the test data into the future:\nx_plot_ext = (-1, 21, 200)\nX_plot_ext = x_plot_ext[:, ]\n\nlw = 2\nfig, ax = ()\nax.set_prop_cycle(color=[\"black\", \"tomato\", \"teal\"])\nax.plot(x_plot_ext, g(x_plot_ext), linewidth=lw, label=\"ground truth\")\nax.scatter(x_train, y_train, label=\"training points\")\n\nfor transformer, label in [\n    ((degree=3, n_knots=10), \"spline\"),\n    (\n        (\n            degree=3,\n            knots=(0, 2 * , 10)[:, None],\n            extrapolation=\"periodic\",\n        ),\n        \"periodic spline\",\n    ),\n]:\n    model = (transformer, (alpha=1e-3))\n    model.fit(X_train, y_train)\n    y_plot_ext = model.predict(X_plot_ext)\n    ax.plot(x_plot_ext, y_plot_ext, label=label)\n\nax.legend()\nfig.show()\n\n\n<img alt=\"plot polynomial interpolation\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_polynomial_interpolation_003.png\" srcset=\"../../_images/sphx_glr_plot_polynomial_interpolation_003.png\"/>",
            "code"
        ],
        [
            "fig, ax = ()\nknots = (0, 2 * , 4)\nsplt = (knots=knots[:, None], degree=3, extrapolation=\"periodic\").fit(\n    X_train\n)\nax.plot(x_plot_ext, splt.transform(X_plot_ext))\nax.legend(ax.lines, [f\"spline {n}\" for n in range(3)])\n()\n\n\n<img alt=\"plot polynomial interpolation\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_polynomial_interpolation_004.png\" srcset=\"../../_images/sphx_glr_plot_polynomial_interpolation_004.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.510 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Quantile regression": [
        [
            "This example illustrates how quantile regression can predict non-trivial\nconditional quantiles.",
            "markdown"
        ],
        [
            "The left figure shows the case when the error distribution is normal,\nbut has non-constant variance, i.e. with heteroscedasticity.",
            "markdown"
        ],
        [
            "The right figure shows an example of an asymmetric error distribution,\nnamely the Pareto distribution.",
            "markdown"
        ],
        [
            "# Authors: David Dale &lt;dale.david@mail.ru\n#          Christian Lorentzen &lt;lorentzen.ch@gmail.com\n#          Guillaume Lemaitre &lt;glemaitre58@gmail.com\n# License: BSD 3 clause",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Quantile regression->Dataset generation": [
        [
            "To illustrate the behaviour of quantile regression, we will generate two\nsynthetic datasets. The true generative random processes for both datasets\nwill be composed by the same expected value with a linear relationship with a\nsingle feature x.",
            "markdown"
        ],
        [
            "import numpy as np\n\nrng = (42)\nx = (start=0, stop=10, num=100)\nX = x[:, ]\ny_true_mean = 10 + 0.5 * x",
            "code"
        ],
        [
            "We will create two subsequent problems by changing the distribution of the\ntarget y while keeping the same expected value:",
            "markdown"
        ],
        [
            "in the first case, a heteroscedastic Normal noise is added;",
            "markdown"
        ],
        [
            "in the second case, an asymmetric Pareto noise is added.",
            "markdown"
        ],
        [
            "y_normal = y_true_mean + rng.normal(loc=0, scale=0.5 + 0.5 * x, size=x.shape[0])\na = 5\ny_pareto = y_true_mean + 10 * (rng.pareto(a, size=x.shape[0]) - 1 / (a - 1))",
            "code"
        ],
        [
            "Let\u2019s first visualize the datasets as well as the distribution of the\nresiduals y - mean(y).",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n\n_, axs = (nrows=2, ncols=2, figsize=(15, 11), sharex=\"row\", sharey=\"row\")\n\naxs[0, 0].plot(x, y_true_mean, label=\"True mean\")\naxs[0, 0].scatter(x, y_normal, color=\"black\", alpha=0.5, label=\"Observations\")\naxs[1, 0].hist(y_true_mean - y_normal, edgecolor=\"black\")\n\n\naxs[0, 1].plot(x, y_true_mean, label=\"True mean\")\naxs[0, 1].scatter(x, y_pareto, color=\"black\", alpha=0.5, label=\"Observations\")\naxs[1, 1].hist(y_true_mean - y_pareto, edgecolor=\"black\")\n\naxs[0, 0].set_title(\"Dataset with heteroscedastic Normal distributed targets\")\naxs[0, 1].set_title(\"Dataset with asymmetric Pareto distributed target\")\naxs[1, 0].set_title(\n    \"Residuals distribution for heteroscedastic Normal distributed targets\"\n)\naxs[1, 1].set_title(\"Residuals distribution for asymmetric Pareto distributed target\")\naxs[0, 0].legend()\naxs[0, 1].legend()\naxs[0, 0].set_ylabel(\"y\")\naxs[1, 0].set_ylabel(\"Counts\")\naxs[0, 1].set_xlabel(\"x\")\naxs[0, 0].set_xlabel(\"x\")\naxs[1, 0].set_xlabel(\"Residuals\")\n_ = axs[1, 1].set_xlabel(\"Residuals\")\n\n\n<img alt=\"Dataset with heteroscedastic Normal distributed targets, Dataset with asymmetric Pareto distributed target, Residuals distribution for heteroscedastic Normal distributed targets, Residuals distribution for asymmetric Pareto distributed target\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_quantile_regression_001.png\" srcset=\"../../_images/sphx_glr_plot_quantile_regression_001.png\"/>",
            "code"
        ],
        [
            "With the heteroscedastic Normal distributed target, we observe that the\nvariance of the noise is increasing when the value of the feature x is\nincreasing.",
            "markdown"
        ],
        [
            "With the asymmetric Pareto distributed target, we observe that the positive\nresiduals are bounded.",
            "markdown"
        ],
        [
            "These types of noisy targets make the estimation via\n less efficient, i.e. we need\nmore data to get stable results and, in addition, large outliers can have a\nhuge impact on the fitted coefficients. (Stated otherwise: in a setting with\nconstant variance, ordinary least squares estimators converge much faster to\nthe true coefficients with increasing sample size.)",
            "markdown"
        ],
        [
            "In this asymmetric setting, the median or different quantiles give additional\ninsights. On top of that, median estimation is much more robust to outliers\nand heavy tailed distributions. But note that extreme quantiles are estimated\nby very view data points. 95% quantile are more or less estimated by the 5%\nlargest values and thus also a bit sensitive outliers.",
            "markdown"
        ],
        [
            "In the remainder of this tutorial, we will show how\n can be used in practice and\ngive the intuition into the properties of the fitted models. Finally,\nwe will compare the both \nand .",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Quantile regression->Fitting a QuantileRegressor": [
        [
            "In this section, we want to estimate the conditional median as well as\na low and high quantile fixed at 5% and 95%, respectively. Thus, we will get\nthree linear models, one for each quantile.",
            "markdown"
        ],
        [
            "We will use the quantiles at 5% and 95% to find the outliers in the training\nsample beyond the central 90% interval.",
            "markdown"
        ],
        [
            "from sklearn.utils.fixes import sp_version, parse_version\n\n# This is line is to avoid incompatibility if older SciPy version.\n# You should use `solver=\"highs\"` with recent version of SciPy.\nsolver = \"highs\" if sp_version = parse_version(\"1.6.0\") else \"interior-point\"",
            "code"
        ],
        [
            "from sklearn.linear_model import \n\nquantiles = [0.05, 0.5, 0.95]\npredictions = {}\nout_bounds_predictions = (y_true_mean, dtype=)\nfor quantile in quantiles:\n    qr = (quantile=quantile, alpha=0, solver=solver)\n    y_pred = qr.fit(X, y_normal).predict(X)\n    predictions[quantile] = y_pred\n\n    if quantile == min(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred = y_normal\n        )\n    elif quantile == max(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred &lt;= y_normal\n        )",
            "code"
        ],
        [
            "Now, we can plot the three linear models and the distinguished samples that\nare within the central 90% interval from samples that are outside this\ninterval.",
            "markdown"
        ],
        [
            "(X, y_true_mean, color=\"black\", linestyle=\"dashed\", label=\"True mean\")\n\nfor quantile, y_pred in predictions.items():\n    (X, y_pred, label=f\"Quantile: {quantile}\")\n\n(\n    x[out_bounds_predictions],\n    y_normal[out_bounds_predictions],\n    color=\"black\",\n    marker=\"+\",\n    alpha=0.5,\n    label=\"Outside interval\",\n)\n(\n    x[~out_bounds_predictions],\n    y_normal[~out_bounds_predictions],\n    color=\"black\",\n    alpha=0.5,\n    label=\"Inside interval\",\n)\n\n()\n(\"x\")\n(\"y\")\n_ = (\"Quantiles of heteroscedastic Normal distributed target\")\n\n\n<img alt=\"Quantiles of heteroscedastic Normal distributed target\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_quantile_regression_002.png\" srcset=\"../../_images/sphx_glr_plot_quantile_regression_002.png\"/>",
            "code"
        ],
        [
            "Since the noise is still Normally distributed, in particular is symmetric,\nthe true conditional mean and the true conditional median coincide. Indeed,\nwe see that the estimated median almost hits the true mean. We observe the\neffect of having an increasing noise variance on the 5% and 95% quantiles:\nthe slopes of those quantiles are very different and the interval between\nthem becomes wider with increasing x.",
            "markdown"
        ],
        [
            "To get an additional intuition regarding the meaning of the 5% and 95%\nquantiles estimators, one can count the number of samples above and below the\npredicted quantiles (represented by a cross on the above plot), considering\nthat we have a total of 100 samples.",
            "markdown"
        ],
        [
            "We can repeat the same experiment using the asymmetric Pareto distributed\ntarget.",
            "markdown"
        ],
        [
            "quantiles = [0.05, 0.5, 0.95]\npredictions = {}\nout_bounds_predictions = (y_true_mean, dtype=)\nfor quantile in quantiles:\n    qr = (quantile=quantile, alpha=0, solver=solver)\n    y_pred = qr.fit(X, y_pareto).predict(X)\n    predictions[quantile] = y_pred\n\n    if quantile == min(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred = y_pareto\n        )\n    elif quantile == max(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred &lt;= y_pareto\n        )",
            "code"
        ],
        [
            "(X, y_true_mean, color=\"black\", linestyle=\"dashed\", label=\"True mean\")\n\nfor quantile, y_pred in predictions.items():\n    (X, y_pred, label=f\"Quantile: {quantile}\")\n\n(\n    x[out_bounds_predictions],\n    y_pareto[out_bounds_predictions],\n    color=\"black\",\n    marker=\"+\",\n    alpha=0.5,\n    label=\"Outside interval\",\n)\n(\n    x[~out_bounds_predictions],\n    y_pareto[~out_bounds_predictions],\n    color=\"black\",\n    alpha=0.5,\n    label=\"Inside interval\",\n)\n\n()\n(\"x\")\n(\"y\")\n_ = (\"Quantiles of asymmetric Pareto distributed target\")\n\n\n<img alt=\"Quantiles of asymmetric Pareto distributed target\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_quantile_regression_003.png\" srcset=\"../../_images/sphx_glr_plot_quantile_regression_003.png\"/>",
            "code"
        ],
        [
            "Due to the asymmetry of the distribution of the noise, we observe that the\ntrue mean and estimated conditional median are different. We also observe\nthat each quantile model has different parameters to better fit the desired\nquantile. Note that ideally, all quantiles would be parallel in this case,\nwhich would become more visible with more data points or less extreme\nquantiles, e.g. 10% and 90%.",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Quantile regression->Comparing QuantileRegressor and LinearRegression": [
        [
            "In this section, we will linger on the difference regarding the error that\n and\n are minimizing.",
            "markdown"
        ],
        [
            "Indeed,  is a least squares\napproach minimizing the mean squared error (MSE) between the training and\npredicted targets. In contrast,\n with quantile=0.5\nminimizes the mean absolute error (MAE) instead.",
            "markdown"
        ],
        [
            "Let\u2019s first compute the training errors of such models in terms of mean\nsquared error and mean absolute error. We will use the asymmetric Pareto\ndistributed target to make it more interesting as mean and median are not\nequal.",
            "markdown"
        ],
        [
            "from sklearn.linear_model import \nfrom sklearn.metrics import \nfrom sklearn.metrics import \n\nlinear_regression = ()\nquantile_regression = (quantile=0.5, alpha=0, solver=solver)\n\ny_pred_lr = linear_regression.fit(X, y_pareto).predict(X)\ny_pred_qr = quantile_regression.fit(X, y_pareto).predict(X)\n\nprint(\n    f\"\"\"Training error (in-sample performance)\n    {linear_regression.__class__.__name__}:\n    MAE = {(y_pareto, y_pred_lr):.3f}\n    MSE = {(y_pareto, y_pred_lr):.3f}\n    {quantile_regression.__class__.__name__}:\n    MAE = {(y_pareto, y_pred_qr):.3f}\n    MSE = {(y_pareto, y_pred_qr):.3f}\n    \"\"\"\n)",
            "code"
        ],
        [
            "Training error (in-sample performance)\n    LinearRegression:\n    MAE = 1.805\n    MSE = 6.486\n    QuantileRegressor:\n    MAE = 1.670\n    MSE = 7.025",
            "code"
        ],
        [
            "On the training set, we see that MAE is lower for\n than\n. In contrast to that, MSE is\nlower for  than\n. These results confirms that\nMAE is the loss minimized by \nwhile MSE is the loss minimized\n.",
            "markdown"
        ],
        [
            "We can make a similar evaluation but looking at the test error obtained by\ncross-validation.",
            "markdown"
        ],
        [
            "from sklearn.model_selection import \n\ncv_results_lr = (\n    linear_regression,\n    X,\n    y_pareto,\n    cv=3,\n    scoring=[\"neg_mean_absolute_error\", \"neg_mean_squared_error\"],\n)\ncv_results_qr = (\n    quantile_regression,\n    X,\n    y_pareto,\n    cv=3,\n    scoring=[\"neg_mean_absolute_error\", \"neg_mean_squared_error\"],\n)\nprint(\n    f\"\"\"Test error (cross-validated performance)\n    {linear_regression.__class__.__name__}:\n    MAE = {-cv_results_lr[\"test_neg_mean_absolute_error\"].mean():.3f}\n    MSE = {-cv_results_lr[\"test_neg_mean_squared_error\"].mean():.3f}\n    {quantile_regression.__class__.__name__}:\n    MAE = {-cv_results_qr[\"test_neg_mean_absolute_error\"].mean():.3f}\n    MSE = {-cv_results_qr[\"test_neg_mean_squared_error\"].mean():.3f}\n    \"\"\"\n)",
            "code"
        ],
        [
            "Test error (cross-validated performance)\n    LinearRegression:\n    MAE = 1.732\n    MSE = 6.690\n    QuantileRegressor:\n    MAE = 1.679\n    MSE = 7.129",
            "code"
        ],
        [
            "We reach similar conclusions on the out-of-sample evaluation.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.596 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Regularization path of L1- Logistic Regression": [
        [
            "Train l1-penalized logistic regression models on a binary classification\nproblem derived from the Iris dataset.",
            "markdown"
        ],
        [
            "The models are ordered from strongest regularized to least regularized. The 4\ncoefficients of the models are collected and plotted as a \u201cregularization\npath\u201d: on the left-hand side of the figure (strong regularizers), all the\ncoefficients are exactly 0. When regularization gets progressively looser,\ncoefficients can get non-zero values one after the other.",
            "markdown"
        ],
        [
            "Here we choose the liblinear solver because it can efficiently optimize for the\nLogistic Regression loss with a non-smooth, sparsity inducing l1 penalty.",
            "markdown"
        ],
        [
            "Also note that we set a low value for the tolerance to make sure that the model\nhas converged before collecting the coefficients.",
            "markdown"
        ],
        [
            "We also use warm_start=True which means that the coefficients of the models are\nreused to initialize the next model fit to speed-up the computation of the\nfull-path.",
            "markdown"
        ],
        [
            "# Author: Alexandre Gramfort &lt;alexandre.gramfort@inria.fr\n# License: BSD 3 clause",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Regularization path of L1- Logistic Regression->Load data": [
        [
            "from sklearn import datasets\n\niris = ()\nX = iris.data\ny = iris.target\n\nX = X[y != 2]\ny = y[y != 2]\n\nX /= X.max()  # Normalize X to speed-up convergence",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Regularization path of L1- Logistic Regression->Compute regularization path": [
        [
            "import numpy as np\n\nfrom sklearn import linear_model\nfrom sklearn.svm import \n\ncs = (X, y, loss=\"log\") * (0, 7, 16)\n\nclf = (\n    penalty=\"l1\",\n    solver=\"liblinear\",\n    tol=1e-6,\n    max_iter=int(1e6),\n    warm_start=True,\n    intercept_scaling=10000.0,\n)\ncoefs_ = []\nfor c in cs:\n    clf.set_params(C=c)\n    clf.fit(X, y)\n    coefs_.append(clf.coef_.ravel().copy())\n\ncoefs_ = (coefs_)",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Regularization path of L1- Logistic Regression->Plot regularization path": [
        [
            "import matplotlib.pyplot as plt\n\n((cs), coefs_, marker=\"o\")\nymin, ymax = ()\n(\"log(C)\")\n(\"Coefficients\")\n(\"Logistic Regression Path\")\n(\"tight\")\n()\n\n\n<img alt=\"Logistic Regression Path\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_logistic_path_001.png\" srcset=\"../../_images/sphx_glr_plot_logistic_path_001.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.133 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Robust linear estimator fitting": [
        [
            "Here a sine function is fit with a polynomial of order 3, for values\nclose to zero.",
            "markdown"
        ],
        [
            "Robust fitting is demoed in different situations:",
            "markdown"
        ],
        [
            "No measurement errors, only modelling errors (fitting a sine with a\npolynomial)",
            "markdown"
        ],
        [
            "Measurement errors in X",
            "markdown"
        ],
        [
            "Measurement errors in y",
            "markdown"
        ],
        [
            "The median absolute deviation to non corrupt new data is used to judge\nthe quality of the prediction.",
            "markdown"
        ],
        [
            "What we can see that:",
            "markdown"
        ],
        [
            "RANSAC is good for strong outliers in the y direction",
            "markdown"
        ],
        [
            "TheilSen is good for small outliers, both in direction X and y, but has\na break point above which it performs worse than OLS.",
            "markdown"
        ],
        [
            "The scores of HuberRegressor may not be compared directly to both TheilSen\nand RANSAC because it does not attempt to completely filter the outliers\nbut lessen their effect.\n\n\n<img alt=\"Modeling Errors Only\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_robust_fit_001.png\" srcset=\"../../_images/sphx_glr_plot_robust_fit_001.png\"/>\n<img alt=\"Corrupt X, Small Deviants\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_robust_fit_002.png\" srcset=\"../../_images/sphx_glr_plot_robust_fit_002.png\"/>\n<img alt=\"Corrupt y, Small Deviants\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_robust_fit_003.png\" srcset=\"../../_images/sphx_glr_plot_robust_fit_003.png\"/>\n<img alt=\"Corrupt X, Large Deviants\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_robust_fit_004.png\" srcset=\"../../_images/sphx_glr_plot_robust_fit_004.png\"/>\n<img alt=\"Corrupt y, Large Deviants\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_robust_fit_005.png\" srcset=\"../../_images/sphx_glr_plot_robust_fit_005.png\"/>",
            "markdown"
        ],
        [
            "from matplotlib import pyplot as plt\nimport numpy as np\n\nfrom sklearn.linear_model import (\n    ,\n    ,\n    ,\n    ,\n)\nfrom sklearn.metrics import \nfrom sklearn.preprocessing import \nfrom sklearn.pipeline import \n\n(42)\n\nX = (size=400)\ny = (X)\n# Make sure that it X is 2D\nX = X[:, ]\n\nX_test = (size=200)\ny_test = (X_test)\nX_test = X_test[:, ]\n\ny_errors = y.copy()\ny_errors[::3] = 3\n\nX_errors = X.copy()\nX_errors[::3] = 3\n\ny_errors_large = y.copy()\ny_errors_large[::3] = 10\n\nX_errors_large = X.copy()\nX_errors_large[::3] = 10\n\nestimators = [\n    (\"OLS\", ()),\n    (\"Theil-Sen\", (random_state=42)),\n    (\"RANSAC\", (random_state=42)),\n    (\"HuberRegressor\", ()),\n]\ncolors = {\n    \"OLS\": \"turquoise\",\n    \"Theil-Sen\": \"gold\",\n    \"RANSAC\": \"lightgreen\",\n    \"HuberRegressor\": \"black\",\n}\nlinestyle = {\"OLS\": \"-\", \"Theil-Sen\": \"-.\", \"RANSAC\": \"--\", \"HuberRegressor\": \"--\"}\nlw = 3\n\nx_plot = (X.min(), X.max())\nfor title, this_X, this_y in [\n    (\"Modeling Errors Only\", X, y),\n    (\"Corrupt X, Small Deviants\", X_errors, y),\n    (\"Corrupt y, Small Deviants\", X, y_errors),\n    (\"Corrupt X, Large Deviants\", X_errors_large, y),\n    (\"Corrupt y, Large Deviants\", X, y_errors_large),\n]:\n    (figsize=(5, 4))\n    (this_X[:, 0], this_y, \"b+\")\n\n    for name, estimator in estimators:\n        model = ((3), estimator)\n        model.fit(this_X, this_y)\n        mse = (model.predict(X_test), y_test)\n        y_plot = model.predict(x_plot[:, ])\n        (\n            x_plot,\n            y_plot,\n            color=colors[name],\n            linestyle=linestyle[name],\n            linewidth=lw,\n            label=\"%s: error = %.3f\" % (name, mse),\n        )\n\n    legend_title = \"Error of Mean\\nAbsolute Deviation\\nto Non-corrupt Data\"\n    legend = (\n        loc=\"upper right\", frameon=False, title=legend_title, prop=dict(size=\"x-small\")\n    )\n    (-4, 10.2)\n    (-2, 10.2)\n    (title)\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  2.020 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Robust linear model estimation using RANSAC": [
        [
            "In this example, we see how to robustly fit a linear model to faulty data using\nthe  algorithm.",
            "markdown"
        ],
        [
            "The ordinary linear regressor is sensitive to outliers, and the fitted line can\neasily be skewed away from the true underlying relationship of data.",
            "markdown"
        ],
        [
            "The RANSAC regressor automatically splits the data into inliers and outliers,\nand the fitted line is determined only by the identified inliers.\n<img alt=\"plot ransac\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_ransac_001.png\" srcset=\"../../_images/sphx_glr_plot_ransac_001.png\"/>",
            "markdown"
        ],
        [
            "Estimated coefficients (true, linear regression, RANSAC):\n82.1903908407869 [54.17236387] [82.08533159]\n\n\n\n<br/>",
            "code"
        ],
        [
            "import numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom sklearn import linear_model, datasets\n\n\nn_samples = 1000\nn_outliers = 50\n\n\nX, y, coef = (\n    n_samples=n_samples,\n    n_features=1,\n    n_informative=1,\n    noise=10,\n    coef=True,\n    random_state=0,\n)\n\n# Add outlier data\n(0)\nX[:n_outliers] = 3 + 0.5 * (size=(n_outliers, 1))\ny[:n_outliers] = -3 + 10 * (size=n_outliers)\n\n# Fit line using all data\nlr = ()\nlr.fit(X, y)\n\n# Robustly fit linear model with RANSAC algorithm\nransac = ()\nransac.fit(X, y)\ninlier_mask = ransac.inlier_mask_\noutlier_mask = (inlier_mask)\n\n# Predict data of estimated models\nline_X = (X.min(), X.max())[:, ]\nline_y = lr.predict(line_X)\nline_y_ransac = ransac.predict(line_X)\n\n# Compare estimated coefficients\nprint(\"Estimated coefficients (true, linear regression, RANSAC):\")\nprint(coef, lr.coef_, ransac.estimator_.coef_)\n\nlw = 2\n(\n    X[inlier_mask], y[inlier_mask], color=\"yellowgreen\", marker=\".\", label=\"Inliers\"\n)\n(\n    X[outlier_mask], y[outlier_mask], color=\"gold\", marker=\".\", label=\"Outliers\"\n)\n(line_X, line_y, color=\"navy\", linewidth=lw, label=\"Linear regressor\")\n(\n    line_X,\n    line_y_ransac,\n    color=\"cornflowerblue\",\n    linewidth=lw,\n    label=\"RANSAC regressor\",\n)\n(loc=\"lower right\")\n(\"Input\")\n(\"Response\")\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.101 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->SGD: Maximum margin separating hyperplane": [
        [
            "Plot the maximum margin separating hyperplane within a two-class\nseparable dataset using a linear Support Vector Machines classifier\ntrained using SGD.\n<img alt=\"plot sgd separating hyperplane\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_sgd_separating_hyperplane_001.png\" srcset=\"../../_images/sphx_glr_plot_sgd_separating_hyperplane_001.png\"/>",
            "markdown"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import \nfrom sklearn.datasets import \n\n# we create 50 separable points\nX, Y = (n_samples=50, centers=2, random_state=0, cluster_std=0.60)\n\n# fit the model\nclf = (loss=\"hinge\", alpha=0.01, max_iter=200)\n\nclf.fit(X, Y)\n\n# plot the line, the points, and the nearest vectors to the plane\nxx = (-1, 5, 10)\nyy = (-1, 5, 10)\n\nX1, X2 = (xx, yy)\nZ = (X1.shape)\nfor (i, j), val in (X1):\n    x1 = val\n    x2 = X2[i, j]\n    p = clf.decision_function([[x1, x2]])\n    Z[i, j] = p[0]\nlevels = [-1.0, 0.0, 1.0]\nlinestyles = [\"dashed\", \"solid\", \"dashed\"]\ncolors = \"k\"\n(X1, X2, Z, levels, colors=colors, linestyles=linestyles)\n(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired, edgecolor=\"black\", s=20)\n\n(\"tight\")\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.073 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->SGD: Penalties": [
        [
            "Contours of where the penalty is equal to 1\nfor the three penalties L1, L2 and elastic-net.",
            "markdown"
        ],
        [
            "All of the above are supported by \nand .\n<img alt=\"plot sgd penalties\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_sgd_penalties_001.png\" srcset=\"../../_images/sphx_glr_plot_sgd_penalties_001.png\"/>",
            "markdown"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\n\nl1_color = \"navy\"\nl2_color = \"c\"\nelastic_net_color = \"darkorange\"\n\nline = (-1.5, 1.5, 1001)\nxx, yy = (line, line)\n\nl2 = xx**2 + yy**2\nl1 = np.abs(xx) + np.abs(yy)\nrho = 0.5\nelastic_net = rho * l1 + (1 - rho) * l2\n\n(figsize=(10, 10), dpi=100)\nax = ()\n\nelastic_net_contour = (\n    xx, yy, elastic_net, levels=[1], colors=elastic_net_color\n)\nl2_contour = (xx, yy, l2, levels=[1], colors=l2_color)\nl1_contour = (xx, yy, l1, levels=[1], colors=l1_color)\nax.set_aspect(\"equal\")\nax.spines[\"left\"].set_position(\"center\")\nax.spines[\"right\"].set_color(\"none\")\nax.spines[\"bottom\"].set_position(\"center\")\nax.spines[\"top\"].set_color(\"none\")\n\n(\n    elastic_net_contour,\n    inline=1,\n    fontsize=18,\n    fmt={1.0: \"elastic-net\"},\n    manual=[(-1, -1)],\n)\n(l2_contour, inline=1, fontsize=18, fmt={1.0: \"L2\"}, manual=[(-1, -1)])\n(l1_contour, inline=1, fontsize=18, fmt={1.0: \"L1\"}, manual=[(-1, -1)])\n\n()\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.251 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->SGD: Weighted samples": [
        [
            "Plot decision function of a weighted dataset, where the size of points\nis proportional to its weight.\n<img alt=\"plot sgd weighted samples\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_sgd_weighted_samples_001.png\" srcset=\"../../_images/sphx_glr_plot_sgd_weighted_samples_001.png\"/>",
            "markdown"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model\n\n# we create 20 points\n(0)\nX = [(10, 2) + [1, 1], (10, 2)]\ny = [1] * 10 + [-1] * 10\nsample_weight = 100 * np.abs((20))\n# and assign a bigger weight to the last 10 samples\nsample_weight[:10] *= 10\n\n# plot the weighted data points\nxx, yy = ((-4, 5, 500), (-4, 5, 500))\nfig, ax = ()\nax.scatter(\n    X[:, 0],\n    X[:, 1],\n    c=y,\n    s=sample_weight,\n    alpha=0.9,\n    cmap=plt.cm.bone,\n    edgecolor=\"black\",\n)\n\n# fit the unweighted model\nclf = (alpha=0.01, max_iter=100)\nclf.fit(X, y)\nZ = clf.decision_function([xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\nno_weights = ax.contour(xx, yy, Z, levels=[0], linestyles=[\"solid\"])\n\n# fit the weighted model\nclf = (alpha=0.01, max_iter=100)\nclf.fit(X, y, sample_weight=sample_weight)\nZ = clf.decision_function([xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\nsamples_weights = ax.contour(xx, yy, Z, levels=[0], linestyles=[\"dashed\"])\n\nno_weights_handles, _ = no_weights.legend_elements()\nweights_handles, _ = samples_weights.legend_elements()\nax.legend(\n    [no_weights_handles[0], weights_handles[0]],\n    [\"no weights\", \"with weights\"],\n    loc=\"lower left\",\n)\n\nax.set(xticks=(), yticks=())\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.077 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->SGD: convex loss functions": [
        [
            "A plot that compares the various convex loss functions supported by\n .\n<img alt=\"plot sgd loss functions\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_sgd_loss_functions_001.png\" srcset=\"../../_images/sphx_glr_plot_sgd_loss_functions_001.png\"/>",
            "markdown"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef modified_huber_loss(y_true, y_pred):\n    z = y_pred * y_true\n    loss = -4 * z\n    loss[z = -1] = (1 - z[z = -1]) ** 2\n    loss[z = 1.0] = 0\n    return loss\n\n\nxmin, xmax = -4, 4\nxx = (xmin, xmax, 100)\nlw = 2\n([xmin, 0, 0, xmax], [1, 1, 0, 0], color=\"gold\", lw=lw, label=\"Zero-one loss\")\n(xx, (xx &lt; 1, 1 - xx, 0), color=\"teal\", lw=lw, label=\"Hinge loss\")\n(xx, -(xx, 0), color=\"yellowgreen\", lw=lw, label=\"Perceptron loss\")\n(xx, (1 + (-xx)), color=\"cornflowerblue\", lw=lw, label=\"Log loss\")\n(\n    xx,\n    (xx &lt; 1, 1 - xx, 0) ** 2,\n    color=\"orange\",\n    lw=lw,\n    label=\"Squared hinge loss\",\n)\n(\n    xx,\n    modified_huber_loss(xx, 1),\n    color=\"darkorchid\",\n    lw=lw,\n    linestyle=\"--\",\n    label=\"Modified Huber loss\",\n)\n((0, 8))\n(loc=\"upper right\")\n(r\"Decision function $f(x)$\")\n(\"$L(y=1, f(x))$\")\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.133 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Sparsity Example: Fitting only features 1  and 2": [
        [
            "Features 1 and 2 of the diabetes-dataset are fitted and\nplotted below. It illustrates that although feature 2\nhas a strong coefficient on the full model, it does not\ngive us much regarding y when compared to just feature 1.",
            "markdown"
        ],
        [
            "# Code source: Ga\u00ebl Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause",
            "code"
        ],
        [
            "First we load the diabetes dataset.",
            "markdown"
        ],
        [
            "from sklearn import datasets\nimport numpy as np\n\nX, y = (return_X_y=True)\nindices = (0, 1)\n\nX_train = X[:-20, indices]\nX_test = X[-20:, indices]\ny_train = y[:-20]\ny_test = y[-20:]",
            "code"
        ],
        [
            "Next we fit a linear regression model.",
            "markdown"
        ],
        [
            "from sklearn import linear_model\n\nols = ()\n_ = ols.fit(X_train, y_train)",
            "code"
        ],
        [
            "Finally we plot the figure from three different views.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n\n# unused but required import for doing 3d projections with matplotlib &lt; 3.2\nimport mpl_toolkits.mplot3d  # noqa: F401\n\n\ndef plot_figs(fig_num, elev, azim, X_train, clf):\n    fig = (fig_num, figsize=(4, 3))\n    ()\n    ax = fig.add_subplot(111, projection=\"3d\", elev=elev, azim=azim)\n\n    ax.scatter(X_train[:, 0], X_train[:, 1], y_train, c=\"k\", marker=\"+\")\n    ax.plot_surface(\n        ([[-0.1, -0.1], [0.15, 0.15]]),\n        ([[-0.1, 0.15], [-0.1, 0.15]]),\n        clf.predict(\n            ([[-0.1, -0.1, 0.15, 0.15], [-0.1, 0.15, -0.1, 0.15]]).T\n        ).reshape((2, 2)),\n        alpha=0.5,\n    )\n    ax.set_xlabel(\"X_1\")\n    ax.set_ylabel(\"X_2\")\n    ax.set_zlabel(\"Y\")\n    ax.xaxis.set_ticklabels([])\n    ax.yaxis.set_ticklabels([])\n    ax.zaxis.set_ticklabels([])\n\n\n# Generate the three different figures from different views\nelev = 43.5\nazim = -110\nplot_figs(1, elev, azim, X_train, ols)\n\nelev = -0.5\nazim = 0\nplot_figs(2, elev, azim, X_train, ols)\n\nelev = -0.5\nazim = 90\nplot_figs(3, elev, azim, X_train, ols)\n\n()\n\n\n\n<img alt=\"plot ols 3d\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_ols_3d_001.png\" srcset=\"../../_images/sphx_glr_plot_ols_3d_001.png\"/>\n<img alt=\"plot ols 3d\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_ols_3d_002.png\" srcset=\"../../_images/sphx_glr_plot_ols_3d_002.png\"/>\n<img alt=\"plot ols 3d\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_ols_3d_003.png\" srcset=\"../../_images/sphx_glr_plot_ols_3d_003.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.190 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Theil-Sen Regression": [
        [
            "Computes a Theil-Sen Regression on a synthetic dataset.",
            "markdown"
        ],
        [
            "See  for more information on the regressor.",
            "markdown"
        ],
        [
            "Compared to the OLS (ordinary least squares) estimator, the Theil-Sen\nestimator is robust against outliers. It has a breakdown point of about 29.3%\nin case of a simple linear regression which means that it can tolerate\narbitrary corrupted data (outliers) of up to 29.3% in the two-dimensional\ncase.",
            "markdown"
        ],
        [
            "The estimation of the model is done by calculating the slopes and intercepts\nof a subpopulation of all possible combinations of p subsample points. If an\nintercept is fitted, p must be greater than or equal to n_features + 1. The\nfinal slope and intercept is then defined as the spatial median of these\nslopes and intercepts.",
            "markdown"
        ],
        [
            "In certain cases Theil-Sen performs better than  which is also a robust method. This is illustrated in the\nsecond example below where outliers with respect to the x-axis perturb RANSAC.\nTuning the residual_threshold parameter of RANSAC remedies this but in\ngeneral a priori knowledge about the data and the nature of the outliers is\nneeded.\nDue to the computational complexity of Theil-Sen it is recommended to use it\nonly for small problems in terms of number of samples and features. For larger\nproblems the max_subpopulation parameter restricts the magnitude of all\npossible combinations of p subsample points to a randomly chosen subset and\ntherefore also limits the runtime. Therefore, Theil-Sen is applicable to larger\nproblems with the drawback of losing some of its mathematical properties since\nit then works on a random subset.",
            "markdown"
        ],
        [
            "# Author: Florian Wilhelm -- &lt;florian.wilhelm@gmail.com\n# License: BSD 3 clause\n\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import , \nfrom sklearn.linear_model import \n\nestimators = [\n    (\"OLS\", ()),\n    (\"Theil-Sen\", (random_state=42)),\n    (\"RANSAC\", (random_state=42)),\n]\ncolors = {\"OLS\": \"turquoise\", \"Theil-Sen\": \"gold\", \"RANSAC\": \"lightgreen\"}\nlw = 2",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Theil-Sen Regression->Outliers only in the y direction": [
        [
            "(0)\nn_samples = 200\n# Linear model y = 3*x + N(2, 0.1**2)\nx = (n_samples)\nw = 3.0\nc = 2.0\nnoise = 0.1 * (n_samples)\ny = w * x + c + noise\n# 10% outliers\ny[-20:] += -20 * x[-20:]\nX = x[:, ]\n\n(x, y, color=\"indigo\", marker=\"x\", s=40)\nline_x = ([-3, 3])\nfor name, estimator in estimators:\n    t0 = ()\n    estimator.fit(X, y)\n    elapsed_time = () - t0\n    y_pred = estimator.predict(line_x.reshape(2, 1))\n    (\n        line_x,\n        y_pred,\n        color=colors[name],\n        linewidth=lw,\n        label=\"%s (fit time: %.2fs)\" % (name, elapsed_time),\n    )\n\n(\"tight\")\n(loc=\"upper left\")\n_ = (\"Corrupt y\")\n\n\n<img alt=\"Corrupt y\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_theilsen_001.png\" srcset=\"../../_images/sphx_glr_plot_theilsen_001.png\"/>",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Theil-Sen Regression->Outliers in the X direction": [
        [
            "(0)\n# Linear model y = 3*x + N(2, 0.1**2)\nx = (n_samples)\nnoise = 0.1 * (n_samples)\ny = 3 * x + 2 + noise\n# 10% outliers\nx[-20:] = 9.9\ny[-20:] += 22\nX = x[:, ]\n\n()\n(x, y, color=\"indigo\", marker=\"x\", s=40)\n\nline_x = ([-3, 10])\nfor name, estimator in estimators:\n    t0 = ()\n    estimator.fit(X, y)\n    elapsed_time = () - t0\n    y_pred = estimator.predict(line_x.reshape(2, 1))\n    (\n        line_x,\n        y_pred,\n        color=colors[name],\n        linewidth=lw,\n        label=\"%s (fit time: %.2fs)\" % (name, elapsed_time),\n    )\n\n(\"tight\")\n(loc=\"upper left\")\n(\"Corrupt x\")\n()\n\n\n<img alt=\"Corrupt x\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_theilsen_002.png\" srcset=\"../../_images/sphx_glr_plot_theilsen_002.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.652 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Tweedie regression on insurance claims": [
        [
            "This example illustrates the use of Poisson, Gamma and Tweedie regression on\nthe , and is inspired by an R tutorial .",
            "markdown"
        ],
        [
            "In this dataset, each sample corresponds to an insurance policy, i.e. a\ncontract within an insurance company and an individual (policyholder).\nAvailable features include driver age, vehicle age, vehicle power, etc.",
            "markdown"
        ],
        [
            "A few definitions: a claim is the request made by a policyholder to the\ninsurer to compensate for a loss covered by the insurance. The claim amount\nis the amount of money that the insurer must pay. The exposure is the\nduration of the insurance coverage of a given policy, in years.",
            "markdown"
        ],
        [
            "Here our goal is to predict the expected\nvalue, i.e. the mean, of the total claim amount per exposure unit also\nreferred to as the pure premium.",
            "markdown"
        ],
        [
            "There are several possibilities to do that, two of which are:",
            "markdown"
        ],
        [
            "Model the number of claims with a Poisson distribution, and the average\nclaim amount per claim, also known as severity, as a Gamma distribution\nand multiply the predictions of both in order to get the total claim\namount.",
            "markdown"
        ],
        [
            "Model the total claim amount per exposure directly, typically with a Tweedie\ndistribution of Tweedie power \\(p \\in (1, 2)\\).",
            "markdown"
        ],
        [
            "In this example we will illustrate both approaches. We start by defining a few\nhelper functions for loading the data and visualizing results.\n\n\n[]",
            "markdown"
        ],
        [
            "A. Noll, R. Salzmann and M.V. Wuthrich, Case Study: French Motor\nThird-Party Liability Claims (November 8, 2018).",
            "markdown"
        ],
        [
            "# Authors: Christian Lorentzen &lt;lorentzen.ch@gmail.com\n#          Roman Yurchak &lt;rth.yurchak@gmail.com\n#          Olivier Grisel &lt;olivier.grisel@ensta.org\n# License: BSD 3 clause",
            "code"
        ],
        [
            "from functools import \n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom sklearn.datasets import \nfrom sklearn.metrics import \nfrom sklearn.metrics import \nfrom sklearn.metrics import \n\n\ndef load_mtpl2(n_samples=None):\n    \"\"\"Fetch the French Motor Third-Party Liability Claims dataset.\n\n    Parameters\n    ----------\n    n_samples: int, default=None\n      number of samples to select (for faster run time). Full dataset has\n      678013 samples.\n    \"\"\"\n    # freMTPL2freq dataset from https://www.openml.org/d/41214\n    df_freq = (data_id=41214, as_frame=True, parser=\"pandas\").data\n    df_freq[\"IDpol\"] = df_freq[\"IDpol\"].astype(int)\n    df_freq.set_index(\"IDpol\", inplace=True)\n\n    # freMTPL2sev dataset from https://www.openml.org/d/41215\n    df_sev = (data_id=41215, as_frame=True, parser=\"pandas\").data\n\n    # sum ClaimAmount over identical IDs\n    df_sev = df_sev.groupby(\"IDpol\").sum()\n\n    df = df_freq.join(df_sev, how=\"left\")\n    df[\"ClaimAmount\"].fillna(0, inplace=True)\n\n    # unquote string fields\n    for column_name in df.columns[df.dtypes.values == object]:\n        df[column_name] = df[column_name].str.strip(\"'\")\n    return df.iloc[:n_samples]\n\n\ndef plot_obs_pred(\n    df,\n    feature,\n    weight,\n    observed,\n    predicted,\n    y_label=None,\n    title=None,\n    ax=None,\n    fill_legend=False,\n):\n    \"\"\"Plot observed and predicted - aggregated per feature level.\n\n    Parameters\n    ----------\n    df : DataFrame\n        input data\n    feature: str\n        a column name of df for the feature to be plotted\n    weight : str\n        column name of df with the values of weights or exposure\n    observed : str\n        a column name of df with the observed target\n    predicted : DataFrame\n        a dataframe, with the same index as df, with the predicted target\n    fill_legend : bool, default=False\n        whether to show fill_between legend\n    \"\"\"\n    # aggregate observed and predicted variables by feature level\n    df_ = df.loc[:, [feature, weight]].copy()\n    df_[\"observed\"] = df[observed] * df[weight]\n    df_[\"predicted\"] = predicted * df[weight]\n    df_ = (\n        df_.groupby([feature])[[weight, \"observed\", \"predicted\"]]\n        .sum()\n        .assign(observed=lambda x: x[\"observed\"] / x[weight])\n        .assign(predicted=lambda x: x[\"predicted\"] / x[weight])\n    )\n\n    ax = df_.loc[:, [\"observed\", \"predicted\"]].plot(style=\".\", ax=ax)\n    y_max = df_.loc[:, [\"observed\", \"predicted\"]].values.max() * 0.8\n    p2 = ax.fill_between(\n        df_.index,\n        0,\n        y_max * df_[weight] / df_[weight].values.max(),\n        color=\"g\",\n        alpha=0.1,\n    )\n    if fill_legend:\n        ax.legend([p2], [\"{} distribution\".format(feature)])\n    ax.set(\n        ylabel=y_label if y_label is not None else None,\n        title=title if title is not None else \"Train: Observed vs Predicted\",\n    )\n\n\ndef score_estimator(\n    estimator,\n    X_train,\n    X_test,\n    df_train,\n    df_test,\n    target,\n    weights,\n    tweedie_powers=None,\n):\n    \"\"\"Evaluate an estimator on train and test sets with different metrics\"\"\"\n\n    metrics = [\n        (\"D\u00b2 explained\", None),  # Use default scorer if it exists\n        (\"mean abs. error\", ),\n        (\"mean squared error\", ),\n    ]\n    if tweedie_powers:\n        metrics += [\n            (\n                \"mean Tweedie dev p={:.4f}\".format(power),\n                (, power=power),\n            )\n            for power in tweedie_powers\n        ]\n\n    res = []\n    for subset_label, X, df in [\n        (\"train\", X_train, df_train),\n        (\"test\", X_test, df_test),\n    ]:\n        y, _weights = df[target], df[weights]\n        for score_label, metric in metrics:\n            if isinstance(estimator, tuple) and len(estimator) == 2:\n                # Score the model consisting of the product of frequency and\n                # severity models.\n                est_freq, est_sev = estimator\n                y_pred = est_freq.predict(X) * est_sev.predict(X)\n            else:\n                y_pred = estimator.predict(X)\n\n            if metric is None:\n                if not hasattr(estimator, \"score\"):\n                    continue\n                score = estimator.score(X, y, sample_weight=_weights)\n            else:\n                score = metric(y, y_pred, sample_weight=_weights)\n\n            res.append({\"subset\": subset_label, \"metric\": score_label, \"score\": score})\n\n    res = (\n        (res)\n        .set_index([\"metric\", \"subset\"])\n        .score.unstack(-1)\n        .round(4)\n        .loc[:, [\"train\", \"test\"]]\n    )\n    return res",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Tweedie regression on insurance claims->Loading datasets, basic feature extraction and target definitions": [
        [
            "We construct the freMTPL2 dataset by joining the freMTPL2freq table,\ncontaining the number of claims (ClaimNb), with the freMTPL2sev table,\ncontaining the claim amount (ClaimAmount) for the same policy ids\n(IDpol).",
            "markdown"
        ],
        [
            "from sklearn.pipeline import \nfrom sklearn.preprocessing import , \nfrom sklearn.preprocessing import , \nfrom sklearn.compose import \n\n\ndf = load_mtpl2()\n\n# Note: filter out claims with zero amount, as the severity model\n# requires strictly positive target values.\ndf.loc[(df[\"ClaimAmount\"] == 0) & (df[\"ClaimNb\"] = 1), \"ClaimNb\"] = 0\n\n# Correct for unreasonable observations (that might be data error)\n# and a few exceptionally large claim amounts\ndf[\"ClaimNb\"] = df[\"ClaimNb\"].clip(upper=4)\ndf[\"Exposure\"] = df[\"Exposure\"].clip(upper=1)\ndf[\"ClaimAmount\"] = df[\"ClaimAmount\"].clip(upper=200000)\n\nlog_scale_transformer = (\n    (func=), ()\n)\n\ncolumn_trans = (\n    [\n        (\n            \"binned_numeric\",\n            (n_bins=10, subsample=int(2e5), random_state=0),\n            [\"VehAge\", \"DrivAge\"],\n        ),\n        (\n            \"onehot_categorical\",\n            (),\n            [\"VehBrand\", \"VehPower\", \"VehGas\", \"Region\", \"Area\"],\n        ),\n        (\"passthrough_numeric\", \"passthrough\", [\"BonusMalus\"]),\n        (\"log_scaled_numeric\", log_scale_transformer, [\"Density\"]),\n    ],\n    remainder=\"drop\",\n)\nX = column_trans.fit_transform(df)\n\n# Insurances companies are interested in modeling the Pure Premium, that is\n# the expected total claim amount per unit of exposure for each policyholder\n# in their portfolio:\ndf[\"PurePremium\"] = df[\"ClaimAmount\"] / df[\"Exposure\"]\n\n# This can be indirectly approximated by a 2-step modeling: the product of the\n# Frequency times the average claim amount per claim:\ndf[\"Frequency\"] = df[\"ClaimNb\"] / df[\"Exposure\"]\ndf[\"AvgClaimAmount\"] = df[\"ClaimAmount\"] / (df[\"ClaimNb\"], 1)\n\nwith (\"display.max_columns\", 15):\n    print(df[df.ClaimAmount  0].head())",
            "code"
        ],
        [
            "ClaimNb  Exposure Area  VehPower  VehAge  DrivAge  BonusMalus VehBrand  \\\nIDpol\n139          1      0.75    F         7       1       61          50      B12\n190          1      0.14    B        12       5       50          60      B12\n414          1      0.14    E         4       0       36          85      B12\n424          2      0.62    F        10       0       51         100      B12\n463          1      0.31    A         5       0       45          50      B12\n\n        VehGas  Density Region  ClaimAmount   PurePremium  Frequency  \\\nIDpol\n139    Regular    27000    R11       303.00    404.000000   1.333333\n190     Diesel       56    R25      1981.84  14156.000000   7.142857\n414    Regular     4792    R11      1456.55  10403.928571   7.142857\n424    Regular    27000    R11     10834.00  17474.193548   3.225806\n463    Regular       12    R73      3986.67  12860.225806   3.225806\n\n       AvgClaimAmount\nIDpol\n139            303.00\n190           1981.84\n414           1456.55\n424           5417.00\n463           3986.67",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Tweedie regression on insurance claims->Frequency model \u2013 Poisson distribution": [
        [
            "The number of claims (ClaimNb) is a positive integer (0 included).\nThus, this target can be modelled by a Poisson distribution.\nIt is then assumed to be the number of discrete events occurring with a\nconstant rate in a given time interval (Exposure, in units of years).\nHere we model the frequency y = ClaimNb / Exposure, which is still a\n(scaled) Poisson distribution, and use Exposure as sample_weight.",
            "markdown"
        ],
        [
            "from sklearn.model_selection import \nfrom sklearn.linear_model import \n\n\ndf_train, df_test, X_train, X_test = (df, X, random_state=0)",
            "code"
        ],
        [
            "Let us keep in mind that despite the seemingly large number of data points in\nthis dataset, the number of evaluation points where the claim amount is\nnon-zero is quite small:",
            "markdown"
        ],
        [
            "len(df_test)",
            "code"
        ],
        [
            "169504",
            "code"
        ],
        [
            "len(df_test[df_test[\"ClaimAmount\"]  0])",
            "code"
        ],
        [
            "6237",
            "code"
        ],
        [
            "As a consequence, we expect a significant variability in our\nevaluation upon random resampling of the train test split.",
            "markdown"
        ],
        [
            "The parameters of the model are estimated by minimizing the Poisson deviance\non the training set via a Newton solver. Some of the features are collinear\n(e.g. because we did not drop any categorical level in the OneHotEncoder),\nwe use a weak L2 penalization to avoid numerical issues.",
            "markdown"
        ],
        [
            "glm_freq = (alpha=1e-4, solver=\"newton-cholesky\")\nglm_freq.fit(X_train, df_train[\"Frequency\"], sample_weight=df_train[\"Exposure\"])\n\nscores = score_estimator(\n    glm_freq,\n    X_train,\n    X_test,\n    df_train,\n    df_test,\n    target=\"Frequency\",\n    weights=\"Exposure\",\n)\nprint(\"Evaluation of PoissonRegressor on target Frequency\")\nprint(scores)",
            "code"
        ],
        [
            "Evaluation of PoissonRegressor on target Frequency\nsubset               train    test\nmetric\nD\u00b2 explained        0.0201  0.0219\nmean abs. error     0.1379  0.1378\nmean squared error  0.2441  0.2246",
            "code"
        ],
        [
            "Note that the score measured on the test set is surprisingly better than on\nthe training set. This might be specific to this random train-test split.\nProper cross-validation could help us to assess the sampling variability of\nthese results.",
            "markdown"
        ],
        [
            "We can visually compare observed and predicted values, aggregated by the\ndrivers age (DrivAge), vehicle age (VehAge) and the insurance\nbonus/malus (BonusMalus).",
            "markdown"
        ],
        [
            "fig, ax = (ncols=2, nrows=2, figsize=(16, 8))\nfig.subplots_adjust(hspace=0.3, wspace=0.2)\n\nplot_obs_pred(\n    df=df_train,\n    feature=\"DrivAge\",\n    weight=\"Exposure\",\n    observed=\"Frequency\",\n    predicted=glm_freq.predict(X_train),\n    y_label=\"Claim Frequency\",\n    title=\"train data\",\n    ax=ax[0, 0],\n)\n\nplot_obs_pred(\n    df=df_test,\n    feature=\"DrivAge\",\n    weight=\"Exposure\",\n    observed=\"Frequency\",\n    predicted=glm_freq.predict(X_test),\n    y_label=\"Claim Frequency\",\n    title=\"test data\",\n    ax=ax[0, 1],\n    fill_legend=True,\n)\n\nplot_obs_pred(\n    df=df_test,\n    feature=\"VehAge\",\n    weight=\"Exposure\",\n    observed=\"Frequency\",\n    predicted=glm_freq.predict(X_test),\n    y_label=\"Claim Frequency\",\n    title=\"test data\",\n    ax=ax[1, 0],\n    fill_legend=True,\n)\n\nplot_obs_pred(\n    df=df_test,\n    feature=\"BonusMalus\",\n    weight=\"Exposure\",\n    observed=\"Frequency\",\n    predicted=glm_freq.predict(X_test),\n    y_label=\"Claim Frequency\",\n    title=\"test data\",\n    ax=ax[1, 1],\n    fill_legend=True,\n)\n\n\n<img alt=\"train data, test data, test data, test data\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_tweedie_regression_insurance_claims_001.png\" srcset=\"../../_images/sphx_glr_plot_tweedie_regression_insurance_claims_001.png\"/>",
            "code"
        ],
        [
            "According to the observed data, the frequency of accidents is higher for\ndrivers younger than 30 years old, and is positively correlated with the\nBonusMalus variable. Our model is able to mostly correctly model this\nbehaviour.",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Tweedie regression on insurance claims->Severity Model -  Gamma distribution": [
        [
            "The mean claim amount or severity (AvgClaimAmount) can be empirically\nshown to follow approximately a Gamma distribution. We fit a GLM model for\nthe severity with the same features as the frequency model.",
            "markdown"
        ],
        [
            "Note:",
            "markdown"
        ],
        [
            "We filter out ClaimAmount == 0 as the Gamma distribution has support\non \\((0, \\infty)\\), not \\([0, \\infty)\\).",
            "markdown"
        ],
        [
            "We use ClaimNb as sample_weight to account for policies that contain\nmore than one claim.",
            "markdown"
        ],
        [
            "from sklearn.linear_model import \n\n\nmask_train = df_train[\"ClaimAmount\"]  0\nmask_test = df_test[\"ClaimAmount\"]  0\n\nglm_sev = (alpha=10.0, solver=\"newton-cholesky\")\n\nglm_sev.fit(\n    X_train[mask_train.values],\n    df_train.loc[mask_train, \"AvgClaimAmount\"],\n    sample_weight=df_train.loc[mask_train, \"ClaimNb\"],\n)\n\nscores = score_estimator(\n    glm_sev,\n    X_train[mask_train.values],\n    X_test[mask_test.values],\n    df_train[mask_train],\n    df_test[mask_test],\n    target=\"AvgClaimAmount\",\n    weights=\"ClaimNb\",\n)\nprint(\"Evaluation of GammaRegressor on target AvgClaimAmount\")\nprint(scores)",
            "code"
        ],
        [
            "Evaluation of GammaRegressor on target AvgClaimAmount\nsubset                     train          test\nmetric\nD\u00b2 explained        2.400000e-03  2.700000e-03\nmean abs. error     1.756746e+03  1.744042e+03\nmean squared error  5.801770e+07  5.030677e+07",
            "code"
        ],
        [
            "Those values of the metrics are not necessarily easy to interpret. It can be\ninsightful to compare them with a model that does not use any input\nfeatures and always predicts a constant value, i.e. the average claim\namount, in the same setting:",
            "markdown"
        ],
        [
            "from sklearn.dummy import \n\ndummy_sev = (strategy=\"mean\")\ndummy_sev.fit(\n    X_train[mask_train.values],\n    df_train.loc[mask_train, \"AvgClaimAmount\"],\n    sample_weight=df_train.loc[mask_train, \"ClaimNb\"],\n)\n\nscores = score_estimator(\n    dummy_sev,\n    X_train[mask_train.values],\n    X_test[mask_test.values],\n    df_train[mask_train],\n    df_test[mask_test],\n    target=\"AvgClaimAmount\",\n    weights=\"ClaimNb\",\n)\nprint(\"Evaluation of a mean predictor on target AvgClaimAmount\")\nprint(scores)",
            "code"
        ],
        [
            "Evaluation of a mean predictor on target AvgClaimAmount\nsubset                     train          test\nmetric\nD\u00b2 explained        0.000000e+00 -0.000000e+00\nmean abs. error     1.756687e+03  1.744497e+03\nmean squared error  5.803882e+07  5.033764e+07",
            "code"
        ],
        [
            "We conclude that the claim amount is very challenging to predict. Still, the\nGammaRegressor is able to leverage some information\nfrom the input features to slighly improve upon the mean baseline in terms\nof D\u00b2.",
            "markdown"
        ],
        [
            "Note that the resulting model is the average claim amount per claim. As such,\nit is conditional on having at least one claim, and cannot be used to predict\nthe average claim amount per policy. For this, it needs to be combined with\na claims frequency model.",
            "markdown"
        ],
        [
            "print(\n    \"Mean AvgClaim Amount per policy:              %.2f \"\n    % df_train[\"AvgClaimAmount\"].mean()\n)\nprint(\n    \"Mean AvgClaim Amount | NbClaim  0:           %.2f\"\n    % df_train[\"AvgClaimAmount\"][df_train[\"AvgClaimAmount\"]  0].mean()\n)\nprint(\n    \"Predicted Mean AvgClaim Amount | NbClaim  0: %.2f\"\n    % glm_sev.predict(X_train).mean()\n)\nprint(\n    \"Predicted Mean AvgClaim Amount (dummy) | NbClaim  0: %.2f\"\n    % dummy_sev.predict(X_train).mean()\n)",
            "code"
        ],
        [
            "Mean AvgClaim Amount per policy:              71.78\nMean AvgClaim Amount | NbClaim  0:           1951.21\nPredicted Mean AvgClaim Amount | NbClaim  0: 1940.95\nPredicted Mean AvgClaim Amount (dummy) | NbClaim  0: 1978.59",
            "code"
        ],
        [
            "We can visually compare observed and predicted values, aggregated for\nthe drivers age (DrivAge).",
            "markdown"
        ],
        [
            "fig, ax = (ncols=1, nrows=2, figsize=(16, 6))\n\nplot_obs_pred(\n    df=df_train.loc[mask_train],\n    feature=\"DrivAge\",\n    weight=\"Exposure\",\n    observed=\"AvgClaimAmount\",\n    predicted=glm_sev.predict(X_train[mask_train.values]),\n    y_label=\"Average Claim Severity\",\n    title=\"train data\",\n    ax=ax[0],\n)\n\nplot_obs_pred(\n    df=df_test.loc[mask_test],\n    feature=\"DrivAge\",\n    weight=\"Exposure\",\n    observed=\"AvgClaimAmount\",\n    predicted=glm_sev.predict(X_test[mask_test.values]),\n    y_label=\"Average Claim Severity\",\n    title=\"test data\",\n    ax=ax[1],\n    fill_legend=True,\n)\n()\n\n\n<img alt=\"train data, test data\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_tweedie_regression_insurance_claims_002.png\" srcset=\"../../_images/sphx_glr_plot_tweedie_regression_insurance_claims_002.png\"/>",
            "code"
        ],
        [
            "Overall, the drivers age (DrivAge) has a weak impact on the claim\nseverity, both in observed and predicted data.",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Tweedie regression on insurance claims->Pure Premium Modeling via a Product Model vs single TweedieRegressor": [
        [
            "As mentioned in the introduction, the total claim amount per unit of\nexposure can be modeled as the product of the prediction of the\nfrequency model by the prediction of the severity model.",
            "markdown"
        ],
        [
            "Alternatively, one can directly model the total loss with a unique\nCompound Poisson Gamma generalized linear model (with a log link function).\nThis model is a special case of the Tweedie GLM with a \u201cpower\u201d parameter\n\\(p \\in (1, 2)\\). Here, we fix apriori the power parameter of the\nTweedie model to some arbitrary value (1.9) in the valid range. Ideally one\nwould select this value via grid-search by minimizing the negative\nlog-likelihood of the Tweedie model, but unfortunately the current\nimplementation does not allow for this (yet).",
            "markdown"
        ],
        [
            "We will compare the performance of both approaches.\nTo quantify the performance of both models, one can compute\nthe mean deviance of the train and test data assuming a Compound\nPoisson-Gamma distribution of the total claim amount. This is equivalent to\na Tweedie distribution with a power parameter between 1 and 2.",
            "markdown"
        ],
        [
            "The  depends on a power\nparameter. As we do not know the true value of the power parameter, we here\ncompute the mean deviances for a grid of possible values, and compare the\nmodels side by side, i.e. we compare them at identical values of power.\nIdeally, we hope that one model will be consistently better than the other,\nregardless of power.",
            "markdown"
        ],
        [
            "from sklearn.linear_model import \n\n\nglm_pure_premium = (power=1.9, alpha=0.1, solver=\"newton-cholesky\")\nglm_pure_premium.fit(\n    X_train, df_train[\"PurePremium\"], sample_weight=df_train[\"Exposure\"]\n)\n\ntweedie_powers = [1.5, 1.7, 1.8, 1.9, 1.99, 1.999, 1.9999]\n\nscores_product_model = score_estimator(\n    (glm_freq, glm_sev),\n    X_train,\n    X_test,\n    df_train,\n    df_test,\n    target=\"PurePremium\",\n    weights=\"Exposure\",\n    tweedie_powers=tweedie_powers,\n)\n\nscores_glm_pure_premium = score_estimator(\n    glm_pure_premium,\n    X_train,\n    X_test,\n    df_train,\n    df_test,\n    target=\"PurePremium\",\n    weights=\"Exposure\",\n    tweedie_powers=tweedie_powers,\n)\n\nscores = (\n    [scores_product_model, scores_glm_pure_premium],\n    axis=1,\n    sort=True,\n    keys=(\"Product Model\", \"TweedieRegressor\"),\n)\nprint(\"Evaluation of the Product Model and the Tweedie Regressor on target PurePremium\")\nwith (\"display.expand_frame_repr\", False):\n    print(scores)",
            "code"
        ],
        [
            "Evaluation of the Product Model and the Tweedie Regressor on target PurePremium\n                          Product Model               TweedieRegressor\nsubset                            train          test            train          test\nmetric\nD\u00b2 explained                        NaN           NaN     1.690000e-02  1.420000e-02\nmean Tweedie dev p=1.5000  7.669930e+01  7.617050e+01     7.640770e+01  7.640880e+01\nmean Tweedie dev p=1.7000  3.695740e+01  3.683980e+01     3.682880e+01  3.692270e+01\nmean Tweedie dev p=1.8000  3.046010e+01  3.040530e+01     3.037600e+01  3.045390e+01\nmean Tweedie dev p=1.9000  3.387580e+01  3.385000e+01     3.382120e+01  3.387830e+01\nmean Tweedie dev p=1.9900  2.015716e+02  2.015414e+02     2.015347e+02  2.015587e+02\nmean Tweedie dev p=1.9990  1.914573e+03  1.914370e+03     1.914538e+03  1.914387e+03\nmean Tweedie dev p=1.9999  1.904751e+04  1.904556e+04     1.904747e+04  1.904558e+04\nmean abs. error            2.730119e+02  2.722128e+02     2.739865e+02  2.731249e+02\nmean squared error         3.295040e+07  3.212197e+07     3.295505e+07  3.213056e+07",
            "code"
        ],
        [
            "In this example, both modeling approaches yield comparable performance\nmetrics. For implementation reasons, the percentage of explained variance\n\\(D^2\\) is not available for the product model.",
            "markdown"
        ],
        [
            "We can additionally validate these models by comparing observed and\npredicted total claim amount over the test and train subsets. We see that,\non average, both model tend to underestimate the total claim (but this\nbehavior depends on the amount of regularization).",
            "markdown"
        ],
        [
            "res = []\nfor subset_label, X, df in [\n    (\"train\", X_train, df_train),\n    (\"test\", X_test, df_test),\n]:\n    exposure = df[\"Exposure\"].values\n    res.append(\n        {\n            \"subset\": subset_label,\n            \"observed\": df[\"ClaimAmount\"].values.sum(),\n            \"predicted, frequency*severity model\": (\n                exposure * glm_freq.predict(X) * glm_sev.predict(X)\n            ),\n            \"predicted, tweedie, power=%.2f\"\n            % glm_pure_premium.power: (exposure * glm_pure_premium.predict(X)),\n        }\n    )\n\nprint((res).set_index(\"subset\").T)",
            "code"
        ],
        [
            "subset                                      train          test\nobserved                             3.917618e+07  1.299546e+07\npredicted, frequency*severity model  3.916555e+07  1.313276e+07\npredicted, tweedie, power=1.90       3.951751e+07  1.325198e+07",
            "code"
        ],
        [
            "Finally, we can compare the two models using a plot of cumulated claims: for\neach model, the policyholders are ranked from safest to riskiest based on the\nmodel predictions and the fraction of observed total cumulated claims is\nplotted on the y axis. This plot is often called the ordered Lorenz curve of\nthe model.",
            "markdown"
        ],
        [
            "The Gini coefficient (based on the area between the curve and the diagonal)\ncan be used as a model selection metric to quantify the ability of the model\nto rank policyholders. Note that this metric does not reflect the ability of\nthe models to make accurate predictions in terms of absolute value of total\nclaim amounts but only in terms of relative amounts as a ranking metric. The\nGini coefficient is upper bounded by 1.0 but even an oracle model that ranks\nthe policyholders by the observed claim amounts cannot reach a score of 1.0.",
            "markdown"
        ],
        [
            "We observe that both models are able to rank policyholders by risky-ness\nsignificantly better than chance although they are also both far from the\noracle model due to the natural difficulty of the prediction problem from a\nfew features: most accidents are not predictable and can be caused by\nenvironmental circumstances that are not described at all by the input\nfeatures of the models.",
            "markdown"
        ],
        [
            "Note that the Gini index only characterizes the ranking performance of the\nmodel but not its calibration: any monotonic transformation of the predictions\nleaves the Gini index of the model unchanged.",
            "markdown"
        ],
        [
            "Finally one should highlight that the Compound Poisson Gamma model that is\ndirectly fit on the pure premium is operationally simpler to develop and\nmaintain as it consists of a single scikit-learn estimator instead of a pair\nof models, each with its own set of hyperparameters.",
            "markdown"
        ],
        [
            "from sklearn.metrics import \n\n\ndef lorenz_curve(y_true, y_pred, exposure):\n    y_true, y_pred = (y_true), (y_pred)\n    exposure = (exposure)\n\n    # order samples by increasing predicted risk:\n    ranking = (y_pred)\n    ranked_exposure = exposure[ranking]\n    ranked_pure_premium = y_true[ranking]\n    cumulated_claim_amount = (ranked_pure_premium * ranked_exposure)\n    cumulated_claim_amount /= cumulated_claim_amount[-1]\n    cumulated_samples = (0, 1, len(cumulated_claim_amount))\n    return cumulated_samples, cumulated_claim_amount\n\n\nfig, ax = (figsize=(8, 8))\n\ny_pred_product = glm_freq.predict(X_test) * glm_sev.predict(X_test)\ny_pred_total = glm_pure_premium.predict(X_test)\n\nfor label, y_pred in [\n    (\"Frequency * Severity model\", y_pred_product),\n    (\"Compound Poisson Gamma\", y_pred_total),\n]:\n    ordered_samples, cum_claims = lorenz_curve(\n        df_test[\"PurePremium\"], y_pred, df_test[\"Exposure\"]\n    )\n    gini = 1 - 2 * (ordered_samples, cum_claims)\n    label += \" (Gini index: {:.3f})\".format(gini)\n    ax.plot(ordered_samples, cum_claims, linestyle=\"-\", label=label)\n\n# Oracle model: y_pred == y_test\nordered_samples, cum_claims = lorenz_curve(\n    df_test[\"PurePremium\"], df_test[\"PurePremium\"], df_test[\"Exposure\"]\n)\ngini = 1 - 2 * (ordered_samples, cum_claims)\nlabel = \"Oracle (Gini index: {:.3f})\".format(gini)\nax.plot(ordered_samples, cum_claims, linestyle=\"-.\", color=\"gray\", label=label)\n\n# Random baseline\nax.plot([0, 1], [0, 1], linestyle=\"--\", color=\"black\", label=\"Random baseline\")\nax.set(\n    title=\"Lorenz Curves\",\n    xlabel=\"Fraction of policyholders\\n(ordered by model from safest to riskiest)\",\n    ylabel=\"Fraction of total claim amount\",\n)\nax.legend(loc=\"upper left\")\n()\n\n\n<img alt=\"Lorenz Curves\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_tweedie_regression_insurance_claims_003.png\" srcset=\"../../_images/sphx_glr_plot_tweedie_regression_insurance_claims_003.png\"/>",
            "code"
        ],
        [
            "[]",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  12.790 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models": [
        [
            "In linear models, the target value is modeled as a linear combination of the\nfeatures (see the  User Guide section for a description of a\nset of linear models available in scikit-learn). Coefficients in multiple linear\nmodels represent the relationship between the given feature, \\(X_i\\) and the\ntarget, \\(y\\), assuming that all the other features remain constant\n(). This is different\nfrom plotting \\(X_i\\) versus \\(y\\) and fitting a linear relationship: in\nthat case all possible values of the other features are taken into account in\nthe estimation (marginal dependence).",
            "markdown"
        ],
        [
            "This example will provide some hints in interpreting coefficient in linear\nmodels, pointing at problems that arise when either the linear model is not\nappropriate to describe the dataset, or when features are correlated.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "Keep in mind that the features \\(X\\) and the outcome \\(y\\) are in\ngeneral the result of a data generating process that is unknown to us.\nMachine learning models are trained to approximate the unobserved\nmathematical function that links \\(X\\) to \\(y\\) from sample data. As\na result, any interpretation made about a model may not necessarily\ngeneralize to the true data generating process. This is especially true when\nthe model is of bad quality or when the sample data is not representative of\nthe population.",
            "markdown"
        ],
        [
            "We will use data from the  from 1985 to predict wage as a function of\nvarious features such as experience, age, or education.",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "import numpy as np\nimport scipy as sp\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns",
            "code"
        ]
    ],
    "Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->": [
        [
            "We fetch the data from .\nNote that setting the parameter as_frame to True will retrieve the data\nas a pandas dataframe.",
            "markdown"
        ],
        [
            "from sklearn.datasets import \n\nsurvey = (data_id=534, as_frame=True, parser=\"pandas\")",
            "code"
        ],
        [
            "Then, we identify features X and targets y: the column WAGE is our\ntarget variable (i.e., the variable which we want to predict).",
            "markdown"
        ],
        [
            "X = survey.data[survey.feature_names]\nX.describe(include=\"all\")\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "Note that the dataset contains categorical and numerical variables.\nWe will need to take this into account when preprocessing the dataset\nthereafter.",
            "markdown"
        ],
        [
            "X.head()\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "Our target for prediction: the wage.\nWages are described as floating-point number in dollars per hour.",
            "markdown"
        ],
        [
            "y = survey.target.values.ravel()\nsurvey.target.head()",
            "code"
        ],
        [
            "0    5.10\n1    4.95\n2    6.67\n3    4.00\n4    7.50\nName: WAGE, dtype: float64",
            "code"
        ],
        [
            "We split the sample into a train and a test dataset.\nOnly the train dataset will be used in the following exploratory analysis.\nThis is a way to emulate a real situation where predictions are performed on\nan unknown target, and we don\u2019t want our analysis and decisions to be biased\nby our knowledge of the test data.",
            "markdown"
        ],
        [
            "from sklearn.model_selection import \n\nX_train, X_test, y_train, y_test = (X, y, random_state=42)",
            "code"
        ],
        [
            "First, let\u2019s get some insights by looking at the variable distributions and\nat the pairwise relationships between them. Only numerical\nvariables will be used. In the following plot, each dot represents a sample.\n\n</blockquote>",
            "markdown"
        ],
        [
            "train_dataset = X_train.copy()\ntrain_dataset.insert(0, \"WAGE\", y_train)\n_ = (train_dataset, kind=\"reg\", diag_kind=\"kde\")\n\n\n<img alt=\"plot linear model coefficient interpretation\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_001.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_001.png\"/>",
            "code"
        ],
        [
            "Looking closely at the WAGE distribution reveals that it has a\nlong tail. For this reason, we should take its logarithm\nto turn it approximately into a normal distribution (linear models such\nas ridge or lasso work best for a normal distribution of error).",
            "markdown"
        ],
        [
            "The WAGE is increasing when EDUCATION is increasing.\nNote that the dependence between WAGE and EDUCATION\nrepresented here is a marginal dependence, i.e., it describes the behavior\nof a specific variable without keeping the others fixed.",
            "markdown"
        ],
        [
            "Also, the EXPERIENCE and AGE are strongly linearly correlated.",
            "markdown"
        ],
        [
            "To design our machine-learning pipeline, we first manually\ncheck the type of data that we are dealing with:",
            "markdown"
        ],
        [
            "survey.data.info()",
            "code"
        ],
        [
            "&lt;class 'pandas.core.frame.DataFrame'\nRangeIndex: 534 entries, 0 to 533\nData columns (total 10 columns):\n #   Column      Non-Null Count  Dtype\n---  ------      --------------  -----\n 0   EDUCATION   534 non-null    int64\n 1   SOUTH       534 non-null    category\n 2   SEX         534 non-null    category\n 3   EXPERIENCE  534 non-null    int64\n 4   UNION       534 non-null    category\n 5   AGE         534 non-null    int64\n 6   RACE        534 non-null    category\n 7   OCCUPATION  534 non-null    category\n 8   SECTOR      534 non-null    category\n 9   MARR        534 non-null    category\ndtypes: category(7), int64(3)\nmemory usage: 17.2 KB",
            "code"
        ],
        [
            "As seen previously, the dataset contains columns with different data types\nand we need to apply a specific preprocessing for each data types.\nIn particular categorical variables cannot be included in linear model if not\ncoded as integers first. In addition, to avoid categorical features to be\ntreated as ordered values, we need to one-hot-encode them.\nOur pre-processor will",
            "markdown"
        ],
        [
            "one-hot encode (i.e., generate a column by category) the categorical\ncolumns, only for non-binary categorical variables;",
            "markdown"
        ],
        [
            "as a first approach (we will see after how the normalisation of numerical\nvalues will affect our discussion), keep numerical values as they are.",
            "markdown"
        ],
        [
            "from sklearn.compose import \nfrom sklearn.preprocessing import \n\ncategorical_columns = [\"RACE\", \"OCCUPATION\", \"SECTOR\", \"MARR\", \"UNION\", \"SEX\", \"SOUTH\"]\nnumerical_columns = [\"EDUCATION\", \"EXPERIENCE\", \"AGE\"]\n\npreprocessor = (\n    ((drop=\"if_binary\"), categorical_columns),\n    remainder=\"passthrough\",\n    verbose_feature_names_out=False,  # avoid to prepend the preprocessor names\n)",
            "code"
        ],
        [
            "To describe the dataset as a linear model we use a ridge regressor\nwith a very small regularization and to model the logarithm of the WAGE.",
            "markdown"
        ],
        [
            "from sklearn.pipeline import \nfrom sklearn.linear_model import \nfrom sklearn.compose import \n\nmodel = (\n    preprocessor,\n    (\n        regressor=(alpha=1e-10), func=, inverse_func=\n    ),\n)",
            "code"
        ],
        [
            "First, we fit the model.",
            "markdown"
        ],
        [
            "model.fit(X_train, y_train)",
            "code"
        ],
        [
            "Pipeline(steps=[('columntransformer',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('onehotencoder',\n                                                  OneHotEncoder(drop='if_binary'),\n                                                  ['RACE', 'OCCUPATION',\n                                                   'SECTOR', 'MARR', 'UNION',\n                                                   'SEX', 'SOUTH'])],\n                                   verbose_feature_names_out=False)),\n                ('transformedtargetregressor',\n                 TransformedTargetRegressor(func=&lt;ufunc 'log10',\n                                            inverse_func=&lt;ufunc 'exp10',\n                                            regressor=Ridge(alpha=1e-10)))])<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-118\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-118\">Pipeline</label>",
            "code"
        ],
        [
            "Pipeline(steps=[('columntransformer',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('onehotencoder',\n                                                  OneHotEncoder(drop='if_binary'),\n                                                  ['RACE', 'OCCUPATION',\n                                                   'SECTOR', 'MARR', 'UNION',\n                                                   'SEX', 'SOUTH'])],\n                                   verbose_feature_names_out=False)),\n                ('transformedtargetregressor',\n                 TransformedTargetRegressor(func=&lt;ufunc 'log10',\n                                            inverse_func=&lt;ufunc 'exp10',\n                                            regressor=Ridge(alpha=1e-10)))])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-119\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-119\">columntransformer: ColumnTransformer</label>",
            "code"
        ],
        [
            "ColumnTransformer(remainder='passthrough',\n                  transformers=[('onehotencoder',\n                                 OneHotEncoder(drop='if_binary'),\n                                 ['RACE', 'OCCUPATION', 'SECTOR', 'MARR',\n                                  'UNION', 'SEX', 'SOUTH'])],\n                  verbose_feature_names_out=False)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-120\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-120\">onehotencoder</label>",
            "code"
        ],
        [
            "['RACE', 'OCCUPATION', 'SECTOR', 'MARR', 'UNION', 'SEX', 'SOUTH']<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-121\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-121\">OneHotEncoder</label>",
            "code"
        ],
        [
            "OneHotEncoder(drop='if_binary')<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-122\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-122\">remainder</label>",
            "code"
        ],
        [
            "['EDUCATION', 'EXPERIENCE', 'AGE']<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-123\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-123\">passthrough</label>",
            "code"
        ],
        [
            "passthrough<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-124\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-124\">transformedtargetregressor: TransformedTargetRegressor</label>",
            "code"
        ],
        [
            "TransformedTargetRegressor(func=&lt;ufunc 'log10', inverse_func=&lt;ufunc 'exp10',\n                           regressor=Ridge(alpha=1e-10))<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-125\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-125\">regressor: Ridge</label>",
            "code"
        ],
        [
            "Ridge(alpha=1e-10)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-126\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-126\">Ridge</label>",
            "code"
        ],
        [
            "Ridge(alpha=1e-10)\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "Then we check the performance of the computed model plotting its predictions\non the test set and computing,\nfor example, the median absolute error of the model.",
            "markdown"
        ],
        [
            "from sklearn.metrics import \nfrom sklearn.metrics import PredictionErrorDisplay\n\nmae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}",
            "code"
        ],
        [
            "_, ax = (figsize=(5, 5))\ndisplay = (\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n)\nax.set_title(\"Ridge model, small regularization\")\nfor name, score in scores.items():\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\nax.legend(loc=\"upper left\")\n()\n\n\n<img alt=\"Ridge model, small regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_002.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_002.png\"/>",
            "code"
        ],
        [
            "The model learnt is far from being a good model making accurate predictions:\nthis is obvious when looking at the plot above, where good predictions\nshould lie on the black dashed line.",
            "markdown"
        ],
        [
            "In the following section, we will interpret the coefficients of the model.\nWhile we do so, we should keep in mind that any conclusion we draw is\nabout the model that we build, rather than about the true (real-world)\ngenerative process of the data.",
            "markdown"
        ],
        [
            "First of all, we can take a look to the values of the coefficients of the\nregressor we have fitted.",
            "markdown"
        ],
        [
            "feature_names = model[:-1].get_feature_names_out()\n\ncoefs = (\n    model[-1].regressor_.coef_,\n    columns=[\"Coefficients\"],\n    index=feature_names,\n)\n\ncoefs\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "The AGE coefficient is expressed in \u201cdollars/hour per living years\u201d while the\nEDUCATION one is expressed in \u201cdollars/hour per years of education\u201d. This\nrepresentation of the coefficients has the benefit of making clear the\npractical predictions of the model: an increase of \\(1\\) year in AGE\nmeans a decrease of \\(0.030867\\) dollars/hour, while an increase of\n\\(1\\) year in EDUCATION means an increase of \\(0.054699\\)\ndollars/hour. On the other hand, categorical variables (as UNION or SEX) are\nadimensional numbers taking either the value 0 or 1. Their coefficients\nare expressed in dollars/hour. Then, we cannot compare the magnitude of\ndifferent coefficients since the features have different natural scales, and\nhence value ranges, because of their different unit of measure. This is more\nvisible if we plot the coefficients.",
            "markdown"
        ],
        [
            "coefs.plot.barh(figsize=(9, 7))\n(\"Ridge model, small regularization\")\n(x=0, color=\".5\")\n(\"Raw coefficient values\")\n(left=0.3)\n\n\n<img alt=\"Ridge model, small regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_003.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_003.png\"/>",
            "code"
        ],
        [
            "Indeed, from the plot above the most important factor in determining WAGE\nappears to be the\nvariable UNION, even if our intuition might tell us that variables\nlike EXPERIENCE should have more impact.",
            "markdown"
        ],
        [
            "Looking at the coefficient plot to gauge feature importance can be\nmisleading as some of them vary on a small scale, while others, like AGE,\nvaries a lot more, several decades.",
            "markdown"
        ],
        [
            "This is visible if we compare the standard deviations of different\nfeatures.",
            "markdown"
        ],
        [
            "X_train_preprocessed = (\n    model[:-1].transform(X_train), columns=feature_names\n)\n\nX_train_preprocessed.std(axis=0).plot.barh(figsize=(9, 7))\n(\"Feature ranges\")\n(\"Std. dev. of feature values\")\n(left=0.3)\n\n\n<img alt=\"Feature ranges\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_004.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_004.png\"/>",
            "code"
        ],
        [
            "Multiplying the coefficients by the standard deviation of the related\nfeature would reduce all the coefficients to the same unit of measure.\nAs we will see  this is equivalent to normalize\nnumerical variables to their standard deviation,\nas \\(y = \\sum{coef_i \\times X_i} =\n\\sum{(coef_i \\times std_i) \\times (X_i / std_i)}\\).",
            "markdown"
        ],
        [
            "In that way, we emphasize that the\ngreater the variance of a feature, the larger the weight of the corresponding\ncoefficient on the output, all else being equal.",
            "markdown"
        ],
        [
            "coefs = (\n    model[-1].regressor_.coef_ * X_train_preprocessed.std(axis=0),\n    columns=[\"Coefficient importance\"],\n    index=feature_names,\n)\ncoefs.plot(kind=\"barh\", figsize=(9, 7))\n(\"Coefficient values corrected by the feature's std. dev.\")\n(\"Ridge model, small regularization\")\n(x=0, color=\".5\")\n(left=0.3)\n\n\n<img alt=\"Ridge model, small regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_005.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_005.png\"/>",
            "code"
        ],
        [
            "Now that the coefficients have been scaled, we can safely compare them.",
            "markdown"
        ],
        [
            "Warning",
            "markdown"
        ],
        [
            "Why does the plot above suggest that an increase in age leads to a\ndecrease in wage? Why the  is telling the opposite?",
            "markdown"
        ],
        [
            "The plot above tells us about dependencies between a specific feature and\nthe target when all other features remain constant, i.e., <strong>conditional\ndependencies</strong>. An increase of the AGE will induce a decrease\nof the WAGE when all other features remain constant. On the contrary, an\nincrease of the EXPERIENCE will induce an increase of the WAGE when all\nother features remain constant.\nAlso, AGE, EXPERIENCE and EDUCATION are the three variables that most\ninfluence the model.",
            "markdown"
        ],
        [
            "We can check the coefficient variability through cross-validation:\nit is a form of data perturbation (related to\n).",
            "markdown"
        ],
        [
            "If coefficients vary significantly when changing the input dataset\ntheir robustness is not guaranteed, and they should probably be interpreted\nwith caution.",
            "markdown"
        ],
        [
            "from sklearn.model_selection import \nfrom sklearn.model_selection import \n\ncv = (n_splits=5, n_repeats=5, random_state=0)\ncv_model = (\n    model,\n    X,\n    y,\n    cv=cv,\n    return_estimator=True,\n    n_jobs=2,\n)\n\ncoefs = (\n    [\n        est[-1].regressor_.coef_ * est[:-1].transform(X.iloc[train_idx]).std(axis=0)\n        for est, (train_idx, _) in zip(cv_model[\"estimator\"], cv.split(X, y))\n    ],\n    columns=feature_names,\n)",
            "code"
        ],
        [
            "(figsize=(9, 7))\n(data=coefs, orient=\"h\", palette=\"dark:k\", alpha=0.5)\n(data=coefs, orient=\"h\", color=\"cyan\", saturation=0.5, whis=10)\n(x=0, color=\".5\")\n(\"Coefficient importance\")\n(\"Coefficient importance and its variability\")\n(\"Ridge model, small regularization\")\n(left=0.3)\n\n\n<img alt=\"Ridge model, small regularization, Coefficient importance and its variability\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_006.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_006.png\"/>",
            "code"
        ],
        [
            "The AGE and EXPERIENCE coefficients are affected by strong variability which\nmight be due to the collinearity between the 2 features: as AGE and\nEXPERIENCE vary together in the data, their effect is difficult to tease\napart.",
            "markdown"
        ],
        [
            "To verify this interpretation we plot the variability of the AGE and\nEXPERIENCE coefficient.",
            "markdown"
        ],
        [
            "(\"Age coefficient\")\n(\"Experience coefficient\")\n(True)\n(-0.4, 0.5)\n(-0.4, 0.5)\n(coefs[\"AGE\"], coefs[\"EXPERIENCE\"])\n_ = (\"Co-variations of coefficients for AGE and EXPERIENCE across folds\")\n\n\n<img alt=\"Co-variations of coefficients for AGE and EXPERIENCE across folds\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_007.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_007.png\"/>",
            "code"
        ],
        [
            "Two regions are populated: when the EXPERIENCE coefficient is\npositive the AGE one is negative and vice-versa.",
            "markdown"
        ],
        [
            "To go further we remove one of the 2 features and check what is the impact\non the model stability.",
            "markdown"
        ],
        [
            "column_to_drop = [\"AGE\"]\n\ncv_model = (\n    model,\n    X.drop(columns=column_to_drop),\n    y,\n    cv=cv,\n    return_estimator=True,\n    n_jobs=2,\n)\n\ncoefs = (\n    [\n        est[-1].regressor_.coef_\n        * est[:-1].transform(X.drop(columns=column_to_drop).iloc[train_idx]).std(axis=0)\n        for est, (train_idx, _) in zip(cv_model[\"estimator\"], cv.split(X, y))\n    ],\n    columns=feature_names[:-1],\n)",
            "code"
        ],
        [
            "(figsize=(9, 7))\n(data=coefs, orient=\"h\", palette=\"dark:k\", alpha=0.5)\n(data=coefs, orient=\"h\", color=\"cyan\", saturation=0.5)\n(x=0, color=\".5\")\n(\"Coefficient importance and its variability\")\n(\"Coefficient importance\")\n(\"Ridge model, small regularization, AGE dropped\")\n(left=0.3)\n\n\n<img alt=\"Ridge model, small regularization, AGE dropped, Coefficient importance and its variability\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_008.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_008.png\"/>",
            "code"
        ],
        [
            "The estimation of the EXPERIENCE coefficient now shows a much reduced\nvariability. EXPERIENCE remains important for all models trained during\ncross-validation.",
            "markdown"
        ],
        [
            "As said above (see \u201c\u201d), we could also choose to scale\nnumerical values before training the model.\nThis can be useful when we apply a similar amount of regularization to all of them\nin the ridge.\nThe preprocessor is redefined in order to subtract the mean and scale\nvariables to unit variance.",
            "markdown"
        ],
        [
            "from sklearn.preprocessing import \n\npreprocessor = (\n    ((drop=\"if_binary\"), categorical_columns),\n    ((), numerical_columns),\n)",
            "code"
        ],
        [
            "The model will stay unchanged.",
            "markdown"
        ],
        [
            "model = (\n    preprocessor,\n    (\n        regressor=(alpha=1e-10), func=, inverse_func=\n    ),\n)\nmodel.fit(X_train, y_train)",
            "code"
        ],
        [
            "Pipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('onehotencoder',\n                                                  OneHotEncoder(drop='if_binary'),\n                                                  ['RACE', 'OCCUPATION',\n                                                   'SECTOR', 'MARR', 'UNION',\n                                                   'SEX', 'SOUTH']),\n                                                 ('standardscaler',\n                                                  StandardScaler(),\n                                                  ['EDUCATION', 'EXPERIENCE',\n                                                   'AGE'])])),\n                ('transformedtargetregressor',\n                 TransformedTargetRegressor(func=&lt;ufunc 'log10',\n                                            inverse_func=&lt;ufunc 'exp10',\n                                            regressor=Ridge(alpha=1e-10)))])<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-127\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-127\">Pipeline</label>",
            "code"
        ],
        [
            "Pipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('onehotencoder',\n                                                  OneHotEncoder(drop='if_binary'),\n                                                  ['RACE', 'OCCUPATION',\n                                                   'SECTOR', 'MARR', 'UNION',\n                                                   'SEX', 'SOUTH']),\n                                                 ('standardscaler',\n                                                  StandardScaler(),\n                                                  ['EDUCATION', 'EXPERIENCE',\n                                                   'AGE'])])),\n                ('transformedtargetregressor',\n                 TransformedTargetRegressor(func=&lt;ufunc 'log10',\n                                            inverse_func=&lt;ufunc 'exp10',\n                                            regressor=Ridge(alpha=1e-10)))])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-128\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-128\">columntransformer: ColumnTransformer</label>",
            "code"
        ],
        [
            "ColumnTransformer(transformers=[('onehotencoder',\n                                 OneHotEncoder(drop='if_binary'),\n                                 ['RACE', 'OCCUPATION', 'SECTOR', 'MARR',\n                                  'UNION', 'SEX', 'SOUTH']),\n                                ('standardscaler', StandardScaler(),\n                                 ['EDUCATION', 'EXPERIENCE', 'AGE'])])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-129\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-129\">onehotencoder</label>",
            "code"
        ],
        [
            "['RACE', 'OCCUPATION', 'SECTOR', 'MARR', 'UNION', 'SEX', 'SOUTH']<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-130\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-130\">OneHotEncoder</label>",
            "code"
        ],
        [
            "OneHotEncoder(drop='if_binary')<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-131\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-131\">standardscaler</label>",
            "code"
        ],
        [
            "['EDUCATION', 'EXPERIENCE', 'AGE']<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-132\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-132\">StandardScaler</label>",
            "code"
        ],
        [
            "StandardScaler()<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-133\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-133\">transformedtargetregressor: TransformedTargetRegressor</label>",
            "code"
        ],
        [
            "TransformedTargetRegressor(func=&lt;ufunc 'log10', inverse_func=&lt;ufunc 'exp10',\n                           regressor=Ridge(alpha=1e-10))<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-134\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-134\">regressor: Ridge</label>",
            "code"
        ],
        [
            "Ridge(alpha=1e-10)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-135\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-135\">Ridge</label>",
            "code"
        ],
        [
            "Ridge(alpha=1e-10)\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "Again, we check the performance of the computed\nmodel using, for example, the median absolute error of the model and the R\nsquared coefficient.",
            "markdown"
        ],
        [
            "mae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}\n\n_, ax = (figsize=(5, 5))\ndisplay = (\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n)\nax.set_title(\"Ridge model, small regularization\")\nfor name, score in scores.items():\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\nax.legend(loc=\"upper left\")\n()\n\n\n<img alt=\"Ridge model, small regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_009.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_009.png\"/>",
            "code"
        ],
        [
            "For the coefficient analysis, scaling is not needed this time because it\nwas performed during the preprocessing step.",
            "markdown"
        ],
        [
            "coefs = (\n    model[-1].regressor_.coef_,\n    columns=[\"Coefficients importance\"],\n    index=feature_names,\n)\ncoefs.plot.barh(figsize=(9, 7))\n(\"Ridge model, small regularization, normalized variables\")\n(\"Raw coefficient values\")\n(x=0, color=\".5\")\n(left=0.3)\n\n\n<img alt=\"Ridge model, small regularization, normalized variables\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_010.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_010.png\"/>",
            "code"
        ],
        [
            "We now inspect the coefficients across several cross-validation folds. As in\nthe above example, we do not need to scale the coefficients by the std. dev.\nof the feature values since this scaling was already\ndone in the preprocessing step of the pipeline.",
            "markdown"
        ],
        [
            "cv_model = (\n    model,\n    X,\n    y,\n    cv=cv,\n    return_estimator=True,\n    n_jobs=2,\n)\ncoefs = (\n    [est[-1].regressor_.coef_ for est in cv_model[\"estimator\"]], columns=feature_names\n)",
            "code"
        ],
        [
            "(figsize=(9, 7))\n(data=coefs, orient=\"h\", palette=\"dark:k\", alpha=0.5)\n(data=coefs, orient=\"h\", color=\"cyan\", saturation=0.5, whis=10)\n(x=0, color=\".5\")\n(\"Coefficient variability\")\n(left=0.3)\n\n\n<img alt=\"Coefficient variability\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_011.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_011.png\"/>",
            "code"
        ],
        [
            "The result is quite similar to the non-normalized case.",
            "markdown"
        ],
        [
            "In machine-learning practice, ridge regression is more often used with\nnon-negligible regularization.",
            "markdown"
        ],
        [
            "Above, we limited this regularization to a very little amount. Regularization\nimproves the conditioning of the problem and reduces the variance of the\nestimates.  applies cross validation\nin order to determine which value of the regularization parameter (alpha)\nis best suited for prediction.",
            "markdown"
        ],
        [
            "from sklearn.linear_model import \n\nalphas = (-10, 10, 21)  # alpha values to be chosen from by cross-validation\nmodel = (\n    preprocessor,\n    (\n        regressor=(alphas=alphas),\n        func=,\n        inverse_func=,\n    ),\n)\nmodel.fit(X_train, y_train)",
            "code"
        ],
        [
            "Pipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('onehotencoder',\n                                                  OneHotEncoder(drop='if_binary'),\n                                                  ['RACE', 'OCCUPATION',\n                                                   'SECTOR', 'MARR', 'UNION',\n                                                   'SEX', 'SOUTH']),\n                                                 ('standardscaler',\n                                                  StandardScaler(),\n                                                  ['EDUCATION', 'EXPERIENCE',\n                                                   'AGE'])])),\n                ('transformedtargetregressor',\n                 TransformedTargetRegressor(func=&lt;ufunc 'log10',\n                                            inverse_func=&lt;ufunc 'exp10',\n                                            regressor=RidgeCV(alphas=array([1.e-10, 1.e-09, 1.e-08, 1.e-07, 1.e-06, 1.e-05, 1.e-04, 1.e-03,\n       1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03, 1.e+04, 1.e+05,\n       1.e+06, 1.e+07, 1.e+08, 1.e+09, 1.e+10]))))])<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-136\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-136\">Pipeline</label>",
            "code"
        ],
        [
            "Pipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('onehotencoder',\n                                                  OneHotEncoder(drop='if_binary'),\n                                                  ['RACE', 'OCCUPATION',\n                                                   'SECTOR', 'MARR', 'UNION',\n                                                   'SEX', 'SOUTH']),\n                                                 ('standardscaler',\n                                                  StandardScaler(),\n                                                  ['EDUCATION', 'EXPERIENCE',\n                                                   'AGE'])])),\n                ('transformedtargetregressor',\n                 TransformedTargetRegressor(func=&lt;ufunc 'log10',\n                                            inverse_func=&lt;ufunc 'exp10',\n                                            regressor=RidgeCV(alphas=array([1.e-10, 1.e-09, 1.e-08, 1.e-07, 1.e-06, 1.e-05, 1.e-04, 1.e-03,\n       1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03, 1.e+04, 1.e+05,\n       1.e+06, 1.e+07, 1.e+08, 1.e+09, 1.e+10]))))])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-137\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-137\">columntransformer: ColumnTransformer</label>",
            "code"
        ],
        [
            "ColumnTransformer(transformers=[('onehotencoder',\n                                 OneHotEncoder(drop='if_binary'),\n                                 ['RACE', 'OCCUPATION', 'SECTOR', 'MARR',\n                                  'UNION', 'SEX', 'SOUTH']),\n                                ('standardscaler', StandardScaler(),\n                                 ['EDUCATION', 'EXPERIENCE', 'AGE'])])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-138\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-138\">onehotencoder</label>",
            "code"
        ],
        [
            "['RACE', 'OCCUPATION', 'SECTOR', 'MARR', 'UNION', 'SEX', 'SOUTH']<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-139\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-139\">OneHotEncoder</label>",
            "code"
        ],
        [
            "OneHotEncoder(drop='if_binary')<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-140\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-140\">standardscaler</label>",
            "code"
        ],
        [
            "['EDUCATION', 'EXPERIENCE', 'AGE']<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-141\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-141\">StandardScaler</label>",
            "code"
        ],
        [
            "StandardScaler()<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-142\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-142\">transformedtargetregressor: TransformedTargetRegressor</label>",
            "code"
        ],
        [
            "TransformedTargetRegressor(func=&lt;ufunc 'log10', inverse_func=&lt;ufunc 'exp10',\n                           regressor=RidgeCV(alphas=array([1.e-10, 1.e-09, 1.e-08, 1.e-07, 1.e-06, 1.e-05, 1.e-04, 1.e-03,\n       1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03, 1.e+04, 1.e+05,\n       1.e+06, 1.e+07, 1.e+08, 1.e+09, 1.e+10])))<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-143\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-143\">regressor: RidgeCV</label>",
            "code"
        ],
        [
            "RidgeCV(alphas=array([1.e-10, 1.e-09, 1.e-08, 1.e-07, 1.e-06, 1.e-05, 1.e-04, 1.e-03,\n       1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03, 1.e+04, 1.e+05,\n       1.e+06, 1.e+07, 1.e+08, 1.e+09, 1.e+10]))<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-144\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-144\">RidgeCV</label>",
            "code"
        ],
        [
            "RidgeCV(alphas=array([1.e-10, 1.e-09, 1.e-08, 1.e-07, 1.e-06, 1.e-05, 1.e-04, 1.e-03,\n       1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03, 1.e+04, 1.e+05,\n       1.e+06, 1.e+07, 1.e+08, 1.e+09, 1.e+10]))\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "First we check which value of \\(\\alpha\\) has been selected.",
            "markdown"
        ],
        [
            "model[-1].regressor_.alpha_",
            "code"
        ],
        [
            "10.0",
            "code"
        ],
        [
            "Then we check the quality of the predictions.",
            "markdown"
        ],
        [
            "mae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}\n\n_, ax = (figsize=(5, 5))\ndisplay = (\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n)\nax.set_title(\"Ridge model, optimum regularization\")\nfor name, score in scores.items():\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\nax.legend(loc=\"upper left\")\n()\n\n\n<img alt=\"Ridge model, optimum regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_012.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_012.png\"/>",
            "code"
        ],
        [
            "The ability to reproduce the data of the regularized model is similar to\nthe one of the non-regularized model.",
            "markdown"
        ],
        [
            "coefs = (\n    model[-1].regressor_.coef_,\n    columns=[\"Coefficients importance\"],\n    index=feature_names,\n)\ncoefs.plot.barh(figsize=(9, 7))\n(\"Ridge model, with regularization, normalized variables\")\n(\"Raw coefficient values\")\n(x=0, color=\".5\")\n(left=0.3)\n\n\n<img alt=\"Ridge model, with regularization, normalized variables\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_013.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_013.png\"/>",
            "code"
        ],
        [
            "The coefficients are significantly different.\nAGE and EXPERIENCE coefficients are both positive but they now have less\ninfluence on the prediction.",
            "markdown"
        ],
        [
            "The regularization reduces the influence of correlated\nvariables on the model because the weight is shared between the two\npredictive variables, so neither alone would have strong weights.",
            "markdown"
        ],
        [
            "On the other hand, the weights obtained with regularization are more\nstable (see the  User Guide section). This\nincreased stability is visible from the plot, obtained from data\nperturbations, in a cross-validation. This plot can be compared with\nthe .",
            "markdown"
        ],
        [
            "cv_model = (\n    model,\n    X,\n    y,\n    cv=cv,\n    return_estimator=True,\n    n_jobs=2,\n)\ncoefs = (\n    [est[-1].regressor_.coef_ for est in cv_model[\"estimator\"]], columns=feature_names\n)",
            "code"
        ],
        [
            "(\"Age coefficient\")\n(\"Experience coefficient\")\n(True)\n(-0.4, 0.5)\n(-0.4, 0.5)\n(coefs[\"AGE\"], coefs[\"EXPERIENCE\"])\n_ = (\"Co-variations of coefficients for AGE and EXPERIENCE across folds\")\n\n\n<img alt=\"Co-variations of coefficients for AGE and EXPERIENCE across folds\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_014.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_014.png\"/>",
            "code"
        ],
        [
            "Another possibility to take into account correlated variables in the dataset,\nis to estimate sparse coefficients. In some way we already did it manually\nwhen we dropped the AGE column in a previous ridge estimation.",
            "markdown"
        ],
        [
            "Lasso models (see the  User Guide section) estimates sparse\ncoefficients.  applies cross\nvalidation in order to determine which value of the regularization parameter\n(alpha) is best suited for the model estimation.",
            "markdown"
        ],
        [
            "from sklearn.linear_model import \n\nalphas = (-10, 10, 21)  # alpha values to be chosen from by cross-validation\nmodel = (\n    preprocessor,\n    (\n        regressor=(alphas=alphas, max_iter=100_000),\n        func=,\n        inverse_func=,\n    ),\n)\n\n_ = model.fit(X_train, y_train)",
            "code"
        ],
        [
            "First we verify which value of \\(\\alpha\\) has been selected.",
            "markdown"
        ],
        [
            "model[-1].regressor_.alpha_",
            "code"
        ],
        [
            "0.001",
            "code"
        ],
        [
            "Then we check the quality of the predictions.",
            "markdown"
        ],
        [
            "mae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}\n\n_, ax = (figsize=(6, 6))\ndisplay = (\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n)\nax.set_title(\"Lasso model, optimum regularization\")\nfor name, score in scores.items():\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\nax.legend(loc=\"upper left\")\n()\n\n\n<img alt=\"Lasso model, optimum regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_015.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_015.png\"/>",
            "code"
        ],
        [
            "For our dataset, again the model is not very predictive.",
            "markdown"
        ],
        [
            "coefs = (\n    model[-1].regressor_.coef_,\n    columns=[\"Coefficients importance\"],\n    index=feature_names,\n)\ncoefs.plot(kind=\"barh\", figsize=(9, 7))\n(\"Lasso model, optimum regularization, normalized variables\")\n(x=0, color=\".5\")\n(left=0.3)\n\n\n<img alt=\"Lasso model, optimum regularization, normalized variables\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_016.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_016.png\"/>",
            "code"
        ],
        [
            "A Lasso model identifies the correlation between\nAGE and EXPERIENCE and suppresses one of them for the sake of the prediction.",
            "markdown"
        ],
        [
            "It is important to keep in mind that the coefficients that have been\ndropped may still be related to the outcome by themselves: the model\nchose to suppress them because they bring little or no additional\ninformation on top of the other features. Additionally, this selection\nis unstable for correlated features, and should be interpreted with\ncaution.",
            "markdown"
        ],
        [
            "Indeed, we can check the variability of the coefficients across folds.",
            "markdown"
        ],
        [
            "cv_model = (\n    model,\n    X,\n    y,\n    cv=cv,\n    return_estimator=True,\n    n_jobs=2,\n)\ncoefs = (\n    [est[-1].regressor_.coef_ for est in cv_model[\"estimator\"]], columns=feature_names\n)",
            "code"
        ],
        [
            "(figsize=(9, 7))\n(data=coefs, orient=\"h\", palette=\"dark:k\", alpha=0.5)\n(data=coefs, orient=\"h\", color=\"cyan\", saturation=0.5, whis=100)\n(x=0, color=\".5\")\n(\"Coefficient variability\")\n(left=0.3)\n\n\n<img alt=\"Coefficient variability\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_017.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_017.png\"/>",
            "code"
        ],
        [
            "We observe that the AGE and EXPERIENCE coefficients are varying a lot\ndepending of the fold.",
            "markdown"
        ],
        [
            "Policy makers might want to know the effect of education on wage to assess\nwhether or not a certain policy designed to entice people to pursue more\neducation would make economic sense. While Machine Learning models are great\nfor measuring statistical associations, they are generally unable to infer\ncausal effects.",
            "markdown"
        ],
        [
            "It might be tempting to look at the coefficient of education on wage from our\nlast model (or any model for that matter) and conclude that it captures the\ntrue effect of a change in the standardized education variable on wages.",
            "markdown"
        ],
        [
            "Unfortunately there are likely unobserved confounding variables that either\ninflate or deflate that coefficient. A confounding variable is a variable that\ncauses both EDUCATION and WAGE. One example of such variable is ability.\nPresumably, more able people are more likely to pursue education while at the\nsame time being more likely to earn a higher hourly wage at any level of\neducation. In this case, ability induces a positive  (OVB) on the EDUCATION\ncoefficient, thereby exaggerating the effect of education on wages.",
            "markdown"
        ],
        [
            "See the \nfor a simulated case of ability OVB.",
            "markdown"
        ],
        [
            "Coefficients must be scaled to the same unit of measure to retrieve\nfeature importance. Scaling them with the standard-deviation of the\nfeature is a useful proxy.",
            "markdown"
        ],
        [
            "Coefficients in multivariate linear models represent the dependency\nbetween a given feature and the target, <strong>conditional</strong> on the other\nfeatures.",
            "markdown"
        ],
        [
            "Correlated features induce instabilities in the coefficients of linear\nmodels and their effects cannot be well teased apart.",
            "markdown"
        ],
        [
            "Different linear models respond differently to feature correlation and\ncoefficients could significantly vary from one another.",
            "markdown"
        ],
        [
            "Inspecting coefficients across the folds of a cross-validation loop\ngives an idea of their stability.",
            "markdown"
        ],
        [
            "Coefficients are unlikely to have any causal meaning. They tend\nto be biased by unobserved confounders.",
            "markdown"
        ],
        [
            "Inspection tools may not necessarily provide insights on the true\ndata generating process.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  16.267 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Inspection->Failure of Machine Learning to infer causal effects": [
        [
            "Machine Learning models are great for measuring statistical associations.\nUnfortunately, unless we\u2019re willing to make strong assumptions about the data,\nthose models are unable to infer causal effects.",
            "markdown"
        ],
        [
            "To illustrate this, we will simulate a situation in which we try to answer one\nof the most important questions in economics of education: <strong>what is the causal\neffect of earning a college degree on hourly wages?</strong> Although the answer to\nthis question is crucial to policy makers,  (OVB) prevent us from\nidentifying that causal effect.",
            "markdown"
        ]
    ],
    "Examples->Inspection->Failure of Machine Learning to infer causal effects->The dataset: simulated hourly wages": [
        [
            "The data generating process is laid out in the code below. Work experience in\nyears and a measure of ability are drawn from Normal distributions; the\nhourly wage of one of the parents is drawn from Beta distribution. We then\ncreate an indicator of college degree which is positively impacted by ability\nand parental hourly wage. Finally, we model hourly wages as a linear function\nof all the previous variables and a random component. Note that all variables\nhave a positive effect on hourly wages.",
            "markdown"
        ],
        [
            "import numpy as np\nimport pandas as pd\n\nn_samples = 10_000\nrng = (32)\n\nexperiences = rng.normal(20, 10, size=n_samples).astype(int)\nexperiences[experiences &lt; 0] = 0\nabilities = rng.normal(0, 0.15, size=n_samples)\nparent_hourly_wages = 50 * rng.beta(2, 8, size=n_samples)\nparent_hourly_wages[parent_hourly_wages &lt; 0] = 0\ncollege_degrees = (\n    9 * abilities + 0.02 * parent_hourly_wages + rng.randn(n_samples)  0.7\n).astype(int)\n\ntrue_coef = (\n    {\n        \"college degree\": 2.0,\n        \"ability\": 5.0,\n        \"experience\": 0.2,\n        \"parent hourly wage\": 1.0,\n    }\n)\nhourly_wages = (\n    true_coef[\"experience\"] * experiences\n    + true_coef[\"parent hourly wage\"] * parent_hourly_wages\n    + true_coef[\"college degree\"] * college_degrees\n    + true_coef[\"ability\"] * abilities\n    + rng.normal(0, 1, size=n_samples)\n)\n\nhourly_wages[hourly_wages &lt; 0] = 0",
            "code"
        ]
    ],
    "Examples->Inspection->Failure of Machine Learning to infer causal effects->Description of the simulated data": [
        [
            "The following plot shows the distribution of each variable, and pairwise\nscatter plots. Key to our OVB story is the positive relationship between\nability and college degree.",
            "markdown"
        ],
        [
            "import seaborn as sns\n\ndf = (\n    {\n        \"college degree\": college_degrees,\n        \"ability\": abilities,\n        \"hourly wage\": hourly_wages,\n        \"experience\": experiences,\n        \"parent hourly wage\": parent_hourly_wages,\n    }\n)\n\ngrid = (df, diag_kind=\"kde\", corner=True)\n\n\n<img alt=\"plot causal interpretation\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_causal_interpretation_001.png\" srcset=\"../../_images/sphx_glr_plot_causal_interpretation_001.png\"/>",
            "code"
        ],
        [
            "In the next section, we train predictive models and we therefore split the\ntarget column from over features and we split the data into a training and a\ntesting set.",
            "markdown"
        ],
        [
            "from sklearn.model_selection import \n\ntarget_name = \"hourly wage\"\nX, y = df.drop(columns=target_name), df[target_name]\nX_train, X_test, y_train, y_test = (X, y, test_size=0.2, random_state=0)",
            "code"
        ]
    ],
    "Examples->Inspection->Failure of Machine Learning to infer causal effects->Income prediction with fully observed variables": [
        [
            "First, we train a predictive model, a\n model. In this experiment,\nwe assume that all variables used by the true generative model are available.",
            "markdown"
        ],
        [
            "from sklearn.linear_model import \nfrom sklearn.metrics import \n\nfeatures_names = [\"experience\", \"parent hourly wage\", \"college degree\", \"ability\"]\n\nregressor_with_ability = ()\nregressor_with_ability.fit(X_train[features_names], y_train)\ny_pred_with_ability = regressor_with_ability.predict(X_test[features_names])\nR2_with_ability = (y_test, y_pred_with_ability)\n\nprint(f\"R2 score with ability: {R2_with_ability:.3f}\")",
            "code"
        ],
        [
            "R2 score with ability: 0.975",
            "code"
        ],
        [
            "This model predicts well the hourly wages as shown by the high R2 score. We\nplot the model coefficients to show that we exactly recover the values of\nthe true generative model.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n\nmodel_coef = (regressor_with_ability.coef_, index=features_names)\ncoef = (\n    [true_coef[features_names], model_coef],\n    keys=[\"Coefficients of true generative model\", \"Model coefficients\"],\n    axis=1,\n)\nax = coef.plot.barh()\nax.set_xlabel(\"Coefficient values\")\nax.set_title(\"Coefficients of the linear regression including the ability features\")\n_ = ()\n\n\n<img alt=\"Coefficients of the linear regression including the ability features\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_causal_interpretation_002.png\" srcset=\"../../_images/sphx_glr_plot_causal_interpretation_002.png\"/>",
            "code"
        ]
    ],
    "Examples->Inspection->Failure of Machine Learning to infer causal effects->Income prediction with partial observations": [
        [
            "In practice, intellectual abilities are not observed or are only estimated\nfrom proxies that inadvertently measure education as well (e.g. by IQ tests).\nBut omitting the \u201cability\u201d feature from a linear model inflates the estimate\nvia a positive OVB.",
            "markdown"
        ],
        [
            "features_names = [\"experience\", \"parent hourly wage\", \"college degree\"]\n\nregressor_without_ability = ()\nregressor_without_ability.fit(X_train[features_names], y_train)\ny_pred_without_ability = regressor_without_ability.predict(X_test[features_names])\nR2_without_ability = (y_test, y_pred_without_ability)\n\nprint(f\"R2 score without ability: {R2_without_ability:.3f}\")",
            "code"
        ],
        [
            "R2 score without ability: 0.968",
            "code"
        ],
        [
            "The predictive power of our model is similar when we omit the ability feature\nin terms of R2 score. We now check if the coefficient of the model are\ndifferent from the true generative model.",
            "markdown"
        ],
        [
            "model_coef = (regressor_without_ability.coef_, index=features_names)\ncoef = (\n    [true_coef[features_names], model_coef],\n    keys=[\"Coefficients of true generative model\", \"Model coefficients\"],\n    axis=1,\n)\nax = coef.plot.barh()\nax.set_xlabel(\"Coefficient values\")\n_ = ax.set_title(\"Coefficients of the linear regression excluding the ability feature\")\n()\n()\n\n\n<img alt=\"Coefficients of the linear regression excluding the ability feature\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_causal_interpretation_003.png\" srcset=\"../../_images/sphx_glr_plot_causal_interpretation_003.png\"/>",
            "code"
        ],
        [
            "To compensate for the omitted variable, the model inflates the coefficient of\nthe college degree feature. Therefore, interpreting this coefficient value\nas a causal effect of the true generative model is incorrect.",
            "markdown"
        ]
    ],
    "Examples->Inspection->Failure of Machine Learning to infer causal effects->Lessons learned": [
        [
            "Machine learning models are not designed for the estimation of causal\neffects. While we showed this with a linear model, OVB can affect any type of\nmodel.",
            "markdown"
        ],
        [
            "Whenever interpreting a coefficient or a change in predictions brought about\nby a change in one of the features, it is important to keep in mind\npotentially unobserved variables that could be correlated with both the\nfeature in question and the target variable. Such variables are called\n. In\norder to still estimate causal effect in the presence of confounding,\nresearchers usually conduct experiments in which the treatment variable (e.g.\ncollege degree) is randomized. When an experiment is prohibitively expensive\nor unethical, researchers can sometimes use other causal inference techniques\nsuch as  (IV)\nestimations.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  1.994 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Inspection->Partial Dependence and Individual Conditional Expectation Plots": [
        [
            "Partial dependence plots show the dependence between the target function \nand a set of features of interest, marginalizing over the values of all other\nfeatures (the complement features). Due to the limits of human perception, the\nsize of the set of features of interest must be small (usually, one or two)\nthus they are usually chosen among the most important features.",
            "markdown"
        ],
        [
            "Similarly, an individual conditional expectation (ICE) plot \nshows the dependence between the target function and a feature of interest.\nHowever, unlike partial dependence plots, which show the average effect of the\nfeatures of interest, ICE plots visualize the dependence of the prediction on a\nfeature for each  separately, with one line per sample.\nOnly one feature of interest is supported for ICE plots.",
            "markdown"
        ],
        [
            "This example shows how to obtain partial dependence and ICE plots from a\n and a\n trained on the\nbike sharing dataset. The example is inspired by .\n\n\n[]",
            "markdown"
        ],
        [
            "[]",
            "markdown"
        ],
        [
            "For classification you can think of it as the regression score before\nthe link function.\n\n\n[]",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Inspection->Partial Dependence and Individual Conditional Expectation Plots->Bike sharing dataset preprocessing": [
        [
            "We will use the bike sharing dataset. The goal is to predict the number of bike\nrentals using weather and season data as well as the datetime information.",
            "markdown"
        ],
        [
            "from sklearn.datasets import \n\nbikes = (\"Bike_Sharing_Demand\", version=2, as_frame=True, parser=\"pandas\")\n# Make an explicit copy to avoid \"SettingWithCopyWarning\" from pandas\nX, y = bikes.data.copy(), bikes.target",
            "code"
        ],
        [
            "The feature \"weather\" has a particularity: the category \"heavy_rain\" is a rare\ncategory.",
            "markdown"
        ],
        [
            "X[\"weather\"].value_counts()",
            "code"
        ],
        [
            "clear         11413\nmisty          4544\nrain           1419\nheavy_rain        3\nName: weather, dtype: int64",
            "code"
        ],
        [
            "Because of this rare category, we collapse it into \"rain\".",
            "markdown"
        ],
        [
            "X[\"weather\"].replace(to_replace=\"heavy_rain\", value=\"rain\", inplace=True)",
            "code"
        ],
        [
            "We now have a closer look at the \"year\" feature:",
            "markdown"
        ],
        [
            "X[\"year\"].value_counts()",
            "code"
        ],
        [
            "1    8734\n0    8645\nName: year, dtype: int64",
            "code"
        ],
        [
            "We see that we have data from two years. We use the first year to train the\nmodel and the second year to test the model.",
            "markdown"
        ],
        [
            "mask_training = X[\"year\"] == 0.0\nX = X.drop(columns=[\"year\"])\nX_train, y_train = X[mask_training], y[mask_training]\nX_test, y_test = X[~mask_training], y[~mask_training]",
            "code"
        ],
        [
            "We can check the dataset information to see that we have heterogeneous data types. We\nhave to preprocess the different columns accordingly.",
            "markdown"
        ],
        [
            "X_train.info()",
            "code"
        ],
        [
            "&lt;class 'pandas.core.frame.DataFrame'\nInt64Index: 8645 entries, 0 to 8644\nData columns (total 11 columns):\n #   Column      Non-Null Count  Dtype\n---  ------      --------------  -----\n 0   season      8645 non-null   category\n 1   month       8645 non-null   int64\n 2   hour        8645 non-null   int64\n 3   holiday     8645 non-null   category\n 4   weekday     8645 non-null   int64\n 5   workingday  8645 non-null   category\n 6   weather     8645 non-null   category\n 7   temp        8645 non-null   float64\n 8   feel_temp   8645 non-null   float64\n 9   humidity    8645 non-null   float64\n 10  windspeed   8645 non-null   float64\ndtypes: category(4), float64(4), int64(3)\nmemory usage: 574.7 KB",
            "code"
        ],
        [
            "From the previous information, we will consider the category columns as nominal\ncategorical features. In addition, we will consider the date and time information as\ncategorical features as well.",
            "markdown"
        ],
        [
            "We manually define the columns containing numerical and categorical\nfeatures.",
            "markdown"
        ],
        [
            "numerical_features = [\n    \"temp\",\n    \"feel_temp\",\n    \"humidity\",\n    \"windspeed\",\n]\ncategorical_features = X_train.columns.drop(numerical_features)",
            "code"
        ],
        [
            "Before we go into the details regarding the preprocessing of the different machine\nlearning pipelines, we will try to get some additional intuition regarding the dataset\nthat will be helpful to understand the model\u2019s statistical performance and results of\nthe partial dependence analysis.",
            "markdown"
        ],
        [
            "We plot the average number of bike rentals by grouping the data by season and\nby year.",
            "markdown"
        ],
        [
            "from itertools import \nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndays = (\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\")\nhours = tuple(range(24))\nxticklabels = [f\"{day}\\n{hour}:00\" for day, hour in (days, hours)]\nxtick_start, xtick_period = 6, 12\n\nfig, axs = (nrows=2, figsize=(8, 6), sharey=True, sharex=True)\naverage_bike_rentals = bikes.frame.groupby([\"year\", \"season\", \"weekday\", \"hour\"]).mean(\n    numeric_only=True\n)[\"count\"]\nfor ax, (idx, df) in zip(axs, average_bike_rentals.groupby(\"year\")):\n    df.groupby(\"season\").plot(ax=ax, legend=True)\n\n    # decorate the plot\n    ax.set_xticks(\n        (\n            start=xtick_start,\n            stop=len(xticklabels),\n            num=len(xticklabels) // xtick_period,\n        )\n    )\n    ax.set_xticklabels(xticklabels[xtick_start::xtick_period])\n    ax.set_xlabel(\"\")\n    ax.set_ylabel(\"Average number of bike rentals\")\n    ax.set_title(\n        f\"Bike rental for {'2010 (train set)' if idx == 0.0 else '2011 (test set)'}\"\n    )\n    ax.set_ylim(0, 1_000)\n    ax.set_xlim(0, len(xticklabels))\n    ax.legend(loc=2)\n\n\n<img alt=\"Bike rental for 2010 (train set), Bike rental for 2011 (test set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_partial_dependence_001.png\" srcset=\"../../_images/sphx_glr_plot_partial_dependence_001.png\"/>",
            "code"
        ],
        [
            "The first striking difference between the train and test set is that the number of\nbike rentals is higher in the test set. For this reason, it will not be surprising to\nget a machine learning model that underestimates the number of bike rentals. We\nalso observe that the number of bike rentals is lower during the spring season. In\naddition, we see that during working days, there is a specific pattern around 6-7\nam and 5-6 pm with some peaks of bike rentals. We can keep in mind these different\ninsights and use them to understand the partial dependence plot.",
            "markdown"
        ]
    ],
    "Examples->Inspection->Partial Dependence and Individual Conditional Expectation Plots->Preprocessor for machine-learning models": [
        [
            "Since we later use two different models, a\n and a\n, we create two different\npreprocessors, specific for each model.",
            "markdown"
        ]
    ],
    "Examples->Inspection->Partial Dependence and Individual Conditional Expectation Plots->Preprocessor for machine-learning models->Preprocessor for the neural network model": [
        [
            "We will use a  to scale the\nnumerical features and encode the categorical features with a\n.",
            "markdown"
        ],
        [
            "from sklearn.compose import \nfrom sklearn.preprocessing import \nfrom sklearn.preprocessing import \n\nmlp_preprocessor = (\n    transformers=[\n        (\"num\", (n_quantiles=100), numerical_features),\n        (\"cat\", (handle_unknown=\"ignore\"), categorical_features),\n    ]\n)\nmlp_preprocessor",
            "code"
        ],
        [
            "ColumnTransformer(transformers=[('num', QuantileTransformer(n_quantiles=100),\n                                 ['temp', 'feel_temp', 'humidity',\n                                  'windspeed']),\n                                ('cat', OneHotEncoder(handle_unknown='ignore'),\n                                 Index(['season', 'month', 'hour', 'holiday', 'weekday', 'workingday',\n       'weather'],\n      dtype='object'))])<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-145\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-145\">ColumnTransformer</label>",
            "code"
        ],
        [
            "ColumnTransformer(transformers=[('num', QuantileTransformer(n_quantiles=100),\n                                 ['temp', 'feel_temp', 'humidity',\n                                  'windspeed']),\n                                ('cat', OneHotEncoder(handle_unknown='ignore'),\n                                 Index(['season', 'month', 'hour', 'holiday', 'weekday', 'workingday',\n       'weather'],\n      dtype='object'))])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-146\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-146\">num</label>",
            "code"
        ],
        [
            "['temp', 'feel_temp', 'humidity', 'windspeed']<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-147\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-147\">QuantileTransformer</label>",
            "code"
        ],
        [
            "QuantileTransformer(n_quantiles=100)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-148\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-148\">cat</label>",
            "code"
        ],
        [
            "Index(['season', 'month', 'hour', 'holiday', 'weekday', 'workingday',\n       'weather'],\n      dtype='object')<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-149\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-149\">OneHotEncoder</label>",
            "code"
        ],
        [
            "OneHotEncoder(handle_unknown='ignore')\n\n<br/>\n<br/>",
            "code"
        ]
    ],
    "Examples->Inspection->Partial Dependence and Individual Conditional Expectation Plots->Preprocessor for machine-learning models->Preprocessor for the gradient boosting model": [
        [
            "For the gradient boosting model, we leave the numerical features as-is and only\nencode the categorical features using a\n.",
            "markdown"
        ],
        [
            "from sklearn.preprocessing import \n\nhgbdt_preprocessor = (\n    transformers=[\n        (\"cat\", (), categorical_features),\n        (\"num\", \"passthrough\", numerical_features),\n    ],\n    sparse_threshold=1,\n    verbose_feature_names_out=False,\n).set_output(transform=\"pandas\")\nhgbdt_preprocessor",
            "code"
        ],
        [
            "ColumnTransformer(sparse_threshold=1,\n                  transformers=[('cat', OrdinalEncoder(),\n                                 Index(['season', 'month', 'hour', 'holiday', 'weekday', 'workingday',\n       'weather'],\n      dtype='object')),\n                                ('num', 'passthrough',\n                                 ['temp', 'feel_temp', 'humidity',\n                                  'windspeed'])],\n                  verbose_feature_names_out=False)<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-150\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-150\">ColumnTransformer</label>",
            "code"
        ],
        [
            "ColumnTransformer(sparse_threshold=1,\n                  transformers=[('cat', OrdinalEncoder(),\n                                 Index(['season', 'month', 'hour', 'holiday', 'weekday', 'workingday',\n       'weather'],\n      dtype='object')),\n                                ('num', 'passthrough',\n                                 ['temp', 'feel_temp', 'humidity',\n                                  'windspeed'])],\n                  verbose_feature_names_out=False)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-151\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-151\">cat</label>",
            "code"
        ],
        [
            "Index(['season', 'month', 'hour', 'holiday', 'weekday', 'workingday',\n       'weather'],\n      dtype='object')<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-152\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-152\">OrdinalEncoder</label>",
            "code"
        ],
        [
            "OrdinalEncoder()<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-153\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-153\">num</label>",
            "code"
        ],
        [
            "['temp', 'feel_temp', 'humidity', 'windspeed']<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-154\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-154\">passthrough</label>",
            "code"
        ],
        [
            "passthrough\n\n<br/>\n<br/>",
            "code"
        ]
    ],
    "Examples->Inspection->Partial Dependence and Individual Conditional Expectation Plots->1-way partial dependence with different models": [
        [
            "In this section, we will compute 1-way partial dependence with two different\nmachine-learning models: (i) a multi-layer perceptron and (ii) a\ngradient-boosting model. With these two models, we illustrate how to compute and\ninterpret both partial dependence plot (PDP) for both numerical and categorical\nfeatures and individual conditional expectation (ICE).",
            "markdown"
        ],
        [
            "Let\u2019s fit a  and compute\nsingle-variable partial dependence plots.",
            "markdown"
        ],
        [
            "from time import \nfrom sklearn.neural_network import \nfrom sklearn.pipeline import \n\nprint(\"Training MLPRegressor...\")\ntic = ()\nmlp_model = (\n    mlp_preprocessor,\n    (\n        hidden_layer_sizes=(30, 15),\n        learning_rate_init=0.01,\n        early_stopping=True,\n        random_state=0,\n    ),\n)\nmlp_model.fit(X_train, y_train)\nprint(f\"done in {() - tic:.3f}s\")\nprint(f\"Test R2 score: {mlp_model.score(X_test, y_test):.2f}\")",
            "code"
        ],
        [
            "Training MLPRegressor...\ndone in 1.544s\nTest R2 score: 0.61",
            "code"
        ],
        [
            "We configured a pipeline using the preprocessor that we created specifically for the\nneural network and tuned the neural network size and learning rate to get a reasonable\ncompromise between training time and predictive performance on a test set.",
            "markdown"
        ],
        [
            "Importantly, this tabular dataset has very different dynamic ranges for its\nfeatures. Neural networks tend to be very sensitive to features with varying\nscales and forgetting to preprocess the numeric feature would lead to a very\npoor model.",
            "markdown"
        ],
        [
            "It would be possible to get even higher predictive performance with a larger\nneural network but the training would also be significantly more expensive.",
            "markdown"
        ],
        [
            "Note that it is important to check that the model is accurate enough on a\ntest set before plotting the partial dependence since there would be little\nuse in explaining the impact of a given feature on the prediction function of\na model with poor predictive performance. In this regard, our MLP model works\nreasonably well.",
            "markdown"
        ],
        [
            "We will plot the averaged partial dependence.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\nfrom sklearn.inspection import PartialDependenceDisplay\n\ncommon_params = {\n    \"subsample\": 50,\n    \"n_jobs\": 2,\n    \"grid_resolution\": 20,\n    \"random_state\": 0,\n}\n\nprint(\"Computing partial dependence plots...\")\nfeatures_info = {\n    # features of interest\n    \"features\": [\"temp\", \"humidity\", \"windspeed\", \"season\", \"weather\", \"hour\"],\n    # type of partial dependence plot\n    \"kind\": \"average\",\n    # information regarding categorical features\n    \"categorical_features\": categorical_features,\n}\ntic = ()\n_, ax = (ncols=3, nrows=2, figsize=(9, 8), constrained_layout=True)\ndisplay = (\n    mlp_model,\n    X_train,\n    **features_info,\n    ax=ax,\n    **common_params,\n)\nprint(f\"done in {() - tic:.3f}s\")\n_ = display.figure_.suptitle(\n    \"Partial dependence of the number of bike rentals\\n\"\n    \"for the bike rental dataset with an MLPRegressor\",\n    fontsize=16,\n)\n\n\n<img alt=\"Partial dependence of the number of bike rentals for the bike rental dataset with an MLPRegressor\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_partial_dependence_002.png\" srcset=\"../../_images/sphx_glr_plot_partial_dependence_002.png\"/>",
            "code"
        ],
        [
            "Computing partial dependence plots...\ndone in 1.951s",
            "code"
        ],
        [
            "Let\u2019s now fit a  and\ncompute the partial dependence on the same features. We also use the\nspecific preprocessor we created for this model.",
            "markdown"
        ],
        [
            "from sklearn.ensemble import \n\nprint(\"Training HistGradientBoostingRegressor...\")\ntic = ()\nhgbdt_model = (\n    hgbdt_preprocessor,\n    (\n        categorical_features=categorical_features, random_state=0\n    ),\n)\nhgbdt_model.fit(X_train, y_train)\nprint(f\"done in {() - tic:.3f}s\")\nprint(f\"Test R2 score: {hgbdt_model.score(X_test, y_test):.2f}\")",
            "code"
        ],
        [
            "Training HistGradientBoostingRegressor...\ndone in 0.303s\nTest R2 score: 0.64",
            "code"
        ],
        [
            "Here, we used the default hyperparameters for the gradient boosting model\nwithout any preprocessing as tree-based models are naturally robust to\nmonotonic transformations of numerical features.",
            "markdown"
        ],
        [
            "Note that on this tabular dataset, Gradient Boosting Machines are both\nsignificantly faster to train and more accurate than neural networks. It is\nalso significantly cheaper to tune their hyperparameters (the defaults tend\nto work well while this is not often the case for neural networks).",
            "markdown"
        ],
        [
            "We will plot the partial dependence for some of the numerical and categorical\nfeatures.",
            "markdown"
        ],
        [
            "print(\"Computing partial dependence plots...\")\ntic = ()\n_, ax = (ncols=3, nrows=2, figsize=(9, 8), constrained_layout=True)\ndisplay = (\n    hgbdt_model,\n    X_train,\n    **features_info,\n    ax=ax,\n    **common_params,\n)\nprint(f\"done in {() - tic:.3f}s\")\n_ = display.figure_.suptitle(\n    \"Partial dependence of the number of bike rentals\\n\"\n    \"for the bike rental dataset with a gradient boosting\",\n    fontsize=16,\n)\n\n\n<img alt=\"Partial dependence of the number of bike rentals for the bike rental dataset with a gradient boosting\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_partial_dependence_003.png\" srcset=\"../../_images/sphx_glr_plot_partial_dependence_003.png\"/>",
            "code"
        ],
        [
            "Computing partial dependence plots...\ndone in 5.205s",
            "code"
        ],
        [
            "We will first look at the PDPs for the numerical features. For both models, the\ngeneral trend of the PDP of the temperature is that the number of bike rentals is\nincreasing with temperature. We can make a similar analysis but with the opposite\ntrend for the humidity features. The number of bike rentals is decreasing when the\nhumidity increases. Finally, we see the same trend for the wind speed feature. The\nnumber of bike rentals is decreasing when the wind speed is increasing for both\nmodels. We also observe that  has much\nsmoother predictions than .",
            "markdown"
        ],
        [
            "Now, we will look at the partial dependence plots for the categorical features.",
            "markdown"
        ],
        [
            "We observe that the spring season is the lowest bar for the season feature. With the\nweather feature, the rain category is the lowest bar. Regarding the hour feature,\nwe see two peaks around the 7 am and 6 pm. These findings are in line with the\nthe observations we made earlier on the dataset.",
            "markdown"
        ],
        [
            "However, it is worth noting that we are creating potential meaningless\nsynthetic samples if features are correlated.",
            "markdown"
        ],
        [
            "PDP is an average of the marginal effects of the features. We are averaging the\nresponse of all samples of the provided set. Thus, some effects could be hidden. In\nthis regard, it is possible to plot each individual response. This representation is\ncalled the Individual Effect Plot (ICE). In the plot below, we plot 50 randomly\nselected ICEs for the temperature and humidity features.",
            "markdown"
        ],
        [
            "print(\"Computing partial dependence plots and individual conditional expectation...\")\ntic = ()\n_, ax = (ncols=2, figsize=(6, 4), sharey=True, constrained_layout=True)\n\nfeatures_info = {\n    \"features\": [\"temp\", \"humidity\"],\n    \"kind\": \"both\",\n    \"centered\": True,\n}\n\ndisplay = (\n    hgbdt_model,\n    X_train,\n    **features_info,\n    ax=ax,\n    **common_params,\n)\nprint(f\"done in {() - tic:.3f}s\")\n_ = display.figure_.suptitle(\"ICE and PDP representations\", fontsize=16)\n\n\n<img alt=\"ICE and PDP representations\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_partial_dependence_004.png\" srcset=\"../../_images/sphx_glr_plot_partial_dependence_004.png\"/>",
            "code"
        ],
        [
            "Computing partial dependence plots and individual conditional expectation...\ndone in 2.092s",
            "code"
        ],
        [
            "We see that the ICE for the temperature feature gives us some additional information:\nSome of the ICE lines are flat while some others show a decrease of the dependence\nfor temperature above 35 degrees Celsius. We observe a similar pattern for the\nhumidity feature: some of the ICEs lines show a sharp decrease when the humidity is\nabove 80%.",
            "markdown"
        ],
        [
            "Not all ICE lines are parallel, this indicates that the model finds\ninteractions between features. We can repeat the experiment by constraining the\ngradient boosting model to not use any interactions between features using the\nparameter interaction_cst:",
            "markdown"
        ],
        [
            "from sklearn.base import clone\n\ninteraction_cst = [[i] for i in range(X_train.shape[1])]\nhgbdt_model_without_interactions = (\n    clone(hgbdt_model)\n    .set_params(histgradientboostingregressor__interaction_cst=interaction_cst)\n    .fit(X_train, y_train)\n)\nprint(f\"Test R2 score: {hgbdt_model_without_interactions.score(X_test, y_test):.2f}\")",
            "code"
        ],
        [
            "Test R2 score: 0.41",
            "code"
        ],
        [
            "_, ax = (ncols=2, figsize=(6, 4), sharey=True, constrained_layout=True)\n\nfeatures_info[\"centered\"] = False\ndisplay = (\n    hgbdt_model_without_interactions,\n    X_train,\n    **features_info,\n    ax=ax,\n    **common_params,\n)\n_ = display.figure_.suptitle(\"ICE and PDP representations\", fontsize=16)\n\n\n<img alt=\"ICE and PDP representations\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_partial_dependence_005.png\" srcset=\"../../_images/sphx_glr_plot_partial_dependence_005.png\"/>",
            "code"
        ]
    ],
    "Examples->Inspection->Partial Dependence and Individual Conditional Expectation Plots->2D interaction plots": [
        [
            "PDPs with two features of interest enable us to visualize interactions among them.\nHowever, ICEs cannot be plotted in an easy manner and thus interpreted. We will show\nthe representation of available in\n that is a 2D\nheatmap.",
            "markdown"
        ],
        [
            "print(\"Computing partial dependence plots...\")\nfeatures_info = {\n    \"features\": [\"temp\", \"humidity\", (\"temp\", \"humidity\")],\n    \"kind\": \"average\",\n}\n_, ax = (ncols=3, figsize=(10, 4), constrained_layout=True)\ntic = ()\ndisplay = (\n    hgbdt_model,\n    X_train,\n    **features_info,\n    ax=ax,\n    **common_params,\n)\nprint(f\"done in {() - tic:.3f}s\")\n_ = display.figure_.suptitle(\n    \"1-way vs 2-way of numerical PDP using gradient boosting\", fontsize=16\n)\n\n\n<img alt=\"1-way vs 2-way of numerical PDP using gradient boosting\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_partial_dependence_006.png\" srcset=\"../../_images/sphx_glr_plot_partial_dependence_006.png\"/>",
            "code"
        ],
        [
            "Computing partial dependence plots...\ndone in 44.693s",
            "code"
        ],
        [
            "The two-way partial dependence plot shows the dependence of the number of bike rentals\non joint values of temperature and humidity.\nWe clearly see an interaction between the two features. For a temperature higher than\n20 degrees Celsius, the humidity has a impact on the number of bike rentals\nthat seems independent on the temperature.",
            "markdown"
        ],
        [
            "On the other hand, for temperatures lower than 20 degrees Celsius, both the\ntemperature and humidity continuously impact the number of bike rentals.",
            "markdown"
        ],
        [
            "Furthermore, the slope of the of the impact ridge of the 20 degrees Celsius\nthreshold is very dependent on the humidity level: the ridge is steep under\ndry conditions but much smoother under wetter conditions above 70% of humidity.",
            "markdown"
        ],
        [
            "We now contrast those results with the same plots computed for the model\nconstrained to learn a prediction function that does not depend on such\nnon-linear feature interactions.",
            "markdown"
        ],
        [
            "print(\"Computing partial dependence plots...\")\nfeatures_info = {\n    \"features\": [\"temp\", \"humidity\", (\"temp\", \"humidity\")],\n    \"kind\": \"average\",\n}\n_, ax = (ncols=3, figsize=(10, 4), constrained_layout=True)\ntic = ()\ndisplay = (\n    hgbdt_model_without_interactions,\n    X_train,\n    **features_info,\n    ax=ax,\n    **common_params,\n)\nprint(f\"done in {() - tic:.3f}s\")\n_ = display.figure_.suptitle(\n    \"1-way vs 2-way of numerical PDP using gradient boosting\", fontsize=16\n)\n\n\n<img alt=\"1-way vs 2-way of numerical PDP using gradient boosting\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_partial_dependence_007.png\" srcset=\"../../_images/sphx_glr_plot_partial_dependence_007.png\"/>",
            "code"
        ],
        [
            "Computing partial dependence plots...\ndone in 41.056s",
            "code"
        ],
        [
            "The 1D partial dependence plots for the model constrained to not model feature\ninteractions show local spikes for each features individually, in particular for\nfor the \u201chumidity\u201d feature. Those spikes might be reflecting a degraded behavior\nof the model that attempts to somehow compensate for the forbidden interactions\nby overfitting particular training points. Note that the predictive performance\nof this model as measured on the test set is significantly worse than that of\nthe original, unconstrained model.",
            "markdown"
        ],
        [
            "Also note that the number of local spikes visible on those plots is depends on\nthe grid resolution parameter of the PD plot itself.",
            "markdown"
        ],
        [
            "Those local spikes result in a noisily gridded 2D PD plot. It is quite\nchallenging to tell whether or not there are no interaction between those\nfeatures because of the high frequency oscillations in the humidity feature.\nHowever it can clearly be seen that the simple interaction effect observed when\nthe temperature crosses the 20 degrees boundary is no longer visible for this\nmodel.",
            "markdown"
        ],
        [
            "The partial dependence between categorical features will provide a discrete\nrepresentation that can be shown as a heatmap. For instance the interaction between\nthe season, the weather, and the target would be as follow:",
            "markdown"
        ],
        [
            "print(\"Computing partial dependence plots...\")\nfeatures_info = {\n    \"features\": [\"season\", \"weather\", (\"season\", \"weather\")],\n    \"kind\": \"average\",\n    \"categorical_features\": categorical_features,\n}\n_, ax = (ncols=3, figsize=(14, 6), constrained_layout=True)\ntic = ()\ndisplay = (\n    hgbdt_model,\n    X_train,\n    **features_info,\n    ax=ax,\n    **common_params,\n)\n\nprint(f\"done in {() - tic:.3f}s\")\n_ = display.figure_.suptitle(\n    \"1-way vs 2-way PDP of categorical features using gradient boosting\", fontsize=16\n)\n\n\n<img alt=\"1-way vs 2-way PDP of categorical features using gradient boosting\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_partial_dependence_008.png\" srcset=\"../../_images/sphx_glr_plot_partial_dependence_008.png\"/>",
            "code"
        ],
        [
            "Computing partial dependence plots...\ndone in 1.620s",
            "code"
        ],
        [
            "Let\u2019s make the same partial dependence plot for the 2 features interaction,\nthis time in 3 dimensions.",
            "markdown"
        ],
        [
            "import numpy as np\n\n# unused but required import for doing 3d projections with matplotlib &lt; 3.2\nimport mpl_toolkits.mplot3d  # noqa: F401\n\nfrom sklearn.inspection import \n\nfig = (figsize=(5.5, 5))\n\nfeatures = (\"temp\", \"humidity\")\npdp = (\n    hgbdt_model, X_train, features=features, kind=\"average\", grid_resolution=10\n)\nXX, YY = (pdp[\"values\"][0], pdp[\"values\"][1])\nZ = pdp.average[0].T\nax = fig.add_subplot(projection=\"3d\")\nfig.add_axes(ax)\n\nsurf = ax.plot_surface(XX, YY, Z, rstride=1, cstride=1, cmap=plt.cm.BuPu, edgecolor=\"k\")\nax.set_xlabel(features[0])\nax.set_ylabel(features[1])\nfig.suptitle(\n    \"PD of number of bike rentals on\\nthe temperature and humidity GBDT model\",\n    fontsize=16,\n)\n# pretty init view\nax.view_init(elev=22, azim=122)\nclb = (surf, pad=0.08, shrink=0.6, aspect=10)\nclb.ax.set_title(\"Partial\\ndependence\")\n()\n\n\n<img alt=\"PD of number of bike rentals on the temperature and humidity GBDT model, Partial dependence\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_partial_dependence_009.png\" srcset=\"../../_images/sphx_glr_plot_partial_dependence_009.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 1 minutes  53.902 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)": [
        [
            "In this example, we will compare the impurity-based feature importance of\n with the\npermutation importance on the titanic dataset using\n. We will show that the\nimpurity-based feature importance can inflate the importance of numerical\nfeatures.",
            "markdown"
        ],
        [
            "Furthermore, the impurity-based feature importance of random forests suffers\nfrom being computed on statistics derived from the training dataset: the\nimportances can be high even for features that are not predictive of the target\nvariable, as long as the model has the capacity to use them to overfit.",
            "markdown"
        ],
        [
            "This example shows how to use Permutation Importances as an alternative that\ncan mitigate those limitations.",
            "markdown"
        ],
        [
            "References:",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "import numpy as np",
            "code"
        ]
    ],
    "Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Data Loading and Feature Engineering": [
        [
            "Let\u2019s use pandas to load a copy of the titanic dataset. The following shows\nhow to apply separate preprocessing on numerical and categorical features.",
            "markdown"
        ],
        [
            "We further include two random variables that are not correlated in any way\nwith the target variable (survived):",
            "markdown"
        ],
        [
            "random_num is a high cardinality numerical variable (as many unique\nvalues as records).",
            "markdown"
        ],
        [
            "random_cat is a low cardinality categorical variable (3 possible\nvalues).",
            "markdown"
        ],
        [
            "from sklearn.datasets import \nfrom sklearn.model_selection import \n\nX, y = (\n    \"titanic\", version=1, as_frame=True, return_X_y=True, parser=\"pandas\"\n)\nrng = (seed=42)\nX[\"random_cat\"] = rng.randint(3, size=X.shape[0])\nX[\"random_num\"] = rng.randn(X.shape[0])\n\ncategorical_columns = [\"pclass\", \"sex\", \"embarked\", \"random_cat\"]\nnumerical_columns = [\"age\", \"sibsp\", \"parch\", \"fare\", \"random_num\"]\n\nX = X[categorical_columns + numerical_columns]\nX_train, X_test, y_train, y_test = (X, y, stratify=y, random_state=42)",
            "code"
        ],
        [
            "We define a predictive model based on a random forest. Therefore, we will make\nthe following preprocessing steps:",
            "markdown"
        ],
        [
            "use  to encode the\ncategorical features;",
            "markdown"
        ],
        [
            "use  to fill missing values for\nnumerical features using a mean strategy.",
            "markdown"
        ],
        [
            "from sklearn.ensemble import \nfrom sklearn.impute import \nfrom sklearn.compose import \nfrom sklearn.pipeline import \nfrom sklearn.preprocessing import \n\ncategorical_encoder = (\n    handle_unknown=\"use_encoded_value\", unknown_value=-1, encoded_missing_value=-1\n)\nnumerical_pipe = (strategy=\"mean\")\n\npreprocessing = (\n    [\n        (\"cat\", categorical_encoder, categorical_columns),\n        (\"num\", numerical_pipe, numerical_columns),\n    ],\n    verbose_feature_names_out=False,\n)\n\nrf = (\n    [\n        (\"preprocess\", preprocessing),\n        (\"classifier\", (random_state=42)),\n    ]\n)\nrf.fit(X_train, y_train)",
            "code"
        ],
        [
            "Pipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OrdinalEncoder(encoded_missing_value=-1,\n                                                                 handle_unknown='use_encoded_value',\n                                                                 unknown_value=-1),\n                                                  ['pclass', 'sex', 'embarked',\n                                                   'random_cat']),\n                                                 ('num', SimpleImputer(),\n                                                  ['age', 'sibsp', 'parch',\n                                                   'fare', 'random_num'])],\n                                   verbose_feature_names_out=False)),\n                ('classifier', RandomForestClassifier(random_state=42))])<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-155\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-155\">Pipeline</label>",
            "code"
        ],
        [
            "Pipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OrdinalEncoder(encoded_missing_value=-1,\n                                                                 handle_unknown='use_encoded_value',\n                                                                 unknown_value=-1),\n                                                  ['pclass', 'sex', 'embarked',\n                                                   'random_cat']),\n                                                 ('num', SimpleImputer(),\n                                                  ['age', 'sibsp', 'parch',\n                                                   'fare', 'random_num'])],\n                                   verbose_feature_names_out=False)),\n                ('classifier', RandomForestClassifier(random_state=42))])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-156\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-156\">preprocess: ColumnTransformer</label>",
            "code"
        ],
        [
            "ColumnTransformer(transformers=[('cat',\n                                 OrdinalEncoder(encoded_missing_value=-1,\n                                                handle_unknown='use_encoded_value',\n                                                unknown_value=-1),\n                                 ['pclass', 'sex', 'embarked', 'random_cat']),\n                                ('num', SimpleImputer(),\n                                 ['age', 'sibsp', 'parch', 'fare',\n                                  'random_num'])],\n                  verbose_feature_names_out=False)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-157\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-157\">cat</label>",
            "code"
        ],
        [
            "['pclass', 'sex', 'embarked', 'random_cat']<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-158\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-158\">OrdinalEncoder</label>",
            "code"
        ],
        [
            "OrdinalEncoder(encoded_missing_value=-1, handle_unknown='use_encoded_value',\n               unknown_value=-1)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-159\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-159\">num</label>",
            "code"
        ],
        [
            "['age', 'sibsp', 'parch', 'fare', 'random_num']<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-160\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-160\">SimpleImputer</label>",
            "code"
        ],
        [
            "SimpleImputer()<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-161\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-161\">RandomForestClassifier</label>",
            "code"
        ],
        [
            "RandomForestClassifier(random_state=42)\n\n<br/>\n<br/>",
            "code"
        ]
    ],
    "Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Accuracy of the Model": [
        [
            "Prior to inspecting the feature importances, it is important to check that\nthe model predictive performance is high enough. Indeed there would be little\ninterest of inspecting the important features of a non-predictive model.",
            "markdown"
        ],
        [
            "Here one can observe that the train accuracy is very high (the forest model\nhas enough capacity to completely memorize the training set) but it can still\ngeneralize well enough to the test set thanks to the built-in bagging of\nrandom forests.",
            "markdown"
        ],
        [
            "It might be possible to trade some accuracy on the training set for a\nslightly better accuracy on the test set by limiting the capacity of the\ntrees (for instance by setting min_samples_leaf=5 or\nmin_samples_leaf=10) so as to limit overfitting while not introducing too\nmuch underfitting.",
            "markdown"
        ],
        [
            "However let\u2019s keep our high capacity random forest model for now so as to\nillustrate some pitfalls with feature importance on variables with many\nunique values.",
            "markdown"
        ],
        [
            "print(f\"RF train accuracy: {rf.score(X_train, y_train):.3f}\")\nprint(f\"RF test accuracy: {rf.score(X_test, y_test):.3f}\")",
            "code"
        ],
        [
            "RF train accuracy: 1.000\nRF test accuracy: 0.814",
            "code"
        ]
    ],
    "Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)": [
        [
            "The impurity-based feature importance ranks the numerical features to be the\nmost important features. As a result, the non-predictive random_num\nvariable is ranked as one of the most important features!",
            "markdown"
        ],
        [
            "This problem stems from two limitations of impurity-based feature\nimportances:",
            "markdown"
        ],
        [
            "impurity-based importances are biased towards high cardinality features;",
            "markdown"
        ],
        [
            "impurity-based importances are computed on training set statistics and\ntherefore do not reflect the ability of feature to be useful to make\npredictions that generalize to the test set (when the model has enough\ncapacity).",
            "markdown"
        ],
        [
            "The bias towards high cardinality features explains why the random_num has\na really large importance in comparison with random_cat while we would\nexpect both random features to have a null importance.",
            "markdown"
        ],
        [
            "The fact that we use training set statistics explains why both the\nrandom_num and random_cat features have a non-null importance.",
            "markdown"
        ],
        [
            "import pandas as pd\n\nfeature_names = rf[:-1].get_feature_names_out()\n\nmdi_importances = (\n    rf[-1].feature_importances_, index=feature_names\n).sort_values(ascending=True)",
            "code"
        ],
        [
            "ax = mdi_importances.plot.barh()\nax.set_title(\"Random Forest Feature Importances (MDI)\")\nax.figure.tight_layout()\n\n\n<img alt=\"Random Forest Feature Importances (MDI)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_permutation_importance_001.png\" srcset=\"../../_images/sphx_glr_plot_permutation_importance_001.png\"/>",
            "code"
        ],
        [
            "As an alternative, the permutation importances of rf are computed on a\nheld out test set. This shows that the low cardinality categorical feature,\nsex and pclass are the most important feature. Indeed, permuting the\nvalues of these features will lead to most decrease in accuracy score of the\nmodel on the test set.",
            "markdown"
        ],
        [
            "Also note that both random features have very low importances (close to 0) as\nexpected.",
            "markdown"
        ],
        [
            "from sklearn.inspection import \n\nresult = (\n    rf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\n\nsorted_importances_idx = result.importances_mean.argsort()\nimportances = (\n    result.importances[sorted_importances_idx].T,\n    columns=X.columns[sorted_importances_idx],\n)\nax = importances.plot.box(vert=False, whis=10)\nax.set_title(\"Permutation Importances (test set)\")\nax.axvline(x=0, color=\"k\", linestyle=\"--\")\nax.set_xlabel(\"Decrease in accuracy score\")\nax.figure.tight_layout()\n\n\n<img alt=\"Permutation Importances (test set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_permutation_importance_002.png\" srcset=\"../../_images/sphx_glr_plot_permutation_importance_002.png\"/>",
            "code"
        ],
        [
            "It is also possible to compute the permutation importances on the training\nset. This reveals that random_num and random_cat get a significantly\nhigher importance ranking than when computed on the test set. The difference\nbetween those two plots is a confirmation that the RF model has enough\ncapacity to use that random numerical and categorical features to overfit.",
            "markdown"
        ],
        [
            "result = (\n    rf, X_train, y_train, n_repeats=10, random_state=42, n_jobs=2\n)\n\nsorted_importances_idx = result.importances_mean.argsort()\nimportances = (\n    result.importances[sorted_importances_idx].T,\n    columns=X.columns[sorted_importances_idx],\n)\nax = importances.plot.box(vert=False, whis=10)\nax.set_title(\"Permutation Importances (train set)\")\nax.axvline(x=0, color=\"k\", linestyle=\"--\")\nax.set_xlabel(\"Decrease in accuracy score\")\nax.figure.tight_layout()\n\n\n<img alt=\"Permutation Importances (train set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_permutation_importance_003.png\" srcset=\"../../_images/sphx_glr_plot_permutation_importance_003.png\"/>",
            "code"
        ],
        [
            "We can further retry the experiment by limiting the capacity of the trees\nto overfit by setting min_samples_leaf at 20 data points.",
            "markdown"
        ],
        [
            "rf.set_params(classifier__min_samples_leaf=20).fit(X_train, y_train)",
            "code"
        ],
        [
            "Pipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OrdinalEncoder(encoded_missing_value=-1,\n                                                                 handle_unknown='use_encoded_value',\n                                                                 unknown_value=-1),\n                                                  ['pclass', 'sex', 'embarked',\n                                                   'random_cat']),\n                                                 ('num', SimpleImputer(),\n                                                  ['age', 'sibsp', 'parch',\n                                                   'fare', 'random_num'])],\n                                   verbose_feature_names_out=False)),\n                ('classifier',\n                 RandomForestClassifier(min_samples_leaf=20, random_state=42))])<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-162\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-162\">Pipeline</label>",
            "code"
        ],
        [
            "Pipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OrdinalEncoder(encoded_missing_value=-1,\n                                                                 handle_unknown='use_encoded_value',\n                                                                 unknown_value=-1),\n                                                  ['pclass', 'sex', 'embarked',\n                                                   'random_cat']),\n                                                 ('num', SimpleImputer(),\n                                                  ['age', 'sibsp', 'parch',\n                                                   'fare', 'random_num'])],\n                                   verbose_feature_names_out=False)),\n                ('classifier',\n                 RandomForestClassifier(min_samples_leaf=20, random_state=42))])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-163\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-163\">preprocess: ColumnTransformer</label>",
            "code"
        ],
        [
            "ColumnTransformer(transformers=[('cat',\n                                 OrdinalEncoder(encoded_missing_value=-1,\n                                                handle_unknown='use_encoded_value',\n                                                unknown_value=-1),\n                                 ['pclass', 'sex', 'embarked', 'random_cat']),\n                                ('num', SimpleImputer(),\n                                 ['age', 'sibsp', 'parch', 'fare',\n                                  'random_num'])],\n                  verbose_feature_names_out=False)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-164\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-164\">cat</label>",
            "code"
        ],
        [
            "['pclass', 'sex', 'embarked', 'random_cat']<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-165\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-165\">OrdinalEncoder</label>",
            "code"
        ],
        [
            "OrdinalEncoder(encoded_missing_value=-1, handle_unknown='use_encoded_value',\n               unknown_value=-1)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-166\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-166\">num</label>",
            "code"
        ],
        [
            "['age', 'sibsp', 'parch', 'fare', 'random_num']<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-167\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-167\">SimpleImputer</label>",
            "code"
        ],
        [
            "SimpleImputer()<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-168\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-168\">RandomForestClassifier</label>",
            "code"
        ],
        [
            "RandomForestClassifier(min_samples_leaf=20, random_state=42)\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "Observing the accuracy score on the training and testing set, we observe that\nthe two metrics are very similar now. Therefore, our model is not overfitting\nanymore. We can then check the permutation importances with this new model.",
            "markdown"
        ],
        [
            "print(f\"RF train accuracy: {rf.score(X_train, y_train):.3f}\")\nprint(f\"RF test accuracy: {rf.score(X_test, y_test):.3f}\")",
            "code"
        ],
        [
            "RF train accuracy: 0.810\nRF test accuracy: 0.832",
            "code"
        ],
        [
            "train_result = (\n    rf, X_train, y_train, n_repeats=10, random_state=42, n_jobs=2\n)\ntest_results = (\n    rf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_importances_idx = train_result.importances_mean.argsort()",
            "code"
        ],
        [
            "train_importances = (\n    train_result.importances[sorted_importances_idx].T,\n    columns=X.columns[sorted_importances_idx],\n)\ntest_importances = (\n    test_results.importances[sorted_importances_idx].T,\n    columns=X.columns[sorted_importances_idx],\n)",
            "code"
        ],
        [
            "for name, importances in zip([\"train\", \"test\"], [train_importances, test_importances]):\n    ax = importances.plot.box(vert=False, whis=10)\n    ax.set_title(f\"Permutation Importances ({name} set)\")\n    ax.set_xlabel(\"Decrease in accuracy score\")\n    ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n    ax.figure.tight_layout()\n\n\n\n<img alt=\"Permutation Importances (train set)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_permutation_importance_004.png\" srcset=\"../../_images/sphx_glr_plot_permutation_importance_004.png\"/>\n<img alt=\"Permutation Importances (test set)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_permutation_importance_005.png\" srcset=\"../../_images/sphx_glr_plot_permutation_importance_005.png\"/>",
            "code"
        ],
        [
            "Now, we can observe that on both sets, the random_num and random_cat\nfeatures have a lower importance compared to the overfitting random forest.\nHowever, the conclusions regarding the importance of the other features are\nstill valid.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  5.757 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Inspection->Permutation Importance with Multicollinear or Correlated Features": [
        [
            "In this example, we compute the permutation importance on the Wisconsin\nbreast cancer dataset using .\nThe  can easily get about 97%\naccuracy on a test dataset. Because this dataset contains multicollinear\nfeatures, the permutation importance will show that none of the features are\nimportant. One approach to handling multicollinearity is by performing\nhierarchical clustering on the features\u2019 Spearman rank-order correlations,\npicking a threshold, and keeping a single feature from each cluster.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "See also",
            "markdown"
        ],
        [
            "from collections import \n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import \nfrom scipy.cluster import hierarchy\nfrom scipy.spatial.distance import \n\nfrom sklearn.datasets import \nfrom sklearn.ensemble import \nfrom sklearn.inspection import \nfrom sklearn.model_selection import",
            "code"
        ]
    ],
    "Examples->Inspection->Permutation Importance with Multicollinear or Correlated Features->Random Forest Feature Importance on Breast Cancer Data": [
        [
            "First, we train a random forest on the breast cancer dataset and evaluate\nits accuracy on a test set:",
            "markdown"
        ],
        [
            "data = ()\nX, y = data.data, data.target\nX_train, X_test, y_train, y_test = (X, y, random_state=42)\n\nclf = (n_estimators=100, random_state=42)\nclf.fit(X_train, y_train)\nprint(\"Accuracy on test data: {:.2f}\".format(clf.score(X_test, y_test)))",
            "code"
        ],
        [
            "Accuracy on test data: 0.97",
            "code"
        ],
        [
            "Next, we plot the tree based feature importance and the permutation\nimportance. The permutation importance plot shows that permuting a feature\ndrops the accuracy by at most 0.012, which would suggest that none of the\nfeatures are important. This is in contradiction with the high test accuracy\ncomputed above: some feature must be important. The permutation importance\nis calculated on the training set to show how much the model relies on each\nfeature during training.",
            "markdown"
        ],
        [
            "result = (clf, X_train, y_train, n_repeats=10, random_state=42)\nperm_sorted_idx = result.importances_mean.argsort()\n\ntree_importance_sorted_idx = (clf.feature_importances_)\ntree_indices = (0, len(clf.feature_importances_)) + 0.5\n\nfig, (ax1, ax2) = (1, 2, figsize=(12, 8))\nax1.barh(tree_indices, clf.feature_importances_[tree_importance_sorted_idx], height=0.7)\nax1.set_yticks(tree_indices)\nax1.set_yticklabels(data.feature_names[tree_importance_sorted_idx])\nax1.set_ylim((0, len(clf.feature_importances_)))\nax2.boxplot(\n    result.importances[perm_sorted_idx].T,\n    vert=False,\n    labels=data.feature_names[perm_sorted_idx],\n)\nfig.tight_layout()\n()\n\n\n<img alt=\"plot permutation importance multicollinear\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_permutation_importance_multicollinear_001.png\" srcset=\"../../_images/sphx_glr_plot_permutation_importance_multicollinear_001.png\"/>",
            "code"
        ]
    ],
    "Examples->Inspection->Permutation Importance with Multicollinear or Correlated Features->Handling Multicollinear Features": [
        [
            "When features are collinear, permutating one feature will have little\neffect on the models performance because it can get the same information\nfrom a correlated feature. One way to handle multicollinear features is by\nperforming hierarchical clustering on the Spearman rank-order correlations,\npicking a threshold, and keeping a single feature from each cluster. First,\nwe plot a heatmap of the correlated features:",
            "markdown"
        ],
        [
            "fig, (ax1, ax2) = (1, 2, figsize=(12, 8))\ncorr = (X).correlation\n\n# Ensure the correlation matrix is symmetric\ncorr = (corr + corr.T) / 2\n(corr, 1)\n\n# We convert the correlation matrix to a distance matrix before performing\n# hierarchical clustering using Ward's linkage.\ndistance_matrix = 1 - np.abs(corr)\ndist_linkage = ((distance_matrix))\ndendro = (\n    dist_linkage, labels=data.feature_names.tolist(), ax=ax1, leaf_rotation=90\n)\ndendro_idx = (0, len(dendro[\"ivl\"]))\n\nax2.imshow(corr[dendro[\"leaves\"], :][:, dendro[\"leaves\"]])\nax2.set_xticks(dendro_idx)\nax2.set_yticks(dendro_idx)\nax2.set_xticklabels(dendro[\"ivl\"], rotation=\"vertical\")\nax2.set_yticklabels(dendro[\"ivl\"])\nfig.tight_layout()\n()\n\n\n<img alt=\"plot permutation importance multicollinear\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_permutation_importance_multicollinear_002.png\" srcset=\"../../_images/sphx_glr_plot_permutation_importance_multicollinear_002.png\"/>",
            "code"
        ],
        [
            "Next, we manually pick a threshold by visual inspection of the dendrogram\nto group our features into clusters and choose a feature from each cluster to\nkeep, select those features from our dataset, and train a new random forest.\nThe test accuracy of the new random forest did not change much compared to\nthe random forest trained on the complete dataset.",
            "markdown"
        ],
        [
            "cluster_ids = (dist_linkage, 1, criterion=\"distance\")\ncluster_id_to_feature_ids = (list)\nfor idx, cluster_id in enumerate(cluster_ids):\n    cluster_id_to_feature_ids[cluster_id].append(idx)\nselected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n\nX_train_sel = X_train[:, selected_features]\nX_test_sel = X_test[:, selected_features]\n\nclf_sel = (n_estimators=100, random_state=42)\nclf_sel.fit(X_train_sel, y_train)\nprint(\n    \"Accuracy on test data with features removed: {:.2f}\".format(\n        clf_sel.score(X_test_sel, y_test)\n    )\n)",
            "code"
        ],
        [
            "Accuracy on test data with features removed: 0.97",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  4.618 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Kernel Approximation->Scalable learning with polynomial kernel approximation": [
        [
            "This example illustrates the use of PolynomialCountSketch to\nefficiently generate polynomial kernel feature-space approximations.\nThis is used to train linear classifiers that approximate the accuracy\nof kernelized ones.",
            "markdown"
        ],
        [
            "We use the Covtype dataset [2], trying to reproduce the experiments on the\noriginal paper of Tensor Sketch [1], i.e. the algorithm implemented by\n.",
            "markdown"
        ],
        [
            "First, we compute the accuracy of a linear classifier on the original\nfeatures. Then, we train linear classifiers on different numbers of\nfeatures (n_components) generated by ,\napproximating the accuracy of a kernelized classifier in a scalable manner.",
            "markdown"
        ],
        [
            "# Author: Daniel Lopez-Sanchez &lt;lope@usal.es\n# License: BSD 3 clause",
            "code"
        ]
    ],
    "Examples->Kernel Approximation->Scalable learning with polynomial kernel approximation->Preparing the data": [
        [
            "Load the Covtype dataset, which contains 581,012 samples\nwith 54 features each, distributed among 6 classes. The goal of this dataset\nis to predict forest cover type from cartographic variables only\n(no remotely sensed data). After loading, we transform it into a binary\nclassification problem to match the version of the dataset in the\nLIBSVM webpage [2], which was the one used in [1].",
            "markdown"
        ],
        [
            "from sklearn.datasets import \n\nX, y = (return_X_y=True)\n\ny[y != 2] = 0\ny[y == 2] = 1  # We will try to separate class 2 from the other 6 classes.",
            "code"
        ]
    ],
    "Examples->Kernel Approximation->Scalable learning with polynomial kernel approximation->Partitioning the data": [
        [
            "Here we select 5,000 samples for training and 10,000 for testing.\nTo actually reproduce the results in the original Tensor Sketch paper,\nselect 100,000 for training.",
            "markdown"
        ],
        [
            "from sklearn.model_selection import \n\nX_train, X_test, y_train, y_test = (\n    X, y, train_size=5_000, test_size=10_000, random_state=42\n)",
            "code"
        ]
    ],
    "Examples->Kernel Approximation->Scalable learning with polynomial kernel approximation->Feature normalization": [
        [
            "Now scale features to the range [0, 1] to match the format of the dataset in\nthe LIBSVM webpage, and then normalize to unit length as done in the\noriginal Tensor Sketch paper [1].",
            "markdown"
        ],
        [
            "from sklearn.preprocessing import , \nfrom sklearn.pipeline import \n\nmm = ((), ())\nX_train = mm.fit_transform(X_train)\nX_test = mm.transform(X_test)",
            "code"
        ]
    ],
    "Examples->Kernel Approximation->Scalable learning with polynomial kernel approximation->Establishing a baseline model": [
        [
            "As a baseline, train a linear SVM on the original features and print the\naccuracy. We also measure and store accuracies and training times to\nplot them later.",
            "markdown"
        ],
        [
            "import time\nfrom sklearn.svm import \n\nresults = {}\n\nlsvm = ()\nstart = ()\nlsvm.fit(X_train, y_train)\nlsvm_time = () - start\nlsvm_score = 100 * lsvm.score(X_test, y_test)\n\nresults[\"LSVM\"] = {\"time\": lsvm_time, \"score\": lsvm_score}\nprint(f\"Linear SVM score on raw features: {lsvm_score:.2f}%\")",
            "code"
        ],
        [
            "Linear SVM score on raw features: 75.62%",
            "code"
        ]
    ],
    "Examples->Kernel Approximation->Scalable learning with polynomial kernel approximation->Establishing the kernel approximation model": [
        [
            "Then we train linear SVMs on the features generated by\n with different values for n_components,\nshowing that these kernel feature approximations improve the accuracy\nof linear classification. In typical application scenarios, n_components\nshould be larger than the number of features in the input representation\nin order to achieve an improvement with respect to linear classification.\nAs a rule of thumb, the optimum of evaluation score / run time cost is\ntypically achieved at around n_components = 10 * n_features, though this\nmight depend on the specific dataset being handled. Note that, since the\noriginal samples have 54 features, the explicit feature map of the\npolynomial kernel of degree four would have approximately 8.5 million\nfeatures (precisely, 54^4). Thanks to , we can\ncondense most of the discriminative information of that feature space into a\nmuch more compact representation. While we run the experiment only a single time\n(n_runs = 1) in this example, in practice one should repeat the experiment several\ntimes to compensate for the stochastic nature of .",
            "markdown"
        ],
        [
            "from sklearn.kernel_approximation import \n\nn_runs = 1\nN_COMPONENTS = [250, 500, 1000, 2000]\n\nfor n_components in N_COMPONENTS:\n\n    ps_lsvm_time = 0\n    ps_lsvm_score = 0\n    for _ in range(n_runs):\n\n        pipeline = (\n            (n_components=n_components, degree=4),\n            (),\n        )\n\n        start = ()\n        pipeline.fit(X_train, y_train)\n        ps_lsvm_time += () - start\n        ps_lsvm_score += 100 * pipeline.score(X_test, y_test)\n\n    ps_lsvm_time /= n_runs\n    ps_lsvm_score /= n_runs\n\n    results[f\"LSVM + PS({n_components})\"] = {\n        \"time\": ps_lsvm_time,\n        \"score\": ps_lsvm_score,\n    }\n    print(\n        f\"Linear SVM score on {n_components} PolynomialCountSketch \"\n        + f\"features: {ps_lsvm_score:.2f}%\"\n    )",
            "code"
        ],
        [
            "Linear SVM score on 250 PolynomialCountSketch features: 76.55%\nLinear SVM score on 500 PolynomialCountSketch features: 76.92%\nLinear SVM score on 1000 PolynomialCountSketch features: 77.79%\nLinear SVM score on 2000 PolynomialCountSketch features: 78.59%",
            "code"
        ]
    ],
    "Examples->Kernel Approximation->Scalable learning with polynomial kernel approximation->Establishing the kernelized SVM model": [
        [
            "Train a kernelized SVM to see how well \nis approximating the performance of the kernel. This, of course, may take\nsome time, as the SVC class has a relatively poor scalability. This is the\nreason why kernel approximators are so useful:",
            "markdown"
        ],
        [
            "from sklearn.svm import \n\nksvm = (C=500.0, kernel=\"poly\", degree=4, coef0=0, gamma=1.0)\n\nstart = ()\nksvm.fit(X_train, y_train)\nksvm_time = () - start\nksvm_score = 100 * ksvm.score(X_test, y_test)\n\nresults[\"KSVM\"] = {\"time\": ksvm_time, \"score\": ksvm_score}\nprint(f\"Kernel-SVM score on raw features: {ksvm_score:.2f}%\")",
            "code"
        ],
        [
            "Kernel-SVM score on raw features: 79.78%",
            "code"
        ]
    ],
    "Examples->Kernel Approximation->Scalable learning with polynomial kernel approximation->Comparing the results": [
        [
            "Finally, plot the results of the different methods against their training\ntimes. As we can see, the kernelized SVM achieves a higher accuracy,\nbut its training time is much larger and, most importantly, will grow\nmuch faster if the number of training samples increases.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n\nfig, ax = (figsize=(7, 7))\nax.scatter(\n    [\n        results[\"LSVM\"][\"time\"],\n    ],\n    [\n        results[\"LSVM\"][\"score\"],\n    ],\n    label=\"Linear SVM\",\n    c=\"green\",\n    marker=\"^\",\n)\n\nax.scatter(\n    [\n        results[\"LSVM + PS(250)\"][\"time\"],\n    ],\n    [\n        results[\"LSVM + PS(250)\"][\"score\"],\n    ],\n    label=\"Linear SVM + PolynomialCountSketch\",\n    c=\"blue\",\n)\n\nfor n_components in N_COMPONENTS:\n    ax.scatter(\n        [\n            results[f\"LSVM + PS({n_components})\"][\"time\"],\n        ],\n        [\n            results[f\"LSVM + PS({n_components})\"][\"score\"],\n        ],\n        c=\"blue\",\n    )\n    ax.annotate(\n        f\"n_comp.={n_components}\",\n        (\n            results[f\"LSVM + PS({n_components})\"][\"time\"],\n            results[f\"LSVM + PS({n_components})\"][\"score\"],\n        ),\n        xytext=(-30, 10),\n        textcoords=\"offset pixels\",\n    )\n\nax.scatter(\n    [\n        results[\"KSVM\"][\"time\"],\n    ],\n    [\n        results[\"KSVM\"][\"score\"],\n    ],\n    label=\"Kernel SVM\",\n    c=\"red\",\n    marker=\"x\",\n)\n\nax.set_xlabel(\"Training time (s)\")\nax.set_ylabel(\"Accuracy (%)\")\nax.legend()\n()\n\n\n<img alt=\"plot scalable poly kernels\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_scalable_poly_kernels_001.png\" srcset=\"../../_images/sphx_glr_plot_scalable_poly_kernels_001.png\"/>",
            "code"
        ]
    ],
    "Examples->Kernel Approximation->Scalable learning with polynomial kernel approximation->Comparing the results->References": [
        [
            "[1] Pham, Ninh and Rasmus Pagh. \u201cFast and scalable polynomial kernels via\nexplicit feature maps.\u201d KDD \u201813 (2013).",
            "markdown"
        ],
        [
            "[2] LIBSVM binary datasets repository",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  16.375 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Manifold learning->Comparison of Manifold Learning methods": [
        [
            "An illustration of dimensionality reduction on the S-curve dataset\nwith various manifold learning methods.",
            "markdown"
        ],
        [
            "For a discussion and comparison of these algorithms, see the",
            "markdown"
        ],
        [
            "For a similar example, where the methods are applied to a\nsphere dataset, see",
            "markdown"
        ],
        [
            "Note that the purpose of the MDS is to find a low-dimensional\nrepresentation of the data (here 2D) in which the distances respect well\nthe distances in the original high-dimensional space, unlike other\nmanifold-learning algorithms, it does not seeks an isotropic\nrepresentation of the data in the low-dimensional space.",
            "markdown"
        ],
        [
            "# Author: Jake Vanderplas -- &lt;vanderplas@astro.washington.edu",
            "code"
        ]
    ],
    "Examples->Manifold learning->Comparison of Manifold Learning methods->Dataset preparation": [
        [
            "We start by generating the S-curve dataset.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\nfrom matplotlib import ticker\n\n# unused but required import for doing 3d projections with matplotlib &lt; 3.2\nimport mpl_toolkits.mplot3d  # noqa: F401\n\nfrom sklearn import manifold, datasets\n\nn_samples = 1500\nS_points, S_color = (n_samples, random_state=0)",
            "code"
        ],
        [
            "Let\u2019s look at the original data. Also define some helping\nfunctions, which we will use further on.",
            "markdown"
        ],
        [
            "def plot_3d(points, points_color, title):\n    x, y, z = points.T\n\n    fig, ax = (\n        figsize=(6, 6),\n        facecolor=\"white\",\n        tight_layout=True,\n        subplot_kw={\"projection\": \"3d\"},\n    )\n    fig.suptitle(title, size=16)\n    col = ax.scatter(x, y, z, c=points_color, s=50, alpha=0.8)\n    ax.view_init(azim=-60, elev=9)\n    ax.xaxis.set_major_locator((1))\n    ax.yaxis.set_major_locator((1))\n    ax.zaxis.set_major_locator((1))\n\n    fig.colorbar(col, ax=ax, orientation=\"horizontal\", shrink=0.6, aspect=60, pad=0.01)\n    ()\n\n\ndef plot_2d(points, points_color, title):\n    fig, ax = (figsize=(3, 3), facecolor=\"white\", constrained_layout=True)\n    fig.suptitle(title, size=16)\n    add_2d_scatter(ax, points, points_color)\n    ()\n\n\ndef add_2d_scatter(ax, points, points_color, title=None):\n    x, y = points.T\n    ax.scatter(x, y, c=points_color, s=50, alpha=0.8)\n    ax.set_title(title)\n    ax.xaxis.set_major_formatter(())\n    ax.yaxis.set_major_formatter(())\n\n\nplot_3d(S_points, S_color, \"Original S-curve samples\")\n\n\n<img alt=\"Original S-curve samples\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_compare_methods_001.png\" srcset=\"../../_images/sphx_glr_plot_compare_methods_001.png\"/>",
            "code"
        ]
    ],
    "Examples->Manifold learning->Comparison of Manifold Learning methods->Define algorithms for the manifold learning": [
        [
            "Manifold learning is an approach to non-linear dimensionality reduction.\nAlgorithms for this task are based on the idea that the dimensionality of\nmany data sets is only artificially high.",
            "markdown"
        ],
        [
            "Read more in the .",
            "markdown"
        ],
        [
            "n_neighbors = 12  # neighborhood which is used to recover the locally linear structure\nn_components = 2  # number of coordinates for the manifold",
            "code"
        ]
    ],
    "Examples->Manifold learning->Comparison of Manifold Learning methods->Define algorithms for the manifold learning->Locally Linear Embeddings": [
        [
            "Locally linear embedding (LLE) can be thought of as a series of local\nPrincipal Component Analyses which are globally compared to find the\nbest non-linear embedding.\nRead more in the .",
            "markdown"
        ],
        [
            "params = {\n    \"n_neighbors\": n_neighbors,\n    \"n_components\": n_components,\n    \"eigen_solver\": \"auto\",\n    \"random_state\": 0,\n}\n\nlle_standard = (method=\"standard\", **params)\nS_standard = lle_standard.fit_transform(S_points)\n\nlle_ltsa = (method=\"ltsa\", **params)\nS_ltsa = lle_ltsa.fit_transform(S_points)\n\nlle_hessian = (method=\"hessian\", **params)\nS_hessian = lle_hessian.fit_transform(S_points)\n\nlle_mod = (method=\"modified\", **params)\nS_mod = lle_mod.fit_transform(S_points)",
            "code"
        ],
        [
            "fig, axs = (\n    nrows=2, ncols=2, figsize=(7, 7), facecolor=\"white\", constrained_layout=True\n)\nfig.suptitle(\"Locally Linear Embeddings\", size=16)\n\nlle_methods = [\n    (\"Standard locally linear embedding\", S_standard),\n    (\"Local tangent space alignment\", S_ltsa),\n    (\"Hessian eigenmap\", S_hessian),\n    (\"Modified locally linear embedding\", S_mod),\n]\nfor ax, method in zip(axs.flat, lle_methods):\n    name, points = method\n    add_2d_scatter(ax, points, S_color, name)\n\n()\n\n\n<img alt=\"Locally Linear Embeddings, Standard locally linear embedding, Local tangent space alignment, Hessian eigenmap, Modified locally linear embedding\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_compare_methods_002.png\" srcset=\"../../_images/sphx_glr_plot_compare_methods_002.png\"/>",
            "code"
        ]
    ],
    "Examples->Manifold learning->Comparison of Manifold Learning methods->Define algorithms for the manifold learning->Isomap Embedding": [
        [
            "Non-linear dimensionality reduction through Isometric Mapping.\nIsomap seeks a lower-dimensional embedding which maintains geodesic\ndistances between all points. Read more in the .",
            "markdown"
        ],
        [
            "isomap = (n_neighbors=n_neighbors, n_components=n_components, p=1)\nS_isomap = isomap.fit_transform(S_points)\n\nplot_2d(S_isomap, S_color, \"Isomap Embedding\")\n\n\n<img alt=\"Isomap Embedding\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_compare_methods_003.png\" srcset=\"../../_images/sphx_glr_plot_compare_methods_003.png\"/>",
            "code"
        ]
    ],
    "Examples->Manifold learning->Comparison of Manifold Learning methods->Define algorithms for the manifold learning->Multidimensional scaling": [
        [
            "Multidimensional scaling (MDS) seeks a low-dimensional representation\nof the data in which the distances respect well the distances in the\noriginal high-dimensional space.\nRead more in the .",
            "markdown"
        ],
        [
            "md_scaling = (\n    n_components=n_components,\n    max_iter=50,\n    n_init=4,\n    random_state=0,\n    normalized_stress=False,\n)\nS_scaling = md_scaling.fit_transform(S_points)\n\nplot_2d(S_scaling, S_color, \"Multidimensional scaling\")\n\n\n<img alt=\"Multidimensional scaling\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_compare_methods_004.png\" srcset=\"../../_images/sphx_glr_plot_compare_methods_004.png\"/>",
            "code"
        ]
    ],
    "Examples->Manifold learning->Comparison of Manifold Learning methods->Define algorithms for the manifold learning->Spectral embedding for non-linear dimensionality reduction": [
        [
            "This implementation uses Laplacian Eigenmaps, which finds a low dimensional\nrepresentation of the data using a spectral decomposition of the graph Laplacian.\nRead more in the .",
            "markdown"
        ],
        [
            "spectral = (\n    n_components=n_components, n_neighbors=n_neighbors\n)\nS_spectral = spectral.fit_transform(S_points)\n\nplot_2d(S_spectral, S_color, \"Spectral Embedding\")\n\n\n<img alt=\"Spectral Embedding\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_compare_methods_005.png\" srcset=\"../../_images/sphx_glr_plot_compare_methods_005.png\"/>",
            "code"
        ]
    ],
    "Examples->Manifold learning->Comparison of Manifold Learning methods->Define algorithms for the manifold learning->T-distributed Stochastic Neighbor Embedding": [
        [
            "It converts similarities between data points to joint probabilities and\ntries to minimize the Kullback-Leibler divergence between the joint probabilities\nof the low-dimensional embedding and the high-dimensional data. t-SNE has a cost\nfunction that is not convex, i.e. with different initializations we can get\ndifferent results. Read more in the .",
            "markdown"
        ],
        [
            "t_sne = (\n    n_components=n_components,\n    perplexity=30,\n    init=\"random\",\n    n_iter=250,\n    random_state=0,\n)\nS_t_sne = t_sne.fit_transform(S_points)\n\nplot_2d(S_t_sne, S_color, \"T-distributed Stochastic  \\n Neighbor Embedding\")\n\n\n<img alt=\"T-distributed Stochastic    Neighbor Embedding\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_compare_methods_006.png\" srcset=\"../../_images/sphx_glr_plot_compare_methods_006.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  13.448 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Manifold learning->Manifold Learning methods on a severed sphere": [
        [
            "An application of the different  techniques\non a spherical data-set. Here one can see the use of\ndimensionality reduction in order to gain some intuition\nregarding the manifold learning methods. Regarding the dataset,\nthe poles are cut from the sphere, as well as a thin slice down its\nside. This enables the manifold learning techniques to\n\u2018spread it open\u2019 whilst projecting it onto two dimensions.",
            "markdown"
        ],
        [
            "For a similar example, where the methods are applied to the\nS-curve dataset, see",
            "markdown"
        ],
        [
            "Note that the purpose of the  is\nto find a low-dimensional representation of the data (here 2D) in\nwhich the distances respect well the distances in the original\nhigh-dimensional space, unlike other manifold-learning algorithms,\nit does not seeks an isotropic representation of the data in\nthe low-dimensional space. Here the manifold problem matches fairly\nthat of representing a flat map of the Earth, as with\n\n<img alt=\"Manifold Learning with 1000 points, 10 neighbors, LLE (0.058 sec), LTSA (0.094 sec), Hessian LLE (0.16 sec), Modified LLE (0.11 sec), Isomap (0.2 sec), MDS (0.64 sec), Spectral Embedding (0.047 sec), t-SNE (4 sec)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_manifold_sphere_001.png\" srcset=\"../../_images/sphx_glr_plot_manifold_sphere_001.png\"/>",
            "markdown"
        ],
        [
            "standard: 0.058 sec\nltsa: 0.094 sec\nhessian: 0.16 sec\nmodified: 0.11 sec\nISO: 0.2 sec\nMDS: 0.64 sec\nSpectral Embedding: 0.047 sec\nt-SNE: 4 sec\n\n\n\n<br/>",
            "code"
        ],
        [
            "# Author: Jaques Grobler &lt;jaques.grobler@inria.fr\n# License: BSD 3 clause\n\nfrom time import \nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import \nfrom sklearn import manifold\nfrom sklearn.utils import \n\n# Unused but required import for doing 3d projections with matplotlib &lt; 3.2\nimport mpl_toolkits.mplot3d  # noqa: F401\n\n# Variables for manifold learning.\nn_neighbors = 10\nn_samples = 1000\n\n# Create our sphere.\nrandom_state = (0)\np = random_state.rand(n_samples) * (2 *  - 0.55)\nt = random_state.rand(n_samples) * \n\n# Sever the poles from the sphere.\nindices = (t &lt; ( - ( / 8))) & (t  (( / 8)))\ncolors = p[indices]\nx, y, z = (\n    (t[indices]) * (p[indices]),\n    (t[indices]) * (p[indices]),\n    (t[indices]),\n)\n\n# Plot our dataset.\nfig = (figsize=(15, 8))\n(\n    \"Manifold Learning with %i points, %i neighbors\" % (1000, n_neighbors), fontsize=14\n)\n\nax = fig.add_subplot(251, projection=\"3d\")\nax.scatter(x, y, z, c=p[indices], cmap=plt.cm.rainbow)\nax.view_init(40, -10)\n\nsphere_data = ([x, y, z]).T\n\n# Perform Locally Linear Embedding Manifold learning\nmethods = [\"standard\", \"ltsa\", \"hessian\", \"modified\"]\nlabels = [\"LLE\", \"LTSA\", \"Hessian LLE\", \"Modified LLE\"]\n\nfor i, method in enumerate(methods):\n    t0 = ()\n    trans_data = (\n        (\n            n_neighbors=n_neighbors, n_components=2, method=method\n        )\n        .fit_transform(sphere_data)\n        .T\n    )\n    t1 = ()\n    print(\"%s: %.2g sec\" % (methods[i], t1 - t0))\n\n    ax = fig.add_subplot(252 + i)\n    (trans_data[0], trans_data[1], c=colors, cmap=plt.cm.rainbow)\n    (\"%s (%.2g sec)\" % (labels[i], t1 - t0))\n    ax.xaxis.set_major_formatter(())\n    ax.yaxis.set_major_formatter(())\n    (\"tight\")\n\n# Perform Isomap Manifold learning.\nt0 = ()\ntrans_data = (\n    (n_neighbors=n_neighbors, n_components=2)\n    .fit_transform(sphere_data)\n    .T\n)\nt1 = ()\nprint(\"%s: %.2g sec\" % (\"ISO\", t1 - t0))\n\nax = fig.add_subplot(257)\n(trans_data[0], trans_data[1], c=colors, cmap=plt.cm.rainbow)\n(\"%s (%.2g sec)\" % (\"Isomap\", t1 - t0))\nax.xaxis.set_major_formatter(())\nax.yaxis.set_major_formatter(())\n(\"tight\")\n\n# Perform Multi-dimensional scaling.\nt0 = ()\nmds = (2, max_iter=100, n_init=1, normalized_stress=\"auto\")\ntrans_data = mds.fit_transform(sphere_data).T\nt1 = ()\nprint(\"MDS: %.2g sec\" % (t1 - t0))\n\nax = fig.add_subplot(258)\n(trans_data[0], trans_data[1], c=colors, cmap=plt.cm.rainbow)\n(\"MDS (%.2g sec)\" % (t1 - t0))\nax.xaxis.set_major_formatter(())\nax.yaxis.set_major_formatter(())\n(\"tight\")\n\n# Perform Spectral Embedding.\nt0 = ()\nse = (n_components=2, n_neighbors=n_neighbors)\ntrans_data = se.fit_transform(sphere_data).T\nt1 = ()\nprint(\"Spectral Embedding: %.2g sec\" % (t1 - t0))\n\nax = fig.add_subplot(259)\n(trans_data[0], trans_data[1], c=colors, cmap=plt.cm.rainbow)\n(\"Spectral Embedding (%.2g sec)\" % (t1 - t0))\nax.xaxis.set_major_formatter(())\nax.yaxis.set_major_formatter(())\n(\"tight\")\n\n# Perform t-distributed stochastic neighbor embedding.\nt0 = ()\ntsne = (n_components=2, random_state=0)\ntrans_data = tsne.fit_transform(sphere_data).T\nt1 = ()\nprint(\"t-SNE: %.2g sec\" % (t1 - t0))\n\nax = fig.add_subplot(2, 5, 10)\n(trans_data[0], trans_data[1], c=colors, cmap=plt.cm.rainbow)\n(\"t-SNE (%.2g sec)\" % (t1 - t0))\nax.xaxis.set_major_formatter(())\nax.yaxis.set_major_formatter(())\n(\"tight\")\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  5.885 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Manifold learning->Manifold learning on handwritten digits: Locally Linear Embedding, Isomap...": [
        [
            "An application of the different  techniques\non a spherical data-set. Here one can see the use of\ndimensionality reduction in order to gain some intuition\nregarding the manifold learning methods. Regarding the dataset,\nthe poles are cut from the sphere, as well as a thin slice down its\nside. This enables the manifold learning techniques to\n\u2018spread it open\u2019 whilst projecting it onto two dimensions.",
            "markdown"
        ],
        [
            "For a similar example, where the methods are applied to the\nS-curve dataset, see",
            "markdown"
        ],
        [
            "Note that the purpose of the  is\nto find a low-dimensional representation of the data (here 2D) in\nwhich the distances respect well the distances in the original\nhigh-dimensional space, unlike other manifold-learning algorithms,\nit does not seeks an isotropic representation of the data in\nthe low-dimensional space. Here the manifold problem matches fairly\nthat of representing a flat map of the Earth, as with\n\n<img alt=\"Manifold Learning with 1000 points, 10 neighbors, LLE (0.058 sec), LTSA (0.094 sec), Hessian LLE (0.16 sec), Modified LLE (0.11 sec), Isomap (0.2 sec), MDS (0.64 sec), Spectral Embedding (0.047 sec), t-SNE (4 sec)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_manifold_sphere_001.png\" srcset=\"../../_images/sphx_glr_plot_manifold_sphere_001.png\"/>",
            "markdown"
        ],
        [
            "standard: 0.058 sec\nltsa: 0.094 sec\nhessian: 0.16 sec\nmodified: 0.11 sec\nISO: 0.2 sec\nMDS: 0.64 sec\nSpectral Embedding: 0.047 sec\nt-SNE: 4 sec\n\n\n\n<br/>",
            "code"
        ],
        [
            "# Author: Jaques Grobler &lt;jaques.grobler@inria.fr\n# License: BSD 3 clause\n\nfrom time import \nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import \nfrom sklearn import manifold\nfrom sklearn.utils import \n\n# Unused but required import for doing 3d projections with matplotlib &lt; 3.2\nimport mpl_toolkits.mplot3d  # noqa: F401\n\n# Variables for manifold learning.\nn_neighbors = 10\nn_samples = 1000\n\n# Create our sphere.\nrandom_state = (0)\np = random_state.rand(n_samples) * (2 *  - 0.55)\nt = random_state.rand(n_samples) * \n\n# Sever the poles from the sphere.\nindices = (t &lt; ( - ( / 8))) & (t  (( / 8)))\ncolors = p[indices]\nx, y, z = (\n    (t[indices]) * (p[indices]),\n    (t[indices]) * (p[indices]),\n    (t[indices]),\n)\n\n# Plot our dataset.\nfig = (figsize=(15, 8))\n(\n    \"Manifold Learning with %i points, %i neighbors\" % (1000, n_neighbors), fontsize=14\n)\n\nax = fig.add_subplot(251, projection=\"3d\")\nax.scatter(x, y, z, c=p[indices], cmap=plt.cm.rainbow)\nax.view_init(40, -10)\n\nsphere_data = ([x, y, z]).T\n\n# Perform Locally Linear Embedding Manifold learning\nmethods = [\"standard\", \"ltsa\", \"hessian\", \"modified\"]\nlabels = [\"LLE\", \"LTSA\", \"Hessian LLE\", \"Modified LLE\"]\n\nfor i, method in enumerate(methods):\n    t0 = ()\n    trans_data = (\n        (\n            n_neighbors=n_neighbors, n_components=2, method=method\n        )\n        .fit_transform(sphere_data)\n        .T\n    )\n    t1 = ()\n    print(\"%s: %.2g sec\" % (methods[i], t1 - t0))\n\n    ax = fig.add_subplot(252 + i)\n    (trans_data[0], trans_data[1], c=colors, cmap=plt.cm.rainbow)\n    (\"%s (%.2g sec)\" % (labels[i], t1 - t0))\n    ax.xaxis.set_major_formatter(())\n    ax.yaxis.set_major_formatter(())\n    (\"tight\")\n\n# Perform Isomap Manifold learning.\nt0 = ()\ntrans_data = (\n    (n_neighbors=n_neighbors, n_components=2)\n    .fit_transform(sphere_data)\n    .T\n)\nt1 = ()\nprint(\"%s: %.2g sec\" % (\"ISO\", t1 - t0))\n\nax = fig.add_subplot(257)\n(trans_data[0], trans_data[1], c=colors, cmap=plt.cm.rainbow)\n(\"%s (%.2g sec)\" % (\"Isomap\", t1 - t0))\nax.xaxis.set_major_formatter(())\nax.yaxis.set_major_formatter(())\n(\"tight\")\n\n# Perform Multi-dimensional scaling.\nt0 = ()\nmds = (2, max_iter=100, n_init=1, normalized_stress=\"auto\")\ntrans_data = mds.fit_transform(sphere_data).T\nt1 = ()\nprint(\"MDS: %.2g sec\" % (t1 - t0))\n\nax = fig.add_subplot(258)\n(trans_data[0], trans_data[1], c=colors, cmap=plt.cm.rainbow)\n(\"MDS (%.2g sec)\" % (t1 - t0))\nax.xaxis.set_major_formatter(())\nax.yaxis.set_major_formatter(())\n(\"tight\")\n\n# Perform Spectral Embedding.\nt0 = ()\nse = (n_components=2, n_neighbors=n_neighbors)\ntrans_data = se.fit_transform(sphere_data).T\nt1 = ()\nprint(\"Spectral Embedding: %.2g sec\" % (t1 - t0))\n\nax = fig.add_subplot(259)\n(trans_data[0], trans_data[1], c=colors, cmap=plt.cm.rainbow)\n(\"Spectral Embedding (%.2g sec)\" % (t1 - t0))\nax.xaxis.set_major_formatter(())\nax.yaxis.set_major_formatter(())\n(\"tight\")\n\n# Perform t-distributed stochastic neighbor embedding.\nt0 = ()\ntsne = (n_components=2, random_state=0)\ntrans_data = tsne.fit_transform(sphere_data).T\nt1 = ()\nprint(\"t-SNE: %.2g sec\" % (t1 - t0))\n\nax = fig.add_subplot(2, 5, 10)\n(trans_data[0], trans_data[1], c=colors, cmap=plt.cm.rainbow)\n(\"t-SNE (%.2g sec)\" % (t1 - t0))\nax.xaxis.set_major_formatter(())\nax.yaxis.set_major_formatter(())\n(\"tight\")\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  5.885 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Manifold learning->Multi-dimensional scaling": [
        [
            "An illustration of the metric and non-metric MDS on generated noisy data.",
            "markdown"
        ],
        [
            "The reconstructed points using the metric MDS and non metric MDS are slightly\nshifted to avoid overlapping.\n<img alt=\"plot mds\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_mds_001.png\" srcset=\"../../_images/sphx_glr_plot_mds_001.png\"/>",
            "markdown"
        ],
        [
            "# Author: Nelle Varoquaux &lt;nelle.varoquaux@gmail.com\n# License: BSD\n\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib.collections import \n\nfrom sklearn import manifold\nfrom sklearn.metrics import euclidean_distances\nfrom sklearn.decomposition import \n\nEPSILON = ().eps\nn_samples = 20\nseed = (seed=3)\nX_true = seed.randint(0, 20, 2 * n_samples).astype(float)\nX_true = X_true.reshape((n_samples, 2))\n# Center the data\nX_true -= X_true.mean()\n\nsimilarities = euclidean_distances(X_true)\n\n# Add noise to the similarities\nnoise = (n_samples, n_samples)\nnoise = noise + noise.T\nnoise[(noise.shape[0]), (noise.shape[0])] = 0\nsimilarities += noise\n\nmds = (\n    n_components=2,\n    max_iter=3000,\n    eps=1e-9,\n    random_state=seed,\n    dissimilarity=\"precomputed\",\n    n_jobs=1,\n    normalized_stress=\"auto\",\n)\npos = mds.fit(similarities).embedding_\n\nnmds = (\n    n_components=2,\n    metric=False,\n    max_iter=3000,\n    eps=1e-12,\n    dissimilarity=\"precomputed\",\n    random_state=seed,\n    n_jobs=1,\n    n_init=1,\n    normalized_stress=\"auto\",\n)\nnpos = nmds.fit_transform(similarities, init=pos)\n\n# Rescale the data\npos *= ((X_true**2).sum()) / ((pos**2).sum())\nnpos *= ((X_true**2).sum()) / ((npos**2).sum())\n\n# Rotate the data\nclf = (n_components=2)\nX_true = clf.fit_transform(X_true)\n\npos = clf.fit_transform(pos)\n\nnpos = clf.fit_transform(npos)\n\nfig = (1)\nax = ([0.0, 0.0, 1.0, 1.0])\n\ns = 100\n(X_true[:, 0], X_true[:, 1], color=\"navy\", s=s, lw=0, label=\"True Position\")\n(pos[:, 0], pos[:, 1], color=\"turquoise\", s=s, lw=0, label=\"MDS\")\n(npos[:, 0], npos[:, 1], color=\"darkorange\", s=s, lw=0, label=\"NMDS\")\n(scatterpoints=1, loc=\"best\", shadow=False)\n\nsimilarities = similarities.max() / (similarities + EPSILON) * 100\n(similarities, 0)\n# Plot the edges\nstart_idx, end_idx = (pos)\n# a sequence of (*line0*, *line1*, *line2*), where::\n#            linen = (x0, y0), (x1, y1), ... (xm, ym)\nsegments = [\n    [X_true[i, :], X_true[j, :]] for i in range(len(pos)) for j in range(len(pos))\n]\nvalues = np.abs(similarities)\nlc = (\n    segments, zorder=0, cmap=plt.cm.Blues, norm=plt.Normalize(0, values.max())\n)\nlc.set_array(similarities.flatten())\nlc.set_linewidths((len(segments), 0.5))\nax.add_collection(lc)\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.193 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Manifold learning->Swiss Roll And Swiss-Hole Reduction": [
        [
            "This notebook seeks to compare two popular non-linear dimensionality\ntechniques, T-distributed Stochastic Neighbor Embedding (t-SNE) and\nLocally Linear Embedding (LLE), on the classic Swiss Roll dataset.\nThen, we will explore how they both deal with the addition of a hole\nin the data.",
            "markdown"
        ]
    ],
    "Examples->Manifold learning->Swiss Roll And Swiss-Hole Reduction->Swiss Roll": [
        [
            "We start by generating the Swiss Roll dataset.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\nfrom sklearn import manifold, datasets\n\n\nsr_points, sr_color = (n_samples=1500, random_state=0)",
            "code"
        ],
        [
            "Now, let\u2019s take a look at our data:",
            "markdown"
        ],
        [
            "fig = (figsize=(8, 6))\nax = fig.add_subplot(111, projection=\"3d\")\nfig.add_axes(ax)\nax.scatter(\n    sr_points[:, 0], sr_points[:, 1], sr_points[:, 2], c=sr_color, s=50, alpha=0.8\n)\nax.set_title(\"Swiss Roll in Ambient Space\")\nax.view_init(azim=-66, elev=12)\n_ = ax.text2D(0.8, 0.05, s=\"n_samples=1500\", transform=ax.transAxes)\n\n\n<img alt=\"Swiss Roll in Ambient Space\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_swissroll_001.png\" srcset=\"../../_images/sphx_glr_plot_swissroll_001.png\"/>",
            "code"
        ],
        [
            "Computing the LLE and t-SNE embeddings, we find that LLE seems to unroll the\nSwiss Roll pretty effectively. t-SNE on the other hand, is able\nto preserve the general structure of the data, but, poorly represents the\ncontinuous nature of our original data. Instead, it seems to unnecessarily\nclump sections of points together.",
            "markdown"
        ],
        [
            "sr_lle, sr_err = (\n    sr_points, n_neighbors=12, n_components=2\n)\n\nsr_tsne = (n_components=2, perplexity=40, random_state=0).fit_transform(\n    sr_points\n)\n\nfig, axs = (figsize=(8, 8), nrows=2)\naxs[0].scatter(sr_lle[:, 0], sr_lle[:, 1], c=sr_color)\naxs[0].set_title(\"LLE Embedding of Swiss Roll\")\naxs[1].scatter(sr_tsne[:, 0], sr_tsne[:, 1], c=sr_color)\n_ = axs[1].set_title(\"t-SNE Embedding of Swiss Roll\")\n\n\n<img alt=\"LLE Embedding of Swiss Roll, t-SNE Embedding of Swiss Roll\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_swissroll_002.png\" srcset=\"../../_images/sphx_glr_plot_swissroll_002.png\"/>",
            "code"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "LLE seems to be stretching the points from the center (purple)\nof the swiss roll. However, we observe that this is simply a byproduct\nof how the data was generated. There is a higher density of points near the\ncenter of the roll, which ultimately affects how LLE reconstructs the\ndata in a lower dimension.",
            "markdown"
        ]
    ],
    "Examples->Manifold learning->Swiss Roll And Swiss-Hole Reduction->Swiss-Hole": [
        [
            "Now let\u2019s take a look at how both algorithms deal with us adding a hole to\nthe data. First, we generate the Swiss-Hole dataset and plot it:",
            "markdown"
        ],
        [
            "sh_points, sh_color = (\n    n_samples=1500, hole=True, random_state=0\n)\n\nfig = (figsize=(8, 6))\nax = fig.add_subplot(111, projection=\"3d\")\nfig.add_axes(ax)\nax.scatter(\n    sh_points[:, 0], sh_points[:, 1], sh_points[:, 2], c=sh_color, s=50, alpha=0.8\n)\nax.set_title(\"Swiss-Hole in Ambient Space\")\nax.view_init(azim=-66, elev=12)\n_ = ax.text2D(0.8, 0.05, s=\"n_samples=1500\", transform=ax.transAxes)\n\n\n<img alt=\"Swiss-Hole in Ambient Space\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_swissroll_003.png\" srcset=\"../../_images/sphx_glr_plot_swissroll_003.png\"/>",
            "code"
        ],
        [
            "Computing the LLE and t-SNE embeddings, we obtain similar results to the\nSwiss Roll. LLE very capably unrolls the data and even preserves\nthe hole. t-SNE, again seems to clump sections of points together, but, we\nnote that it preserves the general topology of the original data.",
            "markdown"
        ],
        [
            "sh_lle, sh_err = (\n    sh_points, n_neighbors=12, n_components=2\n)\n\nsh_tsne = (\n    n_components=2, perplexity=40, init=\"random\", random_state=0\n).fit_transform(sh_points)\n\nfig, axs = (figsize=(8, 8), nrows=2)\naxs[0].scatter(sh_lle[:, 0], sh_lle[:, 1], c=sh_color)\naxs[0].set_title(\"LLE Embedding of Swiss-Hole\")\naxs[1].scatter(sh_tsne[:, 0], sh_tsne[:, 1], c=sh_color)\n_ = axs[1].set_title(\"t-SNE Embedding of Swiss-Hole\")\n\n\n<img alt=\"LLE Embedding of Swiss-Hole, t-SNE Embedding of Swiss-Hole\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_swissroll_004.png\" srcset=\"../../_images/sphx_glr_plot_swissroll_004.png\"/>",
            "code"
        ]
    ],
    "Examples->Manifold learning->Swiss Roll And Swiss-Hole Reduction->Concluding remarks": [
        [
            "We note that t-SNE benefits from testing more combinations of parameters.\nBetter results could probably have been obtained by better tuning these\nparameters.",
            "markdown"
        ],
        [
            "We observe that, as seen in the \u201cManifold learning on\nhandwritten digits\u201d example, t-SNE generally performs better than LLE\non real world data.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  18.659 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Manifold learning->t-SNE: The effect of various perplexity values on the shape": [
        [
            "An illustration of t-SNE on the two concentric circles and the S-curve\ndatasets for different perplexity values.",
            "markdown"
        ],
        [
            "We observe a tendency towards clearer shapes as the perplexity value increases.",
            "markdown"
        ],
        [
            "The size, the distance and the shape of clusters may vary upon initialization,\nperplexity values and does not always convey a meaning.",
            "markdown"
        ],
        [
            "As shown below, t-SNE for higher perplexities finds meaningful topology of\ntwo concentric circles, however the size and the distance of the circles varies\nslightly from the original. Contrary to the two circles dataset, the shapes\nvisually diverge from S-curve topology on the S-curve dataset even for\nlarger perplexity values.",
            "markdown"
        ],
        [
            "For further details, \u201cHow to Use t-SNE Effectively\u201d\n provides a good discussion of the\neffects of various parameters, as well as interactive plots to explore\nthose effects.\n<img alt=\"Perplexity=5, Perplexity=30, Perplexity=50, Perplexity=100, Perplexity=5, Perplexity=30, Perplexity=50, Perplexity=100, Perplexity=5, Perplexity=30, Perplexity=50, Perplexity=100\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_t_sne_perplexity_001.png\" srcset=\"../../_images/sphx_glr_plot_t_sne_perplexity_001.png\"/>",
            "markdown"
        ],
        [
            "circles, perplexity=5 in 0.15 sec\ncircles, perplexity=30 in 0.23 sec\ncircles, perplexity=50 in 0.26 sec\ncircles, perplexity=100 in 0.26 sec\nS-curve, perplexity=5 in 0.18 sec\nS-curve, perplexity=30 in 0.26 sec\nS-curve, perplexity=50 in 0.32 sec\nS-curve, perplexity=100 in 0.29 sec\nuniform grid, perplexity=5 in 0.19 sec\nuniform grid, perplexity=30 in 0.28 sec\nuniform grid, perplexity=50 in 0.3 sec\nuniform grid, perplexity=100 in 0.3 sec\n\n\n\n<br/>",
            "code"
        ],
        [
            "# Author: Narine Kokhlikyan &lt;narine@slice.com\n# License: BSD\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom matplotlib.ticker import \nfrom sklearn import manifold, datasets\nfrom time import \n\nn_samples = 150\nn_components = 2\n(fig, subplots) = (3, 5, figsize=(15, 8))\nperplexities = [5, 30, 50, 100]\n\nX, y = (\n    n_samples=n_samples, factor=0.5, noise=0.05, random_state=0\n)\n\nred = y == 0\ngreen = y == 1\n\nax = subplots[0][0]\nax.scatter(X[red, 0], X[red, 1], c=\"r\")\nax.scatter(X[green, 0], X[green, 1], c=\"g\")\nax.xaxis.set_major_formatter(())\nax.yaxis.set_major_formatter(())\n(\"tight\")\n\nfor i, perplexity in enumerate(perplexities):\n    ax = subplots[0][i + 1]\n\n    t0 = ()\n    tsne = (\n        n_components=n_components,\n        init=\"random\",\n        random_state=0,\n        perplexity=perplexity,\n        n_iter=300,\n    )\n    Y = tsne.fit_transform(X)\n    t1 = ()\n    print(\"circles, perplexity=%d in %.2g sec\" % (perplexity, t1 - t0))\n    ax.set_title(\"Perplexity=%d\" % perplexity)\n    ax.scatter(Y[red, 0], Y[red, 1], c=\"r\")\n    ax.scatter(Y[green, 0], Y[green, 1], c=\"g\")\n    ax.xaxis.set_major_formatter(())\n    ax.yaxis.set_major_formatter(())\n    ax.axis(\"tight\")\n\n# Another example using s-curve\nX, color = (n_samples, random_state=0)\n\nax = subplots[1][0]\nax.scatter(X[:, 0], X[:, 2], c=color)\nax.xaxis.set_major_formatter(())\nax.yaxis.set_major_formatter(())\n\nfor i, perplexity in enumerate(perplexities):\n    ax = subplots[1][i + 1]\n\n    t0 = ()\n    tsne = (\n        n_components=n_components,\n        init=\"random\",\n        random_state=0,\n        perplexity=perplexity,\n        learning_rate=\"auto\",\n        n_iter=300,\n    )\n    Y = tsne.fit_transform(X)\n    t1 = ()\n    print(\"S-curve, perplexity=%d in %.2g sec\" % (perplexity, t1 - t0))\n\n    ax.set_title(\"Perplexity=%d\" % perplexity)\n    ax.scatter(Y[:, 0], Y[:, 1], c=color)\n    ax.xaxis.set_major_formatter(())\n    ax.yaxis.set_major_formatter(())\n    ax.axis(\"tight\")\n\n\n# Another example using a 2D uniform grid\nx = (0, 1, int((n_samples)))\nxx, yy = (x, x)\nX = (\n    [\n        xx.ravel().reshape(-1, 1),\n        yy.ravel().reshape(-1, 1),\n    ]\n)\ncolor = xx.ravel()\nax = subplots[2][0]\nax.scatter(X[:, 0], X[:, 1], c=color)\nax.xaxis.set_major_formatter(())\nax.yaxis.set_major_formatter(())\n\nfor i, perplexity in enumerate(perplexities):\n    ax = subplots[2][i + 1]\n\n    t0 = ()\n    tsne = (\n        n_components=n_components,\n        init=\"random\",\n        random_state=0,\n        perplexity=perplexity,\n        n_iter=400,\n    )\n    Y = tsne.fit_transform(X)\n    t1 = ()\n    print(\"uniform grid, perplexity=%d in %.2g sec\" % (perplexity, t1 - t0))\n\n    ax.set_title(\"Perplexity=%d\" % perplexity)\n    ax.scatter(Y[:, 0], Y[:, 1], c=color)\n    ax.xaxis.set_major_formatter(())\n    ax.yaxis.set_major_formatter(())\n    ax.axis(\"tight\")\n\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  3.599 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Miscellaneous->Advanced Plotting With Partial Dependence": [
        [
            "The  object can be used\nfor plotting without needing to recalculate the partial dependence. In this\nexample, we show how to plot partial dependence plots and how to quickly\ncustomize the plot with the visualization API.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "See also",
            "markdown"
        ],
        [
            "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import \nfrom sklearn.neural_network import \nfrom sklearn.preprocessing import \nfrom sklearn.pipeline import \nfrom sklearn.tree import \nfrom sklearn.inspection import PartialDependenceDisplay",
            "code"
        ]
    ],
    "Examples->Miscellaneous->Advanced Plotting With Partial Dependence->Train models on the diabetes dataset": [
        [
            "First, we train a decision tree and a multi-layer perceptron on the diabetes\ndataset.",
            "markdown"
        ],
        [
            "diabetes = ()\nX = (diabetes.data, columns=diabetes.feature_names)\ny = diabetes.target\n\ntree = ()\nmlp = (\n    (),\n    (hidden_layer_sizes=(100, 100), tol=1e-2, max_iter=500, random_state=0),\n)\ntree.fit(X, y)\nmlp.fit(X, y)",
            "code"
        ],
        [
            "Pipeline(steps=[('standardscaler', StandardScaler()),\n                ('mlpregressor',\n                 MLPRegressor(hidden_layer_sizes=(100, 100), max_iter=500,\n                              random_state=0, tol=0.01))])<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-169\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-169\">Pipeline</label>",
            "code"
        ],
        [
            "Pipeline(steps=[('standardscaler', StandardScaler()),\n                ('mlpregressor',\n                 MLPRegressor(hidden_layer_sizes=(100, 100), max_iter=500,\n                              random_state=0, tol=0.01))])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-170\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-170\">StandardScaler</label>",
            "code"
        ],
        [
            "StandardScaler()<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-171\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-171\">MLPRegressor</label>",
            "code"
        ],
        [
            "MLPRegressor(hidden_layer_sizes=(100, 100), max_iter=500, random_state=0,\n             tol=0.01)\n\n<br/>\n<br/>",
            "code"
        ]
    ],
    "Examples->Miscellaneous->Advanced Plotting With Partial Dependence->Plotting partial dependence for two features": [
        [
            "We plot partial dependence curves for features \u201cage\u201d and \u201cbmi\u201d (body mass\nindex) for the decision tree. With two features,\n expects to plot\ntwo curves. Here the plot function place a grid of two plots using the space\ndefined by ax .",
            "markdown"
        ],
        [
            "fig, ax = (figsize=(12, 6))\nax.set_title(\"Decision Tree\")\ntree_disp = (tree, X, [\"age\", \"bmi\"], ax=ax)\n\n\n<img alt=\"Decision Tree\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_partial_dependence_visualization_api_001.png\" srcset=\"../../_images/sphx_glr_plot_partial_dependence_visualization_api_001.png\"/>",
            "code"
        ],
        [
            "The partial dependence curves can be plotted for the multi-layer perceptron.\nIn this case, line_kw is passed to\n to change the\ncolor of the curve.",
            "markdown"
        ],
        [
            "fig, ax = (figsize=(12, 6))\nax.set_title(\"Multi-layer Perceptron\")\nmlp_disp = (\n    mlp, X, [\"age\", \"bmi\"], ax=ax, line_kw={\"color\": \"red\"}\n)\n\n\n<img alt=\"Multi-layer Perceptron\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_partial_dependence_visualization_api_002.png\" srcset=\"../../_images/sphx_glr_plot_partial_dependence_visualization_api_002.png\"/>",
            "code"
        ]
    ],
    "Examples->Miscellaneous->Advanced Plotting With Partial Dependence->Plotting partial dependence of the two models together": [
        [
            "The tree_disp and mlp_disp\n objects contain all the\ncomputed information needed to recreate the partial dependence curves. This\nmeans we can easily create additional plots without needing to recompute the\ncurves.",
            "markdown"
        ],
        [
            "One way to plot the curves is to place them in the same figure, with the\ncurves of each model on each row. First, we create a figure with two axes\nwithin two rows and one column. The two axes are passed to the\n functions of\ntree_disp and mlp_disp. The given axes will be used by the plotting\nfunction to draw the partial dependence. The resulting plot places the\ndecision tree partial dependence curves in the first row of the\nmulti-layer perceptron in the second row.",
            "markdown"
        ],
        [
            "fig, (ax1, ax2) = (2, 1, figsize=(10, 10))\ntree_disp.plot(ax=ax1)\nax1.set_title(\"Decision Tree\")\nmlp_disp.plot(ax=ax2, line_kw={\"color\": \"red\"})\nax2.set_title(\"Multi-layer Perceptron\")\n\n\n<img alt=\"Decision Tree, Multi-layer Perceptron\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_partial_dependence_visualization_api_003.png\" srcset=\"../../_images/sphx_glr_plot_partial_dependence_visualization_api_003.png\"/>",
            "code"
        ],
        [
            "Text(0.5, 1.0, 'Multi-layer Perceptron')",
            "code"
        ],
        [
            "Another way to compare the curves is to plot them on top of each other. Here,\nwe create a figure with one row and two columns. The axes are passed into the\n function as a list,\nwhich will plot the partial dependence curves of each model on the same axes.\nThe length of the axes list must be equal to the number of plots drawn.",
            "markdown"
        ],
        [
            "fig, (ax1, ax2) = (1, 2, figsize=(10, 6))\ntree_disp.plot(ax=[ax1, ax2], line_kw={\"label\": \"Decision Tree\"})\nmlp_disp.plot(\n    ax=[ax1, ax2], line_kw={\"label\": \"Multi-layer Perceptron\", \"color\": \"red\"}\n)\nax1.legend()\nax2.legend()\n\n\n<img alt=\"plot partial dependence visualization api\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_partial_dependence_visualization_api_004.png\" srcset=\"../../_images/sphx_glr_plot_partial_dependence_visualization_api_004.png\"/>",
            "code"
        ],
        [
            "&lt;matplotlib.legend.Legend object at 0x7f0d0a833190",
            "code"
        ],
        [
            "tree_disp.axes_ is a numpy array container the axes used to draw the\npartial dependence plots. This can be passed to mlp_disp to have the same\naffect of drawing the plots on top of each other. Furthermore, the\nmlp_disp.figure_ stores the figure, which allows for resizing the figure\nafter calling plot. In this case tree_disp.axes_ has two dimensions, thus\nplot will only show the y label and y ticks on the left most plot.",
            "markdown"
        ],
        [
            "tree_disp.plot(line_kw={\"label\": \"Decision Tree\"})\nmlp_disp.plot(\n    line_kw={\"label\": \"Multi-layer Perceptron\", \"color\": \"red\"}, ax=tree_disp.axes_\n)\ntree_disp.figure_.set_size_inches(10, 6)\ntree_disp.axes_[0, 0].legend()\ntree_disp.axes_[0, 1].legend()\n()\n\n\n<img alt=\"plot partial dependence visualization api\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_partial_dependence_visualization_api_005.png\" srcset=\"../../_images/sphx_glr_plot_partial_dependence_visualization_api_005.png\"/>",
            "code"
        ]
    ],
    "Examples->Miscellaneous->Advanced Plotting With Partial Dependence->Plotting partial dependence for one feature": [
        [
            "Here, we plot the partial dependence curves for a single feature, \u201cage\u201d, on\nthe same axes. In this case, tree_disp.axes_ is passed into the second\nplot function.",
            "markdown"
        ],
        [
            "tree_disp = (tree, X, [\"age\"])\nmlp_disp = (\n    mlp, X, [\"age\"], ax=tree_disp.axes_, line_kw={\"color\": \"red\"}\n)\n\n\n<img alt=\"plot partial dependence visualization api\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_partial_dependence_visualization_api_006.png\" srcset=\"../../_images/sphx_glr_plot_partial_dependence_visualization_api_006.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  2.524 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Miscellaneous->Comparing anomaly detection algorithms for outlier detection on toy datasets": [
        [
            "This example shows characteristics of different anomaly detection algorithms\non 2D datasets. Datasets contain one or two modes (regions of high density)\nto illustrate the ability of algorithms to cope with multimodal data.",
            "markdown"
        ],
        [
            "For each dataset, 15% of samples are generated as random uniform noise. This\nproportion is the value given to the nu parameter of the OneClassSVM and the\ncontamination parameter of the other outlier detection algorithms.\nDecision boundaries between inliers and outliers are displayed in black\nexcept for Local Outlier Factor (LOF) as it has no predict method to be applied\non new data when it is used for outlier detection.",
            "markdown"
        ],
        [
            "The  is known to be sensitive to outliers and\nthus does not perform very well for outlier detection. This estimator is best\nsuited for novelty detection when the training set is not contaminated by\noutliers. That said, outlier detection in high-dimension, or without any\nassumptions on the distribution of the inlying data is very challenging, and a\nOne-class SVM might give useful results in these situations depending on the\nvalue of its hyperparameters.",
            "markdown"
        ],
        [
            "The  is an implementation of the\nOne-Class SVM based on stochastic gradient descent (SGD). Combined with kernel\napproximation, this estimator can be used to approximate the solution\nof a kernelized . We note that, although not\nidentical, the decision boundaries of the\n and the ones of\n are very similar. The main advantage of using\n is that it scales linearly with\nthe number of samples.",
            "markdown"
        ],
        [
            "assumes the data is Gaussian and\nlearns an ellipse. It thus degrades when the data is not unimodal. Notice\nhowever that this estimator is robust to outliers.",
            "markdown"
        ],
        [
            "and\n seem to perform reasonably well\nfor multi-modal data sets. The advantage of\n over the other estimators is\nshown for the third data set, where the two modes have different densities.\nThis advantage is explained by the local aspect of LOF, meaning that it only\ncompares the score of abnormality of one sample with the scores of its\nneighbors.",
            "markdown"
        ],
        [
            "Finally, for the last data set, it is hard to say that one sample is more\nabnormal than another sample as they are uniformly distributed in a\nhypercube. Except for the  which overfits a\nlittle, all estimators present decent solutions for this situation. In such a\ncase, it would be wise to look more closely at the scores of abnormality of\nthe samples as a good estimator should assign similar scores to all the\nsamples.",
            "markdown"
        ],
        [
            "While these examples give some intuition about the algorithms, this\nintuition might not apply to very high dimensional data.",
            "markdown"
        ],
        [
            "Finally, note that parameters of the models have been here handpicked but\nthat in practice they need to be adjusted. In the absence of labelled data,\nthe problem is completely unsupervised so model selection can be a challenge.\n<img alt=\"Robust covariance, One-Class SVM, One-Class SVM (SGD), Isolation Forest, Local Outlier Factor\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_anomaly_comparison_001.png\" srcset=\"../../_images/sphx_glr_plot_anomaly_comparison_001.png\"/>",
            "markdown"
        ],
        [
            "# Author: Alexandre Gramfort &lt;alexandre.gramfort@inria.fr\n#         Albert Thomas &lt;albert.thomas@telecom-paristech.fr\n# License: BSD 3 clause\n\nimport time\n\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nfrom sklearn import svm\nfrom sklearn.datasets import , \nfrom sklearn.covariance import \nfrom sklearn.ensemble import \nfrom sklearn.neighbors import \nfrom sklearn.linear_model import \nfrom sklearn.kernel_approximation import \nfrom sklearn.pipeline import \n\n[\"contour.negative_linestyle\"] = \"solid\"\n\n# Example settings\nn_samples = 300\noutliers_fraction = 0.15\nn_outliers = int(outliers_fraction * n_samples)\nn_inliers = n_samples - n_outliers\n\n# define outlier/anomaly detection methods to be compared.\n# the SGDOneClassSVM must be used in a pipeline with a kernel approximation\n# to give similar results to the OneClassSVM\nanomaly_algorithms = [\n    (\n        \"Robust covariance\",\n        (contamination=outliers_fraction, random_state=42),\n    ),\n    (\"One-Class SVM\", (nu=outliers_fraction, kernel=\"rbf\", gamma=0.1)),\n    (\n        \"One-Class SVM (SGD)\",\n        (\n            (gamma=0.1, random_state=42, n_components=150),\n            (\n                nu=outliers_fraction,\n                shuffle=True,\n                fit_intercept=True,\n                random_state=42,\n                tol=1e-6,\n            ),\n        ),\n    ),\n    (\n        \"Isolation Forest\",\n        (contamination=outliers_fraction, random_state=42),\n    ),\n    (\n        \"Local Outlier Factor\",\n        (n_neighbors=35, contamination=outliers_fraction),\n    ),\n]\n\n# Define datasets\nblobs_params = dict(random_state=0, n_samples=n_inliers, n_features=2)\ndatasets = [\n    (centers=[[0, 0], [0, 0]], cluster_std=0.5, **blobs_params)[0],\n    (centers=[[2, 2], [-2, -2]], cluster_std=[0.5, 0.5], **blobs_params)[0],\n    (centers=[[2, 2], [-2, -2]], cluster_std=[1.5, 0.3], **blobs_params)[0],\n    4.0\n    * (\n        (n_samples=n_samples, noise=0.05, random_state=0)[0]\n        - ([0.5, 0.25])\n    ),\n    14.0 * ((42).rand(n_samples, 2) - 0.5),\n]\n\n# Compare given classifiers under given settings\nxx, yy = ((-7, 7, 150), (-7, 7, 150))\n\n(figsize=(len(anomaly_algorithms) * 2 + 4, 12.5))\n(\n    left=0.02, right=0.98, bottom=0.001, top=0.96, wspace=0.05, hspace=0.01\n)\n\nplot_num = 1\nrng = (42)\n\nfor i_dataset, X in enumerate(datasets):\n    # Add outliers\n    X = ([X, rng.uniform(low=-6, high=6, size=(n_outliers, 2))], axis=0)\n\n    for name, algorithm in anomaly_algorithms:\n        t0 = ()\n        algorithm.fit(X)\n        t1 = ()\n        (len(datasets), len(anomaly_algorithms), plot_num)\n        if i_dataset == 0:\n            (name, size=18)\n\n        # fit the data and tag outliers\n        if name == \"Local Outlier Factor\":\n            y_pred = algorithm.fit_predict(X)\n        else:\n            y_pred = algorithm.fit(X).predict(X)\n\n        # plot the levels lines and the points\n        if name != \"Local Outlier Factor\":  # LOF does not implement predict\n            Z = algorithm.predict([xx.ravel(), yy.ravel()])\n            Z = Z.reshape(xx.shape)\n            (xx, yy, Z, levels=[0], linewidths=2, colors=\"black\")\n\n        colors = ([\"#377eb8\", \"#ff7f00\"])\n        (X[:, 0], X[:, 1], s=10, color=colors[(y_pred + 1) // 2])\n\n        (-7, 7)\n        (-7, 7)\n        (())\n        (())\n        (\n            0.99,\n            0.01,\n            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n            transform=().transAxes,\n            size=15,\n            horizontalalignment=\"right\",\n        )\n        plot_num += 1\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  4.681 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Miscellaneous->Comparison of kernel ridge regression and SVR": [
        [
            "Both kernel ridge regression (KRR) and SVR learn a non-linear function by\nemploying the kernel trick, i.e., they learn a linear function in the space\ninduced by the respective kernel which corresponds to a non-linear function in\nthe original space. They differ in the loss functions (ridge versus\nepsilon-insensitive loss). In contrast to SVR, fitting a KRR can be done in\nclosed-form and is typically faster for medium-sized datasets. On the other\nhand, the learned model is non-sparse and thus slower than SVR at\nprediction-time.",
            "markdown"
        ],
        [
            "This example illustrates both methods on an artificial dataset, which\nconsists of a sinusoidal target function and strong noise added to every fifth\ndatapoint.",
            "markdown"
        ],
        [
            "Authors: Jan Hendrik Metzen &lt;>\nLicense: BSD 3 clause",
            "markdown"
        ]
    ],
    "Examples->Miscellaneous->Comparison of kernel ridge regression and SVR->Generate sample data": [
        [
            "import numpy as np\n\nrng = (42)\n\nX = 5 * rng.rand(10000, 1)\ny = (X).ravel()\n\n# Add noise to targets\ny[::5] += 3 * (0.5 - rng.rand(X.shape[0] // 5))\n\nX_plot = (0, 5, 100000)[:, None]",
            "code"
        ]
    ],
    "Examples->Miscellaneous->Comparison of kernel ridge regression and SVR->Construct the kernel-based regression models": [
        [
            "from sklearn.model_selection import \nfrom sklearn.svm import \nfrom sklearn.kernel_ridge import \n\ntrain_size = 100\n\nsvr = (\n    (kernel=\"rbf\", gamma=0.1),\n    param_grid={\"C\": [1e0, 1e1, 1e2, 1e3], \"gamma\": (-2, 2, 5)},\n)\n\nkr = (\n    (kernel=\"rbf\", gamma=0.1),\n    param_grid={\"alpha\": [1e0, 0.1, 1e-2, 1e-3], \"gamma\": (-2, 2, 5)},\n)",
            "code"
        ]
    ],
    "Examples->Miscellaneous->Comparison of kernel ridge regression and SVR->Compare times of SVR and Kernel Ridge Regression": [
        [
            "import time\n\nt0 = ()\nsvr.fit(X[:train_size], y[:train_size])\nsvr_fit = () - t0\nprint(f\"Best SVR with params: {svr.best_params_} and R2 score: {svr.best_score_:.3f}\")\nprint(\"SVR complexity and bandwidth selected and model fitted in %.3f s\" % svr_fit)\n\nt0 = ()\nkr.fit(X[:train_size], y[:train_size])\nkr_fit = () - t0\nprint(f\"Best KRR with params: {kr.best_params_} and R2 score: {kr.best_score_:.3f}\")\nprint(\"KRR complexity and bandwidth selected and model fitted in %.3f s\" % kr_fit)\n\nsv_ratio = svr.best_estimator_.support_.shape[0] / train_size\nprint(\"Support vector ratio: %.3f\" % sv_ratio)\n\nt0 = ()\ny_svr = svr.predict(X_plot)\nsvr_predict = () - t0\nprint(\"SVR prediction for %d inputs in %.3f s\" % (X_plot.shape[0], svr_predict))\n\nt0 = ()\ny_kr = kr.predict(X_plot)\nkr_predict = () - t0\nprint(\"KRR prediction for %d inputs in %.3f s\" % (X_plot.shape[0], kr_predict))",
            "code"
        ],
        [
            "Best SVR with params: {'C': 1.0, 'gamma': 0.09999999999999999} and R2 score: 0.737\nSVR complexity and bandwidth selected and model fitted in 0.576 s\nBest KRR with params: {'alpha': 0.1, 'gamma': 0.09999999999999999} and R2 score: 0.723\nKRR complexity and bandwidth selected and model fitted in 0.159 s\nSupport vector ratio: 0.340\nSVR prediction for 100000 inputs in 0.118 s\nKRR prediction for 100000 inputs in 0.096 s",
            "code"
        ]
    ],
    "Examples->Miscellaneous->Comparison of kernel ridge regression and SVR->Look at the results": [
        [
            "import matplotlib.pyplot as plt\n\nsv_ind = svr.best_estimator_.support_\n(\n    X[sv_ind],\n    y[sv_ind],\n    c=\"r\",\n    s=50,\n    label=\"SVR support vectors\",\n    zorder=2,\n    edgecolors=(0, 0, 0),\n)\n(X[:100], y[:100], c=\"k\", label=\"data\", zorder=1, edgecolors=(0, 0, 0))\n(\n    X_plot,\n    y_svr,\n    c=\"r\",\n    label=\"SVR (fit: %.3fs, predict: %.3fs)\" % (svr_fit, svr_predict),\n)\n(\n    X_plot, y_kr, c=\"g\", label=\"KRR (fit: %.3fs, predict: %.3fs)\" % (kr_fit, kr_predict)\n)\n(\"data\")\n(\"target\")\n(\"SVR versus Kernel Ridge\")\n_ = ()\n\n\n<img alt=\"SVR versus Kernel Ridge\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_kernel_ridge_regression_001.png\" srcset=\"../../_images/sphx_glr_plot_kernel_ridge_regression_001.png\"/>",
            "code"
        ],
        [
            "The previous figure compares the learned model of KRR and SVR when both\ncomplexity/regularization and bandwidth of the RBF kernel are optimized using\ngrid-search. The learned functions are very similar; however, fitting KRR is\napproximatively 3-4 times faster than fitting SVR (both with grid-search).",
            "markdown"
        ],
        [
            "Prediction of 100000 target values could be in theory approximately three\ntimes faster with SVR since it has learned a sparse model using only\napproximately 1/3 of the training datapoints as support vectors. However, in\npractice, this is not necessarily the case because of implementation details\nin the way the kernel function is computed for each model that can make the\nKRR model as fast or even faster despite computing more arithmetic\noperations.",
            "markdown"
        ]
    ],
    "Examples->Miscellaneous->Comparison of kernel ridge regression and SVR->Visualize training and prediction times": [
        [
            "()\n\nsizes = (1, 3.8, 7).astype(int)\nfor name, estimator in {\n    \"KRR\": (kernel=\"rbf\", alpha=0.01, gamma=10),\n    \"SVR\": (kernel=\"rbf\", C=1e2, gamma=10),\n}.items():\n    train_time = []\n    test_time = []\n    for train_test_size in sizes:\n        t0 = ()\n        estimator.fit(X[:train_test_size], y[:train_test_size])\n        train_time.append(() - t0)\n\n        t0 = ()\n        estimator.predict(X_plot[:1000])\n        test_time.append(() - t0)\n\n    (\n        sizes,\n        train_time,\n        \"o-\",\n        color=\"r\" if name == \"SVR\" else \"g\",\n        label=\"%s (train)\" % name,\n    )\n    (\n        sizes,\n        test_time,\n        \"o--\",\n        color=\"r\" if name == \"SVR\" else \"g\",\n        label=\"%s (test)\" % name,\n    )\n\n(\"log\")\n(\"log\")\n(\"Train size\")\n(\"Time (seconds)\")\n(\"Execution Time\")\n_ = (loc=\"best\")\n\n\n<img alt=\"Execution Time\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_kernel_ridge_regression_002.png\" srcset=\"../../_images/sphx_glr_plot_kernel_ridge_regression_002.png\"/>",
            "code"
        ],
        [
            "This figure compares the time for fitting and prediction of KRR and SVR for\ndifferent sizes of the training set. Fitting KRR is faster than SVR for\nmedium-sized training sets (less than a few thousand samples); however, for\nlarger training sets SVR scales better. With regard to prediction time, SVR\nshould be faster than KRR for all sizes of the training set because of the\nlearned sparse solution, however this is not necessarily the case in practice\nbecause of implementation details. Note that the degree of sparsity and thus\nthe prediction time depends on the parameters epsilon and C of the SVR.",
            "markdown"
        ]
    ],
    "Examples->Miscellaneous->Comparison of kernel ridge regression and SVR->Visualize the learning curves": [
        [
            "from sklearn.model_selection import LearningCurveDisplay\n\n_, ax = ()\n\nsvr = (kernel=\"rbf\", C=1e1, gamma=0.1)\nkr = (kernel=\"rbf\", alpha=0.1, gamma=0.1)\n\ncommon_params = {\n    \"X\": X[:100],\n    \"y\": y[:100],\n    \"train_sizes\": (0.1, 1, 10),\n    \"scoring\": \"neg_mean_squared_error\",\n    \"negate_score\": True,\n    \"score_name\": \"Mean Squared Error\",\n    \"std_display_style\": None,\n    \"ax\": ax,\n}\n\n(svr, **common_params)\n(kr, **common_params)\nax.set_title(\"Learning curves\")\nax.legend(handles=ax.get_legend_handles_labels()[0], labels=[\"SVR\", \"KRR\"])\n\n()\n\n\n<img alt=\"Learning curves\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_kernel_ridge_regression_003.png\" srcset=\"../../_images/sphx_glr_plot_kernel_ridge_regression_003.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  9.324 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Miscellaneous->Displaying Pipelines": [
        [
            "The default configuration for displaying a pipeline in a Jupyter Notebook is\n'diagram' where set_config(display='diagram'). To deactivate HTML representation,\nuse set_config(display='text').",
            "markdown"
        ],
        [
            "To see more detailed steps in the visualization of the pipeline, click on the\nsteps in the pipeline.",
            "markdown"
        ]
    ],
    "Examples->Miscellaneous->Displaying Pipelines->Displaying a Pipeline with a Preprocessing Step and Classifier": [
        [
            "This section constructs a  with a preprocessing\nstep, , and classifier,\n, and displays its visual\nrepresentation.\n</blockquote>",
            "markdown"
        ],
        [
            "from sklearn.pipeline import \nfrom sklearn.preprocessing import \nfrom sklearn.linear_model import \nfrom sklearn import \n\nsteps = [\n    (\"preprocessing\", ()),\n    (\"classifier\", ()),\n]\npipe = (steps)",
            "code"
        ],
        [
            "To visualize the diagram, the default is display='diagram'.",
            "markdown"
        ],
        [
            "(display=\"diagram\")\npipe  # click on the diagram below to see the details of each step",
            "code"
        ],
        [
            "Pipeline(steps=[('preprocessing', StandardScaler()),\n                ('classifier', LogisticRegression())])<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-172\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-172\">Pipeline</label>",
            "code"
        ],
        [
            "Pipeline(steps=[('preprocessing', StandardScaler()),\n                ('classifier', LogisticRegression())])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-173\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-173\">StandardScaler</label>",
            "code"
        ],
        [
            "StandardScaler()<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-174\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-174\">LogisticRegression</label>",
            "code"
        ],
        [
            "LogisticRegression()\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "To view the text pipeline, change to display='text'.",
            "markdown"
        ],
        [
            "(display=\"text\")\npipe",
            "code"
        ],
        [
            "Pipeline(steps=[('preprocessing', StandardScaler()),\n                ('classifier', LogisticRegression())])",
            "code"
        ],
        [
            "Put back the default display",
            "markdown"
        ],
        [
            "(display=\"diagram\")",
            "code"
        ]
    ],
    "Examples->Miscellaneous->Displaying Pipelines->Displaying a Pipeline Chaining Multiple Preprocessing Steps & Classifier": [
        [
            "This section constructs a  with multiple\npreprocessing steps,  and\n, and a classifier step,\n, and displays its visual\nrepresentation.\n</blockquote>",
            "markdown"
        ],
        [
            "from sklearn.pipeline import \nfrom sklearn.preprocessing import , \nfrom sklearn.linear_model import \n\nsteps = [\n    (\"standard_scaler\", ()),\n    (\"polynomial\", (degree=3)),\n    (\"classifier\", (C=2.0)),\n]\npipe = (steps)\npipe  # click on the diagram below to see the details of each step",
            "code"
        ],
        [
            "Pipeline(steps=[('standard_scaler', StandardScaler()),\n                ('polynomial', PolynomialFeatures(degree=3)),\n                ('classifier', LogisticRegression(C=2.0))])<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-175\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-175\">Pipeline</label>",
            "code"
        ],
        [
            "Pipeline(steps=[('standard_scaler', StandardScaler()),\n                ('polynomial', PolynomialFeatures(degree=3)),\n                ('classifier', LogisticRegression(C=2.0))])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-176\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-176\">StandardScaler</label>",
            "code"
        ],
        [
            "StandardScaler()<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-177\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-177\">PolynomialFeatures</label>",
            "code"
        ],
        [
            "PolynomialFeatures(degree=3)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-178\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-178\">LogisticRegression</label>",
            "code"
        ],
        [
            "LogisticRegression(C=2.0)\n\n<br/>\n<br/>",
            "code"
        ]
    ],
    "Examples->Miscellaneous->Displaying Pipelines->Displaying a Pipeline and Dimensionality Reduction and Classifier": [
        [
            "This section constructs a  with a\ndimensionality reduction step, ,\na classifier, , and displays its visual\nrepresentation.\n</blockquote>",
            "markdown"
        ],
        [
            "from sklearn.pipeline import \nfrom sklearn.svm import \nfrom sklearn.decomposition import \n\nsteps = [(\"reduce_dim\", (n_components=4)), (\"classifier\", (kernel=\"linear\"))]\npipe = (steps)\npipe  # click on the diagram below to see the details of each step",
            "code"
        ],
        [
            "Pipeline(steps=[('reduce_dim', PCA(n_components=4)),\n                ('classifier', SVC(kernel='linear'))])<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-179\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-179\">Pipeline</label>",
            "code"
        ],
        [
            "Pipeline(steps=[('reduce_dim', PCA(n_components=4)),\n                ('classifier', SVC(kernel='linear'))])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-180\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-180\">PCA</label>",
            "code"
        ],
        [
            "PCA(n_components=4)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-181\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-181\">SVC</label>",
            "code"
        ],
        [
            "SVC(kernel='linear')\n\n<br/>\n<br/>",
            "code"
        ]
    ],
    "Examples->Miscellaneous->Displaying Pipelines->Displaying a Complex Pipeline Chaining a Column Transformer": [
        [
            "This section constructs a complex  with a\n and a classifier,\n, and displays its visual\nrepresentation.\n</blockquote>",
            "markdown"
        ],
        [
            "import numpy as np\nfrom sklearn.pipeline import \nfrom sklearn.pipeline import \nfrom sklearn.impute import \nfrom sklearn.compose import \nfrom sklearn.preprocessing import , \nfrom sklearn.linear_model import \n\nnumeric_preprocessor = (\n    steps=[\n        (\"imputation_mean\", (missing_values=, strategy=\"mean\")),\n        (\"scaler\", ()),\n    ]\n)\n\ncategorical_preprocessor = (\n    steps=[\n        (\n            \"imputation_constant\",\n            (fill_value=\"missing\", strategy=\"constant\"),\n        ),\n        (\"onehot\", (handle_unknown=\"ignore\")),\n    ]\n)\n\npreprocessor = (\n    [\n        (\"categorical\", categorical_preprocessor, [\"state\", \"gender\"]),\n        (\"numerical\", numeric_preprocessor, [\"age\", \"weight\"]),\n    ]\n)\n\npipe = (preprocessor, (max_iter=500))\npipe  # click on the diagram below to see the details of each step",
            "code"
        ],
        [
            "Pipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('categorical',\n                                                  Pipeline(steps=[('imputation_constant',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['state', 'gender']),\n                                                 ('numerical',\n                                                  Pipeline(steps=[('imputation_mean',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age', 'weight'])])),\n                ('logisticregression', LogisticRegression(max_iter=500))])<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-182\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-182\">Pipeline</label>",
            "code"
        ],
        [
            "Pipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('categorical',\n                                                  Pipeline(steps=[('imputation_constant',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['state', 'gender']),\n                                                 ('numerical',\n                                                  Pipeline(steps=[('imputation_mean',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age', 'weight'])])),\n                ('logisticregression', LogisticRegression(max_iter=500))])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-183\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-183\">columntransformer: ColumnTransformer</label>",
            "code"
        ],
        [
            "ColumnTransformer(transformers=[('categorical',\n                                 Pipeline(steps=[('imputation_constant',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehot',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 ['state', 'gender']),\n                                ('numerical',\n                                 Pipeline(steps=[('imputation_mean',\n                                                  SimpleImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['age', 'weight'])])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-184\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-184\">categorical</label>",
            "code"
        ],
        [
            "['state', 'gender']<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-185\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-185\">SimpleImputer</label>",
            "code"
        ],
        [
            "SimpleImputer(fill_value='missing', strategy='constant')<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-186\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-186\">OneHotEncoder</label>",
            "code"
        ],
        [
            "OneHotEncoder(handle_unknown='ignore')<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-187\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-187\">numerical</label>",
            "code"
        ],
        [
            "['age', 'weight']<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-188\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-188\">SimpleImputer</label>",
            "code"
        ],
        [
            "SimpleImputer()<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-189\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-189\">StandardScaler</label>",
            "code"
        ],
        [
            "StandardScaler()<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-190\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-190\">LogisticRegression</label>",
            "code"
        ],
        [
            "LogisticRegression(max_iter=500)\n\n<br/>\n<br/>",
            "code"
        ]
    ],
    "Examples->Miscellaneous->Displaying Pipelines->Displaying a Grid Search over a Pipeline with a Classifier": [
        [
            "This section constructs a \nover a  with\n and displays its visual\nrepresentation.\n</blockquote>",
            "markdown"
        ],
        [
            "import numpy as np\nfrom sklearn.pipeline import \nfrom sklearn.pipeline import \nfrom sklearn.impute import \nfrom sklearn.compose import \nfrom sklearn.preprocessing import , \nfrom sklearn.ensemble import \nfrom sklearn.model_selection import \n\nnumeric_preprocessor = (\n    steps=[\n        (\"imputation_mean\", (missing_values=, strategy=\"mean\")),\n        (\"scaler\", ()),\n    ]\n)\n\ncategorical_preprocessor = (\n    steps=[\n        (\n            \"imputation_constant\",\n            (fill_value=\"missing\", strategy=\"constant\"),\n        ),\n        (\"onehot\", (handle_unknown=\"ignore\")),\n    ]\n)\n\npreprocessor = (\n    [\n        (\"categorical\", categorical_preprocessor, [\"state\", \"gender\"]),\n        (\"numerical\", numeric_preprocessor, [\"age\", \"weight\"]),\n    ]\n)\n\npipe = (\n    steps=[(\"preprocessor\", preprocessor), (\"classifier\", ())]\n)\n\nparam_grid = {\n    \"classifier__n_estimators\": [200, 500],\n    \"classifier__max_features\": [\"auto\", \"sqrt\", \"log2\"],\n    \"classifier__max_depth\": [4, 5, 6, 7, 8],\n    \"classifier__criterion\": [\"gini\", \"entropy\"],\n}\n\ngrid_search = (pipe, param_grid=param_grid, n_jobs=1)\ngrid_search  # click on the diagram below to see the details of each step",
            "code"
        ],
        [
            "GridSearchCV(estimator=Pipeline(steps=[('preprocessor',\n                                        ColumnTransformer(transformers=[('categorical',\n                                                                         Pipeline(steps=[('imputation_constant',\n                                                                                          SimpleImputer(fill_value='missing',\n                                                                                                        strategy='constant')),\n                                                                                         ('onehot',\n                                                                                          OneHotEncoder(handle_unknown='ignore'))]),\n                                                                         ['state',\n                                                                          'gender']),\n                                                                        ('numerical',\n                                                                         Pipeline(steps=[('imputation_mean',\n                                                                                          SimpleImputer()),\n                                                                                         ('scaler',\n                                                                                          StandardScaler())]),\n                                                                         ['age',\n                                                                          'weight'])])),\n                                       ('classifier',\n                                        RandomForestClassifier())]),\n             n_jobs=1,\n             param_grid={'classifier__criterion': ['gini', 'entropy'],\n                         'classifier__max_depth': [4, 5, 6, 7, 8],\n                         'classifier__max_features': ['auto', 'sqrt', 'log2'],\n                         'classifier__n_estimators': [200, 500]})<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-191\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-191\">GridSearchCV</label>",
            "code"
        ],
        [
            "GridSearchCV(estimator=Pipeline(steps=[('preprocessor',\n                                        ColumnTransformer(transformers=[('categorical',\n                                                                         Pipeline(steps=[('imputation_constant',\n                                                                                          SimpleImputer(fill_value='missing',\n                                                                                                        strategy='constant')),\n                                                                                         ('onehot',\n                                                                                          OneHotEncoder(handle_unknown='ignore'))]),\n                                                                         ['state',\n                                                                          'gender']),\n                                                                        ('numerical',\n                                                                         Pipeline(steps=[('imputation_mean',\n                                                                                          SimpleImputer()),\n                                                                                         ('scaler',\n                                                                                          StandardScaler())]),\n                                                                         ['age',\n                                                                          'weight'])])),\n                                       ('classifier',\n                                        RandomForestClassifier())]),\n             n_jobs=1,\n             param_grid={'classifier__criterion': ['gini', 'entropy'],\n                         'classifier__max_depth': [4, 5, 6, 7, 8],\n                         'classifier__max_features': ['auto', 'sqrt', 'log2'],\n                         'classifier__n_estimators': [200, 500]})<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-192\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-192\">estimator: Pipeline</label>",
            "code"
        ],
        [
            "Pipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('categorical',\n                                                  Pipeline(steps=[('imputation_constant',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['state', 'gender']),\n                                                 ('numerical',\n                                                  Pipeline(steps=[('imputation_mean',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age', 'weight'])])),\n                ('classifier', RandomForestClassifier())])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-193\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-193\">preprocessor: ColumnTransformer</label>",
            "code"
        ],
        [
            "ColumnTransformer(transformers=[('categorical',\n                                 Pipeline(steps=[('imputation_constant',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehot',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 ['state', 'gender']),\n                                ('numerical',\n                                 Pipeline(steps=[('imputation_mean',\n                                                  SimpleImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['age', 'weight'])])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-194\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-194\">categorical</label>",
            "code"
        ],
        [
            "['state', 'gender']<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-195\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-195\">SimpleImputer</label>",
            "code"
        ],
        [
            "SimpleImputer(fill_value='missing', strategy='constant')<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-196\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-196\">OneHotEncoder</label>",
            "code"
        ],
        [
            "OneHotEncoder(handle_unknown='ignore')<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-197\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-197\">numerical</label>",
            "code"
        ],
        [
            "['age', 'weight']<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-198\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-198\">SimpleImputer</label>",
            "code"
        ],
        [
            "SimpleImputer()<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-199\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-199\">StandardScaler</label>",
            "code"
        ],
        [
            "StandardScaler()<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-200\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-200\">RandomForestClassifier</label>",
            "code"
        ],
        [
            "RandomForestClassifier()\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.096 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Miscellaneous->Displaying estimators and complex pipelines": [
        [
            "This example illustrates different ways estimators and pipelines can be\ndisplayed.",
            "markdown"
        ],
        [
            "from sklearn.pipeline import \nfrom sklearn.preprocessing import , \nfrom sklearn.impute import \nfrom sklearn.compose import \nfrom sklearn.linear_model import",
            "code"
        ]
    ],
    "Examples->Miscellaneous->Displaying estimators and complex pipelines->Compact text representation": [
        [
            "Estimators will only show the parameters that have been set to non-default\nvalues when displayed as a string. This reduces the visual noise and makes it\neasier to spot what the differences are when comparing instances.",
            "markdown"
        ],
        [
            "lr = (penalty=\"l1\")\nprint(lr)",
            "code"
        ],
        [
            "LogisticRegression(penalty='l1')",
            "code"
        ]
    ],
    "Examples->Miscellaneous->Displaying estimators and complex pipelines->Rich HTML representation": [
        [
            "In notebooks estimators and pipelines will use a rich HTML representation.\nThis is particularly useful to summarise the\nstructure of pipelines and other composite estimators, with interactivity to\nprovide detail.  Click on the example image below to expand Pipeline\nelements.  See  for how you can use\nthis feature.",
            "markdown"
        ],
        [
            "num_proc = ((strategy=\"median\"), ())\n\ncat_proc = (\n    (strategy=\"constant\", fill_value=\"missing\"),\n    (handle_unknown=\"ignore\"),\n)\n\npreprocessor = (\n    (num_proc, (\"feat1\", \"feat3\")), (cat_proc, (\"feat0\", \"feat2\"))\n)\n\nclf = (preprocessor, ())\nclf",
            "code"
        ],
        [
            "Pipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('pipeline-1',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('standardscaler',\n                                                                   StandardScaler())]),\n                                                  ('feat1', 'feat3')),\n                                                 ('pipeline-2',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('onehotencoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ('feat0', 'feat2'))])),\n                ('logisticregression', LogisticRegression())])<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-201\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-201\">Pipeline</label>",
            "code"
        ],
        [
            "Pipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('pipeline-1',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('standardscaler',\n                                                                   StandardScaler())]),\n                                                  ('feat1', 'feat3')),\n                                                 ('pipeline-2',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('onehotencoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ('feat0', 'feat2'))])),\n                ('logisticregression', LogisticRegression())])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-202\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-202\">columntransformer: ColumnTransformer</label>",
            "code"
        ],
        [
            "ColumnTransformer(transformers=[('pipeline-1',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('standardscaler',\n                                                  StandardScaler())]),\n                                 ('feat1', 'feat3')),\n                                ('pipeline-2',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 ('feat0', 'feat2'))])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-203\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-203\">pipeline-1</label>",
            "code"
        ],
        [
            "('feat1', 'feat3')<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-204\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-204\">SimpleImputer</label>",
            "code"
        ],
        [
            "SimpleImputer(strategy='median')<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-205\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-205\">StandardScaler</label>",
            "code"
        ],
        [
            "StandardScaler()<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-206\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-206\">pipeline-2</label>",
            "code"
        ],
        [
            "('feat0', 'feat2')<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-207\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-207\">SimpleImputer</label>",
            "code"
        ],
        [
            "SimpleImputer(fill_value='missing', strategy='constant')<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-208\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-208\">OneHotEncoder</label>",
            "code"
        ],
        [
            "OneHotEncoder(handle_unknown='ignore')<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-209\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-209\">LogisticRegression</label>",
            "code"
        ],
        [
            "LogisticRegression()\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.027 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Miscellaneous->Evaluation of outlier detection estimators": [
        [
            "This example benchmarks outlier detection algorithms, \n(LOF) and  (IForest), using ROC curves on\nclassical anomaly detection datasets. The algorithm performance\nis assessed in an outlier detection context:",
            "markdown"
        ],
        [
            "1. The algorithms are trained on the whole dataset which is assumed to\ncontain outliers.",
            "markdown"
        ],
        [
            "2. The ROC curve from  is computed\non the same dataset using the knowledge of the labels.",
            "markdown"
        ],
        [
            "# Author: Pharuj Rajborirug &lt;pharuj.ra@kmitl.ac.th\n# License: BSD 3 clause\n\nprint(__doc__)",
            "code"
        ]
    ],
    "Examples->Miscellaneous->Evaluation of outlier detection estimators->Define a data preprocessing function": [
        [
            "The example uses real-world datasets available in\n and the sample size of some datasets is reduced\nto speed up computation. After the data preprocessing, the datasets\u2019 targets\nwill have two classes, 0 representing inliers and 1 representing outliers.\nThe preprocess_dataset function returns data and target.",
            "markdown"
        ],
        [
            "import numpy as np\nfrom sklearn.datasets import , , \nfrom sklearn.preprocessing import \nimport pandas as pd\n\nrng = (42)\n\n\ndef preprocess_dataset(dataset_name):\n\n    # loading and vectorization\n    print(f\"Loading {dataset_name} data\")\n    if dataset_name in [\"http\", \"smtp\", \"SA\", \"SF\"]:\n        dataset = (subset=dataset_name, percent10=True, random_state=rng)\n        X = dataset.data\n        y = dataset.target\n        lb = ()\n\n        if dataset_name == \"SF\":\n            idx = rng.choice(X.shape[0], int(X.shape[0] * 0.1), replace=False)\n            X = X[idx]  # reduce the sample size\n            y = y[idx]\n            x1 = lb.fit_transform(X[:, 1].astype(str))\n            X = [X[:, :1], x1, X[:, 2:]]\n        elif dataset_name == \"SA\":\n            idx = rng.choice(X.shape[0], int(X.shape[0] * 0.1), replace=False)\n            X = X[idx]  # reduce the sample size\n            y = y[idx]\n            x1 = lb.fit_transform(X[:, 1].astype(str))\n            x2 = lb.fit_transform(X[:, 2].astype(str))\n            x3 = lb.fit_transform(X[:, 3].astype(str))\n            X = [X[:, :1], x1, x2, x3, X[:, 4:]]\n        y = (y != b\"normal.\").astype(int)\n    if dataset_name == \"forestcover\":\n        dataset = ()\n        X = dataset.data\n        y = dataset.target\n        idx = rng.choice(X.shape[0], int(X.shape[0] * 0.1), replace=False)\n        X = X[idx]  # reduce the sample size\n        y = y[idx]\n\n        # inliers are those with attribute 2\n        # outliers are those with attribute 4\n        s = (y == 2) + (y == 4)\n        X = X[s, :]\n        y = y[s]\n        y = (y != 2).astype(int)\n    if dataset_name in [\"glass\", \"wdbc\", \"cardiotocography\"]:\n        dataset = (\n            name=dataset_name, version=1, as_frame=False, parser=\"pandas\"\n        )\n        X = dataset.data\n        y = dataset.target\n\n        if dataset_name == \"glass\":\n            s = y == \"tableware\"\n            y = s.astype(int)\n        if dataset_name == \"wdbc\":\n            s = y == \"2\"\n            y = s.astype(int)\n            X_mal, y_mal = X[s], y[s]\n            X_ben, y_ben = X[~s], y[~s]\n\n            # downsampled to 39 points (9.8% outliers)\n            idx = rng.choice(y_mal.shape[0], 39, replace=False)\n            X_mal2 = X_mal[idx]\n            y_mal2 = y_mal[idx]\n            X = ((X_ben, X_mal2), axis=0)\n            y = ((y_ben, y_mal2), axis=0)\n        if dataset_name == \"cardiotocography\":\n            s = y == \"3\"\n            y = s.astype(int)\n    # 0 represents inliers, and 1 represents outliers\n    y = (y, dtype=\"category\")\n    return (X, y)",
            "code"
        ]
    ],
    "Examples->Miscellaneous->Evaluation of outlier detection estimators->Define an outlier prediction function": [
        [
            "There is no particular reason to choose algorithms\n and\n. The goal is to show that\ndifferent algorithm performs well on different datasets. The following\ncompute_prediction function returns average outlier score of X.",
            "markdown"
        ],
        [
            "from sklearn.neighbors import \nfrom sklearn.ensemble import \n\n\ndef compute_prediction(X, model_name):\n\n    print(f\"Computing {model_name} prediction...\")\n    if model_name == \"LOF\":\n        clf = (n_neighbors=20, contamination=\"auto\")\n        clf.fit(X)\n        y_pred = clf.negative_outlier_factor_\n    if model_name == \"IForest\":\n        clf = (random_state=rng, contamination=\"auto\")\n        y_pred = clf.fit(X).decision_function(X)\n    return y_pred",
            "code"
        ]
    ],
    "Examples->Miscellaneous->Evaluation of outlier detection estimators->Plot and interpret results": [
        [
            "The algorithm performance relates to how good the true positive rate (TPR)\nis at low value of the false positive rate (FPR). The best algorithms\nhave the curve on the top-left of the plot and the area under curve (AUC)\nclose to 1. The diagonal dashed line represents a random classification\nof outliers and inliers.",
            "markdown"
        ],
        [
            "import math\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import RocCurveDisplay\n\ndatasets_name = [\n    \"http\",\n    \"smtp\",\n    \"SA\",\n    \"SF\",\n    \"forestcover\",\n    \"glass\",\n    \"wdbc\",\n    \"cardiotocography\",\n]\n\nmodels_name = [\n    \"LOF\",\n    \"IForest\",\n]\n\n# plotting parameters\ncols = 2\nlinewidth = 1\npos_label = 0  # mean 0 belongs to positive class\nrows = (len(datasets_name) / cols)\n\nfig, axs = (rows, cols, figsize=(10, rows * 3))\n\nfor i, dataset_name in enumerate(datasets_name):\n    (X, y) = preprocess_dataset(dataset_name=dataset_name)\n\n    for model_name in models_name:\n        y_pred = compute_prediction(X, model_name=model_name)\n        display = (\n            y,\n            y_pred,\n            pos_label=pos_label,\n            name=model_name,\n            linewidth=linewidth,\n            ax=axs[i // cols, i % cols],\n        )\n    axs[i // cols, i % cols].plot([0, 1], [0, 1], linewidth=linewidth, linestyle=\":\")\n    axs[i // cols, i % cols].set_title(dataset_name)\n    axs[i // cols, i % cols].set_xlabel(\"False Positive Rate\")\n    axs[i // cols, i % cols].set_ylabel(\"True Positive Rate\")\n(pad=2.0)  # spacing between subplots\n()\n\n\n<img alt=\"http, smtp, SA, SF, forestcover, glass, wdbc, cardiotocography\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_outlier_detection_bench_001.png\" srcset=\"../../_images/sphx_glr_plot_outlier_detection_bench_001.png\"/>",
            "code"
        ],
        [
            "Loading http data\nComputing LOF prediction...\nComputing IForest prediction...\nLoading smtp data\nComputing LOF prediction...\nComputing IForest prediction...\nLoading SA data\nComputing LOF prediction...\nComputing IForest prediction...\nLoading SF data\nComputing LOF prediction...\nComputing IForest prediction...\nLoading forestcover data\nComputing LOF prediction...\nComputing IForest prediction...\nLoading glass data\nComputing LOF prediction...\nComputing IForest prediction...\nLoading wdbc data\nComputing LOF prediction...\nComputing IForest prediction...\nLoading cardiotocography data\nComputing LOF prediction...\nComputing IForest prediction...",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  46.418 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Miscellaneous->Explicit feature map approximation for RBF kernels": [
        [
            "An example illustrating the approximation of the feature map\nof an RBF kernel.",
            "markdown"
        ],
        [
            "It shows how to use  and  to\napproximate the feature map of an RBF kernel for classification with an SVM on\nthe digits dataset. Results using a linear SVM in the original space, a linear\nSVM using the approximate mappings and using a kernelized SVM are compared.\nTimings and accuracy for varying amounts of Monte Carlo samplings (in the case\nof , which uses random Fourier features) and different sized\nsubsets of the training set (for ) for the approximate mapping\nare shown.",
            "markdown"
        ],
        [
            "Please note that the dataset here is not large enough to show the benefits\nof kernel approximation, as the exact SVM is still reasonably fast.",
            "markdown"
        ],
        [
            "Sampling more dimensions clearly leads to better classification results, but\ncomes at a greater cost. This means there is a tradeoff between runtime and\naccuracy, given by the parameter n_components. Note that solving the Linear\nSVM and also the approximate kernel SVM could be greatly accelerated by using\nstochastic gradient descent via .\nThis is not easily possible for the case of the kernelized SVM.",
            "markdown"
        ]
    ],
    "Examples->Miscellaneous->Explicit feature map approximation for RBF kernels->Python package and dataset imports, load dataset": [
        [
            "# Author: Gael Varoquaux &lt;gael dot varoquaux at normalesup dot org\n#         Andreas Mueller &lt;amueller@ais.uni-bonn.de\n# License: BSD 3 clause\n\n# Standard scientific Python imports\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom time import \n\n# Import datasets, classifiers and performance metrics\nfrom sklearn import datasets, svm, pipeline\nfrom sklearn.kernel_approximation import , \nfrom sklearn.decomposition import \n\n# The digits dataset\ndigits = (n_class=9)",
            "code"
        ]
    ],
    "Examples->Miscellaneous->Explicit feature map approximation for RBF kernels->Timing and accuracy plots": [
        [
            "To apply an classifier on this data, we need to flatten the image, to\nturn the data in a (samples, feature) matrix:",
            "markdown"
        ],
        [
            "n_samples = len(digits.data)\ndata = digits.data / 16.0\ndata -= data.mean(axis=0)\n\n# We learn the digits on the first half of the digits\ndata_train, targets_train = (data[: n_samples // 2], digits.target[: n_samples // 2])\n\n\n# Now predict the value of the digit on the second half:\ndata_test, targets_test = (data[n_samples // 2 :], digits.target[n_samples // 2 :])\n# data_test = scaler.transform(data_test)\n\n# Create a classifier: a support vector classifier\nkernel_svm = (gamma=0.2)\nlinear_svm = ()\n\n# create pipeline from kernel approximation\n# and linear svm\nfeature_map_fourier = (gamma=0.2, random_state=1)\nfeature_map_nystroem = (gamma=0.2, random_state=1)\nfourier_approx_svm = (\n    [(\"feature_map\", feature_map_fourier), (\"svm\", ())]\n)\n\nnystroem_approx_svm = (\n    [(\"feature_map\", feature_map_nystroem), (\"svm\", ())]\n)\n\n# fit and predict using linear and kernel svm:\n\nkernel_svm_time = ()\nkernel_svm.fit(data_train, targets_train)\nkernel_svm_score = kernel_svm.score(data_test, targets_test)\nkernel_svm_time = () - kernel_svm_time\n\nlinear_svm_time = ()\nlinear_svm.fit(data_train, targets_train)\nlinear_svm_score = linear_svm.score(data_test, targets_test)\nlinear_svm_time = () - linear_svm_time\n\nsample_sizes = 30 * (1, 10)\nfourier_scores = []\nnystroem_scores = []\nfourier_times = []\nnystroem_times = []\n\nfor D in sample_sizes:\n    fourier_approx_svm.set_params(feature_map__n_components=D)\n    nystroem_approx_svm.set_params(feature_map__n_components=D)\n    start = ()\n    nystroem_approx_svm.fit(data_train, targets_train)\n    nystroem_times.append(() - start)\n\n    start = ()\n    fourier_approx_svm.fit(data_train, targets_train)\n    fourier_times.append(() - start)\n\n    fourier_score = fourier_approx_svm.score(data_test, targets_test)\n    nystroem_score = nystroem_approx_svm.score(data_test, targets_test)\n    nystroem_scores.append(nystroem_score)\n    fourier_scores.append(fourier_score)\n\n# plot the results:\n(figsize=(16, 4))\naccuracy = (121)\n# second y axis for timings\ntimescale = (122)\n\naccuracy.plot(sample_sizes, nystroem_scores, label=\"Nystroem approx. kernel\")\ntimescale.plot(sample_sizes, nystroem_times, \"--\", label=\"Nystroem approx. kernel\")\n\naccuracy.plot(sample_sizes, fourier_scores, label=\"Fourier approx. kernel\")\ntimescale.plot(sample_sizes, fourier_times, \"--\", label=\"Fourier approx. kernel\")\n\n# horizontal lines for exact rbf and linear kernels:\naccuracy.plot(\n    [sample_sizes[0], sample_sizes[-1]],\n    [linear_svm_score, linear_svm_score],\n    label=\"linear svm\",\n)\ntimescale.plot(\n    [sample_sizes[0], sample_sizes[-1]],\n    [linear_svm_time, linear_svm_time],\n    \"--\",\n    label=\"linear svm\",\n)\n\naccuracy.plot(\n    [sample_sizes[0], sample_sizes[-1]],\n    [kernel_svm_score, kernel_svm_score],\n    label=\"rbf svm\",\n)\ntimescale.plot(\n    [sample_sizes[0], sample_sizes[-1]],\n    [kernel_svm_time, kernel_svm_time],\n    \"--\",\n    label=\"rbf svm\",\n)\n\n# vertical line for dataset dimensionality = 64\naccuracy.plot([64, 64], [0.7, 1], label=\"n_features\")\n\n# legends and labels\naccuracy.set_title(\"Classification accuracy\")\ntimescale.set_title(\"Training times\")\naccuracy.set_xlim(sample_sizes[0], sample_sizes[-1])\naccuracy.set_xticks(())\naccuracy.set_ylim(np.min(fourier_scores), 1)\ntimescale.set_xlabel(\"Sampling steps = transformed feature dimension\")\naccuracy.set_ylabel(\"Classification accuracy\")\ntimescale.set_ylabel(\"Training time in seconds\")\naccuracy.legend(loc=\"best\")\ntimescale.legend(loc=\"best\")\n()\n()\n\n\n<img alt=\"Classification accuracy, Training times\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_kernel_approximation_001.png\" srcset=\"../../_images/sphx_glr_plot_kernel_approximation_001.png\"/>",
            "code"
        ]
    ],
    "Examples->Miscellaneous->Explicit feature map approximation for RBF kernels->Decision Surfaces of RBF Kernel SVM and Linear SVM": [
        [
            "The second plot visualized the decision surfaces of the RBF kernel SVM and\nthe linear SVM with approximate kernel maps.\nThe plot shows decision surfaces of the classifiers projected onto\nthe first two principal components of the data. This visualization should\nbe taken with a grain of salt since it is just an interesting slice through\nthe decision surface in 64 dimensions. In particular note that\na datapoint (represented as a dot) does not necessarily be classified\ninto the region it is lying in, since it will not lie on the plane\nthat the first two principal components span.\nThe usage of  and  is described in detail\nin .",
            "markdown"
        ],
        [
            "# visualize the decision surface, projected down to the first\n# two principal components of the dataset\npca = (n_components=8).fit(data_train)\n\nX = pca.transform(data_train)\n\n# Generate grid along first two principal components\nmultiples = (-2, 2, 0.1)\n# steps along first component\nfirst = multiples[:, ] * pca.components_[0, :]\n# steps along second component\nsecond = multiples[:, ] * pca.components_[1, :]\n# combine\ngrid = first[, :, :] + second[:, , :]\nflat_grid = grid.reshape(-1, data.shape[1])\n\n# title for the plots\ntitles = [\n    \"SVC with rbf kernel\",\n    \"SVC (linear kernel)\\n with Fourier rbf feature map\\nn_components=100\",\n    \"SVC (linear kernel)\\n with Nystroem rbf feature map\\nn_components=100\",\n]\n\n(figsize=(18, 7.5))\nplt.rcParams.update({\"font.size\": 14})\n# predict and plot\nfor i, clf in enumerate((kernel_svm, nystroem_approx_svm, fourier_approx_svm)):\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    (1, 3, i + 1)\n    Z = clf.predict(flat_grid)\n\n    # Put the result into a color plot\n    Z = Z.reshape(grid.shape[:-1])\n    levels = (10)\n    lv_eps = 0.01  # Adjust a mapping from calculated contour levels to color.\n    (\n        multiples,\n        multiples,\n        Z,\n        levels=levels - lv_eps,\n        cmap=plt.cm.tab10,\n        vmin=0,\n        vmax=10,\n        alpha=0.7,\n    )\n    (\"off\")\n\n    # Plot also the training points\n    (\n        X[:, 0],\n        X[:, 1],\n        c=targets_train,\n        cmap=plt.cm.tab10,\n        edgecolors=(0, 0, 0),\n        vmin=0,\n        vmax=10,\n    )\n\n    (titles[i])\n()\n()\n\n\n<img alt=\"SVC with rbf kernel, SVC (linear kernel)  with Fourier rbf feature map n_components=100, SVC (linear kernel)  with Nystroem rbf feature map n_components=100\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_kernel_approximation_002.png\" srcset=\"../../_images/sphx_glr_plot_kernel_approximation_002.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  1.298 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Miscellaneous->Face completion with a multi-output estimators": [
        [
            "This example shows the use of multi-output estimator to complete images.\nThe goal is to predict the lower half of a face given its upper half.",
            "markdown"
        ],
        [
            "The first column of images shows true faces. The next columns illustrate\nhow extremely randomized trees, k nearest neighbors, linear\nregression and ridge regression complete the lower half of those faces.\n<img alt=\"Face completion with multi-output estimators, true faces, Extra trees, K-nn, Linear regression, Ridge\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_multioutput_face_completion_001.png\" srcset=\"../../_images/sphx_glr_plot_multioutput_face_completion_001.png\"/>",
            "markdown"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import \nfrom sklearn.utils.validation import \n\nfrom sklearn.ensemble import \nfrom sklearn.neighbors import \nfrom sklearn.linear_model import \nfrom sklearn.linear_model import \n\n# Load the faces datasets\ndata, targets = (return_X_y=True)\n\ntrain = data[targets &lt; 30]\ntest = data[targets = 30]  # Test on independent people\n\n# Test on a subset of people\nn_faces = 5\nrng = (4)\nface_ids = rng.randint(test.shape[0], size=(n_faces,))\ntest = test[face_ids, :]\n\nn_pixels = data.shape[1]\n# Upper half of the faces\nX_train = train[:, : (n_pixels + 1) // 2]\n# Lower half of the faces\ny_train = train[:, n_pixels // 2 :]\nX_test = test[:, : (n_pixels + 1) // 2]\ny_test = test[:, n_pixels // 2 :]\n\n# Fit estimators\nESTIMATORS = {\n    \"Extra trees\": (\n        n_estimators=10, max_features=32, random_state=0\n    ),\n    \"K-nn\": (),\n    \"Linear regression\": (),\n    \"Ridge\": (),\n}\n\ny_test_predict = dict()\nfor name, estimator in ESTIMATORS.items():\n    estimator.fit(X_train, y_train)\n    y_test_predict[name] = estimator.predict(X_test)\n\n# Plot the completed faces\nimage_shape = (64, 64)\n\nn_cols = 1 + len(ESTIMATORS)\n(figsize=(2.0 * n_cols, 2.26 * n_faces))\n(\"Face completion with multi-output estimators\", size=16)\n\nfor i in range(n_faces):\n    true_face = ((X_test[i], y_test[i]))\n\n    if i:\n        sub = (n_faces, n_cols, i * n_cols + 1)\n    else:\n        sub = (n_faces, n_cols, i * n_cols + 1, title=\"true faces\")\n\n    sub.axis(\"off\")\n    sub.imshow(\n        true_face.reshape(image_shape), cmap=plt.cm.gray, interpolation=\"nearest\"\n    )\n\n    for j, est in enumerate(sorted(ESTIMATORS)):\n        completed_face = ((X_test[i], y_test_predict[est][i]))\n\n        if i:\n            sub = (n_faces, n_cols, i * n_cols + 2 + j)\n\n        else:\n            sub = (n_faces, n_cols, i * n_cols + 2 + j, title=est)\n\n        sub.axis(\"off\")\n        sub.imshow(\n            completed_face.reshape(image_shape),\n            cmap=plt.cm.gray,\n            interpolation=\"nearest\",\n        )\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  1.737 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Miscellaneous->Introducing the `set_output` API": [
        [
            "This example shows the use of multi-output estimator to complete images.\nThe goal is to predict the lower half of a face given its upper half.",
            "markdown"
        ],
        [
            "The first column of images shows true faces. The next columns illustrate\nhow extremely randomized trees, k nearest neighbors, linear\nregression and ridge regression complete the lower half of those faces.\n<img alt=\"Face completion with multi-output estimators, true faces, Extra trees, K-nn, Linear regression, Ridge\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_multioutput_face_completion_001.png\" srcset=\"../../_images/sphx_glr_plot_multioutput_face_completion_001.png\"/>",
            "markdown"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import \nfrom sklearn.utils.validation import \n\nfrom sklearn.ensemble import \nfrom sklearn.neighbors import \nfrom sklearn.linear_model import \nfrom sklearn.linear_model import \n\n# Load the faces datasets\ndata, targets = (return_X_y=True)\n\ntrain = data[targets &lt; 30]\ntest = data[targets = 30]  # Test on independent people\n\n# Test on a subset of people\nn_faces = 5\nrng = (4)\nface_ids = rng.randint(test.shape[0], size=(n_faces,))\ntest = test[face_ids, :]\n\nn_pixels = data.shape[1]\n# Upper half of the faces\nX_train = train[:, : (n_pixels + 1) // 2]\n# Lower half of the faces\ny_train = train[:, n_pixels // 2 :]\nX_test = test[:, : (n_pixels + 1) // 2]\ny_test = test[:, n_pixels // 2 :]\n\n# Fit estimators\nESTIMATORS = {\n    \"Extra trees\": (\n        n_estimators=10, max_features=32, random_state=0\n    ),\n    \"K-nn\": (),\n    \"Linear regression\": (),\n    \"Ridge\": (),\n}\n\ny_test_predict = dict()\nfor name, estimator in ESTIMATORS.items():\n    estimator.fit(X_train, y_train)\n    y_test_predict[name] = estimator.predict(X_test)\n\n# Plot the completed faces\nimage_shape = (64, 64)\n\nn_cols = 1 + len(ESTIMATORS)\n(figsize=(2.0 * n_cols, 2.26 * n_faces))\n(\"Face completion with multi-output estimators\", size=16)\n\nfor i in range(n_faces):\n    true_face = ((X_test[i], y_test[i]))\n\n    if i:\n        sub = (n_faces, n_cols, i * n_cols + 1)\n    else:\n        sub = (n_faces, n_cols, i * n_cols + 1, title=\"true faces\")\n\n    sub.axis(\"off\")\n    sub.imshow(\n        true_face.reshape(image_shape), cmap=plt.cm.gray, interpolation=\"nearest\"\n    )\n\n    for j, est in enumerate(sorted(ESTIMATORS)):\n        completed_face = ((X_test[i], y_test_predict[est][i]))\n\n        if i:\n            sub = (n_faces, n_cols, i * n_cols + 2 + j)\n\n        else:\n            sub = (n_faces, n_cols, i * n_cols + 2 + j, title=est)\n\n        sub.axis(\"off\")\n        sub.imshow(\n            completed_face.reshape(image_shape),\n            cmap=plt.cm.gray,\n            interpolation=\"nearest\",\n        )\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  1.737 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Miscellaneous->Isotonic Regression": [
        [
            "An illustration of the isotonic regression on generated data (non-linear\nmonotonic trend with homoscedastic uniform noise).",
            "markdown"
        ],
        [
            "The isotonic regression algorithm finds a non-decreasing approximation of a\nfunction while minimizing the mean squared error on the training data. The\nbenefit of such a non-parametric model is that it does not assume any shape for\nthe target function besides monotonicity. For comparison a linear regression is\nalso presented.",
            "markdown"
        ],
        [
            "The plot on the right-hand side shows the model prediction function that\nresults from the linear interpolation of thresholds points. The thresholds\npoints are a subset of the training input observations and their matching\ntarget values are computed by the isotonic non-parametric fit.",
            "markdown"
        ],
        [
            "# Author: Nelle Varoquaux &lt;nelle.varoquaux@gmail.com\n#         Alexandre Gramfort &lt;alexandre.gramfort@inria.fr\n# License: BSD\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.collections import \n\nfrom sklearn.linear_model import \nfrom sklearn.isotonic import \nfrom sklearn.utils import \n\nn = 100\nx = (n)\nrs = (0)\ny = rs.randint(-50, 50, size=(n,)) + 50.0 * ((n))",
            "code"
        ],
        [
            "Fit IsotonicRegression and LinearRegression models:",
            "markdown"
        ],
        [
            "ir = (out_of_bounds=\"clip\")\ny_ = ir.fit_transform(x, y)\n\nlr = ()\nlr.fit(x[:, ], y)  # x needs to be 2d for LinearRegression",
            "code"
        ],
        [
            "LinearRegression()<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input checked=\"\" class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-215\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-215\">LinearRegression</label>",
            "code"
        ],
        [
            "LinearRegression()\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "Plot results:",
            "markdown"
        ],
        [
            "segments = [[[i, y[i]], [i, y_[i]]] for i in range(n)]\nlc = (segments, zorder=0)\nlc.set_array((len(y)))\nlc.set_linewidths((n, 0.5))\n\nfig, (ax0, ax1) = (ncols=2, figsize=(12, 6))\n\nax0.plot(x, y, \"C0.\", markersize=12)\nax0.plot(x, y_, \"C1.-\", markersize=12)\nax0.plot(x, lr.predict(x[:, ]), \"C2-\")\nax0.add_collection(lc)\nax0.legend((\"Training data\", \"Isotonic fit\", \"Linear fit\"), loc=\"lower right\")\nax0.set_title(\"Isotonic regression fit on noisy data (n=%d)\" % n)\n\nx_test = (-10, 110, 1000)\nax1.plot(x_test, ir.predict(x_test), \"C1-\")\nax1.plot(ir.X_thresholds_, ir.y_thresholds_, \"C1.\", markersize=12)\nax1.set_title(\"Prediction function (%d thresholds)\" % len(ir.X_thresholds_))\n\n()\n\n\n<img alt=\"Isotonic regression fit on noisy data (n=100), Prediction function (36 thresholds)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_isotonic_regression_001.png\" srcset=\"../../_images/sphx_glr_plot_isotonic_regression_001.png\"/>",
            "code"
        ],
        [
            "Note that we explicitly passed out_of_bounds=\"clip\" to the constructor of\nIsotonicRegression to control the way the model extrapolates outside of the\nrange of data observed in the training set. This \u201cclipping\u201d extrapolation can\nbe seen on the plot of the decision function on the right-hand.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.181 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Miscellaneous->Multilabel classification": [
        [
            "This example simulates a multi-label document classification problem. The\ndataset is generated randomly based on the following process:",
            "markdown"
        ],
        [
            "pick the number of labels: n ~ Poisson(n_labels)",
            "markdown"
        ],
        [
            "n times, choose a class c: c ~ Multinomial(theta)",
            "markdown"
        ],
        [
            "pick the document length: k ~ Poisson(length)",
            "markdown"
        ],
        [
            "k times, choose a word: w ~ Multinomial(theta_c)\n\n</blockquote>",
            "markdown"
        ],
        [
            "In the above process, rejection sampling is used to make sure that n is more\nthan 2, and that the document length is never zero. Likewise, we reject classes\nwhich have already been chosen.  The documents that are assigned to both\nclasses are plotted surrounded by two colored circles.",
            "markdown"
        ],
        [
            "The classification is performed by projecting to the first two principal\ncomponents found by PCA and CCA for visualisation purposes, followed by using\nthe  metaclassifier using two\nSVCs with linear kernels to learn a discriminative model for each class.\nNote that PCA is used to perform an unsupervised dimensionality reduction,\nwhile CCA is used to perform a supervised one.",
            "markdown"
        ],
        [
            "Note: in the plot, \u201cunlabeled samples\u201d does not mean that we don\u2019t know the\nlabels (as in semi-supervised learning) but that the samples simply do not\nhave a label.\n<img alt=\"With unlabeled samples + CCA, With unlabeled samples + PCA, Without unlabeled samples + CCA, Without unlabeled samples + PCA\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_multilabel_001.png\" srcset=\"../../_images/sphx_glr_plot_multilabel_001.png\"/>",
            "markdown"
        ],
        [
            "# Authors: Vlad Niculae, Mathieu Blondel\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import \nfrom sklearn.multiclass import \nfrom sklearn.svm import \nfrom sklearn.decomposition import \nfrom sklearn.cross_decomposition import \n\n\ndef plot_hyperplane(clf, min_x, max_x, linestyle, label):\n    # get the separating hyperplane\n    w = clf.coef_[0]\n    a = -w[0] / w[1]\n    xx = (min_x - 5, max_x + 5)  # make sure the line is long enough\n    yy = a * xx - (clf.intercept_[0]) / w[1]\n    (xx, yy, linestyle, label=label)\n\n\ndef plot_subfigure(X, Y, subplot, title, transform):\n    if transform == \"pca\":\n        X = (n_components=2).fit_transform(X)\n    elif transform == \"cca\":\n        X = (n_components=2).fit(X, Y).transform(X)\n    else:\n        raise ValueError\n\n    min_x = np.min(X[:, 0])\n    max_x = np.max(X[:, 0])\n\n    min_y = np.min(X[:, 1])\n    max_y = np.max(X[:, 1])\n\n    classif = ((kernel=\"linear\"))\n    classif.fit(X, Y)\n\n    (2, 2, subplot)\n    (title)\n\n    zero_class = (Y[:, 0])\n    one_class = (Y[:, 1])\n    (X[:, 0], X[:, 1], s=40, c=\"gray\", edgecolors=(0, 0, 0))\n    (\n        X[zero_class, 0],\n        X[zero_class, 1],\n        s=160,\n        edgecolors=\"b\",\n        facecolors=\"none\",\n        linewidths=2,\n        label=\"Class 1\",\n    )\n    (\n        X[one_class, 0],\n        X[one_class, 1],\n        s=80,\n        edgecolors=\"orange\",\n        facecolors=\"none\",\n        linewidths=2,\n        label=\"Class 2\",\n    )\n\n    plot_hyperplane(\n        classif.estimators_[0], min_x, max_x, \"k--\", \"Boundary\\nfor class 1\"\n    )\n    plot_hyperplane(\n        classif.estimators_[1], min_x, max_x, \"k-.\", \"Boundary\\nfor class 2\"\n    )\n    (())\n    (())\n\n    (min_x - 0.5 * max_x, max_x + 0.5 * max_x)\n    (min_y - 0.5 * max_y, max_y + 0.5 * max_y)\n    if subplot == 2:\n        (\"First principal component\")\n        (\"Second principal component\")\n        (loc=\"upper left\")\n\n\n(figsize=(8, 6))\n\nX, Y = (\n    n_classes=2, n_labels=1, allow_unlabeled=True, random_state=1\n)\n\nplot_subfigure(X, Y, 1, \"With unlabeled samples + CCA\", \"cca\")\nplot_subfigure(X, Y, 2, \"With unlabeled samples + PCA\", \"pca\")\n\nX, Y = (\n    n_classes=2, n_labels=1, allow_unlabeled=False, random_state=1\n)\n\nplot_subfigure(X, Y, 3, \"Without unlabeled samples + CCA\", \"cca\")\nplot_subfigure(X, Y, 4, \"Without unlabeled samples + PCA\", \"pca\")\n\n(0.04, 0.02, 0.97, 0.94, 0.09, 0.2)\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.188 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Miscellaneous->ROC Curve with Visualization API": [
        [
            "Scikit-learn defines a simple API for creating visualizations for machine\nlearning. The key features of this API is to allow for quick plotting and\nvisual adjustments without recalculation. In this example, we will demonstrate\nhow to use the visualization API by comparing ROC curves.",
            "markdown"
        ]
    ],
    "Examples->Miscellaneous->ROC Curve with Visualization API->Load Data and Train a SVC": [
        [
            "First, we load the wine dataset and convert it to a binary classification\nproblem. Then, we train a support vector classifier on a training dataset.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\nfrom sklearn.svm import \nfrom sklearn.ensemble import \nfrom sklearn.metrics import RocCurveDisplay\nfrom sklearn.datasets import \nfrom sklearn.model_selection import \n\nX, y = (return_X_y=True)\ny = y == 2\n\nX_train, X_test, y_train, y_test = (X, y, random_state=42)\nsvc = (random_state=42)\nsvc.fit(X_train, y_train)",
            "code"
        ],
        [
            "SVC(random_state=42)<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input checked=\"\" class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-216\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-216\">SVC</label>",
            "code"
        ],
        [
            "SVC(random_state=42)\n\n<br/>\n<br/>",
            "code"
        ]
    ],
    "Examples->Miscellaneous->ROC Curve with Visualization API->Plotting the ROC Curve": [
        [
            "Next, we plot the ROC curve with a single call to\n. The returned\nsvc_disp object allows us to continue using the already computed ROC curve\nfor the SVC in future plots.",
            "markdown"
        ],
        [
            "svc_disp = (svc, X_test, y_test)\n()\n\n\n<img alt=\"plot roc curve visualization api\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_roc_curve_visualization_api_001.png\" srcset=\"../../_images/sphx_glr_plot_roc_curve_visualization_api_001.png\"/>",
            "code"
        ]
    ],
    "Examples->Miscellaneous->ROC Curve with Visualization API->Training a Random Forest and Plotting the ROC Curve": [
        [
            "We train a random forest classifier and create a plot comparing it to the SVC\nROC curve. Notice how svc_disp uses\n to plot the SVC ROC curve\nwithout recomputing the values of the roc curve itself. Furthermore, we\npass alpha=0.8 to the plot functions to adjust the alpha values of the\ncurves.",
            "markdown"
        ],
        [
            "rfc = (n_estimators=10, random_state=42)\nrfc.fit(X_train, y_train)\nax = ()\nrfc_disp = (rfc, X_test, y_test, ax=ax, alpha=0.8)\nsvc_disp.plot(ax=ax, alpha=0.8)\n()\n\n\n<img alt=\"plot roc curve visualization api\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_roc_curve_visualization_api_002.png\" srcset=\"../../_images/sphx_glr_plot_roc_curve_visualization_api_002.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.149 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Miscellaneous->The Johnson-Lindenstrauss bound for embedding with random projections": [
        [
            "The  states that any high dimensional\ndataset can be randomly projected into a lower dimensional Euclidean\nspace while controlling the distortion in the pairwise distances.",
            "markdown"
        ],
        [
            "import sys\nfrom time import \nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.random_projection import \nfrom sklearn.random_projection import \nfrom sklearn.datasets import \nfrom sklearn.datasets import \nfrom sklearn.metrics.pairwise import euclidean_distances",
            "code"
        ]
    ],
    "Examples->Miscellaneous->The Johnson-Lindenstrauss bound for embedding with random projections->Theoretical bounds": [
        [
            "The distortion introduced by a random projection p is asserted by\nthe fact that p is defining an eps-embedding with good probability\nas defined by:\n\n\\[(1 - eps) \\|u - v\\|^2 &lt; \\|p(u) - p(v)\\|^2 &lt; (1 + eps) \\|u - v\\|^2\\]",
            "markdown"
        ],
        [
            "Where u and v are any rows taken from a dataset of shape (n_samples,\nn_features) and p is a projection by a random Gaussian N(0, 1) matrix\nof shape (n_components, n_features) (or a sparse Achlioptas matrix).",
            "markdown"
        ],
        [
            "The minimum number of components to guarantees the eps-embedding is\ngiven by:\n\n\\[n\\_components \\geq 4 log(n\\_samples) / (eps^2 / 2 - eps^3 / 3)\\]",
            "markdown"
        ],
        [
            "The first plot shows that with an increasing number of samples n_samples,\nthe minimal number of dimensions n_components increased logarithmically\nin order to guarantee an eps-embedding.",
            "markdown"
        ],
        [
            "# range of admissible distortions\neps_range = (0.1, 0.99, 5)\ncolors = plt.cm.Blues((0.3, 1.0, len(eps_range)))\n\n# range of number of samples (observation) to embed\nn_samples_range = (1, 9, 9)\n\n()\nfor eps, color in zip(eps_range, colors):\n    min_n_components = (n_samples_range, eps=eps)\n    (n_samples_range, min_n_components, color=color)\n\n([f\"eps = {eps:0.1f}\" for eps in eps_range], loc=\"lower right\")\n(\"Number of observations to eps-embed\")\n(\"Minimum number of dimensions\")\n(\"Johnson-Lindenstrauss bounds:\\nn_samples vs n_components\")\n()\n\n\n<img alt=\"Johnson-Lindenstrauss bounds: n_samples vs n_components\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_001.png\" srcset=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_001.png\"/>",
            "code"
        ],
        [
            "The second plot shows that an increase of the admissible\ndistortion eps allows to reduce drastically the minimal number of\ndimensions n_components for a given number of samples n_samples",
            "markdown"
        ],
        [
            "# range of admissible distortions\neps_range = (0.01, 0.99, 100)\n\n# range of number of samples (observation) to embed\nn_samples_range = (2, 6, 5)\ncolors = plt.cm.Blues((0.3, 1.0, len(n_samples_range)))\n\n()\nfor n_samples, color in zip(n_samples_range, colors):\n    min_n_components = (n_samples, eps=eps_range)\n    (eps_range, min_n_components, color=color)\n\n([f\"n_samples = {n}\" for n in n_samples_range], loc=\"upper right\")\n(\"Distortion eps\")\n(\"Minimum number of dimensions\")\n(\"Johnson-Lindenstrauss bounds:\\nn_components vs eps\")\n()\n\n\n<img alt=\"Johnson-Lindenstrauss bounds: n_components vs eps\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_002.png\" srcset=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_002.png\"/>",
            "code"
        ]
    ],
    "Examples->Miscellaneous->The Johnson-Lindenstrauss bound for embedding with random projections->Empirical validation": [
        [
            "We validate the above bounds on the 20 newsgroups text document\n(TF-IDF word frequencies) dataset or on the digits dataset:",
            "markdown"
        ],
        [
            "for the 20 newsgroups dataset some 300 documents with 100k\nfeatures in total are projected using a sparse random matrix to smaller\neuclidean spaces with various values for the target number of dimensions\nn_components.",
            "markdown"
        ],
        [
            "for the digits dataset, some 8x8 gray level pixels data for 300\nhandwritten digits pictures are randomly projected to spaces for various\nlarger number of dimensions n_components.",
            "markdown"
        ],
        [
            "The default dataset is the 20 newsgroups dataset. To run the example on the\ndigits dataset, pass the --use-digits-dataset command line argument to\nthis script.",
            "markdown"
        ],
        [
            "if \"--use-digits-dataset\" in :\n    data = ().data[:300]\nelse:\n    data = ().data[:300]",
            "code"
        ],
        [
            "For each value of n_components, we plot:",
            "markdown"
        ],
        [
            "2D distribution of sample pairs with pairwise distances in original\nand projected spaces as x- and y-axis respectively.",
            "markdown"
        ],
        [
            "1D histogram of the ratio of those distances (projected / original).",
            "markdown"
        ],
        [
            "n_samples, n_features = data.shape\nprint(\n    f\"Embedding {n_samples} samples with dim {n_features} using various \"\n    \"random projections\"\n)\n\nn_components_range = ([300, 1_000, 10_000])\ndists = euclidean_distances(data, squared=True).ravel()\n\n# select only non-identical samples pairs\nnonzero = dists != 0\ndists = dists[nonzero]\n\nfor n_components in n_components_range:\n    t0 = ()\n    rp = (n_components=n_components)\n    projected_data = rp.fit_transform(data)\n    print(\n        f\"Projected {n_samples} samples from {n_features} to {n_components} in \"\n        f\"{() - t0:0.3f}s\"\n    )\n    if hasattr(rp, \"components_\"):\n        n_bytes = rp.components_.data.nbytes\n        n_bytes += rp.components_.indices.nbytes\n        print(f\"Random matrix with size: {n_bytes / 1e6:0.3f} MB\")\n\n    projected_dists = euclidean_distances(projected_data, squared=True).ravel()[nonzero]\n\n    ()\n    min_dist = min(projected_dists.min(), dists.min())\n    max_dist = max(projected_dists.max(), dists.max())\n    (\n        dists,\n        projected_dists,\n        gridsize=100,\n        cmap=plt.cm.PuBu,\n        extent=[min_dist, max_dist, min_dist, max_dist],\n    )\n    (\"Pairwise squared distances in original space\")\n    (\"Pairwise squared distances in projected space\")\n    (\"Pairwise distances distribution for n_components=%d\" % n_components)\n    cb = ()\n    cb.set_label(\"Sample pairs counts\")\n\n    rates = projected_dists / dists\n    print(f\"Mean distances rate: {(rates):.2f} ({(rates):.2f})\")\n\n    ()\n    (rates, bins=50, range=(0.0, 2.0), edgecolor=\"k\", density=True)\n    (\"Squared distances rate: projected / original\")\n    (\"Distribution of samples pairs\")\n    (\"Histogram of pairwise distance rates for n_components=%d\" % n_components)\n\n    # TODO: compute the expected value of eps and add them to the previous plot\n    # as vertical lines / region\n\n()\n\n\n\n<img alt=\"Pairwise distances distribution for n_components=300\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_003.png\" srcset=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_003.png\"/>\n<img alt=\"Histogram of pairwise distance rates for n_components=300\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_004.png\" srcset=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_004.png\"/>\n<img alt=\"Pairwise distances distribution for n_components=1000\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_005.png\" srcset=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_005.png\"/>\n<img alt=\"Histogram of pairwise distance rates for n_components=1000\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_006.png\" srcset=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_006.png\"/>\n<img alt=\"Pairwise distances distribution for n_components=10000\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_007.png\" srcset=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_007.png\"/>\n<img alt=\"Histogram of pairwise distance rates for n_components=10000\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_008.png\" srcset=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_008.png\"/>",
            "code"
        ],
        [
            "Embedding 300 samples with dim 130107 using various random projections\nProjected 300 samples from 130107 to 300 in 0.235s\nRandom matrix with size: 1.298 MB\nMean distances rate: 1.02 (0.17)\nProjected 300 samples from 130107 to 1000 in 0.827s\nRandom matrix with size: 4.332 MB\nMean distances rate: 1.00 (0.09)\nProjected 300 samples from 130107 to 10000 in 8.242s\nRandom matrix with size: 43.271 MB\nMean distances rate: 1.01 (0.03)",
            "code"
        ],
        [
            "We can see that for low values of n_components the distribution is wide\nwith many distorted pairs and a skewed distribution (due to the hard\nlimit of zero ratio on the left as distances are always positives)\nwhile for larger values of n_components the distortion is controlled\nand the distances are well preserved by the random projection.",
            "markdown"
        ]
    ],
    "Examples->Miscellaneous->The Johnson-Lindenstrauss bound for embedding with random projections->Remarks": [
        [
            "According to the JL lemma, projecting 300 samples without too much distortion\nwill require at least several thousands dimensions, irrespective of the\nnumber of features of the original dataset.",
            "markdown"
        ],
        [
            "Hence using random projections on the digits dataset which only has 64\nfeatures in the input space does not make sense: it does not allow\nfor dimensionality reduction in this case.",
            "markdown"
        ],
        [
            "On the twenty newsgroups on the other hand the dimensionality can be\ndecreased from 56,436 down to 10,000 while reasonably preserving\npairwise distances.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  11.454 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Miscellaneous->Visualizations with Display Objects": [
        [
            "In this example, we will construct display objects,\n, , and\n directly from their respective metrics. This\nis an alternative to using their corresponding plot functions when\na model\u2019s predictions are already computed or expensive to compute. Note that\nthis is advanced usage, and in general we recommend using their respective\nplot functions.",
            "markdown"
        ]
    ],
    "Examples->Miscellaneous->Visualizations with Display Objects->Load Data and train model": [
        [
            "For this example, we load a blood transfusion service center data set from\nOpenML &lt;https://www.openml.org/d/1464>. This is a binary classification\nproblem where the target is whether an individual donated blood. Then the\ndata is split into a train and test dataset and a logistic regression is\nfitted with the train dataset.",
            "markdown"
        ],
        [
            "from sklearn.datasets import \nfrom sklearn.preprocessing import \nfrom sklearn.pipeline import \nfrom sklearn.linear_model import \nfrom sklearn.model_selection import \n\nX, y = (data_id=1464, return_X_y=True, parser=\"pandas\")\nX_train, X_test, y_train, y_test = (X, y, stratify=y)\n\nclf = ((), (random_state=0))\nclf.fit(X_train, y_train)",
            "code"
        ],
        [
            "Pipeline(steps=[('standardscaler', StandardScaler()),\n                ('logisticregression', LogisticRegression(random_state=0))])<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-217\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-217\">Pipeline</label>",
            "code"
        ],
        [
            "Pipeline(steps=[('standardscaler', StandardScaler()),\n                ('logisticregression', LogisticRegression(random_state=0))])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-218\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-218\">StandardScaler</label>",
            "code"
        ],
        [
            "StandardScaler()<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-219\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-219\">LogisticRegression</label>",
            "code"
        ],
        [
            "LogisticRegression(random_state=0)\n\n<br/>\n<br/>",
            "code"
        ]
    ],
    "Examples->Miscellaneous->Visualizations with Display Objects->Load Data and train model->Create": [
        [
            "With the fitted model, we compute the predictions of the model on the test\ndataset. These predictions are used to compute the confustion matrix which\nis plotted with the \n</blockquote>",
            "markdown"
        ],
        [
            "from sklearn.metrics import \nfrom sklearn.metrics import \n\ny_pred = clf.predict(X_test)\ncm = (y_test, y_pred)\n\ncm_display = (cm).plot()\n\n\n<img alt=\"plot display object visualization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_display_object_visualization_001.png\" srcset=\"../../_images/sphx_glr_plot_display_object_visualization_001.png\"/>",
            "code"
        ],
        [
            "The roc curve requires either the probabilities or the non-thresholded\ndecision values from the estimator. Since the logistic regression provides\na decision function, we will use it to plot the roc curve:\n</blockquote>",
            "markdown"
        ],
        [
            "from sklearn.metrics import \nfrom sklearn.metrics import \n\ny_score = clf.decision_function(X_test)\n\nfpr, tpr, _ = (y_test, y_score, pos_label=clf.classes_[1])\nroc_display = (fpr=fpr, tpr=tpr).plot()\n\n\n<img alt=\"plot display object visualization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_display_object_visualization_002.png\" srcset=\"../../_images/sphx_glr_plot_display_object_visualization_002.png\"/>",
            "code"
        ],
        [
            "Similarly, the precision recall curve can be plotted using y_score from\nthe prevision sections.\n</blockquote>",
            "markdown"
        ],
        [
            "from sklearn.metrics import \nfrom sklearn.metrics import \n\nprec, recall, _ = (y_test, y_score, pos_label=clf.classes_[1])\npr_display = (precision=prec, recall=recall).plot()\n\n\n<img alt=\"plot display object visualization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_display_object_visualization_003.png\" srcset=\"../../_images/sphx_glr_plot_display_object_visualization_003.png\"/>",
            "code"
        ]
    ],
    "Examples->Miscellaneous->Visualizations with Display Objects->Load Data and train model->Combining the display objects into a single plot": [
        [
            "The display objects store the computed values that were passed as arguments.\nThis allows for the visualizations to be easliy combined using matplotlib\u2019s\nAPI. In the following example, we place the displays next to each other in a\nrow.\n</blockquote>",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n\nfig, (ax1, ax2) = (1, 2, figsize=(12, 8))\n\nroc_display.plot(ax=ax1)\npr_display.plot(ax=ax2)\n()\n\n\n<img alt=\"plot display object visualization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_display_object_visualization_004.png\" srcset=\"../../_images/sphx_glr_plot_display_object_visualization_004.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  4.373 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Missing Value Imputation->Imputing missing values before building an estimator": [
        [
            "Missing values can be replaced by the mean, the median or the most frequent\nvalue using the basic .",
            "markdown"
        ],
        [
            "In this example we will investigate different imputation techniques:",
            "markdown"
        ],
        [
            "imputation by the constant value 0",
            "markdown"
        ],
        [
            "imputation by the mean value of each feature combined with a missing-ness\nindicator auxiliary variable",
            "markdown"
        ],
        [
            "k nearest neighbor imputation",
            "markdown"
        ],
        [
            "iterative imputation",
            "markdown"
        ],
        [
            "We will use two datasets: Diabetes dataset which consists of 10 feature\nvariables collected from diabetes patients with an aim to predict disease\nprogression and California Housing dataset for which the target is the median\nhouse value for California districts.",
            "markdown"
        ],
        [
            "As neither of these datasets have missing values, we will remove some\nvalues to create new versions with artificially missing data. The performance\nof\n on the full original dataset\nis then compared the performance on the altered datasets with the artificially\nmissing values imputed using different techniques.",
            "markdown"
        ],
        [
            "# Authors: Maria Telenczuk  &lt;https://github.com/maikia\n# License: BSD 3 clause",
            "code"
        ]
    ],
    "Examples->Missing Value Imputation->Imputing missing values before building an estimator->Download the data and make missing values sets": [
        [
            "First we download the two datasets. Diabetes dataset is shipped with\nscikit-learn. It has 442 entries, each with 10 features. California Housing\ndataset is much larger with 20640 entries and 8 features. It needs to be\ndownloaded. We will only use the first 400 entries for the sake of speeding\nup the calculations but feel free to use the whole dataset.\n</blockquote>",
            "markdown"
        ],
        [
            "import numpy as np\n\nfrom sklearn.datasets import \nfrom sklearn.datasets import \n\n\nrng = (42)\n\nX_diabetes, y_diabetes = (return_X_y=True)\nX_california, y_california = (return_X_y=True)\nX_california = X_california[:300]\ny_california = y_california[:300]\nX_diabetes = X_diabetes[:300]\ny_diabetes = y_diabetes[:300]\n\n\ndef add_missing_values(X_full, y_full):\n    n_samples, n_features = X_full.shape\n\n    # Add missing values in 75% of the lines\n    missing_rate = 0.75\n    n_missing_samples = int(n_samples * missing_rate)\n\n    missing_samples = (n_samples, dtype=bool)\n    missing_samples[:n_missing_samples] = True\n\n    rng.shuffle(missing_samples)\n    missing_features = rng.randint(0, n_features, n_missing_samples)\n    X_missing = X_full.copy()\n    X_missing[missing_samples, missing_features] = \n    y_missing = y_full.copy()\n\n    return X_missing, y_missing\n\n\nX_miss_california, y_miss_california = add_missing_values(X_california, y_california)\n\nX_miss_diabetes, y_miss_diabetes = add_missing_values(X_diabetes, y_diabetes)",
            "code"
        ]
    ],
    "Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score": [
        [
            "Now we will write a function which will score the results on the differently\nimputed data. Let\u2019s look at each imputer separately:",
            "markdown"
        ],
        [
            "rng = (0)\n\nfrom sklearn.ensemble import \n\n# To use the experimental IterativeImputer, we need to explicitly ask for it:\nfrom sklearn.experimental import enable_iterative_imputer  # noqa\nfrom sklearn.impute import , , \nfrom sklearn.model_selection import \nfrom sklearn.pipeline import \n\n\nN_SPLITS = 4\nregressor = (random_state=0)",
            "code"
        ]
    ],
    "Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score->Missing information": [
        [
            "In addition to imputing the missing values, the imputers have an\nadd_indicator parameter that marks the values that were missing, which\nmight carry some information.",
            "markdown"
        ],
        [
            "def get_scores_for_imputer(imputer, X_missing, y_missing):\n    estimator = (imputer, regressor)\n    impute_scores = (\n        estimator, X_missing, y_missing, scoring=\"neg_mean_squared_error\", cv=N_SPLITS\n    )\n    return impute_scores\n\n\nx_labels = []\n\nmses_california = (5)\nstds_california = (5)\nmses_diabetes = (5)\nstds_diabetes = (5)",
            "code"
        ]
    ],
    "Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score->Estimate the score": [
        [
            "First, we want to estimate the score on the original data:",
            "markdown"
        ],
        [
            "def get_full_score(X_full, y_full):\n    full_scores = (\n        regressor, X_full, y_full, scoring=\"neg_mean_squared_error\", cv=N_SPLITS\n    )\n    return full_scores.mean(), full_scores.std()\n\n\nmses_california[0], stds_california[0] = get_full_score(X_california, y_california)\nmses_diabetes[0], stds_diabetes[0] = get_full_score(X_diabetes, y_diabetes)\nx_labels.append(\"Full data\")",
            "code"
        ]
    ],
    "Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score->Replace missing values by 0": [
        [
            "Now we will estimate the score on the data where the missing values are\nreplaced by 0:",
            "markdown"
        ],
        [
            "def get_impute_zero_score(X_missing, y_missing):\n\n    imputer = (\n        missing_values=, add_indicator=True, strategy=\"constant\", fill_value=0\n    )\n    zero_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n    return zero_impute_scores.mean(), zero_impute_scores.std()\n\n\nmses_california[1], stds_california[1] = get_impute_zero_score(\n    X_miss_california, y_miss_california\n)\nmses_diabetes[1], stds_diabetes[1] = get_impute_zero_score(\n    X_miss_diabetes, y_miss_diabetes\n)\nx_labels.append(\"Zero imputation\")",
            "code"
        ]
    ],
    "Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score->kNN-imputation of the missing values": [
        [
            "imputes missing values using the weighted\nor unweighted mean of the desired number of nearest neighbors.",
            "markdown"
        ],
        [
            "def get_impute_knn_score(X_missing, y_missing):\n    imputer = (missing_values=, add_indicator=True)\n    knn_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n    return knn_impute_scores.mean(), knn_impute_scores.std()\n\n\nmses_california[2], stds_california[2] = get_impute_knn_score(\n    X_miss_california, y_miss_california\n)\nmses_diabetes[2], stds_diabetes[2] = get_impute_knn_score(\n    X_miss_diabetes, y_miss_diabetes\n)\nx_labels.append(\"KNN Imputation\")",
            "code"
        ]
    ],
    "Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score->Impute missing values with mean": [
        [
            "def get_impute_mean(X_missing, y_missing):\n    imputer = (missing_values=, strategy=\"mean\", add_indicator=True)\n    mean_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n    return mean_impute_scores.mean(), mean_impute_scores.std()\n\n\nmses_california[3], stds_california[3] = get_impute_mean(\n    X_miss_california, y_miss_california\n)\nmses_diabetes[3], stds_diabetes[3] = get_impute_mean(X_miss_diabetes, y_miss_diabetes)\nx_labels.append(\"Mean Imputation\")",
            "code"
        ]
    ],
    "Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score->Iterative imputation of the missing values": [
        [
            "Another option is the . This uses\nround-robin linear regression, modeling each feature with missing values as a\nfunction of other features, in turn.\nThe version implemented assumes Gaussian (output) variables. If your features\nare obviously non-normal, consider transforming them to look more normal\nto potentially improve performance.",
            "markdown"
        ],
        [
            "def get_impute_iterative(X_missing, y_missing):\n    imputer = (\n        missing_values=,\n        add_indicator=True,\n        random_state=0,\n        n_nearest_features=3,\n        max_iter=1,\n        sample_posterior=True,\n    )\n    iterative_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n    return iterative_impute_scores.mean(), iterative_impute_scores.std()\n\n\nmses_california[4], stds_california[4] = get_impute_iterative(\n    X_miss_california, y_miss_california\n)\nmses_diabetes[4], stds_diabetes[4] = get_impute_iterative(\n    X_miss_diabetes, y_miss_diabetes\n)\nx_labels.append(\"Iterative Imputation\")\n\nmses_diabetes = mses_diabetes * -1\nmses_california = mses_california * -1",
            "code"
        ]
    ],
    "Examples->Missing Value Imputation->Imputing missing values before building an estimator->Plot the results": [
        [
            "Finally we are going to visualize the score:",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n\n\nn_bars = len(mses_diabetes)\nxval = (n_bars)\n\ncolors = [\"r\", \"g\", \"b\", \"orange\", \"black\"]\n\n# plot diabetes results\n(figsize=(12, 6))\nax1 = (121)\nfor j in xval:\n    ax1.barh(\n        j,\n        mses_diabetes[j],\n        xerr=stds_diabetes[j],\n        color=colors[j],\n        alpha=0.6,\n        align=\"center\",\n    )\n\nax1.set_title(\"Imputation Techniques with Diabetes Data\")\nax1.set_xlim(left=np.min(mses_diabetes) * 0.9, right=np.max(mses_diabetes) * 1.1)\nax1.set_yticks(xval)\nax1.set_xlabel(\"MSE\")\nax1.invert_yaxis()\nax1.set_yticklabels(x_labels)\n\n# plot california dataset results\nax2 = (122)\nfor j in xval:\n    ax2.barh(\n        j,\n        mses_california[j],\n        xerr=stds_california[j],\n        color=colors[j],\n        alpha=0.6,\n        align=\"center\",\n    )\n\nax2.set_title(\"Imputation Techniques with California Data\")\nax2.set_yticks(xval)\nax2.set_xlabel(\"MSE\")\nax2.invert_yaxis()\nax2.set_yticklabels([\"\"] * n_bars)\n\n()\n\n\n<img alt=\"Imputation Techniques with Diabetes Data, Imputation Techniques with California Data\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_missing_values_001.png\" srcset=\"../../_images/sphx_glr_plot_missing_values_001.png\"/>",
            "code"
        ],
        [
            "You can also try different techniques. For instance, the median is a more\nrobust estimator for data with high magnitude variables which could dominate\nresults (otherwise known as a \u2018long tail\u2019).",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  7.452 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Missing Value Imputation->Imputing missing values with variants of IterativeImputer": [
        [
            "The  class is very flexible - it can be\nused with a variety of estimators to do round-robin regression, treating every\nvariable as an output in turn.",
            "markdown"
        ],
        [
            "In this example we compare some estimators for the purpose of missing feature\nimputation with :",
            "markdown"
        ],
        [
            ": regularized linear regression",
            "markdown"
        ],
        [
            "RandomForestRegressor: Forests of randomized trees regression",
            "markdown"
        ],
        [
            "Nystroem,\n): a pipeline with the expansion of a degree 2\npolynomial kernel and regularized linear regression",
            "markdown"
        ],
        [
            ": comparable to other KNN\nimputation approaches",
            "markdown"
        ],
        [
            "Of particular interest is the ability of\n to mimic the behavior of missForest, a\npopular imputation package for R.",
            "markdown"
        ],
        [
            "Note that  is different from KNN\nimputation, which learns from samples with missing values by using a distance\nmetric that accounts for missing values, rather than imputing them.",
            "markdown"
        ],
        [
            "The goal is to compare different estimators to see which one is best for the\n when using a\n estimator on the California housing\ndataset with a single value randomly removed from each row.",
            "markdown"
        ],
        [
            "For this particular pattern of missing values we see that\n and\n give the best results.",
            "markdown"
        ],
        [
            "It should be noted that some estimators such as\n can natively deal with\nmissing features and are often recommended over building pipelines with\ncomplex and costly missing values imputation strategies.\n<img alt=\"California Housing Regression with Different Imputation Methods\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_iterative_imputer_variants_comparison_001.png\" srcset=\"../../_images/sphx_glr_plot_iterative_imputer_variants_comparison_001.png\"/>",
            "markdown"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# To use this experimental feature, we need to explicitly ask for it:\nfrom sklearn.experimental import enable_iterative_imputer  # noqa\nfrom sklearn.datasets import \nfrom sklearn.impute import \nfrom sklearn.impute import \nfrom sklearn.linear_model import , \nfrom sklearn.kernel_approximation import \nfrom sklearn.ensemble import \nfrom sklearn.neighbors import \nfrom sklearn.pipeline import \nfrom sklearn.model_selection import \n\nN_SPLITS = 5\n\nrng = (0)\n\nX_full, y_full = (return_X_y=True)\n# ~2k samples is enough for the purpose of the example.\n# Remove the following two lines for a slower run with different error bars.\nX_full = X_full[::10]\ny_full = y_full[::10]\nn_samples, n_features = X_full.shape\n\n# Estimate the score on the entire dataset, with no missing values\nbr_estimator = ()\nscore_full_data = (\n    (\n        br_estimator, X_full, y_full, scoring=\"neg_mean_squared_error\", cv=N_SPLITS\n    ),\n    columns=[\"Full Data\"],\n)\n\n# Add a single missing value to each row\nX_missing = X_full.copy()\ny_missing = y_full\nmissing_samples = (n_samples)\nmissing_features = rng.choice(n_features, n_samples, replace=True)\nX_missing[missing_samples, missing_features] = \n\n# Estimate the score after imputation (mean and median strategies)\nscore_simple_imputer = ()\nfor strategy in (\"mean\", \"median\"):\n    estimator = (\n        (missing_values=, strategy=strategy), br_estimator\n    )\n    score_simple_imputer[strategy] = (\n        estimator, X_missing, y_missing, scoring=\"neg_mean_squared_error\", cv=N_SPLITS\n    )\n\n# Estimate the score after iterative imputation of the missing values\n# with different estimators\nestimators = [\n    (),\n    (\n        # We tuned the hyperparameters of the RandomForestRegressor to get a good\n        # enough predictive performance for a restricted execution time.\n        n_estimators=4,\n        max_depth=10,\n        bootstrap=True,\n        max_samples=0.5,\n        n_jobs=2,\n        random_state=0,\n    ),\n    (\n        (kernel=\"polynomial\", degree=2, random_state=0), (alpha=1e3)\n    ),\n    (n_neighbors=15),\n]\nscore_iterative_imputer = ()\n# iterative imputer is sensible to the tolerance and\n# dependent on the estimator used internally.\n# we tuned the tolerance to keep this example run with limited computational\n# resources while not changing the results too much compared to keeping the\n# stricter default value for the tolerance parameter.\ntolerances = (1e-3, 1e-1, 1e-1, 1e-2)\nfor impute_estimator, tol in zip(estimators, tolerances):\n    estimator = (\n        (\n            random_state=0, estimator=impute_estimator, max_iter=25, tol=tol\n        ),\n        br_estimator,\n    )\n    score_iterative_imputer[impute_estimator.__class__.__name__] = (\n        estimator, X_missing, y_missing, scoring=\"neg_mean_squared_error\", cv=N_SPLITS\n    )\n\nscores = (\n    [score_full_data, score_simple_imputer, score_iterative_imputer],\n    keys=[\"Original\", \"SimpleImputer\", \"IterativeImputer\"],\n    axis=1,\n)\n\n# plot california housing results\nfig, ax = (figsize=(13, 6))\nmeans = -scores.mean()\nerrors = scores.std()\nmeans.plot.barh(xerr=errors, ax=ax)\nax.set_title(\"California Housing Regression with Different Imputation Methods\")\nax.set_xlabel(\"MSE (smaller is better)\")\nax.set_yticks((means.shape[0]))\nax.set_yticklabels([\" w/ \".join(label) for label in means.index.tolist()])\n(pad=1)\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  3.350 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Model Selection->Balance model complexity and cross-validated score": [
        [
            "This example balances model complexity and cross-validated score by\nfinding a decent accuracy within 1 standard deviation of the best accuracy\nscore while minimising the number of PCA components [1].",
            "markdown"
        ],
        [
            "The figure shows the trade-off between cross-validated score and the number\nof PCA components. The balanced case is when n_components=10 and accuracy=0.88,\nwhich falls into the range within 1 standard deviation of the best accuracy\nscore.",
            "markdown"
        ],
        [
            "[1] Hastie, T., Tibshirani, R.,, Friedman, J. (2001). Model Assessment and\nSelection. The Elements of Statistical Learning (pp. 219-260). New York,\nNY, USA: Springer New York Inc..\n<img alt=\"Balance model complexity and cross-validated score\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_grid_search_refit_callable_001.png\" srcset=\"../../_images/sphx_glr_plot_grid_search_refit_callable_001.png\"/>",
            "markdown"
        ],
        [
            "The best_index_ is 2\nThe n_components selected is 10\nThe corresponding accuracy score is 0.88\n\n\n\n<br/>",
            "code"
        ],
        [
            "# Author: Wenhao Zhang &lt;wenhaoz@ucla.edu\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import \nfrom sklearn.decomposition import \nfrom sklearn.model_selection import \nfrom sklearn.pipeline import \nfrom sklearn.svm import \n\n\ndef lower_bound(cv_results):\n    \"\"\"\n    Calculate the lower bound within 1 standard deviation\n    of the best `mean_test_scores`.\n\n    Parameters\n    ----------\n    cv_results : dict of numpy(masked) ndarrays\n        See attribute cv_results_ of `GridSearchCV`\n\n    Returns\n    -------\n    float\n        Lower bound within 1 standard deviation of the\n        best `mean_test_score`.\n    \"\"\"\n    best_score_idx = (cv_results[\"mean_test_score\"])\n\n    return (\n        cv_results[\"mean_test_score\"][best_score_idx]\n        - cv_results[\"std_test_score\"][best_score_idx]\n    )\n\n\ndef best_low_complexity(cv_results):\n    \"\"\"\n    Balance model complexity with cross-validated score.\n\n    Parameters\n    ----------\n    cv_results : dict of numpy(masked) ndarrays\n        See attribute cv_results_ of `GridSearchCV`.\n\n    Return\n    ------\n    int\n        Index of a model that has the fewest PCA components\n        while has its test score within 1 standard deviation of the best\n        `mean_test_score`.\n    \"\"\"\n    threshold = lower_bound(cv_results)\n    candidate_idx = (cv_results[\"mean_test_score\"] = threshold)\n    best_idx = candidate_idx[\n        cv_results[\"param_reduce_dim__n_components\"][candidate_idx].argmin()\n    ]\n    return best_idx\n\n\npipe = (\n    [\n        (\"reduce_dim\", (random_state=42)),\n        (\"classify\", (random_state=42, C=0.01)),\n    ]\n)\n\nparam_grid = {\"reduce_dim__n_components\": [6, 8, 10, 12, 14]}\n\ngrid = (\n    pipe,\n    cv=10,\n    n_jobs=1,\n    param_grid=param_grid,\n    scoring=\"accuracy\",\n    refit=best_low_complexity,\n)\nX, y = (return_X_y=True)\ngrid.fit(X, y)\n\nn_components = grid.cv_results_[\"param_reduce_dim__n_components\"]\ntest_scores = grid.cv_results_[\"mean_test_score\"]\n\n()\n(n_components, test_scores, width=1.3, color=\"b\")\n\nlower = lower_bound(grid.cv_results_)\n(np.max(test_scores), linestyle=\"--\", color=\"y\", label=\"Best score\")\n(lower, linestyle=\"--\", color=\".5\", label=\"Best score - 1 std\")\n\n(\"Balance model complexity and cross-validated score\")\n(\"Number of PCA components used\")\n(\"Digit classification accuracy\")\n(n_components.tolist())\n((0, 1.0))\n(loc=\"upper left\")\n\nbest_index_ = grid.best_index_\n\nprint(\"The best_index_ is %d\" % best_index_)\nprint(\"The n_components selected is %d\" % n_components[best_index_])\nprint(\n    \"The corresponding accuracy score is %.2f\"\n    % grid.cv_results_[\"mean_test_score\"][best_index_]\n)\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  4.668 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Model Selection->Class Likelihood Ratios to measure classification performance": [
        [
            "This example demonstrates the \nfunction, which computes the positive and negative likelihood ratios (LR+,\nLR-) to assess the predictive power of a binary classifier. As we will see,\nthese metrics are independent of the proportion between classes in the test set,\nwhich makes them very useful when the available data for a study has a different\nclass proportion than the target application.",
            "markdown"
        ],
        [
            "A typical use is a case-control study in medicine, which has nearly balanced\nclasses while the general population has large class imbalance. In such\napplication, the pre-test probability of an individual having the target\ncondition can be chosen to be the prevalence, i.e. the proportion of a\nparticular population found to be affected by a medical condition. The post-test\nprobabilities represent then the probability that the condition is truly present\ngiven a positive test result.",
            "markdown"
        ],
        [
            "In this example we first discuss the link between pre-test and post-test odds\ngiven by the . Then we evaluate their behavior in\nsome controlled scenarios. In the last section we plot them as a function of the\nprevalence of the positive class.",
            "markdown"
        ],
        [
            "# Authors:  Arturo Amor &lt;david-arturo.amor-quiroz@inria.fr\n#           Olivier Grisel &lt;olivier.grisel@ensta.org",
            "code"
        ]
    ],
    "Examples->Model Selection->Class Likelihood Ratios to measure classification performance->Pre-test vs. post-test analysis": [
        [
            "Suppose we have a population of subjects with physiological measurements X\nthat can hopefully serve as indirect bio-markers of the disease and actual\ndisease indicators y (ground truth). Most of the people in the population do\nnot carry the disease but a minority (in this case around 10%) does:",
            "markdown"
        ],
        [
            "from sklearn.datasets import \n\nX, y = (n_samples=10_000, weights=[0.9, 0.1], random_state=0)\nprint(f\"Percentage of people carrying the disease: {100*y.mean():.2f}%\")",
            "code"
        ],
        [
            "Percentage of people carrying the disease: 10.37%",
            "code"
        ],
        [
            "A machine learning model is built to diagnose if a person with some given\nphysiological measurements is likely to carry the disease of interest. To\nevaluate the model, we need to assess its performance on a held-out test set:",
            "markdown"
        ],
        [
            "from sklearn.model_selection import \n\nX_train, X_test, y_train, y_test = (X, y, random_state=0)",
            "code"
        ],
        [
            "Then we can fit our diagnosis model and compute the positive likelihood\nratio to evaluate the usefulness of this classifier as a disease diagnosis\ntool:",
            "markdown"
        ],
        [
            "from sklearn.metrics import \nfrom sklearn.linear_model import \n\nestimator = ().fit(X_train, y_train)\ny_pred = estimator.predict(X_test)\npos_LR, neg_LR = (y_test, y_pred)\nprint(f\"LR+: {pos_LR:.3f}\")",
            "code"
        ],
        [
            "LR+: 12.617",
            "code"
        ],
        [
            "Since the positive class likelihood ratio is much larger than 1.0, it means\nthat the machine learning-based diagnosis tool is useful: the post-test odds\nthat the condition is truly present given a positive test result are more than\n12 times larger than the pre-test odds.",
            "markdown"
        ]
    ],
    "Examples->Model Selection->Class Likelihood Ratios to measure classification performance->Cross-validation of likelihood ratios": [
        [
            "We assess the variability of the measurements for the class likelihood ratios\nin some particular cases.",
            "markdown"
        ],
        [
            "import pandas as pd\n\n\ndef scoring(estimator, X, y):\n    y_pred = estimator.predict(X)\n    pos_lr, neg_lr = (y, y_pred, raise_warning=False)\n    return {\"positive_likelihood_ratio\": pos_lr, \"negative_likelihood_ratio\": neg_lr}\n\n\ndef extract_score(cv_results):\n    lr = (\n        {\n            \"positive\": cv_results[\"test_positive_likelihood_ratio\"],\n            \"negative\": cv_results[\"test_negative_likelihood_ratio\"],\n        }\n    )\n    return lr.aggregate([\"mean\", \"std\"])",
            "code"
        ],
        [
            "We first validate the  model\nwith default hyperparameters as used in the previous section.",
            "markdown"
        ],
        [
            "from sklearn.model_selection import \n\nestimator = ()\nextract_score((estimator, X, y, scoring=scoring, cv=10))\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "We confirm that the model is useful: the post-test odds are between 12 and 20\ntimes larger than the pre-test odds.",
            "markdown"
        ],
        [
            "On the contrary, let\u2019s consider a dummy model that will output random\npredictions with similar odds as the average disease prevalence in the\ntraining set:",
            "markdown"
        ],
        [
            "from sklearn.dummy import \n\nestimator = (strategy=\"stratified\", random_state=1234)\nextract_score((estimator, X, y, scoring=scoring, cv=10))\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "Here both class likelihood ratios are compatible with 1.0 which makes this\nclassifier useless as a diagnostic tool to improve disease detection.",
            "markdown"
        ],
        [
            "Another option for the dummy model is to always predict the most frequent\nclass, which in this case is \u201cno-disease\u201d.",
            "markdown"
        ],
        [
            "estimator = (strategy=\"most_frequent\")\nextract_score((estimator, X, y, scoring=scoring, cv=10))\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "The absence of positive predictions means there will be no true positives nor\nfalse positives, leading to an undefined LR+ that by no means should be\ninterpreted as an infinite LR+ (the classifier perfectly identifying\npositive cases). In such situation the\n function returns nan and\nraises a warning by default. Indeed, the value of LR- helps us discard this\nmodel.",
            "markdown"
        ],
        [
            "A similar scenario may arise when cross-validating highly imbalanced data with\nfew samples: some folds will have no samples with the disease and therefore\nthey will output no true positives nor false negatives when used for testing.\nMathematically this leads to an infinite LR+, which should also not be\ninterpreted as the model perfectly identifying positive cases. Such event\nleads to a higher variance of the estimated likelihood ratios, but can still\nbe interpreted as an increment of the post-test odds of having the condition.",
            "markdown"
        ],
        [
            "estimator = ()\nX, y = (n_samples=300, weights=[0.9, 0.1], random_state=0)\nextract_score((estimator, X, y, scoring=scoring, cv=10))\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "code"
        ]
    ],
    "Examples->Model Selection->Class Likelihood Ratios to measure classification performance->Invariance with respect to prevalence": [
        [
            "The likelihood ratios are independent of the disease prevalence and can be\nextrapolated between populations regardless of any possible class imbalance,\n<strong>as long as the same model is applied to all of them</strong>. Notice that in the\nplots below <strong>the decision boundary is constant</strong> (see\n for\na study of the boundary decision for unbalanced classes).",
            "markdown"
        ],
        [
            "Here we train a  base model\non a case-control study with a prevalence of 50%. It is then evaluated over\npopulations with varying prevalence. We use the\n function to ensure the\ndata-generating process is always the same as shown in the plots below. The\nlabel 1 corresponds to the positive class \u201cdisease\u201d, whereas the label 0\nstands for \u201cno-disease\u201d.",
            "markdown"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom collections import \n\npopulations = (list)\ncommon_params = {\n    \"n_samples\": 10_000,\n    \"n_features\": 2,\n    \"n_informative\": 2,\n    \"n_redundant\": 0,\n    \"random_state\": 0,\n}\nweights = (0.1, 0.8, 6)\nweights = weights[::-1]\n\n# fit and evaluate base model on balanced classes\nX, y = (**common_params, weights=[0.5, 0.5])\nestimator = ().fit(X, y)\nlr_base = extract_score((estimator, X, y, scoring=scoring, cv=10))\npos_lr_base, pos_lr_base_std = lr_base[\"positive\"].values\nneg_lr_base, neg_lr_base_std = lr_base[\"negative\"].values",
            "code"
        ],
        [
            "We will now show the decision boundary for each level of prevalence. Note that\nwe only plot a subset of the original data to better assess the linear model\ndecision boundary.",
            "markdown"
        ],
        [
            "fig, axs = (nrows=3, ncols=2, figsize=(15, 12))\n\nfor ax, (n, weight) in zip(axs.ravel(), enumerate(weights)):\n\n    X, y = (\n        **common_params,\n        weights=[weight, 1 - weight],\n    )\n    prevalence = y.mean()\n    populations[\"prevalence\"].append(prevalence)\n    populations[\"X\"].append(X)\n    populations[\"y\"].append(y)\n\n    # down-sample for plotting\n    rng = (1)\n    plot_indices = rng.choice((X.shape[0]), size=500, replace=True)\n    X_plot, y_plot = X[plot_indices], y[plot_indices]\n\n    # plot fixed decision boundary of base model with varying prevalence\n    disp = (\n        estimator,\n        X_plot,\n        response_method=\"predict\",\n        alpha=0.5,\n        ax=ax,\n    )\n    scatter = disp.ax_.scatter(X_plot[:, 0], X_plot[:, 1], c=y_plot, edgecolor=\"k\")\n    disp.ax_.set_title(f\"prevalence = {y_plot.mean():.2f}\")\n    disp.ax_.legend(*scatter.legend_elements())\n\n\n<img alt=\"prevalence = 0.22, prevalence = 0.34, prevalence = 0.45, prevalence = 0.60, prevalence = 0.76, prevalence = 0.88\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_likelihood_ratios_001.png\" srcset=\"../../_images/sphx_glr_plot_likelihood_ratios_001.png\"/>",
            "code"
        ],
        [
            "We define a function for bootstraping.",
            "markdown"
        ],
        [
            "def scoring_on_bootstrap(estimator, X, y, rng, n_bootstrap=100):\n    results_for_prevalence = (list)\n    for _ in range(n_bootstrap):\n        bootstrap_indices = rng.choice(\n            (X.shape[0]), size=X.shape[0], replace=True\n        )\n        for key, value in scoring(\n            estimator, X[bootstrap_indices], y[bootstrap_indices]\n        ).items():\n            results_for_prevalence[key].append(value)\n    return (results_for_prevalence)",
            "code"
        ],
        [
            "We score the base model for each prevalence using bootstraping.",
            "markdown"
        ],
        [
            "results = (list)\nn_bootstrap = 100\nrng = (seed=0)\n\nfor prevalence, X, y in zip(\n    populations[\"prevalence\"], populations[\"X\"], populations[\"y\"]\n):\n\n    results_for_prevalence = scoring_on_bootstrap(\n        estimator, X, y, rng, n_bootstrap=n_bootstrap\n    )\n    results[\"prevalence\"].append(prevalence)\n    results[\"metrics\"].append(\n        results_for_prevalence.aggregate([\"mean\", \"std\"]).unstack()\n    )\n\nresults = (results[\"metrics\"], index=results[\"prevalence\"])\nresults.index.name = \"prevalence\"\nresults\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "In the plots below we observe that the class likelihood ratios re-computed\nwith different prevalences are indeed constant within one standard deviation\nof those computed with on balanced classes.",
            "markdown"
        ],
        [
            "fig, (ax1, ax2) = (nrows=1, ncols=2, figsize=(15, 6))\nresults[\"positive_likelihood_ratio\"][\"mean\"].plot(\n    ax=ax1, color=\"r\", label=\"extrapolation through populations\"\n)\nax1.axhline(y=pos_lr_base + pos_lr_base_std, color=\"r\", linestyle=\"--\")\nax1.axhline(\n    y=pos_lr_base - pos_lr_base_std,\n    color=\"r\",\n    linestyle=\"--\",\n    label=\"base model confidence band\",\n)\nax1.fill_between(\n    results.index,\n    results[\"positive_likelihood_ratio\"][\"mean\"]\n    - results[\"positive_likelihood_ratio\"][\"std\"],\n    results[\"positive_likelihood_ratio\"][\"mean\"]\n    + results[\"positive_likelihood_ratio\"][\"std\"],\n    color=\"r\",\n    alpha=0.3,\n)\nax1.set(\n    title=\"Positive likelihood ratio\",\n    ylabel=\"LR+\",\n    ylim=[0, 5],\n)\nax1.legend(loc=\"lower right\")\n\nax2 = results[\"negative_likelihood_ratio\"][\"mean\"].plot(\n    ax=ax2, color=\"b\", label=\"extrapolation through populations\"\n)\nax2.axhline(y=neg_lr_base + neg_lr_base_std, color=\"b\", linestyle=\"--\")\nax2.axhline(\n    y=neg_lr_base - neg_lr_base_std,\n    color=\"b\",\n    linestyle=\"--\",\n    label=\"base model confidence band\",\n)\nax2.fill_between(\n    results.index,\n    results[\"negative_likelihood_ratio\"][\"mean\"]\n    - results[\"negative_likelihood_ratio\"][\"std\"],\n    results[\"negative_likelihood_ratio\"][\"mean\"]\n    + results[\"negative_likelihood_ratio\"][\"std\"],\n    color=\"b\",\n    alpha=0.3,\n)\nax2.set(\n    title=\"Negative likelihood ratio\",\n    ylabel=\"LR-\",\n    ylim=[0, 0.5],\n)\nax2.legend(loc=\"lower right\")\n\n()\n\n\n<img alt=\"Positive likelihood ratio, Negative likelihood ratio\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_likelihood_ratios_002.png\" srcset=\"../../_images/sphx_glr_plot_likelihood_ratios_002.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  2.573 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Model Selection->Comparing randomized search and grid search for hyperparameter estimation": [
        [
            "Compare randomized search and grid search for optimizing hyperparameters of a\nlinear SVM with SGD training.\nAll parameters that influence the learning are searched simultaneously\n(except for the number of estimators, which poses a time / quality tradeoff).",
            "markdown"
        ],
        [
            "The randomized search and the grid search explore exactly the same space of\nparameters. The result in parameter settings is quite similar, while the run\ntime for randomized search is drastically lower.",
            "markdown"
        ],
        [
            "The performance is may slightly worse for the randomized search, and is likely\ndue to a noise effect and would not carry over to a held-out test set.",
            "markdown"
        ],
        [
            "Note that in practice, one would not search over this many different parameters\nsimultaneously using grid search, but pick only the ones deemed most important.",
            "markdown"
        ],
        [
            "RandomizedSearchCV took 1.19 seconds for 15 candidates parameter settings.\nModel with rank: 1\nMean validation score: 0.993 (std: 0.007)\nParameters: {'alpha': 0.01761326452827255, 'average': False, 'l1_ratio': 0.14458295175799263}\n\nModel with rank: 2\nMean validation score: 0.981 (std: 0.015)\nParameters: {'alpha': 0.2610311314285183, 'average': False, 'l1_ratio': 0.003999234857448686}\n\nModel with rank: 3\nMean validation score: 0.980 (std: 0.011)\nParameters: {'alpha': 0.01672701940764589, 'average': True, 'l1_ratio': 0.5720620992012775}\n\nGridSearchCV took 3.22 seconds for 60 candidate parameter settings.\nModel with rank: 1\nMean validation score: 0.996 (std: 0.005)\nParameters: {'alpha': 0.01, 'average': False, 'l1_ratio': 0.6666666666666666}\n\nModel with rank: 2\nMean validation score: 0.993 (std: 0.009)\nParameters: {'alpha': 0.09999999999999999, 'average': False, 'l1_ratio': 0.2222222222222222}\n\nModel with rank: 3\nMean validation score: 0.993 (std: 0.007)\nParameters: {'alpha': 0.01, 'average': False, 'l1_ratio': 0.1111111111111111}\n\n\n\n<br/>",
            "code"
        ],
        [
            "import numpy as np\n\nfrom time import \nimport scipy.stats as stats\nfrom sklearn.utils.fixes import loguniform\n\nfrom sklearn.model_selection import , \nfrom sklearn.datasets import \nfrom sklearn.linear_model import \n\n# get some data\nX, y = (return_X_y=True, n_class=3)\n\n# build a classifier\nclf = (loss=\"hinge\", penalty=\"elasticnet\", fit_intercept=True)\n\n\n# Utility function to report best scores\ndef report(results, n_top=3):\n    for i in range(1, n_top + 1):\n        candidates = (results[\"rank_test_score\"] == i)\n        for candidate in candidates:\n            print(\"Model with rank: {0}\".format(i))\n            print(\n                \"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n                    results[\"mean_test_score\"][candidate],\n                    results[\"std_test_score\"][candidate],\n                )\n            )\n            print(\"Parameters: {0}\".format(results[\"params\"][candidate]))\n            print(\"\")\n\n\n# specify parameters and distributions to sample from\nparam_dist = {\n    \"average\": [True, False],\n    \"l1_ratio\": (0, 1),\n    \"alpha\": loguniform(1e-2, 1e0),\n}\n\n# run randomized search\nn_iter_search = 15\nrandom_search = (\n    clf, param_distributions=param_dist, n_iter=n_iter_search\n)\n\nstart = ()\nrandom_search.fit(X, y)\nprint(\n    \"RandomizedSearchCV took %.2f seconds for %d candidates parameter settings.\"\n    % ((() - start), n_iter_search)\n)\nreport(random_search.cv_results_)\n\n# use a full grid over all parameters\nparam_grid = {\n    \"average\": [True, False],\n    \"l1_ratio\": (0, 1, num=10),\n    \"alpha\": (10, (-2, 1, dtype=float)),\n}\n\n# run grid search\ngrid_search = (clf, param_grid=param_grid)\nstart = ()\ngrid_search.fit(X, y)\n\nprint(\n    \"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n    % (() - start, len(grid_search.cv_results_[\"params\"]))\n)\nreport(grid_search.cv_results_)",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  4.418 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Model Selection->Comparison between grid search and successive halving": [
        [
            "This example compares the parameter search performed by\n and\n.",
            "markdown"
        ],
        [
            "from time import \n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.svm import \nfrom sklearn import datasets\nfrom sklearn.model_selection import \nfrom sklearn.experimental import enable_halving_search_cv  # noqa\nfrom sklearn.model_selection import",
            "code"
        ],
        [
            "We first define the parameter space for an \nestimator, and compute the time required to train a\n instance, as well as a\n instance.",
            "markdown"
        ],
        [
            "rng = (0)\nX, y = (n_samples=1000, random_state=rng)\n\ngammas = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7]\nCs = [1, 10, 100, 1e3, 1e4, 1e5]\nparam_grid = {\"gamma\": gammas, \"C\": Cs}\n\nclf = (random_state=rng)\n\ntic = ()\ngsh = (\n    estimator=clf, param_grid=param_grid, factor=2, random_state=rng\n)\ngsh.fit(X, y)\ngsh_time = () - tic\n\ntic = ()\ngs = (estimator=clf, param_grid=param_grid)\ngs.fit(X, y)\ngs_time = () - tic",
            "code"
        ],
        [
            "We now plot heatmaps for both search estimators.",
            "markdown"
        ],
        [
            "def make_heatmap(ax, gs, is_sh=False, make_cbar=False):\n    \"\"\"Helper to make a heatmap.\"\"\"\n    results = (gs.cv_results_)\n    results[[\"param_C\", \"param_gamma\"]] = results[[\"param_C\", \"param_gamma\"]].astype(\n        \n    )\n    if is_sh:\n        # SH dataframe: get mean_test_score values for the highest iter\n        scores_matrix = results.sort_values(\"iter\").pivot_table(\n            index=\"param_gamma\",\n            columns=\"param_C\",\n            values=\"mean_test_score\",\n            aggfunc=\"last\",\n        )\n    else:\n        scores_matrix = results.pivot(\n            index=\"param_gamma\", columns=\"param_C\", values=\"mean_test_score\"\n        )\n\n    im = ax.imshow(scores_matrix)\n\n    ax.set_xticks((len(Cs)))\n    ax.set_xticklabels([\"{:.0E}\".format(x) for x in Cs])\n    ax.set_xlabel(\"C\", fontsize=15)\n\n    ax.set_yticks((len(gammas)))\n    ax.set_yticklabels([\"{:.0E}\".format(x) for x in gammas])\n    ax.set_ylabel(\"gamma\", fontsize=15)\n\n    # Rotate the tick labels and set their alignment.\n    (ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n\n    if is_sh:\n        iterations = results.pivot_table(\n            index=\"param_gamma\", columns=\"param_C\", values=\"iter\", aggfunc=\"max\"\n        ).values\n        for i in range(len(gammas)):\n            for j in range(len(Cs)):\n                ax.text(\n                    j,\n                    i,\n                    iterations[i, j],\n                    ha=\"center\",\n                    va=\"center\",\n                    color=\"w\",\n                    fontsize=20,\n                )\n\n    if make_cbar:\n        fig.subplots_adjust(right=0.8)\n        cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n        fig.colorbar(im, cax=cbar_ax)\n        cbar_ax.set_ylabel(\"mean_test_score\", rotation=-90, va=\"bottom\", fontsize=15)\n\n\nfig, axes = (ncols=2, sharey=True)\nax1, ax2 = axes\n\nmake_heatmap(ax1, gsh, is_sh=True)\nmake_heatmap(ax2, gs, make_cbar=True)\n\nax1.set_title(\"Successive Halving\\ntime = {:.3f}s\".format(gsh_time), fontsize=15)\nax2.set_title(\"GridSearch\\ntime = {:.3f}s\".format(gs_time), fontsize=15)\n\n()\n\n\n<img alt=\"Successive Halving time = 1.240s, GridSearch time = 5.943s\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_successive_halving_heatmap_001.png\" srcset=\"../../_images/sphx_glr_plot_successive_halving_heatmap_001.png\"/>",
            "code"
        ],
        [
            "The heatmaps show the mean test score of the parameter combinations for an\n instance. The\n also shows the\niteration at which the combinations where last used. The combinations marked\nas 0 were only evaluated at the first iteration, while the ones with\n5 are the parameter combinations that are considered the best ones.",
            "markdown"
        ],
        [
            "We can see that the \nclass is able to find parameter combinations that are just as accurate as\n, in much less time.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  7.365 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Model Selection->Confusion matrix": [
        [
            "Example of confusion matrix usage to evaluate the quality\nof the output of a classifier on the iris data set. The\ndiagonal elements represent the number of points for which\nthe predicted label is equal to the true label, while\noff-diagonal elements are those that are mislabeled by the\nclassifier. The higher the diagonal values of the confusion\nmatrix the better, indicating many correct predictions.",
            "markdown"
        ],
        [
            "The figures show the confusion matrix with and without\nnormalization by class support size (number of elements\nin each class). This kind of normalization can be\ninteresting in case of class imbalance to have a more\nvisual interpretation of which class is being misclassified.",
            "markdown"
        ],
        [
            "Here the results are not as good as they could be as our\nchoice for the regularization parameter C was not the best.\nIn real life applications this parameter is usually chosen\nusing .\n\n<img alt=\"Confusion matrix, without normalization\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_confusion_matrix_001.png\" srcset=\"../../_images/sphx_glr_plot_confusion_matrix_001.png\"/>\n<img alt=\"Normalized confusion matrix\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_confusion_matrix_002.png\" srcset=\"../../_images/sphx_glr_plot_confusion_matrix_002.png\"/>",
            "markdown"
        ],
        [
            "Confusion matrix, without normalization\n[[13  0  0]\n [ 0 10  6]\n [ 0  0  9]]\nNormalized confusion matrix\n[[1.   0.   0.  ]\n [0.   0.62 0.38]\n [0.   0.   1.  ]]\n\n\n\n<br/>",
            "code"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import svm, datasets\nfrom sklearn.model_selection import \nfrom sklearn.metrics import ConfusionMatrixDisplay\n\n# import some data to play with\niris = ()\nX = iris.data\ny = iris.target\nclass_names = iris.target_names\n\n# Split the data into a training set and a test set\nX_train, X_test, y_train, y_test = (X, y, random_state=0)\n\n# Run classifier, using a model that is too regularized (C too low) to see\n# the impact on the results\nclassifier = (kernel=\"linear\", C=0.01).fit(X_train, y_train)\n\n(precision=2)\n\n# Plot non-normalized confusion matrix\ntitles_options = [\n    (\"Confusion matrix, without normalization\", None),\n    (\"Normalized confusion matrix\", \"true\"),\n]\nfor title, normalize in titles_options:\n    disp = (\n        classifier,\n        X_test,\n        y_test,\n        display_labels=class_names,\n        cmap=plt.cm.Blues,\n        normalize=normalize,\n    )\n    disp.ax_.set_title(title)\n\n    print(title)\n    print(disp.confusion_matrix)\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.188 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Model Selection->Custom refit strategy of a grid search with cross-validation": [
        [
            "This examples shows how a classifier is optimized by cross-validation,\nwhich is done using the  object\non a development set that comprises only half of the available labeled data.",
            "markdown"
        ],
        [
            "The performance of the selected hyper-parameters and trained model is\nthen measured on a dedicated evaluation set that was not used during\nthe model selection step.",
            "markdown"
        ],
        [
            "More details on tools available for model selection can be found in the\nsections on  and .",
            "markdown"
        ]
    ],
    "Examples->Model Selection->Custom refit strategy of a grid search with cross-validation->The dataset": [
        [
            "We will work with the digits dataset. The goal is to classify handwritten\ndigits images.\nWe transform the problem into a binary classification for easier\nunderstanding: the goal is to identify whether a digit is 8 or not.",
            "markdown"
        ],
        [
            "from sklearn import datasets\n\ndigits = ()",
            "code"
        ],
        [
            "In order to train a classifier on images, we need to flatten them into vectors.\nEach image of 8 by 8 pixels needs to be transformed to a vector of 64 pixels.\nThus, we will get a final data array of shape (n_images, n_pixels).",
            "markdown"
        ],
        [
            "n_samples = len(digits.images)\nX = digits.images.reshape((n_samples, -1))\ny = digits.target == 8\nprint(\n    f\"The number of images is {X.shape[0]} and each image contains {X.shape[1]} pixels\"\n)",
            "code"
        ],
        [
            "The number of images is 1797 and each image contains 64 pixels",
            "code"
        ],
        [
            "As presented in the introduction, the data will be split into a training\nand a testing set of equal size.",
            "markdown"
        ],
        [
            "from sklearn.model_selection import \n\nX_train, X_test, y_train, y_test = (X, y, test_size=0.5, random_state=0)",
            "code"
        ]
    ],
    "Examples->Model Selection->Custom refit strategy of a grid search with cross-validation->Define our grid-search strategy": [
        [
            "We will select a classifier by searching the best hyper-parameters on folds\nof the training set. To do this, we need to define\nthe scores to select the best candidate.",
            "markdown"
        ],
        [
            "scores = [\"precision\", \"recall\"]",
            "code"
        ],
        [
            "We can also define a function to be passed to the refit parameter of the\n instance. It will implement the\ncustom strategy to select the best candidate from the cv_results_ attribute\nof the . Once the candidate is\nselected, it is automatically refitted by the\n instance.",
            "markdown"
        ],
        [
            "Here, the strategy is to short-list the models which are the best in terms of\nprecision and recall. From the selected models, we finally select the fastest\nmodel at predicting. Notice that these custom choices are completely\narbitrary.",
            "markdown"
        ],
        [
            "import pandas as pd\n\n\ndef print_dataframe(filtered_cv_results):\n    \"\"\"Pretty print for filtered dataframe\"\"\"\n    for mean_precision, std_precision, mean_recall, std_recall, params in zip(\n        filtered_cv_results[\"mean_test_precision\"],\n        filtered_cv_results[\"std_test_precision\"],\n        filtered_cv_results[\"mean_test_recall\"],\n        filtered_cv_results[\"std_test_recall\"],\n        filtered_cv_results[\"params\"],\n    ):\n        print(\n            f\"precision: {mean_precision:0.3f} (\u00b1{std_precision:0.03f}),\"\n            f\" recall: {mean_recall:0.3f} (\u00b1{std_recall:0.03f}),\"\n            f\" for {params}\"\n        )\n    print()\n\n\ndef refit_strategy(cv_results):\n    \"\"\"Define the strategy to select the best estimator.\n\n    The strategy defined here is to filter-out all results below a precision threshold\n    of 0.98, rank the remaining by recall and keep all models with one standard\n    deviation of the best by recall. Once these models are selected, we can select the\n    fastest model to predict.\n\n    Parameters\n    ----------\n    cv_results : dict of numpy (masked) ndarrays\n        CV results as returned by the `GridSearchCV`.\n\n    Returns\n    -------\n    best_index : int\n        The index of the best estimator as it appears in `cv_results`.\n    \"\"\"\n    # print the info about the grid-search for the different scores\n    precision_threshold = 0.98\n\n    cv_results_ = (cv_results)\n    print(\"All grid-search results:\")\n    print_dataframe(cv_results_)\n\n    # Filter-out all results below the threshold\n    high_precision_cv_results = cv_results_[\n        cv_results_[\"mean_test_precision\"]  precision_threshold\n    ]\n\n    print(f\"Models with a precision higher than {precision_threshold}:\")\n    print_dataframe(high_precision_cv_results)\n\n    high_precision_cv_results = high_precision_cv_results[\n        [\n            \"mean_score_time\",\n            \"mean_test_recall\",\n            \"std_test_recall\",\n            \"mean_test_precision\",\n            \"std_test_precision\",\n            \"rank_test_recall\",\n            \"rank_test_precision\",\n            \"params\",\n        ]\n    ]\n\n    # Select the most performant models in terms of recall\n    # (within 1 sigma from the best)\n    best_recall_std = high_precision_cv_results[\"mean_test_recall\"].std()\n    best_recall = high_precision_cv_results[\"mean_test_recall\"].max()\n    best_recall_threshold = best_recall - best_recall_std\n\n    high_recall_cv_results = high_precision_cv_results[\n        high_precision_cv_results[\"mean_test_recall\"]  best_recall_threshold\n    ]\n    print(\n        \"Out of the previously selected high precision models, we keep all the\\n\"\n        \"the models within one standard deviation of the highest recall model:\"\n    )\n    print_dataframe(high_recall_cv_results)\n\n    # From the best candidates, select the fastest model to predict\n    fastest_top_recall_high_precision_index = high_recall_cv_results[\n        \"mean_score_time\"\n    ].idxmin()\n\n    print(\n        \"\\nThe selected final model is the fastest to predict out of the previously\\n\"\n        \"selected subset of best models based on precision and recall.\\n\"\n        \"Its scoring time is:\\n\\n\"\n        f\"{high_recall_cv_results.loc[fastest_top_recall_high_precision_index]}\"\n    )\n\n    return fastest_top_recall_high_precision_index",
            "code"
        ]
    ],
    "Examples->Model Selection->Custom refit strategy of a grid search with cross-validation->Tuning hyper-parameters": [
        [
            "Once we defined our strategy to select the best model, we define the values\nof the hyper-parameters and create the grid-search instance:",
            "markdown"
        ],
        [
            "from sklearn.model_selection import \nfrom sklearn.svm import \n\ntuned_parameters = [\n    {\"kernel\": [\"rbf\"], \"gamma\": [1e-3, 1e-4], \"C\": [1, 10, 100, 1000]},\n    {\"kernel\": [\"linear\"], \"C\": [1, 10, 100, 1000]},\n]\n\ngrid_search = (\n    (), tuned_parameters, scoring=scores, refit=refit_strategy\n)\ngrid_search.fit(X_train, y_train)",
            "code"
        ],
        [
            "All grid-search results:\nprecision: 1.000 (\u00b10.000), recall: 0.854 (\u00b10.063), for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\nprecision: 1.000 (\u00b10.000), recall: 0.257 (\u00b10.061), for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\nprecision: 1.000 (\u00b10.000), recall: 0.877 (\u00b10.069), for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\nprecision: 0.968 (\u00b10.039), recall: 0.780 (\u00b10.083), for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\nprecision: 1.000 (\u00b10.000), recall: 0.877 (\u00b10.069), for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\nprecision: 0.905 (\u00b10.058), recall: 0.889 (\u00b10.074), for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\nprecision: 1.000 (\u00b10.000), recall: 0.877 (\u00b10.069), for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\nprecision: 0.904 (\u00b10.058), recall: 0.890 (\u00b10.073), for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\nprecision: 0.695 (\u00b10.073), recall: 0.743 (\u00b10.065), for {'C': 1, 'kernel': 'linear'}\nprecision: 0.643 (\u00b10.066), recall: 0.757 (\u00b10.066), for {'C': 10, 'kernel': 'linear'}\nprecision: 0.611 (\u00b10.028), recall: 0.744 (\u00b10.044), for {'C': 100, 'kernel': 'linear'}\nprecision: 0.618 (\u00b10.039), recall: 0.744 (\u00b10.044), for {'C': 1000, 'kernel': 'linear'}\n\nModels with a precision higher than 0.98:\nprecision: 1.000 (\u00b10.000), recall: 0.854 (\u00b10.063), for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\nprecision: 1.000 (\u00b10.000), recall: 0.257 (\u00b10.061), for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\nprecision: 1.000 (\u00b10.000), recall: 0.877 (\u00b10.069), for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\nprecision: 1.000 (\u00b10.000), recall: 0.877 (\u00b10.069), for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\nprecision: 1.000 (\u00b10.000), recall: 0.877 (\u00b10.069), for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n\nOut of the previously selected high precision models, we keep all the\nthe models within one standard deviation of the highest recall model:\nprecision: 1.000 (\u00b10.000), recall: 0.854 (\u00b10.063), for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\nprecision: 1.000 (\u00b10.000), recall: 0.877 (\u00b10.069), for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\nprecision: 1.000 (\u00b10.000), recall: 0.877 (\u00b10.069), for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\nprecision: 1.000 (\u00b10.000), recall: 0.877 (\u00b10.069), for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n\n\nThe selected final model is the fastest to predict out of the previously\nselected subset of best models based on precision and recall.\nIts scoring time is:\n\nmean_score_time                                         0.004384\nmean_test_recall                                        0.853676\nstd_test_recall                                         0.063184\nmean_test_precision                                          1.0\nstd_test_precision                                           0.0\nrank_test_recall                                               6\nrank_test_precision                                            1\nparams                 {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\nName: 0, dtype: object",
            "code"
        ],
        [
            "GridSearchCV(estimator=SVC(),\n             param_grid=[{'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001],\n                          'kernel': ['rbf']},\n                         {'C': [1, 10, 100, 1000], 'kernel': ['linear']}],\n             refit=&lt;function refit_strategy at 0x7f0d3fb6d940,\n             scoring=['precision', 'recall'])<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-220\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-220\">GridSearchCV</label>",
            "code"
        ],
        [
            "GridSearchCV(estimator=SVC(),\n             param_grid=[{'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001],\n                          'kernel': ['rbf']},\n                         {'C': [1, 10, 100, 1000], 'kernel': ['linear']}],\n             refit=&lt;function refit_strategy at 0x7f0d3fb6d940,\n             scoring=['precision', 'recall'])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-221\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-221\">estimator: SVC</label>",
            "code"
        ],
        [
            "SVC()<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-222\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-222\">SVC</label>",
            "code"
        ],
        [
            "SVC()\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "The parameters selected by the grid-search with our custom strategy are:",
            "markdown"
        ],
        [
            "grid_search.best_params_",
            "code"
        ],
        [
            "{'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}",
            "code"
        ],
        [
            "Finally, we evaluate the fine-tuned model on the left-out evaluation set: the\ngrid_search object <strong>has automatically been refit</strong> on the full training\nset with the parameters selected by our custom refit strategy.",
            "markdown"
        ],
        [
            "We can use the classification report to compute standard classification\nmetrics on the left-out set:",
            "markdown"
        ],
        [
            "from sklearn.metrics import \n\ny_pred = grid_search.predict(X_test)\nprint((y_test, y_pred))",
            "code"
        ],
        [
            "precision    recall  f1-score   support\n\n       False       0.98      1.00      0.99       807\n        True       1.00      0.85      0.92        92\n\n    accuracy                           0.98       899\n   macro avg       0.99      0.92      0.95       899\nweighted avg       0.98      0.98      0.98       899",
            "code"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "The problem is too easy: the hyperparameter plateau is too flat and the\noutput model is the same for precision and recall with ties in quality.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  9.525 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Model Selection->Demonstration of multi-metric evaluation on cross_val_score and GridSearchCV": [
        [
            "Multiple metric parameter search can be done by setting the scoring\nparameter to a list of metric scorer names or a dict mapping the scorer names\nto the scorer callables.",
            "markdown"
        ],
        [
            "The scores of all the scorers are available in the cv_results_ dict at keys\nending in '_&lt;scorer_name>' ('mean_test_precision',\n'rank_test_precision', etc\u2026)",
            "markdown"
        ],
        [
            "The best_estimator_, best_index_, best_score_ and best_params_\ncorrespond to the scorer (key) that is set to the refit attribute.",
            "markdown"
        ],
        [
            "# Author: Raghav RV &lt;rvraghav93@gmail.com\n# License: BSD\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import \nfrom sklearn.model_selection import \nfrom sklearn.metrics import \nfrom sklearn.metrics import \nfrom sklearn.tree import",
            "code"
        ]
    ],
    "Examples->Model Selection->Demonstration of multi-metric evaluation on cross_val_score and GridSearchCV->Running GridSearchCV using multiple evaluation metrics": [
        [
            "X, y = (n_samples=8000, random_state=42)\n\n# The scorers can be either one of the predefined metric strings or a scorer\n# callable, like the one returned by make_scorer\nscoring = {\"AUC\": \"roc_auc\", \"Accuracy\": ()}\n\n# Setting refit='AUC', refits an estimator on the whole dataset with the\n# parameter setting that has the best cross-validated AUC score.\n# That estimator is made available at ``gs.best_estimator_`` along with\n# parameters like ``gs.best_score_``, ``gs.best_params_`` and\n# ``gs.best_index_``\ngs = (\n    (random_state=42),\n    param_grid={\"min_samples_split\": range(2, 403, 20)},\n    scoring=scoring,\n    refit=\"AUC\",\n    n_jobs=2,\n    return_train_score=True,\n)\ngs.fit(X, y)\nresults = gs.cv_results_",
            "code"
        ]
    ],
    "Examples->Model Selection->Demonstration of multi-metric evaluation on cross_val_score and GridSearchCV->Plotting the result": [
        [
            "(figsize=(13, 13))\n(\"GridSearchCV evaluating using multiple scorers simultaneously\", fontsize=16)\n\n(\"min_samples_split\")\n(\"Score\")\n\nax = ()\nax.set_xlim(0, 402)\nax.set_ylim(0.73, 1)\n\n# Get the regular numpy array from the MaskedArray\nX_axis = (results[\"param_min_samples_split\"].data, dtype=float)\n\nfor scorer, color in zip(sorted(scoring), [\"g\", \"k\"]):\n    for sample, style in ((\"train\", \"--\"), (\"test\", \"-\")):\n        sample_score_mean = results[\"mean_%s_%s\" % (sample, scorer)]\n        sample_score_std = results[\"std_%s_%s\" % (sample, scorer)]\n        ax.fill_between(\n            X_axis,\n            sample_score_mean - sample_score_std,\n            sample_score_mean + sample_score_std,\n            alpha=0.1 if sample == \"test\" else 0,\n            color=color,\n        )\n        ax.plot(\n            X_axis,\n            sample_score_mean,\n            style,\n            color=color,\n            alpha=1 if sample == \"test\" else 0.7,\n            label=\"%s (%s)\" % (scorer, sample),\n        )\n\n    best_index = (results[\"rank_test_%s\" % scorer] == 1)[0][0]\n    best_score = results[\"mean_test_%s\" % scorer][best_index]\n\n    # Plot a dotted vertical line at the best score for that scorer marked by x\n    ax.plot(\n        [\n            X_axis[best_index],\n        ]\n        * 2,\n        [0, best_score],\n        linestyle=\"-.\",\n        color=color,\n        marker=\"x\",\n        markeredgewidth=3,\n        ms=8,\n    )\n\n    # Annotate the best score for that scorer\n    ax.annotate(\"%0.2f\" % best_score, (X_axis[best_index], best_score + 0.005))\n\n(loc=\"best\")\n(False)\n()\n\n\n<img alt=\"GridSearchCV evaluating using multiple scorers simultaneously\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_multi_metric_evaluation_001.png\" srcset=\"../../_images/sphx_glr_plot_multi_metric_evaluation_001.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  6.390 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Model Selection->Detection error tradeoff (DET) curve": [
        [
            "In this example, we compare two binary classification multi-threshold metrics:\nthe Receiver Operating Characteristic (ROC) and the Detection Error Tradeoff\n(DET). For such purpose, we evaluate two different classifiers for the same\nclassification task.",
            "markdown"
        ],
        [
            "ROC curves feature true positive rate (TPR) on the Y axis, and false positive\nrate (FPR) on the X axis. This means that the top left corner of the plot is the\n\u201cideal\u201d point - a FPR of zero, and a TPR of one.",
            "markdown"
        ],
        [
            "DET curves are a variation of ROC curves where False Negative Rate (FNR) is\nplotted on the y-axis instead of the TPR. In this case the origin (bottom left\ncorner) is the \u201cideal\u201d point.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "See  for further information about ROC\ncurves.",
            "markdown"
        ],
        [
            "See  for further information about\nDET curves.",
            "markdown"
        ],
        [
            "This example is loosely based on\n\nexample.",
            "markdown"
        ],
        [
            "See  for\nan example estimating the variance of the ROC curves and ROC-AUC.",
            "markdown"
        ]
    ],
    "Examples->Model Selection->Detection error tradeoff (DET) curve->Generate synthetic data": [
        [
            "from sklearn.datasets import \nfrom sklearn.model_selection import \nfrom sklearn.preprocessing import \n\nX, y = (\n    n_samples=1_000,\n    n_features=2,\n    n_redundant=0,\n    n_informative=2,\n    random_state=1,\n    n_clusters_per_class=1,\n)\n\nX_train, X_test, y_train, y_test = (X, y, test_size=0.4, random_state=0)",
            "code"
        ]
    ],
    "Examples->Model Selection->Detection error tradeoff (DET) curve->Define the classifiers": [
        [
            "Here we define two different classifiers. The goal is to visualy compare their\nstatistical performance across thresholds using the ROC and DET curves. There\nis no particular reason why these classifiers are chosen other classifiers\navailable in scikit-learn.",
            "markdown"
        ],
        [
            "from sklearn.ensemble import \nfrom sklearn.pipeline import \nfrom sklearn.svm import \n\nclassifiers = {\n    \"Linear SVM\": ((), (C=0.025)),\n    \"Random Forest\": (\n        max_depth=5, n_estimators=10, max_features=1\n    ),\n}",
            "code"
        ]
    ],
    "Examples->Model Selection->Detection error tradeoff (DET) curve->Plot ROC and DET curves": [
        [
            "DET curves are commonly plotted in normal deviate scale. To achieve this the\nDET display transforms the error rates as returned by the\n and the axis scale using\nscipy.stats.norm.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\nfrom sklearn.metrics import DetCurveDisplay, RocCurveDisplay\n\nfig, [ax_roc, ax_det] = (1, 2, figsize=(11, 5))\n\nfor name, clf in classifiers.items():\n    clf.fit(X_train, y_train)\n\n    (clf, X_test, y_test, ax=ax_roc, name=name)\n    (clf, X_test, y_test, ax=ax_det, name=name)\n\nax_roc.set_title(\"Receiver Operating Characteristic (ROC) curves\")\nax_det.set_title(\"Detection Error Tradeoff (DET) curves\")\n\nax_roc.grid(linestyle=\"--\")\nax_det.grid(linestyle=\"--\")\n\n()\n()\n\n\n<img alt=\"Receiver Operating Characteristic (ROC) curves, Detection Error Tradeoff (DET) curves\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_det_001.png\" srcset=\"../../_images/sphx_glr_plot_det_001.png\"/>",
            "code"
        ],
        [
            "Notice that it is easier to visually assess the overall performance of\ndifferent classification algorithms using DET curves than using ROC curves. As\nROC curves are plot in a linear scale, different classifiers usually appear\nsimilar for a large part of the plot and differ the most in the top left\ncorner of the graph. On the other hand, because DET curves represent straight\nlines in normal deviate scale, they tend to be distinguishable as a whole and\nthe area of interest spans a large part of the plot.",
            "markdown"
        ],
        [
            "DET curves give direct feedback of the detection error tradeoff to aid in\noperating point analysis. The user can then decide the FNR they are willing to\naccept at the expense of the FPR (or vice-versa).",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.202 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)": [
        [
            "This example describes the use of the Receiver Operating Characteristic (ROC)\nmetric to evaluate the quality of multiclass classifiers.",
            "markdown"
        ],
        [
            "ROC curves typically feature true positive rate (TPR) on the Y axis, and false\npositive rate (FPR) on the X axis. This means that the top left corner of the\nplot is the \u201cideal\u201d point - a FPR of zero, and a TPR of one. This is not very\nrealistic, but it does mean that a larger area under the curve (AUC) is usually\nbetter. The \u201csteepness\u201d of ROC curves is also important, since it is ideal to\nmaximize the TPR while minimizing the FPR.",
            "markdown"
        ],
        [
            "ROC curves are typically used in binary classification, where the TPR and FPR\ncan be defined unambiguously. In the case of multiclass classification, a notion\nof TPR or FPR is obtained only after binarizing the output. This can be done in\n2 different ways:",
            "markdown"
        ],
        [
            "the One-vs-Rest scheme compares each class against all the others (assumed as\none);",
            "markdown"
        ],
        [
            "the One-vs-One scheme compares every unique pairwise combination of classes.",
            "markdown"
        ],
        [
            "In this example we explore both schemes and demo the concepts of micro and macro\naveraging as different ways of summarizing the information of the multiclass ROC\ncurves.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "See  for\nan extension of the present example estimating the variance of the ROC\ncurves and their respective AUC.",
            "markdown"
        ]
    ],
    "Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->Load and prepare data": [
        [
            "We import the  which contains 3 classes, each one\ncorresponding to a type of iris plant. One class is linearly separable from\nthe other 2; the latter are <strong>not</strong> linearly separable from each other.",
            "markdown"
        ],
        [
            "Here we binarize the output and add noisy features to make the problem harder.",
            "markdown"
        ],
        [
            "import numpy as np\nfrom sklearn.datasets import \nfrom sklearn.model_selection import \n\niris = ()\ntarget_names = iris.target_names\nX, y = iris.data, iris.target\ny = iris.target_names[y]\n\nrandom_state = (0)\nn_samples, n_features = X.shape\nn_classes = len((y))\nX = ([X, random_state.randn(n_samples, 200 * n_features)], axis=1)\n(\n    X_train,\n    X_test,\n    y_train,\n    y_test,\n) = (X, y, test_size=0.5, stratify=y, random_state=0)",
            "code"
        ],
        [
            "We train a  model which can\nnaturally handle multiclass problems, thanks to the use of the multinomial\nformulation.",
            "markdown"
        ],
        [
            "from sklearn.linear_model import \n\nclassifier = ()\ny_score = classifier.fit(X_train, y_train).predict_proba(X_test)",
            "code"
        ]
    ],
    "Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->One-vs-Rest multiclass ROC": [
        [
            "The One-vs-the-Rest (OvR) multiclass strategy, also known as one-vs-all,\nconsists in computing a ROC curve per each of the n_classes. In each step, a\ngiven class is regarded as the positive class and the remaining classes are\nregarded as the negative class as a bulk.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "One should not confuse the OvR strategy used for the <strong>evaluation</strong>\nof multiclass classifiers with the OvR strategy used to <strong>train</strong> a\nmulticlass classifier by fitting a set of binary classifiers (for instance\nvia the  meta-estimator).\nThe OvR ROC evaluation can be used to scrutinize any kind of classification\nmodels irrespectively of how they were trained (see ).",
            "markdown"
        ],
        [
            "In this section we use a  to\nbinarize the target by one-hot-encoding in a OvR fashion. This means that the\ntarget of shape (n_samples,) is mapped to a target of shape (n_samples,\nn_classes).",
            "markdown"
        ],
        [
            "from sklearn.preprocessing import \n\nlabel_binarizer = ().fit(y_train)\ny_onehot_test = label_binarizer.transform(y_test)\ny_onehot_test.shape  # (n_samples, n_classes)",
            "code"
        ],
        [
            "(75, 3)",
            "code"
        ],
        [
            "We can as well easily check the encoding of a specific class:",
            "markdown"
        ],
        [
            "label_binarizer.transform([\"virginica\"])",
            "code"
        ],
        [
            "array([[0, 0, 1]])",
            "code"
        ]
    ],
    "Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->One-vs-Rest multiclass ROC->ROC curve showing a specific class": [
        [
            "In the following plot we show the resulting ROC curve when regarding the iris\nflowers as either \u201cvirginica\u201d (class_id=2) or \u201cnon-virginica\u201d (the rest).",
            "markdown"
        ],
        [
            "class_of_interest = \"virginica\"\nclass_id = (label_binarizer.classes_ == class_of_interest)[0]\nclass_id",
            "code"
        ],
        [
            "2",
            "code"
        ],
        [
            "import matplotlib.pyplot as plt\nfrom sklearn.metrics import RocCurveDisplay\n\n(\n    y_onehot_test[:, class_id],\n    y_score[:, class_id],\n    name=f\"{class_of_interest} vs the rest\",\n    color=\"darkorange\",\n)\n([0, 1], [0, 1], \"k--\", label=\"chance level (AUC = 0.5)\")\n(\"square\")\n(\"False Positive Rate\")\n(\"True Positive Rate\")\n(\"One-vs-Rest ROC curves:\\nVirginica vs (Setosa & Versicolor)\")\n()\n()\n\n\n<img alt=\"One-vs-Rest ROC curves: Virginica vs (Setosa & Versicolor)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_roc_001.png\" srcset=\"../../_images/sphx_glr_plot_roc_001.png\"/>",
            "code"
        ]
    ],
    "Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->One-vs-Rest multiclass ROC->ROC curve using micro-averaged OvR": [
        [
            "Micro-averaging aggregates the contributions from all the classes (using\nnp.ravel) to compute the average metrics as follows:",
            "markdown"
        ],
        [
            "\\(TPR=\\frac{\\sum_{c}TP_c}{\\sum_{c}(TP_c + FN_c)}\\) ;",
            "markdown"
        ],
        [
            "\\(FPR=\\frac{\\sum_{c}FP_c}{\\sum_{c}(FP_c + TN_c)}\\) .",
            "markdown"
        ],
        [
            "We can briefly demo the effect of np.ravel:",
            "markdown"
        ],
        [
            "print(f\"y_score:\\n{y_score[0:2,:]}\")\nprint()\nprint(f\"y_score.ravel():\\n{y_score[0:2,:].ravel()}\")",
            "code"
        ],
        [
            "y_score:\n[[0.38 0.05 0.57]\n [0.07 0.28 0.65]]\n\ny_score.ravel():\n[0.38 0.05 0.57 0.07 0.28 0.65]",
            "code"
        ],
        [
            "In a multi-class classification setup with highly imbalanced classes,\nmicro-averaging is preferable over macro-averaging. In such cases, one can\nalternatively use a weighted macro-averaging, not demoed here.",
            "markdown"
        ],
        [
            "(\n    y_onehot_test.ravel(),\n    y_score.ravel(),\n    name=\"micro-average OvR\",\n    color=\"darkorange\",\n)\n([0, 1], [0, 1], \"k--\", label=\"chance level (AUC = 0.5)\")\n(\"square\")\n(\"False Positive Rate\")\n(\"True Positive Rate\")\n(\"Micro-averaged One-vs-Rest\\nReceiver Operating Characteristic\")\n()\n()\n\n\n<img alt=\"Micro-averaged One-vs-Rest Receiver Operating Characteristic\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_roc_002.png\" srcset=\"../../_images/sphx_glr_plot_roc_002.png\"/>",
            "code"
        ],
        [
            "In the case where the main interest is not the plot but the ROC-AUC score\nitself, we can reproduce the value shown in the plot using\n.",
            "markdown"
        ],
        [
            "from sklearn.metrics import \n\nmicro_roc_auc_ovr = (\n    y_test,\n    y_score,\n    multi_class=\"ovr\",\n    average=\"micro\",\n)\n\nprint(f\"Micro-averaged One-vs-Rest ROC AUC score:\\n{micro_roc_auc_ovr:.2f}\")",
            "code"
        ],
        [
            "Micro-averaged One-vs-Rest ROC AUC score:\n0.77",
            "code"
        ],
        [
            "This is equivalent to computing the ROC curve with\n and then the area under the curve with\n for the raveled true and predicted classes.",
            "markdown"
        ],
        [
            "from sklearn.metrics import , \n\n# store the fpr, tpr, and roc_auc for all averaging strategies\nfpr, tpr, roc_auc = dict(), dict(), dict()\n# Compute micro-average ROC curve and ROC area\nfpr[\"micro\"], tpr[\"micro\"], _ = (y_onehot_test.ravel(), y_score.ravel())\nroc_auc[\"micro\"] = (fpr[\"micro\"], tpr[\"micro\"])\n\nprint(f\"Micro-averaged One-vs-Rest ROC AUC score:\\n{roc_auc['micro']:.2f}\")",
            "code"
        ],
        [
            "Micro-averaged One-vs-Rest ROC AUC score:\n0.77",
            "code"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "By default, the computation of the ROC curve adds a single point at\nthe maximal false positive rate by using linear interpolation and the\nMcClish correction [].",
            "markdown"
        ]
    ],
    "Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->One-vs-Rest multiclass ROC->ROC curve using the OvR macro-average": [
        [
            "Obtaining the macro-average requires computing the metric independently for\neach class and then taking the average over them, hence treating all classes\nequally a priori. We first aggregate the true/false positive rates per class:",
            "markdown"
        ],
        [
            "for i in range(n_classes):\n    fpr[i], tpr[i], _ = (y_onehot_test[:, i], y_score[:, i])\n    roc_auc[i] = (fpr[i], tpr[i])\n\nfpr_grid = (0.0, 1.0, 1000)\n\n# Interpolate all ROC curves at these points\nmean_tpr = (fpr_grid)\n\nfor i in range(n_classes):\n    mean_tpr += (fpr_grid, fpr[i], tpr[i])  # linear interpolation\n\n# Average it and compute AUC\nmean_tpr /= n_classes\n\nfpr[\"macro\"] = fpr_grid\ntpr[\"macro\"] = mean_tpr\nroc_auc[\"macro\"] = (fpr[\"macro\"], tpr[\"macro\"])\n\nprint(f\"Macro-averaged One-vs-Rest ROC AUC score:\\n{roc_auc['macro']:.2f}\")",
            "code"
        ],
        [
            "Macro-averaged One-vs-Rest ROC AUC score:\n0.77",
            "code"
        ],
        [
            "This computation is equivalent to simply calling",
            "markdown"
        ],
        [
            "macro_roc_auc_ovr = (\n    y_test,\n    y_score,\n    multi_class=\"ovr\",\n    average=\"macro\",\n)\n\nprint(f\"Macro-averaged One-vs-Rest ROC AUC score:\\n{macro_roc_auc_ovr:.2f}\")",
            "code"
        ],
        [
            "Macro-averaged One-vs-Rest ROC AUC score:\n0.77",
            "code"
        ]
    ],
    "Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->One-vs-Rest multiclass ROC->Plot all OvR ROC curves together": [
        [
            "from itertools import \n\nfig, ax = (figsize=(6, 6))\n\n(\n    fpr[\"micro\"],\n    tpr[\"micro\"],\n    label=f\"micro-average ROC curve (AUC = {roc_auc['micro']:.2f})\",\n    color=\"deeppink\",\n    linestyle=\":\",\n    linewidth=4,\n)\n\n(\n    fpr[\"macro\"],\n    tpr[\"macro\"],\n    label=f\"macro-average ROC curve (AUC = {roc_auc['macro']:.2f})\",\n    color=\"navy\",\n    linestyle=\":\",\n    linewidth=4,\n)\n\ncolors = ([\"aqua\", \"darkorange\", \"cornflowerblue\"])\nfor class_id, color in zip(range(n_classes), colors):\n    (\n        y_onehot_test[:, class_id],\n        y_score[:, class_id],\n        name=f\"ROC curve for {target_names[class_id]}\",\n        color=color,\n        ax=ax,\n    )\n\n([0, 1], [0, 1], \"k--\", label=\"ROC curve for chance level (AUC = 0.5)\")\n(\"square\")\n(\"False Positive Rate\")\n(\"True Positive Rate\")\n(\"Extension of Receiver Operating Characteristic\\nto One-vs-Rest multiclass\")\n()\n()\n\n\n<img alt=\"Extension of Receiver Operating Characteristic to One-vs-Rest multiclass\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_roc_003.png\" srcset=\"../../_images/sphx_glr_plot_roc_003.png\"/>",
            "code"
        ]
    ],
    "Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->One-vs-One multiclass ROC": [
        [
            "The One-vs-One (OvO) multiclass strategy consists in fitting one classifier\nper class pair. Since it requires to train n_classes * (n_classes - 1) / 2\nclassifiers, this method is usually slower than One-vs-Rest due to its\nO(n_classes ^2) complexity.",
            "markdown"
        ],
        [
            "In this section, we demonstrate the macro-averaged AUC using the OvO scheme\nfor the 3 possible combinations in the : \u201csetosa\u201d vs\n\u201cversicolor\u201d, \u201cversicolor\u201d vs \u201cvirginica\u201d and  \u201cvirginica\u201d vs \u201csetosa\u201d. Notice\nthat micro-averaging is not defined for the OvO scheme.",
            "markdown"
        ]
    ],
    "Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->One-vs-One multiclass ROC->ROC curve using the OvO macro-average": [
        [
            "In the OvO scheme, the first step is to identify all possible unique\ncombinations of pairs. The computation of scores is done by treating one of\nthe elements in a given pair as the positive class and the other element as\nthe negative class, then re-computing the score by inversing the roles and\ntaking the mean of both scores.",
            "markdown"
        ],
        [
            "from itertools import \n\npair_list = list(((y), 2))\nprint(pair_list)",
            "code"
        ],
        [
            "[('setosa', 'versicolor'), ('setosa', 'virginica'), ('versicolor', 'virginica')]",
            "code"
        ],
        [
            "pair_scores = []\nmean_tpr = dict()\n\nfor ix, (label_a, label_b) in enumerate(pair_list):\n\n    a_mask = y_test == label_a\n    b_mask = y_test == label_b\n    ab_mask = (a_mask, b_mask)\n\n    a_true = a_mask[ab_mask]\n    b_true = b_mask[ab_mask]\n\n    idx_a = (label_binarizer.classes_ == label_a)[0]\n    idx_b = (label_binarizer.classes_ == label_b)[0]\n\n    fpr_a, tpr_a, _ = (a_true, y_score[ab_mask, idx_a])\n    fpr_b, tpr_b, _ = (b_true, y_score[ab_mask, idx_b])\n\n    mean_tpr[ix] = (fpr_grid)\n    mean_tpr[ix] += (fpr_grid, fpr_a, tpr_a)\n    mean_tpr[ix] += (fpr_grid, fpr_b, tpr_b)\n    mean_tpr[ix] /= 2\n    mean_score = (fpr_grid, mean_tpr[ix])\n    pair_scores.append(mean_score)\n\n    fig, ax = (figsize=(6, 6))\n    (\n        fpr_grid,\n        mean_tpr[ix],\n        label=f\"Mean {label_a} vs {label_b} (AUC = {mean_score :.2f})\",\n        linestyle=\":\",\n        linewidth=4,\n    )\n    (\n        a_true,\n        y_score[ab_mask, idx_a],\n        ax=ax,\n        name=f\"{label_a} as positive class\",\n    )\n    (\n        b_true,\n        y_score[ab_mask, idx_b],\n        ax=ax,\n        name=f\"{label_b} as positive class\",\n    )\n    ([0, 1], [0, 1], \"k--\", label=\"chance level (AUC = 0.5)\")\n    (\"square\")\n    (\"False Positive Rate\")\n    (\"True Positive Rate\")\n    (f\"{target_names[idx_a]} vs {label_b} ROC curves\")\n    ()\n    ()\n\nprint(f\"Macro-averaged One-vs-One ROC AUC score:\\n{(pair_scores):.2f}\")\n\n\n\n<img alt=\"setosa vs versicolor ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_004.png\" srcset=\"../../_images/sphx_glr_plot_roc_004.png\"/>\n<img alt=\"setosa vs virginica ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_005.png\" srcset=\"../../_images/sphx_glr_plot_roc_005.png\"/>\n<img alt=\"versicolor vs virginica ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_006.png\" srcset=\"../../_images/sphx_glr_plot_roc_006.png\"/>",
            "code"
        ],
        [
            "Macro-averaged One-vs-One ROC AUC score:\n0.77",
            "code"
        ],
        [
            "One can also assert that the macro-average we computed \u201cby hand\u201d is equivalent\nto the implemented average=\"macro\" option of the\n function.",
            "markdown"
        ],
        [
            "macro_roc_auc_ovo = (\n    y_test,\n    y_score,\n    multi_class=\"ovo\",\n    average=\"macro\",\n)\n\nprint(f\"Macro-averaged One-vs-One ROC AUC score:\\n{macro_roc_auc_ovo:.2f}\")",
            "code"
        ],
        [
            "Macro-averaged One-vs-One ROC AUC score:\n0.77",
            "code"
        ]
    ],
    "Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->One-vs-One multiclass ROC->Plot all OvO ROC curves together": [
        [
            "ovo_tpr = (fpr_grid)\n\nfig, ax = (figsize=(6, 6))\nfor ix, (label_a, label_b) in enumerate(pair_list):\n    ovo_tpr += mean_tpr[ix]\n    (\n        fpr_grid,\n        mean_tpr[ix],\n        label=f\"Mean {label_a} vs {label_b} (AUC = {pair_scores[ix]:.2f})\",\n    )\n\novo_tpr /= sum(1 for pair in enumerate(pair_list))\n\n(\n    fpr_grid,\n    ovo_tpr,\n    label=f\"One-vs-One macro-average (AUC = {macro_roc_auc_ovo:.2f})\",\n    linestyle=\":\",\n    linewidth=4,\n)\n([0, 1], [0, 1], \"k--\", label=\"chance level (AUC = 0.5)\")\n(\"square\")\n(\"False Positive Rate\")\n(\"True Positive Rate\")\n(\"Extension of Receiver Operating Characteristic\\nto One-vs-One multiclass\")\n()\n()\n\n\n<img alt=\"Extension of Receiver Operating Characteristic to One-vs-One multiclass\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_roc_007.png\" srcset=\"../../_images/sphx_glr_plot_roc_007.png\"/>",
            "code"
        ],
        [
            "We confirm that the classes \u201cversicolor\u201d and \u201cvirginica\u201d are not well\nidentified by a linear classifier. Notice that the \u201cvirginica\u201d-vs-the-rest\nROC-AUC score (0.77) is between the OvO ROC-AUC scores for \u201cversicolor\u201d vs\n\u201cvirginica\u201d (0.64) and \u201csetosa\u201d vs \u201cvirginica\u201d (0.90). Indeed, the OvO\nstrategy gives additional information on the confusion between a pair of\nclasses, at the expense of computational cost when the number of classes\nis large.",
            "markdown"
        ],
        [
            "The OvO strategy is recommended if the user is mainly interested in correctly\nidentifying a particular class or subset of classes, whereas evaluating the\nglobal performance of a classifier can still be summarized via a given\naveraging strategy.",
            "markdown"
        ],
        [
            "Micro-averaged OvR ROC is dominated by the more frequent class, since the\ncounts are pooled. The macro-averaged alternative better reflects the\nstatistics of the less frequent classes, and then is more appropriate when\nperformance on all the classes is deemed equally important.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.780 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Model Selection->Nested versus non-nested cross-validation": [
        [
            "This example compares non-nested and nested cross-validation strategies on a\nclassifier of the iris data set. Nested cross-validation (CV) is often used to\ntrain a model in which hyperparameters also need to be optimized. Nested CV\nestimates the generalization error of the underlying model and its\n(hyper)parameter search. Choosing the parameters that maximize non-nested CV\nbiases the model to the dataset, yielding an overly-optimistic score.",
            "markdown"
        ],
        [
            "Model selection without nested CV uses the same data to tune model parameters\nand evaluate model performance. Information may thus \u201cleak\u201d into the model\nand overfit the data. The magnitude of this effect is primarily dependent on\nthe size of the dataset and the stability of the model. See Cawley and Talbot\n for an analysis of these issues.",
            "markdown"
        ],
        [
            "To avoid this problem, nested CV effectively uses a series of\ntrain/validation/test set splits. In the inner loop (here executed by\n), the score is\napproximately maximized by fitting a model to each training set, and then\ndirectly maximized in selecting (hyper)parameters over the validation set. In\nthe outer loop (here in ), generalization error is estimated\nby averaging test set scores over several dataset splits.",
            "markdown"
        ],
        [
            "The example below uses a support vector classifier with a non-linear kernel to\nbuild a model with optimized hyperparameters by grid search. We compare the\nperformance of non-nested and nested CV strategies by taking the difference\nbetween their scores.",
            "markdown"
        ],
        [
            "See Also:",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "References:\n\n\n[]",
            "markdown"
        ],
        [
            "<img alt=\"Non-Nested and Nested Cross Validation on Iris Dataset\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_nested_cross_validation_iris_001.png\" srcset=\"../../_images/sphx_glr_plot_nested_cross_validation_iris_001.png\"/>",
            "markdown"
        ],
        [
            "Average difference of 0.007581 with std. dev. of 0.007833.\n\n\n\n<br/>",
            "code"
        ],
        [
            "from sklearn.datasets import \nfrom matplotlib import pyplot as plt\nfrom sklearn.svm import \nfrom sklearn.model_selection import , , \nimport numpy as np\n\n# Number of random trials\nNUM_TRIALS = 30\n\n# Load the dataset\niris = ()\nX_iris = iris.data\ny_iris = iris.target\n\n# Set up possible values of parameters to optimize over\np_grid = {\"C\": [1, 10, 100], \"gamma\": [0.01, 0.1]}\n\n# We will use a Support Vector Classifier with \"rbf\" kernel\nsvm = (kernel=\"rbf\")\n\n# Arrays to store scores\nnon_nested_scores = (NUM_TRIALS)\nnested_scores = (NUM_TRIALS)\n\n# Loop for each trial\nfor i in range(NUM_TRIALS):\n\n    # Choose cross-validation techniques for the inner and outer loops,\n    # independently of the dataset.\n    # E.g \"GroupKFold\", \"LeaveOneOut\", \"LeaveOneGroupOut\", etc.\n    inner_cv = (n_splits=4, shuffle=True, random_state=i)\n    outer_cv = (n_splits=4, shuffle=True, random_state=i)\n\n    # Non_nested parameter search and scoring\n    clf = (estimator=svm, param_grid=p_grid, cv=outer_cv)\n    clf.fit(X_iris, y_iris)\n    non_nested_scores[i] = clf.best_score_\n\n    # Nested CV with parameter optimization\n    clf = (estimator=svm, param_grid=p_grid, cv=inner_cv)\n    nested_score = (clf, X=X_iris, y=y_iris, cv=outer_cv)\n    nested_scores[i] = nested_score.mean()\n\nscore_difference = non_nested_scores - nested_scores\n\nprint(\n    \"Average difference of {:6f} with std. dev. of {:6f}.\".format(\n        score_difference.mean(), score_difference.std()\n    )\n)\n\n# Plot scores on each trial for nested and non-nested CV\n()\n(211)\n(non_nested_scores_line,) = (non_nested_scores, color=\"r\")\n(nested_line,) = (nested_scores, color=\"b\")\n(\"score\", fontsize=\"14\")\n(\n    [non_nested_scores_line, nested_line],\n    [\"Non-Nested CV\", \"Nested CV\"],\n    bbox_to_anchor=(0, 0.4, 0.5, 0),\n)\n(\n    \"Non-Nested and Nested Cross Validation on Iris Dataset\",\n    x=0.5,\n    y=1.1,\n    fontsize=\"15\",\n)\n\n# Plot bar chart of the difference.\n(212)\ndifference_plot = (range(NUM_TRIALS), score_difference)\n(\"Individual Trial #\")\n(\n    [difference_plot],\n    [\"Non-Nested CV - Nested CV Score\"],\n    bbox_to_anchor=(0, 1, 0.8, 0),\n)\n(\"score difference\", fontsize=\"14\")\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  5.466 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Model Selection->Plotting Cross-Validated Predictions": [
        [
            "This example shows how to use\n together with\n to visualize prediction\nerrors.",
            "markdown"
        ],
        [
            "We will load the diabetes dataset and create an instance of a linear\nregression model.",
            "markdown"
        ],
        [
            "from sklearn.datasets import \nfrom sklearn.linear_model import \n\nX, y = (return_X_y=True)\nlr = ()",
            "code"
        ],
        [
            "returns an array of the\nsame size of y where each entry is a prediction obtained by cross\nvalidation.",
            "markdown"
        ],
        [
            "from sklearn.model_selection import \n\ny_pred = (lr, X, y, cv=10)",
            "code"
        ],
        [
            "Since cv=10, it means that we trained 10 models and each model was\nused to predict on one of the 10 folds. We can now use the\n to visualize the\nprediction errors.",
            "markdown"
        ],
        [
            "On the left axis, we plot the observed values \\(y\\) vs. the predicted\nvalues \\(\\hat{y}\\) given by the models. On the right axis, we plot the\nresiduals (i.e. the difference between the observed values and the predicted\nvalues) vs. the predicted values.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\nfrom sklearn.metrics import PredictionErrorDisplay\n\nfig, axs = (ncols=2, figsize=(8, 4))\n(\n    y,\n    y_pred=y_pred,\n    kind=\"actual_vs_predicted\",\n    subsample=100,\n    ax=axs[0],\n    random_state=0,\n)\naxs[0].set_title(\"Actual vs. Predicted values\")\n(\n    y,\n    y_pred=y_pred,\n    kind=\"residual_vs_predicted\",\n    subsample=100,\n    ax=axs[1],\n    random_state=0,\n)\naxs[1].set_title(\"Residuals vs. Predicted Values\")\nfig.suptitle(\"Plotting cross-validated predictions\")\n()\n()\n\n\n<img alt=\"Plotting cross-validated predictions, Actual vs. Predicted values, Residuals vs. Predicted Values\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cv_predict_001.png\" srcset=\"../../_images/sphx_glr_plot_cv_predict_001.png\"/>",
            "code"
        ],
        [
            "It is important to note that we used\n for visualization\npurpose only in this example.",
            "markdown"
        ],
        [
            "It would be problematic to\nquantitatively assess the model performance by computing a single\nperformance metric from the concatenated predictions returned by\n\nwhen the different CV folds vary by size and distributions.",
            "markdown"
        ],
        [
            "In is recommended to compute per-fold performance metrics using:\n or\n instead.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.152 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Model Selection->Plotting Learning Curves and Checking Models' Scalability": [
        [
            "This example shows how to use\n together with\n to visualize prediction\nerrors.",
            "markdown"
        ],
        [
            "We will load the diabetes dataset and create an instance of a linear\nregression model.",
            "markdown"
        ],
        [
            "from sklearn.datasets import \nfrom sklearn.linear_model import \n\nX, y = (return_X_y=True)\nlr = ()",
            "code"
        ],
        [
            "returns an array of the\nsame size of y where each entry is a prediction obtained by cross\nvalidation.",
            "markdown"
        ],
        [
            "from sklearn.model_selection import \n\ny_pred = (lr, X, y, cv=10)",
            "code"
        ],
        [
            "Since cv=10, it means that we trained 10 models and each model was\nused to predict on one of the 10 folds. We can now use the\n to visualize the\nprediction errors.",
            "markdown"
        ],
        [
            "On the left axis, we plot the observed values \\(y\\) vs. the predicted\nvalues \\(\\hat{y}\\) given by the models. On the right axis, we plot the\nresiduals (i.e. the difference between the observed values and the predicted\nvalues) vs. the predicted values.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\nfrom sklearn.metrics import PredictionErrorDisplay\n\nfig, axs = (ncols=2, figsize=(8, 4))\n(\n    y,\n    y_pred=y_pred,\n    kind=\"actual_vs_predicted\",\n    subsample=100,\n    ax=axs[0],\n    random_state=0,\n)\naxs[0].set_title(\"Actual vs. Predicted values\")\n(\n    y,\n    y_pred=y_pred,\n    kind=\"residual_vs_predicted\",\n    subsample=100,\n    ax=axs[1],\n    random_state=0,\n)\naxs[1].set_title(\"Residuals vs. Predicted Values\")\nfig.suptitle(\"Plotting cross-validated predictions\")\n()\n()\n\n\n<img alt=\"Plotting cross-validated predictions, Actual vs. Predicted values, Residuals vs. Predicted Values\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cv_predict_001.png\" srcset=\"../../_images/sphx_glr_plot_cv_predict_001.png\"/>",
            "code"
        ],
        [
            "It is important to note that we used\n for visualization\npurpose only in this example.",
            "markdown"
        ],
        [
            "It would be problematic to\nquantitatively assess the model performance by computing a single\nperformance metric from the concatenated predictions returned by\n\nwhen the different CV folds vary by size and distributions.",
            "markdown"
        ],
        [
            "In is recommended to compute per-fold performance metrics using:\n or\n instead.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.152 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Model Selection->Plotting Validation Curves": [
        [
            "In this plot you can see the training scores and validation scores of an SVM\nfor different values of the kernel parameter gamma. For very low values of\ngamma, you can see that both the training score and the validation score are\nlow. This is called underfitting. Medium values of gamma will result in high\nvalues for both scores, i.e. the classifier is performing fairly well. If gamma\nis too high, the classifier will overfit, which means that the training score\nis good but the validation score is poor.\n<img alt=\"Validation Curve with SVM\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_validation_curve_001.png\" srcset=\"../../_images/sphx_glr_plot_validation_curve_001.png\"/>",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import \nfrom sklearn.svm import \nfrom sklearn.model_selection import \n\nX, y = (return_X_y=True)\nsubset_mask = (y, [1, 2])  # binary classification: 1 vs 2\nX, y = X[subset_mask], y[subset_mask]\n\nparam_range = (-6, -1, 5)\ntrain_scores, test_scores = (\n    (),\n    X,\n    y,\n    param_name=\"gamma\",\n    param_range=param_range,\n    scoring=\"accuracy\",\n    n_jobs=2,\n)\ntrain_scores_mean = (train_scores, axis=1)\ntrain_scores_std = (train_scores, axis=1)\ntest_scores_mean = (test_scores, axis=1)\ntest_scores_std = (test_scores, axis=1)\n\n(\"Validation Curve with SVM\")\n(r\"$\\gamma$\")\n(\"Score\")\n(0.0, 1.1)\nlw = 2\n(\n    param_range, train_scores_mean, label=\"Training score\", color=\"darkorange\", lw=lw\n)\n(\n    param_range,\n    train_scores_mean - train_scores_std,\n    train_scores_mean + train_scores_std,\n    alpha=0.2,\n    color=\"darkorange\",\n    lw=lw,\n)\n(\n    param_range, test_scores_mean, label=\"Cross-validation score\", color=\"navy\", lw=lw\n)\n(\n    param_range,\n    test_scores_mean - test_scores_std,\n    test_scores_mean + test_scores_std,\n    alpha=0.2,\n    color=\"navy\",\n    lw=lw,\n)\n(loc=\"best\")\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.668 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Model Selection->Precision-Recall": [
        [
            "Example of Precision-Recall metric to evaluate classifier output quality.",
            "markdown"
        ],
        [
            "Precision-Recall is a useful measure of success of prediction when the\nclasses are very imbalanced. In information retrieval, precision is a\nmeasure of result relevancy, while recall is a measure of how many truly\nrelevant results are returned.",
            "markdown"
        ],
        [
            "The precision-recall curve shows the tradeoff between precision and\nrecall for different threshold. A high area under the curve represents\nboth high recall and high precision, where high precision relates to a\nlow false positive rate, and high recall relates to a low false negative\nrate. High scores for both show that the classifier is returning accurate\nresults (high precision), as well as returning a majority of all positive\nresults (high recall).",
            "markdown"
        ],
        [
            "A system with high recall but low precision returns many results, but most of\nits predicted labels are incorrect when compared to the training labels. A\nsystem with high precision but low recall is just the opposite, returning very\nfew results, but most of its predicted labels are correct when compared to the\ntraining labels. An ideal system with high precision and high recall will\nreturn many results, with all results labeled correctly.",
            "markdown"
        ],
        [
            "Precision (\\(P\\)) is defined as the number of true positives (\\(T_p\\))\nover the number of true positives plus the number of false positives\n(\\(F_p\\)).",
            "markdown"
        ],
        [
            "\\(P = \\frac{T_p}{T_p+F_p}\\)",
            "markdown"
        ],
        [
            "Recall (\\(R\\)) is defined as the number of true positives (\\(T_p\\))\nover the number of true positives plus the number of false negatives\n(\\(F_n\\)).",
            "markdown"
        ],
        [
            "\\(R = \\frac{T_p}{T_p + F_n}\\)",
            "markdown"
        ],
        [
            "These quantities are also related to the (\\(F_1\\)) score, which is defined\nas the harmonic mean of precision and recall.",
            "markdown"
        ],
        [
            "\\(F1 = 2\\frac{P \\times R}{P+R}\\)",
            "markdown"
        ],
        [
            "Note that the precision may not decrease with recall. The\ndefinition of precision (\\(\\frac{T_p}{T_p + F_p}\\)) shows that lowering\nthe threshold of a classifier may increase the denominator, by increasing the\nnumber of results returned. If the threshold was previously set too high, the\nnew results may all be true positives, which will increase precision. If the\nprevious threshold was about right or too low, further lowering the threshold\nwill introduce false positives, decreasing precision.",
            "markdown"
        ],
        [
            "Recall is defined as \\(\\frac{T_p}{T_p+F_n}\\), where \\(T_p+F_n\\) does\nnot depend on the classifier threshold. This means that lowering the classifier\nthreshold may increase recall, by increasing the number of true positive\nresults. It is also possible that lowering the threshold may leave recall\nunchanged, while the precision fluctuates.",
            "markdown"
        ],
        [
            "The relationship between recall and precision can be observed in the\nstairstep area of the plot - at the edges of these steps a small change\nin the threshold considerably reduces precision, with only a minor gain in\nrecall.",
            "markdown"
        ],
        [
            "<strong>Average precision</strong> (AP) summarizes such a plot as the weighted mean of\nprecisions achieved at each threshold, with the increase in recall from the\nprevious threshold used as the weight:",
            "markdown"
        ],
        [
            "\\(\\text{AP} = \\sum_n (R_n - R_{n-1}) P_n\\)",
            "markdown"
        ],
        [
            "where \\(P_n\\) and \\(R_n\\) are the precision and recall at the\nnth threshold. A pair \\((R_k, P_k)\\) is referred to as an\noperating point.",
            "markdown"
        ],
        [
            "AP and the trapezoidal area under the operating points\n() are common ways to summarize a precision-recall\ncurve that lead to different results. Read more in the\n.",
            "markdown"
        ],
        [
            "Precision-recall curves are typically used in binary classification to study\nthe output of a classifier. In order to extend the precision-recall curve and\naverage precision to multi-class or multi-label classification, it is necessary\nto binarize the output. One curve can be drawn per label, but one can also draw\na precision-recall curve by considering each element of the label indicator\nmatrix as a binary prediction (micro-averaging).",
            "markdown"
        ],
        [
            "Note\n\nSee also ,",
            "markdown"
        ],
        [
            ",\n,\n\n\n</dl>",
            "markdown"
        ]
    ],
    "Examples->Model Selection->Precision-Recall->In binary classification settings->Dataset and model": [
        [
            "We will use a Linear SVC classifier to differentiate two types of irises.",
            "markdown"
        ],
        [
            "import numpy as np\nfrom sklearn.datasets import \nfrom sklearn.model_selection import \n\nX, y = (return_X_y=True)\n\n# Add noisy features\nrandom_state = (0)\nn_samples, n_features = X.shape\nX = ([X, random_state.randn(n_samples, 200 * n_features)], axis=1)\n\n# Limit to the two first classes, and split into training and test\nX_train, X_test, y_train, y_test = (\n    X[y &lt; 2], y[y &lt; 2], test_size=0.5, random_state=random_state\n)",
            "code"
        ],
        [
            "Linear SVC will expect each feature to have a similar range of values. Thus,\nwe will first scale the data using a\n.",
            "markdown"
        ],
        [
            "from sklearn.pipeline import \nfrom sklearn.preprocessing import \nfrom sklearn.svm import \n\nclassifier = ((), (random_state=random_state))\nclassifier.fit(X_train, y_train)",
            "code"
        ],
        [
            "Pipeline(steps=[('standardscaler', StandardScaler()),\n                ('linearsvc',\n                 LinearSVC(random_state=RandomState(MT19937) at 0x7F0D75F41040))])<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-223\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-223\">Pipeline</label>",
            "code"
        ],
        [
            "Pipeline(steps=[('standardscaler', StandardScaler()),\n                ('linearsvc',\n                 LinearSVC(random_state=RandomState(MT19937) at 0x7F0D75F41040))])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-224\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-224\">StandardScaler</label>",
            "code"
        ],
        [
            "StandardScaler()<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-225\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-225\">LinearSVC</label>",
            "code"
        ],
        [
            "LinearSVC(random_state=RandomState(MT19937) at 0x7F0D75F41040)\n\n<br/>\n<br/>",
            "code"
        ]
    ],
    "Examples->Model Selection->Precision-Recall->In binary classification settings->Plot the Precision-Recall curve": [
        [
            "To plot the precision-recall curve, you should use\n. Indeed, there is two\nmethods available depending if you already computed the predictions of the\nclassifier or not.",
            "markdown"
        ],
        [
            "Let\u2019s first plot the precision-recall curve without the classifier\npredictions. We use\n that\ncomputes the predictions for us before plotting the curve.",
            "markdown"
        ],
        [
            "from sklearn.metrics import \n\ndisplay = (\n    classifier, X_test, y_test, name=\"LinearSVC\"\n)\n_ = display.ax_.set_title(\"2-class Precision-Recall curve\")\n\n\n<img alt=\"2-class Precision-Recall curve\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_precision_recall_001.png\" srcset=\"../../_images/sphx_glr_plot_precision_recall_001.png\"/>",
            "code"
        ],
        [
            "If we already got the estimated probabilities or scores for\nour model, then we can use\n.",
            "markdown"
        ],
        [
            "y_score = classifier.decision_function(X_test)\n\ndisplay = (y_test, y_score, name=\"LinearSVC\")\n_ = display.ax_.set_title(\"2-class Precision-Recall curve\")\n\n\n<img alt=\"2-class Precision-Recall curve\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_precision_recall_002.png\" srcset=\"../../_images/sphx_glr_plot_precision_recall_002.png\"/>",
            "code"
        ]
    ],
    "Examples->Model Selection->Precision-Recall->In multi-label settings": [
        [
            "The precision-recall curve does not support the multilabel setting. However,\none can decide how to handle this case. We show such an example below.",
            "markdown"
        ]
    ],
    "Examples->Model Selection->Precision-Recall->In multi-label settings->Create multi-label data, fit, and predict": [
        [
            "We create a multi-label dataset, to illustrate the precision-recall in\nmulti-label settings.",
            "markdown"
        ],
        [
            "from sklearn.preprocessing import \n\n# Use label_binarize to be multi-label like settings\nY = (y, classes=[0, 1, 2])\nn_classes = Y.shape[1]\n\n# Split into training and test\nX_train, X_test, Y_train, Y_test = (\n    X, Y, test_size=0.5, random_state=random_state\n)",
            "code"
        ],
        [
            "We use  for multi-label\nprediction.",
            "markdown"
        ],
        [
            "from sklearn.multiclass import \n\nclassifier = (\n    ((), (random_state=random_state))\n)\nclassifier.fit(X_train, Y_train)\ny_score = classifier.decision_function(X_test)",
            "code"
        ]
    ],
    "Examples->Model Selection->Precision-Recall->In multi-label settings->The average precision score in multi-label settings": [
        [
            "from sklearn.metrics import \nfrom sklearn.metrics import \n\n# For each class\nprecision = dict()\nrecall = dict()\naverage_precision = dict()\nfor i in range(n_classes):\n    precision[i], recall[i], _ = (Y_test[:, i], y_score[:, i])\n    average_precision[i] = (Y_test[:, i], y_score[:, i])\n\n# A \"micro-average\": quantifying score on all classes jointly\nprecision[\"micro\"], recall[\"micro\"], _ = (\n    Y_test.ravel(), y_score.ravel()\n)\naverage_precision[\"micro\"] = (Y_test, y_score, average=\"micro\")",
            "code"
        ]
    ],
    "Examples->Model Selection->Precision-Recall->In multi-label settings->Plot the micro-averaged Precision-Recall curve": [
        [
            "display = (\n    recall=recall[\"micro\"],\n    precision=precision[\"micro\"],\n    average_precision=average_precision[\"micro\"],\n)\ndisplay.plot()\n_ = display.ax_.set_title(\"Micro-averaged over all classes\")\n\n\n<img alt=\"Micro-averaged over all classes\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_precision_recall_003.png\" srcset=\"../../_images/sphx_glr_plot_precision_recall_003.png\"/>",
            "code"
        ]
    ],
    "Examples->Model Selection->Precision-Recall->In multi-label settings->Plot Precision-Recall curve for each class and iso-f1 curves": [
        [
            "import matplotlib.pyplot as plt\nfrom itertools import \n\n# setup plot details\ncolors = ([\"navy\", \"turquoise\", \"darkorange\", \"cornflowerblue\", \"teal\"])\n\n_, ax = (figsize=(7, 8))\n\nf_scores = (0.2, 0.8, num=4)\nlines, labels = [], []\nfor f_score in f_scores:\n    x = (0.01, 1)\n    y = f_score * x / (2 * x - f_score)\n    (l,) = (x[y = 0], y[y = 0], color=\"gray\", alpha=0.2)\n    (\"f1={0:0.1f}\".format(f_score), xy=(0.9, y[45] + 0.02))\n\ndisplay = (\n    recall=recall[\"micro\"],\n    precision=precision[\"micro\"],\n    average_precision=average_precision[\"micro\"],\n)\ndisplay.plot(ax=ax, name=\"Micro-average precision-recall\", color=\"gold\")\n\nfor i, color in zip(range(n_classes), colors):\n    display = (\n        recall=recall[i],\n        precision=precision[i],\n        average_precision=average_precision[i],\n    )\n    display.plot(ax=ax, name=f\"Precision-recall for class {i}\", color=color)\n\n# add the legend for the iso-f1 curves\nhandles, labels = display.ax_.get_legend_handles_labels()\nhandles.extend([l])\nlabels.extend([\"iso-f1 curves\"])\n# set the legend and the axes\nax.set_xlim([0.0, 1.0])\nax.set_ylim([0.0, 1.05])\nax.legend(handles=handles, labels=labels, loc=\"best\")\nax.set_title(\"Extension of Precision-Recall curve to multi-class\")\n\n()\n\n\n<img alt=\"Extension of Precision-Recall curve to multi-class\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_precision_recall_004.png\" srcset=\"../../_images/sphx_glr_plot_precision_recall_004.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.379 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Model Selection->Receiver Operating Characteristic (ROC) with cross validation": [
        [
            "This example presents how to estimate and visualize the variance of the Receiver\nOperating Characteristic (ROC) metric using cross-validation.",
            "markdown"
        ],
        [
            "ROC curves typically feature true positive rate (TPR) on the Y axis, and false\npositive rate (FPR) on the X axis. This means that the top left corner of the\nplot is the \u201cideal\u201d point - a FPR of zero, and a TPR of one. This is not very\nrealistic, but it does mean that a larger Area Under the Curve (AUC) is usually\nbetter. The \u201csteepness\u201d of ROC curves is also important, since it is ideal to\nmaximize the TPR while minimizing the FPR.",
            "markdown"
        ],
        [
            "This example shows the ROC response of different datasets, created from K-fold\ncross-validation. Taking all of these curves, it is possible to calculate the\nmean AUC, and see the variance of the curve when the\ntraining set is split into different subsets. This roughly shows how the\nclassifier output is affected by changes in the training data, and how different\nthe splits generated by K-fold cross-validation are from one another.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "See  for a\ncomplement of the present example explaining the averaging strategies to\ngeneralize the metrics for multiclass classifiers.",
            "markdown"
        ]
    ],
    "Examples->Model Selection->Receiver Operating Characteristic (ROC) with cross validation->Load and prepare data": [
        [
            "We import the  which contains 3 classes, each one\ncorresponding to a type of iris plant. One class is linearly separable from\nthe other 2; the latter are <strong>not</strong> linearly separable from each other.",
            "markdown"
        ],
        [
            "In the following we binarize the dataset by dropping the \u201cvirginica\u201d class\n(class_id=2). This means that the \u201cversicolor\u201d class (class_id=1) is\nregarded as the positive class and \u201csetosa\u201d as the negative class\n(class_id=0).",
            "markdown"
        ],
        [
            "import numpy as np\nfrom sklearn.datasets import \n\niris = ()\ntarget_names = iris.target_names\nX, y = iris.data, iris.target\nX, y = X[y != 2], y[y != 2]\nn_samples, n_features = X.shape",
            "code"
        ],
        [
            "We also add noisy features to make the problem harder.",
            "markdown"
        ],
        [
            "random_state = (0)\nX = ([X, random_state.randn(n_samples, 200 * n_features)], axis=1)",
            "code"
        ]
    ],
    "Examples->Model Selection->Receiver Operating Characteristic (ROC) with cross validation->Load and prepare data->Classification and ROC analysis": [
        [
            "Here we run a  classifier with cross-validation and\nplot the ROC curves fold-wise. Notice that the baseline to define the chance\nlevel (dashed ROC curve) is a classifier that would always predict the most\nfrequent class.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n\nfrom sklearn import svm\nfrom sklearn.metrics import \nfrom sklearn.metrics import RocCurveDisplay\nfrom sklearn.model_selection import \n\ncv = (n_splits=6)\nclassifier = (kernel=\"linear\", probability=True, random_state=random_state)\n\ntprs = []\naucs = []\nmean_fpr = (0, 1, 100)\n\nfig, ax = (figsize=(6, 6))\nfor fold, (train, test) in enumerate(cv.split(X, y)):\n    classifier.fit(X[train], y[train])\n    viz = (\n        classifier,\n        X[test],\n        y[test],\n        name=f\"ROC fold {fold}\",\n        alpha=0.3,\n        lw=1,\n        ax=ax,\n    )\n    interp_tpr = (mean_fpr, viz.fpr, viz.tpr)\n    interp_tpr[0] = 0.0\n    tprs.append(interp_tpr)\n    aucs.append(viz.roc_auc)\nax.plot([0, 1], [0, 1], \"k--\", label=\"chance level (AUC = 0.5)\")\n\nmean_tpr = (tprs, axis=0)\nmean_tpr[-1] = 1.0\nmean_auc = (mean_fpr, mean_tpr)\nstd_auc = (aucs)\nax.plot(\n    mean_fpr,\n    mean_tpr,\n    color=\"b\",\n    label=r\"Mean ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc, std_auc),\n    lw=2,\n    alpha=0.8,\n)\n\nstd_tpr = (tprs, axis=0)\ntprs_upper = (mean_tpr + std_tpr, 1)\ntprs_lower = (mean_tpr - std_tpr, 0)\nax.fill_between(\n    mean_fpr,\n    tprs_lower,\n    tprs_upper,\n    color=\"grey\",\n    alpha=0.2,\n    label=r\"$\\pm$ 1 std. dev.\",\n)\n\nax.set(\n    xlim=[-0.05, 1.05],\n    ylim=[-0.05, 1.05],\n    xlabel=\"False Positive Rate\",\n    ylabel=\"True Positive Rate\",\n    title=f\"Mean ROC curve with variability\\n(Positive label '{target_names[1]}')\",\n)\nax.axis(\"square\")\nax.legend(loc=\"lower right\")\n()\n\n\n<img alt=\"Mean ROC curve with variability (Positive label 'versicolor')\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_roc_crossval_001.png\" srcset=\"../../_images/sphx_glr_plot_roc_crossval_001.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.170 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Model Selection->Sample pipeline for text feature extraction and evaluation": [
        [
            "The dataset used in this example is  which will be\nautomatically downloaded, cached and reused for the document classification\nexample.",
            "markdown"
        ],
        [
            "In this example, we tune the hyperparameters of a particular classifier using a\n. For a demo on the\nperformance of some other classifiers, see the\n\nnotebook.",
            "markdown"
        ],
        [
            "# Author: Olivier Grisel &lt;olivier.grisel@ensta.org\n#         Peter Prettenhofer &lt;peter.prettenhofer@gmail.com\n#         Mathieu Blondel &lt;mathieu@mblondel.org\n#         Arturo Amor &lt;david-arturo.amor-quiroz@inria.fr\n# License: BSD 3 clause",
            "code"
        ]
    ],
    "Examples->Model Selection->Sample pipeline for text feature extraction and evaluation->Data loading": [
        [
            "We load two categories from the training set. You can adjust the number of\ncategories by adding their names to the list or setting categories=None when\ncalling the dataset loader fetch20newsgroups to get\nthe 20 of them.",
            "markdown"
        ],
        [
            "from sklearn.datasets import \n\ncategories = [\n    \"alt.atheism\",\n    \"talk.religion.misc\",\n]\n\ndata_train = (\n    subset=\"train\",\n    categories=categories,\n    shuffle=True,\n    random_state=42,\n    remove=(\"headers\", \"footers\", \"quotes\"),\n)\n\ndata_test = (\n    subset=\"test\",\n    categories=categories,\n    shuffle=True,\n    random_state=42,\n    remove=(\"headers\", \"footers\", \"quotes\"),\n)\n\nprint(f\"Loading 20 newsgroups dataset for {len(data_train.target_names)} categories:\")\nprint(data_train.target_names)\nprint(f\"{len(data_train.data)} documents\")",
            "code"
        ],
        [
            "Loading 20 newsgroups dataset for 2 categories:\n['alt.atheism', 'talk.religion.misc']\n857 documents",
            "code"
        ]
    ],
    "Examples->Model Selection->Sample pipeline for text feature extraction and evaluation->Pipeline with hyperparameter tuning": [
        [
            "We define a pipeline combining a text feature vectorizer with a simple\nclassifier yet effective for text classification.",
            "markdown"
        ],
        [
            "from sklearn.feature_extraction.text import \nfrom sklearn.naive_bayes import \nfrom sklearn.pipeline import \n\npipeline = (\n    [\n        (\"vect\", ()),\n        (\"clf\", ()),\n    ]\n)\npipeline",
            "code"
        ],
        [
            "Pipeline(steps=[('vect', TfidfVectorizer()), ('clf', ComplementNB())])<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-226\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-226\">Pipeline</label>",
            "code"
        ],
        [
            "Pipeline(steps=[('vect', TfidfVectorizer()), ('clf', ComplementNB())])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-227\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-227\">TfidfVectorizer</label>",
            "code"
        ],
        [
            "TfidfVectorizer()<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-228\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-228\">ComplementNB</label>",
            "code"
        ],
        [
            "ComplementNB()\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "We define a grid of hyperparameters to be explored by the\n. Using a\n instead would explore all the\npossible combinations on the grid, which can be costly to compute, whereas the\nparameter n_iter of the \ncontrols the number of different random combination that are evaluated. Notice\nthat setting n_iter larger than the number of possible combinations in a\ngrid would lead to repeating already-explored combinations. We search for the\nbest parameter combination for both the feature extraction (vect__) and the\nclassifier (clf__).",
            "markdown"
        ],
        [
            "import numpy as np\n\nparameter_grid = {\n    \"vect__max_df\": (0.2, 0.4, 0.6, 0.8, 1.0),\n    \"vect__min_df\": (1, 3, 5, 10),\n    \"vect__ngram_range\": ((1, 1), (1, 2)),  # unigrams or bigrams\n    \"vect__norm\": (\"l1\", \"l2\"),\n    \"clf__alpha\": (-6, 6, 13),\n}",
            "code"
        ],
        [
            "In this case n_iter=40 is not an exhaustive search of the hyperparameters\u2019\ngrid. In practice it would be interesting to increase the parameter n_iter\nto get a more informative analysis. As a consequence, the computional time\nincreases. We can reduce it by taking advantage of the parallelisation over\nthe parameter combinations evaluation by increasing the number of CPUs used\nvia the parameter n_jobs.",
            "markdown"
        ],
        [
            "from pprint import \nfrom sklearn.model_selection import \n\nrandom_search = (\n    estimator=pipeline,\n    param_distributions=parameter_grid,\n    n_iter=40,\n    random_state=0,\n    n_jobs=2,\n    verbose=1,\n)\n\nprint(\"Performing grid search...\")\nprint(\"Hyperparameters to be evaluated:\")\n(parameter_grid)",
            "code"
        ],
        [
            "Performing grid search...\nHyperparameters to be evaluated:\n{'clf__alpha': array([1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01,\n       1.e+02, 1.e+03, 1.e+04, 1.e+05, 1.e+06]),\n 'vect__max_df': (0.2, 0.4, 0.6, 0.8, 1.0),\n 'vect__min_df': (1, 3, 5, 10),\n 'vect__ngram_range': ((1, 1), (1, 2)),\n 'vect__norm': ('l1', 'l2')}",
            "code"
        ],
        [
            "from time import \n\nt0 = ()\nrandom_search.fit(data_train.data, data_train.target)\nprint(f\"Done in {() - t0:.3f}s\")",
            "code"
        ],
        [
            "Fitting 5 folds for each of 40 candidates, totalling 200 fits\nDone in 28.793s",
            "code"
        ],
        [
            "print(\"Best parameters combination found:\")\nbest_parameters = random_search.best_estimator_.get_params()\nfor param_name in sorted(parameter_grid.keys()):\n    print(f\"{param_name}: {best_parameters[param_name]}\")",
            "code"
        ],
        [
            "Best parameters combination found:\nclf__alpha: 0.01\nvect__max_df: 0.2\nvect__min_df: 1\nvect__ngram_range: (1, 1)\nvect__norm: l1",
            "code"
        ],
        [
            "test_accuracy = random_search.score(data_test.data, data_test.target)\nprint(\n    \"Accuracy of the best parameters using the inner CV of \"\n    f\"the random search: {random_search.best_score_:.3f}\"\n)\nprint(f\"Accuracy on test set: {test_accuracy:.3f}\")",
            "code"
        ],
        [
            "Accuracy of the best parameters using the inner CV of the random search: 0.816\nAccuracy on test set: 0.709",
            "code"
        ],
        [
            "The prefixes vect and clf are required to avoid possible ambiguities in\nthe pipeline, but are not necessary for visualizing the results. Because of\nthis, we define a function that will rename the tuned hyperparameters and\nimprove the readability.",
            "markdown"
        ],
        [
            "import pandas as pd\n\n\ndef shorten_param(param_name):\n    \"\"\"Remove components' prefixes in param_name.\"\"\"\n    if \"__\" in param_name:\n        return param_name.rsplit(\"__\", 1)[1]\n    return param_name\n\n\ncv_results = (random_search.cv_results_)\ncv_results = cv_results.rename(shorten_param, axis=1)",
            "code"
        ],
        [
            "We can use a \nto visualize the trade-off between scoring time and mean test score (i.e. \u201cCV\nscore\u201d). Passing the cursor over a given point displays the corresponding\nparameters. Error bars correspond to one standard deviation as computed in the\ndifferent folds of the cross-validation.",
            "markdown"
        ],
        [
            "import plotly.express as px\n\nparam_names = [shorten_param(name) for name in parameter_grid.keys()]\nlabels = {\n    \"mean_score_time\": \"CV Score time (s)\",\n    \"mean_test_score\": \"CV score (accuracy)\",\n}\nfig = px.scatter(\n    cv_results,\n    x=\"mean_score_time\",\n    y=\"mean_test_score\",\n    error_x=\"std_score_time\",\n    error_y=\"std_test_score\",\n    hover_data=param_names,\n    labels=labels,\n)\nfig.update_layout(\n    title={\n        \"text\": \"trade-off between scoring time and mean test score\",\n        \"y\": 0.95,\n        \"x\": 0.5,\n        \"xanchor\": \"center\",\n        \"yanchor\": \"top\",\n    }\n)\nfig\n\n\n\n <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script> <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n<script src=\"https://cdn.plot.ly/plotly-2.18.0.min.js\"></script>  <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"9a4b40fa-eebb-4acf-801f-7f06a41f5797\")) {                    Plotly.newPlot(                        \"9a4b40fa-eebb-4acf-801f-7f06a41f5797\",                        [{\"customdata\":[[1.0,3,[1,2],\"l2\",10.0],[0.6,3,[1,2],\"l2\",100.0],[0.6,10,[1,1],\"l1\",0.01],[1.0,10,[1,2],\"l2\",0.001],[0.2,3,[1,2],\"l2\",1.0],[0.2,1,[1,2],\"l2\",1000.0],[0.2,1,[1,2],\"l2\",1.0],[1.0,5,[1,2],\"l1\",100000.0],[0.2,5,[1,2],\"l2\",0.001],[0.4,10,[1,2],\"l1\",0.001],[1.0,5,[1,2],\"l2\",1e-06],[0.4,10,[1,2],\"l2\",100.0],[0.6,3,[1,2],\"l1\",100.0],[0.2,1,[1,1],\"l1\",0.01],[0.6,10,[1,2],\"l2\",0.01],[0.8,3,[1,2],\"l1\",0.001],[0.8,5,[1,1],\"l1\",10000.0],[0.8,1,[1,1],\"l2\",100.0],[0.4,5,[1,2],\"l1\",1.0],[0.8,1,[1,2],\"l1\",1.0],[0.2,1,[1,1],\"l2\",1e-06],[0.4,5,[1,2],\"l2\",1e-06],[0.8,1,[1,2],\"l2\",1.0],[0.4,10,[1,2],\"l2\",1e-06],[0.6,1,[1,1],\"l2\",1.0],[1.0,1,[1,2],\"l1\",1.0],[0.2,1,[1,2],\"l1\",1000000.0],[0.8,1,[1,2],\"l2\",10000.0],[0.8,10,[1,1],\"l1\",0.01],[1.0,5,[1,1],\"l1\",0.001],[0.2,5,[1,2],\"l2\",0.01],[0.8,10,[1,2],\"l2\",100000.0],[0.4,1,[1,2],\"l1\",1e-06],[0.6,3,[1,2],\"l1\",100000.0],[0.2,5,[1,1],\"l2\",10000.0],[1.0,1,[1,1],\"l2\",10000.0],[0.6,3,[1,2],\"l1\",1.0],[0.8,3,[1,2],\"l2\",1000000.0],[0.8,3,[1,2],\"l2\",0.001],[0.2,1,[1,2],\"l2\",0.09999999999999999]],\"error_x\":{\"array\":[0.0074989525013708265,0.004946486027455686,0.003707620691340299,0.005010546244930811,0.00549154412039849,0.009078491629337316,0.00725005995991668,0.005439618159093323,0.006282984768904055,0.005495945792806909,0.005072221781344695,0.005120737127017409,0.004416840440498878,0.0035660495615135843,0.005544929953948123,0.014872847406430051,0.0034340501223700344,0.0037555904827381854,0.005409172964192909,0.008163896343588611,0.0042385138413664775,0.006702660452665178,0.0068722901237735804,0.005543161057459671,0.003370968892173635,0.007270490844933753,0.007676586432134024,0.009472091008904667,0.0030987966174642278,0.004738347187476626,0.006358346839381237,0.005132908740039799,0.0071154139481455604,0.006490966376314975,0.003656690555236237,0.003112002342019827,0.006726392377046396,0.0062815809227272975,0.0065695369375866714,0.008284443068239326]},\"error_y\":{\"array\":[0.021709462916372543,0.019660424372293657,0.02280701551644583,0.04141503839481188,0.025197247740550304,0.02185207140572817,0.03919239713819249,0.0064363918745756095,0.04904124161348298,0.05189316457318038,0.048501681550639386,0.03741855291831512,0.017469874157429344,0.02173863377120521,0.03542621298833128,0.025243761085853186,0.006667969931544837,0.006180189207600158,0.03507679946932842,0.011860875164389911,0.03629456169941096,0.04938433795727698,0.03192696045040785,0.039585633198194775,0.03748169491939798,0.008394295296711565,0.024477745202560856,0.009089774981294351,0.017562478457491922,0.023480916167601486,0.0422343170600075,0.0070716818832471,0.02787646303524185,0.017469874157429344,0.02866436715108689,0.0030358844335549935,0.024286892696920713,0.006307564442123955,0.03887461994008526,0.028260891331683496]},\"hovertemplate\":\"CV Score time (s)=%{x}<br>CV score (accuracy)=%{y}<br>max_df=%{customdata[0]}<br>min_df=%{customdata[1]}<br>ngram_range=%{customdata[2]}<br>norm=%{customdata[3]}<br>alpha=%{customdata[4]}<extra></extra>\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[0.04458394050598145,0.04707136154174805,0.023392200469970703,0.03561687469482422,0.04037842750549316,0.0492828369140625,0.04824423789978027,0.037982940673828125,0.0380521297454834,0.03654317855834961,0.039098358154296874,0.0368809700012207,0.04478645324707031,0.02125234603881836,0.03831219673156738,0.046416950225830075,0.021130466461181642,0.021540355682373048,0.0394838809967041,0.04886045455932617,0.022491979598999023,0.038994884490966795,0.048995256423950195,0.03816032409667969,0.02209053039550781,0.04859938621520996,0.04920797348022461,0.05251688957214355,0.019916343688964843,0.023452281951904297,0.038121604919433595,0.03604555130004883,0.04728374481201172,0.04159135818481445,0.020193862915039062,0.02047595977783203,0.04090127944946289,0.04113917350769043,0.04163060188293457,0.048309040069580075],\"xaxis\":\"x\",\"y\":[0.6067319461444309,0.6114035087719298,0.7444308445532435,0.7385624915000679,0.7735890112879098,0.6988916088671291,0.7350537195702435,0.5775873793009656,0.7723922208622331,0.7327281381748946,0.76890384876921,0.6603835169318646,0.599734802121583,0.8156262749898001,0.73625050999592,0.7958044335645316,0.5729294165646674,0.5705902352781178,0.6661906704746362,0.5857405140758873,0.7841425268597851,0.7665714674282607,0.670875832993336,0.7304229566163472,0.7000271997824018,0.5810825513395892,0.6673806609547123,0.5775873793009656,0.7479464164286685,0.7770909832721339,0.7747314021487828,0.5845913232694139,0.8109547123623011,0.599734802121583,0.7280769753841969,0.5624303005575955,0.6195498436012512,0.5752685978512172,0.7829389364885082,0.8132666938664489],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"CV Score time (s)\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"CV score (accuracy)\"}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60},\"title\":{\"text\":\"trade-off between scoring time and mean test score\",\"y\":0.95,\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"}},                        {\"responsive\": true}                    )                };                            </script> \n\n<br/>\n<br/>",
            "code"
        ],
        [
            "Notice that the cluster of models in the upper-left corner of the plot have\nthe best trade-off between accuracy and scoring time. In this case, using\nbigrams increases the required scoring time without improving considerably the\naccuracy of the pipeline.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "For more information on how to customize an automated tuning to\nmaximize score and minimize scoring time, see the example notebook\n.",
            "markdown"
        ],
        [
            "We can also use a \nto further visualize the mean test score as a function of the tuned\nhyperparameters. This helps finding interactions between more than two\nhyperparameters and provide intuition on their relevance for improving the\nperformance of a pipeline.",
            "markdown"
        ],
        [
            "We apply a math.log10 transformation on the alpha axis to spread the\nactive range and improve the readability of the plot. A value \\(x\\) on\nsaid axis is to be understood as \\(10^x\\).",
            "markdown"
        ],
        [
            "import math\n\ncolumn_results = param_names + [\"mean_test_score\", \"mean_score_time\"]\n\ntransform_funcs = dict.fromkeys(column_results, lambda x: x)\n# Using a logarithmic scale for alpha\ntransform_funcs[\"alpha\"] = \n# L1 norms are mapped to index 1, and L2 norms to index 2\ntransform_funcs[\"norm\"] = lambda x: 2 if x == \"l2\" else 1\n# Unigrams are mapped to index 1 and bigrams to index 2\ntransform_funcs[\"ngram_range\"] = lambda x: x[1]\n\nfig = px.parallel_coordinates(\n    cv_results[column_results].apply(transform_funcs),\n    color=\"mean_test_score\",\n    color_continuous_scale=px.colors.sequential.Viridis_r,\n    labels=labels,\n)\nfig.update_layout(\n    title={\n        \"text\": \"Parallel coordinates plot of text classifier pipeline\",\n        \"y\": 0.99,\n        \"x\": 0.5,\n        \"xanchor\": \"center\",\n        \"yanchor\": \"top\",\n    }\n)\nfig\n\n\n\n <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script> <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n<script src=\"https://cdn.plot.ly/plotly-2.18.0.min.js\"></script>  <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c6acaacd-52bc-4061-a9e6-b279cc3bdbdb\")) {                    Plotly.newPlot(                        \"c6acaacd-52bc-4061-a9e6-b279cc3bdbdb\",                        [{\"dimensions\":[{\"label\":\"max_df\",\"values\":[1.0,0.6,0.6,1.0,0.2,0.2,0.2,1.0,0.2,0.4,1.0,0.4,0.6,0.2,0.6,0.8,0.8,0.8,0.4,0.8,0.2,0.4,0.8,0.4,0.6,1.0,0.2,0.8,0.8,1.0,0.2,0.8,0.4,0.6,0.2,1.0,0.6,0.8,0.8,0.2]},{\"label\":\"min_df\",\"values\":[3,3,10,10,3,1,1,5,5,10,5,10,3,1,10,3,5,1,5,1,1,5,1,10,1,1,1,1,10,5,5,10,1,3,5,1,3,3,3,1]},{\"label\":\"ngram_range\",\"values\":[2,2,1,2,2,2,2,2,2,2,2,2,2,1,2,2,1,1,2,2,1,2,2,2,1,2,2,2,1,1,2,2,2,2,1,1,2,2,2,2]},{\"label\":\"norm\",\"values\":[2,2,1,2,2,2,2,1,2,1,2,2,1,1,2,1,1,2,1,1,2,2,2,2,2,1,1,2,1,1,2,2,1,1,2,2,1,2,2,2]},{\"label\":\"alpha\",\"values\":[1.0,2.0,-2.0,-3.0,0.0,3.0,0.0,5.0,-3.0,-3.0,-6.0,2.0,2.0,-2.0,-2.0,-3.0,4.0,2.0,0.0,0.0,-6.0,-6.0,0.0,-6.0,0.0,0.0,6.0,4.0,-2.0,-3.0,-2.0,5.0,-6.0,5.0,4.0,4.0,0.0,6.0,-3.0,-1.0]},{\"label\":\"CV score (accuracy)\",\"values\":[0.6067319461444309,0.6114035087719298,0.7444308445532435,0.7385624915000679,0.7735890112879098,0.6988916088671291,0.7350537195702435,0.5775873793009656,0.7723922208622331,0.7327281381748946,0.76890384876921,0.6603835169318646,0.599734802121583,0.8156262749898001,0.73625050999592,0.7958044335645316,0.5729294165646674,0.5705902352781178,0.6661906704746362,0.5857405140758873,0.7841425268597851,0.7665714674282607,0.670875832993336,0.7304229566163472,0.7000271997824018,0.5810825513395892,0.6673806609547123,0.5775873793009656,0.7479464164286685,0.7770909832721339,0.7747314021487828,0.5845913232694139,0.8109547123623011,0.599734802121583,0.7280769753841969,0.5624303005575955,0.6195498436012512,0.5752685978512172,0.7829389364885082,0.8132666938664489]},{\"label\":\"CV Score time (s)\",\"values\":[0.04458394050598145,0.04707136154174805,0.023392200469970703,0.03561687469482422,0.04037842750549316,0.0492828369140625,0.04824423789978027,0.037982940673828125,0.0380521297454834,0.03654317855834961,0.039098358154296874,0.0368809700012207,0.04478645324707031,0.02125234603881836,0.03831219673156738,0.046416950225830075,0.021130466461181642,0.021540355682373048,0.0394838809967041,0.04886045455932617,0.022491979598999023,0.038994884490966795,0.048995256423950195,0.03816032409667969,0.02209053039550781,0.04859938621520996,0.04920797348022461,0.05251688957214355,0.019916343688964843,0.023452281951904297,0.038121604919433595,0.03604555130004883,0.04728374481201172,0.04159135818481445,0.020193862915039062,0.02047595977783203,0.04090127944946289,0.04113917350769043,0.04163060188293457,0.048309040069580075]}],\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"line\":{\"color\":[0.6067319461444309,0.6114035087719298,0.7444308445532435,0.7385624915000679,0.7735890112879098,0.6988916088671291,0.7350537195702435,0.5775873793009656,0.7723922208622331,0.7327281381748946,0.76890384876921,0.6603835169318646,0.599734802121583,0.8156262749898001,0.73625050999592,0.7958044335645316,0.5729294165646674,0.5705902352781178,0.6661906704746362,0.5857405140758873,0.7841425268597851,0.7665714674282607,0.670875832993336,0.7304229566163472,0.7000271997824018,0.5810825513395892,0.6673806609547123,0.5775873793009656,0.7479464164286685,0.7770909832721339,0.7747314021487828,0.5845913232694139,0.8109547123623011,0.599734802121583,0.7280769753841969,0.5624303005575955,0.6195498436012512,0.5752685978512172,0.7829389364885082,0.8132666938664489],\"coloraxis\":\"coloraxis\"},\"name\":\"\",\"type\":\"parcoords\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"CV score (accuracy)\"}},\"colorscale\":[[0.0,\"#fde725\"],[0.1111111111111111,\"#b5de2b\"],[0.2222222222222222,\"#6ece58\"],[0.3333333333333333,\"#35b779\"],[0.4444444444444444,\"#1f9e89\"],[0.5555555555555556,\"#26828e\"],[0.6666666666666666,\"#31688e\"],[0.7777777777777778,\"#3e4989\"],[0.8888888888888888,\"#482878\"],[1.0,\"#440154\"]]},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60},\"title\":{\"text\":\"Parallel coordinates plot of text classifier pipeline\",\"y\":0.99,\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"}},                        {\"responsive\": true}                    )                };                            </script> \n\n<br/>\n<br/>",
            "code"
        ],
        [
            "The parallel coordinates plot displays the values of the hyperparameters on\ndifferent columns while the performance metric is color coded. It is possible\nto select a range of results by clicking and holding on any axis of the\nparallel coordinate plot. You can then slide (move) the range selection and\ncross two selections to see the intersections. You can undo a selection by\nclicking once again on the same axis.",
            "markdown"
        ],
        [
            "In particular for this hyperparameter search, it is interesting to notice that\nthe top performing models do not seem to depend on the regularization norm,\nbut they do depend on a trade-off between max_df, min_df and the\nregularization strength alpha. The reason is that including noisy features\n(i.e. max_df close to \\(1.0\\) or min_df close to \\(0\\)) tend to\noverfit and therefore require a stronger regularization to compensate. Having\nless features require less regularization and less scoring time.",
            "markdown"
        ],
        [
            "The best accuracy scores are obtained when alpha is between \\(10^{-6}\\)\nand \\(10^0\\), regardless of the hyperparameter norm.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  31.306 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Model Selection->Statistical comparison of models using grid search": [
        [
            "This example illustrates how to statistically compare the performance of models\ntrained and evaluated using .",
            "markdown"
        ],
        [
            "We will start by simulating moon shaped data (where the ideal separation\nbetween classes is non-linear), adding to it a moderate degree of noise.\nDatapoints will belong to one of two possible classes to be predicted by two\nfeatures. We will simulate 50 samples for each class:",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import \n\nX, y = (noise=0.352, random_state=1, n_samples=100)\n\n(\n    x=X[:, 0], y=X[:, 1], hue=y, marker=\"o\", s=25, edgecolor=\"k\", legend=False\n).set_title(\"Data\")\n()\n\n\n<img alt=\"Data\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_grid_search_stats_001.png\" srcset=\"../../_images/sphx_glr_plot_grid_search_stats_001.png\"/>",
            "code"
        ],
        [
            "We will compare the performance of  estimators that\nvary on their kernel parameter, to decide which choice of this\nhyper-parameter predicts our simulated data best.\nWe will evaluate the performance of the models using\n, repeating 10 times\na 10-fold stratified cross validation using a different randomization of the\ndata in each repetition. The performance will be evaluated using\n.",
            "markdown"
        ],
        [
            "from sklearn.model_selection import , \nfrom sklearn.svm import \n\nparam_grid = [\n    {\"kernel\": [\"linear\"]},\n    {\"kernel\": [\"poly\"], \"degree\": [2, 3]},\n    {\"kernel\": [\"rbf\"]},\n]\n\nsvc = (random_state=0)\n\ncv = (n_splits=10, n_repeats=10, random_state=0)\n\nsearch = (estimator=svc, param_grid=param_grid, scoring=\"roc_auc\", cv=cv)\nsearch.fit(X, y)",
            "code"
        ],
        [
            "GridSearchCV(cv=RepeatedStratifiedKFold(n_repeats=10, n_splits=10, random_state=0),\n             estimator=SVC(random_state=0),\n             param_grid=[{'kernel': ['linear']},\n                         {'degree': [2, 3], 'kernel': ['poly']},\n                         {'kernel': ['rbf']}],\n             scoring='roc_auc')<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-229\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-229\">GridSearchCV</label>",
            "code"
        ],
        [
            "GridSearchCV(cv=RepeatedStratifiedKFold(n_repeats=10, n_splits=10, random_state=0),\n             estimator=SVC(random_state=0),\n             param_grid=[{'kernel': ['linear']},\n                         {'degree': [2, 3], 'kernel': ['poly']},\n                         {'kernel': ['rbf']}],\n             scoring='roc_auc')<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-230\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-230\">estimator: SVC</label>",
            "code"
        ],
        [
            "SVC(random_state=0)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-231\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-231\">SVC</label>",
            "code"
        ],
        [
            "SVC(random_state=0)\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "We can now inspect the results of our search, sorted by their\nmean_test_score:",
            "markdown"
        ],
        [
            "import pandas as pd\n\nresults_df = (search.cv_results_)\nresults_df = results_df.sort_values(by=[\"rank_test_score\"])\nresults_df = results_df.set_index(\n    results_df[\"params\"].apply(lambda x: \"_\".join(str(val) for val in x.values()))\n).rename_axis(\"kernel\")\nresults_df[[\"params\", \"rank_test_score\", \"mean_test_score\", \"std_test_score\"]]\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "We can see that the estimator using the 'rbf' kernel performed best,\nclosely followed by 'linear'. Both estimators with a 'poly' kernel\nperformed worse, with the one using a two-degree polynomial achieving a much\nlower performance than all other models.",
            "markdown"
        ],
        [
            "Usually, the analysis just ends here, but half the story is missing. The\noutput of  does not provide\ninformation on the certainty of the differences between the models.\nWe don\u2019t know if these are <strong>statistically</strong> significant.\nTo evaluate this, we need to conduct a statistical test.\nSpecifically, to contrast the performance of two models we should\nstatistically compare their AUC scores. There are 100 samples (AUC\nscores) for each model as we repreated 10 times a 10-fold cross-validation.",
            "markdown"
        ],
        [
            "However, the scores of the models are not independent: all models are\nevaluated on the <strong>same</strong> 100 partitions, increasing the correlation\nbetween the performance of the models.\nSince some partitions of the data can make the distinction of the classes\nparticularly easy or hard to find for all models, the models scores will\nco-vary.",
            "markdown"
        ],
        [
            "Let\u2019s inspect this partition effect by plotting the performance of all models\nin each fold, and calculating the correlation between models across folds:",
            "markdown"
        ],
        [
            "# create df of model scores ordered by performance\nmodel_scores = results_df.filter(regex=r\"split\\d*_test_score\")\n\n# plot 30 examples of dependency between cv fold and AUC scores\nfig, ax = ()\n(\n    data=model_scores.transpose().iloc[:30],\n    dashes=False,\n    palette=\"Set1\",\n    marker=\"o\",\n    alpha=0.5,\n    ax=ax,\n)\nax.set_xlabel(\"CV test fold\", size=12, labelpad=10)\nax.set_ylabel(\"Model AUC\", size=12)\nax.tick_params(bottom=True, labelbottom=False)\n()\n\n# print correlation of AUC scores across folds\nprint(f\"Correlation of models:\\n {model_scores.transpose().corr()}\")\n\n\n<img alt=\"plot grid search stats\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_grid_search_stats_002.png\" srcset=\"../../_images/sphx_glr_plot_grid_search_stats_002.png\"/>",
            "code"
        ],
        [
            "Correlation of models:\n kernel       rbf    linear    3_poly    2_poly\nkernel\nrbf     1.000000  0.882561  0.783392  0.351390\nlinear  0.882561  1.000000  0.746492  0.298688\n3_poly  0.783392  0.746492  1.000000  0.355440\n2_poly  0.351390  0.298688  0.355440  1.000000",
            "code"
        ],
        [
            "We can observe that the performance of the models highly depends on the fold.",
            "markdown"
        ],
        [
            "As a consequence, if we assume independence between samples we will be\nunderestimating the variance computed in our statistical tests, increasing\nthe number of false positive errors (i.e. detecting a significant difference\nbetween models when such does not exist) .",
            "markdown"
        ],
        [
            "Several variance-corrected statistical tests have been developed for these\ncases. In this example we will show how to implement one of them (the so\ncalled Nadeau and Bengio\u2019s corrected t-test) under two different statistical\nframeworks: frequentist and Bayesian.",
            "markdown"
        ]
    ],
    "Examples->Model Selection->Statistical comparison of models using grid search->Comparing two models: frequentist approach": [
        [
            "We can start by asking: \u201cIs the first model significantly better than the\nsecond model (when ranked by mean_test_score)?\u201d",
            "markdown"
        ],
        [
            "To answer this question using a frequentist approach we could\nrun a paired t-test and compute the p-value. This is also known as\nDiebold-Mariano test in the forecast literature .\nMany variants of such a t-test have been developed to account for the\n\u2018non-independence of samples problem\u2019\ndescribed in the previous section. We will use the one proven to obtain the\nhighest replicability scores (which rate how similar the performance of a\nmodel is when evaluating it on different random partitions of the same\ndataset) while maintaining a low rate of false positives and false negatives:\nthe Nadeau and Bengio\u2019s corrected t-test  that uses a 10 times repeated\n10-fold cross validation .",
            "markdown"
        ],
        [
            "This corrected paired t-test is computed as:\n\n\\[t=\\frac{\\frac{1}{k \\cdot r}\\sum_{i=1}^{k}\\sum_{j=1}^{r}x_{ij}}\n{\\sqrt{(\\frac{1}{k \\cdot r}+\\frac{n_{test}}{n_{train}})\\hat{\\sigma}^2}}\\]",
            "markdown"
        ],
        [
            "where \\(k\\) is the number of folds,\n\\(r\\) the number of repetitions in the cross-validation,\n\\(x\\) is the difference in performance of the models,\n\\(n_{test}\\) is the number of samples used for testing,\n\\(n_{train}\\) is the number of samples used for training,\nand \\(\\hat{\\sigma}^2\\) represents the variance of the observed\ndifferences.",
            "markdown"
        ],
        [
            "Let\u2019s implement a corrected right-tailed paired t-test to evaluate if the\nperformance of the first model is significantly better than that of the\nsecond model. Our null hypothesis is that the second model performs at least\nas good as the first model.",
            "markdown"
        ],
        [
            "import numpy as np\nfrom scipy.stats import \n\n\ndef corrected_std(differences, n_train, n_test):\n    \"\"\"Corrects standard deviation using Nadeau and Bengio's approach.\n\n    Parameters\n    ----------\n    differences : ndarray of shape (n_samples,)\n        Vector containing the differences in the score metrics of two models.\n    n_train : int\n        Number of samples in the training set.\n    n_test : int\n        Number of samples in the testing set.\n\n    Returns\n    -------\n    corrected_std : float\n        Variance-corrected standard deviation of the set of differences.\n    \"\"\"\n    # kr = k times r, r times repeated k-fold crossvalidation,\n    # kr equals the number of times the model was evaluated\n    kr = len(differences)\n    corrected_var = (differences, ddof=1) * (1 / kr + n_test / n_train)\n    corrected_std = (corrected_var)\n    return corrected_std\n\n\ndef compute_corrected_ttest(differences, df, n_train, n_test):\n    \"\"\"Computes right-tailed paired t-test with corrected variance.\n\n    Parameters\n    ----------\n    differences : array-like of shape (n_samples,)\n        Vector containing the differences in the score metrics of two models.\n    df : int\n        Degrees of freedom.\n    n_train : int\n        Number of samples in the training set.\n    n_test : int\n        Number of samples in the testing set.\n\n    Returns\n    -------\n    t_stat : float\n        Variance-corrected t-statistic.\n    p_val : float\n        Variance-corrected p-value.\n    \"\"\"\n    mean = (differences)\n    std = corrected_std(differences, n_train, n_test)\n    t_stat = mean / std\n    p_val = .sf(np.abs(t_stat), df)  # right-tailed t-test\n    return t_stat, p_val",
            "code"
        ],
        [
            "model_1_scores = model_scores.iloc[0].values  # scores of the best model\nmodel_2_scores = model_scores.iloc[1].values  # scores of the second-best model\n\ndifferences = model_1_scores - model_2_scores\n\nn = differences.shape[0]  # number of test sets\ndf = n - 1\nn_train = len(list(cv.split(X, y))[0][0])\nn_test = len(list(cv.split(X, y))[0][1])\n\nt_stat, p_val = compute_corrected_ttest(differences, df, n_train, n_test)\nprint(f\"Corrected t-value: {t_stat:.3f}\\nCorrected p-value: {p_val:.3f}\")",
            "code"
        ],
        [
            "Corrected t-value: 0.750\nCorrected p-value: 0.227",
            "code"
        ],
        [
            "We can compare the corrected t- and p-values with the uncorrected ones:",
            "markdown"
        ],
        [
            "t_stat_uncorrected = (differences) / ((differences, ddof=1) / n)\np_val_uncorrected = .sf(np.abs(t_stat_uncorrected), df)\n\nprint(\n    f\"Uncorrected t-value: {t_stat_uncorrected:.3f}\\n\"\n    f\"Uncorrected p-value: {p_val_uncorrected:.3f}\"\n)",
            "code"
        ],
        [
            "Uncorrected t-value: 2.611\nUncorrected p-value: 0.005",
            "code"
        ],
        [
            "Using the conventional significance alpha level at p=0.05, we observe that\nthe uncorrected t-test concludes that the first model is significantly better\nthan the second.",
            "markdown"
        ],
        [
            "With the corrected approach, in contrast, we fail to detect this difference.",
            "markdown"
        ],
        [
            "In the latter case, however, the frequentist approach does not let us\nconclude that the first and second model have an equivalent performance. If\nwe wanted to make this assertion we need to use a Bayesian approach.",
            "markdown"
        ]
    ],
    "Examples->Model Selection->Statistical comparison of models using grid search->Comparing two models: Bayesian approach": [
        [
            "We can use Bayesian estimation to calculate the probability that the first\nmodel is better than the second. Bayesian estimation will output a\ndistribution followed by the mean \\(\\mu\\) of the differences in the\nperformance of two models.",
            "markdown"
        ],
        [
            "To obtain the posterior distribution we need to define a prior that models\nour beliefs of how the mean is distributed before looking at the data,\nand multiply it by a likelihood function that computes how likely our\nobserved differences are, given the values that the mean of differences\ncould take.",
            "markdown"
        ],
        [
            "Bayesian estimation can be carried out in many forms to answer our question,\nbut in this example we will implement the approach suggested by Benavoli and\ncolleagues .",
            "markdown"
        ],
        [
            "One way of defining our posterior using a closed-form expression is to select\na prior conjugate to the likelihood function. Benavoli and colleagues \nshow that when comparing the performance of two classifiers we can model the\nprior as a Normal-Gamma distribution (with both mean and variance unknown)\nconjugate to a normal likelihood, to thus express the posterior as a normal\ndistribution.\nMarginalizing out the variance from this normal posterior, we can define the\nposterior of the mean parameter as a Student\u2019s t-distribution. Specifically:\n\n\\[St(\\mu;n-1,\\overline{x},(\\frac{1}{n}+\\frac{n_{test}}{n_{train}})\n\\hat{\\sigma}^2)\\]",
            "markdown"
        ],
        [
            "where \\(n\\) is the total number of samples,\n\\(\\overline{x}\\) represents the mean difference in the scores,\n\\(n_{test}\\) is the number of samples used for testing,\n\\(n_{train}\\) is the number of samples used for training,\nand \\(\\hat{\\sigma}^2\\) represents the variance of the observed\ndifferences.",
            "markdown"
        ],
        [
            "Notice that we are using Nadeau and Bengio\u2019s corrected variance in our\nBayesian approach as well.",
            "markdown"
        ],
        [
            "Let\u2019s compute and plot the posterior:",
            "markdown"
        ],
        [
            "# initialize random variable\nt_post = (\n    df, loc=(differences), scale=corrected_std(differences, n_train, n_test)\n)",
            "code"
        ],
        [
            "Let\u2019s plot the posterior distribution:",
            "markdown"
        ],
        [
            "x = (t_post.ppf(0.001), t_post.ppf(0.999), 100)\n\n(x, t_post.pdf(x))\n((-0.04, 0.06, 0.01))\n(x, t_post.pdf(x), 0, facecolor=\"blue\", alpha=0.2)\n(\"Probability density\")\n(r\"Mean difference ($\\mu$)\")\n(\"Posterior distribution\")\n()\n\n\n<img alt=\"Posterior distribution\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_grid_search_stats_003.png\" srcset=\"../../_images/sphx_glr_plot_grid_search_stats_003.png\"/>",
            "code"
        ],
        [
            "We can calculate the probability that the first model is better than the\nsecond by computing the area under the curve of the posterior distribution\nfrom zero to infinity. And also the reverse: we can calculate the probability\nthat the second model is better than the first by computing the area under\nthe curve from minus infinity to zero.",
            "markdown"
        ],
        [
            "better_prob = 1 - t_post.cdf(0)\n\nprint(\n    f\"Probability of {model_scores.index[0]} being more accurate than \"\n    f\"{model_scores.index[1]}: {better_prob:.3f}\"\n)\nprint(\n    f\"Probability of {model_scores.index[1]} being more accurate than \"\n    f\"{model_scores.index[0]}: {1 - better_prob:.3f}\"\n)",
            "code"
        ],
        [
            "Probability of rbf being more accurate than linear: 0.773\nProbability of linear being more accurate than rbf: 0.227",
            "code"
        ],
        [
            "In contrast with the frequentist approach, we can compute the probability\nthat one model is better than the other.",
            "markdown"
        ],
        [
            "Note that we obtained similar results as those in the frequentist approach.\nGiven our choice of priors, we are essentially performing the same\ncomputations, but we are allowed to make different assertions.",
            "markdown"
        ]
    ],
    "Examples->Model Selection->Statistical comparison of models using grid search->Comparing two models: Bayesian approach->Region of Practical Equivalence": [
        [
            "Sometimes we are interested in determining the probabilities that our models\nhave an equivalent performance, where \u201cequivalent\u201d is defined in a practical\nway. A naive approach  would be to define estimators as practically\nequivalent when they differ by less than 1% in their accuracy. But we could\nalso define this practical equivalence taking into account the problem we are\ntrying to solve. For example, a difference of 5% in accuracy would mean an\nincrease of $1000 in sales, and we consider any quantity above that as\nrelevant for our business.",
            "markdown"
        ],
        [
            "In this example we are going to define the\nRegion of Practical Equivalence (ROPE) to be \\([-0.01, 0.01]\\). That is,\nwe will consider two models as practically equivalent if they differ by less\nthan 1% in their performance.",
            "markdown"
        ],
        [
            "To compute the probabilities of the classifiers being practically equivalent,\nwe calculate the area under the curve of the posterior over the ROPE\ninterval:",
            "markdown"
        ],
        [
            "rope_interval = [-0.01, 0.01]\nrope_prob = t_post.cdf(rope_interval[1]) - t_post.cdf(rope_interval[0])\n\nprint(\n    f\"Probability of {model_scores.index[0]} and {model_scores.index[1]} \"\n    f\"being practically equivalent: {rope_prob:.3f}\"\n)",
            "code"
        ],
        [
            "Probability of rbf and linear being practically equivalent: 0.432",
            "code"
        ],
        [
            "We can plot how the posterior is distributed over the ROPE interval:",
            "markdown"
        ],
        [
            "x_rope = (rope_interval[0], rope_interval[1], 100)\n\n(x, t_post.pdf(x))\n((-0.04, 0.06, 0.01))\n([-0.01, 0.01], ymin=0, ymax=(np.max(t_post.pdf(x)) + 1))\n(x_rope, t_post.pdf(x_rope), 0, facecolor=\"blue\", alpha=0.2)\n(\"Probability density\")\n(r\"Mean difference ($\\mu$)\")\n(\"Posterior distribution under the ROPE\")\n()\n\n\n<img alt=\"Posterior distribution under the ROPE\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_grid_search_stats_004.png\" srcset=\"../../_images/sphx_glr_plot_grid_search_stats_004.png\"/>",
            "code"
        ],
        [
            "As suggested in , we can further interpret these probabilities using the\nsame criteria as the frequentist approach: is the probability of falling\ninside the ROPE bigger than 95% (alpha value of 5%)?  In that case we can\nconclude that both models are practically equivalent.",
            "markdown"
        ],
        [
            "The Bayesian estimation approach also allows us to compute how uncertain we\nare about our estimation of the difference. This can be calculated using\ncredible intervals. For a given probability, they show the range of values\nthat the estimated quantity, in our case the mean difference in\nperformance, can take.\nFor example, a 50% credible interval [x, y] tells us that there is a 50%\nprobability that the true (mean) difference of performance between models is\nbetween x and y.",
            "markdown"
        ],
        [
            "Let\u2019s determine the credible intervals of our data using 50%, 75% and 95%:",
            "markdown"
        ],
        [
            "cred_intervals = []\nintervals = [0.5, 0.75, 0.95]\n\nfor interval in intervals:\n    cred_interval = list(t_post.interval(interval))\n    cred_intervals.append([interval, cred_interval[0], cred_interval[1]])\n\ncred_int_df = (\n    cred_intervals, columns=[\"interval\", \"lower value\", \"upper value\"]\n).set_index(\"interval\")\ncred_int_df\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "As shown in the table, there is a 50% probability that the true mean\ndifference between models will be between 0.000977 and 0.019023, 70%\nprobability that it will be between -0.005422 and 0.025422, and 95%\nprobability that it will be between -0.016445   and 0.036445.",
            "markdown"
        ]
    ],
    "Examples->Model Selection->Statistical comparison of models using grid search->Pairwise comparison of all models: frequentist approach": [
        [
            "We could also be interested in comparing the performance of all our models\nevaluated with . In this case\nwe would be running our statistical test multiple times, which leads us to\nthe .",
            "markdown"
        ],
        [
            "There are many possible ways to tackle this problem, but a standard approach\nis to apply a . Bonferroni can be\ncomputed by multiplying the p-value by the number of comparisons we are\ntesting.",
            "markdown"
        ],
        [
            "Let\u2019s compare the performance of the models using the corrected t-test:",
            "markdown"
        ],
        [
            "from itertools import \nfrom math import \n\nn_comparisons = (len(model_scores)) / (\n    (2) * (len(model_scores) - 2)\n)\npairwise_t_test = []\n\nfor model_i, model_k in (range(len(model_scores)), 2):\n    model_i_scores = model_scores.iloc[model_i].values\n    model_k_scores = model_scores.iloc[model_k].values\n    differences = model_i_scores - model_k_scores\n    t_stat, p_val = compute_corrected_ttest(differences, df, n_train, n_test)\n    p_val *= n_comparisons  # implement Bonferroni correction\n    # Bonferroni can output p-values higher than 1\n    p_val = 1 if p_val  1 else p_val\n    pairwise_t_test.append(\n        [model_scores.index[model_i], model_scores.index[model_k], t_stat, p_val]\n    )\n\npairwise_comp_df = (\n    pairwise_t_test, columns=[\"model_1\", \"model_2\", \"t_stat\", \"p_val\"]\n).round(3)\npairwise_comp_df\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "We observe that after correcting for multiple comparisons, the only model\nthat significantly differs from the others is '2_poly'.\n'rbf', the model ranked first by\n, does not significantly\ndiffer from 'linear' or '3_poly'.",
            "markdown"
        ]
    ],
    "Examples->Model Selection->Statistical comparison of models using grid search->Pairwise comparison of all models: Bayesian approach": [
        [
            "When using Bayesian estimation to compare multiple models, we don\u2019t need to\ncorrect for multiple comparisons (for reasons why see ).",
            "markdown"
        ],
        [
            "We can carry out our pairwise comparisons the same way as in the first\nsection:",
            "markdown"
        ],
        [
            "pairwise_bayesian = []\n\nfor model_i, model_k in (range(len(model_scores)), 2):\n    model_i_scores = model_scores.iloc[model_i].values\n    model_k_scores = model_scores.iloc[model_k].values\n    differences = model_i_scores - model_k_scores\n    t_post = (\n        df, loc=(differences), scale=corrected_std(differences, n_train, n_test)\n    )\n    worse_prob = t_post.cdf(rope_interval[0])\n    better_prob = 1 - t_post.cdf(rope_interval[1])\n    rope_prob = t_post.cdf(rope_interval[1]) - t_post.cdf(rope_interval[0])\n\n    pairwise_bayesian.append([worse_prob, better_prob, rope_prob])\n\npairwise_bayesian_df = (\n    pairwise_bayesian, columns=[\"worse_prob\", \"better_prob\", \"rope_prob\"]\n).round(3)\n\npairwise_comp_df = pairwise_comp_df.join(pairwise_bayesian_df)\npairwise_comp_df\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "Using the Bayesian approach we can compute the probability that a model\nperforms better, worse or practically equivalent to another.",
            "markdown"
        ],
        [
            "Results show that the model ranked first by\n 'rbf', has approximately a\n6.8% chance of being worse than 'linear', and a 1.8% chance of being worse\nthan '3_poly'.\n'rbf' and 'linear' have a 43% probability of being practically\nequivalent, while 'rbf' and '3_poly' have a 10% chance of being so.",
            "markdown"
        ],
        [
            "Similarly to the conclusions obtained using the frequentist approach, all\nmodels have a 100% probability of being better than '2_poly', and none have\na practically equivalent performance with the latter.",
            "markdown"
        ]
    ],
    "Examples->Model Selection->Statistical comparison of models using grid search->Take-home messages": [
        [
            "Small differences in performance measures might easily turn out to be\nmerely by chance, but not because one model predicts systematically better\nthan the other. As shown in this example, statistics can tell you how\nlikely that is.",
            "markdown"
        ],
        [
            "When statistically comparing the performance of two models evaluated in\nGridSearchCV, it is necessary to correct the calculated variance which\ncould be underestimated since the scores of the models are not independent\nfrom each other.",
            "markdown"
        ],
        [
            "A frequentist approach that uses a (variance-corrected) paired t-test can\ntell us if the performance of one model is better than another with a\ndegree of certainty above chance.",
            "markdown"
        ],
        [
            "A Bayesian approach can provide the probabilities of one model being\nbetter, worse or practically equivalent than another. It can also tell us\nhow confident we are of knowing that the true differences of our models\nfall under a certain range of values.",
            "markdown"
        ],
        [
            "If multiple models are statistically compared, a multiple comparisons\ncorrection is needed when using the frequentist approach.",
            "markdown"
        ],
        [
            "References\n\n\n[]",
            "markdown"
        ],
        [
            "Dietterich, T. G. (1998). .\nNeural computation, 10(7).\n\n\n[]",
            "markdown"
        ],
        [
            "Nadeau, C., & Bengio, Y. (2000). .\nIn Advances in neural information processing systems.\n\n\n[]",
            "markdown"
        ],
        [
            "Bouckaert, R. R., & Frank, E. (2004). .\nIn Pacific-Asia Conference on Knowledge Discovery and Data Mining.\n\n\n[4]\n(,,,,)",
            "markdown"
        ],
        [
            "Benavoli, A., Corani, G., Dem\u0161ar, J., & Zaffalon, M. (2017). .\nThe Journal of Machine Learning Research, 18(1). See the Python\nlibrary that accompanies this paper .\n\n\n[]",
            "markdown"
        ],
        [
            "Diebold, F.X. & Mariano R.S. (1995). \nJournal of Business & economic statistics, 20(1), 134-144.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  1.433 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Model Selection->Successive Halving Iterations": [
        [
            "This example illustrates how a successive halving search\n( and\n)\niteratively chooses the best parameter combination out of\nmultiple candidates.",
            "markdown"
        ],
        [
            "import pandas as pd\nfrom sklearn import datasets\nimport matplotlib.pyplot as plt\nfrom scipy.stats import \nimport numpy as np\n\nfrom sklearn.experimental import enable_halving_search_cv  # noqa\nfrom sklearn.model_selection import \nfrom sklearn.ensemble import",
            "code"
        ],
        [
            "We first define the parameter space and train a\n instance.",
            "markdown"
        ],
        [
            "rng = (0)\n\nX, y = (n_samples=400, n_features=12, random_state=rng)\n\nclf = (n_estimators=20, random_state=rng)\n\nparam_dist = {\n    \"max_depth\": [3, None],\n    \"max_features\": (1, 6),\n    \"min_samples_split\": (2, 11),\n    \"bootstrap\": [True, False],\n    \"criterion\": [\"gini\", \"entropy\"],\n}\n\nrsh = (\n    estimator=clf, param_distributions=param_dist, factor=2, random_state=rng\n)\nrsh.fit(X, y)",
            "code"
        ],
        [
            "HalvingRandomSearchCV(estimator=RandomForestClassifier(n_estimators=20,\n                                                       random_state=RandomState(MT19937) at 0x7F0D3F51C440),\n                      factor=2,\n                      param_distributions={'bootstrap': [True, False],\n                                           'criterion': ['gini', 'entropy'],\n                                           'max_depth': [3, None],\n                                           'max_features': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x7f0d547a2250,\n                                           'min_samples_split': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x7f0d54e0ef10},\n                      random_state=RandomState(MT19937) at 0x7F0D3F51C440)<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-232\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-232\">HalvingRandomSearchCV</label>",
            "code"
        ],
        [
            "HalvingRandomSearchCV(estimator=RandomForestClassifier(n_estimators=20,\n                                                       random_state=RandomState(MT19937) at 0x7F0D3F51C440),\n                      factor=2,\n                      param_distributions={'bootstrap': [True, False],\n                                           'criterion': ['gini', 'entropy'],\n                                           'max_depth': [3, None],\n                                           'max_features': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x7f0d547a2250,\n                                           'min_samples_split': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x7f0d54e0ef10},\n                      random_state=RandomState(MT19937) at 0x7F0D3F51C440)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-233\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-233\">estimator: RandomForestClassifier</label>",
            "code"
        ],
        [
            "RandomForestClassifier(n_estimators=20,\n                       random_state=RandomState(MT19937) at 0x7F0D3F51C440)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-234\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-234\">RandomForestClassifier</label>",
            "code"
        ],
        [
            "RandomForestClassifier(n_estimators=20,\n                       random_state=RandomState(MT19937) at 0x7F0D3F51C440)\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "We can now use the cv_results_ attribute of the search estimator to inspect\nand plot the evolution of the search.",
            "markdown"
        ],
        [
            "results = (rsh.cv_results_)\nresults[\"params_str\"] = results.params.apply(str)\nresults.drop_duplicates(subset=(\"params_str\", \"iter\"), inplace=True)\nmean_scores = results.pivot(\n    index=\"iter\", columns=\"params_str\", values=\"mean_test_score\"\n)\nax = mean_scores.plot(legend=False, alpha=0.6)\n\nlabels = [\n    f\"iter={i}\\nn_samples={rsh.n_resources_[i]}\\nn_candidates={rsh.n_candidates_[i]}\"\n    for i in range(rsh.n_iterations_)\n]\n\nax.set_xticks(range(rsh.n_iterations_))\nax.set_xticklabels(labels, rotation=45, multialignment=\"left\")\nax.set_title(\"Scores of candidates over iterations\")\nax.set_ylabel(\"mean test score\", fontsize=15)\nax.set_xlabel(\"iterations\", fontsize=15)\n()\n()\n\n\n<img alt=\"Scores of candidates over iterations\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_successive_halving_iterations_001.png\" srcset=\"../../_images/sphx_glr_plot_successive_halving_iterations_001.png\"/>",
            "code"
        ]
    ],
    "Examples->Model Selection->Successive Halving Iterations->Number of candidates and amount of resource at each iteration": [
        [
            "At the first iteration, a small amount of resources is used. The resource\nhere is the number of samples that the estimators are trained on. All\ncandidates are evaluated.",
            "markdown"
        ],
        [
            "At the second iteration, only the best half of the candidates is evaluated.\nThe number of allocated resources is doubled: candidates are evaluated on\ntwice as many samples.",
            "markdown"
        ],
        [
            "This process is repeated until the last iteration, where only 2 candidates\nare left. The best candidate is the candidate that has the best score at the\nlast iteration.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  4.884 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Model Selection->Test with permutations the significance of a classification score": [
        [
            "This example demonstrates the use of\n to evaluate the\nsignificance of a cross-validated score using permutations.",
            "markdown"
        ],
        [
            "# Authors:  Alexandre Gramfort &lt;alexandre.gramfort@inria.fr\n#           Lucy Liu\n# License: BSD 3 clause",
            "code"
        ]
    ],
    "Examples->Model Selection->Test with permutations the significance of a classification score->Dataset": [
        [
            "We will use the , which consists of measurements taken\nfrom 3 types of irises.",
            "markdown"
        ],
        [
            "from sklearn.datasets import \n\niris = ()\nX = iris.data\ny = iris.target",
            "code"
        ],
        [
            "We will also generate some random feature data (i.e., 20 features),\nuncorrelated with the class labels in the iris dataset.",
            "markdown"
        ],
        [
            "import numpy as np\n\nn_uncorrelated_features = 20\nrng = (seed=0)\n# Use same number of samples as in iris and 20 features\nX_rand = rng.normal(size=(X.shape[0], n_uncorrelated_features))",
            "code"
        ]
    ],
    "Examples->Model Selection->Test with permutations the significance of a classification score->Permutation test score": [
        [
            "Next, we calculate the\n using the original\niris dataset, which strongly predict the labels and\nthe randomly generated features and iris labels, which should have\nno dependency between features and labels. We use the\n classifier and  to evaluate\nthe model at each round.",
            "markdown"
        ],
        [
            "generates a null\ndistribution by calculating the accuracy of the classifier\non 1000 different permutations of the dataset, where features\nremain the same but labels undergo different permutations. This is the\ndistribution for the null hypothesis which states there is no dependency\nbetween the features and labels. An empirical p-value is then calculated as\nthe percentage of permutations for which the score obtained is greater\nthat the score obtained using the original data.",
            "markdown"
        ],
        [
            "from sklearn.svm import \nfrom sklearn.model_selection import \nfrom sklearn.model_selection import \n\nclf = (kernel=\"linear\", random_state=7)\ncv = (2, shuffle=True, random_state=0)\n\nscore_iris, perm_scores_iris, pvalue_iris = (\n    clf, X, y, scoring=\"accuracy\", cv=cv, n_permutations=1000\n)\n\nscore_rand, perm_scores_rand, pvalue_rand = (\n    clf, X_rand, y, scoring=\"accuracy\", cv=cv, n_permutations=1000\n)",
            "code"
        ]
    ],
    "Examples->Model Selection->Test with permutations the significance of a classification score->Permutation test score->Original data": [
        [
            "Below we plot a histogram of the permutation scores (the null\ndistribution). The red line indicates the score obtained by the classifier\non the original data. The score is much better than those obtained by\nusing permuted data and the p-value is thus very low. This indicates that\nthere is a low likelihood that this good score would be obtained by chance\nalone. It provides evidence that the iris dataset contains real dependency\nbetween features and labels and the classifier was able to utilize this\nto obtain good results.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n\nfig, ax = ()\n\nax.hist(perm_scores_iris, bins=20, density=True)\nax.axvline(score_iris, ls=\"--\", color=\"r\")\nscore_label = f\"Score on original\\ndata: {score_iris:.2f}\\n(p-value: {pvalue_iris:.3f})\"\nax.text(0.7, 10, score_label, fontsize=12)\nax.set_xlabel(\"Accuracy score\")\n_ = ax.set_ylabel(\"Probability\")\n\n\n<img alt=\"plot permutation tests for classification\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_permutation_tests_for_classification_001.png\" srcset=\"../../_images/sphx_glr_plot_permutation_tests_for_classification_001.png\"/>",
            "code"
        ]
    ],
    "Examples->Model Selection->Test with permutations the significance of a classification score->Permutation test score->Random data": [
        [
            "Below we plot the null distribution for the randomized data. The permutation\nscores are similar to those obtained using the original iris dataset\nbecause the permutation always destroys any feature label dependency present.\nThe score obtained on the original randomized data in this case though, is\nvery poor. This results in a large p-value, confirming that there was no\nfeature label dependency in the original data.",
            "markdown"
        ],
        [
            "fig, ax = ()\n\nax.hist(perm_scores_rand, bins=20, density=True)\nax.set_xlim(0.13)\nax.axvline(score_rand, ls=\"--\", color=\"r\")\nscore_label = f\"Score on original\\ndata: {score_rand:.2f}\\n(p-value: {pvalue_rand:.3f})\"\nax.text(0.14, 7.5, score_label, fontsize=12)\nax.set_xlabel(\"Accuracy score\")\nax.set_ylabel(\"Probability\")\n()\n\n\n<img alt=\"plot permutation tests for classification\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_permutation_tests_for_classification_002.png\" srcset=\"../../_images/sphx_glr_plot_permutation_tests_for_classification_002.png\"/>",
            "code"
        ],
        [
            "Another possible reason for obtaining a high p-value is that the classifier\nwas not able to use the structure in the data. In this case, the p-value\nwould only be low for classifiers that are able to utilize the dependency\npresent. In our case above, where the data is random, all classifiers would\nhave a high p-value as there is no structure present in the data.",
            "markdown"
        ],
        [
            "Finally, note that this test has been shown to produce low p-values even\nif there is only weak structure in the data .",
            "markdown"
        ],
        [
            "References:\n\n\n[]",
            "markdown"
        ],
        [
            "Ojala and Garriga. . The\nJournal of Machine Learning Research (2010) vol. 11",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  10.554 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Model Selection->Train error vs Test error": [
        [
            "Illustration of how the performance of an estimator on unseen data (test data)\nis not the same as the performance on training data. As the regularization\nincreases the performance on train decreases while the performance on test\nis optimal within a range of values of the regularization parameter.\nThe example with an Elastic-Net regression model and the performance is\nmeasured using the explained variance a.k.a. R^2.",
            "markdown"
        ],
        [
            "# Author: Alexandre Gramfort &lt;alexandre.gramfort@inria.fr\n# License: BSD 3 clause",
            "code"
        ]
    ],
    "Examples->Model Selection->Train error vs Test error->Generate sample data": [
        [
            "import numpy as np\nfrom sklearn import linear_model\nfrom sklearn.datasets import \nfrom sklearn.model_selection import \n\nn_samples_train, n_samples_test, n_features = 75, 150, 500\nX, y, coef = (\n    n_samples=n_samples_train + n_samples_test,\n    n_features=n_features,\n    n_informative=50,\n    shuffle=False,\n    noise=1.0,\n    coef=True,\n)\nX_train, X_test, y_train, y_test = (\n    X, y, train_size=n_samples_train, test_size=n_samples_test, shuffle=False\n)",
            "code"
        ]
    ],
    "Examples->Model Selection->Train error vs Test error->Compute train and test errors": [
        [
            "alphas = (-5, 1, 60)\nenet = (l1_ratio=0.7, max_iter=10000)\ntrain_errors = list()\ntest_errors = list()\nfor alpha in alphas:\n    enet.set_params(alpha=alpha)\n    enet.fit(X_train, y_train)\n    train_errors.append(enet.score(X_train, y_train))\n    test_errors.append(enet.score(X_test, y_test))\n\ni_alpha_optim = (test_errors)\nalpha_optim = alphas[i_alpha_optim]\nprint(\"Optimal regularization parameter : %s\" % alpha_optim)\n\n# Estimate the coef_ on full data with optimal regularization parameter\nenet.set_params(alpha=alpha_optim)\ncoef_ = enet.fit(X, y).coef_",
            "code"
        ],
        [
            "Optimal regularization parameter : 0.0002652948464431897",
            "code"
        ]
    ],
    "Examples->Model Selection->Train error vs Test error->Plot results functions": [
        [
            "import matplotlib.pyplot as plt\n\n(2, 1, 1)\n(alphas, train_errors, label=\"Train\")\n(alphas, test_errors, label=\"Test\")\n(\n    alpha_optim,\n    ()[0],\n    np.max(test_errors),\n    color=\"k\",\n    linewidth=3,\n    label=\"Optimum on test\",\n)\n(loc=\"lower right\")\n([0, 1.2])\n(\"Regularization parameter\")\n(\"Performance\")\n\n# Show estimated coef_ vs true coef\n(2, 1, 2)\n(coef, label=\"True coef\")\n(coef_, label=\"Estimated coef\")\n()\n(0.09, 0.04, 0.94, 0.94, 0.26, 0.26)\n()\n\n\n<img alt=\"plot train error vs test error\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_train_error_vs_test_error_001.png\" srcset=\"../../_images/sphx_glr_plot_train_error_vs_test_error_001.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  7.034 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Model Selection->Underfitting vs. Overfitting": [
        [
            "This example demonstrates the problems of underfitting and overfitting and\nhow we can use linear regression with polynomial features to approximate\nnonlinear functions. The plot shows the function that we want to approximate,\nwhich is a part of the cosine function. In addition, the samples from the\nreal function and the approximations of different models are displayed. The\nmodels have polynomial features of different degrees. We can see that a\nlinear function (polynomial with degree 1) is not sufficient to fit the\ntraining samples. This is called <strong>underfitting</strong>. A polynomial of degree 4\napproximates the true function almost perfectly. However, for higher degrees\nthe model will <strong>overfit</strong> the training data, i.e. it learns the noise of the\ntraining data.\nWe evaluate quantitatively <strong>overfitting</strong> / <strong>underfitting</strong> by using\ncross-validation. We calculate the mean squared error (MSE) on the validation\nset, the higher, the less likely the model generalizes correctly from the\ntraining data.\n<img alt=\"Degree 1 MSE = 4.08e-01(+/- 4.25e-01), Degree 4 MSE = 4.32e-02(+/- 7.08e-02), Degree 15 MSE = 1.82e+08(+/- 5.46e+08)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_underfitting_overfitting_001.png\" srcset=\"../../_images/sphx_glr_plot_underfitting_overfitting_001.png\"/>",
            "markdown"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import \nfrom sklearn.preprocessing import \nfrom sklearn.linear_model import \nfrom sklearn.model_selection import \n\n\ndef true_fun(X):\n    return (1.5 *  * X)\n\n\n(0)\n\nn_samples = 30\ndegrees = [1, 4, 15]\n\nX = ((n_samples))\ny = true_fun(X) + (n_samples) * 0.1\n\n(figsize=(14, 5))\nfor i in range(len(degrees)):\n    ax = (1, len(degrees), i + 1)\n    (ax, xticks=(), yticks=())\n\n    polynomial_features = (degree=degrees[i], include_bias=False)\n    linear_regression = ()\n    pipeline = (\n        [\n            (\"polynomial_features\", polynomial_features),\n            (\"linear_regression\", linear_regression),\n        ]\n    )\n    pipeline.fit(X[:, ], y)\n\n    # Evaluate the models using crossvalidation\n    scores = (\n        pipeline, X[:, ], y, scoring=\"neg_mean_squared_error\", cv=10\n    )\n\n    X_test = (0, 1, 100)\n    (X_test, pipeline.predict(X_test[:, ]), label=\"Model\")\n    (X_test, true_fun(X_test), label=\"True function\")\n    (X, y, edgecolor=\"b\", s=20, label=\"Samples\")\n    (\"x\")\n    (\"y\")\n    ((0, 1))\n    ((-2, 2))\n    (loc=\"best\")\n    (\n        \"Degree {}\\nMSE = {:.2e}(+/- {:.2e})\".format(\n            degrees[i], -scores.mean(), scores.std()\n        )\n    )\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.200 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Model Selection->Visualizing cross-validation behavior in scikit-learn": [
        [
            "Choosing the right cross-validation object is a crucial part of fitting a\nmodel properly. There are many ways to split data into training and test\nsets in order to avoid model overfitting, to standardize the number of\ngroups in test sets, etc.",
            "markdown"
        ],
        [
            "This example visualizes the behavior of several common scikit-learn objects\nfor comparison.",
            "markdown"
        ],
        [
            "from sklearn.model_selection import (\n    ,\n    ,\n    ,\n    ,\n    ,\n    ,\n    ,\n    ,\n)\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import \n\nrng = (1338)\ncmap_data = plt.cm.Paired\ncmap_cv = plt.cm.coolwarm\nn_splits = 4",
            "code"
        ]
    ],
    "Examples->Model Selection->Visualizing cross-validation behavior in scikit-learn->Visualize our data": [
        [
            "First, we must understand the structure of our data. It has 100 randomly\ngenerated input datapoints, 3 classes split unevenly across datapoints,\nand 10 \u201cgroups\u201d split evenly across datapoints.",
            "markdown"
        ],
        [
            "As we\u2019ll see, some cross-validation objects do specific things with\nlabeled data, others behave differently with grouped data, and others\ndo not use this information.",
            "markdown"
        ],
        [
            "To begin, we\u2019ll visualize our data.",
            "markdown"
        ],
        [
            "# Generate the class/group data\nn_points = 100\nX = rng.randn(100, 10)\n\npercentiles_classes = [0.1, 0.3, 0.6]\ny = ([[ii] * int(100 * perc) for ii, perc in enumerate(percentiles_classes)])\n\n# Generate uneven groups\ngroup_prior = rng.dirichlet([2] * 10)\ngroups = ((10), rng.multinomial(100, group_prior))\n\n\ndef visualize_groups(classes, groups, name):\n    # Visualize dataset groups\n    fig, ax = ()\n    ax.scatter(\n        range(len(groups)),\n        [0.5] * len(groups),\n        c=groups,\n        marker=\"_\",\n        lw=50,\n        cmap=cmap_data,\n    )\n    ax.scatter(\n        range(len(groups)),\n        [3.5] * len(groups),\n        c=classes,\n        marker=\"_\",\n        lw=50,\n        cmap=cmap_data,\n    )\n    ax.set(\n        ylim=[-1, 5],\n        yticks=[0.5, 3.5],\n        yticklabels=[\"Data\\ngroup\", \"Data\\nclass\"],\n        xlabel=\"Sample index\",\n    )\n\n\nvisualize_groups(y, groups, \"no groups\")\n\n\n<img alt=\"plot cv indices\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cv_indices_001.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_001.png\"/>",
            "code"
        ]
    ],
    "Examples->Model Selection->Visualizing cross-validation behavior in scikit-learn->Define a function to visualize cross-validation behavior": [
        [
            "We\u2019ll define a function that lets us visualize the behavior of each\ncross-validation object. We\u2019ll perform 4 splits of the data. On each\nsplit, we\u2019ll visualize the indices chosen for the training set\n(in blue) and the test set (in red).",
            "markdown"
        ],
        [
            "def plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\n    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n\n    # Generate the training/testing visualizations for each CV split\n    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y, groups=group)):\n        # Fill in indices with the training/test groups\n        indices = ([] * len(X))\n        indices[tt] = 1\n        indices[tr] = 0\n\n        # Visualize the results\n        ax.scatter(\n            range(len(indices)),\n            [ii + 0.5] * len(indices),\n            c=indices,\n            marker=\"_\",\n            lw=lw,\n            cmap=cmap_cv,\n            vmin=-0.2,\n            vmax=1.2,\n        )\n\n    # Plot the data classes and groups at the end\n    ax.scatter(\n        range(len(X)), [ii + 1.5] * len(X), c=y, marker=\"_\", lw=lw, cmap=cmap_data\n    )\n\n    ax.scatter(\n        range(len(X)), [ii + 2.5] * len(X), c=group, marker=\"_\", lw=lw, cmap=cmap_data\n    )\n\n    # Formatting\n    yticklabels = list(range(n_splits)) + [\"class\", \"group\"]\n    ax.set(\n        yticks=(n_splits + 2) + 0.5,\n        yticklabels=yticklabels,\n        xlabel=\"Sample index\",\n        ylabel=\"CV iteration\",\n        ylim=[n_splits + 2.2, -0.2],\n        xlim=[0, 100],\n    )\n    ax.set_title(\"{}\".format(type(cv).__name__), fontsize=15)\n    return ax",
            "code"
        ],
        [
            "Let\u2019s see how it looks for the \ncross-validation object:",
            "markdown"
        ],
        [
            "fig, ax = ()\ncv = (n_splits)\nplot_cv_indices(cv, X, y, groups, ax, n_splits)\n\n\n<img alt=\"KFold\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cv_indices_002.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_002.png\"/>",
            "code"
        ],
        [
            "&lt;AxesSubplot: title={'center': 'KFold'}, xlabel='Sample index', ylabel='CV iteration'",
            "code"
        ],
        [
            "As you can see, by default the KFold cross-validation iterator does not\ntake either datapoint class or group into consideration. We can change this\nby using either:",
            "markdown"
        ],
        [
            "StratifiedKFold to preserve the percentage of samples for each class.",
            "markdown"
        ],
        [
            "GroupKFold to ensure that the same group will not appear in two\ndifferent folds.",
            "markdown"
        ],
        [
            "StratifiedGroupKFold to keep the constraint of GroupKFold while\nattempting to return stratified folds.",
            "markdown"
        ],
        [
            "cvs = [, , ]\n\nfor cv in cvs:\n    fig, ax = (figsize=(6, 3))\n    plot_cv_indices(cv(n_splits), X, y, groups, ax, n_splits)\n    ax.legend(\n        [(color=cmap_cv(0.8)), (color=cmap_cv(0.02))],\n        [\"Testing set\", \"Training set\"],\n        loc=(1.02, 0.8),\n    )\n    # Make the legend fit\n    ()\n    fig.subplots_adjust(right=0.7)\n\n\n\n<img alt=\"StratifiedKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_003.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_003.png\"/>\n<img alt=\"GroupKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_004.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_004.png\"/>\n<img alt=\"StratifiedGroupKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_005.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_005.png\"/>",
            "code"
        ],
        [
            "Next we\u2019ll visualize this behavior for a number of CV iterators.",
            "markdown"
        ]
    ],
    "Examples->Model Selection->Visualizing cross-validation behavior in scikit-learn->Visualize cross-validation indices for many CV objects": [
        [
            "Let\u2019s visually compare the cross validation behavior for many\nscikit-learn cross-validation objects. Below we will loop through several\ncommon cross-validation objects, visualizing the behavior of each.",
            "markdown"
        ],
        [
            "Note how some use the group/class information while others do not.",
            "markdown"
        ],
        [
            "cvs = [\n    ,\n    ,\n    ,\n    ,\n    ,\n    ,\n    ,\n    ,\n]\n\n\nfor cv in cvs:\n    this_cv = cv(n_splits=n_splits)\n    fig, ax = (figsize=(6, 3))\n    plot_cv_indices(this_cv, X, y, groups, ax, n_splits)\n\n    ax.legend(\n        [(color=cmap_cv(0.8)), (color=cmap_cv(0.02))],\n        [\"Testing set\", \"Training set\"],\n        loc=(1.02, 0.8),\n    )\n    # Make the legend fit\n    ()\n    fig.subplots_adjust(right=0.7)\n()\n\n\n\n<img alt=\"KFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_006.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_006.png\"/>\n<img alt=\"GroupKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_007.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_007.png\"/>\n<img alt=\"ShuffleSplit\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_008.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_008.png\"/>\n<img alt=\"StratifiedKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_009.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_009.png\"/>\n<img alt=\"StratifiedGroupKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_010.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_010.png\"/>\n<img alt=\"GroupShuffleSplit\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_011.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_011.png\"/>\n<img alt=\"StratifiedShuffleSplit\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_012.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_012.png\"/>\n<img alt=\"TimeSeriesSplit\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_013.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_013.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  1.087 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Multioutput methods->Classifier Chain": [
        [
            "Example of using classifier chain on a multilabel dataset.",
            "markdown"
        ],
        [
            "For this example we will use the  dataset which contains\n2417 datapoints each with 103 features and 14 possible labels. Each\ndata point has at least one label. As a baseline we first train a logistic\nregression classifier for each of the 14 labels. To evaluate the performance of\nthese classifiers we predict on a held-out test set and calculate the\n for each sample.",
            "markdown"
        ],
        [
            "Next we create 10 classifier chains. Each classifier chain contains a\nlogistic regression model for each of the 14 labels. The models in each\nchain are ordered randomly. In addition to the 103 features in the dataset,\neach model gets the predictions of the preceding models in the chain as\nfeatures (note that by default at training time each model gets the true\nlabels as features). These additional features allow each chain to exploit\ncorrelations among the classes. The Jaccard similarity score for each chain\ntends to be greater than that of the set independent logistic models.",
            "markdown"
        ],
        [
            "Because the models in each chain are arranged randomly there is significant\nvariation in performance among the chains. Presumably there is an optimal\nordering of the classes in a chain that will yield the best performance.\nHowever we do not know that ordering a priori. Instead we can construct an\nvoting ensemble of classifier chains by averaging the binary predictions of\nthe chains and apply a threshold of 0.5. The Jaccard similarity score of the\nensemble is greater than that of the independent models and tends to exceed\nthe score of each chain in the ensemble (although this is not guaranteed\nwith randomly ordered chains).\n<img alt=\"Classifier Chain Ensemble Performance Comparison\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_classifier_chain_yeast_001.png\" srcset=\"../../_images/sphx_glr_plot_classifier_chain_yeast_001.png\"/>",
            "markdown"
        ],
        [
            "# Author: Adam Kleczewski\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import \nfrom sklearn.multioutput import \nfrom sklearn.model_selection import \nfrom sklearn.multiclass import \nfrom sklearn.metrics import \nfrom sklearn.linear_model import \n\n# Load a multi-label dataset from https://www.openml.org/d/40597\nX, Y = (\"yeast\", version=4, return_X_y=True, parser=\"pandas\")\nY = Y == \"TRUE\"\nX_train, X_test, Y_train, Y_test = (X, Y, test_size=0.2, random_state=0)\n\n# Fit an independent logistic regression model for each class using the\n# OneVsRestClassifier wrapper.\nbase_lr = ()\novr = (base_lr)\novr.fit(X_train, Y_train)\nY_pred_ovr = ovr.predict(X_test)\novr_jaccard_score = (Y_test, Y_pred_ovr, average=\"samples\")\n\n# Fit an ensemble of logistic regression classifier chains and take the\n# take the average prediction of all the chains.\nchains = [(base_lr, order=\"random\", random_state=i) for i in range(10)]\nfor chain in chains:\n    chain.fit(X_train, Y_train)\n\nY_pred_chains = ([chain.predict(X_test) for chain in chains])\nchain_jaccard_scores = [\n    (Y_test, Y_pred_chain = 0.5, average=\"samples\")\n    for Y_pred_chain in Y_pred_chains\n]\n\nY_pred_ensemble = Y_pred_chains.mean(axis=0)\nensemble_jaccard_score = (\n    Y_test, Y_pred_ensemble = 0.5, average=\"samples\"\n)\n\nmodel_scores = [ovr_jaccard_score] + chain_jaccard_scores\nmodel_scores.append(ensemble_jaccard_score)\n\nmodel_names = (\n    \"Independent\",\n    \"Chain 1\",\n    \"Chain 2\",\n    \"Chain 3\",\n    \"Chain 4\",\n    \"Chain 5\",\n    \"Chain 6\",\n    \"Chain 7\",\n    \"Chain 8\",\n    \"Chain 9\",\n    \"Chain 10\",\n    \"Ensemble\",\n)\n\nx_pos = (len(model_names))\n\n# Plot the Jaccard similarity scores for the independent model, each of the\n# chains, and the ensemble (note that the vertical axis on this plot does\n# not begin at 0).\n\nfig, ax = (figsize=(7, 4))\nax.grid(True)\nax.set_title(\"Classifier Chain Ensemble Performance Comparison\")\nax.set_xticks(x_pos)\nax.set_xticklabels(model_names, rotation=\"vertical\")\nax.set_ylabel(\"Jaccard Similarity Score\")\nax.set_ylim([min(model_scores) * 0.9, max(model_scores) * 1.1])\ncolors = [\"r\"] + [\"b\"] * len(chain_jaccard_scores) + [\"g\"]\nax.bar(x_pos, model_scores, alpha=0.5, color=colors)\n()\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  8.054 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Nearest Neighbors->Approximate nearest neighbors in TSNE": [
        [
            "This example presents how to chain KNeighborsTransformer and TSNE in a pipeline.\nIt also shows how to wrap the packages nmslib and pynndescent to replace\nKNeighborsTransformer and perform approximate nearest neighbors. These packages\ncan be installed with pip install nmslib pynndescent.",
            "markdown"
        ],
        [
            "Note: In KNeighborsTransformer we use the definition which includes each\ntraining point as its own neighbor in the count of n_neighbors, and for\ncompatibility reasons, one extra neighbor is computed when mode == 'distance'.\nPlease note that we do the same in the proposed nmslib wrapper.",
            "markdown"
        ],
        [
            "# Author: Tom Dupre la Tour\n# License: BSD 3 clause",
            "code"
        ],
        [
            "First we try to import the packages and warn the user in case they are\nmissing.",
            "markdown"
        ],
        [
            "import sys\n\ntry:\n    import nmslib\nexcept ImportError:\n    print(\"The package 'nmslib' is required to run this example.\")\n    ()\n\ntry:\n    from pynndescent import PyNNDescentTransformer\nexcept ImportError:\n    print(\"The package 'pynndescent' is required to run this example.\")\n    ()",
            "code"
        ],
        [
            "We define a wrapper class for implementing the scikit-learn API to the\nnmslib, as well as a loading function.",
            "markdown"
        ],
        [
            "import joblib\nimport numpy as np\nfrom scipy.sparse import \nfrom sklearn.base import , \nfrom sklearn.datasets import \nfrom sklearn.utils import \n\n\nclass NMSlibTransformer(, ):\n    \"\"\"Wrapper for using nmslib as sklearn's KNeighborsTransformer\"\"\"\n\n    def __init__(self, n_neighbors=5, metric=\"euclidean\", method=\"sw-graph\", n_jobs=-1):\n        self.n_neighbors = n_neighbors\n        self.method = method\n        self.metric = metric\n        self.n_jobs = n_jobs\n\n    def fit(self, X):\n        self.n_samples_fit_ = X.shape[0]\n\n        # see more metric in the manual\n        # https://github.com/nmslib/nmslib/tree/master/manual\n        space = {\n            \"euclidean\": \"l2\",\n            \"cosine\": \"cosinesimil\",\n            \"l1\": \"l1\",\n            \"l2\": \"l2\",\n        }[self.metric]\n\n        self.nmslib_ = nmslib.init(method=self.method, space=space)\n        self.nmslib_.addDataPointBatch(X.copy())\n        self.nmslib_.createIndex()\n        return self\n\n    def transform(self, X):\n        n_samples_transform = X.shape[0]\n\n        # For compatibility reasons, as each sample is considered as its own\n        # neighbor, one extra neighbor will be computed.\n        n_neighbors = self.n_neighbors + 1\n\n        if self.n_jobs &lt; 0:\n            # Same handling as done in joblib for negative values of n_jobs:\n            # in particular, `n_jobs == -1` means \"as many threads as CPUs\".\n            num_threads = joblib.cpu_count() + self.n_jobs + 1\n        else:\n            num_threads = self.n_jobs\n\n        results = self.nmslib_.knnQueryBatch(\n            X.copy(), k=n_neighbors, num_threads=num_threads\n        )\n        indices, distances = zip(*results)\n        indices, distances = (indices), (distances)\n\n        indptr = (0, n_samples_transform * n_neighbors + 1, n_neighbors)\n        kneighbors_graph = (\n            (distances.ravel(), indices.ravel(), indptr),\n            shape=(n_samples_transform, self.n_samples_fit_),\n        )\n\n        return kneighbors_graph\n\n\ndef load_mnist(n_samples):\n    \"\"\"Load MNIST, shuffle the data, and return only n_samples.\"\"\"\n    mnist = (\"mnist_784\", as_frame=False, parser=\"pandas\")\n    X, y = (mnist.data, mnist.target, random_state=2)\n    return X[:n_samples] / 255, y[:n_samples]",
            "code"
        ],
        [
            "We benchmark the different exact/approximate nearest neighbors transformers.",
            "markdown"
        ],
        [
            "import time\n\nfrom sklearn.manifold import \nfrom sklearn.neighbors import \nfrom sklearn.pipeline import \n\ndatasets = [\n    (\"MNIST_10000\", load_mnist(n_samples=10_000)),\n    (\"MNIST_20000\", load_mnist(n_samples=20_000)),\n]\n\nn_iter = 500\nperplexity = 30\nmetric = \"euclidean\"\n# TSNE requires a certain number of neighbors which depends on the\n# perplexity parameter.\n# Add one since we include each sample as its own neighbor.\nn_neighbors = int(3.0 * perplexity + 1) + 1\n\ntsne_params = dict(\n    init=\"random\",  # pca not supported for sparse matrices\n    perplexity=perplexity,\n    method=\"barnes_hut\",\n    random_state=42,\n    n_iter=n_iter,\n    learning_rate=\"auto\",\n)\n\ntransformers = [\n    (\n        \"KNeighborsTransformer\",\n        (n_neighbors=n_neighbors, mode=\"distance\", metric=metric),\n    ),\n    (\n        \"NMSlibTransformer\",\n        NMSlibTransformer(n_neighbors=n_neighbors, metric=metric),\n    ),\n    (\n        \"PyNNDescentTransformer\",\n        PyNNDescentTransformer(\n            n_neighbors=n_neighbors, metric=metric, parallel_batch_queries=True\n        ),\n    ),\n]\n\nfor dataset_name, (X, y) in datasets:\n\n    msg = f\"Benchmarking on {dataset_name}:\"\n    print(f\"\\n{msg}\\n\" + str(\"-\" * len(msg)))\n\n    for transformer_name, transformer in transformers:\n        longest = np.max([len(name) for name, model in transformers])\n        start = ()\n        transformer.fit(X)\n        fit_duration = () - start\n        print(f\"{transformer_name:&lt;{longest}} {fit_duration:.3f} sec (fit)\")\n        start = ()\n        Xt = transformer.transform(X)\n        transform_duration = () - start\n        print(f\"{transformer_name:&lt;{longest}} {transform_duration:.3f} sec (transform)\")\n        if transformer_name == \"PyNNDescentTransformer\":\n            start = ()\n            Xt = transformer.transform(X)\n            transform_duration = () - start\n            print(\n                f\"{transformer_name:&lt;{longest}} {transform_duration:.3f} sec\"\n                \" (transform)\"\n            )",
            "code"
        ],
        [
            "Sample output:",
            "markdown"
        ],
        [
            "Benchmarking on MNIST_10000:\n----------------------------\n  0.007 sec (fit)\n  1.139 sec (transform)\nNMSlibTransformer      0.208 sec (fit)\nNMSlibTransformer      0.315 sec (transform)\nPyNNDescentTransformer 4.823 sec (fit)\nPyNNDescentTransformer 4.884 sec (transform)\nPyNNDescentTransformer 0.744 sec (transform)\n\nBenchmarking on MNIST_20000:\n----------------------------\n  0.011 sec (fit)\n  5.769 sec (transform)\nNMSlibTransformer      0.733 sec (fit)\nNMSlibTransformer      1.077 sec (transform)\nPyNNDescentTransformer 14.448 sec (fit)\nPyNNDescentTransformer 7.103 sec (transform)\nPyNNDescentTransformer 1.759 sec (transform)",
            "code"
        ],
        [
            "Notice that the PyNNDescentTransformer takes more time during the first\nfit and the first transform due to the overhead of the numba just in time\ncompiler. But after the first call, the compiled Python code is kept in a\ncache by numba and subsequent calls do not suffer from this initial overhead.\nBoth  and NMSlibTransformer\nare only run once here as they would show more stable fit and transform\ntimes (they don\u2019t have the cold start problem of PyNNDescentTransformer).",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\nfrom matplotlib.ticker import \n\ntransformers = [\n    (\"TSNE with internal NearestNeighbors\", (metric=metric, **tsne_params)),\n    (\n        \"TSNE with KNeighborsTransformer\",\n        (\n            (\n                n_neighbors=n_neighbors, mode=\"distance\", metric=metric\n            ),\n            (metric=\"precomputed\", **tsne_params),\n        ),\n    ),\n    (\n        \"TSNE with NMSlibTransformer\",\n        (\n            NMSlibTransformer(n_neighbors=n_neighbors, metric=metric),\n            (metric=\"precomputed\", **tsne_params),\n        ),\n    ),\n]\n\n# init the plot\nnrows = len(datasets)\nncols = ([1 for name, model in transformers if \"TSNE\" in name])\nfig, axes = (\n    nrows=nrows, ncols=ncols, squeeze=False, figsize=(5 * ncols, 4 * nrows)\n)\naxes = axes.ravel()\ni_ax = 0\n\nfor dataset_name, (X, y) in datasets:\n\n    msg = f\"Benchmarking on {dataset_name}:\"\n    print(f\"\\n{msg}\\n\" + str(\"-\" * len(msg)))\n\n    for transformer_name, transformer in transformers:\n        longest = np.max([len(name) for name, model in transformers])\n        start = ()\n        Xt = transformer.fit_transform(X)\n        transform_duration = () - start\n        print(\n            f\"{transformer_name:&lt;{longest}} {transform_duration:.3f} sec\"\n            \" (fit_transform)\"\n        )\n\n        # plot TSNE embedding which should be very similar across methods\n        axes[i_ax].set_title(transformer_name + \"\\non \" + dataset_name)\n        axes[i_ax].scatter(\n            Xt[:, 0],\n            Xt[:, 1],\n            c=y.astype(),\n            alpha=0.2,\n            cmap=plt.cm.viridis,\n        )\n        axes[i_ax].xaxis.set_major_formatter(())\n        axes[i_ax].yaxis.set_major_formatter(())\n        axes[i_ax].axis(\"tight\")\n        i_ax += 1\n\nfig.tight_layout()\n()",
            "code"
        ],
        [
            "Sample output:",
            "markdown"
        ],
        [
            "Benchmarking on MNIST_10000:\n----------------------------\n with internal NearestNeighbors 24.828 sec (fit_transform)\n with      20.111 sec (fit_transform)\n with NMSlibTransformer         21.757 sec (fit_transform)\n\nBenchmarking on MNIST_20000:\n----------------------------\n with internal NearestNeighbors 51.955 sec (fit_transform)\n with      50.994 sec (fit_transform)\n with NMSlibTransformer         43.536 sec (fit_transform)",
            "code"
        ],
        [
            "We can observe that the default  estimator with\nits internal  implementation is\nroughly equivalent to the pipeline with  and\n in terms of performance.\nThis is expected because both pipelines rely internally on the same\n implementation that performs\nexacts neighbors search. The approximate NMSlibTransformer is already\nslightly faster than the exact search on the smallest dataset but this speed\ndifference is expected to become more significant on datasets with a larger\nnumber of samples.",
            "markdown"
        ],
        [
            "Notice however that not all approximate search methods are guaranteed to\nimprove the speed of the default exact search method: indeed the exact search\nimplementation significantly improved since scikit-learn 1.1. Furthermore, the\nbrute-force exact search method does not require building an index at fit\ntime. So, to get an overall performance improvement in the context of the\n pipeline, the gains of the approximate search\nat transform need to be larger than the extra time spent to build the\napproximate search index at fit time.",
            "markdown"
        ],
        [
            "Finally, the TSNE algorithm itself is also computationally intensive,\nirrespective of the nearest neighbors search. So speeding-up the nearest\nneighbors search step by a factor of 5 would not result in a speed up by a\nfactor of 5 for the overall pipeline.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Nearest Neighbors->Caching nearest neighbors": [
        [
            "This examples demonstrates how to precompute the k nearest neighbors before\nusing them in KNeighborsClassifier. KNeighborsClassifier can compute the\nnearest neighbors internally, but precomputing them can have several benefits,\nsuch as finer parameter control, caching for multiple use, or custom\nimplementations.",
            "markdown"
        ],
        [
            "Here we use the caching property of pipelines to cache the nearest neighbors\ngraph between multiple fits of KNeighborsClassifier. The first call is slow\nsince it computes the neighbors graph, while subsequent call are faster as they\ndo not need to recompute the graph. Here the durations are small since the\ndataset is small, but the gain can be more substantial when the dataset grows\nlarger, or when the grid of parameter to search is large.\n<img alt=\"Classification accuracy, Fit time (with caching)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_caching_nearest_neighbors_001.png\" srcset=\"../../_images/sphx_glr_plot_caching_nearest_neighbors_001.png\"/>",
            "markdown"
        ],
        [
            "# Author: Tom Dupre la Tour\n#\n# License: BSD 3 clause\nfrom tempfile import \nimport matplotlib.pyplot as plt\n\nfrom sklearn.neighbors import , \nfrom sklearn.model_selection import \nfrom sklearn.datasets import \nfrom sklearn.pipeline import \n\nX, y = (return_X_y=True)\nn_neighbors_list = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n# The transformer computes the nearest neighbors graph using the maximum number\n# of neighbors necessary in the grid search. The classifier model filters the\n# nearest neighbors graph as required by its own n_neighbors parameter.\ngraph_model = (n_neighbors=max(n_neighbors_list), mode=\"distance\")\nclassifier_model = (metric=\"precomputed\")\n\n# Note that we give `memory` a directory to cache the graph computation\n# that will be used several times when tuning the hyperparameters of the\n# classifier.\nwith (prefix=\"sklearn_graph_cache_\") as tmpdir:\n    full_model = (\n        steps=[(\"graph\", graph_model), (\"classifier\", classifier_model)], memory=tmpdir\n    )\n\n    param_grid = {\"classifier__n_neighbors\": n_neighbors_list}\n    grid_model = (full_model, param_grid)\n    grid_model.fit(X, y)\n\n# Plot the results of the grid search.\nfig, axes = (1, 2, figsize=(8, 4))\naxes[0].errorbar(\n    x=n_neighbors_list,\n    y=grid_model.cv_results_[\"mean_test_score\"],\n    yerr=grid_model.cv_results_[\"std_test_score\"],\n)\naxes[0].set(xlabel=\"n_neighbors\", title=\"Classification accuracy\")\naxes[1].errorbar(\n    x=n_neighbors_list,\n    y=grid_model.cv_results_[\"mean_fit_time\"],\n    yerr=grid_model.cv_results_[\"std_fit_time\"],\n    color=\"r\",\n)\naxes[1].set(xlabel=\"n_neighbors\", title=\"Fit time (with caching)\")\nfig.tight_layout()\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.965 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Nearest Neighbors->Comparing Nearest Neighbors with and without Neighborhood Components Analysis": [
        [
            "An example comparing nearest neighbors classification with and without\nNeighborhood Components Analysis.",
            "markdown"
        ],
        [
            "It will plot the class decision boundaries given by a Nearest Neighbors\nclassifier when using the Euclidean distance on the original features, versus\nusing the Euclidean distance after the transformation learned by Neighborhood\nComponents Analysis. The latter aims to find a linear transformation that\nmaximises the (stochastic) nearest neighbor classification accuracy on the\ntraining set.\n\n<img alt=\"KNN (k = 1)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_nca_classification_001.png\" srcset=\"../../_images/sphx_glr_plot_nca_classification_001.png\"/>\n<img alt=\"NCA, KNN (k = 1)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_nca_classification_002.png\" srcset=\"../../_images/sphx_glr_plot_nca_classification_002.png\"/>",
            "markdown"
        ],
        [
            "# License: BSD 3 clause\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import \nfrom sklearn import datasets\nfrom sklearn.model_selection import \nfrom sklearn.preprocessing import \nfrom sklearn.neighbors import , \nfrom sklearn.pipeline import \nfrom sklearn.inspection import DecisionBoundaryDisplay\n\n\nn_neighbors = 1\n\ndataset = ()\nX, y = dataset.data, dataset.target\n\n# we only take two features. We could avoid this ugly\n# slicing by using a two-dim dataset\nX = X[:, [0, 2]]\n\nX_train, X_test, y_train, y_test = (\n    X, y, stratify=y, test_size=0.7, random_state=42\n)\n\nh = 0.05  # step size in the mesh\n\n# Create color maps\ncmap_light = ([\"#FFAAAA\", \"#AAFFAA\", \"#AAAAFF\"])\ncmap_bold = ([\"#FF0000\", \"#00FF00\", \"#0000FF\"])\n\nnames = [\"KNN\", \"NCA, KNN\"]\n\nclassifiers = [\n    (\n        [\n            (\"scaler\", ()),\n            (\"knn\", (n_neighbors=n_neighbors)),\n        ]\n    ),\n    (\n        [\n            (\"scaler\", ()),\n            (\"nca\", ()),\n            (\"knn\", (n_neighbors=n_neighbors)),\n        ]\n    ),\n]\n\nfor name, clf in zip(names, classifiers):\n\n    clf.fit(X_train, y_train)\n    score = clf.score(X_test, y_test)\n\n    _, ax = ()\n    (\n        clf,\n        X,\n        cmap=cmap_light,\n        alpha=0.8,\n        ax=ax,\n        response_method=\"predict\",\n        plot_method=\"pcolormesh\",\n        shading=\"auto\",\n    )\n\n    # Plot also the training and testing points\n    (X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor=\"k\", s=20)\n    (\"{} (k = {})\".format(name, n_neighbors))\n    (\n        0.9,\n        0.1,\n        \"{:.2f}\".format(score),\n        size=15,\n        ha=\"center\",\n        va=\"center\",\n        transform=().transAxes,\n    )\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.490 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Nearest Neighbors->Dimensionality Reduction with Neighborhood Components Analysis": [
        [
            "Sample usage of Neighborhood Components Analysis for dimensionality reduction.",
            "markdown"
        ],
        [
            "This example compares different (linear) dimensionality reduction methods\napplied on the Digits data set. The data set contains images of digits from\n0 to 9 with approximately 180 samples of each class. Each image is of\ndimension 8x8 = 64, and is reduced to a two-dimensional data point.",
            "markdown"
        ],
        [
            "Principal Component Analysis (PCA) applied to this data identifies the\ncombination of attributes (principal components, or directions in the\nfeature space) that account for the most variance in the data. Here we\nplot the different samples on the 2 first principal components.",
            "markdown"
        ],
        [
            "Linear Discriminant Analysis (LDA) tries to identify attributes that\naccount for the most variance between classes. In particular,\nLDA, in contrast to PCA, is a supervised method, using known class labels.",
            "markdown"
        ],
        [
            "Neighborhood Components Analysis (NCA) tries to find a feature space such\nthat a stochastic nearest neighbor algorithm will give the best accuracy.\nLike LDA, it is a supervised method.",
            "markdown"
        ],
        [
            "One can see that NCA enforces a clustering of the data that is visually\nmeaningful despite the large reduction in dimension.\n\n<img alt=\"PCA, KNN (k=3) Test accuracy = 0.52\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_nca_dim_reduction_001.png\" srcset=\"../../_images/sphx_glr_plot_nca_dim_reduction_001.png\"/>\n<img alt=\"LDA, KNN (k=3) Test accuracy = 0.66\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_nca_dim_reduction_002.png\" srcset=\"../../_images/sphx_glr_plot_nca_dim_reduction_002.png\"/>\n<img alt=\"NCA, KNN (k=3) Test accuracy = 0.70\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_nca_dim_reduction_003.png\" srcset=\"../../_images/sphx_glr_plot_nca_dim_reduction_003.png\"/>",
            "markdown"
        ],
        [
            "# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.model_selection import \nfrom sklearn.decomposition import \nfrom sklearn.discriminant_analysis import \nfrom sklearn.neighbors import , \nfrom sklearn.pipeline import \nfrom sklearn.preprocessing import \n\nn_neighbors = 3\nrandom_state = 0\n\n# Load Digits dataset\nX, y = (return_X_y=True)\n\n# Split into train/test\nX_train, X_test, y_train, y_test = (\n    X, y, test_size=0.5, stratify=y, random_state=random_state\n)\n\ndim = len(X[0])\nn_classes = len((y))\n\n# Reduce dimension to 2 with PCA\npca = ((), (n_components=2, random_state=random_state))\n\n# Reduce dimension to 2 with LinearDiscriminantAnalysis\nlda = ((), (n_components=2))\n\n# Reduce dimension to 2 with NeighborhoodComponentAnalysis\nnca = (\n    (),\n    (n_components=2, random_state=random_state),\n)\n\n# Use a nearest neighbor classifier to evaluate the methods\nknn = (n_neighbors=n_neighbors)\n\n# Make a list of the methods to be compared\ndim_reduction_methods = [(\"PCA\", pca), (\"LDA\", lda), (\"NCA\", nca)]\n\n# plt.figure()\nfor i, (name, model) in enumerate(dim_reduction_methods):\n    ()\n    # plt.subplot(1, 3, i + 1, aspect=1)\n\n    # Fit the method's model\n    model.fit(X_train, y_train)\n\n    # Fit a nearest neighbor classifier on the embedded training set\n    knn.fit(model.transform(X_train), y_train)\n\n    # Compute the nearest neighbor accuracy on the embedded test set\n    acc_knn = knn.score(model.transform(X_test), y_test)\n\n    # Embed the data set in 2 dimensions using the fitted model\n    X_embedded = model.transform(X)\n\n    # Plot the projected points and show the evaluation score\n    (X_embedded[:, 0], X_embedded[:, 1], c=y, s=30, cmap=\"Set1\")\n    (\n        \"{}, KNN (k={})\\nTest accuracy = {:.2f}\".format(name, n_neighbors, acc_knn)\n    )\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  1.716 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Nearest Neighbors->Kernel Density Estimate of Species Distributions": [
        [
            "This shows an example of a neighbors-based query (in particular a kernel\ndensity estimate) on geospatial data, using a Ball Tree built upon the\nHaversine distance metric \u2013 i.e. distances over points in latitude/longitude.\nThe dataset is provided by Phillips et. al. (2006).\nIf available, the example uses\n\nto plot the coast lines and national boundaries of South America.",
            "markdown"
        ],
        [
            "This example does not perform any learning over the data\n(see  for\nan example of classification based on the attributes in this dataset).  It\nsimply shows the kernel density estimate of observed data points in\ngeospatial coordinates.",
            "markdown"
        ],
        [
            "The two species are:",
            "markdown"
        ],
        [
            ",\nthe Brown-throated Sloth.",
            "markdown"
        ],
        [
            ",\nalso known as the Forest Small Rice Rat, a rodent that lives in Peru,\nColombia, Ecuador, Peru, and Venezuela.\n\n</blockquote>",
            "markdown"
        ]
    ],
    "Examples->Nearest Neighbors->Kernel Density Estimate of Species Distributions->References": [
        [
            "S. J. Phillips, R. P. Anderson, R. E. Schapire - Ecological Modelling,\n190:231-259, 2006.\n\n</blockquote>\n<img alt=\"Bradypus Variegatus, Microryzomys Minutus\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_species_kde_001.png\" srcset=\"../../_images/sphx_glr_plot_species_kde_001.png\"/>",
            "markdown"
        ],
        [
            "- computing KDE in spherical coordinates\n- plot coastlines from coverage\n- computing KDE in spherical coordinates\n- plot coastlines from coverage\n\n\n\n<br/>",
            "code"
        ],
        [
            "# Author: Jake Vanderplas &lt;jakevdp@cs.washington.edu\n#\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import \nfrom sklearn.neighbors import \n\n# if basemap is available, we'll use it.\n# otherwise, we'll improvise later...\ntry:\n    from mpl_toolkits.basemap import Basemap\n\n    basemap = True\nexcept ImportError:\n    basemap = False\n\n\ndef construct_grids(batch):\n    \"\"\"Construct the map grid from the batch object\n\n    Parameters\n    ----------\n    batch : Batch object\n        The object returned by :func:`fetch_species_distributions`\n\n    Returns\n    -------\n    (xgrid, ygrid) : 1-D arrays\n        The grid corresponding to the values in batch.coverages\n    \"\"\"\n    # x,y coordinates for corner cells\n    xmin = batch.x_left_lower_corner + batch.grid_size\n    xmax = xmin + (batch.Nx * batch.grid_size)\n    ymin = batch.y_left_lower_corner + batch.grid_size\n    ymax = ymin + (batch.Ny * batch.grid_size)\n\n    # x coordinates of the grid cells\n    xgrid = (xmin, xmax, batch.grid_size)\n    # y coordinates of the grid cells\n    ygrid = (ymin, ymax, batch.grid_size)\n\n    return (xgrid, ygrid)\n\n\n# Get matrices/arrays of species IDs and locations\ndata = ()\nspecies_names = [\"Bradypus Variegatus\", \"Microryzomys Minutus\"]\n\nXtrain = ([data[\"train\"][\"dd lat\"], data[\"train\"][\"dd long\"]]).T\nytrain = (\n    [d.decode(\"ascii\").startswith(\"micro\") for d in data[\"train\"][\"species\"]],\n    dtype=\"int\",\n)\nXtrain *=  / 180.0  # Convert lat/long to radians\n\n# Set up the data grid for the contour plot\nxgrid, ygrid = construct_grids(data)\nX, Y = (xgrid[::5], ygrid[::5][::-1])\nland_reference = data.coverages[6][::5, ::5]\nland_mask = (land_reference  -9999).ravel()\n\nxy = ([Y.ravel(), X.ravel()]).T\nxy = xy[land_mask]\nxy *=  / 180.0\n\n# Plot map of South America with distributions of each species\nfig = ()\nfig.subplots_adjust(left=0.05, right=0.95, wspace=0.05)\n\nfor i in range(2):\n    (1, 2, i + 1)\n\n    # construct a kernel density estimate of the distribution\n    print(\" - computing KDE in spherical coordinates\")\n    kde = (\n        bandwidth=0.04, metric=\"haversine\", kernel=\"gaussian\", algorithm=\"ball_tree\"\n    )\n    kde.fit(Xtrain[ytrain == i])\n\n    # evaluate only on the land: -9999 indicates ocean\n    Z = (land_mask.shape[0], -9999, dtype=\"int\")\n    Z[land_mask] = (kde.score_samples(xy))\n    Z = Z.reshape(X.shape)\n\n    # plot contours of the density\n    levels = (0, Z.max(), 25)\n    (X, Y, Z, levels=levels, cmap=plt.cm.Reds)\n\n    if basemap:\n        print(\" - plot coastlines using basemap\")\n        m = Basemap(\n            projection=\"cyl\",\n            llcrnrlat=Y.min(),\n            urcrnrlat=Y.max(),\n            llcrnrlon=X.min(),\n            urcrnrlon=X.max(),\n            resolution=\"c\",\n        )\n        m.drawcoastlines()\n        m.drawcountries()\n    else:\n        print(\" - plot coastlines from coverage\")\n        (\n            X, Y, land_reference, levels=[-9998], colors=\"k\", linestyles=\"solid\"\n        )\n        ([])\n        ([])\n\n    (species_names[i])\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  3.507 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Nearest Neighbors->Kernel Density Estimation": [
        [
            "This example shows how kernel density estimation (KDE), a powerful\nnon-parametric density estimation technique, can be used to learn\na generative model for a dataset.  With this generative model in place,\nnew samples can be drawn.  These new samples reflect the underlying model\nof the data.\n<img alt=\"Selection from the input data, \" class=\"sphx-glr-single-img\" density=\"\" digits=\"\" drawn=\"\" from=\"\" kernel=\"\" model=\"\" new=\"\" src=\"../../_images/sphx_glr_plot_digits_kde_sampling_001.png\" srcset=\"../../_images/sphx_glr_plot_digits_kde_sampling_001.png\" the=\"\"/>",
            "markdown"
        ],
        [
            "best bandwidth: 3.79269019073225\n\n\n\n<br/>",
            "code"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import \nfrom sklearn.neighbors import \nfrom sklearn.decomposition import \nfrom sklearn.model_selection import \n\n# load the data\ndigits = ()\n\n# project the 64-dimensional data to a lower dimension\npca = (n_components=15, whiten=False)\ndata = pca.fit_transform(digits.data)\n\n# use grid search cross-validation to optimize the bandwidth\nparams = {\"bandwidth\": (-1, 1, 20)}\ngrid = ((), params)\ngrid.fit(data)\n\nprint(\"best bandwidth: {0}\".format(grid.best_estimator_.bandwidth))\n\n# use the best estimator to compute the kernel density estimate\nkde = grid.best_estimator_\n\n# sample 44 new points from the data\nnew_data = kde.sample(44, random_state=0)\nnew_data = pca.inverse_transform(new_data)\n\n# turn data into a 4x11 grid\nnew_data = new_data.reshape((4, 11, -1))\nreal_data = digits.data[:44].reshape((4, 11, -1))\n\n# plot real digits and resampled digits\nfig, ax = (9, 11, subplot_kw=dict(xticks=[], yticks=[]))\nfor j in range(11):\n    ax[4, j].set_visible(False)\n    for i in range(4):\n        im = ax[i, j].imshow(\n            real_data[i, j].reshape((8, 8)), cmap=plt.cm.binary, interpolation=\"nearest\"\n        )\n        im.set_clim(0, 16)\n        im = ax[i + 5, j].imshow(\n            new_data[i, j].reshape((8, 8)), cmap=plt.cm.binary, interpolation=\"nearest\"\n        )\n        im.set_clim(0, 16)\n\nax[0, 5].set_title(\"Selection from the input data\")\nax[5, 5].set_title('\"New\" digits drawn from the kernel density model')\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  4.495 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Nearest Neighbors->Nearest Centroid Classification": [
        [
            "Sample usage of Nearest Centroid classification.\nIt will plot the decision boundaries for each class.\n\n<img alt=\"3-Class classification (shrink_threshold=None)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_nearest_centroid_001.png\" srcset=\"../../_images/sphx_glr_plot_nearest_centroid_001.png\"/>\n<img alt=\"3-Class classification (shrink_threshold=0.2)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_nearest_centroid_002.png\" srcset=\"../../_images/sphx_glr_plot_nearest_centroid_002.png\"/>",
            "markdown"
        ],
        [
            "None 0.8133333333333334\n0.2 0.82\n\n\n\n<br/>",
            "code"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import \nfrom sklearn import datasets\nfrom sklearn.neighbors import \nfrom sklearn.inspection import DecisionBoundaryDisplay\n\nn_neighbors = 15\n\n# import some data to play with\niris = ()\n# we only take the first two features. We could avoid this ugly\n# slicing by using a two-dim dataset\nX = iris.data[:, :2]\ny = iris.target\n\n# Create color maps\ncmap_light = ([\"orange\", \"cyan\", \"cornflowerblue\"])\ncmap_bold = ([\"darkorange\", \"c\", \"darkblue\"])\n\nfor shrinkage in [None, 0.2]:\n    # we create an instance of Neighbours Classifier and fit the data.\n    clf = (shrink_threshold=shrinkage)\n    clf.fit(X, y)\n    y_pred = clf.predict(X)\n    print(shrinkage, (y == y_pred))\n\n    _, ax = ()\n    (\n        clf, X, cmap=cmap_light, ax=ax, response_method=\"predict\"\n    )\n\n    # Plot also the training points\n    (X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor=\"k\", s=20)\n    (\"3-Class classification (shrink_threshold=%r)\" % shrinkage)\n    (\"tight\")\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.164 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Nearest Neighbors->Nearest Neighbors Classification": [
        [
            "Sample usage of Nearest Neighbors classification.\nIt will plot the decision boundaries for each class.\n\n<img alt=\"3-Class classification (k = 15, weights = 'uniform')\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_classification_001.png\" srcset=\"../../_images/sphx_glr_plot_classification_001.png\"/>\n<img alt=\"3-Class classification (k = 15, weights = 'distance')\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_classification_002.png\" srcset=\"../../_images/sphx_glr_plot_classification_002.png\"/>",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.colors import \nfrom sklearn import neighbors, datasets\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\nn_neighbors = 15\n\n# import some data to play with\niris = ()\n\n# we only take the first two features. We could avoid this ugly\n# slicing by using a two-dim dataset\nX = iris.data[:, :2]\ny = iris.target\n\n# Create color maps\ncmap_light = ([\"orange\", \"cyan\", \"cornflowerblue\"])\ncmap_bold = [\"darkorange\", \"c\", \"darkblue\"]\n\nfor weights in [\"uniform\", \"distance\"]:\n    # we create an instance of Neighbours Classifier and fit the data.\n    clf = (n_neighbors, weights=weights)\n    clf.fit(X, y)\n\n    _, ax = ()\n    (\n        clf,\n        X,\n        cmap=cmap_light,\n        ax=ax,\n        response_method=\"predict\",\n        plot_method=\"pcolormesh\",\n        xlabel=iris.feature_names[0],\n        ylabel=iris.feature_names[1],\n        shading=\"auto\",\n    )\n\n    # Plot also the training points\n    (\n        x=X[:, 0],\n        y=X[:, 1],\n        hue=iris.target_names[y],\n        palette=cmap_bold,\n        alpha=1.0,\n        edgecolor=\"black\",\n    )\n    (\n        \"3-Class classification (k = %i, weights = '%s')\" % (n_neighbors, weights)\n    )\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.505 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Nearest Neighbors->Nearest Neighbors regression": [
        [
            "Demonstrate the resolution of a regression problem\nusing a k-Nearest Neighbor and the interpolation of the\ntarget using both barycenter and constant weights.",
            "markdown"
        ],
        [
            "# Author: Alexandre Gramfort &lt;alexandre.gramfort@inria.fr\n#         Fabian Pedregosa &lt;fabian.pedregosa@inria.fr\n#\n# License: BSD 3 clause (C) INRIA",
            "code"
        ]
    ],
    "Examples->Nearest Neighbors->Nearest Neighbors regression->Generate sample data": [
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import neighbors\n\n(0)\nX = (5 * (40, 1), axis=0)\nT = (0, 5, 500)[:, ]\ny = (X).ravel()\n\n# Add noise to targets\ny[::5] += 1 * (0.5 - (8))",
            "code"
        ]
    ],
    "Examples->Nearest Neighbors->Nearest Neighbors regression->Fit regression model": [
        [
            "n_neighbors = 5\n\nfor i, weights in enumerate([\"uniform\", \"distance\"]):\n    knn = (n_neighbors, weights=weights)\n    y_ = knn.fit(X, y).predict(T)\n\n    (2, 1, i + 1)\n    (X, y, color=\"darkorange\", label=\"data\")\n    (T, y_, color=\"navy\", label=\"prediction\")\n    (\"tight\")\n    ()\n    (\"KNeighborsRegressor (k = %i, weights = '%s')\" % (n_neighbors, weights))\n\n()\n()\n\n\n<img alt=\"KNeighborsRegressor (k = 5, weights = 'uniform'), KNeighborsRegressor (k = 5, weights = 'distance')\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_regression_001.png\" srcset=\"../../_images/sphx_glr_plot_regression_001.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.161 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Nearest Neighbors->Neighborhood Components Analysis Illustration": [
        [
            "This example illustrates a learned distance metric that maximizes\nthe nearest neighbors classification accuracy. It provides a visual\nrepresentation of this metric compared to the original point\nspace. Please refer to the  for more information.",
            "markdown"
        ],
        [
            "# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import \nfrom sklearn.neighbors import \nfrom matplotlib import cm\nfrom scipy.special import",
            "code"
        ]
    ],
    "Examples->Nearest Neighbors->Neighborhood Components Analysis Illustration->Original points": [
        [
            "First we create a data set of 9 samples from 3 classes, and plot the points\nin the original space. For this example, we focus on the classification of\npoint no. 3. The thickness of a link between point no. 3 and another point\nis proportional to their distance.",
            "markdown"
        ],
        [
            "X, y = (\n    n_samples=9,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_classes=3,\n    n_clusters_per_class=1,\n    class_sep=1.0,\n    random_state=0,\n)\n\n(1)\nax = ()\nfor i in range(X.shape[0]):\n    ax.text(X[i, 0], X[i, 1], str(i), va=\"center\", ha=\"center\")\n    ax.scatter(X[i, 0], X[i, 1], s=300, c=cm.Set1(y[[i]]), alpha=0.4)\n\nax.set_title(\"Original points\")\nax.axes.get_xaxis().set_visible(False)\nax.axes.get_yaxis().set_visible(False)\nax.axis(\"equal\")  # so that boundaries are displayed correctly as circles\n\n\ndef link_thickness_i(X, i):\n    diff_embedded = X[i] - X\n    dist_embedded = (\"ij,ij-i\", diff_embedded, diff_embedded)\n    dist_embedded[i] = \n\n    # compute exponentiated distances (use the log-sum-exp trick to\n    # avoid numerical instabilities\n    exp_dist_embedded = (-dist_embedded - (-dist_embedded))\n    return exp_dist_embedded\n\n\ndef relate_point(X, i, ax):\n    pt_i = X[i]\n    for j, pt_j in enumerate(X):\n        thickness = link_thickness_i(X, i)\n        if i != j:\n            line = ([pt_i[0], pt_j[0]], [pt_i[1], pt_j[1]])\n            ax.plot(*line, c=cm.Set1(y[j]), linewidth=5 * thickness[j])\n\n\ni = 3\nrelate_point(X, i, ax)\n()\n\n\n<img alt=\"Original points\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_nca_illustration_001.png\" srcset=\"../../_images/sphx_glr_plot_nca_illustration_001.png\"/>",
            "code"
        ]
    ],
    "Examples->Nearest Neighbors->Neighborhood Components Analysis Illustration->Learning an embedding": [
        [
            "We use  to learn an\nembedding and plot the points after the transformation. We then take the\nembedding and find the nearest neighbors.",
            "markdown"
        ],
        [
            "nca = (max_iter=30, random_state=0)\nnca = nca.fit(X, y)\n\n(2)\nax2 = ()\nX_embedded = nca.transform(X)\nrelate_point(X_embedded, i, ax2)\n\nfor i in range(len(X)):\n    ax2.text(X_embedded[i, 0], X_embedded[i, 1], str(i), va=\"center\", ha=\"center\")\n    ax2.scatter(X_embedded[i, 0], X_embedded[i, 1], s=300, c=cm.Set1(y[[i]]), alpha=0.4)\n\nax2.set_title(\"NCA embedding\")\nax2.axes.get_xaxis().set_visible(False)\nax2.axes.get_yaxis().set_visible(False)\nax2.axis(\"equal\")\n()\n\n\n<img alt=\"NCA embedding\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_nca_illustration_002.png\" srcset=\"../../_images/sphx_glr_plot_nca_illustration_002.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.160 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Nearest Neighbors->Novelty detection with Local Outlier Factor (LOF)": [
        [
            "The Local Outlier Factor (LOF) algorithm is an unsupervised anomaly detection\nmethod which computes the local density deviation of a given data point with\nrespect to its neighbors. It considers as outliers the samples that have a\nsubstantially lower density than their neighbors. This example shows how to\nuse LOF for novelty detection. Note that when LOF is used for novelty\ndetection you MUST not use predict, decision_function and score_samples on the\ntraining set as this would lead to wrong results. You must only use these\nmethods on new unseen data (which are not in the training set). See\n: for details on the difference between\noutlier detection and novelty detection and how to use LOF for outlier\ndetection.",
            "markdown"
        ],
        [
            "The number of neighbors considered, (parameter n_neighbors) is typically\nset 1) greater than the minimum number of samples a cluster has to contain,\nso that other samples can be local outliers relative to this cluster, and 2)\nsmaller than the maximum number of close by samples that can potentially be\nlocal outliers.\nIn practice, such information is generally not available, and taking\nn_neighbors=20 appears to work well in general.\n<img alt=\"Novelty Detection with LOF\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_lof_novelty_detection_001.png\" srcset=\"../../_images/sphx_glr_plot_lof_novelty_detection_001.png\"/>",
            "markdown"
        ],
        [
            "import numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import \n\n(42)\n\nxx, yy = ((-5, 5, 500), (-5, 5, 500))\n# Generate normal (not abnormal) training observations\nX = 0.3 * (100, 2)\nX_train = [X + 2, X - 2]\n# Generate new normal (not abnormal) observations\nX = 0.3 * (20, 2)\nX_test = [X + 2, X - 2]\n# Generate some abnormal novel observations\nX_outliers = (low=-4, high=4, size=(20, 2))\n\n# fit the model for novelty detection (novelty=True)\nclf = (n_neighbors=20, novelty=True, contamination=0.1)\nclf.fit(X_train)\n# DO NOT use predict, decision_function and score_samples on X_train as this\n# would give wrong results but only on new unseen data (not used in X_train),\n# e.g. X_test, X_outliers or the meshgrid\ny_pred_test = clf.predict(X_test)\ny_pred_outliers = clf.predict(X_outliers)\nn_error_test = y_pred_test[y_pred_test == -1].size\nn_error_outliers = y_pred_outliers[y_pred_outliers == 1].size\n\n# plot the learned frontier, the points, and the nearest vectors to the plane\nZ = clf.decision_function([xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\n(\"Novelty Detection with LOF\")\n(xx, yy, Z, levels=(Z.min(), 0, 7), cmap=plt.cm.PuBu)\na = (xx, yy, Z, levels=[0], linewidths=2, colors=\"darkred\")\n(xx, yy, Z, levels=[0, Z.max()], colors=\"palevioletred\")\n\ns = 40\nb1 = (X_train[:, 0], X_train[:, 1], c=\"white\", s=s, edgecolors=\"k\")\nb2 = (X_test[:, 0], X_test[:, 1], c=\"blueviolet\", s=s, edgecolors=\"k\")\nc = (X_outliers[:, 0], X_outliers[:, 1], c=\"gold\", s=s, edgecolors=\"k\")\n(\"tight\")\n((-5, 5))\n((-5, 5))\n(\n    [a.collections[0], b1, b2, c],\n    [\n        \"learned frontier\",\n        \"training observations\",\n        \"new regular observations\",\n        \"new abnormal observations\",\n    ],\n    loc=\"upper left\",\n    prop=(size=11),\n)\n(\n    \"errors novel regular: %d/40 ; errors novel abnormal: %d/40\"\n    % (n_error_test, n_error_outliers)\n)\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.704 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Nearest Neighbors->Outlier detection with Local Outlier Factor (LOF)": [
        [
            "The Local Outlier Factor (LOF) algorithm is an unsupervised anomaly detection\nmethod which computes the local density deviation of a given data point with\nrespect to its neighbors. It considers as outliers the samples that have a\nsubstantially lower density than their neighbors. This example shows how to\nuse LOF for outlier detection which is the default use case of this estimator\nin scikit-learn. Note that when LOF is used for outlier detection it has no\npredict, decision_function and score_samples methods. See\n: for details on the difference between\noutlier detection and novelty detection and how to use LOF for novelty\ndetection.",
            "markdown"
        ],
        [
            "The number of neighbors considered (parameter n_neighbors) is typically\nset 1) greater than the minimum number of samples a cluster has to contain,\nso that other samples can be local outliers relative to this cluster, and 2)\nsmaller than the maximum number of close by samples that can potentially be\nlocal outliers.\nIn practice, such information is generally not available, and taking\nn_neighbors=20 appears to work well in general.\n<img alt=\"Local Outlier Factor (LOF)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_lof_outlier_detection_001.png\" srcset=\"../../_images/sphx_glr_plot_lof_outlier_detection_001.png\"/>",
            "markdown"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import \n\n(42)\n\n# Generate train data\nX_inliers = 0.3 * (100, 2)\nX_inliers = [X_inliers + 2, X_inliers - 2]\n\n# Generate some outliers\nX_outliers = (low=-4, high=4, size=(20, 2))\nX = [X_inliers, X_outliers]\n\nn_outliers = len(X_outliers)\nground_truth = (len(X), dtype=int)\nground_truth[-n_outliers:] = -1\n\n# fit the model for outlier detection (default)\nclf = (n_neighbors=20, contamination=0.1)\n# use fit_predict to compute the predicted labels of the training samples\n# (when LOF is used for outlier detection, the estimator has no predict,\n# decision_function and score_samples methods).\ny_pred = clf.fit_predict(X)\nn_errors = (y_pred != ground_truth).sum()\nX_scores = clf.negative_outlier_factor_\n\n(\"Local Outlier Factor (LOF)\")\n(X[:, 0], X[:, 1], color=\"k\", s=3.0, label=\"Data points\")\n# plot circles with radius proportional to the outlier scores\nradius = (X_scores.max() - X_scores) / (X_scores.max() - X_scores.min())\n(\n    X[:, 0],\n    X[:, 1],\n    s=1000 * radius,\n    edgecolors=\"r\",\n    facecolors=\"none\",\n    label=\"Outlier scores\",\n)\n(\"tight\")\n((-5, 5))\n((-5, 5))\n(\"prediction errors: %d\" % (n_errors))\nlegend = (loc=\"upper left\")\nlegend.legendHandles[0]._sizes = [10]\nlegend.legendHandles[1]._sizes = [20]\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.085 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Nearest Neighbors->Simple 1D Kernel Density Estimation": [
        [
            "This example uses the  class to\ndemonstrate the principles of Kernel Density Estimation in one dimension.",
            "markdown"
        ],
        [
            "The first plot shows one of the problems with using histograms to visualize\nthe density of points in 1D. Intuitively, a histogram can be thought of as a\nscheme in which a unit \u201cblock\u201d is stacked above each point on a regular grid.\nAs the top two panels show, however, the choice of gridding for these blocks\ncan lead to wildly divergent ideas about the underlying shape of the density\ndistribution.  If we instead center each block on the point it represents, we\nget the estimate shown in the bottom left panel.  This is a kernel density\nestimation with a \u201ctop hat\u201d kernel.  This idea can be generalized to other\nkernel shapes: the bottom-right panel of the first figure shows a Gaussian\nkernel density estimate over the same distribution.",
            "markdown"
        ],
        [
            "Scikit-learn implements efficient kernel density estimation using either\na Ball Tree or KD Tree structure, through the\n estimator.  The available kernels\nare shown in the second figure of this example.",
            "markdown"
        ],
        [
            "The third figure compares kernel density estimates for a distribution of 100\nsamples in 1 dimension.  Though this example uses 1D distributions, kernel\ndensity estimation is easily and efficiently extensible to higher dimensions\nas well.\n\n<img alt=\"plot kde 1d\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_kde_1d_001.png\" srcset=\"../../_images/sphx_glr_plot_kde_1d_001.png\"/>\n<img alt=\"Available Kernels\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_kde_1d_002.png\" srcset=\"../../_images/sphx_glr_plot_kde_1d_002.png\"/>\n<img alt=\"plot kde 1d\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_kde_1d_003.png\" srcset=\"../../_images/sphx_glr_plot_kde_1d_003.png\"/>",
            "markdown"
        ],
        [
            "# Author: Jake Vanderplas &lt;jakevdp@cs.washington.edu\n#\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import \nfrom sklearn.neighbors import \n\n# ----------------------------------------------------------------------\n# Plot the progression of histograms to kernels\n(1)\nN = 20\nX = (\n    ((0, 1, int(0.3 * N)), (5, 1, int(0.7 * N)))\n)[:, ]\nX_plot = (-5, 10, 1000)[:, ]\nbins = (-5, 10, 10)\n\nfig, ax = (2, 2, sharex=True, sharey=True)\nfig.subplots_adjust(hspace=0.05, wspace=0.05)\n\n# histogram 1\nax[0, 0].hist(X[:, 0], bins=bins, fc=\"#AAAAFF\", density=True)\nax[0, 0].text(-3.5, 0.31, \"Histogram\")\n\n# histogram 2\nax[0, 1].hist(X[:, 0], bins=bins + 0.75, fc=\"#AAAAFF\", density=True)\nax[0, 1].text(-3.5, 0.31, \"Histogram, bins shifted\")\n\n# tophat KDE\nkde = (kernel=\"tophat\", bandwidth=0.75).fit(X)\nlog_dens = kde.score_samples(X_plot)\nax[1, 0].fill(X_plot[:, 0], (log_dens), fc=\"#AAAAFF\")\nax[1, 0].text(-3.5, 0.31, \"Tophat Kernel Density\")\n\n# Gaussian KDE\nkde = (kernel=\"gaussian\", bandwidth=0.75).fit(X)\nlog_dens = kde.score_samples(X_plot)\nax[1, 1].fill(X_plot[:, 0], (log_dens), fc=\"#AAAAFF\")\nax[1, 1].text(-3.5, 0.31, \"Gaussian Kernel Density\")\n\nfor axi in ax.ravel():\n    axi.plot(X[:, 0], (X.shape[0], -0.01), \"+k\")\n    axi.set_xlim(-4, 9)\n    axi.set_ylim(-0.02, 0.34)\n\nfor axi in ax[:, 0]:\n    axi.set_ylabel(\"Normalized Density\")\n\nfor axi in ax[1, :]:\n    axi.set_xlabel(\"x\")\n\n# ----------------------------------------------------------------------\n# Plot all available kernels\nX_plot = (-6, 6, 1000)[:, None]\nX_src = ((1, 1))\n\nfig, ax = (2, 3, sharex=True, sharey=True)\nfig.subplots_adjust(left=0.05, right=0.95, hspace=0.05, wspace=0.05)\n\n\ndef format_func(x, loc):\n    if x == 0:\n        return \"0\"\n    elif x == 1:\n        return \"h\"\n    elif x == -1:\n        return \"-h\"\n    else:\n        return \"%ih\" % x\n\n\nfor i, kernel in enumerate(\n    [\"gaussian\", \"tophat\", \"epanechnikov\", \"exponential\", \"linear\", \"cosine\"]\n):\n    axi = ax.ravel()[i]\n    log_dens = (kernel=kernel).fit(X_src).score_samples(X_plot)\n    axi.fill(X_plot[:, 0], (log_dens), \"-k\", fc=\"#AAAAFF\")\n    axi.text(-2.6, 0.95, kernel)\n\n    axi.xaxis.set_major_formatter(plt.FuncFormatter(format_func))\n    axi.xaxis.set_major_locator(plt.MultipleLocator(1))\n    axi.yaxis.set_major_locator(plt.NullLocator())\n\n    axi.set_ylim(0, 1.05)\n    axi.set_xlim(-2.9, 2.9)\n\nax[0, 1].set_title(\"Available Kernels\")\n\n# ----------------------------------------------------------------------\n# Plot a 1D density example\nN = 100\n(1)\nX = (\n    ((0, 1, int(0.3 * N)), (5, 1, int(0.7 * N)))\n)[:, ]\n\nX_plot = (-5, 10, 1000)[:, ]\n\ntrue_dens = 0.3 * (0, 1).pdf(X_plot[:, 0]) + 0.7 * (5, 1).pdf(X_plot[:, 0])\n\nfig, ax = ()\nax.fill(X_plot[:, 0], true_dens, fc=\"black\", alpha=0.2, label=\"input distribution\")\ncolors = [\"navy\", \"cornflowerblue\", \"darkorange\"]\nkernels = [\"gaussian\", \"tophat\", \"epanechnikov\"]\nlw = 2\n\nfor color, kernel in zip(colors, kernels):\n    kde = (kernel=kernel, bandwidth=0.5).fit(X)\n    log_dens = kde.score_samples(X_plot)\n    ax.plot(\n        X_plot[:, 0],\n        (log_dens),\n        color=color,\n        lw=lw,\n        linestyle=\"-\",\n        label=\"kernel = '{0}'\".format(kernel),\n    )\n\nax.text(6, 0.38, \"N={0} points\".format(N))\n\nax.legend(loc=\"upper left\")\nax.plot(X[:, 0], -0.005 - 0.01 * (X.shape[0]), \"+k\")\n\nax.set_xlim(-4, 9)\nax.set_ylim(-0.02, 0.4)\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.608 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Neural Networks->Compare Stochastic learning strategies for MLPClassifier": [
        [
            "This example visualizes some training loss curves for different stochastic\nlearning strategies, including SGD and Adam. Because of time-constraints, we\nuse several small datasets, for which L-BFGS might be more suitable. The\ngeneral trend shown in these examples seems to carry over to larger datasets,\nhowever.",
            "markdown"
        ],
        [
            "Note that those results can be highly dependent on the value of\nlearning_rate_init.\n<img alt=\"iris, digits, circles, moons\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_mlp_training_curves_001.png\" srcset=\"../../_images/sphx_glr_plot_mlp_training_curves_001.png\"/>",
            "markdown"
        ],
        [
            "learning on dataset iris\ntraining: constant learning-rate\nTraining set score: 0.980000\nTraining set loss: 0.096950\ntraining: constant with momentum\nTraining set score: 0.980000\nTraining set loss: 0.049530\ntraining: constant with Nesterov's momentum\nTraining set score: 0.980000\nTraining set loss: 0.049540\ntraining: inv-scaling learning-rate\nTraining set score: 0.360000\nTraining set loss: 0.978444\ntraining: inv-scaling with momentum\nTraining set score: 0.860000\nTraining set loss: 0.503452\ntraining: inv-scaling with Nesterov's momentum\nTraining set score: 0.860000\nTraining set loss: 0.504185\ntraining: adam\nTraining set score: 0.980000\nTraining set loss: 0.045311\n\nlearning on dataset digits\ntraining: constant learning-rate\nTraining set score: 0.956038\nTraining set loss: 0.243802\ntraining: constant with momentum\nTraining set score: 0.992766\nTraining set loss: 0.041297\ntraining: constant with Nesterov's momentum\nTraining set score: 0.993879\nTraining set loss: 0.042898\ntraining: inv-scaling learning-rate\nTraining set score: 0.638843\nTraining set loss: 1.855465\ntraining: inv-scaling with momentum\nTraining set score: 0.912632\nTraining set loss: 0.290584\ntraining: inv-scaling with Nesterov's momentum\nTraining set score: 0.909293\nTraining set loss: 0.318387\ntraining: adam\nTraining set score: 0.991653\nTraining set loss: 0.045934\n\nlearning on dataset circles\ntraining: constant learning-rate\nTraining set score: 0.840000\nTraining set loss: 0.601052\ntraining: constant with momentum\nTraining set score: 0.940000\nTraining set loss: 0.157334\ntraining: constant with Nesterov's momentum\nTraining set score: 0.940000\nTraining set loss: 0.154453\ntraining: inv-scaling learning-rate\nTraining set score: 0.500000\nTraining set loss: 0.692470\ntraining: inv-scaling with momentum\nTraining set score: 0.500000\nTraining set loss: 0.689143\ntraining: inv-scaling with Nesterov's momentum\nTraining set score: 0.500000\nTraining set loss: 0.689751\ntraining: adam\nTraining set score: 0.940000\nTraining set loss: 0.150527\n\nlearning on dataset moons\ntraining: constant learning-rate\nTraining set score: 0.850000\nTraining set loss: 0.341523\ntraining: constant with momentum\nTraining set score: 0.850000\nTraining set loss: 0.336188\ntraining: constant with Nesterov's momentum\nTraining set score: 0.850000\nTraining set loss: 0.335919\ntraining: inv-scaling learning-rate\nTraining set score: 0.500000\nTraining set loss: 0.689015\ntraining: inv-scaling with momentum\nTraining set score: 0.830000\nTraining set loss: 0.512595\ntraining: inv-scaling with Nesterov's momentum\nTraining set score: 0.830000\nTraining set loss: 0.513034\ntraining: adam\nTraining set score: 0.930000\nTraining set loss: 0.170087\n\n\n\n<br/>",
            "code"
        ],
        [
            "import warnings\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.neural_network import \nfrom sklearn.preprocessing import \nfrom sklearn import datasets\nfrom sklearn.exceptions import \n\n# different learning rate schedules and momentum parameters\nparams = [\n    {\n        \"solver\": \"sgd\",\n        \"learning_rate\": \"constant\",\n        \"momentum\": 0,\n        \"learning_rate_init\": 0.2,\n    },\n    {\n        \"solver\": \"sgd\",\n        \"learning_rate\": \"constant\",\n        \"momentum\": 0.9,\n        \"nesterovs_momentum\": False,\n        \"learning_rate_init\": 0.2,\n    },\n    {\n        \"solver\": \"sgd\",\n        \"learning_rate\": \"constant\",\n        \"momentum\": 0.9,\n        \"nesterovs_momentum\": True,\n        \"learning_rate_init\": 0.2,\n    },\n    {\n        \"solver\": \"sgd\",\n        \"learning_rate\": \"invscaling\",\n        \"momentum\": 0,\n        \"learning_rate_init\": 0.2,\n    },\n    {\n        \"solver\": \"sgd\",\n        \"learning_rate\": \"invscaling\",\n        \"momentum\": 0.9,\n        \"nesterovs_momentum\": True,\n        \"learning_rate_init\": 0.2,\n    },\n    {\n        \"solver\": \"sgd\",\n        \"learning_rate\": \"invscaling\",\n        \"momentum\": 0.9,\n        \"nesterovs_momentum\": False,\n        \"learning_rate_init\": 0.2,\n    },\n    {\"solver\": \"adam\", \"learning_rate_init\": 0.01},\n]\n\nlabels = [\n    \"constant learning-rate\",\n    \"constant with momentum\",\n    \"constant with Nesterov's momentum\",\n    \"inv-scaling learning-rate\",\n    \"inv-scaling with momentum\",\n    \"inv-scaling with Nesterov's momentum\",\n    \"adam\",\n]\n\nplot_args = [\n    {\"c\": \"red\", \"linestyle\": \"-\"},\n    {\"c\": \"green\", \"linestyle\": \"-\"},\n    {\"c\": \"blue\", \"linestyle\": \"-\"},\n    {\"c\": \"red\", \"linestyle\": \"--\"},\n    {\"c\": \"green\", \"linestyle\": \"--\"},\n    {\"c\": \"blue\", \"linestyle\": \"--\"},\n    {\"c\": \"black\", \"linestyle\": \"-\"},\n]\n\n\ndef plot_on_dataset(X, y, ax, name):\n    # for each dataset, plot learning for each learning strategy\n    print(\"\\nlearning on dataset %s\" % name)\n    ax.set_title(name)\n\n    X = ().fit_transform(X)\n    mlps = []\n    if name == \"digits\":\n        # digits is larger but converges fairly quickly\n        max_iter = 15\n    else:\n        max_iter = 400\n\n    for label, param in zip(labels, params):\n        print(\"training: %s\" % label)\n        mlp = (random_state=0, max_iter=max_iter, **param)\n\n        # some parameter combinations will not converge as can be seen on the\n        # plots so they are ignored here\n        with ():\n            (\n                \"ignore\", category=, module=\"sklearn\"\n            )\n            mlp.fit(X, y)\n\n        mlps.append(mlp)\n        print(\"Training set score: %f\" % mlp.score(X, y))\n        print(\"Training set loss: %f\" % mlp.loss_)\n    for mlp, label, args in zip(mlps, labels, plot_args):\n        ax.plot(mlp.loss_curve_, label=label, **args)\n\n\nfig, axes = (2, 2, figsize=(15, 10))\n# load / generate some toy datasets\niris = ()\nX_digits, y_digits = (return_X_y=True)\ndata_sets = [\n    (iris.data, iris.target),\n    (X_digits, y_digits),\n    (noise=0.2, factor=0.5, random_state=1),\n    (noise=0.3, random_state=0),\n]\n\nfor ax, data, name in zip(\n    axes.ravel(), data_sets, [\"iris\", \"digits\", \"circles\", \"moons\"]\n):\n    plot_on_dataset(*data, ax=ax, name=name)\n\nfig.legend(ax.get_lines(), labels, ncol=3, loc=\"upper center\")\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  2.961 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Neural Networks->Restricted Boltzmann Machine features for digit classification": [
        [
            "For greyscale image data where pixel values can be interpreted as degrees of\nblackness on a white background, like handwritten digit recognition, the\nBernoulli Restricted Boltzmann machine model () can perform effective non-linear\nfeature extraction.",
            "markdown"
        ],
        [
            "# Authors: Yann N. Dauphin, Vlad Niculae, Gabriel Synnaeve\n# License: BSD",
            "code"
        ]
    ],
    "Examples->Neural Networks->Restricted Boltzmann Machine features for digit classification->Generate data": [
        [
            "In order to learn good latent representations from a small dataset, we\nartificially generate more labeled data by perturbing the training data with\nlinear shifts of 1 pixel in each direction.",
            "markdown"
        ],
        [
            "import numpy as np\n\nfrom scipy.ndimage import \n\nfrom sklearn import datasets\nfrom sklearn.preprocessing import \n\nfrom sklearn.model_selection import \n\n\ndef nudge_dataset(X, Y):\n    \"\"\"\n    This produces a dataset 5 times bigger than the original one,\n    by moving the 8x8 images in X around by 1px to left, right, down, up\n    \"\"\"\n    direction_vectors = [\n        [[0, 1, 0], [0, 0, 0], [0, 0, 0]],\n        [[0, 0, 0], [1, 0, 0], [0, 0, 0]],\n        [[0, 0, 0], [0, 0, 1], [0, 0, 0]],\n        [[0, 0, 0], [0, 0, 0], [0, 1, 0]],\n    ]\n\n    def shift(x, w):\n        return (x.reshape((8, 8)), mode=\"constant\", weights=w).ravel()\n\n    X = (\n        [X] + [(shift, 1, X, vector) for vector in direction_vectors]\n    )\n    Y = ([Y for _ in range(5)], axis=0)\n    return X, Y\n\n\nX, y = (return_X_y=True)\nX = (X, \"float32\")\nX, Y = nudge_dataset(X, y)\nX = (X, feature_range=(0, 1))  # 0-1 scaling\n\nX_train, X_test, Y_train, Y_test = (X, Y, test_size=0.2, random_state=0)",
            "code"
        ]
    ],
    "Examples->Neural Networks->Restricted Boltzmann Machine features for digit classification->Models definition": [
        [
            "We build a classification pipeline with a BernoulliRBM feature extractor and\na \nclassifier.",
            "markdown"
        ],
        [
            "from sklearn import linear_model\nfrom sklearn.neural_network import \nfrom sklearn.pipeline import \n\nlogistic = (solver=\"newton-cg\", tol=1)\nrbm = (random_state=0, verbose=True)\n\nrbm_features_classifier = (steps=[(\"rbm\", rbm), (\"logistic\", logistic)])",
            "code"
        ]
    ],
    "Examples->Neural Networks->Restricted Boltzmann Machine features for digit classification->Training": [
        [
            "The hyperparameters of the entire model (learning rate, hidden layer size,\nregularization) were optimized by grid search, but the search is not\nreproduced here because of runtime constraints.",
            "markdown"
        ],
        [
            "from sklearn.base import clone\n\n# Hyper-parameters. These were set by cross-validation,\n# using a GridSearchCV. Here we are not performing cross-validation to\n# save time.\nrbm.learning_rate = 0.06\nrbm.n_iter = 10\n\n# More components tend to give better prediction performance, but larger\n# fitting time\nrbm.n_components = 100\nlogistic.C = 6000\n\n# Training RBM-Logistic Pipeline\nrbm_features_classifier.fit(X_train, Y_train)\n\n# Training the Logistic regression classifier directly on the pixel\nraw_pixel_classifier = clone(logistic)\nraw_pixel_classifier.C = 100.0\nraw_pixel_classifier.fit(X_train, Y_train)",
            "code"
        ],
        [
            "[BernoulliRBM] Iteration 1, pseudo-likelihood = -25.57, time = 0.09s\n[BernoulliRBM] Iteration 2, pseudo-likelihood = -23.68, time = 0.14s\n[BernoulliRBM] Iteration 3, pseudo-likelihood = -22.88, time = 0.14s\n[BernoulliRBM] Iteration 4, pseudo-likelihood = -21.91, time = 0.13s\n[BernoulliRBM] Iteration 5, pseudo-likelihood = -21.79, time = 0.13s\n[BernoulliRBM] Iteration 6, pseudo-likelihood = -20.96, time = 0.12s\n[BernoulliRBM] Iteration 7, pseudo-likelihood = -20.88, time = 0.12s\n[BernoulliRBM] Iteration 8, pseudo-likelihood = -20.50, time = 0.12s\n[BernoulliRBM] Iteration 9, pseudo-likelihood = -20.34, time = 0.12s\n[BernoulliRBM] Iteration 10, pseudo-likelihood = -20.21, time = 0.12s",
            "code"
        ],
        [
            "LogisticRegression(C=100.0, solver='newton-cg', tol=1)<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input checked=\"\" class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-235\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-235\">LogisticRegression</label>",
            "code"
        ],
        [
            "LogisticRegression(C=100.0, solver='newton-cg', tol=1)\n\n<br/>\n<br/>",
            "code"
        ]
    ],
    "Examples->Neural Networks->Restricted Boltzmann Machine features for digit classification->Evaluation": [
        [
            "from sklearn import metrics\n\nY_pred = rbm_features_classifier.predict(X_test)\nprint(\n    \"Logistic regression using RBM features:\\n%s\\n\"\n    % ((Y_test, Y_pred))\n)",
            "code"
        ],
        [
            "Logistic regression using RBM features:\n              precision    recall  f1-score   support\n\n           0       1.00      0.98      0.99       174\n           1       0.90      0.92      0.91       184\n           2       0.93      0.95      0.94       166\n           3       0.94      0.90      0.92       194\n           4       0.96      0.94      0.95       186\n           5       0.95      0.92      0.93       181\n           6       0.98      0.97      0.97       207\n           7       0.94      0.99      0.97       154\n           8       0.90      0.90      0.90       182\n           9       0.89      0.92      0.90       169\n\n    accuracy                           0.94      1797\n   macro avg       0.94      0.94      0.94      1797\nweighted avg       0.94      0.94      0.94      1797",
            "code"
        ],
        [
            "Y_pred = raw_pixel_classifier.predict(X_test)\nprint(\n    \"Logistic regression using raw pixel features:\\n%s\\n\"\n    % ((Y_test, Y_pred))\n)",
            "code"
        ],
        [
            "Logistic regression using raw pixel features:\n              precision    recall  f1-score   support\n\n           0       0.90      0.92      0.91       174\n           1       0.60      0.58      0.59       184\n           2       0.76      0.85      0.80       166\n           3       0.78      0.79      0.79       194\n           4       0.82      0.84      0.83       186\n           5       0.76      0.76      0.76       181\n           6       0.90      0.87      0.89       207\n           7       0.85      0.88      0.87       154\n           8       0.67      0.58      0.62       182\n           9       0.76      0.76      0.76       169\n\n    accuracy                           0.78      1797\n   macro avg       0.78      0.78      0.78      1797\nweighted avg       0.78      0.78      0.78      1797",
            "code"
        ],
        [
            "The features extracted by the BernoulliRBM help improve the classification\naccuracy with respect to the logistic regression on raw pixels.",
            "markdown"
        ]
    ],
    "Examples->Neural Networks->Restricted Boltzmann Machine features for digit classification->Plotting": [
        [
            "import matplotlib.pyplot as plt\n\n(figsize=(4.2, 4))\nfor i, comp in enumerate(rbm.components_):\n    (10, 10, i + 1)\n    (comp.reshape((8, 8)), cmap=plt.cm.gray_r, interpolation=\"nearest\")\n    (())\n    (())\n(\"100 components extracted by RBM\", fontsize=16)\n(0.08, 0.02, 0.92, 0.85, 0.08, 0.23)\n\n()\n\n\n<img alt=\"100 components extracted by RBM\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_rbm_logistic_classification_001.png\" srcset=\"../../_images/sphx_glr_plot_rbm_logistic_classification_001.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  4.563 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Neural Networks->Varying regularization in Multi-layer Perceptron": [
        [
            "A comparison of different values for regularization parameter \u2018alpha\u2019 on\nsynthetic datasets. The plot shows that different alphas yield different\ndecision functions.",
            "markdown"
        ],
        [
            "Alpha is a parameter for regularization term, aka penalty term, that combats\noverfitting by constraining the size of the weights. Increasing alpha may fix\nhigh variance (a sign of overfitting) by encouraging smaller weights, resulting\nin a decision boundary plot that appears with lesser curvatures.\nSimilarly, decreasing alpha may fix high bias (a sign of underfitting) by\nencouraging larger weights, potentially resulting in a more complicated\ndecision boundary.\n<img alt=\"alpha 0.10, alpha 0.32, alpha 1.00, alpha 3.16, alpha 10.00, alpha 0.10, alpha 0.32, alpha 1.00, alpha 3.16, alpha 10.00, alpha 0.10, alpha 0.32, alpha 1.00, alpha 3.16, alpha 10.00\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_mlp_alpha_001.png\" srcset=\"../../_images/sphx_glr_plot_mlp_alpha_001.png\"/>",
            "markdown"
        ],
        [
            "# Author: Issam H. Laradji\n# License: BSD 3 clause\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom matplotlib.colors import \nfrom sklearn.model_selection import \nfrom sklearn.preprocessing import \nfrom sklearn.datasets import , , \nfrom sklearn.neural_network import \nfrom sklearn.pipeline import \n\nh = 0.02  # step size in the mesh\n\nalphas = (-1, 1, 5)\n\nclassifiers = []\nnames = []\nfor alpha in alphas:\n    classifiers.append(\n        (\n            (),\n            (\n                solver=\"lbfgs\",\n                alpha=alpha,\n                random_state=1,\n                max_iter=2000,\n                early_stopping=True,\n                hidden_layer_sizes=[10, 10],\n            ),\n        )\n    )\n    names.append(f\"alpha {alpha:.2f}\")\n\nX, y = (\n    n_features=2, n_redundant=0, n_informative=2, random_state=0, n_clusters_per_class=1\n)\nrng = (2)\nX += 2 * rng.uniform(size=X.shape)\nlinearly_separable = (X, y)\n\ndatasets = [\n    (noise=0.3, random_state=0),\n    (noise=0.2, factor=0.5, random_state=1),\n    linearly_separable,\n]\n\nfigure = (figsize=(17, 9))\ni = 1\n# iterate over datasets\nfor X, y in datasets:\n    # split into training and test part\n    X_train, X_test, y_train, y_test = (\n        X, y, test_size=0.4, random_state=42\n    )\n\n    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n    xx, yy = ((x_min, x_max, h), (y_min, y_max, h))\n\n    # just plot the dataset first\n    cm = plt.cm.RdBu\n    cm_bright = ([\"#FF0000\", \"#0000FF\"])\n    ax = (len(datasets), len(classifiers) + 1, i)\n    # Plot the training points\n    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n    # and testing points\n    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6)\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n    ax.set_xticks(())\n    ax.set_yticks(())\n    i += 1\n\n    # iterate over classifiers\n    for name, clf in zip(names, classifiers):\n        ax = (len(datasets), len(classifiers) + 1, i)\n        clf.fit(X_train, y_train)\n        score = clf.score(X_test, y_test)\n\n        # Plot the decision boundary. For that, we will assign a color to each\n        # point in the mesh [x_min, x_max] x [y_min, y_max].\n        if hasattr(clf, \"decision_function\"):\n            Z = clf.decision_function(([xx.ravel(), yy.ravel()]))\n        else:\n            Z = clf.predict_proba(([xx.ravel(), yy.ravel()]))[:, 1]\n\n        # Put the result into a color plot\n        Z = Z.reshape(xx.shape)\n        ax.contourf(xx, yy, Z, cmap=cm, alpha=0.8)\n\n        # Plot also the training points\n        ax.scatter(\n            X_train[:, 0],\n            X_train[:, 1],\n            c=y_train,\n            cmap=cm_bright,\n            edgecolors=\"black\",\n            s=25,\n        )\n        # and testing points\n        ax.scatter(\n            X_test[:, 0],\n            X_test[:, 1],\n            c=y_test,\n            cmap=cm_bright,\n            alpha=0.6,\n            edgecolors=\"black\",\n            s=25,\n        )\n\n        ax.set_xlim(xx.min(), xx.max())\n        ax.set_ylim(yy.min(), yy.max())\n        ax.set_xticks(())\n        ax.set_yticks(())\n        ax.set_title(name)\n        ax.text(\n            xx.max() - 0.3,\n            yy.min() + 0.3,\n            f\"{score:.3f}\".lstrip(\"0\"),\n            size=15,\n            horizontalalignment=\"right\",\n        )\n        i += 1\n\nfigure.subplots_adjust(left=0.02, right=0.98)\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  1.991 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Neural Networks->Visualization of MLP weights on MNIST": [
        [
            "Sometimes looking at the learned coefficients of a neural network can provide\ninsight into the learning behavior. For example if weights look unstructured,\nmaybe some were not used at all, or if very large coefficients exist, maybe\nregularization was too low or the learning rate too high.",
            "markdown"
        ],
        [
            "This example shows how to plot some of the first layer weights in a\nMLPClassifier trained on the MNIST dataset.",
            "markdown"
        ],
        [
            "The input data consists of 28x28 pixel handwritten digits, leading to 784\nfeatures in the dataset. Therefore the first layer weight matrix has the shape\n(784, hidden_layer_sizes[0]).  We can therefore visualize a single column of\nthe weight matrix as a 28x28 pixel image.",
            "markdown"
        ],
        [
            "To make the example run faster, we use very few hidden units, and train only\nfor a very short time. Training longer would result in weights with a much\nsmoother spatial appearance. The example will throw a warning because it\ndoesn\u2019t converge, in this case this is what we want because of resource\nusage constraints on our Continuous Integration infrastructure that is used\nto build this documentation on a regular basis.\n<img alt=\"plot mnist filters\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_mnist_filters_001.png\" srcset=\"../../_images/sphx_glr_plot_mnist_filters_001.png\"/>",
            "markdown"
        ],
        [
            "Iteration 1, loss = 0.44139186\nIteration 2, loss = 0.19174891\nIteration 3, loss = 0.13983521\nIteration 4, loss = 0.11378556\nIteration 5, loss = 0.09443967\nIteration 6, loss = 0.07846529\nIteration 7, loss = 0.06506307\nIteration 8, loss = 0.05534985\nTraining set score: 0.986429\nTest set score: 0.953061\n\n\n\n<br/>",
            "code"
        ],
        [
            "import warnings\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import \nfrom sklearn.exceptions import \nfrom sklearn.neural_network import \nfrom sklearn.model_selection import \n\n# Load data from https://www.openml.org/d/554\nX, y = (\n    \"mnist_784\", version=1, return_X_y=True, as_frame=False, parser=\"pandas\"\n)\nX = X / 255.0\n\n# Split data into train partition and test partition\nX_train, X_test, y_train, y_test = (X, y, random_state=0, test_size=0.7)\n\nmlp = (\n    hidden_layer_sizes=(40,),\n    max_iter=8,\n    alpha=1e-4,\n    solver=\"sgd\",\n    verbose=10,\n    random_state=1,\n    learning_rate_init=0.2,\n)\n\n# this example won't converge because of resource usage constraints on\n# our Continuous Integration infrastructure, so we catch the warning and\n# ignore it here\nwith ():\n    (\"ignore\", category=, module=\"sklearn\")\n    mlp.fit(X_train, y_train)\n\nprint(\"Training set score: %f\" % mlp.score(X_train, y_train))\nprint(\"Test set score: %f\" % mlp.score(X_test, y_test))\n\nfig, axes = (4, 4)\n# use global min / max to ensure all weights are shown on the same scale\nvmin, vmax = mlp.coefs_[0].min(), mlp.coefs_[0].max()\nfor coef, ax in zip(mlp.coefs_[0].T, axes.ravel()):\n    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=0.5 * vmin, vmax=0.5 * vmax)\n    ax.set_xticks(())\n    ax.set_yticks(())\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  6.494 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Pipelines and composite estimators->Column Transformer with Heterogeneous Data Sources": [
        [
            "Datasets can often contain components that require different feature\nextraction and processing pipelines. This scenario might occur when:",
            "markdown"
        ],
        [
            "your dataset consists of heterogeneous data types (e.g. raster images and\ntext captions),",
            "markdown"
        ],
        [
            "your dataset is stored in a  and different columns\nrequire different processing pipelines.",
            "markdown"
        ],
        [
            "This example demonstrates how to use\n on a dataset containing\ndifferent types of features. The choice of features is not particularly\nhelpful, but serves to illustrate the technique.",
            "markdown"
        ],
        [
            "# Author: Matt Terry &lt;matt.terry@gmail.com\n#\n# License: BSD 3 clause\n\nimport numpy as np\n\nfrom sklearn.preprocessing import \nfrom sklearn.datasets import \nfrom sklearn.decomposition import \nfrom sklearn.feature_extraction import \nfrom sklearn.feature_extraction.text import \nfrom sklearn.metrics import \nfrom sklearn.pipeline import \nfrom sklearn.compose import \nfrom sklearn.svm import",
            "code"
        ]
    ],
    "Examples->Pipelines and composite estimators->Column Transformer with Heterogeneous Data Sources->20 newsgroups dataset": [
        [
            "We will use the , which\ncomprises posts from newsgroups on 20 topics. This dataset is split\ninto train and test subsets based on messages posted before and after\na specific date. We will only use posts from 2 categories to speed up running\ntime.",
            "markdown"
        ],
        [
            "categories = [\"sci.med\", \"sci.space\"]\nX_train, y_train = (\n    random_state=1,\n    subset=\"train\",\n    categories=categories,\n    remove=(\"footers\", \"quotes\"),\n    return_X_y=True,\n)\nX_test, y_test = (\n    random_state=1,\n    subset=\"test\",\n    categories=categories,\n    remove=(\"footers\", \"quotes\"),\n    return_X_y=True,\n)",
            "code"
        ],
        [
            "Each feature comprises meta information about that post, such as the subject,\nand the body of the news post.",
            "markdown"
        ],
        [
            "print(X_train[0])",
            "code"
        ],
        [
            "From: mccall@mksol.dseg.ti.com (fred j mccall 575-3539)\nSubject: Re: Metric vs English\nArticle-I.D.: mksol.1993Apr6.131900.8407\nOrganization: Texas Instruments Inc\nLines: 31\n\n\n\n\nAmerican, perhaps, but nothing military about it.  I learned (mostly)\nslugs when we talked English units in high school physics and while\nthe teacher was an ex-Navy fighter jock the book certainly wasn't\nproduced by the military.\n\n[Poundals were just too flinking small and made the math come out\nfunny; sort of the same reason proponents of SI give for using that.]\n\n--\n\"Insisting on perfect safety is for people who don't have the balls to live\n in the real world.\"   -- Mary Shafer, NASA Ames Dryden",
            "code"
        ]
    ],
    "Examples->Pipelines and composite estimators->Column Transformer with Heterogeneous Data Sources->Creating transformers": [
        [
            "First, we would like a transformer that extracts the subject and\nbody of each post. Since this is a stateless transformation (does not\nrequire state information from training data), we can define a function that\nperforms the data transformation then use\n to create a scikit-learn\ntransformer.",
            "markdown"
        ],
        [
            "def subject_body_extractor(posts):\n    # construct object dtype array with two columns\n    # first column = 'subject' and second column = 'body'\n    features = (shape=(len(posts), 2), dtype=object)\n    for i, text in enumerate(posts):\n        # temporary variable `_` stores '\\n\\n'\n        headers, _, body = text.partition(\"\\n\\n\")\n        # store body text in second column\n        features[i, 1] = body\n\n        prefix = \"Subject:\"\n        sub = \"\"\n        # save text after 'Subject:' in first column\n        for line in headers.split(\"\\n\"):\n            if line.startswith(prefix):\n                sub = line[len(prefix) :]\n                break\n        features[i, 0] = sub\n\n    return features\n\n\nsubject_body_transformer = (subject_body_extractor)",
            "code"
        ],
        [
            "We will also create a transformer that extracts the\nlength of the text and the number of sentences.",
            "markdown"
        ],
        [
            "def text_stats(posts):\n    return [{\"length\": len(text), \"num_sentences\": text.count(\".\")} for text in posts]\n\n\ntext_stats_transformer = (text_stats)",
            "code"
        ]
    ],
    "Examples->Pipelines and composite estimators->Column Transformer with Heterogeneous Data Sources->Classification pipeline": [
        [
            "The pipeline below extracts the subject and body from each post using\nSubjectBodyExtractor, producing a (n_samples, 2) array. This array is\nthen used to compute standard bag-of-words features for the subject and body\nas well as text length and number of sentences on the body, using\nColumnTransformer. We combine them, with weights, then train a\nclassifier on the combined set of features.",
            "markdown"
        ],
        [
            "pipeline = (\n    [\n        # Extract subject & body\n        (\"subjectbody\", subject_body_transformer),\n        # Use ColumnTransformer to combine the subject and body features\n        (\n            \"union\",\n            (\n                [\n                    # bag-of-words for subject (col 0)\n                    (\"subject\", (min_df=50), 0),\n                    # bag-of-words with decomposition for body (col 1)\n                    (\n                        \"body_bow\",\n                        (\n                            [\n                                (\"tfidf\", ()),\n                                (\"best\", (n_components=50)),\n                            ]\n                        ),\n                        1,\n                    ),\n                    # Pipeline for pulling text stats from post's body\n                    (\n                        \"body_stats\",\n                        (\n                            [\n                                (\n                                    \"stats\",\n                                    text_stats_transformer,\n                                ),  # returns a list of dicts\n                                (\n                                    \"vect\",\n                                    (),\n                                ),  # list of dicts - feature matrix\n                            ]\n                        ),\n                        1,\n                    ),\n                ],\n                # weight above ColumnTransformer features\n                transformer_weights={\n                    \"subject\": 0.8,\n                    \"body_bow\": 0.5,\n                    \"body_stats\": 1.0,\n                },\n            ),\n        ),\n        # Use a SVC classifier on the combined features\n        (\"svc\", (dual=False)),\n    ],\n    verbose=True,\n)",
            "code"
        ],
        [
            "Finally, we fit our pipeline on the training data and use it to predict\ntopics for X_test. Performance metrics of our pipeline are then printed.",
            "markdown"
        ],
        [
            "pipeline.fit(X_train, y_train)\ny_pred = pipeline.predict(X_test)\nprint(\"Classification report:\\n\\n{}\".format((y_test, y_pred)))",
            "code"
        ],
        [
            "[Pipeline] ....... (step 1 of 3) Processing subjectbody, total=   0.0s\n[Pipeline] ............. (step 2 of 3) Processing union, total=   0.4s\n[Pipeline] ............... (step 3 of 3) Processing svc, total=   0.0s\nClassification report:\n\n              precision    recall  f1-score   support\n\n           0       0.84      0.87      0.86       396\n           1       0.87      0.83      0.85       394\n\n    accuracy                           0.85       790\n   macro avg       0.85      0.85      0.85       790\nweighted avg       0.85      0.85      0.85       790",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  2.603 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Pipelines and composite estimators->Column Transformer with Mixed Types": [
        [
            "This example illustrates how to apply different preprocessing and feature\nextraction pipelines to different subsets of features, using\n. This is particularly handy for the\ncase of datasets that contain heterogeneous data types, since we may want to\nscale the numeric features and one-hot encode the categorical ones.",
            "markdown"
        ],
        [
            "In this example, the numeric data is standard-scaled after mean-imputation. The\ncategorical data is one-hot encoded via OneHotEncoder, which\ncreates a new category for missing values. We further reduce the dimensionality\nby selecting categories using a chi-squared test.",
            "markdown"
        ],
        [
            "In addition, we show two different ways to dispatch the columns to the\nparticular pre-processor: by column names and by column data types.",
            "markdown"
        ],
        [
            "Finally, the preprocessing pipeline is integrated in a full prediction pipeline\nusing , together with a simple classification\nmodel.",
            "markdown"
        ],
        [
            "# Author: Pedro Morales &lt;part.morales@gmail.com\n#\n# License: BSD 3 clause",
            "code"
        ],
        [
            "import numpy as np\n\nfrom sklearn.compose import \nfrom sklearn.datasets import \nfrom sklearn.pipeline import \nfrom sklearn.impute import \nfrom sklearn.preprocessing import , \nfrom sklearn.linear_model import \nfrom sklearn.model_selection import , \nfrom sklearn.feature_selection import , \n\n(0)",
            "code"
        ],
        [
            "Load data from",
            "markdown"
        ],
        [
            "X, y = (\n    \"titanic\", version=1, as_frame=True, return_X_y=True, parser=\"pandas\"\n)\n\n# Alternatively X and y can be obtained directly from the frame attribute:\n# X = titanic.frame.drop('survived', axis=1)\n# y = titanic.frame['survived']",
            "code"
        ],
        [
            "Use ColumnTransformer by selecting column by names",
            "markdown"
        ],
        [
            "We will train our classifier with the following features:",
            "markdown"
        ],
        [
            "Numeric Features:",
            "markdown"
        ],
        [
            "age: float;",
            "markdown"
        ],
        [
            "fare: float.",
            "markdown"
        ],
        [
            "Categorical Features:",
            "markdown"
        ],
        [
            "embarked: categories encoded as strings {'C', 'S', 'Q'};",
            "markdown"
        ],
        [
            "sex: categories encoded as strings {'female', 'male'};",
            "markdown"
        ],
        [
            "pclass: ordinal integers {1, 2, 3}.",
            "markdown"
        ],
        [
            "We create the preprocessing pipelines for both numeric and categorical data.\nNote that pclass could either be treated as a categorical or numeric\nfeature.",
            "markdown"
        ],
        [
            "numeric_features = [\"age\", \"fare\"]\nnumeric_transformer = (\n    steps=[(\"imputer\", (strategy=\"median\")), (\"scaler\", ())]\n)\n\ncategorical_features = [\"embarked\", \"sex\", \"pclass\"]\ncategorical_transformer = (\n    steps=[\n        (\"encoder\", (handle_unknown=\"ignore\")),\n        (\"selector\", (, percentile=50)),\n    ]\n)\npreprocessor = (\n    transformers=[\n        (\"num\", numeric_transformer, numeric_features),\n        (\"cat\", categorical_transformer, categorical_features),\n    ]\n)",
            "code"
        ],
        [
            "Append classifier to preprocessing pipeline.\nNow we have a full prediction pipeline.",
            "markdown"
        ],
        [
            "clf = (\n    steps=[(\"preprocessor\", preprocessor), (\"classifier\", ())]\n)\n\nX_train, X_test, y_train, y_test = (X, y, test_size=0.2, random_state=0)\n\nclf.fit(X_train, y_train)\nprint(\"model score: %.3f\" % clf.score(X_test, y_test))",
            "code"
        ],
        [
            "model score: 0.798",
            "code"
        ],
        [
            "HTML representation of Pipeline (display diagram)",
            "markdown"
        ],
        [
            "When the Pipeline is printed out in a jupyter notebook an HTML\nrepresentation of the estimator is displayed:",
            "markdown"
        ],
        [
            "clf",
            "code"
        ],
        [
            "Pipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age', 'fare']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('encoder',\n                                                                   OneHotEncoder(handle_unknown='ignore')),\n                                                                  ('selector',\n                                                                   SelectPercentile(percentile=50,\n                                                                                    score_func=&lt;function chi2 at 0x7f0d703fe700))]),\n                                                  ['embarked', 'sex',\n                                                   'pclass'])])),\n                ('classifier', LogisticRegression())])<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-236\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-236\">Pipeline</label>",
            "code"
        ],
        [
            "Pipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age', 'fare']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('encoder',\n                                                                   OneHotEncoder(handle_unknown='ignore')),\n                                                                  ('selector',\n                                                                   SelectPercentile(percentile=50,\n                                                                                    score_func=&lt;function chi2 at 0x7f0d703fe700))]),\n                                                  ['embarked', 'sex',\n                                                   'pclass'])])),\n                ('classifier', LogisticRegression())])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-237\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-237\">preprocessor: ColumnTransformer</label>",
            "code"
        ],
        [
            "ColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('scaler', StandardScaler())]),\n                                 ['age', 'fare']),\n                                ('cat',\n                                 Pipeline(steps=[('encoder',\n                                                  OneHotEncoder(handle_unknown='ignore')),\n                                                 ('selector',\n                                                  SelectPercentile(percentile=50,\n                                                                   score_func=&lt;function chi2 at 0x7f0d703fe700))]),\n                                 ['embarked', 'sex', 'pclass'])])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-238\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-238\">num</label>",
            "code"
        ],
        [
            "['age', 'fare']<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-239\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-239\">SimpleImputer</label>",
            "code"
        ],
        [
            "SimpleImputer(strategy='median')<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-240\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-240\">StandardScaler</label>",
            "code"
        ],
        [
            "StandardScaler()<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-241\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-241\">cat</label>",
            "code"
        ],
        [
            "['embarked', 'sex', 'pclass']<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-242\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-242\">OneHotEncoder</label>",
            "code"
        ],
        [
            "OneHotEncoder(handle_unknown='ignore')<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-243\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-243\">SelectPercentile</label>",
            "code"
        ],
        [
            "SelectPercentile(percentile=50, score_func=&lt;function chi2 at 0x7f0d703fe700)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-244\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-244\">LogisticRegression</label>",
            "code"
        ],
        [
            "LogisticRegression()\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "Use ColumnTransformer by selecting column by data types",
            "markdown"
        ],
        [
            "When dealing with a cleaned dataset, the preprocessing can be automatic by\nusing the data types of the column to decide whether to treat a column as a\nnumerical or categorical feature.\n gives this possibility.\nFirst, let\u2019s only select a subset of columns to simplify our\nexample.",
            "markdown"
        ],
        [
            "subset_feature = [\"embarked\", \"sex\", \"pclass\", \"age\", \"fare\"]\nX_train, X_test = X_train[subset_feature], X_test[subset_feature]",
            "code"
        ],
        [
            "Then, we introspect the information regarding each column data type.",
            "markdown"
        ],
        [
            "X_train.info()",
            "code"
        ],
        [
            "&lt;class 'pandas.core.frame.DataFrame'\nInt64Index: 1047 entries, 1118 to 684\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype\n---  ------    --------------  -----\n 0   embarked  1045 non-null   category\n 1   sex       1047 non-null   category\n 2   pclass    1047 non-null   int64\n 3   age       841 non-null    float64\n 4   fare      1046 non-null   float64\ndtypes: category(2), float64(2), int64(1)\nmemory usage: 35.0 KB",
            "code"
        ],
        [
            "We can observe that the embarked and sex columns were tagged as\ncategory columns when loading the data with fetch_openml. Therefore, we\ncan use this information to dispatch the categorical columns to the\ncategorical_transformer and the remaining columns to the\nnumerical_transformer.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "In practice, you will have to handle yourself the column data type.\nIf you want some columns to be considered as category, you will have to\nconvert them into categorical columns. If you are using pandas, you can\nrefer to their documentation regarding .",
            "markdown"
        ],
        [
            "from sklearn.compose import make_column_selector as \n\npreprocessor = (\n    transformers=[\n        (\"num\", numeric_transformer, (dtype_exclude=\"category\")),\n        (\"cat\", categorical_transformer, (dtype_include=\"category\")),\n    ]\n)\nclf = (\n    steps=[(\"preprocessor\", preprocessor), (\"classifier\", ())]\n)\n\n\nclf.fit(X_train, y_train)\nprint(\"model score: %.3f\" % clf.score(X_test, y_test))\nclf",
            "code"
        ],
        [
            "model score: 0.798",
            "code"
        ],
        [
            "Pipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d6dc8d040),\n                                                 ('cat',\n                                                  Pipeline(steps=[('encoder',\n                                                                   OneHotEncoder(handle_unknown='ignore')),\n                                                                  ('selector',\n                                                                   SelectPercentile(percentile=50,\n                                                                                    score_func=&lt;function chi2 at 0x7f0d703fe700))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d6dc8d4c0)])),\n                ('classifier', LogisticRegression())])<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-245\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-245\">Pipeline</label>",
            "code"
        ],
        [
            "Pipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d6dc8d040),\n                                                 ('cat',\n                                                  Pipeline(steps=[('encoder',\n                                                                   OneHotEncoder(handle_unknown='ignore')),\n                                                                  ('selector',\n                                                                   SelectPercentile(percentile=50,\n                                                                                    score_func=&lt;function chi2 at 0x7f0d703fe700))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d6dc8d4c0)])),\n                ('classifier', LogisticRegression())])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-246\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-246\">preprocessor: ColumnTransformer</label>",
            "code"
        ],
        [
            "ColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('scaler', StandardScaler())]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d6dc8d040),\n                                ('cat',\n                                 Pipeline(steps=[('encoder',\n                                                  OneHotEncoder(handle_unknown='ignore')),\n                                                 ('selector',\n                                                  SelectPercentile(percentile=50,\n                                                                   score_func=&lt;function chi2 at 0x7f0d703fe700))]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d6dc8d4c0)])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-247\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-247\">num</label>",
            "code"
        ],
        [
            "&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d6dc8d040<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-248\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-248\">SimpleImputer</label>",
            "code"
        ],
        [
            "SimpleImputer(strategy='median')<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-249\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-249\">StandardScaler</label>",
            "code"
        ],
        [
            "StandardScaler()<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-250\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-250\">cat</label>",
            "code"
        ],
        [
            "&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d6dc8d4c0<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-251\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-251\">OneHotEncoder</label>",
            "code"
        ],
        [
            "OneHotEncoder(handle_unknown='ignore')<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-252\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-252\">SelectPercentile</label>",
            "code"
        ],
        [
            "SelectPercentile(percentile=50, score_func=&lt;function chi2 at 0x7f0d703fe700)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-253\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-253\">LogisticRegression</label>",
            "code"
        ],
        [
            "LogisticRegression()\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "The resulting score is not exactly the same as the one from the previous\npipeline because the dtype-based selector treats the pclass column as\na numeric feature instead of a categorical feature as previously:",
            "markdown"
        ],
        [
            "(dtype_exclude=\"category\")(X_train)",
            "code"
        ],
        [
            "['pclass', 'age', 'fare']",
            "code"
        ],
        [
            "(dtype_include=\"category\")(X_train)",
            "code"
        ],
        [
            "['embarked', 'sex']",
            "code"
        ],
        [
            "Using the prediction pipeline in a grid search",
            "markdown"
        ],
        [
            "Grid search can also be performed on the different preprocessing steps\ndefined in the ColumnTransformer object, together with the classifier\u2019s\nhyperparameters as part of the Pipeline.\nWe will search for both the imputer strategy of the numeric preprocessing\nand the regularization parameter of the logistic regression using\n. This\nhyperparameter search randomly selects a fixed number of parameter\nsettings configured by n_iter. Alternatively, one can use\n but the cartesian product of\nthe parameter space will be evaluated.",
            "markdown"
        ],
        [
            "param_grid = {\n    \"preprocessor__num__imputer__strategy\": [\"mean\", \"median\"],\n    \"preprocessor__cat__selector__percentile\": [10, 30, 50, 70],\n    \"classifier__C\": [0.1, 1.0, 10, 100],\n}\n\nsearch_cv = (clf, param_grid, n_iter=10, random_state=0)\nsearch_cv",
            "code"
        ],
        [
            "RandomizedSearchCV(estimator=Pipeline(steps=[('preprocessor',\n                                              ColumnTransformer(transformers=[('num',\n                                                                               Pipeline(steps=[('imputer',\n                                                                                                SimpleImputer(strategy='median')),\n                                                                                               ('scaler',\n                                                                                                StandardScaler())]),\n                                                                               &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d6dc8d040),\n                                                                              ('cat',\n                                                                               Pipeline(steps=[('encoder',\n                                                                                                OneHotEncoder(handle_unknown='ignore')),\n                                                                                               ('s...\n                                                                                                                 score_func=&lt;function chi2 at 0x7f0d703fe700))]),\n                                                                               &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d6dc8d4c0)])),\n                                             ('classifier',\n                                              LogisticRegression())]),\n                   param_distributions={'classifier__C': [0.1, 1.0, 10, 100],\n                                        'preprocessor__cat__selector__percentile': [10,\n                                                                                    30,\n                                                                                    50,\n                                                                                    70],\n                                        'preprocessor__num__imputer__strategy': ['mean',\n                                                                                 'median']},\n                   random_state=0)<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-254\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-254\">RandomizedSearchCV</label>",
            "code"
        ],
        [
            "RandomizedSearchCV(estimator=Pipeline(steps=[('preprocessor',\n                                              ColumnTransformer(transformers=[('num',\n                                                                               Pipeline(steps=[('imputer',\n                                                                                                SimpleImputer(strategy='median')),\n                                                                                               ('scaler',\n                                                                                                StandardScaler())]),\n                                                                               &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d6dc8d040),\n                                                                              ('cat',\n                                                                               Pipeline(steps=[('encoder',\n                                                                                                OneHotEncoder(handle_unknown='ignore')),\n                                                                                               ('s...\n                                                                                                                 score_func=&lt;function chi2 at 0x7f0d703fe700))]),\n                                                                               &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d6dc8d4c0)])),\n                                             ('classifier',\n                                              LogisticRegression())]),\n                   param_distributions={'classifier__C': [0.1, 1.0, 10, 100],\n                                        'preprocessor__cat__selector__percentile': [10,\n                                                                                    30,\n                                                                                    50,\n                                                                                    70],\n                                        'preprocessor__num__imputer__strategy': ['mean',\n                                                                                 'median']},\n                   random_state=0)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-255\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-255\">estimator: Pipeline</label>",
            "code"
        ],
        [
            "Pipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d6dc8d040),\n                                                 ('cat',\n                                                  Pipeline(steps=[('encoder',\n                                                                   OneHotEncoder(handle_unknown='ignore')),\n                                                                  ('selector',\n                                                                   SelectPercentile(percentile=50,\n                                                                                    score_func=&lt;function chi2 at 0x7f0d703fe700))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d6dc8d4c0)])),\n                ('classifier', LogisticRegression())])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-256\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-256\">preprocessor: ColumnTransformer</label>",
            "code"
        ],
        [
            "ColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('scaler', StandardScaler())]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d6dc8d040),\n                                ('cat',\n                                 Pipeline(steps=[('encoder',\n                                                  OneHotEncoder(handle_unknown='ignore')),\n                                                 ('selector',\n                                                  SelectPercentile(percentile=50,\n                                                                   score_func=&lt;function chi2 at 0x7f0d703fe700))]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d6dc8d4c0)])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-257\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-257\">num</label>",
            "code"
        ],
        [
            "&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d6dc8d040<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-258\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-258\">SimpleImputer</label>",
            "code"
        ],
        [
            "SimpleImputer(strategy='median')<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-259\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-259\">StandardScaler</label>",
            "code"
        ],
        [
            "StandardScaler()<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-260\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-260\">cat</label>",
            "code"
        ],
        [
            "&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f0d6dc8d4c0<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-261\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-261\">OneHotEncoder</label>",
            "code"
        ],
        [
            "OneHotEncoder(handle_unknown='ignore')<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-262\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-262\">SelectPercentile</label>",
            "code"
        ],
        [
            "SelectPercentile(percentile=50, score_func=&lt;function chi2 at 0x7f0d703fe700)<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-263\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-263\">LogisticRegression</label>",
            "code"
        ],
        [
            "LogisticRegression()\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "Calling \u2018fit\u2019 triggers the cross-validated search for the best\nhyper-parameters combination:",
            "markdown"
        ],
        [
            "search_cv.fit(X_train, y_train)\n\nprint(\"Best params:\")\nprint(search_cv.best_params_)",
            "code"
        ],
        [
            "Best params:\n{'preprocessor__num__imputer__strategy': 'mean', 'preprocessor__cat__selector__percentile': 30, 'classifier__C': 100}",
            "code"
        ],
        [
            "The internal cross-validation scores obtained by those parameters is:",
            "markdown"
        ],
        [
            "print(f\"Internal CV score: {search_cv.best_score_:.3f}\")",
            "code"
        ],
        [
            "Internal CV score: 0.786",
            "code"
        ],
        [
            "We can also introspect the top grid search results as a pandas dataframe:",
            "markdown"
        ],
        [
            "import pandas as pd\n\ncv_results = (search_cv.cv_results_)\ncv_results = cv_results.sort_values(\"mean_test_score\", ascending=False)\ncv_results[\n    [\n        \"mean_test_score\",\n        \"std_test_score\",\n        \"param_preprocessor__num__imputer__strategy\",\n        \"param_preprocessor__cat__selector__percentile\",\n        \"param_classifier__C\",\n    ]\n].head(5)\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "The best hyper-parameters have be used to re-fit a final model on the full\ntraining set. We can evaluate that final model on held out test data that was\nnot used for hyperparameter tuning.",
            "markdown"
        ],
        [
            "print(\n    \"accuracy of the best model from randomized search: \"\n    f\"{search_cv.score(X_test, y_test):.3f}\"\n)",
            "code"
        ],
        [
            "accuracy of the best model from randomized search: 0.798",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  1.139 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Pipelines and composite estimators->Concatenating multiple feature extraction methods": [
        [
            "In many real-world examples, there are many ways to extract features from a\ndataset. Often it is beneficial to combine several methods to obtain good\nperformance. This example shows how to use FeatureUnion to combine\nfeatures obtained by PCA and univariate selection.",
            "markdown"
        ],
        [
            "Combining features using this transformer has the benefit that it allows\ncross validation and grid searches over the whole process.",
            "markdown"
        ],
        [
            "The combination used in this example is not particularly helpful on this\ndataset and is only used to illustrate the usage of FeatureUnion.",
            "markdown"
        ],
        [
            "Combined space has 3 features\nFitting 5 folds for each of 18 candidates, totalling 90 fits\n[CV 1/5; 1/18] START features__pca__n_components=1, features__univ_select__k=1, svm__C=0.1\n[CV 1/5; 1/18] END features__pca__n_components=1, features__univ_select__k=1, svm__C=0.1;, score=0.933 total time=   0.0s\n[CV 2/5; 1/18] START features__pca__n_components=1, features__univ_select__k=1, svm__C=0.1\n[CV 2/5; 1/18] END features__pca__n_components=1, features__univ_select__k=1, svm__C=0.1;, score=0.933 total time=   0.0s\n[CV 3/5; 1/18] START features__pca__n_components=1, features__univ_select__k=1, svm__C=0.1\n[CV 3/5; 1/18] END features__pca__n_components=1, features__univ_select__k=1, svm__C=0.1;, score=0.867 total time=   0.0s\n[CV 4/5; 1/18] START features__pca__n_components=1, features__univ_select__k=1, svm__C=0.1\n[CV 4/5; 1/18] END features__pca__n_components=1, features__univ_select__k=1, svm__C=0.1;, score=0.933 total time=   0.0s\n[CV 5/5; 1/18] START features__pca__n_components=1, features__univ_select__k=1, svm__C=0.1\n[CV 5/5; 1/18] END features__pca__n_components=1, features__univ_select__k=1, svm__C=0.1;, score=1.000 total time=   0.0s\n[CV 1/5; 2/18] START features__pca__n_components=1, features__univ_select__k=1, svm__C=1\n[CV 1/5; 2/18] END features__pca__n_components=1, features__univ_select__k=1, svm__C=1;, score=0.900 total time=   0.0s\n[CV 2/5; 2/18] START features__pca__n_components=1, features__univ_select__k=1, svm__C=1\n[CV 2/5; 2/18] END features__pca__n_components=1, features__univ_select__k=1, svm__C=1;, score=1.000 total time=   0.0s\n[CV 3/5; 2/18] START features__pca__n_components=1, features__univ_select__k=1, svm__C=1\n[CV 3/5; 2/18] END features__pca__n_components=1, features__univ_select__k=1, svm__C=1;, score=0.867 total time=   0.0s\n[CV 4/5; 2/18] START features__pca__n_components=1, features__univ_select__k=1, svm__C=1\n[CV 4/5; 2/18] END features__pca__n_components=1, features__univ_select__k=1, svm__C=1;, score=0.933 total time=   0.0s\n[CV 5/5; 2/18] START features__pca__n_components=1, features__univ_select__k=1, svm__C=1\n[CV 5/5; 2/18] END features__pca__n_components=1, features__univ_select__k=1, svm__C=1;, score=1.000 total time=   0.0s\n[CV 1/5; 3/18] START features__pca__n_components=1, features__univ_select__k=1, svm__C=10\n[CV 1/5; 3/18] END features__pca__n_components=1, features__univ_select__k=1, svm__C=10;, score=0.933 total time=   0.0s\n[CV 2/5; 3/18] START features__pca__n_components=1, features__univ_select__k=1, svm__C=10\n[CV 2/5; 3/18] END features__pca__n_components=1, features__univ_select__k=1, svm__C=10;, score=1.000 total time=   0.0s\n[CV 3/5; 3/18] START features__pca__n_components=1, features__univ_select__k=1, svm__C=10\n[CV 3/5; 3/18] END features__pca__n_components=1, features__univ_select__k=1, svm__C=10;, score=0.900 total time=   0.0s\n[CV 4/5; 3/18] START features__pca__n_components=1, features__univ_select__k=1, svm__C=10\n[CV 4/5; 3/18] END features__pca__n_components=1, features__univ_select__k=1, svm__C=10;, score=0.933 total time=   0.0s\n[CV 5/5; 3/18] START features__pca__n_components=1, features__univ_select__k=1, svm__C=10\n[CV 5/5; 3/18] END features__pca__n_components=1, features__univ_select__k=1, svm__C=10;, score=1.000 total time=   0.0s\n[CV 1/5; 4/18] START features__pca__n_components=1, features__univ_select__k=2, svm__C=0.1\n[CV 1/5; 4/18] END features__pca__n_components=1, features__univ_select__k=2, svm__C=0.1;, score=0.933 total time=   0.0s\n[CV 2/5; 4/18] START features__pca__n_components=1, features__univ_select__k=2, svm__C=0.1\n[CV 2/5; 4/18] END features__pca__n_components=1, features__univ_select__k=2, svm__C=0.1;, score=0.967 total time=   0.0s\n[CV 3/5; 4/18] START features__pca__n_components=1, features__univ_select__k=2, svm__C=0.1\n[CV 3/5; 4/18] END features__pca__n_components=1, features__univ_select__k=2, svm__C=0.1;, score=0.933 total time=   0.0s\n[CV 4/5; 4/18] START features__pca__n_components=1, features__univ_select__k=2, svm__C=0.1\n[CV 4/5; 4/18] END features__pca__n_components=1, features__univ_select__k=2, svm__C=0.1;, score=0.933 total time=   0.0s\n[CV 5/5; 4/18] START features__pca__n_components=1, features__univ_select__k=2, svm__C=0.1\n[CV 5/5; 4/18] END features__pca__n_components=1, features__univ_select__k=2, svm__C=0.1;, score=1.000 total time=   0.0s\n[CV 1/5; 5/18] START features__pca__n_components=1, features__univ_select__k=2, svm__C=1\n[CV 1/5; 5/18] END features__pca__n_components=1, features__univ_select__k=2, svm__C=1;, score=0.933 total time=   0.0s\n[CV 2/5; 5/18] START features__pca__n_components=1, features__univ_select__k=2, svm__C=1\n[CV 2/5; 5/18] END features__pca__n_components=1, features__univ_select__k=2, svm__C=1;, score=0.967 total time=   0.0s\n[CV 3/5; 5/18] START features__pca__n_components=1, features__univ_select__k=2, svm__C=1\n[CV 3/5; 5/18] END features__pca__n_components=1, features__univ_select__k=2, svm__C=1;, score=0.933 total time=   0.0s\n[CV 4/5; 5/18] START features__pca__n_components=1, features__univ_select__k=2, svm__C=1\n[CV 4/5; 5/18] END features__pca__n_components=1, features__univ_select__k=2, svm__C=1;, score=0.933 total time=   0.0s\n[CV 5/5; 5/18] START features__pca__n_components=1, features__univ_select__k=2, svm__C=1\n[CV 5/5; 5/18] END features__pca__n_components=1, features__univ_select__k=2, svm__C=1;, score=1.000 total time=   0.0s\n[CV 1/5; 6/18] START features__pca__n_components=1, features__univ_select__k=2, svm__C=10\n[CV 1/5; 6/18] END features__pca__n_components=1, features__univ_select__k=2, svm__C=10;, score=0.967 total time=   0.0s\n[CV 2/5; 6/18] START features__pca__n_components=1, features__univ_select__k=2, svm__C=10\n[CV 2/5; 6/18] END features__pca__n_components=1, features__univ_select__k=2, svm__C=10;, score=0.967 total time=   0.0s\n[CV 3/5; 6/18] START features__pca__n_components=1, features__univ_select__k=2, svm__C=10\n[CV 3/5; 6/18] END features__pca__n_components=1, features__univ_select__k=2, svm__C=10;, score=0.933 total time=   0.0s\n[CV 4/5; 6/18] START features__pca__n_components=1, features__univ_select__k=2, svm__C=10\n[CV 4/5; 6/18] END features__pca__n_components=1, features__univ_select__k=2, svm__C=10;, score=0.933 total time=   0.0s\n[CV 5/5; 6/18] START features__pca__n_components=1, features__univ_select__k=2, svm__C=10\n[CV 5/5; 6/18] END features__pca__n_components=1, features__univ_select__k=2, svm__C=10;, score=1.000 total time=   0.0s\n[CV 1/5; 7/18] START features__pca__n_components=2, features__univ_select__k=1, svm__C=0.1\n[CV 1/5; 7/18] END features__pca__n_components=2, features__univ_select__k=1, svm__C=0.1;, score=0.933 total time=   0.0s\n[CV 2/5; 7/18] START features__pca__n_components=2, features__univ_select__k=1, svm__C=0.1\n[CV 2/5; 7/18] END features__pca__n_components=2, features__univ_select__k=1, svm__C=0.1;, score=1.000 total time=   0.0s\n[CV 3/5; 7/18] START features__pca__n_components=2, features__univ_select__k=1, svm__C=0.1\n[CV 3/5; 7/18] END features__pca__n_components=2, features__univ_select__k=1, svm__C=0.1;, score=0.867 total time=   0.0s\n[CV 4/5; 7/18] START features__pca__n_components=2, features__univ_select__k=1, svm__C=0.1\n[CV 4/5; 7/18] END features__pca__n_components=2, features__univ_select__k=1, svm__C=0.1;, score=0.933 total time=   0.0s\n[CV 5/5; 7/18] START features__pca__n_components=2, features__univ_select__k=1, svm__C=0.1\n[CV 5/5; 7/18] END features__pca__n_components=2, features__univ_select__k=1, svm__C=0.1;, score=1.000 total time=   0.0s\n[CV 1/5; 8/18] START features__pca__n_components=2, features__univ_select__k=1, svm__C=1\n[CV 1/5; 8/18] END features__pca__n_components=2, features__univ_select__k=1, svm__C=1;, score=0.967 total time=   0.0s\n[CV 2/5; 8/18] START features__pca__n_components=2, features__univ_select__k=1, svm__C=1\n[CV 2/5; 8/18] END features__pca__n_components=2, features__univ_select__k=1, svm__C=1;, score=1.000 total time=   0.0s\n[CV 3/5; 8/18] START features__pca__n_components=2, features__univ_select__k=1, svm__C=1\n[CV 3/5; 8/18] END features__pca__n_components=2, features__univ_select__k=1, svm__C=1;, score=0.933 total time=   0.0s\n[CV 4/5; 8/18] START features__pca__n_components=2, features__univ_select__k=1, svm__C=1\n[CV 4/5; 8/18] END features__pca__n_components=2, features__univ_select__k=1, svm__C=1;, score=0.933 total time=   0.0s\n[CV 5/5; 8/18] START features__pca__n_components=2, features__univ_select__k=1, svm__C=1\n[CV 5/5; 8/18] END features__pca__n_components=2, features__univ_select__k=1, svm__C=1;, score=1.000 total time=   0.0s\n[CV 1/5; 9/18] START features__pca__n_components=2, features__univ_select__k=1, svm__C=10\n[CV 1/5; 9/18] END features__pca__n_components=2, features__univ_select__k=1, svm__C=10;, score=0.967 total time=   0.0s\n[CV 2/5; 9/18] START features__pca__n_components=2, features__univ_select__k=1, svm__C=10\n[CV 2/5; 9/18] END features__pca__n_components=2, features__univ_select__k=1, svm__C=10;, score=0.967 total time=   0.0s\n[CV 3/5; 9/18] START features__pca__n_components=2, features__univ_select__k=1, svm__C=10\n[CV 3/5; 9/18] END features__pca__n_components=2, features__univ_select__k=1, svm__C=10;, score=0.900 total time=   0.0s\n[CV 4/5; 9/18] START features__pca__n_components=2, features__univ_select__k=1, svm__C=10\n[CV 4/5; 9/18] END features__pca__n_components=2, features__univ_select__k=1, svm__C=10;, score=0.933 total time=   0.0s\n[CV 5/5; 9/18] START features__pca__n_components=2, features__univ_select__k=1, svm__C=10\n[CV 5/5; 9/18] END features__pca__n_components=2, features__univ_select__k=1, svm__C=10;, score=1.000 total time=   0.0s\n[CV 1/5; 10/18] START features__pca__n_components=2, features__univ_select__k=2, svm__C=0.1\n[CV 1/5; 10/18] END features__pca__n_components=2, features__univ_select__k=2, svm__C=0.1;, score=0.967 total time=   0.0s\n[CV 2/5; 10/18] START features__pca__n_components=2, features__univ_select__k=2, svm__C=0.1\n[CV 2/5; 10/18] END features__pca__n_components=2, features__univ_select__k=2, svm__C=0.1;, score=1.000 total time=   0.0s\n[CV 3/5; 10/18] START features__pca__n_components=2, features__univ_select__k=2, svm__C=0.1\n[CV 3/5; 10/18] END features__pca__n_components=2, features__univ_select__k=2, svm__C=0.1;, score=0.933 total time=   0.0s\n[CV 4/5; 10/18] START features__pca__n_components=2, features__univ_select__k=2, svm__C=0.1\n[CV 4/5; 10/18] END features__pca__n_components=2, features__univ_select__k=2, svm__C=0.1;, score=0.933 total time=   0.0s\n[CV 5/5; 10/18] START features__pca__n_components=2, features__univ_select__k=2, svm__C=0.1\n[CV 5/5; 10/18] END features__pca__n_components=2, features__univ_select__k=2, svm__C=0.1;, score=1.000 total time=   0.0s\n[CV 1/5; 11/18] START features__pca__n_components=2, features__univ_select__k=2, svm__C=1\n[CV 1/5; 11/18] END features__pca__n_components=2, features__univ_select__k=2, svm__C=1;, score=0.967 total time=   0.0s\n[CV 2/5; 11/18] START features__pca__n_components=2, features__univ_select__k=2, svm__C=1\n[CV 2/5; 11/18] END features__pca__n_components=2, features__univ_select__k=2, svm__C=1;, score=1.000 total time=   0.0s\n[CV 3/5; 11/18] START features__pca__n_components=2, features__univ_select__k=2, svm__C=1\n[CV 3/5; 11/18] END features__pca__n_components=2, features__univ_select__k=2, svm__C=1;, score=0.933 total time=   0.0s\n[CV 4/5; 11/18] START features__pca__n_components=2, features__univ_select__k=2, svm__C=1\n[CV 4/5; 11/18] END features__pca__n_components=2, features__univ_select__k=2, svm__C=1;, score=0.967 total time=   0.0s\n[CV 5/5; 11/18] START features__pca__n_components=2, features__univ_select__k=2, svm__C=1\n[CV 5/5; 11/18] END features__pca__n_components=2, features__univ_select__k=2, svm__C=1;, score=1.000 total time=   0.0s\n[CV 1/5; 12/18] START features__pca__n_components=2, features__univ_select__k=2, svm__C=10\n[CV 1/5; 12/18] END features__pca__n_components=2, features__univ_select__k=2, svm__C=10;, score=0.967 total time=   0.0s\n[CV 2/5; 12/18] START features__pca__n_components=2, features__univ_select__k=2, svm__C=10\n[CV 2/5; 12/18] END features__pca__n_components=2, features__univ_select__k=2, svm__C=10;, score=1.000 total time=   0.0s\n[CV 3/5; 12/18] START features__pca__n_components=2, features__univ_select__k=2, svm__C=10\n[CV 3/5; 12/18] END features__pca__n_components=2, features__univ_select__k=2, svm__C=10;, score=0.900 total time=   0.0s\n[CV 4/5; 12/18] START features__pca__n_components=2, features__univ_select__k=2, svm__C=10\n[CV 4/5; 12/18] END features__pca__n_components=2, features__univ_select__k=2, svm__C=10;, score=0.933 total time=   0.0s\n[CV 5/5; 12/18] START features__pca__n_components=2, features__univ_select__k=2, svm__C=10\n[CV 5/5; 12/18] END features__pca__n_components=2, features__univ_select__k=2, svm__C=10;, score=1.000 total time=   0.0s\n[CV 1/5; 13/18] START features__pca__n_components=3, features__univ_select__k=1, svm__C=0.1\n[CV 1/5; 13/18] END features__pca__n_components=3, features__univ_select__k=1, svm__C=0.1;, score=0.967 total time=   0.0s\n[CV 2/5; 13/18] START features__pca__n_components=3, features__univ_select__k=1, svm__C=0.1\n[CV 2/5; 13/18] END features__pca__n_components=3, features__univ_select__k=1, svm__C=0.1;, score=1.000 total time=   0.0s\n[CV 3/5; 13/18] START features__pca__n_components=3, features__univ_select__k=1, svm__C=0.1\n[CV 3/5; 13/18] END features__pca__n_components=3, features__univ_select__k=1, svm__C=0.1;, score=0.933 total time=   0.0s\n[CV 4/5; 13/18] START features__pca__n_components=3, features__univ_select__k=1, svm__C=0.1\n[CV 4/5; 13/18] END features__pca__n_components=3, features__univ_select__k=1, svm__C=0.1;, score=0.967 total time=   0.0s\n[CV 5/5; 13/18] START features__pca__n_components=3, features__univ_select__k=1, svm__C=0.1\n[CV 5/5; 13/18] END features__pca__n_components=3, features__univ_select__k=1, svm__C=0.1;, score=1.000 total time=   0.0s\n[CV 1/5; 14/18] START features__pca__n_components=3, features__univ_select__k=1, svm__C=1\n[CV 1/5; 14/18] END features__pca__n_components=3, features__univ_select__k=1, svm__C=1;, score=0.967 total time=   0.0s\n[CV 2/5; 14/18] START features__pca__n_components=3, features__univ_select__k=1, svm__C=1\n[CV 2/5; 14/18] END features__pca__n_components=3, features__univ_select__k=1, svm__C=1;, score=1.000 total time=   0.0s\n[CV 3/5; 14/18] START features__pca__n_components=3, features__univ_select__k=1, svm__C=1\n[CV 3/5; 14/18] END features__pca__n_components=3, features__univ_select__k=1, svm__C=1;, score=0.933 total time=   0.0s\n[CV 4/5; 14/18] START features__pca__n_components=3, features__univ_select__k=1, svm__C=1\n[CV 4/5; 14/18] END features__pca__n_components=3, features__univ_select__k=1, svm__C=1;, score=0.967 total time=   0.0s\n[CV 5/5; 14/18] START features__pca__n_components=3, features__univ_select__k=1, svm__C=1\n[CV 5/5; 14/18] END features__pca__n_components=3, features__univ_select__k=1, svm__C=1;, score=1.000 total time=   0.0s\n[CV 1/5; 15/18] START features__pca__n_components=3, features__univ_select__k=1, svm__C=10\n[CV 1/5; 15/18] END features__pca__n_components=3, features__univ_select__k=1, svm__C=10;, score=1.000 total time=   0.0s\n[CV 2/5; 15/18] START features__pca__n_components=3, features__univ_select__k=1, svm__C=10\n[CV 2/5; 15/18] END features__pca__n_components=3, features__univ_select__k=1, svm__C=10;, score=1.000 total time=   0.0s\n[CV 3/5; 15/18] START features__pca__n_components=3, features__univ_select__k=1, svm__C=10\n[CV 3/5; 15/18] END features__pca__n_components=3, features__univ_select__k=1, svm__C=10;, score=0.933 total time=   0.0s\n[CV 4/5; 15/18] START features__pca__n_components=3, features__univ_select__k=1, svm__C=10\n[CV 4/5; 15/18] END features__pca__n_components=3, features__univ_select__k=1, svm__C=10;, score=0.967 total time=   0.0s\n[CV 5/5; 15/18] START features__pca__n_components=3, features__univ_select__k=1, svm__C=10\n[CV 5/5; 15/18] END features__pca__n_components=3, features__univ_select__k=1, svm__C=10;, score=1.000 total time=   0.0s\n[CV 1/5; 16/18] START features__pca__n_components=3, features__univ_select__k=2, svm__C=0.1\n[CV 1/5; 16/18] END features__pca__n_components=3, features__univ_select__k=2, svm__C=0.1;, score=0.967 total time=   0.0s\n[CV 2/5; 16/18] START features__pca__n_components=3, features__univ_select__k=2, svm__C=0.1\n[CV 2/5; 16/18] END features__pca__n_components=3, features__univ_select__k=2, svm__C=0.1;, score=1.000 total time=   0.0s\n[CV 3/5; 16/18] START features__pca__n_components=3, features__univ_select__k=2, svm__C=0.1\n[CV 3/5; 16/18] END features__pca__n_components=3, features__univ_select__k=2, svm__C=0.1;, score=0.933 total time=   0.0s\n[CV 4/5; 16/18] START features__pca__n_components=3, features__univ_select__k=2, svm__C=0.1\n[CV 4/5; 16/18] END features__pca__n_components=3, features__univ_select__k=2, svm__C=0.1;, score=0.967 total time=   0.0s\n[CV 5/5; 16/18] START features__pca__n_components=3, features__univ_select__k=2, svm__C=0.1\n[CV 5/5; 16/18] END features__pca__n_components=3, features__univ_select__k=2, svm__C=0.1;, score=1.000 total time=   0.0s\n[CV 1/5; 17/18] START features__pca__n_components=3, features__univ_select__k=2, svm__C=1\n[CV 1/5; 17/18] END features__pca__n_components=3, features__univ_select__k=2, svm__C=1;, score=0.967 total time=   0.0s\n[CV 2/5; 17/18] START features__pca__n_components=3, features__univ_select__k=2, svm__C=1\n[CV 2/5; 17/18] END features__pca__n_components=3, features__univ_select__k=2, svm__C=1;, score=1.000 total time=   0.0s\n[CV 3/5; 17/18] START features__pca__n_components=3, features__univ_select__k=2, svm__C=1\n[CV 3/5; 17/18] END features__pca__n_components=3, features__univ_select__k=2, svm__C=1;, score=0.967 total time=   0.0s\n[CV 4/5; 17/18] START features__pca__n_components=3, features__univ_select__k=2, svm__C=1\n[CV 4/5; 17/18] END features__pca__n_components=3, features__univ_select__k=2, svm__C=1;, score=0.967 total time=   0.0s\n[CV 5/5; 17/18] START features__pca__n_components=3, features__univ_select__k=2, svm__C=1\n[CV 5/5; 17/18] END features__pca__n_components=3, features__univ_select__k=2, svm__C=1;, score=1.000 total time=   0.0s\n[CV 1/5; 18/18] START features__pca__n_components=3, features__univ_select__k=2, svm__C=10\n[CV 1/5; 18/18] END features__pca__n_components=3, features__univ_select__k=2, svm__C=10;, score=1.000 total time=   0.0s\n[CV 2/5; 18/18] START features__pca__n_components=3, features__univ_select__k=2, svm__C=10\n[CV 2/5; 18/18] END features__pca__n_components=3, features__univ_select__k=2, svm__C=10;, score=1.000 total time=   0.0s\n[CV 3/5; 18/18] START features__pca__n_components=3, features__univ_select__k=2, svm__C=10\n[CV 3/5; 18/18] END features__pca__n_components=3, features__univ_select__k=2, svm__C=10;, score=0.900 total time=   0.0s\n[CV 4/5; 18/18] START features__pca__n_components=3, features__univ_select__k=2, svm__C=10\n[CV 4/5; 18/18] END features__pca__n_components=3, features__univ_select__k=2, svm__C=10;, score=0.967 total time=   0.0s\n[CV 5/5; 18/18] START features__pca__n_components=3, features__univ_select__k=2, svm__C=10\n[CV 5/5; 18/18] END features__pca__n_components=3, features__univ_select__k=2, svm__C=10;, score=1.000 total time=   0.0s\nPipeline(steps=[('features',\n                 FeatureUnion(transformer_list=[('pca', PCA(n_components=3)),\n                                                ('univ_select',\n                                                 SelectKBest(k=1))])),\n                ('svm', SVC(C=10, kernel='linear'))])\n\n\n\n<br/>",
            "code"
        ],
        [
            "# Author: Andreas Mueller &lt;amueller@ais.uni-bonn.de\n#\n# License: BSD 3 clause\n\nfrom sklearn.pipeline import , \nfrom sklearn.model_selection import \nfrom sklearn.svm import \nfrom sklearn.datasets import \nfrom sklearn.decomposition import \nfrom sklearn.feature_selection import \n\niris = ()\n\nX, y = iris.data, iris.target\n\n# This dataset is way too high-dimensional. Better do PCA:\npca = (n_components=2)\n\n# Maybe some original features were good, too?\nselection = (k=1)\n\n# Build estimator from PCA and Univariate selection:\n\ncombined_features = ([(\"pca\", pca), (\"univ_select\", selection)])\n\n# Use combined features to transform dataset:\nX_features = combined_features.fit(X, y).transform(X)\nprint(\"Combined space has\", X_features.shape[1], \"features\")\n\nsvm = (kernel=\"linear\")\n\n# Do grid search over k, n_components and C:\n\npipeline = ([(\"features\", combined_features), (\"svm\", svm)])\n\nparam_grid = dict(\n    features__pca__n_components=[1, 2, 3],\n    features__univ_select__k=[1, 2],\n    svm__C=[0.1, 1, 10],\n)\n\ngrid_search = (pipeline, param_grid=param_grid, verbose=10)\ngrid_search.fit(X, y)\nprint(grid_search.best_estimator_)",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.366 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Pipelines and composite estimators->Effect of transforming the targets in regression model": [
        [
            "In this example, we give an overview of\n. We use two examples\nto illustrate the benefit of transforming the targets before learning a linear\nregression model. The first example uses synthetic data while the second\nexample is based on the Ames housing data set.",
            "markdown"
        ],
        [
            "# Author: Guillaume Lemaitre &lt;guillaume.lemaitre@inria.fr\n# License: BSD 3 clause\n\nprint(__doc__)",
            "code"
        ]
    ],
    "Examples->Pipelines and composite estimators->Effect of transforming the targets in regression model->Synthetic example": [
        [
            "A synthetic random regression dataset is generated. The targets y are\nmodified by:",
            "markdown"
        ],
        [
            "translating all targets such that all entries are\nnon-negative (by adding the absolute value of the lowest y) and",
            "markdown"
        ],
        [
            "applying an exponential function to obtain non-linear\ntargets which cannot be fitted using a simple linear model.\n\n</blockquote>",
            "markdown"
        ],
        [
            "Therefore, a logarithmic (np.log1p) and an exponential function\n(np.expm1) will be used to transform the targets before training a linear\nregression model and using it for prediction.\n</blockquote>",
            "markdown"
        ],
        [
            "import numpy as np\nfrom sklearn.datasets import \n\nX, y = (n_samples=10_000, noise=100, random_state=0)\ny = ((y + abs(y.min())) / 200)\ny_trans = (y)",
            "code"
        ],
        [
            "Below we plot the probability density functions of the target\nbefore and after applying the logarithmic functions.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\nfrom sklearn.model_selection import \n\nf, (ax0, ax1) = (1, 2)\n\nax0.hist(y, bins=100, density=True)\nax0.set_xlim([0, 2000])\nax0.set_ylabel(\"Probability\")\nax0.set_xlabel(\"Target\")\nax0.set_title(\"Target distribution\")\n\nax1.hist(y_trans, bins=100, density=True)\nax1.set_ylabel(\"Probability\")\nax1.set_xlabel(\"Target\")\nax1.set_title(\"Transformed target distribution\")\n\nf.suptitle(\"Synthetic data\", y=1.05)\n()\n\nX_train, X_test, y_train, y_test = (X, y, random_state=0)\n\n\n<img alt=\"Synthetic data, Target distribution, Transformed target distribution\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_transformed_target_001.png\" srcset=\"../../_images/sphx_glr_plot_transformed_target_001.png\"/>",
            "code"
        ],
        [
            "At first, a linear model will be applied on the original targets. Due to the\nnon-linearity, the model trained will not be precise during\nprediction. Subsequently, a logarithmic function is used to linearize the\ntargets, allowing better prediction even with a similar linear model as\nreported by the median absolute error (MedAE).",
            "markdown"
        ],
        [
            "from sklearn.metrics import , \n\n\ndef compute_score(y_true, y_pred):\n    return {\n        \"R2\": f\"{(y_true, y_pred):.3f}\",\n        \"MedAE\": f\"{(y_true, y_pred):.3f}\",\n    }",
            "code"
        ],
        [
            "from sklearn.compose import \nfrom sklearn.linear_model import \nfrom sklearn.metrics import PredictionErrorDisplay\n\nf, (ax0, ax1) = (1, 2, sharey=True)\n\nridge_cv = ().fit(X_train, y_train)\ny_pred_ridge = ridge_cv.predict(X_test)\n\nridge_cv_with_trans_target = (\n    regressor=(), func=, inverse_func=\n).fit(X_train, y_train)\ny_pred_ridge_with_trans_target = ridge_cv_with_trans_target.predict(X_test)\n\n(\n    y_test,\n    y_pred_ridge,\n    kind=\"actual_vs_predicted\",\n    ax=ax0,\n    scatter_kwargs={\"alpha\": 0.5},\n)\n(\n    y_test,\n    y_pred_ridge_with_trans_target,\n    kind=\"actual_vs_predicted\",\n    ax=ax1,\n    scatter_kwargs={\"alpha\": 0.5},\n)\n\n# Add the score in the legend of each axis\nfor ax, y_pred in zip([ax0, ax1], [y_pred_ridge, y_pred_ridge_with_trans_target]):\n    for name, score in compute_score(y_test, y_pred).items():\n        ax.plot([], [], \" \", label=f\"{name}={score}\")\n    ax.legend(loc=\"upper left\")\n\nax0.set_title(\"Ridge regression \\n without target transformation\")\nax1.set_title(\"Ridge regression \\n with target transformation\")\nf.suptitle(\"Synthetic data\", y=1.05)\n()\n\n\n<img alt=\"Synthetic data, Ridge regression   without target transformation, Ridge regression   with target transformation\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_transformed_target_002.png\" srcset=\"../../_images/sphx_glr_plot_transformed_target_002.png\"/>",
            "code"
        ]
    ],
    "Examples->Pipelines and composite estimators->Effect of transforming the targets in regression model->Real-world data set": [
        [
            "In a similar manner, the Ames housing data set is used to show the impact\nof transforming the targets before learning a model. In this example, the\ntarget to be predicted is the selling price of each house.\n</blockquote>",
            "markdown"
        ],
        [
            "from sklearn.datasets import \nfrom sklearn.preprocessing import \n\names = (name=\"house_prices\", as_frame=True, parser=\"pandas\")\n# Keep only numeric columns\nX = ames.data.select_dtypes()\n# Remove columns with NaN or Inf values\nX = X.drop(columns=[\"LotFrontage\", \"GarageYrBlt\", \"MasVnrArea\"])\n# Let the price be in k$\ny = ames.target / 1000\ny_trans = (\n    y.to_frame(), n_quantiles=900, output_distribution=\"normal\", copy=True\n).squeeze()",
            "code"
        ],
        [
            "A  is used to normalize\nthe target distribution before applying a\n model.",
            "markdown"
        ],
        [
            "f, (ax0, ax1) = (1, 2)\n\nax0.hist(y, bins=100, density=True)\nax0.set_ylabel(\"Probability\")\nax0.set_xlabel(\"Target\")\nax0.set_title(\"Target distribution\")\n\nax1.hist(y_trans, bins=100, density=True)\nax1.set_ylabel(\"Probability\")\nax1.set_xlabel(\"Target\")\nax1.set_title(\"Transformed target distribution\")\n\nf.suptitle(\"Ames housing data: selling price\", y=1.05)\n()\n\n\n<img alt=\"Ames housing data: selling price, Target distribution, Transformed target distribution\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_transformed_target_003.png\" srcset=\"../../_images/sphx_glr_plot_transformed_target_003.png\"/>",
            "code"
        ],
        [
            "X_train, X_test, y_train, y_test = (X, y, random_state=1)",
            "code"
        ],
        [
            "The effect of the transformer is weaker than on the synthetic data. However,\nthe transformation results in an increase in \\(R^2\\) and large decrease\nof the MedAE. The residual plot (predicted target - true target vs predicted\ntarget) without target transformation takes on a curved, \u2018reverse smile\u2019\nshape due to residual values that vary depending on the value of predicted\ntarget. With target transformation, the shape is more linear indicating\nbetter model fit.",
            "markdown"
        ],
        [
            "from sklearn.preprocessing import \n\nf, (ax0, ax1) = (2, 2, sharey=\"row\", figsize=(6.5, 8))\n\nridge_cv = ().fit(X_train, y_train)\ny_pred_ridge = ridge_cv.predict(X_test)\n\nridge_cv_with_trans_target = (\n    regressor=(),\n    transformer=(n_quantiles=900, output_distribution=\"normal\"),\n).fit(X_train, y_train)\ny_pred_ridge_with_trans_target = ridge_cv_with_trans_target.predict(X_test)\n\n# plot the actual vs predicted values\n(\n    y_test,\n    y_pred_ridge,\n    kind=\"actual_vs_predicted\",\n    ax=ax0[0],\n    scatter_kwargs={\"alpha\": 0.5},\n)\n(\n    y_test,\n    y_pred_ridge_with_trans_target,\n    kind=\"actual_vs_predicted\",\n    ax=ax0[1],\n    scatter_kwargs={\"alpha\": 0.5},\n)\n\n# Add the score in the legend of each axis\nfor ax, y_pred in zip([ax0[0], ax0[1]], [y_pred_ridge, y_pred_ridge_with_trans_target]):\n    for name, score in compute_score(y_test, y_pred).items():\n        ax.plot([], [], \" \", label=f\"{name}={score}\")\n    ax.legend(loc=\"upper left\")\n\nax0[0].set_title(\"Ridge regression \\n without target transformation\")\nax0[1].set_title(\"Ridge regression \\n with target transformation\")\n\n# plot the residuals vs the predicted values\n(\n    y_test,\n    y_pred_ridge,\n    kind=\"residual_vs_predicted\",\n    ax=ax1[0],\n    scatter_kwargs={\"alpha\": 0.5},\n)\n(\n    y_test,\n    y_pred_ridge_with_trans_target,\n    kind=\"residual_vs_predicted\",\n    ax=ax1[1],\n    scatter_kwargs={\"alpha\": 0.5},\n)\nax1[0].set_title(\"Ridge regression \\n without target transformation\")\nax1[1].set_title(\"Ridge regression \\n with target transformation\")\n\nf.suptitle(\"Ames housing data: selling price\", y=1.05)\n()\n()\n\n\n<img alt=\"Ames housing data: selling price, Ridge regression   without target transformation, Ridge regression   with target transformation, Ridge regression   without target transformation, Ridge regression   with target transformation\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_transformed_target_004.png\" srcset=\"../../_images/sphx_glr_plot_transformed_target_004.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  1.168 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Pipelines and composite estimators->Pipelining: chaining a PCA and a logistic regression": [
        [
            "The PCA does an unsupervised dimensionality reduction, while the logistic\nregression does the prediction.",
            "markdown"
        ],
        [
            "We use a GridSearchCV to set the dimensionality of the PCA\n<img alt=\"plot digits pipe\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_digits_pipe_001.png\" srcset=\"../../_images/sphx_glr_plot_digits_pipe_001.png\"/>",
            "markdown"
        ],
        [
            "Best parameter (CV score=0.924):\n{'logistic__C': 0.046415888336127774, 'pca__n_components': 60}\n\n\n\n<br/>",
            "code"
        ],
        [
            "# Code source: Ga\u00ebl Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom sklearn import datasets\nfrom sklearn.decomposition import \nfrom sklearn.linear_model import \nfrom sklearn.pipeline import \nfrom sklearn.model_selection import \nfrom sklearn.preprocessing import \n\n# Define a pipeline to search for the best combination of PCA truncation\n# and classifier regularization.\npca = ()\n# Define a Standard Scaler to normalize inputs\nscaler = ()\n\n# set the tolerance to a large value to make the example faster\nlogistic = (max_iter=10000, tol=0.1)\npipe = (steps=[(\"scaler\", scaler), (\"pca\", pca), (\"logistic\", logistic)])\n\nX_digits, y_digits = (return_X_y=True)\n# Parameters of pipelines can be set using '__' separated parameter names:\nparam_grid = {\n    \"pca__n_components\": [5, 15, 30, 45, 60],\n    \"logistic__C\": (-4, 4, 4),\n}\nsearch = (pipe, param_grid, n_jobs=2)\nsearch.fit(X_digits, y_digits)\nprint(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\nprint(search.best_params_)\n\n# Plot the PCA spectrum\npca.fit(X_digits)\n\nfig, (ax0, ax1) = (nrows=2, sharex=True, figsize=(6, 6))\nax0.plot(\n    (1, pca.n_components_ + 1), pca.explained_variance_ratio_, \"+\", linewidth=2\n)\nax0.set_ylabel(\"PCA explained variance ratio\")\n\nax0.axvline(\n    search.best_estimator_.named_steps[\"pca\"].n_components,\n    linestyle=\":\",\n    label=\"n_components chosen\",\n)\nax0.legend(prop=dict(size=12))\n\n# For each number of components, find the best classifier results\nresults = (search.cv_results_)\ncomponents_col = \"param_pca__n_components\"\nbest_clfs = results.groupby(components_col).apply(\n    lambda g: g.nlargest(1, \"mean_test_score\")\n)\n\nbest_clfs.plot(\n    x=components_col, y=\"mean_test_score\", yerr=\"std_test_score\", legend=False, ax=ax1\n)\nax1.set_ylabel(\"Classification accuracy (val)\")\nax1.set_xlabel(\"n_components\")\n\n(-1, 70)\n\n()\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  3.929 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Pipelines and composite estimators->Selecting dimensionality reduction with Pipeline and GridSearchCV": [
        [
            "This example constructs a pipeline that does dimensionality\nreduction followed by prediction with a support vector\nclassifier. It demonstrates the use of GridSearchCV and\nPipeline to optimize over different classes of estimators in a\nsingle CV run \u2013 unsupervised PCA and NMF dimensionality\nreductions are compared to univariate feature selection during\nthe grid search.",
            "markdown"
        ],
        [
            "Additionally, Pipeline can be instantiated with the memory\nargument to memoize the transformers within the pipeline, avoiding to fit\nagain the same transformers over and over.",
            "markdown"
        ],
        [
            "Note that the use of memory to enable caching becomes interesting when the\nfitting of a transformer is costly.",
            "markdown"
        ],
        [
            "# Authors: Robert McGibbon\n#          Joel Nothman\n#          Guillaume Lemaitre",
            "code"
        ]
    ],
    "Examples->Pipelines and composite estimators->Selecting dimensionality reduction with Pipeline and GridSearchCV->Illustration of Pipeline and GridSearchCV": [
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import \nfrom sklearn.model_selection import \nfrom sklearn.pipeline import \nfrom sklearn.svm import \nfrom sklearn.decomposition import , \nfrom sklearn.feature_selection import , \nfrom sklearn.preprocessing import \n\nX, y = (return_X_y=True)\n\npipe = (\n    [\n        (\"scaling\", ()),\n        # the reduce_dim stage is populated by the param_grid\n        (\"reduce_dim\", \"passthrough\"),\n        (\"classify\", (dual=False, max_iter=10000)),\n    ]\n)\n\nN_FEATURES_OPTIONS = [2, 4, 8]\nC_OPTIONS = [1, 10, 100, 1000]\nparam_grid = [\n    {\n        \"reduce_dim\": [(iterated_power=7), (max_iter=1_000)],\n        \"reduce_dim__n_components\": N_FEATURES_OPTIONS,\n        \"classify__C\": C_OPTIONS,\n    },\n    {\n        \"reduce_dim\": [()],\n        \"reduce_dim__k\": N_FEATURES_OPTIONS,\n        \"classify__C\": C_OPTIONS,\n    },\n]\nreducer_labels = [\"PCA\", \"NMF\", \"KBest(mutual_info_classif)\"]\n\ngrid = (pipe, n_jobs=1, param_grid=param_grid)\ngrid.fit(X, y)",
            "code"
        ],
        [
            "GridSearchCV(estimator=Pipeline(steps=[('scaling', MinMaxScaler()),\n                                       ('reduce_dim', 'passthrough'),\n                                       ('classify',\n                                        LinearSVC(dual=False,\n                                                  max_iter=10000))]),\n             n_jobs=1,\n             param_grid=[{'classify__C': [1, 10, 100, 1000],\n                          'reduce_dim': [PCA(iterated_power=7, n_components=8),\n                                         NMF(max_iter=1000)],\n                          'reduce_dim__n_components': [2, 4, 8]},\n                         {'classify__C': [1, 10, 100, 1000],\n                          'reduce_dim': [SelectKBest(score_func=&lt;function mutual_info_classif at 0x7f0d7038c280)],\n                          'reduce_dim__k': [2, 4, 8]}])<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-264\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-264\">GridSearchCV</label>",
            "code"
        ],
        [
            "GridSearchCV(estimator=Pipeline(steps=[('scaling', MinMaxScaler()),\n                                       ('reduce_dim', 'passthrough'),\n                                       ('classify',\n                                        LinearSVC(dual=False,\n                                                  max_iter=10000))]),\n             n_jobs=1,\n             param_grid=[{'classify__C': [1, 10, 100, 1000],\n                          'reduce_dim': [PCA(iterated_power=7, n_components=8),\n                                         NMF(max_iter=1000)],\n                          'reduce_dim__n_components': [2, 4, 8]},\n                         {'classify__C': [1, 10, 100, 1000],\n                          'reduce_dim': [SelectKBest(score_func=&lt;function mutual_info_classif at 0x7f0d7038c280)],\n                          'reduce_dim__k': [2, 4, 8]}])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-265\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-265\">estimator: Pipeline</label>",
            "code"
        ],
        [
            "Pipeline(steps=[('scaling', MinMaxScaler()), ('reduce_dim', 'passthrough'),\n                ('classify', LinearSVC(dual=False, max_iter=10000))])<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-266\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-266\">MinMaxScaler</label>",
            "code"
        ],
        [
            "MinMaxScaler()<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-267\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-267\">passthrough</label>",
            "code"
        ],
        [
            "passthrough<input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-268\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-268\">LinearSVC</label>",
            "code"
        ],
        [
            "LinearSVC(dual=False, max_iter=10000)\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "import pandas as pd\n\nmean_scores = (grid.cv_results_[\"mean_test_score\"])\n# scores are in the order of param_grid iteration, which is alphabetical\nmean_scores = mean_scores.reshape(len(C_OPTIONS), -1, len(N_FEATURES_OPTIONS))\n# select score for best C\nmean_scores = mean_scores.max(axis=0)\n# create a dataframe to ease plotting\nmean_scores = (\n    mean_scores.T, index=N_FEATURES_OPTIONS, columns=reducer_labels\n)\n\nax = mean_scores.plot.bar()\nax.set_title(\"Comparing feature reduction techniques\")\nax.set_xlabel(\"Reduced number of features\")\nax.set_ylabel(\"Digit classification accuracy\")\nax.set_ylim((0, 1))\nax.legend(loc=\"upper left\")\n\n()\n\n\n<img alt=\"Comparing feature reduction techniques\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_compare_reduction_001.png\" srcset=\"../../_images/sphx_glr_plot_compare_reduction_001.png\"/>",
            "code"
        ]
    ],
    "Examples->Pipelines and composite estimators->Selecting dimensionality reduction with Pipeline and GridSearchCV->Caching transformers within a Pipeline": [
        [
            "It is sometimes worthwhile storing the state of a specific transformer\nsince it could be used again. Using a pipeline in GridSearchCV triggers\nsuch situations. Therefore, we use the argument memory to enable caching.",
            "markdown"
        ],
        [
            "Warning",
            "markdown"
        ],
        [
            "Note that this example is, however, only an illustration since for this\nspecific case fitting PCA is not necessarily slower than loading the\ncache. Hence, use the memory constructor parameter when the fitting\nof a transformer is costly.\n\n</blockquote>",
            "markdown"
        ],
        [
            "from joblib import \nfrom shutil import \n\n# Create a temporary folder to store the transformers of the pipeline\nlocation = \"cachedir\"\nmemory = (location=location, verbose=10)\ncached_pipe = (\n    [(\"reduce_dim\", ()), (\"classify\", (dual=False, max_iter=10000))],\n    memory=memory,\n)\n\n# This time, a cached pipeline will be used within the grid search\n\n\n# Delete the temporary cache before exiting\nmemory.clear(warn=False)\n(location)",
            "code"
        ],
        [
            "The PCA fitting is only computed at the evaluation of the first\nconfiguration of the C parameter of the LinearSVC classifier. The\nother configurations of C will trigger the loading of the cached PCA\nestimator data, leading to save processing time. Therefore, the use of\ncaching the pipeline using memory is highly beneficial when fitting\na transformer is costly.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  43.276 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Preprocessing->Compare the effect of different scalers on data with outliers": [
        [
            "Feature 0 (median income in a block) and feature 5 (average house occupancy) of\nthe  have very\ndifferent scales and contain some very large outliers. These two\ncharacteristics lead to difficulties to visualize the data and, more\nimportantly, they can degrade the predictive performance of many machine\nlearning algorithms. Unscaled data can also slow down or even prevent the\nconvergence of many gradient-based estimators.",
            "markdown"
        ],
        [
            "Indeed many estimators are designed with the assumption that each feature takes\nvalues close to zero or more importantly that all features vary on comparable\nscales. In particular, metric-based and gradient-based estimators often assume\napproximately standardized data (centered features with unit variances). A\nnotable exception are decision tree-based estimators that are robust to\narbitrary scaling of the data.",
            "markdown"
        ],
        [
            "This example uses different scalers, transformers, and normalizers to bring the\ndata within a pre-defined range.",
            "markdown"
        ],
        [
            "Scalers are linear (or more precisely affine) transformers and differ from each\nother in the way they estimate the parameters used to shift and scale each\nfeature.",
            "markdown"
        ],
        [
            "provides non-linear\ntransformations in which distances\nbetween marginal outliers and inliers are shrunk.\n provides\nnon-linear transformations in which data is mapped to a normal distribution to\nstabilize variance and minimize skewness.",
            "markdown"
        ],
        [
            "Unlike the previous transformations, normalization refers to a per sample\ntransformation instead of a per feature transformation.",
            "markdown"
        ],
        [
            "The following code is a bit verbose, feel free to jump directly to the analysis\nof the .",
            "markdown"
        ],
        [
            "# Author:  Raghav RV &lt;rvraghav93@gmail.com\n#          Guillaume Lemaitre &lt;g.lemaitre58@gmail.com\n#          Thomas Unterthiner\n# License: BSD 3 clause\n\nimport numpy as np\n\nimport matplotlib as mpl\nfrom matplotlib import pyplot as plt\nfrom matplotlib import \n\nfrom sklearn.preprocessing import \nfrom sklearn.preprocessing import \nfrom sklearn.preprocessing import \nfrom sklearn.preprocessing import \nfrom sklearn.preprocessing import \nfrom sklearn.preprocessing import \nfrom sklearn.preprocessing import \nfrom sklearn.preprocessing import \n\nfrom sklearn.datasets import \n\ndataset = ()\nX_full, y_full = dataset.data, dataset.target\nfeature_names = dataset.feature_names\n\nfeature_mapping = {\n    \"MedInc\": \"Median income in block\",\n    \"HousAge\": \"Median house age in block\",\n    \"AveRooms\": \"Average number of rooms\",\n    \"AveBedrms\": \"Average number of bedrooms\",\n    \"Population\": \"Block population\",\n    \"AveOccup\": \"Average house occupancy\",\n    \"Latitude\": \"House block latitude\",\n    \"Longitude\": \"House block longitude\",\n}\n\n# Take only 2 features to make visualization easier\n# Feature MedInc has a long tail distribution.\n# Feature AveOccup has a few but very large outliers.\nfeatures = [\"MedInc\", \"AveOccup\"]\nfeatures_idx = [feature_names.index(feature) for feature in features]\nX = X_full[:, features_idx]\ndistributions = [\n    (\"Unscaled data\", X),\n    (\"Data after standard scaling\", ().fit_transform(X)),\n    (\"Data after min-max scaling\", ().fit_transform(X)),\n    (\"Data after max-abs scaling\", ().fit_transform(X)),\n    (\n        \"Data after robust scaling\",\n        (quantile_range=(25, 75)).fit_transform(X),\n    ),\n    (\n        \"Data after power transformation (Yeo-Johnson)\",\n        (method=\"yeo-johnson\").fit_transform(X),\n    ),\n    (\n        \"Data after power transformation (Box-Cox)\",\n        (method=\"box-cox\").fit_transform(X),\n    ),\n    (\n        \"Data after quantile transformation (uniform pdf)\",\n        (output_distribution=\"uniform\").fit_transform(X),\n    ),\n    (\n        \"Data after quantile transformation (gaussian pdf)\",\n        (output_distribution=\"normal\").fit_transform(X),\n    ),\n    (\"Data after sample-wise L2 normalizing\", ().fit_transform(X)),\n]\n\n# scale the output between 0 and 1 for the colorbar\ny = (y_full)\n\n# plasma does not exist in matplotlib &lt; 1.5\ncmap = getattr(, \"plasma_r\", .hot_r)\n\n\ndef create_axes(title, figsize=(16, 6)):\n    fig = (figsize=figsize)\n    fig.suptitle(title)\n\n    # define the axis for the first plot\n    left, width = 0.1, 0.22\n    bottom, height = 0.1, 0.7\n    bottom_h = height + 0.15\n    left_h = left + width + 0.02\n\n    rect_scatter = [left, bottom, width, height]\n    rect_histx = [left, bottom_h, width, 0.1]\n    rect_histy = [left_h, bottom, 0.05, height]\n\n    ax_scatter = (rect_scatter)\n    ax_histx = (rect_histx)\n    ax_histy = (rect_histy)\n\n    # define the axis for the zoomed-in plot\n    left = width + left + 0.2\n    left_h = left + width + 0.02\n\n    rect_scatter = [left, bottom, width, height]\n    rect_histx = [left, bottom_h, width, 0.1]\n    rect_histy = [left_h, bottom, 0.05, height]\n\n    ax_scatter_zoom = (rect_scatter)\n    ax_histx_zoom = (rect_histx)\n    ax_histy_zoom = (rect_histy)\n\n    # define the axis for the colorbar\n    left, width = width + left + 0.13, 0.01\n\n    rect_colorbar = [left, bottom, width, height]\n    ax_colorbar = (rect_colorbar)\n\n    return (\n        (ax_scatter, ax_histy, ax_histx),\n        (ax_scatter_zoom, ax_histy_zoom, ax_histx_zoom),\n        ax_colorbar,\n    )\n\n\ndef plot_distribution(axes, X, y, hist_nbins=50, title=\"\", x0_label=\"\", x1_label=\"\"):\n    ax, hist_X1, hist_X0 = axes\n\n    ax.set_title(title)\n    ax.set_xlabel(x0_label)\n    ax.set_ylabel(x1_label)\n\n    # The scatter plot\n    colors = cmap(y)\n    ax.scatter(X[:, 0], X[:, 1], alpha=0.5, marker=\"o\", s=5, lw=0, c=colors)\n\n    # Removing the top and the right spine for aesthetics\n    # make nice axis layout\n    ax.spines[\"top\"].set_visible(False)\n    ax.spines[\"right\"].set_visible(False)\n    ax.get_xaxis().tick_bottom()\n    ax.get_yaxis().tick_left()\n    ax.spines[\"left\"].set_position((\"outward\", 10))\n    ax.spines[\"bottom\"].set_position((\"outward\", 10))\n\n    # Histogram for axis X1 (feature 5)\n    hist_X1.set_ylim(ax.get_ylim())\n    hist_X1.hist(\n        X[:, 1], bins=hist_nbins, orientation=\"horizontal\", color=\"grey\", ec=\"grey\"\n    )\n    hist_X1.axis(\"off\")\n\n    # Histogram for axis X0 (feature 0)\n    hist_X0.set_xlim(ax.get_xlim())\n    hist_X0.hist(\n        X[:, 0], bins=hist_nbins, orientation=\"vertical\", color=\"grey\", ec=\"grey\"\n    )\n    hist_X0.axis(\"off\")",
            "code"
        ],
        [
            "Two plots will be shown for each scaler/normalizer/transformer. The left\nfigure will show a scatter plot of the full data set while the right figure\nwill exclude the extreme values considering only 99 % of the data set,\nexcluding marginal outliers. In addition, the marginal distributions for each\nfeature will be shown on the sides of the scatter plot.",
            "markdown"
        ],
        [
            "def make_plot(item_idx):\n    title, X = distributions[item_idx]\n    ax_zoom_out, ax_zoom_in, ax_colorbar = create_axes(title)\n    axarr = (ax_zoom_out, ax_zoom_in)\n    plot_distribution(\n        axarr[0],\n        X,\n        y,\n        hist_nbins=200,\n        x0_label=feature_mapping[features[0]],\n        x1_label=feature_mapping[features[1]],\n        title=\"Full data\",\n    )\n\n    # zoom-in\n    zoom_in_percentile_range = (0, 99)\n    cutoffs_X0 = (X[:, 0], zoom_in_percentile_range)\n    cutoffs_X1 = (X[:, 1], zoom_in_percentile_range)\n\n    non_outliers_mask = (X  [cutoffs_X0[0], cutoffs_X1[0]], axis=1) & (\n        X &lt; [cutoffs_X0[1], cutoffs_X1[1]], axis=1\n    )\n    plot_distribution(\n        axarr[1],\n        X[non_outliers_mask],\n        y[non_outliers_mask],\n        hist_nbins=50,\n        x0_label=feature_mapping[features[0]],\n        x1_label=feature_mapping[features[1]],\n        title=\"Zoom-in\",\n    )\n\n    norm = (y_full.min(), y_full.max())\n    (\n        ax_colorbar,\n        cmap=cmap,\n        norm=norm,\n        orientation=\"vertical\",\n        label=\"Color mapping for values of y\",\n    )",
            "code"
        ]
    ],
    "Examples->Preprocessing->Compare the effect of different scalers on data with outliers->Original data": [
        [
            "Each transformation is plotted showing two transformed features, with the\nleft plot showing the entire dataset, and the right zoomed-in to show the\ndataset without the marginal outliers. A large majority of the samples are\ncompacted to a specific range, [0, 10] for the median income and [0, 6] for\nthe average house occupancy. Note that there are some marginal outliers (some\nblocks have average occupancy of more than 1200). Therefore, a specific\npre-processing can be very beneficial depending of the application. In the\nfollowing, we present some insights and behaviors of those pre-processing\nmethods in the presence of marginal outliers.",
            "markdown"
        ],
        [
            "make_plot(0)\n\n\n<img alt=\"Unscaled data, Full data, Zoom-in\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_all_scaling_001.png\" srcset=\"../../_images/sphx_glr_plot_all_scaling_001.png\"/>",
            "code"
        ]
    ],
    "Examples->Preprocessing->Compare the effect of different scalers on data with outliers->StandardScaler": [
        [
            "removes the mean and scales\nthe data to unit variance. The scaling shrinks the range of the feature\nvalues as shown in the left figure below.\nHowever, the outliers have an influence when computing the empirical mean and\nstandard deviation. Note in particular that because the outliers on each\nfeature have different magnitudes, the spread of the transformed data on\neach feature is very different: most of the data lie in the [-2, 4] range for\nthe transformed median income feature while the same data is squeezed in the\nsmaller [-0.2, 0.2] range for the transformed average house occupancy.",
            "markdown"
        ],
        [
            "therefore cannot guarantee\nbalanced feature scales in the\npresence of outliers.",
            "markdown"
        ],
        [
            "make_plot(1)\n\n\n<img alt=\"Data after standard scaling, Full data, Zoom-in\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_all_scaling_002.png\" srcset=\"../../_images/sphx_glr_plot_all_scaling_002.png\"/>",
            "code"
        ]
    ],
    "Examples->Preprocessing->Compare the effect of different scalers on data with outliers->MinMaxScaler": [
        [
            "rescales the data set such that\nall feature values are in\nthe range [0, 1] as shown in the right panel below. However, this scaling\ncompresses all inliers into the narrow range [0, 0.005] for the transformed\naverage house occupancy.",
            "markdown"
        ],
        [
            "Both  and\n are very sensitive to the\npresence of outliers.",
            "markdown"
        ],
        [
            "make_plot(2)\n\n\n<img alt=\"Data after min-max scaling, Full data, Zoom-in\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_all_scaling_003.png\" srcset=\"../../_images/sphx_glr_plot_all_scaling_003.png\"/>",
            "code"
        ]
    ],
    "Examples->Preprocessing->Compare the effect of different scalers on data with outliers->MaxAbsScaler": [
        [
            "is similar to\n except that the\nvalues are mapped across several ranges depending on whether negative\nOR positive values are present. If only positive values are present, the\nrange is [0, 1]. If only negative values are present, the range is [-1, 0].\nIf both negative and positive values are present, the range is [-1, 1].\nOn positive only data, both \nand  behave similarly.\n therefore also suffers from\nthe presence of large outliers.",
            "markdown"
        ],
        [
            "make_plot(3)\n\n\n<img alt=\"Data after max-abs scaling, Full data, Zoom-in\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_all_scaling_004.png\" srcset=\"../../_images/sphx_glr_plot_all_scaling_004.png\"/>",
            "code"
        ]
    ],
    "Examples->Preprocessing->Compare the effect of different scalers on data with outliers->RobustScaler": [
        [
            "Unlike the previous scalers, the centering and scaling statistics of\n\nare based on percentiles and are therefore not influenced by a small\nnumber of very large marginal outliers. Consequently, the resulting range of\nthe transformed feature values is larger than for the previous scalers and,\nmore importantly, are approximately similar: for both features most of the\ntransformed values lie in a [-2, 3] range as seen in the zoomed-in figure.\nNote that the outliers themselves are still present in the transformed data.\nIf a separate outlier clipping is desirable, a non-linear transformation is\nrequired (see below).",
            "markdown"
        ],
        [
            "make_plot(4)\n\n\n<img alt=\"Data after robust scaling, Full data, Zoom-in\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_all_scaling_005.png\" srcset=\"../../_images/sphx_glr_plot_all_scaling_005.png\"/>",
            "code"
        ]
    ],
    "Examples->Preprocessing->Compare the effect of different scalers on data with outliers->PowerTransformer": [
        [
            "applies a power\ntransformation to each feature to make the data more Gaussian-like in order\nto stabilize variance and minimize skewness. Currently the Yeo-Johnson\nand Box-Cox transforms are supported and the optimal\nscaling factor is determined via maximum likelihood estimation in both\nmethods. By default,  applies\nzero-mean, unit variance normalization. Note that\nBox-Cox can only be applied to strictly positive data. Income and average\nhouse occupancy happen to be strictly positive, but if negative values are\npresent the Yeo-Johnson transformed is preferred.",
            "markdown"
        ],
        [
            "make_plot(5)\nmake_plot(6)\n\n\n\n<img alt=\"Data after power transformation (Yeo-Johnson), Full data, Zoom-in\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_all_scaling_006.png\" srcset=\"../../_images/sphx_glr_plot_all_scaling_006.png\"/>\n<img alt=\"Data after power transformation (Box-Cox), Full data, Zoom-in\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_all_scaling_007.png\" srcset=\"../../_images/sphx_glr_plot_all_scaling_007.png\"/>",
            "code"
        ]
    ],
    "Examples->Preprocessing->Compare the effect of different scalers on data with outliers->QuantileTransformer (uniform output)": [
        [
            "applies a non-linear\ntransformation such that the\nprobability density function of each feature will be mapped to a uniform\nor Gaussian distribution. In this case, all the data, including outliers,\nwill be mapped to a uniform distribution with the range [0, 1], making\noutliers indistinguishable from inliers.",
            "markdown"
        ],
        [
            "and\n are robust to outliers in\nthe sense that adding or removing outliers in the training set will yield\napproximately the same transformation. But contrary to\n,\n will also automatically\ncollapse any outlier by setting them to the a priori defined range boundaries\n(0 and 1). This can result in saturation artifacts for extreme values.",
            "markdown"
        ],
        [
            "make_plot(7)\n\n\n<img alt=\"Data after quantile transformation (uniform pdf), Full data, Zoom-in\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_all_scaling_008.png\" srcset=\"../../_images/sphx_glr_plot_all_scaling_008.png\"/>",
            "code"
        ]
    ],
    "Examples->Preprocessing->Compare the effect of different scalers on data with outliers->QuantileTransformer (Gaussian output)": [
        [
            "To map to a Gaussian distribution, set the parameter\noutput_distribution='normal'.",
            "markdown"
        ],
        [
            "make_plot(8)\n\n\n<img alt=\"Data after quantile transformation (gaussian pdf), Full data, Zoom-in\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_all_scaling_009.png\" srcset=\"../../_images/sphx_glr_plot_all_scaling_009.png\"/>",
            "code"
        ]
    ],
    "Examples->Preprocessing->Compare the effect of different scalers on data with outliers->Normalizer": [
        [
            "The  rescales the vector for each\nsample to have unit norm,\nindependently of the distribution of the samples. It can be seen on both\nfigures below where all samples are mapped onto the unit circle. In our\nexample the two selected features have only positive values; therefore the\ntransformed data only lie in the positive quadrant. This would not be the\ncase if some original features had a mix of positive and negative values.",
            "markdown"
        ],
        [
            "make_plot(9)\n\n()\n\n\n<img alt=\"Data after sample-wise L2 normalizing, Full data, Zoom-in\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_all_scaling_010.png\" srcset=\"../../_images/sphx_glr_plot_all_scaling_010.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  8.424 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Preprocessing->Demonstrating the different strategies of KBinsDiscretizer": [
        [
            "This example presents the different strategies implemented in KBinsDiscretizer:",
            "markdown"
        ],
        [
            "\u2018uniform\u2019: The discretization is uniform in each feature, which means that\nthe bin widths are constant in each dimension.",
            "markdown"
        ],
        [
            "quantile\u2019: The discretization is done on the quantiled values, which means\nthat each bin has approximately the same number of samples.",
            "markdown"
        ],
        [
            "\u2018kmeans\u2019: The discretization is based on the centroids of a KMeans clustering\nprocedure.",
            "markdown"
        ],
        [
            "The plot shows the regions where the discretized encoding is constant.\n<img alt=\"Input data, strategy='uniform', strategy='quantile', strategy='kmeans'\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_discretization_strategies_001.png\" srcset=\"../../_images/sphx_glr_plot_discretization_strategies_001.png\"/>",
            "markdown"
        ],
        [
            "# Author: Tom Dupr\u00e9 la Tour\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import \nfrom sklearn.datasets import \n\nstrategies = [\"uniform\", \"quantile\", \"kmeans\"]\n\nn_samples = 200\ncenters_0 = ([[0, 0], [0, 5], [2, 4], [8, 8]])\ncenters_1 = ([[0, 0], [3, 1]])\n\n# construct the datasets\nrandom_state = 42\nX_list = [\n    (random_state).uniform(-3, 3, size=(n_samples, 2)),\n    (\n        n_samples=[\n            n_samples // 10,\n            n_samples * 4 // 10,\n            n_samples // 10,\n            n_samples * 4 // 10,\n        ],\n        cluster_std=0.5,\n        centers=centers_0,\n        random_state=random_state,\n    )[0],\n    (\n        n_samples=[n_samples // 5, n_samples * 4 // 5],\n        cluster_std=0.5,\n        centers=centers_1,\n        random_state=random_state,\n    )[0],\n]\n\nfigure = (figsize=(14, 9))\ni = 1\nfor ds_cnt, X in enumerate(X_list):\n\n    ax = (len(X_list), len(strategies) + 1, i)\n    ax.scatter(X[:, 0], X[:, 1], edgecolors=\"k\")\n    if ds_cnt == 0:\n        ax.set_title(\"Input data\", size=14)\n\n    xx, yy = (\n        (X[:, 0].min(), X[:, 0].max(), 300),\n        (X[:, 1].min(), X[:, 1].max(), 300),\n    )\n    grid = [xx.ravel(), yy.ravel()]\n\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n    ax.set_xticks(())\n    ax.set_yticks(())\n\n    i += 1\n    # transform the dataset with KBinsDiscretizer\n    for strategy in strategies:\n        enc = (n_bins=4, encode=\"ordinal\", strategy=strategy)\n        enc.fit(X)\n        grid_encoded = enc.transform(grid)\n\n        ax = (len(X_list), len(strategies) + 1, i)\n\n        # horizontal stripes\n        horizontal = grid_encoded[:, 0].reshape(xx.shape)\n        ax.contourf(xx, yy, horizontal, alpha=0.5)\n        # vertical stripes\n        vertical = grid_encoded[:, 1].reshape(xx.shape)\n        ax.contourf(xx, yy, vertical, alpha=0.5)\n\n        ax.scatter(X[:, 0], X[:, 1], edgecolors=\"k\")\n        ax.set_xlim(xx.min(), xx.max())\n        ax.set_ylim(yy.min(), yy.max())\n        ax.set_xticks(())\n        ax.set_yticks(())\n        if ds_cnt == 0:\n            ax.set_title(\"strategy='%s'\" % (strategy,), size=14)\n\n        i += 1\n\n()\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.660 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Preprocessing->Feature discretization": [
        [
            "A demonstration of feature discretization on synthetic classification datasets.\nFeature discretization decomposes each feature into a set of bins, here equally\ndistributed in width. The discrete values are then one-hot encoded, and given\nto a linear classifier. This preprocessing enables a non-linear behavior even\nthough the classifier is linear.",
            "markdown"
        ],
        [
            "On this example, the first two rows represent linearly non-separable datasets\n(moons and concentric circles) while the third is approximately linearly\nseparable. On the two linearly non-separable datasets, feature discretization\nlargely increases the performance of linear classifiers. On the linearly\nseparable dataset, feature discretization decreases the performance of linear\nclassifiers. Two non-linear classifiers are also shown for comparison.",
            "markdown"
        ],
        [
            "This example should be taken with a grain of salt, as the intuition conveyed\ndoes not necessarily carry over to real datasets. Particularly in\nhigh-dimensional spaces, data can more easily be separated linearly. Moreover,\nusing feature discretization and one-hot encoding increases the number of\nfeatures, which easily lead to overfitting when the number of samples is small.",
            "markdown"
        ],
        [
            "The plots show training points in solid colors and testing points\nsemi-transparent. The lower right shows the classification accuracy on the test\nset.\n<img alt=\"Input data, LogisticRegression, LinearSVC, KBinsDiscretizer LogisticRegression, KBinsDiscretizer LinearSVC, GradientBoostingClassifier, SVC\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_discretization_classification_001.png\" srcset=\"../../_images/sphx_glr_plot_discretization_classification_001.png\"/>",
            "markdown"
        ],
        [
            "dataset 0\n---------\nLogisticRegression: 0.86\nLinearSVC: 0.86\nKBinsDiscretizer + LogisticRegression: 0.86\nKBinsDiscretizer + LinearSVC: 0.94\nGradientBoostingClassifier: 0.90\nSVC: 0.94\n\ndataset 1\n---------\nLogisticRegression: 0.40\nLinearSVC: 0.40\nKBinsDiscretizer + LogisticRegression: 0.78\nKBinsDiscretizer + LinearSVC: 0.80\nGradientBoostingClassifier: 0.84\nSVC: 0.84\n\ndataset 2\n---------\nLogisticRegression: 0.98\nLinearSVC: 0.96\nKBinsDiscretizer + LogisticRegression: 0.94\nKBinsDiscretizer + LinearSVC: 0.94\nGradientBoostingClassifier: 0.94\nSVC: 0.98\n\n\n\n<br/>",
            "code"
        ],
        [
            "# Code source: Tom Dupr\u00e9 la Tour\n# Adapted from plot_classifier_comparison by Ga\u00ebl Varoquaux and Andreas M\u00fcller\n#\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import \nfrom sklearn.model_selection import \nfrom sklearn.preprocessing import \nfrom sklearn.datasets import , , \nfrom sklearn.linear_model import \nfrom sklearn.model_selection import \nfrom sklearn.pipeline import \nfrom sklearn.preprocessing import \nfrom sklearn.svm import , \nfrom sklearn.ensemble import \nfrom sklearn.utils._testing import ignore_warnings\nfrom sklearn.exceptions import \n\nh = 0.02  # step size in the mesh\n\n\ndef get_name(estimator):\n    name = estimator.__class__.__name__\n    if name == \"Pipeline\":\n        name = [get_name(est[1]) for est in estimator.steps]\n        name = \" + \".join(name)\n    return name\n\n\n# list of (estimator, param_grid), where param_grid is used in GridSearchCV\n# The parameter spaces in this example are limited to a narrow band to reduce\n# its runtime. In a real use case, a broader search space for the algorithms\n# should be used.\nclassifiers = [\n    (\n        ((), (random_state=0)),\n        {\"logisticregression__C\": (-1, 1, 3)},\n    ),\n    (\n        ((), (random_state=0)),\n        {\"linearsvc__C\": (-1, 1, 3)},\n    ),\n    (\n        (\n            (),\n            (encode=\"onehot\"),\n            (random_state=0),\n        ),\n        {\n            \"kbinsdiscretizer__n_bins\": (5, 8),\n            \"logisticregression__C\": (-1, 1, 3),\n        },\n    ),\n    (\n        (\n            (),\n            (encode=\"onehot\"),\n            (random_state=0),\n        ),\n        {\n            \"kbinsdiscretizer__n_bins\": (5, 8),\n            \"linearsvc__C\": (-1, 1, 3),\n        },\n    ),\n    (\n        (\n            (), (n_estimators=5, random_state=0)\n        ),\n        {\"gradientboostingclassifier__learning_rate\": (-2, 0, 5)},\n    ),\n    (\n        ((), (random_state=0)),\n        {\"svc__C\": (-1, 1, 3)},\n    ),\n]\n\nnames = [get_name(e).replace(\"StandardScaler + \", \"\") for e, _ in classifiers]\n\nn_samples = 100\ndatasets = [\n    (n_samples=n_samples, noise=0.2, random_state=0),\n    (n_samples=n_samples, noise=0.2, factor=0.5, random_state=1),\n    (\n        n_samples=n_samples,\n        n_features=2,\n        n_redundant=0,\n        n_informative=2,\n        random_state=2,\n        n_clusters_per_class=1,\n    ),\n]\n\nfig, axes = (\n    nrows=len(datasets), ncols=len(classifiers) + 1, figsize=(21, 9)\n)\n\ncm_piyg = plt.cm.PiYG\ncm_bright = ([\"#b30065\", \"#178000\"])\n\n# iterate over datasets\nfor ds_cnt, (X, y) in enumerate(datasets):\n    print(f\"\\ndataset {ds_cnt}\\n---------\")\n\n    # split into training and test part\n    X_train, X_test, y_train, y_test = (\n        X, y, test_size=0.5, random_state=42\n    )\n\n    # create the grid for background colors\n    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n    xx, yy = ((x_min, x_max, h), (y_min, y_max, h))\n\n    # plot the dataset first\n    ax = axes[ds_cnt, 0]\n    if ds_cnt == 0:\n        ax.set_title(\"Input data\")\n    # plot the training points\n    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\")\n    # and testing points\n    ax.scatter(\n        X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6, edgecolors=\"k\"\n    )\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n    ax.set_xticks(())\n    ax.set_yticks(())\n\n    # iterate over classifiers\n    for est_idx, (name, (estimator, param_grid)) in enumerate(zip(names, classifiers)):\n        ax = axes[ds_cnt, est_idx + 1]\n\n        clf = (estimator=estimator, param_grid=param_grid)\n        with ignore_warnings(category=):\n            clf.fit(X_train, y_train)\n        score = clf.score(X_test, y_test)\n        print(f\"{name}: {score:.2f}\")\n\n        # plot the decision boundary. For that, we will assign a color to each\n        # point in the mesh [x_min, x_max]*[y_min, y_max].\n        if hasattr(clf, \"decision_function\"):\n            Z = clf.decision_function(([xx.ravel(), yy.ravel()]))\n        else:\n            Z = clf.predict_proba(([xx.ravel(), yy.ravel()]))[:, 1]\n\n        # put the result into a color plot\n        Z = Z.reshape(xx.shape)\n        ax.contourf(xx, yy, Z, cmap=cm_piyg, alpha=0.8)\n\n        # plot the training points\n        ax.scatter(\n            X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\"\n        )\n        # and testing points\n        ax.scatter(\n            X_test[:, 0],\n            X_test[:, 1],\n            c=y_test,\n            cmap=cm_bright,\n            edgecolors=\"k\",\n            alpha=0.6,\n        )\n        ax.set_xlim(xx.min(), xx.max())\n        ax.set_ylim(yy.min(), yy.max())\n        ax.set_xticks(())\n        ax.set_yticks(())\n\n        if ds_cnt == 0:\n            ax.set_title(name.replace(\" + \", \"\\n\"))\n        ax.text(\n            0.95,\n            0.06,\n            (f\"{score:.2f}\").lstrip(\"0\"),\n            size=15,\n            bbox=dict(boxstyle=\"round\", alpha=0.8, facecolor=\"white\"),\n            transform=ax.transAxes,\n            horizontalalignment=\"right\",\n        )\n\n\n()\n\n# Add suptitles above the figure\n(top=0.90)\nsuptitles = [\n    \"Linear classifiers\",\n    \"Feature discretization and linear classifiers\",\n    \"Non-linear classifiers\",\n]\nfor i, suptitle in zip([1, 3, 5], suptitles):\n    ax = axes[0, i]\n    ax.text(\n        1.05,\n        1.25,\n        suptitle,\n        transform=ax.transAxes,\n        horizontalalignment=\"center\",\n        size=\"x-large\",\n    )\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  3.237 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Preprocessing->Importance of Feature Scaling": [
        [
            "Feature scaling through standardization, also called Z-score normalization, is\nan important preprocessing step for many machine learning algorithms. It\ninvolves rescaling each feature such that it has a standard deviation of 1 and a\nmean of 0.",
            "markdown"
        ],
        [
            "Even if tree based models are (almost) not affected by scaling, many other\nalgorithms require features to be normalized, often for different reasons: to\nease the convergence (such as a non-penalized logistic regression), to create a\ncompletely different model fit compared to the fit with unscaled data (such as\nKNeighbors models). The latter is demoed on the first part of the present\nexample.",
            "markdown"
        ],
        [
            "On the second part of the example we show how Principle Component Analysis (PCA)\nis impacted by normalization of features. To illustrate this, we compare the\nprincipal components found using  on unscaled\ndata with those obatined when using a\n to scale data first.",
            "markdown"
        ],
        [
            "In the last part of the example we show the effect of the normalization on the\naccuracy of a model trained on PCA-reduced data.",
            "markdown"
        ],
        [
            "# Author: Tyler Lanigan &lt;tylerlanigan@gmail.com\n#         Sebastian Raschka &lt;mail@sebastianraschka.com\n#         Arturo Amor &lt;david-arturo.amor-quiroz@inria.fr\n# License: BSD 3 clause",
            "code"
        ]
    ],
    "Examples->Preprocessing->Importance of Feature Scaling->Load and prepare data": [
        [
            "The dataset used is the  available at UCI. This dataset has\ncontinuous features that are heterogeneous in scale due to differing\nproperties that they measure (e.g. alcohol content and malic acid).",
            "markdown"
        ],
        [
            "from sklearn.datasets import \nfrom sklearn.model_selection import \nfrom sklearn.preprocessing import \n\nX, y = (return_X_y=True, as_frame=True)\nscaler = ().set_output(transform=\"pandas\")\n\nX_train, X_test, y_train, y_test = (\n    X, y, test_size=0.30, random_state=42\n)\nscaled_X_train = scaler.fit_transform(X_train)",
            "code"
        ]
    ],
    "Examples->Preprocessing->Importance of Feature Scaling->Effect of rescaling on a k-neighbors models": [
        [
            "For the sake of visualizing the decision boundary of a\n, in this section we select a\nsubset of 2 features that have values with different orders of magnitude.",
            "markdown"
        ],
        [
            "Keep in mind that using a subset of the features to train the model may likely\nleave out feature with high predictive impact, resulting in a decision\nboundary that is much worse in comparison to a model trained on the full set\nof features.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.neighbors import \n\n\nX_plot = X[[\"proline\", \"hue\"]]\nX_plot_scaled = scaler.fit_transform(X_plot)\nclf = (n_neighbors=20)\n\n\ndef fit_and_plot_model(X_plot, y, clf, ax):\n    clf.fit(X_plot, y)\n    disp = (\n        clf,\n        X_plot,\n        response_method=\"predict\",\n        alpha=0.5,\n        ax=ax,\n    )\n    disp.ax_.scatter(X_plot[\"proline\"], X_plot[\"hue\"], c=y, s=20, edgecolor=\"k\")\n    disp.ax_.set_xlim((X_plot[\"proline\"].min(), X_plot[\"proline\"].max()))\n    disp.ax_.set_ylim((X_plot[\"hue\"].min(), X_plot[\"hue\"].max()))\n    return disp.ax_\n\n\nfig, (ax1, ax2) = (ncols=2, figsize=(12, 6))\n\nfit_and_plot_model(X_plot, y, clf, ax1)\nax1.set_title(\"KNN without scaling\")\n\nfit_and_plot_model(X_plot_scaled, y, clf, ax2)\nax2.set_xlabel(\"scaled proline\")\nax2.set_ylabel(\"scaled hue\")\n_ = ax2.set_title(\"KNN with scaling\")\n\n\n<img alt=\"KNN without scaling, KNN with scaling\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_scaling_importance_001.png\" srcset=\"../../_images/sphx_glr_plot_scaling_importance_001.png\"/>",
            "code"
        ],
        [
            "Here the desicion boundary shows that fitting scaled or non-scaled data lead\nto completely different models. The reason is that the variable \u201cproline\u201d has\nvalues which vary between 0 and 1,000; whereas the variable \u201chue\u201d varies\nbetween 1 and 10. Because of this, distances between samples are mostly\nimpacted by differences in values of \u201cproline\u201d, while values of the \u201chue\u201d will\nbe comparatively ignored. If one uses\n to normalize this database,\nboth scaled values lay approximately between -3 and 3 and the neighbors\nstructure will be impacted more or less equivalently by both variables.",
            "markdown"
        ]
    ],
    "Examples->Preprocessing->Importance of Feature Scaling->Effect of rescaling on a PCA dimensional reduction": [
        [
            "Dimensional reduction using  consists of\nfinding the features that maximize the variance. If one feature varies more\nthan the others only because of their respective scales,\n would determine that such feature\ndominates the direction of the principal components.",
            "markdown"
        ],
        [
            "We can inspect the first principal components using all the original features:",
            "markdown"
        ],
        [
            "import pandas as pd\nfrom sklearn.decomposition import \n\npca = (n_components=2).fit(X_train)\nscaled_pca = (n_components=2).fit(scaled_X_train)\nX_train_transformed = pca.transform(X_train)\nX_train_std_transformed = scaled_pca.transform(scaled_X_train)\n\nfirst_pca_component = (\n    pca.components_[0], index=X.columns, columns=[\"without scaling\"]\n)\nfirst_pca_component[\"with scaling\"] = scaled_pca.components_[0]\nfirst_pca_component.plot.bar(\n    title=\"Weights of the first principal component\", figsize=(6, 8)\n)\n\n_ = ()\n\n\n<img alt=\"Weights of the first principal component\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_scaling_importance_002.png\" srcset=\"../../_images/sphx_glr_plot_scaling_importance_002.png\"/>",
            "code"
        ],
        [
            "Indeed we find that the \u201cproline\u201d feature dominates the direction of the first\nprincipal component without scaling, being about two orders of magnitude above\nthe other features. This is contrasted when observing the first principal\ncomponent for the scaled version of the data, where the orders of magnitude\nare roughly the same across all the features.",
            "markdown"
        ],
        [
            "We can visualize the distribution of the principal components in both cases:",
            "markdown"
        ],
        [
            "fig, (ax1, ax2) = (nrows=1, ncols=2, figsize=(10, 5))\n\ntarget_classes = range(0, 3)\ncolors = (\"blue\", \"red\", \"green\")\nmarkers = (\"^\", \"s\", \"o\")\n\nfor target_class, color, marker in zip(target_classes, colors, markers):\n    ax1.scatter(\n        x=X_train_transformed[y_train == target_class, 0],\n        y=X_train_transformed[y_train == target_class, 1],\n        color=color,\n        label=f\"class {target_class}\",\n        alpha=0.5,\n        marker=marker,\n    )\n\n    ax2.scatter(\n        x=X_train_std_transformed[y_train == target_class, 0],\n        y=X_train_std_transformed[y_train == target_class, 1],\n        color=color,\n        label=f\"class {target_class}\",\n        alpha=0.5,\n        marker=marker,\n    )\n\nax1.set_title(\"Unscaled training dataset after PCA\")\nax2.set_title(\"Standardized training dataset after PCA\")\n\nfor ax in (ax1, ax2):\n    ax.set_xlabel(\"1st principal component\")\n    ax.set_ylabel(\"2nd principal component\")\n    ax.legend(loc=\"upper right\")\n    ax.grid()\n\n_ = ()\n\n\n<img alt=\"Unscaled training dataset after PCA, Standardized training dataset after PCA\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_scaling_importance_003.png\" srcset=\"../../_images/sphx_glr_plot_scaling_importance_003.png\"/>",
            "code"
        ],
        [
            "From the plot above we observe that scaling the features before reducing the\ndimensionality results in components with the same order of magnitude. In this\ncase it also improves the separability of the clases. Indeed, in the next\nsection we confirm that a better separability has a good repercussion on the\noverall model\u2019s performance.",
            "markdown"
        ]
    ],
    "Examples->Preprocessing->Importance of Feature Scaling->Effect of rescaling on model\u2019s performance": [
        [
            "First we show how the optimal regularization of a\n depends on the scaling or\nnon-scaling of the data:",
            "markdown"
        ],
        [
            "import numpy as np\nfrom sklearn.pipeline import \nfrom sklearn.linear_model import \n\nCs = (-5, 5, 20)\n\nunscaled_clf = (pca, (Cs=Cs))\nunscaled_clf.fit(X_train, y_train)\n\nscaled_clf = (scaler, pca, (Cs=Cs))\nscaled_clf.fit(X_train, y_train)\n\nprint(f\"Optimal C for the unscaled PCA: {unscaled_clf[-1].C_[0]:.4f}\\n\")\nprint(f\"Optimal C for the standardized data with PCA: {scaled_clf[-1].C_[0]:.2f}\")",
            "code"
        ],
        [
            "Optimal C for the unscaled PCA: 0.0004\n\nOptimal C for the standardized data with PCA: 20.69",
            "code"
        ],
        [
            "The need for regularization is higher (lower values of C) for the data that\nwas not scaled before applying PCA. We now evaluate the effect of scaling on\nthe accuracy and the mean log-loss of the optimal models:",
            "markdown"
        ],
        [
            "from sklearn.metrics import \nfrom sklearn.metrics import \n\ny_pred = unscaled_clf.predict(X_test)\ny_pred_scaled = scaled_clf.predict(X_test)\ny_proba = unscaled_clf.predict_proba(X_test)\ny_proba_scaled = scaled_clf.predict_proba(X_test)\n\nprint(\"Test accuracy for the unscaled PCA\")\nprint(f\"{(y_test, y_pred):.2%}\\n\")\nprint(\"Test accuracy for the standardized data with PCA\")\nprint(f\"{(y_test, y_pred_scaled):.2%}\\n\")\nprint(\"Log-loss for the unscaled PCA\")\nprint(f\"{(y_test, y_proba):.3}\\n\")\nprint(\"Log-loss for the standardized data with PCA\")\nprint(f\"{(y_test, y_proba_scaled):.3}\")",
            "code"
        ],
        [
            "Test accuracy for the unscaled PCA\n35.19%\n\nTest accuracy for the standardized data with PCA\n96.30%\n\nLog-loss for the unscaled PCA\n2.07\n\nLog-loss for the standardized data with PCA\n0.0824",
            "code"
        ],
        [
            "A clear difference in prediction accuracies is observed when the data is\nscaled before , as it vastly outperforms\nthe unscaled version. This corresponds to the intuition obtained from the plot\nin the previous section, where the components become linearly separable when\nscaling before using .",
            "markdown"
        ],
        [
            "Notice that in this case the models with scaled features perform better than\nthe models with non-scaled features because all the variables are expected to\nbe predictive and we rather avoid some of them being comparatively ignored.",
            "markdown"
        ],
        [
            "If the variables in lower scales were not predictive, one may experience a\ndecrease of the performance after scaling the features: noisy features would\ncontribute more to the prediction after scaling and therefore scaling would\nincrease overfitting.",
            "markdown"
        ],
        [
            "Last but not least, we observe that one achieves a lower log-loss by means of\nthe scaling step.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  1.386 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Preprocessing->Map data to a normal distribution": [
        [
            "This example demonstrates the use of the Box-Cox and Yeo-Johnson transforms\nthrough  to map data from various\ndistributions to a normal distribution.",
            "markdown"
        ],
        [
            "The power transform is useful as a transformation in modeling problems where\nhomoscedasticity and normality are desired. Below are examples of Box-Cox and\nYeo-Johnwon applied to six different probability distributions: Lognormal,\nChi-squared, Weibull, Gaussian, Uniform, and Bimodal.",
            "markdown"
        ],
        [
            "Note that the transformations successfully map the data to a normal\ndistribution when applied to certain datasets, but are ineffective with others.\nThis highlights the importance of visualizing the data before and after\ntransformation.",
            "markdown"
        ],
        [
            "Also note that even though Box-Cox seems to perform better than Yeo-Johnson for\nlognormal and chi-squared distributions, keep in mind that Box-Cox does not\nsupport inputs with negative values.",
            "markdown"
        ],
        [
            "For comparison, we also add the output from\n. It can force any arbitrary\ndistribution into a gaussian, provided that there are enough training samples\n(thousands). Because it is a non-parametric method, it is harder to interpret\nthan the parametric ones (Box-Cox and Yeo-Johnson).",
            "markdown"
        ],
        [
            "On \u201csmall\u201d datasets (less than a few hundred points), the quantile transformer\nis prone to overfitting. The use of the power transform is then recommended.\n<img alt=\"Lognormal, Chi-squared, Weibull, After Box-Cox $\\lambda$ = 0.07, After Box-Cox $\\lambda$ = 0.3, After Box-Cox $\\lambda$ = 13.1, After Yeo-Johnson $\\lambda$ = -0.76, After Yeo-Johnson $\\lambda$ = -0.1, After Yeo-Johnson $\\lambda$ = 25.51, After Quantile transform, After Quantile transform, After Quantile transform, Gaussian, Uniform, Bimodal, After Box-Cox $\\lambda$ = 4.48, After Box-Cox $\\lambda$ = 0.67, After Box-Cox $\\lambda$ = -0.61, After Yeo-Johnson $\\lambda$ = 4.51, After Yeo-Johnson $\\lambda$ = 0.78, After Yeo-Johnson $\\lambda$ = -0.62, After Quantile transform, After Quantile transform, After Quantile transform\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_map_data_to_normal_001.png\" srcset=\"../../_images/sphx_glr_plot_map_data_to_normal_001.png\"/>",
            "markdown"
        ],
        [
            "# Author: Eric Chang &lt;ericchang2017@u.northwestern.edu\n#         Nicolas Hug &lt;contact@nicolas-hug.com\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import \nfrom sklearn.preprocessing import \nfrom sklearn.model_selection import \n\n\nN_SAMPLES = 1000\nFONT_SIZE = 6\nBINS = 30\n\n\nrng = (304)\nbc = (method=\"box-cox\")\nyj = (method=\"yeo-johnson\")\n# n_quantiles is set to the training set size rather than the default value\n# to avoid a warning being raised by this example\nqt = (\n    n_quantiles=500, output_distribution=\"normal\", random_state=rng\n)\nsize = (N_SAMPLES, 1)\n\n\n# lognormal distribution\nX_lognormal = rng.lognormal(size=size)\n\n# chi-squared distribution\ndf = 3\nX_chisq = rng.chisquare(df=df, size=size)\n\n# weibull distribution\na = 50\nX_weibull = rng.weibull(a=a, size=size)\n\n# gaussian distribution\nloc = 100\nX_gaussian = rng.normal(loc=loc, size=size)\n\n# uniform distribution\nX_uniform = rng.uniform(low=0, high=1, size=size)\n\n# bimodal distribution\nloc_a, loc_b = 100, 105\nX_a, X_b = rng.normal(loc=loc_a, size=size), rng.normal(loc=loc_b, size=size)\nX_bimodal = ([X_a, X_b], axis=0)\n\n\n# create plots\ndistributions = [\n    (\"Lognormal\", X_lognormal),\n    (\"Chi-squared\", X_chisq),\n    (\"Weibull\", X_weibull),\n    (\"Gaussian\", X_gaussian),\n    (\"Uniform\", X_uniform),\n    (\"Bimodal\", X_bimodal),\n]\n\ncolors = [\"#D81B60\", \"#0188FF\", \"#FFC107\", \"#B7A2FF\", \"#000000\", \"#2EC5AC\"]\n\nfig, axes = (nrows=8, ncols=3, figsize=plt.figaspect(2))\naxes = axes.flatten()\naxes_idxs = [\n    (0, 3, 6, 9),\n    (1, 4, 7, 10),\n    (2, 5, 8, 11),\n    (12, 15, 18, 21),\n    (13, 16, 19, 22),\n    (14, 17, 20, 23),\n]\naxes_list = [(axes[i], axes[j], axes[k], axes[l]) for (i, j, k, l) in axes_idxs]\n\n\nfor distribution, color, axes in zip(distributions, colors, axes_list):\n    name, X = distribution\n    X_train, X_test = (X, test_size=0.5)\n\n    # perform power transforms and quantile transform\n    X_trans_bc = bc.fit(X_train).transform(X_test)\n    lmbda_bc = round(bc.lambdas_[0], 2)\n    X_trans_yj = yj.fit(X_train).transform(X_test)\n    lmbda_yj = round(yj.lambdas_[0], 2)\n    X_trans_qt = qt.fit(X_train).transform(X_test)\n\n    ax_original, ax_bc, ax_yj, ax_qt = axes\n\n    ax_original.hist(X_train, color=color, bins=BINS)\n    ax_original.set_title(name, fontsize=FONT_SIZE)\n    ax_original.tick_params(axis=\"both\", which=\"major\", labelsize=FONT_SIZE)\n\n    for ax, X_trans, meth_name, lmbda in zip(\n        (ax_bc, ax_yj, ax_qt),\n        (X_trans_bc, X_trans_yj, X_trans_qt),\n        (\"Box-Cox\", \"Yeo-Johnson\", \"Quantile transform\"),\n        (lmbda_bc, lmbda_yj, None),\n    ):\n        ax.hist(X_trans, color=color, bins=BINS)\n        title = \"After {}\".format(meth_name)\n        if lmbda is not None:\n            title += \"\\n$\\\\lambda$ = {}\".format(lmbda)\n        ax.set_title(title, fontsize=FONT_SIZE)\n        ax.tick_params(axis=\"both\", which=\"major\", labelsize=FONT_SIZE)\n        ax.set_xlim([-3.5, 3.5])\n\n\n()\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  1.616 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Preprocessing->Using KBinsDiscretizer to discretize continuous features": [
        [
            "The example compares prediction result of linear regression (linear model)\nand decision tree (tree based model) with and without discretization of\nreal-valued features.",
            "markdown"
        ],
        [
            "As is shown in the result before discretization, linear model is fast to\nbuild and relatively straightforward to interpret, but can only model\nlinear relationships, while decision tree can build a much more complex model\nof the data. One way to make linear model more powerful on continuous data\nis to use discretization (also known as binning). In the example, we\ndiscretize the feature and one-hot encode the transformed data. Note that if\nthe bins are not reasonably wide, there would appear to be a substantially\nincreased risk of overfitting, so the discretizer parameters should usually\nbe tuned under cross validation.",
            "markdown"
        ],
        [
            "After discretization, linear regression and decision tree make exactly the\nsame prediction. As features are constant within each bin, any model must\npredict the same value for all points within a bin. Compared with the result\nbefore discretization, linear model become much more flexible while decision\ntree gets much less flexible. Note that binning features generally has no\nbeneficial effect for tree-based models, as these models can learn to split\nup the data anywhere.\n<img alt=\"Result before discretization, Result after discretization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_discretization_001.png\" srcset=\"../../_images/sphx_glr_plot_discretization_001.png\"/>",
            "markdown"
        ],
        [
            "# Author: Andreas M\u00fcller\n#         Hanmin Qin &lt;qinhanmin2005@sina.com\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import \nfrom sklearn.preprocessing import \nfrom sklearn.tree import \n\n# construct the dataset\nrnd = (42)\nX = rnd.uniform(-3, 3, size=100)\ny = (X) + rnd.normal(size=len(X)) / 3\nX = X.reshape(-1, 1)\n\n# transform the dataset with KBinsDiscretizer\nenc = (n_bins=10, encode=\"onehot\")\nX_binned = enc.fit_transform(X)\n\n# predict with original dataset\nfig, (ax1, ax2) = (ncols=2, sharey=True, figsize=(10, 4))\nline = (-3, 3, 1000, endpoint=False).reshape(-1, 1)\nreg = ().fit(X, y)\nax1.plot(line, reg.predict(line), linewidth=2, color=\"green\", label=\"linear regression\")\nreg = (min_samples_split=3, random_state=0).fit(X, y)\nax1.plot(line, reg.predict(line), linewidth=2, color=\"red\", label=\"decision tree\")\nax1.plot(X[:, 0], y, \"o\", c=\"k\")\nax1.legend(loc=\"best\")\nax1.set_ylabel(\"Regression output\")\nax1.set_xlabel(\"Input feature\")\nax1.set_title(\"Result before discretization\")\n\n# predict with transformed dataset\nline_binned = enc.transform(line)\nreg = ().fit(X_binned, y)\nax2.plot(\n    line,\n    reg.predict(line_binned),\n    linewidth=2,\n    color=\"green\",\n    linestyle=\"-\",\n    label=\"linear regression\",\n)\nreg = (min_samples_split=3, random_state=0).fit(X_binned, y)\nax2.plot(\n    line,\n    reg.predict(line_binned),\n    linewidth=2,\n    color=\"red\",\n    linestyle=\":\",\n    label=\"decision tree\",\n)\nax2.plot(X[:, 0], y, \"o\", c=\"k\")\nax2.vlines(enc.bin_edges_[0], *().get_ylim(), linewidth=1, alpha=0.2)\nax2.legend(loc=\"best\")\nax2.set_xlabel(\"Input feature\")\nax2.set_title(\"Result after discretization\")\n\n()\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.165 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Semi Supervised Classification->Decision boundary of semi-supervised classifiers versus SVM on the Iris dataset": [
        [
            "A comparison for the decision boundaries generated on the iris dataset\nby Label Spreading, Self-training and SVM.",
            "markdown"
        ],
        [
            "This example demonstrates that Label Spreading and Self-training can learn\ngood boundaries even when small amounts of labeled data are available.",
            "markdown"
        ],
        [
            "Note that Self-training with 100% of the data is omitted as it is functionally\nidentical to training the SVC on 100% of the data.\n<img alt=\"Unlabeled points are colored white, Label Spreading 30% data, Self-training 30% data, Label Spreading 50% data, Self-training 50% data, Label Spreading 100% data, SVC with rbf kernel\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_semi_supervised_versus_svm_iris_001.png\" srcset=\"../../_images/sphx_glr_plot_semi_supervised_versus_svm_iris_001.png\"/>",
            "markdown"
        ],
        [
            "# Authors: Clay Woolam   &lt;clay@woolam.org\n#          Oliver Rausch &lt;rauscho@ethz.ch\n# License: BSD\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.svm import \nfrom sklearn.semi_supervised import \nfrom sklearn.semi_supervised import \n\n\niris = ()\n\nX = iris.data[:, :2]\ny = iris.target\n\n# step size in the mesh\nh = 0.02\n\nrng = (0)\ny_rand = rng.rand(y.shape[0])\ny_30 = (y)\ny_30[y_rand &lt; 0.3] = -1  # set random samples to be unlabeled\ny_50 = (y)\ny_50[y_rand &lt; 0.5] = -1\n# we create an instance of SVM and fit out data. We do not scale our\n# data since we want to plot the support vectors\nls30 = (().fit(X, y_30), y_30, \"Label Spreading 30% data\")\nls50 = (().fit(X, y_50), y_50, \"Label Spreading 50% data\")\nls100 = (().fit(X, y), y, \"Label Spreading 100% data\")\n\n# the base classifier for self-training is identical to the SVC\nbase_classifier = (kernel=\"rbf\", gamma=0.5, probability=True)\nst30 = (\n    (base_classifier).fit(X, y_30),\n    y_30,\n    \"Self-training 30% data\",\n)\nst50 = (\n    (base_classifier).fit(X, y_50),\n    y_50,\n    \"Self-training 50% data\",\n)\n\nrbf_svc = ((kernel=\"rbf\", gamma=0.5).fit(X, y), y, \"SVC with rbf kernel\")\n\n# create a mesh to plot in\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = ((x_min, x_max, h), (y_min, y_max, h))\n\ncolor_map = {-1: (1, 1, 1), 0: (0, 0, 0.9), 1: (1, 0, 0), 2: (0.8, 0.6, 0)}\n\nclassifiers = (ls30, st30, ls50, st50, ls100, rbf_svc)\nfor i, (clf, y_train, title) in enumerate(classifiers):\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    (3, 2, i + 1)\n    Z = clf.predict([xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    (xx, yy, Z, cmap=plt.cm.Paired)\n    (\"off\")\n\n    # Plot also the training points\n    colors = [color_map[y] for y in y_train]\n    (X[:, 0], X[:, 1], c=colors, edgecolors=\"black\")\n\n    (title)\n\n(\"Unlabeled points are colored white\", y=0.1)\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.978 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Semi Supervised Classification->Effect of varying threshold for self-training": [
        [
            "This example illustrates the effect of a varying threshold on self-training.\nThe breast_cancer dataset is loaded, and labels are deleted such that only 50\nout of 569 samples have labels. A SelfTrainingClassifier is fitted on this\ndataset, with varying thresholds.",
            "markdown"
        ],
        [
            "The upper graph shows the amount of labeled samples that the classifier has\navailable by the end of fit, and the accuracy of the classifier. The lower\ngraph shows the last iteration in which a sample was labeled. All values are\ncross validated with 3 folds.",
            "markdown"
        ],
        [
            "At low thresholds (in [0.4, 0.5]), the classifier learns from samples that were\nlabeled with a low confidence. These low-confidence samples are likely have\nincorrect predicted labels, and as a result, fitting on these incorrect labels\nproduces a poor accuracy. Note that the classifier labels almost all of the\nsamples, and only takes one iteration.",
            "markdown"
        ],
        [
            "For very high thresholds (in [0.9, 1)) we observe that the classifier does not\naugment its dataset (the amount of self-labeled samples is 0). As a result, the\naccuracy achieved with a threshold of 0.9999 is the same as a normal supervised\nclassifier would achieve.",
            "markdown"
        ],
        [
            "The optimal accuracy lies in between both of these extremes at a threshold of\naround 0.7.\n<img alt=\"plot self training varying threshold\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_self_training_varying_threshold_001.png\" srcset=\"../../_images/sphx_glr_plot_self_training_varying_threshold_001.png\"/>",
            "markdown"
        ],
        [
            "# Authors: Oliver Rausch &lt;rauscho@ethz.ch\n# License: BSD\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.svm import \nfrom sklearn.model_selection import \nfrom sklearn.semi_supervised import \nfrom sklearn.metrics import \nfrom sklearn.utils import \n\nn_splits = 3\n\nX, y = (return_X_y=True)\nX, y = (X, y, random_state=42)\ny_true = y.copy()\ny[50:] = -1\ntotal_samples = y.shape[0]\n\nbase_classifier = (probability=True, gamma=0.001, random_state=42)\n\nx_values = (0.4, 1.05, 0.05)\nx_values = (x_values, 0.99999)\nscores = ((x_values.shape[0], n_splits))\namount_labeled = ((x_values.shape[0], n_splits))\namount_iterations = ((x_values.shape[0], n_splits))\n\nfor i, threshold in enumerate(x_values):\n    self_training_clf = (base_classifier, threshold=threshold)\n\n    # We need manual cross validation so that we don't treat -1 as a separate\n    # class when computing accuracy\n    skfolds = (n_splits=n_splits)\n    for fold, (train_index, test_index) in enumerate(skfolds.split(X, y)):\n        X_train = X[train_index]\n        y_train = y[train_index]\n        X_test = X[test_index]\n        y_test = y[test_index]\n        y_test_true = y_true[test_index]\n\n        self_training_clf.fit(X_train, y_train)\n\n        # The amount of labeled samples that at the end of fitting\n        amount_labeled[i, fold] = (\n            total_samples\n            - (self_training_clf.labeled_iter_, return_counts=True)[1][0]\n        )\n        # The last iteration the classifier labeled a sample in\n        amount_iterations[i, fold] = np.max(self_training_clf.labeled_iter_)\n\n        y_pred = self_training_clf.predict(X_test)\n        scores[i, fold] = (y_test_true, y_pred)\n\n\nax1 = (211)\nax1.errorbar(\n    x_values, scores.mean(axis=1), yerr=scores.std(axis=1), capsize=2, color=\"b\"\n)\nax1.set_ylabel(\"Accuracy\", color=\"b\")\nax1.tick_params(\"y\", colors=\"b\")\n\nax2 = ax1.twinx()\nax2.errorbar(\n    x_values,\n    amount_labeled.mean(axis=1),\n    yerr=amount_labeled.std(axis=1),\n    capsize=2,\n    color=\"g\",\n)\nax2.set_ylim(bottom=0)\nax2.set_ylabel(\"Amount of labeled samples\", color=\"g\")\nax2.tick_params(\"y\", colors=\"g\")\n\nax3 = (212, sharex=ax1)\nax3.errorbar(\n    x_values,\n    amount_iterations.mean(axis=1),\n    yerr=amount_iterations.std(axis=1),\n    capsize=2,\n    color=\"b\",\n)\nax3.set_ylim(bottom=0)\nax3.set_ylabel(\"Amount of iterations\")\nax3.set_xlabel(\"Threshold\")\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  5.749 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Semi Supervised Classification->Label Propagation digits active learning": [
        [
            "Demonstrates an active learning technique to learn handwritten digits\nusing label propagation.",
            "markdown"
        ],
        [
            "We start by training a label propagation model with only 10 labeled points,\nthen we select the top five most uncertain points to label. Next, we train\nwith 15 labeled points (original 10 + 5 new ones). We repeat this process\nfour times to have a model trained with 30 labeled examples. Note you can\nincrease this to label more than 30 by changing max_iterations. Labeling\nmore than 30 can be useful to get a sense for the speed of convergence of\nthis active learning technique.",
            "markdown"
        ],
        [
            "A plot will appear showing the top 5 most uncertain digits for each iteration\nof training. These may or may not contain mistakes, but we will train the next\nmodel with their true labels.\n<img alt=\"Active learning with Label Propagation. Rows show 5 most uncertain labels to learn with the next model., predict: 1 true: 1, predict: 2 true: 1, predict: 1 true: 1, predict: 1 true: 1, predict: 3 true: 3, predict: 4 true: 4, predict: 4 true: 4, predict: 4 true: 4, predict: 8 true: 2, predict: 8 true: 7, predict: 2 true: 2, predict: 9 true: 5, predict: 9 true: 5, predict: 5 true: 9, predict: 7 true: 7, predict: 8 true: 8, predict: 1 true: 8, predict: 3 true: 3, predict: 4 true: 4, predict: 8 true: 8, predict: 1 true: 1, predict: 1 true: 1, predict: 7 true: 7, predict: 7 true: 7, predict: 1 true: 1\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_label_propagation_digits_active_learning_001.png\" srcset=\"../../_images/sphx_glr_plot_label_propagation_digits_active_learning_001.png\"/>",
            "markdown"
        ],
        [
            "Iteration 0 ______________________________________________________________________\nLabel Spreading model: 40 labeled & 290 unlabeled (330 total)\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        22\n           1       0.78      0.69      0.73        26\n           2       0.93      0.93      0.93        29\n           3       1.00      0.89      0.94        27\n           4       0.92      0.96      0.94        23\n           5       0.96      0.70      0.81        33\n           6       0.97      0.97      0.97        35\n           7       0.94      0.91      0.92        33\n           8       0.62      0.89      0.74        28\n           9       0.73      0.79      0.76        34\n\n    accuracy                           0.87       290\n   macro avg       0.89      0.87      0.87       290\nweighted avg       0.88      0.87      0.87       290\n\nConfusion matrix\n[[22  0  0  0  0  0  0  0  0  0]\n [ 0 18  2  0  0  0  1  0  5  0]\n [ 0  0 27  0  0  0  0  0  2  0]\n [ 0  0  0 24  0  0  0  0  3  0]\n [ 0  1  0  0 22  0  0  0  0  0]\n [ 0  0  0  0  0 23  0  0  0 10]\n [ 0  1  0  0  0  0 34  0  0  0]\n [ 0  0  0  0  0  0  0 30  3  0]\n [ 0  3  0  0  0  0  0  0 25  0]\n [ 0  0  0  0  2  1  0  2  2 27]]\nIteration 1 ______________________________________________________________________\nLabel Spreading model: 45 labeled & 285 unlabeled (330 total)\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        22\n           1       0.79      1.00      0.88        22\n           2       1.00      0.93      0.96        29\n           3       1.00      1.00      1.00        26\n           4       0.92      0.96      0.94        23\n           5       0.96      0.70      0.81        33\n           6       1.00      0.97      0.99        35\n           7       0.94      0.91      0.92        33\n           8       0.77      0.86      0.81        28\n           9       0.73      0.79      0.76        34\n\n    accuracy                           0.90       285\n   macro avg       0.91      0.91      0.91       285\nweighted avg       0.91      0.90      0.90       285\n\nConfusion matrix\n[[22  0  0  0  0  0  0  0  0  0]\n [ 0 22  0  0  0  0  0  0  0  0]\n [ 0  0 27  0  0  0  0  0  2  0]\n [ 0  0  0 26  0  0  0  0  0  0]\n [ 0  1  0  0 22  0  0  0  0  0]\n [ 0  0  0  0  0 23  0  0  0 10]\n [ 0  1  0  0  0  0 34  0  0  0]\n [ 0  0  0  0  0  0  0 30  3  0]\n [ 0  4  0  0  0  0  0  0 24  0]\n [ 0  0  0  0  2  1  0  2  2 27]]\nIteration 2 ______________________________________________________________________\nLabel Spreading model: 50 labeled & 280 unlabeled (330 total)\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        22\n           1       0.85      1.00      0.92        22\n           2       1.00      1.00      1.00        28\n           3       1.00      1.00      1.00        26\n           4       0.87      1.00      0.93        20\n           5       0.96      0.70      0.81        33\n           6       1.00      0.97      0.99        35\n           7       0.94      1.00      0.97        32\n           8       0.92      0.86      0.89        28\n           9       0.73      0.79      0.76        34\n\n    accuracy                           0.92       280\n   macro avg       0.93      0.93      0.93       280\nweighted avg       0.93      0.92      0.92       280\n\nConfusion matrix\n[[22  0  0  0  0  0  0  0  0  0]\n [ 0 22  0  0  0  0  0  0  0  0]\n [ 0  0 28  0  0  0  0  0  0  0]\n [ 0  0  0 26  0  0  0  0  0  0]\n [ 0  0  0  0 20  0  0  0  0  0]\n [ 0  0  0  0  0 23  0  0  0 10]\n [ 0  1  0  0  0  0 34  0  0  0]\n [ 0  0  0  0  0  0  0 32  0  0]\n [ 0  3  0  0  1  0  0  0 24  0]\n [ 0  0  0  0  2  1  0  2  2 27]]\nIteration 3 ______________________________________________________________________\nLabel Spreading model: 55 labeled & 275 unlabeled (330 total)\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        22\n           1       0.85      1.00      0.92        22\n           2       1.00      1.00      1.00        27\n           3       1.00      1.00      1.00        26\n           4       0.87      1.00      0.93        20\n           5       0.96      0.87      0.92        31\n           6       1.00      0.97      0.99        35\n           7       1.00      1.00      1.00        31\n           8       0.92      0.86      0.89        28\n           9       0.88      0.85      0.86        33\n\n    accuracy                           0.95       275\n   macro avg       0.95      0.95      0.95       275\nweighted avg       0.95      0.95      0.95       275\n\nConfusion matrix\n[[22  0  0  0  0  0  0  0  0  0]\n [ 0 22  0  0  0  0  0  0  0  0]\n [ 0  0 27  0  0  0  0  0  0  0]\n [ 0  0  0 26  0  0  0  0  0  0]\n [ 0  0  0  0 20  0  0  0  0  0]\n [ 0  0  0  0  0 27  0  0  0  4]\n [ 0  1  0  0  0  0 34  0  0  0]\n [ 0  0  0  0  0  0  0 31  0  0]\n [ 0  3  0  0  1  0  0  0 24  0]\n [ 0  0  0  0  2  1  0  0  2 28]]\nIteration 4 ______________________________________________________________________\nLabel Spreading model: 60 labeled & 270 unlabeled (330 total)\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        22\n           1       0.96      1.00      0.98        22\n           2       1.00      0.96      0.98        27\n           3       0.96      1.00      0.98        25\n           4       0.86      1.00      0.93        19\n           5       0.96      0.87      0.92        31\n           6       1.00      0.97      0.99        35\n           7       1.00      1.00      1.00        31\n           8       0.92      0.96      0.94        25\n           9       0.88      0.85      0.86        33\n\n    accuracy                           0.96       270\n   macro avg       0.95      0.96      0.96       270\nweighted avg       0.96      0.96      0.96       270\n\nConfusion matrix\n[[22  0  0  0  0  0  0  0  0  0]\n [ 0 22  0  0  0  0  0  0  0  0]\n [ 0  0 26  1  0  0  0  0  0  0]\n [ 0  0  0 25  0  0  0  0  0  0]\n [ 0  0  0  0 19  0  0  0  0  0]\n [ 0  0  0  0  0 27  0  0  0  4]\n [ 0  1  0  0  0  0 34  0  0  0]\n [ 0  0  0  0  0  0  0 31  0  0]\n [ 0  0  0  0  1  0  0  0 24  0]\n [ 0  0  0  0  2  1  0  0  2 28]]\n\n\n\n<br/>",
            "code"
        ],
        [
            "# Authors: Clay Woolam &lt;clay@woolam.org\n# License: BSD\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nfrom sklearn import datasets\nfrom sklearn.semi_supervised import \nfrom sklearn.metrics import , \n\ndigits = ()\nrng = (0)\nindices = (len(digits.data))\nrng.shuffle(indices)\n\nX = digits.data[indices[:330]]\ny = digits.target[indices[:330]]\nimages = digits.images[indices[:330]]\n\nn_total_samples = len(y)\nn_labeled_points = 40\nmax_iterations = 5\n\nunlabeled_indices = (n_total_samples)[n_labeled_points:]\nf = ()\n\nfor i in range(max_iterations):\n    if len(unlabeled_indices) == 0:\n        print(\"No unlabeled items left to label.\")\n        break\n    y_train = (y)\n    y_train[unlabeled_indices] = -1\n\n    lp_model = (gamma=0.25, max_iter=20)\n    lp_model.fit(X, y_train)\n\n    predicted_labels = lp_model.transduction_[unlabeled_indices]\n    true_labels = y[unlabeled_indices]\n\n    cm = (true_labels, predicted_labels, labels=lp_model.classes_)\n\n    print(\"Iteration %i %s\" % (i, 70 * \"_\"))\n    print(\n        \"Label Spreading model: %d labeled & %d unlabeled (%d total)\"\n        % (n_labeled_points, n_total_samples - n_labeled_points, n_total_samples)\n    )\n\n    print((true_labels, predicted_labels))\n\n    print(\"Confusion matrix\")\n    print(cm)\n\n    # compute the entropies of transduced label distributions\n    pred_entropies = (lp_model.label_distributions_.T)\n\n    # select up to 5 digit examples that the classifier is most uncertain about\n    uncertainty_index = (pred_entropies)[::-1]\n    uncertainty_index = uncertainty_index[\n        (uncertainty_index, unlabeled_indices)\n    ][:5]\n\n    # keep track of indices that we get labels for\n    delete_indices = ([], dtype=int)\n\n    # for more than 5 iterations, visualize the gain only on the first 5\n    if i &lt; 5:\n        f.text(\n            0.05,\n            (1 - (i + 1) * 0.183),\n            \"model %d\\n\\nfit with\\n%d labels\" % ((i + 1), i * 5 + 10),\n            size=10,\n        )\n    for index, image_index in enumerate(uncertainty_index):\n        image = images[image_index]\n\n        # for more than 5 iterations, visualize the gain only on the first 5\n        if i &lt; 5:\n            sub = f.add_subplot(5, 5, index + 1 + (5 * i))\n            sub.imshow(image, cmap=plt.cm.gray_r, interpolation=\"none\")\n            sub.set_title(\n                \"predict: %i\\ntrue: %i\"\n                % (lp_model.transduction_[image_index], y[image_index]),\n                size=10,\n            )\n            sub.axis(\"off\")\n\n        # labeling 5 points, remote from labeled set\n        (delete_index,) = (unlabeled_indices == image_index)\n        delete_indices = ((delete_indices, delete_index))\n\n    unlabeled_indices = (unlabeled_indices, delete_indices)\n    n_labeled_points += len(uncertainty_index)\n\nf.suptitle(\n    \"Active learning with Label Propagation.\\nRows show 5 most \"\n    \"uncertain labels to learn with the next model.\",\n    y=1.15,\n)\n(left=0.2, bottom=0.03, right=0.9, top=0.9, wspace=0.2, hspace=0.85)\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.542 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Semi Supervised Classification->Label Propagation digits: Demonstrating performance": [
        [
            "This example demonstrates the power of semisupervised learning by\ntraining a Label Spreading model to classify handwritten digits\nwith sets of very few labels.",
            "markdown"
        ],
        [
            "The handwritten digit dataset has 1797 total points. The model will\nbe trained using all points, but only 30 will be labeled. Results\nin the form of a confusion matrix and a series of metrics over each\nclass will be very good.",
            "markdown"
        ],
        [
            "At the end, the top 10 most uncertain predictions will be shown.",
            "markdown"
        ],
        [
            "# Authors: Clay Woolam &lt;clay@woolam.org\n# License: BSD",
            "code"
        ]
    ],
    "Examples->Semi Supervised Classification->Label Propagation digits: Demonstrating performance->Data generation": [
        [
            "We use the digits dataset. We only use a subset of randomly selected samples.",
            "markdown"
        ],
        [
            "from sklearn import datasets\nimport numpy as np\n\ndigits = ()\nrng = (2)\nindices = (len(digits.data))\nrng.shuffle(indices)",
            "code"
        ],
        [
            "We selected 340 samples of which only 40 will be associated with a known label.\nTherefore, we store the indices of the 300 other samples for which we are not\nsupposed to know their labels.",
            "markdown"
        ],
        [
            "X = digits.data[indices[:340]]\ny = digits.target[indices[:340]]\nimages = digits.images[indices[:340]]\n\nn_total_samples = len(y)\nn_labeled_points = 40\n\nindices = (n_total_samples)\n\nunlabeled_set = indices[n_labeled_points:]",
            "code"
        ],
        [
            "Shuffle everything around",
            "markdown"
        ],
        [
            "y_train = (y)\ny_train[unlabeled_set] = -1",
            "code"
        ]
    ],
    "Examples->Semi Supervised Classification->Label Propagation digits: Demonstrating performance->Semi-supervised learning": [
        [
            "We fit a  and use it to predict\nthe unknown labels.",
            "markdown"
        ],
        [
            "from sklearn.semi_supervised import \nfrom sklearn.metrics import \n\nlp_model = (gamma=0.25, max_iter=20)\nlp_model.fit(X, y_train)\npredicted_labels = lp_model.transduction_[unlabeled_set]\ntrue_labels = y[unlabeled_set]\n\nprint(\n    \"Label Spreading model: %d labeled & %d unlabeled points (%d total)\"\n    % (n_labeled_points, n_total_samples - n_labeled_points, n_total_samples)\n)",
            "code"
        ],
        [
            "Label Spreading model: 40 labeled & 300 unlabeled points (340 total)",
            "code"
        ],
        [
            "Classification report",
            "markdown"
        ],
        [
            "print((true_labels, predicted_labels))",
            "code"
        ],
        [
            "precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        27\n           1       0.82      1.00      0.90        37\n           2       1.00      0.86      0.92        28\n           3       1.00      0.80      0.89        35\n           4       0.92      1.00      0.96        24\n           5       0.74      0.94      0.83        34\n           6       0.89      0.96      0.92        25\n           7       0.94      0.89      0.91        35\n           8       1.00      0.68      0.81        31\n           9       0.81      0.88      0.84        24\n\n    accuracy                           0.90       300\n   macro avg       0.91      0.90      0.90       300\nweighted avg       0.91      0.90      0.90       300",
            "code"
        ],
        [
            "Confusion matrix",
            "markdown"
        ],
        [
            "from sklearn.metrics import ConfusionMatrixDisplay\n\n(\n    true_labels, predicted_labels, labels=lp_model.classes_\n)\n\n\n<img alt=\"plot label propagation digits\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_label_propagation_digits_001.png\" srcset=\"../../_images/sphx_glr_plot_label_propagation_digits_001.png\"/>",
            "code"
        ],
        [
            "&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay object at 0x7f0cf101c2b0",
            "code"
        ]
    ],
    "Examples->Semi Supervised Classification->Label Propagation digits: Demonstrating performance->Plot the most uncertain predictions": [
        [
            "Here, we will pick and show the 10 most uncertain predictions.",
            "markdown"
        ],
        [
            "from scipy import stats\n\npred_entropies = (lp_model.label_distributions_.T)",
            "code"
        ],
        [
            "Pick the top 10 most uncertain labels",
            "markdown"
        ],
        [
            "uncertainty_index = (pred_entropies)[-10:]",
            "code"
        ],
        [
            "Plot",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n\nf = (figsize=(7, 5))\nfor index, image_index in enumerate(uncertainty_index):\n    image = images[image_index]\n\n    sub = f.add_subplot(2, 5, index + 1)\n    sub.imshow(image, cmap=plt.cm.gray_r)\n    ([])\n    ([])\n    sub.set_title(\n        \"predict: %i\\ntrue: %i\" % (lp_model.transduction_[image_index], y[image_index])\n    )\n\nf.suptitle(\"Learning with small amount of labeled data\")\n()\n\n\n<img alt=\"Learning with small amount of labeled data, predict: 1 true: 2, predict: 2 true: 2, predict: 8 true: 8, predict: 1 true: 8, predict: 1 true: 8, predict: 1 true: 8, predict: 3 true: 3, predict: 8 true: 8, predict: 2 true: 2, predict: 7 true: 2\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_label_propagation_digits_002.png\" srcset=\"../../_images/sphx_glr_plot_label_propagation_digits_002.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.370 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Semi Supervised Classification->Label Propagation learning a complex structure": [
        [
            "Example of LabelPropagation learning a complex internal structure\nto demonstrate \u201cmanifold learning\u201d. The outer circle should be\nlabeled \u201cred\u201d and the inner circle \u201cblue\u201d. Because both label groups\nlie inside their own distinct shape, we can see that the labels\npropagate correctly around the circle.",
            "markdown"
        ],
        [
            "# Authors: Clay Woolam &lt;clay@woolam.org\n#          Andreas Mueller &lt;amueller@ais.uni-bonn.de\n# License: BSD",
            "code"
        ],
        [
            "We generate a dataset with two concentric circles. In addition, a label\nis associated with each sample of the dataset that is: 0 (belonging to\nthe outer circle), 1 (belonging to the inner circle), and -1 (unknown).\nHere, all labels but two are tagged as unknown.",
            "markdown"
        ],
        [
            "import numpy as np\nfrom sklearn.datasets import \n\nn_samples = 200\nX, y = (n_samples=n_samples, shuffle=False)\nouter, inner = 0, 1\nlabels = (n_samples, -1.0)\nlabels[0] = outer\nlabels[-1] = inner",
            "code"
        ],
        [
            "Plot raw data",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n\n(figsize=(4, 4))\n(\n    X[labels == outer, 0],\n    X[labels == outer, 1],\n    color=\"navy\",\n    marker=\"s\",\n    lw=0,\n    label=\"outer labeled\",\n    s=10,\n)\n(\n    X[labels == inner, 0],\n    X[labels == inner, 1],\n    color=\"c\",\n    marker=\"s\",\n    lw=0,\n    label=\"inner labeled\",\n    s=10,\n)\n(\n    X[labels == -1, 0],\n    X[labels == -1, 1],\n    color=\"darkorange\",\n    marker=\".\",\n    label=\"unlabeled\",\n)\n(scatterpoints=1, shadow=False, loc=\"center\")\n_ = (\"Raw data (2 classes=outer and inner)\")\n\n\n<img alt=\"Raw data (2 classes=outer and inner)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_label_propagation_structure_001.png\" srcset=\"../../_images/sphx_glr_plot_label_propagation_structure_001.png\"/>",
            "code"
        ],
        [
            "The aim of  is to associate\na label to sample where the label is initially unknown.",
            "markdown"
        ],
        [
            "from sklearn.semi_supervised import \n\nlabel_spread = (kernel=\"knn\", alpha=0.8)\nlabel_spread.fit(X, labels)",
            "code"
        ],
        [
            "LabelSpreading(alpha=0.8, kernel='knn')<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b><input checked=\"\" class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-269\" type=\"checkbox\"/><label class=\"sk-toggleable__label sk-toggleable__label-arrow\" for=\"sk-estimator-id-269\">LabelSpreading</label>",
            "code"
        ],
        [
            "LabelSpreading(alpha=0.8, kernel='knn')\n\n<br/>\n<br/>",
            "code"
        ],
        [
            "Now, we can check which labels have been associated with each sample\nwhen the label was unknown.",
            "markdown"
        ],
        [
            "output_labels = label_spread.transduction_\noutput_label_array = (output_labels)\nouter_numbers = (output_label_array == outer)[0]\ninner_numbers = (output_label_array == inner)[0]\n\n(figsize=(4, 4))\n(\n    X[outer_numbers, 0],\n    X[outer_numbers, 1],\n    color=\"navy\",\n    marker=\"s\",\n    lw=0,\n    s=10,\n    label=\"outer learned\",\n)\n(\n    X[inner_numbers, 0],\n    X[inner_numbers, 1],\n    color=\"c\",\n    marker=\"s\",\n    lw=0,\n    s=10,\n    label=\"inner learned\",\n)\n(scatterpoints=1, shadow=False, loc=\"center\")\n(\"Labels learned with Label Spreading (KNN)\")\n()\n\n\n<img alt=\"Labels learned with Label Spreading (KNN)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_label_propagation_structure_002.png\" srcset=\"../../_images/sphx_glr_plot_label_propagation_structure_002.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.157 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Semi Supervised Classification->Semi-supervised Classification on a Text Dataset": [
        [
            "In this example, semi-supervised classifiers are trained on the 20 newsgroups\ndataset (which will be automatically downloaded).",
            "markdown"
        ],
        [
            "You can adjust the number of categories by giving their names to the dataset\nloader or setting them to None to get all 20 of them.",
            "markdown"
        ],
        [
            "2823 documents\n5 categories\n\nSupervised SGDClassifier on 100% of the data:\nNumber of training samples: 2117\nUnlabeled samples in training set: 0\nMicro-averaged F1 score on test set: 0.891\n----------\n\nSupervised SGDClassifier on 20% of the training data:\nNumber of training samples: 408\nUnlabeled samples in training set: 0\nMicro-averaged F1 score on test set: 0.752\n----------\n\nSelfTrainingClassifier on 20% of the training data (rest is unlabeled):\nNumber of training samples: 2117\nUnlabeled samples in training set: 1709\nEnd of iteration 1, added 1091 new labels.\nEnd of iteration 2, added 190 new labels.\nEnd of iteration 3, added 76 new labels.\nEnd of iteration 4, added 42 new labels.\nEnd of iteration 5, added 24 new labels.\nEnd of iteration 6, added 6 new labels.\nEnd of iteration 7, added 3 new labels.\nEnd of iteration 8, added 4 new labels.\nEnd of iteration 9, added 6 new labels.\nEnd of iteration 10, added 1 new labels.\nMicro-averaged F1 score on test set: 0.839\n----------\n\nLabelSpreading on 20% of the data (rest is unlabeled):\nNumber of training samples: 2117\nUnlabeled samples in training set: 1709\nMicro-averaged F1 score on test set: 0.666\n----------\n\n\n\n<br/>",
            "code"
        ],
        [
            "import numpy as np\n\nfrom sklearn.datasets import \nfrom sklearn.feature_extraction.text import \nfrom sklearn.feature_extraction.text import \nfrom sklearn.preprocessing import \nfrom sklearn.linear_model import \nfrom sklearn.model_selection import \nfrom sklearn.pipeline import \nfrom sklearn.semi_supervised import \nfrom sklearn.semi_supervised import \nfrom sklearn.metrics import \n\n# Loading dataset containing first five categories\ndata = (\n    subset=\"train\",\n    categories=[\n        \"alt.atheism\",\n        \"comp.graphics\",\n        \"comp.os.ms-windows.misc\",\n        \"comp.sys.ibm.pc.hardware\",\n        \"comp.sys.mac.hardware\",\n    ],\n)\nprint(\"%d documents\" % len(data.filenames))\nprint(\"%d categories\" % len(data.target_names))\nprint()\n\n# Parameters\nsdg_params = dict(alpha=1e-5, penalty=\"l2\", loss=\"log_loss\")\nvectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)\n\n# Supervised Pipeline\npipeline = (\n    [\n        (\"vect\", (**vectorizer_params)),\n        (\"tfidf\", ()),\n        (\"clf\", (**sdg_params)),\n    ]\n)\n# SelfTraining Pipeline\nst_pipeline = (\n    [\n        (\"vect\", (**vectorizer_params)),\n        (\"tfidf\", ()),\n        (\"clf\", ((**sdg_params), verbose=True)),\n    ]\n)\n# LabelSpreading Pipeline\nls_pipeline = (\n    [\n        (\"vect\", (**vectorizer_params)),\n        (\"tfidf\", ()),\n        # LabelSpreading does not support dense matrices\n        (\"toarray\", (lambda x: x.toarray())),\n        (\"clf\", ()),\n    ]\n)\n\n\ndef eval_and_print_metrics(clf, X_train, y_train, X_test, y_test):\n    print(\"Number of training samples:\", len(X_train))\n    print(\"Unlabeled samples in training set:\", sum(1 for x in y_train if x == -1))\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    print(\n        \"Micro-averaged F1 score on test set: %0.3f\"\n        % (y_test, y_pred, average=\"micro\")\n    )\n    print(\"-\" * 10)\n    print()\n\n\nif __name__ == \"__main__\":\n    X, y = data.data, data.target\n    X_train, X_test, y_train, y_test = (X, y)\n\n    print(\"Supervised SGDClassifier on 100% of the data:\")\n    eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n\n    # select a mask of 20% of the train dataset\n    y_mask = (len(y_train)) &lt; 0.2\n\n    # X_20 and y_20 are the subset of the train dataset indicated by the mask\n    X_20, y_20 = map(\n        list, zip(*((x, y) for x, y, m in zip(X_train, y_train, y_mask) if m))\n    )\n    print(\"Supervised SGDClassifier on 20% of the training data:\")\n    eval_and_print_metrics(pipeline, X_20, y_20, X_test, y_test)\n\n    # set the non-masked subset to be unlabeled\n    y_train[~y_mask] = -1\n    print(\"SelfTrainingClassifier on 20% of the training data (rest is unlabeled):\")\n    eval_and_print_metrics(st_pipeline, X_train, y_train, X_test, y_test)\n\n    print(\"LabelSpreading on 20% of the data (rest is unlabeled):\")\n    eval_and_print_metrics(ls_pipeline, X_train, y_train, X_test, y_test)",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  6.790 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Support Vector Machines->Non-linear SVM": [
        [
            "Perform binary classification using non-linear SVC\nwith RBF kernel. The target to predict is a XOR of the\ninputs.",
            "markdown"
        ],
        [
            "The color map illustrates the decision function learned by the SVC.\n<img alt=\"plot svm nonlinear\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_svm_nonlinear_001.png\" srcset=\"../../_images/sphx_glr_plot_svm_nonlinear_001.png\"/>",
            "markdown"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\n\nxx, yy = ((-3, 3, 500), (-3, 3, 500))\n(0)\nX = (300, 2)\nY = (X[:, 0]  0, X[:, 1]  0)\n\n# fit the model\nclf = (gamma=\"auto\")\nclf.fit(X, Y)\n\n# plot the decision function for each datapoint on the grid\nZ = clf.decision_function([xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\n(\n    Z,\n    interpolation=\"nearest\",\n    extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n    aspect=\"auto\",\n    origin=\"lower\",\n    cmap=plt.cm.PuOr_r,\n)\ncontours = (xx, yy, Z, levels=[0], linewidths=2, linestyles=\"dashed\")\n(X[:, 0], X[:, 1], s=30, c=Y, cmap=plt.cm.Paired, edgecolors=\"k\")\n(())\n(())\n([-3, 3, -3, 3])\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  1.506 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Support Vector Machines->One-class SVM with non-linear kernel (RBF)": [
        [
            "An example using a one-class SVM for novelty detection.",
            "markdown"
        ],
        [
            "is an unsupervised\nalgorithm that learns a decision function for novelty detection:\nclassifying new data as similar or different to the training set.\n<img alt=\"Novelty Detection\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_oneclass_001.png\" srcset=\"../../_images/sphx_glr_plot_oneclass_001.png\"/>",
            "markdown"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager\nfrom sklearn import svm\n\nxx, yy = ((-5, 5, 500), (-5, 5, 500))\n# Generate train data\nX = 0.3 * (100, 2)\nX_train = [X + 2, X - 2]\n# Generate some regular novel observations\nX = 0.3 * (20, 2)\nX_test = [X + 2, X - 2]\n# Generate some abnormal novel observations\nX_outliers = (low=-4, high=4, size=(20, 2))\n\n# fit the model\nclf = (nu=0.1, kernel=\"rbf\", gamma=0.1)\nclf.fit(X_train)\ny_pred_train = clf.predict(X_train)\ny_pred_test = clf.predict(X_test)\ny_pred_outliers = clf.predict(X_outliers)\nn_error_train = y_pred_train[y_pred_train == -1].size\nn_error_test = y_pred_test[y_pred_test == -1].size\nn_error_outliers = y_pred_outliers[y_pred_outliers == 1].size\n\n# plot the line, the points, and the nearest vectors to the plane\nZ = clf.decision_function([xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\n(\"Novelty Detection\")\n(xx, yy, Z, levels=(Z.min(), 0, 7), cmap=plt.cm.PuBu)\na = (xx, yy, Z, levels=[0], linewidths=2, colors=\"darkred\")\n(xx, yy, Z, levels=[0, Z.max()], colors=\"palevioletred\")\n\ns = 40\nb1 = (X_train[:, 0], X_train[:, 1], c=\"white\", s=s, edgecolors=\"k\")\nb2 = (X_test[:, 0], X_test[:, 1], c=\"blueviolet\", s=s, edgecolors=\"k\")\nc = (X_outliers[:, 0], X_outliers[:, 1], c=\"gold\", s=s, edgecolors=\"k\")\n(\"tight\")\n((-5, 5))\n((-5, 5))\n(\n    [a.collections[0], b1, b2, c],\n    [\n        \"learned frontier\",\n        \"training observations\",\n        \"new regular observations\",\n        \"new abnormal observations\",\n    ],\n    loc=\"upper left\",\n    prop=(size=11),\n)\n(\n    \"error train: %d/200 ; errors novel regular: %d/40 ; errors novel abnormal: %d/40\"\n    % (n_error_train, n_error_test, n_error_outliers)\n)\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.375 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Support Vector Machines->Plot different SVM classifiers in the iris dataset": [
        [
            "Comparison of different linear SVM classifiers on a 2D projection of the iris\ndataset. We only consider the first 2 features of this dataset:",
            "markdown"
        ],
        [
            "Sepal length",
            "markdown"
        ],
        [
            "Sepal width",
            "markdown"
        ],
        [
            "This example shows how to plot the decision surface for four SVM classifiers\nwith different kernels.",
            "markdown"
        ],
        [
            "The linear models LinearSVC() and SVC(kernel='linear') yield slightly\ndifferent decision boundaries. This can be a consequence of the following\ndifferences:",
            "markdown"
        ],
        [
            "LinearSVC minimizes the squared hinge loss while SVC minimizes the\nregular hinge loss.",
            "markdown"
        ],
        [
            "LinearSVC uses the One-vs-All (also known as One-vs-Rest) multiclass\nreduction while SVC uses the One-vs-One multiclass reduction.",
            "markdown"
        ],
        [
            "Both linear models have linear decision boundaries (intersecting hyperplanes)\nwhile the non-linear kernel models (polynomial or Gaussian RBF) have more\nflexible non-linear decision boundaries with shapes that depend on the kind of\nkernel and its parameters.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "while plotting the decision function of classifiers for toy 2D\ndatasets can help get an intuitive understanding of their respective\nexpressive power, be aware that those intuitions don\u2019t always generalize to\nmore realistic high-dimensional problems.\n\n<img alt=\"SVC with linear kernel, LinearSVC (linear kernel), SVC with RBF kernel, SVC with polynomial (degree 3) kernel\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_iris_svc_001.png\" srcset=\"../../_images/sphx_glr_plot_iris_svc_001.png\"/>",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\nfrom sklearn import svm, datasets\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\n\n# import some data to play with\niris = ()\n# Take the first two features. We could avoid this by using a two-dim dataset\nX = iris.data[:, :2]\ny = iris.target\n\n# we create an instance of SVM and fit out data. We do not scale our\n# data since we want to plot the support vectors\nC = 1.0  # SVM regularization parameter\nmodels = (\n    (kernel=\"linear\", C=C),\n    (C=C, max_iter=10000),\n    (kernel=\"rbf\", gamma=0.7, C=C),\n    (kernel=\"poly\", degree=3, gamma=\"auto\", C=C),\n)\nmodels = (clf.fit(X, y) for clf in models)\n\n# title for the plots\ntitles = (\n    \"SVC with linear kernel\",\n    \"LinearSVC (linear kernel)\",\n    \"SVC with RBF kernel\",\n    \"SVC with polynomial (degree 3) kernel\",\n)\n\n# Set-up 2x2 grid for plotting.\nfig, sub = (2, 2)\n(wspace=0.4, hspace=0.4)\n\nX0, X1 = X[:, 0], X[:, 1]\n\nfor clf, title, ax in zip(models, titles, sub.flatten()):\n    disp = (\n        clf,\n        X,\n        response_method=\"predict\",\n        cmap=plt.cm.coolwarm,\n        alpha=0.8,\n        ax=ax,\n        xlabel=iris.feature_names[0],\n        ylabel=iris.feature_names[1],\n    )\n    ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors=\"k\")\n    ax.set_xticks(())\n    ax.set_yticks(())\n    ax.set_title(title)\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.227 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Support Vector Machines->Plot the support vectors in LinearSVC": [
        [
            "Unlike SVC (based on LIBSVM), LinearSVC (based on LIBLINEAR) does not provide\nthe support vectors. This example demonstrates how to obtain the support\nvectors in LinearSVC.\n<img alt=\"C=1, C=100\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linearsvc_support_vectors_001.png\" srcset=\"../../_images/sphx_glr_plot_linearsvc_support_vectors_001.png\"/>",
            "markdown"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import \nfrom sklearn.svm import \nfrom sklearn.inspection import DecisionBoundaryDisplay\n\nX, y = (n_samples=40, centers=2, random_state=0)\n\n(figsize=(10, 5))\nfor i, C in enumerate([1, 100]):\n    # \"hinge\" is the standard SVM loss\n    clf = (C=C, loss=\"hinge\", random_state=42).fit(X, y)\n    # obtain the support vectors through the decision function\n    decision_function = clf.decision_function(X)\n    # we can also calculate the decision function manually\n    # decision_function = np.dot(X, clf.coef_[0]) + clf.intercept_[0]\n    # The support vectors are the samples that lie within the margin\n    # boundaries, whose size is conventionally constrained to 1\n    support_vector_indices = (np.abs(decision_function) &lt;= 1 + 1e-15)[0]\n    support_vectors = X[support_vector_indices]\n\n    (1, 2, i + 1)\n    (X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n    ax = ()\n    (\n        clf,\n        X,\n        ax=ax,\n        grid_resolution=50,\n        plot_method=\"contour\",\n        colors=\"k\",\n        levels=[-1, 0, 1],\n        alpha=0.5,\n        linestyles=[\"--\", \"-\", \"--\"],\n    )\n    (\n        support_vectors[:, 0],\n        support_vectors[:, 1],\n        s=100,\n        linewidth=1,\n        facecolors=\"none\",\n        edgecolors=\"k\",\n    )\n    (\"C=\" + str(C))\n()\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.146 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Support Vector Machines->RBF SVM parameters": [
        [
            "This example illustrates the effect of the parameters gamma and C of\nthe Radial Basis Function (RBF) kernel SVM.",
            "markdown"
        ],
        [
            "Intuitively, the gamma parameter defines how far the influence of a single\ntraining example reaches, with low values meaning \u2018far\u2019 and high values meaning\n\u2018close\u2019. The gamma parameters can be seen as the inverse of the radius of\ninfluence of samples selected by the model as support vectors.",
            "markdown"
        ],
        [
            "The C parameter trades off correct classification of training examples\nagainst maximization of the decision function\u2019s margin. For larger values of\nC, a smaller margin will be accepted if the decision function is better at\nclassifying all training points correctly. A lower C will encourage a\nlarger margin, therefore a simpler decision function, at the cost of training\naccuracy. In other words C behaves as a regularization parameter in the\nSVM.",
            "markdown"
        ],
        [
            "The first plot is a visualization of the decision function for a variety of\nparameter values on a simplified classification problem involving only 2 input\nfeatures and 2 possible target classes (binary classification). Note that this\nkind of plot is not possible to do for problems with more features or target\nclasses.",
            "markdown"
        ],
        [
            "The second plot is a heatmap of the classifier\u2019s cross-validation accuracy as a\nfunction of C and gamma. For this example we explore a relatively large\ngrid for illustration purposes. In practice, a logarithmic grid from\n\\(10^{-3}\\) to \\(10^3\\) is usually sufficient. If the best parameters\nlie on the boundaries of the grid, it can be extended in that direction in a\nsubsequent search.",
            "markdown"
        ],
        [
            "Note that the heat map plot has a special colorbar with a midpoint value close\nto the score values of the best performing models so as to make it easy to tell\nthem apart in the blink of an eye.",
            "markdown"
        ],
        [
            "The behavior of the model is very sensitive to the gamma parameter. If\ngamma is too large, the radius of the area of influence of the support\nvectors only includes the support vector itself and no amount of\nregularization with C will be able to prevent overfitting.",
            "markdown"
        ],
        [
            "When gamma is very small, the model is too constrained and cannot capture\nthe complexity or \u201cshape\u201d of the data. The region of influence of any selected\nsupport vector would include the whole training set. The resulting model will\nbehave similarly to a linear model with a set of hyperplanes that separate the\ncenters of high density of any pair of two classes.",
            "markdown"
        ],
        [
            "For intermediate values, we can see on the second plot that good models can\nbe found on a diagonal of C and gamma. Smooth models (lower gamma\nvalues) can be made more complex by increasing the importance of classifying\neach point correctly (larger C values) hence the diagonal of good\nperforming models.",
            "markdown"
        ],
        [
            "Finally, one can also observe that for some intermediate values of gamma we\nget equally performing models when C becomes very large. This suggests that\nthe set of support vectors does not change anymore. The radius of the RBF\nkernel alone acts as a good structural regularizer. Increasing C further\ndoesn\u2019t help, likely because there are no more training points in violation\n(inside the margin or wrongly classified), or at least no better solution can\nbe found. Scores being equal, it may make sense to use the smaller C\nvalues, since very high C values typically increase fitting time.",
            "markdown"
        ],
        [
            "On the other hand, lower C values generally lead to more support vectors,\nwhich may increase prediction time. Therefore, lowering the value of C\ninvolves a trade-off between fitting time and prediction time.",
            "markdown"
        ],
        [
            "We should also note that small differences in scores results from the random\nsplits of the cross-validation procedure. Those spurious variations can be\nsmoothed out by increasing the number of CV iterations n_splits at the\nexpense of compute time. Increasing the value number of C_range and\ngamma_range steps will increase the resolution of the hyper-parameter heat\nmap.",
            "markdown"
        ],
        [
            "Utility class to move the midpoint of a colormap to be around\nthe values of interest.",
            "markdown"
        ],
        [
            "import numpy as np\nfrom matplotlib.colors import \n\n\nclass MidpointNormalize():\n    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n        self.midpoint = midpoint\n        .__init__(self, vmin, vmax, clip)\n\n    def __call__(self, value, clip=None):\n        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n        return ((value, x, y))",
            "code"
        ]
    ],
    "Examples->Support Vector Machines->RBF SVM parameters->Load and prepare data set": [
        [
            "dataset for grid search",
            "markdown"
        ],
        [
            "from sklearn.datasets import \n\niris = ()\nX = iris.data\ny = iris.target",
            "code"
        ],
        [
            "Dataset for decision function visualization: we only keep the first two\nfeatures in X and sub-sample the dataset to keep only 2 classes and\nmake it a binary classification problem.",
            "markdown"
        ],
        [
            "X_2d = X[:, :2]\nX_2d = X_2d[y  0]\ny_2d = y[y  0]\ny_2d -= 1",
            "code"
        ],
        [
            "It is usually a good idea to scale the data for SVM training.\nWe are cheating a bit in this example in scaling all of the data,\ninstead of fitting the transformation on the training set and\njust applying it on the test set.",
            "markdown"
        ],
        [
            "from sklearn.preprocessing import \n\nscaler = ()\nX = scaler.fit_transform(X)\nX_2d = scaler.fit_transform(X_2d)",
            "code"
        ]
    ],
    "Examples->Support Vector Machines->RBF SVM parameters->Train classifiers": [
        [
            "For an initial search, a logarithmic grid with basis\n10 is often helpful. Using a basis of 2, a finer\ntuning can be achieved but at a much higher cost.",
            "markdown"
        ],
        [
            "from sklearn.svm import \nfrom sklearn.model_selection import \nfrom sklearn.model_selection import \n\nC_range = (-2, 10, 13)\ngamma_range = (-9, 3, 13)\nparam_grid = dict(gamma=gamma_range, C=C_range)\ncv = (n_splits=5, test_size=0.2, random_state=42)\ngrid = ((), param_grid=param_grid, cv=cv)\ngrid.fit(X, y)\n\nprint(\n    \"The best parameters are %s with a score of %0.2f\"\n    % (grid.best_params_, grid.best_score_)\n)",
            "code"
        ],
        [
            "The best parameters are {'C': 1.0, 'gamma': 0.09999999999999999} with a score of 0.97",
            "code"
        ],
        [
            "Now we need to fit a classifier for all parameters in the 2d version\n(we use a smaller set of parameters here because it takes a while to train)",
            "markdown"
        ],
        [
            "C_2d_range = [1e-2, 1, 1e2]\ngamma_2d_range = [1e-1, 1, 1e1]\nclassifiers = []\nfor C in C_2d_range:\n    for gamma in gamma_2d_range:\n        clf = (C=C, gamma=gamma)\n        clf.fit(X_2d, y_2d)\n        classifiers.append((C, gamma, clf))",
            "code"
        ]
    ],
    "Examples->Support Vector Machines->RBF SVM parameters->Visualization": [
        [
            "draw visualization of parameter effects",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n\n(figsize=(8, 6))\nxx, yy = ((-3, 3, 200), (-3, 3, 200))\nfor k, (C, gamma, clf) in enumerate(classifiers):\n    # evaluate decision function in a grid\n    Z = clf.decision_function([xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    # visualize decision function for these parameters\n    (len(C_2d_range), len(gamma_2d_range), k + 1)\n    (\"gamma=10^%d, C=10^%d\" % ((gamma), (C)), size=\"medium\")\n\n    # visualize parameter's effect on decision function\n    (xx, yy, -Z, cmap=plt.cm.RdBu)\n    (X_2d[:, 0], X_2d[:, 1], c=y_2d, cmap=plt.cm.RdBu_r, edgecolors=\"k\")\n    (())\n    (())\n    (\"tight\")\n\nscores = grid.cv_results_[\"mean_test_score\"].reshape(len(C_range), len(gamma_range))\n\n\n<img alt=\"gamma=10^-1, C=10^-2, gamma=10^0, C=10^-2, gamma=10^1, C=10^-2, gamma=10^-1, C=10^0, gamma=10^0, C=10^0, gamma=10^1, C=10^0, gamma=10^-1, C=10^2, gamma=10^0, C=10^2, gamma=10^1, C=10^2\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_rbf_parameters_001.png\" srcset=\"../../_images/sphx_glr_plot_rbf_parameters_001.png\"/>",
            "code"
        ],
        [
            "Draw heatmap of the validation accuracy as a function of gamma and C",
            "markdown"
        ],
        [
            "The score are encoded as colors with the hot colormap which varies from dark\nred to bright yellow. As the most interesting scores are all located in the\n0.92 to 0.97 range we use a custom normalizer to set the mid-point to 0.92 so\nas to make it easier to visualize the small variations of score values in the\ninteresting range while not brutally collapsing all the low score values to\nthe same color.",
            "markdown"
        ],
        [
            "(figsize=(8, 6))\n(left=0.2, right=0.95, bottom=0.15, top=0.95)\n(\n    scores,\n    interpolation=\"nearest\",\n    cmap=plt.cm.hot,\n    norm=MidpointNormalize(vmin=0.2, midpoint=0.92),\n)\n(\"gamma\")\n(\"C\")\n()\n((len(gamma_range)), gamma_range, rotation=45)\n((len(C_range)), C_range)\n(\"Validation accuracy\")\n()\n\n\n<img alt=\"Validation accuracy\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_rbf_parameters_002.png\" srcset=\"../../_images/sphx_glr_plot_rbf_parameters_002.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  4.716 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Support Vector Machines->SVM Margins Example": [
        [
            "The plots below illustrate the effect the parameter C has\non the separation line. A large value of C basically tells\nour model that we do not have that much faith in our data\u2019s\ndistribution, and will only consider points close to line\nof separation.",
            "markdown"
        ],
        [
            "A small value of C includes more/all the observations, allowing\nthe margins to be calculated using all the data in the area.\n\n<img alt=\"plot svm margin\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_svm_margin_001.png\" srcset=\"../../_images/sphx_glr_plot_svm_margin_001.png\"/>\n<img alt=\"plot svm margin\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_svm_margin_002.png\" srcset=\"../../_images/sphx_glr_plot_svm_margin_002.png\"/>",
            "markdown"
        ],
        [
            "/home/circleci/project/examples/svm/plot_svm_margin.py:61: UserWarning:\n\nNo data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n\n/home/circleci/project/examples/svm/plot_svm_margin.py:61: UserWarning:\n\nNo data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n\n\n\n<br/>",
            "code"
        ],
        [
            "# Code source: Ga\u00ebl Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom sklearn import svm\n\n# we create 40 separable points\n(0)\nX = [(20, 2) - [2, 2], (20, 2) + [2, 2]]\nY = [0] * 20 + [1] * 20\n\n# figure number\nfignum = 1\n\n# fit the model\nfor name, penalty in ((\"unreg\", 1), (\"reg\", 0.05)):\n\n    clf = (kernel=\"linear\", C=penalty)\n    clf.fit(X, Y)\n\n    # get the separating hyperplane\n    w = clf.coef_[0]\n    a = -w[0] / w[1]\n    xx = (-5, 5)\n    yy = a * xx - (clf.intercept_[0]) / w[1]\n\n    # plot the parallels to the separating hyperplane that pass through the\n    # support vectors (margin away from hyperplane in direction\n    # perpendicular to hyperplane). This is sqrt(1+a^2) away vertically in\n    # 2-d.\n    margin = 1 / ((clf.coef_**2))\n    yy_down = yy - (1 + a**2) * margin\n    yy_up = yy + (1 + a**2) * margin\n\n    # plot the line, the points, and the nearest vectors to the plane\n    (fignum, figsize=(4, 3))\n    ()\n    (xx, yy, \"k-\")\n    (xx, yy_down, \"k--\")\n    (xx, yy_up, \"k--\")\n\n    (\n        clf.support_vectors_[:, 0],\n        clf.support_vectors_[:, 1],\n        s=80,\n        facecolors=\"none\",\n        zorder=10,\n        edgecolors=\"k\",\n        cmap=(\"RdBu\"),\n    )\n    (\n        X[:, 0], X[:, 1], c=Y, zorder=10, cmap=(\"RdBu\"), edgecolors=\"k\"\n    )\n\n    (\"tight\")\n    x_min = -4.8\n    x_max = 4.2\n    y_min = -6\n    y_max = 6\n\n    YY, XX = (yy, xx)\n    xy = ([XX.ravel(), YY.ravel()]).T\n    Z = clf.decision_function(xy).reshape(XX.shape)\n\n    # Put the result into a contour plot\n    (XX, YY, Z, cmap=(\"RdBu\"), alpha=0.5, linestyles=[\"-\"])\n\n    (x_min, x_max)\n    (y_min, y_max)\n\n    (())\n    (())\n    fignum = fignum + 1\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.081 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Support Vector Machines->SVM Tie Breaking Example": [
        [
            "Tie breaking is costly if decision_function_shape='ovr', and therefore it\nis not enabled by default. This example illustrates the effect of the\nbreak_ties parameter for a multiclass classification problem and\ndecision_function_shape='ovr'.",
            "markdown"
        ],
        [
            "The two plots differ only in the area in the middle where the classes are\ntied. If break_ties=False, all input in that area would be classified as\none class, whereas if break_ties=True, the tie-breaking mechanism will\ncreate a non-convex decision boundary in that area.\n<img alt=\"break_ties = False, break_ties = True\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_svm_tie_breaking_001.png\" srcset=\"../../_images/sphx_glr_plot_svm_tie_breaking_001.png\"/>",
            "markdown"
        ],
        [
            "# Code source: Andreas Mueller, Adrin Jalali\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import \nfrom sklearn.datasets import \n\nX, y = (random_state=27)\n\nfig, sub = (2, 1, figsize=(5, 8))\ntitles = (\"break_ties = False\", \"break_ties = True\")\n\nfor break_ties, title, ax in zip((False, True), titles, sub.flatten()):\n\n    svm = (\n        kernel=\"linear\", C=1, break_ties=break_ties, decision_function_shape=\"ovr\"\n    ).fit(X, y)\n\n    xlim = [X[:, 0].min(), X[:, 0].max()]\n    ylim = [X[:, 1].min(), X[:, 1].max()]\n\n    xs = (xlim[0], xlim[1], 1000)\n    ys = (ylim[0], ylim[1], 1000)\n    xx, yy = (xs, ys)\n\n    pred = svm.predict([xx.ravel(), yy.ravel()])\n\n    colors = [plt.cm.Accent(i) for i in [0, 4, 7]]\n\n    points = ax.scatter(X[:, 0], X[:, 1], c=y, cmap=\"Accent\")\n    classes = [(0, 1), (0, 2), (1, 2)]\n    line = (X[:, 1].min() - 5, X[:, 1].max() + 5)\n    ax.imshow(\n        -pred.reshape(xx.shape),\n        cmap=\"Accent\",\n        alpha=0.2,\n        extent=(xlim[0], xlim[1], ylim[1], ylim[0]),\n    )\n\n    for coef, intercept, col in zip(svm.coef_, svm.intercept_, classes):\n        line2 = -(line * coef[1] + intercept) / coef[0]\n        ax.plot(line2, line, \"-\", c=colors[col[0]])\n        ax.plot(line2, line, \"--\", c=colors[col[1]])\n    ax.set_xlim(xlim)\n    ax.set_ylim(ylim)\n    ax.set_title(title)\n    ax.set_aspect(\"equal\")\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  1.020 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Support Vector Machines->SVM with custom kernel": [
        [
            "Simple usage of Support Vector Machines to classify a sample. It will\nplot the decision surface and the support vectors.\n<img alt=\"3-Class classification using Support Vector Machine with custom kernel\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_custom_kernel_001.png\" srcset=\"../../_images/sphx_glr_plot_custom_kernel_001.png\"/>",
            "markdown"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm, datasets\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\n# import some data to play with\niris = ()\nX = iris.data[:, :2]  # we only take the first two features. We could\n# avoid this ugly slicing by using a two-dim dataset\nY = iris.target\n\n\ndef my_kernel(X, Y):\n    \"\"\"\n    We create a custom kernel:\n\n                 (2  0)\n    k(X, Y) = X  (    ) Y.T\n                 (0  1)\n    \"\"\"\n    M = ([[2, 0], [0, 1.0]])\n    return ((X, M), Y.T)\n\n\nh = 0.02  # step size in the mesh\n\n# we create an instance of SVM and fit out data.\nclf = (kernel=my_kernel)\nclf.fit(X, Y)\n\nax = ()\n(\n    clf,\n    X,\n    cmap=plt.cm.Paired,\n    ax=ax,\n    response_method=\"predict\",\n    plot_method=\"pcolormesh\",\n    shading=\"auto\",\n)\n\n# Plot also the training points\n(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired, edgecolors=\"k\")\n(\"3-Class classification using Support Vector Machine with custom kernel\")\n(\"tight\")\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.090 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Support Vector Machines->SVM-Anova: SVM with univariate feature selection": [
        [
            "This example shows how to perform univariate feature selection before running a\nSVC (support vector classifier) to improve the classification scores. We use\nthe iris dataset (4 features) and add 36 non-informative features. We can find\nthat our model achieves best performance when we select around 10% of features.",
            "markdown"
        ]
    ],
    "Examples->Support Vector Machines->SVM-Anova: SVM with univariate feature selection->Load some data to play with": [
        [
            "import numpy as np\nfrom sklearn.datasets import \n\nX, y = (return_X_y=True)\n\n# Add non-informative features\nrng = (0)\nX = ((X, 2 * rng.random((X.shape[0], 36))))",
            "code"
        ]
    ],
    "Examples->Support Vector Machines->SVM-Anova: SVM with univariate feature selection->Create the pipeline": [
        [
            "from sklearn.pipeline import \nfrom sklearn.feature_selection import , \nfrom sklearn.preprocessing import \nfrom sklearn.svm import \n\n# Create a feature-selection transform, a scaler and an instance of SVM that we\n# combine together to have a full-blown estimator\n\nclf = (\n    [\n        (\"anova\", ()),\n        (\"scaler\", ()),\n        (\"svc\", (gamma=\"auto\")),\n    ]\n)",
            "code"
        ]
    ],
    "Examples->Support Vector Machines->SVM-Anova: SVM with univariate feature selection->Plot the cross-validation score as a function of percentile of features": [
        [
            "import matplotlib.pyplot as plt\nfrom sklearn.model_selection import \n\nscore_means = list()\nscore_stds = list()\npercentiles = (1, 3, 6, 10, 15, 20, 30, 40, 60, 80, 100)\n\nfor percentile in percentiles:\n    clf.set_params(anova__percentile=percentile)\n    this_scores = (clf, X, y)\n    score_means.append(this_scores.mean())\n    score_stds.append(this_scores.std())\n\n(percentiles, score_means, (score_stds))\n(\"Performance of the SVM-Anova varying the percentile of features selected\")\n((0, 100, 11, endpoint=True))\n(\"Percentile\")\n(\"Accuracy Score\")\n(\"tight\")\n()\n\n\n<img alt=\"Performance of the SVM-Anova varying the percentile of features selected\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_svm_anova_001.png\" srcset=\"../../_images/sphx_glr_plot_svm_anova_001.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.282 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Support Vector Machines->SVM-Kernels": [
        [
            "Three different types of SVM-Kernels are displayed below.\nThe polynomial and RBF are especially useful when the\ndata-points are not linearly separable.\n\n<img alt=\"plot svm kernels\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_svm_kernels_001.png\" srcset=\"../../_images/sphx_glr_plot_svm_kernels_001.png\"/>\n<img alt=\"plot svm kernels\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_svm_kernels_002.png\" srcset=\"../../_images/sphx_glr_plot_svm_kernels_002.png\"/>\n<img alt=\"plot svm kernels\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_svm_kernels_003.png\" srcset=\"../../_images/sphx_glr_plot_svm_kernels_003.png\"/>",
            "markdown"
        ],
        [
            "# Code source: Ga\u00ebl Varoquaux\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\n\n\n# Our dataset and targets\nX = [\n    (0.4, -0.7),\n    (-1.5, -1),\n    (-1.4, -0.9),\n    (-1.3, -1.2),\n    (-1.1, -0.2),\n    (-1.2, -0.4),\n    (-0.5, 1.2),\n    (-1.5, 2.1),\n    (1, 1),\n    # --\n    (1.3, 0.8),\n    (1.2, 0.5),\n    (0.2, -2),\n    (0.5, -2.4),\n    (0.2, -2.3),\n    (0, -2.7),\n    (1.3, 2.1),\n].T\nY = [0] * 8 + [1] * 8\n\n# figure number\nfignum = 1\n\n# fit the model\nfor kernel in (\"linear\", \"poly\", \"rbf\"):\n    clf = (kernel=kernel, gamma=2)\n    clf.fit(X, Y)\n\n    # plot the line, the points, and the nearest vectors to the plane\n    (fignum, figsize=(4, 3))\n    ()\n\n    (\n        clf.support_vectors_[:, 0],\n        clf.support_vectors_[:, 1],\n        s=80,\n        facecolors=\"none\",\n        zorder=10,\n        edgecolors=\"k\",\n    )\n    (X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.cm.Paired, edgecolors=\"k\")\n\n    (\"tight\")\n    x_min = -3\n    x_max = 3\n    y_min = -3\n    y_max = 3\n\n    XX, YY = [x_min:x_max:200j, y_min:y_max:200j]\n    Z = clf.decision_function([XX.ravel(), YY.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(XX.shape)\n    (fignum, figsize=(4, 3))\n    (XX, YY, Z  0, cmap=plt.cm.Paired)\n    (\n        XX,\n        YY,\n        Z,\n        colors=[\"k\", \"k\", \"k\"],\n        linestyles=[\"--\", \"-\", \"--\"],\n        levels=[-0.5, 0, 0.5],\n    )\n\n    (x_min, x_max)\n    (y_min, y_max)\n\n    (())\n    (())\n    fignum = fignum + 1\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.190 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Support Vector Machines->SVM: Maximum margin separating hyperplane": [
        [
            "Plot the maximum margin separating hyperplane within a two-class\nseparable dataset using a Support Vector Machine classifier with\nlinear kernel.\n<img alt=\"plot separating hyperplane\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_separating_hyperplane_001.png\" srcset=\"../../_images/sphx_glr_plot_separating_hyperplane_001.png\"/>",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\nfrom sklearn import svm\nfrom sklearn.datasets import \nfrom sklearn.inspection import DecisionBoundaryDisplay\n\n\n# we create 40 separable points\nX, y = (n_samples=40, centers=2, random_state=6)\n\n# fit the model, don't regularize for illustration purposes\nclf = (kernel=\"linear\", C=1000)\nclf.fit(X, y)\n\n(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n\n# plot the decision function\nax = ()\n(\n    clf,\n    X,\n    plot_method=\"contour\",\n    colors=\"k\",\n    levels=[-1, 0, 1],\n    alpha=0.5,\n    linestyles=[\"--\", \"-\", \"--\"],\n    ax=ax,\n)\n# plot support vectors\nax.scatter(\n    clf.support_vectors_[:, 0],\n    clf.support_vectors_[:, 1],\n    s=100,\n    linewidth=1,\n    facecolors=\"none\",\n    edgecolors=\"k\",\n)\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.067 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Support Vector Machines->SVM: Separating hyperplane for unbalanced classes": [
        [
            "Find the optimal separating hyperplane using an SVC for classes that\nare unbalanced.",
            "markdown"
        ],
        [
            "We first find the separating plane with a plain SVC and then plot\n(dashed) the separating hyperplane with automatically correction for\nunbalanced classes.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "This example will also work by replacing SVC(kernel=\"linear\")\nwith SGDClassifier(loss=\"hinge\"). Setting the loss parameter\nof the  equal to hinge will yield behaviour\nsuch as that of a SVC with a linear kernel.",
            "markdown"
        ],
        [
            "For example try instead of the SVC:",
            "markdown"
        ],
        [
            "clf = SGDClassifier(n_iter=100, alpha=0.01)\n\n\n\n<img alt=\"plot separating hyperplane unbalanced\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_separating_hyperplane_unbalanced_001.png\" srcset=\"../../_images/sphx_glr_plot_separating_hyperplane_unbalanced_001.png\"/>",
            "code"
        ],
        [
            "import matplotlib.pyplot as plt\nfrom sklearn import svm\nfrom sklearn.datasets import \nfrom sklearn.inspection import DecisionBoundaryDisplay\n\n# we create two clusters of random points\nn_samples_1 = 1000\nn_samples_2 = 100\ncenters = [[0.0, 0.0], [2.0, 2.0]]\nclusters_std = [1.5, 0.5]\nX, y = (\n    n_samples=[n_samples_1, n_samples_2],\n    centers=centers,\n    cluster_std=clusters_std,\n    random_state=0,\n    shuffle=False,\n)\n\n# fit the model and get the separating hyperplane\nclf = (kernel=\"linear\", C=1.0)\nclf.fit(X, y)\n\n# fit the model and get the separating hyperplane using weighted classes\nwclf = (kernel=\"linear\", class_weight={1: 10})\nwclf.fit(X, y)\n\n# plot the samples\n(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, edgecolors=\"k\")\n\n# plot the decision functions for both classifiers\nax = ()\ndisp = (\n    clf,\n    X,\n    plot_method=\"contour\",\n    colors=\"k\",\n    levels=[0],\n    alpha=0.5,\n    linestyles=[\"-\"],\n    ax=ax,\n)\n\n# plot decision boundary and margins for weighted classes\nwdisp = (\n    wclf,\n    X,\n    plot_method=\"contour\",\n    colors=\"r\",\n    levels=[0],\n    alpha=0.5,\n    linestyles=[\"-\"],\n    ax=ax,\n)\n\n(\n    [disp.surface_.collections[0], wdisp.surface_.collections[0]],\n    [\"non weighted\", \"weighted\"],\n    loc=\"upper right\",\n)\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.169 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Support Vector Machines->SVM: Weighted samples": [
        [
            "Plot decision function of a weighted dataset, where the size of points\nis proportional to its weight.",
            "markdown"
        ],
        [
            "The sample weighting rescales the C parameter, which means that the classifier\nputs more emphasis on getting these points right. The effect might often be\nsubtle.\nTo emphasize the effect here, we particularly weight outliers, making the\ndeformation of the decision boundary very visible.\n<img alt=\"Constant weights, Modified weights\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_weighted_samples_001.png\" srcset=\"../../_images/sphx_glr_plot_weighted_samples_001.png\"/>",
            "markdown"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\n\n\ndef plot_decision_function(classifier, sample_weight, axis, title):\n    # plot the decision function\n    xx, yy = ((-4, 5, 500), (-4, 5, 500))\n\n    Z = classifier.decision_function([xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    # plot the line, the points, and the nearest vectors to the plane\n    axis.contourf(xx, yy, Z, alpha=0.75, cmap=plt.cm.bone)\n    axis.scatter(\n        X[:, 0],\n        X[:, 1],\n        c=y,\n        s=100 * sample_weight,\n        alpha=0.9,\n        cmap=plt.cm.bone,\n        edgecolors=\"black\",\n    )\n\n    axis.axis(\"off\")\n    axis.set_title(title)\n\n\n# we create 20 points\n(0)\nX = [(10, 2) + [1, 1], (10, 2)]\ny = [1] * 10 + [-1] * 10\nsample_weight_last_ten = abs((len(X)))\nsample_weight_constant = (len(X))\n# and bigger weights to some outliers\nsample_weight_last_ten[15:] *= 5\nsample_weight_last_ten[9] *= 15\n\n# Fit the models.\n\n# This model does not take into account sample weights.\nclf_no_weights = (gamma=1)\nclf_no_weights.fit(X, y)\n\n# This other model takes into account some dedicated sample weights.\nclf_weights = (gamma=1)\nclf_weights.fit(X, y, sample_weight=sample_weight_last_ten)\n\nfig, axes = (1, 2, figsize=(14, 6))\nplot_decision_function(\n    clf_no_weights, sample_weight_constant, axes[0], \"Constant weights\"\n)\nplot_decision_function(clf_weights, sample_weight_last_ten, axes[1], \"Modified weights\")\n\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.499 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Support Vector Machines->Scaling the regularization parameter for SVCs": [
        [
            "The following example illustrates the effect of scaling the\nregularization parameter when using  for\n.\nFor SVC classification, we are interested in a risk minimization for the\nequation:\n\n\\[C \\sum_{i=1, n} \\mathcal{L} (f(x_i), y_i) + \\Omega (w)\\]",
            "markdown"
        ],
        [
            "where",
            "markdown"
        ],
        [
            "\\(C\\) is used to set the amount of regularization",
            "markdown"
        ],
        [
            "\\(\\mathcal{L}\\) is a loss function of our samples\nand our model parameters.",
            "markdown"
        ],
        [
            "\\(\\Omega\\) is a penalty function of our model parameters\n\n</blockquote>",
            "markdown"
        ],
        [
            "If we consider the loss function to be the individual error per\nsample, then the data-fit term, or the sum of the error for each sample, will\nincrease as we add more samples. The penalization term, however, will not\nincrease.",
            "markdown"
        ],
        [
            "When using, for example, , to\nset the amount of regularization with C, there will be a\ndifferent amount of samples between the main problem and the smaller problems\nwithin the folds of the cross validation.",
            "markdown"
        ],
        [
            "Since our loss function is dependent on the amount of samples, the latter\nwill influence the selected value of C.\nThe question that arises is \u201cHow do we optimally adjust C to\naccount for the different amount of training samples?\u201d",
            "markdown"
        ],
        [
            "In the remainder of this example, we will investigate the effect of scaling\nthe value of the regularization parameter C in regards to the number of\nsamples for both L1 and L2 penalty. We will generate some synthetic datasets\nthat are appropriate for each type of regularization.",
            "markdown"
        ],
        [
            "# Author: Andreas Mueller &lt;amueller@ais.uni-bonn.de\n#         Jaques Grobler &lt;jaques.grobler@inria.fr\n# License: BSD 3 clause",
            "code"
        ]
    ],
    "Examples->Support Vector Machines->Scaling the regularization parameter for SVCs->L1-penalty case": [
        [
            "In the L1 case, theory says that prediction consistency (i.e. that under\ngiven hypothesis, the estimator learned predicts as well as a model knowing\nthe true distribution) is not possible because of the bias of the L1. It\ndoes say, however, that model consistency, in terms of finding the right set\nof non-zero parameters as well as their signs, can be achieved by scaling\nC.",
            "markdown"
        ],
        [
            "We will demonstrate this effect by using a synthetic dataset. This\ndataset will be sparse, meaning that only a few features will be informative\nand useful for the model.",
            "markdown"
        ],
        [
            "from sklearn.datasets import \n\nn_samples, n_features = 100, 300\nX, y = (\n    n_samples=n_samples, n_features=n_features, n_informative=5, random_state=1\n)",
            "code"
        ],
        [
            "Now, we can define a linear SVC with the l1 penalty.",
            "markdown"
        ],
        [
            "from sklearn.svm import \n\nmodel_l1 = (penalty=\"l1\", loss=\"squared_hinge\", dual=False, tol=1e-3)",
            "code"
        ],
        [
            "We will compute the mean test score for different values of C.",
            "markdown"
        ],
        [
            "import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import , \n\nCs = (-2.3, -1.3, 10)\ntrain_sizes = (0.3, 0.7, 3)\nlabels = [f\"fraction: {train_size}\" for train_size in train_sizes]\n\nresults = {\"C\": Cs}\nfor label, train_size in zip(labels, train_sizes):\n    cv = (train_size=train_size, test_size=0.3, n_splits=50, random_state=1)\n    train_scores, test_scores = (\n        model_l1, X, y, param_name=\"C\", param_range=Cs, cv=cv\n    )\n    results[label] = test_scores.mean(axis=1)\nresults = (results)",
            "code"
        ],
        [
            "import matplotlib.pyplot as plt\n\nfig, axes = (nrows=1, ncols=2, sharey=True, figsize=(12, 6))\n\n# plot results without scaling C\nresults.plot(x=\"C\", ax=axes[0], logx=True)\naxes[0].set_ylabel(\"CV score\")\naxes[0].set_title(\"No scaling\")\n\n# plot results by scaling C\nfor train_size_idx, label in enumerate(labels):\n    results_scaled = results[[label]].assign(\n        C_scaled=Cs * float(n_samples * train_sizes[train_size_idx])\n    )\n    results_scaled.plot(x=\"C_scaled\", ax=axes[1], logx=True, label=label)\naxes[1].set_title(\"Scaling C by 1 / n_samples\")\n\n_ = fig.suptitle(\"Effect of scaling C with L1 penalty\")\n\n\n<img alt=\"Effect of scaling C with L1 penalty, No scaling, Scaling C by 1 / n_samples\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_svm_scale_c_001.png\" srcset=\"../../_images/sphx_glr_plot_svm_scale_c_001.png\"/>",
            "code"
        ],
        [
            "Here, we observe that the cross-validation-error correlates best with the\ntest-error, when scaling our C with the number of samples, n.",
            "markdown"
        ]
    ],
    "Examples->Support Vector Machines->Scaling the regularization parameter for SVCs->L2-penalty case": [
        [
            "We can repeat a similar experiment with the l2 penalty. In this case, we\ndon\u2019t need to use a sparse dataset.",
            "markdown"
        ],
        [
            "In this case, the theory says that in order to achieve prediction\nconsistency, the penalty parameter should be kept constant as the number of\nsamples grow.",
            "markdown"
        ],
        [
            "So we will repeat the same experiment by creating a linear SVC classifier\nwith the l2 penalty and check the test score via cross-validation and\nplot the results with and without scaling the parameter C.",
            "markdown"
        ],
        [
            "rng = (1)\ny = (0.5 - rng.rand(n_samples))\nX = rng.randn(n_samples, n_features // 5) + y[:, ]\nX += 5 * rng.randn(n_samples, n_features // 5)",
            "code"
        ],
        [
            "model_l2 = (penalty=\"l2\", loss=\"squared_hinge\", dual=True)\nCs = (-4.5, -2, 10)\n\nlabels = [f\"fraction: {train_size}\" for train_size in train_sizes]\nresults = {\"C\": Cs}\nfor label, train_size in zip(labels, train_sizes):\n    cv = (train_size=train_size, test_size=0.3, n_splits=50, random_state=1)\n    train_scores, test_scores = (\n        model_l2, X, y, param_name=\"C\", param_range=Cs, cv=cv\n    )\n    results[label] = test_scores.mean(axis=1)\nresults = (results)",
            "code"
        ],
        [
            "import matplotlib.pyplot as plt\n\nfig, axes = (nrows=1, ncols=2, sharey=True, figsize=(12, 6))\n\n# plot results without scaling C\nresults.plot(x=\"C\", ax=axes[0], logx=True)\naxes[0].set_ylabel(\"CV score\")\naxes[0].set_title(\"No scaling\")\n\n# plot results by scaling C\nfor train_size_idx, label in enumerate(labels):\n    results_scaled = results[[label]].assign(\n        C_scaled=Cs * float(n_samples * train_sizes[train_size_idx])\n    )\n    results_scaled.plot(x=\"C_scaled\", ax=axes[1], logx=True, label=label)\naxes[1].set_title(\"Scaling C by 1 / n_samples\")\n\n_ = fig.suptitle(\"Effect of scaling C with L2 penalty\")\n\n\n<img alt=\"Effect of scaling C with L2 penalty, No scaling, Scaling C by 1 / n_samples\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_svm_scale_c_002.png\" srcset=\"../../_images/sphx_glr_plot_svm_scale_c_002.png\"/>",
            "code"
        ],
        [
            "So or the L2 penalty case, the best result comes from the case where C is\nnot scaled.",
            "markdown"
        ],
        [
            "()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  5.944 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Support Vector Machines->Support Vector Regression (SVR) using linear and non-linear kernels": [
        [
            "Toy example of 1D regression using linear, polynomial and RBF kernels.",
            "markdown"
        ],
        [
            "import numpy as np\nfrom sklearn.svm import \nimport matplotlib.pyplot as plt",
            "code"
        ]
    ],
    "Examples->Support Vector Machines->Support Vector Regression (SVR) using linear and non-linear kernels->Generate sample data": [
        [
            "X = (5 * (40, 1), axis=0)\ny = (X).ravel()\n\n# add noise to targets\ny[::5] += 3 * (0.5 - (8))",
            "code"
        ]
    ],
    "Examples->Support Vector Machines->Support Vector Regression (SVR) using linear and non-linear kernels->Fit regression model": [
        [
            "svr_rbf = (kernel=\"rbf\", C=100, gamma=0.1, epsilon=0.1)\nsvr_lin = (kernel=\"linear\", C=100, gamma=\"auto\")\nsvr_poly = (kernel=\"poly\", C=100, gamma=\"auto\", degree=3, epsilon=0.1, coef0=1)",
            "code"
        ]
    ],
    "Examples->Support Vector Machines->Support Vector Regression (SVR) using linear and non-linear kernels->Look at the results": [
        [
            "lw = 2\n\nsvrs = [svr_rbf, svr_lin, svr_poly]\nkernel_label = [\"RBF\", \"Linear\", \"Polynomial\"]\nmodel_color = [\"m\", \"c\", \"g\"]\n\nfig, axes = (nrows=1, ncols=3, figsize=(15, 10), sharey=True)\nfor ix, svr in enumerate(svrs):\n    axes[ix].plot(\n        X,\n        svr.fit(X, y).predict(X),\n        color=model_color[ix],\n        lw=lw,\n        label=\"{} model\".format(kernel_label[ix]),\n    )\n    axes[ix].scatter(\n        X[svr.support_],\n        y[svr.support_],\n        facecolor=\"none\",\n        edgecolor=model_color[ix],\n        s=50,\n        label=\"{} support vectors\".format(kernel_label[ix]),\n    )\n    axes[ix].scatter(\n        X[((len(X)), svr.support_)],\n        y[((len(X)), svr.support_)],\n        facecolor=\"none\",\n        edgecolor=\"k\",\n        s=50,\n        label=\"other training data\",\n    )\n    axes[ix].legend(\n        loc=\"upper center\",\n        bbox_to_anchor=(0.5, 1.1),\n        ncol=1,\n        fancybox=True,\n        shadow=True,\n    )\n\nfig.text(0.5, 0.04, \"data\", ha=\"center\", va=\"center\")\nfig.text(0.06, 0.5, \"target\", ha=\"center\", va=\"center\", rotation=\"vertical\")\nfig.suptitle(\"Support Vector Regression\", fontsize=14)\n()\n\n\n<img alt=\"Support Vector Regression\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_svm_regression_001.png\" srcset=\"../../_images/sphx_glr_plot_svm_regression_001.png\"/>",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  2.575 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Tutorial exercises->Cross-validation on Digits Dataset Exercise": [
        [
            "A tutorial exercise using Cross-validation with an SVM on the Digits dataset.",
            "markdown"
        ],
        [
            "This exercise is used in the  part of the\n section of the .\n<img alt=\"plot cv digits\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cv_digits_001.png\" srcset=\"../../_images/sphx_glr_plot_cv_digits_001.png\"/>",
            "markdown"
        ],
        [
            "import numpy as np\nfrom sklearn.model_selection import \nfrom sklearn import datasets, svm\n\nX, y = (return_X_y=True)\n\nsvc = (kernel=\"linear\")\nC_s = (-10, 0, 10)\n\nscores = list()\nscores_std = list()\nfor C in C_s:\n    svc.C = C\n    this_scores = (svc, X, y, n_jobs=1)\n    scores.append((this_scores))\n    scores_std.append((this_scores))\n\n# Do the plotting\nimport matplotlib.pyplot as plt\n\n()\n(C_s, scores)\n(C_s, (scores) + (scores_std), \"b--\")\n(C_s, (scores) - (scores_std), \"b--\")\nlocs, labels = ()\n(locs, list(map(lambda x: \"%g\" % x, locs)))\n(\"CV score\")\n(\"Parameter C\")\n(0, 1.1)\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  4.692 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Tutorial exercises->Cross-validation on diabetes Dataset Exercise": [
        [
            "A tutorial exercise which uses cross-validation with linear models.",
            "markdown"
        ],
        [
            "This exercise is used in the  part of the\n section of the .",
            "markdown"
        ]
    ],
    "Examples->Tutorial exercises->Cross-validation on diabetes Dataset Exercise->Load dataset and apply GridSearchCV": [
        [
            "import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn import datasets\nfrom sklearn.linear_model import \nfrom sklearn.model_selection import \n\nX, y = (return_X_y=True)\nX = X[:150]\ny = y[:150]\n\nlasso = (random_state=0, max_iter=10000)\nalphas = (-4, -0.5, 30)\n\ntuned_parameters = [{\"alpha\": alphas}]\nn_folds = 5\n\nclf = (lasso, tuned_parameters, cv=n_folds, refit=False)\nclf.fit(X, y)\nscores = clf.cv_results_[\"mean_test_score\"]\nscores_std = clf.cv_results_[\"std_test_score\"]",
            "code"
        ]
    ],
    "Examples->Tutorial exercises->Cross-validation on diabetes Dataset Exercise->Plot error lines showing +/- std. errors of the scores": [
        [
            "().set_size_inches(8, 6)\n(alphas, scores)\n\nstd_error = scores_std / (n_folds)\n\n(alphas, scores + std_error, \"b--\")\n(alphas, scores - std_error, \"b--\")\n\n# alpha=0.2 controls the translucency of the fill color\n(alphas, scores + std_error, scores - std_error, alpha=0.2)\n\n(\"CV score +/- std error\")\n(\"alpha\")\n(np.max(scores), linestyle=\"--\", color=\".5\")\n([alphas[0], alphas[-1]])\n\n\n<img alt=\"plot cv diabetes\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cv_diabetes_001.png\" srcset=\"../../_images/sphx_glr_plot_cv_diabetes_001.png\"/>",
            "code"
        ],
        [
            "(9.999999999999999e-05, 0.31622776601683794)",
            "code"
        ]
    ],
    "Examples->Tutorial exercises->Cross-validation on diabetes Dataset Exercise->Bonus: how much can you trust the selection of alpha?": [
        [
            "# To answer this question we use the LassoCV object that sets its alpha\n# parameter automatically from the data by internal cross-validation (i.e. it\n# performs cross-validation on the training data it receives).\n# We use external cross-validation to see how much the automatically obtained\n# alphas differ across different cross-validation folds.\n\nfrom sklearn.linear_model import \nfrom sklearn.model_selection import \n\nlasso_cv = (alphas=alphas, random_state=0, max_iter=10000)\nk_fold = (3)\n\nprint(\"Answer to the bonus question:\", \"how much can you trust the selection of alpha?\")\nprint()\nprint(\"Alpha parameters maximising the generalization score on different\")\nprint(\"subsets of the data:\")\nfor k, (train, test) in enumerate(k_fold.split(X, y)):\n    lasso_cv.fit(X[train], y[train])\n    print(\n        \"[fold {0}] alpha: {1:.5f}, score: {2:.5f}\".format(\n            k, lasso_cv.alpha_, lasso_cv.score(X[test], y[test])\n        )\n    )\nprint()\nprint(\"Answer: Not very much since we obtained different alphas for different\")\nprint(\"subsets of the data and moreover, the scores for these alphas differ\")\nprint(\"quite substantially.\")\n\n()",
            "code"
        ],
        [
            "Answer to the bonus question: how much can you trust the selection of alpha?\n\nAlpha parameters maximising the generalization score on different\nsubsets of the data:\n[fold 0] alpha: 0.05968, score: 0.54209\n[fold 1] alpha: 0.04520, score: 0.15521\n[fold 2] alpha: 0.07880, score: 0.45192\n\nAnswer: Not very much since we obtained different alphas for different\nsubsets of the data and moreover, the scores for these alphas differ\nquite substantially.",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.473 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Tutorial exercises->Digits Classification Exercise": [
        [
            "A tutorial exercise regarding the use of classification techniques on\nthe Digits dataset.",
            "markdown"
        ],
        [
            "This exercise is used in the  part of the\n section of the\n.",
            "markdown"
        ],
        [
            "KNN score: 0.961111\nLogisticRegression score: 0.933333\n\n\n\n<br/>",
            "code"
        ],
        [
            "from sklearn import datasets, neighbors, linear_model\n\nX_digits, y_digits = (return_X_y=True)\nX_digits = X_digits / X_digits.max()\n\nn_samples = len(X_digits)\n\nX_train = X_digits[: int(0.9 * n_samples)]\ny_train = y_digits[: int(0.9 * n_samples)]\nX_test = X_digits[int(0.9 * n_samples) :]\ny_test = y_digits[int(0.9 * n_samples) :]\n\nknn = ()\nlogistic = (max_iter=1000)\n\nprint(\"KNN score: %f\" % knn.fit(X_train, y_train).score(X_test, y_test))\nprint(\n    \"LogisticRegression score: %f\"\n    % logistic.fit(X_train, y_train).score(X_test, y_test)\n)",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.140 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Tutorial exercises->SVM Exercise": [
        [
            "A tutorial exercise for using different SVM kernels.",
            "markdown"
        ],
        [
            "This exercise is used in the  part of the\n section of the .\n\n<img alt=\"linear\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_iris_exercise_001.png\" srcset=\"../../_images/sphx_glr_plot_iris_exercise_001.png\"/>\n<img alt=\"rbf\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_iris_exercise_002.png\" srcset=\"../../_images/sphx_glr_plot_iris_exercise_002.png\"/>\n<img alt=\"poly\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_iris_exercise_003.png\" srcset=\"../../_images/sphx_glr_plot_iris_exercise_003.png\"/>",
            "markdown"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets, svm\n\niris = ()\nX = iris.data\ny = iris.target\n\nX = X[y != 0, :2]\ny = y[y != 0]\n\nn_sample = len(X)\n\n(0)\norder = (n_sample)\nX = X[order]\ny = y[order].astype(float)\n\nX_train = X[: int(0.9 * n_sample)]\ny_train = y[: int(0.9 * n_sample)]\nX_test = X[int(0.9 * n_sample) :]\ny_test = y[int(0.9 * n_sample) :]\n\n# fit the model\nfor kernel in (\"linear\", \"rbf\", \"poly\"):\n    clf = (kernel=kernel, gamma=10)\n    clf.fit(X_train, y_train)\n\n    ()\n    ()\n    (\n        X[:, 0], X[:, 1], c=y, zorder=10, cmap=plt.cm.Paired, edgecolor=\"k\", s=20\n    )\n\n    # Circle out the test data\n    (\n        X_test[:, 0], X_test[:, 1], s=80, facecolors=\"none\", zorder=10, edgecolor=\"k\"\n    )\n\n    (\"tight\")\n    x_min = X[:, 0].min()\n    x_max = X[:, 0].max()\n    y_min = X[:, 1].min()\n    y_max = X[:, 1].max()\n\n    XX, YY = [x_min:x_max:200j, y_min:y_max:200j]\n    Z = clf.decision_function([XX.ravel(), YY.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(XX.shape)\n    (XX, YY, Z  0, cmap=plt.cm.Paired)\n    (\n        XX,\n        YY,\n        Z,\n        colors=[\"k\", \"k\", \"k\"],\n        linestyles=[\"--\", \"-\", \"--\"],\n        levels=[-0.5, 0, 0.5],\n    )\n\n    (kernel)\n()",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  5.009 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Working with text documents->Classification of text documents using sparse features": [
        [
            "This is an example showing how scikit-learn can be used to classify documents by\ntopics using a . This example uses a\nTf-idf-weighted document-term sparse matrix to encode the features and\ndemonstrates various classifiers that can efficiently handle sparse matrices.",
            "markdown"
        ],
        [
            "For document analysis via an unsupervised learning approach, see the example\nscript .",
            "markdown"
        ],
        [
            "# Author: Peter Prettenhofer &lt;peter.prettenhofer@gmail.com\n#         Olivier Grisel &lt;olivier.grisel@ensta.org\n#         Mathieu Blondel &lt;mathieu@mblondel.org\n#         Arturo Amor &lt;david-arturo.amor-quiroz@inria.fr\n#         Lars Buitinck\n# License: BSD 3 clause",
            "code"
        ]
    ],
    "Examples->Working with text documents->Classification of text documents using sparse features->Loading and vectorizing the 20 newsgroups text dataset": [
        [
            "We define a function to load data from , which\ncomprises around 18,000 newsgroups posts on 20 topics split in two subsets:\none for training (or development) and the other one for testing (or for\nperformance evaluation). Note that, by default, the text samples contain some\nmessage metadata such as 'headers', 'footers' (signatures) and 'quotes'\nto other posts. The fetch_20newsgroups function therefore accepts a\nparameter named remove to attempt stripping such information that can make\nthe classification problem \u201ctoo easy\u201d. This is achieved using simple\nheuristics that are neither perfect nor standard, hence disabled by default.",
            "markdown"
        ],
        [
            "from sklearn.datasets import \nfrom sklearn.feature_extraction.text import \nfrom time import \n\ncategories = [\n    \"alt.atheism\",\n    \"talk.religion.misc\",\n    \"comp.graphics\",\n    \"sci.space\",\n]\n\n\ndef size_mb(docs):\n    return sum(len(s.encode(\"utf-8\")) for s in docs) / 1e6\n\n\ndef load_dataset(verbose=False, remove=()):\n    \"\"\"Load and vectorize the 20 newsgroups dataset.\"\"\"\n\n    data_train = (\n        subset=\"train\",\n        categories=categories,\n        shuffle=True,\n        random_state=42,\n        remove=remove,\n    )\n\n    data_test = (\n        subset=\"test\",\n        categories=categories,\n        shuffle=True,\n        random_state=42,\n        remove=remove,\n    )\n\n    # order of labels in `target_names` can be different from `categories`\n    target_names = data_train.target_names\n\n    # split target in a training set and a test set\n    y_train, y_test = data_train.target, data_test.target\n\n    # Extracting features from the training data using a sparse vectorizer\n    t0 = ()\n    vectorizer = (\n        sublinear_tf=True, max_df=0.5, min_df=5, stop_words=\"english\"\n    )\n    X_train = vectorizer.fit_transform(data_train.data)\n    duration_train = () - t0\n\n    # Extracting features from the test data using the same vectorizer\n    t0 = ()\n    X_test = vectorizer.transform(data_test.data)\n    duration_test = () - t0\n\n    feature_names = vectorizer.get_feature_names_out()\n\n    if verbose:\n\n        # compute size of loaded data\n        data_train_size_mb = size_mb(data_train.data)\n        data_test_size_mb = size_mb(data_test.data)\n\n        print(\n            f\"{len(data_train.data)} documents - \"\n            f\"{data_train_size_mb:.2f}MB (training set)\"\n        )\n        print(f\"{len(data_test.data)} documents - {data_test_size_mb:.2f}MB (test set)\")\n        print(f\"{len(target_names)} categories\")\n        print(\n            f\"vectorize training done in {duration_train:.3f}s \"\n            f\"at {data_train_size_mb / duration_train:.3f}MB/s\"\n        )\n        print(f\"n_samples: {X_train.shape[0]}, n_features: {X_train.shape[1]}\")\n        print(\n            f\"vectorize testing done in {duration_test:.3f}s \"\n            f\"at {data_test_size_mb / duration_test:.3f}MB/s\"\n        )\n        print(f\"n_samples: {X_test.shape[0]}, n_features: {X_test.shape[1]}\")\n\n    return X_train, X_test, y_train, y_test, feature_names, target_names",
            "code"
        ]
    ],
    "Examples->Working with text documents->Classification of text documents using sparse features->Analysis of a bag-of-words document classifier": [
        [
            "We will now train a classifier twice, once on the text samples including\nmetadata and once after stripping the metadata. For both cases we will analyze\nthe classification errors on a test set using a confusion matrix and inspect\nthe coefficients that define the classification function of the trained\nmodels.",
            "markdown"
        ]
    ],
    "Examples->Working with text documents->Classification of text documents using sparse features->Analysis of a bag-of-words document classifier->Model without metadata stripping": [
        [
            "We start by using the custom function load_dataset to load the data without\nmetadata stripping.",
            "markdown"
        ],
        [
            "X_train, X_test, y_train, y_test, feature_names, target_names = load_dataset(\n    verbose=True\n)",
            "code"
        ],
        [
            "2034 documents - 3.98MB (training set)\n1353 documents - 2.87MB (test set)\n4 categories\nvectorize training done in 0.370s at 10.743MB/s\nn_samples: 2034, n_features: 7831\nvectorize testing done in 0.234s at 12.267MB/s\nn_samples: 1353, n_features: 7831",
            "code"
        ],
        [
            "Our first model is an instance of the\n class. This is a linear\nclassification model that uses the mean squared error on {-1, 1} encoded\ntargets, one for each possible class. Contrary to\n,\n does not\nprovide probabilistic predictions (no predict_proba method),\nbut it is often faster to train.",
            "markdown"
        ],
        [
            "from sklearn.linear_model import \n\nclf = (tol=1e-2, solver=\"sparse_cg\")\nclf.fit(X_train, y_train)\npred = clf.predict(X_test)",
            "code"
        ],
        [
            "We plot the confusion matrix of this classifier to find if there is a pattern\nin the classification errors.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nfig, ax = (figsize=(10, 5))\n(y_test, pred, ax=ax)\nax.xaxis.set_ticklabels(target_names)\nax.yaxis.set_ticklabels(target_names)\n_ = ax.set_title(\n    f\"Confusion Matrix for {clf.__class__.__name__}\\non the original documents\"\n)\n\n\n<img alt=\"Confusion Matrix for RidgeClassifier on the original documents\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_document_classification_20newsgroups_001.png\" srcset=\"../../_images/sphx_glr_plot_document_classification_20newsgroups_001.png\"/>",
            "code"
        ],
        [
            "The confusion matrix highlights that documents of the alt.atheism class are\noften confused with documents with the class talk.religion.misc class and\nvice-versa which is expected since the topics are semantically related.",
            "markdown"
        ],
        [
            "We also observe that some documents of the sci.space class can be misclassified as\ncomp.graphics while the converse is much rarer. A manual inspection of those\nbadly classified documents would be required to get some insights on this\nasymmetry. It could be the case that the vocabulary of the space topic could\nbe more specific than the vocabulary for computer graphics.",
            "markdown"
        ],
        [
            "We can gain a deeper understanding of how this classifier makes its decisions\nby looking at the words with the highest average feature effects:",
            "markdown"
        ],
        [
            "import pandas as pd\nimport numpy as np\n\n\ndef plot_feature_effects():\n    # learned coefficients weighted by frequency of appearance\n    average_feature_effects = clf.coef_ * (X_train.mean(axis=0)).ravel()\n\n    for i, label in enumerate(target_names):\n        top5 = (average_feature_effects[i])[-5:][::-1]\n        if i == 0:\n            top = (feature_names[top5], columns=[label])\n            top_indices = top5\n        else:\n            top[label] = feature_names[top5]\n            top_indices = ((top_indices, top5), axis=None)\n    top_indices = (top_indices)\n    predictive_words = feature_names[top_indices]\n\n    # plot feature effects\n    bar_size = 0.25\n    padding = 0.75\n    y_locs = (len(top_indices)) * (4 * bar_size + padding)\n\n    fig, ax = (figsize=(10, 8))\n    for i, label in enumerate(target_names):\n        ax.barh(\n            y_locs + (i - 2) * bar_size,\n            average_feature_effects[i, top_indices],\n            height=bar_size,\n            label=label,\n        )\n    ax.set(\n        yticks=y_locs,\n        yticklabels=predictive_words,\n        ylim=[\n            0 - 4 * bar_size,\n            len(top_indices) * (4 * bar_size + padding) - 4 * bar_size,\n        ],\n    )\n    ax.legend(loc=\"lower right\")\n\n    print(\"top 5 keywords per class:\")\n    print(top)\n\n    return ax\n\n\n_ = plot_feature_effects().set_title(\"Average feature effect on the original data\")\n\n\n<img alt=\"Average feature effect on the original data\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_document_classification_20newsgroups_002.png\" srcset=\"../../_images/sphx_glr_plot_document_classification_20newsgroups_002.png\"/>",
            "code"
        ],
        [
            "top 5 keywords per class:\n  alt.atheism comp.graphics sci.space talk.religion.misc\n0       keith      graphics     space          christian\n1         god    university      nasa                com\n2    atheists        thanks     orbit                god\n3      people          does      moon           morality\n4     caltech         image    access             people",
            "code"
        ],
        [
            "We can observe that the most predictive words are often strongly positively\nassociated with a single class and negatively associated with all the other\nclasses. Most of those positive associations are quite easy to interpret.\nHowever, some words such as \"god\" and \"people\" are positively associated to\nboth \"talk.misc.religion\" and \"alt.atheism\" as those two classes expectedly\nshare some common vocabulary. Notice however that there are also words such as\n\"christian\" and \"morality\" that are only positively associated with\n\"talk.misc.religion\". Furthermore, in this version of the dataset, the word\n\"caltech\" is one of the top predictive features for atheism due to pollution\nin the dataset coming from some sort of metadata such as the email addresses\nof the sender of previous emails in the discussion as can be seen below:",
            "markdown"
        ],
        [
            "data_train = (\n    subset=\"train\", categories=categories, shuffle=True, random_state=42\n)\n\nfor doc in data_train.data:\n    if \"caltech\" in doc:\n        print(doc)\n        break",
            "code"
        ],
        [
            "From: livesey@solntze.wpd.sgi.com (Jon Livesey)\nSubject: Re: Morality? (was Re: &lt;Political Atheists?)\nOrganization: sgi\nLines: 93\nDistribution: world\nNNTP-Posting-Host: solntze.wpd.sgi.com\n\nIn article &lt;1qlettINN8oi@gap.caltech.edu, keith@cco.caltech.edu (Keith Allan Schneider) writes:\n| livesey@solntze.wpd.sgi.com (Jon Livesey) writes:\n|\n| Explain to me\n| how instinctive acts can be moral acts, and I am happy to listen.\n| For example, if it were instinctive not to murder...\n| \n| Then not murdering would have no moral significance, since there\n| would be nothing voluntary about it.\n|\n| See, there you go again, saying that a moral act is only significant\n| if it is \"voluntary.\"  Why do you think this?\n\nIf you force me to do something, am I morally responsible for it?\n\n|\n| And anyway, humans have the ability to disregard some of their instincts.\n\nWell, make up your mind.    Is it to be \"instinctive not to murder\"\nor not?\n\n|\n| So, only intelligent beings can be moral, even if the bahavior of other\n| beings mimics theirs?\n| \n| You are starting to get the point.  Mimicry is not necessarily the\n| same as the action being imitated.  A Parrot saying \"Pretty Polly\"\n| isn't necessarily commenting on the pulchritude of Polly.\n|\n| You are attaching too many things to the term \"moral,\" I think.\n| Let's try this:  is it \"good\" that animals of the same species\n| don't kill each other.  Or, do you think this is right?\n\nIt's not even correct.    Animals of the same species do kill\none another.\n\n|\n| Or do you think that animals are machines, and that nothing they do\n| is either right nor wrong?\n\nSigh.   I wonder how many times we have been round this loop.\n\nI think that instinctive bahaviour has no moral significance.\nI am quite prepared to believe that higher animals, such as\nprimates, have the beginnings of a moral sense, since they seem\nto exhibit self-awareness.\n\n|\n|\n| Animals of the same species could kill each other arbitarily, but\n| they don't.\n| \n| They do.  I and other posters have given you many examples of exactly\n| this, but you seem to have a very short memory.\n|\n| Those weren't arbitrary killings.  They were slayings related to some\n| sort of mating ritual or whatnot.\n\nSo what?     Are you trying to say that some killing in animals\nhas a moral significance and some does not?   Is this your\nnatural morality\n\n\n|\n| Are you trying to say that this isn't an act of morality because\n| most animals aren't intelligent enough to think like we do?\n| \n| I'm saying:\n|     \"There must be the possibility that the organism - it's not\n|     just people we are talking about - can consider alternatives.\"\n| \n| It's right there in the posting you are replying to.\n|\n| Yes it was, but I still don't understand your distinctions.  What\n| do you mean by \"consider?\"  Can a small child be moral?  How about\n| a gorilla?  A dolphin?  A platypus?  Where is the line drawn?  Does\n| the being need to be self aware?\n\nAre you blind?   What do you think that this sentence means?\n\n        \"There must be the possibility that the organism - it's not\n        just people we are talking about - can consider alternatives.\"\n\nWhat would that imply?\n\n|\n| What *do* you call the mechanism which seems to prevent animals of\n| the same species from (arbitrarily) killing each other?  Don't\n| you find the fact that they don't at all significant?\n\nI find the fact that they do to be significant.\n\njon.",
            "code"
        ],
        [
            "Such headers, signature footers (and quoted metadata from previous messages)\ncan be considered side information that artificially reveals the newsgroup by\nidentifying the registered members and one would rather want our text\nclassifier to only learn from the \u201cmain content\u201d of each text document instead\nof relying on the leaked identity of the writers.",
            "markdown"
        ]
    ],
    "Examples->Working with text documents->Classification of text documents using sparse features->Analysis of a bag-of-words document classifier->Model with metadata stripping": [
        [
            "The remove option of the 20 newsgroups dataset loader in scikit-learn allows\nto heuristically attempt to filter out some of this unwanted metadata that\nmakes the classification problem artificially easier. Be aware that such\nfiltering of the text contents is far from perfect.",
            "markdown"
        ],
        [
            "Let us try to leverage this option to train a text classifier that does not\nrely too much on this kind of metadata to make its decisions:",
            "markdown"
        ],
        [
            "(\n    X_train,\n    X_test,\n    y_train,\n    y_test,\n    feature_names,\n    target_names,\n) = load_dataset(remove=(\"headers\", \"footers\", \"quotes\"))\n\nclf = (tol=1e-2, solver=\"sparse_cg\")\nclf.fit(X_train, y_train)\npred = clf.predict(X_test)\n\nfig, ax = (figsize=(10, 5))\n(y_test, pred, ax=ax)\nax.xaxis.set_ticklabels(target_names)\nax.yaxis.set_ticklabels(target_names)\n_ = ax.set_title(\n    f\"Confusion Matrix for {clf.__class__.__name__}\\non filtered documents\"\n)\n\n\n<img alt=\"Confusion Matrix for RidgeClassifier on filtered documents\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_document_classification_20newsgroups_003.png\" srcset=\"../../_images/sphx_glr_plot_document_classification_20newsgroups_003.png\"/>",
            "code"
        ],
        [
            "By looking at the confusion matrix, it is more evident that the scores of the\nmodel trained with metadata were over-optimistic. The classification problem\nwithout access to the metadata is less accurate but more representative of the\nintended text classification problem.",
            "markdown"
        ],
        [
            "_ = plot_feature_effects().set_title(\"Average feature effects on filtered documents\")\n\n\n<img alt=\"Average feature effects on filtered documents\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_document_classification_20newsgroups_004.png\" srcset=\"../../_images/sphx_glr_plot_document_classification_20newsgroups_004.png\"/>",
            "code"
        ],
        [
            "top 5 keywords per class:\n  alt.atheism comp.graphics sci.space talk.religion.misc\n0         don      graphics     space                god\n1      people          file      like          christian\n2         say        thanks      nasa              jesus\n3    religion         image     orbit         christians\n4        post          does    launch              wrong",
            "code"
        ],
        [
            "In the next section we keep the dataset without metadata to compare several\nclassifiers.",
            "markdown"
        ]
    ],
    "Examples->Working with text documents->Classification of text documents using sparse features->Benchmarking classifiers": [
        [
            "Scikit-learn provides many different kinds of classification algorithms. In\nthis section we will train a selection of those classifiers on the same text\nclassification problem and measure both their generalization performance\n(accuracy on the test set) and their computation performance (speed), both at\ntraining time and testing time. For such purpose we define the following\nbenchmarking utilities:",
            "markdown"
        ],
        [
            "from sklearn.utils.extmath import \nfrom sklearn import metrics\n\n\ndef benchmark(clf, custom_name=False):\n    print(\"_\" * 80)\n    print(\"Training: \")\n    print(clf)\n    t0 = ()\n    clf.fit(X_train, y_train)\n    train_time = () - t0\n    print(f\"train time: {train_time:.3}s\")\n\n    t0 = ()\n    pred = clf.predict(X_test)\n    test_time = () - t0\n    print(f\"test time:  {test_time:.3}s\")\n\n    score = (y_test, pred)\n    print(f\"accuracy:   {score:.3}\")\n\n    if hasattr(clf, \"coef_\"):\n        print(f\"dimensionality: {clf.coef_.shape[1]}\")\n        print(f\"density: {(clf.coef_)}\")\n        print()\n\n    print()\n    if custom_name:\n        clf_descr = str(custom_name)\n    else:\n        clf_descr = clf.__class__.__name__\n    return clf_descr, score, train_time, test_time",
            "code"
        ],
        [
            "We now train and test the datasets with 8 different classification models and\nget performance results for each model. The goal of this study is to highlight\nthe computation/accuracy tradeoffs of different types of classifiers for\nsuch a multi-class text classification problem.",
            "markdown"
        ],
        [
            "Notice that the most important hyperparameters values were tuned using a grid\nsearch procedure not shown in this notebook for the sake of simplicity. See\nthe example script\n\nfor a demo on how such tuning can be done.",
            "markdown"
        ],
        [
            "from sklearn.linear_model import \nfrom sklearn.svm import \nfrom sklearn.linear_model import \nfrom sklearn.naive_bayes import \nfrom sklearn.neighbors import \nfrom sklearn.neighbors import \nfrom sklearn.ensemble import \n\n\nresults = []\nfor clf, name in (\n    ((C=5, max_iter=1000), \"Logistic Regression\"),\n    ((alpha=1.0, solver=\"sparse_cg\"), \"Ridge Classifier\"),\n    ((n_neighbors=100), \"kNN\"),\n    ((), \"Random Forest\"),\n    # L2 penalty Linear SVC\n    ((C=0.1, dual=False, max_iter=1000), \"Linear SVC\"),\n    # L2 penalty Linear SGD\n    (\n        (\n            loss=\"log_loss\", alpha=1e-4, n_iter_no_change=3, early_stopping=True\n        ),\n        \"log-loss SGD\",\n    ),\n    # NearestCentroid (aka Rocchio classifier)\n    ((), \"NearestCentroid\"),\n    # Sparse naive Bayes classifier\n    ((alpha=0.1), \"Complement naive Bayes\"),\n):\n    print(\"=\" * 80)\n    print(name)\n    results.append(benchmark(clf, name))",
            "code"
        ],
        [
            "================================================================================\nLogistic Regression\n________________________________________________________________________________\nTraining:\nLogisticRegression(C=5, max_iter=1000)\ntrain time: 0.31s\ntest time:  0.000581s\naccuracy:   0.773\ndimensionality: 5316\ndensity: 1.0\n\n\n================================================================================\nRidge Classifier\n________________________________________________________________________________\nTraining:\nRidgeClassifier(solver='sparse_cg')\ntrain time: 0.0358s\ntest time:  0.000641s\naccuracy:   0.76\ndimensionality: 5316\ndensity: 1.0\n\n\n================================================================================\nkNN\n________________________________________________________________________________\nTraining:\nKNeighborsClassifier(n_neighbors=100)\ntrain time: 0.000882s\ntest time:  5.86s\naccuracy:   0.753\n\n================================================================================\nRandom Forest\n________________________________________________________________________________\nTraining:\nRandomForestClassifier()\ntrain time: 1.19s\ntest time:  0.0644s\naccuracy:   0.704\n\n================================================================================\nLinear SVC\n________________________________________________________________________________\nTraining:\nLinearSVC(C=0.1, dual=False)\ntrain time: 0.0291s\ntest time:  0.000607s\naccuracy:   0.752\ndimensionality: 5316\ndensity: 1.0\n\n\n================================================================================\nlog-loss SGD\n________________________________________________________________________________\nTraining:\nSGDClassifier(early_stopping=True, loss='log_loss', n_iter_no_change=3)\ntrain time: 0.0268s\ntest time:  0.000557s\naccuracy:   0.762\ndimensionality: 5316\ndensity: 1.0\n\n\n================================================================================\nNearestCentroid\n________________________________________________________________________________\nTraining:\nNearestCentroid()\ntrain time: 0.00272s\ntest time:  0.00102s\naccuracy:   0.748\n\n================================================================================\nComplement naive Bayes\n________________________________________________________________________________\nTraining:\nComplementNB(alpha=0.1)\ntrain time: 0.00181s\ntest time:  0.000466s\naccuracy:   0.779",
            "code"
        ]
    ],
    "Examples->Working with text documents->Classification of text documents using sparse features->Plot accuracy, training and test time of each classifier": [
        [
            "The scatter plots show the trade-off between the test accuracy and the\ntraining and testing time of each classifier.",
            "markdown"
        ],
        [
            "indices = (len(results))\n\nresults = [[x[i] for x in results] for i in range(4)]\n\nclf_names, score, training_time, test_time = results\ntraining_time = (training_time)\ntest_time = (test_time)\n\nfig, ax1 = (figsize=(10, 8))\nax1.scatter(score, training_time, s=60)\nax1.set(\n    title=\"Score-training time trade-off\",\n    yscale=\"log\",\n    xlabel=\"test accuracy\",\n    ylabel=\"training time (s)\",\n)\nfig, ax2 = (figsize=(10, 8))\nax2.scatter(score, test_time, s=60)\nax2.set(\n    title=\"Score-test time trade-off\",\n    yscale=\"log\",\n    xlabel=\"test accuracy\",\n    ylabel=\"test time (s)\",\n)\n\nfor i, txt in enumerate(clf_names):\n    ax1.annotate(txt, (score[i], training_time[i]))\n    ax2.annotate(txt, (score[i], test_time[i]))\n\n\n\n<img alt=\"Score-training time trade-off\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_document_classification_20newsgroups_005.png\" srcset=\"../../_images/sphx_glr_plot_document_classification_20newsgroups_005.png\"/>\n<img alt=\"Score-test time trade-off\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_document_classification_20newsgroups_006.png\" srcset=\"../../_images/sphx_glr_plot_document_classification_20newsgroups_006.png\"/>",
            "code"
        ],
        [
            "The naive Bayes model has the best trade-off between score and\ntraining/testing time, while Random Forest is both slow to train, expensive to\npredict and has a comparatively bad accuracy. This is expected: for\nhigh-dimensional prediction problems, linear models are often better suited as\nmost problems become linearly separable when the feature space has 10,000\ndimensions or more.",
            "markdown"
        ],
        [
            "The difference in training speed and accuracy of the linear models can be\nexplained by the choice of the loss function they optimize and the kind of\nregularization they use. Be aware that some linear models with the same loss\nbut a different solver or regularization configuration may yield different\nfitting times and test accuracy. We can observe on the second plot that once\ntrained, all linear models have approximately the same prediction speed which\nis expected because they all implement the same prediction function.",
            "markdown"
        ],
        [
            "KNeighborsClassifier has a relatively low accuracy and has the highest testing\ntime. The long prediction time is also expected: for each prediction the model\nhas to compute the pairwise distances between the testing sample and each\ndocument in the training set, which is computationally expensive. Furthermore,\nthe \u201ccurse of dimensionality\u201d harms the ability of this model to yield\ncompetitive accuracy in the high dimensional feature space of text\nclassification problems.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  12.198 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Working with text documents->Clustering text documents using k-means": [
        [
            "This is an example showing how the scikit-learn API can be used to cluster\ndocuments by topics using a .",
            "markdown"
        ],
        [
            "Two algorithms are demoed:  and its more\nscalable variant, . Additionally,\nlatent semantic analysis is used to reduce dimensionality and discover latent\npatterns in the data.",
            "markdown"
        ],
        [
            "This example uses two different text vectorizers: a\n and a\n. See the example\nnotebook \nfor more information on vectorizers and a comparison of their processing times.",
            "markdown"
        ],
        [
            "For document analysis via a supervised learning approach, see the example script\n.",
            "markdown"
        ],
        [
            "# Author: Peter Prettenhofer &lt;peter.prettenhofer@gmail.com\n#         Lars Buitinck\n#         Olivier Grisel &lt;olivier.grisel@ensta.org\n#         Arturo Amor &lt;david-arturo.amor-quiroz@inria.fr\n# License: BSD 3 clause",
            "code"
        ]
    ],
    "Examples->Working with text documents->Clustering text documents using k-means->Loading text data": [
        [
            "We load data from , which comprises around 18,000\nnewsgroups posts on 20 topics. For illustrative purposes and to reduce the\ncomputational cost, we select a subset of 4 topics only accounting for around\n3,400 documents. See the example\n\nto gain intuition on the overlap of such topics.",
            "markdown"
        ],
        [
            "Notice that, by default, the text samples contain some message metadata such\nas \"headers\", \"footers\" (signatures) and \"quotes\" to other posts. We use\nthe remove parameter from  to\nstrip those features and have a more sensible clustering problem.",
            "markdown"
        ],
        [
            "import numpy as np\nfrom sklearn.datasets import \n\ncategories = [\n    \"alt.atheism\",\n    \"talk.religion.misc\",\n    \"comp.graphics\",\n    \"sci.space\",\n]\n\ndataset = (\n    remove=(\"headers\", \"footers\", \"quotes\"),\n    subset=\"all\",\n    categories=categories,\n    shuffle=True,\n    random_state=42,\n)\n\nlabels = dataset.target\nunique_labels, category_sizes = (labels, return_counts=True)\ntrue_k = unique_labels.shape[0]\n\nprint(f\"{len(dataset.data)} documents - {true_k} categories\")",
            "code"
        ],
        [
            "3387 documents - 4 categories",
            "code"
        ]
    ],
    "Examples->Working with text documents->Clustering text documents using k-means->Quantifying the quality of clustering results": [
        [
            "In this section we define a function to score different clustering pipelines\nusing several metrics.",
            "markdown"
        ],
        [
            "Clustering algorithms are fundamentally unsupervised learning methods.\nHowever, since we happen to have class labels for this specific dataset, it is\npossible to use evaluation metrics that leverage this \u201csupervised\u201d ground\ntruth information to quantify the quality of the resulting clusters. Examples\nof such metrics are the following:",
            "markdown"
        ],
        [
            "homogeneity, which quantifies how much clusters contain only members of a\nsingle class;",
            "markdown"
        ],
        [
            "completeness, which quantifies how much members of a given class are\nassigned to the same clusters;",
            "markdown"
        ],
        [
            "V-measure, the harmonic mean of completeness and homogeneity;",
            "markdown"
        ],
        [
            "Rand-Index, which measures how frequently pairs of data points are grouped\nconsistently according to the result of the clustering algorithm and the\nground truth class assignment;",
            "markdown"
        ],
        [
            "Adjusted Rand-Index, a chance-adjusted Rand-Index such that random cluster\nassignment have an ARI of 0.0 in expectation.",
            "markdown"
        ],
        [
            "If the ground truth labels are not known, evaluation can only be performed\nusing the model results itself. In that case, the Silhouette Coefficient comes\nin handy.",
            "markdown"
        ],
        [
            "For more reference, see .",
            "markdown"
        ],
        [
            "from collections import \nfrom sklearn import metrics\nfrom time import \n\nevaluations = []\nevaluations_std = []\n\n\ndef fit_and_evaluate(km, X, name=None, n_runs=5):\n    name = km.__class__.__name__ if name is None else name\n\n    train_times = []\n    scores = (list)\n    for seed in range(n_runs):\n        km.set_params(random_state=seed)\n        t0 = ()\n        km.fit(X)\n        train_times.append(() - t0)\n        scores[\"Homogeneity\"].append((labels, km.labels_))\n        scores[\"Completeness\"].append((labels, km.labels_))\n        scores[\"V-measure\"].append((labels, km.labels_))\n        scores[\"Adjusted Rand-Index\"].append(\n            (labels, km.labels_)\n        )\n        scores[\"Silhouette Coefficient\"].append(\n            (X, km.labels_, sample_size=2000)\n        )\n    train_times = (train_times)\n\n    print(f\"clustering done in {train_times.mean():.2f} \u00b1 {train_times.std():.2f} s \")\n    evaluation = {\n        \"estimator\": name,\n        \"train_time\": train_times.mean(),\n    }\n    evaluation_std = {\n        \"estimator\": name,\n        \"train_time\": train_times.std(),\n    }\n    for score_name, score_values in scores.items():\n        mean_score, std_score = (score_values), (score_values)\n        print(f\"{score_name}: {mean_score:.3f} \u00b1 {std_score:.3f}\")\n        evaluation[score_name] = mean_score\n        evaluation_std[score_name] = std_score\n    evaluations.append(evaluation)\n    evaluations_std.append(evaluation_std)",
            "code"
        ]
    ],
    "Examples->Working with text documents->Clustering text documents using k-means->K-means clustering on text features": [
        [
            "Two feature extraction methods are used in this example:",
            "markdown"
        ],
        [
            "uses an in-memory\nvocabulary (a Python dict) to map the most frequent words to features\nindices and hence compute a word occurrence frequency (sparse) matrix. The\nword frequencies are then reweighted using the Inverse Document Frequency\n(IDF) vector collected feature-wise over the corpus.",
            "markdown"
        ],
        [
            "hashes word\noccurrences to a fixed dimensional space, possibly with collisions. The word\ncount vectors are then normalized to each have l2-norm equal to one\n(projected to the euclidean unit-sphere) which seems to be important for\nk-means to work in high dimensional space.",
            "markdown"
        ],
        [
            "Furthermore it is possible to post-process those extracted features using\ndimensionality reduction. We will explore the impact of those choices on the\nclustering quality in the following.",
            "markdown"
        ]
    ],
    "Examples->Working with text documents->Clustering text documents using k-means->K-means clustering on text features->Feature Extraction using TfidfVectorizer": [
        [
            "We first benchmark the estimators using a dictionary vectorizer along with an\nIDF normalization as provided by\n.",
            "markdown"
        ],
        [
            "from sklearn.feature_extraction.text import \n\nvectorizer = (\n    max_df=0.5,\n    min_df=5,\n    stop_words=\"english\",\n)\nt0 = ()\nX_tfidf = vectorizer.fit_transform(dataset.data)\n\nprint(f\"vectorization done in {() - t0:.3f} s\")\nprint(f\"n_samples: {X_tfidf.shape[0]}, n_features: {X_tfidf.shape[1]}\")",
            "code"
        ],
        [
            "vectorization done in 0.399 s\nn_samples: 3387, n_features: 7929",
            "code"
        ],
        [
            "After ignoring terms that appear in more than 50% of the documents (as set by\nmax_df=0.5) and terms that are not present in at least 5 documents (set by\nmin_df=5), the resulting number of unique terms n_features is around\n8,000. We can additionally quantify the sparsity of the X_tfidf matrix as\nthe fraction of non-zero entries devided by the total number of elements.",
            "markdown"
        ],
        [
            "print(f\"{X_tfidf.nnz / (X_tfidf.shape):.3f}\")",
            "code"
        ],
        [
            "0.007",
            "code"
        ],
        [
            "We find that around 0.7% of the entries of the X_tfidf matrix are non-zero.",
            "markdown"
        ]
    ],
    "Examples->Working with text documents->Clustering text documents using k-means->K-means clustering on text features->Clustering sparse data with k-means": [
        [
            "As both  and\n optimize a non-convex objective\nfunction, their clustering is not guaranteed to be optimal for a given random\ninit. Even further, on sparse high-dimensional data such as text vectorized\nusing the Bag of Words approach, k-means can initialize centroids on extremely\nisolated data points. Those data points can stay their own centroids all\nalong.",
            "markdown"
        ],
        [
            "The following code illustrates how the previous phenomenon can sometimes lead\nto highly imbalanced clusters, depending on the random initialization:",
            "markdown"
        ],
        [
            "from sklearn.cluster import \n\nfor seed in range(5):\n    kmeans = (\n        n_clusters=true_k,\n        max_iter=100,\n        n_init=1,\n        random_state=seed,\n    ).fit(X_tfidf)\n    cluster_ids, cluster_sizes = (kmeans.labels_, return_counts=True)\n    print(f\"Number of elements asigned to each cluster: {cluster_sizes}\")\nprint()\nprint(\n    \"True number of documents in each category according to the class labels: \"\n    f\"{category_sizes}\"\n)",
            "code"
        ],
        [
            "Number of elements asigned to each cluster: [   1    1 3384    1]\nNumber of elements asigned to each cluster: [1733  717  238  699]\nNumber of elements asigned to each cluster: [1115  256 1417  599]\nNumber of elements asigned to each cluster: [1695  649  446  597]\nNumber of elements asigned to each cluster: [ 254 2117  459  557]\n\nTrue number of documents in each category according to the class labels: [799 973 987 628]",
            "code"
        ],
        [
            "To avoid this problem, one possibility is to increase the number of runs with\nindependent random initiations n_init. In such case the clustering with the\nbest inertia (objective function of k-means) is chosen.",
            "markdown"
        ],
        [
            "kmeans = (\n    n_clusters=true_k,\n    max_iter=100,\n    n_init=5,\n)\n\nfit_and_evaluate(kmeans, X_tfidf, name=\"KMeans\\non tf-idf vectors\")",
            "code"
        ],
        [
            "clustering done in 0.18 \u00b1 0.04 s\nHomogeneity: 0.347 \u00b1 0.009\nCompleteness: 0.397 \u00b1 0.006\nV-measure: 0.370 \u00b1 0.007\nAdjusted Rand-Index: 0.197 \u00b1 0.014\nSilhouette Coefficient: 0.007 \u00b1 0.001",
            "code"
        ],
        [
            "All those clustering evaluation metrics have a maximum value of 1.0 (for a\nperfect clustering result). Higher values are better. Values of the Adjusted\nRand-Index close to 0.0 correspond to a random labeling. Notice from the\nscores above that the cluster assignment is indeed well above chance level,\nbut the overall quality can certainly improve.",
            "markdown"
        ],
        [
            "Keep in mind that the class labels may not reflect accurately the document\ntopics and therefore metrics that use labels are not necessarily the best to\nevaluate the quality of our clustering pipeline.",
            "markdown"
        ]
    ],
    "Examples->Working with text documents->Clustering text documents using k-means->K-means clustering on text features->Performing dimensionality reduction using LSA": [
        [
            "A n_init=1 can still be used as long as the dimension of the vectorized\nspace is reduced first to make k-means more stable. For such purpose we use\n, which works on term count/tf-idf\nmatrices. Since SVD results are not normalized, we redo the normalization to\nimprove the  result. Using SVD to reduce the\ndimensionality of TF-IDF document vectors is often known as  (LSA) in\nthe information retrieval and text mining literature.",
            "markdown"
        ],
        [
            "from sklearn.decomposition import \nfrom sklearn.pipeline import \nfrom sklearn.preprocessing import \n\n\nlsa = ((n_components=100), (copy=False))\nt0 = ()\nX_lsa = lsa.fit_transform(X_tfidf)\nexplained_variance = lsa[0].explained_variance_ratio_.sum()\n\nprint(f\"LSA done in {() - t0:.3f} s\")\nprint(f\"Explained variance of the SVD step: {explained_variance * 100:.1f}%\")",
            "code"
        ],
        [
            "LSA done in 0.348 s\nExplained variance of the SVD step: 18.4%",
            "code"
        ],
        [
            "Using a single initialization means the processing time will be reduced for\nboth  and\n.",
            "markdown"
        ],
        [
            "kmeans = (\n    n_clusters=true_k,\n    max_iter=100,\n    n_init=1,\n)\n\nfit_and_evaluate(kmeans, X_lsa, name=\"KMeans\\nwith LSA on tf-idf vectors\")",
            "code"
        ],
        [
            "clustering done in 0.02 \u00b1 0.00 s\nHomogeneity: 0.396 \u00b1 0.005\nCompleteness: 0.424 \u00b1 0.024\nV-measure: 0.409 \u00b1 0.013\nAdjusted Rand-Index: 0.341 \u00b1 0.036\nSilhouette Coefficient: 0.029 \u00b1 0.001",
            "code"
        ],
        [
            "We can observe that clustering on the LSA representation of the document is\nsignificantly faster (both because of n_init=1 and because the\ndimensionality of the LSA feature space is much smaller). Furthermore, all the\nclustering evaluation metrics have improved. We repeat the experiment with\n.",
            "markdown"
        ],
        [
            "from sklearn.cluster import \n\nminibatch_kmeans = (\n    n_clusters=true_k,\n    n_init=1,\n    init_size=1000,\n    batch_size=1000,\n)\n\nfit_and_evaluate(\n    minibatch_kmeans,\n    X_lsa,\n    name=\"MiniBatchKMeans\\nwith LSA on tf-idf vectors\",\n)",
            "code"
        ],
        [
            "clustering done in 0.02 \u00b1 0.00 s\nHomogeneity: 0.353 \u00b1 0.072\nCompleteness: 0.384 \u00b1 0.018\nV-measure: 0.365 \u00b1 0.051\nAdjusted Rand-Index: 0.316 \u00b1 0.093\nSilhouette Coefficient: 0.025 \u00b1 0.006",
            "code"
        ]
    ],
    "Examples->Working with text documents->Clustering text documents using k-means->K-means clustering on text features->Top terms per cluster": [
        [
            "Since  can be\ninverted we can identify the cluster centers, which provide an intuition of\nthe most influential words <strong>for each cluster</strong>. See the example script\n\nfor a comparison with the most predictive words <strong>for each target class</strong>.",
            "markdown"
        ],
        [
            "original_space_centroids = lsa[0].inverse_transform(kmeans.cluster_centers_)\norder_centroids = original_space_centroids.argsort()[:, ::-1]\nterms = vectorizer.get_feature_names_out()\n\nfor i in range(true_k):\n    print(f\"Cluster {i}: \", end=\"\")\n    for ind in order_centroids[i, :10]:\n        print(f\"{terms[ind]} \", end=\"\")\n    print()",
            "code"
        ],
        [
            "Cluster 0: think just don people like know want say good really\nCluster 1: god people jesus say religion did does christian said evidence\nCluster 2: thanks graphics image know edu does files file program mail\nCluster 3: space launch orbit earth shuttle like nasa moon mission time",
            "code"
        ]
    ],
    "Examples->Working with text documents->Clustering text documents using k-means->K-means clustering on text features->HashingVectorizer": [
        [
            "An alternative vectorization can be done using a\n instance, which\ndoes not provide IDF weighting as this is a stateless model (the fit method\ndoes nothing). When IDF weighting is needed it can be added by pipelining the\n output to a\n instance. In this\ncase we also add LSA to the pipeline to reduce the dimension and sparcity of\nthe hashed vector space.",
            "markdown"
        ],
        [
            "from sklearn.feature_extraction.text import \nfrom sklearn.feature_extraction.text import \n\nlsa_vectorizer = (\n    (stop_words=\"english\", n_features=50_000),\n    (),\n    (n_components=100, random_state=0),\n    (copy=False),\n)\n\nt0 = ()\nX_hashed_lsa = lsa_vectorizer.fit_transform(dataset.data)\nprint(f\"vectorization done in {() - t0:.3f} s\")",
            "code"
        ],
        [
            "vectorization done in 1.801 s",
            "code"
        ],
        [
            "One can observe that the LSA step takes a relatively long time to fit,\nespecially with hashed vectors. The reason is that a hashed space is typically\nlarge (set to n_features=50_000 in this example). One can try lowering the\nnumber of features at the expense of having a larger fraction of features with\nhash collisions as shown in the example notebook\n.",
            "markdown"
        ],
        [
            "We now fit and evaluate the kmeans and minibatch_kmeans instances on this\nhashed-lsa-reduced data:",
            "markdown"
        ],
        [
            "fit_and_evaluate(kmeans, X_hashed_lsa, name=\"KMeans\\nwith LSA on hashed vectors\")",
            "code"
        ],
        [
            "clustering done in 0.02 \u00b1 0.01 s\nHomogeneity: 0.390 \u00b1 0.013\nCompleteness: 0.439 \u00b1 0.016\nV-measure: 0.413 \u00b1 0.014\nAdjusted Rand-Index: 0.328 \u00b1 0.013\nSilhouette Coefficient: 0.030 \u00b1 0.001",
            "code"
        ],
        [
            "fit_and_evaluate(\n    minibatch_kmeans,\n    X_hashed_lsa,\n    name=\"MiniBatchKMeans\\nwith LSA on hashed vectors\",\n)",
            "code"
        ],
        [
            "clustering done in 0.02 \u00b1 0.00 s\nHomogeneity: 0.354 \u00b1 0.052\nCompleteness: 0.366 \u00b1 0.055\nV-measure: 0.359 \u00b1 0.053\nAdjusted Rand-Index: 0.312 \u00b1 0.074\nSilhouette Coefficient: 0.026 \u00b1 0.004",
            "code"
        ],
        [
            "Both methods lead to good results that are similar to running the same models\non the traditional LSA vectors (without hashing).",
            "markdown"
        ]
    ],
    "Examples->Working with text documents->Clustering text documents using k-means->Clustering evaluation summary": [
        [
            "import pandas as pd\nimport matplotlib.pyplot as plt\n\nfig, (ax0, ax1) = (ncols=2, figsize=(16, 6), sharey=True)\n\ndf = (evaluations[::-1]).set_index(\"estimator\")\ndf_std = (evaluations_std[::-1]).set_index(\"estimator\")\n\ndf.drop(\n    [\"train_time\"],\n    axis=\"columns\",\n).plot.barh(ax=ax0, xerr=df_std)\nax0.set_xlabel(\"Clustering scores\")\nax0.set_ylabel(\"\")\n\ndf[\"train_time\"].plot.barh(ax=ax1, xerr=df_std[\"train_time\"])\nax1.set_xlabel(\"Clustering time (s)\")\n()\n\n\n<img alt=\"plot document clustering\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_document_clustering_001.png\" srcset=\"../../_images/sphx_glr_plot_document_clustering_001.png\"/>",
            "code"
        ],
        [
            "and \nsuffer from the phenomenon called the  for high dimensional\ndatasets such as text data. That is the reason why the overall scores improve\nwhen using LSA. Using LSA reduced data also improves the stability and\nrequires lower clustering time, though keep in mind that the LSA step itself\ntakes a long time, especially with hashed vectors.",
            "markdown"
        ],
        [
            "The Silhouette Coefficient is defined between 0 and 1. In all cases we obtain\nvalues close to 0 (even if they improve a bit after using LSA) because its\ndefinition requires measuring distances, in contrast with other evaluation\nmetrics such as the V-measure and the Adjusted Rand Index which are only based\non cluster assignments rather than distances. Notice that strictly speaking,\none should not compare the Silhouette Coefficient between spaces of different\ndimension, due to the different notions of distance they imply.",
            "markdown"
        ],
        [
            "The homogeneity, completeness and hence v-measure metrics do not yield a\nbaseline with regards to random labeling: this means that depending on the\nnumber of samples, clusters and ground truth classes, a completely random\nlabeling will not always yield the same values. In particular random labeling\nwon\u2019t yield zero scores, especially when the number of clusters is large. This\nproblem can safely be ignored when the number of samples is more than a\nthousand and the number of clusters is less than 10, which is the case of the\npresent example. For smaller sample sizes or larger number of clusters it is\nsafer to use an adjusted index such as the Adjusted Rand Index (ARI). See the\nexample\n for\na demo on the effect of random labeling.",
            "markdown"
        ],
        [
            "The size of the error bars show that \nis less stable than  for this relatively small\ndataset. It is more interesting to use when the number of samples is much\nbigger, but it can come at the expense of a small degradation in clustering\nquality compared to the traditional k-means algorithm.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  7.412 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->Working with text documents->FeatureHasher and DictVectorizer Comparison": [
        [
            "In this example we illustrate text vectorization, which is the process of\nrepresenting non-numerical input data (such as dictionaries or text documents)\nas vectors of real numbers.",
            "markdown"
        ],
        [
            "We first compare  and\n by using both methods to\nvectorize text documents that are preprocessed (tokenized) with the help of a\ncustom Python function.",
            "markdown"
        ],
        [
            "Later we introduce and analyze the text-specific vectorizers\n,\n and\n that handle both the\ntokenization and the assembling of the feature matrix within a single class.",
            "markdown"
        ],
        [
            "The objective of the example is to demonstrate the usage of text vectorization\nAPI and to compare their processing time. See the example scripts\n\nand  for actual\nlearning on text documents.",
            "markdown"
        ],
        [
            "# Author: Lars Buitinck\n#         Olivier Grisel &lt;olivier.grisel@ensta.org\n#         Arturo Amor &lt;david-arturo.amor-quiroz@inria.fr\n# License: BSD 3 clause",
            "code"
        ]
    ],
    "Examples->Working with text documents->FeatureHasher and DictVectorizer Comparison->Load Data": [
        [
            "We load data from , which comprises around\n18000 newsgroups posts on 20 topics split in two subsets: one for training and\none for testing. For the sake of simplicity and reducing the computational\ncost, we select a subset of 7 topics and use the training set only.",
            "markdown"
        ],
        [
            "from sklearn.datasets import \n\ncategories = [\n    \"alt.atheism\",\n    \"comp.graphics\",\n    \"comp.sys.ibm.pc.hardware\",\n    \"misc.forsale\",\n    \"rec.autos\",\n    \"sci.space\",\n    \"talk.religion.misc\",\n]\n\nprint(\"Loading 20 newsgroups training data\")\nraw_data, _ = (subset=\"train\", categories=categories, return_X_y=True)\ndata_size_mb = sum(len(s.encode(\"utf-8\")) for s in raw_data) / 1e6\nprint(f\"{len(raw_data)} documents - {data_size_mb:.3f}MB\")",
            "code"
        ],
        [
            "Loading 20 newsgroups training data\n3803 documents - 6.245MB",
            "code"
        ]
    ],
    "Examples->Working with text documents->FeatureHasher and DictVectorizer Comparison->Define preprocessing functions": [
        [
            "A token may be a word, part of a word or anything comprised between spaces or\nsymbols in a string. Here we define a function that extracts the tokens using\na simple regular expression (regex) that matches Unicode word characters. This\nincludes most characters that can be part of a word in any language, as well\nas numbers and the underscore:",
            "markdown"
        ],
        [
            "import re\n\n\ndef tokenize(doc):\n    \"\"\"Extract tokens from doc.\n\n    This uses a simple regex that matches word characters to break strings\n    into tokens. For a more principled approach, see CountVectorizer or\n    TfidfVectorizer.\n    \"\"\"\n    return (tok.lower() for tok in (r\"\\w+\", doc))\n\n\nlist(tokenize(\"This is a simple example, isn't it?\"))",
            "code"
        ],
        [
            "['this', 'is', 'a', 'simple', 'example', 'isn', 't', 'it']",
            "code"
        ],
        [
            "We define an additional function that counts the (frequency of) occurrence of\neach token in a given document. It returns a frequency dictionary to be used\nby the vectorizers.",
            "markdown"
        ],
        [
            "from collections import \n\n\ndef token_freqs(doc):\n    \"\"\"Extract a dict mapping tokens from doc to their occurrences.\"\"\"\n\n    freq = (int)\n    for tok in tokenize(doc):\n        freq[tok] += 1\n    return freq\n\n\ntoken_freqs(\"That is one example, but this is another one\")",
            "code"
        ],
        [
            "defaultdict(&lt;class 'int', {'that': 1, 'is': 2, 'one': 2, 'example': 1, 'but': 1, 'this': 1, 'another': 1})",
            "code"
        ],
        [
            "Observe in particular that the repeated token \"is\" is counted twice for\ninstance.",
            "markdown"
        ],
        [
            "Breaking a text document into word tokens, potentially losing the order\ninformation between the words in a sentence is often called a .",
            "markdown"
        ]
    ],
    "Examples->Working with text documents->FeatureHasher and DictVectorizer Comparison->DictVectorizer": [
        [
            "First we benchmark the ,\nthen we compare it to  as\nboth of them receive dictionaries as input.",
            "markdown"
        ],
        [
            "from time import \nfrom sklearn.feature_extraction import \n\ndict_count_vectorizers = (list)\n\nt0 = ()\nvectorizer = ()\nvectorizer.fit_transform(token_freqs(d) for d in raw_data)\nduration = () - t0\ndict_count_vectorizers[\"vectorizer\"].append(\n    vectorizer.__class__.__name__ + \"\\non freq dicts\"\n)\ndict_count_vectorizers[\"speed\"].append(data_size_mb / duration)\nprint(f\"done in {duration:.3f} s at {data_size_mb / duration:.1f} MB/s\")\nprint(f\"Found {len(vectorizer.get_feature_names_out())} unique terms\")",
            "code"
        ],
        [
            "done in 0.987 s at 6.3 MB/s\nFound 47928 unique terms",
            "code"
        ],
        [
            "The actual mapping from text token to column index is explicitly stored in\nthe .vocabulary_ attribute which is a potentially very large Python\ndictionary:",
            "markdown"
        ],
        [
            "type(vectorizer.vocabulary_)",
            "code"
        ],
        [
            "len(vectorizer.vocabulary_)",
            "code"
        ],
        [
            "47928",
            "code"
        ],
        [
            "vectorizer.vocabulary_[\"example\"]",
            "code"
        ],
        [
            "19145",
            "code"
        ]
    ],
    "Examples->Working with text documents->FeatureHasher and DictVectorizer Comparison->FeatureHasher": [
        [
            "Dictionaries take up a large amount of storage space and grow in size as the\ntraining set grows. Instead of growing the vectors along with a dictionary,\nfeature hashing builds a vector of pre-defined length by applying a hash\nfunction h to the features (e.g., tokens), then using the hash values\ndirectly as feature indices and updating the resulting vector at those\nindices. When the feature space is not large enough, hashing functions tend to\nmap distinct values to the same hash code (hash collisions). As a result, it\nis impossible to determine what object generated any particular hash code.",
            "markdown"
        ],
        [
            "Because of the above it is impossible to recover the original tokens from the\nfeature matrix and the best approach to estimate the number of unique terms in\nthe original dictionary is to count the number of active columns in the\nencoded feature matrix. For such a purpose we define the following function:",
            "markdown"
        ],
        [
            "import numpy as np\n\n\ndef n_nonzero_columns(X):\n    \"\"\"Number of columns with at least one non-zero value in a CSR matrix.\n\n    This is useful to count the number of features columns that are effectively\n    active when using the FeatureHasher.\n    \"\"\"\n    return len((X.nonzero()[1]))",
            "code"
        ],
        [
            "The default number of features for the\n is 2**20. Here we set\nn_features = 2**18 to illustrate hash collisions.",
            "markdown"
        ],
        [
            "<strong>FeatureHasher on frequency dictionaries</strong>",
            "markdown"
        ],
        [
            "from sklearn.feature_extraction import \n\nt0 = ()\nhasher = (n_features=2**18)\nX = hasher.transform(token_freqs(d) for d in raw_data)\nduration = () - t0\ndict_count_vectorizers[\"vectorizer\"].append(\n    hasher.__class__.__name__ + \"\\non freq dicts\"\n)\ndict_count_vectorizers[\"speed\"].append(data_size_mb / duration)\nprint(f\"done in {duration:.3f} s at {data_size_mb / duration:.1f} MB/s\")\nprint(f\"Found {n_nonzero_columns(X)} unique tokens\")",
            "code"
        ],
        [
            "done in 0.550 s at 11.4 MB/s\nFound 43873 unique tokens",
            "code"
        ],
        [
            "The number of unique tokens when using the\n is lower than those obtained\nusing the . This is due to\nhash collisions.",
            "markdown"
        ],
        [
            "The number of collisions can be reduced by increasing the feature space.\nNotice that the speed of the vectorizer does not change significantly when\nsetting a large number of features, though it causes larger coefficient\ndimensions and then requires more memory usage to store them, even if a\nmajority of them is inactive.",
            "markdown"
        ],
        [
            "t0 = ()\nhasher = (n_features=2**22)\nX = hasher.transform(token_freqs(d) for d in raw_data)\nduration = () - t0\n\nprint(f\"done in {duration:.3f} s at {data_size_mb / duration:.1f} MB/s\")\nprint(f\"Found {n_nonzero_columns(X)} unique tokens\")",
            "code"
        ],
        [
            "done in 0.548 s at 11.4 MB/s\nFound 47668 unique tokens",
            "code"
        ],
        [
            "We confirm that the number of unique tokens gets closer to the number of\nunique terms found by the .",
            "markdown"
        ],
        [
            "<strong>FeatureHasher on raw tokens</strong>",
            "markdown"
        ],
        [
            "Alternatively, one can set input_type=\"string\" in the\n to vectorize the strings\noutput directly from the customized tokenize function. This is equivalent to\npassing a dictionary with an implied frequency of 1 for each feature name.",
            "markdown"
        ],
        [
            "t0 = ()\nhasher = (n_features=2**18, input_type=\"string\")\nX = hasher.transform(tokenize(d) for d in raw_data)\nduration = () - t0\ndict_count_vectorizers[\"vectorizer\"].append(\n    hasher.__class__.__name__ + \"\\non raw tokens\"\n)\ndict_count_vectorizers[\"speed\"].append(data_size_mb / duration)\nprint(f\"done in {duration:.3f} s at {data_size_mb / duration:.1f} MB/s\")\nprint(f\"Found {n_nonzero_columns(X)} unique tokens\")",
            "code"
        ],
        [
            "done in 0.492 s at 12.7 MB/s\nFound 43873 unique tokens",
            "code"
        ],
        [
            "We now plot the speed of the above methods for vectorizing.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n\nfig, ax = (figsize=(12, 6))\n\ny_pos = (len(dict_count_vectorizers[\"vectorizer\"]))\nax.barh(y_pos, dict_count_vectorizers[\"speed\"], align=\"center\")\nax.set_yticks(y_pos)\nax.set_yticklabels(dict_count_vectorizers[\"vectorizer\"])\nax.invert_yaxis()\n_ = ax.set_xlabel(\"speed (MB/s)\")\n\n\n<img alt=\"plot hashing vs dict vectorizer\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_hashing_vs_dict_vectorizer_001.png\" srcset=\"../../_images/sphx_glr_plot_hashing_vs_dict_vectorizer_001.png\"/>",
            "code"
        ],
        [
            "In both cases  is\napproximately twice as fast as\n. This is handy when dealing\nwith large amounts of data, with the downside of losing the invertibility of\nthe transformation, which in turn makes the interpretation of a model a more\ncomplex task.",
            "markdown"
        ],
        [
            "The FeatureHeasher with input_type=\"string\" is slightly faster than the\nvariant that works on frequency dict because it does not count repeated\ntokens: each token is implicitly counted once, even if it was repeated.\nDepending on the downstream machine learning task, it can be a limitation or\nnot.",
            "markdown"
        ]
    ],
    "Examples->Working with text documents->FeatureHasher and DictVectorizer Comparison->Comparison with special purpose text vectorizers": [
        [
            "accepts raw data as\nit internally implements tokenization and occurrence counting. It is similar\nto the  when used along with\nthe customized function token_freqs as done in the previous section. The\ndifference being that \nis more flexible. In particular it accepts various regex patterns through the\ntoken_pattern parameter.",
            "markdown"
        ],
        [
            "from sklearn.feature_extraction.text import \n\nt0 = ()\nvectorizer = ()\nvectorizer.fit_transform(raw_data)\nduration = () - t0\ndict_count_vectorizers[\"vectorizer\"].append(vectorizer.__class__.__name__)\ndict_count_vectorizers[\"speed\"].append(data_size_mb / duration)\nprint(f\"done in {duration:.3f} s at {data_size_mb / duration:.1f} MB/s\")\nprint(f\"Found {len(vectorizer.get_feature_names_out())} unique terms\")",
            "code"
        ],
        [
            "done in 0.660 s at 9.5 MB/s\nFound 47885 unique terms",
            "code"
        ],
        [
            "We see that using the \nimplementation is approximately twice as fast as using the\n along with the simple\nfunction we defined for mapping the tokens. The reason is that\n is optimized by\nreusing a compiled regular expression for the full training set instead of\ncreating one per document as done in our naive tokenize function.",
            "markdown"
        ],
        [
            "Now we make a similar experiment with the\n, which is\nequivalent to combining the \u201chashing trick\u201d implemented by the\n class and the text\npreprocessing and tokenization of the\n.",
            "markdown"
        ],
        [
            "from sklearn.feature_extraction.text import \n\nt0 = ()\nvectorizer = (n_features=2**18)\nvectorizer.fit_transform(raw_data)\nduration = () - t0\ndict_count_vectorizers[\"vectorizer\"].append(vectorizer.__class__.__name__)\ndict_count_vectorizers[\"speed\"].append(data_size_mb / duration)\nprint(f\"done in {duration:.3f} s at {data_size_mb / duration:.1f} MB/s\")",
            "code"
        ],
        [
            "done in 0.479 s at 13.1 MB/s",
            "code"
        ],
        [
            "We can observe that this is the fastest text tokenization strategy so far,\nassuming that the downstream machine learning task can tolerate a few\ncollisions.",
            "markdown"
        ]
    ],
    "Examples->Working with text documents->FeatureHasher and DictVectorizer Comparison->TfidfVectorizer": [
        [
            "In a large text corpus, some words appear with higher frequency (e.g. \u201cthe\u201d,\n\u201ca\u201d, \u201cis\u201d in English) and do not carry meaningful information about the actual\ncontents of a document. If we were to feed the word count data directly to a\nclassifier, those very common terms would shadow the frequencies of rarer yet\nmore informative terms. In order to re-weight the count features into floating\npoint values suitable for usage by a classifier it is very common to use the\ntf\u2013idf transform as implemented by the\n. TF stands for\n\u201cterm-frequency\u201d while \u201ctf\u2013idf\u201d means term-frequency times inverse\ndocument-frequency.",
            "markdown"
        ],
        [
            "We now benchmark the ,\nwhich is equivalent to combining the tokenization and occurrence counting of\nthe  along with the\nnormalizing and weighting from a\n.",
            "markdown"
        ],
        [
            "from sklearn.feature_extraction.text import \n\nt0 = ()\nvectorizer = ()\nvectorizer.fit_transform(raw_data)\nduration = () - t0\ndict_count_vectorizers[\"vectorizer\"].append(vectorizer.__class__.__name__)\ndict_count_vectorizers[\"speed\"].append(data_size_mb / duration)\nprint(f\"done in {duration:.3f} s at {data_size_mb / duration:.1f} MB/s\")\nprint(f\"Found {len(vectorizer.get_feature_names_out())} unique terms\")",
            "code"
        ],
        [
            "done in 0.670 s at 9.3 MB/s\nFound 47885 unique terms",
            "code"
        ]
    ],
    "Examples->Working with text documents->FeatureHasher and DictVectorizer Comparison->Summary": [
        [
            "Let\u2019s conclude this notebook by summarizing all the recorded processing speeds\nin a single plot:",
            "markdown"
        ],
        [
            "fig, ax = (figsize=(12, 6))\n\ny_pos = (len(dict_count_vectorizers[\"vectorizer\"]))\nax.barh(y_pos, dict_count_vectorizers[\"speed\"], align=\"center\")\nax.set_yticks(y_pos)\nax.set_yticklabels(dict_count_vectorizers[\"vectorizer\"])\nax.invert_yaxis()\n_ = ax.set_xlabel(\"speed (MB/s)\")\n\n\n<img alt=\"plot hashing vs dict vectorizer\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_hashing_vs_dict_vectorizer_002.png\" srcset=\"../../_images/sphx_glr_plot_hashing_vs_dict_vectorizer_002.png\"/>",
            "code"
        ],
        [
            "Notice from the plot that\n is slightly slower\nthan  because of the\nextra operation induced by the\n.",
            "markdown"
        ],
        [
            "Also notice that, by setting the number of features n_features = 2**18, the\n performs better\nthan the  at the\nexpense of inversibility of the transformation due to hash collisions.",
            "markdown"
        ],
        [
            "We highlight that  and\n perform better than\ntheir equivalent  and\n on manually tokenized\ndocuments since the internal tokenization step of the former vectorizers\ncompiles a regular expression once and then reuses it for all the documents.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  4.891 seconds)",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Tutorials->An introduction to machine learning with scikit-learn": [
        [
            "Section contents",
            "markdown"
        ],
        [
            "In this section, we introduce the \nvocabulary that we use throughout scikit-learn and give a\nsimple learning example.",
            "markdown"
        ]
    ],
    "Tutorials->An introduction to machine learning with scikit-learn->Machine learning: the problem setting": [
        [
            "In general, a learning problem considers a set of n\n of\ndata and then tries to predict properties of unknown data. If each sample is\nmore than a single number and, for instance, a multi-dimensional entry\n(aka \ndata), it is said to have several attributes or <strong>features</strong>.",
            "markdown"
        ],
        [
            "Learning problems fall into a few categories:",
            "markdown"
        ],
        [
            ",\nin which the data comes with additional attributes that we want to predict\n(\nto go to the scikit-learn supervised learning page).This problem\ncan be either:",
            "markdown"
        ],
        [
            ":\nsamples belong to two or more classes and we\nwant to learn from already labeled data how to predict the class\nof unlabeled data. An example of a classification problem would\nbe handwritten digit recognition, in which the aim is\nto assign each input vector to one of a finite number of discrete\ncategories.  Another way to think of classification is as a discrete\n(as opposed to continuous) form of supervised learning where one has a\nlimited number of categories and for each of the n samples provided,\none is to try to label them with the correct category or class.",
            "markdown"
        ],
        [
            ":\nif the desired output consists of one or more\ncontinuous variables, then the task is called regression. An\nexample of a regression problem would be the prediction of the\nlength of a salmon as a function of its age and weight.\n\n</blockquote>",
            "markdown"
        ],
        [
            ",\nin which the training data consists of a set of input vectors x\nwithout any corresponding target values. The goal in such problems\nmay be to discover groups of similar examples within the data, where\nit is called ,\nor to determine the distribution of data within the input space, known as\n, or\nto project the data from a high-dimensional space down to two or three\ndimensions for the purpose of visualization\n(\nto go to the Scikit-Learn unsupervised learning page).\n\n</blockquote>",
            "markdown"
        ],
        [
            "Training set and testing set",
            "markdown"
        ],
        [
            "Machine learning is about learning some properties of a data set\nand then testing those properties against another data set. A common\npractice in machine learning is to evaluate an algorithm by splitting a data\nset into two. We call one of those sets the <strong>training set</strong>, on which we\nlearn some properties; we call the other set the <strong>testing set</strong>, on which\nwe test the learned properties.",
            "markdown"
        ]
    ],
    "Tutorials->An introduction to machine learning with scikit-learn->Loading an example dataset": [
        [
            "scikit-learn comes with a few standard datasets, for instance the\n and \ndatasets for classification and the  for regression.",
            "markdown"
        ],
        [
            "In the following, we start a Python interpreter from our shell and then\nload the iris and digits datasets.  Our notational convention is that\n$ denotes the shell prompt while >>> denotes the Python\ninterpreter prompt:",
            "markdown"
        ],
        [
            "$ python\n from sklearn import datasets\n iris = datasets.load_iris()\n digits = datasets.load_digits()",
            "code"
        ],
        [
            "A dataset is a dictionary-like object that holds all the data and some\nmetadata about the data. This data is stored in the .data member,\nwhich is a n_samples, n_features array. In the case of supervised\nproblems, one or more response variables are stored in the .target member. More\ndetails on the different datasets can be found in the .",
            "markdown"
        ],
        [
            "For instance, in the case of the digits dataset, digits.data gives\naccess to the features that can be used to classify the digits samples:",
            "markdown"
        ],
        [
            "print(digits.data)\n[[ 0.   0.   5. ...   0.   0.   0.]\n [ 0.   0.   0. ...  10.   0.   0.]\n [ 0.   0.   0. ...  16.   9.   0.]\n ...\n [ 0.   0.   1. ...   6.   0.   0.]\n [ 0.   0.   2. ...  12.   0.   0.]\n [ 0.   0.  10. ...  12.   1.   0.]]",
            "code"
        ],
        [
            "and digits.target gives the ground truth for the digit dataset, that\nis the number corresponding to each digit image that we are trying to\nlearn:",
            "markdown"
        ],
        [
            "digits.target\narray([0, 1, 2, ..., 8, 9, 8])",
            "code"
        ],
        [
            "Shape of the data arrays",
            "markdown"
        ],
        [
            "The data is always a 2D array, shape (n_samples, n_features), although\nthe original data may have had a different shape. In the case of the\ndigits, each original sample is an image of shape (8, 8) and can be\naccessed using:",
            "markdown"
        ],
        [
            "digits.images[0]\narray([[  0.,   0.,   5.,  13.,   9.,   1.,   0.,   0.],\n       [  0.,   0.,  13.,  15.,  10.,  15.,   5.,   0.],\n       [  0.,   3.,  15.,   2.,   0.,  11.,   8.,   0.],\n       [  0.,   4.,  12.,   0.,   0.,   8.,   8.,   0.],\n       [  0.,   5.,   8.,   0.,   0.,   9.,   8.,   0.],\n       [  0.,   4.,  11.,   0.,   1.,  12.,   7.,   0.],\n       [  0.,   2.,  14.,   5.,  10.,  12.,   0.,   0.],\n       [  0.,   0.,   6.,  13.,  10.,   0.,   0.,   0.]])",
            "code"
        ],
        [
            "The  illustrates how starting\nfrom the original problem one can shape the data for consumption in\nscikit-learn.",
            "markdown"
        ],
        [
            "Loading from external datasets",
            "markdown"
        ],
        [
            "To load from an external dataset, please refer to .",
            "markdown"
        ]
    ],
    "Tutorials->An introduction to machine learning with scikit-learn->Learning and predicting": [
        [
            "In the case of the digits dataset, the task is to predict, given an image,\nwhich digit it represents. We are given samples of each of the 10\npossible classes (the digits zero through nine) on which we fit an\n to be able to predict\nthe classes to which unseen samples belong.",
            "markdown"
        ],
        [
            "In scikit-learn, an estimator for classification is a Python object that\nimplements the methods fit(X, y) and predict(T).",
            "markdown"
        ],
        [
            "An example of an estimator is the class sklearn.svm.SVC, which\nimplements . The\nestimator\u2019s constructor takes as arguments the model\u2019s parameters.",
            "markdown"
        ],
        [
            "For now, we will consider the estimator as a black box:",
            "markdown"
        ],
        [
            "from sklearn import svm\n clf = svm.SVC(gamma=0.001, C=100.)",
            "code"
        ],
        [
            "Choosing the parameters of the model",
            "markdown"
        ],
        [
            "In this example, we set the value of gamma manually.\nTo find good values for these parameters, we can use tools\nsuch as  and .",
            "markdown"
        ],
        [
            "The clf (for classifier) estimator instance is first\nfitted to the model; that is, it must learn from the model. This is\ndone by passing our training set to the fit method. For the training\nset, we\u2019ll use all the images from our dataset, except for the last\nimage, which we\u2019ll reserve for our predicting. We select the training set with\nthe [:-1] Python syntax, which produces a new array that contains all but\nthe last item from digits.data:",
            "markdown"
        ],
        [
            "clf.fit(digits.data[:-1], digits.target[:-1])\nSVC(C=100.0, gamma=0.001)",
            "code"
        ],
        [
            "Now you can predict new values. In this case, you\u2019ll predict using the last\nimage from digits.data. By predicting, you\u2019ll determine the image from the\ntraining set that best matches the last image.",
            "markdown"
        ],
        [
            "clf.predict(digits.data[-1:])\narray([8])",
            "code"
        ],
        [
            "The corresponding image is:",
            "markdown"
        ],
        [
            "As you can see, it is a challenging task: after all, the images are of poor\nresolution. Do you agree with the classifier?",
            "markdown"
        ],
        [
            "A complete example of this classification problem is available as an\nexample that you can run and study:\n.",
            "markdown"
        ]
    ],
    "Tutorials->An introduction to machine learning with scikit-learn->Conventions": [
        [
            "scikit-learn estimators follow certain rules to make their behavior more\npredictive.  These are described in more detail in the .",
            "markdown"
        ]
    ],
    "Tutorials->An introduction to machine learning with scikit-learn->Conventions->Type casting": [
        [
            "Where possible, input of type float32 will maintain its data type. Otherwise\ninput will be cast to float64:",
            "markdown"
        ],
        [
            "import numpy as np\n from sklearn import kernel_approximation\n\n rng = np.random.RandomState(0)\n X = rng.rand(10, 2000)\n X = np.array(X, dtype='float32')\n X.dtype\ndtype('float32')\n\n transformer = kernel_approximation.RBFSampler()\n X_new = transformer.fit_transform(X)\n X_new.dtype\ndtype('float32')",
            "code"
        ],
        [
            "In this example, X is float32, and is unchanged by fit_transform(X).",
            "markdown"
        ],
        [
            "Using float32-typed training (or testing) data is often more\nefficient than using the usual float64 dtype: it allows to\nreduce the memory usage and sometimes also reduces processing time\nby leveraging the vector instructions of the CPU. However it can\nsometimes lead to numerical stability problems causing the algorithm\nto be more sensitive to the scale of the values and .",
            "markdown"
        ],
        [
            "Keep in mind however that not all scikit-learn estimators attempt to\nwork in float32 mode. For instance, some transformers will always\ncast there input to float64 and return float64 transformed\nvalues as a result.",
            "markdown"
        ],
        [
            "Regression targets are cast to float64 and classification targets are\nmaintained:",
            "markdown"
        ],
        [
            "from sklearn import datasets\n from sklearn.svm import SVC\n iris = datasets.load_iris()\n clf = SVC()\n clf.fit(iris.data, iris.target)\nSVC()\n\n list(clf.predict(iris.data[:3]))\n[0, 0, 0]\n\n clf.fit(iris.data, iris.target_names[iris.target])\nSVC()\n\n list(clf.predict(iris.data[:3]))\n['setosa', 'setosa', 'setosa']",
            "code"
        ],
        [
            "Here, the first predict() returns an integer array, since iris.target\n(an integer array) was used in fit. The second predict() returns a string\narray, since iris.target_names was for fitting.",
            "markdown"
        ]
    ],
    "Tutorials->An introduction to machine learning with scikit-learn->Conventions->Refitting and updating parameters": [
        [
            "Hyper-parameters of an estimator can be updated after it has been constructed\nvia the  method. Calling fit() more than\nonce will overwrite what was learned by any previous fit():",
            "markdown"
        ],
        [
            "import numpy as np\n from sklearn.datasets import load_iris\n from sklearn.svm import SVC\n X, y = load_iris(return_X_y=True)\n\n clf = SVC()\n clf.set_params(kernel='linear').fit(X, y)\nSVC(kernel='linear')\n clf.predict(X[:5])\narray([0, 0, 0, 0, 0])\n\n clf.set_params(kernel='rbf').fit(X, y)\nSVC()\n clf.predict(X[:5])\narray([0, 0, 0, 0, 0])",
            "code"
        ],
        [
            "Here, the default kernel rbf is first changed to linear via\n after the estimator has\nbeen constructed, and changed back to rbf to refit the estimator and to\nmake a second prediction.",
            "markdown"
        ]
    ],
    "Tutorials->An introduction to machine learning with scikit-learn->Conventions->Multiclass vs. multilabel fitting": [
        [
            "When using ,\nthe learning and prediction task that is performed is dependent on the format of\nthe target data fit upon:",
            "markdown"
        ],
        [
            "from sklearn.svm import SVC\n from sklearn.multiclass import OneVsRestClassifier\n from sklearn.preprocessing import LabelBinarizer\n\n X = [[1, 2], [2, 4], [4, 5], [3, 2], [3, 1]]\n y = [0, 0, 1, 1, 2]\n\n classif = OneVsRestClassifier(estimator=SVC(random_state=0))\n classif.fit(X, y).predict(X)\narray([0, 0, 1, 1, 2])",
            "code"
        ],
        [
            "In the above case, the classifier is fit on a 1d array of multiclass labels and\nthe predict() method therefore provides corresponding multiclass predictions.\nIt is also possible to fit upon a 2d array of binary label indicators:",
            "markdown"
        ],
        [
            "y = LabelBinarizer().fit_transform(y)\n classif.fit(X, y).predict(X)\narray([[1, 0, 0],\n       [1, 0, 0],\n       [0, 1, 0],\n       [0, 0, 0],\n       [0, 0, 0]])",
            "code"
        ],
        [
            "Here, the classifier is fit()  on a 2d binary label representation of y,\nusing the .\nIn this case predict() returns a 2d array representing the corresponding\nmultilabel predictions.",
            "markdown"
        ],
        [
            "Note that the fourth and fifth instances returned all zeroes, indicating that\nthey matched none of the three labels fit upon. With multilabel outputs, it\nis similarly possible for an instance to be assigned multiple labels:",
            "markdown"
        ],
        [
            "from sklearn.preprocessing import MultiLabelBinarizer\n y = [[0, 1], [0, 2], [1, 3], [0, 2, 3], [2, 4]]\n y = MultiLabelBinarizer().fit_transform(y)\n classif.fit(X, y).predict(X)\narray([[1, 1, 0, 0, 0],\n       [1, 0, 1, 0, 0],\n       [0, 1, 0, 1, 0],\n       [1, 0, 1, 0, 0],\n       [1, 0, 1, 0, 0]])",
            "code"
        ],
        [
            "In this case, the classifier is fit upon instances each assigned multiple labels.\nThe  is\nused to binarize the 2d array of multilabels to fit upon. As a result,\npredict() returns a 2d array with multiple predicted labels for each instance.",
            "markdown"
        ]
    ],
    "Tutorials->A tutorial on statistical-learning for scientific data processing": [
        [
            "Statistical learning",
            "markdown"
        ],
        [
            "is\na technique with a growing importance, as the\nsize of the datasets experimental sciences are facing is rapidly\ngrowing. Problems it tackles range from building a prediction function\nlinking different observations, to classifying observations, or\nlearning the structure in an unlabeled dataset.",
            "markdown"
        ],
        [
            "This tutorial will explore statistical learning, the use of\nmachine learning techniques with the goal of :\ndrawing conclusions on the data at hand.",
            "markdown"
        ],
        [
            "Scikit-learn is a Python module integrating classic machine\nlearning algorithms in the tightly-knit world of scientific Python\npackages (, , ).",
            "markdown"
        ]
    ],
    "Tutorials->Working With Text Data": [
        [
            "The goal of this guide is to explore some of the main scikit-learn\ntools on a single practical task: analyzing a collection of text\ndocuments (newsgroups posts) on twenty different topics.",
            "markdown"
        ],
        [
            "In this section we will see how to:",
            "markdown"
        ],
        [
            "load the file contents and the categories",
            "markdown"
        ],
        [
            "extract feature vectors suitable for machine learning",
            "markdown"
        ],
        [
            "train a linear model to perform categorization",
            "markdown"
        ],
        [
            "use a grid search strategy to find a good configuration of both\nthe feature extraction components and the classifier\n\n</blockquote>",
            "markdown"
        ]
    ],
    "Tutorials->Working With Text Data->Tutorial setup": [
        [
            "To get started with this tutorial, you must first install\nscikit-learn and all of its required dependencies.",
            "markdown"
        ],
        [
            "Please refer to the \npage for more information and for system-specific instructions.",
            "markdown"
        ],
        [
            "The source of this tutorial can be found within your scikit-learn folder:",
            "markdown"
        ],
        [
            "scikit-learn/doc/tutorial/text_analytics/",
            "code"
        ],
        [
            "The source can also be found .",
            "markdown"
        ],
        [
            "The tutorial folder should contain the following sub-folders:",
            "markdown"
        ],
        [
            "*.rst files - the source of the tutorial document written with sphinx",
            "markdown"
        ],
        [
            "data - folder to put the datasets used during the tutorial",
            "markdown"
        ],
        [
            "skeletons - sample incomplete scripts for the exercises",
            "markdown"
        ],
        [
            "solutions - solutions of the exercises\n\n</blockquote>",
            "markdown"
        ],
        [
            "You can already copy the skeletons into a new folder somewhere\non your hard-drive named sklearn_tut_workspace, where you\nwill edit your own files for the exercises while keeping\nthe original skeletons intact:",
            "markdown"
        ],
        [
            "cp -r skeletons work_directory/sklearn_tut_workspace",
            "code"
        ],
        [
            "Machine learning algorithms need data. Go to each $TUTORIAL_HOME/data\nsub-folder and run the fetch_data.py script from there (after\nhaving read them first).",
            "markdown"
        ],
        [
            "For instance:",
            "markdown"
        ],
        [
            "cd $TUTORIAL_HOME/data/languages\nless fetch_data.py\npython fetch_data.py",
            "code"
        ]
    ],
    "Tutorials->Working With Text Data->Loading the 20 newsgroups dataset": [
        [
            "The dataset is called \u201cTwenty Newsgroups\u201d. Here is the official\ndescription, quoted from the :",
            "markdown"
        ],
        [
            "The 20 Newsgroups data set is a collection of approximately 20,000\nnewsgroup documents, partitioned (nearly) evenly across 20 different\nnewsgroups. To the best of our knowledge, it was originally collected\nby Ken Lang, probably for his paper \u201cNewsweeder: Learning to filter\nnetnews,\u201d though he does not explicitly mention this collection.\nThe 20 newsgroups collection has become a popular data set for\nexperiments in text applications of machine learning techniques,\nsuch as text classification and text clustering.\n</blockquote>",
            "markdown"
        ],
        [
            "In the following we will use the built-in dataset loader for 20 newsgroups\nfrom scikit-learn. Alternatively, it is possible to download the dataset\nmanually from the website and use the \nfunction by pointing it to the 20news-bydate-train sub-folder of the\nuncompressed archive folder.",
            "markdown"
        ],
        [
            "In order to get faster execution times for this first example, we will\nwork on a partial dataset with only 4 categories out of the 20 available\nin the dataset:",
            "markdown"
        ],
        [
            "categories = ['alt.atheism', 'soc.religion.christian',\n...               'comp.graphics', 'sci.med']",
            "code"
        ],
        [
            "We can now load the list of files matching those categories as follows:",
            "markdown"
        ],
        [
            "from sklearn.datasets import fetch_20newsgroups\n twenty_train = fetch_20newsgroups(subset='train',\n...     categories=categories, shuffle=True, random_state=42)",
            "code"
        ],
        [
            "The returned dataset is a scikit-learn \u201cbunch\u201d: a simple holder\nobject with fields that can be both accessed as python dict\nkeys or object attributes for convenience, for instance the\ntarget_names holds the list of the requested category names:",
            "markdown"
        ],
        [
            "twenty_train.target_names\n['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']",
            "code"
        ],
        [
            "The files themselves are loaded in memory in the data attribute. For\nreference the filenames are also available:",
            "markdown"
        ],
        [
            "len(twenty_train.data)\n2257\n len(twenty_train.filenames)\n2257",
            "code"
        ],
        [
            "Let\u2019s print the first lines of the first loaded file:",
            "markdown"
        ],
        [
            "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3]))\nFrom: sd345@city.ac.uk (Michael Collier)\nSubject: Converting images to HP LaserJet III?\nNntp-Posting-Host: hampton\n\n print(twenty_train.target_names[twenty_train.target[0]])\ncomp.graphics",
            "code"
        ],
        [
            "Supervised learning algorithms will require a category label for each\ndocument in the training set. In this case the category is the name of the\nnewsgroup which also happens to be the name of the folder holding the\nindividual documents.",
            "markdown"
        ],
        [
            "For speed and space efficiency reasons, scikit-learn loads the\ntarget attribute as an array of integers that corresponds to the\nindex of the category name in the target_names list. The category\ninteger id of each sample is stored in the target attribute:",
            "markdown"
        ],
        [
            "twenty_train.target[:10]\narray([1, 1, 3, 3, 3, 3, 3, 2, 2, 2])",
            "code"
        ],
        [
            "It is possible to get back the category names as follows:",
            "markdown"
        ],
        [
            "for t in twenty_train.target[:10]:\n...     print(twenty_train.target_names[t])\n...\ncomp.graphics\ncomp.graphics\nsoc.religion.christian\nsoc.religion.christian\nsoc.religion.christian\nsoc.religion.christian\nsoc.religion.christian\nsci.med\nsci.med\nsci.med",
            "code"
        ],
        [
            "You might have noticed that the samples were shuffled randomly when we called\nfetch_20newsgroups(..., shuffle=True, random_state=42): this is useful if\nyou wish to select only a subset of samples to quickly train a model and get a\nfirst idea of the results before re-training on the complete dataset later.",
            "markdown"
        ]
    ],
    "Tutorials->Working With Text Data->Extracting features from text files": [
        [
            "In order to perform machine learning on text documents, we first need to\nturn the text content into numerical feature vectors.",
            "markdown"
        ]
    ],
    "Tutorials->Working With Text Data->Extracting features from text files->Bags of words": [
        [
            "The most intuitive way to do so is to use a bags of words representation:",
            "markdown"
        ],
        [
            "Assign a fixed integer id to each word occurring in any document\nof the training set (for instance by building a dictionary\nfrom words to integer indices).",
            "markdown"
        ],
        [
            "For each document #i, count the number of occurrences of each\nword w and store it in X[i, j] as the value of feature\n#j where j is the index of word w in the dictionary.\n\n</blockquote>",
            "markdown"
        ],
        [
            "The bags of words representation implies that n_features is\nthe number of distinct words in the corpus: this number is typically\nlarger than 100,000.",
            "markdown"
        ],
        [
            "If n_samples == 10000, storing X as a NumPy array of type\nfloat32 would require 10000 x 100000 x 4 bytes = <strong>4GB in RAM</strong> which\nis barely manageable on today\u2019s computers.",
            "markdown"
        ],
        [
            "Fortunately, <strong>most values in X will be zeros</strong> since for a given\ndocument less than a few thousand distinct words will be\nused. For this reason we say that bags of words are typically\n<strong>high-dimensional sparse datasets</strong>. We can save a lot of memory by\nonly storing the non-zero parts of the feature vectors in memory.",
            "markdown"
        ],
        [
            "scipy.sparse matrices are data structures that do exactly this,\nand scikit-learn has built-in support for these structures.",
            "markdown"
        ]
    ],
    "Tutorials->Working With Text Data->Extracting features from text files->Tokenizing text with scikit-learn": [
        [
            "Text preprocessing, tokenizing and filtering of stopwords are all included\nin , which builds a dictionary of features and\ntransforms documents to feature vectors:",
            "markdown"
        ],
        [
            "from sklearn.feature_extraction.text import CountVectorizer\n count_vect = CountVectorizer()\n X_train_counts = count_vect.fit_transform(twenty_train.data)\n X_train_counts.shape\n(2257, 35788)",
            "code"
        ],
        [
            "supports counts of N-grams of words or consecutive\ncharacters. Once fitted, the vectorizer has built a dictionary of feature\nindices:",
            "markdown"
        ],
        [
            "count_vect.vocabulary_.get(u'algorithm')\n4690",
            "code"
        ],
        [
            "The index value of a word in the vocabulary is linked to its frequency\nin the whole training corpus.",
            "markdown"
        ]
    ],
    "Tutorials->Working With Text Data->Extracting features from text files->From occurrences to frequencies": [
        [
            "Occurrence count is a good start but there is an issue: longer\ndocuments will have higher average count values than shorter documents,\neven though they might talk about the same topics.",
            "markdown"
        ],
        [
            "To avoid these potential discrepancies it suffices to divide the\nnumber of occurrences of each word in a document by the total number\nof words in the document: these new features are called tf for Term\nFrequencies.",
            "markdown"
        ],
        [
            "Another refinement on top of tf is to downscale weights for words\nthat occur in many documents in the corpus and are therefore less\ninformative than those that occur only in a smaller portion of the\ncorpus.",
            "markdown"
        ],
        [
            "This downscaling is called  for \u201cTerm Frequency times\nInverse Document Frequency\u201d.",
            "markdown"
        ],
        [
            "Both <strong>tf</strong> and <strong>tf\u2013idf</strong> can be computed as follows using\n:",
            "markdown"
        ],
        [
            "from sklearn.feature_extraction.text import TfidfTransformer\n tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n X_train_tf = tf_transformer.transform(X_train_counts)\n X_train_tf.shape\n(2257, 35788)",
            "code"
        ],
        [
            "In the above example-code, we firstly use the fit(..) method to fit our\nestimator to the data and secondly the transform(..) method to transform\nour count-matrix to a tf-idf representation.\nThese two steps can be combined to achieve the same end result faster\nby skipping redundant processing. This is done through using the\nfit_transform(..) method as shown below, and as mentioned in the note\nin the previous section:",
            "markdown"
        ],
        [
            "tfidf_transformer = TfidfTransformer()\n X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n X_train_tfidf.shape\n(2257, 35788)",
            "code"
        ]
    ],
    "Tutorials->Working With Text Data->Training a classifier": [
        [
            "Now that we have our features, we can train a classifier to try to predict\nthe category of a post. Let\u2019s start with a \nclassifier, which\nprovides a nice baseline for this task. scikit-learn includes several\nvariants of this classifier, and the one most suitable for word counts is the\nmultinomial variant:",
            "markdown"
        ],
        [
            "from sklearn.naive_bayes import MultinomialNB\n clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)",
            "code"
        ],
        [
            "To try to predict the outcome on a new document we need to extract\nthe features using almost the same feature extracting chain as before.\nThe difference is that we call transform instead of fit_transform\non the transformers, since they have already been fit to the training set:",
            "markdown"
        ],
        [
            "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n X_new_counts = count_vect.transform(docs_new)\n X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n\n predicted = clf.predict(X_new_tfidf)\n\n for doc, category in zip(docs_new, predicted):\n...     print('%r = %s' % (doc, twenty_train.target_names[category]))\n...\n'God is love' = soc.religion.christian\n'OpenGL on the GPU is fast' = comp.graphics",
            "code"
        ]
    ],
    "Tutorials->Working With Text Data->Building a pipeline": [
        [
            "In order to make the vectorizer => transformer => classifier easier\nto work with, scikit-learn provides a  class that behaves\nlike a compound classifier:",
            "markdown"
        ],
        [
            "from sklearn.pipeline import Pipeline\n text_clf = Pipeline([\n...     ('vect', CountVectorizer()),\n...     ('tfidf', TfidfTransformer()),\n...     ('clf', MultinomialNB()),\n... ])",
            "code"
        ],
        [
            "The names vect, tfidf and clf (classifier) are arbitrary.\nWe will use them to perform grid search for suitable hyperparameters below.\nWe can now train the model with a single command:",
            "markdown"
        ],
        [
            "text_clf.fit(twenty_train.data, twenty_train.target)\nPipeline(...)",
            "code"
        ]
    ],
    "Tutorials->Working With Text Data->Evaluation of the performance on the test set": [
        [
            "Evaluating the predictive accuracy of the model is equally easy:",
            "markdown"
        ],
        [
            "import numpy as np\n twenty_test = fetch_20newsgroups(subset='test',\n...     categories=categories, shuffle=True, random_state=42)\n docs_test = twenty_test.data\n predicted = text_clf.predict(docs_test)\n np.mean(predicted == twenty_test.target)\n0.8348...",
            "code"
        ],
        [
            "We achieved 83.5% accuracy. Let\u2019s see if we can do better with a\nlinear ,\nwhich is widely regarded as one of\nthe best text classification algorithms (although it\u2019s also a bit slower\nthan na\u00efve Bayes). We can change the learner by simply plugging a different\nclassifier object into our pipeline:",
            "markdown"
        ],
        [
            "from sklearn.linear_model import SGDClassifier\n text_clf = Pipeline([\n...     ('vect', CountVectorizer()),\n...     ('tfidf', TfidfTransformer()),\n...     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n...                           alpha=1e-3, random_state=42,\n...                           max_iter=5, tol=None)),\n... ])\n\n text_clf.fit(twenty_train.data, twenty_train.target)\nPipeline(...)\n predicted = text_clf.predict(docs_test)\n np.mean(predicted == twenty_test.target)\n0.9101...",
            "code"
        ],
        [
            "We achieved 91.3% accuracy using the SVM. scikit-learn provides further\nutilities for more detailed performance analysis of the results:",
            "markdown"
        ],
        [
            "from sklearn import metrics\n print(metrics.classification_report(twenty_test.target, predicted,\n...     target_names=twenty_test.target_names))\n                        precision    recall  f1-score   support\n\n           alt.atheism       0.95      0.80      0.87       319\n         comp.graphics       0.87      0.98      0.92       389\n               sci.med       0.94      0.89      0.91       396\nsoc.religion.christian       0.90      0.95      0.93       398\n\n              accuracy                           0.91      1502\n             macro avg       0.91      0.91      0.91      1502\n          weighted avg       0.91      0.91      0.91      1502\n\n\n metrics.confusion_matrix(twenty_test.target, predicted)\narray([[256,  11,  16,  36],\n       [  4, 380,   3,   2],\n       [  5,  35, 353,   3],\n       [  5,  11,   4, 378]])",
            "code"
        ],
        [
            "As expected the confusion matrix shows that posts from the newsgroups\non atheism and Christianity are more often confused for one another than\nwith computer graphics.",
            "markdown"
        ]
    ],
    "Tutorials->Working With Text Data->Parameter tuning using grid search": [
        [
            "We\u2019ve already encountered some parameters such as use_idf in the\nTfidfTransformer. Classifiers tend to have many parameters as well;\ne.g., MultinomialNB includes a smoothing parameter alpha and\nSGDClassifier has a penalty parameter alpha and configurable loss\nand penalty terms in the objective function (see the module documentation,\nor use the Python help function to get a description of these).",
            "markdown"
        ],
        [
            "Instead of tweaking the parameters of the various components of the\nchain, it is possible to run an exhaustive search of the best\nparameters on a grid of possible values. We try out all classifiers\non either words or bigrams, with or without idf, and with a penalty\nparameter of either 0.01 or 0.001 for the linear SVM:",
            "markdown"
        ],
        [
            "from sklearn.model_selection import GridSearchCV\n parameters = {\n...     'vect__ngram_range': [(1, 1), (1, 2)],\n...     'tfidf__use_idf': (True, False),\n...     'clf__alpha': (1e-2, 1e-3),\n... }",
            "code"
        ],
        [
            "Obviously, such an exhaustive search can be expensive. If we have multiple\nCPU cores at our disposal, we can tell the grid searcher to try these eight\nparameter combinations in parallel with the n_jobs parameter. If we give\nthis parameter a value of -1, grid search will detect how many cores\nare installed and use them all:",
            "markdown"
        ],
        [
            "gs_clf = GridSearchCV(text_clf, parameters, cv=5, n_jobs=-1)",
            "code"
        ],
        [
            "The grid search instance behaves like a normal scikit-learn\nmodel. Let\u2019s perform the search on a smaller subset of the training data\nto speed up the computation:",
            "markdown"
        ],
        [
            "gs_clf = gs_clf.fit(twenty_train.data[:400], twenty_train.target[:400])",
            "code"
        ],
        [
            "The result of calling fit on a GridSearchCV object is a classifier\nthat we can use to predict:",
            "markdown"
        ],
        [
            "twenty_train.target_names[gs_clf.predict(['God is love'])[0]]\n'soc.religion.christian'",
            "code"
        ],
        [
            "The object\u2019s best_score_ and best_params_ attributes store the best\nmean score and the parameters setting corresponding to that score:",
            "markdown"
        ],
        [
            "gs_clf.best_score_\n0.9...\n for param_name in sorted(parameters.keys()):\n...     print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))\n...\nclf__alpha: 0.001\ntfidf__use_idf: True\nvect__ngram_range: (1, 1)",
            "code"
        ],
        [
            "A more detailed summary of the search is available at gs_clf.cv_results_.",
            "markdown"
        ],
        [
            "The cv_results_ parameter can be easily imported into pandas as a\nDataFrame for further inspection.",
            "markdown"
        ]
    ],
    "Tutorials->Working With Text Data->Parameter tuning using grid search->Exercises": [
        [
            "To do the exercises, copy the content of the \u2018skeletons\u2019 folder as\na new folder named \u2018workspace\u2019:",
            "markdown"
        ],
        [
            "cp -r skeletons workspace",
            "code"
        ],
        [
            "You can then edit the content of the workspace without fear of losing\nthe original exercise instructions.",
            "markdown"
        ],
        [
            "Then fire an ipython shell and run the work-in-progress script with:",
            "markdown"
        ],
        [
            "[1] %run workspace/exercise_XX_script.py arg1 arg2 arg3",
            "code"
        ],
        [
            "If an exception is triggered, use %debug to fire-up a post\nmortem ipdb session.",
            "markdown"
        ],
        [
            "Refine the implementation and iterate until the exercise is solved.",
            "markdown"
        ],
        [
            "<strong>For each exercise, the skeleton file provides all the necessary import\nstatements, boilerplate code to load the data and sample code to evaluate\nthe predictive accuracy of the model.</strong>",
            "markdown"
        ]
    ],
    "Tutorials->Working With Text Data->Exercise 1: Language identification": [
        [
            "Write a text classification pipeline using a custom preprocessor and\nCharNGramAnalyzer using data from Wikipedia articles as training set.",
            "markdown"
        ],
        [
            "Evaluate the performance on some held out test set.",
            "markdown"
        ],
        [
            "ipython command line:",
            "markdown"
        ],
        [
            "%run workspace/exercise_01_language_train_model.py data/languages/paragraphs/",
            "code"
        ]
    ],
    "Tutorials->Working With Text Data->Exercise 2: Sentiment Analysis on movie reviews": [
        [
            "Write a text classification pipeline to classify movie reviews as either\npositive or negative.",
            "markdown"
        ],
        [
            "Find a good set of parameters using grid search.",
            "markdown"
        ],
        [
            "Evaluate the performance on a held out test set.",
            "markdown"
        ],
        [
            "ipython command line:",
            "markdown"
        ],
        [
            "%run workspace/exercise_02_sentiment.py data/movie_reviews/txt_sentoken/",
            "code"
        ]
    ],
    "Tutorials->Working With Text Data->Exercise 3: CLI text classification utility": [
        [
            "Using the results of the previous exercises and the cPickle\nmodule of the standard library, write a command line utility that\ndetects the language of some text provided on stdin and estimate\nthe polarity (positive or negative) if the text is written in\nEnglish.",
            "markdown"
        ],
        [
            "Bonus point if the utility is able to give a confidence level for its\npredictions.",
            "markdown"
        ]
    ],
    "Tutorials->Working With Text Data->Where to from here": [
        [
            "Here are a few suggestions to help further your scikit-learn intuition\nupon the completion of this tutorial:",
            "markdown"
        ],
        [
            "Try playing around with the analyzer and token normalisation under\n.",
            "markdown"
        ],
        [
            "If you don\u2019t have labels, try using\n\non your problem.",
            "markdown"
        ],
        [
            "If you have multiple labels per document, e.g categories, have a look\nat the .",
            "markdown"
        ],
        [
            "Try using  for\n.",
            "markdown"
        ],
        [
            "Have a look at using\n to\nlearn from data that would not fit into the computer main memory.",
            "markdown"
        ],
        [
            "Have a look at the \nas a memory efficient alternative to .",
            "markdown"
        ]
    ],
    "Tutorials->Choosing the right estimator": [
        [
            "Often the hardest part of solving a machine learning problem can\nbe finding the right estimator for the job.",
            "markdown"
        ],
        [
            "Different estimators are better suited for different types of data\nand different problems.",
            "markdown"
        ],
        [
            "The flowchart below is designed to give users a bit of\na rough guide on how to approach problems with regard to\nwhich estimators to try on your data.",
            "markdown"
        ],
        [
            "Click on any estimator in the chart below to see its documentation.\n<img alt=\"Move mouse over image\" class=\"map\" src=\"../../_static/ml_map.png\" usemap=\"#imgmap\"/>\n<map name=\"imgmap\">\n<area coords=\"97,1094, 76,1097, 56,1105, 40,1120, 35,1132, 34,1145, 35,1153, 40,1162, 46,1171, 54,1177, 62,1182, 72,1187, 81,1188, 100,1189, 118,1186, 127,1182, 136,1177, 146,1170, 152,1162, 155,1158, 158,1146, 158,1126, 143,1110, 138,1105, 127,1100, 97,1094\" href=\"../../documentation.html\" shape=\"poly\" title=\"Back to Documentation\"/>\n<area coords=\"1556,446, 1556,446, 1556,476, 1556,476, 1556,476, 1676,476, 1676,476, 1676,476, 1676,446, 1676,446, 1676,446, 1556,446, 1556,446\" data-maphilight='{\"strokeColor\":\"0000ff\",\"strokeWidth\":5,\"fillColor\":\"66FF66\",\"fillOpacity\":0.4}' href=\"../../modules/linear_model.html#elastic-net\" shape=\"poly\" title=\"Elastic Net Documentation\"/>\n<area coords=\"209,200, 209,200, 209,252, 209,252, 209,252, 332,252, 332,252, 332,252, 332,200, 332,200, 332,200, 209,200, 209,200\" data-maphilight='{\"strokeColor\":\"0000ff\",\"strokeWidth\":5,\"fillColor\":\"66FF66\",\"fillOpacity\":0.4}' href=\"../../modules/ensemble.html\" shape=\"poly\" title=\"Ensembe Methods Documentation\"/>\n<area coords=\"1828,506, 1828,506, 1828,544, 1828,544, 1828,544, 2054,544, 2054,544, 2054,544, 2054,506, 2054,506, 2054,506, 1828,506, 1828,506\" data-maphilight='{\"strokeColor\":\"0000ff\",\"strokeWidth\":5,\"fillColor\":\"66FF66\",\"fillOpacity\":0.4}' href=\"../../modules/ensemble.html\" shape=\"poly\" title=\"Ensembe Methods Documentation\"/>\n<area coords=\"142,637, 142,637, 142,667, 142,667, 142,667, 265,667, 265,667, 265,667, 265,637, 265,637, 265,637, 142,637, 142,637\" data-maphilight='{\"strokeColor\":\"0000ff\",\"strokeWidth\":5,\"fillColor\":\"66FF66\",\"fillOpacity\":0.4}' href=\"../../modules/mixture.html\" shape=\"poly\" title=\"Gaussian mixture models Documentation\"/>\n<area coords=\"1500,799, 1500,799, 1500,844, 1500,844, 1500,844, 1618,844, 1618,844, 1618,844, 1618,800, 1618,800, 1618,800, 1500,799, 1500,799\" data-maphilight='{\"strokeColor\":\"0000ff\",\"strokeWidth\":5,\"fillColor\":\"66FF66\",\"fillOpacity\":0.4}' href=\"../../modules/manifold.html#isomap\" shape=\"poly\" title=\"Isomap Documentation\"/>\n<area coords=\"1477,982, 1477,982, 1477,1055, 1477,1055, 1477,1055, 1638,1055, 1638,1055, 1638,1055, 1638,982, 1638,982, 1638,982, 1477,982, 1477,982\" data-maphilight='{\"strokeColor\":\"0000ff\",\"strokeWidth\":5,\"fillColor\":\"66FF66\",\"fillOpacity\":0.4}' href=\"../../modules/kernel_approximation.html\" shape=\"poly\" title=\"Kernel Approximation Documentation\"/>\n<area coords=\"472,100, 472,100, 472,173, 472,173, 472,173, 634,173, 634,173, 634,173, 634,100, 634,100, 634,100, 472,100, 472,100\" data-maphilight='{\"strokeColor\":\"0000ff\",\"strokeWidth\":5,\"fillColor\":\"66FF66\",\"fillOpacity\":0.4}' href=\"../../modules/kernel_approximation.html\" shape=\"poly\" title=\"Kernel Approximation Documentation\"/>\n<area coords=\"377,605, 377,605, 377,655, 377,655, 377,655, 476,655, 476,655, 476,655, 476,605, 476,605, 476,605, 377,605, 377,605\" data-maphilight='{\"strokeColor\":\"0000ff\",\"strokeWidth\":5,\"fillColor\":\"66FF66\",\"fillOpacity\":0.4}' href=\"../../modules/clustering.html#k-means\" shape=\"poly\" title=\"KMeans Documentation\"/>\n<area coords=\"440,219, 440,219, 440,293, 440,293, 440,293, 574,293, 574,293, 574,293, 574,219, 574,219, 574,219, 440,219, 440,219\" data-maphilight='{\"strokeColor\":\"0000ff\",\"strokeWidth\":5,\"fillColor\":\"66FF66\",\"fillOpacity\":0.4}' href=\"../../modules/neighbors.html\" shape=\"poly\" title=\"Nearest Neighbors\"/>\n<area coords=\"1550,408, 1550,408, 1550,436, 1550,436, 1550,436, 1671,436, 1671,436, 1671,436, 1671,408, 1671,408, 1671,408, 1550,408, 1550,408\" data-maphilight='{\"strokeColor\":\"0000ff\",\"strokeWidth\":5,\"fillColor\":\"66FF66\",\"fillOpacity\":0.4}' href=\"../../modules/linear_model.html#lasso\" shape=\"poly\" title=\"Lasso Documentation\"/>\n<area coords=\"609,419, 609,419, 609,492, 609,492, 609,492, 693,492, 693,492, 693,492, 693,419, 693,419, 693,419, 609,419, 609,419\" data-maphilight='{\"strokeColor\":\"0000ff\",\"strokeWidth\":5,\"fillColor\":\"66FF66\",\"fillOpacity\":0.4}' href=\"../../modules/svm.html#classification\" shape=\"poly\" title=\"LinearSVC Documentation\"/>\n<area coords=\"1719,888, 1719,888, 1719,945, 1719,945, 1719,945, 1819,945, 1819,945, 1819,945, 1819,888, 1819,888, 1819,888, 1719,888, 1719,888\" data-maphilight='{\"strokeColor\":\"0000ff\",\"strokeWidth\":5,\"fillColor\":\"66FF66\",\"fillOpacity\":0.4}' href=\"../../modules/manifold.html#locally-linear-embedding\" shape=\"poly\" title=\"Locally Linear Embedding Documentation\"/>\n<area coords=\"562,949, 562,949, 562,981, 562,981, 562,981, 682,981, 682,981, 682,981, 682,949, 682,949, 682,949, 562,949, 562,949\" data-maphilight='{\"strokeColor\":\"0000ff\",\"strokeWidth\":5,\"fillColor\":\"66FF66\",\"fillOpacity\":0.4}' href=\"../../modules/clustering.html#mean-shift\" shape=\"poly\" title=\"Mean Shift Documentation\"/>\n<area coords=\"343,917, 343,917, 343,990, 343,990, 343,990, 461,990, 461,990, 461,990, 461,917, 461,917, 461,917, 343,917, 343,917\" data-maphilight='{\"strokeColor\":\"0000ff\",\"strokeWidth\":5,\"fillColor\":\"66FF66\",\"fillOpacity\":0.4}' href=\"../../modules/clustering.html#mini-batch-k-means\" shape=\"poly\" title=\"Mini Batch K-means Documentation\"/>\n<area coords=\"194,339, 194,339, 194,412, 194,412, 194,412, 294,412, 294,412, 294,412, 294,339, 294,339, 294,339, 194,339, 194,339\" data-maphilight='{\"strokeColor\":\"0000ff\",\"strokeWidth\":5,\"fillColor\":\"66FF66\",\"fillOpacity\":0.4}' href=\"../../modules/naive_bayes.html\" shape=\"poly\" title=\"Naive Bayes Documentation\"/>\n<area coords=\"1208,778, 1208,778, 1208,851, 1208,851, 1208,851, 1350,851, 1350,851, 1350,851, 1350,778, 1350,778, 1350,778, 1208,778, 1208,778\" data-maphilight='{\"strokeColor\":\"0000ff\",\"strokeWidth\":5,\"fillColor\":\"66FF66\",\"fillOpacity\":0.4}' href=\"../../modules/decomposition.html#principal-component-analysis-pca\" shape=\"poly\" title=\"Principal Component Analysis Documentation\"/>\n<area coords=\"1696,648, 1696,648, 1696,687, 1696,687, 1696,687, 1890,687, 1890,687, 1890,687, 1890,648, 1890,648, 1890,648, 1696,648, 1696,648\" data-maphilight='{\"strokeColor\":\"0000ff\",\"strokeWidth\":5,\"fillColor\":\"66FF66\",\"fillOpacity\":0.4}' href=\"../../modules/linear_model.html#ridge-regression\" shape=\"poly\" title=\"Ridge Regression Documentation\"/>\n<area coords=\"691,205, 691,205, 691,278, 691,278, 691,278, 803,278, 803,278, 803,278, 803,205, 803,205, 803,205, 691,205, 691,205\" data-maphilight='{\"strokeColor\":\"0000ff\",\"strokeWidth\":5,\"fillColor\":\"66FF66\",\"fillOpacity\":0.4}' href=\"../../modules/sgd.html#classification\" shape=\"poly\" title=\"SGD Classifier Documentation\"/>\n<area coords=\"1317,425, 1317,425, 1317,498, 1317,498, 1317,498, 1436,498, 1436,498, 1436,498, 1436,425, 1436,425, 1436,425, 1317,425, 1317,425\" data-maphilight='{\"strokeColor\":\"0000ff\",\"strokeWidth\":5,\"fillColor\":\"66FF66\",\"fillOpacity\":0.4}' href=\"../../modules/sgd.html#regression\" shape=\"poly\" title=\"SGD Regression Documentation\"/>\n<area coords=\"145,572, 145,572, 145,631, 145,631, 145,631, 267,631, 267,631, 267,631, 267,572, 267,572, 267,572, 145,572, 145,572\" data-maphilight='{\"strokeColor\":\"0000ff\",\"strokeWidth\":5,\"fillColor\":\"66FF66\",\"fillOpacity\":0.4}' href=\"../../modules/clustering.html#spectral-clustering\" shape=\"poly\" title=\"Spectral Clustering Documentation\"/>\n<area coords=\"1502,849, 1502,849, 1502,910, 1502,910, 1502,910, 1618,910, 1618,910, 1618,910, 1618,849, 1618,849, 1618,849, 1502,849, 1502,849\" data-maphilight='{\"strokeColor\":\"0000ff\",\"strokeWidth\":5,\"fillColor\":\"66FF66\",\"fillOpacity\":0.4}' href=\"../../modules/manifold.html#spectral-embedding\" shape=\"poly\" title=\"Spectral Embedding Documentation\"/>\n<area coords=\"210,157, 210,157, 210,194, 210,194, 210,194, 333,194, 333,194, 333,194, 333,157, 333,157, 333,157, 210,157, 210,157\" data-maphilight='{\"strokeColor\":\"0000ff\",\"strokeWidth\":5,\"fillColor\":\"66FF66\",\"fillOpacity\":0.4}' href=\"../../modules/svm.html#classification\" shape=\"poly\" title=\"SVC Documentation\"/>\n<area coords=\"1696,692, 1696,692, 1696,732, 1696,732, 1696,732, 1890,732, 1890,732, 1890,732, 1890,692, 1890,692, 1890,692, 1696,692, 1696,692\" data-maphilight='{\"strokeColor\":\"0000ff\",\"strokeWidth\":5,\"fillColor\":\"66FF66\",\"fillOpacity\":0.4}' href=\"../../modules/svm.html#regression\" shape=\"poly\" title=\"SVR Documentation\"/>\n<area coords=\"1831,458, 1831,458, 1831,496, 1831,496, 1831,496, 2052,496, 2052,496, 2052,496, 2052,458, 2052,458, 2052,458, 1831,458, 1831,458\" data-maphilight='{\"strokeColor\":\"0000ff\",\"strokeWidth\":5,\"fillColor\":\"66FF66\",\"fillOpacity\":0.4}' href=\"../../modules/svm.html#regression\" shape=\"poly\" title=\"SVR Documentation\"/>\n<area coords=\"562,994, 562,994, 562,1026, 562,1026, 562,1026, 682,1026, 682,1026, 682,1026, 682,994, 682,994, 682,994, 562,994, 562,994\" data-maphilight='{\"strokeColor\":\"0000ff\",\"strokeWidth\":5,\"fillColor\":\"66FF66\",\"fillOpacity\":0.4}' href=\"../../modules/mixture.html#bgmm\" shape=\"poly\" title=\" Bayesian GMM Documentation\"/>\n</map>",
            "markdown"
        ]
    ],
    "Tutorials->External Resources, Videos and Talks": [
        [
            "For written tutorials, see the  of\nthe documentation.",
            "markdown"
        ]
    ],
    "Tutorials->External Resources, Videos and Talks->New to Scientific Python?": [
        [
            "For those that are still new to the scientific Python ecosystem, we highly\nrecommend the . This will help you find your footing a\nbit and will definitely improve your scikit-learn experience.  A basic\nunderstanding of NumPy arrays is recommended to make the most of scikit-learn.",
            "markdown"
        ]
    ],
    "Tutorials->External Resources, Videos and Talks->External Tutorials": [
        [
            "There are several online tutorials available which are geared toward\nspecific subject areas:",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Tutorials->External Resources, Videos and Talks->Videos": [
        [
            "An introduction to scikit-learn  and\n at Scipy 2013\nby ,   and . Notebooks on\n.",
            "markdown"
        ],
        [
            "by  at\nICML 2010",
            "markdown"
        ],
        [
            "A three minute video from a very early stage of scikit-learn, explaining the\nbasic idea and approach we are following.\n</blockquote>",
            "markdown"
        ],
        [
            "by  at SciPy 2011",
            "markdown"
        ],
        [
            "An extensive tutorial, consisting of four sessions of one hour.\nThe tutorial covers the basics of machine learning,\nmany algorithms and how to apply them using scikit-learn. The\nmaterial corresponding is now in the scikit-learn documentation\nsection .\n</blockquote>",
            "markdown"
        ],
        [
            "(and )\nby  at PyCon 2011",
            "markdown"
        ],
        [
            "Thirty minute introduction to text classification. Explains how to\nuse NLTK and scikit-learn to solve real-world text classification\ntasks and compares against cloud-based solutions.\n</blockquote>",
            "markdown"
        ],
        [
            "by  at PyCon 2012",
            "markdown"
        ],
        [
            "3-hours long introduction to prediction tasks using scikit-learn.\n</blockquote>",
            "markdown"
        ],
        [
            "by  at the 2012 PyData workshop at Google",
            "markdown"
        ],
        [
            "Interactive demonstration of some scikit-learn features. 75 minutes.\n</blockquote>",
            "markdown"
        ],
        [
            "by  at PyData NYC 2012",
            "markdown"
        ],
        [
            "Presentation using the online tutorial, 45 minutes.\n</blockquote>",
            "markdown"
        ]
    ]
}