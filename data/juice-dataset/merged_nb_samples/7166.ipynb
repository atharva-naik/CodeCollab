{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e1ac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the tail of our cluster df\n",
    "clusters_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67e85b0",
   "metadata": {},
   "source": [
    "Now that we have our data organized by cluster assignment, we're ready to check for tradeable relationships. To do this, we will create every possible pair combination within a respective cluster. We can then later run the CADF test on specific pairs from our clusters. Let's create a method that will take the symbols from a specific cluster as an input, compute the possible pair combinations of a respective cluster and store our pairs into a separate list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c879516",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating method to identify each possible pair \n",
    "def create_pairs(symbolList):\n",
    "    #creating a list to hold each possible pair\n",
    "    pairs=[]\n",
    "    #initializing placeholders for the symbols in each pair\n",
    "    x=0\n",
    "    y=0\n",
    "    for count,symbol in enumerate(symbolList):\n",
    "        for nextCount,nextSymbol in enumerate(symbolList):\n",
    "            x=symbol\n",
    "            y=nextSymbol\n",
    "            if x !=y:\n",
    "                pairs.append([x,y])          \n",
    "    \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c092d2fe",
   "metadata": {},
   "source": [
    "We will now create a list of symbols. Let's use the stock symbols from cluster 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921d544f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating list of symbols from cluster 0\n",
    "symbol_list_0=['ADBE','AET','AIG','ANTM','AMAT']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1887fcda",
   "metadata": {},
   "source": [
    "We will now use our create pairs method above to create a list of lists of the possible pair combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256ee83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of lists of pairs\n",
    "all_pairs=create_pairs(symbol_list_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cb3dab",
   "metadata": {},
   "source": [
    "Let's check our pair combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ba951b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing list of all_pairs from cluster 0\n",
    "all_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7dea80",
   "metadata": {},
   "source": [
    "Okay. Now that we have our possible pair combinations from cluster 0, let's import our intraday data for these stocks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910819bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing our stock variables\n",
    "adbe=pd.read_csv('ADBE_5min.csv')\n",
    "aet=pd.read_csv('AET_5min.csv')\n",
    "aig=pd.read_csv('AIG_5min.csv')\n",
    "antm=pd.read_csv('ANTM_5min.csv')\n",
    "amat=pd.read_csv('AMAT_5min.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1611a9",
   "metadata": {},
   "source": [
    "Let's check our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2b90fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating instance of updated statarb strategy\n",
    "adbe_antm_spo=statarb_update(adbe_test, antm_test, 2,17,-2, 2,17,'05/01/18','06/12/18',adbe_antm_regime_predictions,'Target',avoid1=0,target1=1,\n",
    "                  exit_zscore=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4226d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating spread\n",
    "adbe_antm_spo.create_spread()\n",
    "#generating signals\n",
    "adbe_antm_spo.generate_signals()\n",
    "#getting performance\n",
    "#notice that we are passing in our weight from our efficient frontier analysis here\n",
    "adbe_antm_spo.create_returns(adbe_antm_allocation,'ADBE_ANTM SPO Framework')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5370c443",
   "metadata": {},
   "source": [
    "Let's repeat this process for our AET_ANTM implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aceeb79c",
   "metadata": {},
   "source": [
    "#### AET_ANTM SPO Framework Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5b9345",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating instance of updated statarb strategy\n",
    "aet_antm_spo=statarb_update(aet_test, antm_test, 1,12,-2, 2,12,'05/01/18','06/12/18',aet_antm_regime_predictions,'Avoid',avoid1=1,target1=0,\n",
    "                  exit_zscore=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6055048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating spread\n",
    "aet_antm_spo.create_spread()\n",
    "#generating signals\n",
    "aet_antm_spo.generate_signals()\n",
    "#getting performance\n",
    "#notice that we are passing in our efficient frontier weight amount here\n",
    "aet_antm_spo.create_returns(aet_antm_allocation,'AET_ANTM SPO Framework')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc1bc2c",
   "metadata": {},
   "source": [
    "#### We Can Now Compose Our SPO Framework Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332fa935",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataframe for SPO Portfolio\n",
    "spo_portfolio=pd.DataFrame()\n",
    "spo_portfolio['ADBE_ANTM']=adbe_antm_spo.portfolio['Portfolio Value']\n",
    "spo_portfolio['AET_ANTM']=aet_antm_spo.portfolio['Portfolio Value']\n",
    "spo_portfolio['Cash']=10000\n",
    "spo_portfolio['Total Portfolio Value']=spo_portfolio['ADBE_ANTM']+spo_portfolio['AET_ANTM']+spo_portfolio['Cash']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de913d0c",
   "metadata": {},
   "source": [
    "#### Let's Now Compute the Returns of Our SPO Framework Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117bbec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding returns column to SPO Portfolio dataframe\n",
    "spo_portfolio['Returns']=np.log(spo_portfolio['Total Portfolio Value']/spo_portfolio['Total Portfolio Value'].shift(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c1103a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spo_portfolio_mu=spo_portfolio['Returns'].mean()\n",
    "spo_portfolio_sigma=spo_portfolio['Returns'].std()\n",
    "\n",
    "#recall that we initialized our interest assumption earlier\n",
    "spo_portfolio_sharpe=(spo_portfolio_mu-rate)/spo_portfolio_sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b0a72b",
   "metadata": {},
   "source": [
    "#### Let's Plot the Equity Curve of Our SPO Portfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a7d1a2",
   "metadata": {},
   "source": [
    "Let's check our returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceb83b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking returns\n",
    "returns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764e6e02",
   "metadata": {},
   "source": [
    "We can now create our mean returns an annualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a06012",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_returns_252=returns.mean()*252"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2cb9a0",
   "metadata": {},
   "source": [
    "Let's check our average annualized returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd34d85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking average annualized returns\n",
    "avg_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487b201f",
   "metadata": {},
   "source": [
    "The variance of our portfolio can be expressed as $$\\large  \\sum_{i\\in I} \\rm \\sum_{j\\in I} w_{i}w_{j}\\rm \\sigma_{ij} $$ where $\\large w_i$ is the weight of the $\\large i_{th} $ asset, $\\large w_j$ is the weight of the $\\large j_{th}$ asset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849fa745",
   "metadata": {},
   "source": [
    "Now we will use our returns to create our annualized covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d14606",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance_matrix=returns.cov()*252"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932e33a4",
   "metadata": {},
   "source": [
    "Let's check our covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925e8e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b43ae53",
   "metadata": {},
   "source": [
    "We can now create a variable to hold our weights for each strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c928f8",
   "metadata": {},
   "source": [
    "Now that we have our cluster assignments, let's group our data by the cluster assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da72f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataframe to hold data\n",
    "clusters_df=pd.DataFrame()\n",
    "#grouping our data by cluster for clusters with atleast 2 stocks in it.\n",
    "clusters_df=pd.concat(i for clusters_df, i in features_copy.groupby(features_copy['Cluster']) if len(i) >1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb1e351",
   "metadata": {},
   "source": [
    "Let's now check of our new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6839b752",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the head of clusters df\n",
    "clusters_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaa0c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking head of ADBE\n",
    "adbe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44f629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking tail of AIG\n",
    "aig.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0555a36a",
   "metadata": {},
   "source": [
    "We can see that we have 5 min bar data for our stocks ranging from January 3 2018 to the first half of the session on June 11 2018. We can now test our pairs for cointegration. Recall that we are going to use part of our data as our assessment period so we don't want to include it in our test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb021371",
   "metadata": {},
   "source": [
    "\n",
    "We'll use January to May as our training period and assess our portfolios over the remainder of our data. Let's create variables to hold our training period data. We'll store use our Close column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b28a06",
   "metadata": {},
   "source": [
    "So that we can parse out our data by our training period, we'll make our date column the index of our data. We begin by making a copy of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b110ba",
   "metadata": {},
   "source": [
    "Let's create a method to perform this across our symbols data. We'll store our original symbols data in a dictionary and pass it into our function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f4f6b8",
   "metadata": {},
   "source": [
    "Now let's use our find_k method to find the optimal value for K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47876466",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding K\n",
    "find_k(features_copy.fillna(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3104047",
   "metadata": {},
   "source": [
    "Okay we will use 15 as our value for K. Now we're ready to implement our K-Means Clustering algorithm with K equal to 15 and look for tradable relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3307508",
   "metadata": {},
   "source": [
    "Let's begin by initializing our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b851c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialzing K-Means algorithm\n",
    "kmeans=KMeans(n_clusters=15,random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e6aaaa",
   "metadata": {},
   "source": [
    "Notice that I used random_state=101. This is so that you can recreate the same results posted here. We will now fit our K-Means Algorithm to our features data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3856ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting kmeans to our features data\n",
    "kmeans.fit(features_copy.fillna(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150f869f",
   "metadata": {},
   "source": [
    "Note that we called the fillna method on our features_copy dataframe and replaced NaNs values with 0. We can now review our clusters. Let's start by checking our our labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97d4343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting cluster labels\n",
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abf4462",
   "metadata": {},
   "source": [
    "We will now add our cluster assignments back to our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d7f263",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding cluster labels to dataframe\n",
    "features_copy['Cluster']=kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6055c9b9",
   "metadata": {},
   "source": [
    "Let's now review our features dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e88edaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reviewing features dataframe\n",
    "features_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4985dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling tail method our dataframe\n",
    "features_copy.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e3e614",
   "metadata": {},
   "source": [
    "We can see that Cluster 0 actually contains a total of 76 symbols. Given that we found 3 tradeable relationships, we'll use these. However, I will illustrate how, in the event that none of the symbols in our first cluster were cointegrated, we would go about checking the symbols in a specific cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb962dd",
   "metadata": {},
   "source": [
    " We'll need to create a method that will allow us to iterate over our clusters column and return the symbols that are within a specific cluster. To begin we'll add our symbols back to our dataframe as a column so that we can retrieve them if our cluster condition is met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e58ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding Symbol Column back to cluster_df\n",
    "clusters_df['Symbol']=clusters_df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f76fdb",
   "metadata": {},
   "source": [
    "Let's recheck our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49d3bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking update to cluster_df\n",
    "clusters_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8369161",
   "metadata": {},
   "source": [
    "Now we're ready to create our method of retrieving and storing our symbols in a list based on our cluster. For this task, we'll use a list comprehension. We'll apply our method to cluster 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4337b1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols_cluster_9=[ clusters_df['Symbol'][count] for count,value in enumerate(clusters_df['Cluster'].values) if value == 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccc78c1",
   "metadata": {},
   "source": [
    "Let's check the length of our symbols_cluster_9 list to ensure that our method worked correctly. Recall that our Counter method returned a value of 4 for the number of symbols in this cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8721050",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the length of our cluster 9 list\n",
    "len(symbols_cluster_9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9aee6f",
   "metadata": {},
   "source": [
    "Great! We see that our list comprehension is working properly. Now we can use this list of symbols to create every possible pair combination for cluster 9. Let's check what symbols are in cluster 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef13ffa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the symbols in cluster 9\n",
    "symbols_cluster_9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c86d1ab",
   "metadata": {},
   "source": [
    "We'll now use this list to create our pair combinations for cluster 9 using our create_pairs method from earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d262ec03",
   "metadata": {},
   "source": [
    "We can repeat this process for our remaining portfolios and compare our Sharpe ratios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c00ef1b",
   "metadata": {},
   "source": [
    "# Portfolio Construction: Efficient Frontier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d303d42",
   "metadata": {},
   "source": [
    "Thus far we've created our Equally Weighted Portfolio and checked our Sharpe Ratio and Equity Curve. We are no interested in what our Sharpe might be if we found the optimal weightes for each of our StatArb implementations as well as the resulting effect on our equity curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbfce66",
   "metadata": {},
   "source": [
    "We will now turn our attention to simulating our portfolio's mean and variance by assigning random weights to each of our pairs. We will store our portfolio's mean return and sigma and create a scatter plot that will aid us in selecting the most efficient weights for each of our strategy implementations. To best decipher how we should weight our strategy implementations we will include logic that computes the Sharpe Ratios for each portfolio created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dfb391",
   "metadata": {},
   "source": [
    "We can then create new instances of our StatArb implementations using updated allocations and survey the effects on our Sharpe Ratio and equity curve. Though our Bottom Up Portfolio will be an extension of our Equally Weighted Portfolio in that it will only apply our machine learning concepts to the microstructure components, our SPO Framework will combine the weights found within this portfolio construcution process and combine it with the process of constructing the Bottom Up Portfolio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00013258",
   "metadata": {},
   "source": [
    "To begin, let's retieve the MUs and Sigmas of our pairs. Recall that we created a method within class that stores our mean and sigma. We can use our strategy objects to retrieve these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfded76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialzing Mus and Sigmas\n",
    "#ADBE & ANTM\n",
    "adbe_antm_mu=adbe_antm.mu\n",
    "adbe_antm_sigma=adbe_antm.sigma\n",
    "#ANTM & AET\n",
    "antm_aet_mu=antm_aet.mu\n",
    "antm_aet_sigma=antm_aet.sigma\n",
    "#AET & ANTM\n",
    "aet_antm_mu=aet_antm.mu\n",
    "aet_antm_sigma=aet_antm.sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3699c4",
   "metadata": {},
   "source": [
    "The return of our portfolio can be expressed as "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f537e7",
   "metadata": {},
   "source": [
    "$$\\large \\sum_i \\rm w_i \\rm r_i $$ or as $$\\large w_1 \\rm r_1 + w_2 \\rm r_2 + w_i \\rm r_i $$ where $\\large w_i$ is the weight of the $\\large i_{th}$ asset and $\\large r_i$ is the return of the $\\large i_{th}$ asset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cb9c93",
   "metadata": {},
   "source": [
    "Let's create a method to compute returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf22745c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing log returns for our portfolio values\n",
    "returns=np.log(equally_weighted[['ADBE_ANTM','ANTM_AET','AET_ANTM']]/equally_weighted[['ADBE_ANTM','ANTM_AET','AET_ANTM']].shift(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae5b8ef",
   "metadata": {},
   "source": [
    "Okay. Let's recheck our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7c3ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0a0579",
   "metadata": {},
   "source": [
    "Now we're ready to drop the Symbol column from our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c6889f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping symbol column\n",
    "features_copy.drop('Symbol',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0432c7de",
   "metadata": {},
   "source": [
    "Let's recheck our dataframe by calling the head method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f1b50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking head of AET price series\n",
    "aet_test_price_series[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a36238f",
   "metadata": {},
   "source": [
    "We can now construct our StatArb class and create our individual strategies. We will then combine these individual strategies into an equally weighted portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0060fea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class statarb(object):\n",
    "    \n",
    "    def __init__(self,df1, df2,ma,floor, ceiling,beta_lookback,start,end,exit_zscore=0):\n",
    "        #setting the attributes \n",
    "        self.df1=df1 #array of prices for X\n",
    "        self.df2=df2 #array of prices for Y\n",
    "        self.ma=ma# the lookback period \n",
    "        self.floor=floor #the buy threshold for the z-score\n",
    "        self.ceiling=ceiling #the sell threshold for the z-score\n",
    "        self.Close='Close Long'  #used as close signal for longs\n",
    "        self.Cover='Cover Short' #used as close signal for shorts\n",
    "        self.exit_zscore=exit_zscore  #the z-score\n",
    "        self.beta_lookback=beta_lookback #the lookback for hedge ratio\n",
    "        self.start=start #the beginning of test period as a string\n",
    "        self.end=end    # the end of test period as a string\n",
    "        \n",
    "    #create price spread\n",
    "    def create_spread(self):\n",
    "        \n",
    "        #creating new dataframe\n",
    "        self.df=pd.DataFrame(index=range(0,len(self.df1)))\n",
    "        \n",
    "        try:\n",
    "            self.df['X']=self.df1\n",
    "            self.df['Y']=self.df2\n",
    "        except:\n",
    "            print('Length of self.df:')\n",
    "            print(len(self.df))\n",
    "            print('')\n",
    "            print('Length of self.df1:')\n",
    "            print(len(self.df1))\n",
    "            print('')\n",
    "            print('Length of self.df2:')\n",
    "            print(len(self.df2))\n",
    "            \n",
    "        \n",
    "        #calculating the beta of the pairs \n",
    "        ols=linregress(self.df['Y'],self.df['X'])\n",
    "        self.df['Beta']=ols[0]\n",
    "        \n",
    "        #calculating the spread\n",
    "        self.df['Spread']=self.df['Y']-(self.df['Beta'].rolling(window=self.beta_lookback).mean()*self.df['X'])\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        return self.df.head()\n",
    "    \n",
    "            \n",
    "    def generate_signals(self):\n",
    "        \n",
    "\n",
    "            #creating the z-score\n",
    "            self.df['Z-Score']=(self.df['Spread']-self.df['Spread'].rolling(window=self.ma).mean())/self.df['Spread'].rolling(window=self.ma).std()\n",
    "\n",
    "            #prior z-score\n",
    "            self.df['Prior Z-Score']=self.df['Z-Score'].shift(1)\n",
    "\n",
    "            #Creating Buy and Sell Signals; when to be long, short, exit\n",
    "            self.df['Longs']=(self.df['Z-Score']<=self.floor)*1.0 #buy the spread\n",
    "            self.df['Shorts']=(self.df['Z-Score']>=self.ceiling)*1.0 #short the spread\n",
    "            self.df['Exit']=(self.df['Z-Score']<=self.exit_zscore)*1.0\n",
    "\n",
    "            #tracking positions via for loop implementation\n",
    "            self.df['Long_Market']=0.0\n",
    "            self.df['Short_Market']=0.0\n",
    "\n",
    "            #Setting Variables to track whether or not to be long while iterating over df\n",
    "            self.long_market=0\n",
    "            self.short_market=0\n",
    "\n",
    "            #Determining when to trade\n",
    "            for i,value in enumerate(self.df.iterrows()):\n",
    "                #Calculate longs\n",
    "                if value[1]['Longs']==1.0:\n",
    "                    self.long_market=1\n",
    "\n",
    "                if value[1]['Shorts']==1.0:\n",
    "                    self.short_market=1\n",
    "\n",
    "                if value[1]['Exit']==1.0:\n",
    "\n",
    "                    self.long_market=0\n",
    "                    self.short_market=0\n",
    "\n",
    "                self.df.iloc[i]['Long_Market']=self.long_market\n",
    "                self.df.iloc[i]['Short_Market']=self.short_market\n",
    "\n",
    "\n",
    "        \n",
    "            return \n",
    "        \n",
    "    def create_returns(self, allocation,pair_number):\n",
    "            '''\n",
    "            PARAMETERS\n",
    "            ##########\n",
    "            allocation - the amount of capital alotted for pair\n",
    "            pair_number - string to annotate the plots\n",
    "\n",
    "            '''\n",
    "            self.allocation=allocation \n",
    "            self.pair=pair_number\n",
    "\n",
    "            self.portfolio=pd.DataFrame(index=self.df.index)\n",
    "            self.portfolio['Positions']=self.df['Long_Market']-self.df['Short_Market']\n",
    "            self.portfolio['X']=-1.0*self.df['X']*self.portfolio['Positions']\n",
    "            self.portfolio['Y']=self.df['Y']*self.portfolio['Positions']\n",
    "            self.portfolio['Total']=self.portfolio['X']+self.portfolio['Y']\n",
    "\n",
    "\n",
    "            #creating a percentage return stream\n",
    "            self.portfolio['Returns']=self.portfolio['Total'].pct_change()\n",
    "            self.portfolio['Returns'].fillna(0.0,inplace=True)\n",
    "            self.portfolio['Returns'].replace([np.inf,-np.inf],0.0,inplace=True)\n",
    "            self.portfolio['Returns'].replace(-1.0,0.0,inplace=True)\n",
    "\n",
    "\n",
    "            #calculating metrics\n",
    "            self.mu=(self.portfolio['Returns'].mean())\n",
    "            self.sigma=(self.portfolio['Returns'].std())\n",
    "            self.portfolio['Win']=np.where(self.portfolio['Returns']>0,1,0)\n",
    "            self.portfolio['Loss']=np.where(self.portfolio['Returns']<0,1,0)\n",
    "            self.wins=self.portfolio['Win'].sum()\n",
    "            self.losses=self.portfolio['Loss'].sum()\n",
    "            self.total_trades=self.wins+self.losses\n",
    "            #calculating sharpe ratio with interest rate of \n",
    "            #interest_rate_assumption=0.75\n",
    "            #self.sharp=(self.mu-interest_rate_assumption)/self.sigma\n",
    "\n",
    "            #win loss ratio; \n",
    "            self.win_loss_ratio=(self.wins/self.losses)\n",
    "\n",
    "            #probability of win\n",
    "            self.prob_of_win=(self.wins/self.total_trades)\n",
    "            #probability of loss\n",
    "            self.prob_of_loss=(self.losses/self.total_trades)\n",
    "\n",
    "            #average return of wins\n",
    "            self.avg_win_return=(self.portfolio['Returns']>0).mean()\n",
    "            #average returns of losses\n",
    "            self.avg_loss_return=(self.portfolio['Returns']<0).mean()\n",
    "            #calculating payout ratio\n",
    "            self.payout_ratio=(self.avg_win_return/self.avg_loss_return)\n",
    "\n",
    "            #calculate equity curve\n",
    "            self.portfolio['Returns']=(self.portfolio['Returns']+1.0).cumprod()\n",
    "            self.portfolio['Trade Returns']=(self.portfolio['Total'].pct_change()) #non cumulative Returns\n",
    "            self.portfolio['Portfolio Value']=(self.allocation*self.portfolio['Returns'])\n",
    "            self.portfolio['Portfolio Returns']=self.portfolio['Portfolio Value'].pct_change()\n",
    "            self.portfolio['Initial Value']=self.allocation\n",
    "\n",
    "            with plt.style.context(['ggplot','seaborn-paper']):\n",
    "                #Plotting Portfolio Value   \n",
    "                plt.plot(self.portfolio['Portfolio Value'])\n",
    "                plt.plot(self.portfolio['Initial Value'])\n",
    "                plt.title('%s Strategy Returns '%(self.pair))\n",
    "                plt.legend(loc=0)\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "            return \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005d6548",
   "metadata": {},
   "source": [
    "Okay we're ready to apply our StatArb class to create our strategies. Recall that our pairs are ADBE & ANTM, ANTM & AET and AET & ANTM. You may be wondering why we're using ANTM & AET and AET & ANTM. This is because though both pairs use the same symbols, they are in fact two completely different relationships. In the first pair, ANTM is the X and AET is the Y. In a sense, we're asking what degree of variation in AET can be explained by a unit change in ANTM. In contrast, in the AET & ANTM pair, we're asking what degree of variation in ANTM can be explained by a unit change in AET. When we compute the slope or derivative, we get two completly different numbers, and thus two different relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fd04ba",
   "metadata": {},
   "source": [
    "Let's now apply our StatArb class to our first pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790624e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADBE & ANTM statarb initialization\n",
    "#passing in X, Y, MA, Floor, Ceiling, Beta Lookback, Start, End\n",
    "adbe_antm=statarb(adbe_test_price_series,antm_test_price_series,17,-2,2,17,adbe_test.iloc[0],adbe_test.iloc[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103790f1",
   "metadata": {},
   "source": [
    "Let's now create our spread and generate our signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dbecb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating spread\n",
    "adbe_antm.create_spread()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e5189b",
   "metadata": {},
   "source": [
    "We can now generate our signals and compute our returns on our allocation of $30k USD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8babeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating signals\n",
    "adbe_antm.generate_signals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22a8382",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating returns and passing in our allocation amount\n",
    "adbe_antm.create_returns(30000,'ADBE_ANTM')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb617f4",
   "metadata": {},
   "source": [
    "We can repeat this process for our remaining pairs. Note, our create_spread method returned the head of our dataframe containing our spread. We used a lookback of 17 which is explains why those values are NaNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdb5998",
   "metadata": {},
   "source": [
    "Let's check our pair combinations for cluster 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3f852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking cluster 9 pair combinations.\n",
    "cluster_9_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908f9f98",
   "metadata": {},
   "source": [
    "Okay. We've learned that we have three tradeable relationships from Cluster 0 from the first five symbols we tested. We've also walked through how we would find other pairs in other clusters if we had not found these three pairs. After finding the pairs above, we would apply the same method to check for cointegration as was used to identify our three pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9d8b8b",
   "metadata": {},
   "source": [
    "From here we are ready to begin constructing our portfolios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291c32be",
   "metadata": {},
   "source": [
    "## Brief Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd9a958",
   "metadata": {},
   "source": [
    "Let's take a step back and review what we've accomplished thus far. We began by gaining an understanding of the SPO Framework. We then performed K-Means Clustering on the S&P 500. We found an optimal value for K by creating the elbow graph using our fundamental data features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2787ae9c",
   "metadata": {},
   "source": [
    "After finding K, we created our K-Means algorithm and selected stocks from Cluster 0 to test whether or not we could identify tradeable relationships. Out of the first five symbols in Cluster 0, we found that 3 of the 20 possible combinations were cointegrated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bb6a6e",
   "metadata": {},
   "source": [
    "To illustrate how we would proceed had we not found any tradeable relationships, we created a method that would allow us to select all of the stocks from within a specific cluster and use these to create all possible pair combinations. With these pairs in hand, we could then test these pairs to see if we could find tradeable relationships. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df123f0b",
   "metadata": {},
   "source": [
    "# Portfolio Construction: Equal Weighted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437c9268",
   "metadata": {},
   "source": [
    "Now that we have found some likely tradeable relationships, we're ready to construct our portfolio. In this section we're going to create a portfolio that equally weighs our strategy. We'll assume that we have a portfolio value of $100k USD with 10% in cash."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceef2c28",
   "metadata": {},
   "source": [
    "We will allocate $30k USD to each of our pairs. To begin we will create a class that will allow us to create our StatArb strategies. Recall, our training period for our portfolios that will be employing the use of machine learning is 1/4/2018 to 4/30/2018, thus because this portfolio will not be employing the use of any optimization method, we will construct it over the period beginning 5/1/2018 so that it is consistent with the test period of our remaining portfolios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f5c889",
   "metadata": {},
   "source": [
    "Let's now create the variables of each of our symbols that will hold the data beginning 5/1/2018. We'll make a copy of our data and set our date as the index so that we can parse out our test period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78009250",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating copies of our data\n",
    "adbe_copy=adbe.copy()\n",
    "aet_copy=aet.copy()\n",
    "antm_copy=antm.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec580f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reindexing our data\n",
    "adbe_copy=adbe_copy.reindex(index=adbe_copy['Date'],columns=adbe_copy.columns)\n",
    "aet_copy=aet_copy.reindex(index=aet_copy['Date'],columns=aet_copy.columns)\n",
    "antm_copy=antm_copy.reindex(index=antm_copy['Date'],columns=antm_copy.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93693a2b",
   "metadata": {},
   "source": [
    "Okay. Let's check our dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca877e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking dataframe after reindexing\n",
    "adbe_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c1e234",
   "metadata": {},
   "source": [
    "Now let's restore the values back to our columns. We'll also drop the date column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79071bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping date columns\n",
    "adbe_copy.drop('Date',axis=1,inplace=True)\n",
    "aet_copy.drop('Date',axis=1,inplace=True)\n",
    "antm_copy.drop('Date',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccda608",
   "metadata": {},
   "outputs": [],
   "source": [
    "#restoring our column values back to our data\n",
    "adbe_copy[[' Time',' Open',' High',' Low',' Close',' Volume',' NumberOfTrades',' BidVolume',' AskVolume']]=adbe[[' Time',' Open',' High',' Low',' Close',' Volume',' NumberOfTrades',' BidVolume',' AskVolume']].values\n",
    "aet_copy[[' Time',' Open',' High',' Low',' Close',' Volume',' NumberOfTrades',' BidVolume',' AskVolume']]=aet[[' Time',' Open',' High',' Low',' Close',' Volume',' NumberOfTrades',' BidVolume',' AskVolume']].values\n",
    "antm_copy[[' Time',' Open',' High',' Low',' Close',' Volume',' NumberOfTrades',' BidVolume',' AskVolume']]=antm[[' Time',' Open',' High',' Low',' Close',' Volume',' NumberOfTrades',' BidVolume',' AskVolume']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a2801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rechecking our dataframe\n",
    "adbe_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1fc64b",
   "metadata": {},
   "source": [
    "Now we're ready to slice out our test period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdd74f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating variables to hold testing period data\n",
    "adbe_test=adbe_copy.loc['2018/05/01':]\n",
    "aet_test=aet_copy.loc['2018/05/01':]\n",
    "antm_test=antm_copy.loc['2018/05/01':]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777469f1",
   "metadata": {},
   "source": [
    "Let's check the head and tail of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e973c07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating spread\n",
    "antm_aet_bottom_up.create_spread()\n",
    "#generating signals\n",
    "antm_aet_bottom_up.generate_signals()\n",
    "#getting performance\n",
    "#notice that we are passing in our equal weight amount here\n",
    "antm_aet_bottom_up.create_returns(30000,'ANTM AET Bottom Up')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7133739b",
   "metadata": {},
   "source": [
    "After using our analysis of the historical regimes that our strategy fell within and updating our statarb implementation to avoid Regime 0, we can see that our ANTM_AET implementation didn't trade over our test period. This is an indication that observations fell within Regime 0 over this period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe48e0a",
   "metadata": {},
   "source": [
    "#### AET_ANTM Bottom Up Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfbb067",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating instance of updated statarb strategy\n",
    "aet_antm_bottom_up=statarb_update(aet_test, antm_test, 1,12,-2, 2,12,'05/01/18','06/12/18',aet_antm_regime_predictions,'Avoid',avoid1=1,target1=0,\n",
    "                  exit_zscore=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dcf4b2",
   "metadata": {},
   "source": [
    "Note that in our AET_ANTM implementation we chose not to avoid or target any specific regime. Thus, in the third parameter, denoting portfolio type, we passed in 1 which is our original implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddfb833",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating spread\n",
    "aet_antm_bottom_up.create_spread()\n",
    "#generating signals\n",
    "aet_antm_bottom_up.generate_signals()\n",
    "#getting performance\n",
    "#notice that we are passing in our equal weight amount here\n",
    "aet_antm_bottom_up.create_returns(30000,'AET_ANTM Bottom Up')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ebafb4",
   "metadata": {},
   "source": [
    "### Step 6: Getting Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8a9d5a",
   "metadata": {},
   "source": [
    "We can now aggregate our strategies to form our Bottom Up Portfolio and store our Sharpe Ratio in a variable for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2865ef70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataframe for Bottom Up Portfolio\n",
    "bottom_up_portfolio=pd.DataFrame()\n",
    "bottom_up_portfolio['ADBE_ANTM']=adbe_antm_bottom_up.portfolio['Portfolio Value']\n",
    "bottom_up_portfolio['AET_ANTM']=aet_antm_bottom_up.portfolio['Portfolio Value']\n",
    "bottom_up_portfolio['ANTM_AET']=antm_aet_bottom_up.portfolio['Portfolio Value']\n",
    "bottom_up_portfolio['Cash']=10000\n",
    "bottom_up_portfolio['Total Portfolio Value']=bottom_up_portfolio['ADBE_ANTM']+bottom_up_portfolio['ANTM_AET']+bottom_up_portfolio['AET_ANTM']+bottom_up_portfolio['Cash']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f46232a",
   "metadata": {},
   "source": [
    "We can now add a column to hold the returns of our Bottom Up Portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55214653",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding returns column to Bottom Up Dataframe\n",
    "bottom_up_portfolio['Returns']=np.log(bottom_up_portfolio['Total Portfolio Value']/bottom_up_portfolio['Total Portfolio Value'].shift(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8325a55",
   "metadata": {},
   "source": [
    "Let's now plot the equity curve of our Bottom Up Portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eae908",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(bottom_up_portfolio['Total Portfolio Value'])\n",
    "plt.title('Bottom Up Portfolio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4961aa0",
   "metadata": {},
   "source": [
    "Let's wrap up our Bottom Up Portfolio by creating and storing our Sharpe in a variable for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec64a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_up_portfolio_mu=bottom_up_portfolio['Returns'].mean()\n",
    "bottom_up_portfolio_sigma=bottom_up_portfolio['Returns'].std()\n",
    "\n",
    "#recall that we initialized our interest assumption earlier\n",
    "bottom_up_portfolio_sharpe=(bottom_up_portfolio_mu-rate)/bottom_up_portfolio_sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6f8a50",
   "metadata": {},
   "source": [
    "# Portfolio Construction: SPO Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5b90f5",
   "metadata": {},
   "source": [
    "The SPO Framework combines the logic behind the Efficient Frontier, or top down optimization, with that of the bottom up optimization portfolio. Here we repeat the process used to create the bottom up portfolio except instead of equally weighing our strategies, we weight them according to the weights found when we created the Efficient Frontier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5a8fa9",
   "metadata": {},
   "source": [
    "Recall that our Efficient Frontier Analysis gave our ANTM_AET pair a weight of 0. This is congruent with what we saw when implementing our Bottom Up Portfolio. Thus, we'll create our SPO Implementations on our ADBE_ANTM and AET_ANTM pairs using our Efficient Frontier Weights and regime predictions found during our Bottom Up implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0631b2d",
   "metadata": {},
   "source": [
    "#### ADBE_ANTM SPO Framework Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5075059",
   "metadata": {},
   "source": [
    "We will be using 5minute data over the period of 2018, beginning 1/4/18. Though at this point, we're unsure as to exactly which stocks we will be trading, we know that we will have the following four portfolios: 1) Equally Weighted, 2) Efficient Frontier, 3)Bottom Up Optimization and 4) SPO Framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41019d35",
   "metadata": {},
   "source": [
    "The Equally Weighted Portfolio is the simplest to construct but because the remaining portfolios will require a training period, our test or assessment period will be the second half of 2017, indicating that our equally weighted portfolio will be constructed over the second half of our 2018 data. This period will begin May 1 and end June 12 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317c175e",
   "metadata": {},
   "source": [
    "The remainder of our portfolios will be constructed using the first half of  2017 as a training period and the second half as the test or assessment period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7f9039",
   "metadata": {},
   "source": [
    "# Finding Tradable Relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d312d9f2",
   "metadata": {},
   "source": [
    "As we have covered in prior posts, a key initial problem to solve with developing a statistical arbitrage strategy is that of pair selection. We will once again use K-Means clustering to solve this problem. We will apply K-Means to the S&P 500, create subgroups, and then choose five tradeable relationships to construct our portfolios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2a4934",
   "metadata": {},
   "source": [
    "To get started, let's import our usual libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7896748",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#data analysis and manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "#statistics and machine learning\n",
    "from statsmodels.tsa.api import adfuller\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture as GM\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67009549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db23aef1",
   "metadata": {},
   "source": [
    "We're going to use the P/E, EPS, and Market Cap as our features to identify subgroups within the S&P 500. Let's import our features data now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40d61a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the Excel file that contains our features data\n",
    "fundamentals=pd.ExcelFile('SPO_Data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6422f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parsing the Fundamentals sheet from our Excel file of which holds our P/E, EPS, and MarketCap data\n",
    "features=fundamentals.parse('Fundamentals')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c83b07",
   "metadata": {},
   "source": [
    "Now that we have our features for our K-Means algorithm, let's check the head of our feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6590351f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa223538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating our elbow technique method\n",
    "def find_k(features):\n",
    "    #intializing a list to hold costs or errors\n",
    "    costs=[]\n",
    "    \n",
    "    #iterating over possible values for k\n",
    "    for k in range(1,51):\n",
    "        model=KMeans(n_clusters=k) \n",
    "        model.fit(features)\n",
    "        costs.append(sum(np.min(cdist(features,model.cluster_centers_,'euclidean'),axis=1)))\n",
    "    \n",
    "    #plotting our elbow graph    \n",
    "    with plt.style.context(['classic','ggplot']):\n",
    "        plt.figure(figsize=(10,6))\n",
    "        plt.plot(costs)\n",
    "        plt.xlabel('Clusters')\n",
    "        plt.ylabel('Errors')\n",
    "        plt.title('Finding K')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b234e7",
   "metadata": {},
   "source": [
    "Now that we have our method let's use it to find the optimal value for K. First, let's update our dataframe by making using our Symbols as our index. We'll first make a copy of our original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f9246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a copy of our original features dataframe\n",
    "features_copy=features.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ebe046",
   "metadata": {},
   "source": [
    "Now we can reindex our features dataframe by our Symbol column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624d6a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reindexing our features dataframe\n",
    "features_copy=features_copy.reindex(index=features_copy['Symbol'],columns=features_copy.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d424212",
   "metadata": {},
   "source": [
    "Now that we have reindex our features_copy dataframe, let's add back the values to our columns and drop the Symbol column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cdced6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding our data back to their respective columns\n",
    "features_copy['P/E']=features['P/E'].values\n",
    "features_copy['EPS']=features['EPS'].values\n",
    "features_copy['MarketCap']=features['MarketCap'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebcc8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating list to hold original data\n",
    "original_data={'ADBE':adbe,'AET':aet,'AIG':aig,'ANTM':antm,'AMAT':amat}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5751d126",
   "metadata": {},
   "source": [
    "We can now create our function to copy our original data and create a dataframe for our training period data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90162d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to parse out training period data\n",
    "def get_training_data(original_data,symbol_list,start,end):\n",
    "    '''\n",
    "    PARAMETERS:\n",
    "    \n",
    "    original_data - the dictionary we created that holds our dataframes\n",
    "    symbol_list - the list of symbols; data type are strings\n",
    "    start - the beginning date of our training period as a string\n",
    "    end - the ending date of our training period as a string\n",
    "    '''\n",
    "    \n",
    "    #creating a dataframe to hold our parsed series\n",
    "    training_df=pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    #iterating over our symbol list\n",
    "    for count, symbol in enumerate(symbol_list):\n",
    "        \n",
    "        try:\n",
    "        \n",
    "            #making a copy of our original data for each symbol\n",
    "            copy=original_data[symbol].copy()\n",
    "\n",
    "            #reindexing our copied data by Date column\n",
    "            copy=copy.reindex(index=copy['Date'],columns=copy.columns)\n",
    "\n",
    "            #restoring values of close column from our original data\n",
    "            copy[' Close']=original_data[symbol][' Close'].values\n",
    "\n",
    "            #parsing out our training period\n",
    "            copy=copy.loc[start:end][' Close']\n",
    "            \n",
    "            #adding training data to dataframe\n",
    "            training_df[str(symbol)]=copy.values\n",
    "        \n",
    "        except:\n",
    "            print(str(symbol),'Threw an Exception')\n",
    "            print('Current Symbol Length:')\n",
    "            print(len(copy.loc[start:end]))\n",
    "            print(\"\")\n",
    "            print('training_df Length:')\n",
    "            print(len(training_df))\n",
    "            continue\n",
    "    \n",
    "    return training_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d082fde",
   "metadata": {},
   "source": [
    "We're now ready to create our training data. We'll use our get_training_data function to parse out our training data and then use this dataframe to parse out the respective series for our pair combinations to perform our CADF test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13f9ca4",
   "metadata": {},
   "source": [
    "Okay. Let's now store our regime predictions in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec81c990",
   "metadata": {},
   "outputs": [],
   "source": [
    "antm_aet_regime_predictions=antm_aet_gmm_rf.base_portfolio_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da02b28",
   "metadata": {},
   "source": [
    "#### AET_ANTM Random Forests Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabb9ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the train_random_forests method on our antm_aet_gmm_rf object\n",
    "aet_antm_gmm_rf.train_random_forests()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a99cc8",
   "metadata": {},
   "source": [
    "We'll now store our predictions in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257a34d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "aet_antm_regime_predictions=aet_antm_gmm_rf.base_portfolio_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2088be4d",
   "metadata": {},
   "source": [
    "We can now use these regime predictions to create our updated strategies and compose our Bottom Up Portfolio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8500b820",
   "metadata": {},
   "source": [
    "### Step 5: Using the Model's Predictions in Strategy Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eef98dd",
   "metadata": {},
   "source": [
    "We'll begin by creating an updated version of our statarb class that will allow us to pass in our regime predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7d0116",
   "metadata": {},
   "outputs": [],
   "source": [
    "class statarb_update(object):\n",
    "     #np.seterr(divide='ignore',invalid='ignore')\n",
    "    \n",
    "     def __init__(self,df1, df2, ptype,ma,floor, ceiling,beta_lookback,start,end,regimePredictions,p2Objective,avoid1=0,target1=0,\n",
    "                  exit_zscore=0):\n",
    "        #setting the attributes of the data cleaning object\n",
    "        self.df1=df1 #the complete dataframe of X\n",
    "        self.df2=df2 # the comlete dataframe of Y\n",
    "        self.df=pd.DataFrame(index=df1.index) #creates a new dataframe in the create_spread method\n",
    "        self.ptype=ptype #the portfolio type 1= standard implementation 2=machine learning implementation\n",
    "        self.ma=ma# the moving average period for the model\n",
    "        self.floor=floor #the buy threshold for the z-score\n",
    "        self.ceiling=ceiling #the sell threshold for the z-score\n",
    "        self.Close='Close Long'  #used as close signal for longs\n",
    "        self.Cover='Cover Short' #used as close signal for shorts\n",
    "        self.exit_zscore=exit_zscore  #the z-score\n",
    "        self.beta_lookback=beta_lookback #the lookback of beta for hedge ratio\n",
    "        self.start=start #the beginning of test period as a string\n",
    "        self.end=end    # the end of test period as a string\n",
    "        self.regimePredictions=regimePredictions.reshape(-1,1)  #the regime predictions from GMM for p2=2 implementation\n",
    "        self.avoid1=avoid1 #the regime to avoid\n",
    "        self.target1=target1 #the regime to target\n",
    "        self.p2Objective=p2Objective #type:string;#the objective of p2 implementation; can be 'Avoid','Target',or 'None'; \n",
    "                                \n",
    "        \n",
    "     #create price spread\n",
    "     def create_spread(self):\n",
    "            if self.ptype==1:\n",
    "                #setting the new dataframe values for x and y of the closing \n",
    "                #prices of the two dataframes passed in\n",
    "                self.df['X']=self.df1[' Close']\n",
    "                self.df['Y']=self.df2[' Close']\n",
    "\n",
    "                #calculating the beta of the pairs             \n",
    "                self.ols=linregress(self.df['Y'],self.df['X'])\n",
    "\n",
    "                #setting the hedge ratio\n",
    "                self.df['Hedge Ratio']=self.ols[0]\n",
    "\n",
    "                self.df['Spread']=self.df['Y']-(self.df['Hedge Ratio']*self.df['X'])\n",
    "\n",
    "            if self.ptype==2:\n",
    "                #setting the new dataframe values for x and y of the closing \n",
    "                #prices of the two dataframes passed in\n",
    "                self.df['X']=self.df1[' Close']\n",
    "                self.df['Y']=self.df2[' Close']\n",
    "\n",
    "\n",
    "                #calculating the beta of the pairs                \n",
    "                self.ols=linregress(self.df['Y'],self.df['X'])\n",
    "\n",
    "                #setting the hedge ratio\n",
    "                self.df['Hedge Ratio']=self.ols[0]\n",
    "                #creating spread\n",
    "                self.df['Spread']=self.df['Y']-(self.df['Hedge Ratio']*self.df['X'])\n",
    "                \n",
    "                #creating the z-score\n",
    "                self.df['Z-Score']=(self.df['Spread']-self.df['Spread'].rolling(window=self.ma).mean())/self.df['Spread'].rolling(window=self.ma).std()\n",
    "                \n",
    "                \n",
    "                #Creating the features columns\n",
    "                self.df['6 X Vol']=self.df['X'].rolling(window=6).std()\n",
    "                self.df['6 Y Vol']=self.df['Y'].rolling(window=6).std()\n",
    "                self.df['6 Spread Vol']=self.df['Spread'].rolling(window=6).std()\n",
    "                self.df['6 Z-Score Vol']=self.df['Z-Score'].rolling(window=6).std()\n",
    "\n",
    "                self.df['12 X Vol']=self.df['X'].rolling(window=12).std()\n",
    "                self.df['12 Y Vol']=self.df['Y'].rolling(window=12).std()\n",
    "                self.df['12 Spread Vol']=self.df['Spread'].rolling(window=12).std()\n",
    "                self.df['12 Z-Score Vol']=self.df['Z-Score'].rolling(window=12).std()\n",
    "\n",
    "                self.df['15 X Vol']=self.df['X'].rolling(window=15).std()\n",
    "                self.df['15 Y Vol']=self.df['Y'].rolling(window=15).std()\n",
    "                self.df['15 Spread Vol']=self.df['Spread'].rolling(window=15).std()\n",
    "                self.df['15 Z-Score Vol']=self.df['Z-Score'].rolling(window=15).std()\n",
    "                #Creating the Regime Prediction Column\n",
    "                self.df['Regime']=0\n",
    "                self.df['Regime']=self.regimePredictions.astype(int)\n",
    "                \n",
    "\n",
    "                \n",
    "\n",
    "            return\n",
    "    \n",
    "            \n",
    "     def generate_signals(self):\n",
    "            if self.ptype==1:\n",
    "            \n",
    "                #creating the z-score\n",
    "                self.df['Z-Score']=(self.df['Spread']-self.df['Spread'].rolling(window=self.ma).mean())/self.df['Spread'].rolling(window=self.ma).std()\n",
    "                \n",
    "                #prior z-score\n",
    "                self.df['Prior Z-Score']=self.df['Z-Score'].shift(1)\n",
    "                \n",
    "                #Creating Buy and Sell Signals; when to be long, short, exit\n",
    "                \n",
    "                #self.df['Signal']=np.where(self.df['Z-Score']<=self.floor,'BUY', np.where(self.df['Z-Score']>=self.ceiling,'SELL','FLAT'))\n",
    "                #self.df['Prior Signal']=self.df['Signal'].shift(1)\n",
    "                \n",
    "                self.df['Longs']=(self.df['Z-Score']<=self.floor)*1.0 #buy the spread\n",
    "                self.df['Shorts']=(self.df['Z-Score']>=self.ceiling)*1.0 #short the spread\n",
    "                self.df['Exit']=(self.df['Z-Score']<=self.exit_zscore)*1.0\n",
    "                \n",
    "                #tracking positions via for loop implementation\n",
    "                self.df['Long_Market']=0.0\n",
    "                self.df['Short_Market']=0.0\n",
    "                \n",
    "                #Setting Variables to track whether or not to be long while iterating over df\n",
    "                self.long_market=0\n",
    "                self.short_market=0\n",
    "                \n",
    "                #Determining when to trade\n",
    "                for i,value in enumerate(self.df.iterrows()):\n",
    "                    #Calculate logns\n",
    "                    if value[1]['Longs']==1.0:\n",
    "                        self.long_market=1\n",
    "                        \n",
    "                    if value[1]['Shorts']==1.0:\n",
    "                        self.short_market=1\n",
    "                        \n",
    "                    if value[1]['Exit']==1.0:\n",
    "                        \n",
    "                        self.long_market=0\n",
    "                        self.short_market=0\n",
    "                        \n",
    "                    self.df.iloc[i]['Long_Market']=self.long_market\n",
    "                    self.df.iloc[i]['Short_Market']=self.short_market\n",
    "  \n",
    "                    \n",
    "            if self.ptype==2:        \n",
    "                \n",
    "                \n",
    "                \n",
    "                self.df['Longs']=(self.df['Z-Score']<=self.floor)*1.0 #buy the spread\n",
    "                self.df['Shorts']=(self.df['Z-Score']>=self.ceiling)*1.0 #short the spread\n",
    "                self.df['Exit']=(self.df['Z-Score']<=self.exit_zscore)*1.0\n",
    "                \n",
    "                #tracking positions via for loop implementation\n",
    "                self.df['Long_Market']=0.0\n",
    "                self.df['Short_Market']=0.0\n",
    "                \n",
    "                #Setting Variables to track whether or not to be long while iterating over df\n",
    "                self.long_market=0\n",
    "                self.short_market=0\n",
    "                \n",
    "                #Determining when to trade\n",
    "                for i,value in enumerate(self.df.iterrows()):\n",
    "                    if self.p2Objective=='Avoid':\n",
    "                        if value[1]['Regime']!= self.avoid1:\n",
    "                            #Calculate longs\n",
    "                            if value[1]['Longs']==1.0:\n",
    "                                self.long_market=1\n",
    "\n",
    "                            if value[1]['Shorts']==1.0:\n",
    "                                self.short_market=1\n",
    "\n",
    "                            if value[1]['Exit']==1.0:\n",
    "\n",
    "                                self.long_market=0\n",
    "                                self.short_market=0\n",
    "                                \n",
    "                        self.df.iloc[i]['Long_Market']=value[1]['Longs']#self.long_market\n",
    "                        self.df.iloc[i]['Short_Market']=value[1]['Shorts']#self.short_market\n",
    "                                \n",
    "                    elif self.p2Objective=='Target':\n",
    "                        if value[1]['Regime']==self.target1:\n",
    "                            #Calculate longs\n",
    "                            if value[1]['Longs']==1.0:\n",
    "                                self.long_market=1\n",
    "\n",
    "                            if value[1]['Shorts']==1.0:\n",
    "                                self.short_market=1\n",
    "\n",
    "                            if value[1]['Exit']==1.0:\n",
    "\n",
    "                                self.long_market=0\n",
    "                                self.short_market=0\n",
    "                        \n",
    "                        self.df.iloc[i]['Long_Market']=value[1]['Longs']#self.long_market\n",
    "                        self.df.iloc[i]['Short_Market']=value[1]['Shorts']#self.short_market\n",
    "                                \n",
    "                    elif self.p2Objective=='None':\n",
    "                        \n",
    "                        #Calculate longs\n",
    "                        if value[1]['Longs']==1.0:\n",
    "                            self.long_market=1\n",
    "                        #Calculate Shorts\n",
    "                        if value[1]['Shorts']==1.0:\n",
    "                            self.short_market=1\n",
    "\n",
    "                        if value[1]['Exit']==1.0:\n",
    "\n",
    "                            self.long_market=0\n",
    "                            self.short_market=0\n",
    "\n",
    "                        self.df.iloc[i]['Long_Market']=value[1]['Longs']#self.long_market\n",
    "                        self.df.iloc[i]['Short_Market']=value[1]['Shorts']#self.short_market\n",
    "  \n",
    "                \n",
    "                \n",
    "               \n",
    "            return self.df                    \n",
    "        \n",
    "     def create_returns(self, allocation,pair_number):\n",
    "        if self.ptype==1:         \n",
    "            '''\n",
    "            PARAMETERS\n",
    "            ##########\n",
    "            allocation - the amount of capital alotted for pair\n",
    "            pair_number - string to annotate the plots\n",
    "\n",
    "            '''\n",
    "            self.allocation=allocation \n",
    "            self.pair=pair_number\n",
    "\n",
    "            self.portfolio=pd.DataFrame(index=self.df.index)\n",
    "            self.portfolio['Positions']=self.df['Long_Market']-self.df['Short_Market']\n",
    "            self.portfolio['X']=-1.0*self.df['X']*self.portfolio['Positions']\n",
    "            self.portfolio['Y']=self.df['Y']*self.portfolio['Positions']\n",
    "            self.portfolio['Total']=self.portfolio['X']+self.portfolio['Y']\n",
    "\n",
    "\n",
    "            #creating a percentage return stream\n",
    "            self.portfolio['Returns']=self.portfolio['Total'].pct_change()\n",
    "            self.portfolio['Returns'].fillna(0.0,inplace=True)\n",
    "            self.portfolio['Returns'].replace([np.inf,-np.inf],0.0,inplace=True)\n",
    "            self.portfolio['Returns'].replace(-1.0,0.0,inplace=True)\n",
    "\n",
    "\n",
    "            #calculating metrics\n",
    "            self.mu=(self.portfolio['Returns'].mean())\n",
    "            self.sigma=(self.portfolio['Returns'].std())\n",
    "            self.portfolio['Win']=np.where(self.portfolio['Returns']>0,1,0)\n",
    "            self.portfolio['Loss']=np.where(self.portfolio['Returns']<0,1,0)\n",
    "            self.wins=self.portfolio['Win'].sum()\n",
    "            self.losses=self.portfolio['Loss'].sum()\n",
    "            self.total_trades=self.wins+self.losses\n",
    "            #calculating sharpe ratio with interest rate of \n",
    "            #interest_rate_assumption=0.75\n",
    "            #self.sharp=(self.mu-interest_rate_assumption)/self.sigma\n",
    "\n",
    "            #win loss ratio; \n",
    "            self.win_loss_ratio=(self.wins/self.losses)\n",
    "\n",
    "            #probability of win\n",
    "            self.prob_of_win=(self.wins/self.total_trades)\n",
    "            #probability of loss\n",
    "            self.prob_of_loss=(self.losses/self.total_trades)\n",
    "\n",
    "            #average return of wins\n",
    "            self.avg_win_return=(self.portfolio['Returns']>0).mean()\n",
    "            #average returns of losses\n",
    "            self.avg_loss_return=(self.portfolio['Returns']<0).mean()\n",
    "            #calculating payout ratio\n",
    "            self.payout_ratio=(self.avg_win_return/self.avg_loss_return)\n",
    "\n",
    "            #calculate equity curve\n",
    "            self.portfolio['Returns']=(self.portfolio['Returns']+1.0).cumprod()\n",
    "            self.portfolio['Trade Returns']=(self.portfolio['Total'].pct_change()) #non cumulative Returns\n",
    "            self.portfolio['Portfolio Value']=(self.allocation*self.portfolio['Returns'])\n",
    "            self.portfolio['Portfolio Returns']=self.portfolio['Portfolio Value'].pct_change()\n",
    "            self.portfolio['Initial Value']=self.allocation\n",
    "\n",
    "            with plt.style.context(['ggplot','seaborn-paper']):\n",
    "                #Plotting Portfolio Value   \n",
    "                plt.plot(self.portfolio['Portfolio Value'])\n",
    "                plt.plot(self.portfolio['Initial Value'])\n",
    "                plt.title('%s Strategy Returns '%(self.pair))\n",
    "                plt.legend(loc=0)\n",
    "                plt.show()\n",
    "\n",
    "    \n",
    "                \n",
    "        \n",
    "        if self.ptype==2:\n",
    "            '''\n",
    "            PARAMETERS\n",
    "            ##########\n",
    "            allocation - the amount of capital alotted for pair\n",
    "            pair_number - string to annotate the plots\n",
    "\n",
    "            '''\n",
    "            self.allocation=allocation \n",
    "            self.pair=pair_number\n",
    "\n",
    "            self.portfolio=pd.DataFrame(index=self.df.index)\n",
    "            self.portfolio['Positions']=self.df['Longs']-self.df['Shorts']\n",
    "            self.portfolio['X']=-1.0*self.df['X']*self.portfolio['Positions']\n",
    "            self.portfolio['Y']=self.df['Y']*self.portfolio['Positions']\n",
    "            self.portfolio['Total']=self.portfolio['X']+self.portfolio['Y']\n",
    "            \n",
    "            self.portfolio.fillna(0.0,inplace=True)\n",
    "\n",
    "\n",
    "            #creating a percentage return stream\n",
    "            self.portfolio['Returns']=self.portfolio['Total'].pct_change()\n",
    "            self.portfolio['Returns'].fillna(0.0,inplace=True)\n",
    "            self.portfolio['Returns'].replace([np.inf,-np.inf],0.0,inplace=True)\n",
    "            self.portfolio['Returns'].replace(-1.0,0.0,inplace=True)\n",
    "\n",
    "\n",
    "            #calculating metrics\n",
    "            self.mu=(self.portfolio['Returns'].mean())\n",
    "            self.sigma=(self.portfolio['Returns'].std())\n",
    "            self.portfolio['Win']=np.where(self.portfolio['Returns']>0,1,0)\n",
    "            self.portfolio['Loss']=np.where(self.portfolio['Returns']<0,1,0)\n",
    "            self.wins=self.portfolio['Win'].sum()\n",
    "            self.losses=self.portfolio['Loss'].sum()\n",
    "            self.total_trades=self.wins+self.losses\n",
    "            #calculating sharpe ratio with interest rate of \n",
    "            #interest_rate_assumption=0.75\n",
    "            #self.sharp=(self.mu-interest_rate_assumption)/self.sigma\n",
    "\n",
    "            #win loss ratio; \n",
    "            self.win_loss_ratio=(self.wins/self.losses)\n",
    "\n",
    "            #probability of win\n",
    "            self.prob_of_win=(self.wins/self.total_trades)\n",
    "            #probability of loss\n",
    "            self.prob_of_loss=(self.losses/self.total_trades)\n",
    "\n",
    "            #average return of wins\n",
    "            self.avg_win_return=(self.portfolio['Returns']>0).mean()\n",
    "            #average returns of losses\n",
    "            self.avg_loss_return=(self.portfolio['Returns']<0).mean()\n",
    "            #calculating payout ratio\n",
    "            self.payout_ratio=(self.avg_win_return/self.avg_loss_return)\n",
    "\n",
    "            #calculate equity curve\n",
    "            self.portfolio['Returns']=(self.portfolio['Returns']+1.0).cumprod()\n",
    "            self.portfolio['Trade Returns']=(self.portfolio['Total'].pct_change()) #non cumulative Returns\n",
    "            self.portfolio['Portfolio Value']=(self.allocation*self.portfolio['Returns'])\n",
    "            self.portfolio['Portfolio Returns']=self.portfolio['Portfolio Value'].pct_change()\n",
    "            self.portfolio['Initial Value']=self.allocation\n",
    "\n",
    "            with plt.style.context(['ggplot','seaborn-paper']):\n",
    "                #Plotting Portfolio Value   \n",
    "                plt.plot(self.portfolio['Portfolio Value'])\n",
    "                plt.plot(self.portfolio['Initial Value'])\n",
    "                plt.title('%s Strategy Returns '%(self.pair))\n",
    "                plt.legend(loc=0)\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "        return #self.portfolio['Portfolio Value'].plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14da9ab",
   "metadata": {},
   "source": [
    "Let's now use our updated statarb class to create our strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca65f2b",
   "metadata": {},
   "source": [
    "#### ADBE_ANTM Bottom Up Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0502a69c",
   "metadata": {},
   "source": [
    "These feature will be used within our Ranfom Forests to predict the regimes identified by our Gaussian Mixture Model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8efbe20",
   "metadata": {},
   "source": [
    "### Step 2: Creating Strategies Over Historical Period"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49895f8d",
   "metadata": {},
   "source": [
    "We'll begin with our first pair, ADBE_ANTM. Before we begin our implementation we need to split our historical period. Recall that we imported intraday data from 1/4/18 to 6/12/18. Our assessment period, or the period in which we are comparing our Sharpe ratios is from 5/1/18 to 6/12/18. In our first two portfolios, we simply applied our strategies to our assessment period. However in our later two portfolios we need to first apply it to our historical period to train our models then apply it to our assessment period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5289c2",
   "metadata": {},
   "source": [
    "To assess our models before applying them to our true testing or assessment period we need to split our historical period into a historical training and testing set. This will enable us to train our models on part of our total training data and then test it on our historical testing data. We can then apply them to our overall testing or assessment period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7e2967",
   "metadata": {},
   "source": [
    "For simplicity, we'll manually calculate an 80/20 train_test_split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9387ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking length of training_df\n",
    "len(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb0223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing 80% of training_df length\n",
    "len(training_df)*.80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cbf392",
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing 20% of training_df length\n",
    "len(training_df)*-.20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c445ce5f",
   "metadata": {},
   "source": [
    "Okay now that we now the index values that represent the 80% and 20% marks of our training_df dataframe, we can use them to slice our data. The way that we can do this is for our 80%, or the first 4992 rows we can simply use the notation [0:4992]. For our testing period over our historical training period we can parse the last 20% by using the notation [-1248]. These notations will be used to split the returns we generate over our entire historical period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e9a856",
   "metadata": {},
   "source": [
    "Let's create our ADBE_ANTM implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a60a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "adbe_antm_3_historical=statarb(training_df['ADBE'],training_df['ANTM'],17,-2,2,17,adbe_test.iloc[0],adbe_test.iloc[-1])\n",
    "adbe_antm_3_historical.create_spread()\n",
    "adbe_antm_3_historical.generate_signals()\n",
    "\n",
    "#notice that we are equally weighing our strategy\n",
    "adbe_antm_3_historical.create_returns(30000,'ADBE_ANTM_3 Over Training Period')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3171eec",
   "metadata": {},
   "source": [
    "Okay now that we've created our strategy, let's check our returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4e79b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb96d016",
   "metadata": {},
   "source": [
    "Now we'll write our method to create our cointegrated pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a309423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cointegrated(all_pairs,training_df):\n",
    "    '''\n",
    "    PARAMETERS\n",
    "    #########\n",
    "    \n",
    "    all_pairs - the list of all possible pair combinations from Cluster 0\n",
    "    training_df - our dataframe holding our stock data for stocks in Cluster 0 over the training period \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #creating a list to hold cointegrated pairs\n",
    "    cointegrated=[]\n",
    "    \n",
    "        \n",
    "    #iterate over each pair in possible pairs list; pair is a list of our 2 stock symbols\n",
    "    for count, pair in enumerate(all_pairs):\n",
    "        try:\n",
    "            \n",
    "            #getting data for each stock in pair from training_df\n",
    "            ols=linregress(training_df[str(pair[1])],training_df[str(pair[0])]) #note scipy's linregress takes in Y then X\n",
    "                        \n",
    "            #storing slope or hedge ratio in variable\n",
    "            slope=ols[0]\n",
    "            \n",
    "            \n",
    "            #creating spread \n",
    "            spread=training_df[str(pair[1])]-(slope*training_df[str(pair[0])])\n",
    "                       \n",
    "            #testing spread for cointegration\n",
    "            cadf=adfuller(spread,1)\n",
    "            \n",
    "            #checking to see if spread is cointegrated, if so then store pair in cointegrated list\n",
    "            if cadf[0] < cadf[4]['1%']:\n",
    "                print('Pair Cointegrated at 99% Confidence Interval')\n",
    "                #appending the X and Y of pair\n",
    "                cointegrated.append([pair[0],pair[1]])\n",
    "            elif cadf[0] < cadf[4]['5%']:\n",
    "                print('Pair Cointegrated at 95% Confidence Interval')\n",
    "                #appending the X and Y of pair\n",
    "                cointegrated.append([pair[0],pair[1]])\n",
    "            elif cadf[0] < cadf[4]['10%']:\n",
    "                print('Pair Cointegrated at 90% Confidence Interval')\n",
    "                cointegrated.append(pair[0],pair[1])\n",
    "            else:\n",
    "                print('Pair Not Cointegrated ')\n",
    "                continue\n",
    "        except:\n",
    "            print('Exception: Symbol not in Dataframe')\n",
    "            continue\n",
    "        \n",
    "    return cointegrated\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a2ecee",
   "metadata": {},
   "source": [
    "Let's initialize our get_cointegrated function and find our cointegrated pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129c8310",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting our cointegrated pairs\n",
    "cointegrated_from_cluster_0=get_cointegrated(all_pairs,training_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6900cb",
   "metadata": {},
   "source": [
    "Let's check our cointegrated list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57c6bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cointegrated_from_cluster_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b70ed4b",
   "metadata": {},
   "source": [
    "Okay. We've found that 3 of our possible 20 pairs are cointegrated each at the 90% confidence interval. Recall that we called the head method on our clusters_df dataframe of which returns only the first five rows. Let's use the Counter method to get an idea of how our symbols are distributed across all our clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076bd538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the Counter method\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb7ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling Counter method on our clusters\n",
    "Counter(clusters_df['Cluster'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8c0ee6",
   "metadata": {},
   "source": [
    "The reason Random Forests chooses a fraction of the feature space at each split is because if there is a very strong feature in the space, it is likely to be chosen as the root node of every tree. This would mean that each tree would likely be closely correlated and thus defeats the purpose of reducing variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a8ad4c",
   "metadata": {},
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f317216",
   "metadata": {},
   "source": [
    "We need to construct a portfolio of statistical arbitrage strategies in the most efficient manner possible. We will be trading on an intraday basis in the U.S. equities market, namely across stocks within the S&P 500. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ea0b58",
   "metadata": {},
   "source": [
    "We will design multiple portfolios that use various methods of addressing the portfolio optimization problem. To assess our efforts we will create the Sharpe Ratio of each portfolio and make relative comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df6a078",
   "metadata": {},
   "source": [
    "Each portfolio will be composed of the same relationships to ensure an apples to apples comparision. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382271e1",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3997c8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rechecking our data\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc694d7e",
   "metadata": {},
   "source": [
    "Recall, an early consideration for implementing K-Means Clustering is determing the value of K that should be used. We'll use the elbow technique here to determine what value of K we should use. This technique will compare our value for K with the respective error. Our goal is to choose the value for K that minimizes the error or cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96390ac0",
   "metadata": {},
   "source": [
    "Once we trained our Random Forests we were then able to pass in our features from over the 05/01/18 to 06/12/18 period and use it to predict the regimes. To test our accuracy we compared these results to the actual regimes created by our Gaussian Mixture Model over the 05/01/18 to 06/12/18 period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbeffba2",
   "metadata": {},
   "source": [
    "Our results showed that our Random Forests had perfect precision. This is because that over our hypothetical live trading period, our observations came from the same distribution, of which can be seen from our classification report above. We can now create a variable to store our regime predictions from our hypothetical live trading period (i.e. 05/01/18 to 06/12/18) and feed these into an updated statarb implementation and see how our results would change. We will tell our updated statarb strategy which regimes to avoid using the chart we created earlier showing our returns and volatility levels per regime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d11002",
   "metadata": {},
   "source": [
    "Not that we've gotten a better understanding of our process, in future implementations we'll comment out the confusion matrix and classication reports so that we won't actually know which regimes our data fell into over our hypothetical live trading period. We'll solely rely upon our analysis of the volatiltiy and returns characteristics of our regimes and feed this information into our strategy implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f13ba1b",
   "metadata": {},
   "source": [
    "Let's store our regime preditions for our ADBE_ANTM pair into a varible that we can use for our updated  StatArb implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4e2713",
   "metadata": {},
   "outputs": [],
   "source": [
    "adbe_antm_regime_predictions=adbe_antm_gmm_rf.base_portfolio_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac8bf62",
   "metadata": {},
   "source": [
    "#### ANTM_AET Random Forests Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9b87cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the train_random_forests method on our antm_aet_gmm_rf object\n",
    "antm_aet_gmm_rf.train_random_forests()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66562a66",
   "metadata": {},
   "source": [
    "Let's create our second instance of the ADBE_ANTM pair. We'll create a total_allocation variable that we can use to compute our allocation to our strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fe1d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#total allocation variable\n",
    "total_allocation=90000 #100k less 10k cash\n",
    "\n",
    "#ADBE_ANTM Allocation\n",
    "adbe_antm_allocation=round(total_allocation*optimal_weights[0][0],2)\n",
    "\n",
    "#AET_ANTM Allocation\n",
    "aet_antm_allocation=round(total_allocation*optimal_weights[0][2],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e077db3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating 2nd instance of first pair\n",
    "adbe_antm_2=statarb(adbe_test_price_series,antm_test_price_series,17,-2,2,17,adbe_test.iloc[0],adbe_test.iloc[-1])\n",
    "adbe_antm_2.create_spread()\n",
    "adbe_antm_2.generate_signals()\n",
    "\n",
    "#notice here we're using our updated allocation\n",
    "adbe_antm_2.create_returns(adbe_antm_allocation,'ADBE_ANTM_Portfolio _2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dd777e",
   "metadata": {},
   "source": [
    "Recall that our second pair recieved a 0% allocation. We will thus create our 3rd pair with the updated weight and create our new portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2b96ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AET_ANTM 2nd implementation\n",
    "aet_antm_2=statarb(aet_test_price_series,antm_test_price_series,12,-2,2,12,aet_test.iloc[0],aet_test.iloc[-1])\n",
    "aet_antm_2.create_spread()\n",
    "aet_antm_2.generate_signals()\n",
    "aet_antm_2.create_returns(aet_antm_allocation,'AET & ANTM')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592d23a0",
   "metadata": {},
   "source": [
    "We are now ready to compose our Efficient Frontier Portfolio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4161e7",
   "metadata": {},
   "source": [
    "### Creating the Efficient Frontier Portfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a75c6e",
   "metadata": {},
   "source": [
    "Let's create our Efficient Frontier Portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d41bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataframe for Efficient Frontier Portfolio\n",
    "efficient_frontier_portfolio=pd.DataFrame()\n",
    "efficient_frontier_portfolio['ADBE_ANTM']=adbe_antm_2.portfolio['Portfolio Value']\n",
    "efficient_frontier_portfolio['AET_ANTM']=aet_antm_2.portfolio['Portfolio Value']\n",
    "efficient_frontier_portfolio['Cash']=10000\n",
    "efficient_frontier_portfolio['Total Portfolio Value']=efficient_frontier_portfolio['ADBE_ANTM']+efficient_frontier_portfolio['AET_ANTM']+efficient_frontier_portfolio['Cash']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a041e0",
   "metadata": {},
   "source": [
    "We can now add our returns column to our Efficient Frontier Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ad052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding returns column to Efficient Frontier Dataframe\n",
    "efficient_frontier_portfolio['Returns']=np.log(efficient_frontier_portfolio['Total Portfolio Value']/efficient_frontier_portfolio['Total Portfolio Value'].shift(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b69bd7b",
   "metadata": {},
   "source": [
    "We can now check our Efficient Frontier Portfolio Dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01a54b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeating process for ANTM and AET\n",
    "antm_aet=statarb(antm_test_price_series,aet_test_price_series,6,-2,2,6,antm_test.iloc[0],antm_test.iloc[-1])\n",
    "antm_aet.create_spread()\n",
    "antm_aet.generate_signals()\n",
    "antm_aet.create_returns(30000,'ANTM & AET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea0cce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeating process for AET and ANTM\n",
    "aet_antm=statarb(aet_test_price_series,antm_test_price_series,12,-2,2,12,aet_test.iloc[0],aet_test.iloc[-1])\n",
    "aet_antm.create_spread()\n",
    "aet_antm.generate_signals()\n",
    "aet_antm.create_returns(30000,'AET & ANTM')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38323a05",
   "metadata": {},
   "source": [
    "Okay. Not that we have create our individual StatArb implementations, we can combine these into a portfolio, calculate our portfolio returns, mu, sigma, and Sharpe ratio. Note, to compute the Sharpe we will need to make an assumption about the level of interest rates. In as of December 2017, the fed funds rate was 1.5%. We'll use this as our interest rate assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf211ea1",
   "metadata": {},
   "source": [
    "Recall that we started with a portfolio value of 100k USD. We allocated 10k USD to cash and equally weighted our StatArb strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebbf8c5",
   "metadata": {},
   "source": [
    "### Creating the Equally Weighted Portfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ef9285",
   "metadata": {},
   "source": [
    "We included a portfolio value variable in our statarb class. We'll use this to create our total equally weighted portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cb5658",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataframe for equally weighted portfolio\n",
    "equally_weighted=pd.DataFrame()\n",
    "equally_weighted['ADBE_ANTM']=adbe_antm.portfolio['Portfolio Value']\n",
    "equally_weighted['ANTM_AET']=antm_aet.portfolio['Portfolio Value']\n",
    "equally_weighted['AET_ANTM']=aet_antm.portfolio['Portfolio Value']\n",
    "equally_weighted['Cash']=10000\n",
    "equally_weighted['Total Portfolio Value']=equally_weighted['ADBE_ANTM']+equally_weighted['ANTM_AET']+equally_weighted['AET_ANTM']+equally_weighted['Cash']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea88633f",
   "metadata": {},
   "source": [
    "Let's check our equally_weighted portfolio dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e17c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "equally_weighted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7b0f81",
   "metadata": {},
   "source": [
    "We can now add our returns column and then use it to compute our mu,sigma and Sharpe ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7a0a92",
   "metadata": {},
   "source": [
    "We are now ready to engineer our features. In order to predict what regime our strategy is in based on its historical performance we will need features or explanatory variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74df96f5",
   "metadata": {},
   "source": [
    "Market-Microstructure encompassess a vast array of components including liquidity, volatility, market depth, etc. For this illustration we will use volatility. We will track the volatility of each of our underlying components, our spread, and our z-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ff4a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating instance of updated statarb strategy\n",
    "adbe_antm_bottom_up=statarb_update(adbe_test, antm_test, 2,17,-2, 2,17,'05/01/18','06/12/18',adbe_antm_regime_predictions,'Target',avoid1=0,target1=1,\n",
    "                  exit_zscore=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fba365",
   "metadata": {},
   "source": [
    "In the above code block, we pass in our standard parameters with the exception of new fields, namely a 'Target', target1, avoid1, and regime predictions. The Target parameter is our selection for our p2objective. It simply means that we are seeking to target a specific regime. If we set this parameter to 'Avoid', we would simply initialize the avoid1 parameter as the regime we wanted to avoid. Given that we set this parameter to 'Target', we simply initialize the target 1 parameter to the regime we would like to target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32085aeb",
   "metadata": {},
   "source": [
    "Recall that we identified our historical regimes and trained our Random Forests to predict these regimes using the features we engineered. We later applied our Random Forests model to our testing period and stored the regime predictions in a variable. We now pass those predictions in here and use them to update our signal generator based on our analysis of our historical regime. Let's finish this implementation bny calling our remaining methods on our object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23867dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting Equally Weighted Equity Curve\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(equally_weighted['Total Portfolio Value'])\n",
    "plt.title('Equally Weighted Portfolio Equity Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366cf2a7",
   "metadata": {},
   "source": [
    "We can see that even though our equally weighted portfolio value increased, it still had a negative Sharpe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dda1da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking beginning of test period\n",
    "adbe_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cde0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the end of test period\n",
    "adbe_test.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a020dc98",
   "metadata": {},
   "source": [
    "We can now parse out our closing values from our test period dataframes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fc0ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Closing price series for data\n",
    "adbe_test_price_series=np.array(adbe_test[' Close'])\n",
    "aet_test_price_series=np.array(aet_test[' Close'])\n",
    "antm_test_price_series=np.array(antm_test[' Close'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54369fc3",
   "metadata": {},
   "source": [
    "Let's check one of our price series. We'll use AET."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7343ee9a",
   "metadata": {},
   "source": [
    "An important thing to keep mind is that our GMM and Random Forests models are trained using our historical data. So when we provide the models with data from our Efficient Frontier and Equally Weighted Portfolios, it's data that our models have not seen. This was the purpose of separating our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b048af53",
   "metadata": {},
   "source": [
    "#### ADBE_ANTM GMM Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eadd8e",
   "metadata": {},
   "source": [
    "We'll begin with our first pair. Let's initialize our method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d746d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing GMM Random Forests method\n",
    "#passing in the 1)returns for 80% of the training period(i.e.01/04/18-04/30/18),2)returns for 20% of the training period(i.e.01/04/18-04/30/18)\n",
    "#3) Returns for actual or overall test period; (i.e. 05/01/18-06/12/18) for Equally Weighted, 4) count for GMMs, \n",
    "#5)Dataframe for Equally Weighted, 6)\n",
    "adbe_antm_gmm_rf=gmm_randomForests(adbe_antm_3_rets_train,adbe_antm_3_rets_test,adbe_antm.portfolio['Returns'],5,\n",
    "                                 adbe_antm_3_historical.df, adbe_antm.df,1871,-468)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ec0df0",
   "metadata": {},
   "source": [
    "Now that we have initiated our object, let's use it to get the regimes of our adbe_antm pair. Note these are based on our historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a7095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling our analyze historical regimes method\n",
    "adbe_antm_gmm_rf.analyze_historical_regimes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9efe6f",
   "metadata": {},
   "source": [
    "We can now plot our regimes using our historical_regime_returns_volatility method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996a1978",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note this method takes in a string to title our plot\n",
    "adbe_antm_gmm_rf.historical_regime_returns_volatility('ADBE_ANTM GMM Analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0401106e",
   "metadata": {},
   "source": [
    "From looking at the above plot, we can see that the regime most favorable for our strategy is Regime 1. This regime has the lowest relative volatility but also yields the highest return. We will make a note of this and incorporate this into our updated statarb implemention. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4992f63e",
   "metadata": {},
   "source": [
    "One thing to note is that we created these regimes over our historical or training period. Recall we performed an 80/20 split on our data. This means that any one of these regimes could be the actual regime in which our observations fall in over the period 05/01/18 to 06/21/18. In practice the idea would be to look at a historical period(i.e. trailing 3 months) and apply the analysis to the current one month. We would update and perform this analysis at the end of each month."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb66357",
   "metadata": {},
   "source": [
    "Let's do the same for the remainder of our pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564b42de",
   "metadata": {},
   "source": [
    "#### ANTM_AET GMM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010dbf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding returns column\n",
    "equally_weighted['Returns']=np.log(equally_weighted['Total Portfolio Value']/equally_weighted['Total Portfolio Value'].shift(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13853bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rechecking our dataframe\n",
    "equally_weighted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b874543",
   "metadata": {},
   "source": [
    "We'll now get our Mu, Sigma, and Sharpe and store them in variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90abe628",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing Equally_Weighted portfolio metrics\n",
    "equally_weighted_mu=equally_weighted['Returns'].mean()\n",
    "equally_weighted_sigma=equally_weighted['Returns'].std()\n",
    "\n",
    "#initializing interest rate assumption of 1.5%\n",
    "rate=0.015\n",
    "\n",
    "#computing Sharpe\n",
    "equally_weighted_Sharpe=round((equally_weighted_mu-rate)/equally_weighted_sigma,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a04b28",
   "metadata": {},
   "source": [
    "Okay, let's check the Sharpe Ratio of our Equally Weighted Portfolio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8a3734",
   "metadata": {},
   "source": [
    "### Step 3: Using Gaussian Mixture Model to  Identify Historical Regimes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe25613",
   "metadata": {},
   "source": [
    "We now have the returns of our StatArb strategies over our historical period. Our next task is to identify our historical regimes. Our goal is to identify the regimes in which our stratetgies have not performed well on a risk to reward basis and avoid these periods. To achieve this, we must 1) identify our historical regimes and 2) engineer features that can be used to predict these regimes. We can then augment our signal generator and thus optimize our portfolio from the microstructure level up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3986ab",
   "metadata": {},
   "source": [
    "We'll begin by splitting our returns data into an 80/20 train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ffafb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting historical returns data\n",
    "#ADBE_ANTM\n",
    "#getting length of returns data\n",
    "adbe_antm_3_rets_len=len(adbe_antm_3_historical_rets)\n",
    "adbe_antm_3_rets_train=adbe_antm_3_historical_rets[0:4992]\n",
    "adbe_antm_3_rets_test=adbe_antm_3_historical_rets[-1248:]\n",
    "\n",
    "#ANTM_AET\n",
    "#getting length of returns data\n",
    "antm_aet_3_rets_len=len(antm_aet_3_historical_rets)\n",
    "antm_aet_3_rets_train=antm_aet_3_historical_rets[0:4992]\n",
    "antm_aet_3_rets_test=antm_aet_3_historical_rets[-1248:]\n",
    "\n",
    "#AET_ANTM\n",
    "#getting length of returns data\n",
    "aet_antm_3_rets_len=len(aet_antm_3_historical_rets)\n",
    "aet_antm_3_rets_train=aet_antm_3_historical_rets[0:4992]\n",
    "aet_antm_3_rets_test=aet_antm_3_historical_rets[-1248:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee8b47f",
   "metadata": {},
   "source": [
    "Now that we have created our train and test splits we're ready to identify our regimes. We'll start by training our GMM on our training period data, then we'll apply it to our testing period data. We'll create method that we can use for our GMM and Random Forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d1f78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class gmm_randomForests(object):\n",
    "    def __init__(self,historical_rets_train,historical_rets_test,base_portfolio_rets,gmm_components,df,base_portfolio_df,\n",
    "                 internal_test_start,internal_test_end):\n",
    "\n",
    "            '''\n",
    "            PARAMETERS\n",
    "            ##########\n",
    "            The first 3 parameters should have already been sliced from the\n",
    "            entire sample; ie specific dates parsed from dataframe that contains\n",
    "            trade history, returns, and features; other features will be added\n",
    "            later in development within the object\n",
    "            \n",
    "            historical_rets_train- the returns data over the historical period used to train on\n",
    "                                this is the internal train of the internal 80/20 split;the 80%\n",
    "                                of the total historical training set\n",
    "                                \n",
    "            historical_rets_test- retuns data for 20% of internal training test set\n",
    "            \n",
    "            base_portfolio_rets - this is our figurative live data;ie returns; 5/01/18-6/12/18 from either\n",
    "                                Equally Weighted or Efficient Frontier Portfolios dependent upon implementation\n",
    "                                of Bottom Up or SPO Framework\n",
    "            \n",
    "            ex. data over 01/04/18-04/30/18\n",
    "            we would first split this 80/20\n",
    "            the 80% is our training set\n",
    "            the 20% is our testing set\n",
    "            \n",
    "            we would then do another split on our training set created above\n",
    "            this is so that if we can better understand the historical\n",
    "            regimes and recalibrate our models if necessary before actually\n",
    "            predicting our 5/1/18-6/12/18 testing set\n",
    "            \n",
    "            in this ex. our gmm_training_train is 80% of the period 01/04/18-4/30/18\n",
    "            our gmm_training_test is 20% of the period 01/04/18-4/30/18 and our \n",
    "            gmm_test_actual is 05/01/18-6/12/18\n",
    "            \n",
    "            gmm_components - type:int; for number of components for GMM\n",
    "            \n",
    "            df - the entire dataframe containing prior trading history; the dataframe from either Equally Weighted or \n",
    "                 Efficient Frontier Portfolios; Our Random Forests Implementation will take this dataframe created by\n",
    "                 our statarb class(i.e. from the prior portfolios) and add our features to it. It will then use these \n",
    "                 features to predict the regimes of our test period. Recall that our Equally Weighted and Efficient \n",
    "                 Frontier Portfolios were constructed over our assessment period of 5/1/18 to 6/12/18. We will then\n",
    "                 be able to store our predictions in a varible for our test period. These predictions will be passed \n",
    "                 into a new statarb object as a parameter and be used to create the Bottom Up and SPO Framework Portfolios.\n",
    "            \n",
    "            total_training_start- type: string; the beginning of the historical period for analysis; input as string\n",
    "                                    from example above would be 2012\n",
    "            \n",
    "            total_training_end - type:string; the end of the historical period for analysis; input as string;\n",
    "                                    from example above would be 2017\n",
    "                                    \n",
    "            base_portfolio_df - (i.e.adbe_antm.df,etc)Note: for the Bottom Up Implementation this df would be the Equally Weighted df but for the \n",
    "                            SPO Framework df this would be the df from the Efficient Frontier implementation\n",
    "            \n",
    "            internal_test_start- type:int; this is the testing period for the total training period; in the example,\n",
    "                                this is the assessment period of 05/01/18-06/12/18; start thus is len(strategy_object)*.80\n",
    "                                \n",
    "                                \n",
    "            internal_test_end - type:int; this is the end of the assessment period; ie. the 20% testing split\n",
    "                                of the broader split...this value is -len(strategy_object)*.20...Note this value is \n",
    "                                is negative because we want the last 20% of the data\n",
    "                                                    \n",
    "                                The last two methods are created as such because our Equally Weighted and Efficient Frontier \n",
    "                                Portfolios are not indexed by dates but are indexed by a sequence of ints. This means \n",
    "                                that in a later method(i.e. Random Forests) we have to pass in the int value of the index\n",
    "                                position we want to parse\n",
    "            '''\n",
    "            self.historical_rets_train=historical_rets_train\n",
    "            self.historical_rets_test=historical_rets_test\n",
    "            self.base_portfolio_rets=base_portfolio_rets\n",
    "            self.gmm_components=gmm_components\n",
    "            self.max_iter=300\n",
    "            self.random_state=101\n",
    "            self.df=df\n",
    "            #self.total_training_start=total_training_start\n",
    "            #self.total_training_end=total_training_end\n",
    "            self.base_portfolio_df=base_portfolio_df\n",
    "            self.internal_test_start=internal_test_start\n",
    "            self.internal_test_end=internal_test_end\n",
    "            #creating volatility\n",
    "            self.volatility=self.historical_rets_train.rolling(window=5).std()\n",
    "            self.negative_volatility=np.where(self.historical_rets_train<0,self.historical_rets_train.rolling(window=5).std(),0)\n",
    "        \n",
    "            \n",
    "    def make_gmm(self):\n",
    "        model_kwds=dict(n_components=self.gmm_components,max_iter=self.max_iter,n_init=100,random_state=self.random_state)\n",
    "        \n",
    "        gmm=GM(**model_kwds)\n",
    "        \n",
    "        return gmm\n",
    "    \n",
    "    def analyze_historical_regimes(self):\n",
    "        #Creating a Gaussian Mixture Model\n",
    "        self.gmm=self.make_gmm()\n",
    "        \n",
    "        #instantiating the XTrain as the gmm_training_train; (i.e. the 80% of total training period)\n",
    "        self.gmm_XTrain=np.array(self.historical_rets_train).reshape(-1,1)\n",
    "        \n",
    "        #Fitting the GMM on the Training Set(note this is the internal training set within the broader training set)\n",
    "        self.gmm.fit(self.gmm_XTrain.astype(int))\n",
    "        \n",
    "        #Making predictions on the historical period; ie. the gmm_training_train\n",
    "        self.gmm_historical_predictions=self.gmm.predict(self.gmm_XTrain.astype(int))\n",
    "        \n",
    "        #Making Predictions on the gmm_training_test (i.e. the 20% of total training period;)\n",
    "        self.gmm_XTest=np.array(self.historical_rets_test).reshape(-1,1)\n",
    "        self.gmm_training_test_predictions=self.gmm.predict(self.gmm_XTest.astype(int))\n",
    "        \n",
    "        #Fitting the Model on ACTUAL data we want to Predict Regimes For\n",
    "        self.gmm_Actual=np.array(self.base_portfolio_rets).reshape(-1,1)\n",
    "        self.base_portfolio_predictions=self.gmm.predict(self.gmm_Actual)\n",
    "        \n",
    "        \n",
    "        return\n",
    "\n",
    "    def historical_regime_returns_volatility(self,plotTitle):\n",
    "        self.plotTitle=plotTitle\n",
    "        data=pd.DataFrame({'Volatility':self.volatility,'Regime':self.gmm_historical_predictions,'Returns':self.historical_rets_train})\n",
    "        \n",
    "        with plt.style.context(['classic','seaborn-paper']):\n",
    "            fig,ax=plt.subplots(figsize=(15,10),nrows=1, ncols=2)\n",
    "            \n",
    "            left   =  0.125  # the left side of the subplots of the figure\n",
    "            right  =  0.9    # the right side of the subplots of the figure\n",
    "            bottom =  .125    # the bottom of the subplots of the figure\n",
    "            top    =  0.9    # the top of the subplots of the figure \n",
    "            wspace =  .5     # the amount of width reserved for blank space between subplots\n",
    "            hspace =  1.1    # the amount of height reserved for white space between subplots\n",
    "            # function that adjusts subplots using the above paramters\n",
    "            plt.subplots_adjust(\n",
    "                left    =  left, \n",
    "                bottom  =  bottom, \n",
    "                right   =  right, \n",
    "                top     =  top, \n",
    "                wspace  =  wspace, \n",
    "                hspace  =  hspace\n",
    "            )\n",
    "            \n",
    "            # The amount of space above titles\n",
    "            y_title_margin = 2\n",
    "            \n",
    "            plt.suptitle(self.plotTitle, y = 1, fontsize=20)\n",
    "            \n",
    "            plt.subplot(121)\n",
    "            sns.swarmplot(x='Regime',y='Volatility',data=data)#,ax=ax[0][0])\n",
    "            plt.title('Regime to Volatility')\n",
    "            \n",
    "            plt.subplot(122)\n",
    "            sns.swarmplot(x='Regime',y='Returns',data=data)#, ax=ax[0][1]) \n",
    "            plt.title('Regime to Returns')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            return\n",
    "\n",
    "                \n",
    "    def train_random_forests(self):\n",
    "            #adding Features to the DataFrame\n",
    "            #assumption is that this is the df \n",
    "            #over the entire period\n",
    "            \n",
    "            '''CAN UNCOMMENT THE SELF.VIX CODE TO INCLUDE VIX IN ANALYSIS..NOTE CHANGES TO PANDAS_DATAREADER API'''\n",
    "            #getting vix to add as feature\n",
    "            #self.VIX=pdr.get_data_yahoo('^VIX',start=self.total_training_start,end=self.total_training_end)\n",
    "\n",
    "            #creating features\n",
    "            #self.df['VIX']=self.VIX['Close']\n",
    "            self.df['6 X Vol']=self.df['X'].rolling(window=6).std()\n",
    "            self.df['6 Y Vol']=self.df['Y'].rolling(window=6).std()\n",
    "            self.df['6 Spread Vol']=self.df['Spread'].rolling(window=6).std()\n",
    "            self.df['6 Z-Score Vol']=self.df['Z-Score'].rolling(window=6).std()\n",
    "\n",
    "            self.df['12 X Vol']=self.df['X'].rolling(window=12).std()\n",
    "            self.df['12 Y Vol']=self.df['Y'].rolling(window=12).std()\n",
    "            self.df['12 Spread Vol']=self.df['Spread'].rolling(window=12).std()\n",
    "            self.df['12 Z-Score Vol']=self.df['Z-Score'].rolling(window=12).std()\n",
    "\n",
    "            self.df['15 X Vol']=self.df['X'].rolling(window=15).std()\n",
    "            self.df['15 Y Vol']=self.df['Y'].rolling(window=15).std()\n",
    "            self.df['15 Spread Vol']=self.df['Spread'].rolling(window=15).std()\n",
    "            self.df['15 Z-Score Vol']=self.df['Z-Score'].rolling(window=15).std()\n",
    "            \n",
    "            #adding features to our historical df\n",
    "            #self.base_portfolio_df['VIX']=self.VIX['Close']\n",
    "            self.base_portfolio_df['6 X Vol']=self.df['X'].rolling(window=6).std()\n",
    "            self.base_portfolio_df['6 Y Vol']=self.df['Y'].rolling(window=6).std()\n",
    "            self.base_portfolio_df['6 Spread Vol']=self.df['Spread'].rolling(window=6).std()\n",
    "            self.base_portfolio_df['6 Z-Score Vol']=self.df['Z-Score'].rolling(window=6).std()\n",
    "\n",
    "            self.base_portfolio_df['12 X Vol']=self.df['X'].rolling(window=12).std()\n",
    "            self.base_portfolio_df['12 Y Vol']=self.df['Y'].rolling(window=12).std()\n",
    "            self.base_portfolio_df['12 Spread Vol']=self.df['Spread'].rolling(window=12).std()\n",
    "            self.base_portfolio_df['12 Z-Score Vol']=self.df['Z-Score'].rolling(window=12).std()\n",
    "\n",
    "            self.base_portfolio_df['15 X Vol']=self.df['X'].rolling(window=15).std()\n",
    "            self.base_portfolio_df['15 Y Vol']=self.df['Y'].rolling(window=15).std()\n",
    "            self.base_portfolio_df['15 Spread Vol']=self.df['Spread'].rolling(window=15).std()\n",
    "            self.base_portfolio_df['15 Z-Score Vol']=self.df['Z-Score'].rolling(window=15).std()\n",
    "\n",
    "\n",
    "            #replacing na values\n",
    "            self.df.fillna(0, inplace=True)\n",
    "            #Creating X_Train for RF over the Historical Period; Will train\n",
    "            #over Historical period, ie. self.historical_training_start/end\n",
    "            #then predict \n",
    "            self.RF_X_TRAIN=self.df[0:4992][['6 X Vol','6 Y Vol','6 Spread Vol','6 Z-Score Vol','12 X Vol','12 Y Vol',\n",
    "                                               '12 Spread Vol','12 Z-Score Vol','15 X Vol','15 Y Vol','15 Spread Vol','15 Z-Score Vol']]\n",
    "            #dropping unnecessary columns from train data\n",
    "            #self.RF_X_TRAIN.drop(['X','Y','Longs','Shorts','Exit','Long_Market','Short_Market'],inplace=True,axis=1)\n",
    "            \n",
    "            #setting Y_Train for the RF to the predictions of GMM over historical period\n",
    "            self.RF_Y_TRAIN=self.gmm_historical_predictions\n",
    "\n",
    "            #Creating X_Test for the RF; ie the gmm_training_test period\n",
    "            #our features for RF\n",
    "            #that haven't been seen by the model\n",
    "            self.RF_X_TEST=self.base_portfolio_df[['6 X Vol','6 Y Vol','6 Spread Vol','6 Z-Score Vol','12 X Vol','12 Y Vol',\n",
    "                                               '12 Spread Vol','12 Z-Score Vol','15 X Vol','15 Y Vol','15 Spread Vol','15 Z-Score Vol']]\\\n",
    "            \n",
    "            #dropping unnecessary columns from train data\n",
    "            #self.RF_X_TEST.drop(['X','Y','Longs','Shorts','Exit','Long_Market','Short_Market'],inplace=True,axis=1)\n",
    "\n",
    "            #predictions for the x test over the internal testing period\n",
    "            self.RF_Y_TEST=self.base_portfolio_predictions #regime predictions for base portfolio\n",
    "\n",
    "            #build the RandomForest and check precision\n",
    "            self.RF_MODEL=RF(n_estimators=100) #Recall imported RandomForestsClassifier as RF\n",
    "            \n",
    "            #training the random forests model on assessment period data\n",
    "            self.RF_MODEL.fit(self.RF_X_TRAIN.fillna(0),self.RF_Y_TRAIN)\n",
    "            \n",
    "            #Making predictions for base portfolio period\n",
    "            self.RF_BASE_PORTFOLIO_PREDICTIONS=self.RF_MODEL.predict(self.RF_X_TEST.fillna(0))\n",
    "\n",
    "            #Checking Precision of Predictions\n",
    "            #print(confusion_matrix(self.RF_Y_TEST,self.RF_BASE_PORTFOLIO_PREDICTIONS))\n",
    "            #print('\\n')\n",
    "            #print(classification_report(self.RF_Y_TEST,self.RF_BASE_PORTFOLIO_PREDICTIONS))\n",
    "\n",
    "\n",
    "                \n",
    "            return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0cc4b4",
   "metadata": {},
   "source": [
    "Okay. Though I've heavily commented the above object, I'd like to reiterate the most important inputs to our gmm_randomForests method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a28693d",
   "metadata": {},
   "source": [
    "The most critical parameters are 1)gmm_training_train, 2)gmm_training_test, 3)gmm_test_actual, and 4)df. gmm_training_train is the 80% split of the historical period returns we created earlier. gmm_training_test is the 20% slice of the historical period returns we created. gmm_test actual is are the returns from either our Equally Weighted or Efficient Frontier Portfolios. The Bottom Up Optimization builds on the Equally Weighted Portfolio in that though it offers an optimization technique, the allocations remain equally weighted. This means that we can use the returns from our Equally Weighted Portfolio, predict the regimes, and create a new statarb object that uses the regimes(i.e. Bottom Up Optimization)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040a502f",
   "metadata": {},
   "source": [
    "The same logic is true for our Efficient Frontier Portfolio. The SPO Framework builds on the Efficient Frontier Portfolio. Thus when we design our SPO Framework Portfolio, we will get the regime predictions from our Efficient Frontier implementation. We will then create a new statarb object that takes those regime predictions in as a parameter and use it to create our SPO Framework Portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d136b9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the head of our features dataframe\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5197a86c",
   "metadata": {},
   "source": [
    "We're now ready to create our K-Means method and find subgroups within the S&P 500. Before we do so, let's drop the Name column from our features data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb83d180",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping name column\n",
    "features.drop('Name',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d522b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(spo_portfolio['Total Portfolio Value'])\n",
    "plt.title('SPO Portfolio Equity Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd2e9a6",
   "metadata": {},
   "source": [
    "We can now compare the Sharpe Ratios of each of our portfolios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad79948b",
   "metadata": {},
   "source": [
    "# Relative Portfolio Performance Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e024e53",
   "metadata": {},
   "source": [
    "Whew! We can now create a dataframe to hold our Sharpe ratios and compare our results. Let's create the variables that we use to create our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d3cb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list to hold portfolio names\n",
    "names=['Equally Weighted','Efficient Frontier','Bottom Up','SPO Framework']\n",
    "#variable to hold column name\n",
    "column_name='Sharpe Ratio'\n",
    "#list to hold Sharpe Ratios\n",
    "sharpes=[equally_weighted_Sharpe,efficient_frontier_portfolio_sharpe,bottom_up_portfolio_sharpe,spo_portfolio_sharpe]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9d2683",
   "metadata": {},
   "source": [
    "We'll now use our variables to create our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8914c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "adbe_antm_3_historical_rets=adbe_antm_3_historical.portfolio['Returns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284d3166",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking head of returns\n",
    "adbe_antm_3_historical_rets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34135b13",
   "metadata": {},
   "source": [
    "We will now repeat this process for our remaining pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06165d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANTM_AET Bottom Up Historical Implementation\n",
    "antm_aet_3_historical=statarb(training_df['ANTM'],training_df['AET'],6,-2,2,6,antm_test.iloc[0],antm_test.iloc[-1])\n",
    "antm_aet_3_historical.create_spread()\n",
    "antm_aet_3_historical.generate_signals()\n",
    "antm_aet_3_historical.create_returns(30000,'ANTM & AET_3 Over Hist. Train Period')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce0e085",
   "metadata": {},
   "source": [
    "Let's store our returns from the ANTM_AET training period in a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6e8174",
   "metadata": {},
   "outputs": [],
   "source": [
    "antm_aet_3_historical_rets=antm_aet_3_historical.portfolio['Returns']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d712cb",
   "metadata": {},
   "source": [
    "Okay we finish up with our AET_ANTM pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0302a9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "aet_antm_3_historical=statarb(training_df['AET'],training_df['ANTM'],12,-2,2,12,aet_test.iloc[0],aet_test.iloc[-1])\n",
    "aet_antm_3_historical.create_spread()\n",
    "aet_antm_3_historical.generate_signals()\n",
    "aet_antm_3_historical.create_returns(30000,'AET & ANTM')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1d36f3",
   "metadata": {},
   "source": [
    "Let's store our AET_ANTM returns in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435a2ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_weights(weights):\n",
    "    #Finding Most Optimal Weights\n",
    "    #variables for optimization\n",
    "    constraints=({'type':'eq','fun':lambda x: np.sum(x)-1})\n",
    "    bounds=tuple((0,1) for x in range(len(returns.columns)))\n",
    "\n",
    "    starting_weights=len(returns.columns)*[1./len(returns.columns)]\n",
    "    most_optimal=sco.minimize(minimize_func,starting_weights, method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "    best_weights=most_optimal['x'].round(3)\n",
    "\n",
    "    return best_weights, print('Weights:',best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb90072",
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing optimal weights in a variable\n",
    "optimal_weights=get_optimal_weights(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f0ed31",
   "metadata": {},
   "source": [
    "Okay. We can see that the most optimal weights are to allocate 2% of our capital to ADBE_ANTM, 0% to ANTM_AET and 97.8% to AET_ANTM. Recall that we our method was designed to optimize our Sharpe ratio. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66237e0",
   "metadata": {},
   "source": [
    "We can now implement another instance of our strategies using these weights and create our portfolio and compute its equity curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff626de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataframe to compare Sharpe Ratios of Portfolios\n",
    "portfolio_assessment=pd.DataFrame({column_name:sharpes},index=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867290d3",
   "metadata": {},
   "source": [
    "Okay. We can now check our dataframe and compare our Sharpe Ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67c5063",
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b5674f",
   "metadata": {},
   "source": [
    "Let's create a dataframe to compare the ending values of our portfolios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcd268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating our training data dataframe using our training period start and end dates\n",
    "training_df=get_training_data(original_data,symbol_list_0,'2018/01/04','2018/04/30')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21abb8b",
   "metadata": {},
   "source": [
    "Let's check our training_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd18112",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d18c91",
   "metadata": {},
   "source": [
    "After checking our dataframe, we find that we have our trainging period data for all of our stocks. This means that we should have 20 possible pairs. We can write a method to check our math."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd1b05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking head of Efficient Frontier Portfolio dataframe\n",
    "efficient_frontier_portfolio.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b221b927",
   "metadata": {},
   "source": [
    "We'll now plot the equity curve for our Efficient Frontier Portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ec8504",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting Equity Curve for Efficient Frontier Portfolio\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(efficient_frontier_portfolio['Total Portfolio Value'])\n",
    "plt.title('Efficient Frontier Portfolio Equity Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dadda69",
   "metadata": {},
   "source": [
    "We can now store our Mu, Sigma, and Sharpe for our Efficient Frontier Portfolio in variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66aa2b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating method to check possible pair combinations\n",
    "def possible_combinations(n):\n",
    "    #Parameters#\n",
    "    ############\n",
    "    #n- represents the number of items or in our case stocks\n",
    "    \n",
    "    possible_pairs=(n*(n-1))\n",
    "    \n",
    "    return possible_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0aee1a",
   "metadata": {},
   "source": [
    "Now let's check our possible pair combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a68f753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking possible pair combinations\n",
    "possible_combinations(5) # we pass in 5 for our 5 stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93d2579",
   "metadata": {},
   "source": [
    "We're now ready to check our pairs for cointegration. We'll create a method that will allow us to iterate over our pairs, compute the slope and then perform the CADF test. The pairs that are cointegrated will be stored in a list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93493a00",
   "metadata": {},
   "source": [
    "Let's import our OLS method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec5e1b9",
   "metadata": {},
   "source": [
    "#### ADBE_ANTM Random Forests Implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67e62a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the train_random_forests method on our adbe_antm_gmm_rf object\n",
    "adbe_antm_gmm_rf.train_random_forests()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e365b3b",
   "metadata": {},
   "source": [
    "Let's ensure that we understand what we've achieved thus far. Recall that we created variables to hold the returns of our training period 01/04/18 to 04/30/18. We used the training_df dataframe to provide the historical data and applied our StatArb strategy to it. We then split that training period returns, of which we stored in variables, into an 80% training and 20% testing subset. The period of 05/01/18 to 06/12/18 was our hypothetical live market data and is the period in which we created our Equally Weighted and Efficient Frontier Portfolios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2d3092",
   "metadata": {},
   "source": [
    "Our objective with the Bottom Up Portfolio was to build upon our Equally Weighted Portfolio by optimizing using the features we engineered to predict the regime that our strategy was in based on regimes seen historically. To achieve this we needed to train our Gaussian Mixture Model using our training over the period 01/04/18 to 04/30/18 and use our GMM Model to predict the regimes from our 05/01/18 to 06/12/18 period. We could then use our regimes from the 01/04/18 to 04/30/18 period as labels and the features that we engineered as parameters for our Random Forest training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad74c121",
   "metadata": {},
   "source": [
    "### Step 4: Training Random Forests to Predict Regimes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8e9bf6",
   "metadata": {},
   "source": [
    "Now that we have completed our regime analysis we can use our Random Forests to predict the regimes of our assessment period. Recall that we passed in the dataframe from our Equally Weighted Portfolio created by our adbe_antm object. We can call our train_random_forests method on our gmm_rf objects to train our Random Forests using the features we selected. We can then create a variable to hold our predictions. These predictions will then be passed into our new statarb object and used to generate our Bottom Up Portfolio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a56cc1",
   "metadata": {},
   "source": [
    "#### ANTM_AET Bottom Up Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a58a50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating instance of updated statarb strategy\n",
    "antm_aet_bottom_up=statarb_update(antm_test, aet_test, 2,6,-2, 2,6,'05/01/18','06/12/18',antm_aet_regime_predictions,'Target',avoid1=0,target1=1,\n",
    "                  exit_zscore=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fa271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating spread\n",
    "adbe_antm_bottom_up.create_spread()\n",
    "#generating signals\n",
    "adbe_antm_bottom_up.generate_signals()\n",
    "#getting performance\n",
    "#notice that we are passing in our equal weight amount here\n",
    "adbe_antm_bottom_up.create_returns(30000,'ADBE_ANTM Bottom Up')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af40061",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing optimization function\n",
    "import scipy.optimize as sco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5cd1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def efficient_frontier(returns,rate=0.015):\n",
    "   \n",
    "    #creating a list to hold our portfolio returns, variance and Sharpe values\n",
    "    portfolio_returns=[]\n",
    "    portfolio_volatility=[]\n",
    "    p_sharpes=[]\n",
    "    \n",
    "   # returns=returns_df\n",
    "    \n",
    "    for i in range(500):\n",
    "        #assigning weights\n",
    "        weights=np.random.random(len(returns.columns))\n",
    "        weights/=np.sum(weights)\n",
    "\n",
    "        #getting returns\n",
    "        current_return=np.sum(returns.mean()*weights)*252\n",
    "        portfolio_returns.append(current_return)\n",
    "        \n",
    "        #getting variances\n",
    "        variance=np.dot(weights.T,np.dot(returns.cov()*252,weights))\n",
    "        #getting volatility\n",
    "        volatility=np.sqrt(variance)\n",
    "        portfolio_volatility.append(volatility)\n",
    "        \n",
    "        #getting Sharpe ratios\n",
    "        ratio=(current_return-rate)/volatility\n",
    "        #storing Sharpe in list\n",
    "        p_sharpes.append(ratio)\n",
    "    \n",
    "    p_returns=np.array(portfolio_returns)\n",
    "    p_volatility=np.array(portfolio_volatility)\n",
    "    p_sharpes=np.array(p_sharpes)\n",
    "    \n",
    "    #plotting\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(p_volatility,p_returns,c=p_sharpes, marker='o')\n",
    "    plt.xlabel('Expected Volatility')\n",
    "    plt.ylabel('Expected Return')\n",
    "    plt.title('Efficient Frontier')\n",
    "    plt.colorbar(label='Sharpe Ratio')\n",
    "    plt.show()\n",
    "    \n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfbb257",
   "metadata": {},
   "outputs": [],
   "source": [
    "efficient_frontier(returns.fillna(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34036e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(weights,rate=0.015):\n",
    "    weights=np.array(weights)\n",
    "    p_returns=np.sum(returns.mean()*weights)*252\n",
    "    p_volatility=np.sqrt(np.dot(weights.T,np.dot(returns.cov()*252,weights)))\n",
    "    p_sharpe=(p_returns-rate)/p_volatility\n",
    "    \n",
    "    return np.array([p_returns,p_volatility,p_sharpe])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dbfa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing stats method\n",
    "stats(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f397fdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating function for optimization\n",
    "def minimize_func(weights):\n",
    "        return -stats(weights)[2]\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b2f64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing optimization function\n",
    "minimize_func(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779036f8",
   "metadata": {},
   "source": [
    "We won't make any changes to the implementation of this strategy in this illustration but further analysis could be conducted to determine the sign of returns given those volatility bursts and the same could be used to augment the strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c131ec7",
   "metadata": {},
   "source": [
    "Let's begin using our Random Forests to predict our regimes and update our StatArb strategy by using our regime predictions to answer the above question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185a402d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting Equally Weighted Portfolio Sharpe\n",
    "\n",
    "print('Equally Weighted Portfolio Sharpe:',equally_weighted_Sharpe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37a8ce7",
   "metadata": {},
   "source": [
    "Let's plot our portfolio equity curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376c1aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "aet_antm_3_historical_rets=aet_antm_3_historical.portfolio['Returns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a23e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking AET_ANTM returns over training period\n",
    "aet_antm_3_historical_rets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8425902",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assigning weights\n",
    "weights=np.random.random(len(returns.columns))\n",
    "weights/=np.sum(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917d5f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reviewing weights\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faee59b",
   "metadata": {},
   "source": [
    "The following method is an adaptation from Dr. Yves Hilpisch's \"Python for Finance\". We'll use it to plot our Efficient Frontier and find the most optimial weights for our strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5670dcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting regime returns and volatility...Pass in String for Title of Plot\n",
    "antm_aet_gmm_rf.historical_regime_returns_volatility('ANTM AET GMM Analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fcc366",
   "metadata": {},
   "source": [
    "Okay we can see that Regime 1 appears to offer the best risk/reward tradeoff for our ANTM_AET relationship. Our returns remain positive throughout the relative volatility distribution but it's more desirable to get equivalent returns with less volatility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f50deb",
   "metadata": {},
   "source": [
    "#### AET_ANTM GMM Implemenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e3323e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing object\n",
    "aet_antm_gmm_rf=gmm_randomForests(aet_antm_3_rets_train,aet_antm_3_rets_test,aet_antm.portfolio['Returns'],5,\n",
    "                                 aet_antm_3_historical.df, aet_antm.df,1871,-468)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf3b66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling our analyze historical regimes method\n",
    "aet_antm_gmm_rf.analyze_historical_regimes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f03b92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting regime returns and volatility...Pass in String for Title of Plot\n",
    "aet_antm_gmm_rf.historical_regime_returns_volatility('AET_ANTM GMM Analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b9c89b",
   "metadata": {},
   "source": [
    "We can see once again that despite the level of volatility, our returns are still positive. However, one thing that we can note from the above plots is that for the most part the levels of volatility are similar between Regime 0 and Regime 1. However, we do see some bursts in volatility within Regime 1 and a signicant portion of our historical returns, though they were positive, were less than those of Regime 0, of which didn't display any significant burts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4ddd8c",
   "metadata": {},
   "source": [
    "We have the prices of our symbols from 1/4/18 to 4/30/18. No pair containing AMAT was found to be cointegrated at atleast 90% confidence interval so we won't be using it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9508a4b",
   "metadata": {},
   "source": [
    "We can now recreate our equally weighted strategy implementations using our training period data. Once we have the returns for our implementations we can then engineer some features to use in training our Gaussian Mixture Model. We will then use these predictions as labels for our Random Forests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dc55ce",
   "metadata": {},
   "source": [
    "Once we've trained our Random Forests to predict the regimes found by our Gaussian Mixture Model, we can then use those predictions to augment our signal generator, avoiding troublesome regimes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5269e1e3",
   "metadata": {},
   "source": [
    "Let's begin creating our strategy implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9462392b",
   "metadata": {},
   "source": [
    "### Step 1: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74f5d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting pair combinations for cluster 9\n",
    "cluster_9_pairs=create_pairs(symbols_cluster_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c05975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing object\n",
    "antm_aet_gmm_rf=gmm_randomForests(antm_aet_3_rets_train,antm_aet_3_rets_test,antm_aet.portfolio['Returns'],5,\n",
    "                                 antm_aet_3_historical.df, antm_aet.df,1871,-468)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d428b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling our analyze historical regimes method\n",
    "antm_aet_gmm_rf.analyze_historical_regimes()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
