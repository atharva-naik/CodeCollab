{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acb436bf",
   "metadata": {},
   "source": [
    "## Assignment 2: Text Scraping & Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fba76f",
   "metadata": {},
   "source": [
    "This notebook is the second part of COMP41680 Assignment 2 and covers corpus exploration. The nltk.download, as seen at the bottom of the list of imports, was needed during implementation to run the lemmatizer function. Uncomment the download if it is required. There is also a stemming tokenizer which can be used instead if issues occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5160b473",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import  TfidfVectorizer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import scipy.cluster.hierarchy as hac\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import MDS\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import mpld3\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from IPython.display import display, HTML\n",
    "#nltk.download('wordnet')\n",
    "%matplotlib inline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fb1fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_tokenizer(text):\n",
    "    # use the standard scikit-learn tokenizer first\n",
    "    standard_tokenizer = CountVectorizer().build_tokenizer()\n",
    "    tokens = standard_tokenizer(text)\n",
    "    # then use NLTK to perform stemming on each token\n",
    "    stemmer = PorterStemmer()\n",
    "    stems = []\n",
    "    for token in tokens:\n",
    "        stems.append( stemmer.stem(token) )\n",
    "    return stems\n",
    "\n",
    "def lemma_tokenizer(docs):\n",
    "    # use the standard scikit-learn tokenizer first\n",
    "    standard_tokenizer = CountVectorizer().build_tokenizer()\n",
    "    tokens = standard_tokenizer(docs)\n",
    "    # then use NLTK to perform lemmatisation on each token\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    lemma_tokens = []\n",
    "    for token in tokens:\n",
    "        lemma_tokens.append( lemmatizer.lemmatize(token) )\n",
    "    return lemma_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c72c24",
   "metadata": {},
   "source": [
    "### Part 1: Read and Process Text\n",
    "Each text file contains two strings, the first is the title and the second is the article. These are stored in a directory named data which is located in the same directory as this jupyter notebook. These text files will now be read in. Each article is appended to a list of strings named docs. Each title is read into a seperate list of strings named titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25cfb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data/\"\n",
    "for root, dirs, files in os.walk(DATA_DIR):\n",
    "    docs=[]\n",
    "    titles = []\n",
    "    file_names = []\n",
    "    for file in files:\n",
    "        with open(root + file, 'r') as file_input:\n",
    "            raw_document = file_input.readlines()\n",
    "            docs.append(raw_document[1])\n",
    "            titles.append(raw_document[0].replace('\\n', ''))\n",
    "            \n",
    "    # Print some info to see what was read in\n",
    "    print(\"root:\",root)\n",
    "    print(\"\\nFirst 2 docs:\\n\", docs[0:2])\n",
    "    print(\"\\nFirst 10 titles:\\n\", titles[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c42b5f",
   "metadata": {},
   "source": [
    "### Part 2: Process Text and Create Document Term Matrix\n",
    "The articles will now be pre-processed and the document term matrix created. This pre-processing involves making all characters lower case and removing terms which occur in more than 80% of documents and less than 5 documents. English stop words are also removed. 1, 2 and 3 ngrams are used and lemmatization is applied. The maximum number of features is set to 200,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7223c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer =  TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=5, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=lemma_tokenizer, ngram_range=(1,3))\n",
    "tfidf_matrix = vectorizer.fit_transform(docs)\n",
    "tfidf_list = tfidf_matrix.todense().tolist()\n",
    "df_tfidf = pd.DataFrame(tfidf_list , index=titles, columns=vectorizer.get_feature_names())\n",
    "\n",
    "print(\"Number of terms in tfidf matrix: \", df_tfidf.shape[1])\n",
    "print(\"Number of documents in tfidf matrix: \", df_tfidf.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3e8785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the pandas dataframe which represents the tfidf matrix\n",
    "display(df_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d04e3c",
   "metadata": {},
   "source": [
    "### Part 3: Summarise the Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68c2bad",
   "metadata": {},
   "source": [
    "In order to explore the corpus the most common terms and highest weighted terms were found and plotted. The total number of terms is also shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54c1697",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = 1 - cosine_similarity(tfidf_matrix)\n",
    "print(dist[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc04872a",
   "metadata": {},
   "source": [
    "Convert to two components as we're plotting points in a two-dimensional plane.\n",
    "dissimilarity is set to precomputed because a distance matrix is provided.\n",
    "random_state is set so that the plot is reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc2ea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
    "pos = mds.fit_transform(dist)\n",
    "xs, ys = pos[:, 0], pos[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a683250",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up colors per clusters using a dict\n",
    "cluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3', 3: '#e7298a'}\n",
    "\n",
    "#set up cluster names using a dict\n",
    "cluster_names = {0: 'Technology', \n",
    "                 1: 'Finance', \n",
    "                 2: 'Health', \n",
    "                 3: 'Sport'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825d2047",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define custom toolbar location\n",
    "class TopToolbar(mpld3.plugins.PluginBase):\n",
    "    \"\"\"Plugin for moving toolbar to top of figure\"\"\"\n",
    "\n",
    "    JAVASCRIPT = \"\"\"\n",
    "    mpld3.register_plugin(\"toptoolbar\", TopToolbar);\n",
    "    TopToolbar.prototype = Object.create(mpld3.Plugin.prototype);\n",
    "    TopToolbar.prototype.constructor = TopToolbar;\n",
    "    function TopToolbar(fig, props){\n",
    "        mpld3.Plugin.call(this, fig, props);\n",
    "    };\n",
    "\n",
    "    TopToolbar.prototype.draw = function(){\n",
    "      // the toolbar svg doesn't exist\n",
    "      // yet, so first draw it\n",
    "      this.fig.toolbar.draw();\n",
    "\n",
    "      // then change the y position to be\n",
    "      // at the top of the figure\n",
    "      this.fig.toolbar.toolbar.attr(\"x\", 150);\n",
    "      this.fig.toolbar.toolbar.attr(\"y\", 400);\n",
    "\n",
    "      // then remove the draw function,\n",
    "      // so that it is not called again\n",
    "      this.fig.toolbar.draw = function() {}\n",
    "    }\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.dict_ = {\"type\": \"toptoolbar\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe196718",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define custom css to format the font and to remove the axis labeling\n",
    "css = \"\"\"\n",
    "text.mpld3-text, div.mpld3-tooltip {\n",
    "  font-family:Arial, Helvetica, sans-serif;\n",
    "}\n",
    "\n",
    "g.mpld3-xaxis, g.mpld3-yaxis {\n",
    "display: none; }\n",
    "\n",
    "svg.mpld3-figure {\n",
    "margin-left: -200px;}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dceeaa",
   "metadata": {},
   "source": [
    "### Part 5: K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48593a5",
   "metadata": {},
   "source": [
    "Create clusters using kmeans algorithm and display clusters. The clusters are grouped in a dataframe and this allows the content to be investigated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efdce21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the kmeans clusters\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "kmeans = km.fit(tfidf_matrix)\n",
    "clusters_kmeans = km.labels_.tolist()\n",
    "\n",
    "# Display 10 of the cluster values\n",
    "print(clusters_kmeans[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aeaa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create data frame that has the result of the MDS plus the cluster numbers and titles\n",
    "df_kmeans = pd.DataFrame(dict(x=xs, y=ys, label=clusters_kmeans, title=titles)) \n",
    "\n",
    "#group by cluster\n",
    "groups_kmeans = df_kmeans.groupby('label')\n",
    "for index in range(len(cluster_names)):\n",
    "    display(groups_kmeans.get_group(index)[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f820acd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot is interactive\n",
    "fig, ax = plt.subplots(figsize=(14,8)) #set plot size\n",
    "ax.margins(0.15) # Optional, just adds 15% padding to the autoscaling\n",
    "\n",
    "#iterate through groups to layer the plot\n",
    "#note that cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\n",
    "for name, group in groups_kmeans:\n",
    "    points = ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, \n",
    "                     label=cluster_names[name], mec='none', \n",
    "                     color=cluster_colors[name])\n",
    "    labels = [i for i in group.title]\n",
    "    \n",
    "    #set tooltip using points, labels and the already defined 'css'\n",
    "    tooltip = mpld3.plugins.PointHTMLTooltip(points[0], labels, voffset=10, hoffset=10, css=css)\n",
    "    #connect tooltip to fig\n",
    "    mpld3.plugins.connect(fig, tooltip, TopToolbar())    \n",
    "    \n",
    "    #set tick marks as blank\n",
    "    ax.axes.get_xaxis().set_ticks([])\n",
    "    ax.axes.get_yaxis().set_ticks([])\n",
    "    \n",
    "    #set axis as blank\n",
    "    ax.axes.get_xaxis().set_visible(False)\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "    \n",
    "ax.legend(numpoints=1) #show legend with only one dot\n",
    "\n",
    "mpld3.display() #show the plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2dfef4",
   "metadata": {},
   "source": [
    "### Part 6: Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98591088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices for terms in order of frequency\n",
    "indices = np.argsort(vectorizer.idf_)[::-1]\n",
    "print(\"indices of the 10 most highly weighted words:\",indices[0:10])\n",
    "\n",
    "# Get all terms\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "# Order terms by weight\n",
    "top_terms = [terms[i] for i in indices[:]]\n",
    "\n",
    "# Display number of terms in corpus and most highly weighted words in corpus\n",
    "print(\"\\nNumber of terms in document term matrix: \", len(terms))\n",
    "print(\"\\nTop 10 terms in Corpus:\")\n",
    "print(top_terms[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab133542",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "df_tfidf.astype(bool).sum(axis=0).sort_values(ascending=False)[:50].plot.bar()\n",
    "# plt.bar(list(range(len(terms))),)\n",
    "plt.title('Frequency of most common words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48581777",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(terms))\n",
    "# print(len(df_tfidf.astype(bool).sum(axis=0).sort()))\n",
    "plt.figure(figsize=(16,10))\n",
    "df_tfidf.sum(axis=0).sort_values(ascending=False)[:50].plot.bar()\n",
    "# plt.bar(list(range(len(terms))),)\n",
    "plt.title('Words with highest weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6e71d6",
   "metadata": {},
   "source": [
    "### Part 4: Pre-processing before Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6c1b15",
   "metadata": {},
   "source": [
    "The cosine similarities were calculated and multi dimensional scaling was used to reduce the dimensionality down to two dimensions so that the data could be visulised. Some CSS and Javascript was also included in order to display interactive plots. More info on this can be found at the source: http://brandonrose.org/clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e135d960",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fcf102",
   "metadata": {},
   "source": [
    "The cosine similarity is used to measure the similarity between documents. It ranges between 0 and 1 where a value of 1 indicates identical documents and a value of 0 indicates documents which share no terms in common. To demonstrate this, the cosine similarity between documents 1 and 2 can be seen to be 0.2.  The cosine similarity between documents 1 and 779 can be seen to be 0.82. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24e3a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"cos(D0,D2) = %.2f\" % cosine_similarity(tfidf_matrix[0], tfidf_matrix[2]))\n",
    "print( \"cos(D0,D779) = %.2f\" % cosine_similarity(tfidf_matrix[0], tfidf_matrix[779]))\n",
    "\n",
    "print(\"Titles of documents 0, 2 and 779:\")\n",
    "print(titles[0])\n",
    "print(titles[2])\n",
    "print(titles[779])"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
