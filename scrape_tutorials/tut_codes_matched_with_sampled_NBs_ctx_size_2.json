{
    "750332": {
        "jupyter_code_cell": "ax = a.plot(kind='bar',figsize=(7,5),ylim=(50,100))\nax.set_ylabel(\"Project Completion Percent\")\nbx = b.plot(kind='bar',figsize=(7,5))\nbx.set_ylabel(\"Project Duration\")",
        "matched_tutorial_code_inds": [
            5481,
            6704,
            3702,
            3771,
            6251
        ],
        "matched_tutorial_codes": [
            "fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111, title=\"Standardized Deviance Residuals\")\nax.hist(resid_std, bins=25, density=True)\nax.plot(kde_resid.support, kde_resid.density, \"r\")",
            "ax = dta.mean().plot(grid=False)\nax.set_xlabel(\"Year\", size=17)\nax.set_ylabel(\"Fertility rate\", size=17)\nax.set_xlim(0, 51)",
            "plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "win_percent.sort_values().plot.barh(figsize=(6, 12), width=.85, color='k')\nplt.tight_layout()\nsns.despine()\nplt.xlabel(\"Win Percent\")",
            "ax = oildata.plot()\nax.set_xlabel(\"Year\")\nax.set_ylabel(\"Oil (millions of tonnes)\")\nprint(\"Figure 7.1: Oil production in Saudi Arabia from 1996 to 2007.\")"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Histogram of standardized deviance residuals with Kernel Density Estimate overlaid"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Simple Exponential Smoothing"
            ]
        ]
    },
    "114564": {
        "jupyter_code_cell": "imr = Imputer(missing_values='NaN', strategy='mean', axis=0) \nimr = imr.fit(df)\nimputed_data = imr.transform(df.values)\nimputed_data\nimr = Imputer(missing_values='NaN', strategy='mean', axis=1)\nimr = imr.fit(df)\nimputed_data = imr.transform(df.values)\nimputed_data",
        "matched_tutorial_code_inds": [
            2844,
            2860,
            5643,
            2864,
            2943
        ],
        "matched_tutorial_codes": [
            "mae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}\n\n_, ax = (figsize=(5, 5))\ndisplay = (\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n)\nax.set_title(\"Ridge model, small regularization\")\nfor name, score in scores.items():\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\nax.legend(loc=\"upper left\")\n()\n\n\n<img alt=\"Ridge model, small regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_009.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_009.png\"/>",
            "mae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}\n\n_, ax = (figsize=(5, 5))\ndisplay = (\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n)\nax.set_title(\"Ridge model, optimum regularization\")\nfor name, score in scores.items():\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\nax.legend(loc=\"upper left\")\n()\n\n\n<img alt=\"Ridge model, optimum regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_012.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_012.png\"/>",
            "glm = smf.glm(\n    \"affairs ~ rate_marriage + yrs_married\",\n    data=dc,\n    family=sm.families.Poisson(),\n    freq_weights=np.asarray(dc[\"freq\"]),\n)\nres_f2 = glm.fit()\n# print(res_f2.summary())\nres_f2.pearson_chi2 - res_f.pearson_chi2, res_f2.deviance - res_f.deviance, res_f2.llf - res_f.llf",
            "mae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}\n\n_, ax = (figsize=(6, 6))\ndisplay = (\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n)\nax.set_title(\"Lasso model, optimum regularization\")\nfor name, score in scores.items():\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\nax.legend(loc=\"upper left\")\n()\n\n\n<img alt=\"Lasso model, optimum regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_015.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_015.png\"/>",
            "train_result = (\n    rf, X_train, y_train, n_repeats=10, random_state=42, n_jobs=2\n)\ntest_results = (\n    rf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_importances_idx = train_result.importances_mean.argsort()"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->original observations and frequency weights"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)"
            ]
        ]
    },
    "1438958": {
        "jupyter_code_cell": "os.system(\"{0} freyberg.pst\".format(pestchek))\nos.system(\"{0} freyberg.pst\".format(ppp))",
        "matched_tutorial_code_inds": [
            5539,
            1698,
            4003,
            1458,
            161
        ],
        "matched_tutorial_codes": [
            "kde = sm.nonparametric.KDEUnivariate(obs_dist)\nkde.fit()  # Estimate the densities",
            "rng = default_rng()\n\nbefore_sample = rng.choice(before_lock, size=30, replace=False)\nafter_sample = rng.choice(after_lock, size=30, replace=False)",
            "fmri = sns.load_dataset(\"fmri\")\nsns.relplot(data=fmri, x=\"timepoint\", y=\"signal\", kind=\"line\")\n",
            "load_xy = np.load(\"x_y-squared.npz\")\n\nprint(load_xy.files)",
            "compare.trim_significant_figures()\ncompare.colorize()\ncompare.print()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->Fitting with the default arguments"
            ],
            [
                "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Paired Student\u2019s t-test on the AQIs->Sampling"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty",
                "seaborn->Plotting functions->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty"
            ],
            [
                "numpy->NumPy Features->Saving and sharing your NumPy arrays->Remove the saved arrays and load them back with NumPy\u2019s"
            ],
            [
                "torch->PyTorch Recipes->PyTorch Benchmark->Steps->5. Comparing benchmark results"
            ]
        ]
    },
    "826529": {
        "jupyter_code_cell": "airports = airports[['iata_code', 'city', 'state']]\nflights_tmp = pd.merge(flights, airports, left_on='origin_airport', right_on = 'iata_code') \nflights_context_info = pd.merge(flights_tmp, airports, left_on='destination_airport', right_on = 'iata_code', suffixes =('_origin','_destination')) \ndel flights_tmp\nflights_context_info['oridst'] = flights_context_info.city_origin + '-' + flights_context_info.city_destination\nflights_context_info.groupby('oridst').is_delayed.sum().sort_values(ascending = False).head(10)",
        "matched_tutorial_code_inds": [
            3649,
            3650,
            3698,
            6060,
            23
        ],
        "matched_tutorial_codes": [
            "airports = ['DSM', 'ORD', 'JFK', 'PDX']\n\ng = sns.FacetGrid(weather.sort_index().loc[airports].reset_index(),\n                  col='station', hue='station', col_wrap=2, size=4)\ng.map(sns.regplot, 'sped', 'gust_mph')\nplt.savefig('../content/images/indexes_wind_gust_facet.png');",
            "airports = ['DSM', 'ORD', 'JFK', 'PDX']\n\ng = sns.FacetGrid(weather.sort_index().loc[airports].reset_index(),\n                  col='station', hue='station', col_wrap=2, size=4)\ng.map(sns.regplot, 'sped', 'gust_mph')\nplt.savefig('../content/images/indexes_wind_gust_facet.svg', transparent=True);",
            "files = glob.glob('weather/*.csv')\ncolumns = ['station', 'date', 'tmpf', 'relh', 'sped', 'mslp',\n           'p01i', 'vsby', 'gust_mph', 'skyc1', 'skyc2', 'skyc3']\n\n# init empty DataFrame, like you might for a list\nweather = pd.DataFrame(columns=columns)\n\nfor fp in files:\n    city = pd.read_csv(fp, columns=columns)\n    weather.append(city)",
            "data = pdr.get_data_fred(\"HOUSTNSA\", \"1959-01-01\", \"2019-06-01\")\nhousing = data.HOUSTNSA.pct_change().dropna()\n# Scale by 100 to get percentages\nhousing = 100 * housing.asfreq(\"MS\")\nfig, ax = plt.subplots()\nax = housing.plot(ax=ax)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_6_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_6_0.png\"/>",
            "face_dataset = FaceLandmarksDataset(csv_file='faces/face_landmarks.csv',\n                                    root_dir='faces/')\n\nfig = plt.figure()\n\nfor i in range(len(face_dataset)):\n    sample = face_dataset[i]\n\n    print(i, sample['image'].shape, sample['landmarks'].shape)\n\n    ax = plt.subplot(1, 4, i + 1)\n    plt.tight_layout()\n    ax.set_title('Sample #{}'.format(i))\n    ax.axis('off')\n    show_landmarks(**sample)\n\n    if i == 3:\n        plt.show()\n        break"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Indexes"
            ],
            [
                "pandas_toms_blog->Indexes"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions"
            ],
            [
                "torch->PyTorch Recipes->Developing Custom PyTorch Dataloaders->Part 1: The Dataset->1.3 Iterate through data samples"
            ]
        ]
    },
    "859230": {
        "jupyter_code_cell": "def processData(events):\n    inter = []\n    free = []\n    official = []\n    lat = []\n    lon = []\n    duration = []\n    month = []\n    for i,event in events.iterrows():\n            inter.append(event['interested_count'])\n            if event['is_free']:\n                free.append(1)\n            else:\n                free.append(0)\n            if event['is_official']:\n                official.append(1)\n            else:\n                official.append(0)\n            lat.append(event['latitude'])\n            lon.append(event['longitude'])\n            d1 = datetime.strptime(event['time_start'].split(' ')[0], \"%Y-%m-%d\")\n            d2 = datetime.strptime(event['time_end'].split(' ')[0], \"%Y-%m-%d\")\n            month.append(np.cos(d1.month/12))\n            duration.append((d2 - d1).days + 1)\n    columns = [inter,free,official,lat,lon,duration,month]\n    return sps.coo_matrix(np.array(columns).T)\ncategory = processCategorial(e['category'])\nids = processText(e['id'],200)\ndata = processData(e)",
        "matched_tutorial_code_inds": [
            6276,
            3316,
            3623,
            2398,
            470
        ],
        "matched_tutorial_codes": [
            "def add_stl_plot(fig, res, legend):\n    \"\"\"Add 3 plots from a second STL fit\"\"\"\n    axs = fig.get_axes()\n    comps = [\"trend\", \"seasonal\", \"resid\"]\n    for ax, comp in zip(axs[1:], comps):\n        series = getattr(res, comp)\n        if comp == \"resid\":\n            ax.plot(series, marker=\"o\", linestyle=\"none\")\n        else:\n            ax.plot(series)\n            if comp == \"trend\":\n                ax.legend(legend, frameon=False)\n\n\nstl = STL(elec_equip, period=12, robust=True)\nres_robust = stl.fit()\nfig = res_robust.plot()\nres_non_robust = STL(elec_equip, period=12, robust=False).fit()\nadd_stl_plot(fig, res_non_robust, [\"Robust\", \"Non-robust\"])\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_stl_decomposition_10_0.png\" src=\"../../../_images/examples_notebooks_generated_stl_decomposition_10_0.png\"/>",
            "def subject_body_extractor(posts):\n    # construct object dtype array with two columns\n    # first column = 'subject' and second column = 'body'\n    features = (shape=(len(posts), 2), dtype=object)\n    for i, text in enumerate(posts):\n        # temporary variable `_` stores '\\n\\n'\n        headers, _, body = text.partition(\"\\n\\n\")\n        # store body text in second column\n        features[i, 1] = body\n\n        prefix = \"Subject:\"\n        sub = \"\"\n        # save text after 'Subject:' in first column\n        for line in headers.split(\"\\n\"):\n            if line.startswith(prefix):\n                sub = line[len(prefix) :]\n                break\n        features[i, 0] = sub\n\n    return features\n\n\nsubject_body_transformer = (subject_body_extractor)",
            "def read(fp):\n    df = (pd.read_csv(fp)\n            .rename(columns=str.lower)\n            .drop('unnamed: 36', axis=1)\n            .pipe(extract_city_name)\n            .pipe(time_to_datetime, ['dep_time', 'arr_time', 'crs_arr_time', 'crs_dep_time'])\n            .assign(fl_date=lambda x: pd.to_datetime(x['fl_date']),\n                    dest=lambda x: pd.Categorical(x['dest']),\n                    origin=lambda x: pd.Categorical(x['origin']),\n                    tail_num=lambda x: pd.Categorical(x['tail_num']),\n                    unique_carrier=lambda x: pd.Categorical(x['unique_carrier']),\n                    cancellation_code=lambda x: pd.Categorical(x['cancellation_code'])))\n    return df\n\ndef extract_city_name(df):\n    '''\n    Chicago, IL -&gt; Chicago for origin_city_name and dest_city_name\n    '''\n    cols = ['origin_city_name', 'dest_city_name']\n    city = df[cols].apply(lambda x: x.str.extract(\"(.*), \\w{2}\", expand=False))\n    df = df.copy()\n    df[['origin_city_name', 'dest_city_name']] = city\n    return df\n\ndef time_to_datetime(df, columns):\n    '''\n    Combine all time items into datetimes.\n\n    2014-01-01,0914 -&gt; 2014-01-01 09:14:00\n    '''\n    df = df.copy()\n    def converter(col):\n        timepart = (col.astype(str)\n                       .str.replace('\\.0$', '')  # NaNs force float dtype\n                       .str.pad(4, fillchar='0'))\n        return pd.to_datetime(df['fl_date'] + ' ' +\n                               timepart.str.slice(0, 2) + ':' +\n                               timepart.str.slice(2, 4),\n                               errors='coerce')\n    df[columns] = df[columns].apply(converter)\n    return df\n\noutput = 'data/flights.h5'\n\nif not os.path.exists(output):\n    df = read(\"data/627361791_T_ONTIME.csv\")\n    df.to_hdf(output, 'flights', format='table')\nelse:\n    df = pd.read_hdf(output, 'flights', format='table')\ndf.info()",
            "def plot_accuracy(x, y, x_legend):\n    \"\"\"Plot accuracy as a function of x.\"\"\"\n    x = (x)\n    y = (y)\n    (\"Classification accuracy as a function of %s\" % x_legend)\n    (\"%s\" % x_legend)\n    (\"Accuracy\")\n    (True)\n    (x, y)\n\n\n[\"legend.fontsize\"] = 10\ncls_names = list(sorted(cls_stats.keys()))\n\n# Plot accuracy evolution\n()\nfor _, stats in sorted(cls_stats.items()):\n    # Plot accuracy evolution with #examples\n    accuracy, n_examples = zip(*stats[\"accuracy_history\"])\n    plot_accuracy(n_examples, accuracy, \"training examples (#)\")\n    ax = ()\n    ax.set_ylim((0.8, 1))\n(cls_names, loc=\"best\")\n\n()\nfor _, stats in sorted(cls_stats.items()):\n    # Plot accuracy evolution with runtime\n    accuracy, runtime = zip(*stats[\"runtime_history\"])\n    plot_accuracy(runtime, accuracy, \"runtime (s)\")\n    ax = ()\n    ax.set_ylim((0.8, 1))\n(cls_names, loc=\"best\")\n\n# Plot fitting times\n()\nfig = ()\ncls_runtime = [stats[\"total_fit_time\"] for cls_name, stats in sorted(cls_stats.items())]\n\ncls_runtime.append(total_vect_time)\ncls_names.append(\"Vectorization\")\nbar_colors = [\"b\", \"g\", \"r\", \"c\", \"m\", \"y\"]\n\nax = (111)\nrectangles = (range(len(cls_names)), cls_runtime, width=0.5, color=bar_colors)\n\nax.set_xticks((0, len(cls_names) - 1, len(cls_names)))\nax.set_xticklabels(cls_names, fontsize=10)\nymax = max(cls_runtime) * 1.2\nax.set_ylim((0, ymax))\nax.set_ylabel(\"runtime (s)\")\nax.set_title(\"Training Times\")\n\n\ndef autolabel(rectangles):\n    \"\"\"attach some text vi autolabel on rectangles.\"\"\"\n    for rect in rectangles:\n        height = rect.get_height()\n        ax.text(\n            rect.get_x() + rect.get_width() / 2.0,\n            1.05 * height,\n            \"%.4f\" % height,\n            ha=\"center\",\n            va=\"bottom\",\n        )\n        (()[1], rotation=30)\n\n\nautolabel(rectangles)\n()\n()\n\n# Plot prediction times\n()\ncls_runtime = []\ncls_names = list(sorted(cls_stats.keys()))\nfor cls_name, stats in sorted(cls_stats.items()):\n    cls_runtime.append(stats[\"prediction_time\"])\ncls_runtime.append(parsing_time)\ncls_names.append(\"Read/Parse\\n+Feat.Extr.\")\ncls_runtime.append(vectorizing_time)\ncls_names.append(\"Hashing\\n+Vect.\")\n\nax = (111)\nrectangles = (range(len(cls_names)), cls_runtime, width=0.5, color=bar_colors)\n\nax.set_xticks((0, len(cls_names) - 1, len(cls_names)))\nax.set_xticklabels(cls_names, fontsize=8)\n(()[1], rotation=30)\nymax = max(cls_runtime) * 1.2\nax.set_ylim((0, ymax))\nax.set_ylabel(\"runtime (s)\")\nax.set_title(\"Prediction Times (%d instances)\" % n_test_documents)\nautolabel(rectangles)\n()\n()\n\n\n\n<img alt=\"Classification accuracy as a function of training examples (#)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_out_of_core_classification_001.png\" srcset=\"../../_images/sphx_glr_plot_out_of_core_classification_001.png\"/>\n<img alt=\"Classification accuracy as a function of runtime (s)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_out_of_core_classification_002.png\" srcset=\"../../_images/sphx_glr_plot_out_of_core_classification_002.png\"/>\n<img alt=\"Training Times\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_out_of_core_classification_003.png\" srcset=\"../../_images/sphx_glr_plot_out_of_core_classification_003.png\"/>\n<img alt=\"Prediction Times (1000 instances)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_out_of_core_classification_004.png\" srcset=\"../../_images/sphx_glr_plot_out_of_core_classification_004.png\"/>",
            "def predict(input_line, n_predictions=3):\n    print('\\n&gt; %s' % input_line)\n    with ():\n         = evaluate(lineToTensor(input_line))\n\n        # Get top N categories\n        topv, topi = .topk(n_predictions, 1, True)\n        predictions = []\n\n        for i in range(n_predictions):\n            value = topv[0][i].item()\n            category_index = topi[0][i].item()\n            print('(%.2f) %s' % (value, all_categories[category_index]))\n            predictions.append([value, all_categories[category_index]])\n\npredict('Dovesky')\npredict('Jackson')\npredict('Satoshi')"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition->Robust Fitting"
            ],
            [
                "sklearn->Examples->Pipelines and composite estimators->Column Transformer with Heterogeneous Data Sources->Creating transformers"
            ],
            [
                "pandas_toms_blog->Method Chaining->Method Chaining"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Out-of-core classification of text documents->Plot results"
            ],
            [
                "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN->Evaluating the Results->Running on User Input"
            ]
        ]
    },
    "67925": {
        "jupyter_code_cell": "f, ax = plt.subplots(figsize=(20, 6))\ntest_corr = test.corr()\nsns.heatmap(test_corr, mask=np.zeros_like(test_corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            square=True, ax=ax)\ndtree=DecisionTreeRegressor()\nx = train['hour']\ny = train['registered']\nx = x.reshape(-1, 1)\ndtree.fit(x,y)\ndot_data = StringIO()\nexport_graphviz(dtree, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(graph.create_png())",
        "matched_tutorial_code_inds": [
            3922,
            3981,
            3680,
            6715,
            4956
        ],
        "matched_tutorial_codes": [
            "f, axs = plt.subplots(1, 2, figsize=(8, 4), gridspec_kw=dict(width_ratios=[4, 3]))\nsns.scatterplot(data=penguins, x=\"flipper_length_mm\", y=\"bill_length_mm\", hue=\"species\", ax=axs[0])\nsns.histplot(data=penguins, x=\"species\", hue=\"species\", shrink=.8, alpha=.8, legend=False, ax=axs[1])\nf.tight_layout()\n",
            "f = mpl.figure.Figure(figsize=(8, 4))\nsf1, sf2 = f.subfigures(1, 2)\n(\n    so.Plot(penguins, x=\"body_mass_g\", y=\"flipper_length_mm\")\n    .add(so.Dots())\n    .on(sf1)\n    .plot()\n)\n(\n    so.Plot(penguins, x=\"body_mass_g\")\n    .facet(row=\"sex\")\n    .add(so.Bars(), so.Hist())\n    .on(sf2)\n    .plot()\n)\n",
            "fig, ax = plt.subplots()\nax.scatter(res.fittedvalues, res.resid, color='k', marker='.', alpha=.25)\nax.set(xlabel='Predicted', ylabel='Residual')\nsns.despine()\nplt.savefig('../content/images/indexes_resid_fit.png', transparent=True)",
            "fig, ax = plt.subplots()\npca_model.loadings.plot.scatter(x=\"comp_00\", y=\"comp_01\", ax=ax)\nax.set_xlabel(\"PC 1\", size=17)\nax.set_ylabel(\"PC 2\", size=17)\ndta.index[pca_model.loadings.iloc[:, 1]  0.2].values",
            "fig, axs = plt.subplots(2, 1, figsize=(5, 3), tight_layout=True)\naxs[0].plot(x1, y1)\naxs[1].plot(x1, y1)\nticks = np.arange(0., 8.1, 2.)\naxs[1].xaxis.set_ticks(ticks)\naxs[1].xaxis.set_major_formatter('{x:1.1f}')\naxs[1].set_xlim(axs[0].get_xlim())\nplt.show()\n\n\n<img alt=\"text intro\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_text_intro_012.png\" srcset=\"../../_images/sphx_glr_text_intro_012.png, ../../_images/sphx_glr_text_intro_012_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Overview of seaborn plotting functions->Figure-level vs. axes-level functions->Axes-level functions make self-contained plots",
                "seaborn->API Overview->Overview of seaborn plotting functions->Figure-level vs. axes-level functions->Axes-level functions make self-contained plots"
            ],
            [
                "seaborn->User guide and tutorial->The seaborn.objects interface->Building and displaying the plot->Integrating with matplotlib",
                "seaborn->Objects interface->The seaborn.objects interface->Building and displaying the plot->Integrating with matplotlib"
            ],
            [
                "pandas_toms_blog->Indexes->Merging->The merge version"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "matplotlib->Tutorials->Text->Text in Matplotlib Plots->Ticks and ticklabels->Tick Locators and Formatters"
            ]
        ]
    },
    "1118014": {
        "jupyter_code_cell": "plot_sales_per('year')\nprint(train_df[train_df.open == 0]['sales'].sum())\nprint(train_df[train_df.open == 0]['customers'].sum())",
        "matched_tutorial_code_inds": [
            5499,
            5143,
            3702,
            3852,
            4077
        ],
        "matched_tutorial_codes": [
            "pred_choice = predicted.argmax(1)\nprint('Fraction of correct choice predictions')\nprint((np.asarray(data_student['apply'].values.codes) == pred_choice).mean())",
            "sqtab = sm.stats.SquareTable.from_data(data[['left', 'right']])\nprint(sqtab.summary())",
            "plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "pred = res_seasonal.get_prediction(start='2001-03-01')\npred_ci = pred.conf_int()",
            "diamonds = sns.load_dataset(\"diamonds\")\nsns.catplot(\n    data=diamonds.sort_values(\"color\"),\n    x=\"color\", y=\"price\", kind=\"boxen\",\n)\n"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Logit ordinal regression:"
            ],
            [
                "statsmodels->User Guide->Statistics and Tools->Contingency tables->Symmetry and homogeneity"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Forecasting"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing categorical data->Comparing distributions->Boxplots",
                "seaborn->Plotting functions->Visualizing categorical data->Comparing distributions->Boxplots"
            ]
        ]
    },
    "138265": {
        "jupyter_code_cell": "df.dtypes\ndf.scheduled_day = pd.to_datetime(df.scheduled_day)\ndf.appt_day = pd.to_datetime(df.appt_day)\ndf.appt_day.dt.weekday[0:5]",
        "matched_tutorial_code_inds": [
            5703,
            3616,
            3852,
            5779,
            5381
        ],
        "matched_tutorial_codes": [
            "summ_df = infl.summary_frame()\nsumm_df.sort_values(\"cooks_d\", ascending=False)[:10]",
            "hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()",
            "pred = res_seasonal.get_prediction(start='2001-03-01')\npred_ci = pred.conf_int()",
            "huber = sm.robust.scale.Huber()\nloc, scale = huber(fat_tails)\nprint(loc, scale)",
            "spector_data = sm.datasets.spector.load()\nspector_data.exog = sm.add_constant(spector_data.exog, prepend=False)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Generalized Linear Models->Influence Measures for GLMs->get the influence measures"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Forecasting"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Scale Estimators"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Data"
            ]
        ]
    },
    "1127152": {
        "jupyter_code_cell": "[(yr, abs(pct_change_df[yr].max() - pct_change_df[yr].min(0))) for yr in yrs]\npct_change_df['2010'].std()",
        "matched_tutorial_code_inds": [
            5472,
            2576,
            5470,
            5413,
            5703
        ],
        "matched_tutorial_codes": [
            "resp25 = glm_mod.predict(pd.DataFrame(means25).T)\nresp75 = glm_mod.predict(pd.DataFrame(means75).T)\ndiff = resp75 - resp25",
            "X = (co2_data.index.year + co2_data.index.month / 12).to_numpy().reshape(-1, 1)\ny = co2_data[\"co2\"].to_numpy()",
            "means75 = exog.mean()\nmeans75[\"LOWINC\"] = exog[\"LOWINC\"].quantile(0.75)\nprint(means75)",
            "dta[\"affair\"] = (dta[\"affairs\"]  0).astype(float)\nprint(dta.head(10))",
            "summ_df = infl.summary_frame()\nsumm_df.sort_values(\"cooks_d\", ascending=False)[:10]"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) on Mauna Loa CO2 data->Build the dataset"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Influence Measures for GLMs->get the influence measures"
            ]
        ]
    },
    "1002849": {
        "jupyter_code_cell": "sample_json_df = pd.read_json('data/world_bank_projects_less.json')\nsample_json_df\nworld_bank_projects = pd.read_json('data/world_bank_projects.json')[['countrycode', 'countryshortname']]\nworld_bank_projects = world_bank_projects.groupby('countryshortname').count().sort_values('countrycode', ascending=False)\nworld_bank_projects.columns = ['Total Projects']\nworld_bank_projects[:10]",
        "matched_tutorial_code_inds": [
            1475,
            5643,
            5956,
            5645,
            1762
        ],
        "matched_tutorial_codes": [
            "totals_row = 35\nlocations = np.delete(locations, (totals_row), axis=0)\nnbcases = np.delete(nbcases, (totals_row), axis=0)\n\nchina_total = nbcases[locations[:, 1] == \"China\"].sum(axis=0)\nchina_total",
            "glm = smf.glm(\n    \"affairs ~ rate_marriage + yrs_married\",\n    data=dc,\n    family=sm.families.Poisson(),\n    freq_weights=np.asarray(dc[\"freq\"]),\n)\nres_f2 = glm.fit()\n# print(res_f2.summary())\nres_f2.pearson_chi2 - res_f.pearson_chi2, res_f2.deviance - res_f.deviance, res_f2.llf - res_f.llf",
            "kidney_lm = ols(\"np.log(Days+1) ~ C(Duration) * C(Weight)\", data=kt).fit()\n\ntable10 = anova_lm(kidney_lm)\n\nprint(\n    anova_lm(ols(\"np.log(Days+1) ~ C(Duration) + C(Weight)\", data=kt).fit(), kidney_lm)\n)\nprint(\n    anova_lm(\n        ols(\"np.log(Days+1) ~ C(Duration)\", data=kt).fit(),\n        ols(\"np.log(Days+1) ~ C(Duration) + C(Weight, Sum)\", data=kt).fit(),\n    )\n)\nprint(\n    anova_lm(\n        ols(\"np.log(Days+1) ~ C(Weight)\", data=kt).fit(),\n        ols(\"np.log(Days+1) ~ C(Duration) + C(Weight, Sum)\", data=kt).fit(),\n    )\n)",
            "glm = smf.glm(\n    \"affairs_sum ~ rate_marriage + yrs_married\",\n    data=df_a,\n    family=sm.families.Poisson(),\n    exposure=np.asarray(df_a[\"affairs_count\"]),\n)\nres_e2 = glm.fit()\nres_e2.pearson_chi2 - res_e.pearson_chi2, res_e2.deviance - res_e.deviance, res_e2.llf - res_e.llf",
            "hidden_dim = 64\ninput_dim = emb_matrix['memory'].shape[0]\nlearning_rate = 0.001\nepochs = 10\nparameters = initialise_params(hidden_dim,\n                               input_dim)\nv, s = initialise_mav(hidden_dim,\n                      input_dim,\n                      parameters)"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Features->Masked Arrays->Exploring the data"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->original observations and frequency weights"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA->Two-way ANOVA"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->aggregated data: exposure and var_weights"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Training the Network"
            ]
        ]
    },
    "345352": {
        "jupyter_code_cell": "clus = KMeans(n_clusters=5)\nclus.fit(pca_ap)\nlabels = clus.labels_\nfig = plt.figure(figsize=(10,7))\nax = fig.add_subplot(111)\nax.scatter(pca_ap[:,0],pca_ap[:,1],c=labels, cmap=cm.get_cmap('rainbow'))\nax.set(xlabel='1st PC', ylabel='2nd PC')\nplt.show()\nZ = linkage(pca_ap, 'ward')\nc, coph_dists = cophenet(Z, pdist(pca_ap))\nplt.title('Dendrogram')\nplt.xlabel('Index Numbers')\nplt.ylabel('Distance')\ndendrogram(\n    Z,\n    leaf_rotation=90.,  \n    leaf_font_size=8.,)\nplt.show()",
        "matched_tutorial_code_inds": [
            1966,
            3122,
            5998,
            2116,
            5845
        ],
        "matched_tutorial_codes": [
            "af = (preference=-50, random_state=0).fit(X)\ncluster_centers_indices = af.cluster_centers_indices_\nlabels = af.labels_\n\nn_clusters_ = len(cluster_centers_indices)\n\nprint(\"Estimated number of clusters: %d\" % n_clusters_)\nprint(\"Homogeneity: %0.3f\" % (labels_true, labels))\nprint(\"Completeness: %0.3f\" % (labels_true, labels))\nprint(\"V-measure: %0.3f\" % (labels_true, labels))\nprint(\"Adjusted Rand Index: %0.3f\" % (labels_true, labels))\nprint(\n    \"Adjusted Mutual Information: %0.3f\"\n    % (labels_true, labels)\n)\nprint(\n    \"Silhouette Coefficient: %0.3f\"\n    % (X, labels, metric=\"sqeuclidean\")\n)",
            "results = (list)\nn_bootstrap = 100\nrng = (seed=0)\n\nfor prevalence, X, y in zip(\n    populations[\"prevalence\"], populations[\"X\"], populations[\"y\"]\n):\n\n    results_for_prevalence = scoring_on_bootstrap(\n        estimator, X, y, rng, n_bootstrap=n_bootstrap\n    )\n    results[\"prevalence\"].append(prevalence)\n    results[\"metrics\"].append(\n        results_for_prevalence.aggregate([\"mean\", \"std\"]).unstack()\n    )\n\nresults = (results[\"metrics\"], index=results[\"prevalence\"])\nresults.index.name = \"prevalence\"\nresults\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "res5 = combine_effects(eff, var_eff, method_re=\"chi2\", use_t=False)\nres5_df = res5.summary_frame()\nprint(\"method RE:\", res5.method_re)\nprint(res5.summary_frame())\nfig = res5.plot_forest()\nfig.set_figheight(8)\nfig.set_figwidth(6)",
            "fa_estimator = (n_components=n_components, max_iter=20)\nfa_estimator.fit(faces_centered)\nplot_gallery(\"Factor Analysis (FA)\", fa_estimator.components_[:n_components])\n\n# --- Pixelwise variance\n(figsize=(3.2, 3.6), facecolor=\"white\", tight_layout=True)\nvec = fa_estimator.noise_variance_\nvmax = max(vec.max(), -vec.min())\n(\n    vec.reshape(image_shape),\n    cmap=plt.cm.gray,\n    interpolation=\"nearest\",\n    vmin=-vmax,\n    vmax=vmax,\n)\n(\"off\")\n(\"Pixelwise variance from \\n Factor Analysis (FA)\", size=16, wrap=True)\n(orientation=\"horizontal\", shrink=0.8, pad=0.03)\n()\n\n\n\n<img alt=\"Factor Analysis (FA)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_008.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_008.png\"/>\n<img alt=\"Pixelwise variance from   Factor Analysis (FA)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_009.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_009.png\"/>",
            "huber_t = sm.RLM(data.endog, data.exog, M=sm.robust.norms.HuberT())\nhub_results = huber_t.fit()\nprint(hub_results.params)\nprint(hub_results.bse)\nprint(\n    hub_results.summary(\n        yname=\"y\", xname=[\"var_%d\" % i for i in range(len(hub_results.params))]\n    )\n)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Clustering->Demo of affinity propagation clustering algorithm->Compute Affinity Propagation"
            ],
            [
                "sklearn->Examples->Model Selection->Class Likelihood Ratios to measure classification performance->Invariance with respect to prevalence"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Meta-analysis of proportions->changing data to have positive random effects variance"
            ],
            [
                "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition->Factor Analysis components - FA"
            ],
            [
                "statsmodels->Examples->Robust Regression->Comparing OLS and RLM->Estimation"
            ]
        ]
    },
    "213412": {
        "jupyter_code_cell": "df_failed_midwifing_txns = (df_all_midwifing_txns[~df_all_midwifing_txns['hash'].isin(df_profitability.index)])[['hash','from','to','gasUsed','gasPrice']]\ndf_failed_midwifing_txns.columns = ['transactionHash','midwife','midwife_smartcontract','gasUsed','gasPrice']\ndf_failed_midwifing_txns['gasPrice'] = df_failed_midwifing_txns['gasPrice'].apply(lambda x: int(x,16))\ndf_failed_midwifing_txns['kitties_delivered'] = 0\ndf_failed_midwifing_txns['revenue'] = 0\ndf_failed_midwifing_txns['fee'] = df_failed_midwifing_txns['gasUsed'] * df_failed_midwifing_txns['gasPrice'] * 1e-18\ndf_failed_midwifing_txns['profit'] = -df_failed_midwifing_txns['fee']\ndf_failed_midwifing_txns = df_failed_midwifing_txns.set_index('transactionHash')\nlen(df_failed_midwifing_txns)\ndf_profitability = df_profitability.append(df_failed_midwifing_txns, sort=True)\nlen(df_profitability)",
        "matched_tutorial_code_inds": [
            3773,
            5613,
            2116,
            6257,
            6253
        ],
        "matched_tutorial_codes": [
            "df = df.assign(away_strength=df['away_team'].map(win_percent),\n               home_strength=df['home_team'].map(win_percent),\n               point_diff=df['home_points'] - df['away_points'],\n               rest_diff=df['home_rest'] - df['away_rest'])\ndf.head()",
            "gr = data[\"affairs rate_marriage age yrs_married\".split()].groupby(\n    \"rate_marriage age yrs_married\".split()\n)\ndf_a = gr.agg([\"mean\", \"sum\", \"count\"])\n\n\ndef merge_tuple(tpl):\n    if isinstance(tpl, tuple) and len(tpl)  1:\n        return \"_\".join(map(str, tpl))\n    else:\n        return tpl\n\n\ndf_a.columns = df_a.columns.map(merge_tuple)\ndf_a.reset_index(inplace=True)\nprint(df_a.shape)\ndf_a.head()",
            "fa_estimator = (n_components=n_components, max_iter=20)\nfa_estimator.fit(faces_centered)\nplot_gallery(\"Factor Analysis (FA)\", fa_estimator.components_[:n_components])\n\n# --- Pixelwise variance\n(figsize=(3.2, 3.6), facecolor=\"white\", tight_layout=True)\nvec = fa_estimator.noise_variance_\nvmax = max(vec.max(), -vec.min())\n(\n    vec.reshape(image_shape),\n    cmap=plt.cm.gray,\n    interpolation=\"nearest\",\n    vmin=-vmax,\n    vmax=vmax,\n)\n(\"off\")\n(\"Pixelwise variance from \\n Factor Analysis (FA)\", size=16, wrap=True)\n(orientation=\"horizontal\", shrink=0.8, pad=0.03)\n()\n\n\n\n<img alt=\"Factor Analysis (FA)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_008.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_008.png\"/>\n<img alt=\"Pixelwise variance from   Factor Analysis (FA)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_009.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_009.png\"/>",
            "fit1 = SimpleExpSmoothing(livestock2, initialization_method=\"estimated\").fit()\nfit2 = Holt(livestock2, initialization_method=\"estimated\").fit()\nfit3 = Holt(livestock2, exponential=True, initialization_method=\"estimated\").fit()\nfit4 = Holt(livestock2, damped_trend=True, initialization_method=\"estimated\").fit(\n    damping_trend=0.98\n)\nfit5 = Holt(\n    livestock2, exponential=True, damped_trend=True, initialization_method=\"estimated\"\n).fit()\nparams = [\n    \"smoothing_level\",\n    \"smoothing_trend\",\n    \"damping_trend\",\n    \"initial_level\",\n    \"initial_trend\",\n]\nresults = pd.DataFrame(\n    index=[r\"$\\alpha$\", r\"$\\beta$\", r\"$\\phi$\", r\"$l_0$\", \"$b_0$\", \"SSE\"],\n    columns=[\"SES\", \"Holt's\", \"Exponential\", \"Additive\", \"Multiplicative\"],\n)\nresults[\"SES\"] = [fit1.params[p] for p in params] + [fit1.sse]\nresults[\"Holt's\"] = [fit2.params[p] for p in params] + [fit2.sse]\nresults[\"Exponential\"] = [fit3.params[p] for p in params] + [fit3.sse]\nresults[\"Additive\"] = [fit4.params[p] for p in params] + [fit4.sse]\nresults[\"Multiplicative\"] = [fit5.params[p] for p in params] + [fit5.sse]\nresults",
            "fit1 = SimpleExpSmoothing(oildata, initialization_method=\"heuristic\").fit(\n    smoothing_level=0.2, optimized=False\n)\nfcast1 = fit1.forecast(3).rename(r\"$\\alpha=0.2$\")\nfit2 = SimpleExpSmoothing(oildata, initialization_method=\"heuristic\").fit(\n    smoothing_level=0.6, optimized=False\n)\nfcast2 = fit2.forecast(3).rename(r\"$\\alpha=0.6$\")\nfit3 = SimpleExpSmoothing(oildata, initialization_method=\"estimated\").fit()\nfcast3 = fit3.forecast(3).rename(r\"$\\alpha=%s$\" % fit3.model.params[\"smoothing_level\"])\n\nplt.figure(figsize=(12, 8))\nplt.plot(oildata, marker=\"o\", color=\"black\")\nplt.plot(fit1.fittedvalues, marker=\"o\", color=\"blue\")\n(line1,) = plt.plot(fcast1, marker=\"o\", color=\"blue\")\nplt.plot(fit2.fittedvalues, marker=\"o\", color=\"red\")\n(line2,) = plt.plot(fcast2, marker=\"o\", color=\"red\")\nplt.plot(fit3.fittedvalues, marker=\"o\", color=\"green\")\n(line3,) = plt.plot(fcast3, marker=\"o\", color=\"green\")\nplt.legend([line1, line2, line3], [fcast1.name, fcast2.name, fcast3.name])"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Condensing and Aggregating observations->Dataset with unique explanatory variables (exog)"
            ],
            [
                "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition->Factor Analysis components - FA"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Method->Seasonally adjusted data"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Simple Exponential Smoothing"
            ]
        ]
    },
    "678314": {
        "jupyter_code_cell": "ds[variable].plot()\ndf = ds[variable].to_pandas().to_frame(name=variable)\ndf.describe(percentiles=[.1,.25,.5,.75,.9])",
        "matched_tutorial_code_inds": [
            3861,
            3771,
            6158,
            6205,
            4142
        ],
        "matched_tutorial_codes": [
            "df = dd.read_parquet(\"data/indiv-*.parquet\", engine='pyarrow', columns=['occupation'])\n\nmost_common = df.occupation.value_counts().nlargest(100)\nmost_common.compute().sort_values(ascending=False)",
            "win_percent.sort_values().plot.barh(figsize=(6, 12), width=.85, color='k')\nplt.tight_layout()\nsns.despine()\nplt.xlabel(\"Win Percent\")",
            "fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nax.plot(arma_t.generate_sample(nsample=50))",
            "fig = plt.figure(figsize=(14, 10))\nax = fig.add_subplot(111)\ncf_cycles.plot(ax=ax, style=[\"r--\", \"b-\"])",
            "g = sns.PairGrid(iris)\ng.map_upper(sns.scatterplot)\ng.map_lower(sns.kdeplot)\ng.map_diag(sns.kdeplot, lw=3, legend=False)\n"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Scaling->Using Dask"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data->Simulated ARMA(4,1): Model Identification is Difficult"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Time Series Filters->Christiano-Fitzgerald approximate band-pass filter: Inflation and Unemployment"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships"
            ]
        ]
    },
    "1061813": {
        "jupyter_code_cell": "plt.bar([0,1,2,3], [10,11,12,13])\nplt.bar([0,1,2,3], [10,11,12,13], yerr=[1,2,1,3])",
        "matched_tutorial_code_inds": [
            5885,
            4721,
            6197,
            6205,
            6158
        ],
        "matched_tutorial_codes": [
            "_ = plt.boxplot([scales[0][0], scales[0][1], scales[1][0], scales[1][1]])\nplt.ylabel(\"Estimated scale\")",
            "fig2 = plt.figure()\nax2 = fig2.add_axes([0.15, 0.1, 0.7, 0.3])",
            "fig = plt.figure(figsize=(12, 10))\nax = fig.add_subplot(111)\nbk_cycles.plot(ax=ax, style=[\"r--\", \"b-\"])",
            "fig = plt.figure(figsize=(14, 10))\nax = fig.add_subplot(111)\ncf_cycles.plot(ax=ax, style=[\"r--\", \"b-\"])",
            "fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nax.plot(arma_t.generate_sample(nsample=50))"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Generalized Estimating Equations->GEE Score Tests"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Artist tutorial"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Time Series Filters->Baxter-King approximate band-pass filter: Inflation and Unemployment->Explore the hypothesis that inflation and unemployment are counter-cyclical."
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Time Series Filters->Christiano-Fitzgerald approximate band-pass filter: Inflation and Unemployment"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data->Simulated ARMA(4,1): Model Identification is Difficult"
            ]
        ]
    },
    "132599": {
        "jupyter_code_cell": "pd.DataFrame(\n    model_prob.pred_table(threshold=.5).astype('int'),\n    index=[['Actual','Actual'],['inactive','active']],\n    columns=[['Predicted','Predicted'],['inactive','active']])\nout_sample = customers.xs(2015,level='year').copy()\nout_sample['prob_predicted'] = pd.Series(model_prob.predict(out_sample),index=out_sample.index)\nout_sample.drop(['revenue','next_revenue','next_is_active'],axis=1,inplace=True)\nout_sample.head()",
        "matched_tutorial_code_inds": [
            3670,
            3643,
            5639,
            3671,
            5638
        ],
        "matched_tutorial_codes": [
            "pd.concat([temp, sped], axis=1).head()",
            "pd.DataFrame(js['features']).head().to_html()",
            "pd.concat([r.pvalues for r in results_all], axis=1, keys=names)",
            "pd.concat([temp, sped], axis=1, join='inner')",
            "pd.concat([r.bse for r in results_all], axis=1, keys=names)"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Indexes->Merging->Concat Version"
            ],
            [
                "pandas_toms_blog->Indexes"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Comparison"
            ],
            [
                "pandas_toms_blog->Indexes->Merging->Concat Version"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Comparison"
            ]
        ]
    },
    "1005762": {
        "jupyter_code_cell": "DB_PATH = 'archives/mydb_Aug13_Final'\nrows = queryTripsMinMax(DB_PATH, 1, 23000)\nformattedRows = [[dt.datetime.fromtimestamp(int(list[0])), list[1], list[2], float(list[3])]  for list in rows]\ndf = pd.DataFrame(formattedRows, columns=['datetime','highway','direction','simpleMPH'])",
        "matched_tutorial_code_inds": [
            1118,
            1092,
            4927,
            4928,
            6060
        ],
        "matched_tutorial_codes": [
            "SHAPE_COUNT = 20\ndynamic_sizes = deepcopy(input_size)\n\ninputs1: List[] = []\ninputs2: List[] = []\ngrad_outputs: List[] = []\n\n\n# Create some random shapes\nfor _ in range(SHAPE_COUNT):\n    dynamic_sizes[0] = input_size[0] + random.randrange(-2, 3)\n    dynamic_sizes[1] = input_size[1] + random.randrange(-2, 3)\n    input = (*dynamic_sizes, device=device, =, requires_grad=True)\n    inputs1.append(input)\n    inputs2.append((input))\n    grad_outputs.append((input))",
            "data_path = '~/.data/imagenet'\nsaved_model_dir = 'data/'\nfloat_model_file = 'mobilenet_pretrained_float.pth'\nscripted_float_model_file = 'mobilenet_quantization_scripted.pth'\nscripted_quantized_model_file = 'mobilenet_quantization_scripted_quantized.pth'\n\ntrain_batch_size = 30\neval_batch_size = 50\n\ndata_loader, data_loader_test = prepare_data_loaders(data_path)\ncriterion = nn.CrossEntropyLoss()\nfloat_model = load_model(saved_model_dir + float_model_file).to('cpu')\n\n# Next, we'll \"fuse modules\"; this can both make the model faster by saving on memory access\n# while also improving numerical accuracy. While this can be used with any model, this is\n# especially common with quantized models.\n\nprint('\\n Inverted Residual Block: Before fusion \\n\\n', float_model.features[1].conv)\nfloat_model.eval()\n\n# Fuses modules\nfloat_model.fuse_model()\n\n# Note fusion of Conv+BN+Relu and Conv+Relu\nprint('\\n Inverted Residual Block: After fusion\\n\\n',float_model.features[1].conv)",
            "N = 100\nX, Y = np.mgrid[-3:3:complex(0, N), -2:2:complex(0, N)]\nZ1 = np.exp(-X**2 - Y**2)\nZ2 = np.exp(-(X - 1)**2 - (Y - 1)**2)\nZ = (Z1 - Z2) * 2\n\nfig, ax = plt.subplots(2, 1)\n\npcm = ax[0].pcolormesh(X, Y, Z,\n                       norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,\n                                              vmin=-1.0, vmax=1.0, base=10),\n                       cmap='RdBu_r', shading='auto')\nfig.colorbar(pcm, ax=ax[0], extend='both')\n\npcm = ax[1].pcolormesh(X, Y, Z, cmap='RdBu_r', vmin=-np.max(Z), shading='auto')\nfig.colorbar(pcm, ax=ax[1], extend='both')\nplt.show()\n\n\n<img alt=\"colormapnorms\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_colormapnorms_003.png\" srcset=\"../../_images/sphx_glr_colormapnorms_003.png, ../../_images/sphx_glr_colormapnorms_003_2_0x.png 2.0x\"/>",
            "N = 100\nX, Y = np.mgrid[0:3:complex(0, N), 0:2:complex(0, N)]\nZ1 = (1 + np.sin(Y * 10.)) * X**2\n\nfig, ax = plt.subplots(2, 1, constrained_layout=True)\n\npcm = ax[0].pcolormesh(X, Y, Z1, norm=colors.PowerNorm(gamma=0.5),\n                       cmap='PuBu_r', shading='auto')\nfig.colorbar(pcm, ax=ax[0], extend='max')\nax[0].set_title('PowerNorm()')\n\npcm = ax[1].pcolormesh(X, Y, Z1, cmap='PuBu_r', shading='auto')\nfig.colorbar(pcm, ax=ax[1], extend='max')\nax[1].set_title('Normalize()')\nplt.show()\n\n\n<img alt=\"PowerNorm(), Normalize()\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_colormapnorms_004.png\" srcset=\"../../_images/sphx_glr_colormapnorms_004.png, ../../_images/sphx_glr_colormapnorms_004_2_0x.png 2.0x\"/>",
            "data = pdr.get_data_fred(\"HOUSTNSA\", \"1959-01-01\", \"2019-06-01\")\nhousing = data.HOUSTNSA.pct_change().dropna()\n# Scale by 100 to get percentages\nhousing = 100 * housing.asfreq(\"MS\")\nfig, ax = plt.subplots()\nax = housing.plot(ax=ax)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_6_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_6_0.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Model Optimization->Getting Started - Accelerate Your Scripts with nvFuser->nvFuser & Dynamic Shapes"
            ],
            [
                "torch->Model Optimization->(beta) Static Quantization with Eager Mode in PyTorch->3. Define dataset and data loaders->ImageNet Data"
            ],
            [
                "matplotlib->Tutorials->Colors->Colormap Normalization->Symmetric logarithmic"
            ],
            [
                "matplotlib->Tutorials->Colors->Colormap Normalization->Power-law"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions"
            ]
        ]
    },
    "1055213": {
        "jupyter_code_cell": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import Imputer",
        "matched_tutorial_code_inds": [
            2984,
            3993,
            2697,
            6306,
            6035
        ],
        "matched_tutorial_codes": [
            "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import \nfrom sklearn.neural_network import \nfrom sklearn.preprocessing import \nfrom sklearn.pipeline import \nfrom sklearn.tree import \nfrom sklearn.inspection import PartialDependenceDisplay",
            "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n",
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import",
            "import numpy as np\nimport pandas as pd\nfrom scipy.stats import norm\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport requests\nfrom io import BytesIO",
            "import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom statsmodels.stats.mediation import Mediation"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Miscellaneous->Advanced Plotting With Partial Dependence"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships",
                "seaborn->Plotting functions->Visualizing statistical relationships"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Non-negative least squares"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction"
            ],
            [
                "statsmodels->Examples->Statistics->Mediation analysis with duration data"
            ]
        ]
    },
    "312353": {
        "jupyter_code_cell": "sns.countplot(y='EmploymentStatus', data=mcq)\nsns.countplot(y='Tenure', data=mcq)",
        "matched_tutorial_code_inds": [
            5468,
            161,
            3792,
            4064,
            5032
        ],
        "matched_tutorial_codes": [
            "means25[\"LOWINC\"] = exog[\"LOWINC\"].quantile(0.25)\nprint(means25)",
            "compare.trim_significant_figures()\ncompare.colorize()\ncompare.print()",
            "sns.countplot(x='cut', data=df)\nsns.despine()\nplt.tight_layout()",
            "sns.relplot(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\nsns.rugplot(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\n",
            "ax.axis[\"right\"].set_visible(False)\nax.axis[\"top\"].set_visible(False)\n\n\n<figure class=\"align-center\">\n<img alt=\"../../_images/sphx_glr_simple_axisline3_001.png\" src=\"../../_images/sphx_glr_simple_axisline3_001.png\"/>\n</figure>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution"
            ],
            [
                "torch->PyTorch Recipes->PyTorch Benchmark->Steps->5. Comparing benchmark results"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Distribution visualization in other settings->Plotting joint and marginal distributions",
                "seaborn->Plotting functions->Visualizing distributions of data->Distribution visualization in other settings->Plotting joint and marginal distributions"
            ],
            [
                "matplotlib->Tutorials->Toolkits->The axisartist toolkit->axisartist"
            ]
        ]
    },
    "1391789": {
        "jupyter_code_cell": "yearList = [1990, 1995, 2000, 2005, 2010, 2014]\ngenreList = ['Action', 'Adventure', 'Animation', 'Biography', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Family', 'Fantasy', 'History', 'Horror', 'Musical', 'Mystery' ,'Romance', 'Sci-Fi', 'Sport', 'Thriller', 'War', 'Western']",
        "matched_tutorial_code_inds": [
            1513,
            78,
            1204,
            1762,
            3698
        ],
        "matched_tutorial_codes": [
            "year:\t\t [1971. 1972. 1973. 1974. 1974. 1974. 1974. 1975. 1975. 1975.]\ntrans. cnt:\t [2250. 3500. 2500. 3000. 4100. 6000. 8000. 4528. 4000. 5000.]",
            "batch_size = 512 # Try, for example, 128, 256, 513.\nin_size = 4096\nout_size = 4096\nnum_layers = 3\nnum_batches = 50\nepochs = 3\n\n# Creates data in default precision.\n# The same data is used for both default and mixed precision trials below.\n# You don't need to manually change inputs' dtype when enabling mixed precision.\ndata = [(batch_size, in_size, device=\"cuda\") for _ in range(num_batches)]\ntargets = [(batch_size, out_size, device=\"cuda\") for _ in range(num_batches)]\n\nloss_fn = ().cuda()",
            "batch_size = 32\nmax_sequence_len = 256\n = (batch_size, max_sequence_len,\n               embed_dimension, device=device, =)\nprint(\n    f\"The non compiled module runs in  {benchmark_torch_function_in_microseconds(model, ):.3f} microseconds\")\n\n\n = (model)\n# Let's compile it\n()\nprint(\n    f\"The compiled module runs in  {benchmark_torch_function_in_microseconds(, ):.3f} microseconds\")",
            "hidden_dim = 64\ninput_dim = emb_matrix['memory'].shape[0]\nlearning_rate = 0.001\nepochs = 10\nparameters = initialise_params(hidden_dim,\n                               input_dim)\nv, s = initialise_mav(hidden_dim,\n                      input_dim,\n                      parameters)",
            "files = glob.glob('weather/*.csv')\ncolumns = ['station', 'date', 'tmpf', 'relh', 'sped', 'mslp',\n           'p01i', 'vsby', 'gust_mph', 'skyc1', 'skyc2', 'skyc3']\n\n# init empty DataFrame, like you might for a list\nweather = pd.DataFrame(columns=columns)\n\nfor fp in files:\n    city = pd.read_csv(fp, columns=columns)\n    weather.append(city)"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy->Loading historical manufacturing data to your workspace"
            ],
            [
                "torch->PyTorch Recipes->Automatic Mixed Precision->A simple network"
            ],
            [
                "torch->Model Optimization->Using SDPA with torch.compile"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Training the Network"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ]
        ]
    },
    "17393": {
        "jupyter_code_cell": "np.random.seed(1)\ncats,n,m = 4,80,1000\ncnodes = pd.concat([\n           pd.DataFrame.from_records([(\"node\"+str(i+100*c),\"c\"+str(c)) for i in range(n)], \n                        columns=['name','cat']) \n             for c in range(cats)], ignore_index=True)\ncnodes.cat=cnodes.cat.astype('category')\ncedges = pd.concat([\n           pd.DataFrame(np.random.randint(n*c,n*(c+1), size=(m, 2)), \n                        columns=['source', 'target'])\n         for c in range(cats)], ignore_index=True)\nrd = random_layout(     cnodes, cedges)\nfd = forceatlas2_layout(cnodes, cedges)\ntf.Images(rd_d,fd_d,rd_b,fd_b).cols(2)",
        "matched_tutorial_code_inds": [
            4630,
            5833,
            4796,
            4795,
            3916
        ],
        "matched_tutorial_codes": [
            "np.random.seed(19680801)  # seed the random number generator.\ndata = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nfig, ax = plt.subplots(figsize=(5, 2.7), layout='constrained')\nax.scatter('a', 'b', c='c', s='d', data=data)\nax.set_xlabel('entry a')\nax.set_ylabel('entry b')\n\n\n<img alt=\"quick start\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_quick_start_002.png\" srcset=\"../../_images/sphx_glr_quick_start_002.png, ../../_images/sphx_glr_quick_start_002_2_0x.png 2.0x\"/>",
            "np.random.seed(12345)\nnobs = 200\nbeta_true = np.array([3, 1, 2.5, 3, -4])\nX = np.random.uniform(-20, 20, size=(nobs, len(beta_true) - 1))\n# stack a constant in front\nX = sm.add_constant(X, prepend=True)  # np.c_[np.ones(nobs), X]\nmc_iter = 500\ncontaminate = 0.25  # percentage of response variables to contaminate",
            "plt.rcParams['figure.constrained_layout.use'] = False\nfig = plt.figure(layout=\"constrained\")\n\ngs1 = gridspec.GridSpec(2, 1, figure=fig)\nax1 = fig.add_subplot(gs1[0])\nax2 = fig.add_subplot(gs1[1])\n\nexample_plot(ax1)\nexample_plot(ax2)\n\n\n<img alt=\"Title, Title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_constrainedlayout_guide_019.png\" srcset=\"../../_images/sphx_glr_constrainedlayout_guide_019.png, ../../_images/sphx_glr_constrainedlayout_guide_019_2_0x.png 2.0x\"/>",
            "plt.rcParams['figure.constrained_layout.use'] = True\nfig, axs = plt.subplots(2, 2, figsize=(3, 3))\nfor ax in axs.flat:\n    example_plot(ax)\n\n\n<img alt=\"Title, Title, Title, Title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_constrainedlayout_guide_018.png\" srcset=\"../../_images/sphx_glr_constrainedlayout_guide_018.png, ../../_images/sphx_glr_constrainedlayout_guide_018_2_0x.png 2.0x\"/>",
            "sns.set_theme(style=\"ticks\", font_scale=1.25)\ng = sns.relplot(\n    data=penguins,\n    x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"body_mass_g\",\n    palette=\"crest\", marker=\"x\", s=100,\n)\ng.set_axis_labels(\"Bill length (mm)\", \"Bill depth (mm)\", labelpad=10)\ng.legend.set_title(\"Body mass (g)\")\ng.figure.set_size_inches(6.5, 4.5)\ng.ax.margins(.15)\ng.despine(trim=True)\n"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Introductory->Quick start guide->Types of inputs to plotting functions"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Exercise: Breakdown points of M-estimator"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Constrained Layout Guide->Use with GridSpec"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Constrained Layout Guide->rcParams"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->Opinionated defaults and flexible customization"
            ]
        ]
    },
    "414066": {
        "jupyter_code_cell": "X_train['TIME_LEFT']=np.where(X_train.MONTH_SILVER > 8, 12-(X_train.MONTH_SILVER-8),8-X_train.MONTH_SILVER)\nX_train['STILL_TIME']=np.where(( (X_train.CUM_SILVER_FY < 6) & (X_train.TIME_LEFT >= (6-X_train.CUM_SILVER_FY))) | ((X_train.CUM_SILVER_FY >= 6) & (X_train.TIME_LEFT >= (12-X_train.CUM_SILVER_FY))) ,1,0)\ncntry_apps=pd.read_csv( 's3://migrationmodeldigitalproto/silvergold/data/inputs/country_apps.csv')\nadditional_imc_dat=pd.read_csv( 's3://migrationmodeldigitalproto/silvergold/data/inputs/additional_imc_dat.csv')\nprint(X_train.shape)\nX_train=pd.merge(X_train,cntry_apps, right_on=['MO_YR_KEY_NO', 'CNTRY_KEY_NO'],left_on=['MO_YR_KEY_NO', 'CNTRY_KEY_NO'],how=\"left\")\nprint(X_train.shape)\nX_train=pd.merge(X_train,additional_imc_dat, right_on=['MO_YR_KEY_NO', 'IMC_KEY_NO'],left_on=['MO_YR_KEY_NO', 'IMC_KEY_NO'],how=\"left\")\nprint(X_train.shape)\nX_train=X_train.fillna(0)\nX_train=X_train.replace(np.inf, 0)\nX_test['TIME_LEFT']=np.where(X_test.MONTH_SILVER > 8, 12-(X_test.MONTH_SILVER-8),8-X_test.MONTH_SILVER)\nX_test['STILL_TIME']=np.where(( (X_test.CUM_SILVER_FY < 6) & (X_test.TIME_LEFT >= (6-X_test.CUM_SILVER_FY))) | ((X_test.CUM_SILVER_FY >= 6) & (X_test.TIME_LEFT >= (12-X_test.CUM_SILVER_FY))) ,1,0)\nprint(X_test.shape)\nX_test=pd.merge(X_test,cntry_apps, right_on=['MO_YR_KEY_NO', 'CNTRY_KEY_NO'],left_on=['MO_YR_KEY_NO', 'CNTRY_KEY_NO'],how=\"left\")\nprint(X_test.shape)\nX_test=pd.merge(X_test,additional_imc_dat, right_on=['MO_YR_KEY_NO', 'IMC_KEY_NO'],left_on=['MO_YR_KEY_NO', 'IMC_KEY_NO'],how=\"left\")\nprint(X_test.shape)\nX_test=X_test.fillna(0)\nX_test=X_test.replace(np.inf, 0)\nkeep_vars2=['PRCNT_PV_CUST',\n,\n,\n,\n,      \n,\n,         \n,\n,        \n,\n  ,     \n,\n,\n,\n,\n,\n,\n,\n,\n,\n,\n,        \n,\n,\n,\n, \n,\n,\n    'STILL_TIME',\n    'RATIO_APPS_3_14',\n       'PROP_LAST_3DAY_PV_LC_AMT_3', \n        'PROP_RTURN_PV_LC_AMT_3',\n       'AVG_DMD_ORD_CNT_3',\n            'APP_1_AGE',\n            'APP_2_POP', \n            'LPY_FLAGS', \n            'CONTRIBUTORS'\n           ]             ",
        "matched_tutorial_code_inds": [
            2119,
            2826,
            2118,
            5917,
            1523
        ],
        "matched_tutorial_codes": [
            "dict_pos_code_estimator = (\n    n_components=n_components,\n    alpha=0.1,\n    max_iter=50,\n    batch_size=3,\n    fit_algorithm=\"cd\",\n    random_state=rng,\n    positive_code=True,\n)\ndict_pos_code_estimator.fit(faces_centered)\nplot_gallery(\n    \"Dictionary learning - positive code\",\n    dict_pos_code_estimator.components_[:n_components],\n    cmap=plt.cm.RdBu,\n)\n\n\n<img alt=\"Dictionary learning - positive code\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_012.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_012.png\"/>",
            "X_train_preprocessed = (\n    model[:-1].transform(X_train), columns=feature_names\n)\n\nX_train_preprocessed.std(axis=0).plot.barh(figsize=(9, 7))\n(\"Feature ranges\")\n(\"Std. dev. of feature values\")\n(left=0.3)\n\n\n<img alt=\"Feature ranges\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_004.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_004.png\"/>",
            "dict_pos_dict_estimator = (\n    n_components=n_components,\n    alpha=0.1,\n    max_iter=50,\n    batch_size=3,\n    random_state=rng,\n    positive_dict=True,\n)\ndict_pos_dict_estimator.fit(faces_centered)\nplot_gallery(\n    \"Dictionary learning - positive dictionary\",\n    dict_pos_dict_estimator.components_[:n_components],\n    cmap=plt.cm.RdBu,\n)\n\n\n<img alt=\"Dictionary learning - positive dictionary\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_011.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_011.png\"/>",
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "transistor_count_predicted = np.exp(B) * np.exp(A * year)\ntransistor_Moores_law = Moores_law(year)\nplt.style.use(\"fivethirtyeight\")\nplt.semilogy(year, transistor_count, \"s\", label=\"MOS transistor count\")\nplt.semilogy(year, transistor_count_predicted, label=\"linear regression\")\n\n\nplt.plot(year, transistor_Moores_law, label=\"Moore's Law\")\nplt.title(\n    \"MOS transistor count per microprocessor\\n\"\n    + \"every two years \\n\"\n    + \"Transistor count was x{:.2f} higher\".format(np.exp(A * 2))\n)\nplt.xlabel(\"year introduced\")\nplt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\nplt.ylabel(\"# of transistors\\nper microprocessor\")"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition: Dictionary learning->Dictionary learning - positive code"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition: Dictionary learning->Dictionary learning - positive dictionary"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy->Calculating the historical growth curve for transistors"
            ]
        ]
    },
    "606145": {
        "jupyter_code_cell": "by_team17.plot.scatter(x = 'Percentage', y = 'Avg_Viewers', title = 'Win Percentage vs Average Team Viewership')\nby_team17.plot.scatter(x = 'Votes', y = 'Avg_Viewers', title = 'All Star Votes vs Average Team Viewership')",
        "matched_tutorial_code_inds": [
            2339,
            3770,
            3951,
            6835,
            3829
        ],
        "matched_tutorial_codes": [
            "()  # 0.95 score\n()  # 0.94 score\nAdaBoost(DecisionTree(max_depth=3))  # 0.94 score\nDecisionTree(max_depth=None)  # 0.94 score",
            "team\nAtlanta Hawks        0.585366\nBoston Celtics       0.585366\nBrooklyn Nets        0.256098\nCharlotte Hornets    0.585366\nChicago Bulls        0.512195\ndtype: float64",
            "flights_array = flights_wide.to_numpy()\nsns.relplot(data=flights_array, kind=\"line\")\n",
            "# Compute the root mean square error\nrmse = (flattened**2).mean(axis=1)**0.5\n\nprint(rmse)",
            "ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Ensemble methods->Plot the decision surfaces of ensembles of trees on the iris dataset"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing wide-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing wide-form data"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example",
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example->Using extend"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ]
        ]
    },
    "492406": {
        "jupyter_code_cell": "X = words\ny = jobs.ifSenior.values\nXtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=3)\nprint 'baseline', 1- float(y.sum())/len(y)\nclf = RandomForestClassifier(max_depth=2, random_state=0)\nclf.fit(Xtrain, ytrain)\nyhat = clf.predict(Xtest)\nprint 'RandomForest: ', accuracy_score(ytest, yhat)\nlogr = LogisticRegression()\nlogr.fit(Xtrain, ytrain)\nyhat2 = logr.predict(Xtest)\nprint 'Logistic Reg: ', accuracy_score(ytest, yhat2)\nskb_f = SelectKBest(f_classif, k=5)\nskb_chi2 = SelectKBest(chi2, k=5)\nskb_f.fit(X, y)\nskb_chi2.fit(X, y)\nkbest = pd.DataFrame([list(X.columns), list(skb_f.scores_), list(skb_chi2.scores_)], \n                     index=['feature','f_classif','chi2 score']).T.sort_values('f_classif', ascending=False)\nkbest.head(10)\ny = jobs.salarylevel.values\nX = jobs.iloc[:, 5:17]\nX_train, X_test, y_train, y_test = train_test_split(jobs.title, jobs.salarylevel, train_size = .5, random_state=90)\nmodel = make_pipeline(CountVectorizer(stop_words='english'),\n                      LogisticRegression())\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nyhat_pp = model.predict_proba(X_test)",
        "matched_tutorial_code_inds": [
            93,
            3257,
            6795,
            2826,
            2330
        ],
        "matched_tutorial_codes": [
            "x = (-5, 5, 0.1).view(-1, 1)\ny = -5 * x + 0.1 * (x.size())\n\nmodel = (1, 1)\ncriterion = ()\noptimizer = (model.parameters(), lr = 0.1)\n\ndef train_model(iter):\n    for epoch in range(iter):\n        y1 = model(x)\n        loss = criterion(y1, y)\n        writer.add_scalar(\"Loss/train\", loss, epoch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\ntrain_model(10)\nwriter.flush()",
            "alphas = (-5, 1, 60)\nenet = (l1_ratio=0.7, max_iter=10000)\ntrain_errors = list()\ntest_errors = list()\nfor alpha in alphas:\n    enet.set_params(alpha=alpha)\n    enet.fit(X_train, y_train)\n    train_errors.append(enet.score(X_train, y_train))\n    test_errors.append(enet.score(X_test, y_test))\n\ni_alpha_optim = (test_errors)\nalpha_optim = alphas[i_alpha_optim]\nprint(\"Optimal regularization parameter : %s\" % alpha_optim)\n\n# Estimate the coef_ on full data with optimal regularization parameter\nenet.set_params(alpha=alpha_optim)\ncoef_ = enet.fit(X, y).coef_",
            "x1n = np.linspace(20.5, 25, 10)\nXnew = np.column_stack((x1n, np.sin(x1n), (x1n - 5) ** 2))\nXnew = sm.add_constant(Xnew)\nynewpred = olsres.predict(Xnew)  # predict out of sample\nprint(ynewpred)",
            "X_train_preprocessed = (\n    model[:-1].transform(X_train), columns=feature_names\n)\n\nX_train_preprocessed.std(axis=0).plot.barh(figsize=(9, 7))\n(\"Feature ranges\")\n(\"Std. dev. of feature values\")\n(left=0.3)\n\n\n<img alt=\"Feature ranges\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_004.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_004.png\"/>",
            "X, y = (return_X_y=True)\n\n# Train classifiers\nreg1 = (random_state=1)\nreg2 = (random_state=1)\nreg3 = ()\n\nreg1.fit(X, y)\nreg2.fit(X, y)\nreg3.fit(X, y)\n\nereg = ([(\"gb\", reg1), (\"rf\", reg2), (\"lr\", reg3)])\nereg.fit(X, y)"
        ],
        "matched_tutorial_paths": [
            [
                "torch->PyTorch Recipes->How to use TensorBoard with PyTorch->Log scalars"
            ],
            [
                "sklearn->Examples->Model Selection->Train error vs Test error->Compute train and test errors"
            ],
            [
                "statsmodels->Examples->User Notes->Prediction->Create a new sample of explanatory variables Xnew, predict and plot"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "sklearn->Examples->Ensemble methods->Plot individual and voting regression predictions->Training classifiers"
            ]
        ]
    },
    "996843": {
        "jupyter_code_cell": "plt.figure(figsize=(6,10))\nlow, high = loans_frame.open_closed_ratio.quantile([0.05, 0.95])\nax = sns.boxplot(x=\"loan_status\", y=\"open_closed_ratio\", data=loans_frame[loans_frame.open_closed_ratio.between(low,high)])\nplt.show()\ndef dateformat(earliest_cr_line_date):\n    date_split = earliest_cr_line_date.split('-')\n    if int(date_split[1]) > 18:\n        date_split[1] = '19' +  date_split[1]\n    else:\n        date_split[1] = '20' +  date_split[1]\n    return '-'.join(date_split)\nloans_frame['earliest_cr_line_mod'] = loans_frame['earliest_cr_line'].apply(dateformat)",
        "matched_tutorial_code_inds": [
            5890,
            550,
            401,
            3636,
            3772
        ],
        "matched_tutorial_codes": [
            "plt.figure(figsize=(6, 6))\nsymbols = [\"D\", \"^\"]\ncolors = [\"r\", \"g\", \"blue\"]\nfactor_groups = salary_table.groupby([\"E\", \"M\"])\nfor values, group in factor_groups:\n    i, j = values\n    plt.scatter(group[\"X\"], group[\"S\"], marker=symbols[j], color=colors[i - 1], s=144)\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "plt.figure(figsize=(10, 10))\nplt.subplot(2, 2, 1)\nplt.plot(logs[\"reward\"])\nplt.title(\"training rewards (average)\")\nplt.subplot(2, 2, 2)\nplt.plot(logs[\"step_count\"])\nplt.title(\"Max step count (training)\")\nplt.subplot(2, 2, 3)\nplt.plot(logs[\"eval reward (sum)\"])\nplt.title(\"Return (test)\")\nplt.subplot(2, 2, 4)\nplt.plot(logs[\"eval step_count\"])\nplt.title(\"Max step count (test)\")\nplt.show()\n\n\n<img alt=\"training rewards (average), Max step count (training), Return (test), Max step count (test)\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_reinforcement_ppo_001.png\" srcset=\"../_images/sphx_glr_reinforcement_ppo_001.png\"/>",
            "plt.figure(figsize=(5,5))\nplt.plot(epsilons, accuracies, \"*-\")\nplt.yticks(np.arange(0, 1.1, step=0.1))\nplt.xticks(np.arange(0, .35, step=0.05))\nplt.title(\"Accuracy vs Epsilon\")\nplt.xlabel(\"Epsilon\")\nplt.ylabel(\"Accuracy\")\nplt.show()\n\n\n<img alt=\"Accuracy vs Epsilon\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_fgsm_tutorial_001.png\" srcset=\"../_images/sphx_glr_fgsm_tutorial_001.png\"/>",
            "plt.figure(figsize=(15, 5))\n(df[['fl_date', 'tail_num', 'dep_time', 'dep_delay']]\n    .dropna()\n    .assign(hour=lambda x: x.dep_time.dt.hour)\n    .query('5 &lt; dep_delay &lt; 600')\n    .pipe((sns.boxplot, 'data'), 'hour', 'dep_delay'))\nsns.despine()",
            "plt.figure(figsize=(8, 5))\n(wins.win_pct\n    .unstack()\n    .assign(**{'Home Win % - Away %': lambda x: x.home_team - x.away_team,\n               'Overall %': lambda x: (x.home_team + x.away_team) / 2})\n     .pipe((sns.regplot, 'data'), x='Overall %', y='Home Win % - Away %')\n)\nsns.despine()\nplt.tight_layout()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "torch->Reinforcement Learning->Reinforcement Learning (PPO) with TorchRL Tutorial->Results"
            ],
            [
                "torch->Image and Video->Adversarial Example Generation->Results->Accuracy vs Epsilon"
            ],
            [
                "pandas_toms_blog->Method Chaining->Method Chaining->Application"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team"
            ]
        ]
    },
    "967051": {
        "jupyter_code_cell": "data = data[data['3 Year Volatility'] != '-']\ndata = data[data['3 Year Annualised Return'] != '-']",
        "matched_tutorial_code_inds": [
            5468,
            5176,
            2149,
            1643,
            161
        ],
        "matched_tutorial_codes": [
            "means25[\"LOWINC\"] = exog[\"LOWINC\"].quantile(0.25)\nprint(means25)",
            "print(X[:5, :])\nprint(y[:5])\nprint(groups)\nprint(dummy[:5, :])",
            "best n_components by PCA CV = 5\nbest n_components by FactorAnalysis CV = 5\nbest n_components by PCA MLE = 5\nbest n_components by PCA CV = 20\nbest n_components by FactorAnalysis CV = 5\nbest n_components by PCA MLE = 18",
            "Reaction force = [-4.16025147  0.          2.77350098]\nReaction moment = [0.         8.32050294 0.        ]",
            "compare.trim_significant_figures()\ncompare.colorize()\ncompare.print()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->OLS with dummy variables"
            ],
            [
                "sklearn->Examples->Decomposition->Model selection with Probabilistic PCA and Factor Analysis (FA)->Fit the models"
            ],
            [
                "numpy->NumPy Applications->Determining Static Equilibrium in NumPy->Finding values with physical properties"
            ],
            [
                "torch->PyTorch Recipes->PyTorch Benchmark->Steps->5. Comparing benchmark results"
            ]
        ]
    },
    "1124099": {
        "jupyter_code_cell": "print(df[discount_percentage>50]['Product code'].count(),\"items are over 50% off\")\npd.options.display.max_rows = 200\ndf[discount_percentage>50]",
        "matched_tutorial_code_inds": [
            5470,
            5233,
            459,
            5799,
            5818
        ],
        "matched_tutorial_codes": [
            "means75 = exog.mean()\nmeans75[\"LOWINC\"] = exog[\"LOWINC\"].quantile(0.75)\nprint(means75)",
            "print(res.recursive_coefficients.filtered[0])\nres.plot_recursive_coefficient(range(mod.k_exog), alpha=None, figsize=(10, 6))",
            "def categoryFromOutput():\n    top_n, top_i = .topk(1)\n    category_i = top_i[0].item()\n    return all_categories[category_i], category_i\n\nprint(categoryFromOutput())",
            "sidak = ols_model.outlier_test(\"sidak\")\nsidak.sort_values(\"unadj_p\", inplace=True)\nprint(sidak)",
            "sidak2 = ols_model.outlier_test(\"sidak\")\nsidak2.sort_values(\"unadj_p\", inplace=True)\nprint(sidak2)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 1: Copper"
            ],
            [
                "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN->Training->Preparing for Training"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Duncan\u2019s Occupational Prestige data - M-estimation for outliers"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Hertzprung Russell data for Star Cluster CYG 0B1 - Leverage Points"
            ]
        ]
    },
    "1416954": {
        "jupyter_code_cell": "import numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import Lasso\nfrom sklearn.metrics import r2_score\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\nfrom scipy.stats.stats import pearsonr\ntrain = pd.read_csv(\"data/train.csv\")\ntrain.head()",
        "matched_tutorial_code_inds": [
            2700,
            3436,
            5559,
            3322,
            5565
        ],
        "matched_tutorial_codes": [
            "from sklearn.linear_model import \n\nreg_nnls = (positive=True)\ny_pred_nnls = reg_nnls.fit(X_train, y_train).predict(X_test)\nr2_score_nnls = (y_test, y_pred_nnls)\nprint(\"NNLS R2 score\", r2_score_nnls)",
            "import numpy as np\nfrom sklearn.datasets import \n\nn_samples = 200\nX, y = (n_samples=n_samples, shuffle=False)\nouter, inner = 0, 1\nlabels = (n_samples, -1.0)\nlabels[0] = outer\nlabels[-1] = inner",
            "import numpy as np\nimport pylab\nimport seaborn as sns\nimport statsmodels.api as sm\n\nsns.set_style(\"darkgrid\")\npylab.rc(\"figure\", figsize=(16, 8))\npylab.rc(\"font\", size=14)",
            "import numpy as np\n\nfrom sklearn.compose import \nfrom sklearn.datasets import \nfrom sklearn.pipeline import \nfrom sklearn.impute import \nfrom sklearn.preprocessing import , \nfrom sklearn.linear_model import \nfrom sklearn.model_selection import , \nfrom sklearn.feature_selection import , \n\n(0)",
            "import numpy as np\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom matplotlib import pyplot as plt\n\nplt.rc(\"figure\", figsize=(16,8))\nplt.rc(\"font\", size=14)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Generalized Linear Models->Non-negative least squares"
            ],
            [
                "sklearn->Examples->Semi Supervised Classification->Label Propagation learning a complex structure"
            ],
            [
                "statsmodels->Examples->Nonparametric Statistics->Lowess Regression"
            ],
            [
                "sklearn->Examples->Pipelines and composite estimators->Column Transformer with Mixed Types"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Generalized Linear Models Overview"
            ]
        ]
    },
    "473630": {
        "jupyter_code_cell": "msno.matrix(df)\nmsno.bar(df, color=\"blue\", figsize=(30,18))",
        "matched_tutorial_code_inds": [
            161,
            4160,
            4084,
            4035,
            1458
        ],
        "matched_tutorial_codes": [
            "compare.trim_significant_figures()\ncompare.colorize()\ncompare.print()",
            "sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\nsinplot()\n",
            "sns.catplot(data=titanic, x=\"age\", y=\"deck\", errorbar=(\"pi\", 95), kind=\"bar\")\n",
            "sns.displot(penguins, x=\"flipper_length_mm\", hue=\"species\", stat=\"density\", common_norm=False)\n",
            "load_xy = np.load(\"x_y-squared.npz\")\n\nprint(load_xy.files)"
        ],
        "matched_tutorial_paths": [
            [
                "torch->PyTorch Recipes->PyTorch Benchmark->Steps->5. Comparing benchmark results"
            ],
            [
                "seaborn->User guide and tutorial->Controlling figure aesthetics->Overriding elements of the seaborn styles",
                "seaborn->Figure aesthetics->Controlling figure aesthetics->Overriding elements of the seaborn styles"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing categorical data->Estimating central tendency->Bar plots",
                "seaborn->Plotting functions->Visualizing categorical data->Estimating central tendency->Bar plots"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Normalized histogram statistics",
                "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Normalized histogram statistics"
            ],
            [
                "numpy->NumPy Features->Saving and sharing your NumPy arrays->Remove the saved arrays and load them back with NumPy\u2019s"
            ]
        ]
    },
    "1425124": {
        "jupyter_code_cell": "import pandas as pd\ndf_oz = pd.read_csv('D:\\\\TeachingMaterials\\\\BusinessAnalytics\\\\Visualization\\\\VizData\\\\ozone.csv')\ndf_con = pd.read_csv('D:\\\\TeachingMaterials\\\\BusinessAnalytics\\\\Visualization\\\\VizData\\\\construction.csv')\ndf_test = pd.read_csv('D:\\\\TeachingMaterials\\\\BusinessAnalytics\\\\Visualization\\\\VizData\\\\test.csv')",
        "matched_tutorial_code_inds": [
            2571,
            1473,
            3252,
            6721,
            4772
        ],
        "matched_tutorial_codes": [
            "import pandas as pd\n\nco2_data = co2.frame\nco2_data[\"date\"] = (co2_data[[\"year\", \"month\", \"day\"]])\nco2_data = co2_data[[\"date\", \"co2\"]].set_index(\"date\")\nco2_data.head()\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "import matplotlib.pyplot as plt\n\nselected_dates = [0, 3, 11, 13]\nplt.plot(dates, nbcases.T, \"--\")\nplt.xticks(selected_dates, dates[selected_dates])\nplt.title(\"COVID-19 cumulative cases from Jan 21 to Feb 3 2020\")",
            "import numpy as np\n\nn_uncorrelated_features = 20\nrng = (seed=0)\n# Use same number of samples as in iris and 20 features\nX_rand = rng.normal(size=(X.shape[0], n_uncorrelated_features))",
            "from patsy.contrasts import Treatment\n\nlevels = [1, 2, 3, 4]\ncontrast = Treatment(reference=0).code_without_intercept(levels)\nprint(contrast.matrix)",
            "from cycler import cycler\ncc = (cycler(color=list('rgb')) +\n      cycler(linestyle=['-', '--', '-.']))\nfor d in cc:\n    print(d)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) on Mauna Loa CO2 data->Build the dataset"
            ],
            [
                "numpy->NumPy Features->Masked Arrays->Exploring the data"
            ],
            [
                "sklearn->Examples->Model Selection->Test with permutations the significance of a classification score->Dataset"
            ],
            [
                "statsmodels->Examples->User Notes->Contrasts->Treatment (Dummy) Coding"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Styling with cycler->Cycling through multiple properties"
            ]
        ]
    },
    "311291": {
        "jupyter_code_cell": "~dup2.index.duplicated(keep='first')\ndup2.drop_duplicates(subset=index)",
        "matched_tutorial_code_inds": [
            3595,
            660,
            2312,
            4480,
            3792
        ],
        "matched_tutorial_codes": [
            "twenty_train.target_names[gs_clf.predict(['God is love'])[0]]\n'soc.religion.christian'",
            "interp = (rn18)\n(input)\nprint(interp.summary(True))",
            "gbdt_with_monotonic_cst = (monotonic_cst=[1, -1])\ngbdt_with_monotonic_cst.fit(X, y)",
            "quad(deterministic.pdf, -1e-3, 1e-3)  # warning removed\n(1.000076872229173, 0.0010625571718182458)",
            "sns.countplot(x='cut', data=df)\nsns.despine()\nplt.tight_layout()"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Tutorials->Working With Text Data->Parameter tuning using grid search"
            ],
            [
                "torch->Code Transforms with FX->(beta) Building a Simple CPU Performance Profiler with FX->Investigating the Performance of ResNet18"
            ],
            [
                "sklearn->Examples->Ensemble methods->Monotonic Constraints"
            ],
            [
                "scipy->Statistics (scipy.stats)->Building specific distributions->Making a continuous distribution, i.e., subclassing rv_continuous"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ]
        ]
    },
    "1061434": {
        "jupyter_code_cell": "under_sample_x = under_sample.iloc[:,0:11]  \nunder_sample_y= under_sample.iloc[:,-1] \nunder_sample_x1 = under_sample1.iloc[:,0:11]  \nunder_sample_y1= under_sample1.iloc[:,-1] \nunder_sample_x2 = under_sample2.iloc[:,0:11]  \nunder_sample_y2= under_sample2.iloc[:,-1] \nsys.path.append(\"/users/bhavani/appdata/roaming/python/python36/site-packages\")\nfrom imblearn.over_sampling import SMOTE\nsmt = SMOTE()\nX_up_train, y_up_train = smt.fit_sample(X_train, y_train)",
        "matched_tutorial_code_inds": [
            6337,
            5299,
            1615,
            2981,
            6527
        ],
        "matched_tutorial_codes": [
            "intercept = [\n    ar0_res.params[0],\n    sarimax_res.params[0],\n    arima_res.params[0],\n    autoreg_res.params[0],\n]\nrho_hat = [0] + [r.params[1] for r in (sarimax_res, arima_res, autoreg_res)]\nlong_run = [\n    ar0_res.params[0],\n    sarimax_res.params[0] / (1 - sarimax_res.params[1]),\n    arima_res.params[0],\n    autoreg_res.params[0] / (1 - autoreg_res.params[1]),\n]\ncols = [\"AR(0)\", \"SARIMAX\", \"ARIMA\", \"AutoReg\"]\npd.DataFrame(\n    [intercept, rho_hat, long_run],\n    columns=cols,\n    index=[\"delta-or-phi\", \"rho\", \"long-run mean\"],\n)",
            "resid1 = res_ols.resid[w == 1.0]\nvar1 = resid1.var(ddof=int(res_ols.df_model) + 1)\nresid2 = res_ols.resid[w != 1.0]\nvar2 = resid2.var(ddof=int(res_ols.df_model) + 1)\nw_est = w.copy()\nw_est[w != 1.0] = np.sqrt(var2) / np.sqrt(var1)\nres_fwls = sm.WLS(y, X, 1.0 / ((w_est ** 2))).fit()\nprint(res_fwls.summary())",
            "fourier_gaussian = ndimage.fourier_gaussian(xray_image, sigma=0.05)\n\nx_prewitt = ndimage.prewitt(fourier_gaussian, axis=0)\ny_prewitt = ndimage.prewitt(fourier_gaussian, axis=1)\n\nxray_image_canny = np.hypot(x_prewitt, y_prewitt)\n\nxray_image_canny *= 255.0 / np.max(xray_image_canny)\n\nprint(\"The data type - \", xray_image_canny.dtype)",
            "sh_lle, sh_err = (\n    sh_points, n_neighbors=12, n_components=2\n)\n\nsh_tsne = (\n    n_components=2, perplexity=40, init=\"random\", random_state=0\n).fit_transform(sh_points)\n\nfig, axs = (figsize=(8, 8), nrows=2)\naxs[0].scatter(sh_lle[:, 0], sh_lle[:, 1], c=sh_color)\naxs[0].set_title(\"LLE Embedding of Swiss-Hole\")\naxs[1].scatter(sh_tsne[:, 0], sh_tsne[:, 1], c=sh_color)\n_ = axs[1].set_title(\"t-SNE Embedding of Swiss-Hole\")\n\n\n<img alt=\"LLE Embedding of Swiss-Hole, t-SNE Embedding of Swiss-Hole\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_swissroll_004.png\" srcset=\"../../_images/sphx_glr_plot_swissroll_004.png\"/>",
            "initial_obs_cov = np.cov(y.T)\ninitial_state_cov_diag = [0.01] * mod.k_states\n\n# Update H and Q\nmod.update_variances(initial_obs_cov, initial_state_cov_diag)\n\n# Perform Kalman filtering and smoothing\n# (the [] is just an empty list that in some models might contain\n# additional parameters. Here, we don't have any additional parameters\n# so we just pass an empty list)\ninitial_res = mod.smooth([])"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Comparing trends and exogenous variables in SARIMAX, ARIMA and AutoReg"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Weighted Least Squares->Feasible Weighted Least Squares (2-stage FWLS)"
            ],
            [
                "numpy->NumPy Applications->X-ray image processing->Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters->The Canny filter"
            ],
            [
                "sklearn->Examples->Manifold learning->Swiss Roll And Swiss-Hole Reduction->Swiss-Hole"
            ],
            [
                "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Application: Bayesian analysis of a TVP-VAR model by MCMC->Preliminary investigation with ad-hoc parameters in H, Q"
            ]
        ]
    },
    "292213": {
        "jupyter_code_cell": "for n in range(0,4):\n    print('{} years later'.format(n))\n    rating_352_SP_Overall_later, best_list_352_SP_Overall_later = get_best_squad_n_n_yr_later(n, squad_352_strict, 'Spain')\n    print('Average rating: {:.1f}'.format(rating_352_SP_Overall_later))\n    print(best_list_352_SP_Overall_later)\nlist_of_countries = ['England', 'Italy', 'Spain', 'Germany', 'Brazil']\nn_yr = 15\nrating_combine = pd.DataFrame(index = range(0, n_yr), columns = list_of_countries)\nfor c in list_of_countries:\n    for n in range(0,n_yr):\n        rating_352_Overall_later, _ = get_best_squad_n_n_yr_later(n, squad_352_strict, c)\n        rating_combine[c].iloc[n] = rating_352_Overall_later\nax = rating_combine.plot(kind = 'line', figsize = (15,10), title = 'Country 3-5-2 best 11 rating by time')\nax.set_xlabel(\"n years later\")\nax.set_ylabel(\"team rating\")",
        "matched_tutorial_code_inds": [
            6258,
            6655,
            3167,
            1085,
            2162
        ],
        "matched_tutorial_codes": [
            "for fit in [fit2, fit4]:\n    pd.DataFrame(np.c_[fit.level, fit.trend]).rename(\n        columns={0: \"level\", 1: \"slope\"}\n    ).plot(subplots=True)\nplt.show()\nprint(\n    \"Figure 7.4: Level and slope components for Holt\u2019s linear trend method and the additive damped trend method.\"\n)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_0.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_0.png\"/>\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_1.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_1.png\"/>",
            "for i in range(simulated.shape[1]):\n    simulated.iloc[:, i].plot(label=\"_\", color=\"gray\", alpha=0.1)\ndf[\"mean\"].plot(label=\"mean prediction\")\ndf[\"pi_lower\"].plot(linestyle=\"--\", color=\"tab:blue\", label=\"95% interval\")\ndf[\"pi_upper\"].plot(linestyle=\"--\", color=\"tab:blue\", label=\"_\")\npred.endog.plot(label=\"data\")\nplt.legend()",
            "for i in range(n_classes):\n    fpr[i], tpr[i], _ = (y_onehot_test[:, i], y_score[:, i])\n    roc_auc[i] = (fpr[i], tpr[i])\n\nfpr_grid = (0.0, 1.0, 1000)\n\n# Interpolate all ROC curves at these points\nmean_tpr = (fpr_grid)\n\nfor i in range(n_classes):\n    mean_tpr += (fpr_grid, fpr[i], tpr[i])  # linear interpolation\n\n# Average it and compute AUC\nmean_tpr /= n_classes\n\nfpr[\"macro\"] = fpr_grid\ntpr[\"macro\"] = mean_tpr\nroc_auc[\"macro\"] = (fpr[\"macro\"], tpr[\"macro\"])\n\nprint(f\"Macro-averaged One-vs-Rest ROC AUC score:\\n{roc_auc['macro']:.2f}\")",
            "for param in model_ft.parameters():\n  param.requires_grad = True\n\nmodel_ft.to(device)  # We can fine-tune on GPU if available\n\ncriterion = nn.CrossEntropyLoss()\n\n# Note that we are training everything, so the learning rate is lower\n# Notice the smaller learning rate\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=1e-3, momentum=0.9, weight_decay=0.1)\n\n# Decay LR by a factor of 0.3 every several epochs\nexp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=5, gamma=0.3)\n\nmodel_ft_tuned = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n                             num_epochs=25, device=device)",
            "for pipe in (hist_dropped, hist_one_hot, hist_ordinal, hist_native):\n    pipe.set_params(\n        histgradientboostingregressor__max_depth=3,\n        histgradientboostingregressor__max_iter=15,\n    )\n\ndropped_result = (hist_dropped, X, y, cv=n_cv_folds, scoring=scoring)\none_hot_result = (hist_one_hot, X, y, cv=n_cv_folds, scoring=scoring)\nordinal_result = (hist_ordinal, X, y, cv=n_cv_folds, scoring=scoring)\nnative_result = (hist_native, X, y, cv=n_cv_folds, scoring=scoring)\n\nplot_results(\"Gradient Boosting on Ames Housing (few and small trees)\")\n\n()\n\n\n<img alt=\"Gradient Boosting on Ames Housing (few and small trees), Fit times (s), Mean Absolute Percentage Error\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_002.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Method->Plots of Seasonally Adjusted Data"
            ],
            [
                "statsmodels->Examples->State space models->ETS models->Predictions"
            ],
            [
                "sklearn->Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->One-vs-Rest multiclass ROC->ROC curve using the OvR macro-average"
            ],
            [
                "torch->Model Optimization->(beta) Quantized Transfer Learning for Computer Vision Tutorial->Part 2. Finetuning the Quantizable Model->Finetuning the model"
            ],
            [
                "sklearn->Examples->Ensemble methods->Categorical Feature Support in Gradient Boosting->Limiting the number of splits"
            ]
        ]
    },
    "779867": {
        "jupyter_code_cell": "omni_data = omni_data[omni_data.index > pd.Timestamp('1995-08-01')]\nprint len(omni_data)\nmissing_b_pct = np.isnan(omni_data['Bz']).sum() / float(len(omni_data))\nmissing_pressure_pct = np.isnan(omni_data['DynamicPressure']).sum() / float(len(omni_data))\nprint('Missing B percent: %s' % missing_b_pct)\nprint('Missing Dynamic Pressure percent: %s' % missing_pressure_pct)\n_ = omni_data.fillna({col: omni_data[col].mean() for col in omni_data.columns}, inplace=True)",
        "matched_tutorial_code_inds": [
            1523,
            5917,
            2575,
            1685,
            1615
        ],
        "matched_tutorial_codes": [
            "transistor_count_predicted = np.exp(B) * np.exp(A * year)\ntransistor_Moores_law = Moores_law(year)\nplt.style.use(\"fivethirtyeight\")\nplt.semilogy(year, transistor_count, \"s\", label=\"MOS transistor count\")\nplt.semilogy(year, transistor_count_predicted, label=\"linear regression\")\n\n\nplt.plot(year, transistor_Moores_law, label=\"Moore's Law\")\nplt.title(\n    \"MOS transistor count per microprocessor\\n\"\n    + \"every two years \\n\"\n    + \"Transistor count was x{:.2f} higher\".format(np.exp(A * 2))\n)\nplt.xlabel(\"year introduced\")\nplt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\nplt.ylabel(\"# of transistors\\nper microprocessor\")",
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "co2_data = co2_data.resample(\"M\").mean().dropna(axis=\"index\", how=\"any\")\nco2_data.plot()\n(\"Monthly average of CO$_2$ concentration (ppm)\")\n_ = (\n    \"Monthly average of air samples measurements\\nfrom the Mauna Loa Observatory\"\n)\n\n\n<img alt=\"Monthly average of air samples measurements from the Mauna Loa Observatory\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_co2_002.png\" srcset=\"../../_images/sphx_glr_plot_gpr_co2_002.png\"/>",
            "pollutant_data = np.loadtxt(\"air-quality-data.csv\", dtype=float, delimiter=\",\",\n                            skiprows=1, usecols=range(1, 8))\npollutants_A = pollutant_data[:, 0:5]\npollutants_B = pollutant_data[:, 5:]\n\nprint(pollutants_A.shape)\nprint(pollutants_B.shape)",
            "fourier_gaussian = ndimage.fourier_gaussian(xray_image, sigma=0.05)\n\nx_prewitt = ndimage.prewitt(fourier_gaussian, axis=0)\ny_prewitt = ndimage.prewitt(fourier_gaussian, axis=1)\n\nxray_image_canny = np.hypot(x_prewitt, y_prewitt)\n\nxray_image_canny *= 255.0 / np.max(xray_image_canny)\n\nprint(\"The data type - \", xray_image_canny.dtype)"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy->Calculating the historical growth curve for transistors"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) on Mauna Loa CO2 data->Build the dataset"
            ],
            [
                "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Building the dataset"
            ],
            [
                "numpy->NumPy Applications->X-ray image processing->Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters->The Canny filter"
            ]
        ]
    },
    "1113305": {
        "jupyter_code_cell": "label = LE()\nreligion_labeled = label.fit_transform(dems['religion']).reshape(-1,1)\nenc = OHE()\nenc.fit(religion_labeled)\nreligion_encoded = enc.transform(religion_labeled).toarray() \nreligion_df = pd.DataFrame(religion_encoded, dtype = 'int')\nreligion_df = rename_one_hot(dems, 'religion', religion_df, 'religion_')\ndems = pd.concat([dems, religion_df], axis = 1)\ndems.drop(['religion'], axis = 1, inplace = True)\nanswers = np.array('Republican Party; Republican Party; Democratic Party; Representative; Representative; Representative; Senator; Senator; Judge'.split('; '))\npk_score = []\nfor row in dems.index:\n    response = np.array(dems.loc[row, 'pk_ideo_baseline':'pk_SCJ_baseline'])\n    pk_score.append(sum(response==answers))\nold_pk_list = [col for col in dems.columns if col.startswith('pk_')]\ndems.drop(old_pk_list, axis = 1, inplace = True)\ndems.drop('pol_knowledge', axis =1, inplace = True)\ndems['pk_score'] = pk_score",
        "matched_tutorial_code_inds": [
            2114,
            5308,
            2113,
            5917,
            5998
        ],
        "matched_tutorial_codes": [
            "batch_dict_estimator = (\n    n_components=n_components, alpha=0.1, max_iter=50, batch_size=3, random_state=rng\n)\nbatch_dict_estimator.fit(faces_centered)\nplot_gallery(\"Dictionary learning\", batch_dict_estimator.components_[:n_components])\n\n\n<img alt=\"Dictionary learning\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_006.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_006.png\"/>",
            "model1 = sm.MixedLM.from_formula(\n    \"y ~ 1\",\n    re_formula=\"1\",\n    vc_formula={\"group2\": \"0 + C(group2)\"},\n    groups=\"group1\",\n    data=df,\n)\nresult1 = model1.fit()\nprint(result1.summary())",
            "batch_pca_estimator = (\n    n_components=n_components, alpha=0.1, max_iter=100, batch_size=3, random_state=rng\n)\nbatch_pca_estimator.fit(faces_centered)\nplot_gallery(\n    \"Sparse components - MiniBatchSparsePCA\",\n    batch_pca_estimator.components_[:n_components],\n)\n\n\n<img alt=\"Sparse components - MiniBatchSparsePCA\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_005.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_005.png\"/>",
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "res5 = combine_effects(eff, var_eff, method_re=\"chi2\", use_t=False)\nres5_df = res5.summary_frame()\nprint(\"method RE:\", res5.method_re)\nprint(res5.summary_frame())\nfig = res5.plot_forest()\nfig.set_figheight(8)\nfig.set_figwidth(6)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition->Dictionary learning"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Nested analysis"
            ],
            [
                "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition->Sparse components - MiniBatchSparsePCA"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Meta-analysis of proportions->changing data to have positive random effects variance"
            ]
        ]
    },
    "1435816": {
        "jupyter_code_cell": "def weightedRandomImputation(df):\n    for col in df:\n        nan_count=df[col].isnull().sum()\n        if col=='age':\n            df=handleOutlierAge(df)\n        if nan_count>0 and col!='age': \n            df_counts=df[col].value_counts() \n            Total_minus_unknown = 0\n            Total_minus_unknown = len(df[col]) - len(df_counts) \n            ratio_list=[]\n            for i in range(len(df_counts)):\n                ratio_list.append(float(df_counts[i])*100/float(Total_minus_unknown))  \n            min_ratio = min(ratio_list)  \n            ratio_list = [int(x/min_ratio) for x in ratio_list] \n            counts_list=df_counts.index.tolist() \n            pairs = list(zip(ratio_list,counts_list)) \n            df[col]=df[col].apply(lambda x: weightedRandomHelper(pairs) if(pd.isnull(x)) else x)\n        if col=='signup_flow': \n            bins = [-1,5,10,15,20,28]\n            group_names = [0,1,2,3,4]\n            df['signup_flow_bins'] = pd.cut(df['signup_flow'], bins, labels=group_names)\n    return df\ndef weightedRandomHelper(pairs):  \n    total = sum(pair[0] for pair in pairs)\n    r = randint(1, total)\n    for (weight, value) in pairs:\n        r -= weight\n        if r <= 0: return value",
        "matched_tutorial_code_inds": [
            1691,
            6528,
            5364,
            470,
            3623
        ],
        "matched_tutorial_codes": [
            "def compute_indices(pol, con):\n    bp = breakpoints[pol]\n    \n    if pol == 'CO':\n        inc = 0.1\n    else:\n        inc = 1\n    \n    if bp[0] &lt;= con &lt; bp[1]:\n        Bl = bp[0]\n        Bh = bp[1] - inc\n        Ih = AQI[1] - inc\n        Il = AQI[0]\n\n    elif bp[1] &lt;= con &lt; bp[2]:\n        Bl = bp[1]\n        Bh = bp[2] - inc\n        Ih = AQI[2] - inc\n        Il = AQI[1]\n\n    elif bp[2] &lt;= con &lt; bp[3]:\n        Bl = bp[2]\n        Bh = bp[3] - inc\n        Ih = AQI[3] - inc\n        Il = AQI[2]\n\n    elif bp[3] &lt;= con &lt; bp[4]:\n        Bl = bp[3]\n        Bh = bp[4] - inc\n        Ih = AQI[4] - inc\n        Il = AQI[3]\n\n    elif bp[4] &lt;= con &lt; bp[5]:\n        Bl = bp[4]\n        Bh = bp[5] - inc\n        Ih = AQI[5] - inc\n        Il = AQI[4]\n\n    elif bp[5] &lt;= con:\n        Bl = bp[5]\n        Bh = bp[5] + bp[4] - (2 * inc)\n        Ih = AQI[6]\n        Il = AQI[5]\n\n    else:\n        print(\"Concentration out of range!\")\n        \n    return ((Ih - Il) / (Bh - Bl)) * (con - Bl) + Il",
            "def plot_coefficients_by_equation(states):\n    fig, axes = plt.subplots(2, 2, figsize=(15, 8))\n\n    # The way we defined Z_t implies that the first 5 elements of the\n    # state vector correspond to the first variable in y_t, which is GDP growth\n    ax = axes[0, 0]\n    states.iloc[:, :5].plot(ax=ax)\n    ax.set_title('GDP growth')\n    ax.legend()\n\n    # The next 5 elements correspond to inflation\n    ax = axes[0, 1]\n    states.iloc[:, 5:10].plot(ax=ax)\n    ax.set_title('Inflation rate')\n    ax.legend();\n\n    # The next 5 elements correspond to unemployment\n    ax = axes[1, 0]\n    states.iloc[:, 10:15].plot(ax=ax)\n    ax.set_title('Unemployment equation')\n    ax.legend()\n\n    # The last 5 elements correspond to the interest rate\n    ax = axes[1, 1]\n    states.iloc[:, 15:20].plot(ax=ax)\n    ax.set_title('Interest rate equation')\n    ax.legend();\n\n    return ax\n<br/>",
            "def beanplot(data, plot_opts={}, jitter=False):\n    \"\"\"helper function to try out different plot options\"\"\"\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    plot_opts_ = {\n        \"cutoff_val\": 5,\n        \"cutoff_type\": \"abs\",\n        \"label_fontsize\": \"small\",\n        \"label_rotation\": 30,\n    }\n    plot_opts_.update(plot_opts)\n    sm.graphics.beanplot(\n        data, ax=ax, labels=labels, jitter=jitter, plot_opts=plot_opts_\n    )\n    ax.set_xlabel(\"Party identification of respondent.\")\n    ax.set_ylabel(\"Age\")",
            "def predict(input_line, n_predictions=3):\n    print('\\n&gt; %s' % input_line)\n    with ():\n         = evaluate(lineToTensor(input_line))\n\n        # Get top N categories\n        topv, topi = .topk(n_predictions, 1, True)\n        predictions = []\n\n        for i in range(n_predictions):\n            value = topv[0][i].item()\n            category_index = topi[0][i].item()\n            print('(%.2f) %s' % (value, all_categories[category_index]))\n            predictions.append([value, all_categories[category_index]])\n\npredict('Dovesky')\npredict('Jackson')\npredict('Satoshi')",
            "def read(fp):\n    df = (pd.read_csv(fp)\n            .rename(columns=str.lower)\n            .drop('unnamed: 36', axis=1)\n            .pipe(extract_city_name)\n            .pipe(time_to_datetime, ['dep_time', 'arr_time', 'crs_arr_time', 'crs_dep_time'])\n            .assign(fl_date=lambda x: pd.to_datetime(x['fl_date']),\n                    dest=lambda x: pd.Categorical(x['dest']),\n                    origin=lambda x: pd.Categorical(x['origin']),\n                    tail_num=lambda x: pd.Categorical(x['tail_num']),\n                    unique_carrier=lambda x: pd.Categorical(x['unique_carrier']),\n                    cancellation_code=lambda x: pd.Categorical(x['cancellation_code'])))\n    return df\n\ndef extract_city_name(df):\n    '''\n    Chicago, IL -&gt; Chicago for origin_city_name and dest_city_name\n    '''\n    cols = ['origin_city_name', 'dest_city_name']\n    city = df[cols].apply(lambda x: x.str.extract(\"(.*), \\w{2}\", expand=False))\n    df = df.copy()\n    df[['origin_city_name', 'dest_city_name']] = city\n    return df\n\ndef time_to_datetime(df, columns):\n    '''\n    Combine all time items into datetimes.\n\n    2014-01-01,0914 -&gt; 2014-01-01 09:14:00\n    '''\n    df = df.copy()\n    def converter(col):\n        timepart = (col.astype(str)\n                       .str.replace('\\.0$', '')  # NaNs force float dtype\n                       .str.pad(4, fillchar='0'))\n        return pd.to_datetime(df['fl_date'] + ' ' +\n                               timepart.str.slice(0, 2) + ':' +\n                               timepart.str.slice(2, 4),\n                               errors='coerce')\n    df[columns] = df[columns].apply(converter)\n    return df\n\noutput = 'data/flights.h5'\n\nif not os.path.exists(output):\n    df = read(\"data/627361791_T_ONTIME.csv\")\n    df.to_hdf(output, 'flights', format='table')\nelse:\n    df = pd.read_hdf(output, 'flights', format='table')\ndf.info()"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Calculating the Air Quality Index->Sub-indices"
            ],
            [
                "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Application: Bayesian analysis of a TVP-VAR model by MCMC->Preliminary investigation with ad-hoc parameters in H, Q"
            ],
            [
                "statsmodels->Examples->Plotting->Box Plots->Bean Plots"
            ],
            [
                "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN->Evaluating the Results->Running on User Input"
            ],
            [
                "pandas_toms_blog->Method Chaining->Method Chaining"
            ]
        ]
    },
    "23260": {
        "jupyter_code_cell": "features[-10:]\nfeature_matrix_encoded, features_encoded = ft.encode_features(feature_matrix, features)\npipeline_preprocessing = [(\"imputer\",\n                           SimpleImputer()),\n                          (\"scaler\", RobustScaler(with_centering=True))]\nfeature_matrix_encoded.tail()",
        "matched_tutorial_code_inds": [
            958,
            446,
            4905,
            55,
            6492
        ],
        "matched_tutorial_codes": [
            "transform = (\n    [(224),\n     (),\n     ((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\ntrain_set = (root='./data', train=True, download=True, transform=transform)\ntrain_loader = (train_set, batch_size=32, shuffle=True)",
            "model.to(DEVICE)\nmodel_input = model_input.to(DEVICE)\n\nprint(\"slow path:\")\nprint(\"==========\")\nwith torch.autograd.profiler.profile(use_cuda=True) as prof:\n  for i in range(ITERATIONS):\n    output = model(model_input)\nprint(prof)\n\nmodel.eval()\n\nprint(\"fast path:\")\nprint(\"==========\")\nwith torch.autograd.profiler.profile(use_cuda=True) as prof:\n  with torch.no_grad():\n    for i in range(ITERATIONS):\n      output = model(model_input)\nprint(prof)",
            "viridis.colors [[0.267004 0.004874 0.329415 1.      ]\n [0.275191 0.194905 0.496005 1.      ]\n [0.212395 0.359683 0.55171  1.      ]\n [0.153364 0.497    0.557724 1.      ]\n [0.122312 0.633153 0.530398 1.      ]\n [0.288921 0.758394 0.428426 1.      ]\n [0.626579 0.854645 0.223353 1.      ]\n [0.993248 0.906157 0.143936 1.      ]]\nviridis(range(8)) [[0.267004 0.004874 0.329415 1.      ]\n [0.275191 0.194905 0.496005 1.      ]\n [0.212395 0.359683 0.55171  1.      ]\n [0.153364 0.497    0.557724 1.      ]\n [0.122312 0.633153 0.530398 1.      ]\n [0.288921 0.758394 0.428426 1.      ]\n [0.626579 0.854645 0.223353 1.      ]\n [0.993248 0.906157 0.143936 1.      ]]\nviridis(np.linspace(0, 1, 8)) [[0.267004 0.004874 0.329415 1.      ]\n [0.275191 0.194905 0.496005 1.      ]\n [0.212395 0.359683 0.55171  1.      ]\n [0.153364 0.497    0.557724 1.      ]\n [0.122312 0.633153 0.530398 1.      ]\n [0.288921 0.758394 0.428426 1.      ]\n [0.626579 0.854645 0.223353 1.      ]\n [0.993248 0.906157 0.143936 1.      ]]",
            "transform = (\n    [(),\n     ((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\ntrainset = (root='./data', train=True,\n                                        download=True, transform=transform)\ntrainloader = (trainset, batch_size=4,\n                                          shuffle=True, num_workers=2)\n\ntestset = (root='./data', train=False,\n                                       download=True, transform=transform)\ntestloader = (testset, batch_size=4,\n                                         shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')",
            "time_s = np.s_[:50]  # After this they basically agree\nfig1 = plt.figure()\nax1 = fig1.add_subplot(111)\nidx = np.asarray(series.index)\nh1, = ax1.plot(idx[time_s], res_f.freq_seasonal[0].filtered[time_s], label='Double Freq. Seas')\nh2, = ax1.plot(idx[time_s], res_tf.seasonal.filtered[time_s], label='Mixed Domain Seas')\nh3, = ax1.plot(idx[time_s], true_seasonal_10_3[time_s], label='True Seasonal 10(3)')\nplt.legend([h1, h2, h3], ['Double Freq. Seasonal','Mixed Domain Seasonal','Truth'], loc=2)\nplt.title('Seasonal 10(3) component')\nplt.show()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_seasonal_21_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_seasonal_21_0.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Model Optimization->PyTorch Profiler With TensorBoard->Steps->1. Prepare the data and model"
            ],
            [
                "torch->Text->Fast Transformer Inference with Better Transformer->Additional Information",
                "torch->Text->Fast Transformer Inference with Better Transformer->Additional Information"
            ],
            [
                "matplotlib->Tutorials->Colors->Creating Colormaps in Matplotlib->Getting colormaps and accessing their values->ListedColormap"
            ],
            [
                "torch->PyTorch Recipes->Zeroing out gradients in PyTorch->Steps->2. Load and normalize the dataset"
            ],
            [
                "statsmodels->Examples->State space models->Seasonality in Time Series Data->Comparison of filtered estimates"
            ]
        ]
    },
    "190611": {
        "jupyter_code_cell": "%matplotlib inline\nimport seaborn as sb\nsb.pairplot(college[college.columns[0:10]], hue=\"Private\")\nsb.boxplot(x=college['Private'], y=college['Outstate'], data=college, order=[\"No\", \"Yes\"])",
        "matched_tutorial_code_inds": [
            6539,
            6518,
            3697,
            6207,
            3745
        ],
        "matched_tutorial_codes": [
            "%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pymc3 as pm\nimport statsmodels.api as sm\nimport theano\nimport theano.tensor as tt\nfrom pandas.plotting import register_matplotlib_converters\nfrom pandas_datareader.data import DataReader\n\nplt.style.use(\"seaborn\")\nregister_matplotlib_converters()",
            "%matplotlib inline\n\nfrom importlib import reload\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\nfrom scipy.stats import invwishart, invgamma\n\n# Get the macro dataset\ndta = sm.datasets.macrodata.load_pandas().data\ndta.index = pd.date_range('1959Q1', '2009Q3', freq='QS')",
            "%matplotlib inline\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nif int(os.environ.get(\"MODERN_PANDAS_EPUB\", 0)):\n    import prep # noqa\n\nsns.set_style('ticks')\nsns.set_context('talk')\npd.options.display.max_rows = 10",
            "%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\n# NBER recessions\nfrom pandas_datareader.data import DataReader\nfrom datetime import datetime\n\nusrec = DataReader(\n    \"USREC\", \"fred\", start=datetime(1947, 1, 1), end=datetime(2013, 4, 1)\n)",
            "%matplotlib inline\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nif int(os.environ.get(\"MODERN_PANDAS_EPUB\", 0)):\n    import prep # noqa\n\npd.options.display.max_rows = 10\nsns.set(style='ticks', context='talk')"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->SARIMAX estimation with Bayesian methods->Introduction->1. Import external dependencies"
            ],
            [
                "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing"
            ],
            [
                "pandas_toms_blog->Fast Pandas"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Markov switching dynamic regression"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data"
            ]
        ]
    },
    "192037": {
        "jupyter_code_cell": "import pandas as pd\ncat_retained = pd.read_csv(\"/home/data/kaggle/csv_cat_cut1.csv\", nrows=100)",
        "matched_tutorial_code_inds": [
            3993,
            1146,
            5035,
            1598,
            3784
        ],
        "matched_tutorial_codes": [
            "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n",
            "from ax.service.utils.report_utils import exp_to_df\n\ndf = exp_to_df(experiment)\ndf.head(10)",
            "import mpl_toolkits.axisartist as AA\nfrom mpl_toolkits.axes_grid1 import host_subplot\n\nhost = host_subplot(111, axes_class=AA.Axes)",
            "import os\nimport imageio\n\nDIR = \"tutorial-x-ray-image-processing\"\n\nxray_image = imageio.v3.imread(os.path.join(DIR, \"00000011_001.png\"))",
            "import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships",
                "seaborn->Plotting functions->Visualizing statistical relationships"
            ],
            [
                "torch->Model Optimization->Multi-Objective NAS with Ax->Evaluating the results"
            ],
            [
                "matplotlib->Tutorials->Toolkits->The axisartist toolkit->axisartist->axisartist with ParasiteAxes"
            ],
            [
                "numpy->NumPy Applications->X-ray image processing->Examine an X-ray with imageio"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples"
            ]
        ]
    },
    "1035636": {
        "jupyter_code_cell": "useless_data.append('db_comments_count')\nuseless_data.append('db_creators_url')",
        "matched_tutorial_code_inds": [
            1741,
            4064,
            161,
            1569,
            6804
        ],
        "matched_tutorial_codes": [
            "imdb_train = data.fetch('imdb_train.txt')\nimdb_test = data.fetch('imdb_test.txt')",
            "sns.relplot(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\nsns.rugplot(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\n",
            "compare.trim_significant_figures()\ncompare.colorize()\ncompare.print()",
            "print(training_labels[0])\nprint(training_labels[1])\nprint(training_labels[2])",
            "endog = macrodata['infl']\nendog.plot(figsize=(15, 5))"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Distribution visualization in other settings->Plotting joint and marginal distributions",
                "seaborn->Plotting functions->Visualizing distributions of data->Distribution visualization in other settings->Plotting joint and marginal distributions"
            ],
            [
                "torch->PyTorch Recipes->PyTorch Benchmark->Steps->5. Comparing benchmark results"
            ],
            [
                "numpy->NumPy Applications->Deep learning on MNIST->2. Preprocess the data->Convert the labels to floating point through categorical/one-hot encoding"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example"
            ]
        ]
    },
    "594910": {
        "jupyter_code_cell": "active_by_city = udf.groupby('city')['active'].sum() / udf.groupby('city')['active'].size()\nactive_by_city = active_by_city.sort_values()\n_ = active_by_city.plot(kind = 'bar', rot = 0)\n_ = plt.xlabel(\"\")\n_ = plt.title(\"Proportion of users active by city\")\nplt.show()\nactive_by_phone = udf.groupby('phone')['active'].sum() / udf.groupby('phone')['active'].size()\nactive_by_phone = active_by_phone.sort_values()\n_ = active_by_phone.plot(kind = 'bar', rot = 0)\n_ = plt.xlabel(\"\")\n_ = plt.title(\"Proportion of users active by type of phone\")\nplt.show()",
        "matched_tutorial_code_inds": [
            6255,
            2298,
            2088,
            6269,
            6253
        ],
        "matched_tutorial_codes": [
            "fit1 = Holt(air, initialization_method=\"estimated\").fit(\n    smoothing_level=0.8, smoothing_trend=0.2, optimized=False\n)\nfcast1 = fit1.forecast(5).rename(\"Holt's linear trend\")\nfit2 = Holt(air, exponential=True, initialization_method=\"estimated\").fit(\n    smoothing_level=0.8, smoothing_trend=0.2, optimized=False\n)\nfcast2 = fit2.forecast(5).rename(\"Exponential trend\")\nfit3 = Holt(air, damped_trend=True, initialization_method=\"estimated\").fit(\n    smoothing_level=0.8, smoothing_trend=0.2\n)\nfcast3 = fit3.forecast(5).rename(\"Additive damped trend\")\n\nplt.figure(figsize=(12, 8))\nplt.plot(air, marker=\"o\", color=\"black\")\nplt.plot(fit1.fittedvalues, color=\"blue\")\n(line1,) = plt.plot(fcast1, marker=\"o\", color=\"blue\")\nplt.plot(fit2.fittedvalues, color=\"red\")\n(line2,) = plt.plot(fcast2, marker=\"o\", color=\"red\")\nplt.plot(fit3.fittedvalues, color=\"green\")\n(line3,) = plt.plot(fcast3, marker=\"o\", color=\"green\")\nplt.legend([line1, line2, line3], [fcast1.name, fcast2.name, fcast3.name])",
            "feature_importance = reg.feature_importances_\nsorted_idx = (feature_importance)\npos = (sorted_idx.shape[0]) + 0.5\nfig = (figsize=(12, 6))\n(1, 2, 1)\n(pos, feature_importance[sorted_idx], align=\"center\")\n(pos, (diabetes.feature_names)[sorted_idx])\n(\"Feature Importance (MDI)\")\n\nresult = (\n    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_idx = result.importances_mean.argsort()\n(1, 2, 2)\n(\n    result.importances[sorted_idx].T,\n    vert=False,\n    labels=(diabetes.feature_names)[sorted_idx],\n)\n(\"Permutation Importance (test set)\")\nfig.tight_layout()\n()\n\n\n<img alt=\"Feature Importance (MDI), Permutation Importance (test set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\"/>",
            "train_scores = [clf.score(X_train, y_train) for clf in clfs]\ntest_scores = [clf.score(X_test, y_test) for clf in clfs]\n\nfig, ax = ()\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker=\"o\", label=\"test\", drawstyle=\"steps-post\")\nax.legend()\n()\n\n\n<img alt=\"Accuracy vs alpha for training and testing sets\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cost_complexity_pruning_003.png\" srcset=\"../../_images/sphx_glr_plot_cost_complexity_pruning_003.png\"/>",
            "states1 = pd.DataFrame(\n    np.c_[fit1.level, fit1.trend, fit1.season],\n    columns=[\"level\", \"slope\", \"seasonal\"],\n    index=aust.index,\n)\nstates2 = pd.DataFrame(\n    np.c_[fit2.level, fit2.trend, fit2.season],\n    columns=[\"level\", \"slope\", \"seasonal\"],\n    index=aust.index,\n)\nfig, [[ax1, ax4], [ax2, ax5], [ax3, ax6]] = plt.subplots(3, 2, figsize=(12, 8))\nstates1[[\"level\"]].plot(ax=ax1)\nstates1[[\"slope\"]].plot(ax=ax2)\nstates1[[\"seasonal\"]].plot(ax=ax3)\nstates2[[\"level\"]].plot(ax=ax4)\nstates2[[\"slope\"]].plot(ax=ax5)\nstates2[[\"seasonal\"]].plot(ax=ax6)\nplt.show()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_22_0.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_22_0.png\"/>",
            "fit1 = SimpleExpSmoothing(oildata, initialization_method=\"heuristic\").fit(\n    smoothing_level=0.2, optimized=False\n)\nfcast1 = fit1.forecast(3).rename(r\"$\\alpha=0.2$\")\nfit2 = SimpleExpSmoothing(oildata, initialization_method=\"heuristic\").fit(\n    smoothing_level=0.6, optimized=False\n)\nfcast2 = fit2.forecast(3).rename(r\"$\\alpha=0.6$\")\nfit3 = SimpleExpSmoothing(oildata, initialization_method=\"estimated\").fit()\nfcast3 = fit3.forecast(3).rename(r\"$\\alpha=%s$\" % fit3.model.params[\"smoothing_level\"])\n\nplt.figure(figsize=(12, 8))\nplt.plot(oildata, marker=\"o\", color=\"black\")\nplt.plot(fit1.fittedvalues, marker=\"o\", color=\"blue\")\n(line1,) = plt.plot(fcast1, marker=\"o\", color=\"blue\")\nplt.plot(fit2.fittedvalues, marker=\"o\", color=\"red\")\n(line2,) = plt.plot(fcast2, marker=\"o\", color=\"red\")\nplt.plot(fit3.fittedvalues, marker=\"o\", color=\"green\")\n(line3,) = plt.plot(fcast3, marker=\"o\", color=\"green\")\nplt.legend([line1, line2, line3], [fcast1.name, fcast2.name, fcast3.name])"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Method"
            ],
            [
                "sklearn->Examples->Ensemble methods->Gradient Boosting regression->Plot feature importance"
            ],
            [
                "sklearn->Examples->Decision Trees->Post pruning decision trees with cost complexity pruning->Accuracy vs alpha for training and testing sets"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Winters Seasonal->The Internals"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Simple Exponential Smoothing"
            ]
        ]
    },
    "262091": {
        "jupyter_code_cell": "print(len(cast1))\ncast1= cast1.replace('\\\\n',\" \").replace('.','')\nprint(len(cast1))\ntokenizer = RegexpTokenizer(r'\\w+')\ncast_tokens = tokenizer.tokenize(cast1)\nprint(len(cast_tokens))\nstopset = set(stopwords.words('english'))\ncast_tokens = [w for w in cast_tokens if not w in stopset]\nprint(len(cast_tokens))\nprint(len(set(cast_tokens)))\ncast_unique_words = set(cast_tokens)\ncast_counts = Counter(cast_tokens)\nindex = 0 \ntf_idf_cast = []\nfor n in cast_unique_words.union(cast_unique_words):\n    n_t = 0\n    if n in cast_unique_words:\n        n_t = n_t+1\n    word_idf = math.log10(2/n_t)    \n    tf_idf_cast.append((n, cast_counts[n]*word_idf))\nfrom PIL import Image\nfrom wordcloud import WordCloud\nsortedlist = sorted(tf_idf_cast, key = lambda x: x[1], reverse =True)\ntext = \"\"\nfor i in range(100):\n    text = text + int(sortedlist[i][1])*(sortedlist[i][0] + \" \")\nwc = WordCloud(background_color=\"white\", max_words=300, collocations = False)\nwc.generate(text)\nplt.figure(figsize=(15,10))\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show() \ncast1=\"\".join(unpopularmovie_casts)",
        "matched_tutorial_code_inds": [
            2791,
            1039,
            1521,
            1550,
            4483
        ],
        "matched_tutorial_codes": [
            "print(\n    \"Mean AvgClaim Amount per policy:              %.2f \"\n    % df_train[\"AvgClaimAmount\"].mean()\n)\nprint(\n    \"Mean AvgClaim Amount | NbClaim  0:           %.2f\"\n    % df_train[\"AvgClaimAmount\"][df_train[\"AvgClaimAmount\"]  0].mean()\n)\nprint(\n    \"Predicted Mean AvgClaim Amount | NbClaim  0: %.2f\"\n    % glm_sev.predict(X_train).mean()\n)\nprint(\n    \"Predicted Mean AvgClaim Amount (dummy) | NbClaim  0: %.2f\"\n    % dummy_sev.predict(X_train).mean()\n)",
            "print(\n    \"Sparsity in conv1.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Sparsity in conv2.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Sparsity in fc1.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Sparsity in fc2.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Sparsity in fc3.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Global sparsity: {:.2f}%\".format(\n        100. * float(\n            ( == 0)\n            + ( == 0)\n            + ( == 0)\n            + ( == 0)\n            + ( == 0)\n        )\n        / float(\n            .nelement()\n            + .nelement()\n            + .nelement()\n            + .nelement()\n            + .nelement()\n        )\n    )\n)",
            "print(\"Rate of semiconductors added on a chip every 2 years:\")\nprint(\n    \"\\tx{:.2f} +/- {:.2f} semiconductors per chip\".format(\n        np.exp((A) * 2), 2 * A * np.exp(2 * A) * 0.006\n    )\n)",
            "print(\n    \"The shape of training images: {} and training labels: {}\".format(\n        x_train.shape, y_train.shape\n    )\n)\nprint(\n    \"The shape of test images: {} and test labels: {}\".format(\n        x_test.shape, y_test.shape\n    )\n)",
            "print('mean = %6.4f, variance = %6.4f, skew = %6.4f, kurtosis = %6.4f' %\n...       normdiscrete.stats(moments='mvsk'))\nmean = -0.0000, variance = 6.3302, skew = 0.0000, kurtosis = -0.0076"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Generalized Linear Models->Tweedie regression on insurance claims->Severity Model -  Gamma distribution"
            ],
            [
                "torch->Model Optimization->Pruning Tutorial->Global pruning"
            ],
            [
                "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy->Calculating the historical growth curve for transistors"
            ],
            [
                "numpy->NumPy Applications->Deep learning on MNIST->1. Load the MNIST dataset"
            ],
            [
                "scipy->Statistics (scipy.stats)->Building specific distributions->Subclassing rv_discrete"
            ]
        ]
    },
    "539624": {
        "jupyter_code_cell": "demo05['WTINT2YR'].sum()\ntrig05['WTSAF2YR'].sum()",
        "matched_tutorial_code_inds": [
            5548,
            1248,
            4094,
            5123,
            4095
        ],
        "matched_tutorial_codes": [
            "kde = sm.nonparametric.KDEUnivariate(obs_dist)\nkde.fit()",
            "model = Net().to(rank)\nmodel = DDP(model)",
            "plot_errorbars(\"se\")\n",
            "sf.quantile(0.25)\nsf.quantile_ci(0.25)",
            "plot_errorbars(\"ci\")\n"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->A more difficult case"
            ],
            [
                "torch->Parallel and Distributed Training->Getting Started with Fully Sharded Data Parallel(FSDP)->How to use FSDP"
            ],
            [
                "seaborn->User guide and tutorial->Statistical estimation and error bars->Measures of estimate uncertainty->Standard error bars",
                "seaborn->Statistical operations->Statistical estimation and error bars->Measures of estimate uncertainty->Standard error bars"
            ],
            [
                "statsmodels->User Guide->Other Models->Methods for Survival and Duration Analysis->Survival function estimation and inference"
            ],
            [
                "seaborn->User guide and tutorial->Statistical estimation and error bars->Measures of estimate uncertainty->Confidence interval error bars",
                "seaborn->Statistical operations->Statistical estimation and error bars->Measures of estimate uncertainty->Confidence interval error bars"
            ]
        ]
    },
    "577184": {
        "jupyter_code_cell": "df.A=[1]*len(df.index)\ndf[\"F\"]=[2]*len(df.index)\nprint(df)\ndf[\"F\"]=np.nan\nprint(\"after df.A.dropna()\\n\",df.A.dropna())\nprint(\"after df.dropna()\\n\",df.dropna())\ndf.head()\nprint(df)\ndf[[\"A\",\"B\"]].drop_duplicates()\nprint(\"set the first row to na:\\n\")\ndf[0:1]=np.nan\nprint(\"after dropna:\\n\",df.dropna())\nprint(\"after fillna:\\n\",df.fillna(0))\nprint(\"df.mean\\n\",df.mean())\nvalues=dict(df.mean())\nprint(\"after fillna with mean of each column:\\n\",df.fillna(values))\ndf.head()",
        "matched_tutorial_code_inds": [
            3773,
            5611,
            5998,
            5613,
            4654
        ],
        "matched_tutorial_codes": [
            "df = df.assign(away_strength=df['away_team'].map(win_percent),\n               home_strength=df['home_team'].map(win_percent),\n               point_diff=df['home_points'] - df['away_points'],\n               rest_diff=df['home_rest'] - df['away_rest'])\ndf.head()",
            "data2 = data.copy()\ndata2[\"const\"] = 1\ndc = (\n    data2[\"affairs rate_marriage age yrs_married const\".split()]\n    .groupby(\"affairs rate_marriage age yrs_married\".split())\n    .count()\n)\ndc.reset_index(inplace=True)\ndc.rename(columns={\"const\": \"freq\"}, inplace=True)\nprint(dc.shape)\ndc.head()",
            "res5 = combine_effects(eff, var_eff, method_re=\"chi2\", use_t=False)\nres5_df = res5.summary_frame()\nprint(\"method RE:\", res5.method_re)\nprint(res5.summary_frame())\nfig = res5.plot_forest()\nfig.set_figheight(8)\nfig.set_figwidth(6)",
            "gr = data[\"affairs rate_marriage age yrs_married\".split()].groupby(\n    \"rate_marriage age yrs_married\".split()\n)\ndf_a = gr.agg([\"mean\", \"sum\", \"count\"])\n\n\ndef merge_tuple(tpl):\n    if isinstance(tpl, tuple) and len(tpl)  1:\n        return \"_\".join(map(str, tpl))\n    else:\n        return tpl\n\n\ndf_a.columns = df_a.columns.map(merge_tuple)\ndf_a.reset_index(inplace=True)\nprint(df_a.shape)\ndf_a.head()",
            "data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Condensing and Aggregating observations->Dataset with unique observations"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Meta-analysis of proportions->changing data to have positive random effects variance"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Condensing and Aggregating observations->Dataset with unique explanatory variables (exog)"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings"
            ]
        ]
    },
    "107315": {
        "jupyter_code_cell": "flag=[]\nstate=[]\nabbrev = []\nISO = []\nPostal =[]\nType = []\nCapital = []\npopulation = []\nArea = []\nfor row in right_table.findAll(\"tr\"):\n    cells = row.findAll('td')\n    if len(cells) > 0 and len(cells) == 9:\n        flag.append(cells[0].find(text=True))\n        state.append(cells[1].find(text=True))\n        abbrev.append(cells[2].find(text=True))\n        ISO.append(cells[3].find(text=True))\n        Postal.append(cells[4].find(text=True))\n        Type.append(cells[5].find(text=True))\n        Capital.append(cells[6].find(text=True))\n        population.append(cells[7].find(text=True))\n        Area.append(cells[8].find(text=True))\ndf_au = pd.DataFrame()\ndf_au[header_list[0]] = flag\ndf_au[header_list[1]] = state\ndf_au[header_list[2]]=abbrev\ndf_au[header_list[3]]=ISO\ndf_au[header_list[4]]=Postal\ndf_au[header_list[5]]=Type\ndf_au[header_list[6]]=Capital\ndf_au[header_list[7]]=population\ndf_au[header_list[8]]=Area",
        "matched_tutorial_code_inds": [
            3173,
            2096,
            2753,
            1767,
            4655
        ],
        "matched_tutorial_codes": [
            "pair_scores = []\nmean_tpr = dict()\n\nfor ix, (label_a, label_b) in enumerate(pair_list):\n\n    a_mask = y_test == label_a\n    b_mask = y_test == label_b\n    ab_mask = (a_mask, b_mask)\n\n    a_true = a_mask[ab_mask]\n    b_true = b_mask[ab_mask]\n\n    idx_a = (label_binarizer.classes_ == label_a)[0]\n    idx_b = (label_binarizer.classes_ == label_b)[0]\n\n    fpr_a, tpr_a, _ = (a_true, y_score[ab_mask, idx_a])\n    fpr_b, tpr_b, _ = (b_true, y_score[ab_mask, idx_b])\n\n    mean_tpr[ix] = (fpr_grid)\n    mean_tpr[ix] += (fpr_grid, fpr_a, tpr_a)\n    mean_tpr[ix] += (fpr_grid, fpr_b, tpr_b)\n    mean_tpr[ix] /= 2\n    mean_score = (fpr_grid, mean_tpr[ix])\n    pair_scores.append(mean_score)\n\n    fig, ax = (figsize=(6, 6))\n    (\n        fpr_grid,\n        mean_tpr[ix],\n        label=f\"Mean {label_a} vs {label_b} (AUC = {mean_score :.2f})\",\n        linestyle=\":\",\n        linewidth=4,\n    )\n    (\n        a_true,\n        y_score[ab_mask, idx_a],\n        ax=ax,\n        name=f\"{label_a} as positive class\",\n    )\n    (\n        b_true,\n        y_score[ab_mask, idx_b],\n        ax=ax,\n        name=f\"{label_b} as positive class\",\n    )\n    ([0, 1], [0, 1], \"k--\", label=\"chance level (AUC = 0.5)\")\n    (\"square\")\n    (\"False Positive Rate\")\n    (\"True Positive Rate\")\n    (f\"{target_names[idx_a]} vs {label_b} ROC curves\")\n    ()\n    ()\n\nprint(f\"Macro-averaged One-vs-One ROC AUC score:\\n{(pair_scores):.2f}\")\n\n\n\n<img alt=\"setosa vs versicolor ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_004.png\" srcset=\"../../_images/sphx_glr_plot_roc_004.png\"/>\n<img alt=\"setosa vs virginica ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_005.png\" srcset=\"../../_images/sphx_glr_plot_roc_005.png\"/>\n<img alt=\"versicolor vs virginica ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_006.png\" srcset=\"../../_images/sphx_glr_plot_roc_006.png\"/>",
            "node_indicator = clf.decision_path(X_test)\nleaf_id = clf.apply(X_test)\n\nsample_id = 0\n# obtain ids of the nodes `sample_id` goes through, i.e., row `sample_id`\nnode_index = node_indicator.indices[\n    node_indicator.indptr[sample_id] : node_indicator.indptr[sample_id + 1]\n]\n\nprint(\"Rules used to predict sample {id}:\\n\".format(id=sample_id))\nfor node_id in node_index:\n    # continue to the next node if it is a leaf node\n    if leaf_id[sample_id] == node_id:\n        continue\n\n    # check if value of the split feature for sample 0 is below threshold\n    if X_test[sample_id, feature[node_id]] &lt;= threshold[node_id]:\n        threshold_sign = \"&lt;=\"\n    else:\n        threshold_sign = \"\"\n\n    print(\n        \"decision node {node} : (X_test[{sample}, {feature}] = {value}) \"\n        \"{inequality} {threshold})\".format(\n            node=node_id,\n            sample=sample_id,\n            feature=feature[node_id],\n            value=X_test[sample_id, feature[node_id]],\n            inequality=threshold_sign,\n            threshold=threshold[node_id],\n        )\n    )",
            "quantiles = [0.05, 0.5, 0.95]\npredictions = {}\nout_bounds_predictions = (y_true_mean, dtype=)\nfor quantile in quantiles:\n    qr = (quantile=quantile, alpha=0, solver=solver)\n    y_pred = qr.fit(X, y_pareto).predict(X)\n    predictions[quantile] = y_pred\n\n    if quantile == min(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred = y_pareto\n        )\n    elif quantile == max(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred &lt;= y_pareto\n        )",
            "x_axis = []\ndata = {'positive sentiment': [], 'negative sentiment': []}\nfor speaker in predictions:\n    # The speakers will be used to label the x-axis in our plot\n    x_axis.append(speaker)\n    # number of paras with positive sentiment\n    no_pos_paras = len(predictions[speaker]['pos_paras'])\n    # number of paras with negative sentiment\n    no_neg_paras = len(predictions[speaker]['neg_paras'])\n    # Obtain percentage of paragraphs with positive predicted sentiment\n    pos_perc = no_pos_paras / (no_pos_paras + no_neg_paras)\n    # Store positive and negative percentages\n    data['positive sentiment'].append(pos_perc*100)\n    data['negative sentiment'].append(100*(1-pos_perc))\n\nindex = pd.Index(x_axis, name='speaker')\ndf = pd.DataFrame(data, index=index)\nax = df.plot(kind='bar', stacked=True)\nax.set_ylabel('percentage')\nax.legend(title='labels', bbox_to_anchor=(1, 1), loc='upper left')\nplt.show()",
            "names = ['group_a', 'group_b', 'group_c']\nvalues = [1, 10, 100]\n\nplt.figure(figsize=(9, 3))\n\nplt.subplot(131)\nplt.bar(names, values)\nplt.subplot(132)\nplt.scatter(names, values)\nplt.subplot(133)\nplt.plot(names, values)\nplt.suptitle('Categorical Plotting')\nplt.show()\n\n\n<img alt=\"Categorical Plotting\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_006.png\" srcset=\"../../_images/sphx_glr_pyplot_006.png, ../../_images/sphx_glr_pyplot_006_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->One-vs-One multiclass ROC->ROC curve using the OvO macro-average"
            ],
            [
                "sklearn->Examples->Decision Trees->Understanding the decision tree structure->Decision path"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Quantile regression->Fitting a QuantileRegressor"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Sentiment Analysis on the Speech Data"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with categorical variables"
            ]
        ]
    },
    "781315": {
        "jupyter_code_cell": "import numpy as np\nimport scipy as sp\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels import discrete\nimport re\nimport pandas as pd\nimport math \nimport csv\nimport time\nimport dateutil\nfrom datetime import datetime\nimport seaborn as sns\npd.set_option('display.width', 1000)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\npd.options.display.float_format = '{:,.2f}'.format\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\nfrom matplotlib import ticker\nfrom IPython.display import Image\nfrom IPython.core.display import HTML\nmillnames = ['',' Thousand',' Million',' Billion',' Trillion']\ndef millify(n, pos):\n    n = float(n)\n    millidx = max(0,min(len(millnames)-1,\n                        int(math.floor(0 if n == 0 else math.log10(abs(n))/3))))\n    thingtoreturn = n / 10**(3 * millidx)\n    if thingtoreturn % 1 == 0:\n        return '{:.0f}{}'.format(thingtoreturn, millnames[millidx])\n    elif thingtoreturn % 0.1 == 0:\n        return '{:.1f}{}'.format(thingtoreturn, millnames[millidx])\n    else:\n        return '{:.2f}{}'.format(thingtoreturn, millnames[millidx])\nImage(\"./exports/figures/Political Affiliation of Municipality Presidents in Serbia 2012-2016.png\", width=1000)",
        "matched_tutorial_code_inds": [
            4700,
            3384,
            5149,
            3119,
            1838
        ],
        "matched_tutorial_codes": [
            "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom cycler import cycler\nmpl.rcParams['lines.linewidth'] = 2\nmpl.rcParams['lines.linestyle'] = '--'\ndata = np.random.randn(50)\nplt.plot(data)\n\n\n<img alt=\"customizing\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_customizing_001.png\" srcset=\"../../_images/sphx_glr_customizing_001.png, ../../_images/sphx_glr_customizing_001_2_0x.png 2.0x\"/>",
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import \nfrom sklearn.model_selection import \nfrom sklearn.pipeline import \nfrom sklearn.svm import \nfrom sklearn.decomposition import , \nfrom sklearn.feature_selection import , \nfrom sklearn.preprocessing import \n\nX, y = (return_X_y=True)\n\npipe = (\n    [\n        (\"scaling\", ()),\n        # the reduce_dim stage is populated by the param_grid\n        (\"reduce_dim\", \"passthrough\"),\n        (\"classify\", (dual=False, max_iter=10000)),\n    ]\n)\n\nN_FEATURES_OPTIONS = [2, 4, 8]\nC_OPTIONS = [1, 10, 100, 1000]\nparam_grid = [\n    {\n        \"reduce_dim\": [(iterated_power=7), (max_iter=1_000)],\n        \"reduce_dim__n_components\": N_FEATURES_OPTIONS,\n        \"classify__C\": C_OPTIONS,\n    },\n    {\n        \"reduce_dim\": [()],\n        \"reduce_dim__k\": N_FEATURES_OPTIONS,\n        \"classify__C\": C_OPTIONS,\n    },\n]\nreducer_labels = [\"PCA\", \"NMF\", \"KBest(mutual_info_classif)\"]\n\ngrid = (pipe, n_jobs=1, param_grid=param_grid)\ngrid.fit(X, y)",
            "import pyarrow as pa\nimport pyarrow.parquet as pq\nimport statsmodels.formula.api as smf\n\nclass DataSet(dict):\n    def __init__(self, path):\n        self.parquet = pq.ParquetFile(path)\n\n    def __getitem__(self, key):\n        try:\n            return self.parquet.read([key]).to_pandas()[key]\n        except:\n            raise KeyError\n\nLargeData = DataSet('LargeData.parquet')\n\nres = smf.ols('Profit ~ Sugar + Power + Women', data=LargeData).fit()",
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom collections import \n\npopulations = (list)\ncommon_params = {\n    \"n_samples\": 10_000,\n    \"n_features\": 2,\n    \"n_informative\": 2,\n    \"n_redundant\": 0,\n    \"random_state\": 0,\n}\nweights = (0.1, 0.8, 6)\nweights = weights[::-1]\n\n# fit and evaluate base model on balanced classes\nX, y = (**common_params, weights=[0.5, 0.5])\nestimator = ().fit(X, y)\nlr_base = extract_score((estimator, X, y, scoring=scoring, cv=10))\npos_lr_base, pos_lr_base_std = lr_base[\"positive\"].values\nneg_lr_base, neg_lr_base_std = lr_base[\"negative\"].values",
            "import scipy\nimport numpy as np\nfrom sklearn.model_selection import \nfrom sklearn.cluster import \nfrom sklearn.datasets import \nfrom sklearn.metrics import \n\nrng = (0)\nX, y = (random_state=rng)\nX = (X)\nX_train, X_test, _, y_test = (X, y, random_state=rng)\nkmeans = (n_init=\"auto\").fit(X_train)\nprint((kmeans.predict(X_test), y_test))"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Introductory->Customizing Matplotlib with style sheets and rcParams->Runtime rc settings"
            ],
            [
                "sklearn->Examples->Pipelines and composite estimators->Selecting dimensionality reduction with Pipeline and GridSearchCV->Illustration of Pipeline and GridSearchCV"
            ],
            [
                "statsmodels->User Guide->Statistics and Tools->Working with Large Data Sets->Subsetting your data"
            ],
            [
                "sklearn->Examples->Model Selection->Class Likelihood Ratios to measure classification performance->Invariance with respect to prevalence"
            ],
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 0.23->Scalability and stability improvements to KMeans"
            ]
        ]
    },
    "918807": {
        "jupyter_code_cell": "def bq_builder(strs,outfile,legacy_sql=True):\n    query_str = \"bq query\"\n    if(legacy_sql):\n        query_str += \" --use_legacy_sql=TRUE\"\n    else:\n        query_str += \" --use_legacy_sql=FALSE\"\n    query_str += \" --format=csv\"\n    query_str += \" --max_rows=10000\"\n    query_str += \" \\\"\"\n    for substr in strs:\n        query_str += \" \"\n        query_str += substr\n    query_str += \" \\\"\"\n    query_str += \" > \"\n    query_str += outfile\n    return(query_str)\nq00_strs = [\"SELECT HOUR(trips.start_date) AS start_hour,\",\n            \"trips.subscriber_type,\"\n            \"COUNT(trips.trip_id) AS num_trips\",\n            \"FROM [bigquery-public-data:san_francisco.bikeshare_trips] AS trips\",\n            \"WHERE DAYOFWEEK(trips.start_date) BETWEEN 2 AND 6\",\n            \"GROUP BY start_hour, trips.subscriber_type\",\n            \"ORDER BY start_hour, trips.subscriber_type ASC\"]\nq00_out = \"q00.csv\"\nq00_cmd = bq_builder(q00_strs,outfile=q00_out,legacy_sql=True)\nexit_stat = os.system(q00_cmd)",
        "matched_tutorial_code_inds": [
            6276,
            2391,
            35,
            353,
            3102
        ],
        "matched_tutorial_codes": [
            "def add_stl_plot(fig, res, legend):\n    \"\"\"Add 3 plots from a second STL fit\"\"\"\n    axs = fig.get_axes()\n    comps = [\"trend\", \"seasonal\", \"resid\"]\n    for ax, comp in zip(axs[1:], comps):\n        series = getattr(res, comp)\n        if comp == \"resid\":\n            ax.plot(series, marker=\"o\", linestyle=\"none\")\n        else:\n            ax.plot(series)\n            if comp == \"trend\":\n                ax.legend(legend, frameon=False)\n\n\nstl = STL(elec_equip, period=12, robust=True)\nres_robust = stl.fit()\nfig = res_robust.plot()\nres_non_robust = STL(elec_equip, period=12, robust=False).fit()\nadd_stl_plot(fig, res_non_robust, [\"Robust\", \"Non-robust\"])\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_stl_decomposition_10_0.png\" src=\"../../../_images/examples_notebooks_generated_stl_decomposition_10_0.png\"/>",
            "def _count_nonzero_coefficients(estimator):\n    a = estimator.coef_.toarray()\n    return (a)\n\n\nconfigurations = [\n    {\n        \"estimator\": ,\n        \"tuned_params\": {\n            \"penalty\": \"elasticnet\",\n            \"alpha\": 0.001,\n            \"loss\": \"modified_huber\",\n            \"fit_intercept\": True,\n            \"tol\": 1e-1,\n            \"n_iter_no_change\": 2,\n        },\n        \"changing_param\": \"l1_ratio\",\n        \"changing_param_values\": [0.25, 0.5, 0.75, 0.9],\n        \"complexity_label\": \"non_zero coefficients\",\n        \"complexity_computer\": _count_nonzero_coefficients,\n        \"prediction_performance_computer\": ,\n        \"prediction_performance_label\": \"Hamming Loss (Misclassification Ratio)\",\n        \"postfit_hook\": lambda x: x.sparsify(),\n        \"data\": classification_data,\n        \"n_samples\": 5,\n    },\n    {\n        \"estimator\": ,\n        \"tuned_params\": {\"C\": 1e3, \"gamma\": 2**-15},\n        \"changing_param\": \"nu\",\n        \"changing_param_values\": [0.05, 0.1, 0.2, 0.35, 0.5],\n        \"complexity_label\": \"n_support_vectors\",\n        \"complexity_computer\": lambda x: len(x.support_vectors_),\n        \"data\": regression_data,\n        \"postfit_hook\": lambda x: x,\n        \"prediction_performance_computer\": ,\n        \"prediction_performance_label\": \"MSE\",\n        \"n_samples\": 15,\n    },\n    {\n        \"estimator\": ,\n        \"tuned_params\": {\n            \"loss\": \"squared_error\",\n            \"learning_rate\": 0.05,\n            \"max_depth\": 2,\n        },\n        \"changing_param\": \"n_estimators\",\n        \"changing_param_values\": [10, 25, 50, 75, 100],\n        \"complexity_label\": \"n_trees\",\n        \"complexity_computer\": lambda x: x.n_estimators,\n        \"data\": regression_data,\n        \"postfit_hook\": lambda x: x,\n        \"prediction_performance_computer\": ,\n        \"prediction_performance_label\": \"MSE\",\n        \"n_samples\": 15,\n    },\n]",
            "def print_size_of_model(model, label=\"\"):\n    (model.state_dict(), \"temp.p\")\n    size=os.path.getsize(\"temp.p\")\n    print(\"model: \",label,' \\t','Size (KB):', size/1e3)\n    os.remove('temp.p')\n    return size\n\n# compare the sizes\nf=print_size_of_model(float_lstm,\"fp32\")\nq=print_size_of_model(quantized_lstm,\"int8\")\nprint(\"{0:.2f} times smaller\".format(f/q))",
            "def preprocess(x, y):\n    return x.view(-1, 1, 28, 28).to(), y.to()\n\n\ntrain_dl, valid_dl = get_data(, , bs)\ntrain_dl = WrappedDataLoader(train_dl, preprocess)\nvalid_dl = WrappedDataLoader(valid_dl, preprocess)",
            "def get_impute_mean(X_missing, y_missing):\n    imputer = (missing_values=, strategy=\"mean\", add_indicator=True)\n    mean_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n    return mean_impute_scores.mean(), mean_impute_scores.std()\n\n\nmses_california[3], stds_california[3] = get_impute_mean(\n    X_miss_california, y_miss_california\n)\nmses_diabetes[3], stds_diabetes[3] = get_impute_mean(X_miss_diabetes, y_miss_diabetes)\nx_labels.append(\"Mean Imputation\")"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition->Robust Fitting"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Model Complexity Influence->Choose parameters"
            ],
            [
                "torch->PyTorch Recipes->Dynamic Quantization->Steps->3. Look at Model Size"
            ],
            [
                "torch->Learning PyTorch->What is torch.nn really?->Using your GPU"
            ],
            [
                "sklearn->Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score->Impute missing values with mean"
            ]
        ]
    },
    "1382528": {
        "jupyter_code_cell": "%matplotlib inline\nX_CA_H = ca[['longitude','latitude','housingMedianAge','totalRooms',\n             'totalBedrooms','population','households','medianIncome']]\nprint('Complete dataset shape is {}'.format(X_CA_H.shape))\nprint('Sample median house values:')\nprint(ca.medianHouseValue.head())\ny_CA_H = ca.medianHouseValue;\nplt.scatter(range(len(y_CA_H)), y_CA_H, c=\"slategray\", alpha=0.3, linewidths=0.2)\nplt.xlabel('Samples in Order')\nplt.ylabel('Median House Value');\nX_CA_H, y_CA_H = utils.shuffle(X_CA_H, y_CA_H, random_state=1)\nX_CA_H_train, X_CA_H_test, y_CA_H_train, y_CA_H_test = model_selection.train_test_split(\n    X_CA_H, y_CA_H, test_size=0.4, random_state=0)\nprint((X_CA_H_train.shape), y_CA_H_train.shape)\nprint((X_CA_H_test.shape), y_CA_H_test.shape)",
        "matched_tutorial_code_inds": [
            3637,
            6518,
            6207,
            3697,
            6233
        ],
        "matched_tutorial_codes": [
            "%matplotlib inline\n\nimport json\nimport glob\nimport datetime\nfrom io import StringIO\n\nimport requests\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_style('ticks')\n\n# States are broken into networks. The networks have a list of ids, each representing a station.\n# We will take that list of ids and pass them as query parameters to the URL we built up ealier.\nstates = \"\"\"AK AL AR AZ CA CO CT DE FL GA HI IA ID IL IN KS KY LA MA MD ME\n MI MN MO MS MT NC ND NE NH NJ NM NV NY OH OK OR PA RI SC SD TN TX UT VA VT\n WA WI WV WY\"\"\".split()\n\n# IEM has Iowa AWOS sites in its own labeled network\nnetworks = ['AWOS'] + ['{}_ASOS'.format(state) for state in states]",
            "%matplotlib inline\n\nfrom importlib import reload\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\nfrom scipy.stats import invwishart, invgamma\n\n# Get the macro dataset\ndta = sm.datasets.macrodata.load_pandas().data\ndta.index = pd.date_range('1959Q1', '2009Q3', freq='QS')",
            "%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\n# NBER recessions\nfrom pandas_datareader.data import DataReader\nfrom datetime import datetime\n\nusrec = DataReader(\n    \"USREC\", \"fred\", start=datetime(1947, 1, 1), end=datetime(2013, 4, 1)\n)",
            "%matplotlib inline\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nif int(os.environ.get(\"MODERN_PANDAS_EPUB\", 0)):\n    import prep # noqa\n\nsns.set_style('ticks')\nsns.set_context('talk')\npd.options.display.max_rows = 10",
            "%matplotlib inline\n\nfrom datetime import datetime\nfrom io import BytesIO\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport requests\nimport statsmodels.api as sm\n\n# NBER recessions\nfrom pandas_datareader.data import DataReader\n\nusrec = DataReader(\n    \"USREC\", \"fred\", start=datetime(1947, 1, 1), end=datetime(2013, 4, 1)\n)"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Indexes"
            ],
            [
                "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Markov switching dynamic regression"
            ],
            [
                "pandas_toms_blog->Fast Pandas"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Markov switching autoregression"
            ]
        ]
    },
    "1268717": {
        "jupyter_code_cell": "import matplotlib.pyplot as plt\n%matplotlib inline\nplt.hist(movies['Fandango_Stars'])\nplt.show()\nimport numpy\nFandango_mean = movies['Fandango_Stars'].mean()\nMetacritic_mean = movies['Metacritic_norm_round'].mean()\nFandango_median = movies['Fandango_Stars'].median()\nMetacritic_median = movies['Metacritic_norm_round'].median()\nFandango_STD = numpy.std(movies['Fandango_Stars'])\nMetacritic_STD = numpy.std(movies['Metacritic_norm_round'])\nprint(Fandango_mean, Metacritic_mean, Fandango_median, Metacritic_median, Fandango_STD, Metacritic_STD)",
        "matched_tutorial_code_inds": [
            5256,
            4650,
            4661,
            1957,
            1212
        ],
        "matched_tutorial_codes": [
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pandas_datareader as pdr\nimport seaborn\n\nimport statsmodels.api as sm\nfrom statsmodels.regression.rolling import RollingOLS\n\nseaborn.set_style(\"darkgrid\")\npd.plotting.register_matplotlib_converters()\n%matplotlib inline",
            "import matplotlib.pyplot as plt\nplt.plot([1, 2, 3, 4])\nplt.ylabel('some numbers')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_001.png\" srcset=\"../../_images/sphx_glr_pyplot_001.png, ../../_images/sphx_glr_pyplot_001_2_0x.png 2.0x\"/>",
            "import matplotlib.pyplot as plt\nplt.figure(1)                # the first figure\nplt.subplot(211)             # the first subplot in the first figure\nplt.plot([1, 2, 3])\nplt.subplot(212)             # the second subplot in the first figure\nplt.plot([4, 5, 6])\n\n\nplt.figure(2)                # a second figure\nplt.plot([4, 5, 6])          # creates a subplot() by default\n\nplt.figure(1)                # first figure current;\n                             # subplot(212) still current\nplt.subplot(211)             # make subplot(211) in the first figure\n                             # current\nplt.title('Easy as 1, 2, 3') # subplot 211 title",
            "import matplotlib.pyplot as plt\n\n(X[:, 0], X[:, 1])\n()\n\n\n<img alt=\"plot dbscan\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_dbscan_001.png\" srcset=\"../../_images/sphx_glr_plot_dbscan_001.png\"/>",
            "import matplotlib.pyplot as plt\nplt.switch_backend('Agg')\nimport numpy as np\nimport timeit\n\nnum_repeat = 10\n\nstmt = \"train(model)\"\n\nsetup = \"model = ModelParallelResNet50()\"\nmp_run_times = timeit.repeat(\n    stmt, setup, number=1, repeat=num_repeat, globals=globals())\nmp_mean, mp_std = np.mean(mp_run_times), np.std(mp_run_times)\n\nsetup = \"import torchvision.models as models;\" + \\\n        \"model = models.resnet50(num_classes=num_classes).to('cuda:0')\"\nrn_run_times = timeit.repeat(\n    stmt, setup, number=1, repeat=num_repeat, globals=globals())\nrn_mean, rn_std = np.mean(rn_run_times), np.std(rn_run_times)\n\n\ndef plot(means, stds, , fig_name):\n    fig, ax = plt.subplots()\n    ax.bar(np.arange(len(means)), means, yerr=stds,\n           align='center', alpha=0.5, ecolor='red', capsize=10, width=0.6)\n    ax.set_ylabel('ResNet50 Execution Time (Second)')\n    ax.set_xticks(np.arange(len(means)))\n    ax.set_xticklabels()\n    ax.yaxis.grid(True)\n    plt.tight_layout()\n    plt.savefig(fig_name)\n    plt.close(fig)\n\n\nplot([mp_mean, rn_mean],\n     [mp_std, rn_std],\n     ['Model Parallel', 'Single GPU'],\n     'mp_vs_rn.png')\n\n\n\n<img alt=\"\" src=\"../_images/mp_vs_rn.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Rolling Least Squares"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Introduction to pyplot"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Working with multiple figures and axes"
            ],
            [
                "sklearn->Examples->Clustering->Demo of DBSCAN clustering algorithm->Data generation"
            ],
            [
                "torch->Parallel and Distributed Training->Single-Machine Model Parallel Best Practices->Apply Model Parallel to Existing Modules"
            ]
        ]
    },
    "692454": {
        "jupyter_code_cell": "students = pd.concat([student_mat, student_por]).groupby([\"school\",\n                                                          \"sex\",\n                                                          \"age\",\n                                                          \"address\",\n                                                          \"famsize\",\n                                                          \"Pstatus\",\n                                                          \"Medu\",\n                                                          \"Fedu\",\n                                                          \"Mjob\",\n                                                          \"Fjob\",\n                                                          \"reason\",\n                                                          \"nursery\",\n                                                          \"internet\"]).mean().reset_index()\nstudents['alc'] = students['alc'].apply(np.rint)\nstudents.shape\nstudents.hist(figsize=(20, 20));",
        "matched_tutorial_code_inds": [
            5802,
            6646,
            5800,
            189,
            1740
        ],
        "matched_tutorial_codes": [
            "student_resid   unadj_p  fdr_bh(p)\nminister                 3.134519  0.003177   0.142974\nreporter                -2.397022  0.021170   0.476332\ncontractor               2.043805  0.047433   0.596233\ninsurance.agent         -1.930919  0.060428   0.596233\nmachinist                1.887047  0.066248   0.596233\nstore.clerk             -1.760491  0.085783   0.616782\nconductor               -1.704032  0.095944   0.616782\nfactory.owner            1.602429  0.116738   0.656653\nmail.carrier            -1.433249  0.159369   0.796844\nstreetcar.motorman      -1.104485  0.275823   0.999436\ncarpenter                1.068858  0.291386   0.999436\ncoal.miner               1.018527  0.314400   0.999436\nbartender               -0.902422  0.372104   0.999436\nbookkeeper              -0.902388  0.372122   0.999436\nsoda.clerk              -0.883095  0.382334   0.999436\nchemist                  0.826578  0.413261   0.999436\nRR.engineer              0.808922  0.423229   0.999436\nprofessor                0.768277  0.446725   0.999436\nelectrician              0.731949  0.468363   0.999436\ngas.stn.attendant       -0.666596  0.508764   0.999436\nauto.repairman           0.522735  0.603972   0.999436\nwatchman                -0.513502  0.610357   0.999436\nbanker                   0.508388  0.613906   0.999436\nmachine.operator         0.499922  0.619802   0.999436\ndentist                 -0.498082  0.621088   0.999436\nwaiter                  -0.475972  0.636621   0.999436\nshoe.shiner             -0.429357  0.669912   0.999436\nwelfare.worker          -0.411406  0.682918   0.999436\nplumber                 -0.377954  0.707414   0.999436\nphysician                0.355687  0.723898   0.999436\npilot                    0.340920  0.734905   0.999436\nengineer                 0.306225  0.760983   0.999436\naccountant               0.303900  0.762741   0.999436\nlawyer                  -0.303082  0.763360   0.999436\nundertaker              -0.187339  0.852319   0.999436\nbarber                   0.173805  0.862874   0.999436\nstore.manager            0.142425  0.887442   0.999436\ntruck.driver            -0.129227  0.897810   0.999436\ncook                     0.127207  0.899399   0.999436\njanitor                 -0.079890  0.936713   0.999436\npoliceman                0.078847  0.937538   0.999436\narchitect                0.072256  0.942750   0.999436\nteacher                  0.050510  0.959961   0.999436\ntaxi.driver              0.023322  0.981507   0.999436\nauthor                   0.000711  0.999436   0.999436",
            "austourists_data = [\n    30.05251300,\n    19.14849600,\n    25.31769200,\n    27.59143700,\n    32.07645600,\n    23.48796100,\n    28.47594000,\n    35.12375300,\n    36.83848500,\n    25.00701700,\n    30.72223000,\n    28.69375900,\n    36.64098600,\n    23.82460900,\n    29.31168300,\n    31.77030900,\n    35.17787700,\n    19.77524400,\n    29.60175000,\n    34.53884200,\n    41.27359900,\n    26.65586200,\n    28.27985900,\n    35.19115300,\n    42.20566386,\n    24.64917133,\n    32.66733514,\n    37.25735401,\n    45.24246027,\n    29.35048127,\n    36.34420728,\n    41.78208136,\n    49.27659843,\n    31.27540139,\n    37.85062549,\n    38.83704413,\n    51.23690034,\n    31.83855162,\n    41.32342126,\n    42.79900337,\n    55.70835836,\n    33.40714492,\n    42.31663797,\n    45.15712257,\n    59.57607996,\n    34.83733016,\n    44.84168072,\n    46.97124960,\n    60.01903094,\n    38.37117851,\n    46.97586413,\n    50.73379646,\n    61.64687319,\n    39.29956937,\n    52.67120908,\n    54.33231689,\n    66.83435838,\n    40.87118847,\n    51.82853579,\n    57.49190993,\n    65.25146985,\n    43.06120822,\n    54.76075713,\n    59.83447494,\n    73.25702747,\n    47.69662373,\n    61.09776802,\n    66.05576122,\n]\nindex = pd.date_range(\"1999-03-01\", \"2015-12-01\", freq=\"3MS\")\naustourists = pd.Series(austourists_data, index=index)\naustourists.plot()\nplt.ylabel(\"Australian Tourists\")",
            "student_resid   unadj_p  sidak(p)\nminister                 3.134519  0.003177  0.133421\nreporter                -2.397022  0.021170  0.618213\ncontractor               2.043805  0.047433  0.887721\ninsurance.agent         -1.930919  0.060428  0.939485\nmachinist                1.887047  0.066248  0.954247\nstore.clerk             -1.760491  0.085783  0.982331\nconductor               -1.704032  0.095944  0.989315\nfactory.owner            1.602429  0.116738  0.996250\nmail.carrier            -1.433249  0.159369  0.999595\nstreetcar.motorman      -1.104485  0.275823  1.000000\ncarpenter                1.068858  0.291386  1.000000\ncoal.miner               1.018527  0.314400  1.000000\nbartender               -0.902422  0.372104  1.000000\nbookkeeper              -0.902388  0.372122  1.000000\nsoda.clerk              -0.883095  0.382334  1.000000\nchemist                  0.826578  0.413261  1.000000\nRR.engineer              0.808922  0.423229  1.000000\nprofessor                0.768277  0.446725  1.000000\nelectrician              0.731949  0.468363  1.000000\ngas.stn.attendant       -0.666596  0.508764  1.000000\nauto.repairman           0.522735  0.603972  1.000000\nwatchman                -0.513502  0.610357  1.000000\nbanker                   0.508388  0.613906  1.000000\nmachine.operator         0.499922  0.619802  1.000000\ndentist                 -0.498082  0.621088  1.000000\nwaiter                  -0.475972  0.636621  1.000000\nshoe.shiner             -0.429357  0.669912  1.000000\nwelfare.worker          -0.411406  0.682918  1.000000\nplumber                 -0.377954  0.707414  1.000000\nphysician                0.355687  0.723898  1.000000\npilot                    0.340920  0.734905  1.000000\nengineer                 0.306225  0.760983  1.000000\naccountant               0.303900  0.762741  1.000000\nlawyer                  -0.303082  0.763360  1.000000\nundertaker              -0.187339  0.852319  1.000000\nbarber                   0.173805  0.862874  1.000000\nstore.manager            0.142425  0.887442  1.000000\ntruck.driver            -0.129227  0.897810  1.000000\ncook                     0.127207  0.899399  1.000000\njanitor                 -0.079890  0.936713  1.000000\npoliceman                0.078847  0.937538  1.000000\narchitect                0.072256  0.942750  1.000000\nteacher                  0.050510  0.959961  1.000000\ntaxi.driver              0.023322  0.981507  1.000000\nauthor                   0.000711  0.999436  1.000000",
            "classes = [\n    \"T-shirt/top\",\n    \"Trouser\",\n    \"Pullover\",\n    \"Dress\",\n    \"Coat\",\n    \"Sandal\",\n    \"Shirt\",\n    \"Sneaker\",\n    \"Bag\",\n    \"Ankle boot\",\n]\n\n()\n, y = [0][0], [0][1]\nwith ():\n     = model()\n    predicted, actual = classes[[0].argmax(0)], classes[y]\n    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')",
            "data = pooch.create(\n    # folder where the data will be stored in the\n    # default cache folder of your Operating System\n    path=pooch.os_cache(\"numpy-nlp-tutorial\"),\n    # Base URL of the remote data store\n    base_url=\"\",\n    # The cache file registry. A dictionary with all files managed by this pooch.\n    # The keys are the file names and values are their respective hash codes which\n    # ensure we download the same, uncorrupted file each time.\n    registry={\n        \"imdb_train.txt\": \"6a38ea6ab5e1902cc03f6b9294ceea5e8ab985af991f35bcabd301a08ea5b3f0\",\n         \"imdb_test.txt\": \"7363ef08ad996bf4233b115008d6d7f9814b7cc0f4d13ab570b938701eadefeb\",\n        \"glove.6B.50d.zip\": \"617afb2fe6cbd085c235baf7a465b96f4112bd7f7ccb2b2cbd649fed9cbcf2fb\",\n    },\n    # Now specify custom URLs for some of the files in the registry.\n    urls={\n        \"imdb_train.txt\": \"doi:10.5281/zenodo.4117827/imdb_train.txt\",\n        \"imdb_test.txt\": \"doi:10.5281/zenodo.4117827/imdb_test.txt\",\n        \"glove.6B.50d.zip\": 'https://nlp.stanford.edu/data/glove.6B.zip'\n    }\n)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Duncan\u2019s Occupational Prestige data - M-estimation for outliers"
            ],
            [
                "statsmodels->Examples->State space models->ETS models->Holt-Winters\u2019 seasonal method"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Duncan\u2019s Occupational Prestige data - M-estimation for outliers"
            ],
            [
                "torch->Introduction to PyTorch->Quickstart->Loading Models"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ]
        ]
    },
    "225745": {
        "jupyter_code_cell": "mov2 = mov[(mov.Genre == 'action') | (mov.Genre == 'adventure') | (mov.Genre == 'animation') | (mov.Genre == 'comedy') | (mov.Genre == 'drama')]\nmov3 = mov2[(mov2.Studio == 'Buena Vista Studios') | (mov2.Studio == 'Fox') | (mov2.Studio == 'Paramount Pictures') | (mov2.Studio == 'Sony') | (mov2.Studio == 'Universal') | (mov2.Studio == 'WB')]",
        "matched_tutorial_code_inds": [
            1745,
            3756,
            1095,
            2575,
            5917
        ],
        "matched_tutorial_codes": [
            "speech_data_path = 'tutorial-nlp-from-scratch/speeches.csv'\nspeech_df = pd.read_csv(speech_data_path)\nX_pred = textproc.cleantext(speech_df,\n                            text_column='speech',\n                            remove_stopwords=True,\n                            remove_punc=False)\nspeakers = speech_df['speaker'].to_numpy()",
            "delta = (by_game.home_rest - by_game.away_rest).dropna().astype(int)\nax = (delta.value_counts()\n    .reindex(np.arange(delta.min(), delta.max() + 1), fill_value=0)\n    .sort_index()\n    .plot(kind='bar', color='k', width=.9, rot=0, figsize=(12, 6))\n)\nsns.despine()\nax.set(xlabel='Difference in Rest (Home - Away)', ylabel='Games');",
            "per_channel_quantized_model = load_model(saved_model_dir + float_model_file)\nper_channel_quantized_model.eval()\nper_channel_quantized_model.fuse_model()\n# The old 'fbgemm' is still available but 'x86' is the recommended default.\nper_channel_quantized_model.qconfig = torch.ao.quantization.get_default_qconfig('x86')\nprint(per_channel_quantized_model.qconfig)\n\ntorch.ao.quantization.prepare(per_channel_quantized_model, inplace=True)\nevaluate(per_channel_quantized_model,criterion, data_loader, num_calibration_batches)\ntorch.ao.quantization.convert(per_channel_quantized_model, inplace=True)\ntop1, top5 = evaluate(per_channel_quantized_model, criterion, data_loader_test, neval_batches=num_eval_batches)\nprint('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\ntorch.jit.save(torch.jit.script(per_channel_quantized_model), saved_model_dir + scripted_quantized_model_file)",
            "co2_data = co2_data.resample(\"M\").mean().dropna(axis=\"index\", how=\"any\")\nco2_data.plot()\n(\"Monthly average of CO$_2$ concentration (ppm)\")\n_ = (\n    \"Monthly average of air samples measurements\\nfrom the Mauna Loa Observatory\"\n)\n\n\n<img alt=\"Monthly average of air samples measurements from the Mauna Loa Observatory\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_co2_002.png\" srcset=\"../../_images/sphx_glr_plot_gpr_co2_002.png\"/>",
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data"
            ],
            [
                "torch->Model Optimization->(beta) Static Quantization with Eager Mode in PyTorch->4. Post-training static quantization"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) on Mauna Loa CO2 data->Build the dataset"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ]
        ]
    },
    "587296": {
        "jupyter_code_cell": "def extract_features(imgs, cspace='RGB', orient=9, \n                        pix_per_cell=8, cell_per_block=2, hog_channel=0):\n    features = []\n    for file in imgs:\n        image = mpimg.imread(file)\n        if cspace != 'RGB':\n            if cspace == 'HSV':\n                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n            elif cspace == 'LUV':\n                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2LUV)\n            elif cspace == 'HLS':\n                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n            elif cspace == 'YUV':\n                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)\n            elif cspace == 'YCrCb':\n                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2YCrCb)\n        else: feature_image = np.copy(image)      \n        if hog_channel == 'ALL':\n            hog_features = []\n            for channel in range(feature_image.shape[2]):\n                hog_features.append(get_hog_features(feature_image[:,:,channel], \n                                    orient, pix_per_cell, cell_per_block, \n                                    vis=False, feature_vec=True))\n            hog_features = np.ravel(hog_features)        \n        else:\n            hog_features = get_hog_features(feature_image[:,:,hog_channel], orient, \n                        pix_per_cell, cell_per_block, vis=False, feature_vec=True)\n        features.append(hog_features)\n    return features\ncar_list = glob.glob('datasets/vehicles/**/*.png')\nncar_list = glob.glob('datasets/non-vehicles/**/*.png')\nprint(len(car_list), len(ncar_list))",
        "matched_tutorial_code_inds": [
            3128,
            5724,
            380,
            3836,
            2480
        ],
        "matched_tutorial_codes": [
            "def make_heatmap(ax, gs, is_sh=False, make_cbar=False):\n    \"\"\"Helper to make a heatmap.\"\"\"\n    results = (gs.cv_results_)\n    results[[\"param_C\", \"param_gamma\"]] = results[[\"param_C\", \"param_gamma\"]].astype(\n        \n    )\n    if is_sh:\n        # SH dataframe: get mean_test_score values for the highest iter\n        scores_matrix = results.sort_values(\"iter\").pivot_table(\n            index=\"param_gamma\",\n            columns=\"param_C\",\n            values=\"mean_test_score\",\n            aggfunc=\"last\",\n        )\n    else:\n        scores_matrix = results.pivot(\n            index=\"param_gamma\", columns=\"param_C\", values=\"mean_test_score\"\n        )\n\n    im = ax.imshow(scores_matrix)\n\n    ax.set_xticks((len(Cs)))\n    ax.set_xticklabels([\"{:.0E}\".format(x) for x in Cs])\n    ax.set_xlabel(\"C\", fontsize=15)\n\n    ax.set_yticks((len(gammas)))\n    ax.set_yticklabels([\"{:.0E}\".format(x) for x in gammas])\n    ax.set_ylabel(\"gamma\", fontsize=15)\n\n    # Rotate the tick labels and set their alignment.\n    (ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n\n    if is_sh:\n        iterations = results.pivot_table(\n            index=\"param_gamma\", columns=\"param_C\", values=\"iter\", aggfunc=\"max\"\n        ).values\n        for i in range(len(gammas)):\n            for j in range(len(Cs)):\n                ax.text(\n                    j,\n                    i,\n                    iterations[i, j],\n                    ha=\"center\",\n                    va=\"center\",\n                    color=\"w\",\n                    fontsize=20,\n                )\n\n    if make_cbar:\n        fig.subplots_adjust(right=0.8)\n        cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n        fig.colorbar(im, cax=cbar_ax)\n        cbar_ax.set_ylabel(\"mean_test_score\", rotation=-90, va=\"bottom\", fontsize=15)\n\n\nfig, axes = (ncols=2, sharey=True)\nax1, ax2 = axes\n\nmake_heatmap(ax1, gsh, is_sh=True)\nmake_heatmap(ax2, gs, make_cbar=True)\n\nax1.set_title(\"Successive Halving\\ntime = {:.3f}s\".format(gsh_time), fontsize=15)\nax2.set_title(\"GridSearch\\ntime = {:.3f}s\".format(gs_time), fontsize=15)\n\n()\n\n\n<img alt=\"Successive Halving time = 1.240s, GridSearch time = 5.943s\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_successive_halving_heatmap_001.png\" srcset=\"../../_images/sphx_glr_plot_successive_halving_heatmap_001.png\"/>",
            "def plot_weights(support, weights_func, xlabels, xticks):\n    fig = plt.figure(figsize=(12, 8))\n    ax = fig.add_subplot(111)\n    ax.plot(support, weights_func(support))\n    ax.set_xticks(xticks)\n    ax.set_xticklabels(xlabels, fontsize=16)\n    ax.set_ylim(-0.1, 1.1)\n    return ax",
            "def imshow(inp, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\n\n# Get a batch of training data\n,  = next(iter(dataloaders['train']))\n\n# Make a grid from batch\n = ()\n\nimshow(, title=[class_names[x] for x in ])\n\n\n<img alt=\"['bees', 'bees', 'ants', 'bees']\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_transfer_learning_tutorial_001.png\" srcset=\"../_images/sphx_glr_transfer_learning_tutorial_001.png\"/>",
            "def tsplot(y, lags=None, figsize=(10, 8)):\n    fig = plt.figure(figsize=figsize)\n    layout = (2, 2)\n    ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n    acf_ax = plt.subplot2grid(layout, (1, 0))\n    pacf_ax = plt.subplot2grid(layout, (1, 1))\n    \n    y.plot(ax=ts_ax)\n    smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n    smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n    [ax.set_xlim(1.5) for ax in [acf_ax, pacf_ax]]\n    sns.despine()\n    plt.tight_layout()\n    return ts_ax, acf_ax, pacf_ax",
            "def get_adjacency_matrix(redirects_filename, page_links_filename, limit=None):\n    \"\"\"Extract the adjacency graph as a scipy sparse matrix\n\n    Redirects are resolved first.\n\n    Returns X, the scipy sparse adjacency matrix, redirects as python\n    dict from article names to article names and index_map a python dict\n    from article names to python int (article indexes).\n    \"\"\"\n\n    print(\"Computing the redirect map\")\n    redirects = get_redirects(redirects_filename)\n\n    print(\"Computing the integer index map\")\n    index_map = dict()\n    links = list()\n    for l, line in enumerate((page_links_filename)):\n        split = line.split()\n        if len(split) != 4:\n            print(\"ignoring malformed line: \" + line)\n            continue\n        i = index(redirects, index_map, short_name(split[0]))\n        j = index(redirects, index_map, short_name(split[2]))\n        links.append((i, j))\n        if l % 1000000 == 0:\n            print(\"[%s] line: %08d\" % (datetime.now().isoformat(), l))\n\n        if limit is not None and l = limit - 1:\n            break\n\n    print(\"Computing the adjacency matrix\")\n    X = ((len(index_map), len(index_map)), dtype=)\n    for i, j in links:\n        X[i, j] = 1.0\n    del links\n    print(\"Converting to CSR representation\")\n    X = X.tocsr()\n    print(\"CSR conversion done\")\n    return X, redirects, index_map\n\n\n# stop after 5M links to make it possible to work in RAM\nX, redirects, index_map = get_adjacency_matrix(\n    redirects_filename, page_links_filename, limit=5000000\n)\nnames = {i: name for name, i in index_map.items()}"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Model Selection->Comparison between grid search and successive halving"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression"
            ],
            [
                "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->Load Data->Visualize a few images"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series->Autocorrelation"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Wikipedia principal eigenvector->Computing the Adjacency matrix"
            ]
        ]
    },
    "645460": {
        "jupyter_code_cell": "sns.jointplot(x=\"Mean Travel Time\", y=\"PRICE\", data=merged4_MortgageListings, kind = 'reg', size = 7)\nplt.show()\nsns.jointplot(x=\"% Vacant\", y=\"PRICE\", data=merged4_MortgageListings, kind = 'reg', size = 7)\nplt.show()",
        "matched_tutorial_code_inds": [
            6269,
            4064,
            5643,
            2118,
            6264
        ],
        "matched_tutorial_codes": [
            "states1 = pd.DataFrame(\n    np.c_[fit1.level, fit1.trend, fit1.season],\n    columns=[\"level\", \"slope\", \"seasonal\"],\n    index=aust.index,\n)\nstates2 = pd.DataFrame(\n    np.c_[fit2.level, fit2.trend, fit2.season],\n    columns=[\"level\", \"slope\", \"seasonal\"],\n    index=aust.index,\n)\nfig, [[ax1, ax4], [ax2, ax5], [ax3, ax6]] = plt.subplots(3, 2, figsize=(12, 8))\nstates1[[\"level\"]].plot(ax=ax1)\nstates1[[\"slope\"]].plot(ax=ax2)\nstates1[[\"seasonal\"]].plot(ax=ax3)\nstates2[[\"level\"]].plot(ax=ax4)\nstates2[[\"slope\"]].plot(ax=ax5)\nstates2[[\"seasonal\"]].plot(ax=ax6)\nplt.show()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_22_0.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_22_0.png\"/>",
            "sns.relplot(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\nsns.rugplot(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\n",
            "glm = smf.glm(\n    \"affairs ~ rate_marriage + yrs_married\",\n    data=dc,\n    family=sm.families.Poisson(),\n    freq_weights=np.asarray(dc[\"freq\"]),\n)\nres_f2 = glm.fit()\n# print(res_f2.summary())\nres_f2.pearson_chi2 - res_f.pearson_chi2, res_f2.deviance - res_f.deviance, res_f2.llf - res_f.llf",
            "dict_pos_dict_estimator = (\n    n_components=n_components,\n    alpha=0.1,\n    max_iter=50,\n    batch_size=3,\n    random_state=rng,\n    positive_dict=True,\n)\ndict_pos_dict_estimator.fit(faces_centered)\nplot_gallery(\n    \"Dictionary learning - positive dictionary\",\n    dict_pos_dict_estimator.components_[:n_components],\n    cmap=plt.cm.RdBu,\n)\n\n\n<img alt=\"Dictionary learning - positive dictionary\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_011.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_011.png\"/>",
            "fit1 = ExponentialSmoothing(\n    aust,\n    seasonal_periods=4,\n    trend=\"add\",\n    seasonal=\"add\",\n    initialization_method=\"estimated\",\n).fit()\nfit2 = ExponentialSmoothing(\n    aust,\n    seasonal_periods=4,\n    trend=\"add\",\n    seasonal=\"mul\",\n    initialization_method=\"estimated\",\n).fit()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Winters Seasonal->The Internals"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Distribution visualization in other settings->Plotting joint and marginal distributions",
                "seaborn->Plotting functions->Visualizing distributions of data->Distribution visualization in other settings->Plotting joint and marginal distributions"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->original observations and frequency weights"
            ],
            [
                "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition: Dictionary learning->Dictionary learning - positive dictionary"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Winters Seasonal->The Internals"
            ]
        ]
    },
    "804978": {
        "jupyter_code_cell": "pd.set_option('display.max_columns', 65)\npd.set_option('display.max_rows', 100)",
        "matched_tutorial_code_inds": [
            4745,
            161,
            4064,
            1569,
            5032
        ],
        "matched_tutorial_codes": [
            "axis.get_ticklabels(minor=True)\naxis.get_ticklines(minor=True)",
            "compare.trim_significant_figures()\ncompare.colorize()\ncompare.print()",
            "sns.relplot(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\nsns.rugplot(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\n",
            "print(training_labels[0])\nprint(training_labels[1])\nprint(training_labels[2])",
            "ax.axis[\"right\"].set_visible(False)\nax.axis[\"top\"].set_visible(False)\n\n\n<figure class=\"align-center\">\n<img alt=\"../../_images/sphx_glr_simple_axisline3_001.png\" src=\"../../_images/sphx_glr_simple_axisline3_001.png\"/>\n</figure>"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Intermediate->Artist tutorial->Object containers->Axis containers"
            ],
            [
                "torch->PyTorch Recipes->PyTorch Benchmark->Steps->5. Comparing benchmark results"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Distribution visualization in other settings->Plotting joint and marginal distributions",
                "seaborn->Plotting functions->Visualizing distributions of data->Distribution visualization in other settings->Plotting joint and marginal distributions"
            ],
            [
                "numpy->NumPy Applications->Deep learning on MNIST->2. Preprocess the data->Convert the labels to floating point through categorical/one-hot encoding"
            ],
            [
                "matplotlib->Tutorials->Toolkits->The axisartist toolkit->axisartist"
            ]
        ]
    },
    "503330": {
        "jupyter_code_cell": "for a in ('CG', 'BFGS', 'Newton-CG', 'L-BFGS-B', 'TNC', 'SLSQP'):\n    beta = ox.linreg_ridge_gd(y,X, 5.0, algorithm=a)\n    print(beta, a)\nprint(ox.linreg_ridge_lu(y,X, 5.0))",
        "matched_tutorial_code_inds": [
            6258,
            2162,
            6655,
            1085,
            3167
        ],
        "matched_tutorial_codes": [
            "for fit in [fit2, fit4]:\n    pd.DataFrame(np.c_[fit.level, fit.trend]).rename(\n        columns={0: \"level\", 1: \"slope\"}\n    ).plot(subplots=True)\nplt.show()\nprint(\n    \"Figure 7.4: Level and slope components for Holt\u2019s linear trend method and the additive damped trend method.\"\n)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_0.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_0.png\"/>\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_1.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_1.png\"/>",
            "for pipe in (hist_dropped, hist_one_hot, hist_ordinal, hist_native):\n    pipe.set_params(\n        histgradientboostingregressor__max_depth=3,\n        histgradientboostingregressor__max_iter=15,\n    )\n\ndropped_result = (hist_dropped, X, y, cv=n_cv_folds, scoring=scoring)\none_hot_result = (hist_one_hot, X, y, cv=n_cv_folds, scoring=scoring)\nordinal_result = (hist_ordinal, X, y, cv=n_cv_folds, scoring=scoring)\nnative_result = (hist_native, X, y, cv=n_cv_folds, scoring=scoring)\n\nplot_results(\"Gradient Boosting on Ames Housing (few and small trees)\")\n\n()\n\n\n<img alt=\"Gradient Boosting on Ames Housing (few and small trees), Fit times (s), Mean Absolute Percentage Error\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_002.png\"/>",
            "for i in range(simulated.shape[1]):\n    simulated.iloc[:, i].plot(label=\"_\", color=\"gray\", alpha=0.1)\ndf[\"mean\"].plot(label=\"mean prediction\")\ndf[\"pi_lower\"].plot(linestyle=\"--\", color=\"tab:blue\", label=\"95% interval\")\ndf[\"pi_upper\"].plot(linestyle=\"--\", color=\"tab:blue\", label=\"_\")\npred.endog.plot(label=\"data\")\nplt.legend()",
            "for param in model_ft.parameters():\n  param.requires_grad = True\n\nmodel_ft.to(device)  # We can fine-tune on GPU if available\n\ncriterion = nn.CrossEntropyLoss()\n\n# Note that we are training everything, so the learning rate is lower\n# Notice the smaller learning rate\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=1e-3, momentum=0.9, weight_decay=0.1)\n\n# Decay LR by a factor of 0.3 every several epochs\nexp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=5, gamma=0.3)\n\nmodel_ft_tuned = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n                             num_epochs=25, device=device)",
            "for i in range(n_classes):\n    fpr[i], tpr[i], _ = (y_onehot_test[:, i], y_score[:, i])\n    roc_auc[i] = (fpr[i], tpr[i])\n\nfpr_grid = (0.0, 1.0, 1000)\n\n# Interpolate all ROC curves at these points\nmean_tpr = (fpr_grid)\n\nfor i in range(n_classes):\n    mean_tpr += (fpr_grid, fpr[i], tpr[i])  # linear interpolation\n\n# Average it and compute AUC\nmean_tpr /= n_classes\n\nfpr[\"macro\"] = fpr_grid\ntpr[\"macro\"] = mean_tpr\nroc_auc[\"macro\"] = (fpr[\"macro\"], tpr[\"macro\"])\n\nprint(f\"Macro-averaged One-vs-Rest ROC AUC score:\\n{roc_auc['macro']:.2f}\")"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Method->Plots of Seasonally Adjusted Data"
            ],
            [
                "sklearn->Examples->Ensemble methods->Categorical Feature Support in Gradient Boosting->Limiting the number of splits"
            ],
            [
                "statsmodels->Examples->State space models->ETS models->Predictions"
            ],
            [
                "torch->Model Optimization->(beta) Quantized Transfer Learning for Computer Vision Tutorial->Part 2. Finetuning the Quantizable Model->Finetuning the model"
            ],
            [
                "sklearn->Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->One-vs-Rest multiclass ROC->ROC curve using the OvR macro-average"
            ]
        ]
    },
    "390910": {
        "jupyter_code_cell": "rests = pd.read_csv('./data/food_businesses.csv')\nrests['isfood'] = 1\nreviews = pd.merge(reviews,rests,how='left')\ndel rests\nreviews = pd.merge(reviews,\n    (reviews.\n     groupby(['business_id']).\n     size().\n     reset_index().\n     rename(columns={0:'business_review_count'})\n    ),how='left'\n    )",
        "matched_tutorial_code_inds": [
            3266,
            5355,
            2446,
            5998,
            2463
        ],
        "matched_tutorial_codes": [
            "cvs = [, , ]\n\nfor cv in cvs:\n    fig, ax = (figsize=(6, 3))\n    plot_cv_indices(cv(n_splits), X, y, groups, ax, n_splits)\n    ax.legend(\n        [(color=cmap_cv(0.8)), (color=cmap_cv(0.02))],\n        [\"Testing set\", \"Training set\"],\n        loc=(1.02, 0.8),\n    )\n    # Make the legend fit\n    ()\n    fig.subplots_adjust(right=0.7)\n\n\n\n<img alt=\"StratifiedKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_003.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_003.png\"/>\n<img alt=\"GroupKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_004.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_004.png\"/>\n<img alt=\"StratifiedGroupKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_005.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_005.png\"/>",
            "weights = rob_crime_model.weights\nidx = weights  0\nX = rob_crime_model.model.exog[idx.values]\nww = weights[idx] / weights[idx].mean()\nhat_matrix_diag = ww * (X * np.linalg.pinv(X).T).sum(1)\nresid = rob_crime_model.resid\nresid2 = resid ** 2\nresid2 /= resid2.sum()\nnobs = int(idx.sum())\nhm = hat_matrix_diag.mean()\nrm = resid2.mean()",
            "last_hours = slice(-96, None)\nfig, ax = (figsize=(12, 4))\nfig.suptitle(\"Predictions by linear models\")\nax.plot(\n    y.iloc[test_0].values[last_hours],\n    \"x-\",\n    alpha=0.2,\n    label=\"Actual demand\",\n    color=\"black\",\n)\nax.plot(naive_linear_predictions[last_hours], \"x-\", label=\"Ordinal time features\")\nax.plot(\n    cyclic_cossin_linear_predictions[last_hours],\n    \"x-\",\n    label=\"Trigonometric time features\",\n)\nax.plot(\n    cyclic_spline_linear_predictions[last_hours],\n    \"x-\",\n    label=\"Spline-based time features\",\n)\nax.plot(\n    one_hot_linear_predictions[last_hours],\n    \"x-\",\n    label=\"One-hot time features\",\n)\n_ = ax.legend()\n\n\n<img alt=\"Predictions by linear models\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_006.png\" srcset=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_006.png\"/>",
            "res5 = combine_effects(eff, var_eff, method_re=\"chi2\", use_t=False)\nres5_df = res5.summary_frame()\nprint(\"method RE:\", res5.method_re)\nprint(res5.summary_frame())\nfig = res5.plot_forest()\nfig.set_figheight(8)\nfig.set_figwidth(6)",
            "last_hours = slice(-96, None)\nfig, ax = (figsize=(12, 4))\nfig.suptitle(\"Predictions by non-linear regression models\")\nax.plot(\n    y.iloc[test_0].values[last_hours],\n    \"x-\",\n    alpha=0.2,\n    label=\"Actual demand\",\n    color=\"black\",\n)\nax.plot(\n    gbrt_predictions[last_hours],\n    \"x-\",\n    label=\"Gradient Boosted Trees\",\n)\nax.plot(\n    one_hot_poly_predictions[last_hours],\n    \"x-\",\n    label=\"One-hot + polynomial kernel\",\n)\nax.plot(\n    cyclic_spline_poly_predictions[last_hours],\n    \"x-\",\n    label=\"Splines + polynomial kernel\",\n)\n_ = ax.legend()\n\n\n<img alt=\"Predictions by non-linear regression models\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_007.png\" srcset=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_007.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Model Selection->Visualizing cross-validation behavior in scikit-learn->Define a function to visualize cross-validation behavior"
            ],
            [
                "statsmodels->Examples->Plotting->Regression Plots->Statewide Crime 2009 Dataset->Using robust regression to correct for outliers."
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Qualitative analysis of the impact of features on linear model predictions"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Meta-analysis of proportions->changing data to have positive random effects variance"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Modeling non-linear feature interactions with kernels"
            ]
        ]
    },
    "59583": {
        "jupyter_code_cell": "crawl = crawler(db)\ncrawl.createindextables()",
        "matched_tutorial_code_inds": [
            3939,
            3932,
            3802,
            6804,
            657
        ],
        "matched_tutorial_codes": [
            "anagrams = sns.load_dataset(\"anagrams\")\nanagrams\n",
            "flights = sns.load_dataset(\"flights\")\nflights.head()\n",
            "scores = pd.DataFrame(est.cv_results_)\nscores.head()",
            "endog = macrodata['infl']\nendog.plot(figsize=(15, 5))",
            "traced_rn18 = (rn18)\nprint()"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data",
                "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Long-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Long-form data"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example"
            ],
            [
                "torch->Code Transforms with FX->(beta) Building a Simple CPU Performance Profiler with FX->Capturing the Model with Symbolic Tracing"
            ]
        ]
    },
    "1447862": {
        "jupyter_code_cell": "weeklymean.head()\nax = weeklymean.plot(figsize=(12,4), fontsize=8, linewidth=1.0, legend=False)\nax.set_title('MTA Ridership in NYC by week', fontsize=10)\nax.set_xlabel('Week', fontsize=8)\nax.set_ylabel('Ridership', fontsize=8)",
        "matched_tutorial_code_inds": [
            5611,
            3904,
            4017,
            2446,
            2463
        ],
        "matched_tutorial_codes": [
            "data2 = data.copy()\ndata2[\"const\"] = 1\ndc = (\n    data2[\"affairs rate_marriage age yrs_married const\".split()]\n    .groupby(\"affairs rate_marriage age yrs_married\".split())\n    .count()\n)\ndc.reset_index(inplace=True)\ndc.rename(columns={\"const\": \"freq\"}, inplace=True)\nprint(dc.shape)\ndc.head()",
            "dots = sns.load_dataset(\"dots\")\nsns.relplot(\n    data=dots, kind=\"line\",\n    x=\"time\", y=\"firing_rate\", col=\"align\",\n    hue=\"choice\", size=\"coherence\", style=\"choice\",\n    facet_kws=dict(sharex=False),\n)\n",
            "healthexp = sns.load_dataset(\"healthexp\").sort_values(\"Year\")\nsns.relplot(\n    data=healthexp, kind=\"line\",\n    x=\"Spending_USD\", y=\"Life_Expectancy\", hue=\"Country\",\n    sort=False\n)\n",
            "last_hours = slice(-96, None)\nfig, ax = (figsize=(12, 4))\nfig.suptitle(\"Predictions by linear models\")\nax.plot(\n    y.iloc[test_0].values[last_hours],\n    \"x-\",\n    alpha=0.2,\n    label=\"Actual demand\",\n    color=\"black\",\n)\nax.plot(naive_linear_predictions[last_hours], \"x-\", label=\"Ordinal time features\")\nax.plot(\n    cyclic_cossin_linear_predictions[last_hours],\n    \"x-\",\n    label=\"Trigonometric time features\",\n)\nax.plot(\n    cyclic_spline_linear_predictions[last_hours],\n    \"x-\",\n    label=\"Spline-based time features\",\n)\nax.plot(\n    one_hot_linear_predictions[last_hours],\n    \"x-\",\n    label=\"One-hot time features\",\n)\n_ = ax.legend()\n\n\n<img alt=\"Predictions by linear models\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_006.png\" srcset=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_006.png\"/>",
            "last_hours = slice(-96, None)\nfig, ax = (figsize=(12, 4))\nfig.suptitle(\"Predictions by non-linear regression models\")\nax.plot(\n    y.iloc[test_0].values[last_hours],\n    \"x-\",\n    alpha=0.2,\n    label=\"Actual demand\",\n    color=\"black\",\n)\nax.plot(\n    gbrt_predictions[last_hours],\n    \"x-\",\n    label=\"Gradient Boosted Trees\",\n)\nax.plot(\n    one_hot_poly_predictions[last_hours],\n    \"x-\",\n    label=\"One-hot + polynomial kernel\",\n)\nax.plot(\n    cyclic_spline_poly_predictions[last_hours],\n    \"x-\",\n    label=\"Splines + polynomial kernel\",\n)\n_ = ax.legend()\n\n\n<img alt=\"Predictions by non-linear regression models\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_007.png\" srcset=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_007.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Condensing and Aggregating observations->Dataset with unique observations"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Emphasizing continuity with line plots->Controlling sorting and orientation",
                "seaborn->Plotting functions->Visualizing statistical relationships->Emphasizing continuity with line plots->Controlling sorting and orientation"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Qualitative analysis of the impact of features on linear model predictions"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Modeling non-linear feature interactions with kernels"
            ]
        ]
    },
    "1245662": {
        "jupyter_code_cell": "df_stats = pd.read_csv('hover_stats.csv')\ndf_stats[['total_reward']].plot(title=\"Episode Rewards\")\ndf_stats = pd.read_csv('landing_stats.csv')\ndf_stats[['total_reward']].plot(title=\"Episode Rewards\")",
        "matched_tutorial_code_inds": [
            6704,
            3702,
            4751,
            5481,
            6251
        ],
        "matched_tutorial_codes": [
            "ax = dta.mean().plot(grid=False)\nax.set_xlabel(\"Year\", size=17)\nax.set_ylabel(\"Fertility rate\", size=17)\nax.set_xlim(0, 51)",
            "plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "fig, ax = plt.subplots()\nline_up, = ax.plot([1, 2, 3], label='Line 2')\nline_down, = ax.plot([3, 2, 1], label='Line 1')\nax.legend([line_up, line_down], ['Line Up', 'Line Down'])",
            "fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111, title=\"Standardized Deviance Residuals\")\nax.hist(resid_std, bins=25, density=True)\nax.plot(kde_resid.support, kde_resid.density, \"r\")",
            "ax = oildata.plot()\nax.set_xlabel(\"Year\")\nax.set_ylabel(\"Oil (millions of tonnes)\")\nprint(\"Figure 7.1: Oil production in Saudi Arabia from 1996 to 2007.\")"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Legend guide->Controlling the legend entries"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Histogram of standardized deviance residuals with Kernel Density Estimate overlaid"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Simple Exponential Smoothing"
            ]
        ]
    },
    "792057": {
        "jupyter_code_cell": "x = np.linspace(-6, 6, 200)\ndf=[1, 2, 3, 10]\nfor nu in df:\n    plt.plot(x, stats.t.pdf(x, nu))\nlegend = ['$\\\\nu=%1.0f$' % nu for nu in df]  \nplt.plot(x, stats.norm.pdf(x))\nlegend.append('Normal')\nplt.legend(legend)\nplt.title(\"Student's $t$ Densities\")\nplt.xlabel('x')\nplt.ylabel('$f_{\\\\nu}(x)$')\nplt.savefig('img/tdists.svg')\nplt.close()\ndf, m, h = stats.t.fit(r)  \nVaR_t = -stats.t.ppf(0.01, df, loc=m, scale=h)\nVaR_t",
        "matched_tutorial_code_inds": [
            5225,
            4632,
            93,
            2741,
            6795
        ],
        "matched_tutorial_codes": [
            "x = np.arange(data.income.min(), data.income.max(), 50)\nget_y = lambda a, b: a + b * x\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nfor i in range(models.shape[0]):\n    y = get_y(models.a[i], models.b[i])\n    ax.plot(x, y, linestyle=\"dotted\", color=\"grey\")\n\ny = get_y(ols[\"a\"], ols[\"b\"])\n\nax.plot(x, y, color=\"red\", label=\"OLS\")\nax.scatter(data.income, data.foodexp, alpha=0.2)\nax.set_xlim((240, 3000))\nax.set_ylim((240, 2000))\nlegend = ax.legend()\nax.set_xlabel(\"Income\", fontsize=16)\nax.set_ylabel(\"Food expenditure\", fontsize=16)",
            "x = np.linspace(0, 2, 100)  # Sample data.\n\nplt.figure(figsize=(5, 2.7), layout='constrained')\nplt.plot(x, x, label='linear')  # Plot some data on the (implicit) axes.\nplt.plot(x, x**2, label='quadratic')  # etc.\nplt.plot(x, x**3, label='cubic')\nplt.xlabel('x label')\nplt.ylabel('y label')\nplt.title(\"Simple Plot\")\nplt.legend()\n\n\n<img alt=\"Simple Plot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_quick_start_004.png\" srcset=\"../../_images/sphx_glr_quick_start_004.png, ../../_images/sphx_glr_quick_start_004_2_0x.png 2.0x\"/>",
            "x = (-5, 5, 0.1).view(-1, 1)\ny = -5 * x + 0.1 * (x.size())\n\nmodel = (1, 1)\ncriterion = ()\noptimizer = (model.parameters(), lr = 0.1)\n\ndef train_model(iter):\n    for epoch in range(iter):\n        y1 = model(x)\n        loss = criterion(y1, y)\n        writer.add_scalar(\"Loss/train\", loss, epoch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\ntrain_model(10)\nwriter.flush()",
            "x_train = (0, 10, 100)\nrng = (0)\nx_train = (rng.choice(x_train, size=20, replace=False))\ny_train = f(x_train)\n\n# create 2D-array versions of these arrays to feed to transformers\nX_train = x_train[:, ]\nX_plot = x_plot[:, ]",
            "x1n = np.linspace(20.5, 25, 10)\nXnew = np.column_stack((x1n, np.sin(x1n), (x1n - 5) ** 2))\nXnew = sm.add_constant(Xnew)\nynewpred = olsres.predict(Xnew)  # predict out of sample\nprint(ynewpred)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Quantile Regression->Visualizing the results->First plot"
            ],
            [
                "matplotlib->Tutorials->Introductory->Quick start guide->Coding styles->The explicit and the implicit interfaces"
            ],
            [
                "torch->PyTorch Recipes->How to use TensorBoard with PyTorch->Log scalars"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Polynomial and Spline interpolation"
            ],
            [
                "statsmodels->Examples->User Notes->Prediction->Create a new sample of explanatory variables Xnew, predict and plot"
            ]
        ]
    },
    "696137": {
        "jupyter_code_cell": "df['month'] = df['date'].map(lambda x: x.month).astype('category')\ndf_month = pd.get_dummies(df['month'], drop_first=False)\ndf_month.columns = ['Jan','Feb','Mar','Apr', 'May', 'June', 'July', 'Aug','Sep', 'Oct','Nov','Dec']\ndf['hour'] = df['date'].map(lambda x: x.hour).astype('category')\ndf_hour = pd.get_dummies(df['hour'],drop_first=False)",
        "matched_tutorial_code_inds": [
            6255,
            6253,
            2662,
            3773,
            5917
        ],
        "matched_tutorial_codes": [
            "fit1 = Holt(air, initialization_method=\"estimated\").fit(\n    smoothing_level=0.8, smoothing_trend=0.2, optimized=False\n)\nfcast1 = fit1.forecast(5).rename(\"Holt's linear trend\")\nfit2 = Holt(air, exponential=True, initialization_method=\"estimated\").fit(\n    smoothing_level=0.8, smoothing_trend=0.2, optimized=False\n)\nfcast2 = fit2.forecast(5).rename(\"Exponential trend\")\nfit3 = Holt(air, damped_trend=True, initialization_method=\"estimated\").fit(\n    smoothing_level=0.8, smoothing_trend=0.2\n)\nfcast3 = fit3.forecast(5).rename(\"Additive damped trend\")\n\nplt.figure(figsize=(12, 8))\nplt.plot(air, marker=\"o\", color=\"black\")\nplt.plot(fit1.fittedvalues, color=\"blue\")\n(line1,) = plt.plot(fcast1, marker=\"o\", color=\"blue\")\nplt.plot(fit2.fittedvalues, color=\"red\")\n(line2,) = plt.plot(fcast2, marker=\"o\", color=\"red\")\nplt.plot(fit3.fittedvalues, color=\"green\")\n(line3,) = plt.plot(fcast3, marker=\"o\", color=\"green\")\nplt.legend([line1, line2, line3], [fcast1.name, fcast2.name, fcast3.name])",
            "fit1 = SimpleExpSmoothing(oildata, initialization_method=\"heuristic\").fit(\n    smoothing_level=0.2, optimized=False\n)\nfcast1 = fit1.forecast(3).rename(r\"$\\alpha=0.2$\")\nfit2 = SimpleExpSmoothing(oildata, initialization_method=\"heuristic\").fit(\n    smoothing_level=0.6, optimized=False\n)\nfcast2 = fit2.forecast(3).rename(r\"$\\alpha=0.6$\")\nfit3 = SimpleExpSmoothing(oildata, initialization_method=\"estimated\").fit()\nfcast3 = fit3.forecast(3).rename(r\"$\\alpha=%s$\" % fit3.model.params[\"smoothing_level\"])\n\nplt.figure(figsize=(12, 8))\nplt.plot(oildata, marker=\"o\", color=\"black\")\nplt.plot(fit1.fittedvalues, marker=\"o\", color=\"blue\")\n(line1,) = plt.plot(fcast1, marker=\"o\", color=\"blue\")\nplt.plot(fit2.fittedvalues, marker=\"o\", color=\"red\")\n(line2,) = plt.plot(fcast2, marker=\"o\", color=\"red\")\nplt.plot(fit3.fittedvalues, marker=\"o\", color=\"green\")\n(line3,) = plt.plot(fcast3, marker=\"o\", color=\"green\")\nplt.legend([line1, line2, line3], [fcast1.name, fcast2.name, fcast3.name])",
            "m, s, _ = (\n    (enet.coef_)[0],\n    enet.coef_[enet.coef_ != 0],\n    markerfmt=\"x\",\n    label=\"Elastic net coefficients\",\n)\n([m, s], color=\"#2ca02c\")\nm, s, _ = (\n    (lasso.coef_)[0],\n    lasso.coef_[lasso.coef_ != 0],\n    markerfmt=\"x\",\n    label=\"Lasso coefficients\",\n)\n([m, s], color=\"#ff7f0e\")\n(\n    (coef)[0],\n    coef[coef != 0],\n    label=\"true coefficients\",\n    markerfmt=\"bx\",\n)\n\n(loc=\"best\")\n(\n    \"Lasso $R^2$: %.3f, Elastic Net $R^2$: %.3f\" % (r2_score_lasso, r2_score_enet)\n)\n()\n\n\n<img alt=\"Lasso $R^2$: 0.658, Elastic Net $R^2$: 0.643\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_lasso_and_elasticnet_001.png\" srcset=\"../../_images/sphx_glr_plot_lasso_and_elasticnet_001.png\"/>",
            "df = df.assign(away_strength=df['away_team'].map(win_percent),\n               home_strength=df['home_team'].map(win_percent),\n               point_diff=df['home_points'] - df['away_points'],\n               rest_diff=df['home_rest'] - df['away_rest'])\ndf.head()",
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Method"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Simple Exponential Smoothing"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Lasso and Elastic Net for Sparse Signals->Plot"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ]
        ]
    },
    "575836": {
        "jupyter_code_cell": "from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(x_train, y_train)",
        "matched_tutorial_code_inds": [
            3304,
            3581,
            6109,
            5191,
            3583
        ],
        "matched_tutorial_codes": [
            "from sklearn import metrics\n\nY_pred = rbm_features_classifier.predict(X_test)\nprint(\n    \"Logistic regression using RBM features:\\n%s\\n\"\n    % ((Y_test, Y_pred))\n)",
            "from sklearn.feature_extraction.text import CountVectorizer\n count_vect = CountVectorizer()\n X_train_counts = count_vect.fit_transform(twenty_train.data)\n X_train_counts.shape\n(2257, 35788)",
            "from statsmodels.tsa.api import AutoReg\n\nmod = AutoReg(y, 1, trend=\"n\", deterministic=det_proc)\nres = mod.fit()\nprint(res.summary())",
            "ols_model = sm.OLS(y, X)\nols_results = ols_model.fit()\nprint(ols_results.summary())",
            "from sklearn.feature_extraction.text import TfidfTransformer\n tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n X_train_tf = tf_transformer.transform(X_train_counts)\n X_train_tf.shape\n(2257, 35788)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Neural Networks->Restricted Boltzmann Machine features for digit classification->Evaluation"
            ],
            [
                "sklearn->Tutorials->Working With Text Data->Extracting features from text files->Tokenizing text with scikit-learn"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Deterministic Terms->Model Support->Simulate Some Data"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity"
            ],
            [
                "sklearn->Tutorials->Working With Text Data->Extracting features from text files->From occurrences to frequencies"
            ]
        ]
    },
    "1088479": {
        "jupyter_code_cell": "cma_opt.optimize(objective_fct=eval_experiment, iterations=50, verb_disp=1)\nobservations_2 = test_data['observations_2'].unstack(level=0).values\nobservation_dim = test_data['observations_2'].ndim\nobservations_2 = observations.reshape(-1, observation_dim, num_test_episodes)\nevaluation.setup_evaluation(train_data=train_matrices, test_observations=observations,\n                            test_groundtruth=groundtruth)",
        "matched_tutorial_code_inds": [
            2846,
            6204,
            5246,
            6707,
            5523
        ],
        "matched_tutorial_codes": [
            "cv_model = (\n    model,\n    X,\n    y,\n    cv=cv,\n    return_estimator=True,\n    n_jobs=2,\n)\ncoefs = (\n    [est[-1].regressor_.coef_ for est in cv_model[\"estimator\"]], columns=feature_names\n)",
            "infl_cycle  unemp_cycle\n1959-03-31    0.237927    -0.216867\n1959-06-30    0.770007    -0.343779\n1959-09-30    1.177736    -0.511024\n1959-12-31    1.256754    -0.686967\n1960-03-31    0.972128    -0.770793\n1960-06-30    0.491889    -0.640601\n1960-09-30    0.070189    -0.249741\n1960-12-31   -0.130432     0.301545\n1961-03-31   -0.134155     0.788992\n1961-06-30   -0.092073     0.985356",
            "res.plot_recursive_coefficient(1, alpha=None)",
            "fig = pca_model.plot_scree(log_scale=False)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_13_0.png\" src=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_13_0.png\"/>",
            "resfd2_logit.predict(data_student.iloc[:5])"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->",
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->",
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Time Series Filters->Christiano-Fitzgerald approximate band-pass filter: Inflation and Unemployment"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 2: Quantity theory of money"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model"
            ]
        ]
    },
    "1191539": {
        "jupyter_code_cell": "index_95 = np.argmax(cum_ratio >= 0.95)\nprint(index_95)\nprint(cum_ratio[index_95])\npca = PCA(n_components=0.95)\nX_pca = pca.fit_transform(X)\nprint(X_pca.shape)",
        "matched_tutorial_code_inds": [
            5312,
            6704,
            6317,
            5319,
            5289
        ],
        "matched_tutorial_codes": [
            "oo = np.ones(df.shape[0])\nmodel2 = sm.MixedLM(df.y, oo, exog_re=oo, groups=df.group1, exog_vc=vcs)\nresult2 = model2.fit()\nprint(result2.summary())",
            "ax = dta.mean().plot(grid=False)\nax.set_xlabel(\"Year\", size=17)\nax.set_ylabel(\"Fertility rate\", size=17)\nax.set_xlim(0, 51)",
            "# Fit the model\nmod = sm.tsa.statespace.SARIMAX(data['ln_wpi'], trend='c', order=(1,1,1))\nres = mod.fit(disp=False)\nprint(res.summary())",
            "oo = np.ones(df.shape[0])\nmodel4 = sm.MixedLM(df.y, oo[:, None], exog_re=None, groups=oo, exog_vc=vcs)\nresult4 = model4.fit()\nprint(result4.summary())",
            "mod_wls = sm.WLS(y, X, weights=1.0 / (w ** 2))\nres_wls = mod_wls.fit()\nprint(res_wls.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Nested analysis"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Example 2: Arima with additive seasonal effects"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Crossed analysis"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Weighted Least Squares->WLS Estimation->WLS knowing the true variance ratio of heteroscedasticity"
            ]
        ]
    },
    "226200": {
        "jupyter_code_cell": "df_fa.shape\ncolor = ['red', 'blue']\nplt.figure()\nfor color, i, name in zip(color, [0,1], ['no_churn', 'churn']):\n    plt.scatter(df_fa[df_fa['is_churn'] == i]['date_featuresdatelistening_tenure'],\n               df_fa[df_fa['is_churn'] == i]['within_days_7num_unqmean'], color = color, alpha = 0.2, label = name)\nplt.legend(loc = 'best')\nplt.xlabel('Listening Tenure')\nplt.ylabel('Mean Number of Unique listening Periods in the last 7 days')",
        "matched_tutorial_code_inds": [
            3596,
            446,
            4905,
            2875,
            6492
        ],
        "matched_tutorial_codes": [
            "gs_clf.best_score_\n0.9...\n for param_name in sorted(parameters.keys()):\n...     print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))\n...\nclf__alpha: 0.001\ntfidf__use_idf: True\nvect__ngram_range: (1, 1)",
            "model.to(DEVICE)\nmodel_input = model_input.to(DEVICE)\n\nprint(\"slow path:\")\nprint(\"==========\")\nwith torch.autograd.profiler.profile(use_cuda=True) as prof:\n  for i in range(ITERATIONS):\n    output = model(model_input)\nprint(prof)\n\nmodel.eval()\n\nprint(\"fast path:\")\nprint(\"==========\")\nwith torch.autograd.profiler.profile(use_cuda=True) as prof:\n  with torch.no_grad():\n    for i in range(ITERATIONS):\n      output = model(model_input)\nprint(prof)",
            "viridis.colors [[0.267004 0.004874 0.329415 1.      ]\n [0.275191 0.194905 0.496005 1.      ]\n [0.212395 0.359683 0.55171  1.      ]\n [0.153364 0.497    0.557724 1.      ]\n [0.122312 0.633153 0.530398 1.      ]\n [0.288921 0.758394 0.428426 1.      ]\n [0.626579 0.854645 0.223353 1.      ]\n [0.993248 0.906157 0.143936 1.      ]]\nviridis(range(8)) [[0.267004 0.004874 0.329415 1.      ]\n [0.275191 0.194905 0.496005 1.      ]\n [0.212395 0.359683 0.55171  1.      ]\n [0.153364 0.497    0.557724 1.      ]\n [0.122312 0.633153 0.530398 1.      ]\n [0.288921 0.758394 0.428426 1.      ]\n [0.626579 0.854645 0.223353 1.      ]\n [0.993248 0.906157 0.143936 1.      ]]\nviridis(np.linspace(0, 1, 8)) [[0.267004 0.004874 0.329415 1.      ]\n [0.275191 0.194905 0.496005 1.      ]\n [0.212395 0.359683 0.55171  1.      ]\n [0.153364 0.497    0.557724 1.      ]\n [0.122312 0.633153 0.530398 1.      ]\n [0.288921 0.758394 0.428426 1.      ]\n [0.626579 0.854645 0.223353 1.      ]\n [0.993248 0.906157 0.143936 1.      ]]",
            "model_coef = (regressor_without_ability.coef_, index=features_names)\ncoef = (\n    [true_coef[features_names], model_coef],\n    keys=[\"Coefficients of true generative model\", \"Model coefficients\"],\n    axis=1,\n)\nax = coef.plot.barh()\nax.set_xlabel(\"Coefficient values\")\n_ = ax.set_title(\"Coefficients of the linear regression excluding the ability feature\")\n()\n()\n\n\n<img alt=\"Coefficients of the linear regression excluding the ability feature\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_causal_interpretation_003.png\" srcset=\"../../_images/sphx_glr_plot_causal_interpretation_003.png\"/>",
            "time_s = np.s_[:50]  # After this they basically agree\nfig1 = plt.figure()\nax1 = fig1.add_subplot(111)\nidx = np.asarray(series.index)\nh1, = ax1.plot(idx[time_s], res_f.freq_seasonal[0].filtered[time_s], label='Double Freq. Seas')\nh2, = ax1.plot(idx[time_s], res_tf.seasonal.filtered[time_s], label='Mixed Domain Seas')\nh3, = ax1.plot(idx[time_s], true_seasonal_10_3[time_s], label='True Seasonal 10(3)')\nplt.legend([h1, h2, h3], ['Double Freq. Seasonal','Mixed Domain Seasonal','Truth'], loc=2)\nplt.title('Seasonal 10(3) component')\nplt.show()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_seasonal_21_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_seasonal_21_0.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Tutorials->Working With Text Data->Parameter tuning using grid search"
            ],
            [
                "torch->Text->Fast Transformer Inference with Better Transformer->Additional Information",
                "torch->Text->Fast Transformer Inference with Better Transformer->Additional Information"
            ],
            [
                "matplotlib->Tutorials->Colors->Creating Colormaps in Matplotlib->Getting colormaps and accessing their values->ListedColormap"
            ],
            [
                "sklearn->Examples->Inspection->Failure of Machine Learning to infer causal effects->Income prediction with partial observations"
            ],
            [
                "statsmodels->Examples->State space models->Seasonality in Time Series Data->Comparison of filtered estimates"
            ]
        ]
    },
    "1307607": {
        "jupyter_code_cell": "import pandas as pd\npath=r'noExpired.xlsx'\nall=pd.read_excel(path).dropna()",
        "matched_tutorial_code_inds": [
            3784,
            3861,
            628,
            5381,
            1053
        ],
        "matched_tutorial_codes": [
            "import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "df = dd.read_parquet(\"data/indiv-*.parquet\", engine='pyarrow', columns=['occupation'])\n\nmost_common = df.occupation.value_counts().nlargest(100)\nmost_common.compute().sort_values(ascending=False)",
            "import onnx\n\nonnx_model = onnx.load(\"super_resolution.onnx\")\nonnx.checker.check_model(onnx_model)",
            "spector_data = sm.datasets.spector.load()\nspector_data.exog = sm.add_constant(spector_data.exog, prepend=False)",
            "import torch.quantization\n\nquantized_model = torch.quantization.quantize_dynamic(\n    model, {, }, dtype=\n)\nprint(quantized_model)"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples"
            ],
            [
                "pandas_toms_blog->Scaling->Using Dask"
            ],
            [
                "torch->Deploying PyTorch Models in Production->(optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Data"
            ],
            [
                "torch->Model Optimization->(beta) Dynamic Quantization on an LSTM Word Language Model->4. Test dynamic quantization"
            ]
        ]
    },
    "1421220": {
        "jupyter_code_cell": "from sklearn.feature_extraction.text import TfidfTransformer\ntfidf = TfidfTransformer(smooth_idf=False, norm=None)\ntfidf.fit_transform(tf).toarray()[-1][:3]\ntf_and = 1\ndf_and = 1 \ntf_and * (np.log(n_docs / df_and) + 1)",
        "matched_tutorial_code_inds": [
            3338,
            2901,
            3510,
            3554,
            2458
        ],
        "matched_tutorial_codes": [
            "from sklearn.compose import make_column_selector as \n\npreprocessor = (\n    transformers=[\n        (\"num\", numeric_transformer, (dtype_exclude=\"category\")),\n        (\"cat\", categorical_transformer, (dtype_include=\"category\")),\n    ]\n)\nclf = (\n    steps=[(\"preprocessor\", preprocessor), (\"classifier\", ())]\n)\n\n\nclf.fit(X_train, y_train)\nprint(\"model score: %.3f\" % clf.score(X_test, y_test))\nclf",
            "from sklearn.ensemble import \n\nprint(\"Training HistGradientBoostingRegressor...\")\ntic = ()\nhgbdt_model = (\n    hgbdt_preprocessor,\n    (\n        categorical_features=categorical_features, random_state=0\n    ),\n)\nhgbdt_model.fit(X_train, y_train)\nprint(f\"done in {() - tic:.3f}s\")\nprint(f\"Test R2 score: {hgbdt_model.score(X_test, y_test):.2f}\")",
            "from sklearn.feature_extraction.text import \n\nvectorizer = (\n    max_df=0.5,\n    min_df=5,\n    stop_words=\"english\",\n)\nt0 = ()\nX_tfidf = vectorizer.fit_transform(dataset.data)\n\nprint(f\"vectorization done in {() - t0:.3f} s\")\nprint(f\"n_samples: {X_tfidf.shape[0]}, n_features: {X_tfidf.shape[1]}\")",
            "from sklearn.feature_extraction.text import \n\nt0 = ()\nvectorizer = (n_features=2**18)\nvectorizer.fit_transform(raw_data)\nduration = () - t0\ndict_count_vectorizers[\"vectorizer\"].append(vectorizer.__class__.__name__)\ndict_count_vectorizers[\"speed\"].append(data_size_mb / duration)\nprint(f\"done in {duration:.3f} s at {data_size_mb / duration:.1f} MB/s\")",
            "from sklearn.kernel_approximation import \n\n\ncyclic_spline_poly_pipeline = (\n    cyclic_spline_transformer,\n    (kernel=\"poly\", degree=2, n_components=300, random_state=0),\n    (alphas=alphas),\n)\nevaluate(cyclic_spline_poly_pipeline, X, y, cv=ts_cv)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Pipelines and composite estimators->Column Transformer with Mixed Types"
            ],
            [
                "sklearn->Examples->Inspection->Partial Dependence and Individual Conditional Expectation Plots->1-way partial dependence with different models"
            ],
            [
                "sklearn->Examples->Working with text documents->Clustering text documents using k-means->K-means clustering on text features->Feature Extraction using TfidfVectorizer"
            ],
            [
                "sklearn->Examples->Working with text documents->FeatureHasher and DictVectorizer Comparison->Comparison with special purpose text vectorizers"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Modeling non-linear feature interactions with kernels"
            ]
        ]
    },
    "1043714": {
        "jupyter_code_cell": "import pandas as pd\nimport numpy as np\nimport matplotlib\nfrom matplotlib import pyplot as plt\nimport random as rand",
        "matched_tutorial_code_inds": [
            2799,
            6399,
            6455,
            5863,
            4358
        ],
        "matched_tutorial_codes": [
            "import numpy as np\nimport scipy as sp\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns",
            "import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt",
            "import numpy as np\nfrom scipy import stats\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm",
            "import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm",
            "import numpy as np\n import scipy.signal as signal\n import matplotlib.pyplot as plt"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models"
            ],
            [
                "statsmodels->Examples->State space models->VARMAX: Introduction",
                "statsmodels->Examples->State space models->Trends and cycles in unemployment"
            ],
            [
                "statsmodels->Examples->State space models->Statespace ARMA: Sunspots Data"
            ],
            [
                "statsmodels->Examples->Generalized Estimating Equations->GEE Nested Covariance Structure",
                "statsmodels->Examples->User Notes->Least squares fitting of models to data"
            ],
            [
                "scipy->Signal Processing (scipy.signal)->Filtering->Filter Design->FIR Filter",
                "scipy->Signal Processing (scipy.signal)->Filtering->Filter Design->FIR Filter",
                "scipy->Signal Processing (scipy.signal)->Filtering->Filter Design->IIR Filter",
                "scipy->Signal Processing (scipy.signal)->Filtering->Analog Filter Design",
                "scipy->Signal Processing (scipy.signal)->Spectral Analysis->Periodogram Measurements",
                "scipy->Signal Processing (scipy.signal)->Spectral Analysis->Spectral Analysis using Welch\u00e2\u0080\u0099s Method",
                "scipy->Signal Processing (scipy.signal)->Detrend"
            ]
        ]
    },
    "421411": {
        "jupyter_code_cell": "data= pd.read_csv('k-means/xclara.csv')\nprint(data.shape)\ndata.head()\nf1 = data[\"V1\"].values\nf2 = data[\"V2\"].values\nX = np.array(list(zip(f1,f2)))  \nplt.scatter(f1,f2,c=\"black\",s=7) ",
        "matched_tutorial_code_inds": [
            4654,
            6060,
            5963,
            1092,
            3904
        ],
        "matched_tutorial_codes": [
            "data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>",
            "data = pdr.get_data_fred(\"HOUSTNSA\", \"1959-01-01\", \"2019-06-01\")\nhousing = data.HOUSTNSA.pct_change().dropna()\n# Scale by 100 to get percentages\nhousing = 100 * housing.asfreq(\"MS\")\nfig, ax = plt.subplots()\nax = housing.plot(ax=ax)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_6_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_6_0.png\"/>",
            "data = [\n    [\"Carroll\", 94, 22, 60, 92, 20, 60],\n    [\"Grant\", 98, 21, 65, 92, 22, 65],\n    [\"Peck\", 98, 28, 40, 88, 26, 40],\n    [\"Donat\", 94, 19, 200, 82, 17, 200],\n    [\"Stewart\", 98, 21, 50, 88, 22, 45],\n    [\"Young\", 96, 21, 85, 92, 22, 85],\n]\ncolnames = [\"study\", \"mean_t\", \"sd_t\", \"n_t\", \"mean_c\", \"sd_c\", \"n_c\"]\nrownames = [i[0] for i in data]\ndframe1 = pd.DataFrame(data, columns=colnames)\nrownames",
            "data_path = '~/.data/imagenet'\nsaved_model_dir = 'data/'\nfloat_model_file = 'mobilenet_pretrained_float.pth'\nscripted_float_model_file = 'mobilenet_quantization_scripted.pth'\nscripted_quantized_model_file = 'mobilenet_quantization_scripted_quantized.pth'\n\ntrain_batch_size = 30\neval_batch_size = 50\n\ndata_loader, data_loader_test = prepare_data_loaders(data_path)\ncriterion = nn.CrossEntropyLoss()\nfloat_model = load_model(saved_model_dir + float_model_file).to('cpu')\n\n# Next, we'll \"fuse modules\"; this can both make the model faster by saving on memory access\n# while also improving numerical accuracy. While this can be used with any model, this is\n# especially common with quantized models.\n\nprint('\\n Inverted Residual Block: Before fusion \\n\\n', float_model.features[1].conv)\nfloat_model.eval()\n\n# Fuses modules\nfloat_model.fuse_model()\n\n# Note fusion of Conv+BN+Relu and Conv+Relu\nprint('\\n Inverted Residual Block: After fusion\\n\\n',float_model.features[1].conv)",
            "dots = sns.load_dataset(\"dots\")\nsns.relplot(\n    data=dots, kind=\"line\",\n    x=\"time\", y=\"firing_rate\", col=\"align\",\n    hue=\"choice\", size=\"coherence\", style=\"choice\",\n    facet_kws=dict(sharex=False),\n)\n"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Example"
            ],
            [
                "torch->Model Optimization->(beta) Static Quantization with Eager Mode in PyTorch->3. Define dataset and data loaders->ImageNet Data"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics"
            ]
        ]
    },
    "1468887": {
        "jupyter_code_cell": "(cast\n .loc[cast.character.isin(['Superman', 'Batman'])]\n .groupby(['year', 'character'])\n .size()\n .unstack()\n .fillna(0)\n .apply(lambda x: 1 if x['Batman'] > x['Superman'] else 0, axis = 'columns')\n .sum())\n(cast\n .loc[cast.character.isin(['Superman', 'Batman'])]\n .pivot_table(index='year', columns='character', aggfunc='size')\n .fillna(0)\n .apply(lambda x: 1 if x['Batman'] > x['Superman'] else 0, axis=1)\n .sum())",
        "matched_tutorial_code_inds": [
            2567,
            2548,
            2555,
            2557,
            596
        ],
        "matched_tutorial_codes": [
            "(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\n(\n    X_train,\n    y_train_noisy,\n    noise_std,\n    linestyle=\"None\",\n    color=\"tab:blue\",\n    marker=\".\",\n    markersize=10,\n    label=\"Observations\",\n)\n(X, mean_prediction, label=\"Mean prediction\")\n(\n    X.ravel(),\n    mean_prediction - 1.96 * std_prediction,\n    mean_prediction + 1.96 * std_prediction,\n    color=\"tab:orange\",\n    alpha=0.5,\n    label=r\"95% confidence interval\",\n)\n()\n(\"$x$\")\n(\"$f(x)$\")\n_ = (\"Gaussian process regression on a noisy dataset\")\n\n\n<img alt=\"Gaussian process regression on a noisy dataset\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_noisy_targets_003.png\" srcset=\"../../_images/sphx_glr_plot_gpr_noisy_targets_003.png\"/>",
            "(data, target, label=\"True signal\", linewidth=2, linestyle=\"dashed\")\n(\n    training_data,\n    training_noisy_target,\n    color=\"black\",\n    label=\"Noisy measurements\",\n)\n(\n    data,\n    predictions_kr,\n    label=\"Kernel ridge\",\n    linewidth=2,\n    linestyle=\"dashdot\",\n)\n(loc=\"lower right\")\n(\"data\")\n(\"target\")\n_ = (\n    \"Kernel ridge regression with an exponential sine squared\\n \"\n    \"kernel using tuned hyperparameters\"\n)\n\n\n<img alt=\"Kernel ridge regression with an exponential sine squared  kernel using tuned hyperparameters\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_compare_gpr_krr_004.png\" srcset=\"../../_images/sphx_glr_plot_compare_gpr_krr_004.png\"/>",
            "(data, target, label=\"True signal\", linewidth=2, linestyle=\"dashed\")\n(\n    training_data,\n    training_noisy_target,\n    color=\"black\",\n    label=\"Noisy measurements\",\n)\n# Plot the predictions of the kernel ridge\n(\n    data,\n    predictions_kr,\n    label=\"Kernel ridge\",\n    linewidth=2,\n    linestyle=\"dashdot\",\n)\n# Plot the predictions of the gaussian process regressor\n(\n    data,\n    mean_predictions_gpr,\n    label=\"Gaussian process regressor\",\n    linewidth=2,\n    linestyle=\"dotted\",\n)\n(\n    data.ravel(),\n    mean_predictions_gpr - std_predictions_gpr,\n    mean_predictions_gpr + std_predictions_gpr,\n    color=\"tab:green\",\n    alpha=0.2,\n)\n(loc=\"lower right\")\n(\"data\")\n(\"target\")\n_ = (\"Comparison between kernel ridge and gaussian process regressor\")\n\n\n<img alt=\"Comparison between kernel ridge and gaussian process regressor\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_compare_gpr_krr_005.png\" srcset=\"../../_images/sphx_glr_plot_compare_gpr_krr_005.png\"/>",
            "(data, target, label=\"True signal\", linewidth=2, linestyle=\"dashed\")\n(\n    training_data,\n    training_noisy_target,\n    color=\"black\",\n    label=\"Noisy measurements\",\n)\n# Plot the predictions of the kernel ridge\n(\n    data,\n    predictions_kr,\n    label=\"Kernel ridge\",\n    linewidth=2,\n    linestyle=\"dashdot\",\n)\n# Plot the predictions of the gaussian process regressor\n(\n    data,\n    mean_predictions_gpr,\n    label=\"Gaussian process regressor\",\n    linewidth=2,\n    linestyle=\"dotted\",\n)\n(\n    data.ravel(),\n    mean_predictions_gpr - std_predictions_gpr,\n    mean_predictions_gpr + std_predictions_gpr,\n    color=\"tab:green\",\n    alpha=0.2,\n)\n(loc=\"lower right\")\n(\"data\")\n(\"target\")\n_ = (\"Effect of using a radial basis function kernel\")\n\n\n<img alt=\"Effect of using a radial basis function kernel\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_compare_gpr_krr_006.png\" srcset=\"../../_images/sphx_glr_plot_compare_gpr_krr_006.png\"/>",
            "(tensor([[ 0.8503,  0.7027, -0.1229,  0.7019],\n        [ 0.4636,  0.3581,  0.4940,  0.0266],\n        [ 0.7999,  0.8178, -0.1399,  0.5714]], grad_fn=&lt;TanhBackward0&gt;), tensor([[ 0.8503,  0.7027, -0.1229,  0.7019],\n        [ 0.4636,  0.3581,  0.4940,  0.0266],\n        [ 0.7999,  0.8178, -0.1399,  0.5714]], grad_fn=&lt;TanhBackward0&gt;))\n(tensor([[ 0.8503,  0.7027, -0.1229,  0.7019],\n        [ 0.4636,  0.3581,  0.4940,  0.0266],\n        [ 0.7999,  0.8178, -0.1399,  0.5714]],\n       grad_fn=&lt;DifferentiableGraphBackward&gt;), tensor([[ 0.8503,  0.7027, -0.1229,  0.7019],\n        [ 0.4636,  0.3581,  0.4940,  0.0266],\n        [ 0.7999,  0.8178, -0.1399,  0.5714]],\n       grad_fn=&lt;DifferentiableGraphBackward&gt;))"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian Processes regression: basic introductory example->Example with noisy targets"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Comparison of kernel ridge and Gaussian process regression->Kernel methods: kernel ridge and Gaussian process->Kernel ridge"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Comparison of kernel ridge and Gaussian process regression->Kernel methods: kernel ridge and Gaussian process->Gaussian process regression"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Comparison of kernel ridge and Gaussian process regression->Final conclusion"
            ],
            [
                "torch->Deploying PyTorch Models in Production->Introduction to TorchScript->Basics of TorchScript->Tracing Modules"
            ]
        ]
    },
    "740194": {
        "jupyter_code_cell": "for feature in features_for_ks:\n    hist(data_agreement[data_agreement.signal == 1][feature].values, label='MC', **hist_kw)\n    hist(data_agreement[data_agreement.signal == 0][feature].values, label='real', **hist_kw)\n    legend()\n    title(feature)\n    show()\nfor feature in features_for_ks:\n    hist(data_agreement[data_agreement.signal == 1][feature].values,\n         weights=data_agreement[data_agreement.signal == 1]['weight'].values, label='MC', **hist_kw)\n    hist(data_agreement[data_agreement.signal == 0][feature].values,\n         weights=data_agreement[data_agreement.signal == 0]['weight'].values, label='real', **hist_kw)\n    title(feature)\n    legend()\n    show()",
        "matched_tutorial_code_inds": [
            6655,
            6258,
            3167,
            4423,
            2662
        ],
        "matched_tutorial_codes": [
            "for i in range(simulated.shape[1]):\n    simulated.iloc[:, i].plot(label=\"_\", color=\"gray\", alpha=0.1)\ndf[\"mean\"].plot(label=\"mean prediction\")\ndf[\"pi_lower\"].plot(linestyle=\"--\", color=\"tab:blue\", label=\"95% interval\")\ndf[\"pi_upper\"].plot(linestyle=\"--\", color=\"tab:blue\", label=\"_\")\npred.endog.plot(label=\"data\")\nplt.legend()",
            "for fit in [fit2, fit4]:\n    pd.DataFrame(np.c_[fit.level, fit.trend]).rename(\n        columns={0: \"level\", 1: \"slope\"}\n    ).plot(subplots=True)\nplt.show()\nprint(\n    \"Figure 7.4: Level and slope components for Holt\u2019s linear trend method and the additive damped trend method.\"\n)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_0.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_0.png\"/>\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_1.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_1.png\"/>",
            "for i in range(n_classes):\n    fpr[i], tpr[i], _ = (y_onehot_test[:, i], y_score[:, i])\n    roc_auc[i] = (fpr[i], tpr[i])\n\nfpr_grid = (0.0, 1.0, 1000)\n\n# Interpolate all ROC curves at these points\nmean_tpr = (fpr_grid)\n\nfor i in range(n_classes):\n    mean_tpr += (fpr_grid, fpr[i], tpr[i])  # linear interpolation\n\n# Average it and compute AUC\nmean_tpr /= n_classes\n\nfpr[\"macro\"] = fpr_grid\ntpr[\"macro\"] = mean_tpr\nroc_auc[\"macro\"] = (fpr[\"macro\"], tpr[\"macro\"])\n\nprint(f\"Macro-averaged One-vs-Rest ROC AUC score:\\n{roc_auc['macro']:.2f}\")",
            "for j, p in enumerate(points):\n...     plt.text(p[0]-0.03, p[1]+0.03, j, ha='right') # label the points\n for j, s in enumerate(tri.simplices):\n...     p = points[s].mean(axis=0)\n...     plt.text(p[0], p[1], '#%d' % j, ha='center') # label triangles\n plt.xlim(-0.5, 1.5); plt.ylim(-0.5, 1.5)\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code generates an X-Y plot with four green points annotated 0 through 3 roughly in the shape of a box. The box is outlined with a diagonal line between points 0 and 3 forming two adjacent triangles. The top triangle is annotated as #1 and the bottom triangle is annotated as #0.\"' class=\"plot-directive\" src=\"../_images/spatial-1.png\"/>\n</figure>",
            "m, s, _ = (\n    (enet.coef_)[0],\n    enet.coef_[enet.coef_ != 0],\n    markerfmt=\"x\",\n    label=\"Elastic net coefficients\",\n)\n([m, s], color=\"#2ca02c\")\nm, s, _ = (\n    (lasso.coef_)[0],\n    lasso.coef_[lasso.coef_ != 0],\n    markerfmt=\"x\",\n    label=\"Lasso coefficients\",\n)\n([m, s], color=\"#ff7f0e\")\n(\n    (coef)[0],\n    coef[coef != 0],\n    label=\"true coefficients\",\n    markerfmt=\"bx\",\n)\n\n(loc=\"best\")\n(\n    \"Lasso $R^2$: %.3f, Elastic Net $R^2$: %.3f\" % (r2_score_lasso, r2_score_enet)\n)\n()\n\n\n<img alt=\"Lasso $R^2$: 0.658, Elastic Net $R^2$: 0.643\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_lasso_and_elasticnet_001.png\" srcset=\"../../_images/sphx_glr_plot_lasso_and_elasticnet_001.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->ETS models->Predictions"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Method->Plots of Seasonally Adjusted Data"
            ],
            [
                "sklearn->Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->One-vs-Rest multiclass ROC->ROC curve using the OvR macro-average"
            ],
            [
                "scipy->Spatial data structures and algorithms (scipy.spatial)->Delaunay triangulations"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Lasso and Elastic Net for Sparse Signals->Plot"
            ]
        ]
    },
    "113426": {
        "jupyter_code_cell": "w=pd.DataFrame(priorities(comp_df), index = labels, columns = ('priorities',))\ndisplay(w.round(3))\nupdate_norm_priorities(w)\nlabels = ['Ticket', 'Add. services', 'Reductions']\ncomp_mat = np.array([[1,7,9],[1/7,1,3],[1/9,1/3,1]])\ncomp_df = pd.DataFrame(comp_mat, index = labels, columns=labels)\ndisplay(comp_df)\nind, mat_E = inconsistency(comp_df)\nprint('The consistency index is:',ind,'\\n')\nprint('The consistency deviation matrix:')\nprint(mat_E-1,'\\n')",
        "matched_tutorial_code_inds": [
            5998,
            5976,
            5969,
            5643,
            3773
        ],
        "matched_tutorial_codes": [
            "res5 = combine_effects(eff, var_eff, method_re=\"chi2\", use_t=False)\nres5_df = res5.summary_frame()\nprint(\"method RE:\", res5.method_re)\nprint(res5.summary_frame())\nfig = res5.plot_forest()\nfig.set_figheight(8)\nfig.set_figwidth(6)",
            "res3 = combine_effects(eff, var_eff, method_re=\"chi2\", use_t=False, row_names=rownames)\n# TODO: we still need better information about conf_int of individual samples\n# We don't have enough information in the model for individual confidence intervals\n# if those are not based on normal distribution.\nres3.conf_int_samples(nobs=np.array(nobs1 + nobs2))\nprint(res3.summary_frame())",
            "res3 = combine_effects(eff, var_eff, method_re=\"chi2\", use_t=True, row_names=rownames)\n# TODO: we still need better information about conf_int of individual samples\n# We don't have enough information in the model for individual confidence intervals\n# if those are not based on normal distribution.\nres3.conf_int_samples(nobs=np.array(nobs1 + nobs2))\nprint(res3.summary_frame())",
            "glm = smf.glm(\n    \"affairs ~ rate_marriage + yrs_married\",\n    data=dc,\n    family=sm.families.Poisson(),\n    freq_weights=np.asarray(dc[\"freq\"]),\n)\nres_f2 = glm.fit()\n# print(res_f2.summary())\nres_f2.pearson_chi2 - res_f.pearson_chi2, res_f2.deviance - res_f.deviance, res_f2.llf - res_f.llf",
            "df = df.assign(away_strength=df['away_team'].map(win_percent),\n               home_strength=df['home_team'].map(win_percent),\n               point_diff=df['home_points'] - df['away_points'],\n               rest_diff=df['home_rest'] - df['away_rest'])\ndf.head()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Meta-analysis of proportions->changing data to have positive random effects variance"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Example->Using one-step chi2, DerSimonian-Laird estimate for random effects variance tau"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Example->Using one-step chi2, DerSimonian-Laird estimate for random effects variance tau"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->original observations and frequency weights"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team"
            ]
        ]
    },
    "567628": {
        "jupyter_code_cell": "import matplotlib.pyplot as plt\nimport numpy as np\nimport sklearn \nimport pandas as pd",
        "matched_tutorial_code_inds": [
            1505,
            4713,
            6183,
            6455,
            4626
        ],
        "matched_tutorial_codes": [
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport statsmodels.api as sm",
            "import matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport numpy as np",
            "import pandas as pd\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm",
            "import numpy as np\nfrom scipy import stats\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm",
            "import matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy->What you\u2019ll need",
                "statsmodels->Examples->Robust Regression->Comparing OLS and RLM"
            ],
            [
                "matplotlib->Tutorials->Introductory->Animations using Matplotlib"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Time Series Filters"
            ],
            [
                "statsmodels->Examples->State space models->Statespace ARMA: Sunspots Data"
            ],
            [
                "matplotlib->Tutorials->Introductory->Quick start guide"
            ]
        ]
    },
    "1481291": {
        "jupyter_code_cell": "grid = sns.FacetGrid(df, col='quality',col_wrap = 2)\ngrid.map(plt.scatter,'alcohol','free sulfur dioxide',alpha = 0.2)\ngrid = sns.FacetGrid(df, col='quality',col_wrap = 2)\ngrid.map(plt.scatter,'pH','total sulfur dioxide',alpha = 0.2)",
        "matched_tutorial_code_inds": [
            5613,
            5976,
            5969,
            3773,
            2981
        ],
        "matched_tutorial_codes": [
            "gr = data[\"affairs rate_marriage age yrs_married\".split()].groupby(\n    \"rate_marriage age yrs_married\".split()\n)\ndf_a = gr.agg([\"mean\", \"sum\", \"count\"])\n\n\ndef merge_tuple(tpl):\n    if isinstance(tpl, tuple) and len(tpl)  1:\n        return \"_\".join(map(str, tpl))\n    else:\n        return tpl\n\n\ndf_a.columns = df_a.columns.map(merge_tuple)\ndf_a.reset_index(inplace=True)\nprint(df_a.shape)\ndf_a.head()",
            "res3 = combine_effects(eff, var_eff, method_re=\"chi2\", use_t=False, row_names=rownames)\n# TODO: we still need better information about conf_int of individual samples\n# We don't have enough information in the model for individual confidence intervals\n# if those are not based on normal distribution.\nres3.conf_int_samples(nobs=np.array(nobs1 + nobs2))\nprint(res3.summary_frame())",
            "res3 = combine_effects(eff, var_eff, method_re=\"chi2\", use_t=True, row_names=rownames)\n# TODO: we still need better information about conf_int of individual samples\n# We don't have enough information in the model for individual confidence intervals\n# if those are not based on normal distribution.\nres3.conf_int_samples(nobs=np.array(nobs1 + nobs2))\nprint(res3.summary_frame())",
            "df = df.assign(away_strength=df['away_team'].map(win_percent),\n               home_strength=df['home_team'].map(win_percent),\n               point_diff=df['home_points'] - df['away_points'],\n               rest_diff=df['home_rest'] - df['away_rest'])\ndf.head()",
            "sh_lle, sh_err = (\n    sh_points, n_neighbors=12, n_components=2\n)\n\nsh_tsne = (\n    n_components=2, perplexity=40, init=\"random\", random_state=0\n).fit_transform(sh_points)\n\nfig, axs = (figsize=(8, 8), nrows=2)\naxs[0].scatter(sh_lle[:, 0], sh_lle[:, 1], c=sh_color)\naxs[0].set_title(\"LLE Embedding of Swiss-Hole\")\naxs[1].scatter(sh_tsne[:, 0], sh_tsne[:, 1], c=sh_color)\n_ = axs[1].set_title(\"t-SNE Embedding of Swiss-Hole\")\n\n\n<img alt=\"LLE Embedding of Swiss-Hole, t-SNE Embedding of Swiss-Hole\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_swissroll_004.png\" srcset=\"../../_images/sphx_glr_plot_swissroll_004.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Condensing and Aggregating observations->Dataset with unique explanatory variables (exog)"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Example->Using one-step chi2, DerSimonian-Laird estimate for random effects variance tau"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Example->Using one-step chi2, DerSimonian-Laird estimate for random effects variance tau"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team"
            ],
            [
                "sklearn->Examples->Manifold learning->Swiss Roll And Swiss-Hole Reduction->Swiss-Hole"
            ]
        ]
    },
    "335237": {
        "jupyter_code_cell": "df_list = []\nvp_list = ['06', '07', '10', '11']\nsection_list = ['1', '2', '3']\nfor vp in vp_list:\n    for sec in section_list:\n        path = os.path.join(os.getcwd(),'..','data','data_behavioral','Expt1Pain_Behaviour_vp' + vp + '_' + sec + '.txt')\n        df = pd.read_csv(path, sep=\"\\t\", skiprows = [0])\n        df['vp'] = vp\n        df['section'] = sec\n        df_list.append(df)\ndf = pd.concat(df_list, ignore_index = True, join = 'inner')",
        "matched_tutorial_code_inds": [
            3173,
            1767,
            5998,
            2753,
            3122
        ],
        "matched_tutorial_codes": [
            "pair_scores = []\nmean_tpr = dict()\n\nfor ix, (label_a, label_b) in enumerate(pair_list):\n\n    a_mask = y_test == label_a\n    b_mask = y_test == label_b\n    ab_mask = (a_mask, b_mask)\n\n    a_true = a_mask[ab_mask]\n    b_true = b_mask[ab_mask]\n\n    idx_a = (label_binarizer.classes_ == label_a)[0]\n    idx_b = (label_binarizer.classes_ == label_b)[0]\n\n    fpr_a, tpr_a, _ = (a_true, y_score[ab_mask, idx_a])\n    fpr_b, tpr_b, _ = (b_true, y_score[ab_mask, idx_b])\n\n    mean_tpr[ix] = (fpr_grid)\n    mean_tpr[ix] += (fpr_grid, fpr_a, tpr_a)\n    mean_tpr[ix] += (fpr_grid, fpr_b, tpr_b)\n    mean_tpr[ix] /= 2\n    mean_score = (fpr_grid, mean_tpr[ix])\n    pair_scores.append(mean_score)\n\n    fig, ax = (figsize=(6, 6))\n    (\n        fpr_grid,\n        mean_tpr[ix],\n        label=f\"Mean {label_a} vs {label_b} (AUC = {mean_score :.2f})\",\n        linestyle=\":\",\n        linewidth=4,\n    )\n    (\n        a_true,\n        y_score[ab_mask, idx_a],\n        ax=ax,\n        name=f\"{label_a} as positive class\",\n    )\n    (\n        b_true,\n        y_score[ab_mask, idx_b],\n        ax=ax,\n        name=f\"{label_b} as positive class\",\n    )\n    ([0, 1], [0, 1], \"k--\", label=\"chance level (AUC = 0.5)\")\n    (\"square\")\n    (\"False Positive Rate\")\n    (\"True Positive Rate\")\n    (f\"{target_names[idx_a]} vs {label_b} ROC curves\")\n    ()\n    ()\n\nprint(f\"Macro-averaged One-vs-One ROC AUC score:\\n{(pair_scores):.2f}\")\n\n\n\n<img alt=\"setosa vs versicolor ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_004.png\" srcset=\"../../_images/sphx_glr_plot_roc_004.png\"/>\n<img alt=\"setosa vs virginica ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_005.png\" srcset=\"../../_images/sphx_glr_plot_roc_005.png\"/>\n<img alt=\"versicolor vs virginica ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_006.png\" srcset=\"../../_images/sphx_glr_plot_roc_006.png\"/>",
            "x_axis = []\ndata = {'positive sentiment': [], 'negative sentiment': []}\nfor speaker in predictions:\n    # The speakers will be used to label the x-axis in our plot\n    x_axis.append(speaker)\n    # number of paras with positive sentiment\n    no_pos_paras = len(predictions[speaker]['pos_paras'])\n    # number of paras with negative sentiment\n    no_neg_paras = len(predictions[speaker]['neg_paras'])\n    # Obtain percentage of paragraphs with positive predicted sentiment\n    pos_perc = no_pos_paras / (no_pos_paras + no_neg_paras)\n    # Store positive and negative percentages\n    data['positive sentiment'].append(pos_perc*100)\n    data['negative sentiment'].append(100*(1-pos_perc))\n\nindex = pd.Index(x_axis, name='speaker')\ndf = pd.DataFrame(data, index=index)\nax = df.plot(kind='bar', stacked=True)\nax.set_ylabel('percentage')\nax.legend(title='labels', bbox_to_anchor=(1, 1), loc='upper left')\nplt.show()",
            "res5 = combine_effects(eff, var_eff, method_re=\"chi2\", use_t=False)\nres5_df = res5.summary_frame()\nprint(\"method RE:\", res5.method_re)\nprint(res5.summary_frame())\nfig = res5.plot_forest()\nfig.set_figheight(8)\nfig.set_figwidth(6)",
            "quantiles = [0.05, 0.5, 0.95]\npredictions = {}\nout_bounds_predictions = (y_true_mean, dtype=)\nfor quantile in quantiles:\n    qr = (quantile=quantile, alpha=0, solver=solver)\n    y_pred = qr.fit(X, y_pareto).predict(X)\n    predictions[quantile] = y_pred\n\n    if quantile == min(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred = y_pareto\n        )\n    elif quantile == max(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred &lt;= y_pareto\n        )",
            "results = (list)\nn_bootstrap = 100\nrng = (seed=0)\n\nfor prevalence, X, y in zip(\n    populations[\"prevalence\"], populations[\"X\"], populations[\"y\"]\n):\n\n    results_for_prevalence = scoring_on_bootstrap(\n        estimator, X, y, rng, n_bootstrap=n_bootstrap\n    )\n    results[\"prevalence\"].append(prevalence)\n    results[\"metrics\"].append(\n        results_for_prevalence.aggregate([\"mean\", \"std\"]).unstack()\n    )\n\nresults = (results[\"metrics\"], index=results[\"prevalence\"])\nresults.index.name = \"prevalence\"\nresults\n\n\n\n\n\n\n\n\n<br/>\n<br/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->One-vs-One multiclass ROC->ROC curve using the OvO macro-average"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Sentiment Analysis on the Speech Data"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Meta-analysis of proportions->changing data to have positive random effects variance"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Quantile regression->Fitting a QuantileRegressor"
            ],
            [
                "sklearn->Examples->Model Selection->Class Likelihood Ratios to measure classification performance->Invariance with respect to prevalence"
            ]
        ]
    },
    "925431": {
        "jupyter_code_cell": "base_rate = class_data['target'].mean()\nprint(\"The class base rate is {}.\".format(round(base_rate*100,2)))\nbase_rate_dropped_duplicates = class_data.drop_duplicates()['target'].mean()\nprint(\"The class base rate when duplicates are dropped is {}.\".format(round(base_rate_dropped_duplicates*100,2)))\nclass_data.info()",
        "matched_tutorial_code_inds": [
            2944,
            2943,
            2860,
            2864,
            2844
        ],
        "matched_tutorial_codes": [
            "train_importances = (\n    train_result.importances[sorted_importances_idx].T,\n    columns=X.columns[sorted_importances_idx],\n)\ntest_importances = (\n    test_results.importances[sorted_importances_idx].T,\n    columns=X.columns[sorted_importances_idx],\n)",
            "train_result = (\n    rf, X_train, y_train, n_repeats=10, random_state=42, n_jobs=2\n)\ntest_results = (\n    rf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_importances_idx = train_result.importances_mean.argsort()",
            "mae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}\n\n_, ax = (figsize=(5, 5))\ndisplay = (\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n)\nax.set_title(\"Ridge model, optimum regularization\")\nfor name, score in scores.items():\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\nax.legend(loc=\"upper left\")\n()\n\n\n<img alt=\"Ridge model, optimum regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_012.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_012.png\"/>",
            "mae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}\n\n_, ax = (figsize=(6, 6))\ndisplay = (\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n)\nax.set_title(\"Lasso model, optimum regularization\")\nfor name, score in scores.items():\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\nax.legend(loc=\"upper left\")\n()\n\n\n<img alt=\"Lasso model, optimum regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_015.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_015.png\"/>",
            "mae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}\n\n_, ax = (figsize=(5, 5))\ndisplay = (\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n)\nax.set_title(\"Ridge model, small regularization\")\nfor name, score in scores.items():\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\nax.legend(loc=\"upper left\")\n()\n\n\n<img alt=\"Ridge model, small regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_009.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_009.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)"
            ],
            [
                "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ]
        ]
    },
    "618636": {
        "jupyter_code_cell": "df.Embarked.fillna('C', inplace=True)\ndf.iloc[[61,829]]\nnull_ages = df[df.Age.isnull()].groupby(['Pclass', 'Sex']).count().Name.unstack()\nnull_ages",
        "matched_tutorial_code_inds": [
            3827,
            6146,
            3616,
            3831,
            6704
        ],
        "matched_tutorial_codes": [
            "daily = df.fl_date.value_counts().sort_index()\ny = daily.resample('MS').mean()\ny.head()",
            "table = pd.DataFrame(data, columns=[\"lag\", \"AC\", \"Q\", \"Prob(Q)\"])\nprint(table.set_index(\"lag\"))",
            "hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()",
            "X = (pd.concat([y.shift(i) for i in range(6)], axis=1,\n               keys=['y'] + ['L%s' % i for i in range(1, 6)])\n       .dropna())\nX.head()",
            "ax = dta.mean().plot(grid=False)\nax.set_xlabel(\"Year\", size=17)\nax.set_ylabel(\"Fertility rate\", size=17)\nax.set_xlim(0, 51)"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ]
        ]
    },
    "1277599": {
        "jupyter_code_cell": "%matplotlib inline\nimport os\nimport numpy as np\nimport matplotlib.pyplot as mplt\nimport xarray as xr\nimport xarray.plot as xplt\nimport pandas as pd\nimport bottleneck as bn\nfrom ipywidgets import interact\necv_base = 'D:\\\\EOData\\\\CCI-TBX'\noc_pattern = os.path.join(ecv_base, 'occci-v2.0/data/geographic/netcdf/monthly/chlor_a/2010/*.nc')\nst_pattern = os.path.join(ecv_base, 'sst/data/lt/Analysis/L4/v01.1/2010/*.nc')\noc_ds = xr.open_mfdataset(oc_pattern)\nsst_ds = xr.open_mfdataset(st_pattern)",
        "matched_tutorial_code_inds": [
            6518,
            6207,
            3637,
            3745,
            3697
        ],
        "matched_tutorial_codes": [
            "%matplotlib inline\n\nfrom importlib import reload\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\nfrom scipy.stats import invwishart, invgamma\n\n# Get the macro dataset\ndta = sm.datasets.macrodata.load_pandas().data\ndta.index = pd.date_range('1959Q1', '2009Q3', freq='QS')",
            "%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\n# NBER recessions\nfrom pandas_datareader.data import DataReader\nfrom datetime import datetime\n\nusrec = DataReader(\n    \"USREC\", \"fred\", start=datetime(1947, 1, 1), end=datetime(2013, 4, 1)\n)",
            "%matplotlib inline\n\nimport json\nimport glob\nimport datetime\nfrom io import StringIO\n\nimport requests\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_style('ticks')\n\n# States are broken into networks. The networks have a list of ids, each representing a station.\n# We will take that list of ids and pass them as query parameters to the URL we built up ealier.\nstates = \"\"\"AK AL AR AZ CA CO CT DE FL GA HI IA ID IL IN KS KY LA MA MD ME\n MI MN MO MS MT NC ND NE NH NJ NM NV NY OH OK OR PA RI SC SD TN TX UT VA VT\n WA WI WV WY\"\"\".split()\n\n# IEM has Iowa AWOS sites in its own labeled network\nnetworks = ['AWOS'] + ['{}_ASOS'.format(state) for state in states]",
            "%matplotlib inline\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nif int(os.environ.get(\"MODERN_PANDAS_EPUB\", 0)):\n    import prep # noqa\n\npd.options.display.max_rows = 10\nsns.set(style='ticks', context='talk')",
            "%matplotlib inline\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nif int(os.environ.get(\"MODERN_PANDAS_EPUB\", 0)):\n    import prep # noqa\n\nsns.set_style('ticks')\nsns.set_context('talk')\npd.options.display.max_rows = 10"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Markov switching dynamic regression"
            ],
            [
                "pandas_toms_blog->Indexes"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data"
            ],
            [
                "pandas_toms_blog->Fast Pandas"
            ]
        ]
    },
    "905025": {
        "jupyter_code_cell": "from itertools import cycle\nplt.figure(1)\nplt.clf()\ncolors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')\nfor k, col in zip(range(n_clusters_), colors):\n    class_members = labels == k\n    cluster_center = X_train1[cluster_centers_indices[k]]\n    plt.plot(X_train1[class_members, 0], X_train1[class_members, 1], col + '.')\n    plt.plot(cluster_center[0],\n             cluster_center[1],\n             'o',\n             markerfacecolor=col,\n             markeredgecolor='k')\n    for x in X_train1[class_members]:\n        plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)\nplt.title('Estimated number of clusters: {}'.format(n_clusters_))\nplt.show()\nbandwidth = estimate_bandwidth(X_train2, quantile=0.2, n_samples=500)\nms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\nms.fit(X_train2)\nlabels = ms.labels_\ncluster_centers = ms.cluster_centers_\nn_clusters_ = len(np.unique(labels))\nprint(\"Number of estimated clusters: {}\".format(n_clusters_))",
        "matched_tutorial_code_inds": [
            1895,
            3170,
            4760,
            5808,
            2338
        ],
        "matched_tutorial_codes": [
            "from matplotlib import cm\nimport matplotlib.pyplot as plt\n\n()\ny_unique = (y)\ncolors = cm.rainbow((0.0, 1.0, y_unique.size))\nfor this_y, color in zip(y_unique, colors):\n    this_X = X_train[y_train == this_y]\n    this_sw = sw_train[y_train == this_y]\n    (\n        this_X[:, 0],\n        this_X[:, 1],\n        s=this_sw * 50,\n        c=color[, :],\n        alpha=0.5,\n        edgecolor=\"k\",\n        label=\"Class %s\" % this_y,\n    )\n(loc=\"best\")\n(\"Data\")\n\n()\n\norder = ((prob_pos_clf,))\n(prob_pos_clf[order], \"r\", label=\"No calibration (%1.3f)\" % clf_score)\n(\n    prob_pos_isotonic[order],\n    \"g\",\n    linewidth=3,\n    label=\"Isotonic calibration (%1.3f)\" % clf_isotonic_score,\n)\n(\n    prob_pos_sigmoid[order],\n    \"b\",\n    linewidth=3,\n    label=\"Sigmoid calibration (%1.3f)\" % clf_sigmoid_score,\n)\n(\n    (0, y_test.size, 51)[1::2],\n    y_test[order].reshape(25, -1).mean(1),\n    \"k\",\n    linewidth=3,\n    label=r\"Empirical\",\n)\n([-0.05, 1.05])\n(\"Instances sorted according to predicted probability (uncalibrated GNB)\")\n(\"P(y=1)\")\n(loc=\"upper left\")\n(\"Gaussian naive Bayes probabilities\")\n\n()\n\n\n\n<img alt=\"Data\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_calibration_001.png\" srcset=\"../../_images/sphx_glr_plot_calibration_001.png\"/>\n<img alt=\"Gaussian naive Bayes probabilities\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_calibration_002.png\" srcset=\"../../_images/sphx_glr_plot_calibration_002.png\"/>",
            "from itertools import \n\nfig, ax = (figsize=(6, 6))\n\n(\n    fpr[\"micro\"],\n    tpr[\"micro\"],\n    label=f\"micro-average ROC curve (AUC = {roc_auc['micro']:.2f})\",\n    color=\"deeppink\",\n    linestyle=\":\",\n    linewidth=4,\n)\n\n(\n    fpr[\"macro\"],\n    tpr[\"macro\"],\n    label=f\"macro-average ROC curve (AUC = {roc_auc['macro']:.2f})\",\n    color=\"navy\",\n    linestyle=\":\",\n    linewidth=4,\n)\n\ncolors = ([\"aqua\", \"darkorange\", \"cornflowerblue\"])\nfor class_id, color in zip(range(n_classes), colors):\n    (\n        y_onehot_test[:, class_id],\n        y_score[:, class_id],\n        name=f\"ROC curve for {target_names[class_id]}\",\n        color=color,\n        ax=ax,\n    )\n\n([0, 1], [0, 1], \"k--\", label=\"ROC curve for chance level (AUC = 0.5)\")\n(\"square\")\n(\"False Positive Rate\")\n(\"True Positive Rate\")\n(\"Extension of Receiver Operating Characteristic\\nto One-vs-Rest multiclass\")\n()\n()\n\n\n<img alt=\"Extension of Receiver Operating Characteristic to One-vs-Rest multiclass\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_roc_003.png\" srcset=\"../../_images/sphx_glr_plot_roc_003.png\"/>",
            "from numpy.random import randn\n\nz = randn(10)\n\nfig, ax = plt.subplots()\nred_dot, = ax.plot(z, \"ro\", markersize=15)\n# Put a white cross over some of the data.\nwhite_cross, = ax.plot(z[:5], \"w+\", markeredgewidth=3, markersize=15)\n\nax.legend([red_dot, (red_dot, white_cross)], [\"Attr A\", \"Attr A+B\"])\n\n\n<img alt=\"legend guide\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_legend_guide_009.png\" srcset=\"../../_images/sphx_glr_legend_guide_009.png, ../../_images/sphx_glr_legend_guide_009_2_0x.png 2.0x\"/>",
            "from matplotlib.patches import Ellipse\n\nfig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(\n    111,\n    xlabel=\"log(Temp)\",\n    ylabel=\"log(Light)\",\n    title=\"Hertzsprung-Russell Diagram of Star Cluster CYG OB1\",\n)\nax.scatter(*dta.values.T)\n# highlight outliers\ne = Ellipse((3.5, 6), 0.2, 1, alpha=0.25, color=\"r\")\nax.add_patch(e)\nax.annotate(\n    \"Red giants\",\n    xy=(3.6, 6),\n    xytext=(3.8, 6),\n    arrowprops=dict(facecolor=\"black\", shrink=0.05, width=2),\n    horizontalalignment=\"left\",\n    verticalalignment=\"bottom\",\n    clip_on=True,  # clip to the axes bounding box\n    fontsize=16,\n)\n# annotate these with their index\nfor i, row in dta.loc[dta[\"log.Te\"] &lt; 3.8].iterrows():\n    ax.annotate(i, row, row + 0.01, fontsize=14)\nxlim, ylim = ax.get_xlim(), ax.get_ylim()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_robust_models_1_75_0.png\" src=\"../../../_images/examples_notebooks_generated_robust_models_1_75_0.png\"/>",
            "from itertools import \n\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets\nfrom sklearn.tree import \nfrom sklearn.neighbors import \nfrom sklearn.svm import \nfrom sklearn.ensemble import \nfrom sklearn.inspection import DecisionBoundaryDisplay\n\n# Loading some example data\niris = ()\nX = iris.data[:, [0, 2]]\ny = iris.target\n\n# Training classifiers\nclf1 = (max_depth=4)\nclf2 = (n_neighbors=7)\nclf3 = (gamma=0.1, kernel=\"rbf\", probability=True)\neclf = (\n    estimators=[(\"dt\", clf1), (\"knn\", clf2), (\"svc\", clf3)],\n    voting=\"soft\",\n    weights=[2, 1, 2],\n)\n\nclf1.fit(X, y)\nclf2.fit(X, y)\nclf3.fit(X, y)\neclf.fit(X, y)\n\n# Plotting decision regions\nf, axarr = (2, 2, sharex=\"col\", sharey=\"row\", figsize=(10, 8))\nfor idx, clf, tt in zip(\n    ([0, 1], [0, 1]),\n    [clf1, clf2, clf3, eclf],\n    [\"Decision Tree (depth=4)\", \"KNN (k=7)\", \"Kernel SVM\", \"Soft Voting\"],\n):\n    (\n        clf, X, alpha=0.4, ax=axarr[idx[0], idx[1]], response_method=\"predict\"\n    )\n    axarr[idx[0], idx[1]].scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor=\"k\")\n    axarr[idx[0], idx[1]].set_title(tt)\n\n()"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Calibration->Probability calibration of classifiers->Plot data and the predicted probabilities"
            ],
            [
                "sklearn->Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->One-vs-Rest multiclass ROC->Plot all OvR ROC curves together"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Legend guide->Legend Handlers"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Hertzprung Russell data for Star Cluster CYG 0B1 - Leverage Points"
            ],
            [
                "sklearn->Examples->Ensemble methods->Plot the decision boundaries of a VotingClassifier"
            ]
        ]
    },
    "950328": {
        "jupyter_code_cell": "df.rename(columns={'q(C) (|e|)': 'q(C)', 'Total C WBI': 'C_WBI',\n                  'C 2pz NAO occupancies (|e|)':'NAO_occ','gap(eV)':'gap',\n                   'Volume (A^3)':'volume'\n                  }, inplace=True)\ndf.head(1)\npal = sns.diverging_palette(220,10, sep=80, n=2)\ncmap = sns.diverging_palette(220,10, sep=80, as_cmap=True)\nsns.palplot(pal)",
        "matched_tutorial_code_inds": [
            6267,
            6265,
            3773,
            5712,
            3801
        ],
        "matched_tutorial_codes": [
            "df = pd.DataFrame(\n    np.c_[aust, fit2.level, fit2.trend, fit2.season, fit2.fittedvalues],\n    columns=[r\"$y_t$\", r\"$l_t$\", r\"$b_t$\", r\"$s_t$\", r\"$\\hat{y}_t$\"],\n    index=aust.index,\n)\ndf.append(fit2.forecast(8).rename(r\"$\\hat{y}_t$\").to_frame(), sort=True)",
            "df = pd.DataFrame(\n    np.c_[aust, fit1.level, fit1.trend, fit1.season, fit1.fittedvalues],\n    columns=[r\"$y_t$\", r\"$l_t$\", r\"$b_t$\", r\"$s_t$\", r\"$\\hat{y}_t$\"],\n    index=aust.index,\n)\ndf.append(fit1.forecast(8).rename(r\"$\\hat{y}_t$\").to_frame(), sort=True)",
            "df = df.assign(away_strength=df['away_team'].map(win_percent),\n               home_strength=df['home_team'].map(win_percent),\n               point_diff=df['home_points'] - df['away_points'],\n               rest_diff=df['home_rest'] - df['away_rest'])\ndf.head()",
            "df = pd.read_csv(raw, header=None)\ndf = df.melt()\ndf[\"site\"] = 1 + np.floor(df.index / 10).astype(int)\ndf[\"variety\"] = 1 + (df.index % 10)\ndf = df.rename(columns={\"value\": \"blotch\"})\ndf = df.drop(\"variable\", axis=1)\ndf[\"blotch\"] /= 100",
            "df = sns.load_dataset('titanic')\n\nclf = RandomForestClassifier()\nparam_grid = dict(max_depth=[1, 2, 5, 10, 20, 30, 40],\n                  min_samples_split=[2, 5, 10],\n                  min_samples_leaf=[2, 3, 5])\nest = GridSearchCV(clf, param_grid=param_grid, n_jobs=4)\n\ny = df['survived']\nX = df.drop(['survived', 'who', 'alive'], axis=1)\n\nX = pd.get_dummies(X, drop_first=True)\nX = X.fillna(value=X.median())\nest.fit(X, y);"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Winters Seasonal->The Internals"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Winters Seasonal->The Internals"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Quasi-binomial regression"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ]
        ]
    },
    "167514": {
        "jupyter_code_cell": "data.head()\ndata['plot_clean'] = data['plot'].apply(clean_sentence)",
        "matched_tutorial_code_inds": [
            3656,
            2801,
            3792,
            3802,
            6804
        ],
        "matched_tutorial_codes": [
            "weather2 = weather.reset_index()\nweather2[weather2['station'] == 'DSM'].head()",
            "X = survey.data[survey.feature_names]\nX.describe(include=\"all\")\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "sns.countplot(x='cut', data=df)\nsns.despine()\nplt.tight_layout()",
            "scores = pd.DataFrame(est.cv_results_)\nscores.head()",
            "endog = macrodata['infl']\nendog.plot(figsize=(15, 5))"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Indexes->Flavors->Row Slicing"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example"
            ]
        ]
    },
    "778867": {
        "jupyter_code_cell": "n=5000\nmu=[0,0]\nsigma = [[5,5],[5,10]]\nalpha_0=0.01\ndegreesFreedom=len(mu)\nmormalizerMatrix, eigenValues, eigenVectors=mvNormalizerMatrix(sigma)\nnormalSampleDF = sc.parallelize(np.ones(n))                 .map(lambda y: multivariateNormalVector(y,mu,mormalizerMatrix))                 .toDF(['x_1','x_2'])",
        "matched_tutorial_code_inds": [
            2060,
            5175,
            5168,
            6790,
            2124
        ],
        "matched_tutorial_codes": [
            "n = 1000\np = 10\nX = (size=n * p).reshape((n, p))\ny = X[:, 0] + 2 * X[:, 1] + (size=n * 1) + 5\npls1 = (n_components=3)\npls1.fit(X, y)\n# note that the number of components exceeds 1 (the dimension of y)\nprint(\"Estimated betas\")\nprint(np.round(pls1.coef_, 1))",
            "nsample = 50\ngroups = np.zeros(nsample, int)\ngroups[20:40] = 1\ngroups[40:] = 2\n# dummy = (groups[:,None] == np.unique(groups)).astype(float)\n\ndummy = pd.get_dummies(groups).values\nx = np.linspace(0, 20, nsample)\n# drop reference category\nX = np.column_stack((x, dummy[:, 1:]))\nX = sm.add_constant(X, prepend=False)\n\nbeta = [1.0, 3, -3, 10]\ny_true = np.dot(X, beta)\ne = np.random.normal(size=nsample)\ny = y_true + e",
            "nsample = 50\nsig = 0.5\nx = np.linspace(0, 20, nsample)\nX = np.column_stack((x, np.sin(x), (x - 5) ** 2, np.ones(nsample)))\nbeta = [0.5, 0.5, -0.02, 5.0]\n\ny_true = np.dot(X, beta)\ny = y_true + sig * np.random.normal(size=nsample)",
            "nsample = 50\nsig = 0.25\nx1 = np.linspace(0, 20, nsample)\nX = np.column_stack((x1, np.sin(x1), (x1 - 5) ** 2))\nX = sm.add_constant(X)\nbeta = [5.0, 0.5, 0.5, -0.02]\ny_true = np.dot(X, beta)\ny = y_true + sig * np.random.normal(size=nsample)",
            "n_comps = 2\n\nmethods = [\n    (\"PCA\", ()),\n    (\"Unrotated FA\", ()),\n    (\"Varimax FA\", (rotation=\"varimax\")),\n]\nfig, axes = (ncols=len(methods), figsize=(10, 8), sharey=True)\n\nfor ax, (method, fa) in zip(axes, methods):\n    fa.set_params(n_components=n_comps)\n    fa.fit(X)\n\n    components = fa.components_.T\n    print(\"\\n\\n %s :\\n\" % method)\n    print(components)\n\n    vmax = np.abs(components).max()\n    ax.imshow(components, cmap=\"RdBu_r\", vmax=vmax, vmin=-vmax)\n    ax.set_yticks((len(feature_names)))\n    ax.set_yticklabels(feature_names)\n    ax.set_title(str(method))\n    ax.set_xticks([0, 1])\n    ax.set_xticklabels([\"Comp. 1\", \"Comp. 2\"])\nfig.suptitle(\"Factors\")\n()\n()\n\n\n<img alt=\"Factors, PCA, Unrotated FA, Varimax FA\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_varimax_fa_002.png\" srcset=\"../../_images/sphx_glr_plot_varimax_fa_002.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Cross decomposition->Compare cross decomposition methods->PLS regression, with univariate response, a.k.a. PLS1"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->OLS with dummy variables"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->OLS non-linear curve but linear in parameters"
            ],
            [
                "statsmodels->Examples->User Notes->Prediction->Artificial data"
            ],
            [
                "sklearn->Examples->Decomposition->Factor Analysis (with rotation) to visualize patterns"
            ]
        ]
    },
    "253067": {
        "jupyter_code_cell": "total_restaurants_boro = dba_boro_restaurant['BORO'].value_counts() \nboro_fraction = non_chains / total_restaurants_boro\nboro_fraction.plot(kind='bar')\ncuisine = df[['CUISINE DESCRIPTION','RESTAURANT']].drop_duplicates(subset='RESTAURANT')\ncuisine['CUISINE DESCRIPTION'].value_counts()[:20].plot(kind = 'bar')",
        "matched_tutorial_code_inds": [
            2298,
            6700,
            6337,
            1962,
            5643
        ],
        "matched_tutorial_codes": [
            "feature_importance = reg.feature_importances_\nsorted_idx = (feature_importance)\npos = (sorted_idx.shape[0]) + 0.5\nfig = (figsize=(12, 6))\n(1, 2, 1)\n(pos, feature_importance[sorted_idx], align=\"center\")\n(pos, (diabetes.feature_names)[sorted_idx])\n(\"Feature Importance (MDI)\")\n\nresult = (\n    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_idx = result.importances_mean.argsort()\n(1, 2, 2)\n(\n    result.importances[sorted_idx].T,\n    vert=False,\n    labels=(diabetes.feature_names)[sorted_idx],\n)\n(\"Permutation Importance (test set)\")\nfig.tight_layout()\n()\n\n\n<img alt=\"Feature Importance (MDI), Permutation Importance (test set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\"/>",
            "ln_pce = np.log(pce.PCE)\nforecasts = {\"ln PCE\": ln_pce}\nfor year in range(1995, 2020, 3):\n    sub = ln_pce[: str(year)]\n    res = ThetaModel(sub).fit()\n    fcast = res.forecast(12)\n    forecasts[str(year)] = fcast\nforecasts = pd.DataFrame(forecasts)\nax = forecasts[\"1995\":].plot(legend=False)\nchildren = ax.get_children()\nchildren[0].set_linewidth(4)\nchildren[0].set_alpha(0.3)\nchildren[0].set_color(\"#000000\")\nax.set_title(\"ln PCE\")\nplt.tight_layout(pad=1.0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_theta-model_22_0.png\" src=\"../../../_images/examples_notebooks_generated_theta-model_22_0.png\"/>",
            "intercept = [\n    ar0_res.params[0],\n    sarimax_res.params[0],\n    arima_res.params[0],\n    autoreg_res.params[0],\n]\nrho_hat = [0] + [r.params[1] for r in (sarimax_res, arima_res, autoreg_res)]\nlong_run = [\n    ar0_res.params[0],\n    sarimax_res.params[0] / (1 - sarimax_res.params[1]),\n    arima_res.params[0],\n    autoreg_res.params[0] / (1 - autoreg_res.params[1]),\n]\ncols = [\"AR(0)\", \"SARIMAX\", \"ARIMA\", \"AutoReg\"]\npd.DataFrame(\n    [intercept, rho_hat, long_run],\n    columns=cols,\n    index=[\"delta-or-phi\", \"rho\", \"long-run mean\"],\n)",
            "unique_labels = set(labels)\ncore_samples_mask = (labels, dtype=bool)\ncore_samples_mask[db.core_sample_indices_] = True\n\ncolors = [plt.cm.Spectral(each) for each in (0, 1, len(unique_labels))]\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        # Black used for noise.\n        col = [0, 0, 0, 1]\n\n    class_member_mask = labels == k\n\n    xy = X[class_member_mask & core_samples_mask]\n    (\n        xy[:, 0],\n        xy[:, 1],\n        \"o\",\n        markerfacecolor=tuple(col),\n        markeredgecolor=\"k\",\n        markersize=14,\n    )\n\n    xy = X[class_member_mask & ~core_samples_mask]\n    (\n        xy[:, 0],\n        xy[:, 1],\n        \"o\",\n        markerfacecolor=tuple(col),\n        markeredgecolor=\"k\",\n        markersize=6,\n    )\n\n(f\"Estimated number of clusters: {n_clusters_}\")\n()\n\n\n<img alt=\"Estimated number of clusters: 3\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_dbscan_002.png\" srcset=\"../../_images/sphx_glr_plot_dbscan_002.png\"/>",
            "glm = smf.glm(\n    \"affairs ~ rate_marriage + yrs_married\",\n    data=dc,\n    family=sm.families.Poisson(),\n    freq_weights=np.asarray(dc[\"freq\"]),\n)\nres_f2 = glm.fit()\n# print(res_f2.summary())\nres_f2.pearson_chi2 - res_f.pearson_chi2, res_f2.deviance - res_f.deviance, res_f2.llf - res_f.llf"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Ensemble methods->Gradient Boosting regression->Plot feature importance"
            ],
            [
                "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Personal Consumption Expenditure"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Comparing trends and exogenous variables in SARIMAX, ARIMA and AutoReg"
            ],
            [
                "sklearn->Examples->Clustering->Demo of DBSCAN clustering algorithm->Plot results"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->original observations and frequency weights"
            ]
        ]
    },
    "687339": {
        "jupyter_code_cell": "from sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB(alpha=1)\nclf.fit(X_train,y_train)\ny_pred = clf.predict(X_test)\nprint(\"Accuracy on Training Data: %f\" % clf.score(X_train,y_train))\nprint(\"Accuracy on Test Data: %f\" % clf.score(X_test,y_test))",
        "matched_tutorial_code_inds": [
            2660,
            2787,
            2658,
            2504,
            2728
        ],
        "matched_tutorial_codes": [
            "from sklearn.linear_model import \n\nenet = (alpha=alpha, l1_ratio=0.7)\n\ny_pred_enet = enet.fit(X_train, y_train).predict(X_test)\nr2_score_enet = (y_test, y_pred_enet)\nprint(enet)\nprint(\"r^2 on test data : %f\" % r2_score_enet)",
            "from sklearn.linear_model import \n\n\nmask_train = df_train[\"ClaimAmount\"]  0\nmask_test = df_test[\"ClaimAmount\"]  0\n\nglm_sev = (alpha=10.0, solver=\"newton-cholesky\")\n\nglm_sev.fit(\n    X_train[mask_train.values],\n    df_train.loc[mask_train, \"AvgClaimAmount\"],\n    sample_weight=df_train.loc[mask_train, \"ClaimNb\"],\n)\n\nscores = score_estimator(\n    glm_sev,\n    X_train[mask_train.values],\n    X_test[mask_test.values],\n    df_train[mask_train],\n    df_test[mask_test],\n    target=\"AvgClaimAmount\",\n    weights=\"ClaimNb\",\n)\nprint(\"Evaluation of GammaRegressor on target AvgClaimAmount\")\nprint(scores)",
            "from sklearn.linear_model import \n\nalpha = 0.1\nlasso = (alpha=alpha)\n\ny_pred_lasso = lasso.fit(X_train, y_train).predict(X_test)\nr2_score_lasso = (y_test, y_pred_lasso)\nprint(lasso)\nprint(\"r^2 on test data : %f\" % r2_score_lasso)",
            "from sklearn.datasets import \n\nX, y = (\n    n_samples=500,\n    n_features=15,\n    n_informative=3,\n    n_redundant=2,\n    n_repeated=0,\n    n_classes=8,\n    n_clusters_per_class=1,\n    class_sep=0.8,\n    random_state=0,\n)",
            "from sklearn.linear_model import \n\nn_samples = df_train.shape[0]\n\npoisson_glm = (\n    [\n        (\"preprocessor\", linear_model_preprocessor),\n        (\"regressor\", (alpha=1e-12, solver=\"newton-cholesky\")),\n    ]\n)\npoisson_glm.fit(\n    df_train, df_train[\"Frequency\"], regressor__sample_weight=df_train[\"Exposure\"]\n)\n\nprint(\"PoissonRegressor evaluation:\")\nscore_estimator(poisson_glm, df_test)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Generalized Linear Models->Lasso and Elastic Net for Sparse Signals->ElasticNet"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Tweedie regression on insurance claims->Severity Model -  Gamma distribution"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Lasso and Elastic Net for Sparse Signals->Lasso"
            ],
            [
                "sklearn->Examples->Feature Selection->Recursive feature elimination with cross-validation->Data generation"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Poisson regression and non-normal loss->(Generalized) linear models"
            ]
        ]
    },
    "219007": {
        "jupyter_code_cell": "from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nwss = []  \nns = range(2, 11)\nfor i in ns:\n    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=500, n_init=20, random_state=0)\n    kmeans.fit(X)\n    wss.append(kmeans.inertia_)",
        "matched_tutorial_code_inds": [
            2355,
            2308,
            2542,
            2018,
            2345
        ],
        "matched_tutorial_codes": [
            "from sklearn.base import clone\n\nalpha = 0.95\nneg_mean_pinball_loss_95p_scorer = (\n    ,\n    alpha=alpha,\n    greater_is_better=False,  # maximize the negative loss\n)\nsearch_95p = clone(search_05p).set_params(\n    estimator__alpha=alpha,\n    scoring=neg_mean_pinball_loss_95p_scorer,\n)\nsearch_95p.fit(X_train, y_train)\n(search_95p.best_params_)",
            "from sklearn.ensemble import \nfrom sklearn.inspection import PartialDependenceDisplay\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nrng = (0)\n\nn_samples = 1000\nf_0 = rng.rand(n_samples)\nf_1 = rng.rand(n_samples)\nX = [f_0, f_1]\nnoise = rng.normal(loc=0.0, scale=0.01, size=n_samples)\n\n# y is positively correlated with f_0, and negatively correlated with f_1\ny = 5 * f_0 + (10 *  * f_0) - 5 * f_1 - (10 *  * f_1) + noise",
            "from sklearn.model_selection import \nfrom sklearn.utils.fixes import loguniform\n\nparam_distributions = {\n    \"alpha\": loguniform(1e0, 1e3),\n    \"kernel__length_scale\": loguniform(1e-2, 1e2),\n    \"kernel__periodicity\": loguniform(1e0, 1e1),\n}\nkernel_ridge_tuned = (\n    kernel_ridge,\n    param_distributions=param_distributions,\n    n_iter=500,\n    random_state=0,\n)\nstart_time = ()\nkernel_ridge_tuned.fit(training_data, training_noisy_target)\nprint(f\"Time for KernelRidge fitting: {() - start_time:.3f} seconds\")",
            "from sklearn.cluster import \nimport matplotlib.pyplot as plt\n\nlabels = (graph, n_clusters=4, eigen_solver=\"arpack\")\nlabel_im = (mask.shape, -1.0)\nlabel_im[mask] = labels\n\nfig, axs = (nrows=1, ncols=2, figsize=(10, 5))\naxs[0].matshow(img)\naxs[1].matshow(label_im)\n\n()\n\n\n<img alt=\"plot segmentation toy\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_segmentation_toy_001.png\" srcset=\"../../_images/sphx_glr_plot_segmentation_toy_001.png\"/>",
            "from sklearn.ensemble import \nfrom sklearn.metrics import , \n\n\nall_models = {}\ncommon_params = dict(\n    learning_rate=0.05,\n    n_estimators=200,\n    max_depth=2,\n    min_samples_leaf=9,\n    min_samples_split=9,\n)\nfor alpha in [0.05, 0.5, 0.95]:\n    gbr = (loss=\"quantile\", alpha=alpha, **common_params)\n    all_models[\"q %1.2f\" % alpha] = gbr.fit(X_train, y_train)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Ensemble methods->Prediction Intervals for Gradient Boosting Regression->Tuning the hyper-parameters of the quantile regressors"
            ],
            [
                "sklearn->Examples->Ensemble methods->Monotonic Constraints"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Comparison of kernel ridge and Gaussian process regression->Kernel methods: kernel ridge and Gaussian process->Kernel ridge"
            ],
            [
                "sklearn->Examples->Clustering->Spectral clustering for image segmentation->Plotting four circles"
            ],
            [
                "sklearn->Examples->Ensemble methods->Prediction Intervals for Gradient Boosting Regression->Fitting non-linear quantile and least squares regressors"
            ]
        ]
    },
    "383696": {
        "jupyter_code_cell": "import pandas as pd\nimport matplotlib.pyplot as plt\nsal_by_reg = pd.read_csv('salaries-by-region.csv', index_col='Region')\nclean_df = sal_by_reg.dropna(axis=0).replace({'\\,':''}, regex = True).replace({'\\$':''}, regex = True)\nten_num = pd.to_numeric(clean_df['Mid-Career 10th Percentile Salary'], errors='coerce')",
        "matched_tutorial_code_inds": [
            3531,
            4946,
            2528,
            3410,
            4682
        ],
        "matched_tutorial_codes": [
            "import pandas as pd\nimport matplotlib.pyplot as plt\n\nfig, (ax0, ax1) = (ncols=2, figsize=(16, 6), sharey=True)\n\ndf = (evaluations[::-1]).set_index(\"estimator\")\ndf_std = (evaluations_std[::-1]).set_index(\"estimator\")\n\ndf.drop(\n    [\"train_time\"],\n    axis=\"columns\",\n).plot.barh(ax=ax0, xerr=df_std)\nax0.set_xlabel(\"Clustering scores\")\nax0.set_ylabel(\"\")\n\ndf[\"train_time\"].plot.barh(ax=ax1, xerr=df_std[\"train_time\"])\nax1.set_xlabel(\"Clustering time (s)\")\n()\n\n\n<img alt=\"plot document clustering\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_document_clustering_001.png\" srcset=\"../../_images/sphx_glr_plot_document_clustering_001.png\"/>",
            "import matplotlib.pyplot as plt\nimport numpy as np\n\nx1 = np.linspace(0.0, 5.0, 100)\ny1 = np.cos(2 * np.pi * x1) * np.exp(-x1)\n\nfig, ax = plt.subplots(figsize=(5, 3))\nfig.subplots_adjust(bottom=0.15, left=0.2)\nax.plot(x1, y1)\nax.set_xlabel('Time [s]')\nax.set_ylabel('Damped oscillation [V]')\n\nplt.show()\n\n\n<img alt=\"text intro\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_text_intro_002.png\" srcset=\"../../_images/sphx_glr_text_intro_002.png, ../../_images/sphx_glr_text_intro_002_2_0x.png 2.0x\"/>",
            "import pandas as pd\n\ndf = (grid_search.cv_results_)[\n    [\"param_n_components\", \"param_covariance_type\", \"mean_test_score\"]\n]\ndf[\"mean_test_score\"] = -df[\"mean_test_score\"]\ndf = df.rename(\n    columns={\n        \"param_n_components\": \"Number of components\",\n        \"param_covariance_type\": \"Type of covariance\",\n        \"mean_test_score\": \"BIC score\",\n    }\n)\ndf.sort_values(by=\"BIC score\").head()\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "import pandas as pd\nfrom sklearn.decomposition import \n\npca = (n_components=2).fit(X_train)\nscaled_pca = (n_components=2).fit(scaled_X_train)\nX_train_transformed = pca.transform(X_train)\nX_train_std_transformed = scaled_pca.transform(scaled_X_train)\n\nfirst_pca_component = (\n    pca.components_[0], index=X.columns, columns=[\"without scaling\"]\n)\nfirst_pca_component[\"with scaling\"] = scaled_pca.components_[0]\nfirst_pca_component.plot.bar(\n    title=\"Weights of the first principal component\", figsize=(6, 8)\n)\n\n_ = ()\n\n\n<img alt=\"Weights of the first principal component\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_scaling_importance_002.png\" srcset=\"../../_images/sphx_glr_plot_scaling_importance_002.png\"/>",
            "import numpy as np\nimport matplotlib.pyplot as plt\n\n\ndata = {'Barton LLC': 109438.50,\n        'Frami, Hills and Schmidt': 103569.59,\n        'Fritsch, Russel and Anderson': 112214.71,\n        'Jerde-Hilpert': 112591.43,\n        'Keeling LLC': 100934.30,\n        'Koepp Ltd': 103660.54,\n        'Kulas Inc': 137351.96,\n        'Trantow-Barrows': 123381.38,\n        'White-Trantow': 135841.99,\n        'Will LLC': 104437.60}\ngroup_data = list(data.values())\ngroup_names = list(data.keys())\ngroup_mean = np.mean(group_data)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Working with text documents->Clustering text documents using k-means->Clustering evaluation summary"
            ],
            [
                "matplotlib->Tutorials->Text->Text in Matplotlib Plots->Labels for x- and y-axis"
            ],
            [
                "sklearn->Examples->Gaussian Mixture Models->Gaussian Mixture Model Selection->Plot the BIC scores"
            ],
            [
                "sklearn->Examples->Preprocessing->Importance of Feature Scaling->Effect of rescaling on a PCA dimensional reduction"
            ],
            [
                "matplotlib->Tutorials->Introductory->The Lifecycle of a Plot->Our data"
            ]
        ]
    },
    "1363504": {
        "jupyter_code_cell": "import pandas as pd \nfrom IPython.display import display\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc \nfrom datetime import datetime as dt \nfrom matplotlib.font_manager import FontProperties\nimport seaborn as sns \nimport numpy as np\nfrom mapsplotlib import mapsplot as mplt",
        "matched_tutorial_code_inds": [
            2984,
            5287,
            6270,
            2037,
            6092
        ],
        "matched_tutorial_codes": [
            "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import \nfrom sklearn.neural_network import \nfrom sklearn.preprocessing import \nfrom sklearn.pipeline import \nfrom sklearn.tree import \nfrom sklearn.inspection import PartialDependenceDisplay",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom statsmodels.iolib.table import SimpleTable, default_txt_fmt\n\nnp.random.seed(1024)",
            "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom pandas.plotting import register_matplotlib_converters\n\nregister_matplotlib_converters()\nsns.set_style(\"darkgrid\")",
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.linalg import , \n\nfrom sklearn.covariance import , \n\n(0)",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nplt.rc(\"figure\", figsize=(16, 9))\nplt.rc(\"font\", size=16)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Miscellaneous->Advanced Plotting With Partial Dependence"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Weighted Least Squares"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition"
            ],
            [
                "sklearn->Examples->Covariance estimation->Ledoit-Wolf vs OAS estimation"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Deterministic Terms"
            ]
        ]
    },
    "798429": {
        "jupyter_code_cell": "median_std_dev_of_same_dof = []\nfor degree_of_freedom in df3['degree_of_freedom'].unique():\n    median = df3[df3['degree_of_freedom']==degree_of_freedom]['standard_deviation'].median()\n    median_std_dev_of_same_dof.append(median)\ndf3['degree_of_freedom'] = df3['degree_of_freedom'].astype(int)",
        "matched_tutorial_code_inds": [
            3240,
            3242,
            1767,
            5643,
            6337
        ],
        "matched_tutorial_codes": [
            "cred_intervals = []\nintervals = [0.5, 0.75, 0.95]\n\nfor interval in intervals:\n    cred_interval = list(t_post.interval(interval))\n    cred_intervals.append([interval, cred_interval[0], cred_interval[1]])\n\ncred_int_df = (\n    cred_intervals, columns=[\"interval\", \"lower value\", \"upper value\"]\n).set_index(\"interval\")\ncred_int_df\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "pairwise_bayesian = []\n\nfor model_i, model_k in (range(len(model_scores)), 2):\n    model_i_scores = model_scores.iloc[model_i].values\n    model_k_scores = model_scores.iloc[model_k].values\n    differences = model_i_scores - model_k_scores\n    t_post = (\n        df, loc=(differences), scale=corrected_std(differences, n_train, n_test)\n    )\n    worse_prob = t_post.cdf(rope_interval[0])\n    better_prob = 1 - t_post.cdf(rope_interval[1])\n    rope_prob = t_post.cdf(rope_interval[1]) - t_post.cdf(rope_interval[0])\n\n    pairwise_bayesian.append([worse_prob, better_prob, rope_prob])\n\npairwise_bayesian_df = (\n    pairwise_bayesian, columns=[\"worse_prob\", \"better_prob\", \"rope_prob\"]\n).round(3)\n\npairwise_comp_df = pairwise_comp_df.join(pairwise_bayesian_df)\npairwise_comp_df\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "x_axis = []\ndata = {'positive sentiment': [], 'negative sentiment': []}\nfor speaker in predictions:\n    # The speakers will be used to label the x-axis in our plot\n    x_axis.append(speaker)\n    # number of paras with positive sentiment\n    no_pos_paras = len(predictions[speaker]['pos_paras'])\n    # number of paras with negative sentiment\n    no_neg_paras = len(predictions[speaker]['neg_paras'])\n    # Obtain percentage of paragraphs with positive predicted sentiment\n    pos_perc = no_pos_paras / (no_pos_paras + no_neg_paras)\n    # Store positive and negative percentages\n    data['positive sentiment'].append(pos_perc*100)\n    data['negative sentiment'].append(100*(1-pos_perc))\n\nindex = pd.Index(x_axis, name='speaker')\ndf = pd.DataFrame(data, index=index)\nax = df.plot(kind='bar', stacked=True)\nax.set_ylabel('percentage')\nax.legend(title='labels', bbox_to_anchor=(1, 1), loc='upper left')\nplt.show()",
            "glm = smf.glm(\n    \"affairs ~ rate_marriage + yrs_married\",\n    data=dc,\n    family=sm.families.Poisson(),\n    freq_weights=np.asarray(dc[\"freq\"]),\n)\nres_f2 = glm.fit()\n# print(res_f2.summary())\nres_f2.pearson_chi2 - res_f.pearson_chi2, res_f2.deviance - res_f.deviance, res_f2.llf - res_f.llf",
            "intercept = [\n    ar0_res.params[0],\n    sarimax_res.params[0],\n    arima_res.params[0],\n    autoreg_res.params[0],\n]\nrho_hat = [0] + [r.params[1] for r in (sarimax_res, arima_res, autoreg_res)]\nlong_run = [\n    ar0_res.params[0],\n    sarimax_res.params[0] / (1 - sarimax_res.params[1]),\n    arima_res.params[0],\n    autoreg_res.params[0] / (1 - autoreg_res.params[1]),\n]\ncols = [\"AR(0)\", \"SARIMAX\", \"ARIMA\", \"AutoReg\"]\npd.DataFrame(\n    [intercept, rho_hat, long_run],\n    columns=cols,\n    index=[\"delta-or-phi\", \"rho\", \"long-run mean\"],\n)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Model Selection->Statistical comparison of models using grid search->Comparing two models: Bayesian approach->Region of Practical Equivalence"
            ],
            [
                "sklearn->Examples->Model Selection->Statistical comparison of models using grid search->Pairwise comparison of all models: Bayesian approach"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Sentiment Analysis on the Speech Data"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->original observations and frequency weights"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Comparing trends and exogenous variables in SARIMAX, ARIMA and AutoReg"
            ]
        ]
    },
    "1328929": {
        "jupyter_code_cell": "smallcounties = OHcounties['Registered Voters'] < 50000\nplt.hist(OHcounties[smallcounties]['TMP'],bins=20)\nplt.show()\ncandperc = pd.DataFrame({c : OH[c] / OH[u'Total Voters'] for c in candidates})\ncandperc[['County Name','Precinct Name','Total Voters']] = OH[['County Name','Precinct Name','Total Voters']]\ncandperc = candperc.set_index(['County Name','Precinct Name'])",
        "matched_tutorial_code_inds": [
            6493,
            6492,
            6494,
            5913,
            6401
        ],
        "matched_tutorial_codes": [
            "time_s = np.s_[:50]  # After this they basically agree\nfig2 = plt.figure()\nax2 = fig2.add_subplot(111)\nh21, = ax2.plot(idx[time_s], res_f.freq_seasonal[1].filtered[time_s], label='Double Freq. Seas')\nh22, = ax2.plot(idx[time_s], res_tf.freq_seasonal[0].filtered[time_s], label='Mixed Domain Seas')\nh23, = ax2.plot(idx[time_s], true_seasonal_100_2[time_s], label='True Seasonal 100(2)')\nplt.legend([h21, h22, h23], ['Double Freq. Seasonal','Mixed Domain Seasonal','Truth'], loc=2)\nplt.title('Seasonal 100(2) component')\nplt.show()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_seasonal_22_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_seasonal_22_0.png\"/>",
            "time_s = np.s_[:50]  # After this they basically agree\nfig1 = plt.figure()\nax1 = fig1.add_subplot(111)\nidx = np.asarray(series.index)\nh1, = ax1.plot(idx[time_s], res_f.freq_seasonal[0].filtered[time_s], label='Double Freq. Seas')\nh2, = ax1.plot(idx[time_s], res_tf.seasonal.filtered[time_s], label='Mixed Domain Seas')\nh3, = ax1.plot(idx[time_s], true_seasonal_10_3[time_s], label='True Seasonal 10(3)')\nplt.legend([h1, h2, h3], ['Double Freq. Seasonal','Mixed Domain Seasonal','Truth'], loc=2)\nplt.title('Seasonal 10(3) component')\nplt.show()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_seasonal_21_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_seasonal_21_0.png\"/>",
            "time_s = np.s_[:100]\n\nfig3 = plt.figure()\nax3 = fig3.add_subplot(111)\nh31, = ax3.plot(idx[time_s], res_f.freq_seasonal[1].filtered[time_s] + res_f.freq_seasonal[0].filtered[time_s], label='Double Freq. Seas')\nh32, = ax3.plot(idx[time_s], res_tf.freq_seasonal[0].filtered[time_s] + res_tf.seasonal.filtered[time_s], label='Mixed Domain Seas')\nh33, = ax3.plot(idx[time_s], true_sum[time_s], label='True Seasonal 100(2)')\nh34, = ax3.plot(idx[time_s], res_lf.freq_seasonal[0].filtered[time_s], label='Lazy Freq. Seas')\nh35, = ax3.plot(idx[time_s], res_lt.seasonal.filtered[time_s], label='Lazy Time Seas')\n\nplt.legend([h31, h32, h33, h34, h35], ['Double Freq. Seasonal','Mixed Domain Seasonal','Truth', 'Lazy Freq. Seas', 'Lazy Time Seas'], loc=1)\nplt.title('Seasonal components combined')\nplt.tight_layout(pad=1.0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_seasonal_23_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_seasonal_23_0.png\"/>",
            "drop_idx = abs(resid).argmax()\nprint(drop_idx)  # zero-based index\nidx = salary_table.index.drop(drop_idx)\n\nlm32 = ols(\"S ~ C(E) + X + C(M)\", data=salary_table, subset=idx).fit()\n\nprint(lm32.summary())\nprint(\"\\n\")\n\ninterX_lm32 = ols(\"S ~ C(E) * X + C(M)\", data=salary_table, subset=idx).fit()\n\nprint(interX_lm32.summary())\nprint(\"\\n\")\n\n\ntable3 = anova_lm(lm32, interX_lm32)\nprint(table3)\nprint(\"\\n\")\n\n\ninterM_lm32 = ols(\"S ~ X + C(E) * C(M)\", data=salary_table, subset=idx).fit()\n\ntable4 = anova_lm(lm32, interM_lm32)\nprint(table4)\nprint(\"\\n\")",
            "exog = endog['dln_consump']\nmod = sm.tsa.VARMAX(endog[['dln_inv', 'dln_inc']], order=(2,0), trend='n', exog=exog)\nres = mod.fit(maxiter=1000, disp=False)\nprint(res.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->Seasonality in Time Series Data->Comparison of filtered estimates"
            ],
            [
                "statsmodels->Examples->State space models->Seasonality in Time Series Data->Comparison of filtered estimates"
            ],
            [
                "statsmodels->Examples->State space models->Seasonality in Time Series Data->Comparison of filtered estimates"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "statsmodels->Examples->State space models->VARMAX: Introduction->Example 1: VAR"
            ]
        ]
    },
    "1203352": {
        "jupyter_code_cell": "full_pred3 = mlp3.predict(X)\npd.crosstab(Y, full_pred3) \nmlp4 = MLPClassifier(activation='logistic', hidden_layer_sizes=(1000, 1000))\nmlp4.fit(X, Y)",
        "matched_tutorial_code_inds": [
            6628,
            2702,
            6607,
            6618,
            6625
        ],
        "matched_tutorial_codes": [
            "mod = TVRegression(y_t, x_t, w_t)\nres_mle = mod.fit(disp=False)\nprint(res_mle.summary())",
            "reg_ols = ()\ny_pred_ols = reg_ols.fit(X_train, y_train).predict(X_test)\nr2_score_ols = (y_test, y_pred_ols)\nprint(\"OLS R2 score\", r2_score_ols)",
            "mod = TVRegression(y_t, x_t, w_t)\nres = mod.fit()\n\nprint(res.summary())",
            "mod = TVRegressionExtended(y_t, x_t, w_t)\nres = mod.fit(maxiter=2000)  # it doesn't converge with 50 iters\nprint(res.summary())",
            "mod = MultipleYsModel(i_hat, s_t, m_hat)\nres = mod.fit()\n\nprint(res.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->Statespace: Custom Models->Bonus: pymc3 for fast Bayesian estimation"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Non-negative least squares"
            ],
            [
                "statsmodels->Examples->State space models->Statespace: Custom Models->Model 1: time-varying coefficients->And then estimate it with our custom model class"
            ],
            [
                "statsmodels->Examples->State space models->Statespace: Custom Models->Model 2: time-varying parameters with non identity transition matrix->1) Change the starting parameters function"
            ],
            [
                "statsmodels->Examples->State space models->Statespace: Custom Models->Model 3: multiple observation and state equations->What do we need to modify?->2) The update() function"
            ]
        ]
    },
    "809529": {
        "jupyter_code_cell": "X, y = loader(seed=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                        test_size=0.2,\n                                                        random_state=42)\ny_pred = np.tile(np.mean(y_train, axis=0),(y_test.shape[0], 1))",
        "matched_tutorial_code_inds": [
            2381,
            2294,
            1852,
            2683,
            2330
        ],
        "matched_tutorial_codes": [
            "X_train, X_test, y_train, y_test = (\n    X, y, stratify=y, random_state=0, train_size=1_000, test_size=100\n)\n\nrng = (0)\nnoise = rng.normal(scale=0.25, size=X_test.shape)\nX_test_noisy = X_test + noise\n\nnoise = rng.normal(scale=0.25, size=X_train.shape)\nX_train_noisy = X_train + noise",
            "X_train, X_test, y_train, y_test = (\n    X, y, test_size=0.1, random_state=13\n)\n\nparams = {\n    \"n_estimators\": 500,\n    \"max_depth\": 4,\n    \"min_samples_split\": 5,\n    \"learning_rate\": 0.01,\n    \"loss\": \"squared_error\",\n}",
            "X, y = (random_state=0)\n\nrf = (random_state=0, ccp_alpha=0).fit(X, y)\nprint(\n    \"Average number of nodes without pruning {:.1f}\".format(\n        ([e.tree_.node_count for e in rf.estimators_])\n    )\n)\n\nrf = (random_state=0, ccp_alpha=0.05).fit(X, y)\nprint(\n    \"Average number of nodes with pruning {:.1f}\".format(\n        ([e.tree_.node_count for e in rf.estimators_])\n    )\n)",
            "X, y = (n_samples=200, n_features=5000, random_state=0)\n# create a copy of X in sparse format\nX_sp = (X)\n\nalpha = 1\nsparse_lasso = (alpha=alpha, fit_intercept=False, max_iter=1000)\ndense_lasso = (alpha=alpha, fit_intercept=False, max_iter=1000)\n\nt0 = ()\nsparse_lasso.fit(X_sp, y)\nprint(f\"Sparse Lasso done in {(() - t0):.3f}s\")\n\nt0 = ()\ndense_lasso.fit(X, y)\nprint(f\"Dense Lasso done in {(() - t0):.3f}s\")\n\n# compare the regression coefficients\ncoeff_diff = (sparse_lasso.coef_ - dense_lasso.coef_)\nprint(f\"Distance between coefficients : {coeff_diff:.2e}\")\n\n#",
            "X, y = (return_X_y=True)\n\n# Train classifiers\nreg1 = (random_state=1)\nreg2 = (random_state=1)\nreg3 = ()\n\nreg1.fit(X, y)\nreg2.fit(X, y)\nreg3.fit(X, y)\n\nereg = ([(\"gb\", reg1), (\"rf\", reg2), (\"lr\", reg3)])\nereg.fit(X, y)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Examples based on real world datasets->Image denoising using kernel PCA->Load the dataset via OpenML"
            ],
            [
                "sklearn->Examples->Ensemble methods->Gradient Boosting regression->Data preprocessing"
            ],
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 0.22->Tree pruning"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Lasso on dense and sparse data->Comparing the two Lasso implementations on Dense data"
            ],
            [
                "sklearn->Examples->Ensemble methods->Plot individual and voting regression predictions->Training classifiers"
            ]
        ]
    },
    "1328995": {
        "jupyter_code_cell": "data = [['Height', 'Weight'],\n        [63, 127], [64, 121], [66, 142], [69, 157], [69, 162], \n        [71, 156], [71, 169], [72, 165], [73, 181], [75, 208]]\ndf = pd.DataFrame(data=data[1:], columns=data[0])\ndf\ndef setax(ax):\n    ax.set_title(\"Height vs Weight\", fontsize=20)\n    ax.set_xlabel(\"Height\", fontsize=15)\n    ax.set_ylabel(\"Weight\", fontsize=15)",
        "matched_tutorial_code_inds": [
            5963,
            4654,
            6901,
            1092,
            6060
        ],
        "matched_tutorial_codes": [
            "data = [\n    [\"Carroll\", 94, 22, 60, 92, 20, 60],\n    [\"Grant\", 98, 21, 65, 92, 22, 65],\n    [\"Peck\", 98, 28, 40, 88, 26, 40],\n    [\"Donat\", 94, 19, 200, 82, 17, 200],\n    [\"Stewart\", 98, 21, 50, 88, 22, 45],\n    [\"Young\", 96, 21, 85, 92, 22, 85],\n]\ncolnames = [\"study\", \"mean_t\", \"sd_t\", \"n_t\", \"mean_c\", \"sd_c\", \"n_c\"]\nrownames = [i[0] for i in data]\ndframe1 = pd.DataFrame(data, columns=colnames)\nrownames",
            "data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>",
            "data = \"\"\"\n  x   y y_err\n201 592    61\n244 401    25\n 47 583    38\n287 402    15\n203 495    21\n 58 173    15\n210 479    27\n202 504    14\n198 510    30\n158 416    16\n165 393    14\n201 442    25\n157 317    52\n131 311    16\n166 400    34\n160 337    31\n186 423    42\n125 334    26\n218 533    16\n146 344    22\n\"\"\"\ntry:\n    from StringIO import StringIO\nexcept ImportError:\n    from io import StringIO\ndata = pd.read_csv(StringIO(data), delim_whitespace=True).astype(float)\n\n# Note: for the results we compare with the paper here, they drop the first four points\ndata.head()",
            "data_path = '~/.data/imagenet'\nsaved_model_dir = 'data/'\nfloat_model_file = 'mobilenet_pretrained_float.pth'\nscripted_float_model_file = 'mobilenet_quantization_scripted.pth'\nscripted_quantized_model_file = 'mobilenet_quantization_scripted_quantized.pth'\n\ntrain_batch_size = 30\neval_batch_size = 50\n\ndata_loader, data_loader_test = prepare_data_loaders(data_path)\ncriterion = nn.CrossEntropyLoss()\nfloat_model = load_model(saved_model_dir + float_model_file).to('cpu')\n\n# Next, we'll \"fuse modules\"; this can both make the model faster by saving on memory access\n# while also improving numerical accuracy. While this can be used with any model, this is\n# especially common with quantized models.\n\nprint('\\n Inverted Residual Block: Before fusion \\n\\n', float_model.features[1].conv)\nfloat_model.eval()\n\n# Fuses modules\nfloat_model.fuse_model()\n\n# Note fusion of Conv+BN+Relu and Conv+Relu\nprint('\\n Inverted Residual Block: After fusion\\n\\n',float_model.features[1].conv)",
            "data = pdr.get_data_fred(\"HOUSTNSA\", \"1959-01-01\", \"2019-06-01\")\nhousing = data.HOUSTNSA.pct_change().dropna()\n# Scale by 100 to get percentages\nhousing = 100 * housing.asfreq(\"MS\")\nfig, ax = plt.subplots()\nax = housing.plot(ax=ax)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_6_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_6_0.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Example"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings"
            ],
            [
                "statsmodels->Examples->User Notes->Least squares fitting of models to data->Linear models"
            ],
            [
                "torch->Model Optimization->(beta) Static Quantization with Eager Mode in PyTorch->3. Define dataset and data loaders->ImageNet Data"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions"
            ]
        ]
    },
    "1166229": {
        "jupyter_code_cell": "plt.plot(ress1)\nress1 = ress1.fillna(0)\ncd1 = np.arange(35000,59991)\ncd = np.arange(0,13500)\nc = np.concatenate((cd,cd1))",
        "matched_tutorial_code_inds": [
            3771,
            3794,
            5345,
            5334,
            5715
        ],
        "matched_tutorial_codes": [
            "win_percent.sort_values().plot.barh(figsize=(6, 12), width=.85, color='k')\nplt.tight_layout()\nsns.despine()\nplt.xlabel(\"Win Percent\")",
            "sns.jointplot(x='carat', y='price', data=df, size=8, alpha=.25,\n              color='k', marker='.')\nplt.tight_layout()",
            "fig = sm.graphics.plot_partregress_grid(crime_model)\nfig.tight_layout(pad=1.0)",
            "fig = sm.graphics.plot_partregress_grid(prestige_model)\nfig.tight_layout(pad=1.0)",
            "plt.clf()\nplt.grid(True)\nplt.plot(result1.predict(linear=True), result1.resid_pearson, \"o\")\nplt.xlabel(\"Linear predictor\")\nplt.ylabel(\"Residual\")"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "statsmodels->Examples->Plotting->Regression Plots->Statewide Crime 2009 Dataset->Partial Regression Plots (Crime Data)"
            ],
            [
                "statsmodels->Examples->Plotting->Regression Plots->Duncan\u2019s Prestige Dataset->Partial Regression Plots (Duncan)"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Quasi-binomial regression"
            ]
        ]
    },
    "256272": {
        "jupyter_code_cell": "data = pd.read_csv('train.csv',sep = ',',header = 0)\ndatetime_data = data[[\"datetime\"]].values\nhour_data = []\nfor dates in datetime_data:\n    work = time.strptime(dates[0], \"%Y-%m-%d %H:%M:%S\")\n    hour_data.append(work.tm_hour)\ndata[[\"datetime\"]] = hour_data\ntrain_data = data[[\"datetime\",\"season\",\"holiday\",\"workingday\",\"weather\",\"temp\",\"atemp\",\"humidity\",\"windspeed\"]]\ntrain_labels = data[[\"count\"]]\ntrain_labels = np.ravel(train_labels)\ntest_data = pd.read_csv('test.csv',sep = ',',header = 0)\nhour_data = []\nsample_data = test_data[\"datetime\"]\ndatetime_data = test_data[[\"datetime\"]].values\nfor dates in datetime_data:\n    work = time.strptime(dates[0], \"%Y-%m-%d %H:%M:%S\")\n    hour_data.append(work.tm_hour)\ntest_data[[\"datetime\"]] = hour_data\none_hot_encoder = OneHotEncoder(categorical_features=[1,2,3,4])\ntrain_data_preprocessed = one_hot_encoder.fit_transform(train_data).toarray()\ntest_data_preprocessed = one_hot_encoder.fit_transform(test_data).toarray()\npast = 0\npredictions = []\nall_predictions = []\nfor i in range(1,25):\n    n_train = 19*24*i \n    training_data = train_data_preprocessed[:n_train]\n    training_labels = train_labels[:n_train]\n    if i in [4,6,9,11,16,18,21,23]:\n       n_test = 11*24\n    elif i in [2,14]:\n        n_test = 9*24\n    else:\n        n_test = 12*24\n    n_test += past\n    testing_data = test_data_preprocessed[past:n_test]\n    past = n_test\n    clf = RandomForestRegressor(n_estimators = 50,max_depth = 150,min_samples_split = 3,min_samples_leaf = 1)\n    clf.fit(training_data,training_labels)\n    predictions = clf.predict(testing_data)\n    for item in predictions:\n        all_predictions.append(int(item))\ndf_sub = pd.DataFrame({ 'datetime' : sample_data,'count' : all_predictions})\ndf_sub.head()\ndf_sub.to_csv('submission_new.csv',index=False,cols=[\"datetime\",\"count\"])\nfrom IPython.display import HTML\ns = \"\"\"<table>\n<tr>\n<th>Model</th>\n<th>RLMSE</th>\n<th># exact predictions</th>\n<th>most extreme difference</th>\n</tr>\n<tr>\n<td>GradientBoostingRegressor</td>\n<td>0.705</td>\n<td>31</td>\n<td>440</td>\n</tr>\n<tr>\n<td>KNeighboursRegressor</td>\n<td>1.027</td>\n<td>13</td>\n<td>598</td>\n</tr>\n<tr>\n<td>RandomForestRegressor</td>\n<td>0.319</td>\n<td>67</td>\n<td>456</td>\n</tr>\n<tr>\n<td>AdaBoostRegressor</td>\n<td>0.799</td>\n<td>11</td>\n<td>589</td>\n</tr>\n<tr>\n<td>DecisionTreeRegressor</td>\n<td>0.394</td>\n<td>57</td>\n<td>603</td>\n</tr>\n<tr>\n<td>BaggingRegressor</td>\n<td>0.315</td>\n<td>77</td>\n<td>442</td>\n</tr>\n</table>\"\"\"\nh = HTML(s); h",
        "matched_tutorial_code_inds": [
            4654,
            5963,
            1092,
            6060,
            5361
        ],
        "matched_tutorial_codes": [
            "data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>",
            "data = [\n    [\"Carroll\", 94, 22, 60, 92, 20, 60],\n    [\"Grant\", 98, 21, 65, 92, 22, 65],\n    [\"Peck\", 98, 28, 40, 88, 26, 40],\n    [\"Donat\", 94, 19, 200, 82, 17, 200],\n    [\"Stewart\", 98, 21, 50, 88, 22, 45],\n    [\"Young\", 96, 21, 85, 92, 22, 85],\n]\ncolnames = [\"study\", \"mean_t\", \"sd_t\", \"n_t\", \"mean_c\", \"sd_c\", \"n_c\"]\nrownames = [i[0] for i in data]\ndframe1 = pd.DataFrame(data, columns=colnames)\nrownames",
            "data_path = '~/.data/imagenet'\nsaved_model_dir = 'data/'\nfloat_model_file = 'mobilenet_pretrained_float.pth'\nscripted_float_model_file = 'mobilenet_quantization_scripted.pth'\nscripted_quantized_model_file = 'mobilenet_quantization_scripted_quantized.pth'\n\ntrain_batch_size = 30\neval_batch_size = 50\n\ndata_loader, data_loader_test = prepare_data_loaders(data_path)\ncriterion = nn.CrossEntropyLoss()\nfloat_model = load_model(saved_model_dir + float_model_file).to('cpu')\n\n# Next, we'll \"fuse modules\"; this can both make the model faster by saving on memory access\n# while also improving numerical accuracy. While this can be used with any model, this is\n# especially common with quantized models.\n\nprint('\\n Inverted Residual Block: Before fusion \\n\\n', float_model.features[1].conv)\nfloat_model.eval()\n\n# Fuses modules\nfloat_model.fuse_model()\n\n# Note fusion of Conv+BN+Relu and Conv+Relu\nprint('\\n Inverted Residual Block: After fusion\\n\\n',float_model.features[1].conv)",
            "data = pdr.get_data_fred(\"HOUSTNSA\", \"1959-01-01\", \"2019-06-01\")\nhousing = data.HOUSTNSA.pct_change().dropna()\n# Scale by 100 to get percentages\nhousing = 100 * housing.asfreq(\"MS\")\nfig, ax = plt.subplots()\nax = housing.plot(ax=ax)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_6_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_6_0.png\"/>",
            "data = sm.datasets.anes96.load_pandas()\nparty_ID = np.arange(7)\nlabels = [\n    \"Strong Democrat\",\n    \"Weak Democrat\",\n    \"Independent-Democrat\",\n    \"Independent-Independent\",\n    \"Independent-Republican\",\n    \"Weak Republican\",\n    \"Strong Republican\",\n]"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Example"
            ],
            [
                "torch->Model Optimization->(beta) Static Quantization with Eager Mode in PyTorch->3. Define dataset and data loaders->ImageNet Data"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions"
            ],
            [
                "statsmodels->Examples->Plotting->Box Plots->Bean Plots"
            ]
        ]
    },
    "364571": {
        "jupyter_code_cell": "plt.scatter(trace_5_8['br'], trace_5_8['bl']);\nsum_blbr = trace_5_8['br'] + trace_5_8['bl']\npm.kdeplot(sum_blbr);",
        "matched_tutorial_code_inds": [
            3799,
            4113,
            4101,
            4438,
            4120
        ],
        "matched_tutorial_codes": [
            "g = sns.FacetGrid(df, col='color', hue='color', col_wrap=4)\ng.map(sns.regplot, 'carat', 'price');",
            "sns.lmplot(x=\"total_bill\", y=\"big_tip\", data=tips,\n           logistic=True, y_jitter=.03);\n",
            "tips = sns.load_dataset(\"tips\")\nsns.regplot(x=\"total_bill\", y=\"tip\", data=tips);\n",
            "plt.plot(points[:, 0], points[:, 1], 'o')\n plt.plot(vor.vertices[:, 0], vor.vertices[:, 1], '*')\n plt.xlim(-1, 3); plt.ylim(-1, 3)",
            "sns.lmplot(x=\"total_bill\", y=\"tip\", hue=\"smoker\",\n           col=\"time\", row=\"sex\", data=tips, height=3);\n"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "seaborn->User guide and tutorial->Estimating regression fits->Fitting different kinds of models",
                "seaborn->Statistical operations->Estimating regression fits->Fitting different kinds of models"
            ],
            [
                "seaborn->User guide and tutorial->Estimating regression fits->Functions for drawing linear regression models",
                "seaborn->Statistical operations->Estimating regression fits->Functions for drawing linear regression models"
            ],
            [
                "scipy->Spatial data structures and algorithms (scipy.spatial)->Voronoi diagrams"
            ],
            [
                "seaborn->User guide and tutorial->Estimating regression fits->Conditioning on other variables",
                "seaborn->Statistical operations->Estimating regression fits->Conditioning on other variables"
            ]
        ]
    },
    "1239657": {
        "jupyter_code_cell": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nimport math\ndef calculate_pi_loop(N): \n    hits = np.zeros(N)\n    for i in range(N): \n        point = np.random.rand(2)\n        dist = point[0]**2 + point[1]**2\n        hits[i] = dist < 1\n    return 4*hits.sum()/N",
        "matched_tutorial_code_inds": [
            4700,
            1393,
            5149,
            4895,
            4946
        ],
        "matched_tutorial_codes": [
            "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom cycler import cycler\nmpl.rcParams['lines.linewidth'] = 2\nmpl.rcParams['lines.linestyle'] = '--'\ndata = np.random.randn(50)\nplt.plot(data)\n\n\n<img alt=\"customizing\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_customizing_001.png\" srcset=\"../../_images/sphx_glr_customizing_001.png, ../../_images/sphx_glr_customizing_001_2_0x.png 2.0x\"/>",
            "import matplotlib.pyplot as plt\nimport numpy as np\nidx = 5\nprint(\"Question: \", dataset[\"train\"][idx][\"question\"])\nprint(\"Answers: \" ,dataset[\"train\"][idx][\"answers\"])\nim = np.asarray(dataset[\"train\"][idx][\"image\"].resize((500,500)))\nplt.imshow(im)\nplt.show()\n\n\n<img alt=\"flava finetuning tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_flava_finetuning_tutorial_001.png\" srcset=\"../_images/sphx_glr_flava_finetuning_tutorial_001.png\"/>",
            "import pyarrow as pa\nimport pyarrow.parquet as pq\nimport statsmodels.formula.api as smf\n\nclass DataSet(dict):\n    def __init__(self, path):\n        self.parquet = pq.ParquetFile(path)\n\n    def __getitem__(self, key):\n        try:\n            return self.parquet.read([key]).to_pandas()[key]\n        except:\n            raise KeyError\n\nLargeData = DataSet('LargeData.parquet')\n\nres = smf.ols('Profit ~ Sugar + Power + Women', data=LargeData).fit()",
            "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nth = np.linspace(0, 2*np.pi, 128)\n\n\ndef demo(sty):\n    mpl.style.use(sty)\n    fig, ax = plt.subplots(figsize=(3, 3))\n\n    ax.set_title('style: {!r}'.format(sty), color='C0')\n\n    ax.plot(th, np.cos(th), 'C1', label='C1')\n    ax.plot(th, np.sin(th), 'C2', label='C2')\n    ax.legend()\n\n\ndemo('default')\ndemo('seaborn-v0_8')\n\n\n\n<img alt=\"style: 'default'\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_colors_002.png\" srcset=\"../../_images/sphx_glr_colors_002.png, ../../_images/sphx_glr_colors_002_2_0x.png 2.0x\"/>\n<img alt=\"style: 'seaborn-v0_8'\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_colors_003.png\" srcset=\"../../_images/sphx_glr_colors_003.png, ../../_images/sphx_glr_colors_003_2_0x.png 2.0x\"/>",
            "import matplotlib.pyplot as plt\nimport numpy as np\n\nx1 = np.linspace(0.0, 5.0, 100)\ny1 = np.cos(2 * np.pi * x1) * np.exp(-x1)\n\nfig, ax = plt.subplots(figsize=(5, 3))\nfig.subplots_adjust(bottom=0.15, left=0.2)\nax.plot(x1, y1)\nax.set_xlabel('Time [s]')\nax.set_ylabel('Damped oscillation [V]')\n\nplt.show()\n\n\n<img alt=\"text intro\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_text_intro_002.png\" srcset=\"../../_images/sphx_glr_text_intro_002.png, ../../_images/sphx_glr_text_intro_002_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Introductory->Customizing Matplotlib with style sheets and rcParams->Runtime rc settings"
            ],
            [
                "torch->Multimodality->TorchMultimodal Tutorial: Finetuning FLAVA->Steps"
            ],
            [
                "statsmodels->User Guide->Statistics and Tools->Working with Large Data Sets->Subsetting your data"
            ],
            [
                "matplotlib->Tutorials->Colors->Specifying colors->\"CN\" color selection"
            ],
            [
                "matplotlib->Tutorials->Text->Text in Matplotlib Plots->Labels for x- and y-axis"
            ]
        ]
    },
    "967874": {
        "jupyter_code_cell": "pca_features = [(item[1] + ' ' + item[0]).title() \n                for item in itertools.product(list(masked_region_features_pbr.columns.levels[1]),                         \n                                              list(masked_region_features_pbr.columns.levels[2]))]\ntransformed_features = sorted(zip(pca_features, *pca.components_[0:4]), key=lambda t: t[0])\nmasked_region_df_pbr['subject_id'] = pd.Categorical(masked_region_df_pbr['subject_id'], pca_results.subject_id)",
        "matched_tutorial_code_inds": [
            2845,
            1620,
            4501,
            2110,
            2298
        ],
        "matched_tutorial_codes": [
            "coefs = (\n    model[-1].regressor_.coef_,\n    columns=[\"Coefficients importance\"],\n    index=feature_names,\n)\ncoefs.plot.barh(figsize=(9, 7))\n(\"Ridge model, small regularization, normalized variables\")\n(\"Raw coefficient values\")\n(x=0, color=\".5\")\n(left=0.3)\n\n\n<img alt=\"Ridge model, small regularization, normalized variables\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_010.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_010.png\"/>",
            "pixel_intensity_distribution = ndimage.histogram(\n    xray_image, min=np.min(xray_image), max=np.max(xray_image), bins=256\n)\n\nplt.plot(pixel_intensity_distribution)\nplt.title(\"Pixel intensity distribution\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/00c12e163a30744c62700ec414e2d514c5c4eae10256e7cd0cf87553d0f4d34c.png\" src=\"../_images/00c12e163a30744c62700ec414e2d514c5c4eae10256e7cd0cf87553d0f4d34c.png\"/>",
            "quantiles = [0.0, 0.01, 0.05, 0.1, 1-0.10, 1-0.05, 1-0.01, 1.0]\n crit = stats.t.ppf(quantiles, 10)\n crit\narray([       -inf, -2.76376946, -1.81246112, -1.37218364,  1.37218364,\n        1.81246112,  2.76376946,         inf])\n n_sample = x.size\n freqcount = np.histogram(x, bins=crit)[0]\n tprob = np.diff(quantiles)\n nprob = np.diff(stats.norm.cdf(crit))\n tch, tpval = stats.chisquare(freqcount, tprob*n_sample)\n nch, npval = stats.chisquare(freqcount, nprob*n_sample)\n print('chisquare for t:      chi2 = %6.2f pvalue = %6.4f' % (tch, tpval))\nchisquare for t:      chi2 =  2.30 pvalue = 0.8901  # random\n print('chisquare for normal: chi2 = %6.2f pvalue = %6.4f' % (nch, npval))\nchisquare for normal: chi2 = 64.60 pvalue = 0.0000  # random",
            "pca_estimator = (\n    n_components=n_components, svd_solver=\"randomized\", whiten=True\n)\npca_estimator.fit(faces_centered)\nplot_gallery(\n    \"Eigenfaces - PCA using randomized SVD\", pca_estimator.components_[:n_components]\n)\n\n\n<img alt=\"Eigenfaces - PCA using randomized SVD\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_002.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_002.png\"/>",
            "feature_importance = reg.feature_importances_\nsorted_idx = (feature_importance)\npos = (sorted_idx.shape[0]) + 0.5\nfig = (figsize=(12, 6))\n(1, 2, 1)\n(pos, feature_importance[sorted_idx], align=\"center\")\n(pos, (diabetes.feature_names)[sorted_idx])\n(\"Feature Importance (MDI)\")\n\nresult = (\n    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_idx = result.importances_mean.argsort()\n(1, 2, 2)\n(\n    result.importances[sorted_idx].T,\n    vert=False,\n    labels=(diabetes.feature_names)[sorted_idx],\n)\n(\"Permutation Importance (test set)\")\nfig.tight_layout()\n()\n\n\n<img alt=\"Feature Importance (MDI), Permutation Importance (test set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "numpy->NumPy Applications->X-ray image processing->Apply masks to X-rays with np.where()"
            ],
            [
                "scipy->Statistics (scipy.stats)->Analysing one sample->Tails of the distribution"
            ],
            [
                "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition->Eigenfaces - PCA using randomized SVD"
            ],
            [
                "sklearn->Examples->Ensemble methods->Gradient Boosting regression->Plot feature importance"
            ]
        ]
    },
    "1209585": {
        "jupyter_code_cell": "print('So, of the total',\n      data.number_outpatient.count(),\n      'patients in this data set, there were',\n      data.number_outpatient[data.number_outpatient == 0].count(),\n      'who had no outpatient visits, or',\n      data.number_outpatient[data.number_outpatient == 0].count() / data.number_outpatient.count()* 100,\n      '%.'\n)\nprint()\nprint('Which means only',\n      data.number_outpatient[data.number_outpatient != 0].count(),\n      'had outpatient visits, or ',\n      data.number_outpatient[data.number_outpatient != 0].count() / data.number_outpatient.count()* 100,\n      '%.'\n)\ndata.number_emergency.value_counts().head(20)",
        "matched_tutorial_code_inds": [
            6668,
            2791,
            1039,
            1550,
            3653
        ],
        "matched_tutorial_codes": [
            "print('Original model')\nprint('var.level     = %.5f' % res.params[0])\nprint('var.irregular = %.5f' % res.params[1])\n\nprint('\\nConcentrated model')\nprint('scale         = %.5f' % res_conc.scale)\nprint('h * scale     = %.5f' % (res_conc.params[0] * res_conc.scale))",
            "print(\n    \"Mean AvgClaim Amount per policy:              %.2f \"\n    % df_train[\"AvgClaimAmount\"].mean()\n)\nprint(\n    \"Mean AvgClaim Amount | NbClaim  0:           %.2f\"\n    % df_train[\"AvgClaimAmount\"][df_train[\"AvgClaimAmount\"]  0].mean()\n)\nprint(\n    \"Predicted Mean AvgClaim Amount | NbClaim  0: %.2f\"\n    % glm_sev.predict(X_train).mean()\n)\nprint(\n    \"Predicted Mean AvgClaim Amount (dummy) | NbClaim  0: %.2f\"\n    % dummy_sev.predict(X_train).mean()\n)",
            "print(\n    \"Sparsity in conv1.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Sparsity in conv2.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Sparsity in fc1.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Sparsity in fc2.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Sparsity in fc3.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Global sparsity: {:.2f}%\".format(\n        100. * float(\n            ( == 0)\n            + ( == 0)\n            + ( == 0)\n            + ( == 0)\n            + ( == 0)\n        )\n        / float(\n            .nelement()\n            + .nelement()\n            + .nelement()\n            + .nelement()\n            + .nelement()\n        )\n    )\n)",
            "print(\n    \"The shape of training images: {} and training labels: {}\".format(\n        x_train.shape, y_train.shape\n    )\n)\nprint(\n    \"The shape of test images: {} and test labels: {}\".format(\n        x_test.shape, y_test.shape\n    )\n)",
            "print(\"Weather, no flights:\\n\\t\", weather_locs.difference(origin_locs | dest_locs), end='\\n\\n')\n\nprint(\"Flights, no weather:\\n\\t\", (origin_locs | dest_locs).difference(weather_locs), end='\\n\\n')\n\nprint(\"Dropped Stations:\\n\\t\", (origin_locs | dest_locs) ^ weather_locs)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Comparing estimates"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Tweedie regression on insurance claims->Severity Model -  Gamma distribution"
            ],
            [
                "torch->Model Optimization->Pruning Tutorial->Global pruning"
            ],
            [
                "numpy->NumPy Applications->Deep learning on MNIST->1. Load the MNIST dataset"
            ],
            [
                "pandas_toms_blog->Indexes->Set Operations"
            ]
        ]
    },
    "319711": {
        "jupyter_code_cell": "sales.pivot(index='Date', columns = 'Salesman', values='Revenue')\nf = pd.read_csv('foods.csv')\nf.head()",
        "matched_tutorial_code_inds": [
            4080,
            5703,
            4079,
            3829,
            4019
        ],
        "matched_tutorial_codes": [
            "sns.catplot(\n    data=tips, x=\"day\", y=\"total_bill\", hue=\"sex\",\n    kind=\"violin\", split=True,\n)\n",
            "summ_df = infl.summary_frame()\nsumm_df.sort_values(\"cooks_d\", ascending=False)[:10]",
            "sns.catplot(\n    data=tips, x=\"total_bill\", y=\"day\", hue=\"sex\",\n    kind=\"violin\", bw=.15, cut=0,\n)\n",
            "ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "sns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\", hue=\"smoker\", col=\"time\",\n)\n"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Visualizing categorical data->Comparing distributions->Violinplots",
                "seaborn->Plotting functions->Visualizing categorical data->Comparing distributions->Violinplots"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Influence Measures for GLMs->get the influence measures"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing categorical data->Comparing distributions->Violinplots",
                "seaborn->Plotting functions->Visualizing categorical data->Comparing distributions->Violinplots"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Showing multiple relationships with facets",
                "seaborn->Plotting functions->Visualizing statistical relationships->Showing multiple relationships with facets"
            ]
        ]
    },
    "59750": {
        "jupyter_code_cell": "fig, ax = plt.subplots(nrows = 1, ncols = 2, sharey = True)\nfig.set_size_inches(20, 6)\ndf_mw[df_mw['nationality']=='Schweizer'].boxplot(column=['youth unemp quote', \n                                                         '25-49y unemp quote', \n                                                         '50+ unemp quote'],\n                                                 ax=ax[0])\nax[0].set_title('Unemployment rates for Swiss people')\ndf_mw[df_mw['nationality'] == 'Ausl\u00e4nder'].boxplot(column=['youth unemp quote', \n                                                           '25-49y unemp quote', \n                                                           '50+ unemp quote'],\n                                                   ax=ax[1])\nax[1].set_title('Unemployment rates for Foreigners')\nax[0].set_ylabel('Unemployment rate')\nax[0].set_xticklabels(['15-24y', '25-49y', '50+y'])\nax[1].set_xticklabels(['15-24y', '25-49y', '50+y'])\nax[1].yaxis.set_ticks_position('left')\nplt.show()\ndf_mw['tot pop'] = np.round(df_mw['tot unemp'].values / (df_mw['tot unemp quote'].values/100)).astype(int)\ndf_mw.sort_values(by='canton',ascending=True).head()",
        "matched_tutorial_code_inds": [
            6453,
            4645,
            4714,
            6708,
            4851
        ],
        "matched_tutorial_codes": [
            "fig, ax = plt.subplots(figsize=(10,4))\n\n# Plot the results\ndf['lff'].plot(ax=ax, style='k.', label='Observations')\npredict.predicted_mean.plot(ax=ax, label='One-step-ahead Prediction')\npredict_ci = predict.conf_int(alpha=0.05)\npredict_index = np.arange(len(predict_ci))\nax.fill_between(predict_index[2:], predict_ci.iloc[2:, 0], predict_ci.iloc[2:, 1], alpha=0.1)\n\nforecast.predicted_mean.plot(ax=ax, style='r', label='Forecast')\nforecast_ci = forecast.conf_int()\nforecast_index = np.arange(len(predict_ci), len(predict_ci) + len(forecast_ci))\nax.fill_between(forecast_index, forecast_ci.iloc[:, 0], forecast_ci.iloc[:, 1], alpha=0.1)\n\n# Cleanup the image\nax.set_ylim((4, 8));\nlegend = ax.legend(loc='lower left');\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_local_linear_trend_11_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_local_linear_trend_11_0.png\"/>",
            "fig, ax = plt.subplots(figsize=(5, 2.7), layout='constrained')\ndates = np.arange(np.datetime64('2021-11-15'), np.datetime64('2021-12-25'),\n                  np.timedelta64(1, 'h'))\ndata = np.cumsum(np.random.randn(len(dates)))\nax.plot(dates, data)\ncdf = mpl.dates.ConciseDateFormatter(ax.xaxis.get_major_locator())\nax.xaxis.set_major_formatter(cdf)\n\n\n<img alt=\"quick start\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_quick_start_014.png\" srcset=\"../../_images/sphx_glr_quick_start_014.png, ../../_images/sphx_glr_quick_start_014_2_0x.png 2.0x\"/>",
            "fig, ax = plt.subplots()\nt = np.linspace(0, 3, 40)\ng = -9.81\nv0 = 12\nz = g * t**2 / 2 + v0 * t\n\nv02 = 5\nz2 = g * t**2 / 2 + v02 * t\n\nscat = ax.scatter(t[0], z[0], c=\"b\", s=5, label=f'v0 = {v0} m/s')\nline2 = ax.plot(t[0], z2[0], label=f'v0 = {v02} m/s')[0]\nax.set(xlim=[0, 3], ylim=[-4, 10], xlabel='Time [s]', ylabel='Z [m]')\nax.legend()\n\n\ndef update(frame):\n    # for each frame, update the data stored on each artist.\n    x = t[:frame]\n    y = z[:frame]\n    # update the scatter plot:\n    data = np.stack([x, y]).T\n    scat.set_offsets(data)\n    # update the line plot:\n    line2.set_xdata(t[:frame])\n    line2.set_ydata(z2[:frame])\n    return (scat, line2)\n\n\nani = animation.FuncAnimation(fig=fig, func=update, frames=40, interval=30)\nplt.show()\n\n\n\n<link href=\"https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css\" rel=\"stylesheet\"/>\n<script language=\"javascript\">\n  function isInternetExplorer() {\n    ua = navigator.userAgent;\n    /* MSIE used to detect old browsers and Trident used to newer ones*/\n    return ua.indexOf(\"MSIE \") > -1 || ua.indexOf(\"Trident/\") > -1;\n  }\n\n  /* Define the Animation class */\n  function Animation(frames, img_id, slider_id, interval, loop_select_id){\n    this.img_id = img_id;\n    this.slider_id = slider_id;\n    this.loop_select_id = loop_select_id;\n    this.interval = interval;\n    this.current_frame = 0;\n    this.direction = 0;\n    this.timer = null;\n    this.frames = new Array(frames.length);\n\n    for (var i=0; i<frames.length; i++)\n    {\n     this.frames[i] = new Image();\n     this.frames[i].src = frames[i];\n    }\n    var slider = document.getElementById(this.slider_id);\n    slider.max = this.frames.length - 1;\n    if (isInternetExplorer()) {\n        // switch from oninput to onchange because IE <= 11 does not conform\n        // with W3C specification. It ignores oninput and onchange behaves\n        // like oninput. In contrast, Microsoft Edge behaves correctly.\n        slider.setAttribute('onchange', slider.getAttribute('oninput'));\n        slider.setAttribute('oninput', null);\n    }\n    this.set_frame(this.current_frame);\n  }\n\n  Animation.prototype.get_loop_state = function(){\n    var button_group = document[this.loop_select_id].state;\n    for (var i = 0; i < button_group.length; i++) {\n        var button = button_group[i];\n        if (button.checked) {\n            return button.value;\n        }\n    }\n    return undefined;\n  }\n\n  Animation.prototype.set_frame = function(frame){\n    this.current_frame = frame;\n    document.getElementById(this.img_id).src =\n            this.frames[this.current_frame].src;\n    document.getElementById(this.slider_id).value = this.current_frame;\n  }\n\n  Animation.prototype.next_frame = function()\n  {\n    this.set_frame(Math.min(this.frames.length - 1, this.current_frame + 1));\n  }\n\n  Animation.prototype.previous_frame = function()\n  {\n    this.set_frame(Math.max(0, this.current_frame - 1));\n  }\n\n  Animation.prototype.first_frame = function()\n  {\n    this.set_frame(0);\n  }\n\n  Animation.prototype.last_frame = function()\n  {\n    this.set_frame(this.frames.length - 1);\n  }\n\n  Animation.prototype.slower = function()\n  {\n    this.interval /= 0.7;\n    if(this.direction > 0){this.play_animation();}\n    else if(this.direction < 0){this.reverse_animation();}\n  }\n\n  Animation.prototype.faster = function()\n  {\n    this.interval *= 0.7;\n    if(this.direction > 0){this.play_animation();}\n    else if(this.direction < 0){this.reverse_animation();}\n  }\n\n  Animation.prototype.anim_step_forward = function()\n  {\n    this.current_frame += 1;\n    if(this.current_frame < this.frames.length){\n      this.set_frame(this.current_frame);\n    }else{\n      var loop_state = this.get_loop_state();\n      if(loop_state == \"loop\"){\n        this.first_frame();\n      }else if(loop_state == \"reflect\"){\n        this.last_frame();\n        this.reverse_animation();\n      }else{\n        this.pause_animation();\n        this.last_frame();\n      }\n    }\n  }\n\n  Animation.prototype.anim_step_reverse = function()\n  {\n    this.current_frame -= 1;\n    if(this.current_frame >= 0){\n      this.set_frame(this.current_frame);\n    }else{\n      var loop_state = this.get_loop_state();\n      if(loop_state == \"loop\"){\n        this.last_frame();\n      }else if(loop_state == \"reflect\"){\n        this.first_frame();\n        this.play_animation();\n      }else{\n        this.pause_animation();\n        this.first_frame();\n      }\n    }\n  }\n\n  Animation.prototype.pause_animation = function()\n  {\n    this.direction = 0;\n    if (this.timer){\n      clearInterval(this.timer);\n      this.timer = null;\n    }\n  }\n\n  Animation.prototype.play_animation = function()\n  {\n    this.pause_animation();\n    this.direction = 1;\n    var t = this;\n    if (!this.timer) this.timer = setInterval(function() {\n        t.anim_step_forward();\n    }, this.interval);\n  }\n\n  Animation.prototype.reverse_animation = function()\n  {\n    this.pause_animation();\n    this.direction = -1;\n    var t = this;\n    if (!this.timer) this.timer = setInterval(function() {\n        t.anim_step_reverse();\n    }, this.interval);\n  }\n</script>\n\n\n<img id=\"_anim_img123703f31d024d2c91309d4c3beae81e\"/>\n\n<input class=\"anim-slider\" id=\"_anim_slider123703f31d024d2c91309d4c3beae81e\" max=\"1\" min=\"0\" name=\"points\" oninput=\"anim123703f31d024d2c91309d4c3beae81e.set_frame(parseInt(this.value));\" step=\"1\" type=\"range\" value=\"0\"/>\n\n<button aria-label=\"Decrease speed\" onclick=\"anim123703f31d024d2c91309d4c3beae81e.slower()\" title=\"Decrease speed\">\n<i class=\"fa fa-minus\"></i></button>\n<button aria-label=\"First frame\" onclick=\"anim123703f31d024d2c91309d4c3beae81e.first_frame()\" title=\"First frame\">\n<i class=\"fa fa-fast-backward\"></i></button>\n<button aria-label=\"Previous frame\" onclick=\"anim123703f31d024d2c91309d4c3beae81e.previous_frame()\" title=\"Previous frame\">\n<i class=\"fa fa-step-backward\"></i></button>\n<button aria-label=\"Play backwards\" onclick=\"anim123703f31d024d2c91309d4c3beae81e.reverse_animation()\" title=\"Play backwards\">\n<i class=\"fa fa-play fa-flip-horizontal\"></i></button>\n<button aria-label=\"Pause\" onclick=\"anim123703f31d024d2c91309d4c3beae81e.pause_animation()\" title=\"Pause\">\n<i class=\"fa fa-pause\"></i></button>\n<button aria-label=\"Play\" onclick=\"anim123703f31d024d2c91309d4c3beae81e.play_animation()\" title=\"Play\">\n<i class=\"fa fa-play\"></i></button>\n<button aria-label=\"Next frame\" onclick=\"anim123703f31d024d2c91309d4c3beae81e.next_frame()\" title=\"Next frame\">\n<i class=\"fa fa-step-forward\"></i></button>\n<button aria-label=\"Last frame\" onclick=\"anim123703f31d024d2c91309d4c3beae81e.last_frame()\" title=\"Last frame\">\n<i class=\"fa fa-fast-forward\"></i></button>\n<button aria-label=\"Increase speed\" onclick=\"anim123703f31d024d2c91309d4c3beae81e.faster()\" title=\"Increase speed\">\n<i class=\"fa fa-plus\"></i></button>\n\n<form action=\"#n\" aria-label=\"Repetition mode\" class=\"anim-state\" name=\"_anim_loop_select123703f31d024d2c91309d4c3beae81e\" title=\"Repetition mode\">\n<input id=\"_anim_radio1_123703f31d024d2c91309d4c3beae81e\" name=\"state\" type=\"radio\" value=\"once\"/>\n<label for=\"_anim_radio1_123703f31d024d2c91309d4c3beae81e\">Once</label>\n<input checked=\"\" id=\"_anim_radio2_123703f31d024d2c91309d4c3beae81e\" name=\"state\" type=\"radio\" value=\"loop\"/>\n<label for=\"_anim_radio2_123703f31d024d2c91309d4c3beae81e\">Loop</label>\n<input id=\"_anim_radio3_123703f31d024d2c91309d4c3beae81e\" name=\"state\" type=\"radio\" value=\"reflect\"/>\n<label for=\"_anim_radio3_123703f31d024d2c91309d4c3beae81e\">Reflect</label>\n</form>\n\n\n<script language=\"javascript\">\n  /* Instantiate the Animation class. */\n  /* The IDs given should match those used in the template above. */\n  (function() {\n    var img_id = \"_anim_img123703f31d024d2c91309d4c3beae81e\";\n    var slider_id = \"_anim_slider123703f31d024d2c91309d4c3beae81e\";\n    var loop_select_id = \"_anim_loop_select123703f31d024d2c91309d4c3beae81e\";\n    var frames = new Array(40);\n\n  frames[0] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAn4AAAHcCAYAAABI7KcYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\\\n\"\n  frames[1] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAn4AAAHcCAYAAABI7KcYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\\\nAwAAMATBDwAAwBAEPwAAAEP8L2FL+pMNqoY+AAAAAElFTkSuQmCC\\\n\"\n  frames[2] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAn4AAAHcCAYAAABI7KcYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\\\nbGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9h\\\nAAAPYQGoP6dpAAAva0lEQVR4nO3deXQUZd728auTkAVDOiwhy0OAsAybsooYYJRAIAIi6CCi0TeA\\\n4OAJMpFnROAoizpER1RQFFSOwKOsg2wSFtkdFFSWjIgKghHyqCQo2g1EE0jq/YOXfo0kIYF0Ksn9\\\n/ZxTB7rqrqpfl2X3xX1XVTssy7IEAACAas/H7gIAAABQMQh+AAAAhiD4AQAAGILgBwAAYAiCHwAA\\\ngCEIfgAAAIYg+AEAABiC4AcAAGAIgh8AAIAhCH4AAACGIPgBAAAYguAHAABgCIIfAACAIQh+AAAA\\\nhiD4AQAAGILgBwAAYAiCHwAAgCEIfgAAAIYg+AEAABiC4AcAAGAIgh8AAIAhCH4AAACGIPgBAAAY\\\nguAHAABgCIIfAACAIQh+AAAAhiD4AQAAGILgBwAAYAiCHwAAgCEIfgAAAIYg+AEAABiC4AcAAGAI\\\ngh8AAIAhCH4AAACGIPgBAAAYguAHAABgCIIfAACAIQh+AAAAhiD4AQAAGILgBwAAYAiCHwAAgCEI\\\nfgAAAIYg+AEAABiC4AcAAGAIgh8AAIAhCH4AAACGIPgBAAAYguAHAABgCIIfAACAIQh+AAAAhiD4\\\nAQAAGILgBwAAYIhqG/w++OADDRgwQFFRUXI4HFq9enWh5ZZlafLkyYqMjFRQUJDi4+P19ddf21Ms\\\nAABABai2we/cuXNq166dXn311SKX//Of/9TLL7+suXPn6uOPP9Z1112nhIQE/fbbbxVcKQAAQMVw\\\nWJZl2V2EtzkcDq1atUqDBg2SdLG3LyoqSv/93/+tv//975Ikl8ul8PBwLViwQEOHDrWxWgAAAO/w\\\ns7sAO2RkZOjkyZOKj4/3zHM6nerSpYt2795dbPDLzc1Vbm6u53VBQYFOnz6tunXryuFweL1uAABw\\\n9SzL0pkzZxQVFSUfn2o76FkiI4PfyZMnJUnh4eGF5oeHh3uWFSU1NVXTpk3zam0AAMC7MjMz1aBB\\\nA7vLsIWRwe9qTZw4UePGjfO8drlcatiwoTIzMxUSEmJjZQAA4Ercbreio6NVq1Ytu0uxjZHBLyIi\\\nQpKUlZWlyMhIz/ysrCy1b9++2PUCAgIUEBBw2fyQkBCCHwAAVYTJl2cZOcAdExOjiIgIbd261TPP\\\n7Xbr448/VmxsrI2VAQAAeE+17fE7e/asjh496nmdkZGh9PR01alTRw0bNlRKSoqeeeYZNW/eXDEx\\\nMXryyScVFRXlufMXAACguqm2wW/v3r2Ki4vzvL50bV5SUpIWLFig8ePH69y5c3rooYf0yy+/qHv3\\\n7tq4caMCAwPtKhkAAMCrjHiOn7e43W45nU65XC6u8QMAmxQUFCgvL8/uMlAJ1KhRQ76+vsUu53u7\\\nGvf4AQCqv7y8PGVkZKigoMDuUlBJhIaGKiIiwugbOEpC8AMAVEmWZemHH36Qr6+voqOjjX0gLy6y\\\nLEs5OTnKzs6WpEJP7cD/R/ADAFRJFy5cUE5OjqKiolSzZk27y0ElEBQUJEnKzs5W/fr1Sxz2NRX/\\\nPAIAVEn5+fmSJH9/f5srQWVy6R8B58+ft7mSyongBwCo0riWC7/H+VAygh8AAIAhCH4AAACGIPgB\\\nAFDJ7NixQx07dlRAQICaNWumBQsWeHV/v/32m4YNG6YbbrhBfn5+Rf6K1cqVK9W7d2+FhYUpJCRE\\\nsbGx2rRpk1friouL07x587y6D9MQ/AAAqEQyMjLUv39/xcXFKT09XSkpKRo5cqRXQ1Z+fr6CgoI0\\\nduxYxcfHF9nmgw8+UO/evbV+/Xrt27dPcXFxGjBggA4cOOCVmk6fPq0PP/xQAwYM8Mr2TUXwAwCg\\\ngrzxxhuKioq67IHTAwcO1IgRIyRJc+fOVUxMjF544QW1atVKY8aM0eDBg/XSSy95ra7rrrtOc+bM\\\n0ahRoxQREVFkm5kzZ2r8+PHq3LmzmjdvrunTp6t58+Z67733it3uggULFBoaqnXr1qlFixaqWbOm\\\nBg8erJycHC1cuFCNGzdW7dq1NXbsWM9d2pekpaWpY8eOCg8P188//6zExESFhYUpKChIzZs31/z5\\\n88v1GJiC4AcAQAW5++679dNPP2n79u2eeadPn9bGjRuVmJgoSdq9e/dlvW4JCQnavXt3sds9ceKE\\\ngoODS5ymT59eru+loKBAZ86cUZ06dUpsl5OTo5dffllLly7Vxo0btWPHDt15551av3691q9fr7ff\\\nfluvv/66VqxYUWi9tWvXauDAgZKkJ598Ul988YU2bNigL7/8UnPmzFG9evXK9f2Yggc4AwCMduGC\\\nNH26tGuX1L27NGmS5Oelb8fatWurb9++Wrx4sXr16iVJWrFiherVq6e4uDhJ0smTJxUeHl5ovfDw\\\ncLndbv3666+ehxT/XlRUlNLT00vc95UCWlnNmDFDZ8+e1ZAhQ0psd/78ec2ZM0dNmzaVJA0ePFhv\\\nv/22srKyFBwcrNatWysuLk7bt2/XPffcI0nKzc3Vxo0bNXXqVEkXg22HDh104403SpIaN25cru/F\\\nJAQ/AIDRpk+Xpk6VLEvasuXivMmTvbe/xMREjRo1Sq+99poCAgK0aNEiDR069Jp+cs7Pz0/NmjUr\\\nxypLtnjxYk2bNk1r1qxR/fr1S2xbs2ZNT+iTLobYxo0bKzg4uNC8Sz+1Jknbtm1T/fr11aZNG0nS\\\nww8/rL/85S/av3+/+vTpo0GDBqlr167l/K7MwFAvAMBou3ZdDH3SxT937fLu/gYMGCDLspSWlqbM\\\nzEz9+9//9gzzSlJERISysrIKrZOVlaWQkJAie/ukih3qXbp0qUaOHKnly5cXeyPI79WoUaPQa4fD\\\nUeS831/3uHbtWt1xxx2e13379tXx48f16KOP6vvvv1evXr3097///RrfiZno8QMAGK1794s9fZYl\\\nORwXX3tTYGCg7rrrLi1atEhHjx5VixYt1LFjR8/y2NhYrV+/vtA6mzdvVmxsbLHbrKih3iVLlmjE\\\niBFaunSp+vfvf83bK4plWXrvvff0zjvvFJofFhampKQkJSUl6c9//rMee+wxzZgxwys1VGcEPwCA\\\n0SZNuvjn76/x87bExETdfvvtOnTokO6///5Cy0aPHq3Zs2dr/PjxGjFihLZt26bly5crLS2t2O2V\\\nx1DvF198oby8PJ0+fVpnzpzxBMn27dtLuji8m5SUpFmzZqlLly46efKkJCkoKEhOp/Oa9v17+/bt\\\nU05Ojrr/LoFPnjxZnTp1Ups2bZSbm6t169apVatW5bZPkxD8AABG8/Pz7jV9RenZs6fq1Kmjw4cP\\\n67777iu0LCYmRmlpaXr00Uc1a9YsNWjQQPPmzVNCQoJXa+rXr5+OHz/ued2hQwdJF3vgpIuPorlw\\\n4YKSk5OVnJzsaZeUlFSuD5hes2aN+vXrJ7/f3WHj7++viRMn6ttvv1VQUJD+/Oc/a+nSpeW2T5M4\\\nrEv/RVFmbrdbTqdTLpdLISEhdpcDAEb57bfflJGRoZiYGAUGBtpdDspJ27Zt9cQTT1zxbuHilHRe\\\n8L3NzR0AAKCSyMvL01/+8hf17dvX7lKqLYZ6AQBApeDv768pU6bYXUa1Ro8fAACAIQh+AAAAhiD4\\\nAQAAGILgBwAAYAiCHwAAgCEIfgAAAIYg+AEAABiC4AcAAGAIgh8AAJXMjh071LFjRwUEBKhZs2bl\\\n+lu4Rfn222/lcDgum/bs2eO1fQ4fPlxPPPGE17aPovHLHQAAVCIZGRnq37+/Ro8erUWLFmnr1q0a\\\nOXKkIiMjlZCQ4NV9b9myRW3atPG8rlu3rlf2k5+fr3Xr1iktLc0r20fx6PEDAKCCvPHGG4qKilJB\\\nQUGh+QMHDtSIESMkSXPnzlVMTIxeeOEFtWrVSmPGjNHgwYP10ksveb2+unXrKiIiwjPVqFGj2LY7\\\nduyQw+HQpk2b1KFDBwUFBalnz57Kzs7Whg0b1KpVK4WEhOi+++5TTk5OoXU/+ugj1ahRQ507d1Ze\\\nXp7GjBmjyMhIBQYGqlGjRkpNTfX2WzUWwQ8AUC1YlqWcvAu2TJZllarGu+++Wz/99JO2b9/umXf6\\\n9Glt3LhRiYmJkqTdu3crPj6+0HoJCQnavXt3sds9ceKEgoODS5ymT59+xfruuOMO1a9fX927d9fa\\\ntWtL9Z6mTp2q2bNn66OPPlJmZqaGDBmimTNnavHixUpLS9P777+vV155pdA6a9eu1YABA+RwOPTy\\\nyy9r7dq1Wr58uQ4fPqxFixapcePGpdo3yo6hXgBAtfDr+Xy1nrzJln1/8VSCavpf+Su1du3a6tu3\\\nrxYvXqxevXpJklasWKF69eopLi5OknTy5EmFh4cXWi88PFxut1u//vqrgoKCLttuVFSU0tPTS9x3\\\nnTp1il0WHBysF154Qd26dZOPj4/effddDRo0SKtXr9Ydd9xR4nafeeYZdevWTZL04IMPauLEiTp2\\\n7JiaNGkiSRo8eLC2b9+uxx9/3LPOmjVrPD2YJ06cUPPmzdW9e3c5HA41atSoxP3h2hD8AACoQImJ\\\niRo1apRee+01BQQEaNGiRRo6dKh8fK5+EM7Pz0/NmjW76vXr1auncePGeV537txZ33//vZ5//vkr\\\nBr+2bdt6/h4eHq6aNWt6Qt+leZ988onn9Zdffqnvv//eE3yHDRum3r17q0WLFrrtttt0++23q0+f\\\nPlf9XlAygh8AoFoIquGrL57y7s0PJe27tAYMGCDLspSWlqbOnTvr3//+d6Hr9yIiIpSVlVVonays\\\nLIWEhBTZ2ydd7DVr3bp1ifudNGmSJk2aVOo6u3Tpos2bN1+x3e+vA3Q4HJddF+hwOApd07h27Vr1\\\n7t1bgYGBkqSOHTsqIyNDGzZs0JYtWzRkyBDFx8drxYoVpa4VpUfwAwBUCw6Ho1TDrXYLDAzUXXfd\\\npUWLFuno0aNq0aKFOnbs6FkeGxur9evXF1pn8+bNio2NLXab1zrUW5T09HRFRkaWaZ3SWLNmjR56\\\n6KFC80JCQnTPPffonnvu0eDBg3Xbbbfp9OnTZa4ZV1b5/w8BAKCaSUxM1O23365Dhw7p/vvvL7Rs\\\n9OjRmj17tsaPH68RI0Zo27ZtWr58eYmPPrnWod6FCxfK399fHTp0kCStXLlSb731lubNm3fV2yxK\\\ndna29u7dW+jGkRdffFGRkZHq0KGDfHx89K9//UsREREKDQ0t133jIoIfAAAVrGfPnqpTp44OHz6s\\\n++67r9CymJgYpaWl6dFHH9WsWbPUoEEDzZs3z+vP8Hv66ad1/Phx+fn5qWXLllq2bJkGDx5crvt4\\\n7733dNNNN6levXqeebVq1dI///lPff311/L19VXnzp21fv36a7rmEcVzWKW9Bx2Xcbvdcjqdcrlc\\\nCgkJsbscADDKb7/9poyMDMXExHiuF0Pldscdd6h79+4aP3681/ZR0nnB9zbP8QMAABWke/fuuvfe\\\ne+0uw2gM9QIAgArhzZ4+lI6xPX75+fl68sknFRMTo6CgIDVt2lRPP/10qZ++DgAAUNUY2+P33HPP\\\nac6cOVq4cKHatGmjvXv3avjw4XI6nRo7dqzd5QEAAJQ7Y4PfRx99pIEDB6p///6SpMaNG2vJkiWF\\\nni4OAKj8GKnB73E+lMzYod6uXbtq69atOnLkiCTpP//5j3bt2qW+ffsWu05ubq7cbnehCQBgD1/f\\\ni7+WkZeXZ3MlqExycnIk6bJfEMFFxvb4TZgwQW63Wy1btpSvr6/y8/P1j3/8Q4mJicWuk5qaqmnT\\\nplVglQCA4vj5+almzZo6deqUatSowXPfDGdZlnJycpSdna3Q0FDPPwxQmLHP8Vu6dKkee+wxPf/8\\\n82rTpo3S09OVkpKiF198UUlJSUWuk5ubq9zcXM9rt9ut6Ohoo58HBAB2ysvLU0ZGRqHfgoXZQkND\\\nFRERIYfDcdkynuNncPCLjo7WhAkTlJyc7Jn3zDPP6J133tFXX31Vqm1wAgGA/QoKChjuhaSLw7sl\\\n9fTxvW3wUG9OTs5lwwK+vr78qxEAqhgfHx9+uQMoJWOD34ABA/SPf/xDDRs2VJs2bXTgwAG9+OKL\\\nGjFihN2lAQAAeIWxQ71nzpzRk08+qVWrVik7O1tRUVG69957NXnyZPn7+5dqG3QZAwBQdfC9bXDw\\\nKw+cQAAAVB18bxv8HD8AAADTEPwAAAAMQfADAAAwBMEPAADAEAQ/AAAAQxD8AAAADEHwAwAAMATB\\\nDwAAwBAEPwAAAEMQ/AAAAAxB8AMAADAEwQ8AAMAQBD8AAABDEPwAAAAMQfADAAAwBMEPAADAEAQ/\\\nAAAAQxD8AAAADEHwAwAAMATBDwAAwBAEPwAAAEMQ/AAAAAxB8AMAADAEwQ8AAMAQBD8AAABDEPwA\\\nAAAMQfADAAAwBMEPAADAEAQ/AAAAQxD8AAAADEHwAwAAMATBDwAAwBAEPwAAAEMQ/AAAAAxB8AMA\\\nADAEwQ8AAMAQBD8AAABDEPwAAAAMQfADAAAwBMEPAADAEAQ/AAAAQxgd/L777jvdf//9qlu3roKC\\\ngnTDDTdo7969dpcFAADgFX52F2CXn3/+Wd26dVNcXJw2bNigsLAwff3116pdu7bdpQEAAHiFscHv\\\nueeeU3R0tObPn++ZFxMTY2NFAAAA3mXsUO/atWt144036u6771b9+vXVoUMHvfnmm3aXBQAA4DXG\\\nBr9vvvlGc+bMUfPmzbVp0yY9/PDDGjt2rBYuXFjsOrm5uXK73YUmAACAqsJhWZZldxF28Pf31403\\\n3qiPPvrIM2/s2LH69NNPtXv37iLXmTp1qqZNm3bZfJfLpZCQEK/VCgAArp3b7ZbT6TT6e9vYHr/I\\\nyEi1bt260LxWrVrpxIkTxa4zceJEuVwuz5SZmentMgEAAMqNsTd3dOvWTYcPHy4078iRI2rUqFGx\\\n6wQEBCggIMDbpQEAAHiFsT1+jz76qPbs2aPp06fr6NGjWrx4sd544w0lJyfbXRoAAIBXGBv8Onfu\\\nrFWrVmnJkiW6/vrr9fTTT2vmzJlKTEy0uzQAAACvMPbmjvLARaIAAFQdfG8b3OMHAABgGoIfAACA\\\nIQh+AAAAhiD4AQAAGILgBwAAYAiCHwAAgCEIfgAAAIYg+AEAABiC4AcAAGAIgh8AAIAhCH4AAACG\\\nIPgBAAAYguAHAABgCIIfAACAIQh+AAAAhiD4AQAAGILgBwAAYAiCHwAAgCEIfgAAAIYg+AEAABiC\\\n4AcAAGAIgh8AAIAhCH4AAACGIPgBAAAYguAHAABgCIIfAACAIQh+AAAAhiD4AQAAGILgBwAAYAiC\\\nHwAAgCEIfgAAAIYg+AEAABiC4AcAAGAIgh8AAIAhCH4AAACGIPgBAAAYguAHAABgCIIfAACAIQh+\\\nAAAAhiD4AQAAGILgBwAAYAiCHwAAgCEIfv/Ps88+K4fDoZSUFLtLAQAA8AqCn6RPP/1Ur7/+utq2\\\nbWt3KQAAAF5jfPA7e/asEhMT9eabb6p27dp2lwMAAOA1xge/5ORk9e/fX/Hx8Vdsm5ubK7fbXWgC\\\nAACoKvzsLsBOS5cu1f79+/Xpp5+Wqn1qaqqmTZvm5aoAAAC8w9gev8zMTP3tb3/TokWLFBgYWKp1\\\nJk6cKJfL5ZkyMzO9XCUAAED5cViWZdldhB1Wr16tO++8U76+vp55+fn5cjgc8vHxUW5ubqFlRXG7\\\n3XI6nXK5XAoJCfF2yQAA4BrwvW3wUG+vXr108ODBQvOGDx+uli1b6vHHH79i6AMAAKhqjA1+tWrV\\\n0vXXX19o3nXXXae6deteNh8AAKA6MPYaPwAAANMY2+NXlB07dthdAgAAgNfQ4wcAAGAIgh8AAIAh\\\nbBnq/eyzz8q8TuvWreXnx8g0AADA1bIlSbVv314Oh0OlfYSgj4+Pjhw5oiZNmni5MgAAgOrLti60\\\njz/+WGFhYVdsZ1kWj1cBAAAoB7YEv1tvvVXNmjVTaGhoqdrfcsstCgoK8m5RAAAA1ZyxP9lWHvjp\\\nFwAAqg6+t7mrFwAAwBi23yZrWZZWrFih7du3Kzs7WwUFBYWWr1y50qbKAAAAqhfbg19KSopef/11\\\nxcXFKTw8XA6Hw+6SAAAAqiXbg9/bb7+tlStXql+/fnaXAgAAUK3Zfo2f0+nk+XwAAAAVwPbgN3Xq\\\nVE2bNk2//vqr3aUAAABUa7YP9Q4ZMkRLlixR/fr11bhxY9WoUaPQ8v3799tUGQAAQPVie/BLSkrS\\\nvn37dP/993NzBwAAgBfZHvzS0tK0adMmde/e3e5SAAAAqjXbr/GLjo429unZAAAAFcn24PfCCy9o\\\n/Pjx+vbbb+0uBQAAoFqzfaj3/vvvV05Ojpo2baqaNWtednPH6dOnbaoMAACgerE9+M2cOdPuEgAA\\\nAIxge/BLSkqyuwQAAAAj2HKNn9vtLlP7M2fOeKkSAAAAc9gS/GrXrq3s7OxSt/+v//ovffPNN16s\\\nCAAAoPqzZajXsizNmzdPwcHBpWp//vx5L1cEAABQ/dkS/Bo2bKg333yz1O0jIiIuu9sXAAAAZWNL\\\n8OOZfQAAABXP9gc4AwAAoGIQ/AAAAAxB8AMAADAEwQ8AAMAQBD8AAABD2Bb8evXqpZUrVxa7/Mcf\\\nf1STJk0qsCIAAIDqzbbgt337dg0ZMkRTpkwpcnl+fr6OHz9ewVUBAABUX7YO9c6ZM0czZ87UnXfe\\\nqXPnztlZCgAAQLVna/AbOHCg9uzZo0OHDunmm2/m93gBAAC8yPabO1q1aqVPP/1U0dHR6ty5s7Zs\\\n2WJ3SQAAANWS7cFPkpxOp9LS0jRq1Cj169dPL730kt0lAQAAVDu2/FavJDkcjsteP/vss2rfvr1G\\\njhypbdu22VQZAABA9WRbj59lWUXOHzp0qHbt2qWDBw9WcEUAAADVm209ftu3b1edOnWKXNa+fXvt\\\n27dPaWlpFVwVAABA9eWwiut6wxW53W45nU65XC6FhITYXQ4AACgB39uV5OYOO6Smpqpz586qVauW\\\n6tevr0GDBunw4cN2lwUAAOA1xga/nTt3Kjk5WXv27NHmzZt1/vx59enThwdJAwCAaouh3v/n1KlT\\\nql+/vnbu3KlbbrmlVOvQZQwAQNXB97bBPX5/5HK5JKnYG04AAACqOtvu6q1MCgoKlJKSom7duun6\\\n668vtl1ubq5yc3M9r91ud0WUBwAAUC7o8ZOUnJyszz//XEuXLi2xXWpqqpxOp2eKjo6uoAoBAACu\\\nnfHX+I0ZM0Zr1qzRBx98oJiYmBLbFtXjFx0dbfS1AgAAVBVc42fwUK9lWXrkkUe0atUq7dix44qh\\\nT5ICAgIUEBBQAdUBAACUP2ODX3JyshYvXqw1a9aoVq1aOnnypCTJ6XQqKCjI5uoAAADKn7FDvQ6H\\\no8j58+fP17Bhw0q1DbqMAQCoOvjeNrjHz9C8CwAADMZdvQAAAIYg+AEAABiC4AcAAGAIgh8AAIAh\\\nCH4AAACGIPgBAAAYguAHAABgCIIfAACAIQh+AAAAhiD4AQAAGILgBwAAYAiCHwAAgCEIfgAAAIYg\\\n+AEAABiC4AcAAGAIgh8AAIAhCH4AAACGIPgBAAAYguAHAABgCIIfAACAIQh+AAAAhiD4VZALF6Sn\\\nnpL69Ln454ULdlcEAABM42d3AaaYPl2aOlWyLGnLlovzJk+2tSQAAGAYevwqyK5dF0OfdPHPXbvs\\\nrQcAAJiH4FdBuneXHI6Lf3c4Lr4GAACoSAz1VpBJky7+uWvXxdB36TUAAEBFIfhVED8/rukDAAD2\\\nYqgXAADAEAQ/AAAAQxD8AAAADEHwAwAAMATBDwAAwBAEPwAAAEMQ/AAAAAxB8AMAADAEwa8CZZ/5\\\nTY+v+Exncy/YXQoAADAQv9xRQT44ckrjlqfrx7N5kqTnBre1uSIAAGAagp+Xnc8v0AvvH9Hcncck\\\nSS0jamnULU1srgoAAJiI4OdFmadz9MiSA0rP/EWS9H9iG2lSv1YKrOFrb2EAAMBIBD8vSfvsB014\\\n9zOdyb2gkEA//XNwO912fYTdZQEAAIMR/MrZb+fz9dS6L7T44xOSpE6NamvW0PZqULumzZUBAADT\\\nGX9X76uvvqrGjRsrMDBQXbp00SeffHLV2zqSdUZ3zN6lxR+fkMMhjYlrpmUP3UzoAwAAlYLRwW/Z\\\nsmUaN26cpkyZov3796tdu3ZKSEhQdnZ2mbZjWZaWfHJCd8zepSNZZxVWK0DvPNhFf09oIT9fow8x\\\nAACoRByWZVl2F2GXLl26qHPnzpo9e7YkqaCgQNHR0XrkkUc0YcKEK67vdrvldDo18s2d2nz0jCTp\\\nlj+F6cUh7VQvOMCrtQMAgLK59L3tcrkUEhJidzm2MLY7Ki8vT/v27VN8fLxnno+Pj+Lj47V79+4y\\\nbWvToSz5+Tg0qV9LLRjWmdAHAAAqJWNv7vjxxx+Vn5+v8PDwQvPDw8P11VdfFblObm6ucnNzPa/d\\\nbrck6YIrUANDuuqhW0K9Vi8AAMC1MrbH72qkpqbK6XR6pujoaEnSD+901eHdofYWBwAAcAXGBr96\\\n9erJ19dXWVlZheZnZWUpIqLo5+1NnDhRLpfLM2VmZl5ccL6Gunf3dsUAAADXxtjg5+/vr06dOmnr\\\n1q2eeQUFBdq6datiY2OLXCcgIEAhISGFJkmaOFGaNKlCygYAALhqxl7jJ0njxo1TUlKSbrzxRt10\\\n002aOXOmzp07p+HDh5dpOxMmSH5GH0kAAFAVGB1X7rnnHp06dUqTJ0/WyZMn1b59e23cuPGyGz4A\\\nAACqA6Of43eteB4QAABVB9/bBl/jBwAAYBqCHwAAgCEIfgAAAIYg+AEAABiC4AcAAGAIgh8AAIAh\\\nCH4AAACGIPgBAAAYguAHAABgCIIfAACAIQh+AAAAhiD4AQAAGILgBwAAYAiCHwAAgCEIfgAAAIYg\\\n+AEAABiC4AcAAGAIgh8AAIAhCH4AAACGIPgBAAAYguAHAABgCIIfAACAIQh+AAAAhiD4AQAAGILg\\\nBwAAYAiCHwAAgCEIfgAAAIYg+AEAABiC4AcAAGAIgh8AAIAhCH4AAACGIPgBAAAYguAHAABgCIIf\\\nAACAIQh+AAAAhiD4AQAAGILgBwAAYAiCHwAAgCEIfgAAAIYg+AEAABiC4AcAAGAIgh8AAIAhjAx+\\\n3377rR588EHFxMQoKChITZs21ZQpU5SXl2d3aQAAAF7jZ3cBdvjqq69UUFCg119/Xc2aNdPnn3+u\\\nUaNG6dy5c5oxY4bd5QEAAHiFw7Isy+4iKoPnn39ec+bM0TfffFPqddxut5xOp1wul0JCQrxYHQAA\\\nuFZ8bxva41cUl8ulOnXqlNgmNzdXubm5ntdut9vbZQEAAJQbI6/x+6OjR4/qlVde0V//+tcS26Wm\\\npsrpdHqm6OjoCqoQAADg2lWr4DdhwgQ5HI4Sp6+++qrQOt99951uu+023X333Ro1alSJ2584caJc\\\nLpdnyszM9ObbAQAAKFfV6hq/U6dO6aeffiqxTZMmTeTv7y9J+v7779WjRw/dfPPNWrBggXx8ypaD\\\nuVYAAICqg+/tanaNX1hYmMLCwkrV9rvvvlNcXJw6deqk+fPnlzn0AQAAVDXVKviV1nfffacePXqo\\\nUaNGmjFjhk6dOuVZFhERYWNlAAAA3mNk8Nu8ebOOHj2qo0ePqkGDBoWWVaORbwAAgEKMHN8cNmyY\\\nLMsqcgIAAKiujAx+AAAAJiL4AQAAGILgBwAAYAiCHwAAgCEIfgAAAIYg+AEAABiC4AcAAGAIgh8A\\\nAIAhCH4AAACGIPgBAAAYguAHAABgCIIfAACAIQh+AAAAhiD4AQAAGILgBwAAYAiCHwAAgCEIfgAA\\\nAIYg+AEAABiC4AcAAGAIgh8AAIAhCH4AAACGIPgBAAAYguAHAABgCIIfAACAIQh+AAAAhiD4AQAA\\\nGILgBwAAYAiCHwAAgCEIfgAAAIYg+AEAABiC4AcAAGAIgh8AAIAhCH4AAACGIPgBAAAYguAHAABg\\\nCIIfAACAIQh+AAAAhiD4AQAAGILgBwAAYAiCHwAAgCEIfgAAAIYwPvjl5uaqffv2cjgcSk9Pt7sc\\\nAAAArzE++I0fP15RUVF2lwEAAOB1Rge/DRs26P3339eMGTPsLgUAAMDr/OwuwC5ZWVkaNWqUVq9e\\\nrZo1a9pdDgAAgNcZGfwsy9KwYcM0evRo3Xjjjfr2229LtV5ubq5yc3M9r10ulyTJ7XZ7o0wAAFCO\\\nLn1fW5ZlcyX2qVbBb8KECXruuedKbPPll1/q/fff15kzZzRx4sQybT81NVXTpk27bH50dHSZtgMA\\\nAOzz008/yel02l2GLRxWNYq9p06d0k8//VRimyZNmmjIkCF677335HA4PPPz8/Pl6+urxMRELVy4\\\nsMh1/9jj98svv6hRo0Y6ceKEsSdQeXC73YqOjlZmZqZCQkLsLqdK41iWD45j+eA4lh+OZflwuVxq\\\n2LChfv75Z4WGhtpdji2qVY9fWFiYwsLCrtju5Zdf1jPPPON5/f333yshIUHLli1Tly5dil0vICBA\\\nAQEBl813Op38j1gOQkJCOI7lhGNZPjiO5YPjWH44luXDx8fce1urVfArrYYNGxZ6HRwcLElq2rSp\\\nGjRoYEdJAAAAXmdu5AUAADCMkT1+f9S4ceOrusMnICBAU6ZMKXL4F6XHcSw/HMvywXEsHxzH8sOx\\\nLB8cx2p2cwcAAACKx1AvAACAIQh+AAAAhiD4AQAAGILgdwWvvvqqGjdurMDAQHXp0kWffPJJie3/\\\n9a9/qWXLlgoMDNQNN9yg9evXV1CllVtZjuOCBQvkcDgKTYGBgRVYbeX0wQcfaMCAAYqKipLD4dDq\\\n1auvuM6OHTvUsWNHBQQEqFmzZlqwYIHX66wKynosd+zYcdk56XA4dPLkyYopuBJKTU1V586dVatW\\\nLdWvX1+DBg3S4cOHr7gen5GXu5pjyefk5ebMmaO2bdt6nnUYGxurDRs2lLiOiecjwa8Ey5Yt07hx\\\n4zRlyhTt379f7dq1U0JCgrKzs4ts/9FHH+nee+/Vgw8+qAMHDmjQoEEaNGiQPv/88wquvHIp63GU\\\nLj6k9IcffvBMx48fr8CKK6dz586pXbt2evXVV0vVPiMjQ/3791dcXJzS09OVkpKikSNHatOmTV6u\\\ntPIr67G85PDhw4XOy/r163upwspv586dSk5O1p49e7R582adP39effr00blz54pdh8/Iol3NsZT4\\\nnPyjBg0a6Nlnn9W+ffu0d+9e9ezZUwMHDtShQ4eKbG/s+WihWDfddJOVnJzseZ2fn29FRUVZqamp\\\nRbYfMmSI1b9//0LzunTpYv31r3/1ap2VXVmP4/z58y2n01lB1VVNkqxVq1aV2Gb8+PFWmzZtCs27\\\n5557rISEBC9WVvWU5lhu377dkmT9/PPPFVJTVZSdnW1Jsnbu3FlsGz4jS6c0x5LPydKpXbu2NW/e\\\nvCKXmXo+0uNXjLy8PO3bt0/x8fGeeT4+PoqPj9fu3buLXGf37t2F2ktSQkJCse1NcDXHUZLOnj2r\\\nRo0aKTo6usR/saF4nI/lr3379oqMjFTv3r314Ycf2l1OpeJyuSRJderUKbYN52TplOZYSnxOliQ/\\\nP19Lly7VuXPnFBsbW2QbU89Hgl8xfvzxR+Xn5ys8PLzQ/PDw8GKv6zl58mSZ2pvgao5jixYt9NZb\\\nb2nNmjV65513VFBQoK5du+p///d/K6LkaqO489HtduvXX3+1qaqqKTIyUnPnztW7776rd999V9HR\\\n0erRo4f2799vd2mVQkFBgVJSUtStWzddf/31xbbjM/LKSnss+Zws2sGDBxUcHKyAgACNHj1aq1at\\\nUuvWrYtsa+r5yC93oNKJjY0t9C+0rl27qlWrVnr99df19NNP21gZTNWiRQu1aNHC87pr1646duyY\\\nXnrpJb399ts2VlY5JCcn6/PPP9euXbvsLqXKK+2x5HOyaC1atFB6erpcLpdWrFihpKQk7dy5s9jw\\\nZyJ6/IpRr149+fr6Kisrq9D8rKwsRUREFLlOREREmdqb4GqO4x/VqFFDHTp00NGjR71RYrVV3PkY\\\nEhKioKAgm6qqPm666SbOSUljxozRunXrtH37djVo0KDEtnxGlqwsx/KP+Jy8yN/fX82aNVOnTp2U\\\nmpqqdu3aadasWUW2NfV8JPgVw9/fX506ddLWrVs98woKCrR169ZirxeIjY0t1F6SNm/eXGx7E1zN\\\ncfyj/Px8HTx4UJGRkd4qs1rifPSu9PR0o89Jy7I0ZswYrVq1Stu2bVNMTMwV1+GcLNrVHMs/4nOy\\\naAUFBcrNzS1ymbHno913l1RmS5cutQICAqwFCxZYX3zxhfXQQw9ZoaGh1smTJy3LsqwHHnjAmjBh\\\ngqf9hx9+aPn5+VkzZsywvvzyS2vKlClWjRo1rIMHD9r1FiqFsh7HadOmWZs2bbKOHTtm7du3zxo6\\\ndKgVGBhoHTp0yK63UCmcOXPGOnDggHXgwAFLkvXiiy9aBw4csI4fP25ZlmVNmDDBeuCBBzztv/nm\\\nG6tmzZrWY489Zn355ZfWq6++avn6+lobN2606y1UGmU9li+99JK1evVq6+uvv7YOHjxo/e1vf7N8\\\nfHysLVu22PUWbPfwww9bTqfT2rFjh/XDDz94ppycHE8bPiNL52qOJZ+Tl5swYYK1c+dOKyMjw/rs\\\ns8+sCRMmWA6Hw3r//fcty+J8vITgdwWvvPKK1bBhQ8vf39+66aabrD179niW3XrrrVZSUlKh9suX\\\nL7f+9Kc/Wf7+/labNm2stLS0Cq64cirLcUxJSfG0DQ8Pt/r162ft37/fhqorl0uPFPnjdOnYJSUl\\\nWbfeeutl67Rv397y9/e3mjRpYs2fP7/C666Mynosn3vuOatp06ZWYGCgVadOHatHjx7Wtm3b7Cm+\\\nkijq+EkqdI7xGVk6V3Ms+Zy83IgRI6xGjRpZ/v7+VlhYmNWrVy9P6LMszsdLHJZlWRXXvwgAAAC7\\\ncI0fAACAIQh+AAAAhiD4AQAAGILgBwAAYAiCHwAAgCEIfgAAAIYg+AEAABiC4AcAAGAIgh+AamPY\\\nsGEaNGhQhe93wYIFcjgccjgcSklJKdU6w4YN86yzevVqr9YHAJf42V0AAJSGw+EocfmUKVM0a9Ys\\\n2fVjRCEhITp8+LCuu+66UrWfNWuWnn32WUVGRnq5MgD4/wh+AKqEH374wfP3ZcuWafLkyTp8+LBn\\\nXnBwsIKDg+0oTdLFYBoREVHq9k6nU06n04sVAcDlGOoFUCVERER4JqfT6Qlal6bg4ODLhnp79Oih\\\nRx55RCkpKapdu7bCw8P15ptv6ty5cxo+fLhq1aqlZs2aacOGDYX29fnnn6tv374KDg5WeHi4Hnjg\\\nAf34449lrvm1115T8+bNFRgYqPDwcA0ePPhaDwMAXBOCH4BqbeHChapXr54++eQTPfLII3r44Yd1\\\n9913q2vXrtq/f7/69OmjBx54QDk5OZKkX375RT179lSHDh20d+9ebdy4UVlZWRoyZEiZ9rt3716N\\\nHTtWTz31lA4fPqyNGzfqlltu8cZbBIBSY6gXQLXWrl07PfHEE5KkiRMn6tlnn1W9evU0atQoSdLk\\\nyZM1Z84cffbZZ7r55ps1e/ZsdejQQdOnT/ds46233lJ0dLSOHDmiP/3pT6Xa74kTJ3Tdddfp9ttv\\\nV61atdSoUSN16NCh/N8gAJQBPX4AqrW2bdt6/u7r66u6devqhhtu8MwLDw+XJGVnZ0uS/vOf/2j7\\\n9u2eawaDg4PVsmVLSdKxY8dKvd/evXurUaNGatKkiR544AEtWrTI06sIAHYh+AGo1mrUqFHotcPh\\\nKDTv0t3CBQUFkqSzZ89qwIABSk9PLzR9/fXXZRqqrVWrlvbv368lS5YoMjJSkydPVrt27fTLL79c\\\n+5sCgKvEUC8A/E7Hjh317rvvqnHjxvLzu7aPSD8/P8XHxys+Pl5TpkxRaGiotm3bprvuuqucqgWA\\\nsqHHDwB+Jzk5WadPn9a9996rTz/9VMeOHdOmTZs0fPhw5efnl3o769at08svv6z09HQdP35c//M/\\\n/6OCggK1aNHCi9UDQMkIfgDwO1FRUfrwww+Vn5+vPn366IYbblBKSopCQ0Pl41P6j8zQ0FCtXLlS\\\nPXv2VKtWrTR37lwtWbJEbdq08WL1AFAyh2XXY+4BoJpYsGCBUlJSrur6PYfDoVWrVtnyU3MAzEOP\\\nHwCUA5fLpeDgYD3++OOlaj969Ghbf2kEgJno8QOAa3TmzBllZWVJujjEW69evSuuk52dLbfbLUmK\\\njIws9W/8AsC1IPgBAAAYgqFeAAAAQxD8AAAADEHwAwAAMATBDwAAwBAEPwAAAEMQ/AAAAAxB8AMA\\\nADAEwQ8AAMAQ/xfVMnaZVQeobQAAAABJRU5ErkJggg==\\\n\"\n  frames[3] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAn4AAAHcCAYAAABI7KcYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\\\nRU5ErkJggg==\\\n\"\n  frames[4] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAn4AAAHcCAYAAABI7KcYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\\\n\"\n  frames[5] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAn4AAAHcCAYAAABI7KcYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\\\n\"\n  frames[6] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAn4AAAHcCAYAAABI7KcYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\\\nY4oAAAAASUVORK5CYII=\\\n\"\n  frames[7] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAn4AAAHcCAYAAABI7KcYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\\\nAAAAQxD8AAAADEHwAwAAMMT/BxnKQyMhhWM5AAAAAElFTkSuQmCC\\\n\"\n  frames[8] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAn4AAAHcCAYAAABI7KcYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\\\n\"\n  frames[9] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAn4AAAHcCAYAAABI7KcYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\\\nAIb4f7ms09fNARWaAAAAAElFTkSuQmCC\\\n\"\n  frames[10] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAn4AAAHcCAYAAABI7KcYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\\\ngg==\\\n\"\n  frames[11] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAn4AAAHcCAYAAABI7KcYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\\\n\"\n  frames[12] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAn4AAAHcCAYAAABI7KcYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\\\n\"\n  frames[13] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAn4AAAHcCAYAAABI7KcYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\\\ncQ3QvbT9AAAAAElFTkSuQmCC\\\n\"\n  frames[14] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAn4AAAHcCAYAAABI7KcYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\\\nwBAEPwAAAEP8fxYHQ77YG9XIAAAAAElFTkSuQmCC\\\n\"\n  frames[15] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAn4AAAHcCAYAAABI7KcYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\\\n\"\n  frames[16] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAn4AAAHcCAYAAABI7KcYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\\\ngCEIfgAAAIYg+AEAABji/wMeyk6afBmNewAAAABJRU5ErkJggg==\\\n\"\n  frames[17] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAn4AAAHcCAYAAABI7KcYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\\\nAAAASUVORK5CYII=\\\n\"\n  frames[18] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAn4AAAHcCAYAAABI7KcYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\\\n/wOP9m0EsqBWQQAAAABJRU5ErkJggg==\\\n\"\n  frames[19] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAn4AAAHcCAYAAABI7KcYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\\\nSUVORK5CYII=\\\n\"\n  frames[20] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAn4AAAHcCAYAAABI7KcYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\\\n\"\n  frames[21] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAn4AAAHcCAYAAABI7KcYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\\\n\"\n  frames[22] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAn4AAAHcCAYAAABI7KcYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\\\nCIIfAACARfx/NlJwQaN/Yn8AAAAASUVORK5CYII=\\\n\"\n  frames[23] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAn4AAAHcCAYAAABI7KcYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\\\n\"\n  frames[24] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAn4AAAHcCAYAAABI7KcYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\\\neh6h6/gZO0sAAAAASUVORK5CYII=\\\n\"\n  frames[25] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAn4AAAHcCAYAAABI7KcYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\\\n\"\n  frames[26] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAn4AAAHcCAYAAABI7KcYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\\\npwAAAABJRU5ErkJggg==\\\n\"\n  frames[27] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAn4AAAHcCAYAAABI7KcYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\\\n\"\n  frames[28] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAn4AAAHcCAYAAABI7KcYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\\\nAAAWQfADAACwiP8Pbev8eN5swmEAAAAASUVORK5CYII=\\\n\"\n  frames[29] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAn4AAAHcCAYAAABI7KcYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\\\nAIv4/z9JJz2mhd0qAAAAAElFTkSuQmCC\\\n\"\n  frames[30] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAn4AAAHcCAYAAABI7KcYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\\\nYII=\\\n\"\n  frames[31] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAn4AAAHcCAYAAABI7KcYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\\\n/7naXWKaWQAsAAAAAElFTkSuQmCC\\\n\"\n  frames[32] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAn4AAAHcCAYAAABI7KcYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\\\nbGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9h\\\nAAAPYQGoP6dpAABEeklEQVR4nO3dd3xUVf7/8fekB0ISICFFAoTeOyIQC9IURFhFLOiPIli+qIuu\\\nIrBKURd0V0UUBdQVWEUQUQQNRapsEEVKpEoNRSAJNQkJqXN/f0SyREhIm9zJ3Nfz8ZjHMHfunfnM\\\n9TrzzjnnnmszDMMQAAAAXJ6b2QUAAACgfBD8AAAALILgBwAAYBEEPwAAAIsg+AEAAFgEwQ8AAMAi\\\nCH4AAAAWQfADAACwCIIfAACARRD8AAAALILgBwAAYBEEPwAAAIsg+AEAAFgEwQ8AAMAiCH4AAAAW\\\nQfADAACwCIIfAACARRD8AAAALILgBwAAYBEEPwAAAIsg+AEAAFgEwQ8AAMAiCH4AAAAWQfADAACw\\\nCIIfAACARRD8AAAALILgBwAAYBEEPwAAAIsg+AEAAFgEwQ8AAMAiCH4AAAAWQfADAACwCIIfAACA\\\nRRD8AAAALILgBwAAYBEEPwAAAIsg+AEAAFgEwQ8AAMAiCH4AAAAWQfADAACwCIIfAACARRD8AAAA\\\nLILgBwAAYBEEPwAAAIsg+AEAAFgEwQ8AAMAiCH4AAAAWQfADAACwCIIfAACARRD8AAAALILgBwAA\\\nYBEEPwAAAItw2eC3YcMG9e3bV+Hh4bLZbPrmm2/yPW8YhsaPH6+wsDD5+vqqe/fuOnDggDnFAgAA\\\nlAOXDX6pqalq1aqV3n///Ws+/89//lPvvvuuZs6cqZ9//lmVK1dWr169lJ6eXs6VAgAAlA+bYRiG\\\n2UU4ms1m0+LFi9W/f39Jua194eHh+tvf/qbnn39ekpSUlKSQkBDNmTNHDzzwgInVAgAAOIaH2QWY\\\nIS4uTvHx8erevXvesoCAAHXs2FGbNm0qMPhlZGQoIyMj77Hdbte5c+dUvXp12Ww2h9cNAABKzjAM\\\npaSkKDw8XG5uLtvpWShLBr/4+HhJUkhISL7lISEhec9dy5QpUzRp0iSH1gYAABzr+PHjqlmzptll\\\nmMKSwa+kxo4dq+eeey7vcVJSkmrVqqXjx4/L39/fxMoAAMD1JCcnKyIiQlWqVDG7FNNYMviFhoZK\\\nkhISEhQWFpa3PCEhQa1bty5wO29vb3l7e1+13N/fn+AHAEAFYeXhWZbs4I6MjFRoaKjWrFmTtyw5\\\nOVk///yzOnXqZGJlAAAAjuOyLX4XL17UwYMH8x7HxcUpNjZW1apVU61atTRq1Ci99tpratCggSIj\\\nI/Xyyy8rPDw878xfAAAAV+OywW/Lli3q2rVr3uPLY/MGDx6sOXPmaPTo0UpNTdVjjz2mCxcuKCoq\\\nSitWrJCPj49ZJQMAADiUJebxc5Tk5GQFBAQoKSmJMX4AYBK73a7MzEyzy4AT8PT0lLu7e4HP87vt\\\nwi1+AADXl5mZqbi4ONntdrNLgZMIDAxUaGiopU/gKAzBDwBQIRmGoVOnTsnd3V0RERGWnZAXuQzD\\\nUFpamhITEyUp36wd+B+CHwCgQsrOzlZaWprCw8NVqVIls8uBE/D19ZUkJSYmqkaNGoV2+1oVfx4B\\\nACqknJwcSZKXl5fJlcCZXP4jICsry+RKnBPBDwBQoTGWC1fieCgcwQ8AAMAiCH4AAAAWQfADAMDJ\\\nrF+/Xm3btpW3t7fq16+vOXPmOPT90tPTNWTIELVo0UIeHh7XvIrV119/rR49eig4OFj+/v7q1KmT\\\nVq5c6dC6unbtqo8//tih72E1BD8AAJxIXFyc+vTpo65duyo2NlajRo3S8OHDHRqycnJy5Ovrq2ee\\\neUbdu3e/5jobNmxQjx49tGzZMm3dulVdu3ZV3759tX37dofUdO7cOW3cuFF9+/Z1yOtbFcEPAIBy\\\n8uGHHyo8PPyqCaf79eunYcOGSZJmzpypyMhIvfXWW2rSpImeeuopDRgwQFOnTnVYXZUrV9aMGTM0\\\nYsQIhYaGXnOdd955R6NHj1aHDh3UoEEDTZ48WQ0aNNC3335b4OvOmTNHgYGB+u6779SoUSNVqlRJ\\\nAwYMUFpamubOnas6deqoatWqeuaZZ/LO0r4sOjpabdu2VUhIiM6fP69BgwYpODhYvr6+atCggWbP\\\nnl2m+8AqCH4AAJST++67T2fPntW6devylp07d04rVqzQoEGDJEmbNm26qtWtV69e2rRpU4Gve+zY\\\nMfn5+RV6mzx5cpl+FrvdrpSUFFWrVq3Q9dLS0vTuu+9qwYIFWrFihdavX6+//OUvWrZsmZYtW6ZP\\\nP/1Us2bN0qJFi/Jtt3TpUvXr10+S9PLLL2vPnj1avny59u7dqxkzZigoKKhMP49VMIEzAMDSsrOl\\\nyZOlmBgpKkoaN07ycNCvY9WqVXXnnXfq888/V7du3SRJixYtUlBQkLp27SpJio+PV0hISL7tQkJC\\\nlJycrEuXLuVNUnyl8PBwxcbGFvre1wtoxfXmm2/q4sWLGjhwYKHrZWVlacaMGapXr54kacCAAfr0\\\n00+VkJAgPz8/NW3aVF27dtW6det0//33S5IyMjK0YsUKTZw4UVJusG3Tpo3at28vSapTp06ZfhYr\\\nIfgBACxt8mRp4kTJMKTVq3OXjR/vuPcbNGiQRowYoQ8++EDe3t6aN2+eHnjggVJdcs7Dw0P169cv\\\nwyoL9/nnn2vSpElasmSJatSoUei6lSpVygt9Um6IrVOnjvz8/PItu3ypNUlau3atatSooWbNmkmS\\\nnnzySd17773atm2bevbsqf79+6tz585l/Kmsga5eAIClxcTkhj4p9z4mxrHv17dvXxmGoejoaB0/\\\nflz//e9/87p5JSk0NFQJCQn5tklISJC/v/81W/uk8u3qXbBggYYPH66FCxcWeCLIlTw9PfM9ttls\\\n11x25bjHpUuX6u677857fOedd+ro0aN69tlndfLkSXXr1k3PP/98KT+JNdHiBwCwtKio3JY+w5Bs\\\nttzHjuTj46N77rlH8+bN08GDB9WoUSO1bds27/lOnTpp2bJl+bZZtWqVOnXqVOBrlldX7/z58zVs\\\n2DAtWLBAffr0KfXrXYthGPr222/12Wef5VseHByswYMHa/Dgwbr55pv1wgsv6M0333RIDa6M4AcA\\\nsLRx43Lvrxzj52iDBg3SXXfdpd27d+vhhx/O99wTTzyh6dOna/To0Ro2bJjWrl2rhQsXKjo6usDX\\\nK4uu3j179igzM1Pnzp1TSkpKXpBs3bq1pNzu3cGDB2vatGnq2LGj4uPjJUm+vr4KCAgo1XtfaevW\\\nrUpLS1PUFQl8/PjxateunZo1a6aMjAx99913atKkSZm9p5UQ/AAAlubh4dgxfddy++23q1q1atq3\\\nb58eeuihfM9FRkYqOjpazz77rKZNm6aaNWvq448/Vq9evRxaU+/evXX06NG8x23atJGU2wIn5U5F\\\nk52drZEjR2rkyJF56w0ePLhMJ5hesmSJevfuLY8rzrDx8vLS2LFjdeTIEfn6+urmm2/WggULyuw9\\\nrcRmXP4vimJLTk5WQECAkpKS5O/vb3Y5AGAp6enpiouLU2RkpHx8fMwuB2WkZcuWeumll657tnBB\\\nCjsu+N3m5A4AAOAkMjMzde+99+rOO+80uxSXRVcvAABwCl5eXpowYYLZZbg0WvwAAAAsguAHAABg\\\nEQQ/AAAAiyD4AQAAWATBDwAAwCIIfgAAABZB8AMAALAIgh8AAIBFEPwAAHAy69evV9u2beXt7a36\\\n9euX6bVwr+XIkSOy2WxX3X766SeHvefQoUP10ksvOez1cW1cuQMAACcSFxenPn366IknntC8efO0\\\nZs0aDR8+XGFhYerVq5dD33v16tVq1qxZ3uPq1as75H1ycnL03XffKTo62iGvj4LR4gcAQDn58MMP\\\nFR4eLrvdnm95v379NGzYMEnSzJkzFRkZqbfeektNmjTRU089pQEDBmjq1KkOr6969eoKDQ3Nu3l6\\\neha47vr162Wz2bRy5Uq1adNGvr6+uv3225WYmKjly5erSZMm8vf310MPPaS0tLR82/7444/y9PRU\\\nhw4dlJmZqaeeekphYWHy8fFR7dq1NWXKFEd/VMsi+AEAXIJhGErLzDblZhhGkWq87777dPbsWa1b\\\nty5v2blz57RixQoNGjRIkrRp0yZ1794933a9evXSpk2bCnzdY8eOyc/Pr9Db5MmTr1vf3XffrRo1\\\naigqKkpLly4t0meaOHGipk+frh9//FHHjx/XwIED9c477+jzzz9XdHS0vv/+e7333nv5tlm6dKn6\\\n9u0rm82md999V0uXLtXChQu1b98+zZs3T3Xq1CnSe6P46OoFALiES1k5ajp+pSnvveeVXqrkdf2f\\\n1KpVq+rOO+/U559/rm7dukmSFi1apKCgIHXt2lWSFB8fr5CQkHzbhYSEKDk5WZcuXZKvr+9Vrxse\\\nHq7Y2NhC37tatWoFPufn56e33npLXbp0kZubm7766iv1799f33zzje6+++5CX/e1115Tly5dJEmP\\\nPvqoxo4dq0OHDqlu3bqSpAEDBmjdunV68cUX87ZZsmRJXgvmsWPH1KBBA0VFRclms6l27dqFvh9K\\\nh+AHAEA5GjRokEaMGKEPPvhA3t7emjdvnh544AG5uZW8E87Dw0P169cv8fZBQUF67rnn8h536NBB\\\nJ0+e1L/+9a/rBr+WLVvm/TskJESVKlXKC32Xl23evDnv8d69e3Xy5Mm84DtkyBD16NFDjRo10h13\\\n3KG77rpLPXv2LPFnQeEIfgAAl+Dr6a49rzj25IfC3ruo+vbtK8MwFB0drQ4dOui///1vvvF7oaGh\\\nSkhIyLdNQkKC/P39r9naJ+W2mjVt2rTQ9x03bpzGjRtX5Do7duyoVatWXXe9K8cB2my2q8YF2my2\\\nfGMaly5dqh49esjHx0eS1LZtW8XFxWn58uVavXq1Bg4cqO7du2vRokVFrhVFR/ADALgEm81WpO5W\\\ns/n4+Oiee+7RvHnzdPDgQTVq1Eht27bNe75Tp05atmxZvm1WrVqlTp06Ffiape3qvZbY2FiFhYUV\\\na5uiWLJkiR577LF8y/z9/XX//ffr/vvv14ABA3THHXfo3Llzxa4Z1+f8/4cAAOBiBg0apLvuuku7\\\nd+/Www8/nO+5J554QtOnT9fo0aM1bNgwrV27VgsXLix06pPSdvXOnTtXXl5eatOmjSTp66+/1ief\\\nfKKPP/64xK95LYmJidqyZUu+E0fefvtthYWFqU2bNnJzc9OXX36p0NBQBQYGlul7IxfBDwCAcnb7\\\n7berWrVq2rdvnx566KF8z0VGRio6OlrPPvuspk2bppo1a+rjjz92+Bx+r776qo4ePSoPDw81btxY\\\nX3zxhQYMGFCm7/Htt9/qxhtvVFBQUN6yKlWq6J///KcOHDggd3d3dejQQcuWLSvVmEcUzGYU9Rx0\\\nXCU5OVkBAQFKSkqSv7+/2eUAgKWkp6crLi5OkZGReePF4NzuvvtuRUVFafTo0Q57j8KOC363mccP\\\nAACUk6ioKD344INml2FpdPUCAIBy4ciWPhSNZVv8cnJy9PLLLysyMlK+vr6qV6+eXn311SLPvg4A\\\nAFDRWLbF74033tCMGTM0d+5cNWvWTFu2bNHQoUMVEBCgZ555xuzyAAAAypxlg9+PP/6ofv36qU+f\\\nPpKkOnXqaP78+flmFwcAOD96anAljofCWbart3PnzlqzZo32798vSfr1118VExOjO++8s8BtMjIy\\\nlJycnO8GADCHu3vu1TIyMzNNrgTOJC0tTZKuuoIIclm2xW/MmDFKTk5W48aN5e7urpycHP3jH//Q\\\noEGDCtxmypQpmjRpUjlWCQAoiIeHhypVqqTTp0/L09OTed8szjAMpaWlKTExUYGBgXl/GCA/y87j\\\nt2DBAr3wwgv617/+pWbNmik2NlajRo3S22+/rcGDB19zm4yMDGVkZOQ9Tk5OVkREhKXnAwIAM2Vm\\\nZiouLi7ftWBhbYGBgQoNDZXNZrvqOebxs3Dwi4iI0JgxYzRy5Mi8Za+99po+++wz/fbbb0V6DQ4g\\\nADCf3W6nuxeScrt3C2vp43fbwl29aWlpV3ULuLu781cjAFQwbm5uXLkDKCLLBr++ffvqH//4h2rV\\\nqqVmzZpp+/btevvttzVs2DCzSwMAAHAIy3b1pqSk6OWXX9bixYuVmJio8PBwPfjggxo/fry8vLyK\\\n9Bo0GQMAUHHwu23h4FcWOIAAAKg4+N228Dx+AAAAVkPwAwAAsAiCHwAAgEUQ/AAAACyC4AcAAGAR\\\nBD8AAACLIPgBAABYBMEPAADAIgh+AAAAFkHwA1Bk2dnSK69IPXvm3mdnO2YbAIBjeJhdAICKY/Jk\\\naeJEyTCk1atzl40fX7bbZGfnbhMTI0VFSePGSR58UwFAmeDrFECRxcTkBjgp9z4mpuy3KUm4BAAU\\\nDV29gEWVpAs2Kkqy2XL/bbPlPi7rbUoSLulOBoCiocUPsKiStKyNG5d7f2U37PUUd5uoqNx6DKPo\\\n4ZJWQgAoGoIfYFElaVnz8Ch+oCruNiUJlyX5LABgRQQ/wEUU96SIkrSslYeShEtn/SwA4GwIfoCL\\\nKG53Z0la1pxVST4LZw8DsCK+5gAXUdzuzpK0rDmrknwWxgUCsCLO6gVcREnOuLUyxgUCsCJa/AAX\\\n4Updt+WBcYEArIjgBzihkow/c6Wu2/JAUAZgRQQ/wAkx/szxCMoArIgxfoATYvyZ8+HqIABcAS1+\\\ngBNi/JnzoRUWgCsg+AFOiPFnzodWWACugOAHOCHGnzkfWmEBuAKCHwAUAa2wAFwBwQ9wMC4N5hpo\\\nhQXgCvj5ARyMkwIAAM6C6VwAB+OkAOtiChgAzoYWP8DBOCnAumjtBeBsCH6Ag3FSgHXR2gvA2RD8\\\nAAfjpADrorUXgLMh+AGAg9DaC8DZEPwAwEFo7QXgbDirFygmztQEAFRUtPgBxcSZmgCAiooWP6CY\\\nOFMTjkSLMgBHosUPKCbO1IQj0aIMwJEIfkAxcaYmHIkWZQCORPADiokzNeFItCgDcCRLj/E7ceKE\\\nHn74YVWvXl2+vr5q0aKFtmzZYnZZACxs3Ljcrt4ePXLvaVEGUJYs2+J3/vx5denSRV27dtXy5csV\\\nHBysAwcOqGrVqmaXBsDCaFEG4EiWDX5vvPGGIiIiNHv27LxlkZGRJlYEAADgWJbt6l26dKnat2+v\\\n++67TzVq1FCbNm300UcfmV0WAACAw1g2+B0+fFgzZsxQgwYNtHLlSj355JN65plnNHfu3AK3ycjI\\\nUHJycr4bKjbmTAMAWIllu3rtdrvat2+vyZMnS5LatGmjXbt2aebMmRo8ePA1t5kyZYomTZpUnmXC\\\nwZgzDQBgJZZt8QsLC1PTpk3zLWvSpImOHTtW4DZjx45VUlJS3u348eOOLhMOxpxpqOhotQZQHJZt\\\n8evSpYv27duXb9n+/ftVu3btArfx9vaWt7e3o0tDOWLONFR0tFoDKA7LBr9nn31WnTt31uTJkzVw\\\n4EBt3rxZH374oT788EOzS0M54iocqOhotQZQHJYNfh06dNDixYs1duxYvfLKK4qMjNQ777yjQYMG\\\nmV0ayhFzpqGio9UaQHHYDOPy34ooruTkZAUEBCgpKUn+/v5mlwPAgrKzc7t7r2y19rDsn/RA4fjd\\\ntnCLHwC4AlqtARSHZc/qBQAAsBqCHwAAgEUQ/AAAACyC4AeXwUS2AAAUjpM74DKYyBYAgMLR4geX\\\nwUS2AAAUjuAHlxEVlTuBrcREtkBhGBYBWBddvXAZXH4NKBqGRQDWRfCDy2AiW6BoGBYBWBddvQBg\\\nMQyLAKyLFj8AsBiGRQDWRfADAIthWARgXXT1AgAAWATBDwAAwCIIfgAAABZB8AMAALAIgh+cElcW\\\nAACg7HFWL5wSVxYAAKDs0eIHp8SVBQAAKHsEPzglriwAOBeGXwCuga5eOCWuLAA4F4ZfAK6B4Aen\\\nxJUFAOfC8AvANdDVCwC4LoZfAK6BFj8AwHUx/AJwDQQ/AMB1MfwCcA109QIAAFgEwQ8AAMAiCH4A\\\nAAAWQfADAACwCIIfAACARRD8UC643BMAAOZjOheUCy73BACA+WjxQ7ngck8AAJiP4IdyweWeAGth\\\neAfgnOjqRbngck+AtTC8A3BOBD+UCy73BFgLwzsA50RXLwCgzDG8A3BOtPgBAMocwzsA50TwAwCU\\\nOYZ3AM6Jrl4AAACLIPgBAABYBMHvD6+//rpsNptGjRpldikAAAAOQfCT9Msvv2jWrFlq2bKl2aUA\\\nAAA4jOWD38WLFzVo0CB99NFHqlq1qtnlAAAAOIzlg9/IkSPVp08fde/e/brrZmRkKDk5Od8NAACg\\\norD0dC4LFizQtm3b9MsvvxRp/SlTpmjSpEkOrgoAAMAxLNvid/z4cf31r3/VvHnz5OPjU6Rtxo4d\\\nq6SkpLzb8ePHHVylc+Li6wAAVEyWbfHbunWrEhMT1bZt27xlOTk52rBhg6ZPn66MjAy5u7vn28bb\\\n21ve3t7lXarT4eLrAABUTJYNft26ddPOnTvzLRs6dKgaN26sF1988arQh//h4usAAFRMlg1+VapU\\\nUfPmzfMtq1y5sqpXr37VcuQXFZXb0mcYXHwdQNnJzs7tUbjy+r4elv2VAhyD/6VQbFx8HYAjMIwE\\\ncDyC3xXWr19vdgkVAhdfB+AIDCMBHM+yZ/UCAJxLVFTu8BGJYSSAo9DiBwBwCgwjARzPlOC3Y8eO\\\nYm/TtGlTeTDKFwBcFsNIAMczJUm1bt1aNptNxuXBHNfh5uam/fv3q27dug6uDAAAwHWZ1oT2888/\\\nKzg4+LrrGYbB9CoAAABlwJTgd+utt6p+/foKDAws0vq33HKLfH19HVsUAACAi7MZRe1vxVWSk5MV\\\nEBCgpKQk+fv7m10OAAAoBL/bTOcCAABgGaafJmsYhhYtWqR169YpMTFRdrs93/Nff/21SZUBAAC4\\\nFtOD36hRozRr1ix17dpVISEhsl2evRMAAABlyvTg9+mnn+rrr79W7969zS4FAADApZk+xi8gIID5\\\n+UyUnS298orUs2fufXa22RUBAABHMT34TZw4UZMmTdKlS5fMLsWSJk+WJk6UVq3KvZ882eyKAACA\\\no5je1Ttw4EDNnz9fNWrUUJ06deTp6Znv+W3btplUmTXExEiXJ/QxjNzHAADANZke/AYPHqytW7fq\\\n4Ycf5uQOE0RFSatX54Y+my33MQAAcE2mB7/o6GitXLlSUSQOU4wbl3sfE5Mb+i4/BoCKIDs7d4jK\\\nld9hHqb/sgHOy/T/PSIiIiw7e7Yz8PCQxo83uwoAKJnL45QNI7f3QuI7DSiM6Sd3vPXWWxo9erSO\\\nHDlidikAgAqGccpA8Zje4vfwww8rLS1N9erVU6VKla46uePcuXMmVQYAcHaMUwaKx/Tg984775hd\\\nAgCggmKcMlA8NsO43EiO4kpOTlZAQICSkpIYpwgAgJPjd9ukMX7JycnFWj8lJcVBlQAAAFiHKcGv\\\natWqSkxMLPL6N9xwgw4fPuzAigAAAFyfKWP8DMPQxx9/LD8/vyKtn5WV5eCKAAAAXJ8pwa9WrVr6\\\n6KOPirx+aGjoVWf7AgAAoHhMCX7M2QcAAFD+TJ/AGQAAAOWD4AcAAGARBD8AAACLIPgBAABYBMHP\\\nxWRnS6+8IvXsmXufnW12RQAAwFmYFvy6deumr7/+usDnz5w5o7p165ZjRa5h8mRp4kRp1arc+8mT\\\nza4IAAA4C9OC37p16zRw4EBNmDDhms/n5OTo6NGj5VxVxRcTI12++rJh5D4GAACQTO7qnTFjht55\\\n5x395S9/UWpqqpmluIyoKMlmy/23zZb7GAAAQDI5+PXr108//fSTdu/erZtuuonr8ZaBceNyu3h7\\\n9Mi9HzfO7IoAwHkwDhpWZ8qVO67UpEkT/fLLL3rwwQfVoUMHffHFF+revbvZZVVYHh7S+PFmVwEA\\\nzunyOGjDkFavzl3GdyasxCnO6g0ICFB0dLRGjBih3r17a+rUqWaXBABwQYyDhtWZ1uJnuzwQ7YrH\\\nr7/+ulq3bq3hw4dr7dq1JlUGAHBVUVG5LX2GwThoWJNpwc+4/CfXnzzwwANq3Lix+vfvX74FAQBc\\\n3uVxzzExuaGPcdCwGtOC37p161StWrVrPte6dWtt3bpV0dHR5VwVAMCVMQ4aVmczCmp6w3UlJycr\\\nICBASUlJ8vf3N7scAABQCH63neTkDjNMmTJFHTp0UJUqVVSjRg31799f+/btM7ssAAAAh7Fs8Pvh\\\nhx80cuRI/fTTT1q1apWysrLUs2dPJpIGAAAui67eP5w+fVo1atTQDz/8oFtuuaVI29BkDABAxcHv\\\ntoVb/P4sKSlJkgo84QQAAKCiM/3KHc7Abrdr1KhR6tKli5o3b17gehkZGcrIyMh7nJycXB7lAQAA\\\nlAla/CSNHDlSu3bt0oIFCwpdb8qUKQoICMi7RURElFOFAAAApWf5MX5PPfWUlixZog0bNigyMrLQ\\\nda/V4hcREWHpsQIAAFQUjPGzcFevYRh6+umntXjxYq1fv/66oU+SvL295e3tXQ7VAQAAlD3LBr+R\\\nI0fq888/15IlS1SlShXFx8dLkgICAuTr62tydQAAAGXPsl29Npvtmstnz56tIUOGFOk1aDIGAKDi\\\n4Hfbwi1+FSHvZmdLkyfnv5i4h2X/iwEAgNIiRjixyZOliRMlw5BWr85dxsXFAQBASTGdixOLickN\\\nfVLufUyMufUAAICKjeDnxKKipMtDEW223McAAAAlRVevExs3Lvf+yjF+AIDyxXhruBIOXSfm4cGY\\\nPgAwG+Ot4Uro6gUAoBCMt4YrIfgBAFAIxlvDldDVCwBAIRhvDVdC8AMAoBCMt4YroasXAADAIgh+\\\nAAAAFkFXL8pFRnaOTpy/pLTMHGXbDeXY7crOMZRjN3IfG4Zycv74t91Qtt2uHHvuaXTV/bwV6u+j\\\nUH8f+ft6yHZ5lDUAACgWgh/KhGEYSrqUpaNn03TsXO7t6NnU3H+fTdOp5PS86RBKw8fTTSH+Pgr5\\\nIwiGBvj88Tg3HF5+zsuDxmwAAP6M4IdiS0xJ16ZDZ7X3VIqOnUv9I+SlKSU9u9DtKnm5K8DXU+5u\\\ntrybh5tN7m5u8nCzyS3v8f/uDUM6czFD8cnpupCWpfQsu46ezX2/gni5u6nZDf5qV6uq2tauqna1\\\nqyrE36esdwMAABUOwQ/XdS41Uz8dPqtNh85q0+GzOph4scB1Q/y9VataJdWqVlm1qlVS7eqVFPHH\\\nffXKXqXqpk3PylFCcrrik9KVkJKhhKR0xSfn3hKS0pWQkq6EpAxl5ti1/dgFbT92QYqJkyTdEOir\\\ntrWrqm2tQLWrXVVNwvzl6U6rIADAWmyGURYdcNaUnJysgIAAJSUlyd/f3+xyykzSpSxtjjunHw+d\\\n0aZDZ/VbfEq+5202qUmov9rVrqo6QZVV+49gV7NqJfl6uZtUdS7DMHT0bJq2HTufezt6Qb/FJ8v+\\\np6Pcx9NNLWsGqm2t3BbBGyOrKcDX05yiAQDlwlV/t4uD4FcKrnIAZWTn5LbmHTqrHw+d1e6TSVcF\\\npUYhVdSpXnXdVLe6bqpbTYGVvMwptgQuZmRrx/EL2nr0jzB47IKSLmXlW8fL3U23NAxW31Zh6t4k\\\nRJW9aQwHAFfjKr/bpUHwK4WKfAAZhqEdvydp0dbftfTXk1cFobpBldWpXvW8sBfk521SpWXPbjd0\\\n+Eyqtv0RBDcfOafDp1Pznvf1dFe3JjXUt1W4bm0YLB9Pc1sxAQBloyL/bpcVgl8pVMQDKDE5XYu3\\\nn9Cirb/rwBVj9UL8vXVbwxp5QS80wFonQ+xPSNG3v57Ut7+e1JErThyp4u2hns1C1bdVmLrUD2Jc\\\nIABUYBXxd7usEfxKoaIcQOlZOVqzN1GLth7XD/tP53Xjenu46Y7mobq3bU11qR8kdzfmxzMMQ7tO\\\nJGvpryf03Y5TOpWUnvdc1UqeurNFmO5uFa4OdaqxvwCggqkov9uORPArBWc+gArrym1Xu6oGtKup\\\nPi3D5O/DCQ0FsdsNbT12Xt/+elLRO07pbGpm3nMh/t566Mba+n+daqtq5Yoz3hEArMyZf7fLC8Gv\\\nFIpzAGVnS5MnSzExUlSUNG5c7oW/y1pKepYWbD6uhVuO5+vKDQvw0T1tb9C9bWuqbrBf2b+xi8vO\\\nsWvT4bP69teTWr4rPm/OQl9Pdz1wY4SG31xXNwT6mlwlAKAwBD+CX6kU5wB65RVp4kTJMHKnQ5k4\\\nURo/vgxrSc/S3I1H9HFMXF7r3uWu3AHtaqpzPbpyy0pGdo5W7IrXzB8Oa++pZEmSu5tNd7cK1+O3\\\n1lXjUGt+mQCAsyP4MYFzuYmJUd4lywwj93FZSLqUpdkb4/RJTJyS/2iFqhtcWcOj6uquVnTlOoK3\\\nh7v6tb5Bd7cK14YDZzRz/SFtOnxWi7ef0OLtJ9S1UbCeuLWeboysxnWFAQBOheBXTqKipNWr/9fi\\\nFxVVute7kJapT2LiNHvjEaVk5Aa++jX89PTt9XVXy3Ba98qBzWbTrQ2DdWvDYP16/IJmbTik5bvi\\\ntW7faa3bd1ptagXqiVvrqUeTELnx3wMA4ATo6i0FM8b4nU/N1McxhzX3x6O6+EfgaxRSRU93q6/e\\\nzcMIGCaLO5OqDzcc1lfbfldmtl2SVC+4sh6/pZ76tQmXtwdzAgJWUF7julE8dPUS/EqlPA+gsxcz\\\n9NF/4/TppiNKzcyRJDUOraK/dmugXs1CCXxOJjElXbM3HtFnPx3NOxEk1N9HY+5srH6tw+kCBlyc\\\no8d1o2QIfgS/UimPA+jMxQx9tOGwPv3pqNL+CHzNwv31TLcGdCFWACnpWZq/+Zj+HROnhOQMSVL7\\\n2lU18e5man5DgMnVAXCUnj2lVav+97hHD+n7782rB7kIfozxc1o5dkPzfj6qf63cl9di1OKGAD3T\\\nrYG6N6lBi1EFUcXHU4/dUk//r1Md/TsmTtPXHtSWo+fVd3qMHugQoed7NlJ1F7ocHoBcZT2uGygr\\\ntPiVgqP+ctj5e5L+/s1O7fg9SZLU/AZ/Pdejobo2IvBVdKeSLun15b9pSexJSVIVHw89272hHulU\\\nm8vBAS6EMX7OiRY/gl+plPUBlJyepbdW7tOnPx2V3cgNBaN7NdJDHWtzlq6L+eXIOU1culu7T+bO\\\nA9ighp8m9G2mqAZBJlcGAK6L4EfwK5WyOoAMw9DSX0/qtei9Op2SOw6sX+tw/b1PE9Wo4lNW5cLJ\\\n5NgNLdxyXP9auU/n/rgcXM+mIXqpT1PVql7J5OoAwPUQ/Ah+pVIWB1DcmVS9/M0uxRw8I0mqG1RZ\\\nr/Zvri71afmxiqS0LE1dvV+f/nRUOXZDXh5ueuzmuvq/rvVUyYu+IQAoKwQ/gl+plOYASs/K0Qfr\\\nD2nm+kPKzLHLy8NNT3Wtr8dvrctcbxa1PyFFk77drY0Hz0rKvb7yy3c1Ve8WYSZXBgCugeBH8CuV\\\nkh5AG/af1vglu3TkbJok6daGwXqlXzPVrl7ZUaWigjAMQyt3J+i16D36/fwlSdI9bW7QxH7NuPwe\\\nAJQSwY/gVyrFPYASU9I16ds9it5xSpIU4u+t8Xc1U+8WoZyti3zSs3I0fe1BfbD+oOyGdEOgr6be\\\n31o3RlYzuzQAqLAIfgS/UinOAfTD/tP628JYnbmYKTebNLhzHT3Xo6Gq0IqDQmw9ek6jvojV8XOX\\\n5GaTnrytnv7araG8PJj6BQCKi+BH8CuVohxAWTl2vfn9Ps364bCk3MusvXlfK67agCJLSc/SK9/u\\\n0Zdbf5eUO5H31Ptbq34NP5MrA4CKheBH8CuV6x1Ax8+l6en52xV7/IIk6ZGbauvvfZrIx5OTN1B8\\\ny3ae0rjFO3UhLUs+nm76e5+merhjLYYJAEAREfwIfqVS2AEUveOUxny1QykZ2fL38dA/B7TUHc05\\\nOxOlE5+Urue//DVv+p/bG9fQG/e2VHAVLvsGANdD8CP4lcq1DqBLmTl65bs9mr/5mCSpba1Avftg\\\nG9WsyoS8KBt2u6E5Px7R6yt+U2a2XdUre+mNe1uqe9MQs0sDAKdG8JMsP0L8/fffV506deTj46OO\\\nHTtq8+bNJX6t/Qkp6vd+jOZvPiabTRrZtZ6+eLwToQ9lys3NpmFRkVr6VBc1Dq2is6mZGv6fLRq3\\\neKfSMrPNLg8A4MQsHfy++OILPffcc5owYYK2bdumVq1aqVevXkpMTCzW6xiGofmbj+nu6THan3BR\\\nwVW89emwjnqhV2N5ult6F8OBGof665uRXTTi5khJ0uc/H9Nd78Zo98kkkysDADgrS3f1duzYUR06\\\ndND06dMlSXa7XREREXr66ac1ZsyY625/ucl4+Ec/aNXBFEnSLQ2D9fbAVgryY8wVys/Gg2f0t4W/\\\nKj45Xb6e7nprYCuu+AEAf0JXr4Vb/DIzM7V161Z17949b5mbm5u6d++uTZs2Feu1Vu5OkIebTWPv\\\nbKw5QzoQ+lDuutQP0opRN+uWhsG6lJWj/5u3TVNX7Zfdbtm/6wAA12DZ4HfmzBnl5OQoJCT/gPiQ\\\nkBDFx8dfc5uMjAwlJyfnu0lSdpKP7vDspMdvrSc3N6bWgDkCK3npk8HtNTwqt+t32poDGvn5Nsb9\\\nAQDyWDb4lcSUKVMUEBCQd4uIiJAknfqss/ZvqmpydYDk4e6ml+5qqn8OaClPd5uW74rXgBmbdOLC\\\nJbNLAwA4AcsGv6CgILm7uyshISHf8oSEBIWGhl5zm7FjxyopKSnvdvz48dwnsjwVFeXoioGiG9g+\\\nQvNH3KQgPy/tOZWsu9+L0ZYj58wuCwBgMssGPy8vL7Vr105r1qzJW2a327VmzRp16tTpmtt4e3vL\\\n398/302Sxo6Vxo0rl7KBImtfp5qWPBWlpmH+OpuaqQc/+kkLfzludlkAABNZNvhJ0nPPPaePPvpI\\\nc+fO1d69e/Xkk08qNTVVQ4cOLdbrjBkjeXg4qEigFG4I9NWiJzvpzuahysoxNPqrHXr1uz3KzrGb\\\nXRoAwASWjiv333+/Tp8+rfHjxys+Pl6tW7fWihUrrjrhA6jIKnl56P2H2urdtQf0zuoD+ndMnA4k\\\nXtR7D7ZRgK+n2eUBAMqRpefxKy3mA0JFs2znKf1t4a+6lJWjukGV9fHg9qob7Gd2WQBQLvjdtnhX\\\nL2A1vVuEadGTnRQe4KPDZ1LV7/2N2rD/tNllAQDKCcEPsJhm4QFa8lSU2tWuqpT0bA2ZvVlfbuGk\\\nDwCwAoIfYEHBVbz1+YiOurdtTdkN6YVFOzRnY5zZZQEAHIzgB1iUt4e73ryvpR7940ofE7/do+lr\\\nD4hhvwDgugh+gIXZbDa91KeJRnVvIEl68/v9en35b4Q/AHBRBD/A4mw2m0Z1b6iX+jSRJM3acFgv\\\nfbNLdjvhDwBcDcEPgCRp+M11NeWeFrLZpHk/H9NzC2OVxUTPAOBSCH4A8jx4Yy1Ne6CNPNxs+ib2\\\npP5v3jalZ+WYXRYAoIwQ/ADkc3ercH34/9rJy8NNq/Yk6NG5vyg1I9vssgAAZYDgB+AqtzcO0Zyh\\\nHVTZy10bD57VI//+WUmXsswuCwBQSgQ/ANfUuV6QPhveUQG+ntp27IIe+PAnnbmYYXZZAIBSIPgB\\\nKFCbWlW14LGbFOTnrb2nkjVw1iadvHDJ7LIAACVE8ANQqCZh/lr4+E251/c9nar7Zm7SkTOpZpcF\\\nACgBgh+A66ob7Kcvn+ysyKDKOnHhkgbO2qRjZ9PMLgsAUEwEPwBFckOgrxY+3kkNQ/yUmJKhQf/+\\\nSfFJ6WaXBQAoBoIfgCILruKtzx7tqNrVK+n4uUt65N8/61xqptllAQCKiOAHoFhq+Pvos0c7KtTf\\\nRwcSL2rwJ5uVks5ULwBQERD8ABRbRLVK+mz4japW2Us7TyTp0blbdCmTK3wAgLMj+AEokfo1qug/\\\nw25UFW8PbY47pyfnbVVmNtf2BQBnRvADUGLNbwjQJ0M7yMfTTev3ndazC2OVYzfMLgsAUACCH4BS\\\n6VCnmmY90l6e7jZF7zilvy/eKcMg/AGAMyL4ASi1WxsGa9oDbeRmkxb8clz/iN5L+AMAJ0TwA1Am\\\nercI0+v3tpQkfRwTp/fWHjS5IgDAnxH8AJSZge0jNP6uppKkt1ft1ycxcSZXBAC4EsEPQJkaFhWp\\\nZ7s3lCS98t0eLdxy3OSKAACXEfwAlLlnutXX8KhISdKYr3Zo+c5TJlcEAJAIfgAcwGaz6e99muj+\\\n9hGyG9IzC7brvwdOm10WAFgewQ+AQ9hsNk2+p4X6tAxTVo6h//tsm/YnpJhdFgBYGsEPgMO4u9k0\\\ndWBr3RhZTSkZ2Ro25xeduZhhdlkAYFkEPwAO5eXhplkPt1Pt6pX0+/lLevzTrUrP4rq+AGAGgh8A\\\nh6ta2Uv/HtxB/j4e2nr0vMZ8tYMJngHABAQ/AOWifg0/zXi4ndzdbPom9qSmM8EzAJQ7gh+ActOl\\\nfpBe7ddckvTWqv36bsdJkysCAGsh+AEoVw91rJU3x9/fFv6q7cfOm1wRAFgHwQ9AuRvbu4m6Na6h\\\njGy7Rvxnq05cuGR2SQBgCQQ/AOXO3c2maQ+2UePQKjpzMUOPzvlFFzOyzS4LAFwewQ+AKfy8PfTv\\\nIR0U5Oet3+JT9Mz87cqxc6YvADgSwQ+AaW4I9NXHg9vL28NNa39L1ORle80uCQBcGsEPgKlaRwTq\\\nrYGtJEn/jonTvJ+PmlwRALgugh8A093VMlx/69FQkjR+yW7FHDhjckUA4JoIfgCcwlO319df2tyg\\\nHLuhJ+dt1cHEi2aXBAAuh+AHwCnYbDa9fm8Lta9dVSnp2Ro25xedT800uywAcCmWDH5HjhzRo48+\\\nqsjISPn6+qpevXqaMGGCMjP5kQHM5O3hrlmPtFNENV8dO5emZxfGys6ZvgBQZiwZ/H777TfZ7XbN\\\nmjVLu3fv1tSpUzVz5kyNGzfO7NIAy6vu561ZD+ee6bt+32l9sJ5r+gJAWbEZhsGf05L+9a9/acaM\\\nGTp8+HCRt0lOTlZAQICSkpLk7+/vwOoA61m45bhGL9ohN5v06aMd1aV+kNklAajg+N22aIvftSQl\\\nJalatWqFrpORkaHk5OR8NwCOMbB9hO5vHyG7IT0zf7vik9LNLgkAKjyCn6SDBw/qvffe0+OPP17o\\\nelOmTFFAQEDeLSIiopwqBKxpUr9mahrmr7OpmXrq823KyrGbXRIAVGguFfzGjBkjm81W6O23337L\\\nt82JEyd0xx136L777tOIESMKff2xY8cqKSkp73b8+HFHfhzA8nw83TXj4baq4uOhLUfP643lv11/\\\nIwBAgVxqjN/p06d19uzZQtepW7euvLy8JEknT57Ubbfdpptuuklz5syRm1vxcjBjBYDysXJ3vB7/\\\ndKskaebDbXVH8zCTKwJQEfG7LXmYXUBZCg4OVnBwcJHWPXHihLp27ap27dpp9uzZxQ59AMpPr2ah\\\neuyWuvpww2G98OUONQr1V2RQZbPLAoAKx5Jp58SJE7rttttUq1Ytvfnmmzp9+rTi4+MVHx9vdmkA\\\nCvBCr0a6sU41pWRk68nPtupSZo7ZJQFAhWPJ4Ldq1SodPHhQa9asUc2aNRUWFpZ3A+CcPN3d9N5D\\\nbRTk56Xf4lP08pJdcqGRKgBQLiwZ/IYMGSLDMK55A+C8Qvx99O6DbeRmkxZt/V0Lt3CCFQAUhyWD\\\nH4CKq3O9IP2tZyNJ0stLdmvXiSSTKwKAioPgB6DCefLWeurWuIYys+36v3nblHQpy+ySAKBCIPgB\\\nqHDc3Gx6a2Ar1azqq2Pn0vT8l78yVAMAioDgB6BCCqzkpQ8GtZWXu5tW7UnQhxuKfp1tALAqgh+A\\\nCqtlzUCN79tUkvTPlfv08+HCJ3AHAKsj+AGo0AZ1rKW/tLlBOXZDT8/frnOpmWaXBABOi+AHoEKz\\\n2Wz6x1+aq34NPyWmZGjc1zsZ7wcABSD4AajwKnl56J37W8vT3aYVu+P15dbfzS4JAJwSwQ+AS2h+\\\nQ4Ce65E7v9+kpbt19GyqyRUBgPMh+AFwGY/dUlc3RlZTamaOnv0iVtk5drNLAgCnQvAD4DLc3Wx6\\\ne2ArVfH20LZjF/TB+kNmlwQAToXgB8Cl1KxaSa/2by5JmrbmgGKPXzC3IABwIgQ/AC6nX+tw9W0V\\\nrhy7oWe/iFVaZrbZJQGAUyD4AXA5NptNr/VrrrAAH8WdSdVr0XvNLgkAnALBD4BLCqjkqbfuayVJ\\\n+vznY1q9J8HkigDAfAQ/AC6rc/0gjbg5UpL04lc7dDolw+SKAMBcBD8ALu35Xo3UOLSKzqZm6sWv\\\ndnBVDwCWRvAD4NK8Pdz1zgOt5eXhprW/JWrez8fMLgkATEPwA+DyGof668U7GkuSXoveo0OnL5pc\\\nEQCYg+AHwBKGdq6jqPpBSs+ya9SCWGVxVQ8AFkTwA2AJbm42vXlfKwX4emrniSRNW33A7JIAoNwR\\\n/ABYRmiAj6bc00KS9MH6g9py5JzJFQFA+SL4AbCU3i3CdG/bmrIb0rMLY5WSnmV2SQBQbgh+ACxn\\\n4t1NVbOqr46fu6RXv9tjdjkAUG4IfgAsp4qPp6be31o2m7Rwy++KOXDG7JIAoFwQ/ABYUoc61fT/\\\nbqotSRrz9Q6lZWabXBEAOB7BD4BlvXBHY90Q6Kvfz1/SW9/vN7scAHA4gh8Ay/Lz9tA//tJckvTJ\\\nxjhtO3be5IoAwLEIfgAs7bZGNXRP2xtkGNKLi3YoIzvH7JIAwGEIfgAs7+U+TRXk56UDiRf1wbpD\\\nZpcDAA5D8ANgeVUre2nS3bldvh+sP6jf4pNNrggAHIPgBwCSercIVY+mIcrKMfTioh3KsRtmlwQA\\\nZY7gBwCSbDabXuvfXFV8PPTr70mavTHO7JIAoMwR/ADgDyH+Pvp77yaSpDe/36ejZ1NNrggAyhbB\\\nDwCucH+HCHWqW13pWXaN+WqnDIMuXwCug+AHAFew2Wx6/d4W8vF006bDZ/XFL8fNLgkAygzBDwD+\\\npHb1ynq+ZyNJ0j+i9yo+Kd3kigCgbBD8AOAahnaJVKuaAUrJyNZL3+yiyxeASyD4AcA1uLvZ9MaA\\\nlvJws2n13gRF7zxldkkAUGoEPwAoQONQf/1f1/qSpIlLd+t8aqbJFQFA6RD8AKAQI7vWU4Mafjpz\\\nMVOvRu8xuxwAKBXLB7+MjAy1bt1aNptNsbGxZpcDwMl4e7jrjQEtZbNJX287ofX7Es0uCQBKzPLB\\\nb/To0QoPDze7DABOrG2tqhraOVKS9PfFu3QxI9vkigCgZCwd/JYvX67vv/9eb775ptmlAHByz/dq\\\nqJpVfXXiwiVNW73f7HIAoEQsG/wSEhI0YsQIffrpp6pUqZLZ5QBwcpW8PPRq/+aSpE82HtG++BST\\\nKwKA4vMwuwAzGIahIUOG6IknnlD79u115MiRIm2XkZGhjIyMvMdJSUmSpOTkZEeUCcDJtAvz0W2R\\\nlbX2t9Mau+BnzR7aQTabzeyyABTR5d9rK8/L6VLBb8yYMXrjjTcKXWfv3r36/vvvlZKSorFjxxbr\\\n9adMmaJJkyZdtTwiIqJYrwOg4jsuafGzZlcBoCTOnj2rgIAAs8swhc1wodh7+vRpnT17ttB16tat\\\nq4EDB+rbb7/N95d6Tk6O3N3dNWjQIM2dO/ea2/65xe/ChQuqXbu2jh07ZtkDqCwkJycrIiJCx48f\\\nl7+/v9nlVGjsy7LBfiwb7Meyw74sG0lJSapVq5bOnz+vwMBAs8sxhUu1+AUHBys4OPi667377rt6\\\n7bXX8h6fPHlSvXr10hdffKGOHTsWuJ23t7e8vb2vWh4QEMD/iGXA39+f/VhG2Jdlg/1YNtiPZYd9\\\nWTbc3Cx7ioNrBb+iqlWrVr7Hfn5+kqR69eqpZs2aZpQEAADgcNaNvAAAABZjyRa/P6tTp06JzvDx\\\n9vbWhAkTrtn9i6JjP5Yd9mXZYD+WDfZj2WFflg32o4ud3AEAAICC0dULAABgEQQ/AAAAiyD4AQAA\\\nWATB7zref/991alTRz4+PurYsaM2b95c6PpffvmlGjduLB8fH7Vo0ULLli0rp0qdW3H245w5c2Sz\\\n2fLdfHx8yrFa57Rhwwb17dtX4eHhstls+uabb667zfr169W2bVt5e3urfv36mjNnjsPrrAiKuy/X\\\nr19/1TFps9kUHx9fPgU7oSlTpqhDhw6qUqWKatSoof79+2vfvn3X3Y7vyKuVZF/yPXm1GTNmqGXL\\\nlnlzHXbq1EnLly8vdBsrHo8Ev0J88cUXeu655zRhwgRt27ZNrVq1Uq9evZSYmHjN9X/88Uc9+OCD\\\nevTRR7V9+3b1799f/fv3165du8q5cudS3P0o5U5SeurUqbzb0aNHy7Fi55SamqpWrVrp/fffL9L6\\\ncXFx6tOnj7p27arY2FiNGjVKw4cP18qVKx1cqfMr7r68bN++ffmOyxo1ajioQuf3ww8/aOTIkfrp\\\np5+0atUqZWVlqWfPnkpNTS1wG74jr60k+1Lie/LPatasqddff11bt27Vli1bdPvtt6tfv37avXv3\\\nNde37PFooEA33nijMXLkyLzHOTk5Rnh4uDFlypRrrj9w4ECjT58++ZZ17NjRePzxxx1ap7Mr7n6c\\\nPXu2ERAQUE7VVUySjMWLFxe6zujRo41mzZrlW3b//fcbvXr1cmBlFU9R9uW6desMScb58+fLpaaK\\\nKDEx0ZBk/PDDDwWuw3dk0RRlX/I9WTRVq1Y1Pv7442s+Z9XjkRa/AmRmZmrr1q3q3r173jI3Nzd1\\\n795dmzZtuuY2mzZtyre+JPXq1avA9a2gJPtRki5evKjatWsrIiKi0L/YUDCOx7LXunVrhYWFqUeP\\\nHtq4caPZ5TiVpKQkSVK1atUKXIdjsmiKsi8lvicLk5OTowULFig1NVWdOnW65jpWPR4JfgU4c+aM\\\ncnJyFBISkm95SEhIgeN64uPji7W+FZRkPzZq1EiffPKJlixZos8++0x2u12dO3fW77//Xh4lu4yC\\\njsfk5GRdunTJpKoqprCwMM2cOVNfffWVvvrqK0VEROi2227Ttm3bzC7NKdjtdo0aNUpdunRR8+bN\\\nC1yP78jrK+q+5Hvy2nbu3Ck/Pz95e3vriSee0OLFi9W0adNrrmvV45Erd8DpdOrUKd9faJ07d1aT\\\nJk00a9YsvfrqqyZWBqtq1KiRGjVqlPe4c+fOOnTokKZOnapPP/3UxMqcw8iRI7Vr1y7FxMSYXUqF\\\nV9R9yffktTVq1EixsbFKSkrSokWLNHjwYP3www8Fhj8rosWvAEFBQXJ3d1dCQkK+5QkJCQoNDb3m\\\nNqGhocVa3wpKsh//zNPTU23atNHBgwcdUaLLKuh49Pf3l6+vr0lVuY4bb7yRY1LSU089pe+++07r\\\n1q1TzZo1C12X78jCFWdf/hnfk7m8vLxUv359tWvXTlOmTFGrVq00bdq0a65r1eOR4FcALy8vtWvX\\\nTmvWrMlbZrfbtWbNmgLHC3Tq1Cnf+pK0atWqAte3gpLsxz/LycnRzp07FRYW5qgyXRLHo2PFxsZa\\\n+pg0DENPPfWUFi9erLVr1yoyMvK623BMXltJ9uWf8T15bXa7XRkZGdd8zrLHo9lnlzizBQsWGN7e\\\n3sacOXOMPXv2GI899pgRGBhoxMfHG4ZhGI888ogxZsyYvPU3btxoeHh4GG+++aaxd+9eY8KECYan\\\np6exc+dOsz6CUyjufpw0aZKxcuVK49ChQ8bWrVuNBx54wPDx8TF2795t1kdwCikpKcb27duN7du3\\\nG5KMt99+29i+fbtx9OhRwzAMY8yYMcYjjzySt/7hw4eNSpUqGS+88IKxd+9e4/333zfc3d2NFStW\\\nmPURnEZx9+XUqVONb775xjhw4ICxc+dO469//avh5uZmrF692qyPYLonn3zSCAgIMNavX2+cOnUq\\\n75aWlpa3Dt+RRVOSfcn35NXGjBlj/PDDD0ZcXJyxY8cOY8yYMYbNZjO+//57wzA4Hi8j+F3He++9\\\nZ9SqVcvw8vIybrzxRuOnn37Ke+7WW281Bg8enG/9hQsXGg0bNjS8vLyMZs2aGdHR0eVcsXMqzn4c\\\nNWpU3rohISFG7969jW3btplQtXO5PKXIn2+X993gwYONW2+99aptWrdubXh5eRl169Y1Zs+eXe51\\\nO6Pi7ss33njDqFevnuHj42NUq1bNuO2224y1a9eaU7yTuNb+k5TvGOM7smhKsi/5nrzasGHDjNq1\\\naxteXl5GcHCw0a1bt7zQZxgcj5fZDMMwyq99EQAAAGZhjB8AAIBFEPwAAAAsguAHAABgEQQ/AAAA\\\niyD4AQAAWATBDwAAwCIIfgAAABZB8AMAALAIgh8AlzFkyBD179+/3N93zpw5stlsstlsGjVqVJG2\\\nGTJkSN4233zzjUPrA4DLPMwuAACKwmazFfr8hAkTNG3aNJl1MSJ/f3/t27dPlStXLtL606ZN0+uv\\\nv66wsDAHVwYA/0PwA1AhnDp1Ku/fX3zxhcaPH699+/blLfPz85Ofn58ZpUnKDaahoaFFXj8gIEAB\\\nAQEOrAgArkZXL4AKITQ0NO8WEBCQF7Qu3/z8/K7q6r3tttv09NNPa9SoUapatapCQkL00UcfKTU1\\\nVUOHDlWVKlVUv359LV++PN977dq1S3feeaf8/PwUEhKiRx55RGfOnCl2zR988IEaNGggHx8fhYSE\\\naMCAAaXdDQBQKgQ/AC5t7ty5CgoK0ubNm/X000/rySef1H333afOnTtr27Zt6tmzpx555BGlpaVJ\\\nki5cuKDbb79dbdq00ZYtW7RixQolJCRo4MCBxXrfLVu26JlnntErr7yiffv2acWKFbrlllsc8REB\\\noMjo6gXg0lq1aqWXXnpJkjR27Fi9/vrrCgoK0ogRIyRJ48eP14wZM7Rjxw7ddNNNmj59utq0aaPJ\\\nkyfnvcYnn3yiiIgI7d+/Xw0bNizS+x47dkyVK1fWXXfdpSpVqqh27dpq06ZN2X9AACgGWvwAuLSW\\\nLVvm/dvd3V3Vq1dXixYt8paFhIRIkhITEyVJv/76q9atW5c3ZtDPz0+NGzeWJB06dKjI79ujRw/V\\\nrl1bdevW1SOPPKJ58+bltSoCgFkIfgBcmqenZ77HNpst37LLZwvb7XZJ0sWLF9W3b1/Fxsbmux04\\\ncKBYXbVVqlTRtm3bNH/+fIWFhWn8+PFq1aqVLly4UPoPBQAlRFcvAFyhbdu2+uqrr1SnTh15eJTu\\\nK9LDw0Pdu3dX9+7dNWHCBAUGBmrt2rW65557yqhaACgeWvwA4AojR47UuXPn9OCDD+qXX37RoUOH\\\ntHLlSg0dOlQ5OTlFfp3vvvtO7777rmJjY3X06FH95z//kd1uV6NGjRxYPQAUjuAHAFcIDw/Xxo0b\\\nlZOTo549e6pFixYaNWqUAgMD5eZW9K/MwMBAff3117r99tvVpEkTzZw5U/Pnz1ezZs0cWD0AFM5m\\\nmDXNPQC4iDlz5mjUqFElGr9ns9m0ePFiUy41B8B6aPEDgDKQlJQkPz8/vfjii0Va/4knnjD1SiMA\\\nrIkWPwAopZSUFCUkJEjK7eINCgq67jaJiYlKTk6WJIWFhRX5Gr8AUBoEPwAAAIugqxcAAMAiCH4A\\\nAAAWQfADAACwCIIfAACARRD8AAAALILgBwAAYBEEPwAAAIsg+AEAAFjE/wetXmfJ6IVrhQAAAABJ\\\nRU5ErkJggg==\\\n\"\n  frames[33] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAn4AAAHcCAYAAABI7KcYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\\\nsAiCHwAAgEUQ/AAAACzi/wMEkaBbhfvaiwAAAABJRU5ErkJggg==\\\n\"\n  frames[34] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAn4AAAHcCAYAAABI7KcYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\\\nAwAAsIj/D5BSzOj49s2lAAAAAElFTkSuQmCC\\\n\"\n  frames[35] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAn4AAAHcCAYAAABI7KcYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\\\nTkSuQmCC\\\n\"\n  frames[36] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAn4AAAHcCAYAAABI7KcYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\\\nACyC4AcAAGARBD8AAACLIPgBAABYxP8HQOoahyBgnhcAAAAASUVORK5CYII=\\\n\"\n  frames[37] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAn4AAAHcCAYAAABI7KcYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\\\nACyC4AcAAGARBD8AAACLIPgBAABYxP8HQOoahyBgnhcAAAAASUVORK5CYII=\\\n\"\n  frames[38] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAn4AAAHcCAYAAABI7KcYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\\\nACyC4AcAAGARBD8AAACLIPgBAABYxP8HQOoahyBgnhcAAAAASUVORK5CYII=\\\n\"\n  frames[39] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAn4AAAHcCAYAAABI7KcYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\\\nACyC4AcAAGARBD8AAACLIPgBAABYxP8HQOoahyBgnhcAAAAASUVORK5CYII=\\\n\"\n\n\n    /* set a timeout to make sure all the above elements are created before\n       the object is initialized. */\n    setTimeout(function() {\n        anim123703f31d024d2c91309d4c3beae81e = new Animation(frames, img_id, slider_id, 29.0,\n                                 loop_select_id);\n    }, 0);\n  })()\n</script>",
            "fig, ax = plt.subplots(figsize=(8, 4))\nlines = ax.plot(pca_model.factors.iloc[:, :3], lw=4, alpha=0.6)\nax.set_xticklabels(dta.columns.values[::10])\nax.set_xlim(0, 51)\nax.set_xlabel(\"Year\", size=17)\nfig.subplots_adjust(0.1, 0.1, 0.85, 0.9)\nlegend = fig.legend(lines, [\"PC 1\", \"PC 2\", \"PC 3\"], loc=\"center right\")\nlegend.draw_frame(False)",
            "fig, ax = plt.subplots(ncols=2, figsize=(12, 8))\nax[0].plot(x, y)\nax[0].set_xlim(left=-1, right=1)\nax[0].plot(x + np.pi * 0.5, y)\nax[0].set_title(\"set_xlim(left=-1, right=1)\\n\")\nax[1].plot(x, y)\nax[1].set_xlim(left=-1, right=1)\nax[1].plot(x + np.pi * 0.5, y)\nax[1].autoscale()\nax[1].set_title(\"set_xlim(left=-1, right=1)\\nautoscale()\")\n\n\n<img alt=\"set_xlim(left=-1, right=1) , set_xlim(left=-1, right=1) autoscale()\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_autoscale_007.png\" srcset=\"../../_images/sphx_glr_autoscale_007.png, ../../_images/sphx_glr_autoscale_007_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->State space modeling: Local Linear Trends"
            ],
            [
                "matplotlib->Tutorials->Introductory->Quick start guide->Axis scales and ticks->Plotting dates and strings"
            ],
            [
                "matplotlib->Tutorials->Introductory->Animations using Matplotlib->Animation Classes->FuncAnimation"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Autoscaling->Controlling autoscale"
            ]
        ]
    },
    "603991": {
        "jupyter_code_cell": "Salary_Data[Salary_Data['playerID']=='martija02']\nSalary_Data[\"salary\"].iloc[6179] = (387500 + 350000) / 2\nSalary_Data[\"salary\"].iloc[12007] = 170000",
        "matched_tutorial_code_inds": [
            3657,
            5257,
            3730,
            3703,
            5258
        ],
        "matched_tutorial_codes": [
            "# With indecies\ntemp = weather['tmpf']\n\nc = (temp - 32) * 5 / 9\nc.to_frame()",
            "factors = pdr.get_data_famafrench(\"F-F_Research_Data_Factors\", start=\"1-1-1926\")[0]\nfactors.head()",
            "%%time\nr = gcd_vec(pairs['LATITUDE_1'], pairs['LONGITUDE_1'],\n            pairs['LATITUDE_2'], pairs['LONGITUDE_2'])",
            "df = pd.read_csv(\"data/347136217_T_ONTIME.csv\")\ndelays = df['DEP_DELAY']",
            "industries = pdr.get_data_famafrench(\"10_Industry_Portfolios\", start=\"1-1-1926\")[0]\nindustries.head()"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Indexes->Flavors->Indexes for Easier Arithmetic, Analysis"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Rolling Least Squares"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Rolling Least Squares"
            ]
        ]
    },
    "700685": {
        "jupyter_code_cell": "in_set = []\nnot_in_set = []\nfor record in actual_tweets:\n    match = re.findall(r'@\\w*', record[1])\n    if match != []:\n        for name in match:\n            if (name[1:] in dataset['username'].unique()) and (record[0] != name[1:]):\n                in_set.append([record[0], name[1:]])\n            elif record[0] != name[1:]:\n                not_in_set.append([record[0], name[1:]])\nin_set = np.array(in_set)\nnot_in_set = np.array(not_in_set)\nfig, ax = plt.subplots(1,2)\nax[0].bar([1,2], [len(np.unique(in_set[:,1])), len(np.unique(not_in_set[:,1]))], align='center')\nax[0].set_xticks([1,2])\nax[0].set_xticklabels(['In', 'Not in'])\nax[0].set_title('Users in vs. not in tweets.csv', fontsize=9)\nax[1].bar([1,2], [len(np.unique(in_set[:,1])), len(dataset['username'].unique())], align='center')\nax[1].set_xticks([1,2])\nax[1].set_xticklabels(['Mentioned', 'Total'])\nax[1].set_title('Mentioned vs. Total in tweets.csv', fontsize=9)\nsender_count = Counter(in_set[:,0])\nreceiver_count = Counter(in_set[:,1])\ntop_5_senders = sender_count.most_common(5)\ntop_5_receivers = receiver_count.most_common(5)\nprint(top_5_senders)\nprint(top_5_receivers)",
        "matched_tutorial_code_inds": [
            1767,
            3173,
            3122,
            2096,
            1962
        ],
        "matched_tutorial_codes": [
            "x_axis = []\ndata = {'positive sentiment': [], 'negative sentiment': []}\nfor speaker in predictions:\n    # The speakers will be used to label the x-axis in our plot\n    x_axis.append(speaker)\n    # number of paras with positive sentiment\n    no_pos_paras = len(predictions[speaker]['pos_paras'])\n    # number of paras with negative sentiment\n    no_neg_paras = len(predictions[speaker]['neg_paras'])\n    # Obtain percentage of paragraphs with positive predicted sentiment\n    pos_perc = no_pos_paras / (no_pos_paras + no_neg_paras)\n    # Store positive and negative percentages\n    data['positive sentiment'].append(pos_perc*100)\n    data['negative sentiment'].append(100*(1-pos_perc))\n\nindex = pd.Index(x_axis, name='speaker')\ndf = pd.DataFrame(data, index=index)\nax = df.plot(kind='bar', stacked=True)\nax.set_ylabel('percentage')\nax.legend(title='labels', bbox_to_anchor=(1, 1), loc='upper left')\nplt.show()",
            "pair_scores = []\nmean_tpr = dict()\n\nfor ix, (label_a, label_b) in enumerate(pair_list):\n\n    a_mask = y_test == label_a\n    b_mask = y_test == label_b\n    ab_mask = (a_mask, b_mask)\n\n    a_true = a_mask[ab_mask]\n    b_true = b_mask[ab_mask]\n\n    idx_a = (label_binarizer.classes_ == label_a)[0]\n    idx_b = (label_binarizer.classes_ == label_b)[0]\n\n    fpr_a, tpr_a, _ = (a_true, y_score[ab_mask, idx_a])\n    fpr_b, tpr_b, _ = (b_true, y_score[ab_mask, idx_b])\n\n    mean_tpr[ix] = (fpr_grid)\n    mean_tpr[ix] += (fpr_grid, fpr_a, tpr_a)\n    mean_tpr[ix] += (fpr_grid, fpr_b, tpr_b)\n    mean_tpr[ix] /= 2\n    mean_score = (fpr_grid, mean_tpr[ix])\n    pair_scores.append(mean_score)\n\n    fig, ax = (figsize=(6, 6))\n    (\n        fpr_grid,\n        mean_tpr[ix],\n        label=f\"Mean {label_a} vs {label_b} (AUC = {mean_score :.2f})\",\n        linestyle=\":\",\n        linewidth=4,\n    )\n    (\n        a_true,\n        y_score[ab_mask, idx_a],\n        ax=ax,\n        name=f\"{label_a} as positive class\",\n    )\n    (\n        b_true,\n        y_score[ab_mask, idx_b],\n        ax=ax,\n        name=f\"{label_b} as positive class\",\n    )\n    ([0, 1], [0, 1], \"k--\", label=\"chance level (AUC = 0.5)\")\n    (\"square\")\n    (\"False Positive Rate\")\n    (\"True Positive Rate\")\n    (f\"{target_names[idx_a]} vs {label_b} ROC curves\")\n    ()\n    ()\n\nprint(f\"Macro-averaged One-vs-One ROC AUC score:\\n{(pair_scores):.2f}\")\n\n\n\n<img alt=\"setosa vs versicolor ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_004.png\" srcset=\"../../_images/sphx_glr_plot_roc_004.png\"/>\n<img alt=\"setosa vs virginica ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_005.png\" srcset=\"../../_images/sphx_glr_plot_roc_005.png\"/>\n<img alt=\"versicolor vs virginica ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_006.png\" srcset=\"../../_images/sphx_glr_plot_roc_006.png\"/>",
            "results = (list)\nn_bootstrap = 100\nrng = (seed=0)\n\nfor prevalence, X, y in zip(\n    populations[\"prevalence\"], populations[\"X\"], populations[\"y\"]\n):\n\n    results_for_prevalence = scoring_on_bootstrap(\n        estimator, X, y, rng, n_bootstrap=n_bootstrap\n    )\n    results[\"prevalence\"].append(prevalence)\n    results[\"metrics\"].append(\n        results_for_prevalence.aggregate([\"mean\", \"std\"]).unstack()\n    )\n\nresults = (results[\"metrics\"], index=results[\"prevalence\"])\nresults.index.name = \"prevalence\"\nresults\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "node_indicator = clf.decision_path(X_test)\nleaf_id = clf.apply(X_test)\n\nsample_id = 0\n# obtain ids of the nodes `sample_id` goes through, i.e., row `sample_id`\nnode_index = node_indicator.indices[\n    node_indicator.indptr[sample_id] : node_indicator.indptr[sample_id + 1]\n]\n\nprint(\"Rules used to predict sample {id}:\\n\".format(id=sample_id))\nfor node_id in node_index:\n    # continue to the next node if it is a leaf node\n    if leaf_id[sample_id] == node_id:\n        continue\n\n    # check if value of the split feature for sample 0 is below threshold\n    if X_test[sample_id, feature[node_id]] &lt;= threshold[node_id]:\n        threshold_sign = \"&lt;=\"\n    else:\n        threshold_sign = \"\"\n\n    print(\n        \"decision node {node} : (X_test[{sample}, {feature}] = {value}) \"\n        \"{inequality} {threshold})\".format(\n            node=node_id,\n            sample=sample_id,\n            feature=feature[node_id],\n            value=X_test[sample_id, feature[node_id]],\n            inequality=threshold_sign,\n            threshold=threshold[node_id],\n        )\n    )",
            "unique_labels = set(labels)\ncore_samples_mask = (labels, dtype=bool)\ncore_samples_mask[db.core_sample_indices_] = True\n\ncolors = [plt.cm.Spectral(each) for each in (0, 1, len(unique_labels))]\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        # Black used for noise.\n        col = [0, 0, 0, 1]\n\n    class_member_mask = labels == k\n\n    xy = X[class_member_mask & core_samples_mask]\n    (\n        xy[:, 0],\n        xy[:, 1],\n        \"o\",\n        markerfacecolor=tuple(col),\n        markeredgecolor=\"k\",\n        markersize=14,\n    )\n\n    xy = X[class_member_mask & ~core_samples_mask]\n    (\n        xy[:, 0],\n        xy[:, 1],\n        \"o\",\n        markerfacecolor=tuple(col),\n        markeredgecolor=\"k\",\n        markersize=6,\n    )\n\n(f\"Estimated number of clusters: {n_clusters_}\")\n()\n\n\n<img alt=\"Estimated number of clusters: 3\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_dbscan_002.png\" srcset=\"../../_images/sphx_glr_plot_dbscan_002.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Sentiment Analysis on the Speech Data"
            ],
            [
                "sklearn->Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->One-vs-One multiclass ROC->ROC curve using the OvO macro-average"
            ],
            [
                "sklearn->Examples->Model Selection->Class Likelihood Ratios to measure classification performance->Invariance with respect to prevalence"
            ],
            [
                "sklearn->Examples->Decision Trees->Understanding the decision tree structure->Decision path"
            ],
            [
                "sklearn->Examples->Clustering->Demo of DBSCAN clustering algorithm->Plot results"
            ]
        ]
    },
    "269949": {
        "jupyter_code_cell": "df = pd.DataFrame(data=[])\nfor index, row in df_url.iterrows():\n\tdf2=scrap(row['url'])\n df2['area'] = row['area']\n df2['neighbourhood'] = row['neighbourhood']\n print(row['area'], \": \", len(df2.index))\n df = df.append(df2)\nprint('============================================')\nprint('total length: ', len(df.index))\ndf.to_csv(pwd + 'output/selangor.csv', sep=';', index=False )",
        "matched_tutorial_code_inds": [
            4410,
            4654,
            4396,
            4835,
            5705
        ],
        "matched_tutorial_codes": [
            "word_bytes = np.ndarray((word_list.size, word_list.itemsize),\n...                         dtype='uint8',\n...                         buffer=word_list.data)\n # each unicode character is four bytes long. We only need first byte\n # we know that there are three characters in each word\n word_bytes = word_bytes[:, ::word_list.itemsize//3]\n word_bytes.shape\n(586, 3)    # may vary",
            "data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>",
            "evals_small, evecs_small = eigsh(X, 3, which='SM', tol=1E-2)\n evals_all[:3]\narray([0.00053181, 0.00298319, 0.01387821])\n evals_small\narray([0.00053181, 0.00298319, 0.01387821])\n np.dot(evecs_small.T, evecs_all[:,:3])\narray([[ 0.99999999  0.00000024 -0.00000049],    # may vary (signs)\n       [-0.00000023  0.99999999  0.00000056],\n       [ 0.00000031 -0.00000037  0.99999852]])",
            "gs_kw = dict(width_ratios=[1.4, 1], height_ratios=[1, 2])\nfig, axd = plt.subplot_mosaic([['upper left', 'right'],\n                               ['lower left', 'right']],\n                              gridspec_kw=gs_kw, figsize=(5.5, 3.5),\n                              layout=\"constrained\")\nfor k in axd:\n    annotate_axes(axd[k], f'axd[\"{k}\"]', fontsize=14)\nfig.suptitle('plt.subplot_mosaic()')\n\n\n<img alt=\"plt.subplot_mosaic()\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_arranging_axes_006.png\" srcset=\"../../_images/sphx_glr_arranging_axes_006.png, ../../_images/sphx_glr_arranging_axes_006_2_0x.png 2.0x\"/>",
            "fig = infl.plot_index(y_var=\"cooks\", threshold=2 * infl.cooks_distance[0].mean())\nfig.tight_layout(pad=1.0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_influence_glm_logit_8_0.png\" src=\"../../../_images/examples_notebooks_generated_influence_glm_logit_8_0.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "scipy->Compressed Sparse Graph Routines (scipy.sparse.csgraph)->Example: Word Ladders"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings"
            ],
            [
                "scipy->Sparse eigenvalue problems with ARPACK->Examples"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Arranging multiple Axes in a Figure->High-level methods for making grids->Variable widths or heights in a grid"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Influence Measures for GLMs->get the influence measures"
            ]
        ]
    },
    "424934": {
        "jupyter_code_cell": "%timeit myOutput = 1/(inputValues)\nimport numpy as np\nimport pandas as pd\nd_data = {\n    'names' : ['tom' , 'tim', 'jim', 'ali'] ,\n    'age' : [12, 44, 56, 9]   \n}\ndf = pd.DataFrame(d_data)\ndf",
        "matched_tutorial_code_inds": [
            3635,
            6518,
            3745,
            3804,
            3697
        ],
        "matched_tutorial_codes": [
            "%config InlineBackend.figure_format = 'png'\nflights = (df[['fl_date', 'tail_num', 'dep_time', 'dep_delay']]\n           .dropna()\n           .sort_values('dep_time')\n           .loc[lambda x: x.dep_delay &lt; 500]\n           .assign(turn = lambda x:\n                x.groupby(['fl_date', 'tail_num'])\n                 .dep_time\n                 .transform('rank').astype(int)))\n\nfig, ax = plt.subplots(figsize=(15, 5))\nsns.boxplot(x='turn', y='dep_delay', data=flights, ax=ax)\nax.set_ylim(-50, 50)\nsns.despine()",
            "%matplotlib inline\n\nfrom importlib import reload\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\nfrom scipy.stats import invwishart, invgamma\n\n# Get the macro dataset\ndta = sm.datasets.macrodata.load_pandas().data\ndta.index = pd.date_range('1959Q1', '2009Q3', freq='QS')",
            "%matplotlib inline\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nif int(os.environ.get(\"MODERN_PANDAS_EPUB\", 0)):\n    import prep # noqa\n\npd.options.display.max_rows = 10\nsns.set(style='ticks', context='talk')",
            "%matplotlib inline\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport pandas_datareader.data as web\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style='ticks', context='talk')\n\nif int(os.environ.get(\"MODERN_PANDAS_EPUB\", 0)):\n    import prep # noqa",
            "%matplotlib inline\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nif int(os.environ.get(\"MODERN_PANDAS_EPUB\", 0)):\n    import prep # noqa\n\nsns.set_style('ticks')\nsns.set_context('talk')\npd.options.display.max_rows = 10"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Method Chaining->Method Chaining->Application"
            ],
            [
                "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries"
            ],
            [
                "pandas_toms_blog->Fast Pandas"
            ]
        ]
    },
    "381195": {
        "jupyter_code_cell": "with open('registration_data.csv', newline='') as f:\n    reg_csv = csv.reader(f, delimiter=',', skipinitialspace=True)\n    reg_list = [line for line in reg_csv]\nreg_labels = reg_list[0]\nreg_list = reg_list[1:]\nreg_list[:5]  ",
        "matched_tutorial_code_inds": [
            1391,
            3763,
            976,
            4703,
            4710
        ],
        "matched_tutorial_codes": [
            "with open(\"data/vocabs/answers_textvqa_more_than_1.txt\") as f:\n  vocab = f.readlines()\n\nanswer_to_idx = {}\nfor idx, entry in enumerate(vocab):\n  answer_to_idx[entry.strip(\"\\n\")] = idx\nprint(len(vocab))\nprint(vocab[:5])\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"textvqa\")",
            "with sns.color_palette() as pal:\n    b, g = pal.as_hex()[:2]\n\nax=(rest.unstack()\n        .query('away_team &lt; 7')\n        .rolling(7)\n        .mean()\n        .plot(figsize=(12, 6), linewidth=3, legend=False))\nax.set(ylabel='Rest (7 day MA)')\nax.annotate(\"Home\", (rest.index[-1][0], 1.02), color=g, size=14)\nax.annotate(\"Away\", (rest.index[-1][0], 0.82), color=b, size=14)\nsns.despine()",
            "with tune.checkpoint_dir(epoch) as checkpoint_dir:\n    path = os.path.join(checkpoint_dir, \"checkpoint\")\n    ((net.state_dict(), optimizer.state_dict()), path)\n\ntune.report(loss=(val_loss / val_steps), accuracy=correct / total)",
            "with mpl.rc_context({'lines.linewidth': 2, 'lines.linestyle': ':'}):\n    plt.plot(data)\n\n\n<img alt=\"customizing\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_customizing_004.png\" srcset=\"../../_images/sphx_glr_customizing_004.png, ../../_images/sphx_glr_customizing_004_2_0x.png 2.0x\"/>",
            "with plt.style.context('dark_background'):\n    plt.plot(np.sin(np.linspace(0, 2 * np.pi)), 'r-o')\nplt.show()\n\n\n<img alt=\"customizing\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_customizing_006.png\" srcset=\"../../_images/sphx_glr_customizing_006.png, ../../_images/sphx_glr_customizing_006_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Multimodality->TorchMultimodal Tutorial: Finetuning FLAVA->Steps"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Stack / Unstack"
            ],
            [
                "torch->Model Optimization->Hyperparameter tuning with Ray Tune->The train function->Communicating with Ray Tune"
            ],
            [
                "matplotlib->Tutorials->Introductory->Customizing Matplotlib with style sheets and rcParams->Runtime rc settings->Temporary rc settings"
            ],
            [
                "matplotlib->Tutorials->Introductory->Customizing Matplotlib with style sheets and rcParams->Using style sheets->Temporary styling"
            ]
        ]
    },
    "93910": {
        "jupyter_code_cell": "voices.iloc[[0,1,2,-1,-2,-3]][voices.columns[11:]]\nvoices.isnull().values.any()",
        "matched_tutorial_code_inds": [
            1965,
            3610,
            6770,
            5703,
            6231
        ],
        "matched_tutorial_codes": [
            "centers = [[1, 1], [-1, -1], [1, -1]]\nX, labels_true = (\n    n_samples=300, centers=centers, cluster_std=0.5, random_state=0\n)",
            "first = df.groupby('unique_carrier').first()\nfirst.ix[10:15, ['fl_date', 'tail_num']]",
            "df = dta.data[[\"Lottery\", \"Literacy\", \"Wealth\", \"Region\"]].dropna()\ndf.head()",
            "summ_df = infl.summary_frame()\nsumm_df.sort_values(\"cooks_d\", ascending=False)[:10]",
            "res_areturns.smoothed_marginal_probabilities[0].plot(\n    title=\"Probability of being in a low-variance regime\", figsize=(12, 3)\n)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Clustering->Demo of affinity propagation clustering algorithm->Generate sample data"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing"
            ],
            [
                "statsmodels->Examples->User Notes->Formulas->OLS regression using formulas"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Influence Measures for GLMs->get the influence measures"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Markov switching dynamic regression->Switching variances"
            ]
        ]
    },
    "1508176": {
        "jupyter_code_cell": "breed_acc_df.sort_values(by='resnet_accuracy', ascending=True).head(10).plot.barh(figsize=(8, 10))\nbreed_acc_df.sort_values(by='inception_accuracy', ascending=True).head(10).plot.barh(figsize=(8, 10))",
        "matched_tutorial_code_inds": [
            5532,
            3584,
            5472,
            4100,
            5870
        ],
        "matched_tutorial_codes": [
            "res_logit_hac = mod_logit.fit(method='bfgs', disp=False, cov_type=\"hac\", cov_kwds={\"maxlags\": 2})\nres_log_hac = mod_log.fit(method='bfgs', disp=False, cov_type=\"hac\", cov_kwds={\"maxlags\": 2})",
            "tfidf_transformer = TfidfTransformer()\n X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n X_train_tfidf.shape\n(2257, 35788)",
            "resp25 = glm_mod.predict(pd.DataFrame(means25).T)\nresp75 = glm_mod.predict(pd.DataFrame(means75).T)\ndiff = resp75 - resp25",
            "x = np.random.normal(0, 1, 50)\ny = x * 2 + np.random.normal(0, 2, size=x.size)\nsns.regplot(x=x, y=y)\n",
            "groups_re = np.sqrt(groups_var) * np.random.normal(size=n // group_size)\nlevel1_re = np.sqrt(level1_var) * np.random.normal(size=n // level1_size)\nlevel2_re = np.sqrt(level2_var) * np.random.normal(size=n // level2_size)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Binary Model compared to Logit"
            ],
            [
                "sklearn->Tutorials->Working With Text Data->Extracting features from text files->From occurrences to frequencies"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution"
            ],
            [
                "seaborn->User guide and tutorial->Statistical estimation and error bars->Error bars on regression fits",
                "seaborn->Statistical operations->Statistical estimation and error bars->Error bars on regression fits"
            ],
            [
                "statsmodels->Examples->Generalized Estimating Equations->GEE Nested Covariance Structure"
            ]
        ]
    },
    "70396": {
        "jupyter_code_cell": "def get_recs3(movie_name, M, num):\n    import numpy as np\n    reviews = []\n    for title in M.columns:\n        if title == movie_name:\n            continue\n        if title in my_ratings.index:\n            continue\n        cor = pearson(M[movie_name], M[title])\n        if np.isnan(cor):\n            continue\n        else:\n            reviews.append((title, cor))\n    reviews.sort(key=lambda tup: tup[1], reverse=True)\n    return reviews[:num]\nrecs2 = get_recs3(fav_movie[0], M, M.shape[1])\nrecs2 = recs2 + get_recs3(fav_movie[1], M, M.shape[1])\nrecs2.sort(key=lambda tup: tup[1], reverse=True)\nrecs2\nmy_ratings = M_copy.ix['Daniel Lee']\nmy_ratings[np.isnan(my_ratings)] = 0\nmy_ratings",
        "matched_tutorial_code_inds": [
            6276,
            2398,
            3103,
            3101,
            35
        ],
        "matched_tutorial_codes": [
            "def add_stl_plot(fig, res, legend):\n    \"\"\"Add 3 plots from a second STL fit\"\"\"\n    axs = fig.get_axes()\n    comps = [\"trend\", \"seasonal\", \"resid\"]\n    for ax, comp in zip(axs[1:], comps):\n        series = getattr(res, comp)\n        if comp == \"resid\":\n            ax.plot(series, marker=\"o\", linestyle=\"none\")\n        else:\n            ax.plot(series)\n            if comp == \"trend\":\n                ax.legend(legend, frameon=False)\n\n\nstl = STL(elec_equip, period=12, robust=True)\nres_robust = stl.fit()\nfig = res_robust.plot()\nres_non_robust = STL(elec_equip, period=12, robust=False).fit()\nadd_stl_plot(fig, res_non_robust, [\"Robust\", \"Non-robust\"])\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_stl_decomposition_10_0.png\" src=\"../../../_images/examples_notebooks_generated_stl_decomposition_10_0.png\"/>",
            "def plot_accuracy(x, y, x_legend):\n    \"\"\"Plot accuracy as a function of x.\"\"\"\n    x = (x)\n    y = (y)\n    (\"Classification accuracy as a function of %s\" % x_legend)\n    (\"%s\" % x_legend)\n    (\"Accuracy\")\n    (True)\n    (x, y)\n\n\n[\"legend.fontsize\"] = 10\ncls_names = list(sorted(cls_stats.keys()))\n\n# Plot accuracy evolution\n()\nfor _, stats in sorted(cls_stats.items()):\n    # Plot accuracy evolution with #examples\n    accuracy, n_examples = zip(*stats[\"accuracy_history\"])\n    plot_accuracy(n_examples, accuracy, \"training examples (#)\")\n    ax = ()\n    ax.set_ylim((0.8, 1))\n(cls_names, loc=\"best\")\n\n()\nfor _, stats in sorted(cls_stats.items()):\n    # Plot accuracy evolution with runtime\n    accuracy, runtime = zip(*stats[\"runtime_history\"])\n    plot_accuracy(runtime, accuracy, \"runtime (s)\")\n    ax = ()\n    ax.set_ylim((0.8, 1))\n(cls_names, loc=\"best\")\n\n# Plot fitting times\n()\nfig = ()\ncls_runtime = [stats[\"total_fit_time\"] for cls_name, stats in sorted(cls_stats.items())]\n\ncls_runtime.append(total_vect_time)\ncls_names.append(\"Vectorization\")\nbar_colors = [\"b\", \"g\", \"r\", \"c\", \"m\", \"y\"]\n\nax = (111)\nrectangles = (range(len(cls_names)), cls_runtime, width=0.5, color=bar_colors)\n\nax.set_xticks((0, len(cls_names) - 1, len(cls_names)))\nax.set_xticklabels(cls_names, fontsize=10)\nymax = max(cls_runtime) * 1.2\nax.set_ylim((0, ymax))\nax.set_ylabel(\"runtime (s)\")\nax.set_title(\"Training Times\")\n\n\ndef autolabel(rectangles):\n    \"\"\"attach some text vi autolabel on rectangles.\"\"\"\n    for rect in rectangles:\n        height = rect.get_height()\n        ax.text(\n            rect.get_x() + rect.get_width() / 2.0,\n            1.05 * height,\n            \"%.4f\" % height,\n            ha=\"center\",\n            va=\"bottom\",\n        )\n        (()[1], rotation=30)\n\n\nautolabel(rectangles)\n()\n()\n\n# Plot prediction times\n()\ncls_runtime = []\ncls_names = list(sorted(cls_stats.keys()))\nfor cls_name, stats in sorted(cls_stats.items()):\n    cls_runtime.append(stats[\"prediction_time\"])\ncls_runtime.append(parsing_time)\ncls_names.append(\"Read/Parse\\n+Feat.Extr.\")\ncls_runtime.append(vectorizing_time)\ncls_names.append(\"Hashing\\n+Vect.\")\n\nax = (111)\nrectangles = (range(len(cls_names)), cls_runtime, width=0.5, color=bar_colors)\n\nax.set_xticks((0, len(cls_names) - 1, len(cls_names)))\nax.set_xticklabels(cls_names, fontsize=8)\n(()[1], rotation=30)\nymax = max(cls_runtime) * 1.2\nax.set_ylim((0, ymax))\nax.set_ylabel(\"runtime (s)\")\nax.set_title(\"Prediction Times (%d instances)\" % n_test_documents)\nautolabel(rectangles)\n()\n()\n\n\n\n<img alt=\"Classification accuracy as a function of training examples (#)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_out_of_core_classification_001.png\" srcset=\"../../_images/sphx_glr_plot_out_of_core_classification_001.png\"/>\n<img alt=\"Classification accuracy as a function of runtime (s)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_out_of_core_classification_002.png\" srcset=\"../../_images/sphx_glr_plot_out_of_core_classification_002.png\"/>\n<img alt=\"Training Times\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_out_of_core_classification_003.png\" srcset=\"../../_images/sphx_glr_plot_out_of_core_classification_003.png\"/>\n<img alt=\"Prediction Times (1000 instances)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_out_of_core_classification_004.png\" srcset=\"../../_images/sphx_glr_plot_out_of_core_classification_004.png\"/>",
            "def get_impute_iterative(X_missing, y_missing):\n    imputer = (\n        missing_values=,\n        add_indicator=True,\n        random_state=0,\n        n_nearest_features=3,\n        max_iter=1,\n        sample_posterior=True,\n    )\n    iterative_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n    return iterative_impute_scores.mean(), iterative_impute_scores.std()\n\n\nmses_california[4], stds_california[4] = get_impute_iterative(\n    X_miss_california, y_miss_california\n)\nmses_diabetes[4], stds_diabetes[4] = get_impute_iterative(\n    X_miss_diabetes, y_miss_diabetes\n)\nx_labels.append(\"Iterative Imputation\")\n\nmses_diabetes = mses_diabetes * -1\nmses_california = mses_california * -1",
            "def get_impute_knn_score(X_missing, y_missing):\n    imputer = (missing_values=, add_indicator=True)\n    knn_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n    return knn_impute_scores.mean(), knn_impute_scores.std()\n\n\nmses_california[2], stds_california[2] = get_impute_knn_score(\n    X_miss_california, y_miss_california\n)\nmses_diabetes[2], stds_diabetes[2] = get_impute_knn_score(\n    X_miss_diabetes, y_miss_diabetes\n)\nx_labels.append(\"KNN Imputation\")",
            "def print_size_of_model(model, label=\"\"):\n    (model.state_dict(), \"temp.p\")\n    size=os.path.getsize(\"temp.p\")\n    print(\"model: \",label,' \\t','Size (KB):', size/1e3)\n    os.remove('temp.p')\n    return size\n\n# compare the sizes\nf=print_size_of_model(float_lstm,\"fp32\")\nq=print_size_of_model(quantized_lstm,\"int8\")\nprint(\"{0:.2f} times smaller\".format(f/q))"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition->Robust Fitting"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Out-of-core classification of text documents->Plot results"
            ],
            [
                "sklearn->Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score->Iterative imputation of the missing values"
            ],
            [
                "sklearn->Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score->kNN-imputation of the missing values"
            ],
            [
                "torch->PyTorch Recipes->Dynamic Quantization->Steps->3. Look at Model Size"
            ]
        ]
    },
    "1382167": {
        "jupyter_code_cell": "y = df['churn']\nz=df.state\nh=df.phone_number\ny = pd.get_dummies(y)[' True.']\nX =df.drop(['churn','phone_number','state'],axis=1)\nX['international_plan'] = pd.get_dummies(X.international_plan)[' yes']\nX['voice_mail'] = pd.get_dummies(X.voice_mail)[' yes']",
        "matched_tutorial_code_inds": [
            2064,
            1677,
            5998,
            6795,
            6401
        ],
        "matched_tutorial_codes": [
            "y = X.dot(pca.components_[1]) + rng.normal(size=n_samples) / 2\n\nfig, axes = (1, 2, figsize=(10, 3))\n\naxes[0].scatter(X.dot(pca.components_[0]), y, alpha=0.3)\naxes[0].set(xlabel=\"Projected data onto first PCA component\", ylabel=\"y\")\naxes[1].scatter(X.dot(pca.components_[1]), y, alpha=0.3)\naxes[1].set(xlabel=\"Projected data onto second PCA component\", ylabel=\"y\")\n()\n()\n\n\n<img alt=\"plot pcr vs pls\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_pcr_vs_pls_002.png\" srcset=\"../../_images/sphx_glr_plot_pcr_vs_pls_002.png\"/>",
            "output = general_julia(mesh, f=g, num_iter=15, radius=2.1)\nkwargs = {'title': 'g(z) = sin(tan(z^2))', 'cmap': 'plasma_r'}\n\nplot_fractal(output, **kwargs);\n\n\n\n\n<img alt=\"../_images/5526a4060ed968ec299d39a61bbae12254e1e2536b7ad21d8175edd94edf1bfd.png\" src=\"../_images/5526a4060ed968ec299d39a61bbae12254e1e2536b7ad21d8175edd94edf1bfd.png\"/>",
            "res5 = combine_effects(eff, var_eff, method_re=\"chi2\", use_t=False)\nres5_df = res5.summary_frame()\nprint(\"method RE:\", res5.method_re)\nprint(res5.summary_frame())\nfig = res5.plot_forest()\nfig.set_figheight(8)\nfig.set_figwidth(6)",
            "x1n = np.linspace(20.5, 25, 10)\nXnew = np.column_stack((x1n, np.sin(x1n), (x1n - 5) ** 2))\nXnew = sm.add_constant(Xnew)\nynewpred = olsres.predict(Xnew)  # predict out of sample\nprint(ynewpred)",
            "exog = endog['dln_consump']\nmod = sm.tsa.VARMAX(endog[['dln_inv', 'dln_inc']], order=(2,0), trend='n', exog=exog)\nres = mod.fit(maxiter=1000, disp=False)\nprint(res.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Cross decomposition->Principal Component Regression vs Partial Least Squares Regression->The data"
            ],
            [
                "numpy->NumPy Applications->Plotting Fractals->Creating your own fractals"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Meta-analysis of proportions->changing data to have positive random effects variance"
            ],
            [
                "statsmodels->Examples->User Notes->Prediction->Create a new sample of explanatory variables Xnew, predict and plot"
            ],
            [
                "statsmodels->Examples->State space models->VARMAX: Introduction->Example 1: VAR"
            ]
        ]
    },
    "833317": {
        "jupyter_code_cell": "tstrt = 20.00 * 60  \nratingPV = 6000.  \nbatvolt = 48 \nbatCap = 800. * batvolt  \nbatDoD = 0.6 \nbatACEff = 0.9 \nbatDCEff = 0.9 \nmaxInverter = 6000  \nmaxBatCharge = 40 \ncalcProfile(tstrt, ratingPV, batvolt, batCap, batDoD, batACEff, batDCEff,\n            maxInverter,maxBatCharge,reqAC,removedAC,cost=1.5, storedinGrid=True);\ntstrt = 20.00 * 60  \nratingPV = 6000.  \nbatvolt = 48 \nbatCap = 800. * batvolt  \nbatDoD = 0.27 \nbatACEff = 0.9 \nbatDCEff = 0.9 \nmaxInverter = 6000  \nmaxBatCharge = 40 \ncalcProfile(tstrt, ratingPV, batvolt, batCap, batDoD, batACEff, batDCEff,\n            maxInverter,maxBatCharge,reqAC,removedAC,cost=1.5, storedinGrid=False);",
        "matched_tutorial_code_inds": [
            1677,
            1162,
            5917,
            2753,
            1661
        ],
        "matched_tutorial_codes": [
            "output = general_julia(mesh, f=g, num_iter=15, radius=2.1)\nkwargs = {'title': 'g(z) = sin(tan(z^2))', 'cmap': 'plasma_r'}\n\nplot_fractal(output, **kwargs);\n\n\n\n\n<img alt=\"../_images/5526a4060ed968ec299d39a61bbae12254e1e2536b7ad21d8175edd94edf1bfd.png\" src=\"../_images/5526a4060ed968ec299d39a61bbae12254e1e2536b7ad21d8175edd94edf1bfd.png\"/>",
            "model = init_model()\n = (())\n\ndef train(mod, data):\n    (True)\n    pred = mod(data[0])\n    loss = ()(pred, data[1])\n    loss.backward()\n    ()\n\neager_times = []\nfor i in range(N_ITERS):\n    inp = generate_data(16)\n    _, eager_time = timed(lambda: train(model, inp))\n    eager_times.append(eager_time)\n    print(f\"eager train time {i}: {eager_time}\")\nprint(\"~\" * 10)\n\nmodel = init_model()\n = (())\ntrain_opt = (train, mode=\"reduce-overhead\")\n\ncompile_times = []\nfor i in range(N_ITERS):\n    inp = generate_data(16)\n    _, compile_time = timed(lambda: train_opt(model, inp))\n    compile_times.append(compile_time)\n    print(f\"compile train time {i}: {compile_time}\")\nprint(\"~\" * 10)\n\neager_med = np.median(eager_times)\ncompile_med = np.median(compile_times)\nspeedup = eager_med / compile_med\nprint(f\"(train) eager median: {eager_med}, compile median: {compile_med}, speedup: {speedup}x\")\nprint(\"~\" * 10)",
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "quantiles = [0.05, 0.5, 0.95]\npredictions = {}\nout_bounds_predictions = (y_true_mean, dtype=)\nfor quantile in quantiles:\n    qr = (quantile=quantile, alpha=0, solver=solver)\n    y_pred = qr.fit(X, y_pareto).predict(X)\n    predictions[quantile] = y_pred\n\n    if quantile == min(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred = y_pareto\n        )\n    elif quantile == max(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred &lt;= y_pareto\n        )",
            "output = julia(mesh, c=-0.75 + 0.4j, num_iter=20)\nkwargs = {'title': 'f(z) = z^2 - \\dfrac{3}{4} + 0.4i', 'cmap': 'Greens_r'}\n\nplot_fractal(output, **kwargs);\n\n\n\n\n<img alt=\"../_images/bfb120678f65f1892658c8e3121fc93f5e95b61f9b4acd4ff184d041d5903eac.png\" src=\"../_images/bfb120678f65f1892658c8e3121fc93f5e95b61f9b4acd4ff184d041d5903eac.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Applications->Plotting Fractals->Creating your own fractals"
            ],
            [
                "torch->Model Optimization->torch.compile Tutorial->Demonstrating Speedups"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Quantile regression->Fitting a QuantileRegressor"
            ],
            [
                "numpy->NumPy Applications->Plotting Fractals->Julia set"
            ]
        ]
    },
    "901914": {
        "jupyter_code_cell": "def Ek2(k):\n    return((1/6)*(1-(1/3)**k)+(4/15)*((-1)**(k+1)*(1/2)**k+(1/3)**k))\ndef V(k):\n    return((1/10)*(1/3)**k+(1/18)-(1/9)*(1/2)**(2*k)+(-1)**(k+1)*(2/45)*(1/2)**k)\nfor j in range(0, k):\n    def Ej(j):\n        return((1/6)*(1-(1/3)**(j))+(4/15)*((-1)**(j+1)*(1/2)**(j)+(1/3)**(j)))\n    def T1(j):\n        return((1/2)*((1/3)**(j-1)*(1-(1/3)**(k-j))))\n    def T2(j):\n        return((1/2)*((1/3)**j-(1/3)**k))\n    def T3(j):\n        return((3/5)*(4*(1/3)**(k+1)+(-1)**(j+k+1)*(1/2)**(k-j-2)*(1/3)**(j+1)))\n    def T4(j):\n        return((1/5)*((-1)**(j+2)*(1/3)*(1/2)**(j-2)-4*(1/3)**(j+1)))\n    def T5(j):\n        return((1/5)*((-1)**(k+1)*(1/3)*(1/2)**(k-2)+(-1)**(k+j)*(1/3)**(j+1)*(1/2)**(k-j-2)))\ndef A1ST(j):\n    return((Ek2(k)-Ej(j)-(1/4)*(T1(j)-2*T2(j)+T3(j))-T4(j)-T5(j))/V(k))\ndef VjA2(j):\n    return((1/3)/(1+a2[j])**2)\ndef VA2(j):\n    productA2 = []\n    for j in range(0, k):\n        productA2.append(1+VjA2(j))\n    return np.product(productA2)-1\ndef VnA2(j):\n    return((VA2(j)+1)/(1+VjA2(j)))\ndef VTjA2(j):\n    return VjA2(j)*(VnA2(j))\ndef A2ST(j):\n    return(VTjA2(j)/VA2(j))\ndef q(j):\n    return(12*(k-0.5)**2)\ndef B1ST(j):\n    return((q(j)+1)**k/((q(j)+1)*((q(j)+1)**k-q(j)**k)))\ndef B2ST(j):\n    return((k+1)**(2*k-2)/(((k+1)**(2*k)-(k**2+2*k)**k)))\ndef VjB3(j):\n    return((1/3)/(1+b3[j])**2)\ndef VB3(j):\n    productB3 = []\n    for j in range(0, k):\n        productB3.append(1+VjB3(j))\n    return np.product(productB3)-1\ndef VnB3(j):\n    return((VB3(j)+1)/(1+VjB3(j)))\ndef VTjB3(j):\n    return VjB3(j)*(VnB3(j))\ndef B3ST(j):\n    return(VTjB3(j)/VB3(j))\ndef C1ST(j):\n    return 4**(k-1)/(4**k-3**k)\ndef C2ST(j):\n    return 4**(k-1)/(4**k-3**k)\ndef create_dict(key, values):\n    return dict(zip(key, values))\nanalyticalValues = [A1ST,A2ST,B1ST,B2ST,B3ST,C1ST,C2ST]\nAE = []\nAE_l = []\nAE_names = []\nAE_namesd = []\nfor iw,w in enumerate(analyticalValues):\n    AE_names.append(str(w.__name__)[:2])\n    for j in range (0,k):\n        AE.append(w(j))\n        AE_namesd.append('AE' + str(w.__name__) + str(j))\nAE_df = pd.DataFrame([AE[k*iw:k*(iw+1)] for iw in range(len(analyticalValues))],AE_names)\nAE_dic = create_dict(AE_namesd, AE)\np = 13\nrun = 50\nn = [3,4,6]\ndf = pd.DataFrame(sobol_seq.i4_sobol_generate(6*k, run*2**p))",
        "matched_tutorial_code_inds": [
            588,
            597,
            1672,
            838,
            2966
        ],
        "matched_tutorial_codes": [
            "class MyDecisionGate():\n    def forward(self, ):\n        if .sum() &gt; 0:\n            return \n        else:\n            return -\n\nclass MyCell():\n    def __init__(self):\n        super(, self).__init__()\n        self.dg = ()\n        self.linear = (4, 4)\n\n    def forward(self, , ):\n        new_h = (self.dg(self.linear()) + )\n        return new_h, new_h\n\nmy_cell = ()\nprint(my_cell)\nprint(my_cell(, ))",
            "class MyDecisionGate():\n    def forward(self, ):\n        if .sum() &gt; 0:\n            return \n        else:\n            return -\n\nclass MyCell():\n    def __init__(self, dg):\n        super(, self).__init__()\n        self.dg = dg\n        self.linear = (4, 4)\n\n    def forward(self, , ):\n        new_h = (self.dg(self.linear()) + )\n        return new_h, new_h\n\nmy_cell = (())\ntraced_cell = (my_cell, (, ))\n\nprint()\nprint()",
            "def sin_sum(z, n=10):\n    total = np.zeros(z.size, dtype=z.dtype)\n    for i in range(1, n+1):\n        total += np.power(np.sin(z), i)\n    return total\n\n\ndef d_sin_sum(z, n=10):\n    total = np.zeros(z.size, dtype=z.dtype)\n    for i in range(1, n+1):\n        total += i * np.power(np.sin(z), i-1) * np.cos(z)\n    return total",
            "def cube_forward(x):\n    return x**3\n\ndef cube_backward(grad_out, x):\n    return grad_out * 3 * x**2\n\ndef cube_backward_backward(grad_out, sav_grad_out, x):\n    return grad_out * sav_grad_out * 6 * x\n\ndef cube_backward_backward_grad_out(grad_out, x):\n    return grad_out * 3 * x**2\n\nclass Cube(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        ctx.save_for_backward(x)\n        return cube_forward(x)\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        x, = ctx.saved_tensors\n        return CubeBackward.apply(grad_out, x)\n\nclass CubeBackward(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, grad_out, x):\n        ctx.save_for_backward(x, grad_out)\n        return cube_backward(grad_out, x)\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        x, sav_grad_out = ctx.saved_tensors\n        dx = cube_backward_backward(grad_out, sav_grad_out, x)\n        dgrad_out = cube_backward_backward_grad_out(grad_out, x)\n        return dgrad_out, dx\n\nx = torch.tensor(2., requires_grad=True, dtype=torch.double)\n\ntorch.autograd.gradcheck(Cube.apply, x)\ntorch.autograd.gradgradcheck(Cube.apply, x)",
            "def plot_3d(points, points_color, title):\n    x, y, z = points.T\n\n    fig, ax = (\n        figsize=(6, 6),\n        facecolor=\"white\",\n        tight_layout=True,\n        subplot_kw={\"projection\": \"3d\"},\n    )\n    fig.suptitle(title, size=16)\n    col = ax.scatter(x, y, z, c=points_color, s=50, alpha=0.8)\n    ax.view_init(azim=-60, elev=9)\n    ax.xaxis.set_major_locator((1))\n    ax.yaxis.set_major_locator((1))\n    ax.zaxis.set_major_locator((1))\n\n    fig.colorbar(col, ax=ax, orientation=\"horizontal\", shrink=0.6, aspect=60, pad=0.01)\n    ()\n\n\ndef plot_2d(points, points_color, title):\n    fig, ax = (figsize=(3, 3), facecolor=\"white\", constrained_layout=True)\n    fig.suptitle(title, size=16)\n    add_2d_scatter(ax, points, points_color)\n    ()\n\n\ndef add_2d_scatter(ax, points, points_color, title=None):\n    x, y = points.T\n    ax.scatter(x, y, c=points_color, s=50, alpha=0.8)\n    ax.set_title(title)\n    ax.xaxis.set_major_formatter(())\n    ax.yaxis.set_major_formatter(())\n\n\nplot_3d(S_points, S_color, \"Original S-curve samples\")\n\n\n<img alt=\"Original S-curve samples\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_compare_methods_001.png\" srcset=\"../../_images/sphx_glr_plot_compare_methods_001.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Deploying PyTorch Models in Production->Introduction to TorchScript->Basics of PyTorch Model Authoring"
            ],
            [
                "torch->Deploying PyTorch Models in Production->Introduction to TorchScript->Using Scripting to Convert Modules"
            ],
            [
                "numpy->NumPy Applications->Plotting Fractals->Generalizing the Julia set->Newton Fractals"
            ],
            [
                "torch->Extending PyTorch->Double Backward with Custom Functions->When Backward is not Tracked"
            ],
            [
                "sklearn->Examples->Manifold learning->Comparison of Manifold Learning methods->Dataset preparation"
            ]
        ]
    },
    "860767": {
        "jupyter_code_cell": "plt.scatter(x=NBA['Wins'], y=NBA['Salary'])\nplt.ylabel('Player Salaries (in terms of $10M)')\nplt.xlabel('Number of Team Wins')\nplt.title('The relationship between Team Total Wins to their Salaries')\nplt.show()\nx=NBA.Wins\ny=NBA.Salary\npearsonr(x, y)\nMedian_Salary_Sorted = Median.sort_values('Salary', ascending=False)\nMedian_Salary_Sorted.Salary.plot(kind='bar', figsize = (10, 5))\nplt.ylabel('Median Salary per team (in terms of $10M)')",
        "matched_tutorial_code_inds": [
            4820,
            5362,
            4827,
            4652,
            4691
        ],
        "matched_tutorial_codes": [
            "plt.close('all')\nfig = plt.figure()\n\nax1 = plt.subplot2grid((3, 3), (0, 0))\nax2 = plt.subplot2grid((3, 3), (0, 1), colspan=2)\nax3 = plt.subplot2grid((3, 3), (1, 0), colspan=2, rowspan=2)\nax4 = plt.subplot2grid((3, 3), (1, 2), rowspan=2)\n\nexample_plot(ax1)\nexample_plot(ax2)\nexample_plot(ax3)\nexample_plot(ax4)\n\nplt.tight_layout()\n\n\n<img alt=\"Title, Title, Title, Title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_tight_layout_guide_007.png\" srcset=\"../../_images/sphx_glr_tight_layout_guide_007.png, ../../_images/sphx_glr_tight_layout_guide_007_2_0x.png 2.0x\"/>",
            "plt.rcParams[\"figure.subplot.bottom\"] = 0.23  # keep labels visible\nplt.rcParams[\"figure.figsize\"] = (10.0, 8.0)  # make plot larger in notebook\nage = [data.exog[\"age\"][data.endog == id] for id in party_ID]\nfig = plt.figure()\nax = fig.add_subplot(111)\nplot_opts = {\n    \"cutoff_val\": 5,\n    \"cutoff_type\": \"abs\",\n    \"label_fontsize\": \"small\",\n    \"label_rotation\": 30,\n}\nsm.graphics.beanplot(age, ax=ax, labels=labels, plot_opts=plot_opts)\nax.set_xlabel(\"Party identification of respondent.\")\nax.set_ylabel(\"Age\")\n# plt.show()",
            "plt.close('all')\narr = np.arange(100).reshape((10, 10))\nfig = plt.figure(figsize=(4, 4))\nim = plt.imshow(arr, interpolation=\"none\")\n\nplt.colorbar(im)\n\nplt.tight_layout()\n\n\n<img alt=\"tight layout guide\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_tight_layout_guide_014.png\" srcset=\"../../_images/sphx_glr_tight_layout_guide_014.png, ../../_images/sphx_glr_tight_layout_guide_014_2_0x.png 2.0x\"/>",
            "plt.plot([1, 2, 3, 4], [1, 4, 9, 16], 'ro')\nplt.axis([0, 6, 0, 20])\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_003.png\" srcset=\"../../_images/sphx_glr_pyplot_003.png, ../../_images/sphx_glr_pyplot_003_2_0x.png 2.0x\"/>",
            "plt.rcParams.update({'figure.autolayout': True})\n\nfig, ax = plt.subplots()\nax.barh(group_names, group_data)\nlabels = ax.get_xticklabels()\nplt.setp(labels, rotation=45, horizontalalignment='right')\n\n\n<img alt=\"lifecycle\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_lifecycle_006.png\" srcset=\"../../_images/sphx_glr_lifecycle_006.png, ../../_images/sphx_glr_lifecycle_006_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Intermediate->Tight Layout guide->Simple Example"
            ],
            [
                "statsmodels->Examples->Plotting->Box Plots->Bean Plots"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Tight Layout guide->Colorbar"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Introduction to pyplot->Formatting the style of your plot"
            ],
            [
                "matplotlib->Tutorials->Introductory->The Lifecycle of a Plot->Customizing the plot"
            ]
        ]
    },
    "657375": {
        "jupyter_code_cell": "from plotnine import *\n(ggplot(df_churn, aes(x='Contract', y='MonthlyCharges')) + geom_jitter(position=position_jitter(0.4)))\nfrom plotnine import *\n(ggplot(df_churn, aes(x='OnlineBackup', y='MonthlyCharges')) + geom_jitter(position=position_jitter(0.4)))",
        "matched_tutorial_code_inds": [
            423,
            2662,
            6253,
            1843,
            1097
        ],
        "matched_tutorial_codes": [
            "with (use_cuda=False) as :\n     = model()\nwith (use_cuda=False) as :\n     = scripted_model()\nwith (use_cuda=False) as :\n     = scripted_quantized_model()\nwith (use_cuda=False) as :\n     = optimized_scripted_quantized_model()\nwith (use_cuda=False) as :\n     = ptl()\n\nprint(\"original model: {:.2f}ms\".format(/1000))\nprint(\"scripted model: {:.2f}ms\".format(/1000))\nprint(\"scripted &amp; quantized model: {:.2f}ms\".format(/1000))\nprint(\"scripted &amp; quantized &amp; optimized model: {:.2f}ms\".format(/1000))\nprint(\"lite model: {:.2f}ms\".format(/1000))",
            "m, s, _ = (\n    (enet.coef_)[0],\n    enet.coef_[enet.coef_ != 0],\n    markerfmt=\"x\",\n    label=\"Elastic net coefficients\",\n)\n([m, s], color=\"#2ca02c\")\nm, s, _ = (\n    (lasso.coef_)[0],\n    lasso.coef_[lasso.coef_ != 0],\n    markerfmt=\"x\",\n    label=\"Lasso coefficients\",\n)\n([m, s], color=\"#ff7f0e\")\n(\n    (coef)[0],\n    coef[coef != 0],\n    label=\"true coefficients\",\n    markerfmt=\"bx\",\n)\n\n(loc=\"best\")\n(\n    \"Lasso $R^2$: %.3f, Elastic Net $R^2$: %.3f\" % (r2_score_lasso, r2_score_enet)\n)\n()\n\n\n<img alt=\"Lasso $R^2$: 0.658, Elastic Net $R^2$: 0.643\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_lasso_and_elasticnet_001.png\" srcset=\"../../_images/sphx_glr_plot_lasso_and_elasticnet_001.png\"/>",
            "fit1 = SimpleExpSmoothing(oildata, initialization_method=\"heuristic\").fit(\n    smoothing_level=0.2, optimized=False\n)\nfcast1 = fit1.forecast(3).rename(r\"$\\alpha=0.2$\")\nfit2 = SimpleExpSmoothing(oildata, initialization_method=\"heuristic\").fit(\n    smoothing_level=0.6, optimized=False\n)\nfcast2 = fit2.forecast(3).rename(r\"$\\alpha=0.6$\")\nfit3 = SimpleExpSmoothing(oildata, initialization_method=\"estimated\").fit()\nfcast3 = fit3.forecast(3).rename(r\"$\\alpha=%s$\" % fit3.model.params[\"smoothing_level\"])\n\nplt.figure(figsize=(12, 8))\nplt.plot(oildata, marker=\"o\", color=\"black\")\nplt.plot(fit1.fittedvalues, marker=\"o\", color=\"blue\")\n(line1,) = plt.plot(fcast1, marker=\"o\", color=\"blue\")\nplt.plot(fit2.fittedvalues, marker=\"o\", color=\"red\")\n(line2,) = plt.plot(fcast2, marker=\"o\", color=\"red\")\nplt.plot(fit3.fittedvalues, marker=\"o\", color=\"green\")\n(line3,) = plt.plot(fcast3, marker=\"o\", color=\"green\")\nplt.legend([line1, line2, line3], [fcast1.name, fcast2.name, fcast3.name])",
            "from sklearn.model_selection import \nfrom sklearn.svm import \n\n# from sklearn.metrics import plot_roc_curve\nfrom sklearn.metrics import RocCurveDisplay\n\nfrom sklearn.ensemble import \nfrom sklearn.datasets import \nimport matplotlib.pyplot as plt\n\nX, y = (random_state=0)\nX_train, X_test, y_train, y_test = (X, y, random_state=42)\n\nsvc = (random_state=42)\nsvc.fit(X_train, y_train)\nrfc = (random_state=42)\nrfc.fit(X_train, y_train)\n\n# plot_roc_curve has been removed in version 1.2. From 1.2, use RocCurveDisplay instead.\n# svc_disp = plot_roc_curve(svc, X_test, y_test)\n# rfc_disp = plot_roc_curve(rfc, X_test, y_test, ax=svc_disp.ax_)\nsvc_disp = (svc, X_test, y_test)\nrfc_disp = (rfc, X_test, y_test, ax=svc_disp.ax_)\nrfc_disp.figure_.suptitle(\"ROC curve comparison\")\n\n()\n\n\n<img alt=\"ROC curve comparison\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_release_highlights_0_22_0_001.png\" srcset=\"../../_images/sphx_glr_plot_release_highlights_0_22_0_001.png\"/>",
            "qat_model = load_model(saved_model_dir + float_model_file)\nqat_model.fuse_model()\n\noptimizer = torch.optim.SGD(qat_model.parameters(), lr = 0.0001)\n# The old 'fbgemm' is still available but 'x86' is the recommended default.\nqat_model.qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Image and Video->Optimizing Vision Transformer Model for Deployment->Comparing Inference Speed",
                "torch->Model Optimization->Optimizing Vision Transformer Model for Deployment->Comparing Inference Speed"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Lasso and Elastic Net for Sparse Signals->Plot"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Simple Exponential Smoothing"
            ],
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 0.22->New plotting API"
            ],
            [
                "torch->Model Optimization->(beta) Static Quantization with Eager Mode in PyTorch->5. Quantization-aware training"
            ]
        ]
    },
    "995131": {
        "jupyter_code_cell": "ax = sns.barplot(x='Month',y='RSPM/PM10',data=nagaon_df).set_title('Mean RSPM/PM10 level in Nagaon')\nsns.barplot(x='Month',y='RSPM/PM10',data=df[df['City/Town/Village/Area']=='Bongaigaon']).set_title('Mean RSPM/PM10 level in Bonaigaon')",
        "matched_tutorial_code_inds": [
            6403,
            4665,
            3853,
            3855,
            2930
        ],
        "matched_tutorial_codes": [
            "ax = res.impulse_responses(10, orthogonalized=True, impulse=[1, 0]).plot(figsize=(13,3))\nax.set(xlabel='t', title='Responses to a shock to `dln_inv`');\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_varmax_8_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_varmax_8_0.png\"/>",
            "ax = plt.subplot()\n\nt = np.arange(0.0, 5.0, 0.01)\ns = np.cos(2*np.pi*t)\nline, = plt.plot(t, s, lw=2)\n\nplt.annotate('local max', xy=(2, 1), xytext=(3, 1.5),\n             arrowprops=dict(facecolor='black', shrink=0.05),\n             )\n\nplt.ylim(-2, 2)\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_009.png\" srcset=\"../../_images/sphx_glr_pyplot_009.png, ../../_images/sphx_glr_pyplot_009_2_0x.png 2.0x\"/>",
            "ax = y.plot(label='observed')\npred.predicted_mean.plot(ax=ax, label='Forecast', alpha=.7)\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.2)\nax.set_ylabel(\"Monthly Flights\")\nplt.legend()\nsns.despine()",
            "ax = y.plot(label='observed')\npred_dy.predicted_mean.plot(ax=ax, label='Forecast')\nax.fill_between(pred_dy_ci.index,\n                pred_dy_ci.iloc[:, 0],\n                pred_dy_ci.iloc[:, 1], color='k', alpha=.25)\nax.set_ylabel(\"Monthly Flights\")\n\n# Highlight the forecast area\nax.fill_betweenx(ax.get_ylim(), pd.Timestamp('2013-01-01'), y.index[-1],\n                 alpha=.1, zorder=-1)\nax.annotate('Dynamic $\\\\longrightarrow$', (pd.Timestamp('2013-02-01'), 550))\n\nplt.legend()\nsns.despine()",
            "ax = mdi_importances.plot.barh()\nax.set_title(\"Random Forest Feature Importances (MDI)\")\nax.figure.tight_layout()\n\n\n<img alt=\"Random Forest Feature Importances (MDI)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_permutation_importance_001.png\" srcset=\"../../_images/sphx_glr_plot_permutation_importance_001.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->VARMAX: Introduction->Example 1: VAR"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Working with text->Annotating text"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Forecasting"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Forecasting"
            ],
            [
                "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)"
            ]
        ]
    },
    "715594": {
        "jupyter_code_cell": "ax = df_count.iloc[4894:9788].rolling(48).mean().plot()\nplt.show()\nax = df_count.rolling(96).mean().plot()\nplt.show()",
        "matched_tutorial_code_inds": [
            5470,
            5703,
            3616,
            3771,
            3827
        ],
        "matched_tutorial_codes": [
            "means75 = exog.mean()\nmeans75[\"LOWINC\"] = exog[\"LOWINC\"].quantile(0.75)\nprint(means75)",
            "summ_df = infl.summary_frame()\nsumm_df.sort_values(\"cooks_d\", ascending=False)[:10]",
            "hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()",
            "win_percent.sort_values().plot.barh(figsize=(6, 12), width=.85, color='k')\nplt.tight_layout()\nsns.despine()\nplt.xlabel(\"Win Percent\")",
            "daily = df.fl_date.value_counts().sort_index()\ny = daily.resample('MS').mean()\ny.head()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Influence Measures for GLMs->get the influence measures"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ]
        ]
    },
    "1054586": {
        "jupyter_code_cell": "corr_parch_sur = cdf['Parch'].corr(cdf['Survived'])\ncorr_parch_sur\nsns.distplot(cdf['Fare'], bins=5, kde=False)\nplt.show()",
        "matched_tutorial_code_inds": [
            3829,
            3702,
            4142,
            6628,
            6704
        ],
        "matched_tutorial_codes": [
            "ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "g = sns.PairGrid(iris)\ng.map_upper(sns.scatterplot)\ng.map_lower(sns.kdeplot)\ng.map_diag(sns.kdeplot, lw=3, legend=False)\n",
            "mod = TVRegression(y_t, x_t, w_t)\nres_mle = mod.fit(disp=False)\nprint(res_mle.summary())",
            "ax = dta.mean().plot(grid=False)\nax.set_xlabel(\"Year\", size=17)\nax.set_ylabel(\"Fertility rate\", size=17)\nax.set_xlim(0, 51)"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships"
            ],
            [
                "statsmodels->Examples->State space models->Statespace: Custom Models->Bonus: pymc3 for fast Bayesian estimation"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ]
        ]
    },
    "969048": {
        "jupyter_code_cell": "for c in ['Gold', 'Silver', 'Bronze', 'Total']:\n    dfn[c]=dfn[c].astype(int)",
        "matched_tutorial_code_inds": [
            3947,
            3829,
            3951,
            3831,
            3743
        ],
        "matched_tutorial_codes": [
            "flights_wide_list = [col for _, col in flights_wide.items()]\nsns.relplot(data=flights_wide_list, kind=\"line\")\n",
            "ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "flights_array = flights_wide.to_numpy()\nsns.relplot(data=flights_array, kind=\"line\")\n",
            "X = (pd.concat([y.shift(i) for i in range(6)], axis=1,\n               keys=['y'] + ['L%s' % i for i in range(1, 6)])\n       .dropna())\nX.head()",
            "c = s.astype('category')\nprint('{:0.2f} KB'.format(c.memory_usage(index=False) / 1000))"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing wide-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing wide-form data"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing wide-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing wide-form data"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Categoricals"
            ]
        ]
    },
    "1514550": {
        "jupyter_code_cell": "g = sns.lmplot(x='x', y='y', data=df, fit_reg=True  \n               ,size=6, scatter_kws={'alpha':0.8, 's':60})\nimport pymc3 as pm  \nfrom scipy.optimize import fmin_powell",
        "matched_tutorial_code_inds": [
            4146,
            6089,
            4082,
            6900,
            4003
        ],
        "matched_tutorial_codes": [
            "g = sns.pairplot(iris, hue=\"species\", palette=\"Set2\", diag_kind=\"kde\", height=2.5)\n",
            "fig = res_glob.plot_predict(start=714, end=732)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_40_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_40_0.png\"/>",
            "g = sns.catplot(data=tips, x=\"day\", y=\"total_bill\", kind=\"violin\", inner=None)\nsns.swarmplot(data=tips, x=\"day\", y=\"total_bill\", color=\"k\", size=3, ax=g.ax)\n",
            "fig = pandas_ar_res.plot_predict(start=\"2005\", end=\"2027\")\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_tsa_dates_12_0.png\" src=\"../../../_images/examples_notebooks_generated_tsa_dates_12_0.png\"/>",
            "fmri = sns.load_dataset(\"fmri\")\nsns.relplot(data=fmri, x=\"timepoint\", y=\"signal\", kind=\"line\")\n"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions->Industrial Production"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing categorical data->Comparing distributions->Violinplots",
                "seaborn->Plotting functions->Visualizing categorical data->Comparing distributions->Violinplots"
            ],
            [
                "statsmodels->Examples->User Notes->Dates in Time-Series Models->Using Pandas"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty",
                "seaborn->Plotting functions->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty"
            ]
        ]
    },
    "87548": {
        "jupyter_code_cell": "noshowappointments.dtypes\nnoshowappointments['PatientId'] = str(noshowappointments['PatientId'])\nnoshowappointments['AppointmentID'] = str(noshowappointments['AppointmentID'])",
        "matched_tutorial_code_inds": [
            5381,
            5388,
            4138,
            4139,
            5423
        ],
        "matched_tutorial_codes": [
            "spector_data = sm.datasets.spector.load()\nspector_data.exog = sm.add_constant(spector_data.exog, prepend=False)",
            "margeff = logit_res.get_margeff()\nprint(margeff.summary())",
            "iris = sns.load_dataset(\"iris\")\ng = sns.PairGrid(iris)\ng.map(sns.scatterplot)\n",
            "g = sns.PairGrid(iris)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n",
            "mfx = affair_mod.get_margeff()\nprint(mfx.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Data"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Logit Model"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data"
            ]
        ]
    },
    "1243566": {
        "jupyter_code_cell": "x = np.arange(0.1, 5.0, 0.1)\nbeta0=0\nbeta1=1\nbeta2=0   \nbeta3=0  \nX = beta0 + beta1*x + beta2*(x**2)+ beta3*(x**3)\ny = np.log(X)\nplt.plot(x,y) \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\nimport pandas as pd\ndf = pd.read_csv(\"https://ibm.box.com/shared/static/7tr6tai74kdk3vik815k2frl54kfuw4h.csv\")\ndf.head()",
        "matched_tutorial_code_inds": [
            5225,
            4632,
            2753,
            93,
            2741
        ],
        "matched_tutorial_codes": [
            "x = np.arange(data.income.min(), data.income.max(), 50)\nget_y = lambda a, b: a + b * x\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nfor i in range(models.shape[0]):\n    y = get_y(models.a[i], models.b[i])\n    ax.plot(x, y, linestyle=\"dotted\", color=\"grey\")\n\ny = get_y(ols[\"a\"], ols[\"b\"])\n\nax.plot(x, y, color=\"red\", label=\"OLS\")\nax.scatter(data.income, data.foodexp, alpha=0.2)\nax.set_xlim((240, 3000))\nax.set_ylim((240, 2000))\nlegend = ax.legend()\nax.set_xlabel(\"Income\", fontsize=16)\nax.set_ylabel(\"Food expenditure\", fontsize=16)",
            "x = np.linspace(0, 2, 100)  # Sample data.\n\nplt.figure(figsize=(5, 2.7), layout='constrained')\nplt.plot(x, x, label='linear')  # Plot some data on the (implicit) axes.\nplt.plot(x, x**2, label='quadratic')  # etc.\nplt.plot(x, x**3, label='cubic')\nplt.xlabel('x label')\nplt.ylabel('y label')\nplt.title(\"Simple Plot\")\nplt.legend()\n\n\n<img alt=\"Simple Plot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_quick_start_004.png\" srcset=\"../../_images/sphx_glr_quick_start_004.png, ../../_images/sphx_glr_quick_start_004_2_0x.png 2.0x\"/>",
            "quantiles = [0.05, 0.5, 0.95]\npredictions = {}\nout_bounds_predictions = (y_true_mean, dtype=)\nfor quantile in quantiles:\n    qr = (quantile=quantile, alpha=0, solver=solver)\n    y_pred = qr.fit(X, y_pareto).predict(X)\n    predictions[quantile] = y_pred\n\n    if quantile == min(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred = y_pareto\n        )\n    elif quantile == max(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred &lt;= y_pareto\n        )",
            "x = (-5, 5, 0.1).view(-1, 1)\ny = -5 * x + 0.1 * (x.size())\n\nmodel = (1, 1)\ncriterion = ()\noptimizer = (model.parameters(), lr = 0.1)\n\ndef train_model(iter):\n    for epoch in range(iter):\n        y1 = model(x)\n        loss = criterion(y1, y)\n        writer.add_scalar(\"Loss/train\", loss, epoch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\ntrain_model(10)\nwriter.flush()",
            "x_train = (0, 10, 100)\nrng = (0)\nx_train = (rng.choice(x_train, size=20, replace=False))\ny_train = f(x_train)\n\n# create 2D-array versions of these arrays to feed to transformers\nX_train = x_train[:, ]\nX_plot = x_plot[:, ]"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Quantile Regression->Visualizing the results->First plot"
            ],
            [
                "matplotlib->Tutorials->Introductory->Quick start guide->Coding styles->The explicit and the implicit interfaces"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Quantile regression->Fitting a QuantileRegressor"
            ],
            [
                "torch->PyTorch Recipes->How to use TensorBoard with PyTorch->Log scalars"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Polynomial and Spline interpolation"
            ]
        ]
    },
    "110251": {
        "jupyter_code_cell": "df1.describe(include=['object'])\ndf1.columns",
        "matched_tutorial_code_inds": [
            6653,
            3613,
            3425,
            5909,
            5123
        ],
        "matched_tutorial_codes": [
            "df = pred.summary_frame(alpha=0.05)\ndf",
            "f = pd.DataFrame({'a':[1,2,3,4,5], 'b':[10,20,30,40,50]})\nf",
            "y_train = (y)\ny_train[unlabeled_set] = -1",
            "interM_lm.model.exog\ninterM_lm.model.exog_names",
            "sf.quantile(0.25)\nsf.quantile_ci(0.25)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->ETS models->Predictions"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->SettingWithCopy"
            ],
            [
                "sklearn->Examples->Semi Supervised Classification->Label Propagation digits: Demonstrating performance->Data generation"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "statsmodels->User Guide->Other Models->Methods for Survival and Duration Analysis->Survival function estimation and inference"
            ]
        ]
    },
    "665484": {
        "jupyter_code_cell": "earthquakes2_geo = earthquakes2_gdf[\"geometry\"]\nlen(earthquakes2_geo)\nboundary_distances2 = pd.read_csv(\"MR_Data/Earthquake_boundary_distances2\")\nboundary_distances2 = boundary_distances2.drop(\"Unnamed: 0\", axis = 1)",
        "matched_tutorial_code_inds": [
            3703,
            5470,
            6052,
            3912,
            3923
        ],
        "matched_tutorial_codes": [
            "df = pd.read_csv(\"data/347136217_T_ONTIME.csv\")\ndelays = df['DEP_DELAY']",
            "means75 = exog.mean()\nmeans75[\"LOWINC\"] = exog[\"LOWINC\"].quantile(0.75)\nprint(means75)",
            "sample = copula.rvs(10000)\nh = sns.jointplot(x=sample[:, 0], y=sample[:, 1], kind=\"hex\")\n_ = h.set_axis_labels(\"X1\", \"X2\", fontsize=16)",
            "penguins = sns.load_dataset(\"penguins\")\nsns.jointplot(data=penguins, x=\"flipper_length_mm\", y=\"bill_length_mm\", hue=\"species\")\n",
            "tips = sns.load_dataset(\"tips\")\ng = sns.relplot(data=tips, x=\"total_bill\", y=\"tip\")\ng.ax.axline(xy1=(10, 2), slope=.2, color=\"b\", dashes=(5, 2))\n"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution"
            ],
            [
                "statsmodels->Examples->Statistics->Copulas->Sampling from a copula"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->Multivariate views on complex datasets"
            ],
            [
                "seaborn->User guide and tutorial->Overview of seaborn plotting functions->Figure-level vs. axes-level functions->Figure-level functions own their figure",
                "seaborn->API Overview->Overview of seaborn plotting functions->Figure-level vs. axes-level functions->Figure-level functions own their figure"
            ]
        ]
    },
    "102825": {
        "jupyter_code_cell": "import pandas as pd\nimport sys\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nsys.path.append('../utils')\nimport DataAggregation as da\nimport AlgoUtils as au\ncmap_bold = ListedColormap(['#00FF00','#FF0000'])",
        "matched_tutorial_code_inds": [
            2946,
            852,
            3243,
            1602,
            2869
        ],
        "matched_tutorial_codes": [
            "from collections import \n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import \nfrom scipy.cluster import hierarchy\nfrom scipy.spatial.distance import \n\nfrom sklearn.datasets import \nfrom sklearn.ensemble import \nfrom sklearn.inspection import \nfrom sklearn.model_selection import",
            "import torch\n\nX = torch.randn(batch_size, input_features)\nh = torch.randn(batch_size, state_size)\nC = torch.randn(batch_size, state_size)\n\nrnn = LLTM(input_features, state_size)\n\nnew_h, new_C = rnn(X, (h, C))",
            "import pandas as pd\nfrom sklearn import datasets\nimport matplotlib.pyplot as plt\nfrom scipy.stats import \nimport numpy as np\n\nfrom sklearn.experimental import enable_halving_search_cv  # noqa\nfrom sklearn.model_selection import \nfrom sklearn.ensemble import",
            "import numpy as np\nnum_imgs = 9\n\ncombined_xray_images_1 = np.array(\n    [imageio.v3.imread(os.path.join(DIR, f\"00000011_00{i}.png\")) for i in range(num_imgs)]\n)",
            "from sklearn.model_selection import \n\ntarget_name = \"hourly wage\"\nX, y = df.drop(columns=target_name), df[target_name]\nX_train, X_test, y_train, y_test = (X, y, test_size=0.2, random_state=0)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Inspection->Permutation Importance with Multicollinear or Correlated Features"
            ],
            [
                "torch->Extending PyTorch->Custom C++ and CUDA Extensions->Motivation and Example"
            ],
            [
                "sklearn->Examples->Model Selection->Successive Halving Iterations"
            ],
            [
                "numpy->NumPy Applications->X-ray image processing->Combine images into a multidimensional array to demonstrate progression"
            ],
            [
                "sklearn->Examples->Inspection->Failure of Machine Learning to infer causal effects->Description of the simulated data"
            ]
        ]
    },
    "451367": {
        "jupyter_code_cell": "df_jdbc['timestamp'] = dates.sample(len(df_jdbc), replace=True).sort_values().reset_index(drop=True)\ndf_jdbc = df_jdbc.sort_index()\ndf_jdbc.head()\ndf_jdbc.loc[0, 'lines'] = 250\ndf_jdbc.head()",
        "matched_tutorial_code_inds": [
            3676,
            1698,
            161,
            4129,
            3802
        ],
        "matched_tutorial_codes": [
            "m = pd.merge(flights, daily.reset_index().rename(columns={'date': 'fl_date', 'station': 'origin'}),\n             on=['fl_date', 'origin']).set_index(idx_cols).sort_index()\n\nm.head()",
            "rng = default_rng()\n\nbefore_sample = rng.choice(before_lock, size=30, replace=False)\nafter_sample = rng.choice(after_lock, size=30, replace=False)",
            "compare.trim_significant_figures()\ncompare.colorize()\ncompare.print()",
            "ordered_days = tips.day.value_counts().index\ng = sns.FacetGrid(tips, row=\"day\", row_order=ordered_days,\n                  height=1.7, aspect=4,)\ng.map(sns.kdeplot, \"total_bill\")\n",
            "scores = pd.DataFrame(est.cv_results_)\nscores.head()"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Indexes->Merging->The merge version"
            ],
            [
                "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Paired Student\u2019s t-test on the AQIs->Sampling"
            ],
            [
                "torch->PyTorch Recipes->PyTorch Benchmark->Steps->5. Comparing benchmark results"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Conditional small multiples",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Conditional small multiples"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ]
        ]
    },
    "185810": {
        "jupyter_code_cell": "sns.factorplot(x='year', y='avg_wage', hue='year', data=ein_empl_lim, kind='box')\nplt.show()\nfacet_histograms = sns.FacetGrid(ein_empl_lim, col='year', hue='year')\nfacet_histograms = facet_histograms.map(plt.hist, 'avg_wage')\nplt.annotate('Source: MO Department of Labor', xy=(0.6,-0.35), xycoords=\"axes fraction\")\nplt.show()",
        "matched_tutorial_code_inds": [
            3916,
            4087,
            3813,
            4145,
            4021
        ],
        "matched_tutorial_codes": [
            "sns.set_theme(style=\"ticks\", font_scale=1.25)\ng = sns.relplot(\n    data=penguins,\n    x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"body_mass_g\",\n    palette=\"crest\", marker=\"x\", s=100,\n)\ng.set_axis_labels(\"Bill length (mm)\", \"Bill depth (mm)\", labelpad=10)\ng.legend.set_title(\"Body mass (g)\")\ng.figure.set_size_inches(6.5, 4.5)\ng.ax.margins(.15)\ng.despine(trim=True)\n",
            "sns.catplot(data=titanic, x=\"sex\", y=\"survived\", hue=\"class\", kind=\"point\")\n",
            "gs.Close.plot(label='Raw')\ngs.Close.rolling(28).mean().plot(label='28D MA')\ngs.Close.expanding().mean().plot(label='Expanding Average')\ngs.Close.ewm(alpha=0.03).mean().plot(label='EWMA($\\\\alpha=.03$)')\n\nplt.legend(bbox_to_anchor=(1.25, .5))\nplt.tight_layout()\nplt.ylabel(\"Close ($)\")\nsns.despine()",
            "sns.pairplot(iris, hue=\"species\", height=2.5)\n",
            "sns.relplot(\n    data=fmri.query(\"region == 'frontal'\"), kind=\"line\",\n    x=\"timepoint\", y=\"signal\", hue=\"event\", style=\"event\",\n    col=\"subject\", col_wrap=5,\n    height=3, aspect=.75, linewidth=2.5,\n)\n"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->Opinionated defaults and flexible customization"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing categorical data->Estimating central tendency->Point plots",
                "seaborn->Plotting functions->Visualizing categorical data->Estimating central tendency->Point plots"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Special Methods->Rolling / Expanding / EW"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Showing multiple relationships with facets",
                "seaborn->Plotting functions->Visualizing statistical relationships->Showing multiple relationships with facets"
            ]
        ]
    },
    "609621": {
        "jupyter_code_cell": "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import log_loss\nimport random\nselected_vars  = [\"bathrooms\", \"bedrooms\", \"price\", \"num_features\", \"num_key_words_description\",\n                   \"dayofyear\", \"weekofyear\", \"weekday\", \"hour\", \"num_photos\", \"latitude\", \"longitude\",\n                    \"building_id\", \"price_diff\"]\nX = train_df[selected_vars]\ny = train_df[\"interest_level\"]\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33)\nrandom.seed(123)\nclf = RandomForestClassifier(n_estimators=1000)\nresult  = clf.fit(X_train[selected_vars], y_train)\ny_val_pred = clf.predict_proba(X_val[selected_vars])\nlog_loss(y_val, y_val_pred)\nselected_vars = np.array(selected_vars)\nimportances = result.feature_importances_\nimportant_names = selected_vars[importances > np.mean(importances)]\nprint (important_names)",
        "matched_tutorial_code_inds": [
            6284,
            3377,
            2611,
            2917,
            2870
        ],
        "matched_tutorial_codes": [
            "from statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.forecasting.stl import STLForecast\n\nelec_equip.index.freq = elec_equip.index.inferred_freq\nstlf = STLForecast(elec_equip, ARIMA, model_kwargs=dict(order=(1, 1, 0), trend=\"t\"))\nstlf_res = stlf.fit()\n\nforecast = stlf_res.forecast(24)\nplt.plot(elec_equip)\nplt.plot(forecast)\nplt.show()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_stl_decomposition_21_0.png\" src=\"../../../_images/examples_notebooks_generated_stl_decomposition_21_0.png\"/>",
            "from sklearn.datasets import \nfrom sklearn.preprocessing import \n\names = (name=\"house_prices\", as_frame=True, parser=\"pandas\")\n# Keep only numeric columns\nX = ames.data.select_dtypes()\n# Remove columns with NaN or Inf values\nX = X.drop(columns=[\"LotFrontage\", \"GarageYrBlt\", \"MasVnrArea\"])\n# Let the price be in k$\ny = ames.target / 1000\ny_trans = (\n    y.to_frame(), n_quantiles=900, output_distribution=\"normal\", copy=True\n).squeeze()",
            "from sklearn.gaussian_process import \nfrom sklearn.gaussian_process.kernels import \n\nkernel = 1.0 * (length_scale=1.0, length_scale_bounds=(1e-1, 10.0))\ngpr = (kernel=kernel, random_state=0)\n\nfig, axs = (nrows=2, sharex=True, sharey=True, figsize=(10, 8))\n\n# plot prior\nplot_gpr_samples(gpr, n_samples=n_samples, ax=axs[0])\naxs[0].set_title(\"Samples from prior distribution\")\n\n# plot posterior\ngpr.fit(X_train, y_train)\nplot_gpr_samples(gpr, n_samples=n_samples, ax=axs[1])\naxs[1].scatter(X_train[:, 0], y_train, color=\"red\", zorder=10, label=\"Observations\")\naxs[1].legend(bbox_to_anchor=(1.05, 1.5), loc=\"upper left\")\naxs[1].set_title(\"Samples from posterior distribution\")\n\nfig.suptitle(\"Radial Basis Function kernel\", fontsize=18)\n()\n\n\n<img alt=\"Radial Basis Function kernel, Samples from prior distribution, Samples from posterior distribution\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_prior_posterior_001.png\" srcset=\"../../_images/sphx_glr_plot_gpr_prior_posterior_001.png\"/>",
            "from sklearn.datasets import \nfrom sklearn.model_selection import \n\nX, y = (\n    \"titanic\", version=1, as_frame=True, return_X_y=True, parser=\"pandas\"\n)\nrng = (seed=42)\nX[\"random_cat\"] = rng.randint(3, size=X.shape[0])\nX[\"random_num\"] = rng.randn(X.shape[0])\n\ncategorical_columns = [\"pclass\", \"sex\", \"embarked\", \"random_cat\"]\nnumerical_columns = [\"age\", \"sibsp\", \"parch\", \"fare\", \"random_num\"]\n\nX = X[categorical_columns + numerical_columns]\nX_train, X_test, y_train, y_test = (X, y, stratify=y, random_state=42)",
            "from sklearn.linear_model import \nfrom sklearn.metrics import \n\nfeatures_names = [\"experience\", \"parent hourly wage\", \"college degree\", \"ability\"]\n\nregressor_with_ability = ()\nregressor_with_ability.fit(X_train[features_names], y_train)\ny_pred_with_ability = regressor_with_ability.predict(X_test[features_names])\nR2_with_ability = (y_test, y_pred_with_ability)\n\nprint(f\"R2 score with ability: {R2_with_ability:.3f}\")"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition->Forecasting with STL"
            ],
            [
                "sklearn->Examples->Pipelines and composite estimators->Effect of transforming the targets in regression model->Real-world data set"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Illustration of prior and posterior Gaussian process for different kernels->Kernel cookbook->Radial Basis Function kernel"
            ],
            [
                "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Data Loading and Feature Engineering"
            ],
            [
                "sklearn->Examples->Inspection->Failure of Machine Learning to infer causal effects->Income prediction with fully observed variables"
            ]
        ]
    },
    "432689": {
        "jupyter_code_cell": "freezing_days = data[data.max_temp <= 32]\nfreezing_days.info()\ncold_days = freezing_days[freezing_days.min_temp >= 20]\ncold_days.info()",
        "matched_tutorial_code_inds": [
            3660,
            6194,
            5470,
            5296,
            5703
        ],
        "matched_tutorial_codes": [
            "dsm = weather.loc['DSM']\n\nhourly = dsm.resample('H').mean()\n\ntemp = hourly['tmpf'].sample(frac=.5, random_state=1).sort_index()\nsped = hourly['sped'].sample(frac=.5, random_state=2).sort_index()",
            "gdp_decomp = dta[[\"realgdp\"]].copy()\ngdp_decomp[\"cycle\"] = gdp_cycle\ngdp_decomp[\"trend\"] = gdp_trend",
            "means75 = exog.mean()\nmeans75[\"LOWINC\"] = exog[\"LOWINC\"].quantile(0.75)\nprint(means75)",
            "pred_ols = res_ols.get_prediction()\niv_l_ols = pred_ols.summary_frame()[\"obs_ci_lower\"]\niv_u_ols = pred_ols.summary_frame()[\"obs_ci_upper\"]",
            "summ_df = infl.summary_frame()\nsumm_df.sort_values(\"cooks_d\", ascending=False)[:10]"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Indexes->Flavors->Indexes for Alignment"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Time Series Filters->Hodrick-Prescott Filter"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Weighted Least Squares->OLS vs.\u00a0WLS"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Influence Measures for GLMs->get the influence measures"
            ]
        ]
    },
    "135317": {
        "jupyter_code_cell": "dfWAR\ndfWAR.Nationality.value_counts()",
        "matched_tutorial_code_inds": [
            3935,
            657,
            5909,
            4229,
            2122
        ],
        "matched_tutorial_codes": [
            "sns.relplot(data=flights_wide, kind=\"line\")\n",
            "traced_rn18 = (rn18)\nprint()",
            "interM_lm.model.exog\ninterM_lm.model.exog_names",
            "y2 = f2(x)\n I2 = integrate.simpson(y2, x)\n print(I2)\n61.5",
            "data = ()\nX = ().fit_transform(data[\"data\"])\nfeature_names = data[\"feature_names\"]"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data"
            ],
            [
                "torch->Code Transforms with FX->(beta) Building a Simple CPU Performance Profiler with FX->Capturing the Model with Symbolic Tracing"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "scipy->Integration (scipy.integrate)->Integrating using Samples"
            ],
            [
                "sklearn->Examples->Decomposition->Factor Analysis (with rotation) to visualize patterns"
            ]
        ]
    },
    "1161768": {
        "jupyter_code_cell": "df = pd.read_csv('pandas_dataframe_importing_csv/example.csv')\ndf\ndf = pd.read_csv('pandas_dataframe_importing_csv/example.csv', header=None)\ndf",
        "matched_tutorial_code_inds": [
            3784,
            1180,
            660,
            3202,
            4124
        ],
        "matched_tutorial_codes": [
            "import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "try:\n    (f3)\nexcept:\n    tb.print_exc()\n\ntry:\n    (f3)\nexcept:\n    tb.print_exc()",
            "interp = (rn18)\n(input)\nprint(interp.summary(True))",
            "from sklearn.feature_extraction.text import \nfrom sklearn.naive_bayes import \nfrom sklearn.pipeline import \n\npipeline = (\n    [\n        (\"vect\", ()),\n        (\"clf\", ()),\n    ]\n)\npipeline",
            "tips = sns.load_dataset(\"tips\")\ng = sns.FacetGrid(tips, col=\"time\")\n"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples"
            ],
            [
                "torch->Model Optimization->torch.compile Tutorial->Comparison to TorchScript and FX Tracing"
            ],
            [
                "torch->Code Transforms with FX->(beta) Building a Simple CPU Performance Profiler with FX->Investigating the Performance of ResNet18"
            ],
            [
                "sklearn->Examples->Model Selection->Sample pipeline for text feature extraction and evaluation->Pipeline with hyperparameter tuning"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Conditional small multiples",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Conditional small multiples"
            ]
        ]
    },
    "1409703": {
        "jupyter_code_cell": "data = pd.read_table(\"../data/WISDM/WISDM_ar_v1.1_raw_cleared.txt\", delimiter=',', header=None)\ndata.columns = ['id_user', 'activity', 'timestamp', 'x', 'y', 'z']\ndata.head()\nclasses = list(set(data['activity']))\nfor activity in classes:\n    nb = np.sum(data['activity'] == activity)\n    print(\"{:<15}{:<9d}{:<5.2f} %\".format(activity, nb, 100. * nb / data.shape[0]))\nprint()\nprint(\"Number of objects: {:d}\".format(data.shape[0]))",
        "matched_tutorial_code_inds": [
            6081,
            6060,
            5963,
            6901,
            4654
        ],
        "matched_tutorial_codes": [
            "data = pdr.get_data_fred(\"INDPRO\", \"1959-01-01\", \"2019-06-01\")\nind_prod = data.INDPRO.pct_change(12).dropna().asfreq(\"MS\")\n_, ax = plt.subplots(figsize=(16, 9))\nind_prod.plot(ax=ax)",
            "data = pdr.get_data_fred(\"HOUSTNSA\", \"1959-01-01\", \"2019-06-01\")\nhousing = data.HOUSTNSA.pct_change().dropna()\n# Scale by 100 to get percentages\nhousing = 100 * housing.asfreq(\"MS\")\nfig, ax = plt.subplots()\nax = housing.plot(ax=ax)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_6_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_6_0.png\"/>",
            "data = [\n    [\"Carroll\", 94, 22, 60, 92, 20, 60],\n    [\"Grant\", 98, 21, 65, 92, 22, 65],\n    [\"Peck\", 98, 28, 40, 88, 26, 40],\n    [\"Donat\", 94, 19, 200, 82, 17, 200],\n    [\"Stewart\", 98, 21, 50, 88, 22, 45],\n    [\"Young\", 96, 21, 85, 92, 22, 85],\n]\ncolnames = [\"study\", \"mean_t\", \"sd_t\", \"n_t\", \"mean_c\", \"sd_c\", \"n_c\"]\nrownames = [i[0] for i in data]\ndframe1 = pd.DataFrame(data, columns=colnames)\nrownames",
            "data = \"\"\"\n  x   y y_err\n201 592    61\n244 401    25\n 47 583    38\n287 402    15\n203 495    21\n 58 173    15\n210 479    27\n202 504    14\n198 510    30\n158 416    16\n165 393    14\n201 442    25\n157 317    52\n131 311    16\n166 400    34\n160 337    31\n186 423    42\n125 334    26\n218 533    16\n146 344    22\n\"\"\"\ntry:\n    from StringIO import StringIO\nexcept ImportError:\n    from io import StringIO\ndata = pd.read_csv(StringIO(data), delim_whitespace=True).astype(float)\n\n# Note: for the results we compare with the paper here, they drop the first four points\ndata.head()",
            "data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions->Industrial Production"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Example"
            ],
            [
                "statsmodels->Examples->User Notes->Least squares fitting of models to data->Linear models"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings"
            ]
        ]
    },
    "850244": {
        "jupyter_code_cell": "fig = plt.figure(1)\nax1 = fig.add_subplot(111)\nax1.set_xlabel('Ticks')\nax1.set_ylabel('Price')\nax1.set_title('Original Plot')\nax1.plot('Ticks', 'Open', data = google);\none_tenth = google.sample(frac = .1, random_state=np.random.randint(10))",
        "matched_tutorial_code_inds": [
            4800,
            5538,
            5856,
            5789,
            4965
        ],
        "matched_tutorial_codes": [
            "fig = plt.figure(layout=\"constrained\")\nsfigs = fig.subfigures(1, 2, width_ratios=[1, 2])\n\naxs_left = sfigs[0].subplots(2, 1)\nfor ax in axs_left.flat:\n    example_plot(ax)\n\naxs_right = sfigs[1].subplots(2, 2)\nfor ax in axs_right.flat:\n    pcm = ax.pcolormesh(arr, **pc_kwargs)\n    ax.set_xlabel('x-label')\n    ax.set_ylabel('y-label')\n    ax.set_title('title')\nfig.colorbar(pcm, ax=axs_right)\nfig.suptitle('Nested plots using subfigures')\n\n\n<img alt=\"Nested plots using subfigures, Title, Title, title, title, title, title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_constrainedlayout_guide_023.png\" srcset=\"../../_images/sphx_glr_constrainedlayout_guide_023.png, ../../_images/sphx_glr_constrainedlayout_guide_023_2_0x.png 2.0x\"/>",
            "fig = plt.figure(figsize=(12, 5))\nax = fig.add_subplot(111)\n\n# Scatter plot of data samples and histogram\nax.scatter(\n    obs_dist,\n    np.abs(np.random.randn(obs_dist.size)),\n    zorder=15,\n    color=\"red\",\n    marker=\"x\",\n    alpha=0.5,\n    label=\"Samples\",\n)\nlines = ax.hist(obs_dist, bins=20, edgecolor=\"k\", label=\"Histogram\")\n\nax.legend(loc=\"best\")\nax.grid(True, zorder=-5)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_kernel_density_7_0.png\" src=\"../../../_images/examples_notebooks_generated_kernel_density_7_0.png\"/>",
            "fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nax.plot(x1, y2, \"o\", label=\"data\")\nax.plot(x1, y_true2, \"b-\", label=\"True\")\npred_ols = res.get_prediction()\niv_l = pred_ols.summary_frame()[\"obs_ci_lower\"]\niv_u = pred_ols.summary_frame()[\"obs_ci_upper\"]\n\nax.plot(x1, res.fittedvalues, \"r-\", label=\"OLS\")\nax.plot(x1, iv_u, \"r--\")\nax.plot(x1, iv_l, \"r--\")\nax.plot(x1, resrlm.fittedvalues, \"g.-\", label=\"RLM\")\nax.legend(loc=\"best\")",
            "fig = plt.figure(figsize=(12, 12))\nax1 = fig.add_subplot(211, xlabel=\"Income\", ylabel=\"Prestige\")\nax1.scatter(prestige.income, prestige.prestige)\nxy_outlier = prestige.loc[\"minister\", [\"income\", \"prestige\"]]\nax1.annotate(\"Minister\", xy_outlier, xy_outlier + 1, fontsize=16)\nax2 = fig.add_subplot(212, xlabel=\"Education\", ylabel=\"Prestige\")\nax2.scatter(prestige.education, prestige.prestige)",
            "fig = plt.figure()\nax = fig.add_subplot(projection='polar')\nr = np.arange(0, 1, 0.001)\ntheta = 2 * 2*np.pi * r\nline, = ax.plot(theta, r, color='#ee8d18', lw=3)\n\nind = 800\nthisr, thistheta = r[ind], theta[ind]\nax.plot([thistheta], [thisr], 'o')\nax.annotate('a polar annotation',\n            xy=(thistheta, thisr),  # theta, radius\n            xytext=(0.05, 0.05),    # fraction, fraction\n            textcoords='figure fraction',\n            arrowprops=dict(facecolor='black', shrink=0.05),\n            horizontalalignment='left',\n            verticalalignment='bottom')\n\n\n<img alt=\"annotations\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_annotations_003.png\" srcset=\"../../_images/sphx_glr_annotations_003.png, ../../_images/sphx_glr_annotations_003_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Intermediate->Constrained Layout Guide->Use with GridSpec"
            ],
            [
                "statsmodels->Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->A univariate example"
            ],
            [
                "statsmodels->Examples->Robust Regression->Comparing OLS and RLM->Comparing OLS and RLM->Example 1: quadratic function with linear truth"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Duncan\u2019s Occupational Prestige data - M-estimation for outliers"
            ],
            [
                "matplotlib->Tutorials->Text->Annotations->Basic annotation->Annotating with arrows"
            ]
        ]
    },
    "982874": {
        "jupyter_code_cell": "model = LassoCV()\nindices = []\nfor i in range(0,len(data['Zip_code'])):\n    if(data['Zip_code'][i].isdigit()):\n        x = int(data['Zip_code'][i])\n    else:\n        indices.append(i)",
        "matched_tutorial_code_inds": [
            6061,
            6659,
            6664,
            6791,
            6628
        ],
        "matched_tutorial_codes": [
            "mod = AutoReg(housing, 3, old_names=False)\nres = mod.fit()\nprint(res.summary())",
            "mod = LocalLevel(dta.infl)\nres = mod.fit(disp=False)\nprint(res.summary())",
            "mod_conc = LocalLevelConcentrated(dta.infl)\nres_conc = mod_conc.fit(disp=False)\nprint(res_conc.summary())",
            "olsmod = sm.OLS(y, X)\nolsres = olsmod.fit()\nprint(olsres.summary())",
            "mod = TVRegression(y_t, x_t, w_t)\nres_mle = mod.fit(disp=False)\nprint(res_mle.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions"
            ],
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Typical approach"
            ],
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Concentrating out the scale"
            ],
            [
                "statsmodels->Examples->User Notes->Prediction->Estimation"
            ],
            [
                "statsmodels->Examples->State space models->Statespace: Custom Models->Bonus: pymc3 for fast Bayesian estimation"
            ]
        ]
    },
    "384142": {
        "jupyter_code_cell": "import dill\nwith open('tutorial_results.pkl', 'wb') as f:\n    dill.dump(successes, f)\n    dill.dump(failures, f)\nimport dill\nwith open('tutorial_results.pkl', 'rb') as f:\n    successes = dill.load(f)\n    failures = dill.load(f)",
        "matched_tutorial_code_inds": [
            183,
            174,
            399,
            3702,
            577
        ],
        "matched_tutorial_codes": [
            "epochs = 5\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(, model, , )\n    test(, model, )\nprint(\"Done!\")",
            "# Download training data from open datasets.\n = (\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=(),\n)\n\n# Download test data from open datasets.\n = (\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=(),\n)",
            "accuracies = []\nexamples = []\n\n# Run test for each epsilon\nfor eps in epsilons:\n    acc, ex = test(model, , , eps)\n    accuracies.append(acc)\n    examples.append(ex)",
            "plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "with open(\"../_static/img/sample_file.jpeg\", 'rb') as f:\n    image_bytes = f.read()\n    print(get_prediction(image_bytes=image_bytes))"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Introduction to PyTorch->Quickstart->Optimizing the Model Parameters"
            ],
            [
                "torch->Introduction to PyTorch->Quickstart->Working with data"
            ],
            [
                "torch->Image and Video->Adversarial Example Generation->Implementation->Run Attack"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ],
            [
                "torch->Deploying PyTorch Models in Production->Deploying PyTorch in Python via a REST API with Flask->Inference->Prediction"
            ]
        ]
    },
    "642078": {
        "jupyter_code_cell": "dfs_piv=dfs_piv.reset_index(level=['group'])\ndfs_piv_1 = dfs_piv[dfs_piv['group']==1]\nfig, axs = plt.subplots(1,3)\ndfs_piv_1.boxplot(column='randomness',ax=axs[0],return_type='dict')\ndfs_piv_1.boxplot(column='real_person',ax=axs[1],return_type='dict')\ndfs_piv_1.boxplot(column='control',ax=axs[2],return_type='dict')",
        "matched_tutorial_code_inds": [
            3079,
            5917,
            5643,
            2985,
            4918
        ],
        "matched_tutorial_codes": [
            "rfc = (n_estimators=10, random_state=42)\nrfc.fit(X_train, y_train)\nax = ()\nrfc_disp = (rfc, X_test, y_test, ax=ax, alpha=0.8)\nsvc_disp.plot(ax=ax, alpha=0.8)\n()\n\n\n<img alt=\"plot roc curve visualization api\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_roc_curve_visualization_api_002.png\" srcset=\"../../_images/sphx_glr_plot_roc_curve_visualization_api_002.png\"/>",
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "glm = smf.glm(\n    \"affairs ~ rate_marriage + yrs_married\",\n    data=dc,\n    family=sm.families.Poisson(),\n    freq_weights=np.asarray(dc[\"freq\"]),\n)\nres_f2 = glm.fit()\n# print(res_f2.summary())\nres_f2.pearson_chi2 - res_f.pearson_chi2, res_f2.deviance - res_f.deviance, res_f2.llf - res_f.llf",
            "diabetes = ()\nX = (diabetes.data, columns=diabetes.feature_names)\ny = diabetes.target\n\ntree = ()\nmlp = (\n    (),\n    (hidden_layer_sizes=(100, 100), tol=1e-2, max_iter=500, random_state=0),\n)\ntree.fit(X, y)\nmlp.fit(X, y)",
            "cdict['red'] = [[0.0,  0.0, 0.3],\n                [0.5,  1.0, 0.9],\n                [1.0,  1.0, 1.0]]\nplot_linearmap(cdict)\n\n\n<img alt=\"colormap manipulation\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_colormap-manipulation_007.png\" srcset=\"../../_images/sphx_glr_colormap-manipulation_007.png, ../../_images/sphx_glr_colormap-manipulation_007_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Miscellaneous->ROC Curve with Visualization API->Training a Random Forest and Plotting the ROC Curve"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->original observations and frequency weights"
            ],
            [
                "sklearn->Examples->Miscellaneous->Advanced Plotting With Partial Dependence->Train models on the diabetes dataset"
            ],
            [
                "matplotlib->Tutorials->Colors->Creating Colormaps in Matplotlib->Creating linear segmented colormaps"
            ]
        ]
    },
    "994549": {
        "jupyter_code_cell": "x['ons'][x['ons']==x['ons'].min()].index[0]\nroute = data.groupby(data['route_name']).sum()[['ons','offs']]\nroute['avg']=route.mean(axis=1)\nroute['avg'][route['avg']==route['avg'].max()].index[0]",
        "matched_tutorial_code_inds": [
            3616,
            3891,
            3610,
            3608,
            3755
        ],
        "matched_tutorial_codes": [
            "hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()",
            "subset = daily.loc['2011':'2016']\nax = subset.div(1000).plot(figsize=(12, 6))\nax.set(ylim=0, title=\"Daily Donations\", ylabel=\"$ (thousands)\",)\nsns.despine();",
            "first = df.groupby('unique_carrier').first()\nfirst.ix[10:15, ['fl_date', 'tail_num']]",
            "first = df.groupby('airline_id')[['fl_date', 'unique_carrier']].first()\nfirst.head()",
            "df['home_win'] = df['home_points'] &gt; df['away_points']\ndf['rest_spread'] = df['home_rest'] - df['away_rest']\ndf.dropna().head()"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing"
            ],
            [
                "pandas_toms_blog->Scaling->Dask"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data"
            ]
        ]
    },
    "687027": {
        "jupyter_code_cell": "comps_sqrt = np.sqrt(comps)\ncomps_sqrt_1diff = comps_sqrt.diff(periods=1)\ncomps_sqrt_2diff = comps_sqrt_1diff.diff(periods=1)\ncomps_sqrt_2diff_sdiff = comps_sqrt_2diff.diff(periods=3)\nvalidate_stationarity(comps_sqrt_2diff_sdiff.dropna())\nfig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(comps_sqrt_2diff_sdiff.dropna(), lags=40, ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(comps_sqrt_2diff_sdiff.dropna(), lags=40, ax=ax2)",
        "matched_tutorial_code_inds": [
            5643,
            5645,
            5917,
            5647,
            1982
        ],
        "matched_tutorial_codes": [
            "glm = smf.glm(\n    \"affairs ~ rate_marriage + yrs_married\",\n    data=dc,\n    family=sm.families.Poisson(),\n    freq_weights=np.asarray(dc[\"freq\"]),\n)\nres_f2 = glm.fit()\n# print(res_f2.summary())\nres_f2.pearson_chi2 - res_f.pearson_chi2, res_f2.deviance - res_f.deviance, res_f2.llf - res_f.llf",
            "glm = smf.glm(\n    \"affairs_sum ~ rate_marriage + yrs_married\",\n    data=df_a,\n    family=sm.families.Poisson(),\n    exposure=np.asarray(df_a[\"affairs_count\"]),\n)\nres_e2 = glm.fit()\nres_e2.pearson_chi2 - res_e.pearson_chi2, res_e2.deviance - res_e.deviance, res_e2.llf - res_e.llf",
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "glm = smf.glm(\n    \"affairs_mean ~ rate_marriage + yrs_married\",\n    data=df_a,\n    family=sm.families.Poisson(),\n    var_weights=np.asarray(df_a[\"affairs_count\"]),\n)\nres_a2 = glm.fit()\nres_a2.pearson_chi2 - res_a.pearson_chi2, res_a2.deviance - res_a.deviance, res_a2.llf - res_a.llf",
            "coef = ((size, size))\ncoef[0:roi_size, 0:roi_size] = -1.0\ncoef[-roi_size:, -roi_size:] = 1.0\n\nX = (n_samples, size**2)\nfor x in X:  # smooth data\n    x[:] = (x.reshape(size, size), sigma=1.0).ravel()\nX -= X.mean(axis=0)\nX /= X.std(axis=0)\n\ny = (X, coef.ravel())"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->original observations and frequency weights"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->aggregated data: exposure and var_weights"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->aggregated data: exposure and var_weights"
            ],
            [
                "sklearn->Examples->Clustering->Feature agglomeration vs. univariate selection"
            ]
        ]
    },
    "1150175": {
        "jupyter_code_cell": "run_fullpipeline(datafolder, n_tiles_y, n_tiles_x, 8, 8, feature_funcs, 4)\nimgfiles = import_data(datafolder)\ndf, feature_names = extract_features(imgfiles, feature_funcs, 10, 10)",
        "matched_tutorial_code_inds": [
            82,
            6714,
            2460,
            2434,
            2114
        ],
        "matched_tutorial_codes": [
            "use_amp = True\n\nnet = make_model(in_size, out_size, num_layers)\nopt = (net.parameters(), lr=0.001)\nscaler = (enabled=use_amp)\n\nstart_timer()\nfor epoch in range(epochs):\n    for input, target in zip(data, targets):\n        with (device_type='cuda', dtype=torch.float16, enabled=use_amp):\n            output = net(input)\n            loss = loss_fn(output, target)\n        scaler.scale(loss).backward()\n        scaler.step(opt)\n        scaler.update()\n        opt.zero_grad() # set_to_none=True here can modestly improve performance\nend_timer_and_print(\"Mixed precision:\")",
            "make_plot(dta.index[idx[:5]])\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_24_0.png\" src=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_24_0.png\"/>",
            "one_hot_poly_pipeline = (\n    (\n        transformers=[\n            (\"categorical\", one_hot_encoder, categorical_columns),\n            (\"one_hot_time\", one_hot_encoder, [\"hour\", \"weekday\", \"month\"]),\n        ],\n        remainder=\"passthrough\",\n    ),\n    (kernel=\"poly\", degree=2, n_components=300, random_state=0),\n    (alphas=alphas),\n)\nevaluate(one_hot_poly_pipeline, X, y, cv=ts_cv)",
            "one_hot_linear_pipeline = (\n    (\n        transformers=[\n            (\"categorical\", one_hot_encoder, categorical_columns),\n            (\"one_hot_time\", one_hot_encoder, [\"hour\", \"weekday\", \"month\"]),\n        ],\n        remainder=(),\n    ),\n    (alphas=alphas),\n)\n\nevaluate(one_hot_linear_pipeline, X, y, cv=ts_cv)",
            "batch_dict_estimator = (\n    n_components=n_components, alpha=0.1, max_iter=50, batch_size=3, random_state=rng\n)\nbatch_dict_estimator.fit(faces_centered)\nplot_gallery(\"Dictionary learning\", batch_dict_estimator.components_[:n_components])\n\n\n<img alt=\"Dictionary learning\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_006.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_006.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "torch->PyTorch Recipes->Automatic Mixed Precision->All together: \u201cAutomatic Mixed Precision\u201d"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Modeling non-linear feature interactions with kernels"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Time-steps as categories"
            ],
            [
                "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition->Dictionary learning"
            ]
        ]
    },
    "353538": {
        "jupyter_code_cell": "coeff=np.array(np.zeros((24,5)))\nfor i in range(2,24,1): \n    LIMIT_L = 1990\n    LIMIT_H = LIMIT_L+3040 \n    PULSE_R = 1738\n    hdf5_file     = ''.join([path,'cal_1u.h5.z'])\n    Channel       = i\n    event_range      = range(0,500,1)\n    coeff[i,0] = find_coeff_II( LIMIT_L, LIMIT_H, PULSE_R,\n                                hdf5_file, Channel, event_range )\n    print(\"CH\",i,'=',coeff[i,0])",
        "matched_tutorial_code_inds": [
            3257,
            1878,
            2753,
            3266,
            4654
        ],
        "matched_tutorial_codes": [
            "alphas = (-5, 1, 60)\nenet = (l1_ratio=0.7, max_iter=10000)\ntrain_errors = list()\ntest_errors = list()\nfor alpha in alphas:\n    enet.set_params(alpha=alpha)\n    enet.fit(X_train, y_train)\n    train_errors.append(enet.score(X_train, y_train))\n    test_errors.append(enet.score(X_test, y_test))\n\ni_alpha_optim = (test_errors)\nalpha_optim = alphas[i_alpha_optim]\nprint(\"Optimal regularization parameter : %s\" % alpha_optim)\n\n# Estimate the coef_ on full data with optimal regularization parameter\nenet.set_params(alpha=alpha_optim)\ncoef_ = enet.fit(X, y).coef_",
            "scores = (list)\nfor i, (clf, name) in enumerate(clf_list):\n    clf.fit(X_train, y_train)\n    y_prob = clf.predict_proba(X_test)\n    y_pred = clf.predict(X_test)\n    scores[\"Classifier\"].append(name)\n\n    for metric in [, ]:\n        score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n        scores[score_name].append(metric(y_test, y_prob[:, 1]))\n\n    for metric in [, , , ]:\n        score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n        scores[score_name].append(metric(y_test, y_pred))\n\n    score_df = (scores).set_index(\"Classifier\")\n    score_df.round(decimals=3)\n\nscore_df\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "quantiles = [0.05, 0.5, 0.95]\npredictions = {}\nout_bounds_predictions = (y_true_mean, dtype=)\nfor quantile in quantiles:\n    qr = (quantile=quantile, alpha=0, solver=solver)\n    y_pred = qr.fit(X, y_pareto).predict(X)\n    predictions[quantile] = y_pred\n\n    if quantile == min(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred = y_pareto\n        )\n    elif quantile == max(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred &lt;= y_pareto\n        )",
            "cvs = [, , ]\n\nfor cv in cvs:\n    fig, ax = (figsize=(6, 3))\n    plot_cv_indices(cv(n_splits), X, y, groups, ax, n_splits)\n    ax.legend(\n        [(color=cmap_cv(0.8)), (color=cmap_cv(0.02))],\n        [\"Testing set\", \"Training set\"],\n        loc=(1.02, 0.8),\n    )\n    # Make the legend fit\n    ()\n    fig.subplots_adjust(right=0.7)\n\n\n\n<img alt=\"StratifiedKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_003.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_003.png\"/>\n<img alt=\"GroupKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_004.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_004.png\"/>\n<img alt=\"StratifiedGroupKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_005.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_005.png\"/>",
            "data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Model Selection->Train error vs Test error->Compute train and test errors"
            ],
            [
                "sklearn->Examples->Calibration->Probability Calibration curves->Calibration curves->Linear support vector classifier"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Quantile regression->Fitting a QuantileRegressor"
            ],
            [
                "sklearn->Examples->Model Selection->Visualizing cross-validation behavior in scikit-learn->Define a function to visualize cross-validation behavior"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings"
            ]
        ]
    },
    "64732": {
        "jupyter_code_cell": "sigma_b_bootstrap = np.sqrt((sigma_V_total**2/D)*N)\nprint('sigma_b_bootstrap = {:1.2f} Ohms'.format(sigma_b_bootstrap))\nplot = df.plot(x='I',y='V', kind='scatter', \n               title=r'$V$ $\\rm vs.$ $I$ (with error bars and trendline)',\n              xerr = 0.05, yerr = 0.1);\nplot.set_xlabel(r'$I$ $\\rm (Amperes)$')\nplot.set_ylabel(r'$V$ $\\rm (Volts)$')\nx1 = 1; y1 = a + b*x1\nx2 = 12; y2 = a + b*x2\nplt.plot([x1,x2],[y1,y2],'k-')\nplt.text(7, 6000, r'$y = %.2f x + (%.2f) $' % (b, a), fontsize=14)",
        "matched_tutorial_code_inds": [
            5917,
            5981,
            5984,
            3079,
            1615
        ],
        "matched_tutorial_codes": [
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "res2_DL = combine_effects(eff, var_eff, method_re=\"dl\", use_t=True, row_names=rownames)\nprint(\"method RE:\", res2_DL.method_re)\nprint(res2_DL.summary_frame())\nfig = res2_DL.plot_forest()\nfig.set_figheight(6)\nfig.set_figwidth(6)",
            "res2_PM = combine_effects(eff, var_eff, method_re=\"pm\", use_t=True, row_names=rownames)\nprint(\"method RE:\", res2_PM.method_re)\nprint(res2_PM.summary_frame())\nfig = res2_PM.plot_forest()\nfig.set_figheight(6)\nfig.set_figwidth(6)",
            "rfc = (n_estimators=10, random_state=42)\nrfc.fit(X_train, y_train)\nax = ()\nrfc_disp = (rfc, X_test, y_test, ax=ax, alpha=0.8)\nsvc_disp.plot(ax=ax, alpha=0.8)\n()\n\n\n<img alt=\"plot roc curve visualization api\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_roc_curve_visualization_api_002.png\" srcset=\"../../_images/sphx_glr_plot_roc_curve_visualization_api_002.png\"/>",
            "fourier_gaussian = ndimage.fourier_gaussian(xray_image, sigma=0.05)\n\nx_prewitt = ndimage.prewitt(fourier_gaussian, axis=0)\ny_prewitt = ndimage.prewitt(fourier_gaussian, axis=1)\n\nxray_image_canny = np.hypot(x_prewitt, y_prewitt)\n\nxray_image_canny *= 255.0 / np.max(xray_image_canny)\n\nprint(\"The data type - \", xray_image_canny.dtype)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Example Kacker interlaboratory mean"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Example Kacker interlaboratory mean"
            ],
            [
                "sklearn->Examples->Miscellaneous->ROC Curve with Visualization API->Training a Random Forest and Plotting the ROC Curve"
            ],
            [
                "numpy->NumPy Applications->X-ray image processing->Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters->The Canny filter"
            ]
        ]
    },
    "642959": {
        "jupyter_code_cell": "import pandas as pd\nimport numpy as np\nimport collections\nimport matplotlib.pyplot as plt\nimport pymc3 as pm\nimport scipy.stats as stats",
        "matched_tutorial_code_inds": [
            6399,
            5484,
            6455,
            5710,
            6183
        ],
        "matched_tutorial_codes": [
            "import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt",
            "import numpy as np\nimport pandas as pd\nimport scipy.stats as stats\n\nfrom statsmodels.miscmodels.ordinal_model import OrderedModel",
            "import numpy as np\nfrom scipy import stats\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm",
            "import statsmodels.api as sm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO",
            "import pandas as pd\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->VARMAX: Introduction",
                "statsmodels->Examples->State space models->Trends and cycles in unemployment"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression"
            ],
            [
                "statsmodels->Examples->State space models->Statespace ARMA: Sunspots Data"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Quasi-binomial regression"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Time Series Filters"
            ]
        ]
    },
    "1133605": {
        "jupyter_code_cell": "colormap = {\n    'Fossil fuels': 'Black',\n    'Lignite': 'SaddleBrown',\n    'Hard coal': 'Black',\n    'Oil': 'Violet',\n    'Natural gas': 'IndianRed',\n    'Combined cycle': '#d57676',\n    'Gas turbine': '#e19d9d',\n    'Other and unknown natural gas': '#c33c3c',\n    'Differently categorized natural gas': 'IndianRed',\n    'Non-renewable waste': 'SandyBrown',\n    'Mixed fossil fuels': 'LightGray',\n    'Other fossil fuels': 'DarkGray',\n    'Differently categorized fossil fuels': 'Gray',\n    'Nuclear': 'Red',\n    'Renewable energy sources': 'Green',\n    'Hydro': 'Navy',\n    'Run-of-river': '#0000b3',\n    'Reservoir': '#0000e6',\n    'Reservoir including pumped storage': '#0000e6',\n    'Pumped storage': '#1a1aff',\n    'Pumped storage with natural inflow': '#1a1aff',\n    'Differently categorized hydro': 'Navy',\n    'Wind': 'SkyBlue',\n    'Onshore': 'LightSkyBlue',\n    'Offshore': 'DeepSkyBlue',\n    'Differently categorized wind': 'SkyBlue',\n    'Solar': 'Yellow',\n    'Photovoltaics': '#ffff33',\n    'Concentrated solar power': '#ffff66',\n    'Differently categorized solar': 'Yellow',\n    'Geothermal': 'DarkRed',\n    'Marine': 'Blue',\n    'Bioenergy and renewable waste': 'Green',\n    'Biomass and biogas': '#00b300',\n    'Sewage and landfill gas': '#00e600',\n    'Other bioenergy and renewable waste': 'Green',\n    'Differently categorized renewable energy sources': 'Green',\n    'Other or unspecified energy sources': 'Orange',\n}\npivot_capacity_level1 = pd.pivot_table(data_selection[data_selection.energy_source_level_1 == True],\n                                       index=('country','year','source'),\n                                       columns='technology',\n                                       values='capacity',\n                                       aggfunc=sum,\n                                       margins=False)\npivot_capacity_level1",
        "matched_tutorial_code_inds": [
            4916,
            3797,
            189,
            4921,
            6619
        ],
        "matched_tutorial_codes": [
            "cdict = {'red':   [[0.0,  0.0, 0.0],\n                   [0.5,  1.0, 1.0],\n                   [1.0,  1.0, 1.0]],\n         'green': [[0.0,  0.0, 0.0],\n                   [0.25, 0.0, 0.0],\n                   [0.75, 1.0, 1.0],\n                   [1.0,  1.0, 1.0]],\n         'blue':  [[0.0,  0.0, 0.0],\n                   [0.5,  0.0, 0.0],\n                   [1.0,  1.0, 1.0]]}\n\n\ndef plot_linearmap(cdict):\n    newcmp = LinearSegmentedColormap('testCmap', segmentdata=cdict, N=256)\n    rgba = newcmp(np.linspace(0, 1, 256))\n    fig, ax = plt.subplots(figsize=(4, 3), constrained_layout=True)\n    col = ['r', 'g', 'b']\n    for xx in [0.25, 0.5, 0.75]:\n        ax.axvline(xx, color='0.7', linestyle='--')\n    for i in range(3):\n        ax.plot(np.arange(256)/256, rgba[:, i], color=col[i])\n    ax.set_xlabel('index')\n    ax.set_ylabel('RGB')\n    plt.show()\n\nplot_linearmap(cdict)\n\n\n<img alt=\"colormap manipulation\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_colormap-manipulation_006.png\" srcset=\"../../_images/sphx_glr_colormap-manipulation_006.png, ../../_images/sphx_glr_colormap-manipulation_006_2_0x.png 2.0x\"/>",
            "cmap = sns.cubehelix_palette(as_cmap=True, dark=0, light=1, reverse=True)\n\n(df.select_dtypes(include=[np.number])\n   .pipe(core)\n   .pipe(sns.PairGrid)\n   .map_upper(plt.scatter, marker='.', alpha=.25)\n   .map_diag(sns.kdeplot)\n   .map_lower(plt.hexbin, cmap=cmap, gridsize=20)\n);",
            "classes = [\n    \"T-shirt/top\",\n    \"Trouser\",\n    \"Pullover\",\n    \"Dress\",\n    \"Coat\",\n    \"Sandal\",\n    \"Shirt\",\n    \"Sneaker\",\n    \"Bag\",\n    \"Ankle boot\",\n]\n\n()\n, y = [0][0], [0][1]\nwith ():\n     = model()\n    predicted, actual = classes[[0].argmax(0)], classes[y]\n    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')",
            "colors = [\"#ffffcc\", \"#a1dab4\", \"#41b6c4\", \"#2c7fb8\", \"#253494\"]\nmy_cmap = ListedColormap(colors, name=\"my_cmap\")\n\nmy_cmap_r = my_cmap.reversed()\n\nplot_examples([my_cmap, my_cmap_r])\n\n\n<img alt=\"colormap manipulation\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_colormap-manipulation_009.png\" srcset=\"../../_images/sphx_glr_colormap-manipulation_009.png, ../../_images/sphx_glr_colormap-manipulation_009_2_0x.png 2.0x\"/>",
            "true_values = {\n    \"var_e1\": 0.01,\n    \"var_e2\": 0.01,\n    \"var_w1\": 0.01,\n    \"var_w2\": 0.01,\n    \"delta1\": 0.8,\n    \"delta2\": 0.5,\n    \"delta3\": 0.7,\n}\n\n\ndef gen_data_for_model3():\n    # Starting values\n    alpha1_0 = 2.1\n    alpha2_0 = 1.1\n\n    t_max = 500\n\n    def gen_i(alpha1, s):\n        return alpha1 * s + np.sqrt(true_values[\"var_e1\"]) * np.random.randn()\n\n    def gen_m_hat(alpha2):\n        return 1 * alpha2 + np.sqrt(true_values[\"var_e2\"]) * np.random.randn()\n\n    def gen_alpha1(alpha1, alpha2):\n        w1 = np.sqrt(true_values[\"var_w1\"]) * np.random.randn()\n        return true_values[\"delta1\"] * alpha1 + true_values[\"delta2\"] * alpha2 + w1\n\n    def gen_alpha2(alpha2):\n        w2 = np.sqrt(true_values[\"var_w2\"]) * np.random.randn()\n        return true_values[\"delta3\"] * alpha2 + w2\n\n    s_t = 0.3 + np.sqrt(1.4) * np.random.randn(t_max)\n    i_hat = np.empty(t_max)\n    m_hat = np.empty(t_max)\n\n    current_alpha1 = alpha1_0\n    current_alpha2 = alpha2_0\n    for t in range(t_max):\n        # Obs eqns\n        i_hat[t] = gen_i(current_alpha1, s_t[t])\n        m_hat[t] = gen_m_hat(current_alpha2)\n\n        # state eqns\n        new_alpha1 = gen_alpha1(current_alpha1, current_alpha2)\n        new_alpha2 = gen_alpha2(current_alpha2)\n\n        # Update states for next period\n        current_alpha1 = new_alpha1\n        current_alpha2 = new_alpha2\n\n    return i_hat, m_hat, s_t\n\n\ni_hat, m_hat, s_t = gen_data_for_model3()"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Colors->Creating Colormaps in Matplotlib->Creating linear segmented colormaps"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "torch->Introduction to PyTorch->Quickstart->Loading Models"
            ],
            [
                "matplotlib->Tutorials->Colors->Creating Colormaps in Matplotlib->Reversing a colormap"
            ],
            [
                "statsmodels->Examples->State space models->Statespace: Custom Models->Model 3: multiple observation and state equations->Matrix notation for the state space model"
            ]
        ]
    },
    "1143077": {
        "jupyter_code_cell": "plt.figure(figsize=(15,15))\nplt.plot(duration_outliers[['x1','x2']],duration_outliers[['y1','y2']],'b',lw=0.2,alpha=0.1)\nplt.scatter(duration_outliers[['x1','x2']],duration_outliers[['y1','y2']],lw=0,s=5,alpha=0.5,c='red')\nplt.xlim([-10,40])\nplt.ylim([-25,25]);\ntrips = trips[trips['trip_duration'] < 3*3600]",
        "matched_tutorial_code_inds": [
            550,
            401,
            5890,
            3772,
            4350
        ],
        "matched_tutorial_codes": [
            "plt.figure(figsize=(10, 10))\nplt.subplot(2, 2, 1)\nplt.plot(logs[\"reward\"])\nplt.title(\"training rewards (average)\")\nplt.subplot(2, 2, 2)\nplt.plot(logs[\"step_count\"])\nplt.title(\"Max step count (training)\")\nplt.subplot(2, 2, 3)\nplt.plot(logs[\"eval reward (sum)\"])\nplt.title(\"Return (test)\")\nplt.subplot(2, 2, 4)\nplt.plot(logs[\"eval step_count\"])\nplt.title(\"Max step count (test)\")\nplt.show()\n\n\n<img alt=\"training rewards (average), Max step count (training), Return (test), Max step count (test)\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_reinforcement_ppo_001.png\" srcset=\"../_images/sphx_glr_reinforcement_ppo_001.png\"/>",
            "plt.figure(figsize=(5,5))\nplt.plot(epsilons, accuracies, \"*-\")\nplt.yticks(np.arange(0, 1.1, step=0.1))\nplt.xticks(np.arange(0, .35, step=0.05))\nplt.title(\"Accuracy vs Epsilon\")\nplt.xlabel(\"Epsilon\")\nplt.ylabel(\"Accuracy\")\nplt.show()\n\n\n<img alt=\"Accuracy vs Epsilon\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_fgsm_tutorial_001.png\" srcset=\"../_images/sphx_glr_fgsm_tutorial_001.png\"/>",
            "plt.figure(figsize=(6, 6))\nsymbols = [\"D\", \"^\"]\ncolors = [\"r\", \"g\", \"blue\"]\nfactor_groups = salary_table.groupby([\"E\", \"M\"])\nfor values, group in factor_groups:\n    i, j = values\n    plt.scatter(group[\"X\"], group[\"S\"], marker=symbols[j], color=colors[i - 1], s=144)\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "plt.figure(figsize=(8, 5))\n(wins.win_pct\n    .unstack()\n    .assign(**{'Home Win % - Away %': lambda x: x.home_team - x.away_team,\n               'Overall %': lambda x: (x.home_team + x.away_team) / 2})\n     .pipe((sns.regplot, 'data'), x='Overall %', y='Home Win % - Away %')\n)\nsns.despine()\nplt.tight_layout()",
            "plt.figure()\n plt.imshow(image)\n plt.gray()\n plt.title('Original image')\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code displays two plots. The first plot is the familiar photo of a raccoon climbing on a palm. The second plot has the FIR filter applied and has the two copies of the photo superimposed due to the twin peaks manually set in the filter kernel definition.\"' class=\"plot-directive\" src=\"../_images/signal-2_00_00.png\"/>\n</figure>"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Reinforcement Learning->Reinforcement Learning (PPO) with TorchRL Tutorial->Results"
            ],
            [
                "torch->Image and Video->Adversarial Example Generation->Results->Accuracy vs Epsilon"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team"
            ],
            [
                "scipy->Signal Processing (scipy.signal)->Filtering->Convolution/Correlation"
            ]
        ]
    },
    "381263": {
        "jupyter_code_cell": "loan.validation_split()\nloan.logistic_summary()\nloan = preprocessing.LoanDefault()\nloan.logistic_bootstrap(3)",
        "matched_tutorial_code_inds": [
            6659,
            6664,
            6061,
            5191,
            5215
        ],
        "matched_tutorial_codes": [
            "mod = LocalLevel(dta.infl)\nres = mod.fit(disp=False)\nprint(res.summary())",
            "mod_conc = LocalLevelConcentrated(dta.infl)\nres_conc = mod_conc.fit(disp=False)\nprint(res_conc.summary())",
            "mod = AutoReg(housing, 3, old_names=False)\nres = mod.fit()\nprint(res.summary())",
            "ols_model = sm.OLS(y, X)\nols_results = ols_model.fit()\nprint(ols_results.summary())",
            "glsar_model = sm.GLSAR(data.endog, data.exog, 1)\nglsar_results = glsar_model.iterative_fit(1)\nprint(glsar_results.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Typical approach"
            ],
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Concentrating out the scale"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Generalized Least Squares"
            ]
        ]
    },
    "359291": {
        "jupyter_code_cell": "tooltips=[(\"Shot Index\", \"$index\"), (\"Shot Type\", \"@SHOT_TYPE\"),\n          (\"Shot Outcome\", \"@shot_outcome\"), (\"Action Type\", \"@ACTION_TYPE\"),\n          (\"Shot Distance\", \"@SHOT_DISTANCE ft\")]\nfig = nba.bokeh_shot_chart(curry_shots_df, fill_color=\"color\",\n                           hover_tool=True, tooltips=tooltips)\nshow(fig)",
        "matched_tutorial_code_inds": [
            4835,
            4654,
            5963,
            220,
            3267
        ],
        "matched_tutorial_codes": [
            "gs_kw = dict(width_ratios=[1.4, 1], height_ratios=[1, 2])\nfig, axd = plt.subplot_mosaic([['upper left', 'right'],\n                               ['lower left', 'right']],\n                              gridspec_kw=gs_kw, figsize=(5.5, 3.5),\n                              layout=\"constrained\")\nfor k in axd:\n    annotate_axes(axd[k], f'axd[\"{k}\"]', fontsize=14)\nfig.suptitle('plt.subplot_mosaic()')\n\n\n<img alt=\"plt.subplot_mosaic()\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_arranging_axes_006.png\" srcset=\"../../_images/sphx_glr_arranging_axes_006.png, ../../_images/sphx_glr_arranging_axes_006_2_0x.png 2.0x\"/>",
            "data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>",
            "data = [\n    [\"Carroll\", 94, 22, 60, 92, 20, 60],\n    [\"Grant\", 98, 21, 65, 92, 22, 65],\n    [\"Peck\", 98, 28, 40, 88, 26, 40],\n    [\"Donat\", 94, 19, 200, 82, 17, 200],\n    [\"Stewart\", 98, 21, 50, 88, 22, 45],\n    [\"Young\", 96, 21, 85, 92, 22, 85],\n]\ncolnames = [\"study\", \"mean_t\", \"sd_t\", \"n_t\", \"mean_c\", \"sd_c\", \"n_c\"]\nrownames = [i[0] for i in data]\ndframe1 = pd.DataFrame(data, columns=colnames)\nrownames",
            "labels_map = {\n    0: \"T-Shirt\",\n    1: \"Trouser\",\n    2: \"Pullover\",\n    3: \"Dress\",\n    4: \"Coat\",\n    5: \"Sandal\",\n    6: \"Shirt\",\n    7: \"Sneaker\",\n    8: \"Bag\",\n    9: \"Ankle Boot\",\n}\nfigure = plt.figure(figsize=(8, 8))\ncols, rows = 3, 3\nfor i in range(1, cols * rows + 1):\n    sample_idx = (len(), size=(1,)).item()\n    ,  = [sample_idx]\n    figure.add_subplot(rows, cols, i)\n    plt.title(labels_map[])\n    plt.axis(\"off\")\n    plt.imshow(.squeeze(), cmap=\"gray\")\nplt.show()\n\n\n<img alt=\"Pullover, Sandal, Bag, Bag, Sneaker, Bag, Pullover, Sandal, Sandal\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_data_tutorial_001.png\" srcset=\"../../_images/sphx_glr_data_tutorial_001.png\"/>",
            "cvs = [\n    ,\n    ,\n    ,\n    ,\n    ,\n    ,\n    ,\n    ,\n]\n\n\nfor cv in cvs:\n    this_cv = cv(n_splits=n_splits)\n    fig, ax = (figsize=(6, 3))\n    plot_cv_indices(this_cv, X, y, groups, ax, n_splits)\n\n    ax.legend(\n        [(color=cmap_cv(0.8)), (color=cmap_cv(0.02))],\n        [\"Testing set\", \"Training set\"],\n        loc=(1.02, 0.8),\n    )\n    # Make the legend fit\n    ()\n    fig.subplots_adjust(right=0.7)\n()\n\n\n\n<img alt=\"KFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_006.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_006.png\"/>\n<img alt=\"GroupKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_007.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_007.png\"/>\n<img alt=\"ShuffleSplit\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_008.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_008.png\"/>\n<img alt=\"StratifiedKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_009.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_009.png\"/>\n<img alt=\"StratifiedGroupKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_010.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_010.png\"/>\n<img alt=\"GroupShuffleSplit\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_011.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_011.png\"/>\n<img alt=\"StratifiedShuffleSplit\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_012.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_012.png\"/>\n<img alt=\"TimeSeriesSplit\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_013.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_013.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Intermediate->Arranging multiple Axes in a Figure->High-level methods for making grids->Variable widths or heights in a grid"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Example"
            ],
            [
                "torch->Introduction to PyTorch->Datasets & DataLoaders->Iterating and Visualizing the Dataset"
            ],
            [
                "sklearn->Examples->Model Selection->Visualizing cross-validation behavior in scikit-learn->Visualize cross-validation indices for many CV objects"
            ]
        ]
    },
    "1325342": {
        "jupyter_code_cell": "import os\nfrom selenium import webdriver\nimport pandas as pd\nimport datetime, time, csv",
        "matched_tutorial_code_inds": [
            4342,
            4441,
            2799,
            4358,
            6399
        ],
        "matched_tutorial_codes": [
            "import numpy as np\n from scipy import signal, datasets\n import matplotlib.pyplot as plt",
            "import numpy as np\n from scipy import spatial\n import matplotlib.pyplot as plt",
            "import numpy as np\nimport scipy as sp\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns",
            "import numpy as np\n import scipy.signal as signal\n import matplotlib.pyplot as plt",
            "import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt"
        ],
        "matched_tutorial_paths": [
            [
                "scipy->Signal Processing (scipy.signal)->B-splines",
                "scipy->Signal Processing (scipy.signal)->Filtering->Convolution/Correlation",
                "scipy->Signal Processing (scipy.signal)->Filtering->Convolution/Correlation"
            ],
            [
                "scipy->Spatial data structures and algorithms (scipy.spatial)->Voronoi diagrams"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models"
            ],
            [
                "scipy->Signal Processing (scipy.signal)->Filtering->Filter Design->FIR Filter",
                "scipy->Signal Processing (scipy.signal)->Filtering->Filter Design->FIR Filter",
                "scipy->Signal Processing (scipy.signal)->Filtering->Filter Design->IIR Filter",
                "scipy->Signal Processing (scipy.signal)->Filtering->Analog Filter Design",
                "scipy->Signal Processing (scipy.signal)->Spectral Analysis->Periodogram Measurements",
                "scipy->Signal Processing (scipy.signal)->Spectral Analysis->Spectral Analysis using Welch\u00e2\u0080\u0099s Method",
                "scipy->Signal Processing (scipy.signal)->Detrend"
            ],
            [
                "statsmodels->Examples->State space models->VARMAX: Introduction",
                "statsmodels->Examples->State space models->Trends and cycles in unemployment"
            ]
        ]
    },
    "85441": {
        "jupyter_code_cell": "fig, axes = plt.subplots(figsize = [15,10])\nplt.plot(f_results['f_svm_g_200'], c = 'blue', label = 'SVM on avg eigenface per class  - 200 imgs' )\nplt.plot(f_results['f_knn_g_200'], c = 'orange', label = 'kNN on avg eigenface per class -  200 imgs')\nplt.plot(f_results['f_svm_g_40'], ls = '--', c = 'blue', label = 'SVM on avg eigenface per class -    40 imgs')\nplt.plot(f_results['f_knn_g_40'], ls = '--', c = 'orange', label = 'kNN on avg eigenface per class -    40 imgs')\nplt.plot(f_results['f_svm_ug_40'], ls = ':', c = 'blue', label = 'SVM on all eigenfaces per class -    40 imgs' )\nplt.plot(f_results['f_knn_ug_40'], ls = ':', c = 'orange', label = 'kNN on all eigenfaces per class -    40 imgs')\naxes.legend(loc='upper right', bbox_to_anchor=(1.35, 1.01))\nplt.title('Performance', fontsize = 16)\nplt.ylabel('Accuracy', fontsize = 15)\nplt.xlabel('# of eigenfaces used to characterize image', fontsize = 15)\nplt.tick_params(axis='both', which='major', labelsize=15)\nplt.show()\nfolder_list = os.listdir('f_center') \nlen_dict = {}\nfor i, folder in enumerate(folder_list):\n    num_files = len(os.listdir('f_center\\%s' %folder)) \n    len_dict[i] = num_files\ndf_target_info = pd.DataFrame(index = list(len_dict.keys()), data = os.listdir('f_center'), columns = ['name'])\ndf_target_info['n_im'] = len_dict.values()",
        "matched_tutorial_code_inds": [
            6453,
            1623,
            6428,
            1617,
            4645
        ],
        "matched_tutorial_codes": [
            "fig, ax = plt.subplots(figsize=(10,4))\n\n# Plot the results\ndf['lff'].plot(ax=ax, style='k.', label='Observations')\npredict.predicted_mean.plot(ax=ax, label='One-step-ahead Prediction')\npredict_ci = predict.conf_int(alpha=0.05)\npredict_index = np.arange(len(predict_ci))\nax.fill_between(predict_index[2:], predict_ci.iloc[2:, 0], predict_ci.iloc[2:, 1], alpha=0.1)\n\nforecast.predicted_mean.plot(ax=ax, style='r', label='Forecast')\nforecast_ci = forecast.conf_int()\nforecast_index = np.arange(len(predict_ci), len(predict_ci) + len(forecast_ci))\nax.fill_between(forecast_index, forecast_ci.iloc[:, 0], forecast_ci.iloc[:, 1], alpha=0.1)\n\n# Cleanup the image\nax.set_ylim((4, 8));\nlegend = ax.legend(loc='lower left');\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_local_linear_trend_11_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_local_linear_trend_11_0.png\"/>",
            "fig, axes = plt.subplots(nrows=1, ncols=9, figsize=(30, 30))\n\naxes[0].set_title(\"Original\")\naxes[0].imshow(xray_image, cmap=\"gray\")\naxes[1].set_title(\"Laplace-Gaussian (edges)\")\naxes[1].imshow(xray_image_laplace_gaussian, cmap=\"gray\")\naxes[2].set_title(\"Gaussian gradient (edges)\")\naxes[2].imshow(x_ray_image_gaussian_gradient, cmap=\"gray\")\naxes[3].set_title(\"Sobel (edges) - grayscale\")\naxes[3].imshow(xray_image_sobel, cmap=\"gray\")\naxes[4].set_title(\"Sobel (edges) - hot\")\naxes[4].imshow(xray_image_sobel, cmap=\"hot\")\naxes[5].set_title(\"Canny (edges) - prism)\")\naxes[5].imshow(xray_image_canny, cmap=\"prism\")\naxes[6].set_title(\"Canny (edges) - nipy_spectral)\")\naxes[6].imshow(xray_image_canny, cmap=\"nipy_spectral\")\naxes[7].set_title(\"Mask (&gt; 150, noisy)\")\naxes[7].imshow(xray_image_mask_noisy, cmap=\"gray\")\naxes[8].set_title(\"Mask (&gt; 150, less noisy)\")\naxes[8].imshow(xray_image_mask_less_noisy, cmap=\"gray\")\nfor i in axes:\n    i.axis(\"off\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/a181aad23123a912880c01644de71ffbbac292fb5c3338ce09893b614bed1188.png\" src=\"../_images/a181aad23123a912880c01644de71ffbbac292fb5c3338ce09893b614bed1188.png\"/>",
            "fig, ax = plt.subplots(figsize=(13,3))\n\n# Compute the index\nextended_coincident_index = compute_coincident_index(extended_mod, extended_res)\n\n# Plot the factor\ndates = endog.index._mpl_repr()\nax.plot(dates, coincident_index, '-', linewidth=1, label='Basic model')\nax.plot(dates, extended_coincident_index, '--', linewidth=3, label='Extended model')\nax.plot(usphci.index._mpl_repr(), usphci, label='USPHCI')\nax.legend(loc='lower right')\nax.set(title='Coincident indices, comparison')\n\n# Retrieve and also plot the NBER recession indicators\nylim = ax.get_ylim()\nax.fill_between(dates[:-3], ylim[0], ylim[1], rec.values[:-4,0], facecolor='k', alpha=0.1);\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_dfm_coincident_35_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_dfm_coincident_35_0.png\"/>",
            "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(20, 15))\n\naxes[0].set_title(\"Original\")\naxes[0].imshow(xray_image, cmap=\"gray\")\naxes[1].set_title(\"Canny (edges) - prism\")\naxes[1].imshow(xray_image_canny, cmap=\"prism\")\naxes[2].set_title(\"Canny (edges) - nipy_spectral\")\naxes[2].imshow(xray_image_canny, cmap=\"nipy_spectral\")\naxes[3].set_title(\"Canny (edges) - terrain\")\naxes[3].imshow(xray_image_canny, cmap=\"terrain\")\nfor i in axes:\n    i.axis(\"off\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/e7ee670442f8d404bfdcfdb117f2063747c3554aad2d41cd1dbc99b51c61c9ae.png\" src=\"../_images/e7ee670442f8d404bfdcfdb117f2063747c3554aad2d41cd1dbc99b51c61c9ae.png\"/>",
            "fig, ax = plt.subplots(figsize=(5, 2.7), layout='constrained')\ndates = np.arange(np.datetime64('2021-11-15'), np.datetime64('2021-12-25'),\n                  np.timedelta64(1, 'h'))\ndata = np.cumsum(np.random.randn(len(dates)))\nax.plot(dates, data)\ncdf = mpl.dates.ConciseDateFormatter(ax.xaxis.get_major_locator())\nax.xaxis.set_major_formatter(cdf)\n\n\n<img alt=\"quick start\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_quick_start_014.png\" srcset=\"../../_images/sphx_glr_quick_start_014.png, ../../_images/sphx_glr_quick_start_014_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->State space modeling: Local Linear Trends"
            ],
            [
                "numpy->NumPy Applications->X-ray image processing->Compare the results"
            ],
            [
                "statsmodels->Examples->State space models->Dynamic Factor Models: Application->Appendix 1: Extending the dynamic factor model"
            ],
            [
                "numpy->NumPy Applications->X-ray image processing->Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters->The Canny filter"
            ],
            [
                "matplotlib->Tutorials->Introductory->Quick start guide->Axis scales and ticks->Plotting dates and strings"
            ]
        ]
    },
    "1081613": {
        "jupyter_code_cell": "plt.figure(figsize=(15,5))\ntopunique.plot(858, cumulative=True)\nquotes_freq = nltk.FreqDist(quotes)\nquotes_freq = pd.DataFrame(quotes_freq.most_common(200), columns=['word','n'])\nquotes_freq['relative_frequency'] = (quotes_freq['n']/float(len(quotes)))\nquotes_freq.head(20)",
        "matched_tutorial_code_inds": [
            3636,
            6561,
            3772,
            6552,
            401
        ],
        "matched_tutorial_codes": [
            "plt.figure(figsize=(15, 5))\n(df[['fl_date', 'tail_num', 'dep_time', 'dep_delay']]\n    .dropna()\n    .assign(hour=lambda x: x.dep_time.dt.hour)\n    .query('5 &lt; dep_delay &lt; 600')\n    .pipe((sns.boxplot, 'data'), 'hour', 'dep_delay'))\nsns.despine()",
            "plt.tight_layout()\n# Note: the syntax here for the lines argument is required for\n# PyMC3 versions = 3.7\n# For version &lt;= 3.6 you can use lines=dict(res_mle.params) instead\n_ = pm.plot_trace(\n    trace_uc,\n    lines=[(k, {}, [v]) for k, v in dict(res_uc_mle.params).items()],\n    combined=True,\n    figsize=(12, 12),\n)",
            "plt.figure(figsize=(8, 5))\n(wins.win_pct\n    .unstack()\n    .assign(**{'Home Win % - Away %': lambda x: x.home_team - x.away_team,\n               'Overall %': lambda x: (x.home_team + x.away_team) / 2})\n     .pipe((sns.regplot, 'data'), x='Overall %', y='Home Win % - Away %')\n)\nsns.despine()\nplt.tight_layout()",
            "plt.tight_layout()\n# Note: the syntax here for the lines argument is required for\n# PyMC3 versions = 3.7\n# For version &lt;= 3.6 you can use lines=dict(res_mle.params) instead\n_ = pm.plot_trace(\n    trace,\n    lines=[(k, {}, [v]) for k, v in dict(res_mle.params).items()],\n    combined=True,\n    figsize=(12, 12),\n)",
            "plt.figure(figsize=(5,5))\nplt.plot(epsilons, accuracies, \"*-\")\nplt.yticks(np.arange(0, 1.1, step=0.1))\nplt.xticks(np.arange(0, .35, step=0.05))\nplt.title(\"Accuracy vs Epsilon\")\nplt.xlabel(\"Epsilon\")\nplt.ylabel(\"Accuracy\")\nplt.show()\n\n\n<img alt=\"Accuracy vs Epsilon\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_fgsm_tutorial_001.png\" srcset=\"../_images/sphx_glr_fgsm_tutorial_001.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Method Chaining->Method Chaining->Application"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX estimation with Bayesian methods->Appendix A. Application to UnobservedComponents models"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX estimation with Bayesian methods->Introduction->5. Bayesian estimation with NUTS"
            ],
            [
                "torch->Image and Video->Adversarial Example Generation->Results->Accuracy vs Epsilon"
            ]
        ]
    },
    "1175637": {
        "jupyter_code_cell": "plt.scatter(tips_df['Meal #'], tips_df['Tip amount ($)'])\nplt.xlabel('Meal #')\nplt.ylabel('Tips($)')\nplt.ylim(0,18)\nplt.axhline(y=tips_mean, color='r', linestyle='--')\nplt.text(4,11,'best-fit line   ' + r'$\\bar y = \\$10$')\ntips_df.loc[4, 'Tip amount ($)']",
        "matched_tutorial_code_inds": [
            4372,
            4374,
            4652,
            1432,
            4346
        ],
        "matched_tutorial_codes": [
            "plt.semilogy(f, Pper_spec)\n plt.xlabel('frequency [Hz]')\n plt.ylabel('PSD')\n plt.grid()\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code displays a single X-Y log-linear plot with the power spectral density on the Y axis vs frequency on the X axis. A single blue trace shows a noise floor with a power level of 1e-3 with a single peak at 1270 Hz up to a power of 1. The noise floor measurements appear noisy and oscillate down to 1e-7.\"' class=\"plot-directive\" src=\"../_images/signal-8.png\"/>\n</figure>",
            "plt.semilogy(f, Pwelch_spec)\n plt.xlabel('frequency [Hz]')\n plt.ylabel('PSD')\n plt.grid()\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code displays a single X-Y log-linear plot with the power spectral density on the Y axis vs frequency on the X axis. A single blue trace shows a smooth noise floor at a power level of 6e-2 with a single peak up to a power level of 2 at 1270 Hz.\"' class=\"plot-directive\" src=\"../_images/signal-9.png\"/>\n</figure>",
            "plt.plot([1, 2, 3, 4], [1, 4, 9, 16], 'ro')\nplt.axis([0, 6, 0, 20])\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_003.png\" srcset=\"../../_images/sphx_glr_pyplot_003.png, ../../_images/sphx_glr_pyplot_003_2_0x.png 2.0x\"/>",
            "plt.plot(s)\nplt.show()\n\n\n\n\n<img alt=\"../_images/b3c5a64421a25e9b8f34f2d5e71f750a53e3ef7b513a82271d088e5c12a26308.png\" src=\"../_images/b3c5a64421a25e9b8f34f2d5e71f750a53e3ef7b513a82271d088e5c12a26308.png\"/>",
            "plt.figure()\n plt.imshow(deriv)\n plt.gray()\n plt.title('Output of spline edge filter')\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code displays two plots. The first plot is a normal grayscale photo of a raccoon climbing on a palm plant. The second plot has the 2-D spline filter applied to the photo and is completely grey except the edges of the photo have been emphasized, especially on the raccoon fur and palm fronds.\"' class=\"plot-directive\" src=\"../_images/signal-1_01_00.png\"/>\n</figure>"
        ],
        "matched_tutorial_paths": [
            [
                "scipy->Signal Processing (scipy.signal)->Spectral Analysis->Periodogram Measurements"
            ],
            [
                "scipy->Signal Processing (scipy.signal)->Spectral Analysis->Spectral Analysis using Welch\u00e2\u0080\u0099s Method"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Introduction to pyplot->Formatting the style of your plot"
            ],
            [
                "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Approximation"
            ],
            [
                "scipy->Signal Processing (scipy.signal)->B-splines"
            ]
        ]
    },
    "797363": {
        "jupyter_code_cell": "display.Image(filename='Figures/wandering.png',width=500)\ndisplay.Image(filename='Figures/plates.png')",
        "matched_tutorial_code_inds": [
            161,
            1458,
            6271,
            3793,
            4157
        ],
        "matched_tutorial_codes": [
            "compare.trim_significant_figures()\ncompare.colorize()\ncompare.print()",
            "load_xy = np.load(\"x_y-squared.npz\")\n\nprint(load_xy.files)",
            "plt.rc(\"figure\", figsize=(16, 12))\nplt.rc(\"font\", size=13)",
            "sns.barplot(x='cut', y='price', data=df)\nsns.despine()\nplt.tight_layout()",
            "sns.set_style(\"whitegrid\")\nsns.boxplot(data=data, palette=\"deep\")\nsns.despine(left=True)\n"
        ],
        "matched_tutorial_paths": [
            [
                "torch->PyTorch Recipes->PyTorch Benchmark->Steps->5. Comparing benchmark results"
            ],
            [
                "numpy->NumPy Features->Saving and sharing your NumPy arrays->Remove the saved arrays and load them back with NumPy\u2019s"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "seaborn->User guide and tutorial->Controlling figure aesthetics->Removing axes spines",
                "seaborn->Figure aesthetics->Controlling figure aesthetics->Removing axes spines"
            ]
        ]
    },
    "1254723": {
        "jupyter_code_cell": "def visualizeBoundaryLinear(model):\n    global ex1_df\n    theta_0 = model.intercept_[0]\n    theta_1 = model.coef_[0,0]\n    theta_2 = model.coef_[0,1]\n    x1 = ex1_df[\"x1\"]\n    xRange = np.linspace(x1.min(), x1.max(), 100)\n    decisionBoundary = -(theta_0 + theta_1*xRange) / theta_2\n    plotData(ex1_df)\n    plt.plot(xRange, decisionBoundary, color=\"g\")\n    plt.show()\n    return\nvisualizeBoundaryLinear(linear_svc_c1)\nlinear_svc_c100 = svm.SVC(C=100, kernel=\"linear\")\nlinear_svc_c100.fit(ex1_X, ex1_y)\nvisualizeBoundaryLinear(linear_svc_c100)",
        "matched_tutorial_code_inds": [
            6276,
            3103,
            2398,
            1070,
            21
        ],
        "matched_tutorial_codes": [
            "def add_stl_plot(fig, res, legend):\n    \"\"\"Add 3 plots from a second STL fit\"\"\"\n    axs = fig.get_axes()\n    comps = [\"trend\", \"seasonal\", \"resid\"]\n    for ax, comp in zip(axs[1:], comps):\n        series = getattr(res, comp)\n        if comp == \"resid\":\n            ax.plot(series, marker=\"o\", linestyle=\"none\")\n        else:\n            ax.plot(series)\n            if comp == \"trend\":\n                ax.legend(legend, frameon=False)\n\n\nstl = STL(elec_equip, period=12, robust=True)\nres_robust = stl.fit()\nfig = res_robust.plot()\nres_non_robust = STL(elec_equip, period=12, robust=False).fit()\nadd_stl_plot(fig, res_non_robust, [\"Robust\", \"Non-robust\"])\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_stl_decomposition_10_0.png\" src=\"../../../_images/examples_notebooks_generated_stl_decomposition_10_0.png\"/>",
            "def get_impute_iterative(X_missing, y_missing):\n    imputer = (\n        missing_values=,\n        add_indicator=True,\n        random_state=0,\n        n_nearest_features=3,\n        max_iter=1,\n        sample_posterior=True,\n    )\n    iterative_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n    return iterative_impute_scores.mean(), iterative_impute_scores.std()\n\n\nmses_california[4], stds_california[4] = get_impute_iterative(\n    X_miss_california, y_miss_california\n)\nmses_diabetes[4], stds_diabetes[4] = get_impute_iterative(\n    X_miss_diabetes, y_miss_diabetes\n)\nx_labels.append(\"Iterative Imputation\")\n\nmses_diabetes = mses_diabetes * -1\nmses_california = mses_california * -1",
            "def plot_accuracy(x, y, x_legend):\n    \"\"\"Plot accuracy as a function of x.\"\"\"\n    x = (x)\n    y = (y)\n    (\"Classification accuracy as a function of %s\" % x_legend)\n    (\"%s\" % x_legend)\n    (\"Accuracy\")\n    (True)\n    (x, y)\n\n\n[\"legend.fontsize\"] = 10\ncls_names = list(sorted(cls_stats.keys()))\n\n# Plot accuracy evolution\n()\nfor _, stats in sorted(cls_stats.items()):\n    # Plot accuracy evolution with #examples\n    accuracy, n_examples = zip(*stats[\"accuracy_history\"])\n    plot_accuracy(n_examples, accuracy, \"training examples (#)\")\n    ax = ()\n    ax.set_ylim((0.8, 1))\n(cls_names, loc=\"best\")\n\n()\nfor _, stats in sorted(cls_stats.items()):\n    # Plot accuracy evolution with runtime\n    accuracy, runtime = zip(*stats[\"runtime_history\"])\n    plot_accuracy(runtime, accuracy, \"runtime (s)\")\n    ax = ()\n    ax.set_ylim((0.8, 1))\n(cls_names, loc=\"best\")\n\n# Plot fitting times\n()\nfig = ()\ncls_runtime = [stats[\"total_fit_time\"] for cls_name, stats in sorted(cls_stats.items())]\n\ncls_runtime.append(total_vect_time)\ncls_names.append(\"Vectorization\")\nbar_colors = [\"b\", \"g\", \"r\", \"c\", \"m\", \"y\"]\n\nax = (111)\nrectangles = (range(len(cls_names)), cls_runtime, width=0.5, color=bar_colors)\n\nax.set_xticks((0, len(cls_names) - 1, len(cls_names)))\nax.set_xticklabels(cls_names, fontsize=10)\nymax = max(cls_runtime) * 1.2\nax.set_ylim((0, ymax))\nax.set_ylabel(\"runtime (s)\")\nax.set_title(\"Training Times\")\n\n\ndef autolabel(rectangles):\n    \"\"\"attach some text vi autolabel on rectangles.\"\"\"\n    for rect in rectangles:\n        height = rect.get_height()\n        ax.text(\n            rect.get_x() + rect.get_width() / 2.0,\n            1.05 * height,\n            \"%.4f\" % height,\n            ha=\"center\",\n            va=\"bottom\",\n        )\n        (()[1], rotation=30)\n\n\nautolabel(rectangles)\n()\n()\n\n# Plot prediction times\n()\ncls_runtime = []\ncls_names = list(sorted(cls_stats.keys()))\nfor cls_name, stats in sorted(cls_stats.items()):\n    cls_runtime.append(stats[\"prediction_time\"])\ncls_runtime.append(parsing_time)\ncls_names.append(\"Read/Parse\\n+Feat.Extr.\")\ncls_runtime.append(vectorizing_time)\ncls_names.append(\"Hashing\\n+Vect.\")\n\nax = (111)\nrectangles = (range(len(cls_names)), cls_runtime, width=0.5, color=bar_colors)\n\nax.set_xticks((0, len(cls_names) - 1, len(cls_names)))\nax.set_xticklabels(cls_names, fontsize=8)\n(()[1], rotation=30)\nymax = max(cls_runtime) * 1.2\nax.set_ylim((0, ymax))\nax.set_ylabel(\"runtime (s)\")\nax.set_title(\"Prediction Times (%d instances)\" % n_test_documents)\nautolabel(rectangles)\n()\n()\n\n\n\n<img alt=\"Classification accuracy as a function of training examples (#)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_out_of_core_classification_001.png\" srcset=\"../../_images/sphx_glr_plot_out_of_core_classification_001.png\"/>\n<img alt=\"Classification accuracy as a function of runtime (s)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_out_of_core_classification_002.png\" srcset=\"../../_images/sphx_glr_plot_out_of_core_classification_002.png\"/>\n<img alt=\"Training Times\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_out_of_core_classification_003.png\" srcset=\"../../_images/sphx_glr_plot_out_of_core_classification_003.png\"/>\n<img alt=\"Prediction Times (1000 instances)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_out_of_core_classification_004.png\" srcset=\"../../_images/sphx_glr_plot_out_of_core_classification_004.png\"/>",
            "def time_model_evaluation(model, configs, tokenizer):\n    eval_start_time = time.time()\n    result = evaluate(configs, model, tokenizer, prefix=\"\")\n    eval_end_time = time.time()\n    eval_duration_time = eval_end_time - eval_start_time\n    print(result)\n    print(\"Evaluate total time (seconds): {0:.1f}\".format(eval_duration_time))\n\n# Evaluate the original FP32 BERT model\ntime_model_evaluation(model, configs, tokenizer)\n\n# Evaluate the INT8 BERT model after the dynamic quantization\ntime_model_evaluation(quantized_model, configs, tokenizer)",
            "def show_landmarks(image, landmarks):\n    \"\"\"Show image with landmarks\"\"\"\n    plt.imshow(image)\n    plt.scatter(landmarks[:, 0], landmarks[:, 1], s=10, marker='.', c='r')\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\nplt.figure()\nshow_landmarks(io.imread(os.path.join('faces/', img_name)),\n               landmarks)\nplt.show()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition->Robust Fitting"
            ],
            [
                "sklearn->Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score->Iterative imputation of the missing values"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Out-of-core classification of text documents->Plot results"
            ],
            [
                "torch->Model Optimization->(beta) Dynamic Quantization on BERT->3. Apply the dynamic quantization->3.2 Evaluate the inference accuracy and time"
            ],
            [
                "torch->PyTorch Recipes->Developing Custom PyTorch Dataloaders->Part 1: The Dataset->1.1 Write a simple helper function to show an image"
            ]
        ]
    },
    "630340": {
        "jupyter_code_cell": "ros = RandomOverSampler() \nx_new_data, y_new_data = ros.fit_sample(x_old_data, y_old_data)\nfrom collections import Counter\nCounter(y_new_data)\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x_new_data, y_new_data, test_size=0.2,random_state=0)",
        "matched_tutorial_code_inds": [
            3079,
            5197,
            5998,
            5976,
            5969
        ],
        "matched_tutorial_codes": [
            "rfc = (n_estimators=10, random_state=42)\nrfc.fit(X_train, y_train)\nax = ()\nrfc_disp = (rfc, X_test, y_test, ax=ax, alpha=0.8)\nsvc_disp.plot(ax=ax, alpha=0.8)\n()\n\n\n<img alt=\"plot roc curve visualization api\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_roc_curve_visualization_api_002.png\" srcset=\"../../_images/sphx_glr_plot_roc_curve_visualization_api_002.png\"/>",
            "ols_results2 = sm.OLS(y.iloc[:14], X.iloc[:14]).fit()\nprint(\n    \"Percentage change %4.2f%%\\n\"\n    * 7\n    % tuple(\n        [\n            i\n            for i in (ols_results2.params - ols_results.params)\n            / ols_results.params\n            * 100\n        ]\n    )\n)",
            "res5 = combine_effects(eff, var_eff, method_re=\"chi2\", use_t=False)\nres5_df = res5.summary_frame()\nprint(\"method RE:\", res5.method_re)\nprint(res5.summary_frame())\nfig = res5.plot_forest()\nfig.set_figheight(8)\nfig.set_figwidth(6)",
            "res3 = combine_effects(eff, var_eff, method_re=\"chi2\", use_t=False, row_names=rownames)\n# TODO: we still need better information about conf_int of individual samples\n# We don't have enough information in the model for individual confidence intervals\n# if those are not based on normal distribution.\nres3.conf_int_samples(nobs=np.array(nobs1 + nobs2))\nprint(res3.summary_frame())",
            "res3 = combine_effects(eff, var_eff, method_re=\"chi2\", use_t=True, row_names=rownames)\n# TODO: we still need better information about conf_int of individual samples\n# We don't have enough information in the model for individual confidence intervals\n# if those are not based on normal distribution.\nres3.conf_int_samples(nobs=np.array(nobs1 + nobs2))\nprint(res3.summary_frame())"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Miscellaneous->ROC Curve with Visualization API->Training a Random Forest and Plotting the ROC Curve"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity->Dropping an observation"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Meta-analysis of proportions->changing data to have positive random effects variance"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Example->Using one-step chi2, DerSimonian-Laird estimate for random effects variance tau"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Example->Using one-step chi2, DerSimonian-Laird estimate for random effects variance tau"
            ]
        ]
    },
    "244408": {
        "jupyter_code_cell": "wp_posts_with_eng = pd.concat([wp_posts, eng_des, image], axis=0)\nwp_posts_with_eng = wp_posts_with_eng.sort_values('ID') ",
        "matched_tutorial_code_inds": [
            5885,
            3951,
            5703,
            5381,
            5296
        ],
        "matched_tutorial_codes": [
            "_ = plt.boxplot([scales[0][0], scales[0][1], scales[1][0], scales[1][1]])\nplt.ylabel(\"Estimated scale\")",
            "flights_array = flights_wide.to_numpy()\nsns.relplot(data=flights_array, kind=\"line\")\n",
            "summ_df = infl.summary_frame()\nsumm_df.sort_values(\"cooks_d\", ascending=False)[:10]",
            "spector_data = sm.datasets.spector.load()\nspector_data.exog = sm.add_constant(spector_data.exog, prepend=False)",
            "pred_ols = res_ols.get_prediction()\niv_l_ols = pred_ols.summary_frame()[\"obs_ci_lower\"]\niv_u_ols = pred_ols.summary_frame()[\"obs_ci_upper\"]"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Generalized Estimating Equations->GEE Score Tests"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing wide-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing wide-form data"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Influence Measures for GLMs->get the influence measures"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Data"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Weighted Least Squares->OLS vs.\u00a0WLS"
            ]
        ]
    },
    "1019751": {
        "jupyter_code_cell": "word_model_ids = [1,2]\nword_embeddings = {}\nloaded_embeddings = neural_embeddings.get_embeddings(\"ft\", model_ids=word_model_ids, load=True)\nword_embeddings[\"core_hate\"] = loaded_embeddings[0]\nword_embeddings[\"core_clean\"] = loaded_embeddings[1]\n_es = elasticsearch_base.connect(settings.ES_URL)\npositive_hs_filter = \"_exists_:hs_keyword_matches\"\nnegative_hs_filter = \"!_exists_:hs_keyword_matches\"\nhs_keywords = set(file_ops.read_csv_file(\"refined_hs_keywords\", settings.TWITTER_SEARCH_PATH))",
        "matched_tutorial_code_inds": [
            2096,
            3313,
            2951,
            383,
            3474
        ],
        "matched_tutorial_codes": [
            "node_indicator = clf.decision_path(X_test)\nleaf_id = clf.apply(X_test)\n\nsample_id = 0\n# obtain ids of the nodes `sample_id` goes through, i.e., row `sample_id`\nnode_index = node_indicator.indices[\n    node_indicator.indptr[sample_id] : node_indicator.indptr[sample_id + 1]\n]\n\nprint(\"Rules used to predict sample {id}:\\n\".format(id=sample_id))\nfor node_id in node_index:\n    # continue to the next node if it is a leaf node\n    if leaf_id[sample_id] == node_id:\n        continue\n\n    # check if value of the split feature for sample 0 is below threshold\n    if X_test[sample_id, feature[node_id]] &lt;= threshold[node_id]:\n        threshold_sign = \"&lt;=\"\n    else:\n        threshold_sign = \"\"\n\n    print(\n        \"decision node {node} : (X_test[{sample}, {feature}] = {value}) \"\n        \"{inequality} {threshold})\".format(\n            node=node_id,\n            sample=sample_id,\n            feature=feature[node_id],\n            value=X_test[sample_id, feature[node_id]],\n            inequality=threshold_sign,\n            threshold=threshold[node_id],\n        )\n    )",
            "categories = [\"sci.med\", \"sci.space\"]\nX_train, y_train = (\n    random_state=1,\n    subset=\"train\",\n    categories=categories,\n    remove=(\"footers\", \"quotes\"),\n    return_X_y=True,\n)\nX_test, y_test = (\n    random_state=1,\n    subset=\"test\",\n    categories=categories,\n    remove=(\"footers\", \"quotes\"),\n    return_X_y=True,\n)",
            "cluster_ids = (dist_linkage, 1, criterion=\"distance\")\ncluster_id_to_feature_ids = (list)\nfor idx, cluster_id in enumerate(cluster_ids):\n    cluster_id_to_feature_ids[cluster_id].append(idx)\nselected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n\nX_train_sel = X_train[:, selected_features]\nX_test_sel = X_test[:, selected_features]\n\nclf_sel = (n_estimators=100, random_state=42)\nclf_sel.fit(X_train_sel, y_train)\nprint(\n    \"Accuracy on test data with features removed: {:.2f}\".format(\n        clf_sel.score(X_test_sel, y_test)\n    )\n)",
            "model_ft = (pretrained=True)\nnum_ftrs = .in_features\n# Here the size of each output sample is set to 2.\n# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n = (num_ftrs, 2)\n\nmodel_ft = ()\n\n = ()\n\n# Observe that all parameters are being optimized\n = ((), lr=0.001, momentum=0.9)\n\n# Decay LR by a factor of 0.1 every 7 epochs\n = (, step_size=7, gamma=0.1)",
            "model_l2 = (penalty=\"l2\", loss=\"squared_hinge\", dual=True)\nCs = (-4.5, -2, 10)\n\nlabels = [f\"fraction: {train_size}\" for train_size in train_sizes]\nresults = {\"C\": Cs}\nfor label, train_size in zip(labels, train_sizes):\n    cv = (train_size=train_size, test_size=0.3, n_splits=50, random_state=1)\n    train_scores, test_scores = (\n        model_l2, X, y, param_name=\"C\", param_range=Cs, cv=cv\n    )\n    results[label] = test_scores.mean(axis=1)\nresults = (results)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Decision Trees->Understanding the decision tree structure->Decision path"
            ],
            [
                "sklearn->Examples->Pipelines and composite estimators->Column Transformer with Heterogeneous Data Sources->20 newsgroups dataset"
            ],
            [
                "sklearn->Examples->Inspection->Permutation Importance with Multicollinear or Correlated Features->Handling Multicollinear Features"
            ],
            [
                "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->Finetuning the convnet"
            ],
            [
                "sklearn->Examples->Support Vector Machines->Scaling the regularization parameter for SVCs->L2-penalty case"
            ]
        ]
    },
    "581774": {
        "jupyter_code_cell": "print(\"Skew is:\", train.SalePrice.skew())\nplt.hist(train.SalePrice, color = \"blue\")\nplt.show()\ntarget = np.log(train.SalePrice)\nprint(\"Skew is:\", target.skew())\nplt.hist(target, color = 'blue')\nplt.show()",
        "matched_tutorial_code_inds": [
            3951,
            6777,
            4144,
            2066,
            6835
        ],
        "matched_tutorial_codes": [
            "flights_array = flights_wide.to_numpy()\nsns.relplot(data=flights_array, kind=\"line\")\n",
            "res1 = ols(formula=\"Lottery ~ Literacy : Wealth - 1\", data=df).fit()\nres2 = ols(formula=\"Lottery ~ Literacy * Wealth - 1\", data=df).fit()\nprint(res1.params, \"\\n\")\nprint(res2.params)",
            "g = sns.PairGrid(tips, hue=\"size\", palette=\"GnBu_d\")\ng.map(plt.scatter, s=50, edgecolor=\"white\")\ng.add_legend()\n",
            "print(f\"PCR r-squared {pcr.score(X_test, y_test):.3f}\")\nprint(f\"PLS r-squared {pls.score(X_test, y_test):.3f}\")",
            "# Compute the root mean square error\nrmse = (flattened**2).mean(axis=1)**0.5\n\nprint(rmse)"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing wide-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing wide-form data"
            ],
            [
                "statsmodels->Examples->User Notes->Formulas->Multiplicative interactions"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships"
            ],
            [
                "sklearn->Examples->Cross decomposition->Principal Component Regression vs Partial Least Squares Regression->Projection on one component and predictive power"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example",
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example->Using extend"
            ]
        ]
    },
    "143534": {
        "jupyter_code_cell": "import pandas as pd\nimport matplotlib.pyplot as plt \nimport numpy as np\nstatictable = pd.read_excel('Lab5/staticmethoddata.xlsx')\nstatictablereverse = pd.read_excel('Lab5/staticreverse.xlsx')\nstaticdata = statictable.as_matrix()\nstaticdatareverse = statictablereverse.as_matrix()\nzero = 0.170\nstaticdata[:, 1] -= zero\nstaticdatareverse[:,1] -= zero\nstatictable",
        "matched_tutorial_code_inds": [
            1393,
            4700,
            4946,
            466,
            3367
        ],
        "matched_tutorial_codes": [
            "import matplotlib.pyplot as plt\nimport numpy as np\nidx = 5\nprint(\"Question: \", dataset[\"train\"][idx][\"question\"])\nprint(\"Answers: \" ,dataset[\"train\"][idx][\"answers\"])\nim = np.asarray(dataset[\"train\"][idx][\"image\"].resize((500,500)))\nplt.imshow(im)\nplt.show()\n\n\n<img alt=\"flava finetuning tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_flava_finetuning_tutorial_001.png\" srcset=\"../_images/sphx_glr_flava_finetuning_tutorial_001.png\"/>",
            "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom cycler import cycler\nmpl.rcParams['lines.linewidth'] = 2\nmpl.rcParams['lines.linestyle'] = '--'\ndata = np.random.randn(50)\nplt.plot(data)\n\n\n<img alt=\"customizing\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_customizing_001.png\" srcset=\"../../_images/sphx_glr_customizing_001.png, ../../_images/sphx_glr_customizing_001_2_0x.png 2.0x\"/>",
            "import matplotlib.pyplot as plt\nimport numpy as np\n\nx1 = np.linspace(0.0, 5.0, 100)\ny1 = np.cos(2 * np.pi * x1) * np.exp(-x1)\n\nfig, ax = plt.subplots(figsize=(5, 3))\nfig.subplots_adjust(bottom=0.15, left=0.2)\nax.plot(x1, y1)\nax.set_xlabel('Time [s]')\nax.set_ylabel('Damped oscillation [V]')\n\nplt.show()\n\n\n<img alt=\"text intro\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_text_intro_002.png\" srcset=\"../../_images/sphx_glr_text_intro_002.png, ../../_images/sphx_glr_text_intro_002_2_0x.png 2.0x\"/>",
            "import matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\nplt.figure()\nplt.plot(all_losses)\n\n\n<img alt=\"char rnn classification tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_char_rnn_classification_tutorial_001.png\" srcset=\"../_images/sphx_glr_char_rnn_classification_tutorial_001.png\"/>",
            "import pandas as pd\n\ncv_results = (search_cv.cv_results_)\ncv_results = cv_results.sort_values(\"mean_test_score\", ascending=False)\ncv_results[\n    [\n        \"mean_test_score\",\n        \"std_test_score\",\n        \"param_preprocessor__num__imputer__strategy\",\n        \"param_preprocessor__cat__selector__percentile\",\n        \"param_classifier__C\",\n    ]\n].head(5)\n\n\n\n\n\n\n\n\n<br/>\n<br/>"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Multimodality->TorchMultimodal Tutorial: Finetuning FLAVA->Steps"
            ],
            [
                "matplotlib->Tutorials->Introductory->Customizing Matplotlib with style sheets and rcParams->Runtime rc settings"
            ],
            [
                "matplotlib->Tutorials->Text->Text in Matplotlib Plots->Labels for x- and y-axis"
            ],
            [
                "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN->Training->Plotting the Results"
            ],
            [
                "sklearn->Examples->Pipelines and composite estimators->Column Transformer with Mixed Types"
            ]
        ]
    },
    "1487228": {
        "jupyter_code_cell": "users = pd.read_table(url2, sep='|', header=0)\nusers.head()\nuser_names = ['user_id', 'age', 'gender', 'occupation', 'zip_code']\nusers = pd.read_table(url2, sep='|', header=None, names=user_names)\nusers.head()",
        "matched_tutorial_code_inds": [
            3699,
            5381,
            3616,
            4066,
            3748
        ],
        "matched_tutorial_codes": [
            "files = glob.glob('weather/*.csv')\nweather_dfs = [pd.read_csv(fp, names=columns) for fp in files]\nweather = pd.concat(weather_dfs)",
            "spector_data = sm.datasets.spector.load()\nspector_data.exog = sm.add_constant(spector_data.exog, prepend=False)",
            "hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()",
            "g = sns.PairGrid(penguins)\ng.map_upper(sns.histplot)\ng.map_lower(sns.kdeplot, fill=True)\ng.map_diag(sns.histplot, kde=True)\n",
            "tidy = pd.melt(games.reset_index(),\n               id_vars=['game_id', 'date'], value_vars=['away_team', 'home_team'],\n               value_name='team')\ntidy.head()"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Data"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions",
                "seaborn->Plotting functions->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data"
            ]
        ]
    },
    "772060": {
        "jupyter_code_cell": "image_check = int(np.random.uniform(30000))\nimage = X_train[image_check,:,:,:]\nfig,ax = plt.subplots(1,3, figsize = (10,5))\nimage1 = transform_image(image,80,0,0)\nimage2 = transform_image(image,0,10,0)\nimage3 = transform_image(image,0,0,10)\nax[0].set_title('Rotating image.')\nax[0].imshow(image1)\nax[1].set_title('Shear transforming image')\nax[1].imshow(image2)\nax[2].set_title('Translating image')\nax[2].imshow(image3)\nprint(\"Image data shape =\", image_shape)\nX_train_valid = np.concatenate((X_train, X_valid), axis = 0)\ny_train_valid = np.concatenate((y_train, y_valid), axis = 0)",
        "matched_tutorial_code_inds": [
            6795,
            3257,
            2741,
            93,
            2124
        ],
        "matched_tutorial_codes": [
            "x1n = np.linspace(20.5, 25, 10)\nXnew = np.column_stack((x1n, np.sin(x1n), (x1n - 5) ** 2))\nXnew = sm.add_constant(Xnew)\nynewpred = olsres.predict(Xnew)  # predict out of sample\nprint(ynewpred)",
            "alphas = (-5, 1, 60)\nenet = (l1_ratio=0.7, max_iter=10000)\ntrain_errors = list()\ntest_errors = list()\nfor alpha in alphas:\n    enet.set_params(alpha=alpha)\n    enet.fit(X_train, y_train)\n    train_errors.append(enet.score(X_train, y_train))\n    test_errors.append(enet.score(X_test, y_test))\n\ni_alpha_optim = (test_errors)\nalpha_optim = alphas[i_alpha_optim]\nprint(\"Optimal regularization parameter : %s\" % alpha_optim)\n\n# Estimate the coef_ on full data with optimal regularization parameter\nenet.set_params(alpha=alpha_optim)\ncoef_ = enet.fit(X, y).coef_",
            "x_train = (0, 10, 100)\nrng = (0)\nx_train = (rng.choice(x_train, size=20, replace=False))\ny_train = f(x_train)\n\n# create 2D-array versions of these arrays to feed to transformers\nX_train = x_train[:, ]\nX_plot = x_plot[:, ]",
            "x = (-5, 5, 0.1).view(-1, 1)\ny = -5 * x + 0.1 * (x.size())\n\nmodel = (1, 1)\ncriterion = ()\noptimizer = (model.parameters(), lr = 0.1)\n\ndef train_model(iter):\n    for epoch in range(iter):\n        y1 = model(x)\n        loss = criterion(y1, y)\n        writer.add_scalar(\"Loss/train\", loss, epoch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\ntrain_model(10)\nwriter.flush()",
            "n_comps = 2\n\nmethods = [\n    (\"PCA\", ()),\n    (\"Unrotated FA\", ()),\n    (\"Varimax FA\", (rotation=\"varimax\")),\n]\nfig, axes = (ncols=len(methods), figsize=(10, 8), sharey=True)\n\nfor ax, (method, fa) in zip(axes, methods):\n    fa.set_params(n_components=n_comps)\n    fa.fit(X)\n\n    components = fa.components_.T\n    print(\"\\n\\n %s :\\n\" % method)\n    print(components)\n\n    vmax = np.abs(components).max()\n    ax.imshow(components, cmap=\"RdBu_r\", vmax=vmax, vmin=-vmax)\n    ax.set_yticks((len(feature_names)))\n    ax.set_yticklabels(feature_names)\n    ax.set_title(str(method))\n    ax.set_xticks([0, 1])\n    ax.set_xticklabels([\"Comp. 1\", \"Comp. 2\"])\nfig.suptitle(\"Factors\")\n()\n()\n\n\n<img alt=\"Factors, PCA, Unrotated FA, Varimax FA\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_varimax_fa_002.png\" srcset=\"../../_images/sphx_glr_plot_varimax_fa_002.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->User Notes->Prediction->Create a new sample of explanatory variables Xnew, predict and plot"
            ],
            [
                "sklearn->Examples->Model Selection->Train error vs Test error->Compute train and test errors"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Polynomial and Spline interpolation"
            ],
            [
                "torch->PyTorch Recipes->How to use TensorBoard with PyTorch->Log scalars"
            ],
            [
                "sklearn->Examples->Decomposition->Factor Analysis (with rotation) to visualize patterns"
            ]
        ]
    },
    "357432": {
        "jupyter_code_cell": "emp = jd.EmploymentData()\nemp_df, industry_name = emp.extract_data_for_naics(211)\nemp.emp_plot(emp_df, industry_name)\nprices = jd.PriceData()\nprices.plot()",
        "matched_tutorial_code_inds": [
            1746,
            5399,
            5588,
            6704,
            5312
        ],
        "matched_tutorial_codes": [
            "glove = data.fetch('glove.6B.50d.zip')\nemb_path = textproc.unzipper(glove, 'glove.6B.300d.txt')\nemb_matrix = textproc.loadGloveModel(emb_path)",
            "rand_data = sm.datasets.randhie.load()\nrand_exog = rand_data.exog\nrand_exog = sm.add_constant(rand_exog, prepend=False)",
            "data2 = sm.datasets.scotland.load()\ndata2.exog = sm.add_constant(data2.exog, prepend=False)\nprint(data2.exog.head())\nprint(data2.endog.head())",
            "ax = dta.mean().plot(grid=False)\nax.set_xlabel(\"Year\", size=17)\nax.set_ylabel(\"Fertility rate\", size=17)\nax.set_xlim(0, 51)",
            "oo = np.ones(df.shape[0])\nmodel2 = sm.MixedLM(df.y, oo, exog_re=oo, groups=df.group1, exog_vc=vcs)\nresult2 = model2.fit()\nprint(result2.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Poisson"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Generalized Linear Models Overview->GLM: Gamma for proportional count response->Load Scottish Parliament Voting data"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Nested analysis"
            ]
        ]
    },
    "1467939": {
        "jupyter_code_cell": "data.describe()\nminyr = data[data.YearBuilt==1196].index[0]\nmaxyr = data[data.YearBuilt==2106].index[0]",
        "matched_tutorial_code_inds": [
            3656,
            5703,
            2423,
            3945,
            5512
        ],
        "matched_tutorial_codes": [
            "weather2 = weather.reset_index()\nweather2[weather2['station'] == 'DSM'].head()",
            "summ_df = infl.summary_frame()\nsumm_df.sort_values(\"cooks_d\", ascending=False)[:10]",
            "all_splits = list(ts_cv.split(X, y))\ntrain_0, test_0 = all_splits[0]",
            "year = flights_avg.index\npassengers = flights_avg[\"passengers\"]\nsns.relplot(x=year, y=passengers, kind=\"line\")\n",
            "data_student.apply_str = pd.Categorical(data_student.apply_str, ordered=True)\ndata_student.public = data_student.public.astype(float)\ndata_student.pared = data_student.pared.astype(float)"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Indexes->Flavors->Row Slicing"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Influence Measures for GLMs->get the influence measures"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Time-based cross-validation"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing long-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing long-form data"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - treatment of endog"
            ]
        ]
    },
    "1026713": {
        "jupyter_code_cell": "allmeans = list(md['chisq']['means'].values())\nc_allmeans = allmeans\npl.figure(figsize=(8, 8))\npl.hist(allmeans, bins=40, normed=True)\npl.xlabel('sample mean', fontsize = 18)\npl.ylabel('Frequency', fontsize = 18)\nfit_mean = np.mean(allmeans)\nfit_var = np.var(allmeans)\nfit_std = np.sqrt(fit_var)\nx = np.linspace(min(allmeans), max(allmeans),100)\npl.plot(x,mlab.normpdf(x,fit_mean,fit_std), color='red', alpha=0.5)\npl.legend(['fit curve'], loc='best')\nmd = {}\nmd['normal'] = np.random.normal(popMean, scale=100, size=100)",
        "matched_tutorial_code_inds": [
            5902,
            2753,
            5645,
            5643,
            5647
        ],
        "matched_tutorial_codes": [
            "resid = lm.resid\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    group_num = i * 2 + j - 1  # for plotting purposes\n    x = [group_num] * len(group)\n    plt.scatter(\n        x,\n        resid[group.index],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\nplt.xlabel(\"Group\")\nplt.ylabel(\"Residuals\")",
            "quantiles = [0.05, 0.5, 0.95]\npredictions = {}\nout_bounds_predictions = (y_true_mean, dtype=)\nfor quantile in quantiles:\n    qr = (quantile=quantile, alpha=0, solver=solver)\n    y_pred = qr.fit(X, y_pareto).predict(X)\n    predictions[quantile] = y_pred\n\n    if quantile == min(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred = y_pareto\n        )\n    elif quantile == max(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred &lt;= y_pareto\n        )",
            "glm = smf.glm(\n    \"affairs_sum ~ rate_marriage + yrs_married\",\n    data=df_a,\n    family=sm.families.Poisson(),\n    exposure=np.asarray(df_a[\"affairs_count\"]),\n)\nres_e2 = glm.fit()\nres_e2.pearson_chi2 - res_e.pearson_chi2, res_e2.deviance - res_e.deviance, res_e2.llf - res_e.llf",
            "glm = smf.glm(\n    \"affairs ~ rate_marriage + yrs_married\",\n    data=dc,\n    family=sm.families.Poisson(),\n    freq_weights=np.asarray(dc[\"freq\"]),\n)\nres_f2 = glm.fit()\n# print(res_f2.summary())\nres_f2.pearson_chi2 - res_f.pearson_chi2, res_f2.deviance - res_f.deviance, res_f2.llf - res_f.llf",
            "glm = smf.glm(\n    \"affairs_mean ~ rate_marriage + yrs_married\",\n    data=df_a,\n    family=sm.families.Poisson(),\n    var_weights=np.asarray(df_a[\"affairs_count\"]),\n)\nres_a2 = glm.fit()\nres_a2.pearson_chi2 - res_a.pearson_chi2, res_a2.deviance - res_a.deviance, res_a2.llf - res_a.llf"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Quantile regression->Fitting a QuantileRegressor"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->aggregated data: exposure and var_weights"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->original observations and frequency weights"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->aggregated data: exposure and var_weights"
            ]
        ]
    },
    "1280451": {
        "jupyter_code_cell": "cantons_pairs_fr = {'Canton':{pair[1]: pair[0] for pair in canton_id_name_fr}}\ncantons_pairs_de = {'Canton':{pair[1]: pair[0] for pair in canton_id_name_de}}\ncantons_pairs_de",
        "matched_tutorial_code_inds": [
            6343,
            155,
            1741,
            5496,
            6360
        ],
        "matched_tutorial_codes": [
            "sarimax_exog_res = SARIMAX(y, exog=x, order=(1, 0, 0), trend=\"c\").fit()\narima_exog_res = ARIMA(y, exog=x, order=(1, 0, 0), trend=\"c\").fit()",
            "m0 = t0.blocked_autorange()\nm1 = t1.blocked_autorange()\n\nprint(m0)\nprint(m1)\n\n\n\nOutput",
            "imdb_train = data.fetch('imdb_train.txt')\nimdb_test = data.fetch('imdb_test.txt')",
            "predicted = res_log.model.predict(res_log.params, exog=data_student[['pared', 'public', 'gpa']])\npredicted",
            "delta_hat, rho_hat = sarima_res.params[:2]\ndelta_hat + rho_hat * y[0]"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Comparing trends and exogenous variables in SARIMAX, ARIMA and AutoReg->Using exog in SARIMAX and ARIMA"
            ],
            [
                "torch->PyTorch Recipes->PyTorch Benchmark->Steps->4. Benchmarking with Blocked Autorange"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Logit ordinal regression:"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Reconstructing residuals, fitted values and forecasts in SARIMAX and ARIMA"
            ]
        ]
    },
    "999816": {
        "jupyter_code_cell": "for (key, content) in result:\n    print(key)\n    print(content)\n    print(\"=========================\")\ngrouped = df.groupby('key1')",
        "matched_tutorial_code_inds": [
            2677,
            1462,
            4774,
            5470,
            183
        ],
        "matched_tutorial_codes": [
            "def highlight_min(x):\n    x_min = x.min()\n    return [\"font-weight: bold\" if v == x_min else \"\" for v in x]\n\n\nresults.style.apply(highlight_min)\n\n\n\n\n\n\n<br/>\n<br/>",
            "array_out = np.block([x[:, np.newaxis], y[:, np.newaxis]])\nprint(\"the output array has shape \", array_out.shape, \" with values:\")\nprint(array_out)",
            "from cycler import cycler\ncc = (cycler(color=list('rgb')) *\n      cycler(linestyle=['-', '--', '-.']))\nfor d in cc:\n    print(d)",
            "means75 = exog.mean()\nmeans75[\"LOWINC\"] = exog[\"LOWINC\"].quantile(0.75)\nprint(means75)",
            "epochs = 5\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(, model, , )\n    test(, model, )\nprint(\"Done!\")"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Generalized Linear Models->Lasso model selection: AIC-BIC / cross-validation->Selecting Lasso via an information criterion"
            ],
            [
                "numpy->NumPy Features->Saving and sharing your NumPy arrays->Rearrange the data into a single 2D array"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Styling with cycler->Cycling through multiple properties"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution"
            ],
            [
                "torch->Introduction to PyTorch->Quickstart->Optimizing the Model Parameters"
            ]
        ]
    },
    "989685": {
        "jupyter_code_cell": "nbDays = '30'\ndf = lf.retrieve_top_tracks_as_dataframe(cursor, nbDays, 10)\niplot(lf.create_figure(df.Track, df.ArtistAlbumTrack, df.PlayCount, 'tracks', nbDays))\nnbDays = '90'\ndf = lf.retrieve_top_tracks_as_dataframe(cursor, nbDays, 10)\niplot(lf.create_figure(df.Track, df.ArtistAlbumTrack, df.PlayCount, 'tracks', nbDays))",
        "matched_tutorial_code_inds": [
            2662,
            6255,
            6253,
            6269,
            6264
        ],
        "matched_tutorial_codes": [
            "m, s, _ = (\n    (enet.coef_)[0],\n    enet.coef_[enet.coef_ != 0],\n    markerfmt=\"x\",\n    label=\"Elastic net coefficients\",\n)\n([m, s], color=\"#2ca02c\")\nm, s, _ = (\n    (lasso.coef_)[0],\n    lasso.coef_[lasso.coef_ != 0],\n    markerfmt=\"x\",\n    label=\"Lasso coefficients\",\n)\n([m, s], color=\"#ff7f0e\")\n(\n    (coef)[0],\n    coef[coef != 0],\n    label=\"true coefficients\",\n    markerfmt=\"bx\",\n)\n\n(loc=\"best\")\n(\n    \"Lasso $R^2$: %.3f, Elastic Net $R^2$: %.3f\" % (r2_score_lasso, r2_score_enet)\n)\n()\n\n\n<img alt=\"Lasso $R^2$: 0.658, Elastic Net $R^2$: 0.643\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_lasso_and_elasticnet_001.png\" srcset=\"../../_images/sphx_glr_plot_lasso_and_elasticnet_001.png\"/>",
            "fit1 = Holt(air, initialization_method=\"estimated\").fit(\n    smoothing_level=0.8, smoothing_trend=0.2, optimized=False\n)\nfcast1 = fit1.forecast(5).rename(\"Holt's linear trend\")\nfit2 = Holt(air, exponential=True, initialization_method=\"estimated\").fit(\n    smoothing_level=0.8, smoothing_trend=0.2, optimized=False\n)\nfcast2 = fit2.forecast(5).rename(\"Exponential trend\")\nfit3 = Holt(air, damped_trend=True, initialization_method=\"estimated\").fit(\n    smoothing_level=0.8, smoothing_trend=0.2\n)\nfcast3 = fit3.forecast(5).rename(\"Additive damped trend\")\n\nplt.figure(figsize=(12, 8))\nplt.plot(air, marker=\"o\", color=\"black\")\nplt.plot(fit1.fittedvalues, color=\"blue\")\n(line1,) = plt.plot(fcast1, marker=\"o\", color=\"blue\")\nplt.plot(fit2.fittedvalues, color=\"red\")\n(line2,) = plt.plot(fcast2, marker=\"o\", color=\"red\")\nplt.plot(fit3.fittedvalues, color=\"green\")\n(line3,) = plt.plot(fcast3, marker=\"o\", color=\"green\")\nplt.legend([line1, line2, line3], [fcast1.name, fcast2.name, fcast3.name])",
            "fit1 = SimpleExpSmoothing(oildata, initialization_method=\"heuristic\").fit(\n    smoothing_level=0.2, optimized=False\n)\nfcast1 = fit1.forecast(3).rename(r\"$\\alpha=0.2$\")\nfit2 = SimpleExpSmoothing(oildata, initialization_method=\"heuristic\").fit(\n    smoothing_level=0.6, optimized=False\n)\nfcast2 = fit2.forecast(3).rename(r\"$\\alpha=0.6$\")\nfit3 = SimpleExpSmoothing(oildata, initialization_method=\"estimated\").fit()\nfcast3 = fit3.forecast(3).rename(r\"$\\alpha=%s$\" % fit3.model.params[\"smoothing_level\"])\n\nplt.figure(figsize=(12, 8))\nplt.plot(oildata, marker=\"o\", color=\"black\")\nplt.plot(fit1.fittedvalues, marker=\"o\", color=\"blue\")\n(line1,) = plt.plot(fcast1, marker=\"o\", color=\"blue\")\nplt.plot(fit2.fittedvalues, marker=\"o\", color=\"red\")\n(line2,) = plt.plot(fcast2, marker=\"o\", color=\"red\")\nplt.plot(fit3.fittedvalues, marker=\"o\", color=\"green\")\n(line3,) = plt.plot(fcast3, marker=\"o\", color=\"green\")\nplt.legend([line1, line2, line3], [fcast1.name, fcast2.name, fcast3.name])",
            "states1 = pd.DataFrame(\n    np.c_[fit1.level, fit1.trend, fit1.season],\n    columns=[\"level\", \"slope\", \"seasonal\"],\n    index=aust.index,\n)\nstates2 = pd.DataFrame(\n    np.c_[fit2.level, fit2.trend, fit2.season],\n    columns=[\"level\", \"slope\", \"seasonal\"],\n    index=aust.index,\n)\nfig, [[ax1, ax4], [ax2, ax5], [ax3, ax6]] = plt.subplots(3, 2, figsize=(12, 8))\nstates1[[\"level\"]].plot(ax=ax1)\nstates1[[\"slope\"]].plot(ax=ax2)\nstates1[[\"seasonal\"]].plot(ax=ax3)\nstates2[[\"level\"]].plot(ax=ax4)\nstates2[[\"slope\"]].plot(ax=ax5)\nstates2[[\"seasonal\"]].plot(ax=ax6)\nplt.show()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_22_0.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_22_0.png\"/>",
            "fit1 = ExponentialSmoothing(\n    aust,\n    seasonal_periods=4,\n    trend=\"add\",\n    seasonal=\"add\",\n    initialization_method=\"estimated\",\n).fit()\nfit2 = ExponentialSmoothing(\n    aust,\n    seasonal_periods=4,\n    trend=\"add\",\n    seasonal=\"mul\",\n    initialization_method=\"estimated\",\n).fit()"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Generalized Linear Models->Lasso and Elastic Net for Sparse Signals->Plot"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Method"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Simple Exponential Smoothing"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Winters Seasonal->The Internals"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Winters Seasonal->The Internals"
            ]
        ]
    },
    "100476": {
        "jupyter_code_cell": "x = model.predict_proba(test_f.iloc[:,1:])\nif toEnsemble:\n    pred_test_l = model_l.predict_proba(test_f.iloc[:, 1:])\n    pred_test_r = model_r.predict_proba(test_f.iloc[:, 1:])\n    pred_test_x = model_x.predict_proba(test_f.iloc[:, 1:])\n    x = 0.0*pred_test_l + 0.5*pred_test_r + 0.5*pred_test_x",
        "matched_tutorial_code_inds": [
            2973,
            3234,
            5959,
            5337,
            6707
        ],
        "matched_tutorial_codes": [
            "t_sne = (\n    n_components=n_components,\n    perplexity=30,\n    init=\"random\",\n    n_iter=250,\n    random_state=0,\n)\nS_t_sne = t_sne.fit_transform(S_points)\n\nplot_2d(S_t_sne, S_color, \"T-distributed Stochastic  \\n Neighbor Embedding\")\n\n\n<img alt=\"T-distributed Stochastic    Neighbor Embedding\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_compare_methods_006.png\" srcset=\"../../_images/sphx_glr_plot_compare_methods_006.png\"/>",
            "x = (t_post.ppf(0.001), t_post.ppf(0.999), 100)\n\n(x, t_post.pdf(x))\n((-0.04, 0.06, 0.01))\n(x, t_post.pdf(x), 0, facecolor=\"blue\", alpha=0.2)\n(\"Probability density\")\n(r\"Mean difference ($\\mu$)\")\n(\"Posterior distribution\")\n()\n\n\n<img alt=\"Posterior distribution\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_grid_search_stats_003.png\" srcset=\"../../_images/sphx_glr_plot_grid_search_stats_003.png\"/>",
            "df     sum_sq   mean_sq          F    PR(F)\nC(Duration, Sum)                  1.0   2.339693  2.339693   4.358293  0.041562\nC(Weight, Sum)                    2.0  16.971291  8.485645  15.806745  0.000004\nC(Duration, Sum):C(Weight, Sum)   2.0   0.635658  0.317829   0.592040  0.556748\nResidual                         54.0  28.989198  0.536837        NaN       NaN\n                                    sum_sq    df          F    PR(F)\nC(Duration, Sum)                  2.339693   1.0   4.358293  0.041562\nC(Weight, Sum)                   16.971291   2.0  15.806745  0.000004\nC(Duration, Sum):C(Weight, Sum)   0.635658   2.0   0.592040  0.556748\nResidual                         28.989198  54.0        NaN       NaN\n                                     sum_sq    df           F        PR(F)\nIntercept                        156.301830   1.0  291.153237  2.077589e-23\nC(Duration, Sum)                   2.339693   1.0    4.358293  4.156170e-02\nC(Weight, Sum)                    16.971291   2.0   15.806745  3.944502e-06\nC(Duration, Sum):C(Weight, Sum)    0.635658   2.0    0.592040  5.567479e-01\nResidual                          28.989198  54.0         NaN           NaN",
            "fig = sm.graphics.plot_ccpr_grid(prestige_model)\nfig.tight_layout(pad=1.0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_regression_plots_26_0.png\" src=\"../../../_images/examples_notebooks_generated_regression_plots_26_0.png\"/>",
            "fig = pca_model.plot_scree(log_scale=False)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_13_0.png\" src=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_13_0.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Manifold learning->Comparison of Manifold Learning methods->Define algorithms for the manifold learning->T-distributed Stochastic Neighbor Embedding"
            ],
            [
                "sklearn->Examples->Model Selection->Statistical comparison of models using grid search->Comparing two models: Bayesian approach"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA->Sum of squares"
            ],
            [
                "statsmodels->Examples->Plotting->Regression Plots->Duncan\u2019s Prestige Dataset->Component-Component plus Residual (CCPR) Plots"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ]
        ]
    },
    "545262": {
        "jupyter_code_cell": "from sklearn import datasets\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nimport numpy as np",
        "matched_tutorial_code_inds": [
            2640,
            3304,
            2492,
            3436,
            2956
        ],
        "matched_tutorial_codes": [
            "from sklearn.linear_model import \n\nn_order = 3\nX_train = (x_train, n_order + 1, increasing=True)\nX_test = (x_test, n_order + 1, increasing=True)\nreg = (tol=1e-6, fit_intercept=False, compute_score=True)",
            "from sklearn import metrics\n\nY_pred = rbm_features_classifier.predict(X_test)\nprint(\n    \"Logistic regression using RBM features:\\n%s\\n\"\n    % ((Y_test, Y_pred))\n)",
            "from sklearn.feature_selection import , \nfrom sklearn.pipeline import \nfrom sklearn.svm import \n\nanova_filter = (, k=3)\nclf = ()\nanova_svm = (anova_filter, clf)\nanova_svm.fit(X_train, y_train)",
            "import numpy as np\nfrom sklearn.datasets import \n\nn_samples = 200\nX, y = (n_samples=n_samples, shuffle=False)\nouter, inner = 0, 1\nlabels = (n_samples, -1.0)\nlabels[0] = outer\nlabels[-1] = inner",
            "from sklearn.preprocessing import , \nfrom sklearn.pipeline import \n\nmm = ((), ())\nX_train = mm.fit_transform(X_train)\nX_test = mm.transform(X_test)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Generalized Linear Models->Curve Fitting with Bayesian Ridge Regression->Fit by cubic polynomial"
            ],
            [
                "sklearn->Examples->Neural Networks->Restricted Boltzmann Machine features for digit classification->Evaluation"
            ],
            [
                "sklearn->Examples->Feature Selection->Pipeline ANOVA SVM"
            ],
            [
                "sklearn->Examples->Semi Supervised Classification->Label Propagation learning a complex structure"
            ],
            [
                "sklearn->Examples->Kernel Approximation->Scalable learning with polynomial kernel approximation->Feature normalization"
            ]
        ]
    },
    "1507299": {
        "jupyter_code_cell": "got_data.drop(['Name', 'Allegiances'], axis = 1, inplace=True)\ngot_data.head()\ngot_data_X = got_data.drop('dead',axis=1)\ngot_data_Y = got_data['dead']\ngot_data_X.head()",
        "matched_tutorial_code_inds": [
            3831,
            3616,
            6065,
            6061,
            6069
        ],
        "matched_tutorial_codes": [
            "X = (pd.concat([y.shift(i) for i in range(6)], axis=1,\n               keys=['y'] + ['L%s' % i for i in range(1, 6)])\n       .dropna())\nX.head()",
            "hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()",
            "sel = ar_select_order(housing, 13, old_names=False)\nsel.ar_lags\nres = sel.model.fit()\nprint(res.summary())",
            "mod = AutoReg(housing, 3, old_names=False)\nres = mod.fit()\nprint(res.summary())",
            "sel = ar_select_order(housing, 13, seasonal=True, old_names=False)\nsel.ar_lags\nres = sel.model.fit()\nprint(res.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions->Seasonal Dummies"
            ]
        ]
    },
    "755868": {
        "jupyter_code_cell": "import psycopg2\nimport pandas as pd\nimport pandas_profiling\nimport io\nfrom sqlalchemy import create_engine\nimport os\nimport geopandas as gpd\ndf = pd.read_csv('storm_data.csv', encoding = 'utf8')",
        "matched_tutorial_code_inds": [
            1948,
            6270,
            3781,
            3869,
            1738
        ],
        "matched_tutorial_codes": [
            "import time\nimport warnings\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import cluster, \nfrom sklearn.preprocessing import \nfrom itertools import , \n\n(0)",
            "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom pandas.plotting import register_matplotlib_converters\n\nregister_matplotlib_converters()\nsns.set_style(\"darkgrid\")",
            "import os\nimport feather\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nif int(os.environ.get(\"MODERN_PANDAS_EPUB\", 0)):\n    import prep # noqa",
            "import dask.dataframe as dd\nfrom dask import compute\nfrom dask.distributed import Client\nimport seaborn as sns\n\nclient = Client(processes=False)",
            "# Importing the necessary packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pooch\nimport string\nimport re\nimport zipfile\nimport os\n\n# Creating the random instance\nrng = np.random.default_rng()"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Clustering->Comparing different hierarchical linkage methods on toy datasets"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples"
            ],
            [
                "pandas_toms_blog->Scaling->Dask"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ]
        ]
    },
    "659175": {
        "jupyter_code_cell": "display(pd.read_csv(os.path.join('data','description','features_description.csv'), encoding='iso-8859-1'))",
        "matched_tutorial_code_inds": [
            1458,
            660,
            127,
            4003,
            4025
        ],
        "matched_tutorial_codes": [
            "load_xy = np.load(\"x_y-squared.npz\")\n\nprint(load_xy.files)",
            "interp = (rn18)\n(input)\nprint(interp.summary(True))",
            "print(delta.transform(extract_fn_name).filter(lambda fn: \"TensorIterator\" in fn))\n\n\n\n<strong>Instruction count delta (filter)</strong>",
            "fmri = sns.load_dataset(\"fmri\")\nsns.relplot(data=fmri, x=\"timepoint\", y=\"signal\", kind=\"line\")\n",
            "tips = sns.load_dataset(\"tips\")\nsns.displot(tips, x=\"size\")\n"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Features->Saving and sharing your NumPy arrays->Remove the saved arrays and load them back with NumPy\u2019s"
            ],
            [
                "torch->Code Transforms with FX->(beta) Building a Simple CPU Performance Profiler with FX->Investigating the Performance of ResNet18"
            ],
            [
                "torch->PyTorch Recipes->Timer quick start->6. A/B testing with Callgrind"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty",
                "seaborn->Plotting functions->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size",
                "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size"
            ]
        ]
    },
    "1001963": {
        "jupyter_code_cell": "mdf = pd.read_pickle('../metadata_step0.pkl')\nmdf\ntest_spec = spec_files[0]\ntest_spec",
        "matched_tutorial_code_inds": [
            5496,
            3613,
            647,
            3939,
            6804
        ],
        "matched_tutorial_codes": [
            "predicted = res_log.model.predict(res_log.params, exog=data_student[['pared', 'public', 'gpa']])\npredicted",
            "f = pd.DataFrame({'a':[1,2,3,4,5], 'b':[10,20,30,40,50]})\nf",
            "traced_model = (model)\nprint(traced_model.graph)",
            "anagrams = sns.load_dataset(\"anagrams\")\nanagrams\n",
            "endog = macrodata['infl']\nendog.plot(figsize=(15, 5))"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Logit ordinal regression:"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->SettingWithCopy"
            ],
            [
                "torch->Code Transforms with FX->(beta) Building a Convolution/Batch Norm fuser in FX->Fusing Convolution with Batch Norm"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data",
                "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example"
            ]
        ]
    },
    "597142": {
        "jupyter_code_cell": "driver.get(url)\nfor i in range(49):\n    button = driver.find_element_by_css_selector('button.btn.btn-secondary-rt.mb-load-btn')\n    driver.execute_script(\"arguments[0].scrollIntoView();\", button)\n    button.click()",
        "matched_tutorial_code_inds": [
            4546,
            4547,
            3916,
            4795,
            4796
        ],
        "matched_tutorial_codes": [
            "engine.reset()\n engine.random(5)\narray([[0.22166437, 0.07980522],  # random\n       [0.72166437, 0.93165708],\n       [0.47166437, 0.41313856],\n       [0.97166437, 0.19091633],\n       [0.01853937, 0.74647189]])",
            "engine.reset()\n engine.fast_forward(5)\n engine.random(5)\narray([[0.51853937, 0.52424967],  # random\n       [0.26853937, 0.30202745],\n       [0.76853937, 0.857583  ],\n       [0.14353937, 0.63536078],\n       [0.64353937, 0.01807683]])",
            "sns.set_theme(style=\"ticks\", font_scale=1.25)\ng = sns.relplot(\n    data=penguins,\n    x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"body_mass_g\",\n    palette=\"crest\", marker=\"x\", s=100,\n)\ng.set_axis_labels(\"Bill length (mm)\", \"Bill depth (mm)\", labelpad=10)\ng.legend.set_title(\"Body mass (g)\")\ng.figure.set_size_inches(6.5, 4.5)\ng.ax.margins(.15)\ng.despine(trim=True)\n",
            "plt.rcParams['figure.constrained_layout.use'] = True\nfig, axs = plt.subplots(2, 2, figsize=(3, 3))\nfor ax in axs.flat:\n    example_plot(ax)\n\n\n<img alt=\"Title, Title, Title, Title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_constrainedlayout_guide_018.png\" srcset=\"../../_images/sphx_glr_constrainedlayout_guide_018.png, ../../_images/sphx_glr_constrainedlayout_guide_018_2_0x.png 2.0x\"/>",
            "plt.rcParams['figure.constrained_layout.use'] = False\nfig = plt.figure(layout=\"constrained\")\n\ngs1 = gridspec.GridSpec(2, 1, figure=fig)\nax1 = fig.add_subplot(gs1[0])\nax2 = fig.add_subplot(gs1[1])\n\nexample_plot(ax1)\nexample_plot(ax2)\n\n\n<img alt=\"Title, Title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_constrainedlayout_guide_019.png\" srcset=\"../../_images/sphx_glr_constrainedlayout_guide_019.png, ../../_images/sphx_glr_constrainedlayout_guide_019_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "scipy->Statistics (scipy.stats)->Quasi-Monte Carlo->Using a QMC engine"
            ],
            [
                "scipy->Statistics (scipy.stats)->Quasi-Monte Carlo->Using a QMC engine"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->Opinionated defaults and flexible customization"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Constrained Layout Guide->rcParams"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Constrained Layout Guide->Use with GridSpec"
            ]
        ]
    },
    "207208": {
        "jupyter_code_cell": "plt.plot(h2s_avalon, h2s_liberty, 'o', color='blue');\navalon = h2s_avalon[np.logical_and(h2s_avalon != np.array(None), h2s_liberty != np.array(None))]\nliberty = h2s_liberty[np.logical_and(h2s_avalon != np.array(None), h2s_liberty != np.array(None))]",
        "matched_tutorial_code_inds": [
            1432,
            4652,
            4819,
            4651,
            6552
        ],
        "matched_tutorial_codes": [
            "plt.plot(s)\nplt.show()\n\n\n\n\n<img alt=\"../_images/b3c5a64421a25e9b8f34f2d5e71f750a53e3ef7b513a82271d088e5c12a26308.png\" src=\"../_images/b3c5a64421a25e9b8f34f2d5e71f750a53e3ef7b513a82271d088e5c12a26308.png\"/>",
            "plt.plot([1, 2, 3, 4], [1, 4, 9, 16], 'ro')\nplt.axis([0, 6, 0, 20])\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_003.png\" srcset=\"../../_images/sphx_glr_pyplot_003.png, ../../_images/sphx_glr_pyplot_003_2_0x.png 2.0x\"/>",
            "plt.close('all')\nfig = plt.figure()\n\nax1 = plt.subplot(221)\nax2 = plt.subplot(223)\nax3 = plt.subplot(122)\n\nexample_plot(ax1)\nexample_plot(ax2)\nexample_plot(ax3)\n\nplt.tight_layout()\n\n\n<img alt=\"Title, Title, Title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_tight_layout_guide_006.png\" srcset=\"../../_images/sphx_glr_tight_layout_guide_006.png, ../../_images/sphx_glr_tight_layout_guide_006_2_0x.png 2.0x\"/>",
            "plt.plot([1, 2, 3, 4], [1, 4, 9, 16])\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_002.png\" srcset=\"../../_images/sphx_glr_pyplot_002.png, ../../_images/sphx_glr_pyplot_002_2_0x.png 2.0x\"/>",
            "plt.tight_layout()\n# Note: the syntax here for the lines argument is required for\n# PyMC3 versions = 3.7\n# For version &lt;= 3.6 you can use lines=dict(res_mle.params) instead\n_ = pm.plot_trace(\n    trace,\n    lines=[(k, {}, [v]) for k, v in dict(res_mle.params).items()],\n    combined=True,\n    figsize=(12, 12),\n)"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Approximation"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Introduction to pyplot->Formatting the style of your plot"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Tight Layout guide->Simple Example"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Introduction to pyplot"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX estimation with Bayesian methods->Introduction->5. Bayesian estimation with NUTS"
            ]
        ]
    },
    "1511473": {
        "jupyter_code_cell": "def plotClusterOutput(Y, x1, x2):\n    f1 = np.ravel(Y[x1, :])\n    f2 = np.ravel(Y[x2, :])\n    plt.scatter(f1, f2, c='blue', s=7)\n    plt.show()\ndef plot_data(X, data_centers):\n    C = np.matrix(data_centers).T\n    X = np.hstack((X, C))\n    data = pd.DataFrame(X, columns=['x1', 'x2', 'c'])\n    fig, ax = plt.subplots(figsize=(12,8))\n    ax.scatter(data.x1, data.x2, label='Data', c=data.c * 10)\n    ax.legend(loc=2)\n    ax.set_xlabel('x1')\n    ax.set_ylabel('x2')\n    ax.set_title('x1 vs. x2')\ndef assignment(X, centroids):\n    C = dict.fromkeys(range(X.shape[0]), np.inf)\n    Z = {}\n    for i in centroids.keys():\n        for j in range(X.shape[0]):\n            dist = abs(np.linalg.norm(X[j] - centroids[i]))\n            if dist < C[j]:\n                C[j] = dist\n                Z[j] = i\n    return Z",
        "matched_tutorial_code_inds": [
            2966,
            507,
            353,
            838,
            2398
        ],
        "matched_tutorial_codes": [
            "def plot_3d(points, points_color, title):\n    x, y, z = points.T\n\n    fig, ax = (\n        figsize=(6, 6),\n        facecolor=\"white\",\n        tight_layout=True,\n        subplot_kw={\"projection\": \"3d\"},\n    )\n    fig.suptitle(title, size=16)\n    col = ax.scatter(x, y, z, c=points_color, s=50, alpha=0.8)\n    ax.view_init(azim=-60, elev=9)\n    ax.xaxis.set_major_locator((1))\n    ax.yaxis.set_major_locator((1))\n    ax.zaxis.set_major_locator((1))\n\n    fig.colorbar(col, ax=ax, orientation=\"horizontal\", shrink=0.6, aspect=60, pad=0.01)\n    ()\n\n\ndef plot_2d(points, points_color, title):\n    fig, ax = (figsize=(3, 3), facecolor=\"white\", constrained_layout=True)\n    fig.suptitle(title, size=16)\n    add_2d_scatter(ax, points, points_color)\n    ()\n\n\ndef add_2d_scatter(ax, points, points_color, title=None):\n    x, y = points.T\n    ax.scatter(x, y, c=points_color, s=50, alpha=0.8)\n    ax.set_title(title)\n    ax.xaxis.set_major_formatter(())\n    ax.yaxis.set_major_formatter(())\n\n\nplot_3d(S_points, S_color, \"Original S-curve samples\")\n\n\n<img alt=\"Original S-curve samples\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_compare_methods_001.png\" srcset=\"../../_images/sphx_glr_plot_compare_methods_001.png\"/>",
            "def generate_square_subsequent_mask(sz):\n    mask = ((((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n    return mask\n\n\ndef create_mask(src, tgt):\n    src_seq_len = src.shape[0]\n    tgt_seq_len = tgt.shape[0]\n\n    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n    src_mask = ((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n\n    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask",
            "def preprocess(x, y):\n    return x.view(-1, 1, 28, 28).to(), y.to()\n\n\ntrain_dl, valid_dl = get_data(, , bs)\ntrain_dl = WrappedDataLoader(train_dl, preprocess)\nvalid_dl = WrappedDataLoader(valid_dl, preprocess)",
            "def cube_forward(x):\n    return x**3\n\ndef cube_backward(grad_out, x):\n    return grad_out * 3 * x**2\n\ndef cube_backward_backward(grad_out, sav_grad_out, x):\n    return grad_out * sav_grad_out * 6 * x\n\ndef cube_backward_backward_grad_out(grad_out, x):\n    return grad_out * 3 * x**2\n\nclass Cube(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        ctx.save_for_backward(x)\n        return cube_forward(x)\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        x, = ctx.saved_tensors\n        return CubeBackward.apply(grad_out, x)\n\nclass CubeBackward(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, grad_out, x):\n        ctx.save_for_backward(x, grad_out)\n        return cube_backward(grad_out, x)\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        x, sav_grad_out = ctx.saved_tensors\n        dx = cube_backward_backward(grad_out, sav_grad_out, x)\n        dgrad_out = cube_backward_backward_grad_out(grad_out, x)\n        return dgrad_out, dx\n\nx = torch.tensor(2., requires_grad=True, dtype=torch.double)\n\ntorch.autograd.gradcheck(Cube.apply, x)\ntorch.autograd.gradgradcheck(Cube.apply, x)",
            "def plot_accuracy(x, y, x_legend):\n    \"\"\"Plot accuracy as a function of x.\"\"\"\n    x = (x)\n    y = (y)\n    (\"Classification accuracy as a function of %s\" % x_legend)\n    (\"%s\" % x_legend)\n    (\"Accuracy\")\n    (True)\n    (x, y)\n\n\n[\"legend.fontsize\"] = 10\ncls_names = list(sorted(cls_stats.keys()))\n\n# Plot accuracy evolution\n()\nfor _, stats in sorted(cls_stats.items()):\n    # Plot accuracy evolution with #examples\n    accuracy, n_examples = zip(*stats[\"accuracy_history\"])\n    plot_accuracy(n_examples, accuracy, \"training examples (#)\")\n    ax = ()\n    ax.set_ylim((0.8, 1))\n(cls_names, loc=\"best\")\n\n()\nfor _, stats in sorted(cls_stats.items()):\n    # Plot accuracy evolution with runtime\n    accuracy, runtime = zip(*stats[\"runtime_history\"])\n    plot_accuracy(runtime, accuracy, \"runtime (s)\")\n    ax = ()\n    ax.set_ylim((0.8, 1))\n(cls_names, loc=\"best\")\n\n# Plot fitting times\n()\nfig = ()\ncls_runtime = [stats[\"total_fit_time\"] for cls_name, stats in sorted(cls_stats.items())]\n\ncls_runtime.append(total_vect_time)\ncls_names.append(\"Vectorization\")\nbar_colors = [\"b\", \"g\", \"r\", \"c\", \"m\", \"y\"]\n\nax = (111)\nrectangles = (range(len(cls_names)), cls_runtime, width=0.5, color=bar_colors)\n\nax.set_xticks((0, len(cls_names) - 1, len(cls_names)))\nax.set_xticklabels(cls_names, fontsize=10)\nymax = max(cls_runtime) * 1.2\nax.set_ylim((0, ymax))\nax.set_ylabel(\"runtime (s)\")\nax.set_title(\"Training Times\")\n\n\ndef autolabel(rectangles):\n    \"\"\"attach some text vi autolabel on rectangles.\"\"\"\n    for rect in rectangles:\n        height = rect.get_height()\n        ax.text(\n            rect.get_x() + rect.get_width() / 2.0,\n            1.05 * height,\n            \"%.4f\" % height,\n            ha=\"center\",\n            va=\"bottom\",\n        )\n        (()[1], rotation=30)\n\n\nautolabel(rectangles)\n()\n()\n\n# Plot prediction times\n()\ncls_runtime = []\ncls_names = list(sorted(cls_stats.keys()))\nfor cls_name, stats in sorted(cls_stats.items()):\n    cls_runtime.append(stats[\"prediction_time\"])\ncls_runtime.append(parsing_time)\ncls_names.append(\"Read/Parse\\n+Feat.Extr.\")\ncls_runtime.append(vectorizing_time)\ncls_names.append(\"Hashing\\n+Vect.\")\n\nax = (111)\nrectangles = (range(len(cls_names)), cls_runtime, width=0.5, color=bar_colors)\n\nax.set_xticks((0, len(cls_names) - 1, len(cls_names)))\nax.set_xticklabels(cls_names, fontsize=8)\n(()[1], rotation=30)\nymax = max(cls_runtime) * 1.2\nax.set_ylim((0, ymax))\nax.set_ylabel(\"runtime (s)\")\nax.set_title(\"Prediction Times (%d instances)\" % n_test_documents)\nautolabel(rectangles)\n()\n()\n\n\n\n<img alt=\"Classification accuracy as a function of training examples (#)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_out_of_core_classification_001.png\" srcset=\"../../_images/sphx_glr_plot_out_of_core_classification_001.png\"/>\n<img alt=\"Classification accuracy as a function of runtime (s)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_out_of_core_classification_002.png\" srcset=\"../../_images/sphx_glr_plot_out_of_core_classification_002.png\"/>\n<img alt=\"Training Times\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_out_of_core_classification_003.png\" srcset=\"../../_images/sphx_glr_plot_out_of_core_classification_003.png\"/>\n<img alt=\"Prediction Times (1000 instances)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_out_of_core_classification_004.png\" srcset=\"../../_images/sphx_glr_plot_out_of_core_classification_004.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Manifold learning->Comparison of Manifold Learning methods->Dataset preparation"
            ],
            [
                "torch->Text->Language Translation with nn.Transformer and torchtext->Seq2Seq Network using Transformer"
            ],
            [
                "torch->Learning PyTorch->What is torch.nn really?->Using your GPU"
            ],
            [
                "torch->Extending PyTorch->Double Backward with Custom Functions->When Backward is not Tracked"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Out-of-core classification of text documents->Plot results"
            ]
        ]
    },
    "365388": {
        "jupyter_code_cell": "df_start_stop['Deficit'].abs().sum()\ndf_round_trip = pd.DataFrame()\ndf_round_trip = df2016.loc[df2016['start station id'] == df2016['end station id'], :]\nlen(df_round_trip)/len(df2016)",
        "matched_tutorial_code_inds": [
            3852,
            6406,
            5470,
            5703,
            3891
        ],
        "matched_tutorial_codes": [
            "pred = res_seasonal.get_prediction(start='2001-03-01')\npred_ci = pred.conf_int()",
            "mod = sm.tsa.VARMAX(endog[['dln_inv', 'dln_inc']], order=(1,1))\nres = mod.fit(maxiter=1000, disp=False)\nprint(res.summary())",
            "means75 = exog.mean()\nmeans75[\"LOWINC\"] = exog[\"LOWINC\"].quantile(0.75)\nprint(means75)",
            "summ_df = infl.summary_frame()\nsumm_df.sort_values(\"cooks_d\", ascending=False)[:10]",
            "subset = daily.loc['2011':'2016']\nax = subset.div(1000).plot(figsize=(12, 6))\nax.set(ylim=0, title=\"Daily Donations\", ylabel=\"$ (thousands)\",)\nsns.despine();"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Time Series->Timeseries->Forecasting"
            ],
            [
                "statsmodels->Examples->State space models->VARMAX: Introduction->Caution: VARMA(p,q) specifications"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Influence Measures for GLMs->get the influence measures"
            ],
            [
                "pandas_toms_blog->Scaling->Dask"
            ]
        ]
    },
    "1317623": {
        "jupyter_code_cell": "sns.countplot(x = 'Decide3', data=va, hue = 'txgot_binary')\nsns.countplot(x = 'Avdsurg3', data=va, hue = 'txgot_binary')",
        "matched_tutorial_code_inds": [
            3792,
            3793,
            4480,
            1698,
            3584
        ],
        "matched_tutorial_codes": [
            "sns.countplot(x='cut', data=df)\nsns.despine()\nplt.tight_layout()",
            "sns.barplot(x='cut', y='price', data=df)\nsns.despine()\nplt.tight_layout()",
            "quad(deterministic.pdf, -1e-3, 1e-3)  # warning removed\n(1.000076872229173, 0.0010625571718182458)",
            "rng = default_rng()\n\nbefore_sample = rng.choice(before_lock, size=30, replace=False)\nafter_sample = rng.choice(after_lock, size=30, replace=False)",
            "tfidf_transformer = TfidfTransformer()\n X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n X_train_tfidf.shape\n(2257, 35788)"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "scipy->Statistics (scipy.stats)->Building specific distributions->Making a continuous distribution, i.e., subclassing rv_continuous"
            ],
            [
                "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Paired Student\u2019s t-test on the AQIs->Sampling"
            ],
            [
                "sklearn->Tutorials->Working With Text Data->Extracting features from text files->From occurrences to frequencies"
            ]
        ]
    },
    "942816": {
        "jupyter_code_cell": "subset = data[(data[\"category\"] == \"comp.graphics\") | \n             (data[\"category\"] == \"comp.os.ms-windows.misc\") |\n             (data[\"category\"] == \"comp.sys.ibm.pc.hardware\") |\n             (data[\"category\"] == \"comp.sys.mac.hardware\") |\n             (data[\"category\"] == \"comp.windows.x\")]\nsubset.shape",
        "matched_tutorial_code_inds": [
            3904,
            4654,
            2601,
            2118,
            2753
        ],
        "matched_tutorial_codes": [
            "dots = sns.load_dataset(\"dots\")\nsns.relplot(\n    data=dots, kind=\"line\",\n    x=\"time\", y=\"firing_rate\", col=\"align\",\n    hue=\"choice\", size=\"coherence\", style=\"choice\",\n    facet_kws=dict(sharex=False),\n)\n",
            "data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>",
            "vmin, vmax = (-log_marginal_likelihood).min(), 50\nlevel = (((vmin), (vmax), num=50), decimals=1)\n(\n    length_scale_grid,\n    noise_level_grid,\n    -log_marginal_likelihood,\n    levels=level,\n    norm=(vmin=vmin, vmax=vmax),\n)\n()\n(\"log\")\n(\"log\")\n(\"Length-scale\")\n(\"Noise-level\")\n(\"Log-marginal-likelihood\")\n()\n\n\n<img alt=\"Log-marginal-likelihood\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_noisy_005.png\" srcset=\"../../_images/sphx_glr_plot_gpr_noisy_005.png\"/>",
            "dict_pos_dict_estimator = (\n    n_components=n_components,\n    alpha=0.1,\n    max_iter=50,\n    batch_size=3,\n    random_state=rng,\n    positive_dict=True,\n)\ndict_pos_dict_estimator.fit(faces_centered)\nplot_gallery(\n    \"Dictionary learning - positive dictionary\",\n    dict_pos_dict_estimator.components_[:n_components],\n    cmap=plt.cm.RdBu,\n)\n\n\n<img alt=\"Dictionary learning - positive dictionary\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_011.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_011.png\"/>",
            "quantiles = [0.05, 0.5, 0.95]\npredictions = {}\nout_bounds_predictions = (y_true_mean, dtype=)\nfor quantile in quantiles:\n    qr = (quantile=quantile, alpha=0, solver=solver)\n    y_pred = qr.fit(X, y_pareto).predict(X)\n    predictions[quantile] = y_pred\n\n    if quantile == min(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred = y_pareto\n        )\n    elif quantile == max(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred &lt;= y_pareto\n        )"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) with noise-level estimation->Optimisation of kernel hyperparameters in GPR"
            ],
            [
                "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition: Dictionary learning->Dictionary learning - positive dictionary"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Quantile regression->Fitting a QuantileRegressor"
            ]
        ]
    },
    "1023140": {
        "jupyter_code_cell": "matplotlib.rcParams['font.size'] = 20\nx= plot(\n    space,\n    (\n        'red orange pink green blue white yellow black '\n        'mother father son daughter aunt uncle '\n        'concept research theory '\n        'car bus tube road bicycle train '\n        'karate fight fencing '\n        'apple company fruit train set '\n        ''.split()\n    )\n)\nmatplotlib.rcParams['font.size'] = 7\nplot(space, sample(list(space.index.values), 1000))",
        "matched_tutorial_code_inds": [
            5917,
            2114,
            6700,
            5643,
            2601
        ],
        "matched_tutorial_codes": [
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "batch_dict_estimator = (\n    n_components=n_components, alpha=0.1, max_iter=50, batch_size=3, random_state=rng\n)\nbatch_dict_estimator.fit(faces_centered)\nplot_gallery(\"Dictionary learning\", batch_dict_estimator.components_[:n_components])\n\n\n<img alt=\"Dictionary learning\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_006.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_006.png\"/>",
            "ln_pce = np.log(pce.PCE)\nforecasts = {\"ln PCE\": ln_pce}\nfor year in range(1995, 2020, 3):\n    sub = ln_pce[: str(year)]\n    res = ThetaModel(sub).fit()\n    fcast = res.forecast(12)\n    forecasts[str(year)] = fcast\nforecasts = pd.DataFrame(forecasts)\nax = forecasts[\"1995\":].plot(legend=False)\nchildren = ax.get_children()\nchildren[0].set_linewidth(4)\nchildren[0].set_alpha(0.3)\nchildren[0].set_color(\"#000000\")\nax.set_title(\"ln PCE\")\nplt.tight_layout(pad=1.0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_theta-model_22_0.png\" src=\"../../../_images/examples_notebooks_generated_theta-model_22_0.png\"/>",
            "glm = smf.glm(\n    \"affairs ~ rate_marriage + yrs_married\",\n    data=dc,\n    family=sm.families.Poisson(),\n    freq_weights=np.asarray(dc[\"freq\"]),\n)\nres_f2 = glm.fit()\n# print(res_f2.summary())\nres_f2.pearson_chi2 - res_f.pearson_chi2, res_f2.deviance - res_f.deviance, res_f2.llf - res_f.llf",
            "vmin, vmax = (-log_marginal_likelihood).min(), 50\nlevel = (((vmin), (vmax), num=50), decimals=1)\n(\n    length_scale_grid,\n    noise_level_grid,\n    -log_marginal_likelihood,\n    levels=level,\n    norm=(vmin=vmin, vmax=vmax),\n)\n()\n(\"log\")\n(\"log\")\n(\"Length-scale\")\n(\"Noise-level\")\n(\"Log-marginal-likelihood\")\n()\n\n\n<img alt=\"Log-marginal-likelihood\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_noisy_005.png\" srcset=\"../../_images/sphx_glr_plot_gpr_noisy_005.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition->Dictionary learning"
            ],
            [
                "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Personal Consumption Expenditure"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->original observations and frequency weights"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) with noise-level estimation->Optimisation of kernel hyperparameters in GPR"
            ]
        ]
    },
    "1017405": {
        "jupyter_code_cell": "two_mode = RCT2.networkTwoMode('keywords', 'authorsFull')\nmk.graphStats(two_mode)\none_mode_multilevel = RCT2.networkMultiLevel('keywords', 'authorsFull')\nmk.graphStats(one_mode_multilevel)",
        "matched_tutorial_code_inds": [
            5215,
            6664,
            5191,
            183,
            1746
        ],
        "matched_tutorial_codes": [
            "glsar_model = sm.GLSAR(data.endog, data.exog, 1)\nglsar_results = glsar_model.iterative_fit(1)\nprint(glsar_results.summary())",
            "mod_conc = LocalLevelConcentrated(dta.infl)\nres_conc = mod_conc.fit(disp=False)\nprint(res_conc.summary())",
            "ols_model = sm.OLS(y, X)\nols_results = ols_model.fit()\nprint(ols_results.summary())",
            "epochs = 5\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(, model, , )\n    test(, model, )\nprint(\"Done!\")",
            "glove = data.fetch('glove.6B.50d.zip')\nemb_path = textproc.unzipper(glove, 'glove.6B.300d.txt')\nemb_matrix = textproc.loadGloveModel(emb_path)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Generalized Least Squares"
            ],
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Concentrating out the scale"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity"
            ],
            [
                "torch->Introduction to PyTorch->Quickstart->Optimizing the Model Parameters"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ]
        ]
    },
    "810669": {
        "jupyter_code_cell": "!mkdir -p data\n!curl 'http://ourairports.com/data/airports.csv' -o data/airports.csv\nairports = pd.read_csv(\"data/airports.csv\",low_memory=False)\nusairports = airports[(airports.iata_code.notna()) & \n                      (airports.iso_country == 'US')][['iata_code','latitude_deg','longitude_deg']]\nairline_dictionary = {'9E': 'Pinnacle Airlines', \n              'AA': 'American Airlines', \n              'AS': 'Alaska Airlines',\n              'B6': 'JetBlue',\n              'DL': 'Delta Air Lines',\n              'EV': 'Atlantic Southeast Airlines',\n              'F9': 'Frontier Airlines',\n              'G4': 'Allegiant Air',\n              'HA': 'Hawaiian Airlines',\n              'MQ': 'Envoy Air',\n              'NK': 'Spirit Airlines',\n              'OH': 'Comair',\n              'OO': 'SkyWest  Airlines',\n              'UA': 'United Airlines',\n              'VX': 'Virgin America',\n              'WN': 'Southwest Airlines',\n              'YV': 'Mesa Airlines',\n              'YX': 'Midwest Airlines'\n             }",
        "matched_tutorial_code_inds": [
            923,
            1064,
            898,
            916,
            1389
        ],
        "matched_tutorial_codes": [
            "$ mkdir build\n$ cd build\n$ cmake -DCMAKE_PREFIX_PATH=\"$(python -c 'import torch.utils; print(torch.utils.cmake_prefix_path)')\" ..\n-- The C compiler identification is GNU 5.4.0\n-- The CXX compiler identification is GNU 5.4.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Looking for pthread.h\n-- Looking for pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE\n-- Found torch: /libtorch/lib/libtorch.so\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /warp_perspective/example_app/build\n$ make -j\nScanning dependencies of target warp_perspective\n[ 25%] Building CXX object warp_perspective/CMakeFiles/warp_perspective.dir/op.cpp.o\n[ 50%] Linking CXX shared library libwarp_perspective.so\n[ 50%] Built target warp_perspective\nScanning dependencies of target example_app\n[ 75%] Building CXX object CMakeFiles/example_app.dir/main.cpp.o\n[100%] Linking CXX executable example_app\n[100%] Built target example_app",
            "export GLUE_DIR=./glue_data\nexport TASK_NAME=MRPC\nexport OUT_DIR=./$TASK_NAME/\npython ./run_glue.py \\\n    --model_type bert \\\n    --model_name_or_path bert-base-uncased \\\n    --task_name $TASK_NAME \\\n    --do_train \\\n    --do_eval \\\n    --do_lower_case \\\n    --data_dir $GLUE_DIR/$TASK_NAME \\\n    --max_seq_length 128 \\\n    --per_gpu_eval_batch_size=8   \\\n    --per_gpu_train_batch_size=8   \\\n    --learning_rate 2e-5 \\\n    --num_train_epochs 3.0 \\\n    --save_steps 100000 \\\n    --output_dir $OUT_DIR",
            "$ mkdir build\n$ cd build\n$ cmake -DCMAKE_PREFIX_PATH=\"$(python -c 'import torch.utils; print(torch.utils.cmake_prefix_path)')\" ..\n-- The C compiler identification is GNU 5.4.0\n-- The CXX compiler identification is GNU 5.4.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Looking for pthread.h\n-- Looking for pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE\n-- Found torch: /libtorch/lib/libtorch.so\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /warp_perspective/build\n$ make -j\nScanning dependencies of target warp_perspective\n[ 50%] Building CXX object CMakeFiles/warp_perspective.dir/op.cpp.o\n[100%] Linking CXX shared library libwarp_perspective.so\n[100%] Built target warp_perspective",
            "$ mkdir build\n$ cd build\n$ cmake -DCMAKE_PREFIX_PATH=\"$(python -c 'import torch.utils; print(torch.utils.cmake_prefix_path)')\" ..\n-- The C compiler identification is GNU 5.4.0\n-- The CXX compiler identification is GNU 5.4.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Looking for pthread.h\n-- Looking for pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE\n-- Found torch: /libtorch/lib/libtorch.so\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /example_app/build\n$ make -j\nScanning dependencies of target example_app\n[ 50%] Building CXX object CMakeFiles/example_app.dir/main.cpp.o\n[100%] Linking CXX executable example_app\n[100%] Built target example_app",
            "!pip install torchmultimodal-nightly\n!pip install datasets\n!pip install transformers"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Extending PyTorch->Extending TorchScript with Custom C++ Operators->Using the TorchScript Custom Operator in C++"
            ],
            [
                "torch->Model Optimization->(beta) Dynamic Quantization on BERT->2. Fine-tune the BERT model"
            ],
            [
                "torch->Extending PyTorch->Extending TorchScript with Custom C++ Operators->Building the Custom Operator->Building with CMake"
            ],
            [
                "torch->Extending PyTorch->Extending TorchScript with Custom C++ Operators->Using the TorchScript Custom Operator in C++"
            ],
            [
                "torch->Multimodality->TorchMultimodal Tutorial: Finetuning FLAVA->Installation"
            ]
        ]
    },
    "1163019": {
        "jupyter_code_cell": "data.head()\nclean_table = data.set_index('date').pivot(columns = 'ticker')\nclean_table.head()",
        "matched_tutorial_code_inds": [
            3656,
            3610,
            3934,
            3793,
            3751
        ],
        "matched_tutorial_codes": [
            "weather2 = weather.reset_index()\nweather2[weather2['station'] == 'DSM'].head()",
            "first = df.groupby('unique_carrier').first()\nfirst.ix[10:15, ['fl_date', 'tail_num']]",
            "flights_wide = flights.pivot(index=\"year\", columns=\"month\", values=\"passengers\")\nflights_wide.head()\n",
            "sns.barplot(x='cut', y='price', data=df)\nsns.despine()\nplt.tight_layout()",
            "tidy['rest'] = tidy.sort_values('date').groupby('team').date.diff().dt.days - 1\ntidy.dropna().head()"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Indexes->Flavors->Row Slicing"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data"
            ]
        ]
    },
    "715874": {
        "jupyter_code_cell": "dst.distplot_subplots(df.iloc[:,:40].sample(n=100), 5, 8, setxlog=False, figsize=(40,20))\nallfeatures = list(df.columns.copy())\nallfeatures.remove('customer_id')\ndf_mat = df.head(20).as_matrix(columns=allfeatures)\ndst.heatmap(df_mat, rownames=df.head(20).index, colnames=allfeatures, xlabel='feature', ylabel='customer ID', figsize=(10,5))",
        "matched_tutorial_code_inds": [
            3916,
            4795,
            4827,
            4630,
            3079
        ],
        "matched_tutorial_codes": [
            "sns.set_theme(style=\"ticks\", font_scale=1.25)\ng = sns.relplot(\n    data=penguins,\n    x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"body_mass_g\",\n    palette=\"crest\", marker=\"x\", s=100,\n)\ng.set_axis_labels(\"Bill length (mm)\", \"Bill depth (mm)\", labelpad=10)\ng.legend.set_title(\"Body mass (g)\")\ng.figure.set_size_inches(6.5, 4.5)\ng.ax.margins(.15)\ng.despine(trim=True)\n",
            "plt.rcParams['figure.constrained_layout.use'] = True\nfig, axs = plt.subplots(2, 2, figsize=(3, 3))\nfor ax in axs.flat:\n    example_plot(ax)\n\n\n<img alt=\"Title, Title, Title, Title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_constrainedlayout_guide_018.png\" srcset=\"../../_images/sphx_glr_constrainedlayout_guide_018.png, ../../_images/sphx_glr_constrainedlayout_guide_018_2_0x.png 2.0x\"/>",
            "plt.close('all')\narr = np.arange(100).reshape((10, 10))\nfig = plt.figure(figsize=(4, 4))\nim = plt.imshow(arr, interpolation=\"none\")\n\nplt.colorbar(im)\n\nplt.tight_layout()\n\n\n<img alt=\"tight layout guide\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_tight_layout_guide_014.png\" srcset=\"../../_images/sphx_glr_tight_layout_guide_014.png, ../../_images/sphx_glr_tight_layout_guide_014_2_0x.png 2.0x\"/>",
            "np.random.seed(19680801)  # seed the random number generator.\ndata = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nfig, ax = plt.subplots(figsize=(5, 2.7), layout='constrained')\nax.scatter('a', 'b', c='c', s='d', data=data)\nax.set_xlabel('entry a')\nax.set_ylabel('entry b')\n\n\n<img alt=\"quick start\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_quick_start_002.png\" srcset=\"../../_images/sphx_glr_quick_start_002.png, ../../_images/sphx_glr_quick_start_002_2_0x.png 2.0x\"/>",
            "rfc = (n_estimators=10, random_state=42)\nrfc.fit(X_train, y_train)\nax = ()\nrfc_disp = (rfc, X_test, y_test, ax=ax, alpha=0.8)\nsvc_disp.plot(ax=ax, alpha=0.8)\n()\n\n\n<img alt=\"plot roc curve visualization api\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_roc_curve_visualization_api_002.png\" srcset=\"../../_images/sphx_glr_plot_roc_curve_visualization_api_002.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->Opinionated defaults and flexible customization"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Constrained Layout Guide->rcParams"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Tight Layout guide->Colorbar"
            ],
            [
                "matplotlib->Tutorials->Introductory->Quick start guide->Types of inputs to plotting functions"
            ],
            [
                "sklearn->Examples->Miscellaneous->ROC Curve with Visualization API->Training a Random Forest and Plotting the ROC Curve"
            ]
        ]
    },
    "1118804": {
        "jupyter_code_cell": "mpl.rcParams['font.size'] = 30.0\nplt.figure(figsize = (30, 30))\nsns.barplot(x='num_videos',y='shares',data=tempdf,hue='num_videos')\nplt.xlabel(' categories of no of images ')\nplt.ylabel('no of shares ')\nmpl.rcParams['font.size'] = 30.0\nplt.figure(figsize = (30, 30))",
        "matched_tutorial_code_inds": [
            5643,
            2860,
            2844,
            2864,
            2662
        ],
        "matched_tutorial_codes": [
            "glm = smf.glm(\n    \"affairs ~ rate_marriage + yrs_married\",\n    data=dc,\n    family=sm.families.Poisson(),\n    freq_weights=np.asarray(dc[\"freq\"]),\n)\nres_f2 = glm.fit()\n# print(res_f2.summary())\nres_f2.pearson_chi2 - res_f.pearson_chi2, res_f2.deviance - res_f.deviance, res_f2.llf - res_f.llf",
            "mae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}\n\n_, ax = (figsize=(5, 5))\ndisplay = (\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n)\nax.set_title(\"Ridge model, optimum regularization\")\nfor name, score in scores.items():\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\nax.legend(loc=\"upper left\")\n()\n\n\n<img alt=\"Ridge model, optimum regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_012.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_012.png\"/>",
            "mae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}\n\n_, ax = (figsize=(5, 5))\ndisplay = (\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n)\nax.set_title(\"Ridge model, small regularization\")\nfor name, score in scores.items():\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\nax.legend(loc=\"upper left\")\n()\n\n\n<img alt=\"Ridge model, small regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_009.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_009.png\"/>",
            "mae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}\n\n_, ax = (figsize=(6, 6))\ndisplay = (\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n)\nax.set_title(\"Lasso model, optimum regularization\")\nfor name, score in scores.items():\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\nax.legend(loc=\"upper left\")\n()\n\n\n<img alt=\"Lasso model, optimum regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_015.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_015.png\"/>",
            "m, s, _ = (\n    (enet.coef_)[0],\n    enet.coef_[enet.coef_ != 0],\n    markerfmt=\"x\",\n    label=\"Elastic net coefficients\",\n)\n([m, s], color=\"#2ca02c\")\nm, s, _ = (\n    (lasso.coef_)[0],\n    lasso.coef_[lasso.coef_ != 0],\n    markerfmt=\"x\",\n    label=\"Lasso coefficients\",\n)\n([m, s], color=\"#ff7f0e\")\n(\n    (coef)[0],\n    coef[coef != 0],\n    label=\"true coefficients\",\n    markerfmt=\"bx\",\n)\n\n(loc=\"best\")\n(\n    \"Lasso $R^2$: %.3f, Elastic Net $R^2$: %.3f\" % (r2_score_lasso, r2_score_enet)\n)\n()\n\n\n<img alt=\"Lasso $R^2$: 0.658, Elastic Net $R^2$: 0.643\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_lasso_and_elasticnet_001.png\" srcset=\"../../_images/sphx_glr_plot_lasso_and_elasticnet_001.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->original observations and frequency weights"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Lasso and Elastic Net for Sparse Signals->Plot"
            ]
        ]
    },
    "850496": {
        "jupyter_code_cell": "for c in heart.columns[:-1]:\n    heart[c] = heart[c].apply(lambda x: heart[heart[c]!='?'][c].astype(float).mean() if x == \"?\" else x)\n    heart[c] = heart[c].astype(float)\nset(heart.loc[:, \"diagnosis\"].values)",
        "matched_tutorial_code_inds": [
            6655,
            6258,
            3167,
            2162,
            1085
        ],
        "matched_tutorial_codes": [
            "for i in range(simulated.shape[1]):\n    simulated.iloc[:, i].plot(label=\"_\", color=\"gray\", alpha=0.1)\ndf[\"mean\"].plot(label=\"mean prediction\")\ndf[\"pi_lower\"].plot(linestyle=\"--\", color=\"tab:blue\", label=\"95% interval\")\ndf[\"pi_upper\"].plot(linestyle=\"--\", color=\"tab:blue\", label=\"_\")\npred.endog.plot(label=\"data\")\nplt.legend()",
            "for fit in [fit2, fit4]:\n    pd.DataFrame(np.c_[fit.level, fit.trend]).rename(\n        columns={0: \"level\", 1: \"slope\"}\n    ).plot(subplots=True)\nplt.show()\nprint(\n    \"Figure 7.4: Level and slope components for Holt\u2019s linear trend method and the additive damped trend method.\"\n)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_0.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_0.png\"/>\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_1.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_1.png\"/>",
            "for i in range(n_classes):\n    fpr[i], tpr[i], _ = (y_onehot_test[:, i], y_score[:, i])\n    roc_auc[i] = (fpr[i], tpr[i])\n\nfpr_grid = (0.0, 1.0, 1000)\n\n# Interpolate all ROC curves at these points\nmean_tpr = (fpr_grid)\n\nfor i in range(n_classes):\n    mean_tpr += (fpr_grid, fpr[i], tpr[i])  # linear interpolation\n\n# Average it and compute AUC\nmean_tpr /= n_classes\n\nfpr[\"macro\"] = fpr_grid\ntpr[\"macro\"] = mean_tpr\nroc_auc[\"macro\"] = (fpr[\"macro\"], tpr[\"macro\"])\n\nprint(f\"Macro-averaged One-vs-Rest ROC AUC score:\\n{roc_auc['macro']:.2f}\")",
            "for pipe in (hist_dropped, hist_one_hot, hist_ordinal, hist_native):\n    pipe.set_params(\n        histgradientboostingregressor__max_depth=3,\n        histgradientboostingregressor__max_iter=15,\n    )\n\ndropped_result = (hist_dropped, X, y, cv=n_cv_folds, scoring=scoring)\none_hot_result = (hist_one_hot, X, y, cv=n_cv_folds, scoring=scoring)\nordinal_result = (hist_ordinal, X, y, cv=n_cv_folds, scoring=scoring)\nnative_result = (hist_native, X, y, cv=n_cv_folds, scoring=scoring)\n\nplot_results(\"Gradient Boosting on Ames Housing (few and small trees)\")\n\n()\n\n\n<img alt=\"Gradient Boosting on Ames Housing (few and small trees), Fit times (s), Mean Absolute Percentage Error\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_002.png\"/>",
            "for param in model_ft.parameters():\n  param.requires_grad = True\n\nmodel_ft.to(device)  # We can fine-tune on GPU if available\n\ncriterion = nn.CrossEntropyLoss()\n\n# Note that we are training everything, so the learning rate is lower\n# Notice the smaller learning rate\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=1e-3, momentum=0.9, weight_decay=0.1)\n\n# Decay LR by a factor of 0.3 every several epochs\nexp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=5, gamma=0.3)\n\nmodel_ft_tuned = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n                             num_epochs=25, device=device)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->ETS models->Predictions"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Method->Plots of Seasonally Adjusted Data"
            ],
            [
                "sklearn->Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->One-vs-Rest multiclass ROC->ROC curve using the OvR macro-average"
            ],
            [
                "sklearn->Examples->Ensemble methods->Categorical Feature Support in Gradient Boosting->Limiting the number of splits"
            ],
            [
                "torch->Model Optimization->(beta) Quantized Transfer Learning for Computer Vision Tutorial->Part 2. Finetuning the Quantizable Model->Finetuning the model"
            ]
        ]
    },
    "508146": {
        "jupyter_code_cell": "plt.figure(figsize=(20,10))\nplt.scatter(np.arange(1948,2014,1), climate_new[climate_new['Country Name'] == 'Australia']['value'])\nplt.xlabel('Year', size=20)\nplt.ylabel('Temperature', size=20)\nplt.title('Australia Temperature change from 1948-2013',size=30)\nnew_merged = merged.merge(climate_new,how='inner',on = ['Country Name','years'])",
        "matched_tutorial_code_inds": [
            5890,
            550,
            401,
            3772,
            3636
        ],
        "matched_tutorial_codes": [
            "plt.figure(figsize=(6, 6))\nsymbols = [\"D\", \"^\"]\ncolors = [\"r\", \"g\", \"blue\"]\nfactor_groups = salary_table.groupby([\"E\", \"M\"])\nfor values, group in factor_groups:\n    i, j = values\n    plt.scatter(group[\"X\"], group[\"S\"], marker=symbols[j], color=colors[i - 1], s=144)\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "plt.figure(figsize=(10, 10))\nplt.subplot(2, 2, 1)\nplt.plot(logs[\"reward\"])\nplt.title(\"training rewards (average)\")\nplt.subplot(2, 2, 2)\nplt.plot(logs[\"step_count\"])\nplt.title(\"Max step count (training)\")\nplt.subplot(2, 2, 3)\nplt.plot(logs[\"eval reward (sum)\"])\nplt.title(\"Return (test)\")\nplt.subplot(2, 2, 4)\nplt.plot(logs[\"eval step_count\"])\nplt.title(\"Max step count (test)\")\nplt.show()\n\n\n<img alt=\"training rewards (average), Max step count (training), Return (test), Max step count (test)\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_reinforcement_ppo_001.png\" srcset=\"../_images/sphx_glr_reinforcement_ppo_001.png\"/>",
            "plt.figure(figsize=(5,5))\nplt.plot(epsilons, accuracies, \"*-\")\nplt.yticks(np.arange(0, 1.1, step=0.1))\nplt.xticks(np.arange(0, .35, step=0.05))\nplt.title(\"Accuracy vs Epsilon\")\nplt.xlabel(\"Epsilon\")\nplt.ylabel(\"Accuracy\")\nplt.show()\n\n\n<img alt=\"Accuracy vs Epsilon\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_fgsm_tutorial_001.png\" srcset=\"../_images/sphx_glr_fgsm_tutorial_001.png\"/>",
            "plt.figure(figsize=(8, 5))\n(wins.win_pct\n    .unstack()\n    .assign(**{'Home Win % - Away %': lambda x: x.home_team - x.away_team,\n               'Overall %': lambda x: (x.home_team + x.away_team) / 2})\n     .pipe((sns.regplot, 'data'), x='Overall %', y='Home Win % - Away %')\n)\nsns.despine()\nplt.tight_layout()",
            "plt.figure(figsize=(15, 5))\n(df[['fl_date', 'tail_num', 'dep_time', 'dep_delay']]\n    .dropna()\n    .assign(hour=lambda x: x.dep_time.dt.hour)\n    .query('5 &lt; dep_delay &lt; 600')\n    .pipe((sns.boxplot, 'data'), 'hour', 'dep_delay'))\nsns.despine()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "torch->Reinforcement Learning->Reinforcement Learning (PPO) with TorchRL Tutorial->Results"
            ],
            [
                "torch->Image and Video->Adversarial Example Generation->Results->Accuracy vs Epsilon"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team"
            ],
            [
                "pandas_toms_blog->Method Chaining->Method Chaining->Application"
            ]
        ]
    },
    "643703": {
        "jupyter_code_cell": "depthbins = pd.DataFrame({'mindepth': [0, 75],\n                          'maxdepth': [60, 200]},\n                         index=['shallow','deep'],\n                         columns=['mindepth', 'maxdepth', 'minz', 'maxz'])\ndepth = np.array(avg.variables['zsalt'])\ndepthindices=list()\nfor d in range(len(depthbins.index)):\n    indicesz=np.where(np.logical_and(depth[:] <= depthbins.maxdepth[d],\n                                     depth[:] >= depthbins.mindepth[d]))\n    depthindices.append(indicesz[0])\n    if len(depthindices[d]) > 0:\n        depthbins.minz[d] = min(depthindices[d])\n        depthbins.maxz[d] = max(depthindices[d])\ndepthbins['label']=['0 to 60 m', '75 to 200 m']\nmetadata = pd.DataFrame({\n         'orgName': [\n                   'icephl_latlon','ben_latlon',\n                   'aice_latlon',\n                 'phs_latlon','phl_latlon','mzl_latlon','cop_latlon','ncao_latlon','ncas_latlon','eup_latlon','det_latlon',\n                 'temp_latlon',\n                 'u_latlon',\n                 'v_latlon',],\n         'name': ['Ice Phytoplankton Concentration',\n                  'Benthos Concentration',\n                  'Sea Ice Area Fraction',\n                  'Small Phytoplankton Concentration',\n                  'Large Phytoplankton Concentration',\n                  'Large Microzooplankton Concentration',\n                  'Small Coastal Copepod Concentration',\n                  'Offshore Neocalanus Concentration',\n                  'Neocalanus Concentration',\n                  'Euphausiids Concentration',\n                  'Detritus Concentration',\n                  'Sea Water Temperature',\n                  'Zonal (U) Current',\n                  'Meridional (V) Current'],\n         'units': ['mgC/m2','mgC/m2',\n                   'Fraction',\n                   'mgC/m3','mgC/m3','mgC/m3','mgC/m3','mgC/m3','mgC/m3','mgC/m3','mgC/m3',\n                   'degrees C',\n                   'm/s',\n                   'm/s']\n       })",
        "matched_tutorial_code_inds": [
            2120,
            220,
            5902,
            4583,
            2118
        ],
        "matched_tutorial_codes": [
            "dict_pos_estimator = (\n    n_components=n_components,\n    alpha=0.1,\n    max_iter=50,\n    batch_size=3,\n    fit_algorithm=\"cd\",\n    random_state=rng,\n    positive_dict=True,\n    positive_code=True,\n)\ndict_pos_estimator.fit(faces_centered)\nplot_gallery(\n    \"Dictionary learning - positive dictionary & code\",\n    dict_pos_estimator.components_[:n_components],\n    cmap=plt.cm.RdBu,\n)\n\n\n<img alt=\"Dictionary learning - positive dictionary & code\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_013.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_013.png\"/>",
            "labels_map = {\n    0: \"T-Shirt\",\n    1: \"Trouser\",\n    2: \"Pullover\",\n    3: \"Dress\",\n    4: \"Coat\",\n    5: \"Sandal\",\n    6: \"Shirt\",\n    7: \"Sneaker\",\n    8: \"Bag\",\n    9: \"Ankle Boot\",\n}\nfigure = plt.figure(figsize=(8, 8))\ncols, rows = 3, 3\nfor i in range(1, cols * rows + 1):\n    sample_idx = (len(), size=(1,)).item()\n    ,  = [sample_idx]\n    figure.add_subplot(rows, cols, i)\n    plt.title(labels_map[])\n    plt.axis(\"off\")\n    plt.imshow(.squeeze(), cmap=\"gray\")\nplt.show()\n\n\n<img alt=\"Pullover, Sandal, Bag, Bag, Sneaker, Bag, Pullover, Sandal, Sandal\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_data_tutorial_001.png\" srcset=\"../../_images/sphx_glr_data_tutorial_001.png\"/>",
            "resid = lm.resid\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    group_num = i * 2 + j - 1  # for plotting purposes\n    x = [group_num] * len(group)\n    plt.scatter(\n        x,\n        resid[group.index],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\nplt.xlabel(\"Group\")\nplt.ylabel(\"Residuals\")",
            "input = np.array([[0, 0, 0, 0, 0, 0, 0],\n...                   [0, 1, 1, 1, 1, 1, 0],\n...                   [0, 1, 0, 0, 0, 1, 0],\n...                   [0, 1, 0, 0, 0, 1, 0],\n...                   [0, 1, 0, 0, 0, 1, 0],\n...                   [0, 1, 1, 1, 1, 1, 0],\n...                   [0, 0, 0, 0, 0, 0, 0]], np.uint8)\n markers = np.array([[1, 0, 0, 0, 0, 0, 0],\n...                     [0, 0, 0, 0, 0, 0, 0],\n...                     [0, 0, 0, 0, 0, 0, 0],\n...                     [0, 0, 0, 2, 0, 0, 0],\n...                     [0, 0, 0, 0, 0, 0, 0],\n...                     [0, 0, 0, 0, 0, 0, 0],\n...                     [0, 0, 0, 0, 0, 0, 0]], np.int8)\n from scipy.ndimage import watershed_ift\n watershed_ift(input, markers)\narray([[1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 2, 2, 2, 1, 1],\n       [1, 2, 2, 2, 2, 2, 1],\n       [1, 2, 2, 2, 2, 2, 1],\n       [1, 2, 2, 2, 2, 2, 1],\n       [1, 1, 2, 2, 2, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1]], dtype=int8)",
            "dict_pos_dict_estimator = (\n    n_components=n_components,\n    alpha=0.1,\n    max_iter=50,\n    batch_size=3,\n    random_state=rng,\n    positive_dict=True,\n)\ndict_pos_dict_estimator.fit(faces_centered)\nplot_gallery(\n    \"Dictionary learning - positive dictionary\",\n    dict_pos_dict_estimator.components_[:n_components],\n    cmap=plt.cm.RdBu,\n)\n\n\n<img alt=\"Dictionary learning - positive dictionary\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_011.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_011.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition: Dictionary learning->Dictionary learning - positive dictionary & code"
            ],
            [
                "torch->Introduction to PyTorch->Datasets & DataLoaders->Iterating and Visualizing the Dataset"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "scipy->Multidimensional image processing (scipy.ndimage)->Segmentation and labeling"
            ],
            [
                "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition: Dictionary learning->Dictionary learning - positive dictionary"
            ]
        ]
    },
    "1159324": {
        "jupyter_code_cell": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=0)\nfrom sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\nscale.fit(X_train)\nX_train_std = scale.transform(X_train)\nX_test_std = scale.transform(X_test)\nfrom sklearn.linear_model import LogisticRegression\nlogr = LogisticRegression(C = 10000, random_state = 0)\nlogr.fit(X_train_std, y_train)\nimport VisualFuncs as vf\nvf.VDR(X_test_std, y_test, classifier = logr)",
        "matched_tutorial_code_inds": [
            1823,
            1818,
            2503,
            1841,
            3408
        ],
        "matched_tutorial_codes": [
            "from sklearn.tree import \nfrom sklearn.model_selection import \nimport numpy as np\n\nn_samples, n_features = 1000, 20\nrng = (0)\nX = rng.randn(n_samples, n_features)\n# positive integer target correlated with X[:, 5] with many zeros:\ny = rng.poisson(lam=(X[:, 5]) / 2)\nX_train, X_test, y_train, y_test = (X, y, random_state=rng)\nregressor = (criterion=\"poisson\", random_state=0)\nregressor.fit(X_train, y_train)",
            "from sklearn.datasets import \nfrom sklearn.pipeline import \nfrom sklearn.model_selection import \nfrom sklearn.preprocessing import \nfrom sklearn.kernel_approximation import \nfrom sklearn.linear_model import \n\nX, y = (return_X_y=True)\npipe = (\n    (),\n    (degree=2, n_components=300),\n    (max_iter=1000),\n)\nX_train, X_test, y_train, y_test = (\n    X, y, train_size=5000, test_size=10000, random_state=42\n)\npipe.fit(X_train, y_train).score(X_test, y_test)",
            "from sklearn.svm import \nfrom sklearn.datasets import \nfrom sklearn.feature_selection import \nimport matplotlib.pyplot as plt\n\n# Load the digits dataset\ndigits = ()\nX = digits.images.reshape((len(digits.images), -1))\ny = digits.target\n\n# Create the RFE object and rank each pixel\nsvc = (kernel=\"linear\", C=1)\nrfe = (estimator=svc, n_features_to_select=1, step=1)\nrfe.fit(X, y)\nranking = rfe.ranking_.reshape(digits.images[0].shape)\n\n# Plot pixel ranking\n(ranking, cmap=plt.cm.Blues)\n()\n(\"Ranking of pixels with RFE\")\n()",
            "from sklearn.model_selection import \nfrom sklearn.datasets import \nfrom sklearn.linear_model import \nimport numpy as np\n\nn_samples, n_features = 1000, 20\nrng = (0)\nX, y = (n_samples, n_features, random_state=rng)\nsample_weight = rng.rand(n_samples)\nX_train, X_test, y_train, y_test, sw_train, sw_test = (\n    X, y, sample_weight, random_state=rng\n)\nreg = ()\nreg.fit(X_train, y_train, sample_weight=sw_train)\nprint(reg.score(X_test, y_test, sw_test))",
            "from sklearn.datasets import \nfrom sklearn.model_selection import \nfrom sklearn.preprocessing import \n\nX, y = (return_X_y=True, as_frame=True)\nscaler = ().set_output(transform=\"pandas\")\n\nX_train, X_test, y_train, y_test = (\n    X, y, test_size=0.30, random_state=42\n)\nscaled_X_train = scaler.fit_transform(X_train)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 0.24->New Poisson splitting criterion for DecisionTreeRegressor"
            ],
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 0.24->New PolynomialCountSketch kernel approximation function"
            ],
            [
                "sklearn->Examples->Feature Selection->Recursive feature elimination"
            ],
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 0.23->Sample-weight support for Lasso and ElasticNet"
            ],
            [
                "sklearn->Examples->Preprocessing->Importance of Feature Scaling->Load and prepare data"
            ]
        ]
    },
    "331630": {
        "jupyter_code_cell": "vg_df.sort_values('Global_Sales', ascending=False).head(10).Name\nx = vg_df.groupby(['Genre', 'Name']).sum().reset_index().groupby('Genre')\nbest_selling_titles_by_genre_df = pd.DataFrame()\nfor name, group in x:\n    temp_col = group.sort_values('Global_Sales', ascending=False).head(10).Name.reset_index(drop=True)\n    best_selling_titles_by_genre_df[name] = temp_col",
        "matched_tutorial_code_inds": [
            2997,
            149,
            3173,
            2753,
            3474
        ],
        "matched_tutorial_codes": [
            "tree_disp = (tree, X, [\"age\"])\nmlp_disp = (\n    mlp, X, [\"age\"], ax=tree_disp.axes_, line_kw={\"color\": \"red\"}\n)\n\n\n<img alt=\"plot partial dependence visualization api\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_partial_dependence_visualization_api_006.png\" srcset=\"../../_images/sphx_glr_plot_partial_dependence_visualization_api_006.png\"/>",
            "num_threads = ()\nprint(f'Benchmarking on {num_threads} threads')\n\nt0 = (\n    stmt='batched_dot_mul_sum(x, x)',\n    setup='from __main__ import batched_dot_mul_sum',\n    globals={'x': x},\n    num_threads=num_threads,\n    label='Multithreaded batch dot',\n    sub_label='Implemented using mul and sum')\n\nt1 = (\n    stmt='batched_dot_bmm(x, x)',\n    setup='from __main__ import batched_dot_bmm',\n    globals={'x': x},\n    num_threads=num_threads,\n    label='Multithreaded batch dot',\n    sub_label='Implemented using bmm')\n\nprint(t0.timeit(100))\nprint(t1.timeit(100))\n\n\n\nOutput",
            "pair_scores = []\nmean_tpr = dict()\n\nfor ix, (label_a, label_b) in enumerate(pair_list):\n\n    a_mask = y_test == label_a\n    b_mask = y_test == label_b\n    ab_mask = (a_mask, b_mask)\n\n    a_true = a_mask[ab_mask]\n    b_true = b_mask[ab_mask]\n\n    idx_a = (label_binarizer.classes_ == label_a)[0]\n    idx_b = (label_binarizer.classes_ == label_b)[0]\n\n    fpr_a, tpr_a, _ = (a_true, y_score[ab_mask, idx_a])\n    fpr_b, tpr_b, _ = (b_true, y_score[ab_mask, idx_b])\n\n    mean_tpr[ix] = (fpr_grid)\n    mean_tpr[ix] += (fpr_grid, fpr_a, tpr_a)\n    mean_tpr[ix] += (fpr_grid, fpr_b, tpr_b)\n    mean_tpr[ix] /= 2\n    mean_score = (fpr_grid, mean_tpr[ix])\n    pair_scores.append(mean_score)\n\n    fig, ax = (figsize=(6, 6))\n    (\n        fpr_grid,\n        mean_tpr[ix],\n        label=f\"Mean {label_a} vs {label_b} (AUC = {mean_score :.2f})\",\n        linestyle=\":\",\n        linewidth=4,\n    )\n    (\n        a_true,\n        y_score[ab_mask, idx_a],\n        ax=ax,\n        name=f\"{label_a} as positive class\",\n    )\n    (\n        b_true,\n        y_score[ab_mask, idx_b],\n        ax=ax,\n        name=f\"{label_b} as positive class\",\n    )\n    ([0, 1], [0, 1], \"k--\", label=\"chance level (AUC = 0.5)\")\n    (\"square\")\n    (\"False Positive Rate\")\n    (\"True Positive Rate\")\n    (f\"{target_names[idx_a]} vs {label_b} ROC curves\")\n    ()\n    ()\n\nprint(f\"Macro-averaged One-vs-One ROC AUC score:\\n{(pair_scores):.2f}\")\n\n\n\n<img alt=\"setosa vs versicolor ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_004.png\" srcset=\"../../_images/sphx_glr_plot_roc_004.png\"/>\n<img alt=\"setosa vs virginica ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_005.png\" srcset=\"../../_images/sphx_glr_plot_roc_005.png\"/>\n<img alt=\"versicolor vs virginica ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_006.png\" srcset=\"../../_images/sphx_glr_plot_roc_006.png\"/>",
            "quantiles = [0.05, 0.5, 0.95]\npredictions = {}\nout_bounds_predictions = (y_true_mean, dtype=)\nfor quantile in quantiles:\n    qr = (quantile=quantile, alpha=0, solver=solver)\n    y_pred = qr.fit(X, y_pareto).predict(X)\n    predictions[quantile] = y_pred\n\n    if quantile == min(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred = y_pareto\n        )\n    elif quantile == max(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred &lt;= y_pareto\n        )",
            "model_l2 = (penalty=\"l2\", loss=\"squared_hinge\", dual=True)\nCs = (-4.5, -2, 10)\n\nlabels = [f\"fraction: {train_size}\" for train_size in train_sizes]\nresults = {\"C\": Cs}\nfor label, train_size in zip(labels, train_sizes):\n    cv = (train_size=train_size, test_size=0.3, n_splits=50, random_state=1)\n    train_scores, test_scores = (\n        model_l2, X, y, param_name=\"C\", param_range=Cs, cv=cv\n    )\n    results[label] = test_scores.mean(axis=1)\nresults = (results)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Miscellaneous->Advanced Plotting With Partial Dependence->Plotting partial dependence for one feature"
            ],
            [
                "torch->PyTorch Recipes->PyTorch Benchmark->Steps->3. Benchmarking with torch.utils.benchmark.Timer"
            ],
            [
                "sklearn->Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->One-vs-One multiclass ROC->ROC curve using the OvO macro-average"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Quantile regression->Fitting a QuantileRegressor"
            ],
            [
                "sklearn->Examples->Support Vector Machines->Scaling the regularization parameter for SVCs->L2-penalty case"
            ]
        ]
    },
    "1026300": {
        "jupyter_code_cell": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nplt.style.use('fivethirtyeight')\n%matplotlib inline\ndef group_A_fit(training_data):\n    crimedata = pd.read_csv(training_data)\n    m1 = smf.ols(formula=\"ViolentCrimesPerPop ~ NumUnderPov + MalePctDivorce + PctPersDenseHous + pctUrban + racepctblack + \\\n        np.power(racepctblack,2) + PctKids2Par\", data=crimedata).fit()\n    return m1\nm1 = group_A_fit(\"crime-train.csv\")\nprint(m1.summary())",
        "matched_tutorial_code_inds": [
            2486,
            1868,
            3285,
            2631,
            5256
        ],
        "matched_tutorial_codes": [
            "import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.linear_model import \n\nridge = (alphas=(-6, 6, num=5)).fit(X, y)\nimportance = np.abs(ridge.coef_)\nfeature_names = (diabetes.feature_names)\n(height=importance, x=feature_names)\n(\"Feature importances via coefficients\")\n()\n\n\n<img alt=\"Feature importances via coefficients\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_select_from_model_diabetes_001.png\" srcset=\"../../_images/sphx_glr_plot_select_from_model_diabetes_001.png\"/>",
            "import matplotlib.pyplot as plt\nfrom matplotlib.gridspec import \n\nfig = (figsize=(10, 10))\ngs = (4, 2)\ncolors = plt.cm.get_cmap(\"Dark2\")\n\nax_calibration_curve = fig.add_subplot(gs[:2, :2])\ncalibration_displays = {}\nmarkers = [\"^\", \"v\", \"s\", \"o\"]\nfor i, (clf, name) in enumerate(clf_list):\n    clf.fit(X_train, y_train)\n    display = (\n        clf,\n        X_test,\n        y_test,\n        n_bins=10,\n        name=name,\n        ax=ax_calibration_curve,\n        color=colors(i),\n        marker=markers[i],\n    )\n    calibration_displays[name] = display\n\nax_calibration_curve.grid()\nax_calibration_curve.set_title(\"Calibration plots\")\n\n# Add histogram\ngrid_positions = [(2, 0), (2, 1), (3, 0), (3, 1)]\nfor i, (_, name) in enumerate(clf_list):\n    row, col = grid_positions[i]\n    ax = fig.add_subplot(gs[row, col])\n\n    ax.hist(\n        calibration_displays[name].y_prob,\n        range=(0, 1),\n        bins=10,\n        label=name,\n        color=colors(i),\n    )\n    ax.set(title=name, xlabel=\"Mean predicted probability\", ylabel=\"Count\")\n\n()\n()\n\n\n<img alt=\"Calibration plots, Logistic, Naive Bayes, SVC, Random forest\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_compare_calibration_001.png\" srcset=\"../../_images/sphx_glr_plot_compare_calibration_001.png\"/>",
            "import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.colors import \nfrom sklearn import neighbors, datasets\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\nn_neighbors = 15\n\n# import some data to play with\niris = ()\n\n# we only take the first two features. We could avoid this ugly\n# slicing by using a two-dim dataset\nX = iris.data[:, :2]\ny = iris.target\n\n# Create color maps\ncmap_light = ([\"orange\", \"cyan\", \"cornflowerblue\"])\ncmap_bold = [\"darkorange\", \"c\", \"darkblue\"]\n\nfor weights in [\"uniform\", \"distance\"]:\n    # we create an instance of Neighbours Classifier and fit the data.\n    clf = (n_neighbors, weights=weights)\n    clf.fit(X, y)\n\n    _, ax = ()\n    (\n        clf,\n        X,\n        cmap=cmap_light,\n        ax=ax,\n        response_method=\"predict\",\n        plot_method=\"pcolormesh\",\n        xlabel=iris.feature_names[0],\n        ylabel=iris.feature_names[1],\n        shading=\"auto\",\n    )\n\n    # Plot also the training points\n    (\n        x=X[:, 0],\n        y=X[:, 1],\n        hue=iris.target_names[y],\n        palette=cmap_bold,\n        alpha=1.0,\n        edgecolor=\"black\",\n    )\n    (\n        \"3-Class classification (k = %i, weights = '%s')\" % (n_neighbors, weights)\n    )\n\n()",
            "import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.colors import \n\n(figsize=(10, 6))\nax = (\n    df.T,\n    norm=(linthresh=10e-4, vmin=-80, vmax=80),\n    cbar_kws={\"label\": \"coefficients' values\"},\n    cmap=\"seismic_r\",\n)\n(\"linear model\")\n(\"coefficients\")\n(rect=(0, 0, 1, 0.95))\n_ = (\"Models' coefficients\")\n\n\n<img alt=\"Models' coefficients\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_ard_001.png\" srcset=\"../../_images/sphx_glr_plot_ard_001.png\"/>",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pandas_datareader as pdr\nimport seaborn\n\nimport statsmodels.api as sm\nfrom statsmodels.regression.rolling import RollingOLS\n\nseaborn.set_style(\"darkgrid\")\npd.plotting.register_matplotlib_converters()\n%matplotlib inline"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Feature Selection->Model-based and sequential feature selection->Feature importance from coefficients"
            ],
            [
                "sklearn->Examples->Calibration->Comparison of Calibration of Classifiers->Calibration curves"
            ],
            [
                "sklearn->Examples->Nearest Neighbors->Nearest Neighbors Classification"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Comparing Linear Bayesian Regressors->Models robustness to recover the ground truth weights->Plot the true and estimated coefficients"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Rolling Least Squares"
            ]
        ]
    },
    "997656": {
        "jupyter_code_cell": "def plot_ccdf(p, f, ax, deltax=None, xlog=False, xlim=[0, 1], deltay=0.25, ylog=False, ylim=[0,1], xlabel = 'x'):\n    ecdf = sm.distributions.ECDF(p)\n    x = ecdf.x\n    y = 1 - ecdf.y\n    assert len(x) == len(y)\n    if deltax is not None:\n        x_ticks = np.arange(xlim[0], xlim[1] + deltax, deltax)\n        ax.set_xticks(x_ticks)\n    ax.set_xlabel(xlabel)\n    ax.set_xlim(xlim[0], xlim[1])\n    ax.vlines(np.mean(p), min(y), max(y), color='red', label='mean', linewidth=2)\n    ax.vlines(np.median(p), min(y), max(y), color='orange', label='median', linewidth=2)\n    ax.vlines(np.mean(p) + 2 * np.std(p), min(y), max(y), color='blue', label='mean + 2 * std', linewidth=2)\n    ax.vlines(np.mean(p) + 3 * np.std(p), min(y), max(y), color='green', label='mean + 3 * std', linewidth=2)\n    y_ticks = np.arange(ylim[0], ylim[1] + deltay, deltay)\n    ax.set_xlabel(f)\n    ax.set_ylabel('CCDF')\n    ax.set_yticks(y_ticks)\n    ax.set_ylim(ylim[0], ylim[1])\n    if xlog is True:\n        ax.set_xscale('log')\n    if ylog is True:\n        ax.set_yscale('log')\n    ax.grid(which='minor', alpha=0.5)\n    ax.grid(which='major', alpha=0.9)\n    ax.legend(loc=4)\n    sns.set_style('whitegrid')\n    sns.regplot(x=x, y=y, fit_reg=False, scatter=True, ax = ax)\nfigure, axes = plt.subplots(figsize=(15,5))\nplot_ccdf(df_data['per_listen'], 'per_listen', ax= axes)\nfeatures = ['per_listen', 'listen_duration', 'n_loops', 'is_loop']\ng = sns.clustermap(df_data[features].corr(), \n                   linewidths=1, \n                   cmap=\"YlGnBu\", \n                   square=True, \n                   annot=True, \n                   fmt='.3f', \n                   figsize = (5, 5))\nplt.setp(g.ax_heatmap.get_xticklabels(), rotation=90); ",
        "matched_tutorial_code_inds": [
            3836,
            2392,
            3128,
            5364,
            3263
        ],
        "matched_tutorial_codes": [
            "def tsplot(y, lags=None, figsize=(10, 8)):\n    fig = plt.figure(figsize=figsize)\n    layout = (2, 2)\n    ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n    acf_ax = plt.subplot2grid(layout, (1, 0))\n    pacf_ax = plt.subplot2grid(layout, (1, 1))\n    \n    y.plot(ax=ts_ax)\n    smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n    smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n    [ax.set_xlim(1.5) for ax in [acf_ax, pacf_ax]]\n    sns.despine()\n    plt.tight_layout()\n    return ts_ax, acf_ax, pacf_ax",
            "def plot_influence(conf, mse_values, prediction_times, complexities):\n    \"\"\"\n    Plot influence of model complexity on both accuracy and latency.\n    \"\"\"\n\n    fig = ()\n    fig.subplots_adjust(right=0.75)\n\n    # first axes (prediction error)\n    ax1 = fig.add_subplot(111)\n    line1 = ax1.plot(complexities, mse_values, c=\"tab:blue\", ls=\"-\")[0]\n    ax1.set_xlabel(\"Model Complexity (%s)\" % conf[\"complexity_label\"])\n    y1_label = conf[\"prediction_performance_label\"]\n    ax1.set_ylabel(y1_label)\n\n    ax1.spines[\"left\"].set_color(line1.get_color())\n    ax1.yaxis.label.set_color(line1.get_color())\n    ax1.tick_params(axis=\"y\", colors=line1.get_color())\n\n    # second axes (latency)\n    ax2 = fig.add_subplot(111, sharex=ax1, frameon=False)\n    line2 = ax2.plot(complexities, prediction_times, c=\"tab:orange\", ls=\"-\")[0]\n    ax2.yaxis.tick_right()\n    ax2.yaxis.set_label_position(\"right\")\n    y2_label = \"Time (s)\"\n    ax2.set_ylabel(y2_label)\n    ax1.spines[\"right\"].set_color(line2.get_color())\n    ax2.yaxis.label.set_color(line2.get_color())\n    ax2.tick_params(axis=\"y\", colors=line2.get_color())\n\n    (\n        (line1, line2), (\"prediction error\", \"prediction latency\"), loc=\"upper center\"\n    )\n\n    (\n        \"Influence of varying '%s' on %s\"\n        % (conf[\"changing_param\"], conf[\"estimator\"].__name__)\n    )\n\n\nfor conf in configurations:\n    prediction_performances, prediction_times, complexities = benchmark_influence(conf)\n    plot_influence(conf, prediction_performances, prediction_times, complexities)\n()\n\n\n\n<img alt=\"Influence of varying 'l1_ratio' on SGDClassifier\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_model_complexity_influence_001.png\" srcset=\"../../_images/sphx_glr_plot_model_complexity_influence_001.png\"/>\n<img alt=\"Influence of varying 'nu' on NuSVR\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_model_complexity_influence_002.png\" srcset=\"../../_images/sphx_glr_plot_model_complexity_influence_002.png\"/>\n<img alt=\"Influence of varying 'n_estimators' on GradientBoostingRegressor\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_model_complexity_influence_003.png\" srcset=\"../../_images/sphx_glr_plot_model_complexity_influence_003.png\"/>",
            "def make_heatmap(ax, gs, is_sh=False, make_cbar=False):\n    \"\"\"Helper to make a heatmap.\"\"\"\n    results = (gs.cv_results_)\n    results[[\"param_C\", \"param_gamma\"]] = results[[\"param_C\", \"param_gamma\"]].astype(\n        \n    )\n    if is_sh:\n        # SH dataframe: get mean_test_score values for the highest iter\n        scores_matrix = results.sort_values(\"iter\").pivot_table(\n            index=\"param_gamma\",\n            columns=\"param_C\",\n            values=\"mean_test_score\",\n            aggfunc=\"last\",\n        )\n    else:\n        scores_matrix = results.pivot(\n            index=\"param_gamma\", columns=\"param_C\", values=\"mean_test_score\"\n        )\n\n    im = ax.imshow(scores_matrix)\n\n    ax.set_xticks((len(Cs)))\n    ax.set_xticklabels([\"{:.0E}\".format(x) for x in Cs])\n    ax.set_xlabel(\"C\", fontsize=15)\n\n    ax.set_yticks((len(gammas)))\n    ax.set_yticklabels([\"{:.0E}\".format(x) for x in gammas])\n    ax.set_ylabel(\"gamma\", fontsize=15)\n\n    # Rotate the tick labels and set their alignment.\n    (ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n\n    if is_sh:\n        iterations = results.pivot_table(\n            index=\"param_gamma\", columns=\"param_C\", values=\"iter\", aggfunc=\"max\"\n        ).values\n        for i in range(len(gammas)):\n            for j in range(len(Cs)):\n                ax.text(\n                    j,\n                    i,\n                    iterations[i, j],\n                    ha=\"center\",\n                    va=\"center\",\n                    color=\"w\",\n                    fontsize=20,\n                )\n\n    if make_cbar:\n        fig.subplots_adjust(right=0.8)\n        cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n        fig.colorbar(im, cax=cbar_ax)\n        cbar_ax.set_ylabel(\"mean_test_score\", rotation=-90, va=\"bottom\", fontsize=15)\n\n\nfig, axes = (ncols=2, sharey=True)\nax1, ax2 = axes\n\nmake_heatmap(ax1, gsh, is_sh=True)\nmake_heatmap(ax2, gs, make_cbar=True)\n\nax1.set_title(\"Successive Halving\\ntime = {:.3f}s\".format(gsh_time), fontsize=15)\nax2.set_title(\"GridSearch\\ntime = {:.3f}s\".format(gs_time), fontsize=15)\n\n()\n\n\n<img alt=\"Successive Halving time = 1.240s, GridSearch time = 5.943s\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_successive_halving_heatmap_001.png\" srcset=\"../../_images/sphx_glr_plot_successive_halving_heatmap_001.png\"/>",
            "def beanplot(data, plot_opts={}, jitter=False):\n    \"\"\"helper function to try out different plot options\"\"\"\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    plot_opts_ = {\n        \"cutoff_val\": 5,\n        \"cutoff_type\": \"abs\",\n        \"label_fontsize\": \"small\",\n        \"label_rotation\": 30,\n    }\n    plot_opts_.update(plot_opts)\n    sm.graphics.beanplot(\n        data, ax=ax, labels=labels, jitter=jitter, plot_opts=plot_opts_\n    )\n    ax.set_xlabel(\"Party identification of respondent.\")\n    ax.set_ylabel(\"Age\")",
            "def plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\n    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n\n    # Generate the training/testing visualizations for each CV split\n    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y, groups=group)):\n        # Fill in indices with the training/test groups\n        indices = ([] * len(X))\n        indices[tt] = 1\n        indices[tr] = 0\n\n        # Visualize the results\n        ax.scatter(\n            range(len(indices)),\n            [ii + 0.5] * len(indices),\n            c=indices,\n            marker=\"_\",\n            lw=lw,\n            cmap=cmap_cv,\n            vmin=-0.2,\n            vmax=1.2,\n        )\n\n    # Plot the data classes and groups at the end\n    ax.scatter(\n        range(len(X)), [ii + 1.5] * len(X), c=y, marker=\"_\", lw=lw, cmap=cmap_data\n    )\n\n    ax.scatter(\n        range(len(X)), [ii + 2.5] * len(X), c=group, marker=\"_\", lw=lw, cmap=cmap_data\n    )\n\n    # Formatting\n    yticklabels = list(range(n_splits)) + [\"class\", \"group\"]\n    ax.set(\n        yticks=(n_splits + 2) + 0.5,\n        yticklabels=yticklabels,\n        xlabel=\"Sample index\",\n        ylabel=\"CV iteration\",\n        ylim=[n_splits + 2.2, -0.2],\n        xlim=[0, 100],\n    )\n    ax.set_title(\"{}\".format(type(cv).__name__), fontsize=15)\n    return ax"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series->Autocorrelation"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Model Complexity Influence->Run the code and plot the results"
            ],
            [
                "sklearn->Examples->Model Selection->Comparison between grid search and successive halving"
            ],
            [
                "statsmodels->Examples->Plotting->Box Plots->Bean Plots"
            ],
            [
                "sklearn->Examples->Model Selection->Visualizing cross-validation behavior in scikit-learn->Define a function to visualize cross-validation behavior"
            ]
        ]
    },
    "1448218": {
        "jupyter_code_cell": "print sql_data_path\nsql_with_features = attach_query_result_by_header(sql_data_path)",
        "matched_tutorial_code_inds": [
            3784,
            660,
            2801,
            127,
            2824
        ],
        "matched_tutorial_codes": [
            "import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "interp = (rn18)\n(input)\nprint(interp.summary(True))",
            "X = survey.data[survey.feature_names]\nX.describe(include=\"all\")\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "print(delta.transform(extract_fn_name).filter(lambda fn: \"TensorIterator\" in fn))\n\n\n\n<strong>Instruction count delta (filter)</strong>",
            "feature_names = model[:-1].get_feature_names_out()\n\ncoefs = (\n    model[-1].regressor_.coef_,\n    columns=[\"Coefficients\"],\n    index=feature_names,\n)\n\ncoefs\n\n\n\n\n\n\n\n\n<br/>\n<br/>"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples"
            ],
            [
                "torch->Code Transforms with FX->(beta) Building a Simple CPU Performance Profiler with FX->Investigating the Performance of ResNet18"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "torch->PyTorch Recipes->Timer quick start->6. A/B testing with Callgrind"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ]
        ]
    },
    "1076097": {
        "jupyter_code_cell": "corrmat = train_df.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.9, square=True)\nplt.show()\nprint (corrmat['SalePrice'].sort_values(ascending=False)[:15], '\\n') \nprint ('----------------------')\nprint (corrmat['SalePrice'].sort_values(ascending=False)[-5:]) \nsns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train_df[cols], size = 2.5)\nplt.show()",
        "matched_tutorial_code_inds": [
            2298,
            5917,
            5641,
            5998,
            5978
        ],
        "matched_tutorial_codes": [
            "feature_importance = reg.feature_importances_\nsorted_idx = (feature_importance)\npos = (sorted_idx.shape[0]) + 0.5\nfig = (figsize=(12, 6))\n(1, 2, 1)\n(pos, feature_importance[sorted_idx], align=\"center\")\n(pos, (diabetes.feature_names)[sorted_idx])\n(\"Feature Importance (MDI)\")\n\nresult = (\n    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_idx = result.importances_mean.argsort()\n(1, 2, 2)\n(\n    result.importances[sorted_idx].T,\n    vert=False,\n    labels=(diabetes.feature_names)[sorted_idx],\n)\n(\"Permutation Importance (test set)\")\nfig.tight_layout()\n()\n\n\n<img alt=\"Feature Importance (MDI), Permutation Importance (test set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\"/>",
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "glm = smf.glm(\n    \"affairs ~ rate_marriage + yrs_married\", data=data, family=sm.families.Poisson()\n)\nres_o2 = glm.fit()\n# print(res_f2.summary())\nres_o2.pearson_chi2 - res_o.pearson_chi2, res_o2.deviance - res_o.deviance, res_o2.llf - res_o.llf",
            "res5 = combine_effects(eff, var_eff, method_re=\"chi2\", use_t=False)\nres5_df = res5.summary_frame()\nprint(\"method RE:\", res5.method_re)\nprint(res5.summary_frame())\nfig = res5.plot_forest()\nfig.set_figheight(8)\nfig.set_figwidth(6)",
            "res4 = combine_effects(\n    eff, var_eff, method_re=\"iterated\", use_t=False, row_names=rownames\n)\nres4_df = res4.summary_frame()\nprint(\"method RE:\", res4.method_re)\nprint(res4.summary_frame())\nfig = res4.plot_forest()"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Ensemble methods->Gradient Boosting regression->Plot feature importance"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->original observations and frequency weights"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Meta-analysis of proportions->changing data to have positive random effects variance"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Example->Using iterated, Paule-Mandel estimate for random effects variance tau"
            ]
        ]
    },
    "180012": {
        "jupyter_code_cell": "gender=['Male','Female']\nchurn_gender=churn.gender.unique()\nset(gender)==set(churn_gender)\nprint('No inappropriate values in gender column')\nagegroup=[0,1]\nchurn_agegroupe=churn.SeniorCitizen.unique()\nset(churn_agegroupe)==set(agegroup)\nprint('No inappropriate values in SeniorCitizen column column')\nprint('Negative values for tenure:')\nprint(sum(churn['tenure']<0))\nprint('Number of columns with 0 values for tenure:')\nprint(sum(churn['tenure']==0))\nprint('Negative or 0 values for MonthlyCharges:')\nprint(sum(churn['MonthlyCharges']<=0))\nprint('Negative or 0 values for TotalCharges:')\nprint(sum(churn['TotalCharges']<=0))\nchurn_1=['Yes','No']\nchurn_csv=churn.Churn.unique()\nset(churn_1)==set(churn_csv)\nprint('No inappropriate values in Churn column')\nprint(\"Gender desription:\")\nprint(churn[\"gender\"].unique())\nprint(\"SeniorCitizen desription:\")\nprint(churn[\"SeniorCitizen\"].unique())\nprint(\"Partner desription:\")\nprint(churn[\"Partner\"].unique())\nprint(\"Dependents  desription:\")\nprint(churn[\"Dependents\"].unique())\nprint(\"MultipleLines:\")\nprint(churn[\"MultipleLines\"].unique())\nprint(\"InternetService:\")\nprint(churn[\"InternetService\"].unique())\nprint(\"OnlineSecurity:\")\nprint(churn[\"OnlineSecurity\"].unique())\nprint(\"OnlineBackup:\")\nprint(churn[\"OnlineBackup\"].unique())\nprint(\"DeviceProtection:\")\nprint(churn[\"DeviceProtection\"].unique())\nprint(\"TechSupport:\")\nprint(churn[\"TechSupport\"].unique())\nprint(\"StreamingTV:\")\nprint(churn[\"StreamingTV\"].unique())\nprint(\"StreamingTV:\")\nprint(churn[\"StreamingTV\"].unique())\nprint(\"Contract:\")\nprint(churn[\"Contract\"].unique())\nprint(\"PaperlessBilling:\")\nprint(churn[\"PaperlessBilling\"].unique())\nprint(\"PaymentMethod:\")\nprint(churn[\"PaymentMethod\"].unique())\nprint(\"TotalCharges:\")\nprint(churn[\"TotalCharges\"].unique())\nprint(\"Churn:\")\nprint(churn[\"Churn\"].unique())\nsns.boxplot(y=\"tenure\", data=churn, fliersize=5)\nplt.show()\nsns.boxplot(y=\"MonthlyCharges\", data=churn, fliersize=5)\nplt.show()\nsns.boxplot(y=\"TotalCharges\", data=churn, fliersize=10)\nplt.show()",
        "matched_tutorial_code_inds": [
            5718,
            5355,
            5963,
            2753,
            2873
        ],
        "matched_tutorial_codes": [
            "bin = sm.families.Binomial()\nbin.variance = vf()\nmodel2 = sm.GLM.from_formula(\"blotch ~ 0 + C(variety) + C(site)\", family=bin, data=df)\nresult2 = model2.fit(scale=\"X2\")\nprint(result2.summary())",
            "weights = rob_crime_model.weights\nidx = weights  0\nX = rob_crime_model.model.exog[idx.values]\nww = weights[idx] / weights[idx].mean()\nhat_matrix_diag = ww * (X * np.linalg.pinv(X).T).sum(1)\nresid = rob_crime_model.resid\nresid2 = resid ** 2\nresid2 /= resid2.sum()\nnobs = int(idx.sum())\nhm = hat_matrix_diag.mean()\nrm = resid2.mean()",
            "data = [\n    [\"Carroll\", 94, 22, 60, 92, 20, 60],\n    [\"Grant\", 98, 21, 65, 92, 22, 65],\n    [\"Peck\", 98, 28, 40, 88, 26, 40],\n    [\"Donat\", 94, 19, 200, 82, 17, 200],\n    [\"Stewart\", 98, 21, 50, 88, 22, 45],\n    [\"Young\", 96, 21, 85, 92, 22, 85],\n]\ncolnames = [\"study\", \"mean_t\", \"sd_t\", \"n_t\", \"mean_c\", \"sd_c\", \"n_c\"]\nrownames = [i[0] for i in data]\ndframe1 = pd.DataFrame(data, columns=colnames)\nrownames",
            "quantiles = [0.05, 0.5, 0.95]\npredictions = {}\nout_bounds_predictions = (y_true_mean, dtype=)\nfor quantile in quantiles:\n    qr = (quantile=quantile, alpha=0, solver=solver)\n    y_pred = qr.fit(X, y_pareto).predict(X)\n    predictions[quantile] = y_pred\n\n    if quantile == min(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred = y_pareto\n        )\n    elif quantile == max(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred &lt;= y_pareto\n        )",
            "features_names = [\"experience\", \"parent hourly wage\", \"college degree\"]\n\nregressor_without_ability = ()\nregressor_without_ability.fit(X_train[features_names], y_train)\ny_pred_without_ability = regressor_without_ability.predict(X_test[features_names])\nR2_without_ability = (y_test, y_pred_without_ability)\n\nprint(f\"R2 score without ability: {R2_without_ability:.3f}\")"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Generalized Linear Models->Quasi-binomial regression"
            ],
            [
                "statsmodels->Examples->Plotting->Regression Plots->Statewide Crime 2009 Dataset->Using robust regression to correct for outliers."
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Example"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Quantile regression->Fitting a QuantileRegressor"
            ],
            [
                "sklearn->Examples->Inspection->Failure of Machine Learning to infer causal effects->Income prediction with partial observations"
            ]
        ]
    },
    "319062": {
        "jupyter_code_cell": "fpr, tpr, thresholds = metrics.roc_curve(Y_test, model.predict_proba(X_test_tfidf)[:,1])\ntprs.append(tpr)\nfprs.append(fpr)\nroc_labels.append(\"Default Tfidf\")\nax = plt.subplot()\nplt.plot(fpr, tpr)\nplt.xlabel(\"fpr\")\nplt.ylabel(\"tpr\")\nplt.title(\"ROC Curve\")\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.show()\nfor fpr, tpr, roc_label in zip(fprs, tprs, roc_labels):\n    plt.plot(fpr, tpr, label=roc_label)\nplt.xlabel(\"fpr\")\nplt.ylabel(\"tpr\")\nplt.title(\"ROC Curves\")\nplt.legend()\nplt.xlim([0, .07])\nplt.ylim([.98, 1])\nplt.show()",
        "matched_tutorial_code_inds": [
            5998,
            2298,
            2601,
            5984,
            5917
        ],
        "matched_tutorial_codes": [
            "res5 = combine_effects(eff, var_eff, method_re=\"chi2\", use_t=False)\nres5_df = res5.summary_frame()\nprint(\"method RE:\", res5.method_re)\nprint(res5.summary_frame())\nfig = res5.plot_forest()\nfig.set_figheight(8)\nfig.set_figwidth(6)",
            "feature_importance = reg.feature_importances_\nsorted_idx = (feature_importance)\npos = (sorted_idx.shape[0]) + 0.5\nfig = (figsize=(12, 6))\n(1, 2, 1)\n(pos, feature_importance[sorted_idx], align=\"center\")\n(pos, (diabetes.feature_names)[sorted_idx])\n(\"Feature Importance (MDI)\")\n\nresult = (\n    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_idx = result.importances_mean.argsort()\n(1, 2, 2)\n(\n    result.importances[sorted_idx].T,\n    vert=False,\n    labels=(diabetes.feature_names)[sorted_idx],\n)\n(\"Permutation Importance (test set)\")\nfig.tight_layout()\n()\n\n\n<img alt=\"Feature Importance (MDI), Permutation Importance (test set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\"/>",
            "vmin, vmax = (-log_marginal_likelihood).min(), 50\nlevel = (((vmin), (vmax), num=50), decimals=1)\n(\n    length_scale_grid,\n    noise_level_grid,\n    -log_marginal_likelihood,\n    levels=level,\n    norm=(vmin=vmin, vmax=vmax),\n)\n()\n(\"log\")\n(\"log\")\n(\"Length-scale\")\n(\"Noise-level\")\n(\"Log-marginal-likelihood\")\n()\n\n\n<img alt=\"Log-marginal-likelihood\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_noisy_005.png\" srcset=\"../../_images/sphx_glr_plot_gpr_noisy_005.png\"/>",
            "res2_PM = combine_effects(eff, var_eff, method_re=\"pm\", use_t=True, row_names=rownames)\nprint(\"method RE:\", res2_PM.method_re)\nprint(res2_PM.summary_frame())\nfig = res2_PM.plot_forest()\nfig.set_figheight(6)\nfig.set_figwidth(6)",
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Meta-analysis of proportions->changing data to have positive random effects variance"
            ],
            [
                "sklearn->Examples->Ensemble methods->Gradient Boosting regression->Plot feature importance"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) with noise-level estimation->Optimisation of kernel hyperparameters in GPR"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Example Kacker interlaboratory mean"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ]
        ]
    },
    "1091790": {
        "jupyter_code_cell": "satdf = dict(type='choropleth', locations=satdf['ST'], locationmode='USA-states', colorscale='Blues',\n        z=satdf['Average New SAT Score'], colorbar = dict(\n          title = \"Average SAT Score\"))\nlayout=dict(geo=dict(scope='usa'),  title = 'Average SAT Score per State')\nmap=go.Figure(data=[satdf], layout=layout)\nplt.savefig('SATScores.png')\npy.iplot(map)\ngrad_rates = os.path.join('Resources','gradrates.csv')\ngrad_df = pd.read_csv(grad_rates)\ngrad_df = grad_df.rename(columns={\"State\":\"ST\"})\ngrad_df.head()",
        "matched_tutorial_code_inds": [
            4090,
            2753,
            6700,
            3646,
            2116
        ],
        "matched_tutorial_codes": [
            "g = sns.catplot(\n    data=titanic,\n    x=\"fare\", y=\"embark_town\", row=\"class\",\n    kind=\"box\", orient=\"h\",\n    sharex=False, margin_titles=True,\n    height=1.5, aspect=4,\n)\ng.set(xlabel=\"Fare\", ylabel=\"\")\ng.set_titles(row_template=\"{row_name} class\")\nfor ax in g.axes.flat:\n    ax.xaxis.set_major_formatter('${x:.0f}')\n",
            "quantiles = [0.05, 0.5, 0.95]\npredictions = {}\nout_bounds_predictions = (y_true_mean, dtype=)\nfor quantile in quantiles:\n    qr = (quantile=quantile, alpha=0, solver=solver)\n    y_pred = qr.fit(X, y_pareto).predict(X)\n    predictions[quantile] = y_pred\n\n    if quantile == min(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred = y_pareto\n        )\n    elif quantile == max(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred &lt;= y_pareto\n        )",
            "ln_pce = np.log(pce.PCE)\nforecasts = {\"ln PCE\": ln_pce}\nfor year in range(1995, 2020, 3):\n    sub = ln_pce[: str(year)]\n    res = ThetaModel(sub).fit()\n    fcast = res.forecast(12)\n    forecasts[str(year)] = fcast\nforecasts = pd.DataFrame(forecasts)\nax = forecasts[\"1995\":].plot(legend=False)\nchildren = ax.get_children()\nchildren[0].set_linewidth(4)\nchildren[0].set_alpha(0.3)\nchildren[0].set_color(\"#000000\")\nax.set_title(\"ln PCE\")\nplt.tight_layout(pad=1.0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_theta-model_22_0.png\" src=\"../../../_images/examples_notebooks_generated_theta-model_22_0.png\"/>",
            "stations = pd.io.json.json_normalize(js['features']).id\nurl = (\"http://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?\"\n       \"&amp;data=tmpf&amp;data=relh&amp;data=sped&amp;data=mslp&amp;data=p01i&amp;data=vsby&amp;data=gust_mph&amp;data=skyc1&amp;data=skyc2&amp;data=skyc3\"\n       \"&amp;tz=Etc/UTC&amp;format=comma&amp;latlon=no\"\n       \"&amp;{start:year1=%Y&amp;month1=%m&amp;day1=%d}\"\n       \"&amp;{end:year2=%Y&amp;month2=%m&amp;day2=%d}&amp;{stations}\")\nstations = \"&amp;\".join(\"station=%s\" % s for s in stations)\nstart = pd.Timestamp('2014-01-01')\nend=pd.Timestamp('2014-01-31')\n\nweather = (pd.read_csv(url.format(start=start, end=end, stations=stations),\n                       comment=\"#\"))",
            "fa_estimator = (n_components=n_components, max_iter=20)\nfa_estimator.fit(faces_centered)\nplot_gallery(\"Factor Analysis (FA)\", fa_estimator.components_[:n_components])\n\n# --- Pixelwise variance\n(figsize=(3.2, 3.6), facecolor=\"white\", tight_layout=True)\nvec = fa_estimator.noise_variance_\nvmax = max(vec.max(), -vec.min())\n(\n    vec.reshape(image_shape),\n    cmap=plt.cm.gray,\n    interpolation=\"nearest\",\n    vmin=-vmax,\n    vmax=vmax,\n)\n(\"off\")\n(\"Pixelwise variance from \\n Factor Analysis (FA)\", size=16, wrap=True)\n(orientation=\"horizontal\", shrink=0.8, pad=0.03)\n()\n\n\n\n<img alt=\"Factor Analysis (FA)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_008.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_008.png\"/>\n<img alt=\"Pixelwise variance from   Factor Analysis (FA)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_009.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_009.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Visualizing categorical data->Showing additional dimensions",
                "seaborn->Plotting functions->Visualizing categorical data->Showing additional dimensions"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Quantile regression->Fitting a QuantileRegressor"
            ],
            [
                "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Personal Consumption Expenditure"
            ],
            [
                "pandas_toms_blog->Indexes"
            ],
            [
                "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition->Factor Analysis components - FA"
            ]
        ]
    },
    "1424865": {
        "jupyter_code_cell": "%%time\nclassifier = LabelPowerset(LogisticRegression())\nclassifier.fit(x_train, y_train)\npredictions = classifier.predict(x_test)\nprint(\"Accuracy = \",accuracy_score(y_test,predictions))\nprint(\"\\n\")\nfrom skmultilearn.adapt import MLkNN\nfrom scipy.sparse import csr_matrix, lil_matrix",
        "matched_tutorial_code_inds": [
            6050,
            513,
            551,
            430,
            3635
        ],
        "matched_tutorial_codes": [
            "%%javascript\nIPython.OutputArea.prototype._should_scroll = function(lines) {\n    return false;\n}\n\n\n\n\n\n\n\n\n<script type=\"text/javascript\">\nvar element = document.currentScript.previousSibling.previousSibling;\nIPython.OutputArea.prototype._should_scroll = function(lines) {\n    return false;\n}\n\n</script>",
            "%%bash\npip3 install gymnasium[classic_control]",
            "%%bash\npip install gym-super-mario-bros==7.4.0",
            "%%bash\npip install torchdata\n\n\n</blockquote>",
            "%config InlineBackend.figure_format = 'png'\nflights = (df[['fl_date', 'tail_num', 'dep_time', 'dep_delay']]\n           .dropna()\n           .sort_values('dep_time')\n           .loc[lambda x: x.dep_delay &lt; 500]\n           .assign(turn = lambda x:\n                x.groupby(['fl_date', 'tail_num'])\n                 .dep_time\n                 .transform('rank').astype(int)))\n\nfig, ax = plt.subplots(figsize=(15, 5))\nsns.boxplot(x='turn', y='dep_delay', data=flights, ax=ax)\nax.set_ylim(-50, 50)\nsns.despine()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->Copulas"
            ],
            [
                "torch->Reinforcement Learning->Reinforcement Learning (DQN) Tutorial"
            ],
            [
                "torch->Reinforcement Learning->Train a Mario-playing RL Agent"
            ],
            [
                "torch->Text->Language Modeling with nn.Transformer and TorchText->Load and batch data"
            ],
            [
                "pandas_toms_blog->Method Chaining->Method Chaining->Application"
            ]
        ]
    },
    "161674": {
        "jupyter_code_cell": "order_prior['add_to_cart_order'].plot(kind='hist',bins=20)\nplt.xlabel('orders in cart')\norder_prior = pd.merge(order_prior, products, on='product_id', how='left')",
        "matched_tutorial_code_inds": [
            3771,
            3767,
            5233,
            3702,
            6218
        ],
        "matched_tutorial_codes": [
            "win_percent.sort_values().plot.barh(figsize=(6, 12), width=.85, color='k')\nplt.tight_layout()\nsns.despine()\nplt.xlabel(\"Win Percent\")",
            "g = sns.FacetGrid(wins.reset_index(), col='team', hue='team', col_wrap=5, size=2)\ng.map(sns.pointplot, 'is_home', 'win_pct')",
            "print(res.recursive_coefficients.filtered[0])\nres.plot_recursive_coefficient(range(mod.k_exog), alpha=None, figsize=(10, 6))",
            "plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "res_fedfunds2.smoothed_marginal_probabilities[0].plot(\n    title=\"Probability of being in the high regime\", figsize=(12, 3)\n)"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 1: Copper"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Markov switching dynamic regression->Federal funds rate with switching intercept and lagged dependent variable"
            ]
        ]
    },
    "187377": {
        "jupyter_code_cell": "from sklearn import decomposition\nfrom sklearn.decomposition import PCA\npca = decomposition.PCA(n_components=2)\npca.fit(user_restaurant_cf.as_matrix())\nuser_pca = pca.transform(user_restaurant_cf)\nplt.figure(figsize=(5,4))\nplt.scatter(user_pca[:,0],user_pca[:,1],s=1)\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.axis((-2,10,-0.4,0.4))\nplt.title('Principle Components Projection of Users')\nplt.show()\nfrom geopy.distance import great_circle\nwithin_x_miles = 1\nlatitude_default, longitude_default = 43.6544, -79.3807 \nmy_location = (latitude_default, longitude_default)\nrestaurants_info_my_dataset = pd.read_csv('restaurants_info_my_dataset.csv')\ndistances_to_my_location = restaurants_info_my_dataset.apply(lambda x: great_circle(np.array(x[['latitude','longitude']]), my_location).miles, axis=1) \nrestaurants_info_my_dataset['distances_to_my_location'] = distances_to_my_location\nrestaurants_info_my_dataset = restaurants_info_my_dataset.set_index('business_id')\nbusiness_id_within_x_miles = list(restaurants_info_my_dataset[np.array(restaurants_info_my_dataset['distances_to_my_location'])<=within_x_miles].index)",
        "matched_tutorial_code_inds": [
            2611,
            2429,
            2012,
            2018,
            1798
        ],
        "matched_tutorial_codes": [
            "from sklearn.gaussian_process import \nfrom sklearn.gaussian_process.kernels import \n\nkernel = 1.0 * (length_scale=1.0, length_scale_bounds=(1e-1, 10.0))\ngpr = (kernel=kernel, random_state=0)\n\nfig, axs = (nrows=2, sharex=True, sharey=True, figsize=(10, 8))\n\n# plot prior\nplot_gpr_samples(gpr, n_samples=n_samples, ax=axs[0])\naxs[0].set_title(\"Samples from prior distribution\")\n\n# plot posterior\ngpr.fit(X_train, y_train)\nplot_gpr_samples(gpr, n_samples=n_samples, ax=axs[1])\naxs[1].scatter(X_train[:, 0], y_train, color=\"red\", zorder=10, label=\"Observations\")\naxs[1].legend(bbox_to_anchor=(1.05, 1.5), loc=\"upper left\")\naxs[1].set_title(\"Samples from posterior distribution\")\n\nfig.suptitle(\"Radial Basis Function kernel\", fontsize=18)\n()\n\n\n<img alt=\"Radial Basis Function kernel, Samples from prior distribution, Samples from posterior distribution\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_prior_posterior_001.png\" srcset=\"../../_images/sphx_glr_plot_gpr_prior_posterior_001.png\"/>",
            "from sklearn.pipeline import \nfrom sklearn.preprocessing import \nfrom sklearn.compose import \nfrom sklearn.ensemble import \nfrom sklearn.model_selection import \n\n\ncategorical_columns = [\n    \"weather\",\n    \"season\",\n    \"holiday\",\n    \"workingday\",\n]\ncategories = [\n    [\"clear\", \"misty\", \"rain\"],\n    [\"spring\", \"summer\", \"fall\", \"winter\"],\n    [\"False\", \"True\"],\n    [\"False\", \"True\"],\n]\nordinal_encoder = (categories=categories)\n\n\ngbrt_pipeline = (\n    (\n        transformers=[\n            (\"categorical\", ordinal_encoder, categorical_columns),\n        ],\n        remainder=\"passthrough\",\n        # Use short feature names to make it easier to specify the categorical\n        # variables in the HistGradientBoostingRegressor in the next\n        # step of the pipeline.\n        verbose_feature_names_out=False,\n    ),\n    (\n        categorical_features=categorical_columns,\n    ),\n).set_output(transform=\"pandas\")",
            "from sklearn.datasets import \nfrom sklearn.cluster import \nfrom sklearn.metrics import , \n\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport numpy as np\n\n# Generating the sample data from make_blobs\n# This particular setting has one distinct cluster and 3 clusters placed close\n# together.\nX, y = (\n    n_samples=500,\n    n_features=2,\n    centers=4,\n    cluster_std=1,\n    center_box=(-10.0, 10.0),\n    shuffle=True,\n    random_state=1,\n)  # For reproducibility\n\nrange_n_clusters = [2, 3, 4, 5, 6]\n\nfor n_clusters in range_n_clusters:\n    # Create a subplot with 1 row and 2 columns\n    fig, (ax1, ax2) = (1, 2)\n    fig.set_size_inches(18, 7)\n\n    # The 1st subplot is the silhouette plot\n    # The silhouette coefficient can range from -1, 1 but in this example all\n    # lie within [-0.1, 1]\n    ax1.set_xlim([-0.1, 1])\n    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n    # plots of individual clusters, to demarcate them clearly.\n    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n\n    # Initialize the clusterer with n_clusters value and a random generator\n    # seed of 10 for reproducibility.\n    clusterer = (n_clusters=n_clusters, n_init=\"auto\", random_state=10)\n    cluster_labels = clusterer.fit_predict(X)\n\n    # The silhouette_score gives the average value for all the samples.\n    # This gives a perspective into the density and separation of the formed\n    # clusters\n    silhouette_avg = (X, cluster_labels)\n    print(\n        \"For n_clusters =\",\n        n_clusters,\n        \"The average silhouette_score is :\",\n        silhouette_avg,\n    )\n\n    # Compute the silhouette scores for each sample\n    sample_silhouette_values = (X, cluster_labels)\n\n    y_lower = 10\n    for i in range(n_clusters):\n        # Aggregate the silhouette scores for samples belonging to\n        # cluster i, and sort them\n        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        color = cm.nipy_spectral(float(i) / n_clusters)\n        ax1.fill_betweenx(\n            (y_lower, y_upper),\n            0,\n            ith_cluster_silhouette_values,\n            facecolor=color,\n            edgecolor=color,\n            alpha=0.7,\n        )\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title(\"The silhouette plot for the various clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\n    # 2nd Plot showing the actual clusters formed\n    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n    ax2.scatter(\n        X[:, 0], X[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n    )\n\n    # Labeling the clusters\n    centers = clusterer.cluster_centers_\n    # Draw white circles at cluster centers\n    ax2.scatter(\n        centers[:, 0],\n        centers[:, 1],\n        marker=\"o\",\n        c=\"white\",\n        alpha=1,\n        s=200,\n        edgecolor=\"k\",\n    )\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\n\n    ax2.set_title(\"The visualization of the clustered data.\")\n    ax2.set_xlabel(\"Feature space for the 1st feature\")\n    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\n    (\n        \"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\"\n        % n_clusters,\n        fontsize=14,\n        fontweight=\"bold\",\n    )\n\n()",
            "from sklearn.cluster import \nimport matplotlib.pyplot as plt\n\nlabels = (graph, n_clusters=4, eigen_solver=\"arpack\")\nlabel_im = (mask.shape, -1.0)\nlabel_im[mask] = labels\n\nfig, axs = (nrows=1, ncols=2, figsize=(10, 5))\naxs[0].matshow(img)\naxs[1].matshow(label_im)\n\n()\n\n\n<img alt=\"plot segmentation toy\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_segmentation_toy_001.png\" srcset=\"../../_images/sphx_glr_plot_segmentation_toy_001.png\"/>",
            "from sklearn.datasets import \nfrom sklearn.cluster import , \nimport matplotlib.pyplot as plt\n\nX, _ = (n_samples=1000, centers=2, random_state=0)\n\nkm = (n_clusters=5, random_state=0, n_init=\"auto\").fit(X)\nbisect_km = (n_clusters=5, random_state=0).fit(X)\n\nfig, ax = (1, 2, figsize=(10, 5))\nax[0].scatter(X[:, 0], X[:, 1], s=10, c=km.labels_)\nax[0].scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], s=20, c=\"r\")\nax[0].set_title(\"KMeans\")\n\nax[1].scatter(X[:, 0], X[:, 1], s=10, c=bisect_km.labels_)\nax[1].scatter(\n    bisect_km.cluster_centers_[:, 0], bisect_km.cluster_centers_[:, 1], s=20, c=\"r\"\n)\n_ = ax[1].set_title(\"BisectingKMeans\")\n\n\n<img alt=\"KMeans, BisectingKMeans\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_release_highlights_1_1_0_003.png\" srcset=\"../../_images/sphx_glr_plot_release_highlights_1_1_0_003.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Illustration of prior and posterior Gaussian process for different kernels->Kernel cookbook->Radial Basis Function kernel"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Gradient Boosting"
            ],
            [
                "sklearn->Examples->Clustering->Selecting the number of clusters with silhouette analysis on KMeans clustering"
            ],
            [
                "sklearn->Examples->Clustering->Spectral clustering for image segmentation->Plotting four circles"
            ],
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 1.1->BisectingKMeans: divide and cluster"
            ]
        ]
    },
    "437875": {
        "jupyter_code_cell": "df.isnull().sum()\ninitial_na=df.isnull().sum()\ndf=df[(df.case_status!='Withdrawn')]\ndf=df.replace({'Certified-Expired':'Certified'})",
        "matched_tutorial_code_inds": [
            6664,
            3827,
            6659,
            5402,
            3796
        ],
        "matched_tutorial_codes": [
            "mod_conc = LocalLevelConcentrated(dta.infl)\nres_conc = mod_conc.fit(disp=False)\nprint(res_conc.summary())",
            "daily = df.fl_date.value_counts().sort_index()\ny = daily.resample('MS').mean()\ny.head()",
            "mod = LocalLevel(dta.infl)\nres = mod.fit(disp=False)\nprint(res.summary())",
            "mod_nbin = sm.NegativeBinomial(rand_data.endog, rand_exog)\nres_nbin = mod_nbin.fit(disp=False)\nprint(res_nbin.summary())",
            "def core(df, \u03b1=.05):\n    mask = (df &gt; df.quantile(\u03b1)).all(1) &amp; (df &lt; df.quantile(1 - \u03b1)).all(1)\n    return df[mask]"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Concentrating out the scale"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Typical approach"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Negative Binomial"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ]
        ]
    },
    "204993": {
        "jupyter_code_cell": "from time import sleep\nimport fitbit\nimport cherrypy\nimport requests\nimport json\nimport datetime\nimport scipy.stats\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nfrom matplotlib import pyplot as plt\nimport seaborn as sns",
        "matched_tutorial_code_inds": [
            3080,
            4765,
            3993,
            3126,
            1146
        ],
        "matched_tutorial_codes": [
            "import sys\nfrom time import \nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.random_projection import \nfrom sklearn.random_projection import \nfrom sklearn.datasets import \nfrom sklearn.datasets import \nfrom sklearn.metrics.pairwise import euclidean_distances",
            "from cycler import cycler\nimport numpy as np\nimport matplotlib.pyplot as plt",
            "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n",
            "from time import \n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.svm import \nfrom sklearn import datasets\nfrom sklearn.model_selection import \nfrom sklearn.experimental import enable_halving_search_cv  # noqa\nfrom sklearn.model_selection import",
            "from ax.service.utils.report_utils import exp_to_df\n\ndf = exp_to_df(experiment)\ndf.head(10)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Miscellaneous->The Johnson-Lindenstrauss bound for embedding with random projections"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Styling with cycler"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships",
                "seaborn->Plotting functions->Visualizing statistical relationships"
            ],
            [
                "sklearn->Examples->Model Selection->Comparison between grid search and successive halving"
            ],
            [
                "torch->Model Optimization->Multi-Objective NAS with Ax->Evaluating the results"
            ]
        ]
    },
    "184313": {
        "jupyter_code_cell": "print(\"Info of train_df:-\")\ntrain_df.info()\nprint(\"NO. of values in different varibles for train_df\")\nprint(train_df.count())\nprint(\"NO. of values in different varibles for test_df\")\nprint(test_df.count())",
        "matched_tutorial_code_inds": [
            1331,
            250,
            5820,
            5233,
            5470
        ],
        "matched_tutorial_codes": [
            "test_loss = evaluate(, )\nprint('=' * 89)\nprint('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n    test_loss, math.exp(test_loss)))\nprint('=' * 89)",
            "print(f\"Model structure: {model}\\n\\n\")\n\nfor name,  in ():\n    print(f\"Layer: {name} | Size: {.size()} | Values : {[:2]} \\n\")",
            "fdr2 = ols_model.outlier_test(\"fdr_bh\")\nfdr2.sort_values(\"unadj_p\", inplace=True)\nprint(fdr2)",
            "print(res.recursive_coefficients.filtered[0])\nres.plot_recursive_coefficient(range(mod.k_exog), alpha=None, figsize=(10, 6))",
            "means75 = exog.mean()\nmeans75[\"LOWINC\"] = exog[\"LOWINC\"].quantile(0.75)\nprint(means75)"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Parallel and Distributed Training->Training Transformer models using Pipeline Parallelism->Evaluate the model with the test dataset"
            ],
            [
                "torch->Introduction to PyTorch->Build the Neural Network->Model Parameters"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Hertzprung Russell data for Star Cluster CYG 0B1 - Leverage Points"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 1: Copper"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution"
            ]
        ]
    },
    "995106": {
        "jupyter_code_cell": "set(counties_df.columns) == set(df.columns)\ncounties_df.head()",
        "matched_tutorial_code_inds": [
            657,
            647,
            660,
            3706,
            2801
        ],
        "matched_tutorial_codes": [
            "traced_rn18 = (rn18)\nprint()",
            "traced_model = (model)\nprint(traced_model.graph)",
            "interp = (rn18)\n(input)\nprint(interp.summary(True))",
            "delays.nsmallest(5).sort_values()",
            "X = survey.data[survey.feature_names]\nX.describe(include=\"all\")\n\n\n\n\n\n\n\n\n<br/>\n<br/>"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Code Transforms with FX->(beta) Building a Simple CPU Performance Profiler with FX->Capturing the Model with Symbolic Tracing"
            ],
            [
                "torch->Code Transforms with FX->(beta) Building a Convolution/Batch Norm fuser in FX->Fusing Convolution with Batch Norm"
            ],
            [
                "torch->Code Transforms with FX->(beta) Building a Simple CPU Performance Profiler with FX->Investigating the Performance of ResNet18"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ]
        ]
    },
    "1129237": {
        "jupyter_code_cell": "orders_df.days_since_prior_order.fillna(orders_df.days_since_prior_order.mean(), inplace=True)\norders_df.head()\nprint(departments_df[departments_df.department == 'missing'])\nmissing_prod_df = products_df[products_df.department_id == 21].reset_index()\nprint (len(missing_prod_df), \"products items are associated with department='missing'\")\nprint(aisles_df[aisles_df.aisle == 'missing'])\nmissing_prod_df.head()",
        "matched_tutorial_code_inds": [
            2753,
            3773,
            6090,
            4837,
            2997
        ],
        "matched_tutorial_codes": [
            "quantiles = [0.05, 0.5, 0.95]\npredictions = {}\nout_bounds_predictions = (y_true_mean, dtype=)\nfor quantile in quantiles:\n    qr = (quantile=quantile, alpha=0, solver=solver)\n    y_pred = qr.fit(X, y_pareto).predict(X)\n    predictions[quantile] = y_pred\n\n    if quantile == min(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred = y_pareto\n        )\n    elif quantile == max(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred &lt;= y_pareto\n        )",
            "df = df.assign(away_strength=df['away_team'].map(win_percent),\n               home_strength=df['home_team'].map(win_percent),\n               point_diff=df['home_points'] - df['away_points'],\n               rest_diff=df['home_rest'] - df['away_rest'])\ndf.head()",
            "res_ar5 = AutoReg(ind_prod, 5, old_names=False).fit()\npredictions = pd.DataFrame(\n    {\n        \"AR(5)\": res_ar5.predict(start=714, end=726),\n        \"AR(13)\": res.predict(start=714, end=726),\n        \"Restr. AR(13)\": res_glob.predict(start=714, end=726),\n    }\n)\n_, ax = plt.subplots()\nax = predictions.plot(ax=ax)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_42_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_42_0.png\"/>",
            "inner = [['innerA'],\n         ['innerB']]\nouter = [['upper left',  inner],\n          ['lower left', 'lower right']]\n\nfig, axd = plt.subplot_mosaic(outer, layout=\"constrained\")\nfor k in axd:\n    annotate_axes(axd[k], f'axd[\"{k}\"]')\n\n\n<img alt=\"arranging axes\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_arranging_axes_008.png\" srcset=\"../../_images/sphx_glr_arranging_axes_008.png, ../../_images/sphx_glr_arranging_axes_008_2_0x.png 2.0x\"/>",
            "tree_disp = (tree, X, [\"age\"])\nmlp_disp = (\n    mlp, X, [\"age\"], ax=tree_disp.axes_, line_kw={\"color\": \"red\"}\n)\n\n\n<img alt=\"plot partial dependence visualization api\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_partial_dependence_visualization_api_006.png\" srcset=\"../../_images/sphx_glr_plot_partial_dependence_visualization_api_006.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Generalized Linear Models->Quantile regression->Fitting a QuantileRegressor"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions->Industrial Production"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Arranging multiple Axes in a Figure->High-level methods for making grids->Nested Axes layouts"
            ],
            [
                "sklearn->Examples->Miscellaneous->Advanced Plotting With Partial Dependence->Plotting partial dependence for one feature"
            ]
        ]
    },
    "510941": {
        "jupyter_code_cell": "def StrIsInt(string):\n    try: \n        int(string)\n        return True\n    except ValueError:\n        return False\ndef orderListOfChr(unordered_chr_list):\n    chr_id = [None]* len(unordered_chr_list)\n    for value in unordered_chr_list:\n        x = value.split(\"chr\")[-1]\n        if StrIsInt(x):\n            chr_id[int(x)-1] = value\n        else:\n            chr_id.append(value)\n    ordered_chr_list = [x for x in chr_id if x is not None]\n    return ordered_chr_list\ndef reorderDFbyChrOrder(df):\n    list_of_reordered_chr = orderListOfChr(df.index)\n    return df.reindex(list_of_reordered_chr)\ndef getChrNames(df):\n    to_keep = []\n    for item in df.columns:\n        if 'chr' in item:\n            to_keep.append(item)\n    return to_keep  \nidxstats_df = getTableFromDB('select * from idxstats_reads_per_chromosome;',database_path)\nidxstats_df.index = idxstats_df['region']\nreads_per_chr_df = reorderDFbyChrOrder(idxstats_df.drop('region', 1))\nprint ('this table shows million reads per chromosome')\nreads_per_chr_df.divide(1000000)",
        "matched_tutorial_code_inds": [
            507,
            353,
            347,
            681,
            35
        ],
        "matched_tutorial_codes": [
            "def generate_square_subsequent_mask(sz):\n    mask = ((((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n    return mask\n\n\ndef create_mask(src, tgt):\n    src_seq_len = src.shape[0]\n    tgt_seq_len = tgt.shape[0]\n\n    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n    src_mask = ((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n\n    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask",
            "def preprocess(x, y):\n    return x.view(-1, 1, 28, 28).to(), y.to()\n\n\ntrain_dl, valid_dl = get_data(, , bs)\ntrain_dl = WrappedDataLoader(train_dl, preprocess)\nvalid_dl = WrappedDataLoader(valid_dl, preprocess)",
            "def preprocess(x, y):\n    return x.view(-1, 1, 28, 28), y\n\n\nclass WrappedDataLoader:\n    def __init__(self, dl, func):\n        self.dl = dl\n        self.func = func\n\n    def __len__(self):\n        return len(self.dl)\n\n    def __iter__(self):\n        batches = iter(self.dl)\n        for b in batches:\n            yield (self.func(*b))\n\ntrain_dl, valid_dl = get_data(, , bs)\ntrain_dl = WrappedDataLoader(train_dl, preprocess)\nvalid_dl = WrappedDataLoader(valid_dl, preprocess)",
            "def contains_cl(args):\n    for t in args:\n        if isinstance(t, ):\n            if t.is_contiguous(memory_format=) and not t.is_contiguous():\n                return True\n        elif isinstance(t, list) or isinstance(t, tuple):\n            if contains_cl(list(t)):\n                return True\n    return False\n\n\ndef print_inputs(args, indent=\"\"):\n    for t in args:\n        if isinstance(t, ):\n            print(indent, t.stride(), t.shape, t.device, t.dtype)\n        elif isinstance(t, list) or isinstance(t, tuple):\n            print(indent, type(t))\n            print_inputs(list(t), indent=indent + \"    \")\n        else:\n            print(indent, t)\n\n\ndef check_wrapper(fn):\n    name = fn.__name__\n\n    def check_cl(*args, **kwargs):\n        was_cl = contains_cl(args)\n        try:\n            result = fn(*args, **kwargs)\n        except Exception as e:\n            print(\"`{}` inputs are:\".format(name))\n            print_inputs(args)\n            print(\"-------------------\")\n            raise e\n        failed = False\n        if was_cl:\n            if isinstance(result, ):\n                if result.dim() == 4 and not result.is_contiguous(memory_format=):\n                    print(\n                        \"`{}` got channels_last input, but output is not channels_last:\".format(name),\n                        result.shape,\n                        result.stride(),\n                        result.device,\n                        result.dtype,\n                    )\n                    failed = True\n        if failed and True:\n            print(\"`{}` inputs are:\".format(name))\n            print_inputs(args)\n            raise Exception(\"Operator `{}` lost channels_last property\".format(name))\n        return result\n\n    return check_cl\n\n\nold_attrs = dict()\n\n\ndef attribute(m):\n    old_attrs[m] = dict()\n    for i in dir(m):\n        e = getattr(m, i)\n        exclude_functions = [\"is_cuda\", \"has_names\", \"numel\", \"stride\", \"Tensor\", \"is_contiguous\", \"__class__\"]\n        if i not in exclude_functions and not i.startswith(\"_\") and \"__call__\" in dir(e):\n            try:\n                old_attrs[m][i] = e\n                setattr(m, i, check_wrapper(e))\n            except Exception as e:\n                print(i)\n                print(e)\n\n\nattribute()\nattribute(torch.nn.functional)\nattribute(torch)",
            "def print_size_of_model(model, label=\"\"):\n    (model.state_dict(), \"temp.p\")\n    size=os.path.getsize(\"temp.p\")\n    print(\"model: \",label,' \\t','Size (KB):', size/1e3)\n    os.remove('temp.p')\n    return size\n\n# compare the sizes\nf=print_size_of_model(float_lstm,\"fp32\")\nq=print_size_of_model(quantized_lstm,\"int8\")\nprint(\"{0:.2f} times smaller\".format(f/q))"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Text->Language Translation with nn.Transformer and torchtext->Seq2Seq Network using Transformer"
            ],
            [
                "torch->Learning PyTorch->What is torch.nn really?->Using your GPU"
            ],
            [
                "torch->Learning PyTorch->What is torch.nn really?->Wrapping DataLoader"
            ],
            [
                "torch->Frontend APIs->(beta) Channels Last Memory Format in PyTorch->Converting existing models"
            ],
            [
                "torch->PyTorch Recipes->Dynamic Quantization->Steps->3. Look at Model Size"
            ]
        ]
    },
    "1021331": {
        "jupyter_code_cell": "df.drop(['ID', 'TargetD'], axis=1, inplace=True)\ndf['DemGender'].head(5)",
        "matched_tutorial_code_inds": [
            3656,
            2801,
            3584,
            3610,
            5468
        ],
        "matched_tutorial_codes": [
            "weather2 = weather.reset_index()\nweather2[weather2['station'] == 'DSM'].head()",
            "X = survey.data[survey.feature_names]\nX.describe(include=\"all\")\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "tfidf_transformer = TfidfTransformer()\n X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n X_train_tfidf.shape\n(2257, 35788)",
            "first = df.groupby('unique_carrier').first()\nfirst.ix[10:15, ['fl_date', 'tail_num']]",
            "means25[\"LOWINC\"] = exog[\"LOWINC\"].quantile(0.25)\nprint(means25)"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Indexes->Flavors->Row Slicing"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "sklearn->Tutorials->Working With Text Data->Extracting features from text files->From occurrences to frequencies"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution"
            ]
        ]
    },
    "68031": {
        "jupyter_code_cell": "df_counts\ndef get_count_df( df ):\n    lead_digit_counts = { str(n): 0 for n in range(1,10)}\n    for t in df.columns:\n        prices = df[t].dropna()\n        d = count_lead_digits( prices )\n        for digit, count in d.items():\n            lead_digit_counts[digit] += count\n    df_counts = pd.DataFrame.from_dict(lead_digit_counts, orient='index')\n    df_counts.columns = ['frequency']\n    df_counts.sort_index(inplace=True)\n    return df_counts",
        "matched_tutorial_code_inds": [
            6492,
            6493,
            6494,
            149,
            3078
        ],
        "matched_tutorial_codes": [
            "time_s = np.s_[:50]  # After this they basically agree\nfig1 = plt.figure()\nax1 = fig1.add_subplot(111)\nidx = np.asarray(series.index)\nh1, = ax1.plot(idx[time_s], res_f.freq_seasonal[0].filtered[time_s], label='Double Freq. Seas')\nh2, = ax1.plot(idx[time_s], res_tf.seasonal.filtered[time_s], label='Mixed Domain Seas')\nh3, = ax1.plot(idx[time_s], true_seasonal_10_3[time_s], label='True Seasonal 10(3)')\nplt.legend([h1, h2, h3], ['Double Freq. Seasonal','Mixed Domain Seasonal','Truth'], loc=2)\nplt.title('Seasonal 10(3) component')\nplt.show()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_seasonal_21_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_seasonal_21_0.png\"/>",
            "time_s = np.s_[:50]  # After this they basically agree\nfig2 = plt.figure()\nax2 = fig2.add_subplot(111)\nh21, = ax2.plot(idx[time_s], res_f.freq_seasonal[1].filtered[time_s], label='Double Freq. Seas')\nh22, = ax2.plot(idx[time_s], res_tf.freq_seasonal[0].filtered[time_s], label='Mixed Domain Seas')\nh23, = ax2.plot(idx[time_s], true_seasonal_100_2[time_s], label='True Seasonal 100(2)')\nplt.legend([h21, h22, h23], ['Double Freq. Seasonal','Mixed Domain Seasonal','Truth'], loc=2)\nplt.title('Seasonal 100(2) component')\nplt.show()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_seasonal_22_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_seasonal_22_0.png\"/>",
            "time_s = np.s_[:100]\n\nfig3 = plt.figure()\nax3 = fig3.add_subplot(111)\nh31, = ax3.plot(idx[time_s], res_f.freq_seasonal[1].filtered[time_s] + res_f.freq_seasonal[0].filtered[time_s], label='Double Freq. Seas')\nh32, = ax3.plot(idx[time_s], res_tf.freq_seasonal[0].filtered[time_s] + res_tf.seasonal.filtered[time_s], label='Mixed Domain Seas')\nh33, = ax3.plot(idx[time_s], true_sum[time_s], label='True Seasonal 100(2)')\nh34, = ax3.plot(idx[time_s], res_lf.freq_seasonal[0].filtered[time_s], label='Lazy Freq. Seas')\nh35, = ax3.plot(idx[time_s], res_lt.seasonal.filtered[time_s], label='Lazy Time Seas')\n\nplt.legend([h31, h32, h33, h34, h35], ['Double Freq. Seasonal','Mixed Domain Seasonal','Truth', 'Lazy Freq. Seas', 'Lazy Time Seas'], loc=1)\nplt.title('Seasonal components combined')\nplt.tight_layout(pad=1.0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_seasonal_23_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_seasonal_23_0.png\"/>",
            "num_threads = ()\nprint(f'Benchmarking on {num_threads} threads')\n\nt0 = (\n    stmt='batched_dot_mul_sum(x, x)',\n    setup='from __main__ import batched_dot_mul_sum',\n    globals={'x': x},\n    num_threads=num_threads,\n    label='Multithreaded batch dot',\n    sub_label='Implemented using mul and sum')\n\nt1 = (\n    stmt='batched_dot_bmm(x, x)',\n    setup='from __main__ import batched_dot_bmm',\n    globals={'x': x},\n    num_threads=num_threads,\n    label='Multithreaded batch dot',\n    sub_label='Implemented using bmm')\n\nprint(t0.timeit(100))\nprint(t1.timeit(100))\n\n\n\nOutput",
            "svc_disp = (svc, X_test, y_test)\n()\n\n\n<img alt=\"plot roc curve visualization api\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_roc_curve_visualization_api_001.png\" srcset=\"../../_images/sphx_glr_plot_roc_curve_visualization_api_001.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->Seasonality in Time Series Data->Comparison of filtered estimates"
            ],
            [
                "statsmodels->Examples->State space models->Seasonality in Time Series Data->Comparison of filtered estimates"
            ],
            [
                "statsmodels->Examples->State space models->Seasonality in Time Series Data->Comparison of filtered estimates"
            ],
            [
                "torch->PyTorch Recipes->PyTorch Benchmark->Steps->3. Benchmarking with torch.utils.benchmark.Timer"
            ],
            [
                "sklearn->Examples->Miscellaneous->ROC Curve with Visualization API->Plotting the ROC Curve"
            ]
        ]
    },
    "1293630": {
        "jupyter_code_cell": "df_new[['CA','UK','US']] = pd.get_dummies(df_new['country'])\ndf_new.drop('CA', axis=1, inplace=True)\ndf_new[['new_page', 'old_page']] = pd.get_dummies(df_new['landing_page'])\ndf_new.head()\npredictorVar = ['intercept', 'new_page', 'US', 'UK']\nlogReg2 = sm.Logit(df_new['converted'], df_new[predictorVar])\nanswer2 = logReg2.fit()\nanswer2.summary2()\nodds = np.exp(answer2.params[1:])\nodds",
        "matched_tutorial_code_inds": [
            3773,
            6444,
            6527,
            5976,
            5969
        ],
        "matched_tutorial_codes": [
            "df = df.assign(away_strength=df['away_team'].map(win_percent),\n               home_strength=df['home_team'].map(win_percent),\n               point_diff=df['home_points'] - df['away_points'],\n               rest_diff=df['home_rest'] - df['away_rest'])\ndf.head()",
            "mod_uc = sm.tsa.UnobservedComponents(\n    endog, 'rwalk',\n    cycle=True, stochastic_cycle=True, damped_cycle=True,\n)\n# Here the powell method gets close to the optimum\nres_uc = mod_uc.fit(method='powell', disp=False)\n# but to get to the highest loglikelihood we do a\n# second round using the L-BFGS method.\nres_uc = mod_uc.fit(res_uc.params, disp=False)\nprint(res_uc.summary())",
            "initial_obs_cov = np.cov(y.T)\ninitial_state_cov_diag = [0.01] * mod.k_states\n\n# Update H and Q\nmod.update_variances(initial_obs_cov, initial_state_cov_diag)\n\n# Perform Kalman filtering and smoothing\n# (the [] is just an empty list that in some models might contain\n# additional parameters. Here, we don't have any additional parameters\n# so we just pass an empty list)\ninitial_res = mod.smooth([])",
            "res3 = combine_effects(eff, var_eff, method_re=\"chi2\", use_t=False, row_names=rownames)\n# TODO: we still need better information about conf_int of individual samples\n# We don't have enough information in the model for individual confidence intervals\n# if those are not based on normal distribution.\nres3.conf_int_samples(nobs=np.array(nobs1 + nobs2))\nprint(res3.summary_frame())",
            "res3 = combine_effects(eff, var_eff, method_re=\"chi2\", use_t=True, row_names=rownames)\n# TODO: we still need better information about conf_int of individual samples\n# We don't have enough information in the model for individual confidence intervals\n# if those are not based on normal distribution.\nres3.conf_int_samples(nobs=np.array(nobs1 + nobs2))\nprint(res3.summary_frame())"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team"
            ],
            [
                "statsmodels->Examples->State space models->Trends and cycles in unemployment->Unobserved components with stochastic cycle (UC)"
            ],
            [
                "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Application: Bayesian analysis of a TVP-VAR model by MCMC->Preliminary investigation with ad-hoc parameters in H, Q"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Example->Using one-step chi2, DerSimonian-Laird estimate for random effects variance tau"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Example->Using one-step chi2, DerSimonian-Laird estimate for random effects variance tau"
            ]
        ]
    },
    "583059": {
        "jupyter_code_cell": "lngth = 40; base_freq = 220; rate = 44100\nts = np.arange(0, lngth, step=lngth/2/rate)\nsweep = np.sqrt((lngth-ts)) * (np.sin(base_freq*ts**2) + np.sin(base_freq*np.power(2, 5/12)*ts**2))\ndisplay.Audio(sweep, rate=rate/5)\nplt.figure(figsize = (15,5))\nplt.hist(muons.M, bins = 500, range=(0,130))\nplt.xlabel(\"Invariant mass (GeV)\", fontsize = 15)\nplt.ylabel(\"Number of events \\n\", fontsize = 15)\nplt.show()",
        "matched_tutorial_code_inds": [
            4926,
            5917,
            2601,
            4868,
            6362
        ],
        "matched_tutorial_codes": [
            "delta = 0.1\nx = np.arange(-3.0, 4.001, delta)\ny = np.arange(-4.0, 3.001, delta)\nX, Y = np.meshgrid(x, y)\nZ1 = np.exp(-X**2 - Y**2)\nZ2 = np.exp(-(X - 1)**2 - (Y - 1)**2)\nZ = (0.9*Z1 - 0.5*Z2) * 2\n\n# select a divergent colormap\ncmap = cm.coolwarm\n\nfig, (ax1, ax2) = plt.subplots(ncols=2)\npc = ax1.pcolormesh(Z, cmap=cmap)\nfig.colorbar(pc, ax=ax1)\nax1.set_title('Normalize()')\n\npc = ax2.pcolormesh(Z, norm=colors.CenteredNorm(), cmap=cmap)\nfig.colorbar(pc, ax=ax2)\nax2.set_title('CenteredNorm()')\n\nplt.show()\n\n\n<img alt=\"Normalize(), CenteredNorm()\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_colormapnorms_002.png\" srcset=\"../../_images/sphx_glr_colormapnorms_002.png, ../../_images/sphx_glr_colormapnorms_002_2_0x.png 2.0x\"/>",
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "vmin, vmax = (-log_marginal_likelihood).min(), 50\nlevel = (((vmin), (vmax), num=50), decimals=1)\n(\n    length_scale_grid,\n    noise_level_grid,\n    -log_marginal_likelihood,\n    levels=level,\n    norm=(vmin=vmin, vmax=vmax),\n)\n()\n(\"log\")\n(\"log\")\n(\"Length-scale\")\n(\"Noise-level\")\n(\"Log-marginal-likelihood\")\n()\n\n\n<img alt=\"Log-marginal-likelihood\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_noisy_005.png\" srcset=\"../../_images/sphx_glr_plot_gpr_noisy_005.png\"/>",
            "nverts = nrects*(1+3+1)\nverts = np.zeros((nverts, 2))\ncodes = np.ones(nverts, int) * path.Path.LINETO\ncodes[0::5] = path.Path.MOVETO\ncodes[4::5] = path.Path.CLOSEPOLY\nverts[0::5, 0] = left\nverts[0::5, 1] = bottom\nverts[1::5, 0] = left\nverts[1::5, 1] = top\nverts[2::5, 0] = right\nverts[2::5, 1] = top\nverts[3::5, 0] = right\nverts[3::5, 1] = bottom",
            "rho = 0.8\nbeta = 10\nepsilon = eta.copy()\nfor i in range(1, eta.shape[0]):\n    epsilon[i] = rho * eta[i - 1] + eta[i]\ny = beta + epsilon\ny = y[200:]\n\nma_res = ARIMA(y, order=(0, 0, 1), trend=\"c\").fit()\nprint_params(ma_res.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Colors->Colormap Normalization->Centered"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) with noise-level estimation->Optimisation of kernel hyperparameters in GPR"
            ],
            [
                "matplotlib->Tutorials->Advanced->Path Tutorial->Compound paths"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Reconstructing residuals, fitted values and forecasts in SARIMAX and ARIMA->Prediction with MA components"
            ]
        ]
    },
    "598798": {
        "jupyter_code_cell": "plt.figure(6)\nplt.plot(tspan, rO*cX, 'b')\nplt.plot(tspan, rC*cX, 'r')\nplt.show()\nYOATP = rATP/rO",
        "matched_tutorial_code_inds": [
            5715,
            5720,
            4531,
            3702,
            5470
        ],
        "matched_tutorial_codes": [
            "plt.clf()\nplt.grid(True)\nplt.plot(result1.predict(linear=True), result1.resid_pearson, \"o\")\nplt.xlabel(\"Linear predictor\")\nplt.ylabel(\"Residual\")",
            "plt.clf()\nplt.grid(True)\nplt.plot(result2.predict(linear=True), result2.resid_pearson, \"o\")\nplt.xlabel(\"Linear predictor\")\nplt.ylabel(\"Residual\")",
            "ax.imshow(np.rot90(Z), cmap=plt.cm.gist_earth_r,\n...           extent=[xmin, xmax, ymin, ymax])\n ax.plot(m1, m2, 'k.', markersize=2)",
            "plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "means75 = exog.mean()\nmeans75[\"LOWINC\"] = exog[\"LOWINC\"].quantile(0.75)\nprint(means75)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Generalized Linear Models->Quasi-binomial regression"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Quasi-binomial regression"
            ],
            [
                "scipy->Statistics (scipy.stats)->Kernel density estimation->Multivariate estimation"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution"
            ]
        ]
    },
    "1432865": {
        "jupyter_code_cell": "filename = 'data/temperatures/annual.land_ocean.90S.90N.df_1901-2000mean.dat'\nfull_globe_temp = pd.read_table(filename, sep=r'\\s+', names=['year', 'mean temp'],\n                                index_col=0, parse_dates=True)\nfull_globe_temp\ngiss_temp = pd.read_table('data/temperatures/GLB.Ts+dSST.txt', sep='\\s+',\n                          skiprows=7, skip_footer=11, engine='python')\ngiss_temp",
        "matched_tutorial_code_inds": [
            6891,
            3746,
            1745,
            1092,
            3747
        ],
        "matched_tutorial_codes": [
            "url = 'https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/csv/COUNT/medpar.csv'\nmedpar = read.csv(url)\nf = los~factor(type)+hmo+white\n\nlibrary(MASS)\nmod = glm.nb(f, medpar)\ncoef(summary(mod))\n                 Estimate Std. Error   z value      Pr(|z|)\n(Intercept)    2.31027893 0.06744676 34.253370 3.885556e-257\nfactor(type)2  0.22124898 0.05045746  4.384861  1.160597e-05\nfactor(type)3  0.70615882 0.07599849  9.291748  1.517751e-20\nhmo           -0.06795522 0.05321375 -1.277024  2.015939e-01\nwhite         -0.12906544 0.06836272 -1.887951  5.903257e-02",
            "fp = 'data/nba.csv'\n\nif not os.path.exists(fp):\n    tables = pd.read_html(\"http://www.basketball-reference.com/leagues/NBA_2016_games.html\")\n    games = tables[0]\n    games.to_csv(fp)\nelse:\n    games = pd.read_csv(fp)\ngames.head()",
            "speech_data_path = 'tutorial-nlp-from-scratch/speeches.csv'\nspeech_df = pd.read_csv(speech_data_path)\nX_pred = textproc.cleantext(speech_df,\n                            text_column='speech',\n                            remove_stopwords=True,\n                            remove_punc=False)\nspeakers = speech_df['speaker'].to_numpy()",
            "data_path = '~/.data/imagenet'\nsaved_model_dir = 'data/'\nfloat_model_file = 'mobilenet_pretrained_float.pth'\nscripted_float_model_file = 'mobilenet_quantization_scripted.pth'\nscripted_quantized_model_file = 'mobilenet_quantization_scripted_quantized.pth'\n\ntrain_batch_size = 30\neval_batch_size = 50\n\ndata_loader, data_loader_test = prepare_data_loaders(data_path)\ncriterion = nn.CrossEntropyLoss()\nfloat_model = load_model(saved_model_dir + float_model_file).to('cpu')\n\n# Next, we'll \"fuse modules\"; this can both make the model faster by saving on memory access\n# while also improving numerical accuracy. While this can be used with any model, this is\n# especially common with quantized models.\n\nprint('\\n Inverted Residual Block: Before fusion \\n\\n', float_model.features[1].conv)\nfloat_model.eval()\n\n# Fuses modules\nfloat_model.fuse_model()\n\n# Note fusion of Conv+BN+Relu and Conv+Relu\nprint('\\n Inverted Residual Block: After fusion\\n\\n',float_model.features[1].conv)",
            "column_names = {'Date': 'date', 'Start (ET)': 'start',\n                'Unamed: 2': 'box', 'Visitor/Neutral': 'away_team', \n                'PTS': 'away_points', 'Home/Neutral': 'home_team',\n                'PTS.1': 'home_points', 'Unamed: 7': 'n_ot'}\n\ngames = (games.rename(columns=column_names)\n    .dropna(thresh=4)\n    [['date', 'away_team', 'away_points', 'home_team', 'home_points']]\n    .assign(date=lambda x: pd.to_datetime(x['date'], format='%a, %b %d, %Y'))\n    .set_index('date', append=True)\n    .rename_axis([\"game_id\", \"date\"])\n    .sort_index())\ngames.head()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 2: Negative Binomial Regression for Count Data->Testing"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ],
            [
                "torch->Model Optimization->(beta) Static Quantization with Eager Mode in PyTorch->3. Define dataset and data loaders->ImageNet Data"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data"
            ]
        ]
    },
    "1045719": {
        "jupyter_code_cell": "Xtest, ytest = dftest.text[dftest.polarity!=2], dftest.polarity[dftest.polarity!=2]\nprint(classification_report(ytest,sentiment_lr.predict(Xtest)))\nimport dill\nf = open('twitter_sentiment_model.pkl','wb')\nr = RegexPreprocess()\ndill.dump(sentiment_lr, f)\nf.close()",
        "matched_tutorial_code_inds": [
            6795,
            2826,
            2330,
            3257,
            3453
        ],
        "matched_tutorial_codes": [
            "x1n = np.linspace(20.5, 25, 10)\nXnew = np.column_stack((x1n, np.sin(x1n), (x1n - 5) ** 2))\nXnew = sm.add_constant(Xnew)\nynewpred = olsres.predict(Xnew)  # predict out of sample\nprint(ynewpred)",
            "X_train_preprocessed = (\n    model[:-1].transform(X_train), columns=feature_names\n)\n\nX_train_preprocessed.std(axis=0).plot.barh(figsize=(9, 7))\n(\"Feature ranges\")\n(\"Std. dev. of feature values\")\n(left=0.3)\n\n\n<img alt=\"Feature ranges\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_004.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_004.png\"/>",
            "X, y = (return_X_y=True)\n\n# Train classifiers\nreg1 = (random_state=1)\nreg2 = (random_state=1)\nreg3 = ()\n\nreg1.fit(X, y)\nreg2.fit(X, y)\nreg3.fit(X, y)\n\nereg = ([(\"gb\", reg1), (\"rf\", reg2), (\"lr\", reg3)])\nereg.fit(X, y)",
            "alphas = (-5, 1, 60)\nenet = (l1_ratio=0.7, max_iter=10000)\ntrain_errors = list()\ntest_errors = list()\nfor alpha in alphas:\n    enet.set_params(alpha=alpha)\n    enet.fit(X_train, y_train)\n    train_errors.append(enet.score(X_train, y_train))\n    test_errors.append(enet.score(X_test, y_test))\n\ni_alpha_optim = (test_errors)\nalpha_optim = alphas[i_alpha_optim]\nprint(\"Optimal regularization parameter : %s\" % alpha_optim)\n\n# Estimate the coef_ on full data with optimal regularization parameter\nenet.set_params(alpha=alpha_optim)\ncoef_ = enet.fit(X, y).coef_",
            "C_2d_range = [1e-2, 1, 1e2]\ngamma_2d_range = [1e-1, 1, 1e1]\nclassifiers = []\nfor C in C_2d_range:\n    for gamma in gamma_2d_range:\n        clf = (C=C, gamma=gamma)\n        clf.fit(X_2d, y_2d)\n        classifiers.append((C, gamma, clf))"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->User Notes->Prediction->Create a new sample of explanatory variables Xnew, predict and plot"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "sklearn->Examples->Ensemble methods->Plot individual and voting regression predictions->Training classifiers"
            ],
            [
                "sklearn->Examples->Model Selection->Train error vs Test error->Compute train and test errors"
            ],
            [
                "sklearn->Examples->Support Vector Machines->RBF SVM parameters->Train classifiers"
            ]
        ]
    },
    "1247369": {
        "jupyter_code_cell": "show_corr_heatmap(names=new_genres_headers)\ndata['genre_game-show'] = data[['genre_game-show', 'genre_reality-tv']].max(axis=1)\ndata['genre_music'] = data[['genre_musical', 'genre_music']].max(axis=1)\ndata['genre_thriller'] = 1-data[['genre_comedy']]",
        "matched_tutorial_code_inds": [
            4938,
            4941,
            4939,
            4942,
            4937
        ],
        "matched_tutorial_codes": [
            "plot_color_gradients('Sequential (2)',\n                     ['binary', 'gist_yarg', 'gist_gray', 'gray', 'bone',\n                      'pink', 'spring', 'summer', 'autumn', 'winter', 'cool',\n                      'Wistia', 'hot', 'afmhot', 'gist_heat', 'copper'])\n\n\n<img alt=\"Sequential (2) colormaps\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_colormaps_003.png\" srcset=\"../../_images/sphx_glr_colormaps_003.png, ../../_images/sphx_glr_colormaps_003_2_0x.png 2.0x\"/>",
            "plot_color_gradients('Qualitative',\n                     ['Pastel1', 'Pastel2', 'Paired', 'Accent', 'Dark2',\n                      'Set1', 'Set2', 'Set3', 'tab10', 'tab20', 'tab20b',\n                      'tab20c'])\n\n\n<img alt=\"Qualitative colormaps\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_colormaps_006.png\" srcset=\"../../_images/sphx_glr_colormaps_006.png, ../../_images/sphx_glr_colormaps_006_2_0x.png 2.0x\"/>",
            "plot_color_gradients('Diverging',\n                     ['PiYG', 'PRGn', 'BrBG', 'PuOr', 'RdGy', 'RdBu', 'RdYlBu',\n                      'RdYlGn', 'Spectral', 'coolwarm', 'bwr', 'seismic'])\n\n\n<img alt=\"Diverging colormaps\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_colormaps_004.png\" srcset=\"../../_images/sphx_glr_colormaps_004.png, ../../_images/sphx_glr_colormaps_004_2_0x.png 2.0x\"/>",
            "plot_color_gradients('Miscellaneous',\n                     ['flag', 'prism', 'ocean', 'gist_earth', 'terrain',\n                      'gist_stern', 'gnuplot', 'gnuplot2', 'CMRmap',\n                      'cubehelix', 'brg', 'gist_rainbow', 'rainbow', 'jet',\n                      'turbo', 'nipy_spectral', 'gist_ncar'])\n\nplt.show()\n\n\n<img alt=\"Miscellaneous colormaps\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_colormaps_007.png\" srcset=\"../../_images/sphx_glr_colormaps_007.png, ../../_images/sphx_glr_colormaps_007_2_0x.png 2.0x\"/>",
            "plot_color_gradients('Sequential',\n                     ['Greys', 'Purples', 'Blues', 'Greens', 'Oranges', 'Reds',\n                      'YlOrBr', 'YlOrRd', 'OrRd', 'PuRd', 'RdPu', 'BuPu',\n                      'GnBu', 'PuBu', 'YlGnBu', 'PuBuGn', 'BuGn', 'YlGn'])\n\n\n<img alt=\"Sequential colormaps\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_colormaps_002.png\" srcset=\"../../_images/sphx_glr_colormaps_002.png, ../../_images/sphx_glr_colormaps_002_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Colors->Choosing Colormaps in Matplotlib->Classes of colormaps->Sequential2"
            ],
            [
                "matplotlib->Tutorials->Colors->Choosing Colormaps in Matplotlib->Classes of colormaps->Qualitative"
            ],
            [
                "matplotlib->Tutorials->Colors->Choosing Colormaps in Matplotlib->Classes of colormaps->Diverging"
            ],
            [
                "matplotlib->Tutorials->Colors->Choosing Colormaps in Matplotlib->Classes of colormaps->Miscellaneous"
            ],
            [
                "matplotlib->Tutorials->Colors->Choosing Colormaps in Matplotlib->Classes of colormaps->Sequential"
            ]
        ]
    },
    "191170": {
        "jupyter_code_cell": "print(df_input.shape)\nprint(df_output.shape)\ninfo=pd.read_csv(\"customer_info_numeric.csv\")\nrfm_train=pd.read_csv(\"rfm train seq.csv\")",
        "matched_tutorial_code_inds": [
            6083,
            1746,
            5312,
            5215,
            6069
        ],
        "matched_tutorial_codes": [
            "sel = ar_select_order(ind_prod, 13, \"bic\", old_names=False)\nres = sel.model.fit()\nprint(res.summary())",
            "glove = data.fetch('glove.6B.50d.zip')\nemb_path = textproc.unzipper(glove, 'glove.6B.300d.txt')\nemb_matrix = textproc.loadGloveModel(emb_path)",
            "oo = np.ones(df.shape[0])\nmodel2 = sm.MixedLM(df.y, oo, exog_re=oo, groups=df.group1, exog_vc=vcs)\nresult2 = model2.fit()\nprint(result2.summary())",
            "glsar_model = sm.GLSAR(data.endog, data.exog, 1)\nglsar_results = glsar_model.iterative_fit(1)\nprint(glsar_results.summary())",
            "sel = ar_select_order(housing, 13, seasonal=True, old_names=False)\nsel.ar_lags\nres = sel.model.fit()\nprint(res.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions->Industrial Production"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Nested analysis"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Generalized Least Squares"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions->Seasonal Dummies"
            ]
        ]
    },
    "625632": {
        "jupyter_code_cell": "sector_data.top_sector.value_counts()\nbanks_data = labels[labels.Bank1.notnull()]\nbanks_data.Bank1 = banks_data.Bank1.apply(clean)\nbanks_data.Bank2 = banks_data.Bank2.apply(clean)",
        "matched_tutorial_code_inds": [
            3616,
            3771,
            5205,
            5394,
            6664
        ],
        "matched_tutorial_codes": [
            "hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()",
            "win_percent.sort_values().plot.barh(figsize=(6, 12), width=.85, color='k')\nplt.tight_layout()\nsns.despine()\nplt.xlabel(\"Win Percent\")",
            "data = sm.datasets.longley.load()\ndata.exog = sm.add_constant(data.exog)\nprint(data.exog.head())",
            "anes_data = sm.datasets.anes96.load()\nanes_exog = anes_data.exog\nanes_exog = sm.add_constant(anes_exog)",
            "mod_conc = LocalLevelConcentrated(dta.infl)\nres_conc = mod_conc.fit(disp=False)\nprint(res_conc.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Generalized Least Squares"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Multinomial Logit"
            ],
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Concentrating out the scale"
            ]
        ]
    },
    "1220759": {
        "jupyter_code_cell": "mc.Tenure.value_counts(normalize=True).to_frame()*100\nsns.countplot(y='LanguageRecommendationSelect', data=mc, palette='Spectral')",
        "matched_tutorial_code_inds": [
            3771,
            6146,
            3829,
            6664,
            3827
        ],
        "matched_tutorial_codes": [
            "win_percent.sort_values().plot.barh(figsize=(6, 12), width=.85, color='k')\nplt.tight_layout()\nsns.despine()\nplt.xlabel(\"Win Percent\")",
            "table = pd.DataFrame(data, columns=[\"lag\", \"AC\", \"Q\", \"Prob(Q)\"])\nprint(table.set_index(\"lag\"))",
            "ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "mod_conc = LocalLevelConcentrated(dta.infl)\nres_conc = mod_conc.fit(disp=False)\nprint(res_conc.summary())",
            "daily = df.fl_date.value_counts().sort_index()\ny = daily.resample('MS').mean()\ny.head()"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Concentrating out the scale"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ]
        ]
    },
    "279837": {
        "jupyter_code_cell": "X_train.shape\ninput_dim = X_train.shape[1]\nencoding_dim = 15",
        "matched_tutorial_code_inds": [
            2423,
            5466,
            5425,
            3159,
            441
        ],
        "matched_tutorial_codes": [
            "all_splits = list(ts_cv.split(X, y))\ntrain_0, test_0 = all_splits[0]",
            "means25 = exog.mean()\nprint(means25)",
            "respondent1000 = dta.iloc[1000]\nprint(respondent1000)",
            "class_of_interest = \"virginica\"\nclass_id = (label_binarizer.classes_ == class_of_interest)[0]\nclass_id",
            "input_batch=big_input_batch\n\nmodel_input = to_tensor(transform(input_batch), padding_value=1)\noutput = model(model_input)\noutput.shape"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Time-based cross-validation"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data"
            ],
            [
                "sklearn->Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->One-vs-Rest multiclass ROC->ROC curve showing a specific class"
            ],
            [
                "torch->Text->Fast Transformer Inference with Better Transformer->Additional Information"
            ]
        ]
    },
    "954218": {
        "jupyter_code_cell": "def context_a_sent(sent):\n    context_list = []\n    for i in range(len(sent)): \n        context_list.append(pos_features(sent, i))\n    return context_list\nsent = comb_words_q1[10]\ncontext = context_a_sent(sent)\nprint (context)\ndef context_tag(comb_q1):\n    context_tags = []\n    for i in range(len(comb_q1)):\n        context_tag = []\n        context = context_a_sent(comb_q1[i])\n        for item in context:\n            x = list(zip(item.keys(), item.values()))  \n            context_tag.append(x[0][0] + x[0][1]+\" \"+x[1][0] + x[1][1])\n        context_tags.append(context_tag)\n    return context_tags",
        "matched_tutorial_code_inds": [
            35,
            2377,
            347,
            353,
            1069
        ],
        "matched_tutorial_codes": [
            "def print_size_of_model(model, label=\"\"):\n    (model.state_dict(), \"temp.p\")\n    size=os.path.getsize(\"temp.p\")\n    print(\"model: \",label,' \\t','Size (KB):', size/1e3)\n    os.remove('temp.p')\n    return size\n\n# compare the sizes\nf=print_size_of_model(float_lstm,\"fp32\")\nq=print_size_of_model(quantized_lstm,\"int8\")\nprint(\"{0:.2f} times smaller\".format(f/q))",
            "def title(y_pred, y_test, target_names, i):\n    pred_name = target_names[y_pred[i]].rsplit(\" \", 1)[-1]\n    true_name = target_names[y_test[i]].rsplit(\" \", 1)[-1]\n    return \"predicted: %s\\ntrue:      %s\" % (pred_name, true_name)\n\n\nprediction_titles = [\n    title(y_pred, y_test, target_names, i) for i in range(y_pred.shape[0])\n]\n\nplot_gallery(X_test, prediction_titles, h, w)\n\n\n<img alt=\"predicted: Bush true:      Bush, predicted: Bush true:      Bush, predicted: Blair true:      Blair, predicted: Bush true:      Bush, predicted: Bush true:      Bush, predicted: Bush true:      Bush, predicted: Schroeder true:      Schroeder, predicted: Powell true:      Powell, predicted: Bush true:      Bush, predicted: Bush true:      Bush, predicted: Bush true:      Bush, predicted: Bush true:      Bush\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_face_recognition_002.png\" srcset=\"../../_images/sphx_glr_plot_face_recognition_002.png\"/>",
            "def preprocess(x, y):\n    return x.view(-1, 1, 28, 28), y\n\n\nclass WrappedDataLoader:\n    def __init__(self, dl, func):\n        self.dl = dl\n        self.func = func\n\n    def __len__(self):\n        return len(self.dl)\n\n    def __iter__(self):\n        batches = iter(self.dl)\n        for b in batches:\n            yield (self.func(*b))\n\ntrain_dl, valid_dl = get_data(, , bs)\ntrain_dl = WrappedDataLoader(train_dl, preprocess)\nvalid_dl = WrappedDataLoader(valid_dl, preprocess)",
            "def preprocess(x, y):\n    return x.view(-1, 1, 28, 28).to(), y.to()\n\n\ntrain_dl, valid_dl = get_data(, , bs)\ntrain_dl = WrappedDataLoader(train_dl, preprocess)\nvalid_dl = WrappedDataLoader(valid_dl, preprocess)",
            "def print_size_of_model(model):\n    torch.save(model.state_dict(), \"temp.p\")\n    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n    os.remove('temp.p')\n\nprint_size_of_model(model)\nprint_size_of_model(quantized_model)"
        ],
        "matched_tutorial_paths": [
            [
                "torch->PyTorch Recipes->Dynamic Quantization->Steps->3. Look at Model Size"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Faces recognition example using eigenfaces and SVMs"
            ],
            [
                "torch->Learning PyTorch->What is torch.nn really?->Wrapping DataLoader"
            ],
            [
                "torch->Learning PyTorch->What is torch.nn really?->Using your GPU"
            ],
            [
                "torch->Model Optimization->(beta) Dynamic Quantization on BERT->3. Apply the dynamic quantization->3.1 Check the model size"
            ]
        ]
    },
    "554471": {
        "jupyter_code_cell": "%matplotlib inline\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib\nimport geopandas\nimport datetime\nimport csv\ndf = pd.read_csv(\"Crimes_-_2001_to_present.csv\")",
        "matched_tutorial_code_inds": [
            5228,
            3622,
            6604,
            6675,
            6701
        ],
        "matched_tutorial_codes": [
            "%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom pandas_datareader.data import DataReader\n\nnp.set_printoptions(suppress=True)",
            "%matplotlib inline\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style='ticks', context='talk')\n\nimport prep",
            "%matplotlib inline\n\nfrom collections import OrderedDict\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\nplt.rc(\"figure\", figsize=(16, 8))\nplt.rc(\"font\", size=15)",
            "%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\nfrom pandas_datareader.data import DataReader",
            "%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom statsmodels.multivariate.pca import PCA\n\nplt.rc(\"figure\", figsize=(16, 8))\nplt.rc(\"font\", size=14)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Recursive Least Squares"
            ],
            [
                "pandas_toms_blog->Method Chaining->Method Chaining"
            ],
            [
                "statsmodels->Examples->State space models->Statespace: Custom Models"
            ],
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: Chandrasekhar recursions"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ]
        ]
    },
    "1049448": {
        "jupyter_code_cell": "import pyLDAvis\nimport pyLDAvis.sklearn\npyLDAvis.enable_notebook()\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nsw = stopwords.words('english')\ndef tokenizer_for_LDAvis(sentence):\n    tokens = ret.tokenize(sentence)\n    sentence = [lem.lemmatize(t.lower()) for t in tokens if t.lower() not in sw and len(t) >1]\n    string_tokenize_title = ' '.join(sentence)\n    return string_tokenize_title",
        "matched_tutorial_code_inds": [
            1838,
            651,
            1393,
            552,
            1212
        ],
        "matched_tutorial_codes": [
            "import scipy\nimport numpy as np\nfrom sklearn.model_selection import \nfrom sklearn.cluster import \nfrom sklearn.datasets import \nfrom sklearn.metrics import \n\nrng = (0)\nX, y = (random_state=rng)\nX = (X)\nX_train, X_test, _, y_test = (X, y, random_state=rng)\nkmeans = (n_init=\"auto\").fit(X_train)\nprint((kmeans.predict(X_test), y_test))",
            "import torchvision.models as models\nimport time\n\nrn18 = ()\nrn18.eval()\n\ninp = (10, 3, 224, 224)\noutput = rn18(inp)\n\ndef benchmark(model, iters=20):\n    for _ in range(10):\n        model(inp)\n    begin = time.time()\n    for _ in range(iters):\n        model(inp)\n    return str(time.time()-begin)\n\nfused_rn18 = fuse(rn18)\nprint(\"Unfused time: \", benchmark(rn18))\nprint(\"Fused time: \", benchmark(fused_rn18))",
            "import matplotlib.pyplot as plt\nimport numpy as np\nidx = 5\nprint(\"Question: \", dataset[\"train\"][idx][\"question\"])\nprint(\"Answers: \" ,dataset[\"train\"][idx][\"answers\"])\nim = np.asarray(dataset[\"train\"][idx][\"image\"].resize((500,500)))\nplt.imshow(im)\nplt.show()\n\n\n<img alt=\"flava finetuning tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_flava_finetuning_tutorial_001.png\" srcset=\"../_images/sphx_glr_flava_finetuning_tutorial_001.png\"/>",
            "import torch\nfrom torch import nn\nfrom torchvision import transforms as T\nfrom PIL import Image\nimport numpy as np\nfrom pathlib import Path\nfrom collections import deque\nimport random, datetime, os, copy\n\n# Gym is an OpenAI toolkit for RL\nimport gym\nfrom gym.spaces import Box\nfrom gym.wrappers import FrameStack\n\n# NES Emulator for OpenAI Gym\nfrom nes_py.wrappers import JoypadSpace\n\n# Super Mario environment for OpenAI Gym\nimport gym_super_mario_bros",
            "import matplotlib.pyplot as plt\nplt.switch_backend('Agg')\nimport numpy as np\nimport timeit\n\nnum_repeat = 10\n\nstmt = \"train(model)\"\n\nsetup = \"model = ModelParallelResNet50()\"\nmp_run_times = timeit.repeat(\n    stmt, setup, number=1, repeat=num_repeat, globals=globals())\nmp_mean, mp_std = np.mean(mp_run_times), np.std(mp_run_times)\n\nsetup = \"import torchvision.models as models;\" + \\\n        \"model = models.resnet50(num_classes=num_classes).to('cuda:0')\"\nrn_run_times = timeit.repeat(\n    stmt, setup, number=1, repeat=num_repeat, globals=globals())\nrn_mean, rn_std = np.mean(rn_run_times), np.std(rn_run_times)\n\n\ndef plot(means, stds, , fig_name):\n    fig, ax = plt.subplots()\n    ax.bar(np.arange(len(means)), means, yerr=stds,\n           align='center', alpha=0.5, ecolor='red', capsize=10, width=0.6)\n    ax.set_ylabel('ResNet50 Execution Time (Second)')\n    ax.set_xticks(np.arange(len(means)))\n    ax.set_xticklabels()\n    ax.yaxis.grid(True)\n    plt.tight_layout()\n    plt.savefig(fig_name)\n    plt.close(fig)\n\n\nplot([mp_mean, rn_mean],\n     [mp_std, rn_std],\n     ['Model Parallel', 'Single GPU'],\n     'mp_vs_rn.png')\n\n\n\n<img alt=\"\" src=\"../_images/mp_vs_rn.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 0.23->Scalability and stability improvements to KMeans"
            ],
            [
                "torch->Code Transforms with FX->(beta) Building a Convolution/Batch Norm fuser in FX->Benchmarking our Fusion on ResNet18"
            ],
            [
                "torch->Multimodality->TorchMultimodal Tutorial: Finetuning FLAVA->Steps"
            ],
            [
                "torch->Reinforcement Learning->Train a Mario-playing RL Agent"
            ],
            [
                "torch->Parallel and Distributed Training->Single-Machine Model Parallel Best Practices->Apply Model Parallel to Existing Modules"
            ]
        ]
    },
    "321538": {
        "jupyter_code_cell": "grouped_sum = articles_grouped['eventStrength'].sum() \narticles_grouped['percentage']  = articles_grouped['eventStrength'].div(grouped_sum)*100 \narticles_grouped.sort_values(['eventStrength', 'contentId'], ascending = [0,1]) ; articles_grouped.head()\nusers = merged['personId'].unique() ; print(\"# of users\" , len(users))",
        "matched_tutorial_code_inds": [
            2298,
            5613,
            5630,
            5963,
            1685
        ],
        "matched_tutorial_codes": [
            "feature_importance = reg.feature_importances_\nsorted_idx = (feature_importance)\npos = (sorted_idx.shape[0]) + 0.5\nfig = (figsize=(12, 6))\n(1, 2, 1)\n(pos, feature_importance[sorted_idx], align=\"center\")\n(pos, (diabetes.feature_names)[sorted_idx])\n(\"Feature Importance (MDI)\")\n\nresult = (\n    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_idx = result.importances_mean.argsort()\n(1, 2, 2)\n(\n    result.importances[sorted_idx].T,\n    vert=False,\n    labels=(diabetes.feature_names)[sorted_idx],\n)\n(\"Permutation Importance (test set)\")\nfig.tight_layout()\n()\n\n\n<img alt=\"Feature Importance (MDI), Permutation Importance (test set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\"/>",
            "gr = data[\"affairs rate_marriage age yrs_married\".split()].groupby(\n    \"rate_marriage age yrs_married\".split()\n)\ndf_a = gr.agg([\"mean\", \"sum\", \"count\"])\n\n\ndef merge_tuple(tpl):\n    if isinstance(tpl, tuple) and len(tpl)  1:\n        return \"_\".join(map(str, tpl))\n    else:\n        return tpl\n\n\ndf_a.columns = df_a.columns.map(merge_tuple)\ndf_a.reset_index(inplace=True)\nprint(df_a.shape)\ndf_a.head()",
            "glm = smf.glm(\n    \"affairs_sum ~ rate_marriage + age + yrs_married\",\n    data=df_a,\n    family=sm.families.Poisson(),\n    exposure=np.asarray(df_a[\"affairs_count\"]),\n)\nres_e = glm.fit()\nprint(res_e.summary())",
            "data = [\n    [\"Carroll\", 94, 22, 60, 92, 20, 60],\n    [\"Grant\", 98, 21, 65, 92, 22, 65],\n    [\"Peck\", 98, 28, 40, 88, 26, 40],\n    [\"Donat\", 94, 19, 200, 82, 17, 200],\n    [\"Stewart\", 98, 21, 50, 88, 22, 45],\n    [\"Young\", 96, 21, 85, 92, 22, 85],\n]\ncolnames = [\"study\", \"mean_t\", \"sd_t\", \"n_t\", \"mean_c\", \"sd_c\", \"n_c\"]\nrownames = [i[0] for i in data]\ndframe1 = pd.DataFrame(data, columns=colnames)\nrownames",
            "pollutant_data = np.loadtxt(\"air-quality-data.csv\", dtype=float, delimiter=\",\",\n                            skiprows=1, usecols=range(1, 8))\npollutants_A = pollutant_data[:, 0:5]\npollutants_B = pollutant_data[:, 5:]\n\nprint(pollutants_A.shape)\nprint(pollutants_B.shape)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Ensemble methods->Gradient Boosting regression->Plot feature importance"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Condensing and Aggregating observations->Dataset with unique explanatory variables (exog)"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->aggregated or averaged data (unique values of explanatory variables)->using exposure"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Example"
            ],
            [
                "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Building the dataset"
            ]
        ]
    },
    "810835": {
        "jupyter_code_cell": "(100*registered.groupby('Titular Type')['Titular Type'].count()/(len(registered))).plot(kind='bar')\nplt.ylabel('% aantal wagens')\nplt.xlabel(' ')\nplt.gca().set_xticklabels(['Rechtsperson', 'Natuurlijk persoon'])\nVL_registered = registered[registered['Region Gewest'] == 'VLA']\n(100*VL_registered.groupby('Titular Type')['Titular Type'].count()/(len(VL_registered))).plot(kind='barh', )\nplt.xlabel('% aantal wagens')\nplt.ylabel(' ')\nplt.gca().set_yticklabels(['Rechtsperson', 'Natuurlijk persoon'])",
        "matched_tutorial_code_inds": [
            2567,
            2548,
            2557,
            2555,
            2587
        ],
        "matched_tutorial_codes": [
            "(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\n(\n    X_train,\n    y_train_noisy,\n    noise_std,\n    linestyle=\"None\",\n    color=\"tab:blue\",\n    marker=\".\",\n    markersize=10,\n    label=\"Observations\",\n)\n(X, mean_prediction, label=\"Mean prediction\")\n(\n    X.ravel(),\n    mean_prediction - 1.96 * std_prediction,\n    mean_prediction + 1.96 * std_prediction,\n    color=\"tab:orange\",\n    alpha=0.5,\n    label=r\"95% confidence interval\",\n)\n()\n(\"$x$\")\n(\"$f(x)$\")\n_ = (\"Gaussian process regression on a noisy dataset\")\n\n\n<img alt=\"Gaussian process regression on a noisy dataset\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_noisy_targets_003.png\" srcset=\"../../_images/sphx_glr_plot_gpr_noisy_targets_003.png\"/>",
            "(data, target, label=\"True signal\", linewidth=2, linestyle=\"dashed\")\n(\n    training_data,\n    training_noisy_target,\n    color=\"black\",\n    label=\"Noisy measurements\",\n)\n(\n    data,\n    predictions_kr,\n    label=\"Kernel ridge\",\n    linewidth=2,\n    linestyle=\"dashdot\",\n)\n(loc=\"lower right\")\n(\"data\")\n(\"target\")\n_ = (\n    \"Kernel ridge regression with an exponential sine squared\\n \"\n    \"kernel using tuned hyperparameters\"\n)\n\n\n<img alt=\"Kernel ridge regression with an exponential sine squared  kernel using tuned hyperparameters\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_compare_gpr_krr_004.png\" srcset=\"../../_images/sphx_glr_plot_compare_gpr_krr_004.png\"/>",
            "(data, target, label=\"True signal\", linewidth=2, linestyle=\"dashed\")\n(\n    training_data,\n    training_noisy_target,\n    color=\"black\",\n    label=\"Noisy measurements\",\n)\n# Plot the predictions of the kernel ridge\n(\n    data,\n    predictions_kr,\n    label=\"Kernel ridge\",\n    linewidth=2,\n    linestyle=\"dashdot\",\n)\n# Plot the predictions of the gaussian process regressor\n(\n    data,\n    mean_predictions_gpr,\n    label=\"Gaussian process regressor\",\n    linewidth=2,\n    linestyle=\"dotted\",\n)\n(\n    data.ravel(),\n    mean_predictions_gpr - std_predictions_gpr,\n    mean_predictions_gpr + std_predictions_gpr,\n    color=\"tab:green\",\n    alpha=0.2,\n)\n(loc=\"lower right\")\n(\"data\")\n(\"target\")\n_ = (\"Effect of using a radial basis function kernel\")\n\n\n<img alt=\"Effect of using a radial basis function kernel\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_compare_gpr_krr_006.png\" srcset=\"../../_images/sphx_glr_plot_compare_gpr_krr_006.png\"/>",
            "(data, target, label=\"True signal\", linewidth=2, linestyle=\"dashed\")\n(\n    training_data,\n    training_noisy_target,\n    color=\"black\",\n    label=\"Noisy measurements\",\n)\n# Plot the predictions of the kernel ridge\n(\n    data,\n    predictions_kr,\n    label=\"Kernel ridge\",\n    linewidth=2,\n    linestyle=\"dashdot\",\n)\n# Plot the predictions of the gaussian process regressor\n(\n    data,\n    mean_predictions_gpr,\n    label=\"Gaussian process regressor\",\n    linewidth=2,\n    linestyle=\"dotted\",\n)\n(\n    data.ravel(),\n    mean_predictions_gpr - std_predictions_gpr,\n    mean_predictions_gpr + std_predictions_gpr,\n    color=\"tab:green\",\n    alpha=0.2,\n)\n(loc=\"lower right\")\n(\"data\")\n(\"target\")\n_ = (\"Comparison between kernel ridge and gaussian process regressor\")\n\n\n<img alt=\"Comparison between kernel ridge and gaussian process regressor\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_compare_gpr_krr_005.png\" srcset=\"../../_images/sphx_glr_plot_compare_gpr_krr_005.png\"/>",
            "(X, y, color=\"black\", linestyle=\"dashed\", label=\"Measurements\")\n(X_test, mean_y_pred, color=\"tab:blue\", alpha=0.4, label=\"Gaussian process\")\n(\n    X_test.ravel(),\n    mean_y_pred - std_y_pred,\n    mean_y_pred + std_y_pred,\n    color=\"tab:blue\",\n    alpha=0.2,\n)\n()\n(\"Year\")\n(\"Monthly average of CO$_2$ concentration (ppm)\")\n_ = (\n    \"Monthly average of air samples measurements\\nfrom the Mauna Loa Observatory\"\n)\n\n\n<img alt=\"Monthly average of air samples measurements from the Mauna Loa Observatory\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_co2_003.png\" srcset=\"../../_images/sphx_glr_plot_gpr_co2_003.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian Processes regression: basic introductory example->Example with noisy targets"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Comparison of kernel ridge and Gaussian process regression->Kernel methods: kernel ridge and Gaussian process->Kernel ridge"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Comparison of kernel ridge and Gaussian process regression->Final conclusion"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Comparison of kernel ridge and Gaussian process regression->Kernel methods: kernel ridge and Gaussian process->Gaussian process regression"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) on Mauna Loa CO2 data->Model fitting and extrapolation"
            ]
        ]
    },
    "161998": {
        "jupyter_code_cell": "s.head(5)\ns.tail(5)\nsymbol = \"CMG\"\nstart = \"2012-01-01\"\nend = \"2016-01-01\"\nprices = get_pricing(symbol, start_date=start, end_date=end, fields=\"price\")\nmonthly_prices = prices.resample('M').median()\nmonthly_prices.head(24)",
        "matched_tutorial_code_inds": [
            3773,
            93,
            2997,
            3669,
            2096
        ],
        "matched_tutorial_codes": [
            "df = df.assign(away_strength=df['away_team'].map(win_percent),\n               home_strength=df['home_team'].map(win_percent),\n               point_diff=df['home_points'] - df['away_points'],\n               rest_diff=df['home_rest'] - df['away_rest'])\ndf.head()",
            "x = (-5, 5, 0.1).view(-1, 1)\ny = -5 * x + 0.1 * (x.size())\n\nmodel = (1, 1)\ncriterion = ()\noptimizer = (model.parameters(), lr = 0.1)\n\ndef train_model(iter):\n    for epoch in range(iter):\n        y1 = model(x)\n        loss = criterion(y1, y)\n        writer.add_scalar(\"Loss/train\", loss, epoch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\ntrain_model(10)\nwriter.flush()",
            "tree_disp = (tree, X, [\"age\"])\nmlp_disp = (\n    mlp, X, [\"age\"], ax=tree_disp.axes_, line_kw={\"color\": \"red\"}\n)\n\n\n<img alt=\"plot partial dependence visualization api\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_partial_dependence_visualization_api_006.png\" srcset=\"../../_images/sphx_glr_plot_partial_dependence_visualization_api_006.png\"/>",
            "temp2 = temp.reset_index()\nsped2 = sped.reset_index()\n\n# Find rows where the operation is defined\ncommon_dates = pd.Index(temp2.date) &amp; sped2.date\npd.concat([\n    # concat to not lose date information\n    sped2.loc[sped2['date'].isin(common_dates), 'date'],\n    (sped2.loc[sped2.date.isin(common_dates), 'sped'] /\n     temp2.loc[temp2.date.isin(common_dates), 'tmpf'])],\n    axis=1).dropna(how='all')",
            "node_indicator = clf.decision_path(X_test)\nleaf_id = clf.apply(X_test)\n\nsample_id = 0\n# obtain ids of the nodes `sample_id` goes through, i.e., row `sample_id`\nnode_index = node_indicator.indices[\n    node_indicator.indptr[sample_id] : node_indicator.indptr[sample_id + 1]\n]\n\nprint(\"Rules used to predict sample {id}:\\n\".format(id=sample_id))\nfor node_id in node_index:\n    # continue to the next node if it is a leaf node\n    if leaf_id[sample_id] == node_id:\n        continue\n\n    # check if value of the split feature for sample 0 is below threshold\n    if X_test[sample_id, feature[node_id]] &lt;= threshold[node_id]:\n        threshold_sign = \"&lt;=\"\n    else:\n        threshold_sign = \"\"\n\n    print(\n        \"decision node {node} : (X_test[{sample}, {feature}] = {value}) \"\n        \"{inequality} {threshold})\".format(\n            node=node_id,\n            sample=sample_id,\n            feature=feature[node_id],\n            value=X_test[sample_id, feature[node_id]],\n            inequality=threshold_sign,\n            threshold=threshold[node_id],\n        )\n    )"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team"
            ],
            [
                "torch->PyTorch Recipes->How to use TensorBoard with PyTorch->Log scalars"
            ],
            [
                "sklearn->Examples->Miscellaneous->Advanced Plotting With Partial Dependence->Plotting partial dependence for one feature"
            ],
            [
                "pandas_toms_blog->Indexes->Flavors->Indexes for Alignment"
            ],
            [
                "sklearn->Examples->Decision Trees->Understanding the decision tree structure->Decision path"
            ]
        ]
    },
    "507192": {
        "jupyter_code_cell": "df2[['control','ab_page']]=pd.get_dummies(df2['group'])\ndf2.head()\ndf2['intercept']=1\nmodel=sm.Logit(df2['converted'],df2[['intercept','ab_page']])\nresult=model.fit()",
        "matched_tutorial_code_inds": [
            6406,
            5143,
            5312,
            3861,
            5571
        ],
        "matched_tutorial_codes": [
            "mod = sm.tsa.VARMAX(endog[['dln_inv', 'dln_inc']], order=(1,1))\nres = mod.fit(maxiter=1000, disp=False)\nprint(res.summary())",
            "sqtab = sm.stats.SquareTable.from_data(data[['left', 'right']])\nprint(sqtab.summary())",
            "oo = np.ones(df.shape[0])\nmodel2 = sm.MixedLM(df.y, oo, exog_re=oo, groups=df.group1, exog_vc=vcs)\nresult2 = model2.fit()\nprint(result2.summary())",
            "df = dd.read_parquet(\"data/indiv-*.parquet\", engine='pyarrow', columns=['occupation'])\n\nmost_common = df.occupation.value_counts().nlargest(100)\nmost_common.compute().sort_values(ascending=False)",
            "glm_binom = sm.GLM(data.endog, data.exog, family=sm.families.Binomial())\nres = glm_binom.fit()\nprint(res.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->VARMAX: Introduction->Caution: VARMA(p,q) specifications"
            ],
            [
                "statsmodels->User Guide->Statistics and Tools->Contingency tables->Symmetry and homogeneity"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Nested analysis"
            ],
            [
                "pandas_toms_blog->Scaling->Using Dask"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Generalized Linear Models Overview->GLM: Binomial response data->Fit and summary"
            ]
        ]
    },
    "918063": {
        "jupyter_code_cell": "plt.figure(figsize=(20, 10))\nmatplotlib.rcParams.update({'font.size': 22})\ndflat = df.loc[df['alt'] > 0]\nplt.plot(dflat['lat'],dflat['count'], linestyle='', marker='.', color='gray')\ndflat = df.loc[df['alt'] > 0]\npf_coef = np.polyfit(dflat['lat'],dflat['count'],1)     \np = np.poly1d(pf_coef)\nplt.plot(sp.linspace(5,70), p(sp.linspace(5,70)), 'r-', c='red', lw=3)\nplt.text(10,3,str(np.poly1d(pf_coef)),color='red')\nr = pearsonr(p(dflat['lat']),dflat['count'])\nplt.text(10,2.8,'r = ' + str(round(r[0],4)),color='r')\nplt.ylim(1,4)\nplt.xlim(5,70)\nplt.xlabel('geographic latitude [degree]')\nplt.ylabel(r'counts per second [s$^-$$^1$]')\nplt.figure(figsize=(20, 5))\ndfselected = dflat\ndfselected['count'] = dfselected['count'] + p(60) - p(dfselected['lat'])\nplt.plot(dfselected['lat'],dfselected['count'], linestyle='', marker='.', color='black')\nprint p(61)-p(60)\nplt.figure(figsize=(20, 10))\nmatplotlib.rcParams.update({'font.size': 22})\ndfalt = dflat\nplt.plot(dflat['alt'],dfalt['count'], linestyle='', marker='.', color='gray')\npf_coef = np.polyfit(dflat['alt'],dflat['count'],1)     \npp = np.poly1d(pf_coef)\nplt.plot(sp.linspace(30000,42000), pp(sp.linspace(30000,42000)), 'r-', c='red', lw=3)\nplt.text(31000,3.7,np.poly1d(pf_coef),color='red')\nr = pearsonr(pp(dflat['alt']),dflat['count'])\nplt.text(31000,3.5,'r = ' + str(round(r[0],4)),color='r')\nplt.ylim(1,4)\nplt.xlabel('altitude [feet]')\nplt.ylabel(r'counts per second [s$^-$$^1$]')\nplt.figure(figsize=(20, 5))\ndfselection = dfalt.loc[dfalt['alt'] > 31000]\ndfselection['count'] = dfselection['count'] + pp(40000) - pp(dfselection['alt'])\nplt.plot(dfselection['alt'],dfselection['count'], linestyle='', marker='.', color='black')\nprint pp(40001)- pp(40000)\ntry:\n    df = df.reset_index()\nexcept:\n    pass\ndf1 = df[['date','alt','lat','count']]\ndf1['date'] = pd.to_datetime(df1['date'])\ndf1['count'] = df1['count'] + pp(40000) - pp(df1['alt']) \ndf1['count'] = df1['count'] + p(60) - p(df1['lat']) \ndf1 = df1.set_index('date')\ndf1 = df1.sort_index()\ndf1.to_csv('./radiation.csv') ",
        "matched_tutorial_code_inds": [
            5362,
            4795,
            4827,
            4820,
            4796
        ],
        "matched_tutorial_codes": [
            "plt.rcParams[\"figure.subplot.bottom\"] = 0.23  # keep labels visible\nplt.rcParams[\"figure.figsize\"] = (10.0, 8.0)  # make plot larger in notebook\nage = [data.exog[\"age\"][data.endog == id] for id in party_ID]\nfig = plt.figure()\nax = fig.add_subplot(111)\nplot_opts = {\n    \"cutoff_val\": 5,\n    \"cutoff_type\": \"abs\",\n    \"label_fontsize\": \"small\",\n    \"label_rotation\": 30,\n}\nsm.graphics.beanplot(age, ax=ax, labels=labels, plot_opts=plot_opts)\nax.set_xlabel(\"Party identification of respondent.\")\nax.set_ylabel(\"Age\")\n# plt.show()",
            "plt.rcParams['figure.constrained_layout.use'] = True\nfig, axs = plt.subplots(2, 2, figsize=(3, 3))\nfor ax in axs.flat:\n    example_plot(ax)\n\n\n<img alt=\"Title, Title, Title, Title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_constrainedlayout_guide_018.png\" srcset=\"../../_images/sphx_glr_constrainedlayout_guide_018.png, ../../_images/sphx_glr_constrainedlayout_guide_018_2_0x.png 2.0x\"/>",
            "plt.close('all')\narr = np.arange(100).reshape((10, 10))\nfig = plt.figure(figsize=(4, 4))\nim = plt.imshow(arr, interpolation=\"none\")\n\nplt.colorbar(im)\n\nplt.tight_layout()\n\n\n<img alt=\"tight layout guide\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_tight_layout_guide_014.png\" srcset=\"../../_images/sphx_glr_tight_layout_guide_014.png, ../../_images/sphx_glr_tight_layout_guide_014_2_0x.png 2.0x\"/>",
            "plt.close('all')\nfig = plt.figure()\n\nax1 = plt.subplot2grid((3, 3), (0, 0))\nax2 = plt.subplot2grid((3, 3), (0, 1), colspan=2)\nax3 = plt.subplot2grid((3, 3), (1, 0), colspan=2, rowspan=2)\nax4 = plt.subplot2grid((3, 3), (1, 2), rowspan=2)\n\nexample_plot(ax1)\nexample_plot(ax2)\nexample_plot(ax3)\nexample_plot(ax4)\n\nplt.tight_layout()\n\n\n<img alt=\"Title, Title, Title, Title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_tight_layout_guide_007.png\" srcset=\"../../_images/sphx_glr_tight_layout_guide_007.png, ../../_images/sphx_glr_tight_layout_guide_007_2_0x.png 2.0x\"/>",
            "plt.rcParams['figure.constrained_layout.use'] = False\nfig = plt.figure(layout=\"constrained\")\n\ngs1 = gridspec.GridSpec(2, 1, figure=fig)\nax1 = fig.add_subplot(gs1[0])\nax2 = fig.add_subplot(gs1[1])\n\nexample_plot(ax1)\nexample_plot(ax2)\n\n\n<img alt=\"Title, Title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_constrainedlayout_guide_019.png\" srcset=\"../../_images/sphx_glr_constrainedlayout_guide_019.png, ../../_images/sphx_glr_constrainedlayout_guide_019_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Plotting->Box Plots->Bean Plots"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Constrained Layout Guide->rcParams"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Tight Layout guide->Colorbar"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Tight Layout guide->Simple Example"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Constrained Layout Guide->Use with GridSpec"
            ]
        ]
    },
    "78081": {
        "jupyter_code_cell": "m = folium.Map(location=[43.6532, -79.3832], zoom_start=12) \nm\nfolium.Marker([43.6426, -79.3871], popup='<i>CN Tower</i>').add_to(m)\nfolium.Marker([43.6677, -79.3948], popup='<b>Royal Ontario Museum</b>').add_to(m)\nm",
        "matched_tutorial_code_inds": [
            4798,
            1667,
            1097,
            4811,
            4812
        ],
        "matched_tutorial_codes": [
            "fig = plt.figure(figsize=(4, 6), layout=\"constrained\")\n\ngs0 = fig.add_gridspec(6, 2)\n\nax1 = fig.add_subplot(gs0[:3, 0])\nax2 = fig.add_subplot(gs0[3:, 0])\n\nexample_plot(ax1)\nexample_plot(ax2)\n\nax = fig.add_subplot(gs0[0:2, 1])\nexample_plot(ax, hide_labels=True)\nax = fig.add_subplot(gs0[2:4, 1])\nexample_plot(ax, hide_labels=True)\nax = fig.add_subplot(gs0[4:, 1])\nexample_plot(ax, hide_labels=True)\nfig.suptitle('Overlapping Gridspecs')\n\n\n<img alt=\"Overlapping Gridspecs, Title, Title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_constrainedlayout_guide_021.png\" srcset=\"../../_images/sphx_glr_constrainedlayout_guide_021.png, ../../_images/sphx_glr_constrainedlayout_guide_021_2_0x.png 2.0x\"/>",
            "p = np.polynomial.Polynomial([-16, 0, 0, 0, 15, 0, 0, 0, 1])\np\n\n\n\n\n\n\\[x \\mapsto \\text{-16.0}\\color{LightGray}{ + \\text{0.0}\\,x}\\color{LightGray}{ + \\text{0.0}\\,x^{2}}\\color{LightGray}{ + \\text{0.0}\\,x^{3}} + \\text{15.0}\\,x^{4}\\color{LightGray}{ + \\text{0.0}\\,x^{5}}\\color{LightGray}{ + \\text{0.0}\\,x^{6}}\\color{LightGray}{ + \\text{0.0}\\,x^{7}} + \\text{1.0}\\,x^{8}\\]",
            "qat_model = load_model(saved_model_dir + float_model_file)\nqat_model.fuse_model()\n\noptimizer = torch.optim.SGD(qat_model.parameters(), lr = 0.0001)\n# The old 'fbgemm' is still available but 'x86' is the recommended default.\nqat_model.qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')",
            "fig, axs = plt.subplots(2, 2, layout=\"constrained\")\nfor ax in axs.flat:\n    im = ax.pcolormesh(arr, **pc_kwargs)\nfig.colorbar(im, ax=axs, shrink=0.6)\nplot_children(fig)\n\n\n<img alt=\"constrainedlayout guide\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_constrainedlayout_guide_033.png\" srcset=\"../../_images/sphx_glr_constrainedlayout_guide_033.png, ../../_images/sphx_glr_constrainedlayout_guide_033_2_0x.png 2.0x\"/>",
            "fig = plt.figure(layout=\"constrained\")\ngs = gridspec.GridSpec(2, 2, figure=fig)\nax = fig.add_subplot(gs[:, 0])\nim = ax.pcolormesh(arr, **pc_kwargs)\nax = fig.add_subplot(gs[0, 1])\nim = ax.pcolormesh(arr, **pc_kwargs)\nax = fig.add_subplot(gs[1, 1])\nim = ax.pcolormesh(arr, **pc_kwargs)\nplot_children(fig)\n\n\n<img alt=\"constrainedlayout guide\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_constrainedlayout_guide_034.png\" srcset=\"../../_images/sphx_glr_constrainedlayout_guide_034.png, ../../_images/sphx_glr_constrainedlayout_guide_034_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Intermediate->Constrained Layout Guide->Use with GridSpec"
            ],
            [
                "numpy->NumPy Applications->Plotting Fractals->Generalizing the Julia set->Newton Fractals"
            ],
            [
                "torch->Model Optimization->(beta) Static Quantization with Eager Mode in PyTorch->5. Quantization-aware training"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Constrained Layout Guide->Notes on the algorithm->Colorbar associated with a Gridspec"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Constrained Layout Guide->Notes on the algorithm->Uneven sized Axes"
            ]
        ]
    },
    "136245": {
        "jupyter_code_cell": "print(np.sqrt(metrics.mean_squared_error(Y_test, Y_pred)))\nX = mushroom_col.iloc[:, 0:9].values\nY = mushroom_col.iloc[:, 1].values\nX_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(X, Y, random_state=1)\nlinreg.fit(X_train, Y_train)\nY_pred = linreg.predict(X_test)\nprint(np.sqrt(metrics.mean_squared_error(Y_test, Y_pred)))",
        "matched_tutorial_code_inds": [
            1521,
            2791,
            1550,
            443,
            6668
        ],
        "matched_tutorial_codes": [
            "print(\"Rate of semiconductors added on a chip every 2 years:\")\nprint(\n    \"\\tx{:.2f} +/- {:.2f} semiconductors per chip\".format(\n        np.exp((A) * 2), 2 * A * np.exp(2 * A) * 0.006\n    )\n)",
            "print(\n    \"Mean AvgClaim Amount per policy:              %.2f \"\n    % df_train[\"AvgClaimAmount\"].mean()\n)\nprint(\n    \"Mean AvgClaim Amount | NbClaim  0:           %.2f\"\n    % df_train[\"AvgClaimAmount\"][df_train[\"AvgClaimAmount\"]  0].mean()\n)\nprint(\n    \"Predicted Mean AvgClaim Amount | NbClaim  0: %.2f\"\n    % glm_sev.predict(X_train).mean()\n)\nprint(\n    \"Predicted Mean AvgClaim Amount (dummy) | NbClaim  0: %.2f\"\n    % dummy_sev.predict(X_train).mean()\n)",
            "print(\n    \"The shape of training images: {} and training labels: {}\".format(\n        x_train.shape, y_train.shape\n    )\n)\nprint(\n    \"The shape of test images: {} and test labels: {}\".format(\n        x_test.shape, y_test.shape\n    )\n)",
            "print(\"slow path:\")\nprint(\"==========\")\nwith torch.autograd.profiler.profile(use_cuda=False) as prof:\n  for i in range(ITERATIONS):\n    output = model(model_input)\nprint(prof)\n\nmodel.eval()\n\nprint(\"fast path:\")\nprint(\"==========\")\nwith torch.autograd.profiler.profile(use_cuda=False) as prof:\n  with torch.no_grad():\n    for i in range(ITERATIONS):\n      output = model(model_input)\nprint(prof)",
            "print('Original model')\nprint('var.level     = %.5f' % res.params[0])\nprint('var.irregular = %.5f' % res.params[1])\n\nprint('\\nConcentrated model')\nprint('scale         = %.5f' % res_conc.scale)\nprint('h * scale     = %.5f' % (res_conc.params[0] * res_conc.scale))"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy->Calculating the historical growth curve for transistors"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Tweedie regression on insurance claims->Severity Model -  Gamma distribution"
            ],
            [
                "numpy->NumPy Applications->Deep learning on MNIST->1. Load the MNIST dataset"
            ],
            [
                "torch->Text->Fast Transformer Inference with Better Transformer->Additional Information"
            ],
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Comparing estimates"
            ]
        ]
    },
    "1306957": {
        "jupyter_code_cell": "dm.shape\ny = dm['HomeWin']  \nX = dm[['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10']] \nX = sm.add_constant(X)  ",
        "matched_tutorial_code_inds": [
            5811,
            3703,
            3616,
            5181,
            5319
        ],
        "matched_tutorial_codes": [
            "y = dta[\"log.light\"]\nX = sm.add_constant(dta[\"log.Te\"], prepend=True)\nols_model = sm.OLS(y, X).fit()\nabline_plot(model_results=ols_model, ax=ax)",
            "df = pd.read_csv(\"data/347136217_T_ONTIME.csv\")\ndelays = df['DEP_DELAY']",
            "hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()",
            "R = [[0, 1, 0, 0], [0, 0, 1, 0]]\nprint(np.array(R))\nprint(res2.f_test(R))",
            "oo = np.ones(df.shape[0])\nmodel4 = sm.MixedLM(df.y, oo[:, None], exog_re=None, groups=oo, exog_vc=vcs)\nresult4 = model4.fit()\nprint(result4.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Hertzprung Russell data for Star Cluster CYG 0B1 - Leverage Points"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->F test"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Crossed analysis"
            ]
        ]
    },
    "346112": {
        "jupyter_code_cell": "final_pn_grouped_data = groupingSONDAData.grouping_pn_data('by month by un', ana_turnstiles_df, mauricio_turnstiles_df)\nfinal_zp_grouped_data = groupingSONDAData.grouping_zp_data('by month by un')\nfinal_pn_grouped_data.reset_index(inplace=True)\nfinal_pn_grouped_data.to_csv(pn_output_path,sep=';',encoding='latin-1')\nfinal_zp_grouped_data.reset_index(inplace=True)\nfinal_zp_grouped_data.to_csv(zp_output_path,sep=';',encoding='latin-1')\npn_output_path = os.path.join(DTPM_TRXDir,'3_DAILY/daily_pn.csv')\nzp_output_path = os.path.join(DTPM_TRXDir,'3_DAILY/daily_zp.csv')",
        "matched_tutorial_code_inds": [
            2298,
            2460,
            5978,
            2116,
            2434
        ],
        "matched_tutorial_codes": [
            "feature_importance = reg.feature_importances_\nsorted_idx = (feature_importance)\npos = (sorted_idx.shape[0]) + 0.5\nfig = (figsize=(12, 6))\n(1, 2, 1)\n(pos, feature_importance[sorted_idx], align=\"center\")\n(pos, (diabetes.feature_names)[sorted_idx])\n(\"Feature Importance (MDI)\")\n\nresult = (\n    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_idx = result.importances_mean.argsort()\n(1, 2, 2)\n(\n    result.importances[sorted_idx].T,\n    vert=False,\n    labels=(diabetes.feature_names)[sorted_idx],\n)\n(\"Permutation Importance (test set)\")\nfig.tight_layout()\n()\n\n\n<img alt=\"Feature Importance (MDI), Permutation Importance (test set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\"/>",
            "one_hot_poly_pipeline = (\n    (\n        transformers=[\n            (\"categorical\", one_hot_encoder, categorical_columns),\n            (\"one_hot_time\", one_hot_encoder, [\"hour\", \"weekday\", \"month\"]),\n        ],\n        remainder=\"passthrough\",\n    ),\n    (kernel=\"poly\", degree=2, n_components=300, random_state=0),\n    (alphas=alphas),\n)\nevaluate(one_hot_poly_pipeline, X, y, cv=ts_cv)",
            "res4 = combine_effects(\n    eff, var_eff, method_re=\"iterated\", use_t=False, row_names=rownames\n)\nres4_df = res4.summary_frame()\nprint(\"method RE:\", res4.method_re)\nprint(res4.summary_frame())\nfig = res4.plot_forest()",
            "fa_estimator = (n_components=n_components, max_iter=20)\nfa_estimator.fit(faces_centered)\nplot_gallery(\"Factor Analysis (FA)\", fa_estimator.components_[:n_components])\n\n# --- Pixelwise variance\n(figsize=(3.2, 3.6), facecolor=\"white\", tight_layout=True)\nvec = fa_estimator.noise_variance_\nvmax = max(vec.max(), -vec.min())\n(\n    vec.reshape(image_shape),\n    cmap=plt.cm.gray,\n    interpolation=\"nearest\",\n    vmin=-vmax,\n    vmax=vmax,\n)\n(\"off\")\n(\"Pixelwise variance from \\n Factor Analysis (FA)\", size=16, wrap=True)\n(orientation=\"horizontal\", shrink=0.8, pad=0.03)\n()\n\n\n\n<img alt=\"Factor Analysis (FA)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_008.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_008.png\"/>\n<img alt=\"Pixelwise variance from   Factor Analysis (FA)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_009.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_009.png\"/>",
            "one_hot_linear_pipeline = (\n    (\n        transformers=[\n            (\"categorical\", one_hot_encoder, categorical_columns),\n            (\"one_hot_time\", one_hot_encoder, [\"hour\", \"weekday\", \"month\"]),\n        ],\n        remainder=(),\n    ),\n    (alphas=alphas),\n)\n\nevaluate(one_hot_linear_pipeline, X, y, cv=ts_cv)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Ensemble methods->Gradient Boosting regression->Plot feature importance"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Modeling non-linear feature interactions with kernels"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Example->Using iterated, Paule-Mandel estimate for random effects variance tau"
            ],
            [
                "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition->Factor Analysis components - FA"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Time-steps as categories"
            ]
        ]
    },
    "1030915": {
        "jupyter_code_cell": "lr = MLPClassifier(hidden_layer_sizes=(10, 10,), batch_size=100,\n                   solver='sgd', learning_rate_init=0.01, early_stopping=True)\nstart = time.time()\nscores = cross_val_score(estimator=lr,\n                         X=X, \n                         y=y,\n                         cv=5)\nprint(\"\\nAccuracy: {}% (+/- {})\".format(round(scores.mean() * 100, 2), round(scores.std(), 3) * 2))\nprint('Finished in {}sec\\n'.format(round(time.time() - start, 2)))\nlr = MLPClassifier(hidden_layer_sizes=(10, 10, 10,), batch_size=100,\n                   solver='sgd', learning_rate_init=0.01, early_stopping=True)\nstart = time.time()\nscores = cross_val_score(estimator=lr,\n                         X=X, \n                         y=y,\n                         cv=5)\nprint(\"\\nAccuracy: {}% (+/- {})\".format(round(scores.mean() * 100, 2), round(scores.std(), 3) * 2))\nprint('Finished in {}sec\\n'.format(round(time.time() - start, 2)))",
        "matched_tutorial_code_inds": [
            6700,
            2997,
            1162,
            2844,
            2860
        ],
        "matched_tutorial_codes": [
            "ln_pce = np.log(pce.PCE)\nforecasts = {\"ln PCE\": ln_pce}\nfor year in range(1995, 2020, 3):\n    sub = ln_pce[: str(year)]\n    res = ThetaModel(sub).fit()\n    fcast = res.forecast(12)\n    forecasts[str(year)] = fcast\nforecasts = pd.DataFrame(forecasts)\nax = forecasts[\"1995\":].plot(legend=False)\nchildren = ax.get_children()\nchildren[0].set_linewidth(4)\nchildren[0].set_alpha(0.3)\nchildren[0].set_color(\"#000000\")\nax.set_title(\"ln PCE\")\nplt.tight_layout(pad=1.0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_theta-model_22_0.png\" src=\"../../../_images/examples_notebooks_generated_theta-model_22_0.png\"/>",
            "tree_disp = (tree, X, [\"age\"])\nmlp_disp = (\n    mlp, X, [\"age\"], ax=tree_disp.axes_, line_kw={\"color\": \"red\"}\n)\n\n\n<img alt=\"plot partial dependence visualization api\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_partial_dependence_visualization_api_006.png\" srcset=\"../../_images/sphx_glr_plot_partial_dependence_visualization_api_006.png\"/>",
            "model = init_model()\n = (())\n\ndef train(mod, data):\n    (True)\n    pred = mod(data[0])\n    loss = ()(pred, data[1])\n    loss.backward()\n    ()\n\neager_times = []\nfor i in range(N_ITERS):\n    inp = generate_data(16)\n    _, eager_time = timed(lambda: train(model, inp))\n    eager_times.append(eager_time)\n    print(f\"eager train time {i}: {eager_time}\")\nprint(\"~\" * 10)\n\nmodel = init_model()\n = (())\ntrain_opt = (train, mode=\"reduce-overhead\")\n\ncompile_times = []\nfor i in range(N_ITERS):\n    inp = generate_data(16)\n    _, compile_time = timed(lambda: train_opt(model, inp))\n    compile_times.append(compile_time)\n    print(f\"compile train time {i}: {compile_time}\")\nprint(\"~\" * 10)\n\neager_med = np.median(eager_times)\ncompile_med = np.median(compile_times)\nspeedup = eager_med / compile_med\nprint(f\"(train) eager median: {eager_med}, compile median: {compile_med}, speedup: {speedup}x\")\nprint(\"~\" * 10)",
            "mae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}\n\n_, ax = (figsize=(5, 5))\ndisplay = (\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n)\nax.set_title(\"Ridge model, small regularization\")\nfor name, score in scores.items():\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\nax.legend(loc=\"upper left\")\n()\n\n\n<img alt=\"Ridge model, small regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_009.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_009.png\"/>",
            "mae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}\n\n_, ax = (figsize=(5, 5))\ndisplay = (\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n)\nax.set_title(\"Ridge model, optimum regularization\")\nfor name, score in scores.items():\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\nax.legend(loc=\"upper left\")\n()\n\n\n<img alt=\"Ridge model, optimum regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_012.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_012.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Personal Consumption Expenditure"
            ],
            [
                "sklearn->Examples->Miscellaneous->Advanced Plotting With Partial Dependence->Plotting partial dependence for one feature"
            ],
            [
                "torch->Model Optimization->torch.compile Tutorial->Demonstrating Speedups"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ]
        ]
    },
    "1341423": {
        "jupyter_code_cell": "num = []\nfor x in data['SHOT_RESULT']:\n    if x == 'missed':\n        num.append(0)\n    if x == 'made':\n        num.append(1)       \nnba['mademissnum'] = num",
        "matched_tutorial_code_inds": [
            399,
            459,
            183,
            3657,
            6597
        ],
        "matched_tutorial_codes": [
            "accuracies = []\nexamples = []\n\n# Run test for each epsilon\nfor eps in epsilons:\n    acc, ex = test(model, , , eps)\n    accuracies.append(acc)\n    examples.append(ex)",
            "def categoryFromOutput():\n    top_n, top_i = .topk(1)\n    category_i = top_i[0].item()\n    return all_categories[category_i], category_i\n\nprint(categoryFromOutput())",
            "epochs = 5\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(, model, , )\n    test(, model, )\nprint(\"Done!\")",
            "# With indecies\ntemp = weather['tmpf']\n\nc = (temp - 32) * 5 / 9\nc.to_frame()",
            "# Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Image and Video->Adversarial Example Generation->Implementation->Run Attack"
            ],
            [
                "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN->Training->Preparing for Training"
            ],
            [
                "torch->Introduction to PyTorch->Quickstart->Optimizing the Model Parameters"
            ],
            [
                "pandas_toms_blog->Indexes->Flavors->Indexes for Easier Arithmetic, Analysis"
            ],
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor"
            ]
        ]
    },
    "951841": {
        "jupyter_code_cell": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.simplefilter('ignore',DeprecationWarning)\nimport seaborn as sns\nimport time\nimport copy\nfrom pylab import rcParams\nfrom tabulate import tabulate\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nfrom __future__ import print_function",
        "matched_tutorial_code_inds": [
            5256,
            1393,
            4700,
            1838,
            466
        ],
        "matched_tutorial_codes": [
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pandas_datareader as pdr\nimport seaborn\n\nimport statsmodels.api as sm\nfrom statsmodels.regression.rolling import RollingOLS\n\nseaborn.set_style(\"darkgrid\")\npd.plotting.register_matplotlib_converters()\n%matplotlib inline",
            "import matplotlib.pyplot as plt\nimport numpy as np\nidx = 5\nprint(\"Question: \", dataset[\"train\"][idx][\"question\"])\nprint(\"Answers: \" ,dataset[\"train\"][idx][\"answers\"])\nim = np.asarray(dataset[\"train\"][idx][\"image\"].resize((500,500)))\nplt.imshow(im)\nplt.show()\n\n\n<img alt=\"flava finetuning tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_flava_finetuning_tutorial_001.png\" srcset=\"../_images/sphx_glr_flava_finetuning_tutorial_001.png\"/>",
            "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom cycler import cycler\nmpl.rcParams['lines.linewidth'] = 2\nmpl.rcParams['lines.linestyle'] = '--'\ndata = np.random.randn(50)\nplt.plot(data)\n\n\n<img alt=\"customizing\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_customizing_001.png\" srcset=\"../../_images/sphx_glr_customizing_001.png, ../../_images/sphx_glr_customizing_001_2_0x.png 2.0x\"/>",
            "import scipy\nimport numpy as np\nfrom sklearn.model_selection import \nfrom sklearn.cluster import \nfrom sklearn.datasets import \nfrom sklearn.metrics import \n\nrng = (0)\nX, y = (random_state=rng)\nX = (X)\nX_train, X_test, _, y_test = (X, y, random_state=rng)\nkmeans = (n_init=\"auto\").fit(X_train)\nprint((kmeans.predict(X_test), y_test))",
            "import matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\nplt.figure()\nplt.plot(all_losses)\n\n\n<img alt=\"char rnn classification tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_char_rnn_classification_tutorial_001.png\" srcset=\"../_images/sphx_glr_char_rnn_classification_tutorial_001.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Rolling Least Squares"
            ],
            [
                "torch->Multimodality->TorchMultimodal Tutorial: Finetuning FLAVA->Steps"
            ],
            [
                "matplotlib->Tutorials->Introductory->Customizing Matplotlib with style sheets and rcParams->Runtime rc settings"
            ],
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 0.23->Scalability and stability improvements to KMeans"
            ],
            [
                "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN->Training->Plotting the Results"
            ]
        ]
    },
    "605365": {
        "jupyter_code_cell": "from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfor k in range (1,21):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    y_pred = knn.predict(X_test)\n    print(\"KNN \" + str(k) + \": \" + str(accuracy_score(y_test, y_pred)))\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_test)\ny_pred",
        "matched_tutorial_code_inds": [
            2345,
            2308,
            1823,
            3261,
            2018
        ],
        "matched_tutorial_codes": [
            "from sklearn.ensemble import \nfrom sklearn.metrics import , \n\n\nall_models = {}\ncommon_params = dict(\n    learning_rate=0.05,\n    n_estimators=200,\n    max_depth=2,\n    min_samples_leaf=9,\n    min_samples_split=9,\n)\nfor alpha in [0.05, 0.5, 0.95]:\n    gbr = (loss=\"quantile\", alpha=alpha, **common_params)\n    all_models[\"q %1.2f\" % alpha] = gbr.fit(X_train, y_train)",
            "from sklearn.ensemble import \nfrom sklearn.inspection import PartialDependenceDisplay\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nrng = (0)\n\nn_samples = 1000\nf_0 = rng.rand(n_samples)\nf_1 = rng.rand(n_samples)\nX = [f_0, f_1]\nnoise = rng.normal(loc=0.0, scale=0.01, size=n_samples)\n\n# y is positively correlated with f_0, and negatively correlated with f_1\ny = 5 * f_0 + (10 *  * f_0) - 5 * f_1 - (10 *  * f_1) + noise",
            "from sklearn.tree import \nfrom sklearn.model_selection import \nimport numpy as np\n\nn_samples, n_features = 1000, 20\nrng = (0)\nX = rng.randn(n_samples, n_features)\n# positive integer target correlated with X[:, 5] with many zeros:\ny = rng.poisson(lam=(X[:, 5]) / 2)\nX_train, X_test, y_train, y_test = (X, y, random_state=rng)\nregressor = (criterion=\"poisson\", random_state=0)\nregressor.fit(X_train, y_train)",
            "from sklearn.model_selection import (\n    ,\n    ,\n    ,\n    ,\n    ,\n    ,\n    ,\n    ,\n)\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import \n\nrng = (1338)\ncmap_data = plt.cm.Paired\ncmap_cv = plt.cm.coolwarm\nn_splits = 4",
            "from sklearn.cluster import \nimport matplotlib.pyplot as plt\n\nlabels = (graph, n_clusters=4, eigen_solver=\"arpack\")\nlabel_im = (mask.shape, -1.0)\nlabel_im[mask] = labels\n\nfig, axs = (nrows=1, ncols=2, figsize=(10, 5))\naxs[0].matshow(img)\naxs[1].matshow(label_im)\n\n()\n\n\n<img alt=\"plot segmentation toy\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_segmentation_toy_001.png\" srcset=\"../../_images/sphx_glr_plot_segmentation_toy_001.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Ensemble methods->Prediction Intervals for Gradient Boosting Regression->Fitting non-linear quantile and least squares regressors"
            ],
            [
                "sklearn->Examples->Ensemble methods->Monotonic Constraints"
            ],
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 0.24->New Poisson splitting criterion for DecisionTreeRegressor"
            ],
            [
                "sklearn->Examples->Model Selection->Visualizing cross-validation behavior in scikit-learn"
            ],
            [
                "sklearn->Examples->Clustering->Spectral clustering for image segmentation->Plotting four circles"
            ]
        ]
    },
    "1137947": {
        "jupyter_code_cell": "print('Train set dimension: {} rows, {} columns'.format(nwid_test_org.shape[0], nwid_test_org.shape[1]))\nnwid_test_org.head()\nnwid_train_org.describe()",
        "matched_tutorial_code_inds": [
            501,
            3741,
            6673,
            5573,
            459
        ],
        "matched_tutorial_codes": [
            "print('Checking the results of test dataset.')\naccu_test = evaluate()\nprint('test accuracy {:8.3f}'.format(accu_test))",
            "import string\n\ns = pd.Series(np.random.choice(list(string.ascii_letters), 100000))\nprint('{:0.2f} KB'.format(s.memory_usage(index=False) / 1000))",
            "print('Optimizer iterations')\nprint('- Original model:     %d' % res_ar.mle_retvals['iterations'])\nprint('- Concentrated model: %d' % res_ar_conc.mle_retvals['iterations'])",
            "print('Total number of trials:',  data.endog.iloc[:, 0].sum())\nprint('Parameters: ', res.params)\nprint('T-values: ', res.tvalues)",
            "def categoryFromOutput():\n    top_n, top_i = .topk(1)\n    category_i = top_i[0].item()\n    return all_categories[category_i], category_i\n\nprint(categoryFromOutput())"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Text->Text classification with the torchtext library->Evaluate the model with test dataset"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Categoricals"
            ],
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: SARIMAX"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Generalized Linear Models Overview->GLM: Binomial response data->Quantities of interest"
            ],
            [
                "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN->Training->Preparing for Training"
            ]
        ]
    },
    "777040": {
        "jupyter_code_cell": "import tensorflow as tf\nfrom tensorflow.contrib import learn\nfrom tensorflow.contrib import layers\nfrom tensorflow.contrib.learn.python import SKCompat\nfrom tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib\ntf.logging.set_verbosity(tf.logging.WARN) \nfeatures_tf = [layers.real_valued_column('', dimension=X_train.shape[1])]\nclf = SKCompat(\n            learn.DNNClassifier(hidden_units=[50], feature_columns=features_tf)\n        )\nclf.fit(X_train,y_train,steps=100)",
        "matched_tutorial_code_inds": [
            1826,
            3256,
            3412,
            3410,
            2050
        ],
        "matched_tutorial_codes": [
            "import numpy as np\nfrom sklearn.model_selection import \nfrom sklearn.linear_model import \nfrom sklearn.ensemble import \n\nn_samples, n_features = 1000, 20\nrng = (0)\nX = rng.randn(n_samples, n_features)\n# positive integer target correlated with X[:, 5] with many zeros:\ny = rng.poisson(lam=(X[:, 5]) / 2)\nX_train, X_test, y_train, y_test = (X, y, random_state=rng)\nglm = ()\ngbdt = (loss=\"poisson\", learning_rate=0.01)\nglm.fit(X_train, y_train)\ngbdt.fit(X_train, y_train)\nprint(glm.score(X_test, y_test))\nprint(gbdt.score(X_test, y_test))",
            "import numpy as np\nfrom sklearn import linear_model\nfrom sklearn.datasets import \nfrom sklearn.model_selection import \n\nn_samples_train, n_samples_test, n_features = 75, 150, 500\nX, y, coef = (\n    n_samples=n_samples_train + n_samples_test,\n    n_features=n_features,\n    n_informative=50,\n    shuffle=False,\n    noise=1.0,\n    coef=True,\n)\nX_train, X_test, y_train, y_test = (\n    X, y, train_size=n_samples_train, test_size=n_samples_test, shuffle=False\n)",
            "import numpy as np\nfrom sklearn.pipeline import \nfrom sklearn.linear_model import \n\nCs = (-5, 5, 20)\n\nunscaled_clf = (pca, (Cs=Cs))\nunscaled_clf.fit(X_train, y_train)\n\nscaled_clf = (scaler, pca, (Cs=Cs))\nscaled_clf.fit(X_train, y_train)\n\nprint(f\"Optimal C for the unscaled PCA: {unscaled_clf[-1].C_[0]:.4f}\\n\")\nprint(f\"Optimal C for the standardized data with PCA: {scaled_clf[-1].C_[0]:.2f}\")",
            "import pandas as pd\nfrom sklearn.decomposition import \n\npca = (n_components=2).fit(X_train)\nscaled_pca = (n_components=2).fit(scaled_X_train)\nX_train_transformed = pca.transform(X_train)\nX_train_std_transformed = scaled_pca.transform(scaled_X_train)\n\nfirst_pca_component = (\n    pca.components_[0], index=X.columns, columns=[\"without scaling\"]\n)\nfirst_pca_component[\"with scaling\"] = scaled_pca.components_[0]\nfirst_pca_component.plot.bar(\n    title=\"Weights of the first principal component\", figsize=(6, 8)\n)\n\n_ = ()\n\n\n<img alt=\"Weights of the first principal component\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_scaling_importance_002.png\" srcset=\"../../_images/sphx_glr_plot_scaling_importance_002.png\"/>",
            "import numpy as np\nfrom scipy import linalg\nfrom sklearn.datasets import \n\nn_samples = 60\nn_features = 20\n\nprng = (1)\nprec = (\n    n_features, alpha=0.98, smallest_coef=0.4, largest_coef=0.7, random_state=prng\n)\ncov = (prec)\nd = ((cov))\ncov /= d\ncov /= d[:, ]\nprec *= d\nprec *= d[:, ]\nX = prng.multivariate_normal((n_features), cov, size=n_samples)\nX -= X.mean(axis=0)\nX /= X.std(axis=0)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 0.23->Generalized Linear Models, and Poisson loss for gradient boosting"
            ],
            [
                "sklearn->Examples->Model Selection->Train error vs Test error->Generate sample data"
            ],
            [
                "sklearn->Examples->Preprocessing->Importance of Feature Scaling->Effect of rescaling on model\u2019s performance"
            ],
            [
                "sklearn->Examples->Preprocessing->Importance of Feature Scaling->Effect of rescaling on a PCA dimensional reduction"
            ],
            [
                "sklearn->Examples->Covariance estimation->Sparse inverse covariance estimation->Generate the data"
            ]
        ]
    },
    "1028081": {
        "jupyter_code_cell": "bymonth = complaintdates.groupby(by='Month').count()\nbymonth\nbyyear = complaintdates.groupby(by='Year').count()\nbyyear",
        "matched_tutorial_code_inds": [
            3864,
            6343,
            1741,
            155,
            1700
        ],
        "matched_tutorial_codes": [
            "most_common = df.occupation.value_counts().nlargest(100)\nmost_common",
            "sarimax_exog_res = SARIMAX(y, exog=x, order=(1, 0, 0), trend=\"c\").fit()\narima_exog_res = ARIMA(y, exog=x, order=(1, 0, 0), trend=\"c\").fit()",
            "imdb_train = data.fetch('imdb_train.txt')\nimdb_test = data.fetch('imdb_test.txt')",
            "m0 = t0.blocked_autorange()\nm1 = t1.blocked_autorange()\n\nprint(m0)\nprint(m1)\n\n\n\nOutput",
            "dof = len(before_sample) - 1\n\np_value = stats.distributions.t.cdf(t_value, dof)\n\nprint(\"The t value is {} and the p value is {}.\".format(t_value, p_value))"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Scaling->Dask"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Comparing trends and exogenous variables in SARIMAX, ARIMA and AutoReg->Using exog in SARIMAX and ARIMA"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ],
            [
                "torch->PyTorch Recipes->PyTorch Benchmark->Steps->4. Benchmarking with Blocked Autorange"
            ],
            [
                "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Paired Student\u2019s t-test on the AQIs->Calculating the test statistics"
            ]
        ]
    },
    "1434696": {
        "jupyter_code_cell": "num_examples, num_words, num_features = X.shape\nnum_tags = np.unique(Y).size\nsequence_lengths = np.full(num_examples, 0, dtype=np.int32)\nfor idx, row in enumerate(X):\n    count = 0\n    for word in row:\n        if np.all(word == -1):\n            break\n        count += 1\n    sequence_lengths[idx] = count\nsplit = 100\nx_test = X[-split:,:,:]\ny_test = Y[-split:,:]\ns_test = sequence_lengths[-split:]\nx = X[0:-split,:,:]\ny = Y[0:-split,:]\nsequence_lengths = sequence_lengths[0:-split]",
        "matched_tutorial_code_inds": [
            3084,
            2108,
            2298,
            2038,
            2093
        ],
        "matched_tutorial_codes": [
            "n_samples, n_features = data.shape\nprint(\n    f\"Embedding {n_samples} samples with dim {n_features} using various \"\n    \"random projections\"\n)\n\nn_components_range = ([300, 1_000, 10_000])\ndists = euclidean_distances(data, squared=True).ravel()\n\n# select only non-identical samples pairs\nnonzero = dists != 0\ndists = dists[nonzero]\n\nfor n_components in n_components_range:\n    t0 = ()\n    rp = (n_components=n_components)\n    projected_data = rp.fit_transform(data)\n    print(\n        f\"Projected {n_samples} samples from {n_features} to {n_components} in \"\n        f\"{() - t0:0.3f}s\"\n    )\n    if hasattr(rp, \"components_\"):\n        n_bytes = rp.components_.data.nbytes\n        n_bytes += rp.components_.indices.nbytes\n        print(f\"Random matrix with size: {n_bytes / 1e6:0.3f} MB\")\n\n    projected_dists = euclidean_distances(projected_data, squared=True).ravel()[nonzero]\n\n    ()\n    min_dist = min(projected_dists.min(), dists.min())\n    max_dist = max(projected_dists.max(), dists.max())\n    (\n        dists,\n        projected_dists,\n        gridsize=100,\n        cmap=plt.cm.PuBu,\n        extent=[min_dist, max_dist, min_dist, max_dist],\n    )\n    (\"Pairwise squared distances in original space\")\n    (\"Pairwise squared distances in projected space\")\n    (\"Pairwise distances distribution for n_components=%d\" % n_components)\n    cb = ()\n    cb.set_label(\"Sample pairs counts\")\n\n    rates = projected_dists / dists\n    print(f\"Mean distances rate: {(rates):.2f} ({(rates):.2f})\")\n\n    ()\n    (rates, bins=50, range=(0.0, 2.0), edgecolor=\"k\", density=True)\n    (\"Squared distances rate: projected / original\")\n    (\"Distribution of samples pairs\")\n    (\"Histogram of pairwise distance rates for n_components=%d\" % n_components)\n\n    # TODO: compute the expected value of eps and add them to the previous plot\n    # as vertical lines / region\n\n()\n\n\n\n<img alt=\"Pairwise distances distribution for n_components=300\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_003.png\" srcset=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_003.png\"/>\n<img alt=\"Histogram of pairwise distance rates for n_components=300\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_004.png\" srcset=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_004.png\"/>\n<img alt=\"Pairwise distances distribution for n_components=1000\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_005.png\" srcset=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_005.png\"/>\n<img alt=\"Histogram of pairwise distance rates for n_components=1000\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_006.png\" srcset=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_006.png\"/>\n<img alt=\"Pairwise distances distribution for n_components=10000\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_007.png\" srcset=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_007.png\"/>\n<img alt=\"Histogram of pairwise distance rates for n_components=10000\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_008.png\" srcset=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_008.png\"/>",
            "n_row, n_col = 2, 3\nn_components = n_row * n_col\nimage_shape = (64, 64)\n\n\ndef plot_gallery(title, images, n_col=n_col, n_row=n_row, cmap=plt.cm.gray):\n    fig, axs = (\n        nrows=n_row,\n        ncols=n_col,\n        figsize=(2.0 * n_col, 2.3 * n_row),\n        facecolor=\"white\",\n        constrained_layout=True,\n    )\n    fig.set_constrained_layout_pads(w_pad=0.01, h_pad=0.02, hspace=0, wspace=0)\n    fig.set_edgecolor(\"black\")\n    fig.suptitle(title, size=16)\n    for ax, vec in zip(axs.flat, images):\n        vmax = max(vec.max(), -vec.min())\n        im = ax.imshow(\n            vec.reshape(image_shape),\n            cmap=cmap,\n            interpolation=\"nearest\",\n            vmin=-vmax,\n            vmax=vmax,\n        )\n        ax.axis(\"off\")\n\n    fig.colorbar(im, ax=axs, orientation=\"horizontal\", shrink=0.99, aspect=40, pad=0.01)\n    ()",
            "feature_importance = reg.feature_importances_\nsorted_idx = (feature_importance)\npos = (sorted_idx.shape[0]) + 0.5\nfig = (figsize=(12, 6))\n(1, 2, 1)\n(pos, feature_importance[sorted_idx], align=\"center\")\n(pos, (diabetes.feature_names)[sorted_idx])\n(\"Feature Importance (MDI)\")\n\nresult = (\n    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_idx = result.importances_mean.argsort()\n(1, 2, 2)\n(\n    result.importances[sorted_idx].T,\n    vert=False,\n    labels=(diabetes.feature_names)[sorted_idx],\n)\n(\"Permutation Importance (test set)\")\nfig.tight_layout()\n()\n\n\n<img alt=\"Feature Importance (MDI), Permutation Importance (test set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\"/>",
            "n_features = 100\n# simulation covariance matrix (AR(1) process)\nr = 0.1\nreal_cov = (r ** (n_features))\ncoloring_matrix = (real_cov)\n\nn_samples_range = (6, 31, 1)\nrepeat = 100\nlw_mse = ((n_samples_range.size, repeat))\noa_mse = ((n_samples_range.size, repeat))\nlw_shrinkage = ((n_samples_range.size, repeat))\noa_shrinkage = ((n_samples_range.size, repeat))\nfor i, n_samples in enumerate(n_samples_range):\n    for j in range(repeat):\n        X = ((size=(n_samples, n_features)), coloring_matrix.T)\n\n        lw = (store_precision=False, assume_centered=True)\n        lw.fit(X)\n        lw_mse[i, j] = lw.error_norm(real_cov, scaling=False)\n        lw_shrinkage[i, j] = lw.shrinkage_\n\n        oa = (store_precision=False, assume_centered=True)\n        oa.fit(X)\n        oa_mse[i, j] = oa.error_norm(real_cov, scaling=False)\n        oa_shrinkage[i, j] = oa.shrinkage_\n\n# plot MSE\n(2, 1, 1)\n(\n    n_samples_range,\n    lw_mse.mean(1),\n    yerr=lw_mse.std(1),\n    label=\"Ledoit-Wolf\",\n    color=\"navy\",\n    lw=2,\n)\n(\n    n_samples_range,\n    oa_mse.mean(1),\n    yerr=oa_mse.std(1),\n    label=\"OAS\",\n    color=\"darkorange\",\n    lw=2,\n)\n(\"Squared error\")\n(loc=\"upper right\")\n(\"Comparison of covariance estimators\")\n(5, 31)\n\n# plot shrinkage coefficient\n(2, 1, 2)\n(\n    n_samples_range,\n    lw_shrinkage.mean(1),\n    yerr=lw_shrinkage.std(1),\n    label=\"Ledoit-Wolf\",\n    color=\"navy\",\n    lw=2,\n)\n(\n    n_samples_range,\n    oa_shrinkage.mean(1),\n    yerr=oa_shrinkage.std(1),\n    label=\"OAS\",\n    color=\"darkorange\",\n    lw=2,\n)\n(\"n_samples\")\n(\"Shrinkage\")\n(loc=\"lower right\")\n(()[0], 1.0 + (()[1] - ()[0]) / 10.0)\n(5, 31)\n\n()\n\n\n<img alt=\"Comparison of covariance estimators\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_lw_vs_oas_001.png\" srcset=\"../../_images/sphx_glr_plot_lw_vs_oas_001.png\"/>",
            "n_nodes = clf.tree_.node_count\nchildren_left = clf.tree_.children_left\nchildren_right = clf.tree_.children_right\nfeature = clf.tree_.feature\nthreshold = clf.tree_.threshold\n\nnode_depth = (shape=n_nodes, dtype=)\nis_leaves = (shape=n_nodes, dtype=bool)\nstack = [(0, 0)]  # start with the root node id (0) and its depth (0)\nwhile len(stack)  0:\n    # `pop` ensures each node is only visited once\n    node_id, depth = stack.pop()\n    node_depth[node_id] = depth\n\n    # If the left and right child of a node is not the same we have a split\n    # node\n    is_split_node = children_left[node_id] != children_right[node_id]\n    # If a split node, append left and right children and depth to `stack`\n    # so we can loop through them\n    if is_split_node:\n        stack.append((children_left[node_id], depth + 1))\n        stack.append((children_right[node_id], depth + 1))\n    else:\n        is_leaves[node_id] = True\n\nprint(\n    \"The binary tree structure has {n} nodes and has \"\n    \"the following tree structure:\\n\".format(n=n_nodes)\n)\nfor i in range(n_nodes):\n    if is_leaves[i]:\n        print(\n            \"{space}node={node} is a leaf node.\".format(\n                space=node_depth[i] * \"\\t\", node=i\n            )\n        )\n    else:\n        print(\n            \"{space}node={node} is a split node: \"\n            \"go to node {left} if X[:, {feature}] &lt;= {threshold} \"\n            \"else to node {right}.\".format(\n                space=node_depth[i] * \"\\t\",\n                node=i,\n                left=children_left[i],\n                feature=feature[i],\n                threshold=threshold[i],\n                right=children_right[i],\n            )\n        )"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Miscellaneous->The Johnson-Lindenstrauss bound for embedding with random projections->Empirical validation"
            ],
            [
                "sklearn->Examples->Decomposition->Faces dataset decompositions->Dataset preparation"
            ],
            [
                "sklearn->Examples->Ensemble methods->Gradient Boosting regression->Plot feature importance"
            ],
            [
                "sklearn->Examples->Covariance estimation->Ledoit-Wolf vs OAS estimation"
            ],
            [
                "sklearn->Examples->Decision Trees->Understanding the decision tree structure->Tree structure"
            ]
        ]
    },
    "899572": {
        "jupyter_code_cell": "from numpy.random import rand\nfac1, fac2, fac3 = np.random.rand(3, 1000)\nticker_subset = tickers.take(np.random.permutation(N)[:1000])\nport = Series(0.7 * fac1 - 1.2 * fac2 + 0.3 * fac3 + rand(1000),\n              index=ticker_subset)\nfactors = DataFrame({'f1': fac1, 'f2': fac2, 'f3': fac3},\n                    index=ticker_subset)\nfactors.corrwith(port)",
        "matched_tutorial_code_inds": [
            4760,
            3170,
            1780,
            2134,
            1895
        ],
        "matched_tutorial_codes": [
            "from numpy.random import randn\n\nz = randn(10)\n\nfig, ax = plt.subplots()\nred_dot, = ax.plot(z, \"ro\", markersize=15)\n# Put a white cross over some of the data.\nwhite_cross, = ax.plot(z[:5], \"w+\", markeredgewidth=3, markersize=15)\n\nax.legend([red_dot, (red_dot, white_cross)], [\"Attr A\", \"Attr A+B\"])\n\n\n<img alt=\"legend guide\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_legend_guide_009.png\" srcset=\"../../_images/sphx_glr_legend_guide_009.png, ../../_images/sphx_glr_legend_guide_009_2_0x.png 2.0x\"/>",
            "from itertools import \n\nfig, ax = (figsize=(6, 6))\n\n(\n    fpr[\"micro\"],\n    tpr[\"micro\"],\n    label=f\"micro-average ROC curve (AUC = {roc_auc['micro']:.2f})\",\n    color=\"deeppink\",\n    linestyle=\":\",\n    linewidth=4,\n)\n\n(\n    fpr[\"macro\"],\n    tpr[\"macro\"],\n    label=f\"macro-average ROC curve (AUC = {roc_auc['macro']:.2f})\",\n    color=\"navy\",\n    linestyle=\":\",\n    linewidth=4,\n)\n\ncolors = ([\"aqua\", \"darkorange\", \"cornflowerblue\"])\nfor class_id, color in zip(range(n_classes), colors):\n    (\n        y_onehot_test[:, class_id],\n        y_score[:, class_id],\n        name=f\"ROC curve for {target_names[class_id]}\",\n        color=color,\n        ax=ax,\n    )\n\n([0, 1], [0, 1], \"k--\", label=\"ROC curve for chance level (AUC = 0.5)\")\n(\"square\")\n(\"False Positive Rate\")\n(\"True Positive Rate\")\n(\"Extension of Receiver Operating Characteristic\\nto One-vs-Rest multiclass\")\n()\n()\n\n\n<img alt=\"Extension of Receiver Operating Characteristic to One-vs-Rest multiclass\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_roc_003.png\" srcset=\"../../_images/sphx_glr_plot_roc_003.png\"/>",
            "from sklearn.ensemble import \nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simple regression function for X * cos(X)\nrng = (42)\nX_1d = (0, 10, num=2000)\nX = X_1d.reshape(-1, 1)\ny = X_1d * (X_1d) + rng.normal(scale=X_1d / 3)\n\nquantiles = [0.95, 0.5, 0.05]\nparameters = dict(loss=\"quantile\", max_bins=32, max_iter=50)\nhist_quantiles = {\n    f\"quantile={quantile:.2f}\": (\n        **parameters, quantile=quantile\n    ).fit(X, y)\n    for quantile in quantiles\n}\n\nfig, ax = ()\nax.plot(X_1d, y, \"o\", alpha=0.5, markersize=1)\nfor quantile, hist in hist_quantiles.items():\n    ax.plot(X_1d, hist.predict(X), label=quantile)\n_ = ax.legend(loc=\"lower left\")\n\n\n<img alt=\"plot release highlights 1 1 0\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_release_highlights_1_1_0_001.png\" srcset=\"../../_images/sphx_glr_plot_release_highlights_1_1_0_001.png\"/>",
            "from sklearn.decomposition import \n\nprint(\"Learning the dictionary...\")\nt0 = ()\ndico = (\n    # increase to 300 for higher quality results at the cost of slower\n    # training times.\n    n_components=50,\n    batch_size=200,\n    alpha=1.0,\n    max_iter=10,\n)\nV = dico.fit(data).components_\ndt = () - t0\nprint(f\"{dico.n_iter_} iterations / {dico.n_steps_} steps in {dt:.2f}.\")\n\n(figsize=(4.2, 4))\nfor i, comp in enumerate(V[:100]):\n    (10, 10, i + 1)\n    (comp.reshape(patch_size), cmap=plt.cm.gray_r, interpolation=\"nearest\")\n    (())\n    (())\n(\n    \"Dictionary learned from face patches\\n\"\n    + \"Train time %.1fs on %d patches\" % (dt, len(data)),\n    fontsize=16,\n)\n(0.08, 0.02, 0.92, 0.85, 0.08, 0.23)\n\n\n<img alt=\"Dictionary learned from face patches Train time 16.0s on 22692 patches\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_image_denoising_002.png\" srcset=\"../../_images/sphx_glr_plot_image_denoising_002.png\"/>",
            "from matplotlib import cm\nimport matplotlib.pyplot as plt\n\n()\ny_unique = (y)\ncolors = cm.rainbow((0.0, 1.0, y_unique.size))\nfor this_y, color in zip(y_unique, colors):\n    this_X = X_train[y_train == this_y]\n    this_sw = sw_train[y_train == this_y]\n    (\n        this_X[:, 0],\n        this_X[:, 1],\n        s=this_sw * 50,\n        c=color[, :],\n        alpha=0.5,\n        edgecolor=\"k\",\n        label=\"Class %s\" % this_y,\n    )\n(loc=\"best\")\n(\"Data\")\n\n()\n\norder = ((prob_pos_clf,))\n(prob_pos_clf[order], \"r\", label=\"No calibration (%1.3f)\" % clf_score)\n(\n    prob_pos_isotonic[order],\n    \"g\",\n    linewidth=3,\n    label=\"Isotonic calibration (%1.3f)\" % clf_isotonic_score,\n)\n(\n    prob_pos_sigmoid[order],\n    \"b\",\n    linewidth=3,\n    label=\"Sigmoid calibration (%1.3f)\" % clf_sigmoid_score,\n)\n(\n    (0, y_test.size, 51)[1::2],\n    y_test[order].reshape(25, -1).mean(1),\n    \"k\",\n    linewidth=3,\n    label=r\"Empirical\",\n)\n([-0.05, 1.05])\n(\"Instances sorted according to predicted probability (uncalibrated GNB)\")\n(\"P(y=1)\")\n(loc=\"upper left\")\n(\"Gaussian naive Bayes probabilities\")\n\n()\n\n\n\n<img alt=\"Data\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_calibration_001.png\" srcset=\"../../_images/sphx_glr_plot_calibration_001.png\"/>\n<img alt=\"Gaussian naive Bayes probabilities\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_calibration_002.png\" srcset=\"../../_images/sphx_glr_plot_calibration_002.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Intermediate->Legend guide->Legend Handlers"
            ],
            [
                "sklearn->Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->One-vs-Rest multiclass ROC->Plot all OvR ROC curves together"
            ],
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 1.1->Quantile loss in"
            ],
            [
                "sklearn->Examples->Decomposition->Image denoising using dictionary learning->Learn the dictionary from reference patches"
            ],
            [
                "sklearn->Examples->Calibration->Probability calibration of classifiers->Plot data and the predicted probabilities"
            ]
        ]
    },
    "452802": {
        "jupyter_code_cell": "import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn import cross_validation",
        "matched_tutorial_code_inds": [
            2984,
            6174,
            2037,
            6478,
            5565
        ],
        "matched_tutorial_codes": [
            "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import \nfrom sklearn.neural_network import \nfrom sklearn.preprocessing import \nfrom sklearn.pipeline import \nfrom sklearn.tree import \nfrom sklearn.inspection import PartialDependenceDisplay",
            "import numpy as np\nimport pandas as pd\n\nfrom statsmodels.graphics.tsaplots import plot_predict\nfrom statsmodels.tsa.arima_process import arma_generate_sample\nfrom statsmodels.tsa.arima.model import ARIMA\n\nnp.random.seed(12345)",
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.linalg import , \n\nfrom sklearn.covariance import , \n\n(0)",
            "import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\nplt.rc(\"figure\", figsize=(16,8))\nplt.rc(\"font\", size=14)",
            "import numpy as np\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom matplotlib import pyplot as plt\n\nplt.rc(\"figure\", figsize=(16,8))\nplt.rc(\"font\", size=14)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Miscellaneous->Advanced Plotting With Partial Dependence"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Artificial Data"
            ],
            [
                "sklearn->Examples->Covariance estimation->Ledoit-Wolf vs OAS estimation"
            ],
            [
                "statsmodels->Examples->State space models->Seasonality in Time Series Data"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Generalized Linear Models Overview"
            ]
        ]
    },
    "1016193": {
        "jupyter_code_cell": "%%time\nforest = RandomForestClassifier(criterion='gini', min_samples_leaf=1, min_samples_split=3, n_estimators=51, random_state=123)\nforest.fit(X_train, yt.ravel())\nprint('--------------------------------')\nprint(\"My Train Score: \", forest.score(X_train, y_train))\nprint('--------------------------------')\ntest_X['churn'] = test_y['churn']\ntest_X.head(2)",
        "matched_tutorial_code_inds": [
            6050,
            551,
            513,
            3635,
            6518
        ],
        "matched_tutorial_codes": [
            "%%javascript\nIPython.OutputArea.prototype._should_scroll = function(lines) {\n    return false;\n}\n\n\n\n\n\n\n\n\n<script type=\"text/javascript\">\nvar element = document.currentScript.previousSibling.previousSibling;\nIPython.OutputArea.prototype._should_scroll = function(lines) {\n    return false;\n}\n\n</script>",
            "%%bash\npip install gym-super-mario-bros==7.4.0",
            "%%bash\npip3 install gymnasium[classic_control]",
            "%config InlineBackend.figure_format = 'png'\nflights = (df[['fl_date', 'tail_num', 'dep_time', 'dep_delay']]\n           .dropna()\n           .sort_values('dep_time')\n           .loc[lambda x: x.dep_delay &lt; 500]\n           .assign(turn = lambda x:\n                x.groupby(['fl_date', 'tail_num'])\n                 .dep_time\n                 .transform('rank').astype(int)))\n\nfig, ax = plt.subplots(figsize=(15, 5))\nsns.boxplot(x='turn', y='dep_delay', data=flights, ax=ax)\nax.set_ylim(-50, 50)\nsns.despine()",
            "%matplotlib inline\n\nfrom importlib import reload\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\nfrom scipy.stats import invwishart, invgamma\n\n# Get the macro dataset\ndta = sm.datasets.macrodata.load_pandas().data\ndta.index = pd.date_range('1959Q1', '2009Q3', freq='QS')"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->Copulas"
            ],
            [
                "torch->Reinforcement Learning->Train a Mario-playing RL Agent"
            ],
            [
                "torch->Reinforcement Learning->Reinforcement Learning (DQN) Tutorial"
            ],
            [
                "pandas_toms_blog->Method Chaining->Method Chaining->Application"
            ],
            [
                "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing"
            ]
        ]
    },
    "124416": {
        "jupyter_code_cell": "REPEAL_COLOR = '#e57373'  \nKEEP_COLOR = '#bdbdbd'  \nax = pov_wide_n24.plot.bar(\n    color=[REPEAL_COLOR, KEEP_COLOR, UBI_COLOR, TUBI_COLOR])\nax.set_xticklabels(('Extreme', 'FPL', '$10k pp'))\nsns.despine(left=True, bottom=True)\nax.legend_.remove()\nLEFT_X = -0.27\nax.text(LEFT_X, 0.24, 'Repeal CTC', horizontalalignment='left',\n        color=REPEAL_COLOR, size=8, weight='bold')\nax.text(LEFT_X, 0.23, 'No change to CTC', horizontalalignment='left',\n        color=KEEP_COLOR, size=8, weight='bold')\nax.text(LEFT_X, 0.22, rn_ubi_str + ' child benefit', \n        horizontalalignment='left', color=UBI_COLOR, size=8, weight='bold')\nax.text(LEFT_X, 0.21, '$2,000 child benefit', \n        horizontalalignment='left', color=TUBI_COLOR, size=8, weight='bold')\nax.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(\n    lambda y, _: '{:.0%}'.format(y)))\nplt.title('Children under poverty lines*, after taxes and transfers', \n          loc='left')\nax.text(LEFT_X, 0.06, '*Not official poverty lines',\n        horizontalalignment='left', color=KEEP_COLOR, size=6)\nax.set(xlabel='')\nax.grid(color='#f5f5f5', axis='y')\nplt.show()\npov_wide_n24['ubi_reduction'] = 1 - (\n    pov_wide_n24.afti_ubi / pov_wide_n24.afti_keep)\npov_wide_n24['tubi_reduction'] = 1 - (\n    pov_wide_n24.afti_tubi / pov_wide_n24.afti_keep)\npov_wide_n24 * 100",
        "matched_tutorial_code_inds": [
            1118,
            5956,
            5911,
            5917,
            1615
        ],
        "matched_tutorial_codes": [
            "SHAPE_COUNT = 20\ndynamic_sizes = deepcopy(input_size)\n\ninputs1: List[] = []\ninputs2: List[] = []\ngrad_outputs: List[] = []\n\n\n# Create some random shapes\nfor _ in range(SHAPE_COUNT):\n    dynamic_sizes[0] = input_size[0] + random.randrange(-2, 3)\n    dynamic_sizes[1] = input_size[1] + random.randrange(-2, 3)\n    input = (*dynamic_sizes, device=device, =, requires_grad=True)\n    inputs1.append(input)\n    inputs2.append((input))\n    grad_outputs.append((input))",
            "kidney_lm = ols(\"np.log(Days+1) ~ C(Duration) * C(Weight)\", data=kt).fit()\n\ntable10 = anova_lm(kidney_lm)\n\nprint(\n    anova_lm(ols(\"np.log(Days+1) ~ C(Duration) + C(Weight)\", data=kt).fit(), kidney_lm)\n)\nprint(\n    anova_lm(\n        ols(\"np.log(Days+1) ~ C(Duration)\", data=kt).fit(),\n        ols(\"np.log(Days+1) ~ C(Duration) + C(Weight, Sum)\", data=kt).fit(),\n    )\n)\nprint(\n    anova_lm(\n        ols(\"np.log(Days+1) ~ C(Weight)\", data=kt).fit(),\n        ols(\"np.log(Days+1) ~ C(Duration) + C(Weight, Sum)\", data=kt).fit(),\n    )\n)",
            "infl = interM_lm.get_influence()\nresid = infl.resid_studentized_internal\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        resid[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\nplt.xlabel(\"X\")\nplt.ylabel(\"standardized resids\")",
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "fourier_gaussian = ndimage.fourier_gaussian(xray_image, sigma=0.05)\n\nx_prewitt = ndimage.prewitt(fourier_gaussian, axis=0)\ny_prewitt = ndimage.prewitt(fourier_gaussian, axis=1)\n\nxray_image_canny = np.hypot(x_prewitt, y_prewitt)\n\nxray_image_canny *= 255.0 / np.max(xray_image_canny)\n\nprint(\"The data type - \", xray_image_canny.dtype)"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Model Optimization->Getting Started - Accelerate Your Scripts with nvFuser->nvFuser & Dynamic Shapes"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA->Two-way ANOVA"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "numpy->NumPy Applications->X-ray image processing->Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters->The Canny filter"
            ]
        ]
    },
    "741644": {
        "jupyter_code_cell": "print 'AIC:{}\\nBIC:{}'.format(arma.aic,arma.bic)\nresids = arma.resid.squeeze()\nplt.title('Residuals')\nplt.plot(resids)",
        "matched_tutorial_code_inds": [
            6673,
            6659,
            501,
            3829,
            3702
        ],
        "matched_tutorial_codes": [
            "print('Optimizer iterations')\nprint('- Original model:     %d' % res_ar.mle_retvals['iterations'])\nprint('- Concentrated model: %d' % res_ar_conc.mle_retvals['iterations'])",
            "mod = LocalLevel(dta.infl)\nres = mod.fit(disp=False)\nprint(res.summary())",
            "print('Checking the results of test dataset.')\naccu_test = evaluate()\nprint('test accuracy {:8.3f}'.format(accu_test))",
            "ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: SARIMAX"
            ],
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Typical approach"
            ],
            [
                "torch->Text->Text classification with the torchtext library->Evaluate the model with test dataset"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ]
        ]
    },
    "400358": {
        "jupyter_code_cell": "from sklearn.linear_model import LogisticRegression\nlogr = LogisticRegression()\nscores = [log_loss_kfold(logr,\n                         features.iloc[train_idx],\n                         sfcrimes[\"CatCodes\"].iloc[train_idx],\n                         features.iloc[test_idx],\n                         sfcrimes[\"CatCodes\"].iloc[test_idx])\n          for train_idx, test_idx in kf]\nsum(scores)/len(scores)\ng = sns.PairGrid(data = sfcrimes, x_vars = ['X'], y_vars = ['Y'], size = 5.5*1.3)\ng = g.map(plt.scatter)",
        "matched_tutorial_code_inds": [
            5476,
            2907,
            3166,
            2787,
            3005
        ],
        "matched_tutorial_codes": [
            "from statsmodels.graphics.api import abline_plot\n\nfig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111, ylabel=\"Observed Values\", xlabel=\"Fitted Values\")\nax.scatter(yhat, y)\ny_vs_yhat = sm.OLS(y, sm.add_constant(yhat, prepend=True)).fit()\nfig = abline_plot(model_results=y_vs_yhat, ax=ax)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_discrete_choice_example_55_0.png\" src=\"../../../_images/examples_notebooks_generated_discrete_choice_example_55_0.png\"/>",
            "from sklearn.base import clone\n\ninteraction_cst = [[i] for i in range(X_train.shape[1])]\nhgbdt_model_without_interactions = (\n    clone(hgbdt_model)\n    .set_params(histgradientboostingregressor__interaction_cst=interaction_cst)\n    .fit(X_train, y_train)\n)\nprint(f\"Test R2 score: {hgbdt_model_without_interactions.score(X_test, y_test):.2f}\")",
            "from sklearn.metrics import , \n\n# store the fpr, tpr, and roc_auc for all averaging strategies\nfpr, tpr, roc_auc = dict(), dict(), dict()\n# Compute micro-average ROC curve and ROC area\nfpr[\"micro\"], tpr[\"micro\"], _ = (y_onehot_test.ravel(), y_score.ravel())\nroc_auc[\"micro\"] = (fpr[\"micro\"], tpr[\"micro\"])\n\nprint(f\"Micro-averaged One-vs-Rest ROC AUC score:\\n{roc_auc['micro']:.2f}\")",
            "from sklearn.linear_model import \n\n\nmask_train = df_train[\"ClaimAmount\"]  0\nmask_test = df_test[\"ClaimAmount\"]  0\n\nglm_sev = (alpha=10.0, solver=\"newton-cholesky\")\n\nglm_sev.fit(\n    X_train[mask_train.values],\n    df_train.loc[mask_train, \"AvgClaimAmount\"],\n    sample_weight=df_train.loc[mask_train, \"ClaimNb\"],\n)\n\nscores = score_estimator(\n    glm_sev,\n    X_train[mask_train.values],\n    X_test[mask_test.values],\n    df_train[mask_train],\n    df_test[mask_test],\n    target=\"AvgClaimAmount\",\n    weights=\"ClaimNb\",\n)\nprint(\"Evaluation of GammaRegressor on target AvgClaimAmount\")\nprint(scores)",
            "from sklearn.model_selection import LearningCurveDisplay\n\n_, ax = ()\n\nsvr = (kernel=\"rbf\", C=1e1, gamma=0.1)\nkr = (kernel=\"rbf\", alpha=0.1, gamma=0.1)\n\ncommon_params = {\n    \"X\": X[:100],\n    \"y\": y[:100],\n    \"train_sizes\": (0.1, 1, 10),\n    \"scoring\": \"neg_mean_squared_error\",\n    \"negate_score\": True,\n    \"score_name\": \"Mean Squared Error\",\n    \"std_display_style\": None,\n    \"ax\": ax,\n}\n\n(svr, **common_params)\n(kr, **common_params)\nax.set_title(\"Learning curves\")\nax.legend(handles=ax.get_legend_handles_labels()[0], labels=[\"SVR\", \"KRR\"])\n\n()\n\n\n<img alt=\"Learning curves\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_kernel_ridge_regression_003.png\" srcset=\"../../_images/sphx_glr_plot_kernel_ridge_regression_003.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution"
            ],
            [
                "sklearn->Examples->Inspection->Partial Dependence and Individual Conditional Expectation Plots->1-way partial dependence with different models"
            ],
            [
                "sklearn->Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->One-vs-Rest multiclass ROC->ROC curve using micro-averaged OvR"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Tweedie regression on insurance claims->Severity Model -  Gamma distribution"
            ],
            [
                "sklearn->Examples->Miscellaneous->Comparison of kernel ridge regression and SVR->Visualize the learning curves"
            ]
        ]
    },
    "871432": {
        "jupyter_code_cell": "df.loc[df['Cabin'].isnull(), 'Cabin'] = 'U0'\ndf.head()\ndf.loc[df['Fare'].isnull()].shape",
        "matched_tutorial_code_inds": [
            3656,
            6804,
            2801,
            3802,
            1484
        ],
        "matched_tutorial_codes": [
            "weather2 = weather.reset_index()\nweather2[weather2['station'] == 'DSM'].head()",
            "endog = macrodata['infl']\nendog.plot(figsize=(15, 5))",
            "X = survey.data[survey.feature_names]\nX.describe(include=\"all\")\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "scores = pd.DataFrame(est.cv_results_)\nscores.head()",
            "china_masked = nbcases_ma[locations[:, 1] == \"China\"].sum(axis=0)\nchina_masked"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Indexes->Flavors->Row Slicing"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "numpy->NumPy Features->Masked Arrays->Missing data"
            ]
        ]
    },
    "1214459": {
        "jupyter_code_cell": "nx.degree_assortativity_coefficient(G)\nnx.draw(G)",
        "matched_tutorial_code_inds": [
            660,
            4272,
            2312,
            4271,
            3792
        ],
        "matched_tutorial_codes": [
            "interp = (rn18)\n(input)\nprint(interp.summary(True))",
            "nonlinear_constraint = NonlinearConstraint(cons_f, -np.inf, 1, jac='2-point', hess=BFGS())",
            "gbdt_with_monotonic_cst = (monotonic_cst=[1, -1])\ngbdt_with_monotonic_cst.fit(X, y)",
            "nonlinear_constraint = NonlinearConstraint(cons_f, -np.inf, 1, jac=cons_J, hess='2-point')",
            "sns.countplot(x='cut', data=df)\nsns.despine()\nplt.tight_layout()"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Code Transforms with FX->(beta) Building a Simple CPU Performance Profiler with FX->Investigating the Performance of ResNet18"
            ],
            [
                "scipy->Optimization (scipy.optimize)->Constrained minimization of multivariate scalar functions (minimize)->Trust-Region Constrained Algorithm (method='trust-constr')->Defining Nonlinear Constraints:"
            ],
            [
                "sklearn->Examples->Ensemble methods->Monotonic Constraints"
            ],
            [
                "scipy->Optimization (scipy.optimize)->Constrained minimization of multivariate scalar functions (minimize)->Trust-Region Constrained Algorithm (method='trust-constr')->Defining Nonlinear Constraints:"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ]
        ]
    },
    "603279": {
        "jupyter_code_cell": "import pandas as pd\ndocument = ET.parse( './data/mondial_database.xml' )\ninfmort = []\nfor child in document.getroot():\n    try:\n        infmort.append([child.find('name').text, float(child.find('infant_mortality').text)])\n    except AttributeError:\n        continue\ndf_infmort = pd.DataFrame(infmort, columns=['country', 'infant_mortality'])\ndf_infmort.sort('infant_mortality', ascending=True).head(10)",
        "matched_tutorial_code_inds": [
            1393,
            2264,
            2528,
            3225,
            3367
        ],
        "matched_tutorial_codes": [
            "import matplotlib.pyplot as plt\nimport numpy as np\nidx = 5\nprint(\"Question: \", dataset[\"train\"][idx][\"question\"])\nprint(\"Answers: \" ,dataset[\"train\"][idx][\"answers\"])\nim = np.asarray(dataset[\"train\"][idx][\"image\"].resize((500,500)))\nplt.imshow(im)\nplt.show()\n\n\n<img alt=\"flava finetuning tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_flava_finetuning_tutorial_001.png\" srcset=\"../_images/sphx_glr_flava_finetuning_tutorial_001.png\"/>",
            "import pandas as pd\n\nforest_importances = (importances, index=feature_names)\n\nfig, ax = ()\nforest_importances.plot.bar(yerr=std, ax=ax)\nax.set_title(\"Feature importances using MDI\")\nax.set_ylabel(\"Mean decrease in impurity\")\nfig.tight_layout()\n\n\n<img alt=\"Feature importances using MDI\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_forest_importances_001.png\" srcset=\"../../_images/sphx_glr_plot_forest_importances_001.png\"/>",
            "import pandas as pd\n\ndf = (grid_search.cv_results_)[\n    [\"param_n_components\", \"param_covariance_type\", \"mean_test_score\"]\n]\ndf[\"mean_test_score\"] = -df[\"mean_test_score\"]\ndf = df.rename(\n    columns={\n        \"param_n_components\": \"Number of components\",\n        \"param_covariance_type\": \"Type of covariance\",\n        \"mean_test_score\": \"BIC score\",\n    }\n)\ndf.sort_values(by=\"BIC score\").head()\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "import pandas as pd\n\nresults_df = (search.cv_results_)\nresults_df = results_df.sort_values(by=[\"rank_test_score\"])\nresults_df = results_df.set_index(\n    results_df[\"params\"].apply(lambda x: \"_\".join(str(val) for val in x.values()))\n).rename_axis(\"kernel\")\nresults_df[[\"params\", \"rank_test_score\", \"mean_test_score\", \"std_test_score\"]]\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "import pandas as pd\n\ncv_results = (search_cv.cv_results_)\ncv_results = cv_results.sort_values(\"mean_test_score\", ascending=False)\ncv_results[\n    [\n        \"mean_test_score\",\n        \"std_test_score\",\n        \"param_preprocessor__num__imputer__strategy\",\n        \"param_preprocessor__cat__selector__percentile\",\n        \"param_classifier__C\",\n    ]\n].head(5)\n\n\n\n\n\n\n\n\n<br/>\n<br/>"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Multimodality->TorchMultimodal Tutorial: Finetuning FLAVA->Steps"
            ],
            [
                "sklearn->Examples->Ensemble methods->Feature importances with a forest of trees->Feature importance based on mean decrease in impurity"
            ],
            [
                "sklearn->Examples->Gaussian Mixture Models->Gaussian Mixture Model Selection->Plot the BIC scores"
            ],
            [
                "sklearn->Examples->Model Selection->Statistical comparison of models using grid search"
            ],
            [
                "sklearn->Examples->Pipelines and composite estimators->Column Transformer with Mixed Types"
            ]
        ]
    },
    "1118023": {
        "jupyter_code_cell": "import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.cross_validation import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom IPython.display import Latex\nLatex(r\"\"\"\\begin{eqnarray}\n{\\mathbf{t}}\\,  {\\mathbf{f}}{\\mathbf{(t,d)}} & = \\frac{{\\mathbf{f}}{\\mathbf{(t,d)}}\\, {\\mathbf{+}}\\, \n{\\mathbf{1}} }{\\mathbf{||x||}}\\\\\n\\end{eqnarray}\"\"\")",
        "matched_tutorial_code_inds": [
            4700,
            3384,
            3284,
            3119,
            2517
        ],
        "matched_tutorial_codes": [
            "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom cycler import cycler\nmpl.rcParams['lines.linewidth'] = 2\nmpl.rcParams['lines.linestyle'] = '--'\ndata = np.random.randn(50)\nplt.plot(data)\n\n\n<img alt=\"customizing\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_customizing_001.png\" srcset=\"../../_images/sphx_glr_customizing_001.png, ../../_images/sphx_glr_customizing_001_2_0x.png 2.0x\"/>",
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import \nfrom sklearn.model_selection import \nfrom sklearn.pipeline import \nfrom sklearn.svm import \nfrom sklearn.decomposition import , \nfrom sklearn.feature_selection import , \nfrom sklearn.preprocessing import \n\nX, y = (return_X_y=True)\n\npipe = (\n    [\n        (\"scaling\", ()),\n        # the reduce_dim stage is populated by the param_grid\n        (\"reduce_dim\", \"passthrough\"),\n        (\"classify\", (dual=False, max_iter=10000)),\n    ]\n)\n\nN_FEATURES_OPTIONS = [2, 4, 8]\nC_OPTIONS = [1, 10, 100, 1000]\nparam_grid = [\n    {\n        \"reduce_dim\": [(iterated_power=7), (max_iter=1_000)],\n        \"reduce_dim__n_components\": N_FEATURES_OPTIONS,\n        \"classify__C\": C_OPTIONS,\n    },\n    {\n        \"reduce_dim\": [()],\n        \"reduce_dim__k\": N_FEATURES_OPTIONS,\n        \"classify__C\": C_OPTIONS,\n    },\n]\nreducer_labels = [\"PCA\", \"NMF\", \"KBest(mutual_info_classif)\"]\n\ngrid = (pipe, n_jobs=1, param_grid=param_grid)\ngrid.fit(X, y)",
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import \nfrom sklearn import datasets\nfrom sklearn.neighbors import \nfrom sklearn.inspection import DecisionBoundaryDisplay\n\nn_neighbors = 15\n\n# import some data to play with\niris = ()\n# we only take the first two features. We could avoid this ugly\n# slicing by using a two-dim dataset\nX = iris.data[:, :2]\ny = iris.target\n\n# Create color maps\ncmap_light = ([\"orange\", \"cyan\", \"cornflowerblue\"])\ncmap_bold = ([\"darkorange\", \"c\", \"darkblue\"])\n\nfor shrinkage in [None, 0.2]:\n    # we create an instance of Neighbours Classifier and fit the data.\n    clf = (shrink_threshold=shrinkage)\n    clf.fit(X, y)\n    y_pred = clf.predict(X)\n    print(shrinkage, (y == y_pred))\n\n    _, ax = ()\n    (\n        clf, X, cmap=cmap_light, ax=ax, response_method=\"predict\"\n    )\n\n    # Plot also the training points\n    (X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor=\"k\", s=20)\n    (\"3-Class classification (shrink_threshold=%r)\" % shrinkage)\n    (\"tight\")\n\n()",
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom collections import \n\npopulations = (list)\ncommon_params = {\n    \"n_samples\": 10_000,\n    \"n_features\": 2,\n    \"n_informative\": 2,\n    \"n_redundant\": 0,\n    \"random_state\": 0,\n}\nweights = (0.1, 0.8, 6)\nweights = weights[::-1]\n\n# fit and evaluate base model on balanced classes\nX, y = (**common_params, weights=[0.5, 0.5])\nestimator = ().fit(X, y)\nlr_base = extract_score((estimator, X, y, scoring=scoring, cv=10))\npos_lr_base, pos_lr_base_std = lr_base[\"positive\"].values\nneg_lr_base, neg_lr_base_std = lr_base[\"negative\"].values",
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import \nfrom sklearn import mixture\n\nn_samples = 300\n\n# generate random sample, two components\n(0)\n\n# generate spherical data centered on (20, 20)\nshifted_gaussian = (n_samples, 2) + ([20, 20])\n\n# generate zero centered stretched Gaussian data\nC = ([[0.0, -0.7], [3.5, 0.7]])\nstretched_gaussian = ((n_samples, 2), C)\n\n# concatenate the two datasets into the final training set\nX_train = ([shifted_gaussian, stretched_gaussian])\n\n# fit a Gaussian Mixture Model with two components\nclf = (n_components=2, covariance_type=\"full\")\nclf.fit(X_train)\n\n# display predicted scores by the model as a contour plot\nx = (-20.0, 30.0)\ny = (-20.0, 40.0)\nX, Y = (x, y)\nXX = ([X.ravel(), Y.ravel()]).T\nZ = -clf.score_samples(XX)\nZ = Z.reshape(X.shape)\n\nCS = (\n    X, Y, Z, norm=(vmin=1.0, vmax=1000.0), levels=(0, 3, 10)\n)\nCB = (CS, shrink=0.8, extend=\"both\")\n(X_train[:, 0], X_train[:, 1], 0.8)\n\n(\"Negative log-likelihood predicted by a GMM\")\n(\"tight\")\n()"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Introductory->Customizing Matplotlib with style sheets and rcParams->Runtime rc settings"
            ],
            [
                "sklearn->Examples->Pipelines and composite estimators->Selecting dimensionality reduction with Pipeline and GridSearchCV->Illustration of Pipeline and GridSearchCV"
            ],
            [
                "sklearn->Examples->Nearest Neighbors->Nearest Centroid Classification"
            ],
            [
                "sklearn->Examples->Model Selection->Class Likelihood Ratios to measure classification performance->Invariance with respect to prevalence"
            ],
            [
                "sklearn->Examples->Gaussian Mixture Models->Density Estimation for a Gaussian mixture"
            ]
        ]
    },
    "1098909": {
        "jupyter_code_cell": "from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn import datasets\nfrom sklearn.decomposition import PCA\nfig = plt.figure(1, figsize=(8, 6))\nax = Axes3D(fig, elev=-150, azim=110)\nX_reduced = PCA(n_components=3).fit_transform(df_iris[['Sepal Width', 'Petal Length', 'Petal Width']])\nax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=y,\n           cmap=plt.cm.Set1, edgecolor='k', s=40)\nax.set_title(\"PCA directions - Iris dataset\")\nax.set_xlabel(\"Sepal Width\")\nax.w_xaxis.set_ticklabels([])\nax.set_ylabel(\"Petal Length\")\nax.w_yaxis.set_ticklabels([])\nax.set_zlabel(\"Petal Width\")\nax.w_zaxis.set_ticklabels([])\nplt.show()\ndf_wine.columns = ['Identifier','Alcohol','Malic acid','Ash','Alcalinity of ash','Magnesium','Total phenols','Flavanoids','Nonflavanoid phenols','Proanthocyanins','Color intensity','Hue','OD280/OD315 of diluted wines','Proline']\ndf_wine.head(5)",
        "matched_tutorial_code_inds": [
            2161,
            2153,
            2136,
            4826,
            2617
        ],
        "matched_tutorial_codes": [
            "from sklearn.model_selection import \nimport matplotlib.pyplot as plt\n\nscoring = \"neg_mean_absolute_percentage_error\"\nn_cv_folds = 3\n\ndropped_result = (hist_dropped, X, y, cv=n_cv_folds, scoring=scoring)\none_hot_result = (hist_one_hot, X, y, cv=n_cv_folds, scoring=scoring)\nordinal_result = (hist_ordinal, X, y, cv=n_cv_folds, scoring=scoring)\nnative_result = (hist_native, X, y, cv=n_cv_folds, scoring=scoring)\n\n\ndef plot_results(figure_title):\n    fig, (ax1, ax2) = (1, 2, figsize=(12, 8))\n\n    plot_info = [\n        (\"fit_time\", \"Fit times (s)\", ax1, None),\n        (\"test_score\", \"Mean Absolute Percentage Error\", ax2, None),\n    ]\n\n    x, width = (4), 0.9\n    for key, title, ax, y_limit in plot_info:\n        items = [\n            dropped_result[key],\n            one_hot_result[key],\n            ordinal_result[key],\n            native_result[key],\n        ]\n\n        mape_cv_mean = [(np.abs(item)) for item in items]\n        mape_cv_std = [(item) for item in items]\n\n        ax.bar(\n            x=x,\n            height=mape_cv_mean,\n            width=width,\n            yerr=mape_cv_std,\n            color=[\"C0\", \"C1\", \"C2\", \"C3\"],\n        )\n        ax.set(\n            xlabel=\"Model\",\n            title=title,\n            xticks=x,\n            xticklabels=[\"Dropped\", \"One Hot\", \"Ordinal\", \"Native\"],\n            ylim=y_limit,\n        )\n    fig.suptitle(figure_title)\n\n\nplot_results(\"Gradient Boosting on Ames Housing\")\n\n\n<img alt=\"Gradient Boosting on Ames Housing, Fit times (s), Mean Absolute Percentage Error\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_001.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_001.png\"/>",
            "from sklearn.decomposition import \n\nimport matplotlib.pyplot as plt\n\n# unused but required import for doing 3d projections with matplotlib &lt; 3.2\nimport mpl_toolkits.mplot3d  # noqa: F401\n\n\ndef plot_figs(fig_num, elev, azim):\n    fig = (fig_num, figsize=(4, 3))\n    ()\n    ax = fig.add_subplot(111, projection=\"3d\", elev=elev, azim=azim)\n    ax.set_position([0, 0, 0.95, 1])\n\n    ax.scatter(a[::10], b[::10], c[::10], c=density[::10], marker=\"+\", alpha=0.4)\n    Y = [a, b, c]\n\n    # Using SciPy's SVD, this would be:\n    # _, pca_score, Vt = scipy.linalg.svd(Y, full_matrices=False)\n\n    pca = (n_components=3)\n    pca.fit(Y)\n    V = pca.components_.T\n\n    x_pca_axis, y_pca_axis, z_pca_axis = 3 * V\n    x_pca_plane = [x_pca_axis[:2], -x_pca_axis[1::-1]]\n    y_pca_plane = [y_pca_axis[:2], -y_pca_axis[1::-1]]\n    z_pca_plane = [z_pca_axis[:2], -z_pca_axis[1::-1]]\n    x_pca_plane.shape = (2, 2)\n    y_pca_plane.shape = (2, 2)\n    z_pca_plane.shape = (2, 2)\n    ax.plot_surface(x_pca_plane, y_pca_plane, z_pca_plane)\n    ax.xaxis.set_ticklabels([])\n    ax.yaxis.set_ticklabels([])\n    ax.zaxis.set_ticklabels([])\n\n\nelev = -40\nazim = -80\nplot_figs(1, elev, azim)\n\nelev = 30\nazim = 20\nplot_figs(2, elev, azim)\n\n()\n\n\n\n<img alt=\"plot pca 3d\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_pca_3d_001.png\" srcset=\"../../_images/sphx_glr_plot_pca_3d_001.png\"/>\n<img alt=\"plot pca 3d\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_pca_3d_002.png\" srcset=\"../../_images/sphx_glr_plot_pca_3d_002.png\"/>",
            "from sklearn.feature_extraction.image import \n\nprint(\"Extracting noisy patches... \")\nt0 = ()\ndata = (distorted[:, width // 2 :], patch_size)\ndata = data.reshape(data.shape[0], -1)\nintercept = (data, axis=0)\ndata -= intercept\nprint(\"done in %.2fs.\" % (() - t0))\n\ntransform_algorithms = [\n    (\"Orthogonal Matching Pursuit\\n1 atom\", \"omp\", {\"transform_n_nonzero_coefs\": 1}),\n    (\"Orthogonal Matching Pursuit\\n2 atoms\", \"omp\", {\"transform_n_nonzero_coefs\": 2}),\n    (\"Least-angle regression\\n4 atoms\", \"lars\", {\"transform_n_nonzero_coefs\": 4}),\n    (\"Thresholding\\n alpha=0.1\", \"threshold\", {\"transform_alpha\": 0.1}),\n]\n\nreconstructions = {}\nfor title, transform_algorithm, kwargs in transform_algorithms:\n    print(title + \"...\")\n    reconstructions[title] = raccoon_face.copy()\n    t0 = ()\n    dico.set_params(transform_algorithm=transform_algorithm, **kwargs)\n    code = dico.transform(data)\n    patches = (code, V)\n\n    patches += intercept\n    patches = patches.reshape(len(data), *patch_size)\n    if transform_algorithm == \"threshold\":\n        patches -= patches.min()\n        patches /= patches.max()\n    reconstructions[title][:, width // 2 :] = (\n        patches, (height, width // 2)\n    )\n    dt = () - t0\n    print(\"done in %.2fs.\" % dt)\n    show_with_diff(reconstructions[title], raccoon_face, title + \" (time: %.1fs)\" % dt)\n\n()\n\n\n\n<img alt=\"Orthogonal Matching Pursuit 1 atom (time: 0.7s), Image, Difference (norm: 10.69)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_image_denoising_003.png\" srcset=\"../../_images/sphx_glr_plot_image_denoising_003.png\"/>\n<img alt=\"Orthogonal Matching Pursuit 2 atoms (time: 1.5s), Image, Difference (norm: 9.29)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_image_denoising_004.png\" srcset=\"../../_images/sphx_glr_plot_image_denoising_004.png\"/>\n<img alt=\"Least-angle regression 4 atoms (time: 10.3s), Image, Difference (norm: 13.52)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_image_denoising_005.png\" srcset=\"../../_images/sphx_glr_plot_image_denoising_005.png\"/>\n<img alt=\"Thresholding  alpha=0.1 (time: 0.1s), Image, Difference (norm: 14.15)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_image_denoising_006.png\" srcset=\"../../_images/sphx_glr_plot_image_denoising_006.png\"/>",
            "from mpl_toolkits.axes_grid1 import Grid\n\nplt.close('all')\nfig = plt.figure()\ngrid = Grid(fig, rect=111, nrows_ncols=(2, 2),\n            axes_pad=0.25, label_mode='L',\n            )\n\nfor ax in grid:\n    example_plot(ax)\nax.title.set_visible(False)\n\nplt.tight_layout()\n\n\n<img alt=\"Title, Title, Title, Title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_tight_layout_guide_013.png\" srcset=\"../../_images/sphx_glr_tight_layout_guide_013.png, ../../_images/sphx_glr_tight_layout_guide_013_2_0x.png 2.0x\"/>",
            "from sklearn.gaussian_process.kernels import \n\nkernel = 1.0 * (\n    length_scale=1.0,\n    periodicity=3.0,\n    length_scale_bounds=(0.1, 10.0),\n    periodicity_bounds=(1.0, 10.0),\n)\ngpr = (kernel=kernel, random_state=0)\n\nfig, axs = (nrows=2, sharex=True, sharey=True, figsize=(10, 8))\n\n# plot prior\nplot_gpr_samples(gpr, n_samples=n_samples, ax=axs[0])\naxs[0].set_title(\"Samples from prior distribution\")\n\n# plot posterior\ngpr.fit(X_train, y_train)\nplot_gpr_samples(gpr, n_samples=n_samples, ax=axs[1])\naxs[1].scatter(X_train[:, 0], y_train, color=\"red\", zorder=10, label=\"Observations\")\naxs[1].legend(bbox_to_anchor=(1.05, 1.5), loc=\"upper left\")\naxs[1].set_title(\"Samples from posterior distribution\")\n\nfig.suptitle(\"Exp-Sine-Squared kernel\", fontsize=18)\n()\n\n\n<img alt=\"Exp-Sine-Squared kernel, Samples from prior distribution, Samples from posterior distribution\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_prior_posterior_003.png\" srcset=\"../../_images/sphx_glr_plot_gpr_prior_posterior_003.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Ensemble methods->Categorical Feature Support in Gradient Boosting->Model comparison"
            ],
            [
                "sklearn->Examples->Decomposition->Principal components analysis (PCA)->Plot the figures"
            ],
            [
                "sklearn->Examples->Decomposition->Image denoising using dictionary learning->Extract noisy patches and reconstruct them using the dictionary"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Tight Layout guide->Use with AxesGrid1"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Illustration of prior and posterior Gaussian process for different kernels->Kernel cookbook->Exp-Sine-Squared kernel"
            ]
        ]
    },
    "422152": {
        "jupyter_code_cell": "from arcgis.gis import GIS\nimport pandas as pd",
        "matched_tutorial_code_inds": [
            4511,
            5579,
            5786,
            3712,
            6118
        ],
        "matched_tutorial_codes": [
            "from scipy import stats\n import matplotlib.pyplot as plt",
            "from statsmodels.graphics.api import abline_plot",
            "from statsmodels.graphics.api import abline_plot\nfrom statsmodels.formula.api import ols, rlm",
            "from utils import download_airports\nimport zipfile",
            "from statsmodels.graphics.api import qqplot"
        ],
        "matched_tutorial_paths": [
            [
                "scipy->Statistics (scipy.stats)->Kernel density estimation->Univariate estimation"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Generalized Linear Models Overview->GLM: Binomial response data->Plots"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Duncan\u2019s Occupational Prestige data - M-estimation for outliers"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data",
                "statsmodels->Examples->State space models->Statespace ARMA: Sunspots Data"
            ]
        ]
    },
    "396707": {
        "jupyter_code_cell": "from sklearn.datasets import make_classification\nimport numpy as np\nimport pandas as pd \nimport xgboost as xgb \nfrom matplotlib import pylab as plt \nimport seaborn as sns",
        "matched_tutorial_code_inds": [
            2329,
            2081,
            2497,
            3145,
            2037
        ],
        "matched_tutorial_codes": [
            "import matplotlib.pyplot as plt\n\nfrom sklearn.datasets import \nfrom sklearn.ensemble import \nfrom sklearn.ensemble import \nfrom sklearn.linear_model import \nfrom sklearn.ensemble import",
            "import matplotlib.pyplot as plt\nfrom sklearn.model_selection import \nfrom sklearn.datasets import \nfrom sklearn.tree import",
            "from sklearn.metrics import \n\ny_pred = anova_svm.predict(X_test)\nprint((y_test, y_pred))",
            "from sklearn.metrics import \n\ny_pred = grid_search.predict(X_test)\nprint((y_test, y_pred))",
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.linalg import , \n\nfrom sklearn.covariance import , \n\n(0)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Ensemble methods->Plot individual and voting regression predictions"
            ],
            [
                "sklearn->Examples->Decision Trees->Post pruning decision trees with cost complexity pruning"
            ],
            [
                "sklearn->Examples->Feature Selection->Pipeline ANOVA SVM"
            ],
            [
                "sklearn->Examples->Model Selection->Custom refit strategy of a grid search with cross-validation->Tuning hyper-parameters"
            ],
            [
                "sklearn->Examples->Covariance estimation->Ledoit-Wolf vs OAS estimation"
            ]
        ]
    },
    "717129": {
        "jupyter_code_cell": "df.info()\nax = df.groupby('runtime').popularity.mean().plot(kind='line', title =\"Runtime over Film Popularity\", figsize=(15, 10), legend=True, fontsize=12)\nax.set_xlabel(\"Runtime\", fontsize=12)\nax.set_ylabel(\"Popularity\", fontsize=12)\nplt.show()",
        "matched_tutorial_code_inds": [
            3676,
            5350,
            3789,
            4013,
            3792
        ],
        "matched_tutorial_codes": [
            "m = pd.merge(flights, daily.reset_index().rename(columns={'date': 'fl_date', 'station': 'origin'}),\n             on=['fl_date', 'origin']).set_index(idx_cols).sort_index()\n\nm.head()",
            "fig = sm.graphics.influence_plot(crime_model)\nfig.tight_layout(pad=1.0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_regression_plots_45_0.png\" src=\"../../../_images/examples_notebooks_generated_regression_plots_45_0.png\"/>",
            "df.plot.scatter(x='carat', y='depth', c='k', alpha=.15)\nplt.tight_layout()",
            "palette = sns.cubehelix_palette(light=.8, n_colors=6)\nsns.relplot(\n    data=dots, kind=\"line\",\n    x=\"time\", y=\"firing_rate\",\n    hue=\"coherence\", style=\"choice\", palette=palette,\n)\n",
            "sns.countplot(x='cut', data=df)\nsns.despine()\nplt.tight_layout()"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Indexes->Merging->The merge version"
            ],
            [
                "statsmodels->Examples->Plotting->Regression Plots->Statewide Crime 2009 Dataset->Influence Plot"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Pandas Built-in Plotting"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Emphasizing continuity with line plots->Plotting subsets of data with semantic mappings",
                "seaborn->Plotting functions->Visualizing statistical relationships->Emphasizing continuity with line plots->Plotting subsets of data with semantic mappings"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ]
        ]
    },
    "611386": {
        "jupyter_code_cell": "data_2009 = raw_2009.iloc[557:905,:].reset_index(drop=True)\ndata_2009.start = [datetime.strptime(data_2009.start[i], '%m/%d/%Y') for i in range(len(data_2009.start))]\ndata_2009.end = [datetime.strptime(data_2009.end[i], '%m/%d/%Y') for i in range(len(data_2009.end))]\ndata_2009.iloc[-1].start + timedelta(days=276)\ndata_2009.loc[lambda data_2009: data_2009.iloc[:,1] == '17/05/2015', :]\ndata_2009 = data_2009.iloc[87:,:].reset_index(drop=True)\ndata_2009.head()\ndata_2009.to_hdf('result/n4.h5','obamaapproval',table=True,mode='a')",
        "matched_tutorial_code_inds": [
            4654,
            5963,
            6060,
            2298,
            5917
        ],
        "matched_tutorial_codes": [
            "data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>",
            "data = [\n    [\"Carroll\", 94, 22, 60, 92, 20, 60],\n    [\"Grant\", 98, 21, 65, 92, 22, 65],\n    [\"Peck\", 98, 28, 40, 88, 26, 40],\n    [\"Donat\", 94, 19, 200, 82, 17, 200],\n    [\"Stewart\", 98, 21, 50, 88, 22, 45],\n    [\"Young\", 96, 21, 85, 92, 22, 85],\n]\ncolnames = [\"study\", \"mean_t\", \"sd_t\", \"n_t\", \"mean_c\", \"sd_c\", \"n_c\"]\nrownames = [i[0] for i in data]\ndframe1 = pd.DataFrame(data, columns=colnames)\nrownames",
            "data = pdr.get_data_fred(\"HOUSTNSA\", \"1959-01-01\", \"2019-06-01\")\nhousing = data.HOUSTNSA.pct_change().dropna()\n# Scale by 100 to get percentages\nhousing = 100 * housing.asfreq(\"MS\")\nfig, ax = plt.subplots()\nax = housing.plot(ax=ax)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_6_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_6_0.png\"/>",
            "feature_importance = reg.feature_importances_\nsorted_idx = (feature_importance)\npos = (sorted_idx.shape[0]) + 0.5\nfig = (figsize=(12, 6))\n(1, 2, 1)\n(pos, feature_importance[sorted_idx], align=\"center\")\n(pos, (diabetes.feature_names)[sorted_idx])\n(\"Feature Importance (MDI)\")\n\nresult = (\n    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_idx = result.importances_mean.argsort()\n(1, 2, 2)\n(\n    result.importances[sorted_idx].T,\n    vert=False,\n    labels=(diabetes.feature_names)[sorted_idx],\n)\n(\"Permutation Importance (test set)\")\nfig.tight_layout()\n()\n\n\n<img alt=\"Feature Importance (MDI), Permutation Importance (test set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\"/>",
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Example"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions"
            ],
            [
                "sklearn->Examples->Ensemble methods->Gradient Boosting regression->Plot feature importance"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ]
        ]
    },
    "951620": {
        "jupyter_code_cell": "dtm_A_train = vect_A.transform(train_txt)\nprint(dtm_A_train)\npd.DataFrame(dtm_A_train.toarray(), columns=vect_A.get_feature_names())",
        "matched_tutorial_code_inds": [
            5191,
            6664,
            6659,
            5400,
            6628
        ],
        "matched_tutorial_codes": [
            "ols_model = sm.OLS(y, X)\nols_results = ols_model.fit()\nprint(ols_results.summary())",
            "mod_conc = LocalLevelConcentrated(dta.infl)\nres_conc = mod_conc.fit(disp=False)\nprint(res_conc.summary())",
            "mod = LocalLevel(dta.infl)\nres = mod.fit(disp=False)\nprint(res.summary())",
            "poisson_mod = sm.Poisson(rand_data.endog, rand_exog)\npoisson_res = poisson_mod.fit(method=\"newton\")\nprint(poisson_res.summary())",
            "mod = TVRegression(y_t, x_t, w_t)\nres_mle = mod.fit(disp=False)\nprint(res_mle.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity"
            ],
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Concentrating out the scale"
            ],
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Typical approach"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Poisson"
            ],
            [
                "statsmodels->Examples->State space models->Statespace: Custom Models->Bonus: pymc3 for fast Bayesian estimation"
            ]
        ]
    },
    "117203": {
        "jupyter_code_cell": "mortality_rates = dict()\nfor country in root.iterfind('country'):\n    country_name = value_as(str)(country.find('name'))\n    mortality_rate = value_as(float)(country.find('infant_mortality'))\n    mortality_rates[country_name] = mortality_rate\nmr_df = pd.Series(mortality_rates)\nmr_df.sort_values().head(10)",
        "matched_tutorial_code_inds": [
            3240,
            3257,
            5911,
            1475,
            2085
        ],
        "matched_tutorial_codes": [
            "cred_intervals = []\nintervals = [0.5, 0.75, 0.95]\n\nfor interval in intervals:\n    cred_interval = list(t_post.interval(interval))\n    cred_intervals.append([interval, cred_interval[0], cred_interval[1]])\n\ncred_int_df = (\n    cred_intervals, columns=[\"interval\", \"lower value\", \"upper value\"]\n).set_index(\"interval\")\ncred_int_df\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "alphas = (-5, 1, 60)\nenet = (l1_ratio=0.7, max_iter=10000)\ntrain_errors = list()\ntest_errors = list()\nfor alpha in alphas:\n    enet.set_params(alpha=alpha)\n    enet.fit(X_train, y_train)\n    train_errors.append(enet.score(X_train, y_train))\n    test_errors.append(enet.score(X_test, y_test))\n\ni_alpha_optim = (test_errors)\nalpha_optim = alphas[i_alpha_optim]\nprint(\"Optimal regularization parameter : %s\" % alpha_optim)\n\n# Estimate the coef_ on full data with optimal regularization parameter\nenet.set_params(alpha=alpha_optim)\ncoef_ = enet.fit(X, y).coef_",
            "infl = interM_lm.get_influence()\nresid = infl.resid_studentized_internal\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        resid[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\nplt.xlabel(\"X\")\nplt.ylabel(\"standardized resids\")",
            "totals_row = 35\nlocations = np.delete(locations, (totals_row), axis=0)\nnbcases = np.delete(nbcases, (totals_row), axis=0)\n\nchina_total = nbcases[locations[:, 1] == \"China\"].sum(axis=0)\nchina_total",
            "clfs = []\nfor ccp_alpha in ccp_alphas:\n    clf = (random_state=0, ccp_alpha=ccp_alpha)\n    clf.fit(X_train, y_train)\n    clfs.append(clf)\nprint(\n    \"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n        clfs[-1].tree_.node_count, ccp_alphas[-1]\n    )\n)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Model Selection->Statistical comparison of models using grid search->Comparing two models: Bayesian approach->Region of Practical Equivalence"
            ],
            [
                "sklearn->Examples->Model Selection->Train error vs Test error->Compute train and test errors"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "numpy->NumPy Features->Masked Arrays->Exploring the data"
            ],
            [
                "sklearn->Examples->Decision Trees->Post pruning decision trees with cost complexity pruning->Total impurity of leaves vs effective alphas of pruned tree"
            ]
        ]
    },
    "86231": {
        "jupyter_code_cell": "for path in np.arange(0,N):\n    for i in np.arange(1,x.shape[1]):\n        ds= s[path,i-1]*(r-q)*dt + s[path,i-1]*vol*np.sqrt(dt)*x[path,i]\n        s[path,i]= s[path,i-1] + ds\npayoff = s[:,-1]-k",
        "matched_tutorial_code_inds": [
            6258,
            6655,
            4423,
            3167,
            2162
        ],
        "matched_tutorial_codes": [
            "for fit in [fit2, fit4]:\n    pd.DataFrame(np.c_[fit.level, fit.trend]).rename(\n        columns={0: \"level\", 1: \"slope\"}\n    ).plot(subplots=True)\nplt.show()\nprint(\n    \"Figure 7.4: Level and slope components for Holt\u2019s linear trend method and the additive damped trend method.\"\n)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_0.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_0.png\"/>\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_1.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_1.png\"/>",
            "for i in range(simulated.shape[1]):\n    simulated.iloc[:, i].plot(label=\"_\", color=\"gray\", alpha=0.1)\ndf[\"mean\"].plot(label=\"mean prediction\")\ndf[\"pi_lower\"].plot(linestyle=\"--\", color=\"tab:blue\", label=\"95% interval\")\ndf[\"pi_upper\"].plot(linestyle=\"--\", color=\"tab:blue\", label=\"_\")\npred.endog.plot(label=\"data\")\nplt.legend()",
            "for j, p in enumerate(points):\n...     plt.text(p[0]-0.03, p[1]+0.03, j, ha='right') # label the points\n for j, s in enumerate(tri.simplices):\n...     p = points[s].mean(axis=0)\n...     plt.text(p[0], p[1], '#%d' % j, ha='center') # label triangles\n plt.xlim(-0.5, 1.5); plt.ylim(-0.5, 1.5)\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code generates an X-Y plot with four green points annotated 0 through 3 roughly in the shape of a box. The box is outlined with a diagonal line between points 0 and 3 forming two adjacent triangles. The top triangle is annotated as #1 and the bottom triangle is annotated as #0.\"' class=\"plot-directive\" src=\"../_images/spatial-1.png\"/>\n</figure>",
            "for i in range(n_classes):\n    fpr[i], tpr[i], _ = (y_onehot_test[:, i], y_score[:, i])\n    roc_auc[i] = (fpr[i], tpr[i])\n\nfpr_grid = (0.0, 1.0, 1000)\n\n# Interpolate all ROC curves at these points\nmean_tpr = (fpr_grid)\n\nfor i in range(n_classes):\n    mean_tpr += (fpr_grid, fpr[i], tpr[i])  # linear interpolation\n\n# Average it and compute AUC\nmean_tpr /= n_classes\n\nfpr[\"macro\"] = fpr_grid\ntpr[\"macro\"] = mean_tpr\nroc_auc[\"macro\"] = (fpr[\"macro\"], tpr[\"macro\"])\n\nprint(f\"Macro-averaged One-vs-Rest ROC AUC score:\\n{roc_auc['macro']:.2f}\")",
            "for pipe in (hist_dropped, hist_one_hot, hist_ordinal, hist_native):\n    pipe.set_params(\n        histgradientboostingregressor__max_depth=3,\n        histgradientboostingregressor__max_iter=15,\n    )\n\ndropped_result = (hist_dropped, X, y, cv=n_cv_folds, scoring=scoring)\none_hot_result = (hist_one_hot, X, y, cv=n_cv_folds, scoring=scoring)\nordinal_result = (hist_ordinal, X, y, cv=n_cv_folds, scoring=scoring)\nnative_result = (hist_native, X, y, cv=n_cv_folds, scoring=scoring)\n\nplot_results(\"Gradient Boosting on Ames Housing (few and small trees)\")\n\n()\n\n\n<img alt=\"Gradient Boosting on Ames Housing (few and small trees), Fit times (s), Mean Absolute Percentage Error\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_002.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Method->Plots of Seasonally Adjusted Data"
            ],
            [
                "statsmodels->Examples->State space models->ETS models->Predictions"
            ],
            [
                "scipy->Spatial data structures and algorithms (scipy.spatial)->Delaunay triangulations"
            ],
            [
                "sklearn->Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->One-vs-Rest multiclass ROC->ROC curve using the OvR macro-average"
            ],
            [
                "sklearn->Examples->Ensemble methods->Categorical Feature Support in Gradient Boosting->Limiting the number of splits"
            ]
        ]
    },
    "1077041": {
        "jupyter_code_cell": "weather_append = weather_concat.append(weather.iloc[305,])\nweather_append.info() \nweather.corr() ",
        "matched_tutorial_code_inds": [
            3948,
            5703,
            5470,
            5233,
            6770
        ],
        "matched_tutorial_codes": [
            "two_series = [flights_wide.loc[:1955, \"Jan\"], flights_wide.loc[1952:, \"Aug\"]]\nsns.relplot(data=two_series, kind=\"line\")\n",
            "summ_df = infl.summary_frame()\nsumm_df.sort_values(\"cooks_d\", ascending=False)[:10]",
            "means75 = exog.mean()\nmeans75[\"LOWINC\"] = exog[\"LOWINC\"].quantile(0.75)\nprint(means75)",
            "print(res.recursive_coefficients.filtered[0])\nres.plot_recursive_coefficient(range(mod.k_exog), alpha=None, figsize=(10, 6))",
            "df = dta.data[[\"Lottery\", \"Literacy\", \"Wealth\", \"Region\"]].dropna()\ndf.head()"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing wide-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing wide-form data"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Influence Measures for GLMs->get the influence measures"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 1: Copper"
            ],
            [
                "statsmodels->Examples->User Notes->Formulas->OLS regression using formulas"
            ]
        ]
    },
    "148908": {
        "jupyter_code_cell": "p = Scatter(dftsne, x='x', y='y', color='label.raw', title='HG002 DEL: Raw DBSCAN labels', legend=\"top_left\")\noutput_file(\"raw_DBSCAN_DEL_label.html\")\nshow(p)\nlab = df_dbscan['clusterLabel']\nlab.SVD = df_dbscan['clusterLabel.SVD']\nlab.raw = df_dbscan['clusterLabel.raw']",
        "matched_tutorial_code_inds": [
            3234,
            2996,
            2064,
            5262,
            1667
        ],
        "matched_tutorial_codes": [
            "x = (t_post.ppf(0.001), t_post.ppf(0.999), 100)\n\n(x, t_post.pdf(x))\n((-0.04, 0.06, 0.01))\n(x, t_post.pdf(x), 0, facecolor=\"blue\", alpha=0.2)\n(\"Probability density\")\n(r\"Mean difference ($\\mu$)\")\n(\"Posterior distribution\")\n()\n\n\n<img alt=\"Posterior distribution\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_grid_search_stats_003.png\" srcset=\"../../_images/sphx_glr_plot_grid_search_stats_003.png\"/>",
            "tree_disp.plot(line_kw={\"label\": \"Decision Tree\"})\nmlp_disp.plot(\n    line_kw={\"label\": \"Multi-layer Perceptron\", \"color\": \"red\"}, ax=tree_disp.axes_\n)\ntree_disp.figure_.set_size_inches(10, 6)\ntree_disp.axes_[0, 0].legend()\ntree_disp.axes_[0, 1].legend()\n()\n\n\n<img alt=\"plot partial dependence visualization api\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_partial_dependence_visualization_api_005.png\" srcset=\"../../_images/sphx_glr_plot_partial_dependence_visualization_api_005.png\"/>",
            "y = X.dot(pca.components_[1]) + rng.normal(size=n_samples) / 2\n\nfig, axes = (1, 2, figsize=(10, 3))\n\naxes[0].scatter(X.dot(pca.components_[0]), y, alpha=0.3)\naxes[0].set(xlabel=\"Projected data onto first PCA component\", ylabel=\"y\")\naxes[1].scatter(X.dot(pca.components_[1]), y, alpha=0.3)\naxes[1].set(xlabel=\"Projected data onto second PCA component\", ylabel=\"y\")\n()\n()\n\n\n<img alt=\"plot pcr vs pls\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_pcr_vs_pls_002.png\" srcset=\"../../_images/sphx_glr_plot_pcr_vs_pls_002.png\"/>",
            "fig = rres.plot_recursive_coefficient(variables=[\"Mkt-RF\"], figsize=(14, 6))\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_rolling_ls_10_0.png\" src=\"../../../_images/examples_notebooks_generated_rolling_ls_10_0.png\"/>",
            "p = np.polynomial.Polynomial([-16, 0, 0, 0, 15, 0, 0, 0, 1])\np\n\n\n\n\n\n\\[x \\mapsto \\text{-16.0}\\color{LightGray}{ + \\text{0.0}\\,x}\\color{LightGray}{ + \\text{0.0}\\,x^{2}}\\color{LightGray}{ + \\text{0.0}\\,x^{3}} + \\text{15.0}\\,x^{4}\\color{LightGray}{ + \\text{0.0}\\,x^{5}}\\color{LightGray}{ + \\text{0.0}\\,x^{6}}\\color{LightGray}{ + \\text{0.0}\\,x^{7}} + \\text{1.0}\\,x^{8}\\]"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Model Selection->Statistical comparison of models using grid search->Comparing two models: Bayesian approach"
            ],
            [
                "sklearn->Examples->Miscellaneous->Advanced Plotting With Partial Dependence->Plotting partial dependence of the two models together"
            ],
            [
                "sklearn->Examples->Cross decomposition->Principal Component Regression vs Partial Least Squares Regression->The data"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Rolling Least Squares"
            ],
            [
                "numpy->NumPy Applications->Plotting Fractals->Generalizing the Julia set->Newton Fractals"
            ]
        ]
    },
    "1095879": {
        "jupyter_code_cell": "print(os.getcwd())\nurl = 'https://www.dropbox.com/s/shg31hm4voydqnl/Thinkful%20Workshops%20-%20Predicting%20the%20Oscars.zip?dl=1'\nr = requests.get(url)",
        "matched_tutorial_code_inds": [
            3784,
            5423,
            6061,
            6818,
            6791
        ],
        "matched_tutorial_codes": [
            "import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "mfx = affair_mod.get_margeff()\nprint(mfx.summary())",
            "mod = AutoReg(housing, 3, old_names=False)\nres = mod.fit()\nprint(res.summary())",
            "fcast_res3 = res.get_forecast('2010Q2')\nprint(fcast_res3.summary_frame())",
            "olsmod = sm.OLS(y, X)\nolsres = olsmod.fit()\nprint(olsres.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example->Specifying the number of forecasts"
            ],
            [
                "statsmodels->Examples->User Notes->Prediction->Estimation"
            ]
        ]
    },
    "362761": {
        "jupyter_code_cell": "df.loc[1:5]\ndf.loc[1:5, 'Country':'HasTraffic']",
        "matched_tutorial_code_inds": [
            3656,
            3595,
            2801,
            6804,
            3802
        ],
        "matched_tutorial_codes": [
            "weather2 = weather.reset_index()\nweather2[weather2['station'] == 'DSM'].head()",
            "twenty_train.target_names[gs_clf.predict(['God is love'])[0]]\n'soc.religion.christian'",
            "X = survey.data[survey.feature_names]\nX.describe(include=\"all\")\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "endog = macrodata['infl']\nendog.plot(figsize=(15, 5))",
            "scores = pd.DataFrame(est.cv_results_)\nscores.head()"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Indexes->Flavors->Row Slicing"
            ],
            [
                "sklearn->Tutorials->Working With Text Data->Parameter tuning using grid search"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ]
        ]
    },
    "838692": {
        "jupyter_code_cell": "from sklearn import model_selection\ndef compute_polynomial_regression_holdout(dataset, variable, target, degree, test_size=0.33, random_state=1234):\n    extended_dataset = add_degrees(dataset, variable, degree)\n    train_data, test_data,train_y,test_y = model_selection.train_test_split(extended_dataset\n                                                              [extended_dataset.columns[extended_dataset.columns != target]],\n                                                              extended_dataset[target], test_size=test_size, random_state=random_state)\n    train_x = train_data[generate_variables(variable,degree)].values\n    test_x = test_data[generate_variables(variable,degree)].values\n    regr = linear_model.LinearRegression()\n    regr.fit(train_x, train_y)\n    y_predicted_from_test = regr.predict(test_x)\n    y_predicted = regr.predict(train_x)\n    mse_test = mean_squared_error(y_pred=y_predicted_from_test, y_true=test_y)\n    mse_train = mean_squared_error(y_pred=y_predicted, y_true=train_y)\n    r2_test = r2_score(y_pred=y_predicted_from_test, y_true=test_y)\n    r2_train = r2_score(y_pred=y_predicted, y_true=train_y)\n    return mse_test, r2_test, mse_train, r2_train\nmax_polynomial = 10\nmse = np.zeros(max_polynomial)\nr2 = np.zeros(max_polynomial)\nmse_train = np.zeros(max_polynomial)\nr2_train = np.zeros(max_polynomial)\nfor i in range(max_polynomial):\n    mse[i], r2[i],mse_train[i], r2_train[i] = compute_polynomial_regression_holdout(df,'LSTAT','MEDV',i+1)",
        "matched_tutorial_code_inds": [
            511,
            2536,
            2161,
            2787,
            2504
        ],
        "matched_tutorial_codes": [
            "from timeit import default_timer as timer\nNUM_EPOCHS = 18\n\nfor epoch in range(1, NUM_EPOCHS+1):\n    start_time = timer()\n    train_loss = train_epoch(transformer, optimizer)\n    end_time = timer()\n    val_loss = evaluate(transformer)\n    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n\n\n# function to generate output sequence using greedy algorithm\ndef greedy_decode(model, src, src_mask, max_len, start_symbol):\n    src = src.to(DEVICE)\n    src_mask = src_mask.to(DEVICE)\n\n    memory = model.encode(src, src_mask)\n    ys = (1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n    for i in range(max_len-1):\n        memory = memory.to(DEVICE)\n        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n                    .type(torch.bool)).to(DEVICE)\n        out = model.decode(ys, memory, tgt_mask)\n        out = out.transpose(0, 1)\n        prob = model.generator(out[:, -1])\n        _, next_word = (prob, dim=1)\n        next_word = next_word.item()\n\n        ys = ([ys,\n                        (1, 1).type_as(src.data).fill_(next_word)], dim=0)\n        if next_word == EOS_IDX:\n            break\n    return ys\n\n\n# actual function to translate input sentence into target language\ndef translate(model: , src_sentence: str):\n    model.eval()\n    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n    num_tokens = src.shape[0]\n    src_mask = ((num_tokens, num_tokens)).type(torch.bool)\n    tgt_tokens = greedy_decode(\n        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"&lt;bos&gt;\", \"\").replace(\"&lt;eos&gt;\", \"\")",
            "from sklearn.linear_model import \n\nridge = ().fit(training_data, training_noisy_target)\n\n(data, target, label=\"True signal\", linewidth=2)\n(\n    training_data,\n    training_noisy_target,\n    color=\"black\",\n    label=\"Noisy measurements\",\n)\n(data, ridge.predict(data), label=\"Ridge regression\")\n()\n(\"data\")\n(\"target\")\n_ = (\"Limitation of a linear model such as ridge\")\n\n\n<img alt=\"Limitation of a linear model such as ridge\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_compare_gpr_krr_002.png\" srcset=\"../../_images/sphx_glr_plot_compare_gpr_krr_002.png\"/>",
            "from sklearn.model_selection import \nimport matplotlib.pyplot as plt\n\nscoring = \"neg_mean_absolute_percentage_error\"\nn_cv_folds = 3\n\ndropped_result = (hist_dropped, X, y, cv=n_cv_folds, scoring=scoring)\none_hot_result = (hist_one_hot, X, y, cv=n_cv_folds, scoring=scoring)\nordinal_result = (hist_ordinal, X, y, cv=n_cv_folds, scoring=scoring)\nnative_result = (hist_native, X, y, cv=n_cv_folds, scoring=scoring)\n\n\ndef plot_results(figure_title):\n    fig, (ax1, ax2) = (1, 2, figsize=(12, 8))\n\n    plot_info = [\n        (\"fit_time\", \"Fit times (s)\", ax1, None),\n        (\"test_score\", \"Mean Absolute Percentage Error\", ax2, None),\n    ]\n\n    x, width = (4), 0.9\n    for key, title, ax, y_limit in plot_info:\n        items = [\n            dropped_result[key],\n            one_hot_result[key],\n            ordinal_result[key],\n            native_result[key],\n        ]\n\n        mape_cv_mean = [(np.abs(item)) for item in items]\n        mape_cv_std = [(item) for item in items]\n\n        ax.bar(\n            x=x,\n            height=mape_cv_mean,\n            width=width,\n            yerr=mape_cv_std,\n            color=[\"C0\", \"C1\", \"C2\", \"C3\"],\n        )\n        ax.set(\n            xlabel=\"Model\",\n            title=title,\n            xticks=x,\n            xticklabels=[\"Dropped\", \"One Hot\", \"Ordinal\", \"Native\"],\n            ylim=y_limit,\n        )\n    fig.suptitle(figure_title)\n\n\nplot_results(\"Gradient Boosting on Ames Housing\")\n\n\n<img alt=\"Gradient Boosting on Ames Housing, Fit times (s), Mean Absolute Percentage Error\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_001.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_001.png\"/>",
            "from sklearn.linear_model import \n\n\nmask_train = df_train[\"ClaimAmount\"]  0\nmask_test = df_test[\"ClaimAmount\"]  0\n\nglm_sev = (alpha=10.0, solver=\"newton-cholesky\")\n\nglm_sev.fit(\n    X_train[mask_train.values],\n    df_train.loc[mask_train, \"AvgClaimAmount\"],\n    sample_weight=df_train.loc[mask_train, \"ClaimNb\"],\n)\n\nscores = score_estimator(\n    glm_sev,\n    X_train[mask_train.values],\n    X_test[mask_test.values],\n    df_train[mask_train],\n    df_test[mask_test],\n    target=\"AvgClaimAmount\",\n    weights=\"ClaimNb\",\n)\nprint(\"Evaluation of GammaRegressor on target AvgClaimAmount\")\nprint(scores)",
            "from sklearn.datasets import \n\nX, y = (\n    n_samples=500,\n    n_features=15,\n    n_informative=3,\n    n_redundant=2,\n    n_repeated=0,\n    n_classes=8,\n    n_clusters_per_class=1,\n    class_sep=0.8,\n    random_state=0,\n)"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Text->Language Translation with nn.Transformer and torchtext->Collation"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Comparison of kernel ridge and Gaussian process regression->Limitations of a simple linear model"
            ],
            [
                "sklearn->Examples->Ensemble methods->Categorical Feature Support in Gradient Boosting->Model comparison"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Tweedie regression on insurance claims->Severity Model -  Gamma distribution"
            ],
            [
                "sklearn->Examples->Feature Selection->Recursive feature elimination with cross-validation->Data generation"
            ]
        ]
    },
    "394300": {
        "jupyter_code_cell": "plt.plot(mcp300405['On Time'],mcp300405['350Signal'],c='blue',linestyle='-',label='mcp 300')\nplt.plot(mcp300405['On Time'],mcp300405['405Signal'],c='red',linestyle='-',label='mcp 405')\nplt.xlabel('Time (Hours)')\nplt.ylabel('Signal')\nplt.legend()\nplt.show()\nplt.plot(mcp325405['On Time'],mcp325405['350Signal'],c='blue',linestyle='-',label='mcp 325')\nplt.plot(mcp325405['On Time'],mcp325405['405Signal'],c='red',linestyle='-',label='mcp 405')\nplt.xlabel('Time (Hours)')\nplt.ylabel('Signal')\nplt.legend()\nplt.show()",
        "matched_tutorial_code_inds": [
            6253,
            6255,
            5362,
            6260,
            2662
        ],
        "matched_tutorial_codes": [
            "fit1 = SimpleExpSmoothing(oildata, initialization_method=\"heuristic\").fit(\n    smoothing_level=0.2, optimized=False\n)\nfcast1 = fit1.forecast(3).rename(r\"$\\alpha=0.2$\")\nfit2 = SimpleExpSmoothing(oildata, initialization_method=\"heuristic\").fit(\n    smoothing_level=0.6, optimized=False\n)\nfcast2 = fit2.forecast(3).rename(r\"$\\alpha=0.6$\")\nfit3 = SimpleExpSmoothing(oildata, initialization_method=\"estimated\").fit()\nfcast3 = fit3.forecast(3).rename(r\"$\\alpha=%s$\" % fit3.model.params[\"smoothing_level\"])\n\nplt.figure(figsize=(12, 8))\nplt.plot(oildata, marker=\"o\", color=\"black\")\nplt.plot(fit1.fittedvalues, marker=\"o\", color=\"blue\")\n(line1,) = plt.plot(fcast1, marker=\"o\", color=\"blue\")\nplt.plot(fit2.fittedvalues, marker=\"o\", color=\"red\")\n(line2,) = plt.plot(fcast2, marker=\"o\", color=\"red\")\nplt.plot(fit3.fittedvalues, marker=\"o\", color=\"green\")\n(line3,) = plt.plot(fcast3, marker=\"o\", color=\"green\")\nplt.legend([line1, line2, line3], [fcast1.name, fcast2.name, fcast3.name])",
            "fit1 = Holt(air, initialization_method=\"estimated\").fit(\n    smoothing_level=0.8, smoothing_trend=0.2, optimized=False\n)\nfcast1 = fit1.forecast(5).rename(\"Holt's linear trend\")\nfit2 = Holt(air, exponential=True, initialization_method=\"estimated\").fit(\n    smoothing_level=0.8, smoothing_trend=0.2, optimized=False\n)\nfcast2 = fit2.forecast(5).rename(\"Exponential trend\")\nfit3 = Holt(air, damped_trend=True, initialization_method=\"estimated\").fit(\n    smoothing_level=0.8, smoothing_trend=0.2\n)\nfcast3 = fit3.forecast(5).rename(\"Additive damped trend\")\n\nplt.figure(figsize=(12, 8))\nplt.plot(air, marker=\"o\", color=\"black\")\nplt.plot(fit1.fittedvalues, color=\"blue\")\n(line1,) = plt.plot(fcast1, marker=\"o\", color=\"blue\")\nplt.plot(fit2.fittedvalues, color=\"red\")\n(line2,) = plt.plot(fcast2, marker=\"o\", color=\"red\")\nplt.plot(fit3.fittedvalues, color=\"green\")\n(line3,) = plt.plot(fcast3, marker=\"o\", color=\"green\")\nplt.legend([line1, line2, line3], [fcast1.name, fcast2.name, fcast3.name])",
            "plt.rcParams[\"figure.subplot.bottom\"] = 0.23  # keep labels visible\nplt.rcParams[\"figure.figsize\"] = (10.0, 8.0)  # make plot larger in notebook\nage = [data.exog[\"age\"][data.endog == id] for id in party_ID]\nfig = plt.figure()\nax = fig.add_subplot(111)\nplot_opts = {\n    \"cutoff_val\": 5,\n    \"cutoff_type\": \"abs\",\n    \"label_fontsize\": \"small\",\n    \"label_rotation\": 30,\n}\nsm.graphics.beanplot(age, ax=ax, labels=labels, plot_opts=plot_opts)\nax.set_xlabel(\"Party identification of respondent.\")\nax.set_ylabel(\"Age\")\n# plt.show()",
            "fit1 = SimpleExpSmoothing(livestock2, initialization_method=\"estimated\").fit()\nfcast1 = fit1.forecast(9).rename(\"SES\")\nfit2 = Holt(livestock2, initialization_method=\"estimated\").fit()\nfcast2 = fit2.forecast(9).rename(\"Holt's\")\nfit3 = Holt(livestock2, exponential=True, initialization_method=\"estimated\").fit()\nfcast3 = fit3.forecast(9).rename(\"Exponential\")\nfit4 = Holt(livestock2, damped_trend=True, initialization_method=\"estimated\").fit(\n    damping_trend=0.98\n)\nfcast4 = fit4.forecast(9).rename(\"Additive Damped\")\nfit5 = Holt(\n    livestock2, exponential=True, damped_trend=True, initialization_method=\"estimated\"\n).fit()\nfcast5 = fit5.forecast(9).rename(\"Multiplicative Damped\")\n\nax = livestock2.plot(color=\"black\", marker=\"o\", figsize=(12, 8))\nlivestock3.plot(ax=ax, color=\"black\", marker=\"o\", legend=False)\nfcast1.plot(ax=ax, color=\"red\", legend=True)\nfcast2.plot(ax=ax, color=\"green\", legend=True)\nfcast3.plot(ax=ax, color=\"blue\", legend=True)\nfcast4.plot(ax=ax, color=\"cyan\", legend=True)\nfcast5.plot(ax=ax, color=\"magenta\", legend=True)\nax.set_ylabel(\"Livestock, sheep in Asia (millions)\")\nplt.show()\nprint(\n    \"Figure 7.5: Forecasting livestock, sheep in Asia: comparing forecasting performance of non-seasonal methods.\"\n)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_14_0.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_14_0.png\"/>",
            "m, s, _ = (\n    (enet.coef_)[0],\n    enet.coef_[enet.coef_ != 0],\n    markerfmt=\"x\",\n    label=\"Elastic net coefficients\",\n)\n([m, s], color=\"#2ca02c\")\nm, s, _ = (\n    (lasso.coef_)[0],\n    lasso.coef_[lasso.coef_ != 0],\n    markerfmt=\"x\",\n    label=\"Lasso coefficients\",\n)\n([m, s], color=\"#ff7f0e\")\n(\n    (coef)[0],\n    coef[coef != 0],\n    label=\"true coefficients\",\n    markerfmt=\"bx\",\n)\n\n(loc=\"best\")\n(\n    \"Lasso $R^2$: %.3f, Elastic Net $R^2$: %.3f\" % (r2_score_lasso, r2_score_enet)\n)\n()\n\n\n<img alt=\"Lasso $R^2$: 0.658, Elastic Net $R^2$: 0.643\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_lasso_and_elasticnet_001.png\" srcset=\"../../_images/sphx_glr_plot_lasso_and_elasticnet_001.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Simple Exponential Smoothing"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Method"
            ],
            [
                "statsmodels->Examples->Plotting->Box Plots->Bean Plots"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Comparison"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Lasso and Elastic Net for Sparse Signals->Plot"
            ]
        ]
    },
    "1025041": {
        "jupyter_code_cell": "not_null_Unemployment= data_features['Unemployment'] != -1\nplt.hist(not_null_Unemployment,range=(0,1))\nplt.xlabel('Unemployment')\nplt.ylabel('Frequency of Nan and not null values')\nplt.show()\nusers_2010 = data_features[data_features['Date'] >= '2010-01-01']\nusers_2010 = users_2010[users_2010['Date'] <= '2010-12-31']\nusers_2010\nusers_2011 = data_features[data_features['Date'] >= '2011-01-01']\nusers_2011 = users_2011[users_2011['Date'] <= '2011-12-31']\nusers_2011\nusers_2012 = data_features[data_features['Date'] >= '2012-01-01']\nusers_2012 = users_2012[users_2012['Date'] <= '2012-12-31']\nusers_2012\nusers_2013 = data_features[data_features['Date'] >= '2013-01-01']\nusers_2013 = users_2013[users_2013['Date'] <= '2013-12-31']\nusers_2013",
        "matched_tutorial_code_inds": [
            6401,
            2460,
            6116,
            5917,
            2298
        ],
        "matched_tutorial_codes": [
            "exog = endog['dln_consump']\nmod = sm.tsa.VARMAX(endog[['dln_inv', 'dln_inc']], order=(2,0), trend='n', exog=exog)\nres = mod.fit(maxiter=1000, disp=False)\nprint(res.summary())",
            "one_hot_poly_pipeline = (\n    (\n        transformers=[\n            (\"categorical\", one_hot_encoder, categorical_columns),\n            (\"one_hot_time\", one_hot_encoder, [\"hour\", \"weekday\", \"month\"]),\n        ],\n        remainder=\"passthrough\",\n    ),\n    (kernel=\"poly\", degree=2, n_components=300, random_state=0),\n    (alphas=alphas),\n)\nevaluate(one_hot_poly_pipeline, X, y, cv=ts_cv)",
            "sarimax_forecast = res.forecast(12, exog=det_proc.out_of_sample(12))\ndf = pd.concat([auto_reg_forecast, sarimax_forecast], axis=1)\ndf.columns = columns = [\"AutoReg\", \"SARIMAX\"]\ndf",
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "feature_importance = reg.feature_importances_\nsorted_idx = (feature_importance)\npos = (sorted_idx.shape[0]) + 0.5\nfig = (figsize=(12, 6))\n(1, 2, 1)\n(pos, feature_importance[sorted_idx], align=\"center\")\n(pos, (diabetes.feature_names)[sorted_idx])\n(\"Feature Importance (MDI)\")\n\nresult = (\n    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_idx = result.importances_mean.argsort()\n(1, 2, 2)\n(\n    result.importances[sorted_idx].T,\n    vert=False,\n    labels=(diabetes.feature_names)[sorted_idx],\n)\n(\"Permutation Importance (test set)\")\nfig.tight_layout()\n()\n\n\n<img alt=\"Feature Importance (MDI), Permutation Importance (test set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->VARMAX: Introduction->Example 1: VAR"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Modeling non-linear feature interactions with kernels"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Deterministic Terms->Using with other models"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "sklearn->Examples->Ensemble methods->Gradient Boosting regression->Plot feature importance"
            ]
        ]
    },
    "1405201": {
        "jupyter_code_cell": "from scipy.stats import mode\nlabels = np.zeros_like(clusters)\nfor i in range(4):\n    mask = (clusters == i)\n    labels[mask] = mode(training_target[mask])[0]\nfrom sklearn.metrics import accuracy_score\naccuracy_score(training_target.astype('int'), labels.astype('int'),normalize=False)",
        "matched_tutorial_code_inds": [
            2018,
            2308,
            1941,
            2345,
            2487
        ],
        "matched_tutorial_codes": [
            "from sklearn.cluster import \nimport matplotlib.pyplot as plt\n\nlabels = (graph, n_clusters=4, eigen_solver=\"arpack\")\nlabel_im = (mask.shape, -1.0)\nlabel_im[mask] = labels\n\nfig, axs = (nrows=1, ncols=2, figsize=(10, 5))\naxs[0].matshow(img)\naxs[1].matshow(label_im)\n\n()\n\n\n<img alt=\"plot segmentation toy\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_segmentation_toy_001.png\" srcset=\"../../_images/sphx_glr_plot_segmentation_toy_001.png\"/>",
            "from sklearn.ensemble import \nfrom sklearn.inspection import PartialDependenceDisplay\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nrng = (0)\n\nn_samples = 1000\nf_0 = rng.rand(n_samples)\nf_1 = rng.rand(n_samples)\nX = [f_0, f_1]\nnoise = rng.normal(loc=0.0, scale=0.01, size=n_samples)\n\n# y is positively correlated with f_0, and negatively correlated with f_1\ny = 5 * f_0 + (10 *  * f_0) - 5 * f_1 - (10 *  * f_1) + noise",
            "from sklearn.cluster import \nfrom sklearn.datasets import \nimport matplotlib.pyplot as plt\n\n# Generate sample data\nn_samples = 4000\nn_components = 4\n\nX, y_true = (\n    n_samples=n_samples, centers=n_components, cluster_std=0.60, random_state=0\n)\nX = X[:, ::-1]\n\n# Calculate seeds from k-means++\ncenters_init, indices = (X, n_clusters=4, random_state=0)\n\n# Plot init seeds along side sample data\n(1)\ncolors = [\"#4EACC5\", \"#FF9C34\", \"#4E9A06\", \"m\"]\n\nfor k, col in enumerate(colors):\n    cluster_data = y_true == k\n    (X[cluster_data, 0], X[cluster_data, 1], c=col, marker=\".\", s=10)\n\n(centers_init[:, 0], centers_init[:, 1], c=\"b\", s=50)\n(\"K-Means++ Initialization\")\n([])\n([])\n()",
            "from sklearn.ensemble import \nfrom sklearn.metrics import , \n\n\nall_models = {}\ncommon_params = dict(\n    learning_rate=0.05,\n    n_estimators=200,\n    max_depth=2,\n    min_samples_leaf=9,\n    min_samples_split=9,\n)\nfor alpha in [0.05, 0.5, 0.95]:\n    gbr = (loss=\"quantile\", alpha=alpha, **common_params)\n    all_models[\"q %1.2f\" % alpha] = gbr.fit(X_train, y_train)",
            "from sklearn.feature_selection import \nfrom time import \n\nthreshold = (importance)[-3] + 0.01\n\ntic = ()\nsfm = (ridge, threshold=threshold).fit(X, y)\ntoc = ()\nprint(f\"Features selected by SelectFromModel: {feature_names[sfm.get_support()]}\")\nprint(f\"Done in {toc - tic:.3f}s\")"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Clustering->Spectral clustering for image segmentation->Plotting four circles"
            ],
            [
                "sklearn->Examples->Ensemble methods->Monotonic Constraints"
            ],
            [
                "sklearn->Examples->Clustering->An example of K-Means++ initialization"
            ],
            [
                "sklearn->Examples->Ensemble methods->Prediction Intervals for Gradient Boosting Regression->Fitting non-linear quantile and least squares regressors"
            ],
            [
                "sklearn->Examples->Feature Selection->Model-based and sequential feature selection->Selecting features based on importance"
            ]
        ]
    },
    "196336": {
        "jupyter_code_cell": "df_city = pd.read_json('https://data.cityofnewyork.us/resource/rvhx-8trz.json')\ndf_city.info()\ndf_salaries = pd.read_json('https://data.cityofchicago.org/resource/tt4n-kn4t.json')\ndf_salaries.info()",
        "matched_tutorial_code_inds": [
            1746,
            5588,
            6317,
            3805,
            5953
        ],
        "matched_tutorial_codes": [
            "glove = data.fetch('glove.6B.50d.zip')\nemb_path = textproc.unzipper(glove, 'glove.6B.300d.txt')\nemb_matrix = textproc.loadGloveModel(emb_path)",
            "data2 = sm.datasets.scotland.load()\ndata2.exog = sm.add_constant(data2.exog, prepend=False)\nprint(data2.exog.head())\nprint(data2.endog.head())",
            "# Fit the model\nmod = sm.tsa.statespace.SARIMAX(data['ln_wpi'], trend='c', order=(1,1,1))\nres = mod.fit(disp=False)\nprint(res.summary())",
            "gs = web.DataReader(\"GS\", data_source='yahoo', start='2006-01-01',\n                    end='2010-01-01')\ngs.head().round(2)",
            "try:\n    kidney_table = pd.read_table(\"./kidney.table\")\nexcept:\n    url = \"http://stats191.stanford.edu/data/kidney.table\"\n    kidney_table = pd.read_csv(url, delim_whitespace=True)"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Generalized Linear Models Overview->GLM: Gamma for proportional count response->Load Scottish Parliament Voting data"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Example 2: Arima with additive seasonal effects"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA->Two-way ANOVA"
            ]
        ]
    },
    "601298": {
        "jupyter_code_cell": "def train(inputs, targets, weights, eta, n_iterations):\n    inputs = np.c_[inputs, -np.ones((len(inputs), 1))]\n    for n in range(n_iterations):\n        activations = g(inputs, weights);\n        weights -= eta*np.dot(np.transpose(inputs), activations - targets)\n    return(weights)\ninputs = AND[['x1','x2']]\ntarget = AND['y']\nw = train(inputs, target, w, 0.25, 10)",
        "matched_tutorial_code_inds": [
            347,
            35,
            3101,
            3102,
            736
        ],
        "matched_tutorial_codes": [
            "def preprocess(x, y):\n    return x.view(-1, 1, 28, 28), y\n\n\nclass WrappedDataLoader:\n    def __init__(self, dl, func):\n        self.dl = dl\n        self.func = func\n\n    def __len__(self):\n        return len(self.dl)\n\n    def __iter__(self):\n        batches = iter(self.dl)\n        for b in batches:\n            yield (self.func(*b))\n\ntrain_dl, valid_dl = get_data(, , bs)\ntrain_dl = WrappedDataLoader(train_dl, preprocess)\nvalid_dl = WrappedDataLoader(valid_dl, preprocess)",
            "def print_size_of_model(model, label=\"\"):\n    (model.state_dict(), \"temp.p\")\n    size=os.path.getsize(\"temp.p\")\n    print(\"model: \",label,' \\t','Size (KB):', size/1e3)\n    os.remove('temp.p')\n    return size\n\n# compare the sizes\nf=print_size_of_model(float_lstm,\"fp32\")\nq=print_size_of_model(quantized_lstm,\"int8\")\nprint(\"{0:.2f} times smaller\".format(f/q))",
            "def get_impute_knn_score(X_missing, y_missing):\n    imputer = (missing_values=, add_indicator=True)\n    knn_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n    return knn_impute_scores.mean(), knn_impute_scores.std()\n\n\nmses_california[2], stds_california[2] = get_impute_knn_score(\n    X_miss_california, y_miss_california\n)\nmses_diabetes[2], stds_diabetes[2] = get_impute_knn_score(\n    X_miss_diabetes, y_miss_diabetes\n)\nx_labels.append(\"KNN Imputation\")",
            "def get_impute_mean(X_missing, y_missing):\n    imputer = (missing_values=, strategy=\"mean\", add_indicator=True)\n    mean_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n    return mean_impute_scores.mean(), mean_impute_scores.std()\n\n\nmses_california[3], stds_california[3] = get_impute_mean(\n    X_miss_california, y_miss_california\n)\nmses_diabetes[3], stds_diabetes[3] = get_impute_mean(X_miss_diabetes, y_miss_diabetes)\nx_labels.append(\"Mean Imputation\")",
            "def compute_grad(sample, target):\n    sample = sample.unsqueeze(0)  # prepend batch dimension for processing\n    target = target.unsqueeze(0)\n\n    prediction = model(sample)\n     = loss_fn(prediction, target)\n\n    return (, list(()))\n\n\ndef compute_sample_grads(, ):\n    \"\"\" manually process each sample with per sample gradient \"\"\"\n    sample_grads = [compute_grad([i], [i]) for i in range(batch_size)]\n    sample_grads = zip(*sample_grads)\n    sample_grads = [(shards) for shards in sample_grads]\n    return sample_grads\n\nper_sample_grads = compute_sample_grads(, )"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Learning PyTorch->What is torch.nn really?->Wrapping DataLoader"
            ],
            [
                "torch->PyTorch Recipes->Dynamic Quantization->Steps->3. Look at Model Size"
            ],
            [
                "sklearn->Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score->kNN-imputation of the missing values"
            ],
            [
                "sklearn->Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score->Impute missing values with mean"
            ],
            [
                "torch->Frontend APIs->Per-sample-gradients->What is it?"
            ]
        ]
    },
    "1047227": {
        "jupyter_code_cell": "for i in gametes:\n    print(i[0])\nsmuts = [i['selected'] for i in gametes[0]]",
        "matched_tutorial_code_inds": [
            4407,
            3947,
            4124,
            1795,
            4025
        ],
        "matched_tutorial_codes": [
            "word_list = open('/usr/share/dict/words').readlines()\n word_list = map(str.strip, word_list)",
            "flights_wide_list = [col for _, col in flights_wide.items()]\nsns.relplot(data=flights_wide_list, kind=\"line\")\n",
            "tips = sns.load_dataset(\"tips\")\ng = sns.FacetGrid(tips, col=\"time\")\n",
            "encoded = enc.transform(([[\"dog\"], [\"snake\"], [\"cat\"], [\"rabbit\"]]))\n(encoded, columns=enc.get_feature_names_out())\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "tips = sns.load_dataset(\"tips\")\nsns.displot(tips, x=\"size\")\n"
        ],
        "matched_tutorial_paths": [
            [
                "scipy->Compressed Sparse Graph Routines (scipy.sparse.csgraph)->Example: Word Ladders"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing wide-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing wide-form data"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Conditional small multiples",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Conditional small multiples"
            ],
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 1.1->Grouping infrequent categories in OneHotEncoder"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size",
                "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size"
            ]
        ]
    },
    "197762": {
        "jupyter_code_cell": "sns.set(style=\"darkgrid\", palette=\"muted\")\nsns.violinplot(x=\"type_1\", y=\"attack\", data=df2, scale=\"count\")\nsns.swarmplot(x=\"type_1\", y=\"attack\", data=df2, color=\"black\", alpha=.4)\nplt.ylabel('Attack Power', labelpad=15)\nplt.xlabel('Type 1', labelpad=12)\nplt.title('Combined Plot: Attack Power by Type')\nrcParams['axes.titlepad'] = 10\nplt.show()\ndf3= df[(df['type_1']=='Water') | (df['type_1']=='Normal') \n                                | (df['type_1']=='Grass')]\ndf3= df3[['type_1','attack','defense','hp','speed']]",
        "matched_tutorial_code_inds": [
            3916,
            4085,
            4795,
            4796,
            4028
        ],
        "matched_tutorial_codes": [
            "sns.set_theme(style=\"ticks\", font_scale=1.25)\ng = sns.relplot(\n    data=penguins,\n    x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"body_mass_g\",\n    palette=\"crest\", marker=\"x\", s=100,\n)\ng.set_axis_labels(\"Bill length (mm)\", \"Bill depth (mm)\", labelpad=10)\ng.legend.set_title(\"Body mass (g)\")\ng.figure.set_size_inches(6.5, 4.5)\ng.ax.margins(.15)\ng.despine(trim=True)\n",
            "sns.catplot(data=titanic, x=\"deck\", kind=\"count\", palette=\"ch:.25\")\n",
            "plt.rcParams['figure.constrained_layout.use'] = True\nfig, axs = plt.subplots(2, 2, figsize=(3, 3))\nfor ax in axs.flat:\n    example_plot(ax)\n\n\n<img alt=\"Title, Title, Title, Title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_constrainedlayout_guide_018.png\" srcset=\"../../_images/sphx_glr_constrainedlayout_guide_018.png, ../../_images/sphx_glr_constrainedlayout_guide_018_2_0x.png 2.0x\"/>",
            "plt.rcParams['figure.constrained_layout.use'] = False\nfig = plt.figure(layout=\"constrained\")\n\ngs1 = gridspec.GridSpec(2, 1, figure=fig)\nax1 = fig.add_subplot(gs1[0])\nax2 = fig.add_subplot(gs1[1])\n\nexample_plot(ax1)\nexample_plot(ax2)\n\n\n<img alt=\"Title, Title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_constrainedlayout_guide_019.png\" srcset=\"../../_images/sphx_glr_constrainedlayout_guide_019.png, ../../_images/sphx_glr_constrainedlayout_guide_019_2_0x.png 2.0x\"/>",
            "sns.displot(tips, x=\"day\", shrink=.8)\n"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->Opinionated defaults and flexible customization"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing categorical data->Estimating central tendency->Bar plots",
                "seaborn->Plotting functions->Visualizing categorical data->Estimating central tendency->Bar plots"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Constrained Layout Guide->rcParams"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Constrained Layout Guide->Use with GridSpec"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size",
                "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size"
            ]
        ]
    },
    "267469": {
        "jupyter_code_cell": "import pandas as pd\nschedule = pd.read_csv('../data/UW_Trip_Data_4mo_QC_capacity.csv')\ndh_data = pd.read_csv('../data/4mo_deadhead_results.csv')\nfiftiers = dh_data[dh_data.PctDeadhead > .5]\nfiftiers['Run'] = fiftiers['Run'].astype('str')",
        "matched_tutorial_code_inds": [
            6588,
            2528,
            3391,
            3410,
            3225
        ],
        "matched_tutorial_codes": [
            "import pandas_datareader as pdr\nlevels = pdr.get_data_fred(['PCEPILFE', 'CPILFESL'], start='1999', end='2019').to_period('M')\ninfl = np.log(levels).diff().iloc[1:] * 1200\ninfl.columns = ['PCE', 'CPI']\n\n# Remove two outliers and de-mean the series\ninfl['PCE'].loc['2001-09':'2001-10'] = np.nan",
            "import pandas as pd\n\ndf = (grid_search.cv_results_)[\n    [\"param_n_components\", \"param_covariance_type\", \"mean_test_score\"]\n]\ndf[\"mean_test_score\"] = -df[\"mean_test_score\"]\ndf = df.rename(\n    columns={\n        \"param_n_components\": \"Number of components\",\n        \"param_covariance_type\": \"Type of covariance\",\n        \"mean_test_score\": \"BIC score\",\n    }\n)\ndf.sort_values(by=\"BIC score\").head()\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "import pandas as pd\n\nmean_scores = (grid.cv_results_[\"mean_test_score\"])\n# scores are in the order of param_grid iteration, which is alphabetical\nmean_scores = mean_scores.reshape(len(C_OPTIONS), -1, len(N_FEATURES_OPTIONS))\n# select score for best C\nmean_scores = mean_scores.max(axis=0)\n# create a dataframe to ease plotting\nmean_scores = (\n    mean_scores.T, index=N_FEATURES_OPTIONS, columns=reducer_labels\n)\n\nax = mean_scores.plot.bar()\nax.set_title(\"Comparing feature reduction techniques\")\nax.set_xlabel(\"Reduced number of features\")\nax.set_ylabel(\"Digit classification accuracy\")\nax.set_ylim((0, 1))\nax.legend(loc=\"upper left\")\n\n()\n\n\n<img alt=\"Comparing feature reduction techniques\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_compare_reduction_001.png\" srcset=\"../../_images/sphx_glr_plot_compare_reduction_001.png\"/>",
            "import pandas as pd\nfrom sklearn.decomposition import \n\npca = (n_components=2).fit(X_train)\nscaled_pca = (n_components=2).fit(scaled_X_train)\nX_train_transformed = pca.transform(X_train)\nX_train_std_transformed = scaled_pca.transform(scaled_X_train)\n\nfirst_pca_component = (\n    pca.components_[0], index=X.columns, columns=[\"without scaling\"]\n)\nfirst_pca_component[\"with scaling\"] = scaled_pca.components_[0]\nfirst_pca_component.plot.bar(\n    title=\"Weights of the first principal component\", figsize=(6, 8)\n)\n\n_ = ()\n\n\n<img alt=\"Weights of the first principal component\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_scaling_importance_002.png\" srcset=\"../../_images/sphx_glr_plot_scaling_importance_002.png\"/>",
            "import pandas as pd\n\nresults_df = (search.cv_results_)\nresults_df = results_df.sort_values(by=[\"rank_test_score\"])\nresults_df = results_df.set_index(\n    results_df[\"params\"].apply(lambda x: \"_\".join(str(val) for val in x.values()))\n).rename_axis(\"kernel\")\nresults_df[[\"params\", \"rank_test_score\", \"mean_test_score\", \"std_test_score\"]]\n\n\n\n\n\n\n\n\n<br/>\n<br/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor"
            ],
            [
                "sklearn->Examples->Gaussian Mixture Models->Gaussian Mixture Model Selection->Plot the BIC scores"
            ],
            [
                "sklearn->Examples->Pipelines and composite estimators->Selecting dimensionality reduction with Pipeline and GridSearchCV->Illustration of Pipeline and GridSearchCV"
            ],
            [
                "sklearn->Examples->Preprocessing->Importance of Feature Scaling->Effect of rescaling on a PCA dimensional reduction"
            ],
            [
                "sklearn->Examples->Model Selection->Statistical comparison of models using grid search"
            ]
        ]
    },
    "509262": {
        "jupyter_code_cell": "%load_ext watermark\n%watermark -d -v -m -p numpy,pandas,matplotlib,scikit-learn,seaborn\ntrain_df = pd.read_json(\"./input/train.json\")\nprint(train_df.shape)\ntrain_df.info()",
        "matched_tutorial_code_inds": [
            628,
            1053,
            5399,
            3861,
            4772
        ],
        "matched_tutorial_codes": [
            "import onnx\n\nonnx_model = onnx.load(\"super_resolution.onnx\")\nonnx.checker.check_model(onnx_model)",
            "import torch.quantization\n\nquantized_model = torch.quantization.quantize_dynamic(\n    model, {, }, dtype=\n)\nprint(quantized_model)",
            "rand_data = sm.datasets.randhie.load()\nrand_exog = rand_data.exog\nrand_exog = sm.add_constant(rand_exog, prepend=False)",
            "df = dd.read_parquet(\"data/indiv-*.parquet\", engine='pyarrow', columns=['occupation'])\n\nmost_common = df.occupation.value_counts().nlargest(100)\nmost_common.compute().sort_values(ascending=False)",
            "from cycler import cycler\ncc = (cycler(color=list('rgb')) +\n      cycler(linestyle=['-', '--', '-.']))\nfor d in cc:\n    print(d)"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Deploying PyTorch Models in Production->(optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime"
            ],
            [
                "torch->Model Optimization->(beta) Dynamic Quantization on an LSTM Word Language Model->4. Test dynamic quantization"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Poisson"
            ],
            [
                "pandas_toms_blog->Scaling->Using Dask"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Styling with cycler->Cycling through multiple properties"
            ]
        ]
    },
    "1164439": {
        "jupyter_code_cell": "plot_model(model_mlp, X_train, y_train, (X_valid, y_valid))\nprint(\"Train accuracy: {:.1f}%\".format(model_mlp.score(X_train, y_train)[\"acc\"] * 100))\nprint(\"Valid accuracy: {:.1f}%\".format(model_mlp.score(X_valid, y_valid)[\"acc\"] * 100))",
        "matched_tutorial_code_inds": [
            4941,
            4942,
            4939,
            2117,
            4937
        ],
        "matched_tutorial_codes": [
            "plot_color_gradients('Qualitative',\n                     ['Pastel1', 'Pastel2', 'Paired', 'Accent', 'Dark2',\n                      'Set1', 'Set2', 'Set3', 'tab10', 'tab20', 'tab20b',\n                      'tab20c'])\n\n\n<img alt=\"Qualitative colormaps\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_colormaps_006.png\" srcset=\"../../_images/sphx_glr_colormaps_006.png, ../../_images/sphx_glr_colormaps_006_2_0x.png 2.0x\"/>",
            "plot_color_gradients('Miscellaneous',\n                     ['flag', 'prism', 'ocean', 'gist_earth', 'terrain',\n                      'gist_stern', 'gnuplot', 'gnuplot2', 'CMRmap',\n                      'cubehelix', 'brg', 'gist_rainbow', 'rainbow', 'jet',\n                      'turbo', 'nipy_spectral', 'gist_ncar'])\n\nplt.show()\n\n\n<img alt=\"Miscellaneous colormaps\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_colormaps_007.png\" srcset=\"../../_images/sphx_glr_colormaps_007.png, ../../_images/sphx_glr_colormaps_007_2_0x.png 2.0x\"/>",
            "plot_color_gradients('Diverging',\n                     ['PiYG', 'PRGn', 'BrBG', 'PuOr', 'RdGy', 'RdBu', 'RdYlBu',\n                      'RdYlGn', 'Spectral', 'coolwarm', 'bwr', 'seismic'])\n\n\n<img alt=\"Diverging colormaps\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_colormaps_004.png\" srcset=\"../../_images/sphx_glr_colormaps_004.png, ../../_images/sphx_glr_colormaps_004_2_0x.png 2.0x\"/>",
            "plot_gallery(\"Faces from dataset\", faces_centered[:n_components], cmap=plt.cm.RdBu)\n\n\n<img alt=\"Faces from dataset\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_010.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_010.png\"/>",
            "plot_color_gradients('Sequential',\n                     ['Greys', 'Purples', 'Blues', 'Greens', 'Oranges', 'Reds',\n                      'YlOrBr', 'YlOrRd', 'OrRd', 'PuRd', 'RdPu', 'BuPu',\n                      'GnBu', 'PuBu', 'YlGnBu', 'PuBuGn', 'BuGn', 'YlGn'])\n\n\n<img alt=\"Sequential colormaps\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_colormaps_002.png\" srcset=\"../../_images/sphx_glr_colormaps_002.png, ../../_images/sphx_glr_colormaps_002_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Colors->Choosing Colormaps in Matplotlib->Classes of colormaps->Qualitative"
            ],
            [
                "matplotlib->Tutorials->Colors->Choosing Colormaps in Matplotlib->Classes of colormaps->Miscellaneous"
            ],
            [
                "matplotlib->Tutorials->Colors->Choosing Colormaps in Matplotlib->Classes of colormaps->Diverging"
            ],
            [
                "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition: Dictionary learning"
            ],
            [
                "matplotlib->Tutorials->Colors->Choosing Colormaps in Matplotlib->Classes of colormaps->Sequential"
            ]
        ]
    },
    "833395": {
        "jupyter_code_cell": "from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn import linear_model, metrics, svm, preprocessing\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom xgboost import XGBClassifier as xgb\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.ensemble import VotingClassifier\nlogreg_hitters_params = optimal_parameters.lr_hitters_params(to_predict_hitters,x_hitters, hitter_predictions)",
        "matched_tutorial_code_inds": [
            1844,
            3086,
            1893,
            2870,
            3014
        ],
        "matched_tutorial_codes": [
            "from sklearn.datasets import \nfrom sklearn.svm import \nfrom sklearn.linear_model import \nfrom sklearn.preprocessing import \nfrom sklearn.pipeline import \nfrom sklearn.ensemble import \nfrom sklearn.model_selection import \n\nX, y = (return_X_y=True)\nestimators = [\n    (\"rf\", (n_estimators=10, random_state=42)),\n    (\"svr\", ((), (random_state=42))),\n]\nclf = (estimators=estimators, final_estimator=())\nX_train, X_test, y_train, y_test = (X, y, stratify=y, random_state=42)\nclf.fit(X_train, y_train).score(X_test, y_test)",
            "from sklearn.datasets import \nfrom sklearn.preprocessing import \nfrom sklearn.pipeline import \nfrom sklearn.linear_model import \nfrom sklearn.model_selection import \n\nX, y = (data_id=1464, return_X_y=True, parser=\"pandas\")\nX_train, X_test, y_train, y_test = (X, y, stratify=y)\n\nclf = ((), (random_state=0))\nclf.fit(X_train, y_train)",
            "from sklearn.calibration import \nfrom sklearn.metrics import \nfrom sklearn.naive_bayes import \n\n# With no calibration\nclf = ()\nclf.fit(X_train, y_train)  # GaussianNB itself does not support sample-weights\nprob_pos_clf = clf.predict_proba(X_test)[:, 1]\n\n# With isotonic calibration\nclf_isotonic = (clf, cv=2, method=\"isotonic\")\nclf_isotonic.fit(X_train, y_train, sample_weight=sw_train)\nprob_pos_isotonic = clf_isotonic.predict_proba(X_test)[:, 1]\n\n# With sigmoid calibration\nclf_sigmoid = (clf, cv=2, method=\"sigmoid\")\nclf_sigmoid.fit(X_train, y_train, sample_weight=sw_train)\nprob_pos_sigmoid = clf_sigmoid.predict_proba(X_test)[:, 1]\n\nprint(\"Brier score losses: (the smaller the better)\")\n\nclf_score = (y_test, prob_pos_clf, sample_weight=sw_test)\nprint(\"No calibration: %1.3f\" % clf_score)\n\nclf_isotonic_score = (y_test, prob_pos_isotonic, sample_weight=sw_test)\nprint(\"With isotonic calibration: %1.3f\" % clf_isotonic_score)\n\nclf_sigmoid_score = (y_test, prob_pos_sigmoid, sample_weight=sw_test)\nprint(\"With sigmoid calibration: %1.3f\" % clf_sigmoid_score)",
            "from sklearn.linear_model import \nfrom sklearn.metrics import \n\nfeatures_names = [\"experience\", \"parent hourly wage\", \"college degree\", \"ability\"]\n\nregressor_with_ability = ()\nregressor_with_ability.fit(X_train[features_names], y_train)\ny_pred_with_ability = regressor_with_ability.predict(X_test[features_names])\nR2_with_ability = (y_test, y_pred_with_ability)\n\nprint(f\"R2 score with ability: {R2_with_ability:.3f}\")",
            "from sklearn.pipeline import \nfrom sklearn.preprocessing import , \nfrom sklearn.linear_model import \n\nsteps = [\n    (\"standard_scaler\", ()),\n    (\"polynomial\", (degree=3)),\n    (\"classifier\", (C=2.0)),\n]\npipe = (steps)\npipe  # click on the diagram below to see the details of each step"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 0.22->Stacking Classifier and Regressor"
            ],
            [
                "sklearn->Examples->Miscellaneous->Visualizations with Display Objects->Load Data and train model"
            ],
            [
                "sklearn->Examples->Calibration->Probability calibration of classifiers->Gaussian Naive-Bayes"
            ],
            [
                "sklearn->Examples->Inspection->Failure of Machine Learning to infer causal effects->Income prediction with fully observed variables"
            ],
            [
                "sklearn->Examples->Miscellaneous->Displaying Pipelines->Displaying a Pipeline Chaining Multiple Preprocessing Steps & Classifier"
            ]
        ]
    },
    "1189520": {
        "jupyter_code_cell": "tracks = list(data_json.keys())\nlyrics = list(data_json.values())\nenglish = stopwords.words('english')\nfrench = stopwords.words('french')\nspanish = stopwords.words('spanish')\nportuguese = stopwords.words('portuguese')\nmine = ['yeah', 'get', 'got', 'would', 'nan', 'ca']\nstop = english + french + spanish + portuguese + mine",
        "matched_tutorial_code_inds": [
            78,
            1204,
            3646,
            3505,
            4655
        ],
        "matched_tutorial_codes": [
            "batch_size = 512 # Try, for example, 128, 256, 513.\nin_size = 4096\nout_size = 4096\nnum_layers = 3\nnum_batches = 50\nepochs = 3\n\n# Creates data in default precision.\n# The same data is used for both default and mixed precision trials below.\n# You don't need to manually change inputs' dtype when enabling mixed precision.\ndata = [(batch_size, in_size, device=\"cuda\") for _ in range(num_batches)]\ntargets = [(batch_size, out_size, device=\"cuda\") for _ in range(num_batches)]\n\nloss_fn = ().cuda()",
            "batch_size = 32\nmax_sequence_len = 256\n = (batch_size, max_sequence_len,\n               embed_dimension, device=device, =)\nprint(\n    f\"The non compiled module runs in  {benchmark_torch_function_in_microseconds(model, ):.3f} microseconds\")\n\n\n = (model)\n# Let's compile it\n()\nprint(\n    f\"The compiled module runs in  {benchmark_torch_function_in_microseconds(, ):.3f} microseconds\")",
            "stations = pd.io.json.json_normalize(js['features']).id\nurl = (\"http://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?\"\n       \"&amp;data=tmpf&amp;data=relh&amp;data=sped&amp;data=mslp&amp;data=p01i&amp;data=vsby&amp;data=gust_mph&amp;data=skyc1&amp;data=skyc2&amp;data=skyc3\"\n       \"&amp;tz=Etc/UTC&amp;format=comma&amp;latlon=no\"\n       \"&amp;{start:year1=%Y&amp;month1=%m&amp;day1=%d}\"\n       \"&amp;{end:year2=%Y&amp;month2=%m&amp;day2=%d}&amp;{stations}\")\nstations = \"&amp;\".join(\"station=%s\" % s for s in stations)\nstart = pd.Timestamp('2014-01-01')\nend=pd.Timestamp('2014-01-31')\n\nweather = (pd.read_csv(url.format(start=start, end=end, stations=stations),\n                       comment=\"#\"))",
            "indices = (len(results))\n\nresults = [[x[i] for x in results] for i in range(4)]\n\nclf_names, score, training_time, test_time = results\ntraining_time = (training_time)\ntest_time = (test_time)\n\nfig, ax1 = (figsize=(10, 8))\nax1.scatter(score, training_time, s=60)\nax1.set(\n    title=\"Score-training time trade-off\",\n    yscale=\"log\",\n    xlabel=\"test accuracy\",\n    ylabel=\"training time (s)\",\n)\nfig, ax2 = (figsize=(10, 8))\nax2.scatter(score, test_time, s=60)\nax2.set(\n    title=\"Score-test time trade-off\",\n    yscale=\"log\",\n    xlabel=\"test accuracy\",\n    ylabel=\"test time (s)\",\n)\n\nfor i, txt in enumerate(clf_names):\n    ax1.annotate(txt, (score[i], training_time[i]))\n    ax2.annotate(txt, (score[i], test_time[i]))\n\n\n\n<img alt=\"Score-training time trade-off\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_document_classification_20newsgroups_005.png\" srcset=\"../../_images/sphx_glr_plot_document_classification_20newsgroups_005.png\"/>\n<img alt=\"Score-test time trade-off\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_document_classification_20newsgroups_006.png\" srcset=\"../../_images/sphx_glr_plot_document_classification_20newsgroups_006.png\"/>",
            "names = ['group_a', 'group_b', 'group_c']\nvalues = [1, 10, 100]\n\nplt.figure(figsize=(9, 3))\n\nplt.subplot(131)\nplt.bar(names, values)\nplt.subplot(132)\nplt.scatter(names, values)\nplt.subplot(133)\nplt.plot(names, values)\nplt.suptitle('Categorical Plotting')\nplt.show()\n\n\n<img alt=\"Categorical Plotting\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_006.png\" srcset=\"../../_images/sphx_glr_pyplot_006.png, ../../_images/sphx_glr_pyplot_006_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "torch->PyTorch Recipes->Automatic Mixed Precision->A simple network"
            ],
            [
                "torch->Model Optimization->Using SDPA with torch.compile"
            ],
            [
                "pandas_toms_blog->Indexes"
            ],
            [
                "sklearn->Examples->Working with text documents->Classification of text documents using sparse features->Plot accuracy, training and test time of each classifier"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with categorical variables"
            ]
        ]
    },
    "1213431": {
        "jupyter_code_cell": "print('==== Before EditedNearestNeighbours ====')\ntotal_number_rides = len(y)\ntotal_number_subscribers = Counter(y)[0]\ntotal_number_nonsubscribers = Counter(y)[1]\nsubs_perc = 100*total_number_subscribers / total_number_rides\nnonsubs_perc = 100*total_number_nonsubscribers / total_number_rides\nprint('Total number of Rides: %d' % total_number_rides)\nprint('Total number of Rides by Subscribers: %d (%.2f%% of total rides)' % (total_number_subscribers, subs_perc))\nprint('Total number of Rides by Non-subscribers: %d (%.2f%% of total rides)' % (total_number_nonsubscribers, nonsubs_perc))\ntl = TomekLinks(n_jobs=4, ratio='majority')\nX_res, y_res = tl.fit_sample(X, y)",
        "matched_tutorial_code_inds": [
            2372,
            2374,
            2910,
            5229,
            1995
        ],
        "matched_tutorial_codes": [
            "print(\"Fitting the classifier to the training set\")\nt0 = ()\nparam_grid = {\n    \"C\": loguniform(1e3, 1e5),\n    \"gamma\": loguniform(1e-4, 1e-1),\n}\nclf = (\n    (kernel=\"rbf\", class_weight=\"balanced\"), param_grid, n_iter=10\n)\nclf = clf.fit(X_train_pca, y_train)\nprint(\"done in %0.3fs\" % (() - t0))\nprint(\"Best estimator found by grid search:\")\nprint(clf.best_estimator_)",
            "print(\"Predicting people's names on the test set\")\nt0 = ()\ny_pred = clf.predict(X_test_pca)\nprint(\"done in %0.3fs\" % (() - t0))\n\nprint((y_test, y_pred, target_names=target_names))\n(\n    clf, X_test_pca, y_test, display_labels=target_names, xticks_rotation=\"vertical\"\n)\n()\n()\n\n\n<img alt=\"plot face recognition\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_face_recognition_001.png\" srcset=\"../../_images/sphx_glr_plot_face_recognition_001.png\"/>",
            "print(\"Computing partial dependence plots...\")\nfeatures_info = {\n    \"features\": [\"temp\", \"humidity\", (\"temp\", \"humidity\")],\n    \"kind\": \"average\",\n}\n_, ax = (ncols=3, figsize=(10, 4), constrained_layout=True)\ntic = ()\ndisplay = (\n    hgbdt_model,\n    X_train,\n    **features_info,\n    ax=ax,\n    **common_params,\n)\nprint(f\"done in {() - tic:.3f}s\")\n_ = display.figure_.suptitle(\n    \"1-way vs 2-way of numerical PDP using gradient boosting\", fontsize=16\n)\n\n\n<img alt=\"1-way vs 2-way of numerical PDP using gradient boosting\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_partial_dependence_006.png\" srcset=\"../../_images/sphx_glr_plot_partial_dependence_006.png\"/>",
            "print(sm.datasets.copper.DESCRLONG)\n\ndta = sm.datasets.copper.load_pandas().data\ndta.index = pd.date_range(\"1951-01-01\", \"1975-01-01\", freq=\"AS\")\nendog = dta[\"WORLDCONSUMPTION\"]\n\n# To the regressors in the dataset, we add a column of ones for an intercept\nexog = sm.add_constant(\n    dta[[\"COPPERPRICE\", \"INCOMEINDEX\", \"ALUMPRICE\", \"INVENTORYINDEX\"]]\n)",
            "print(\"Compute structured hierarchical clustering...\")\nst = ()\nward = (\n    n_clusters=6, connectivity=connectivity, linkage=\"ward\"\n).fit(X)\nelapsed_time = () - st\nlabel = ward.labels_\nprint(f\"Elapsed time: {elapsed_time:.2f}s\")\nprint(f\"Number of points: {label.size}\")"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Examples based on real world datasets->Faces recognition example using eigenfaces and SVMs"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Faces recognition example using eigenfaces and SVMs"
            ],
            [
                "sklearn->Examples->Inspection->Partial Dependence and Individual Conditional Expectation Plots->2D interaction plots"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 1: Copper"
            ],
            [
                "sklearn->Examples->Clustering->Hierarchical clustering: structured vs unstructured ward->Compute clustering"
            ]
        ]
    },
    "334256": {
        "jupyter_code_cell": "from urllib.request import urlopen\nfrom datetime import timedelta, date, datetime\nfrom dateutil import tz\nimport json\nimport pandas as pd\nimport time\nimport os.path\ndef daterange(start_date, end_date):\n    for n in range(int ((end_date - start_date).days)):\n        yield start_date + timedelta(n)\nstart_date = date(2017, 6, 30)\nend_date = date(2017, 11, 1)\ndates = []\nfor single_date in daterange(start_date, end_date):\n        dates.append(single_date.strftime(\"%Y-%m-%d\"))",
        "matched_tutorial_code_inds": [
            5889,
            2308,
            3241,
            1849,
            1864
        ],
        "matched_tutorial_codes": [
            "from urllib.request import urlopen\nimport numpy as np\n\nnp.set_printoptions(precision=4, suppress=True)\n\nimport pandas as pd\n\npd.set_option(\"display.width\", 100)\nimport matplotlib.pyplot as plt\nfrom statsmodels.formula.api import ols\nfrom statsmodels.graphics.api import interaction_plot, abline_plot\nfrom statsmodels.stats.anova import anova_lm\n\ntry:\n    salary_table = pd.read_csv(\"salary.table\")\nexcept:  # recent pandas can read URL without urlopen\n    url = \"http://stats191.stanford.edu/data/salary.table\"\n    fh = urlopen(url)\n    salary_table = pd.read_table(fh)\n    salary_table.to_csv(\"salary.table\")\n\nE = salary_table.E\nM = salary_table.M\nX = salary_table.X\nS = salary_table.S",
            "from sklearn.ensemble import \nfrom sklearn.inspection import PartialDependenceDisplay\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nrng = (0)\n\nn_samples = 1000\nf_0 = rng.rand(n_samples)\nf_1 = rng.rand(n_samples)\nX = [f_0, f_1]\nnoise = rng.normal(loc=0.0, scale=0.01, size=n_samples)\n\n# y is positively correlated with f_0, and negatively correlated with f_1\ny = 5 * f_0 + (10 *  * f_0) - 5 * f_1 - (10 *  * f_1) + noise",
            "from itertools import \nfrom math import \n\nn_comparisons = (len(model_scores)) / (\n    (2) * (len(model_scores) - 2)\n)\npairwise_t_test = []\n\nfor model_i, model_k in (range(len(model_scores)), 2):\n    model_i_scores = model_scores.iloc[model_i].values\n    model_k_scores = model_scores.iloc[model_k].values\n    differences = model_i_scores - model_k_scores\n    t_stat, p_val = compute_corrected_ttest(differences, df, n_train, n_test)\n    p_val *= n_comparisons  # implement Bonferroni correction\n    # Bonferroni can output p-values higher than 1\n    p_val = 1 if p_val  1 else p_val\n    pairwise_t_test.append(\n        [model_scores.index[model_i], model_scores.index[model_k], t_stat, p_val]\n    )\n\npairwise_comp_df = (\n    pairwise_t_test, columns=[\"model_1\", \"model_2\", \"t_stat\", \"p_val\"]\n).round(3)\npairwise_comp_df\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "from tempfile import \nfrom sklearn.neighbors import \nfrom sklearn.manifold import \nfrom sklearn.pipeline import \n\nX, y = (random_state=0)\n\nwith (prefix=\"sklearn_cache_\") as tmpdir:\n    estimator = (\n        (n_neighbors=10, mode=\"distance\"),\n        (n_neighbors=10, metric=\"precomputed\"),\n        memory=tmpdir,\n    )\n    estimator.fit(X)\n\n    # We can decrease the number of neighbors and the graph will not be\n    # recomputed.\n    estimator.set_params(isomap__n_neighbors=5)\n    estimator.fit(X)",
            "from collections import \nimport operator\nfrom time import \n\nimport numpy as np\n\nfrom sklearn.cluster import \nfrom sklearn.cluster import \nfrom sklearn.datasets import \nfrom sklearn.feature_extraction.text import \nfrom sklearn.metrics.cluster import \n\n\ndef number_normalizer(tokens):\n    \"\"\"Map all numeric tokens to a placeholder.\n\n    For many applications, tokens that begin with a number are not directly\n    useful, but the fact that such a token exists can be relevant.  By applying\n    this form of dimensionality reduction, some methods may perform better.\n    \"\"\"\n    return (\"#NUMBER\" if token[0].isdigit() else token for token in tokens)\n\n\nclass NumberNormalizingVectorizer():\n    def build_tokenizer(self):\n        tokenize = super().build_tokenizer()\n        return lambda doc: list(number_normalizer(tokenize(doc)))\n\n\n# exclude 'comp.os.ms-windows.misc'\ncategories = [\n    \"alt.atheism\",\n    \"comp.graphics\",\n    \"comp.sys.ibm.pc.hardware\",\n    \"comp.sys.mac.hardware\",\n    \"comp.windows.x\",\n    \"misc.forsale\",\n    \"rec.autos\",\n    \"rec.motorcycles\",\n    \"rec.sport.baseball\",\n    \"rec.sport.hockey\",\n    \"sci.crypt\",\n    \"sci.electronics\",\n    \"sci.med\",\n    \"sci.space\",\n    \"soc.religion.christian\",\n    \"talk.politics.guns\",\n    \"talk.politics.mideast\",\n    \"talk.politics.misc\",\n    \"talk.religion.misc\",\n]\nnewsgroups = (categories=categories)\ny_true = newsgroups.target\n\nvectorizer = NumberNormalizingVectorizer(stop_words=\"english\", min_df=5)\ncocluster = (\n    n_clusters=len(categories), svd_method=\"arpack\", random_state=0\n)\nkmeans = (\n    n_clusters=len(categories), batch_size=20000, random_state=0, n_init=3\n)\n\nprint(\"Vectorizing...\")\nX = vectorizer.fit_transform(newsgroups.data)\n\nprint(\"Coclustering...\")\nstart_time = ()\ncocluster.fit(X)\ny_cocluster = cocluster.row_labels_\nprint(\n    \"Done in {:.2f}s. V-measure: {:.4f}\".format(\n        () - start_time, (y_cocluster, y_true)\n    )\n)\n\nprint(\"MiniBatchKMeans...\")\nstart_time = ()\ny_kmeans = kmeans.fit_predict(X)\nprint(\n    \"Done in {:.2f}s. V-measure: {:.4f}\".format(\n        () - start_time, (y_kmeans, y_true)\n    )\n)\n\nfeature_names = vectorizer.get_feature_names_out()\ndocument_names = list(newsgroups.target_names[i] for i in newsgroups.target)\n\n\ndef bicluster_ncut(i):\n    rows, cols = cocluster.get_indices(i)\n    if not ((rows) and (cols)):\n        import sys\n\n        return sys.float_info.max\n    row_complement = ((cocluster.rows_[i]))[0]\n    col_complement = ((cocluster.columns_[i]))[0]\n    # Note: the following is identical to X[rows[:, np.newaxis],\n    # cols].sum() but much faster in scipy &lt;= 0.16\n    weight = X[rows][:, cols].sum()\n    cut = X[row_complement][:, cols].sum() + X[rows][:, col_complement].sum()\n    return cut / weight\n\n\ndef most_common(d):\n    \"\"\"Items of a defaultdict(int) with the highest values.\n\n    Like Counter.most_common in Python =2.7.\n    \"\"\"\n    return sorted(d.items(), key=(1), reverse=True)\n\n\nbicluster_ncuts = list(bicluster_ncut(i) for i in range(len(newsgroups.target_names)))\nbest_idx = (bicluster_ncuts)[:5]\n\nprint()\nprint(\"Best biclusters:\")\nprint(\"----------------\")\nfor idx, cluster in enumerate(best_idx):\n    n_rows, n_cols = cocluster.get_shape(cluster)\n    cluster_docs, cluster_words = cocluster.get_indices(cluster)\n    if not len(cluster_docs) or not len(cluster_words):\n        continue\n\n    # categories\n    counter = (int)\n    for i in cluster_docs:\n        counter[document_names[i]] += 1\n    cat_string = \", \".join(\n        \"{:.0f}% {}\".format(float(c) / n_rows * 100, name)\n        for name, c in most_common(counter)[:3]\n    )\n\n    # words\n    out_of_cluster_docs = cocluster.row_labels_ != cluster\n    out_of_cluster_docs = (out_of_cluster_docs)[0]\n    word_col = X[:, cluster_words]\n    word_scores = (\n        word_col[cluster_docs, :].sum(axis=0)\n        - word_col[out_of_cluster_docs, :].sum(axis=0)\n    )\n    word_scores = word_scores.ravel()\n    important_words = list(\n        feature_names[cluster_words[i]] for i in word_scores.argsort()[:-11:-1]\n    )\n\n    print(\"bicluster {} : {} documents, {} words\".format(idx, n_rows, n_cols))\n    print(\"categories   : {}\".format(cat_string))\n    print(\"words        : {}\\n\".format(\", \".join(important_words)))"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "sklearn->Examples->Ensemble methods->Monotonic Constraints"
            ],
            [
                "sklearn->Examples->Model Selection->Statistical comparison of models using grid search->Pairwise comparison of all models: frequentist approach"
            ],
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 0.22->Precomputed sparse nearest neighbors graph"
            ],
            [
                "sklearn->Examples->Biclustering->Biclustering documents with the Spectral Co-clustering algorithm"
            ]
        ]
    },
    "913352": {
        "jupyter_code_cell": "bureau_balance = pd.read_csv('../input/Home Credit Default Risk/bureau_balance.csv').sort_values(['SK_ID_BUREAU', 'MONTHS_BALANCE']).reset_index(drop = True)\nbureau_balance = convert_types(bureau_balance, print_info=True)\nbureau_balance = bureau_balance.replace(365243, np.nan)\nprint(bureau_balance.shape)\nbureau_balance.head(5)\ntemp_agg = agg_numeric(bureau_balance, parent_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\ntemp_count = count_categorical(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\ntemp_merged = temp_agg.merge(temp_count, on = 'SK_ID_BUREAU', how ='left')",
        "matched_tutorial_code_inds": [
            1685,
            5917,
            1523,
            2116,
            5956
        ],
        "matched_tutorial_codes": [
            "pollutant_data = np.loadtxt(\"air-quality-data.csv\", dtype=float, delimiter=\",\",\n                            skiprows=1, usecols=range(1, 8))\npollutants_A = pollutant_data[:, 0:5]\npollutants_B = pollutant_data[:, 5:]\n\nprint(pollutants_A.shape)\nprint(pollutants_B.shape)",
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "transistor_count_predicted = np.exp(B) * np.exp(A * year)\ntransistor_Moores_law = Moores_law(year)\nplt.style.use(\"fivethirtyeight\")\nplt.semilogy(year, transistor_count, \"s\", label=\"MOS transistor count\")\nplt.semilogy(year, transistor_count_predicted, label=\"linear regression\")\n\n\nplt.plot(year, transistor_Moores_law, label=\"Moore's Law\")\nplt.title(\n    \"MOS transistor count per microprocessor\\n\"\n    + \"every two years \\n\"\n    + \"Transistor count was x{:.2f} higher\".format(np.exp(A * 2))\n)\nplt.xlabel(\"year introduced\")\nplt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\nplt.ylabel(\"# of transistors\\nper microprocessor\")",
            "fa_estimator = (n_components=n_components, max_iter=20)\nfa_estimator.fit(faces_centered)\nplot_gallery(\"Factor Analysis (FA)\", fa_estimator.components_[:n_components])\n\n# --- Pixelwise variance\n(figsize=(3.2, 3.6), facecolor=\"white\", tight_layout=True)\nvec = fa_estimator.noise_variance_\nvmax = max(vec.max(), -vec.min())\n(\n    vec.reshape(image_shape),\n    cmap=plt.cm.gray,\n    interpolation=\"nearest\",\n    vmin=-vmax,\n    vmax=vmax,\n)\n(\"off\")\n(\"Pixelwise variance from \\n Factor Analysis (FA)\", size=16, wrap=True)\n(orientation=\"horizontal\", shrink=0.8, pad=0.03)\n()\n\n\n\n<img alt=\"Factor Analysis (FA)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_008.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_008.png\"/>\n<img alt=\"Pixelwise variance from   Factor Analysis (FA)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_009.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_009.png\"/>",
            "kidney_lm = ols(\"np.log(Days+1) ~ C(Duration) * C(Weight)\", data=kt).fit()\n\ntable10 = anova_lm(kidney_lm)\n\nprint(\n    anova_lm(ols(\"np.log(Days+1) ~ C(Duration) + C(Weight)\", data=kt).fit(), kidney_lm)\n)\nprint(\n    anova_lm(\n        ols(\"np.log(Days+1) ~ C(Duration)\", data=kt).fit(),\n        ols(\"np.log(Days+1) ~ C(Duration) + C(Weight, Sum)\", data=kt).fit(),\n    )\n)\nprint(\n    anova_lm(\n        ols(\"np.log(Days+1) ~ C(Weight)\", data=kt).fit(),\n        ols(\"np.log(Days+1) ~ C(Duration) + C(Weight, Sum)\", data=kt).fit(),\n    )\n)"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Building the dataset"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy->Calculating the historical growth curve for transistors"
            ],
            [
                "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition->Factor Analysis components - FA"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA->Two-way ANOVA"
            ]
        ]
    },
    "627501": {
        "jupyter_code_cell": "sns.set_context(\"notebook\", font_scale=3)\nsns.set_style(\"darkgrid\")\ng = sns.lmplot(x='age', y='survived', data=titanic_df, y_jitter=.02, logistic=True, size=6, aspect=4)\ng.set(xlim=(0,80),title='Survival Rate of All Passengers\\nUsing Logistic Regression')\ng = sns.lmplot(x='age', y='survived', hue='pclass', data=titanic_df, y_jitter=.02, logistic=True, size=6, aspect=4)\ng.set(xlim=(0,80),title='Survival Rate of All Passengers\\nSeperated by Class\\nUsing Logistic Regression')",
        "matched_tutorial_code_inds": [
            3916,
            3904,
            4158,
            6795,
            5917
        ],
        "matched_tutorial_codes": [
            "sns.set_theme(style=\"ticks\", font_scale=1.25)\ng = sns.relplot(\n    data=penguins,\n    x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"body_mass_g\",\n    palette=\"crest\", marker=\"x\", s=100,\n)\ng.set_axis_labels(\"Bill length (mm)\", \"Bill depth (mm)\", labelpad=10)\ng.legend.set_title(\"Body mass (g)\")\ng.figure.set_size_inches(6.5, 4.5)\ng.ax.margins(.15)\ng.despine(trim=True)\n",
            "dots = sns.load_dataset(\"dots\")\nsns.relplot(\n    data=dots, kind=\"line\",\n    x=\"time\", y=\"firing_rate\", col=\"align\",\n    hue=\"choice\", size=\"coherence\", style=\"choice\",\n    facet_kws=dict(sharex=False),\n)\n",
            "f = plt.figure(figsize=(6, 6))\ngs = f.add_gridspec(2, 2)\n\nwith sns.axes_style(\"darkgrid\"):\n    ax = f.add_subplot(gs[0, 0])\n    sinplot(6)\n\nwith sns.axes_style(\"white\"):\n    ax = f.add_subplot(gs[0, 1])\n    sinplot(6)\n\nwith sns.axes_style(\"ticks\"):\n    ax = f.add_subplot(gs[1, 0])\n    sinplot(6)\n\nwith sns.axes_style(\"whitegrid\"):\n    ax = f.add_subplot(gs[1, 1])\n    sinplot(6)\n\nf.tight_layout()\n",
            "x1n = np.linspace(20.5, 25, 10)\nXnew = np.column_stack((x1n, np.sin(x1n), (x1n - 5) ** 2))\nXnew = sm.add_constant(Xnew)\nynewpred = olsres.predict(Xnew)  # predict out of sample\nprint(ynewpred)",
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->Opinionated defaults and flexible customization"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics"
            ],
            [
                "seaborn->User guide and tutorial->Controlling figure aesthetics->Temporarily setting figure style",
                "seaborn->Figure aesthetics->Controlling figure aesthetics->Temporarily setting figure style"
            ],
            [
                "statsmodels->Examples->User Notes->Prediction->Create a new sample of explanatory variables Xnew, predict and plot"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ]
        ]
    },
    "597055": {
        "jupyter_code_cell": "df_data[\"FareBand\"] = pd.qcut(df_data['Fare'], 4, labels = [1, 2, 3, 4]).astype('int')\ndf_train[\"FareBand\"] = pd.qcut(df_train['Fare'], 4, labels = [1, 2, 3, 4]).astype('int')\ndf_test[\"FareBand\"] = pd.qcut(df_test['Fare'], 4, labels = [1, 2, 3, 4]).astype('int')\nembarked_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\ndf_data[\"Embarked\"] = df_data[\"Embarked\"].map(embarked_mapping)\ndf_train[\"Embarked\"] = df_data[\"Embarked\"][:891]\ndf_test[\"Embarked\"] = df_data[\"Embarked\"][891:]",
        "matched_tutorial_code_inds": [
            5917,
            6253,
            2298,
            2460,
            2844
        ],
        "matched_tutorial_codes": [
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "fit1 = SimpleExpSmoothing(oildata, initialization_method=\"heuristic\").fit(\n    smoothing_level=0.2, optimized=False\n)\nfcast1 = fit1.forecast(3).rename(r\"$\\alpha=0.2$\")\nfit2 = SimpleExpSmoothing(oildata, initialization_method=\"heuristic\").fit(\n    smoothing_level=0.6, optimized=False\n)\nfcast2 = fit2.forecast(3).rename(r\"$\\alpha=0.6$\")\nfit3 = SimpleExpSmoothing(oildata, initialization_method=\"estimated\").fit()\nfcast3 = fit3.forecast(3).rename(r\"$\\alpha=%s$\" % fit3.model.params[\"smoothing_level\"])\n\nplt.figure(figsize=(12, 8))\nplt.plot(oildata, marker=\"o\", color=\"black\")\nplt.plot(fit1.fittedvalues, marker=\"o\", color=\"blue\")\n(line1,) = plt.plot(fcast1, marker=\"o\", color=\"blue\")\nplt.plot(fit2.fittedvalues, marker=\"o\", color=\"red\")\n(line2,) = plt.plot(fcast2, marker=\"o\", color=\"red\")\nplt.plot(fit3.fittedvalues, marker=\"o\", color=\"green\")\n(line3,) = plt.plot(fcast3, marker=\"o\", color=\"green\")\nplt.legend([line1, line2, line3], [fcast1.name, fcast2.name, fcast3.name])",
            "feature_importance = reg.feature_importances_\nsorted_idx = (feature_importance)\npos = (sorted_idx.shape[0]) + 0.5\nfig = (figsize=(12, 6))\n(1, 2, 1)\n(pos, feature_importance[sorted_idx], align=\"center\")\n(pos, (diabetes.feature_names)[sorted_idx])\n(\"Feature Importance (MDI)\")\n\nresult = (\n    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_idx = result.importances_mean.argsort()\n(1, 2, 2)\n(\n    result.importances[sorted_idx].T,\n    vert=False,\n    labels=(diabetes.feature_names)[sorted_idx],\n)\n(\"Permutation Importance (test set)\")\nfig.tight_layout()\n()\n\n\n<img alt=\"Feature Importance (MDI), Permutation Importance (test set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\"/>",
            "one_hot_poly_pipeline = (\n    (\n        transformers=[\n            (\"categorical\", one_hot_encoder, categorical_columns),\n            (\"one_hot_time\", one_hot_encoder, [\"hour\", \"weekday\", \"month\"]),\n        ],\n        remainder=\"passthrough\",\n    ),\n    (kernel=\"poly\", degree=2, n_components=300, random_state=0),\n    (alphas=alphas),\n)\nevaluate(one_hot_poly_pipeline, X, y, cv=ts_cv)",
            "mae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}\n\n_, ax = (figsize=(5, 5))\ndisplay = (\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n)\nax.set_title(\"Ridge model, small regularization\")\nfor name, score in scores.items():\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\nax.legend(loc=\"upper left\")\n()\n\n\n<img alt=\"Ridge model, small regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_009.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_009.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Simple Exponential Smoothing"
            ],
            [
                "sklearn->Examples->Ensemble methods->Gradient Boosting regression->Plot feature importance"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Modeling non-linear feature interactions with kernels"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ]
        ]
    },
    "821955": {
        "jupyter_code_cell": "sd['java':'python']\nsales = [{'account': 'Jones LLC', 'Jan': 150, 'Feb': 200, 'Mar': 140},\n         {'account': 'Alpha Co',  'Jan': 200, 'Feb': 210, 'Mar': 215},\n         {'account': 'Blue Inc',  'Jan': 50,  'Feb': 90,  'Mar': 95 }]\npd.DataFrame(sales)",
        "matched_tutorial_code_inds": [
            4027,
            3907,
            4028,
            4159,
            6136
        ],
        "matched_tutorial_codes": [
            "sns.displot(tips, x=\"size\", discrete=True)\n",
            "sns.displot(data=tips, x=\"total_bill\", col=\"time\", kde=True)\n",
            "sns.displot(tips, x=\"day\", shrink=.8)\n",
            "sns.axes_style()\n",
            "sm.stats.durbin_watson(arma_mod30.resid.values)"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size",
                "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics->Distributional representations"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size",
                "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size"
            ],
            [
                "seaborn->User guide and tutorial->Controlling figure aesthetics->Overriding elements of the seaborn styles",
                "seaborn->Figure aesthetics->Controlling figure aesthetics->Overriding elements of the seaborn styles"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data"
            ]
        ]
    },
    "1242776": {
        "jupyter_code_cell": "sanity_test_url = dresses_set[13]\ntest_sanity(sanity_test_url)\ndef get_street_images_url():\n    GENDER_I = 6\n    PRODUCT_ID = 8\n    all_street_urls = []\n    unique_street_urls = [] \n    products_added = []\n    for key in test_embeddings: \n        if key[GENDER_I] == \"others\": continue \n        all_street_images_urls.append(key)\n        product = key[PRODUCT_ID]\n        if product not in products_added:\n            products_added.append(product)\n            unique_street_urls.append(product)\n    return all_street_urls, unique_street_urls",
        "matched_tutorial_code_inds": [
            5902,
            1475,
            3257,
            3904,
            2951
        ],
        "matched_tutorial_codes": [
            "resid = lm.resid\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    group_num = i * 2 + j - 1  # for plotting purposes\n    x = [group_num] * len(group)\n    plt.scatter(\n        x,\n        resid[group.index],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\nplt.xlabel(\"Group\")\nplt.ylabel(\"Residuals\")",
            "totals_row = 35\nlocations = np.delete(locations, (totals_row), axis=0)\nnbcases = np.delete(nbcases, (totals_row), axis=0)\n\nchina_total = nbcases[locations[:, 1] == \"China\"].sum(axis=0)\nchina_total",
            "alphas = (-5, 1, 60)\nenet = (l1_ratio=0.7, max_iter=10000)\ntrain_errors = list()\ntest_errors = list()\nfor alpha in alphas:\n    enet.set_params(alpha=alpha)\n    enet.fit(X_train, y_train)\n    train_errors.append(enet.score(X_train, y_train))\n    test_errors.append(enet.score(X_test, y_test))\n\ni_alpha_optim = (test_errors)\nalpha_optim = alphas[i_alpha_optim]\nprint(\"Optimal regularization parameter : %s\" % alpha_optim)\n\n# Estimate the coef_ on full data with optimal regularization parameter\nenet.set_params(alpha=alpha_optim)\ncoef_ = enet.fit(X, y).coef_",
            "dots = sns.load_dataset(\"dots\")\nsns.relplot(\n    data=dots, kind=\"line\",\n    x=\"time\", y=\"firing_rate\", col=\"align\",\n    hue=\"choice\", size=\"coherence\", style=\"choice\",\n    facet_kws=dict(sharex=False),\n)\n",
            "cluster_ids = (dist_linkage, 1, criterion=\"distance\")\ncluster_id_to_feature_ids = (list)\nfor idx, cluster_id in enumerate(cluster_ids):\n    cluster_id_to_feature_ids[cluster_id].append(idx)\nselected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n\nX_train_sel = X_train[:, selected_features]\nX_test_sel = X_test[:, selected_features]\n\nclf_sel = (n_estimators=100, random_state=42)\nclf_sel.fit(X_train_sel, y_train)\nprint(\n    \"Accuracy on test data with features removed: {:.2f}\".format(\n        clf_sel.score(X_test_sel, y_test)\n    )\n)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "numpy->NumPy Features->Masked Arrays->Exploring the data"
            ],
            [
                "sklearn->Examples->Model Selection->Train error vs Test error->Compute train and test errors"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics"
            ],
            [
                "sklearn->Examples->Inspection->Permutation Importance with Multicollinear or Correlated Features->Handling Multicollinear Features"
            ]
        ]
    },
    "1349938": {
        "jupyter_code_cell": "my_dataset = data_dict\ndef dict_to_list(key,normalizer):\n    new_list=[]\n    for i in data_dict:\n        if data_dict[i][key]==\"NaN\" or data_dict[i][normalizer]==\"NaN\":\n            new_list.append(0.)\n        elif data_dict[i][key]>=0:\n            new_list.append(float(data_dict[i][key])/float(data_dict[i][normalizer]))\n    return new_list\nfraction_from_poi_email=dict_to_list(\"from_poi_to_this_person\",\"to_messages\")\nfraction_to_poi_email=dict_to_list(\"from_this_person_to_poi\",\"from_messages\")\ncount = 0\nfor i in data_dict:\n    data_dict[i][\"fraction_from_poi_email\"] = fraction_from_poi_email[count]\n    data_dict[i][\"fraction_to_poi_email\"] = fraction_to_poi_email[count]\n    count += 1\nmy_dataset = data_dict\ndata = featureFormat(my_dataset, features_list, sort_keys = True)\nlabels, features = targetFeatureSplit(data)\nk=4\nk_best = SelectKBest(k=k)\nk_best.fit(features, labels)\nscores = k_best.scores_\nprint(scores)\nfeatures_list = ['poi','total_stock_value','fraction_to_poi_email','expenses','shared_receipt_with_poi']",
        "matched_tutorial_code_inds": [
            1244,
            149,
            82,
            2446,
            3257
        ],
        "matched_tutorial_codes": [
            "my_auto_wrap_policy = functools.partial(\n        size_based_auto_wrap_policy, min_num_params=20000\n    )\ntorch.cuda.set_device(rank)\nmodel = Net().to(rank)\n\nmodel = FSDP(model,\n    fsdp_auto_wrap_policy=my_auto_wrap_policy)",
            "num_threads = ()\nprint(f'Benchmarking on {num_threads} threads')\n\nt0 = (\n    stmt='batched_dot_mul_sum(x, x)',\n    setup='from __main__ import batched_dot_mul_sum',\n    globals={'x': x},\n    num_threads=num_threads,\n    label='Multithreaded batch dot',\n    sub_label='Implemented using mul and sum')\n\nt1 = (\n    stmt='batched_dot_bmm(x, x)',\n    setup='from __main__ import batched_dot_bmm',\n    globals={'x': x},\n    num_threads=num_threads,\n    label='Multithreaded batch dot',\n    sub_label='Implemented using bmm')\n\nprint(t0.timeit(100))\nprint(t1.timeit(100))\n\n\n\nOutput",
            "use_amp = True\n\nnet = make_model(in_size, out_size, num_layers)\nopt = (net.parameters(), lr=0.001)\nscaler = (enabled=use_amp)\n\nstart_timer()\nfor epoch in range(epochs):\n    for input, target in zip(data, targets):\n        with (device_type='cuda', dtype=torch.float16, enabled=use_amp):\n            output = net(input)\n            loss = loss_fn(output, target)\n        scaler.scale(loss).backward()\n        scaler.step(opt)\n        scaler.update()\n        opt.zero_grad() # set_to_none=True here can modestly improve performance\nend_timer_and_print(\"Mixed precision:\")",
            "last_hours = slice(-96, None)\nfig, ax = (figsize=(12, 4))\nfig.suptitle(\"Predictions by linear models\")\nax.plot(\n    y.iloc[test_0].values[last_hours],\n    \"x-\",\n    alpha=0.2,\n    label=\"Actual demand\",\n    color=\"black\",\n)\nax.plot(naive_linear_predictions[last_hours], \"x-\", label=\"Ordinal time features\")\nax.plot(\n    cyclic_cossin_linear_predictions[last_hours],\n    \"x-\",\n    label=\"Trigonometric time features\",\n)\nax.plot(\n    cyclic_spline_linear_predictions[last_hours],\n    \"x-\",\n    label=\"Spline-based time features\",\n)\nax.plot(\n    one_hot_linear_predictions[last_hours],\n    \"x-\",\n    label=\"One-hot time features\",\n)\n_ = ax.legend()\n\n\n<img alt=\"Predictions by linear models\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_006.png\" srcset=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_006.png\"/>",
            "alphas = (-5, 1, 60)\nenet = (l1_ratio=0.7, max_iter=10000)\ntrain_errors = list()\ntest_errors = list()\nfor alpha in alphas:\n    enet.set_params(alpha=alpha)\n    enet.fit(X_train, y_train)\n    train_errors.append(enet.score(X_train, y_train))\n    test_errors.append(enet.score(X_test, y_test))\n\ni_alpha_optim = (test_errors)\nalpha_optim = alphas[i_alpha_optim]\nprint(\"Optimal regularization parameter : %s\" % alpha_optim)\n\n# Estimate the coef_ on full data with optimal regularization parameter\nenet.set_params(alpha=alpha_optim)\ncoef_ = enet.fit(X, y).coef_"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Parallel and Distributed Training->Getting Started with Fully Sharded Data Parallel(FSDP)->How to use FSDP"
            ],
            [
                "torch->PyTorch Recipes->PyTorch Benchmark->Steps->3. Benchmarking with torch.utils.benchmark.Timer"
            ],
            [
                "torch->PyTorch Recipes->Automatic Mixed Precision->All together: \u201cAutomatic Mixed Precision\u201d"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Qualitative analysis of the impact of features on linear model predictions"
            ],
            [
                "sklearn->Examples->Model Selection->Train error vs Test error->Compute train and test errors"
            ]
        ]
    },
    "774917": {
        "jupyter_code_cell": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=5)\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(binary=True)\nX_train_transformed = vectorizer.fit_transform(X_train)",
        "matched_tutorial_code_inds": [
            2504,
            2355,
            3338,
            1793,
            3191
        ],
        "matched_tutorial_codes": [
            "from sklearn.datasets import \n\nX, y = (\n    n_samples=500,\n    n_features=15,\n    n_informative=3,\n    n_redundant=2,\n    n_repeated=0,\n    n_classes=8,\n    n_clusters_per_class=1,\n    class_sep=0.8,\n    random_state=0,\n)",
            "from sklearn.base import clone\n\nalpha = 0.95\nneg_mean_pinball_loss_95p_scorer = (\n    ,\n    alpha=alpha,\n    greater_is_better=False,  # maximize the negative loss\n)\nsearch_95p = clone(search_05p).set_params(\n    estimator__alpha=alpha,\n    scoring=neg_mean_pinball_loss_95p_scorer,\n)\nsearch_95p.fit(X_train, y_train)\n(search_95p.best_params_)",
            "from sklearn.compose import make_column_selector as \n\npreprocessor = (\n    transformers=[\n        (\"num\", numeric_transformer, (dtype_exclude=\"category\")),\n        (\"cat\", categorical_transformer, (dtype_include=\"category\")),\n    ]\n)\nclf = (\n    steps=[(\"preprocessor\", preprocessor), (\"classifier\", ())]\n)\n\n\nclf.fit(X_train, y_train)\nprint(\"model score: %.3f\" % clf.score(X_test, y_test))\nclf",
            "from sklearn.preprocessing import \nimport numpy as np\n\nX = (\n    [[\"dog\"] * 5 + [\"cat\"] * 20 + [\"rabbit\"] * 10 + [\"snake\"] * 3], dtype=object\n).T\nenc = (min_frequency=6, sparse_output=False).fit(X)\nenc.infrequent_categories_",
            "from sklearn.preprocessing import \n\n# Use label_binarize to be multi-label like settings\nY = (y, classes=[0, 1, 2])\nn_classes = Y.shape[1]\n\n# Split into training and test\nX_train, X_test, Y_train, Y_test = (\n    X, Y, test_size=0.5, random_state=random_state\n)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Feature Selection->Recursive feature elimination with cross-validation->Data generation"
            ],
            [
                "sklearn->Examples->Ensemble methods->Prediction Intervals for Gradient Boosting Regression->Tuning the hyper-parameters of the quantile regressors"
            ],
            [
                "sklearn->Examples->Pipelines and composite estimators->Column Transformer with Mixed Types"
            ],
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 1.1->Grouping infrequent categories in OneHotEncoder"
            ],
            [
                "sklearn->Examples->Model Selection->Precision-Recall->In multi-label settings->Create multi-label data, fit, and predict"
            ]
        ]
    },
    "1382150": {
        "jupyter_code_cell": "from __future__ import print_function\nfrom IPython.display import display\nfrom ipywidgets import interact, Layout, interactive, fixed, interact_manual\nimport ipywidgets as widgets\nskill_list = ['excel','vba','python','r','matlab','c#','c++','sas','stata','sql','mysql','php','html','java','javascript',\n    'ios','perl','hadoop','nosql','hive','mapreduce','pip','mongodb','hbase','tableau','spark','scala','d3']",
        "matched_tutorial_code_inds": [
            1061,
            393,
            17,
            449,
            791
        ],
        "matched_tutorial_codes": [
            "from __future__ import absolute_import, division, print_function\n\nimport logging\nimport numpy as np\nimport os\nimport random\nimport sys\nimport time\nimport torch\n\nfrom argparse import Namespace\nfrom torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n                              TensorDataset)\nfrom tqdm import tqdm\nfrom transformers import (BertConfig, BertForSequenceClassification, BertTokenizer,)\nfrom transformers import glue_compute_metrics as compute_metrics\nfrom transformers import glue_output_modes as output_modes\nfrom transformers import glue_processors as processors\nfrom transformers import glue_convert_examples_to_features as convert_examples_to_features\n\n# Setup logging\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n                    datefmt = '%m/%d/%Y %H:%M:%S',\n                    level = logging.WARN)\n\nlogging.getLogger(\"transformers.modeling_utils\").setLevel(\n   logging.WARN)  # Reduce logging\n\nprint(torch.__version__)",
            "from __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# NOTE: This is a hack to get around \"User-agent\" limitations when downloading MNIST datasets\n#       see, https://github.com/pytorch/vision/issues/3497 for more information\nfrom six.moves import urllib\nopener = urllib.request.build_opener()\nopener.addheaders = [('User-agent', 'Mozilla/5.0')]\nurllib.request.install_opener(opener)",
            "from __future__ import print_function, division\nimport os\nimport torch\nimport pandas as pd\nfrom skimage import io, transform\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nplt.ion()   # interactive mode",
            "from __future__ import unicode_literals, print_function, division\nfrom io import open\nimport glob\nimport os\n\ndef findFiles(path): return glob.glob(path)\n\nprint(findFiles('data/names/*.txt'))\n\nimport unicodedata\nimport string\n\nall_letters = string.ascii_letters + \" .,;'\"\nn_letters = len(all_letters)\n\n# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\ndef unicodeToAscii(s):\n    return ''.join(\n        c for c in unicodedata.normalize('NFD', s)\n        if unicodedata.category(c) != 'Mn'\n        and c in all_letters\n    )\n\nprint(unicodeToAscii('\u015alus\u00e0rski'))\n\n# Build the category_lines dictionary, a list of names per language\ncategory_lines = {}\nall_categories = []\n\n# Read a file and split into lines\ndef readLines(filename):\n    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n    return [unicodeToAscii(line) for line in lines]\n\nfor filename in findFiles('data/names/*.txt'):\n    category = os.path.splitext(os.path.basename(filename))[0]\n    all_categories.append(category)\n    lines = readLines(filename)\n    category_lines[category] = lines\n\nn_categories = len(all_categories)",
            "from __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport argparse\n\nimport matplotlib.pyplot as plt\nimport torch\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"-i\", \"--sample-file\", required=True)\nparser.add_argument(\"-o\", \"--out-file\", default=\"out.png\")\nparser.add_argument(\"-d\", \"--dimension\", type=int, default=3)\noptions = parser.parse_args()\n\nmodule = torch.jit.load(options.sample_file)\nimages = list(module.parameters())[0]\n\nfor index in range(options.dimension * options.dimension):\n  image = images[index].detach().cpu().reshape(28, 28).mul(255).to(torch.uint8)\n  array = image.numpy()\n  axis = plt.subplot(options.dimension, options.dimension, 1 + index)\n  plt.imshow(array, cmap=\"gray\")\n  axis.get_xaxis().set_visible(False)\n  axis.get_yaxis().set_visible(False)\n\nplt.savefig(options.out_file)\nprint(\"Saved \", options.out_file)"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Model Optimization->(beta) Dynamic Quantization on BERT->1. Setup->1.2 Import the necessary modules"
            ],
            [
                "torch->Image and Video->Adversarial Example Generation->Fast Gradient Sign Attack"
            ],
            [
                "torch->PyTorch Recipes->Developing Custom PyTorch Dataloaders->Setup"
            ],
            [
                "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN->Preparing the Data"
            ],
            [
                "torch->Frontend APIs->Using the PyTorch C++ Frontend->Inspecting Generated Images"
            ]
        ]
    },
    "1141092": {
        "jupyter_code_cell": "data_second_week = data_combine[data_combine.columns[:5]].dropna()\nkf = KFold(data_second_week.shape[0], shuffle = True)\nfor train, test in kf:\n    data_train, data_test = data_second_week.iloc[train], data_second_week.iloc[test]\n    regCv.fit(data_train[['pressure', 'snow', 'temperature', 'Bottledlag1']], data_train['BottledWaterUnits'])\n    print regCv.score(data_test[['pressure', 'snow', 'temperature', 'Bottledlag1']], data_test['BottledWaterUnits'])\ndata_combine.columns",
        "matched_tutorial_code_inds": [
            6090,
            3257,
            2753,
            4654,
            5911
        ],
        "matched_tutorial_codes": [
            "res_ar5 = AutoReg(ind_prod, 5, old_names=False).fit()\npredictions = pd.DataFrame(\n    {\n        \"AR(5)\": res_ar5.predict(start=714, end=726),\n        \"AR(13)\": res.predict(start=714, end=726),\n        \"Restr. AR(13)\": res_glob.predict(start=714, end=726),\n    }\n)\n_, ax = plt.subplots()\nax = predictions.plot(ax=ax)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_42_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_42_0.png\"/>",
            "alphas = (-5, 1, 60)\nenet = (l1_ratio=0.7, max_iter=10000)\ntrain_errors = list()\ntest_errors = list()\nfor alpha in alphas:\n    enet.set_params(alpha=alpha)\n    enet.fit(X_train, y_train)\n    train_errors.append(enet.score(X_train, y_train))\n    test_errors.append(enet.score(X_test, y_test))\n\ni_alpha_optim = (test_errors)\nalpha_optim = alphas[i_alpha_optim]\nprint(\"Optimal regularization parameter : %s\" % alpha_optim)\n\n# Estimate the coef_ on full data with optimal regularization parameter\nenet.set_params(alpha=alpha_optim)\ncoef_ = enet.fit(X, y).coef_",
            "quantiles = [0.05, 0.5, 0.95]\npredictions = {}\nout_bounds_predictions = (y_true_mean, dtype=)\nfor quantile in quantiles:\n    qr = (quantile=quantile, alpha=0, solver=solver)\n    y_pred = qr.fit(X, y_pareto).predict(X)\n    predictions[quantile] = y_pred\n\n    if quantile == min(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred = y_pareto\n        )\n    elif quantile == max(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred &lt;= y_pareto\n        )",
            "data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>",
            "infl = interM_lm.get_influence()\nresid = infl.resid_studentized_internal\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        resid[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\nplt.xlabel(\"X\")\nplt.ylabel(\"standardized resids\")"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions->Industrial Production"
            ],
            [
                "sklearn->Examples->Model Selection->Train error vs Test error->Compute train and test errors"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Quantile regression->Fitting a QuantileRegressor"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ]
        ]
    },
    "1225844": {
        "jupyter_code_cell": "d_male=df1.loc[df1['gender'] == 1]\nd_female=df1.loc[df1['gender'] == 2]\ndf_pie1 = pd.DataFrame([d_male.shape[0], d_female.shape[0]], index=['MALE', 'FEMALE'], columns=['Frequency'])\ndf_pie1.plot(kind='pie', subplots=True, figsize=(10, 10))",
        "matched_tutorial_code_inds": [
            6568,
            3756,
            1620,
            6676,
            5516
        ],
        "matched_tutorial_codes": [
            "y_pre = y.iloc[:-5]\ny_pre.plot(figsize=(15, 3), title='Inflation');\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_news_9_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_news_9_0.png\"/>",
            "delta = (by_game.home_rest - by_game.away_rest).dropna().astype(int)\nax = (delta.value_counts()\n    .reindex(np.arange(delta.min(), delta.max() + 1), fill_value=0)\n    .sort_index()\n    .plot(kind='bar', color='k', width=.9, rot=0, figsize=(12, 6))\n)\nsns.despine()\nax.set(xlabel='Difference in Rest (Home - Away)', ylabel='Games');",
            "pixel_intensity_distribution = ndimage.histogram(\n    xray_image, min=np.min(xray_image), max=np.max(xray_image), bins=256\n)\n\nplt.plot(pixel_intensity_distribution)\nplt.title(\"Pixel intensity distribution\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/00c12e163a30744c62700ec414e2d514c5c4eae10256e7cd0cf87553d0f4d34c.png\" src=\"../_images/00c12e163a30744c62700ec414e2d514c5c4eae10256e7cd0cf87553d0f4d34c.png\"/>",
            "cpi_apparel = DataReader('CPIAPPNS', 'fred', start='1986')\ncpi_apparel.index = pd.DatetimeIndex(cpi_apparel.index, freq='MS')\ninf_apparel = np.log(cpi_apparel).diff().iloc[1:] * 1200\ninf_apparel.plot(figsize=(15, 5));\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_chandrasekhar_10_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_chandrasekhar_10_0.png\"/>",
            "modfd_logit = OrderedModel.from_formula(\"apply ~ 1 + pared + public + gpa + C(dummy)\", data_student,\n                                      distr='logit')\nresfd_logit = modfd_logit.fit(method='bfgs')\nprint(resfd_logit.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Simple univariate example: AR(1)->Step 1: fitting the model on the available dataset"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data"
            ],
            [
                "numpy->NumPy Applications->X-ray image processing->Apply masks to X-rays with np.where()"
            ],
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: Chandrasekhar recursions->Practical example"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model"
            ]
        ]
    },
    "1058006": {
        "jupyter_code_cell": "data = data[data['TOUCH_TIME'] > 0].copy() \ndef scatter_plot_two_features(data, colnames, total_samples):\n    scatter_plot_data = data[[colnames[0], colnames[1]]].sample(n=total_samples)\n    x = scatter_plot_data[colnames[0]]\n    y = scatter_plot_data[colnames[1]]\n    plot = plt.scatter(x, y)\n    return plot\ncols = ['TOUCH_TIME', 'SHOT_DIST']\nscatter_plot_two_features(data, cols, 1000)\ndef rescale_columns(data, colnames, total_samples_preview):\n    scaler = pp.MinMaxScaler()\n    rescaled_data = data[colnames]\n    rescaled_data = scaler.fit_transform(rescaled_data)\n    rescaled_data = pd.DataFrame(rescaled_data, columns=colnames)\n    rescaled_data_sample = rescaled_data.sample(n=5)\n    scatter_plot_two_features(rescaled_data, colnames, total_samples_preview)\n    return rescaled_data_sample\nrescale_columns(data, cols, 500)",
        "matched_tutorial_code_inds": [
            4654,
            5963,
            3904,
            6060,
            5902
        ],
        "matched_tutorial_codes": [
            "data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>",
            "data = [\n    [\"Carroll\", 94, 22, 60, 92, 20, 60],\n    [\"Grant\", 98, 21, 65, 92, 22, 65],\n    [\"Peck\", 98, 28, 40, 88, 26, 40],\n    [\"Donat\", 94, 19, 200, 82, 17, 200],\n    [\"Stewart\", 98, 21, 50, 88, 22, 45],\n    [\"Young\", 96, 21, 85, 92, 22, 85],\n]\ncolnames = [\"study\", \"mean_t\", \"sd_t\", \"n_t\", \"mean_c\", \"sd_c\", \"n_c\"]\nrownames = [i[0] for i in data]\ndframe1 = pd.DataFrame(data, columns=colnames)\nrownames",
            "dots = sns.load_dataset(\"dots\")\nsns.relplot(\n    data=dots, kind=\"line\",\n    x=\"time\", y=\"firing_rate\", col=\"align\",\n    hue=\"choice\", size=\"coherence\", style=\"choice\",\n    facet_kws=dict(sharex=False),\n)\n",
            "data = pdr.get_data_fred(\"HOUSTNSA\", \"1959-01-01\", \"2019-06-01\")\nhousing = data.HOUSTNSA.pct_change().dropna()\n# Scale by 100 to get percentages\nhousing = 100 * housing.asfreq(\"MS\")\nfig, ax = plt.subplots()\nax = housing.plot(ax=ax)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_6_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_6_0.png\"/>",
            "resid = lm.resid\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    group_num = i * 2 + j - 1  # for plotting purposes\n    x = [group_num] * len(group)\n    plt.scatter(\n        x,\n        resid[group.index],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\nplt.xlabel(\"Group\")\nplt.ylabel(\"Residuals\")"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Example"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ]
        ]
    },
    "192534": {
        "jupyter_code_cell": "import numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"whitegrid\", color_codes=True)\nmpl.rcParams['figure.figsize'] = (8, 6)\nsns.countplot(x=\"edibility\", data=md);",
        "matched_tutorial_code_inds": [
            5559,
            6049,
            6687,
            6270,
            5287
        ],
        "matched_tutorial_codes": [
            "import numpy as np\nimport pylab\nimport seaborn as sns\nimport statsmodels.api as sm\n\nsns.set_style(\"darkgrid\")\npylab.rc(\"figure\", figsize=(16, 8))\npylab.rc(\"font\", size=14)",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom scipy import stats\n\nsns.set_style(\"darkgrid\")\nsns.mpl.rc(\"figure\", figsize=(8, 8))",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pandas_datareader as pdr\nimport seaborn as sns\n\nplt.rc(\"figure\", figsize=(16, 8))\nplt.rc(\"font\", size=15)\nplt.rc(\"lines\", linewidth=3)\nsns.set_style(\"darkgrid\")",
            "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom pandas.plotting import register_matplotlib_converters\n\nregister_matplotlib_converters()\nsns.set_style(\"darkgrid\")",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom statsmodels.iolib.table import SimpleTable, default_txt_fmt\n\nnp.random.seed(1024)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Nonparametric Statistics->Lowess Regression"
            ],
            [
                "statsmodels->Examples->Statistics->Copulas"
            ],
            [
                "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Imports"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Weighted Least Squares"
            ]
        ]
    },
    "431172": {
        "jupyter_code_cell": "trade_expire = datetime.now() + timedelta(days=1)\ntrade_expire = trade_expire.isoformat(\"T\") + \"Z\"\ntrade_expire\nresponse = oanda.create_order(account_id,\n                              instrument = \"AUD_USD\",\n                              units=1000,\n                              side=\"buy\",\n                              type=\"limit\",\n                              price=0.7420,\n                              expiry=trade_expire)\nprint(response)",
        "matched_tutorial_code_inds": [
            2446,
            2463,
            6700,
            3904,
            3549
        ],
        "matched_tutorial_codes": [
            "last_hours = slice(-96, None)\nfig, ax = (figsize=(12, 4))\nfig.suptitle(\"Predictions by linear models\")\nax.plot(\n    y.iloc[test_0].values[last_hours],\n    \"x-\",\n    alpha=0.2,\n    label=\"Actual demand\",\n    color=\"black\",\n)\nax.plot(naive_linear_predictions[last_hours], \"x-\", label=\"Ordinal time features\")\nax.plot(\n    cyclic_cossin_linear_predictions[last_hours],\n    \"x-\",\n    label=\"Trigonometric time features\",\n)\nax.plot(\n    cyclic_spline_linear_predictions[last_hours],\n    \"x-\",\n    label=\"Spline-based time features\",\n)\nax.plot(\n    one_hot_linear_predictions[last_hours],\n    \"x-\",\n    label=\"One-hot time features\",\n)\n_ = ax.legend()\n\n\n<img alt=\"Predictions by linear models\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_006.png\" srcset=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_006.png\"/>",
            "last_hours = slice(-96, None)\nfig, ax = (figsize=(12, 4))\nfig.suptitle(\"Predictions by non-linear regression models\")\nax.plot(\n    y.iloc[test_0].values[last_hours],\n    \"x-\",\n    alpha=0.2,\n    label=\"Actual demand\",\n    color=\"black\",\n)\nax.plot(\n    gbrt_predictions[last_hours],\n    \"x-\",\n    label=\"Gradient Boosted Trees\",\n)\nax.plot(\n    one_hot_poly_predictions[last_hours],\n    \"x-\",\n    label=\"One-hot + polynomial kernel\",\n)\nax.plot(\n    cyclic_spline_poly_predictions[last_hours],\n    \"x-\",\n    label=\"Splines + polynomial kernel\",\n)\n_ = ax.legend()\n\n\n<img alt=\"Predictions by non-linear regression models\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_007.png\" srcset=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_007.png\"/>",
            "ln_pce = np.log(pce.PCE)\nforecasts = {\"ln PCE\": ln_pce}\nfor year in range(1995, 2020, 3):\n    sub = ln_pce[: str(year)]\n    res = ThetaModel(sub).fit()\n    fcast = res.forecast(12)\n    forecasts[str(year)] = fcast\nforecasts = pd.DataFrame(forecasts)\nax = forecasts[\"1995\":].plot(legend=False)\nchildren = ax.get_children()\nchildren[0].set_linewidth(4)\nchildren[0].set_alpha(0.3)\nchildren[0].set_color(\"#000000\")\nax.set_title(\"ln PCE\")\nplt.tight_layout(pad=1.0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_theta-model_22_0.png\" src=\"../../../_images/examples_notebooks_generated_theta-model_22_0.png\"/>",
            "dots = sns.load_dataset(\"dots\")\nsns.relplot(\n    data=dots, kind=\"line\",\n    x=\"time\", y=\"firing_rate\", col=\"align\",\n    hue=\"choice\", size=\"coherence\", style=\"choice\",\n    facet_kws=dict(sharex=False),\n)\n",
            "t0 = ()\nhasher = (n_features=2**18, input_type=\"string\")\nX = hasher.transform(tokenize(d) for d in raw_data)\nduration = () - t0\ndict_count_vectorizers[\"vectorizer\"].append(\n    hasher.__class__.__name__ + \"\\non raw tokens\"\n)\ndict_count_vectorizers[\"speed\"].append(data_size_mb / duration)\nprint(f\"done in {duration:.3f} s at {data_size_mb / duration:.1f} MB/s\")\nprint(f\"Found {n_nonzero_columns(X)} unique tokens\")"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Qualitative analysis of the impact of features on linear model predictions"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Modeling non-linear feature interactions with kernels"
            ],
            [
                "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Personal Consumption Expenditure"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics"
            ],
            [
                "sklearn->Examples->Working with text documents->FeatureHasher and DictVectorizer Comparison->FeatureHasher"
            ]
        ]
    },
    "1362139": {
        "jupyter_code_cell": "spots_model = []\ndates_model = []\ni = -1\nwhile True:    \n    i+=1\n    date = evaluationDate + ql.Period(i, ql.Days)\n    if date > term_structure.maxDate() - ql.Period(1, ql.Years):        \n        break\n    dates_model.append(str(date))\n    yrs = dayCount.yearFraction(evaluationDate, date)\n    compounding = ql.Continuous\n    freq = ql.Quarterly\n    zero_rate = term_structure.zeroRate(yrs, compounding, freq)   \n    eq_rate = zero_rate.equivalentRate(day_counter, compounding, freq, evaluationDate,date).rate()\n    spots_model.append(zero_rate.rate())\nfrom datetime import datetime\ndates_by_model = []\nfor i in range(len(dates_model)):\n    dates_points = datetime.strptime(solve(dates_model[i]), '%B %d %Y')\n    dates_by_model.append(dates_points)\ndates_by_points = []\nfor i in range(len(maturities)):\n    dates_points = datetime.strptime(solve(maturities[i]), '%B %d %Y')\n    dates_by_points.append(dates_points)\ndates_by_model_graph = matplotlib.dates.date2num(dates_by_model)\ndates_by_points_graph = matplotlib.dates.date2num(dates_by_points)    ",
        "matched_tutorial_code_inds": [
            2753,
            3257,
            3122,
            3173,
            1214
        ],
        "matched_tutorial_codes": [
            "quantiles = [0.05, 0.5, 0.95]\npredictions = {}\nout_bounds_predictions = (y_true_mean, dtype=)\nfor quantile in quantiles:\n    qr = (quantile=quantile, alpha=0, solver=solver)\n    y_pred = qr.fit(X, y_pareto).predict(X)\n    predictions[quantile] = y_pred\n\n    if quantile == min(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred = y_pareto\n        )\n    elif quantile == max(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred &lt;= y_pareto\n        )",
            "alphas = (-5, 1, 60)\nenet = (l1_ratio=0.7, max_iter=10000)\ntrain_errors = list()\ntest_errors = list()\nfor alpha in alphas:\n    enet.set_params(alpha=alpha)\n    enet.fit(X_train, y_train)\n    train_errors.append(enet.score(X_train, y_train))\n    test_errors.append(enet.score(X_test, y_test))\n\ni_alpha_optim = (test_errors)\nalpha_optim = alphas[i_alpha_optim]\nprint(\"Optimal regularization parameter : %s\" % alpha_optim)\n\n# Estimate the coef_ on full data with optimal regularization parameter\nenet.set_params(alpha=alpha_optim)\ncoef_ = enet.fit(X, y).coef_",
            "results = (list)\nn_bootstrap = 100\nrng = (seed=0)\n\nfor prevalence, X, y in zip(\n    populations[\"prevalence\"], populations[\"X\"], populations[\"y\"]\n):\n\n    results_for_prevalence = scoring_on_bootstrap(\n        estimator, X, y, rng, n_bootstrap=n_bootstrap\n    )\n    results[\"prevalence\"].append(prevalence)\n    results[\"metrics\"].append(\n        results_for_prevalence.aggregate([\"mean\", \"std\"]).unstack()\n    )\n\nresults = (results[\"metrics\"], index=results[\"prevalence\"])\nresults.index.name = \"prevalence\"\nresults\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "pair_scores = []\nmean_tpr = dict()\n\nfor ix, (label_a, label_b) in enumerate(pair_list):\n\n    a_mask = y_test == label_a\n    b_mask = y_test == label_b\n    ab_mask = (a_mask, b_mask)\n\n    a_true = a_mask[ab_mask]\n    b_true = b_mask[ab_mask]\n\n    idx_a = (label_binarizer.classes_ == label_a)[0]\n    idx_b = (label_binarizer.classes_ == label_b)[0]\n\n    fpr_a, tpr_a, _ = (a_true, y_score[ab_mask, idx_a])\n    fpr_b, tpr_b, _ = (b_true, y_score[ab_mask, idx_b])\n\n    mean_tpr[ix] = (fpr_grid)\n    mean_tpr[ix] += (fpr_grid, fpr_a, tpr_a)\n    mean_tpr[ix] += (fpr_grid, fpr_b, tpr_b)\n    mean_tpr[ix] /= 2\n    mean_score = (fpr_grid, mean_tpr[ix])\n    pair_scores.append(mean_score)\n\n    fig, ax = (figsize=(6, 6))\n    (\n        fpr_grid,\n        mean_tpr[ix],\n        label=f\"Mean {label_a} vs {label_b} (AUC = {mean_score :.2f})\",\n        linestyle=\":\",\n        linewidth=4,\n    )\n    (\n        a_true,\n        y_score[ab_mask, idx_a],\n        ax=ax,\n        name=f\"{label_a} as positive class\",\n    )\n    (\n        b_true,\n        y_score[ab_mask, idx_b],\n        ax=ax,\n        name=f\"{label_b} as positive class\",\n    )\n    ([0, 1], [0, 1], \"k--\", label=\"chance level (AUC = 0.5)\")\n    (\"square\")\n    (\"False Positive Rate\")\n    (\"True Positive Rate\")\n    (f\"{target_names[idx_a]} vs {label_b} ROC curves\")\n    ()\n    ()\n\nprint(f\"Macro-averaged One-vs-One ROC AUC score:\\n{(pair_scores):.2f}\")\n\n\n\n<img alt=\"setosa vs versicolor ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_004.png\" srcset=\"../../_images/sphx_glr_plot_roc_004.png\"/>\n<img alt=\"setosa vs virginica ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_005.png\" srcset=\"../../_images/sphx_glr_plot_roc_005.png\"/>\n<img alt=\"versicolor vs virginica ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_006.png\" srcset=\"../../_images/sphx_glr_plot_roc_006.png\"/>",
            "means = []\nstds = []\nsplit_sizes = [1, 3, 5, 8, 10, 12, 20, 40, 60]\n\nfor split_size in split_sizes:\n    setup = \"model = PipelineParallelResNet50(split_size=%d)\" % split_size\n    pp_run_times = timeit.repeat(\n        stmt, setup, number=1, repeat=num_repeat, globals=globals())\n    means.append(np.mean(pp_run_times))\n    stds.append(np.std(pp_run_times))\n\nfig, ax = plt.subplots()\nax.plot(split_sizes, means)\nax.errorbar(split_sizes, means, yerr=stds, ecolor='red', fmt='ro')\nax.set_ylabel('ResNet50 Execution Time (Second)')\nax.set_xlabel('Pipeline Split Size')\nax.set_xticks(split_sizes)\nax.yaxis.grid(True)\nplt.tight_layout()\nplt.savefig(\"split_size_tradeoff.png\")\nplt.close(fig)\n\n\n\n<img alt=\"\" src=\"../_images/split_size_tradeoff.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Generalized Linear Models->Quantile regression->Fitting a QuantileRegressor"
            ],
            [
                "sklearn->Examples->Model Selection->Train error vs Test error->Compute train and test errors"
            ],
            [
                "sklearn->Examples->Model Selection->Class Likelihood Ratios to measure classification performance->Invariance with respect to prevalence"
            ],
            [
                "sklearn->Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->One-vs-One multiclass ROC->ROC curve using the OvO macro-average"
            ],
            [
                "torch->Parallel and Distributed Training->Single-Machine Model Parallel Best Practices->Speed Up by Pipelining Inputs"
            ]
        ]
    },
    "1006350": {
        "jupyter_code_cell": "from sklearn.metrics import precision_score,recall_score, confusion_matrix\nreport_model1 = classification_report(y_true_test, y_pred_test)\nprint (report_model1)",
        "matched_tutorial_code_inds": [
            3145,
            2497,
            1880,
            3154,
            2303
        ],
        "matched_tutorial_codes": [
            "from sklearn.metrics import \n\ny_pred = grid_search.predict(X_test)\nprint((y_test, y_pred))",
            "from sklearn.metrics import \n\ny_pred = anova_svm.predict(X_test)\nprint((y_test, y_pred))",
            "from sklearn.ensemble import \n\nclf = (n_estimators=25)\nclf.fit(X_train_valid, y_train_valid)",
            "from sklearn.linear_model import \n\nclassifier = ()\ny_score = classifier.fit(X_train, y_train).predict_proba(X_test)",
            "from sklearn.ensemble import \n\nclf = (max_samples=100, random_state=0)\nclf.fit(X_train)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Model Selection->Custom refit strategy of a grid search with cross-validation->Tuning hyper-parameters"
            ],
            [
                "sklearn->Examples->Feature Selection->Pipeline ANOVA SVM"
            ],
            [
                "sklearn->Examples->Calibration->Probability Calibration for 3-class classification->Fitting and calibration"
            ],
            [
                "sklearn->Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->Load and prepare data"
            ],
            [
                "sklearn->Examples->Ensemble methods->IsolationForest example->Training of the model"
            ]
        ]
    },
    "285581": {
        "jupyter_code_cell": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew, kurtosis\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn import linear_model\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import AdaBoostRegressor,RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import GridSearchCV\nfrom collections import OrderedDict",
        "matched_tutorial_code_inds": [
            4700,
            3384,
            3119,
            3105,
            2706
        ],
        "matched_tutorial_codes": [
            "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom cycler import cycler\nmpl.rcParams['lines.linewidth'] = 2\nmpl.rcParams['lines.linestyle'] = '--'\ndata = np.random.randn(50)\nplt.plot(data)\n\n\n<img alt=\"customizing\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_customizing_001.png\" srcset=\"../../_images/sphx_glr_customizing_001.png, ../../_images/sphx_glr_customizing_001_2_0x.png 2.0x\"/>",
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import \nfrom sklearn.model_selection import \nfrom sklearn.pipeline import \nfrom sklearn.svm import \nfrom sklearn.decomposition import , \nfrom sklearn.feature_selection import , \nfrom sklearn.preprocessing import \n\nX, y = (return_X_y=True)\n\npipe = (\n    [\n        (\"scaling\", ()),\n        # the reduce_dim stage is populated by the param_grid\n        (\"reduce_dim\", \"passthrough\"),\n        (\"classify\", (dual=False, max_iter=10000)),\n    ]\n)\n\nN_FEATURES_OPTIONS = [2, 4, 8]\nC_OPTIONS = [1, 10, 100, 1000]\nparam_grid = [\n    {\n        \"reduce_dim\": [(iterated_power=7), (max_iter=1_000)],\n        \"reduce_dim__n_components\": N_FEATURES_OPTIONS,\n        \"classify__C\": C_OPTIONS,\n    },\n    {\n        \"reduce_dim\": [()],\n        \"reduce_dim__k\": N_FEATURES_OPTIONS,\n        \"classify__C\": C_OPTIONS,\n    },\n]\nreducer_labels = [\"PCA\", \"NMF\", \"KBest(mutual_info_classif)\"]\n\ngrid = (pipe, n_jobs=1, param_grid=param_grid)\ngrid.fit(X, y)",
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom collections import \n\npopulations = (list)\ncommon_params = {\n    \"n_samples\": 10_000,\n    \"n_features\": 2,\n    \"n_informative\": 2,\n    \"n_redundant\": 0,\n    \"random_state\": 0,\n}\nweights = (0.1, 0.8, 6)\nweights = weights[::-1]\n\n# fit and evaluate base model on balanced classes\nX, y = (**common_params, weights=[0.5, 0.5])\nestimator = ().fit(X, y)\nlr_base = extract_score((estimator, X, y, scoring=scoring, cv=10))\npos_lr_base, pos_lr_base_std = lr_base[\"positive\"].values\nneg_lr_base, neg_lr_base_std = lr_base[\"negative\"].values",
            "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# To use this experimental feature, we need to explicitly ask for it:\nfrom sklearn.experimental import enable_iterative_imputer  # noqa\nfrom sklearn.datasets import \nfrom sklearn.impute import \nfrom sklearn.impute import \nfrom sklearn.linear_model import , \nfrom sklearn.kernel_approximation import \nfrom sklearn.ensemble import \nfrom sklearn.neighbors import \nfrom sklearn.pipeline import \nfrom sklearn.model_selection import \n\nN_SPLITS = 5\n\nrng = (0)\n\nX_full, y_full = (return_X_y=True)\n# ~2k samples is enough for the purpose of the example.\n# Remove the following two lines for a slower run with different error bars.\nX_full = X_full[::10]\ny_full = y_full[::10]\nn_samples, n_features = X_full.shape\n\n# Estimate the score on the entire dataset, with no missing values\nbr_estimator = ()\nscore_full_data = (\n    (\n        br_estimator, X_full, y_full, scoring=\"neg_mean_squared_error\", cv=N_SPLITS\n    ),\n    columns=[\"Full Data\"],\n)\n\n# Add a single missing value to each row\nX_missing = X_full.copy()\ny_missing = y_full\nmissing_samples = (n_samples)\nmissing_features = rng.choice(n_features, n_samples, replace=True)\nX_missing[missing_samples, missing_features] = \n\n# Estimate the score after imputation (mean and median strategies)\nscore_simple_imputer = ()\nfor strategy in (\"mean\", \"median\"):\n    estimator = (\n        (missing_values=, strategy=strategy), br_estimator\n    )\n    score_simple_imputer[strategy] = (\n        estimator, X_missing, y_missing, scoring=\"neg_mean_squared_error\", cv=N_SPLITS\n    )\n\n# Estimate the score after iterative imputation of the missing values\n# with different estimators\nestimators = [\n    (),\n    (\n        # We tuned the hyperparameters of the RandomForestRegressor to get a good\n        # enough predictive performance for a restricted execution time.\n        n_estimators=4,\n        max_depth=10,\n        bootstrap=True,\n        max_samples=0.5,\n        n_jobs=2,\n        random_state=0,\n    ),\n    (\n        (kernel=\"polynomial\", degree=2, random_state=0), (alpha=1e3)\n    ),\n    (n_neighbors=15),\n]\nscore_iterative_imputer = ()\n# iterative imputer is sensible to the tolerance and\n# dependent on the estimator used internally.\n# we tuned the tolerance to keep this example run with limited computational\n# resources while not changing the results too much compared to keeping the\n# stricter default value for the tolerance parameter.\ntolerances = (1e-3, 1e-1, 1e-1, 1e-2)\nfor impute_estimator, tol in zip(estimators, tolerances):\n    estimator = (\n        (\n            random_state=0, estimator=impute_estimator, max_iter=25, tol=tol\n        ),\n        br_estimator,\n    )\n    score_iterative_imputer[impute_estimator.__class__.__name__] = (\n        estimator, X_missing, y_missing, scoring=\"neg_mean_squared_error\", cv=N_SPLITS\n    )\n\nscores = (\n    [score_full_data, score_simple_imputer, score_iterative_imputer],\n    keys=[\"Original\", \"SimpleImputer\", \"IterativeImputer\"],\n    axis=1,\n)\n\n# plot california housing results\nfig, ax = (figsize=(13, 6))\nmeans = -scores.mean()\nerrors = scores.std()\nmeans.plot.barh(xerr=errors, ax=ax)\nax.set_title(\"California Housing Regression with Different Imputation Methods\")\nax.set_xlabel(\"MSE (smaller is better)\")\nax.set_yticks((means.shape[0]))\nax.set_yticklabels([\" w/ \".join(label) for label in means.index.tolist()])\n(pad=1)\n()",
            "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nfrom sklearn.svm import \nfrom sklearn.linear_model import \nfrom sklearn.kernel_approximation import \nfrom sklearn.pipeline import \n\nfont = {\"weight\": \"normal\", \"size\": 15}\n\n(\"font\", **font)\n\nrandom_state = 42\nrng = (random_state)\n\n# Generate train data\nX = 0.3 * rng.randn(500, 2)\nX_train = [X + 2, X - 2]\n# Generate some regular novel observations\nX = 0.3 * rng.randn(20, 2)\nX_test = [X + 2, X - 2]\n# Generate some abnormal novel observations\nX_outliers = rng.uniform(low=-4, high=4, size=(20, 2))\n\nxx, yy = ((-4.5, 4.5, 50), (-4.5, 4.5, 50))\n\n# OCSVM hyperparameters\nnu = 0.05\ngamma = 2.0\n\n# Fit the One-Class SVM\nclf = (gamma=gamma, kernel=\"rbf\", nu=nu)\nclf.fit(X_train)\ny_pred_train = clf.predict(X_train)\ny_pred_test = clf.predict(X_test)\ny_pred_outliers = clf.predict(X_outliers)\nn_error_train = y_pred_train[y_pred_train == -1].size\nn_error_test = y_pred_test[y_pred_test == -1].size\nn_error_outliers = y_pred_outliers[y_pred_outliers == 1].size\n\nZ = clf.decision_function([xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\n\n# Fit the One-Class SVM using a kernel approximation and SGD\ntransform = (gamma=gamma, random_state=random_state)\nclf_sgd = (\n    nu=nu, shuffle=True, fit_intercept=True, random_state=random_state, tol=1e-4\n)\npipe_sgd = (transform, clf_sgd)\npipe_sgd.fit(X_train)\ny_pred_train_sgd = pipe_sgd.predict(X_train)\ny_pred_test_sgd = pipe_sgd.predict(X_test)\ny_pred_outliers_sgd = pipe_sgd.predict(X_outliers)\nn_error_train_sgd = y_pred_train_sgd[y_pred_train_sgd == -1].size\nn_error_test_sgd = y_pred_test_sgd[y_pred_test_sgd == -1].size\nn_error_outliers_sgd = y_pred_outliers_sgd[y_pred_outliers_sgd == 1].size\n\nZ_sgd = pipe_sgd.decision_function([xx.ravel(), yy.ravel()])\nZ_sgd = Z_sgd.reshape(xx.shape)\n\n# plot the level sets of the decision function\n(figsize=(9, 6))\n(\"One Class SVM\")\n(xx, yy, Z, levels=(Z.min(), 0, 7), cmap=plt.cm.PuBu)\na = (xx, yy, Z, levels=[0], linewidths=2, colors=\"darkred\")\n(xx, yy, Z, levels=[0, Z.max()], colors=\"palevioletred\")\n\ns = 20\nb1 = (X_train[:, 0], X_train[:, 1], c=\"white\", s=s, edgecolors=\"k\")\nb2 = (X_test[:, 0], X_test[:, 1], c=\"blueviolet\", s=s, edgecolors=\"k\")\nc = (X_outliers[:, 0], X_outliers[:, 1], c=\"gold\", s=s, edgecolors=\"k\")\n(\"tight\")\n((-4.5, 4.5))\n((-4.5, 4.5))\n(\n    [a.collections[0], b1, b2, c],\n    [\n        \"learned frontier\",\n        \"training observations\",\n        \"new regular observations\",\n        \"new abnormal observations\",\n    ],\n    loc=\"upper left\",\n)\n(\n    \"error train: %d/%d; errors novel regular: %d/%d; errors novel abnormal: %d/%d\"\n    % (\n        n_error_train,\n        X_train.shape[0],\n        n_error_test,\n        X_test.shape[0],\n        n_error_outliers,\n        X_outliers.shape[0],\n    )\n)\n()\n\n(figsize=(9, 6))\n(\"Online One-Class SVM\")\n(xx, yy, Z_sgd, levels=(Z_sgd.min(), 0, 7), cmap=plt.cm.PuBu)\na = (xx, yy, Z_sgd, levels=[0], linewidths=2, colors=\"darkred\")\n(xx, yy, Z_sgd, levels=[0, Z_sgd.max()], colors=\"palevioletred\")\n\ns = 20\nb1 = (X_train[:, 0], X_train[:, 1], c=\"white\", s=s, edgecolors=\"k\")\nb2 = (X_test[:, 0], X_test[:, 1], c=\"blueviolet\", s=s, edgecolors=\"k\")\nc = (X_outliers[:, 0], X_outliers[:, 1], c=\"gold\", s=s, edgecolors=\"k\")\n(\"tight\")\n((-4.5, 4.5))\n((-4.5, 4.5))\n(\n    [a.collections[0], b1, b2, c],\n    [\n        \"learned frontier\",\n        \"training observations\",\n        \"new regular observations\",\n        \"new abnormal observations\",\n    ],\n    loc=\"upper left\",\n)\n(\n    \"error train: %d/%d; errors novel regular: %d/%d; errors novel abnormal: %d/%d\"\n    % (\n        n_error_train_sgd,\n        X_train.shape[0],\n        n_error_test_sgd,\n        X_test.shape[0],\n        n_error_outliers_sgd,\n        X_outliers.shape[0],\n    )\n)\n()"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Introductory->Customizing Matplotlib with style sheets and rcParams->Runtime rc settings"
            ],
            [
                "sklearn->Examples->Pipelines and composite estimators->Selecting dimensionality reduction with Pipeline and GridSearchCV->Illustration of Pipeline and GridSearchCV"
            ],
            [
                "sklearn->Examples->Model Selection->Class Likelihood Ratios to measure classification performance->Invariance with respect to prevalence"
            ],
            [
                "sklearn->Examples->Missing Value Imputation->Imputing missing values with variants of IterativeImputer"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->One-Class SVM versus One-Class SVM using Stochastic Gradient Descent"
            ]
        ]
    },
    "1415485": {
        "jupyter_code_cell": "complete_fo_sw = pd.merge(fo_sw, italian_names_frame, how='inner', on='Canton', sort=True)\ncomplete_fo_sw.head()\nmax_foreing = complete_fo_sw['foreign_unemployment_rate'].max()\nmin_foreign = complete_fo_sw['foreign_unemployment_rate'].min()\nmax_swiss = complete_fo_sw['swiss_unemployment_rate'].max()\nmin_swiss = complete_fo_sw['swiss_unemployment_rate'].min()",
        "matched_tutorial_code_inds": [
            2951,
            5613,
            6444,
            5643,
            5647
        ],
        "matched_tutorial_codes": [
            "cluster_ids = (dist_linkage, 1, criterion=\"distance\")\ncluster_id_to_feature_ids = (list)\nfor idx, cluster_id in enumerate(cluster_ids):\n    cluster_id_to_feature_ids[cluster_id].append(idx)\nselected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n\nX_train_sel = X_train[:, selected_features]\nX_test_sel = X_test[:, selected_features]\n\nclf_sel = (n_estimators=100, random_state=42)\nclf_sel.fit(X_train_sel, y_train)\nprint(\n    \"Accuracy on test data with features removed: {:.2f}\".format(\n        clf_sel.score(X_test_sel, y_test)\n    )\n)",
            "gr = data[\"affairs rate_marriage age yrs_married\".split()].groupby(\n    \"rate_marriage age yrs_married\".split()\n)\ndf_a = gr.agg([\"mean\", \"sum\", \"count\"])\n\n\ndef merge_tuple(tpl):\n    if isinstance(tpl, tuple) and len(tpl)  1:\n        return \"_\".join(map(str, tpl))\n    else:\n        return tpl\n\n\ndf_a.columns = df_a.columns.map(merge_tuple)\ndf_a.reset_index(inplace=True)\nprint(df_a.shape)\ndf_a.head()",
            "mod_uc = sm.tsa.UnobservedComponents(\n    endog, 'rwalk',\n    cycle=True, stochastic_cycle=True, damped_cycle=True,\n)\n# Here the powell method gets close to the optimum\nres_uc = mod_uc.fit(method='powell', disp=False)\n# but to get to the highest loglikelihood we do a\n# second round using the L-BFGS method.\nres_uc = mod_uc.fit(res_uc.params, disp=False)\nprint(res_uc.summary())",
            "glm = smf.glm(\n    \"affairs ~ rate_marriage + yrs_married\",\n    data=dc,\n    family=sm.families.Poisson(),\n    freq_weights=np.asarray(dc[\"freq\"]),\n)\nres_f2 = glm.fit()\n# print(res_f2.summary())\nres_f2.pearson_chi2 - res_f.pearson_chi2, res_f2.deviance - res_f.deviance, res_f2.llf - res_f.llf",
            "glm = smf.glm(\n    \"affairs_mean ~ rate_marriage + yrs_married\",\n    data=df_a,\n    family=sm.families.Poisson(),\n    var_weights=np.asarray(df_a[\"affairs_count\"]),\n)\nres_a2 = glm.fit()\nres_a2.pearson_chi2 - res_a.pearson_chi2, res_a2.deviance - res_a.deviance, res_a2.llf - res_a.llf"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Inspection->Permutation Importance with Multicollinear or Correlated Features->Handling Multicollinear Features"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Condensing and Aggregating observations->Dataset with unique explanatory variables (exog)"
            ],
            [
                "statsmodels->Examples->State space models->Trends and cycles in unemployment->Unobserved components with stochastic cycle (UC)"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->original observations and frequency weights"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->aggregated data: exposure and var_weights"
            ]
        ]
    },
    "775310": {
        "jupyter_code_cell": "import math\nimport numpy as np\nfrom numpy import mean\nimport pandas\nimport csv\nimport seaborn\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport sklearn\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.cross_validation import cross_val_score\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import linear_model\nfrom sklearn import svm\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import classification_report",
        "matched_tutorial_code_inds": [
            1250,
            1838,
            1215,
            1333,
            3384
        ],
        "matched_tutorial_codes": [
            "import os\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom transformers import AutoTokenizer, GPT2TokenizerFast\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nimport functools\nfrom torch.optim.lr_scheduler import StepLR\nimport torch.nn.functional as F\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data.distributed import DistributedSampler\nfrom transformers.models.t5.modeling_t5 import T5Block\n\nfrom torch.distributed.algorithms._checkpoint.checkpoint_wrapper import (\n checkpoint_wrapper,\n CheckpointImpl,\n apply_activation_checkpointing_wrapper)\n\nfrom torch.distributed.fsdp import (\n    FullyShardedDataParallel as FSDP,\n    MixedPrecision,\n    BackwardPrefetch,\n    ShardingStrategy,\n    FullStateDictConfig,\n    StateDictType,\n)\nfrom torch.distributed.fsdp.wrap import (\n    transformer_auto_wrap_policy,\n    enable_wrap,\n    wrap,\n)\nfrom functools import partial\nfrom torch.utils.data import DataLoader\nfrom pathlib import Path\nfrom summarization_dataset import *\nfrom transformers.models.t5.modeling_t5 import T5Block\nfrom typing import Type\nimport time\nimport tqdm\nfrom datetime import datetime",
            "import scipy\nimport numpy as np\nfrom sklearn.model_selection import \nfrom sklearn.cluster import \nfrom sklearn.datasets import \nfrom sklearn.metrics import \n\nrng = (0)\nX, y = (random_state=rng)\nX = (X)\nX_train, X_test, _, y_test = (X, y, random_state=rng)\nkmeans = (n_init=\"auto\").fit(X_train)\nprint((kmeans.predict(X_test), y_test))",
            "import os\nimport sys\nimport tempfile\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.multiprocessing as mp\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\n# On Windows platform, the torch.distributed package only\n# supports Gloo backend, FileStore and TcpStore.\n# For FileStore, set init_method parameter in init_process_group\n# to a local file. Example as follow:\n# init_method=\"file:///f:/libtmp/some_file\"\n# dist.init_process_group(\n#    \"gloo\",\n#    rank=rank,\n#    init_method=init_method,\n#    world_size=world_size)\n# For TcpStore, same way as on Linux.\n\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n\n    # initialize the process group\n    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n\ndef cleanup():\n    dist.destroy_process_group()",
            "import sys\nimport os\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport tempfile\nfrom torch.nn import TransformerEncoder, \n\nclass PositionalEncoding():\n\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = (p=dropout)\n\n        pe = (max_len, d_model)\n        position = (0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = ((0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = (position * div_term)\n        pe[:, 1::2] = (position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.pe = nn.Parameter(pe, requires_grad=False)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)",
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import \nfrom sklearn.model_selection import \nfrom sklearn.pipeline import \nfrom sklearn.svm import \nfrom sklearn.decomposition import , \nfrom sklearn.feature_selection import , \nfrom sklearn.preprocessing import \n\nX, y = (return_X_y=True)\n\npipe = (\n    [\n        (\"scaling\", ()),\n        # the reduce_dim stage is populated by the param_grid\n        (\"reduce_dim\", \"passthrough\"),\n        (\"classify\", (dual=False, max_iter=10000)),\n    ]\n)\n\nN_FEATURES_OPTIONS = [2, 4, 8]\nC_OPTIONS = [1, 10, 100, 1000]\nparam_grid = [\n    {\n        \"reduce_dim\": [(iterated_power=7), (max_iter=1_000)],\n        \"reduce_dim__n_components\": N_FEATURES_OPTIONS,\n        \"classify__C\": C_OPTIONS,\n    },\n    {\n        \"reduce_dim\": [()],\n        \"reduce_dim__k\": N_FEATURES_OPTIONS,\n        \"classify__C\": C_OPTIONS,\n    },\n]\nreducer_labels = [\"PCA\", \"NMF\", \"KBest(mutual_info_classif)\"]\n\ngrid = (pipe, n_jobs=1, param_grid=param_grid)\ngrid.fit(X, y)"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Parallel and Distributed Training->Advanced Model Training with Fully Sharded Data Parallel (FSDP)->Fine-tuning HF T5"
            ],
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 0.23->Scalability and stability improvements to KMeans"
            ],
            [
                "torch->Parallel and Distributed Training->Getting Started with Distributed Data Parallel->Basic Use Case"
            ],
            [
                "torch->Parallel and Distributed Training->Training Transformer models using Distributed Data Parallel and Pipeline Parallelism->Define the model"
            ],
            [
                "sklearn->Examples->Pipelines and composite estimators->Selecting dimensionality reduction with Pipeline and GridSearchCV->Illustration of Pipeline and GridSearchCV"
            ]
        ]
    },
    "245702": {
        "jupyter_code_cell": "plt.figure(figsize=(17,17))\nsns.heatmap(features[temp_col].corr(),annot=True)\nsns.pairplot(features[hum_col])",
        "matched_tutorial_code_inds": [
            6158,
            6197,
            6205,
            3771,
            3702
        ],
        "matched_tutorial_codes": [
            "fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nax.plot(arma_t.generate_sample(nsample=50))",
            "fig = plt.figure(figsize=(12, 10))\nax = fig.add_subplot(111)\nbk_cycles.plot(ax=ax, style=[\"r--\", \"b-\"])",
            "fig = plt.figure(figsize=(14, 10))\nax = fig.add_subplot(111)\ncf_cycles.plot(ax=ax, style=[\"r--\", \"b-\"])",
            "win_percent.sort_values().plot.barh(figsize=(6, 12), width=.85, color='k')\nplt.tight_layout()\nsns.despine()\nplt.xlabel(\"Win Percent\")",
            "plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data->Simulated ARMA(4,1): Model Identification is Difficult"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Time Series Filters->Baxter-King approximate band-pass filter: Inflation and Unemployment->Explore the hypothesis that inflation and unemployment are counter-cyclical."
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Time Series Filters->Christiano-Fitzgerald approximate band-pass filter: Inflation and Unemployment"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ]
        ]
    },
    "1308287": {
        "jupyter_code_cell": "print len(train_clean['msno'])\nprint len(msno)\nprint msno[:5]\ndef dummy_encoding(ds, dic, max_val):\n    for i, count in zip(ds, range(len(ds))):\n        if dic.has_key(i): \n            ds.iloc[count] = dic.get(i)\n        else: \n            max_val += 1\n            dic[i] = max_val\n            ds.iloc[count] = max_val\n    return ds, dic, max_val",
        "matched_tutorial_code_inds": [
            1521,
            2791,
            1064,
            6795,
            5917
        ],
        "matched_tutorial_codes": [
            "print(\"Rate of semiconductors added on a chip every 2 years:\")\nprint(\n    \"\\tx{:.2f} +/- {:.2f} semiconductors per chip\".format(\n        np.exp((A) * 2), 2 * A * np.exp(2 * A) * 0.006\n    )\n)",
            "print(\n    \"Mean AvgClaim Amount per policy:              %.2f \"\n    % df_train[\"AvgClaimAmount\"].mean()\n)\nprint(\n    \"Mean AvgClaim Amount | NbClaim  0:           %.2f\"\n    % df_train[\"AvgClaimAmount\"][df_train[\"AvgClaimAmount\"]  0].mean()\n)\nprint(\n    \"Predicted Mean AvgClaim Amount | NbClaim  0: %.2f\"\n    % glm_sev.predict(X_train).mean()\n)\nprint(\n    \"Predicted Mean AvgClaim Amount (dummy) | NbClaim  0: %.2f\"\n    % dummy_sev.predict(X_train).mean()\n)",
            "export GLUE_DIR=./glue_data\nexport TASK_NAME=MRPC\nexport OUT_DIR=./$TASK_NAME/\npython ./run_glue.py \\\n    --model_type bert \\\n    --model_name_or_path bert-base-uncased \\\n    --task_name $TASK_NAME \\\n    --do_train \\\n    --do_eval \\\n    --do_lower_case \\\n    --data_dir $GLUE_DIR/$TASK_NAME \\\n    --max_seq_length 128 \\\n    --per_gpu_eval_batch_size=8   \\\n    --per_gpu_train_batch_size=8   \\\n    --learning_rate 2e-5 \\\n    --num_train_epochs 3.0 \\\n    --save_steps 100000 \\\n    --output_dir $OUT_DIR",
            "x1n = np.linspace(20.5, 25, 10)\nXnew = np.column_stack((x1n, np.sin(x1n), (x1n - 5) ** 2))\nXnew = sm.add_constant(Xnew)\nynewpred = olsres.predict(Xnew)  # predict out of sample\nprint(ynewpred)",
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy->Calculating the historical growth curve for transistors"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Tweedie regression on insurance claims->Severity Model -  Gamma distribution"
            ],
            [
                "torch->Model Optimization->(beta) Dynamic Quantization on BERT->2. Fine-tune the BERT model"
            ],
            [
                "statsmodels->Examples->User Notes->Prediction->Create a new sample of explanatory variables Xnew, predict and plot"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ]
        ]
    },
    "233409": {
        "jupyter_code_cell": "customers = SQLContext(sc).read.csv('../datasets/customer.csv', header='true',inferSchema='true')\ncustomers.show(5)\ncustomers.printSchema()\nchurns = SQLContext(sc).read.csv('../datasets/churn.csv', header='true',inferSchema='true')\nchurns.show(5)\nchurns.printSchema()\ndata=customers.join(churns,customers['ID']==churns['ID']).select(customers['*'],churns['CHURN'])\ndata.show(5)",
        "matched_tutorial_code_inds": [
            4654,
            5299,
            5915,
            3904,
            2997
        ],
        "matched_tutorial_codes": [
            "data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>",
            "resid1 = res_ols.resid[w == 1.0]\nvar1 = resid1.var(ddof=int(res_ols.df_model) + 1)\nresid2 = res_ols.resid[w != 1.0]\nvar2 = resid2.var(ddof=int(res_ols.df_model) + 1)\nw_est = w.copy()\nw_est[w != 1.0] = np.sqrt(var2) / np.sqrt(var1)\nres_fwls = sm.WLS(y, X, 1.0 / ((w_est ** 2))).fit()\nprint(res_fwls.summary())",
            "resid = interM_lm32.get_influence().summary_frame()[\"standard_resid\"]\n\nplt.figure(figsize=(6, 6))\nresid = resid.reindex(X.index)\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X.loc[idx],\n        resid.loc[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\nplt.xlabel(\"X[~[32]]\")\nplt.ylabel(\"standardized resids\")",
            "dots = sns.load_dataset(\"dots\")\nsns.relplot(\n    data=dots, kind=\"line\",\n    x=\"time\", y=\"firing_rate\", col=\"align\",\n    hue=\"choice\", size=\"coherence\", style=\"choice\",\n    facet_kws=dict(sharex=False),\n)\n",
            "tree_disp = (tree, X, [\"age\"])\nmlp_disp = (\n    mlp, X, [\"age\"], ax=tree_disp.axes_, line_kw={\"color\": \"red\"}\n)\n\n\n<img alt=\"plot partial dependence visualization api\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_partial_dependence_visualization_api_006.png\" srcset=\"../../_images/sphx_glr_plot_partial_dependence_visualization_api_006.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Weighted Least Squares->Feasible Weighted Least Squares (2-stage FWLS)"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics"
            ],
            [
                "sklearn->Examples->Miscellaneous->Advanced Plotting With Partial Dependence->Plotting partial dependence for one feature"
            ]
        ]
    },
    "1487660": {
        "jupyter_code_cell": "shot_corr = shot_data_df.corr()\nshot_corr_tick_labels = ['user follower count', 'user following count', 'is pro user',                         'shot view count', 'shot like count', 'shot comment count', 'shot tag count', 'days since post',                         'is rebound shot', 'rebound count']\nfig, ax = plt.subplots(figsize=(15, 15))\nsns.set(font_scale=2.5)\nheatmap_plt = sns.heatmap(shot_corr, xticklabels=shot_corr_tick_labels, yticklabels=shot_corr_tick_labels,                          linewidths=.5, ax=ax, annot=True, annot_kws={'size': 18})\nheatmap_plt.set_title('Heatmap of Correlation Matrix')\nrebound_time_since_post = []\nfor rebound in all_rebounds:\n    shot_id_for_rebound = int(rebound['rebound_source_url'].replace('https://api.dribbble.com/v1/shots/', ''))\n    rebound_time = pd.to_datetime(rebound['created_at'])\n    for shot in all_shots: \n        if shot['id'] == shot_id_for_rebound:\n            time_diff = get_days_between(pd.to_datetime(shot['created_at']), rebound_time)\n            rebound_time_since_post.append(time_diff)\npd.Series(rebound_time_since_post).describe()",
        "matched_tutorial_code_inds": [
            4654,
            2118,
            23,
            5917,
            2114
        ],
        "matched_tutorial_codes": [
            "data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>",
            "dict_pos_dict_estimator = (\n    n_components=n_components,\n    alpha=0.1,\n    max_iter=50,\n    batch_size=3,\n    random_state=rng,\n    positive_dict=True,\n)\ndict_pos_dict_estimator.fit(faces_centered)\nplot_gallery(\n    \"Dictionary learning - positive dictionary\",\n    dict_pos_dict_estimator.components_[:n_components],\n    cmap=plt.cm.RdBu,\n)\n\n\n<img alt=\"Dictionary learning - positive dictionary\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_011.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_011.png\"/>",
            "face_dataset = FaceLandmarksDataset(csv_file='faces/face_landmarks.csv',\n                                    root_dir='faces/')\n\nfig = plt.figure()\n\nfor i in range(len(face_dataset)):\n    sample = face_dataset[i]\n\n    print(i, sample['image'].shape, sample['landmarks'].shape)\n\n    ax = plt.subplot(1, 4, i + 1)\n    plt.tight_layout()\n    ax.set_title('Sample #{}'.format(i))\n    ax.axis('off')\n    show_landmarks(**sample)\n\n    if i == 3:\n        plt.show()\n        break",
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "batch_dict_estimator = (\n    n_components=n_components, alpha=0.1, max_iter=50, batch_size=3, random_state=rng\n)\nbatch_dict_estimator.fit(faces_centered)\nplot_gallery(\"Dictionary learning\", batch_dict_estimator.components_[:n_components])\n\n\n<img alt=\"Dictionary learning\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_006.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_006.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings"
            ],
            [
                "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition: Dictionary learning->Dictionary learning - positive dictionary"
            ],
            [
                "torch->PyTorch Recipes->Developing Custom PyTorch Dataloaders->Part 1: The Dataset->1.3 Iterate through data samples"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition->Dictionary learning"
            ]
        ]
    },
    "1188623": {
        "jupyter_code_cell": "a, b, c = 1.0, -1.0, -1.0\nx = (1.0 + np.sqrt(5.0)) / 2.0\nval = a * x**2.0 + b * x + c\nprint('{0}x^2 + {1}x + {2} = {3}'.format(a, b, c, val))\nprimes = [2, 3, 5, 7, 11, 13]     \nmore_primes = primes + [17, 19]   \nprint('First few primes are: {primes}'.format(primes=primes))\nprint('Here are the primes up to the number 20: {}'.format(more_primes))",
        "matched_tutorial_code_inds": [
            2108,
            2446,
            3067,
            2601,
            4926
        ],
        "matched_tutorial_codes": [
            "n_row, n_col = 2, 3\nn_components = n_row * n_col\nimage_shape = (64, 64)\n\n\ndef plot_gallery(title, images, n_col=n_col, n_row=n_row, cmap=plt.cm.gray):\n    fig, axs = (\n        nrows=n_row,\n        ncols=n_col,\n        figsize=(2.0 * n_col, 2.3 * n_row),\n        facecolor=\"white\",\n        constrained_layout=True,\n    )\n    fig.set_constrained_layout_pads(w_pad=0.01, h_pad=0.02, hspace=0, wspace=0)\n    fig.set_edgecolor(\"black\")\n    fig.suptitle(title, size=16)\n    for ax, vec in zip(axs.flat, images):\n        vmax = max(vec.max(), -vec.min())\n        im = ax.imshow(\n            vec.reshape(image_shape),\n            cmap=cmap,\n            interpolation=\"nearest\",\n            vmin=-vmax,\n            vmax=vmax,\n        )\n        ax.axis(\"off\")\n\n    fig.colorbar(im, ax=axs, orientation=\"horizontal\", shrink=0.99, aspect=40, pad=0.01)\n    ()",
            "last_hours = slice(-96, None)\nfig, ax = (figsize=(12, 4))\nfig.suptitle(\"Predictions by linear models\")\nax.plot(\n    y.iloc[test_0].values[last_hours],\n    \"x-\",\n    alpha=0.2,\n    label=\"Actual demand\",\n    color=\"black\",\n)\nax.plot(naive_linear_predictions[last_hours], \"x-\", label=\"Ordinal time features\")\nax.plot(\n    cyclic_cossin_linear_predictions[last_hours],\n    \"x-\",\n    label=\"Trigonometric time features\",\n)\nax.plot(\n    cyclic_spline_linear_predictions[last_hours],\n    \"x-\",\n    label=\"Spline-based time features\",\n)\nax.plot(\n    one_hot_linear_predictions[last_hours],\n    \"x-\",\n    label=\"One-hot time features\",\n)\n_ = ax.legend()\n\n\n<img alt=\"Predictions by linear models\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_006.png\" srcset=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_006.png\"/>",
            "n_samples = len(digits.data)\ndata = digits.data / 16.0\ndata -= data.mean(axis=0)\n\n# We learn the digits on the first half of the digits\ndata_train, targets_train = (data[: n_samples // 2], digits.target[: n_samples // 2])\n\n\n# Now predict the value of the digit on the second half:\ndata_test, targets_test = (data[n_samples // 2 :], digits.target[n_samples // 2 :])\n# data_test = scaler.transform(data_test)\n\n# Create a classifier: a support vector classifier\nkernel_svm = (gamma=0.2)\nlinear_svm = ()\n\n# create pipeline from kernel approximation\n# and linear svm\nfeature_map_fourier = (gamma=0.2, random_state=1)\nfeature_map_nystroem = (gamma=0.2, random_state=1)\nfourier_approx_svm = (\n    [(\"feature_map\", feature_map_fourier), (\"svm\", ())]\n)\n\nnystroem_approx_svm = (\n    [(\"feature_map\", feature_map_nystroem), (\"svm\", ())]\n)\n\n# fit and predict using linear and kernel svm:\n\nkernel_svm_time = ()\nkernel_svm.fit(data_train, targets_train)\nkernel_svm_score = kernel_svm.score(data_test, targets_test)\nkernel_svm_time = () - kernel_svm_time\n\nlinear_svm_time = ()\nlinear_svm.fit(data_train, targets_train)\nlinear_svm_score = linear_svm.score(data_test, targets_test)\nlinear_svm_time = () - linear_svm_time\n\nsample_sizes = 30 * (1, 10)\nfourier_scores = []\nnystroem_scores = []\nfourier_times = []\nnystroem_times = []\n\nfor D in sample_sizes:\n    fourier_approx_svm.set_params(feature_map__n_components=D)\n    nystroem_approx_svm.set_params(feature_map__n_components=D)\n    start = ()\n    nystroem_approx_svm.fit(data_train, targets_train)\n    nystroem_times.append(() - start)\n\n    start = ()\n    fourier_approx_svm.fit(data_train, targets_train)\n    fourier_times.append(() - start)\n\n    fourier_score = fourier_approx_svm.score(data_test, targets_test)\n    nystroem_score = nystroem_approx_svm.score(data_test, targets_test)\n    nystroem_scores.append(nystroem_score)\n    fourier_scores.append(fourier_score)\n\n# plot the results:\n(figsize=(16, 4))\naccuracy = (121)\n# second y axis for timings\ntimescale = (122)\n\naccuracy.plot(sample_sizes, nystroem_scores, label=\"Nystroem approx. kernel\")\ntimescale.plot(sample_sizes, nystroem_times, \"--\", label=\"Nystroem approx. kernel\")\n\naccuracy.plot(sample_sizes, fourier_scores, label=\"Fourier approx. kernel\")\ntimescale.plot(sample_sizes, fourier_times, \"--\", label=\"Fourier approx. kernel\")\n\n# horizontal lines for exact rbf and linear kernels:\naccuracy.plot(\n    [sample_sizes[0], sample_sizes[-1]],\n    [linear_svm_score, linear_svm_score],\n    label=\"linear svm\",\n)\ntimescale.plot(\n    [sample_sizes[0], sample_sizes[-1]],\n    [linear_svm_time, linear_svm_time],\n    \"--\",\n    label=\"linear svm\",\n)\n\naccuracy.plot(\n    [sample_sizes[0], sample_sizes[-1]],\n    [kernel_svm_score, kernel_svm_score],\n    label=\"rbf svm\",\n)\ntimescale.plot(\n    [sample_sizes[0], sample_sizes[-1]],\n    [kernel_svm_time, kernel_svm_time],\n    \"--\",\n    label=\"rbf svm\",\n)\n\n# vertical line for dataset dimensionality = 64\naccuracy.plot([64, 64], [0.7, 1], label=\"n_features\")\n\n# legends and labels\naccuracy.set_title(\"Classification accuracy\")\ntimescale.set_title(\"Training times\")\naccuracy.set_xlim(sample_sizes[0], sample_sizes[-1])\naccuracy.set_xticks(())\naccuracy.set_ylim(np.min(fourier_scores), 1)\ntimescale.set_xlabel(\"Sampling steps = transformed feature dimension\")\naccuracy.set_ylabel(\"Classification accuracy\")\ntimescale.set_ylabel(\"Training time in seconds\")\naccuracy.legend(loc=\"best\")\ntimescale.legend(loc=\"best\")\n()\n()\n\n\n<img alt=\"Classification accuracy, Training times\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_kernel_approximation_001.png\" srcset=\"../../_images/sphx_glr_plot_kernel_approximation_001.png\"/>",
            "vmin, vmax = (-log_marginal_likelihood).min(), 50\nlevel = (((vmin), (vmax), num=50), decimals=1)\n(\n    length_scale_grid,\n    noise_level_grid,\n    -log_marginal_likelihood,\n    levels=level,\n    norm=(vmin=vmin, vmax=vmax),\n)\n()\n(\"log\")\n(\"log\")\n(\"Length-scale\")\n(\"Noise-level\")\n(\"Log-marginal-likelihood\")\n()\n\n\n<img alt=\"Log-marginal-likelihood\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_noisy_005.png\" srcset=\"../../_images/sphx_glr_plot_gpr_noisy_005.png\"/>",
            "delta = 0.1\nx = np.arange(-3.0, 4.001, delta)\ny = np.arange(-4.0, 3.001, delta)\nX, Y = np.meshgrid(x, y)\nZ1 = np.exp(-X**2 - Y**2)\nZ2 = np.exp(-(X - 1)**2 - (Y - 1)**2)\nZ = (0.9*Z1 - 0.5*Z2) * 2\n\n# select a divergent colormap\ncmap = cm.coolwarm\n\nfig, (ax1, ax2) = plt.subplots(ncols=2)\npc = ax1.pcolormesh(Z, cmap=cmap)\nfig.colorbar(pc, ax=ax1)\nax1.set_title('Normalize()')\n\npc = ax2.pcolormesh(Z, norm=colors.CenteredNorm(), cmap=cmap)\nfig.colorbar(pc, ax=ax2)\nax2.set_title('CenteredNorm()')\n\nplt.show()\n\n\n<img alt=\"Normalize(), CenteredNorm()\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_colormapnorms_002.png\" srcset=\"../../_images/sphx_glr_colormapnorms_002.png, ../../_images/sphx_glr_colormapnorms_002_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Decomposition->Faces dataset decompositions->Dataset preparation"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Qualitative analysis of the impact of features on linear model predictions"
            ],
            [
                "sklearn->Examples->Miscellaneous->Explicit feature map approximation for RBF kernels->Timing and accuracy plots"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) with noise-level estimation->Optimisation of kernel hyperparameters in GPR"
            ],
            [
                "matplotlib->Tutorials->Colors->Colormap Normalization->Centered"
            ]
        ]
    },
    "1068412": {
        "jupyter_code_cell": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('white')\nsns.set_palette('dark')\nsns.set_context('talk')\n%matplotlib inline\nfrom accessory_functions import preprocess_series_text, nltk_path\ndata = pd.read_csv('../data/spam.csv', sep='\\t')\ndata['text'] = preprocess_series_text(data.text, nltk_path=nltk_path)\ndata.head()",
        "matched_tutorial_code_inds": [
            5256,
            1393,
            4752,
            4700,
            4661
        ],
        "matched_tutorial_codes": [
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pandas_datareader as pdr\nimport seaborn\n\nimport statsmodels.api as sm\nfrom statsmodels.regression.rolling import RollingOLS\n\nseaborn.set_style(\"darkgrid\")\npd.plotting.register_matplotlib_converters()\n%matplotlib inline",
            "import matplotlib.pyplot as plt\nimport numpy as np\nidx = 5\nprint(\"Question: \", dataset[\"train\"][idx][\"question\"])\nprint(\"Answers: \" ,dataset[\"train\"][idx][\"answers\"])\nim = np.asarray(dataset[\"train\"][idx][\"image\"].resize((500,500)))\nplt.imshow(im)\nplt.show()\n\n\n<img alt=\"flava finetuning tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_flava_finetuning_tutorial_001.png\" srcset=\"../_images/sphx_glr_flava_finetuning_tutorial_001.png\"/>",
            "import matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nred_patch = mpatches.Patch(color='red', label='The red data')\nax.legend(handles=[red_patch])\n\nplt.show()\n\n\n<img alt=\"legend guide\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_legend_guide_001.png\" srcset=\"../../_images/sphx_glr_legend_guide_001.png, ../../_images/sphx_glr_legend_guide_001_2_0x.png 2.0x\"/>",
            "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom cycler import cycler\nmpl.rcParams['lines.linewidth'] = 2\nmpl.rcParams['lines.linestyle'] = '--'\ndata = np.random.randn(50)\nplt.plot(data)\n\n\n<img alt=\"customizing\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_customizing_001.png\" srcset=\"../../_images/sphx_glr_customizing_001.png, ../../_images/sphx_glr_customizing_001_2_0x.png 2.0x\"/>",
            "import matplotlib.pyplot as plt\nplt.figure(1)                # the first figure\nplt.subplot(211)             # the first subplot in the first figure\nplt.plot([1, 2, 3])\nplt.subplot(212)             # the second subplot in the first figure\nplt.plot([4, 5, 6])\n\n\nplt.figure(2)                # a second figure\nplt.plot([4, 5, 6])          # creates a subplot() by default\n\nplt.figure(1)                # first figure current;\n                             # subplot(212) still current\nplt.subplot(211)             # make subplot(211) in the first figure\n                             # current\nplt.title('Easy as 1, 2, 3') # subplot 211 title"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Rolling Least Squares"
            ],
            [
                "torch->Multimodality->TorchMultimodal Tutorial: Finetuning FLAVA->Steps"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Legend guide->Creating artists specifically for adding to the legend (aka. Proxy artists)"
            ],
            [
                "matplotlib->Tutorials->Introductory->Customizing Matplotlib with style sheets and rcParams->Runtime rc settings"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Working with multiple figures and axes"
            ]
        ]
    },
    "915120": {
        "jupyter_code_cell": "df2['Married_couple_families_estimate_total'] = df2.Married_couple_families_estimate_total.astype(float)\nhousehold_population = df2.groupby(['County_name'])[['Households_estimate_total', 'Families_estimate_total', \n                                                     'Married_couple_families_estimate_total', \n                                                     'Nonfamily_households_estimate_total']].sum()\nhousehold_population",
        "matched_tutorial_code_inds": [
            6568,
            3756,
            3844,
            2575,
            2719
        ],
        "matched_tutorial_codes": [
            "y_pre = y.iloc[:-5]\ny_pre.plot(figsize=(15, 3), title='Inflation');\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_news_9_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_news_9_0.png\"/>",
            "delta = (by_game.home_rest - by_game.away_rest).dropna().astype(int)\nax = (delta.value_counts()\n    .reindex(np.arange(delta.min(), delta.max() + 1), fill_value=0)\n    .sort_index()\n    .plot(kind='bar', color='k', width=.9, rot=0, figsize=(12, 6))\n)\nsns.despine()\nax.set(xlabel='Difference in Rest (Home - Away)', ylabel='Games');",
            "data = (y.to_frame(name='y')\n         .assign(\u0394y=lambda df: df.y.diff())\n         .assign(L\u0394y=lambda df: df.\u0394y.shift()))\nmod_stationary = smf.ols('\u0394y ~ L\u0394y', data=data.dropna())\nres_stationary = mod_stationary.fit()",
            "co2_data = co2_data.resample(\"M\").mean().dropna(axis=\"index\", how=\"any\")\nco2_data.plot()\n(\"Monthly average of CO$_2$ concentration (ppm)\")\n_ = (\n    \"Monthly average of air samples measurements\\nfrom the Mauna Loa Observatory\"\n)\n\n\n<img alt=\"Monthly average of air samples measurements from the Mauna Loa Observatory\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_co2_002.png\" srcset=\"../../_images/sphx_glr_plot_gpr_co2_002.png\"/>",
            "df[\"Frequency\"] = df[\"ClaimNb\"] / df[\"Exposure\"]\n\nprint(\n    \"Average Frequency = {}\".format((df[\"Frequency\"], weights=df[\"Exposure\"]))\n)\n\nprint(\n    \"Fraction of exposure with zero claims = {0:.1%}\".format(\n        df.loc[df[\"ClaimNb\"] == 0, \"Exposure\"].sum() / df[\"Exposure\"].sum()\n    )\n)\n\nfig, (ax0, ax1, ax2) = (ncols=3, figsize=(16, 4))\nax0.set_title(\"Number of claims\")\n_ = df[\"ClaimNb\"].hist(bins=30, log=True, ax=ax0)\nax1.set_title(\"Exposure in years\")\n_ = df[\"Exposure\"].hist(bins=30, log=True, ax=ax1)\nax2.set_title(\"Frequency (number of claims per year)\")\n_ = df[\"Frequency\"].hist(bins=30, log=True, ax=ax2)\n\n\n<img alt=\"Number of claims, Exposure in years, Frequency (number of claims per year)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_poisson_regression_non_normal_loss_001.png\" srcset=\"../../_images/sphx_glr_plot_poisson_regression_non_normal_loss_001.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Simple univariate example: AR(1)->Step 1: fitting the model on the available dataset"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series->Autocorrelation"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) on Mauna Loa CO2 data->Build the dataset"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Poisson regression and non-normal loss->The French Motor Third-Party Liability Claims dataset"
            ]
        ]
    },
    "1449845": {
        "jupyter_code_cell": "from pylab import rcParams\nrcParams['figure.figsize'] = 12,8\nplt.scatter(y, x_RM, s=5, label = 'RM')\nplt.scatter(y, x_LSTAT, s=5, label = 'LSTAT')\nplt.legend(fontsize=15)\nplt.xlabel('Average number of rooms & Low status population', fontsize=15)\nplt.ylabel('Price', fontsize=15)\nplt.legend()\nplt.show()",
        "matched_tutorial_code_inds": [
            288,
            5889,
            1206,
            4950,
            2018
        ],
        "matched_tutorial_codes": [
            "from matplotlib import pyplot\nimport numpy as np\n\npyplot.imshow([0].reshape((28, 28)), cmap=\"gray\")\nprint(.shape)\n\n\n<img alt=\"nn tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_nn_tutorial_001.png\" srcset=\"../_images/sphx_glr_nn_tutorial_001.png\"/>",
            "from urllib.request import urlopen\nimport numpy as np\n\nnp.set_printoptions(precision=4, suppress=True)\n\nimport pandas as pd\n\npd.set_option(\"display.width\", 100)\nimport matplotlib.pyplot as plt\nfrom statsmodels.formula.api import ols\nfrom statsmodels.graphics.api import interaction_plot, abline_plot\nfrom statsmodels.stats.anova import anova_lm\n\ntry:\n    salary_table = pd.read_csv(\"salary.table\")\nexcept:  # recent pandas can read URL without urlopen\n    url = \"http://stats191.stanford.edu/data/salary.table\"\n    fh = urlopen(url)\n    salary_table = pd.read_table(fh)\n    salary_table.to_csv(\"salary.table\")\n\nE = salary_table.E\nM = salary_table.M\nX = salary_table.X\nS = salary_table.S",
            "from torch.profiler import , record_function, ProfilerActivity\nactivities = []\nif device == 'cuda':\n    activities.append()\n\nwith (activities=activities, record_shapes=False) as :\n    with record_function(\" Non-Compilied Causal Attention\"):\n        for _ in range(25):\n            model()\nprint(().table(sort_by=\"cuda_time_total\", row_limit=10))\n\n\nwith (activities=activities, record_shapes=False) as :\n    with record_function(\"Compiled Causal Attention\"):\n        for _ in range(25):\n            ()\nprint(().table(sort_by=\"cuda_time_total\", row_limit=10))\n\n# For even more insights, you can export the trace and use ``chrome://tracing`` to view the results\n# prof.export_chrome_trace(\"compiled_causal_attention_trace.json\").",
            "from matplotlib.font_manager import FontProperties\n\nfont = FontProperties()\nfont.set_family('serif')\nfont.set_name('Times New Roman')\nfont.set_style('italic')\n\nfig, ax = plt.subplots(figsize=(5, 3))\nfig.subplots_adjust(bottom=0.15, left=0.2)\nax.plot(x1, y1)\nax.set_xlabel('Time [s]', fontsize='large', fontweight='bold')\nax.set_ylabel('Damped oscillation [V]', fontproperties=font)\n\nplt.show()\n\n\n<img alt=\"text intro\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_text_intro_006.png\" srcset=\"../../_images/sphx_glr_text_intro_006.png, ../../_images/sphx_glr_text_intro_006_2_0x.png 2.0x\"/>",
            "from sklearn.cluster import \nimport matplotlib.pyplot as plt\n\nlabels = (graph, n_clusters=4, eigen_solver=\"arpack\")\nlabel_im = (mask.shape, -1.0)\nlabel_im[mask] = labels\n\nfig, axs = (nrows=1, ncols=2, figsize=(10, 5))\naxs[0].matshow(img)\naxs[1].matshow(label_im)\n\n()\n\n\n<img alt=\"plot segmentation toy\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_segmentation_toy_001.png\" srcset=\"../../_images/sphx_glr_plot_segmentation_toy_001.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Learning PyTorch->What is torch.nn really?->MNIST data setup"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "torch->Model Optimization->Using SDPA with torch.compile"
            ],
            [
                "matplotlib->Tutorials->Text->Text in Matplotlib Plots->Labels for x- and y-axis"
            ],
            [
                "sklearn->Examples->Clustering->Spectral clustering for image segmentation->Plotting four circles"
            ]
        ]
    },
    "396334": {
        "jupyter_code_cell": "for i in range(len(ratios)):\n    ratio_df = pd.DataFrame({spin_features[i]:ratios[i]})\n    X = pd.concat([X,ratio_df[spin_features[i]]],axis=1)\nTotal_mass = m",
        "matched_tutorial_code_inds": [
            4246,
            5312,
            5400,
            399,
            6659
        ],
        "matched_tutorial_codes": [
            "def rosen(x):\n...     \"\"\"The Rosenbrock function\"\"\"\n...     return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)",
            "oo = np.ones(df.shape[0])\nmodel2 = sm.MixedLM(df.y, oo, exog_re=oo, groups=df.group1, exog_vc=vcs)\nresult2 = model2.fit()\nprint(result2.summary())",
            "poisson_mod = sm.Poisson(rand_data.endog, rand_exog)\npoisson_res = poisson_mod.fit(method=\"newton\")\nprint(poisson_res.summary())",
            "accuracies = []\nexamples = []\n\n# Run test for each epsilon\nfor eps in epsilons:\n    acc, ex = test(model, , , eps)\n    accuracies.append(acc)\n    examples.append(ex)",
            "mod = LocalLevel(dta.infl)\nres = mod.fit(disp=False)\nprint(res.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "scipy->Optimization (scipy.optimize)->Unconstrained minimization of multivariate scalar functions (minimize)->Nelder-Mead Simplex algorithm (method='Nelder-Mead')"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Nested analysis"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Poisson"
            ],
            [
                "torch->Image and Video->Adversarial Example Generation->Implementation->Run Attack"
            ],
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Typical approach"
            ]
        ]
    },
    "48669": {
        "jupyter_code_cell": "base_model = InceptionV3(weights='imagenet', include_top=False)\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(1024, activation='relu')(x)\npredictions = Dense(classes, activation=final_activation_function)(x)\nmodel_tune = Model(inputs=base_model.input, outputs=predictions)\nfor layer in model_tune.layers:\n    layer.trainable = False\nmodel_tune.compile(optimizer='rmsprop', loss='binary_crossentropy',  metrics=[eval_metric])\nmodel_tune.layers",
        "matched_tutorial_code_inds": [
            2114,
            2118,
            2113,
            2119,
            82
        ],
        "matched_tutorial_codes": [
            "batch_dict_estimator = (\n    n_components=n_components, alpha=0.1, max_iter=50, batch_size=3, random_state=rng\n)\nbatch_dict_estimator.fit(faces_centered)\nplot_gallery(\"Dictionary learning\", batch_dict_estimator.components_[:n_components])\n\n\n<img alt=\"Dictionary learning\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_006.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_006.png\"/>",
            "dict_pos_dict_estimator = (\n    n_components=n_components,\n    alpha=0.1,\n    max_iter=50,\n    batch_size=3,\n    random_state=rng,\n    positive_dict=True,\n)\ndict_pos_dict_estimator.fit(faces_centered)\nplot_gallery(\n    \"Dictionary learning - positive dictionary\",\n    dict_pos_dict_estimator.components_[:n_components],\n    cmap=plt.cm.RdBu,\n)\n\n\n<img alt=\"Dictionary learning - positive dictionary\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_011.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_011.png\"/>",
            "batch_pca_estimator = (\n    n_components=n_components, alpha=0.1, max_iter=100, batch_size=3, random_state=rng\n)\nbatch_pca_estimator.fit(faces_centered)\nplot_gallery(\n    \"Sparse components - MiniBatchSparsePCA\",\n    batch_pca_estimator.components_[:n_components],\n)\n\n\n<img alt=\"Sparse components - MiniBatchSparsePCA\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_005.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_005.png\"/>",
            "dict_pos_code_estimator = (\n    n_components=n_components,\n    alpha=0.1,\n    max_iter=50,\n    batch_size=3,\n    fit_algorithm=\"cd\",\n    random_state=rng,\n    positive_code=True,\n)\ndict_pos_code_estimator.fit(faces_centered)\nplot_gallery(\n    \"Dictionary learning - positive code\",\n    dict_pos_code_estimator.components_[:n_components],\n    cmap=plt.cm.RdBu,\n)\n\n\n<img alt=\"Dictionary learning - positive code\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_012.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_012.png\"/>",
            "use_amp = True\n\nnet = make_model(in_size, out_size, num_layers)\nopt = (net.parameters(), lr=0.001)\nscaler = (enabled=use_amp)\n\nstart_timer()\nfor epoch in range(epochs):\n    for input, target in zip(data, targets):\n        with (device_type='cuda', dtype=torch.float16, enabled=use_amp):\n            output = net(input)\n            loss = loss_fn(output, target)\n        scaler.scale(loss).backward()\n        scaler.step(opt)\n        scaler.update()\n        opt.zero_grad() # set_to_none=True here can modestly improve performance\nend_timer_and_print(\"Mixed precision:\")"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition->Dictionary learning"
            ],
            [
                "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition: Dictionary learning->Dictionary learning - positive dictionary"
            ],
            [
                "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition->Sparse components - MiniBatchSparsePCA"
            ],
            [
                "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition: Dictionary learning->Dictionary learning - positive code"
            ],
            [
                "torch->PyTorch Recipes->Automatic Mixed Precision->All together: \u201cAutomatic Mixed Precision\u201d"
            ]
        ]
    },
    "1194669": {
        "jupyter_code_cell": "import pandas as pd\nimport numpy as np\nimport gensim\nfrom matplotlib import pyplot as plt\nfrom sklearn.manifold import TSNE",
        "matched_tutorial_code_inds": [
            6035,
            5875,
            6117,
            5407,
            1646
        ],
        "matched_tutorial_codes": [
            "import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom statsmodels.stats.mediation import Mediation",
            "import pandas as pd\nimport numpy as np\nfrom scipy.stats.distributions import norm, poisson\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom statsmodels.tsa.arima.model import ARIMA",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom statsmodels.formula.api import logit",
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->Mediation analysis with duration data"
            ],
            [
                "statsmodels->Examples->Generalized Estimating Equations->GEE Score Tests"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data"
            ],
            [
                "numpy->NumPy Applications->Plotting Fractals->What you\u2019ll need"
            ]
        ]
    },
    "145564": {
        "jupyter_code_cell": "mpg_data.dtypes\nmpg_data.horsepower = mpg_data.horsepower.astype('float')\nmpg_data.dtypes",
        "matched_tutorial_code_inds": [
            5909,
            1438,
            6360,
            5496,
            6354
        ],
        "matched_tutorial_codes": [
            "interM_lm.model.exog\ninterM_lm.model.exog_names",
            "img_array_transposed = np.transpose(img_array, (2, 0, 1))\nimg_array_transposed.shape",
            "delta_hat, rho_hat = sarima_res.params[:2]\ndelta_hat + rho_hat * y[0]",
            "predicted = res_log.model.predict(res_log.params, exog=data_student[['pared', 'public', 'gpa']])\npredicted",
            "delta_hat, rho_hat = arima_res.params[:2]\ndelta_hat + rho_hat * (y[0] - delta_hat)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Approximation->Applying to all colors"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Reconstructing residuals, fitted values and forecasts in SARIMAX and ARIMA"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Logit ordinal regression:"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Reconstructing residuals, fitted values and forecasts in SARIMAX and ARIMA"
            ]
        ]
    },
    "297475": {
        "jupyter_code_cell": "print(rsp.text[:500])\nhtml = bs4(rsp.text, 'html.parser')\nprint(html.prettify()[:1000])",
        "matched_tutorial_code_inds": [
            3784,
            4138,
            4124,
            4139,
            4045
        ],
        "matched_tutorial_codes": [
            "import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "iris = sns.load_dataset(\"iris\")\ng = sns.PairGrid(iris)\ng.map(sns.scatterplot)\n",
            "tips = sns.load_dataset(\"tips\")\ng = sns.FacetGrid(tips, col=\"time\")\n",
            "g = sns.PairGrid(iris)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n",
            "diamonds = sns.load_dataset(\"diamonds\")\nsns.displot(diamonds, x=\"carat\", kind=\"kde\")\n"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Conditional small multiples",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Conditional small multiples"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Kernel density estimation->Kernel density estimation pitfalls",
                "seaborn->Plotting functions->Visualizing distributions of data->Kernel density estimation->Kernel density estimation pitfalls"
            ]
        ]
    },
    "1139357": {
        "jupyter_code_cell": "print(loans_2007.shape[1])\nloans_2007 = loans_2007.drop('pymnt_plan', axis=1)\nloans_2007.shape[1]\nloans_2007.isnull().sum()",
        "matched_tutorial_code_inds": [
            6065,
            6659,
            6069,
            5143,
            6818
        ],
        "matched_tutorial_codes": [
            "sel = ar_select_order(housing, 13, old_names=False)\nsel.ar_lags\nres = sel.model.fit()\nprint(res.summary())",
            "mod = LocalLevel(dta.infl)\nres = mod.fit(disp=False)\nprint(res.summary())",
            "sel = ar_select_order(housing, 13, seasonal=True, old_names=False)\nsel.ar_lags\nres = sel.model.fit()\nprint(res.summary())",
            "sqtab = sm.stats.SquareTable.from_data(data[['left', 'right']])\nprint(sqtab.summary())",
            "fcast_res3 = res.get_forecast('2010Q2')\nprint(fcast_res3.summary_frame())"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions"
            ],
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Typical approach"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions->Seasonal Dummies"
            ],
            [
                "statsmodels->User Guide->Statistics and Tools->Contingency tables->Symmetry and homogeneity"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example->Specifying the number of forecasts"
            ]
        ]
    },
    "661865": {
        "jupyter_code_cell": "populations = pd.Series([1357000000, 1252000000, 321068000, 249900000, 200400000, 191854000], \n                        [\"China\", \"India\", \"United States\", \"Indonesia\", \"Brazil\", \"Pakistan\"])\nprint(populations)\npopulations = pd.Series({\"China\":1357000000, \"India\":1252000000, \"United States\":321068000, \"Indonesia\":249900000, \n                         \"Brazil\":200400000, \"Pakistan\":191854000})\nprint(populations)",
        "matched_tutorial_code_inds": [
            5956,
            3904,
            6269,
            4654,
            5643
        ],
        "matched_tutorial_codes": [
            "kidney_lm = ols(\"np.log(Days+1) ~ C(Duration) * C(Weight)\", data=kt).fit()\n\ntable10 = anova_lm(kidney_lm)\n\nprint(\n    anova_lm(ols(\"np.log(Days+1) ~ C(Duration) + C(Weight)\", data=kt).fit(), kidney_lm)\n)\nprint(\n    anova_lm(\n        ols(\"np.log(Days+1) ~ C(Duration)\", data=kt).fit(),\n        ols(\"np.log(Days+1) ~ C(Duration) + C(Weight, Sum)\", data=kt).fit(),\n    )\n)\nprint(\n    anova_lm(\n        ols(\"np.log(Days+1) ~ C(Weight)\", data=kt).fit(),\n        ols(\"np.log(Days+1) ~ C(Duration) + C(Weight, Sum)\", data=kt).fit(),\n    )\n)",
            "dots = sns.load_dataset(\"dots\")\nsns.relplot(\n    data=dots, kind=\"line\",\n    x=\"time\", y=\"firing_rate\", col=\"align\",\n    hue=\"choice\", size=\"coherence\", style=\"choice\",\n    facet_kws=dict(sharex=False),\n)\n",
            "states1 = pd.DataFrame(\n    np.c_[fit1.level, fit1.trend, fit1.season],\n    columns=[\"level\", \"slope\", \"seasonal\"],\n    index=aust.index,\n)\nstates2 = pd.DataFrame(\n    np.c_[fit2.level, fit2.trend, fit2.season],\n    columns=[\"level\", \"slope\", \"seasonal\"],\n    index=aust.index,\n)\nfig, [[ax1, ax4], [ax2, ax5], [ax3, ax6]] = plt.subplots(3, 2, figsize=(12, 8))\nstates1[[\"level\"]].plot(ax=ax1)\nstates1[[\"slope\"]].plot(ax=ax2)\nstates1[[\"seasonal\"]].plot(ax=ax3)\nstates2[[\"level\"]].plot(ax=ax4)\nstates2[[\"slope\"]].plot(ax=ax5)\nstates2[[\"seasonal\"]].plot(ax=ax6)\nplt.show()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_22_0.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_22_0.png\"/>",
            "data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>",
            "glm = smf.glm(\n    \"affairs ~ rate_marriage + yrs_married\",\n    data=dc,\n    family=sm.families.Poisson(),\n    freq_weights=np.asarray(dc[\"freq\"]),\n)\nres_f2 = glm.fit()\n# print(res_f2.summary())\nres_f2.pearson_chi2 - res_f.pearson_chi2, res_f2.deviance - res_f.deviance, res_f2.llf - res_f.llf"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->ANOVA->Two-way ANOVA"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Winters Seasonal->The Internals"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->original observations and frequency weights"
            ]
        ]
    },
    "1164436": {
        "jupyter_code_cell": "total_unique_players = heroes_df['SN'].nunique()\nplayers_df = pd.DataFrame([{'Total Players': total_unique_players}])\nplayers_df.set_index('Total Players', inplace = True)\nplayers_df\ntotal_unique_items = heroes_df['Item Name'].nunique()\ntotal_purchases = heroes_df['Price'].count()\ntotal_revenue = round(heroes_df['Price'].sum(),2)\navg_price = round(total_revenue /total_purchases, 2)\npur_analysis = pd.DataFrame([{\n    \"Number of Unique Items\": total_unique_items,\n    'Average Purchase Price': avg_price,\n    'Total Purchases': total_purchases,\n    'Total Revenue': total_revenue\n}])\npur_analysis.style.format({'Average Purchase Price': '${:.2f}', 'Total Revenue': '${:,.2f}'})",
        "matched_tutorial_code_inds": [
            5917,
            6700,
            2118,
            2093,
            2860
        ],
        "matched_tutorial_codes": [
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "ln_pce = np.log(pce.PCE)\nforecasts = {\"ln PCE\": ln_pce}\nfor year in range(1995, 2020, 3):\n    sub = ln_pce[: str(year)]\n    res = ThetaModel(sub).fit()\n    fcast = res.forecast(12)\n    forecasts[str(year)] = fcast\nforecasts = pd.DataFrame(forecasts)\nax = forecasts[\"1995\":].plot(legend=False)\nchildren = ax.get_children()\nchildren[0].set_linewidth(4)\nchildren[0].set_alpha(0.3)\nchildren[0].set_color(\"#000000\")\nax.set_title(\"ln PCE\")\nplt.tight_layout(pad=1.0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_theta-model_22_0.png\" src=\"../../../_images/examples_notebooks_generated_theta-model_22_0.png\"/>",
            "dict_pos_dict_estimator = (\n    n_components=n_components,\n    alpha=0.1,\n    max_iter=50,\n    batch_size=3,\n    random_state=rng,\n    positive_dict=True,\n)\ndict_pos_dict_estimator.fit(faces_centered)\nplot_gallery(\n    \"Dictionary learning - positive dictionary\",\n    dict_pos_dict_estimator.components_[:n_components],\n    cmap=plt.cm.RdBu,\n)\n\n\n<img alt=\"Dictionary learning - positive dictionary\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_011.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_011.png\"/>",
            "n_nodes = clf.tree_.node_count\nchildren_left = clf.tree_.children_left\nchildren_right = clf.tree_.children_right\nfeature = clf.tree_.feature\nthreshold = clf.tree_.threshold\n\nnode_depth = (shape=n_nodes, dtype=)\nis_leaves = (shape=n_nodes, dtype=bool)\nstack = [(0, 0)]  # start with the root node id (0) and its depth (0)\nwhile len(stack)  0:\n    # `pop` ensures each node is only visited once\n    node_id, depth = stack.pop()\n    node_depth[node_id] = depth\n\n    # If the left and right child of a node is not the same we have a split\n    # node\n    is_split_node = children_left[node_id] != children_right[node_id]\n    # If a split node, append left and right children and depth to `stack`\n    # so we can loop through them\n    if is_split_node:\n        stack.append((children_left[node_id], depth + 1))\n        stack.append((children_right[node_id], depth + 1))\n    else:\n        is_leaves[node_id] = True\n\nprint(\n    \"The binary tree structure has {n} nodes and has \"\n    \"the following tree structure:\\n\".format(n=n_nodes)\n)\nfor i in range(n_nodes):\n    if is_leaves[i]:\n        print(\n            \"{space}node={node} is a leaf node.\".format(\n                space=node_depth[i] * \"\\t\", node=i\n            )\n        )\n    else:\n        print(\n            \"{space}node={node} is a split node: \"\n            \"go to node {left} if X[:, {feature}] &lt;= {threshold} \"\n            \"else to node {right}.\".format(\n                space=node_depth[i] * \"\\t\",\n                node=i,\n                left=children_left[i],\n                feature=feature[i],\n                threshold=threshold[i],\n                right=children_right[i],\n            )\n        )",
            "mae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}\n\n_, ax = (figsize=(5, 5))\ndisplay = (\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n)\nax.set_title(\"Ridge model, optimum regularization\")\nfor name, score in scores.items():\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\nax.legend(loc=\"upper left\")\n()\n\n\n<img alt=\"Ridge model, optimum regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_012.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_012.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Personal Consumption Expenditure"
            ],
            [
                "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition: Dictionary learning->Dictionary learning - positive dictionary"
            ],
            [
                "sklearn->Examples->Decision Trees->Understanding the decision tree structure->Tree structure"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ]
        ]
    },
    "1283162": {
        "jupyter_code_cell": "print(values_list)\ntwitter_archive_master.to_csv(\"/home/workspace/twitter_archive_master.csv\", encoding='utf-8')",
        "matched_tutorial_code_inds": [
            3784,
            3994,
            4025,
            660,
            3792
        ],
        "matched_tutorial_codes": [
            "import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "tips = sns.load_dataset(\"tips\")\nsns.relplot(data=tips, x=\"total_bill\", y=\"tip\")\n",
            "tips = sns.load_dataset(\"tips\")\nsns.displot(tips, x=\"size\")\n",
            "interp = (rn18)\n(input)\nprint(interp.summary(True))",
            "sns.countplot(x='cut', data=df)\nsns.despine()\nplt.tight_layout()"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Relating variables with scatter plots",
                "seaborn->Plotting functions->Visualizing statistical relationships->Relating variables with scatter plots"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size",
                "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size"
            ],
            [
                "torch->Code Transforms with FX->(beta) Building a Simple CPU Performance Profiler with FX->Investigating the Performance of ResNet18"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ]
        ]
    },
    "546488": {
        "jupyter_code_cell": "data.columns\ndata[\"Do you celebrate Thanksgiving?\"].value_counts()",
        "matched_tutorial_code_inds": [
            5431,
            4025,
            3656,
            2801,
            3595
        ],
        "matched_tutorial_codes": [
            "respondent1000 = dta.iloc[[1000]]\naffair_mod.predict(respondent1000)",
            "tips = sns.load_dataset(\"tips\")\nsns.displot(tips, x=\"size\")\n",
            "weather2 = weather.reset_index()\nweather2[weather2['station'] == 'DSM'].head()",
            "X = survey.data[survey.feature_names]\nX.describe(include=\"all\")\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "twenty_train.target_names[gs_clf.predict(['God is love'])[0]]\n'soc.religion.christian'"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size",
                "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size"
            ],
            [
                "pandas_toms_blog->Indexes->Flavors->Row Slicing"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "sklearn->Tutorials->Working With Text Data->Parameter tuning using grid search"
            ]
        ]
    },
    "968489": {
        "jupyter_code_cell": "ggplot(data=sug, mapping=aes(x='datetime', y='sugar')) +    geom_line(color='blue') +    scale_x_date(breaks=['2014-05-20', '2014-06-01','2014-06-15','2014-07-01'],\n                 labels=['20 may','1 june','15 june','1 july'])\nggplot(data=sug, mapping=aes(x='datetime')) +    scale_x_date(breaks=['2014-05-20', '2014-06-01','2014-06-15','2014-07-01'],\n                 labels=['20 may','1 june','15 june','1 july']) +    geom_line(mapping=aes(y='weight'), color='red') +    geom_line(mapping=aes(y='sugar'), color='blue') +    ylab('-')",
        "matched_tutorial_code_inds": [
            1097,
            2662,
            3400,
            4158,
            5643
        ],
        "matched_tutorial_codes": [
            "qat_model = load_model(saved_model_dir + float_model_file)\nqat_model.fuse_model()\n\noptimizer = torch.optim.SGD(qat_model.parameters(), lr = 0.0001)\n# The old 'fbgemm' is still available but 'x86' is the recommended default.\nqat_model.qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')",
            "m, s, _ = (\n    (enet.coef_)[0],\n    enet.coef_[enet.coef_ != 0],\n    markerfmt=\"x\",\n    label=\"Elastic net coefficients\",\n)\n([m, s], color=\"#2ca02c\")\nm, s, _ = (\n    (lasso.coef_)[0],\n    lasso.coef_[lasso.coef_ != 0],\n    markerfmt=\"x\",\n    label=\"Lasso coefficients\",\n)\n([m, s], color=\"#ff7f0e\")\n(\n    (coef)[0],\n    coef[coef != 0],\n    label=\"true coefficients\",\n    markerfmt=\"bx\",\n)\n\n(loc=\"best\")\n(\n    \"Lasso $R^2$: %.3f, Elastic Net $R^2$: %.3f\" % (r2_score_lasso, r2_score_enet)\n)\n()\n\n\n<img alt=\"Lasso $R^2$: 0.658, Elastic Net $R^2$: 0.643\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_lasso_and_elasticnet_001.png\" srcset=\"../../_images/sphx_glr_plot_lasso_and_elasticnet_001.png\"/>",
            "make_plot(5)\nmake_plot(6)\n\n\n\n<img alt=\"Data after power transformation (Yeo-Johnson), Full data, Zoom-in\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_all_scaling_006.png\" srcset=\"../../_images/sphx_glr_plot_all_scaling_006.png\"/>\n<img alt=\"Data after power transformation (Box-Cox), Full data, Zoom-in\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_all_scaling_007.png\" srcset=\"../../_images/sphx_glr_plot_all_scaling_007.png\"/>",
            "f = plt.figure(figsize=(6, 6))\ngs = f.add_gridspec(2, 2)\n\nwith sns.axes_style(\"darkgrid\"):\n    ax = f.add_subplot(gs[0, 0])\n    sinplot(6)\n\nwith sns.axes_style(\"white\"):\n    ax = f.add_subplot(gs[0, 1])\n    sinplot(6)\n\nwith sns.axes_style(\"ticks\"):\n    ax = f.add_subplot(gs[1, 0])\n    sinplot(6)\n\nwith sns.axes_style(\"whitegrid\"):\n    ax = f.add_subplot(gs[1, 1])\n    sinplot(6)\n\nf.tight_layout()\n",
            "glm = smf.glm(\n    \"affairs ~ rate_marriage + yrs_married\",\n    data=dc,\n    family=sm.families.Poisson(),\n    freq_weights=np.asarray(dc[\"freq\"]),\n)\nres_f2 = glm.fit()\n# print(res_f2.summary())\nres_f2.pearson_chi2 - res_f.pearson_chi2, res_f2.deviance - res_f.deviance, res_f2.llf - res_f.llf"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Model Optimization->(beta) Static Quantization with Eager Mode in PyTorch->5. Quantization-aware training"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Lasso and Elastic Net for Sparse Signals->Plot"
            ],
            [
                "sklearn->Examples->Preprocessing->Compare the effect of different scalers on data with outliers->PowerTransformer"
            ],
            [
                "seaborn->User guide and tutorial->Controlling figure aesthetics->Temporarily setting figure style",
                "seaborn->Figure aesthetics->Controlling figure aesthetics->Temporarily setting figure style"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->original observations and frequency weights"
            ]
        ]
    },
    "118313": {
        "jupyter_code_cell": "s.iloc[2]  \ns.describe()   ",
        "matched_tutorial_code_inds": [
            3937,
            3935,
            3938,
            3913,
            4027
        ],
        "matched_tutorial_codes": [
            "sns.relplot(data=flights_wide.transpose(), kind=\"line\")\n",
            "sns.relplot(data=flights_wide, kind=\"line\")\n",
            "sns.catplot(data=flights_wide, kind=\"box\")\n",
            "sns.pairplot(data=penguins, hue=\"species\")\n",
            "sns.displot(tips, x=\"size\", discrete=True)\n"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->Multivariate views on complex datasets",
                "seaborn->User guide and tutorial->Overview of seaborn plotting functions->Combining multiple views on the data",
                "seaborn->API Overview->Overview of seaborn plotting functions->Combining multiple views on the data"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size",
                "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size"
            ]
        ]
    },
    "1168946": {
        "jupyter_code_cell": "cchmmpred_data = {}\nfor entry in entries:\n    f = open('results/%s.profile' % entry, 'r')\n    data = f.readlines()\n    if entry in id2seq.keys():\n        try:\n            prob = data[0].rstrip().split(' ')[7]\n            cc_prob = 0\n            if float(prob) >= 0.5:\n                cc_prob = 1\n                assignment = data[1][5:].rstrip()\n                assignment = assignment.replace('H', '1')\n                assignment = assignment.replace('.', '0')\n            else:\n                assignment = '0' * len(id2seq[entry])\n            assert(len(assignment) == len(id2seq[entry]))\n            cchmmpred_data[entry] = [assignment, cc_prob]\n        except IndexError:\n            print(\"Error: %s\" % (entry))\nlen(entries), len(cchmmpred_data)",
        "matched_tutorial_code_inds": [
            3257,
            3173,
            6700,
            23,
            2085
        ],
        "matched_tutorial_codes": [
            "alphas = (-5, 1, 60)\nenet = (l1_ratio=0.7, max_iter=10000)\ntrain_errors = list()\ntest_errors = list()\nfor alpha in alphas:\n    enet.set_params(alpha=alpha)\n    enet.fit(X_train, y_train)\n    train_errors.append(enet.score(X_train, y_train))\n    test_errors.append(enet.score(X_test, y_test))\n\ni_alpha_optim = (test_errors)\nalpha_optim = alphas[i_alpha_optim]\nprint(\"Optimal regularization parameter : %s\" % alpha_optim)\n\n# Estimate the coef_ on full data with optimal regularization parameter\nenet.set_params(alpha=alpha_optim)\ncoef_ = enet.fit(X, y).coef_",
            "pair_scores = []\nmean_tpr = dict()\n\nfor ix, (label_a, label_b) in enumerate(pair_list):\n\n    a_mask = y_test == label_a\n    b_mask = y_test == label_b\n    ab_mask = (a_mask, b_mask)\n\n    a_true = a_mask[ab_mask]\n    b_true = b_mask[ab_mask]\n\n    idx_a = (label_binarizer.classes_ == label_a)[0]\n    idx_b = (label_binarizer.classes_ == label_b)[0]\n\n    fpr_a, tpr_a, _ = (a_true, y_score[ab_mask, idx_a])\n    fpr_b, tpr_b, _ = (b_true, y_score[ab_mask, idx_b])\n\n    mean_tpr[ix] = (fpr_grid)\n    mean_tpr[ix] += (fpr_grid, fpr_a, tpr_a)\n    mean_tpr[ix] += (fpr_grid, fpr_b, tpr_b)\n    mean_tpr[ix] /= 2\n    mean_score = (fpr_grid, mean_tpr[ix])\n    pair_scores.append(mean_score)\n\n    fig, ax = (figsize=(6, 6))\n    (\n        fpr_grid,\n        mean_tpr[ix],\n        label=f\"Mean {label_a} vs {label_b} (AUC = {mean_score :.2f})\",\n        linestyle=\":\",\n        linewidth=4,\n    )\n    (\n        a_true,\n        y_score[ab_mask, idx_a],\n        ax=ax,\n        name=f\"{label_a} as positive class\",\n    )\n    (\n        b_true,\n        y_score[ab_mask, idx_b],\n        ax=ax,\n        name=f\"{label_b} as positive class\",\n    )\n    ([0, 1], [0, 1], \"k--\", label=\"chance level (AUC = 0.5)\")\n    (\"square\")\n    (\"False Positive Rate\")\n    (\"True Positive Rate\")\n    (f\"{target_names[idx_a]} vs {label_b} ROC curves\")\n    ()\n    ()\n\nprint(f\"Macro-averaged One-vs-One ROC AUC score:\\n{(pair_scores):.2f}\")\n\n\n\n<img alt=\"setosa vs versicolor ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_004.png\" srcset=\"../../_images/sphx_glr_plot_roc_004.png\"/>\n<img alt=\"setosa vs virginica ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_005.png\" srcset=\"../../_images/sphx_glr_plot_roc_005.png\"/>\n<img alt=\"versicolor vs virginica ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_006.png\" srcset=\"../../_images/sphx_glr_plot_roc_006.png\"/>",
            "ln_pce = np.log(pce.PCE)\nforecasts = {\"ln PCE\": ln_pce}\nfor year in range(1995, 2020, 3):\n    sub = ln_pce[: str(year)]\n    res = ThetaModel(sub).fit()\n    fcast = res.forecast(12)\n    forecasts[str(year)] = fcast\nforecasts = pd.DataFrame(forecasts)\nax = forecasts[\"1995\":].plot(legend=False)\nchildren = ax.get_children()\nchildren[0].set_linewidth(4)\nchildren[0].set_alpha(0.3)\nchildren[0].set_color(\"#000000\")\nax.set_title(\"ln PCE\")\nplt.tight_layout(pad=1.0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_theta-model_22_0.png\" src=\"../../../_images/examples_notebooks_generated_theta-model_22_0.png\"/>",
            "face_dataset = FaceLandmarksDataset(csv_file='faces/face_landmarks.csv',\n                                    root_dir='faces/')\n\nfig = plt.figure()\n\nfor i in range(len(face_dataset)):\n    sample = face_dataset[i]\n\n    print(i, sample['image'].shape, sample['landmarks'].shape)\n\n    ax = plt.subplot(1, 4, i + 1)\n    plt.tight_layout()\n    ax.set_title('Sample #{}'.format(i))\n    ax.axis('off')\n    show_landmarks(**sample)\n\n    if i == 3:\n        plt.show()\n        break",
            "clfs = []\nfor ccp_alpha in ccp_alphas:\n    clf = (random_state=0, ccp_alpha=ccp_alpha)\n    clf.fit(X_train, y_train)\n    clfs.append(clf)\nprint(\n    \"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n        clfs[-1].tree_.node_count, ccp_alphas[-1]\n    )\n)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Model Selection->Train error vs Test error->Compute train and test errors"
            ],
            [
                "sklearn->Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->One-vs-One multiclass ROC->ROC curve using the OvO macro-average"
            ],
            [
                "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Personal Consumption Expenditure"
            ],
            [
                "torch->PyTorch Recipes->Developing Custom PyTorch Dataloaders->Part 1: The Dataset->1.3 Iterate through data samples"
            ],
            [
                "sklearn->Examples->Decision Trees->Post pruning decision trees with cost complexity pruning->Total impurity of leaves vs effective alphas of pruned tree"
            ]
        ]
    },
    "78187": {
        "jupyter_code_cell": "def shift_histogram_right(frame, shift=0.3):\n    new_frame = []\n    for i in range(0, len(frame)):\n        new_value = frame[i] + shift\n        if new_value > 1.0:\n            new_frame.append(np.float64(1.0))\n        else:\n            new_frame.append(new_value)\n    return np.array(new_frame).astype(np.float32)\nnp.random.seed(0)\nsample = np.random.rand(2) * len(train_data)\nr_shift = 0.30\nfor i in sample:\n    before = train_data[i]\n    after = shift_histogram_right(train_data[i], shift=r_shift)\n    compare_histograms(before, after)\n    compare_images(before, train_labels[i], after, train_labels[i])\nstart = time.time()\ntrain_shift_right = np.array(map(shift_histogram_right, train_data))\nprint(\"Finished shifting RIGHT train_data in %4f seconds\" % (time.time() - start))\ndef shift_histogram_left(frame, shift=0.2):\n    new_frame = []\n    for i in range(0, len(frame)):\n        new_value = frame[i] - shift\n        if new_value < 0.0:\n            new_frame.append(np.float64(0.0))\n        else:\n            new_frame.append(new_value)\n    return np.array(new_frame).astype(np.float32)\nnp.random.seed(0)\nsample = np.random.rand(2) * len(train_data)\nl_shift = 0.2\nfor i in sample:\n    before = train_data[i]\n    after = shift_histogram_left(train_data[i], shift=l_shift)\n    compare_histograms(before, after)\n    compare_images(before, train_labels[i], after, train_labels[i])\nstart = time.time()\ntrain_map_shift_left = np.array(map(shift_histogram_left, train_data))\nprint(\"Finished shifting LEFT train_data in %4f seconds\" % (time.time() - start))\ndef shift_histogram_right_all(train_data=train_data):\n    train_shift_right = np.zeros((len(train_data), 9216))\n    for index in range(0, len(train_data)):\n        train_shift_right[index] = np.array(shift_histogram_right(train_data[index]))\n    return np.array(train_shift_right)\ndef shift_histogram_left_all(train_data=train_data):\n    train_shift_left = np.zeros((len(train_data), 9216))\n    for index in range(0, len(train_data)):\n        train_shift_left[index] = np.array(shift_histogram_left(train_data[index]))\n    return np.array(train_shift_left)\nstart = time.time()\nprint(\"Starting shift right of all data\")\ntrain_shift_right = shift_histogram_right_all()\nprint(\"Finished shifting all right in %4f seconds\" % (time.time() - start))\nstart = time.time()\nprint(\"Starting shift left of all data\")\ntrain_shift_left = shift_histogram_left_all()\nprint(\"Finished shifting all left in %4f seconds\" % (time.time() - start))\nprint(len(train_shift_left))",
        "matched_tutorial_code_inds": [
            2966,
            681,
            347,
            353,
            268
        ],
        "matched_tutorial_codes": [
            "def plot_3d(points, points_color, title):\n    x, y, z = points.T\n\n    fig, ax = (\n        figsize=(6, 6),\n        facecolor=\"white\",\n        tight_layout=True,\n        subplot_kw={\"projection\": \"3d\"},\n    )\n    fig.suptitle(title, size=16)\n    col = ax.scatter(x, y, z, c=points_color, s=50, alpha=0.8)\n    ax.view_init(azim=-60, elev=9)\n    ax.xaxis.set_major_locator((1))\n    ax.yaxis.set_major_locator((1))\n    ax.zaxis.set_major_locator((1))\n\n    fig.colorbar(col, ax=ax, orientation=\"horizontal\", shrink=0.6, aspect=60, pad=0.01)\n    ()\n\n\ndef plot_2d(points, points_color, title):\n    fig, ax = (figsize=(3, 3), facecolor=\"white\", constrained_layout=True)\n    fig.suptitle(title, size=16)\n    add_2d_scatter(ax, points, points_color)\n    ()\n\n\ndef add_2d_scatter(ax, points, points_color, title=None):\n    x, y = points.T\n    ax.scatter(x, y, c=points_color, s=50, alpha=0.8)\n    ax.set_title(title)\n    ax.xaxis.set_major_formatter(())\n    ax.yaxis.set_major_formatter(())\n\n\nplot_3d(S_points, S_color, \"Original S-curve samples\")\n\n\n<img alt=\"Original S-curve samples\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_compare_methods_001.png\" srcset=\"../../_images/sphx_glr_plot_compare_methods_001.png\"/>",
            "def contains_cl(args):\n    for t in args:\n        if isinstance(t, ):\n            if t.is_contiguous(memory_format=) and not t.is_contiguous():\n                return True\n        elif isinstance(t, list) or isinstance(t, tuple):\n            if contains_cl(list(t)):\n                return True\n    return False\n\n\ndef print_inputs(args, indent=\"\"):\n    for t in args:\n        if isinstance(t, ):\n            print(indent, t.stride(), t.shape, t.device, t.dtype)\n        elif isinstance(t, list) or isinstance(t, tuple):\n            print(indent, type(t))\n            print_inputs(list(t), indent=indent + \"    \")\n        else:\n            print(indent, t)\n\n\ndef check_wrapper(fn):\n    name = fn.__name__\n\n    def check_cl(*args, **kwargs):\n        was_cl = contains_cl(args)\n        try:\n            result = fn(*args, **kwargs)\n        except Exception as e:\n            print(\"`{}` inputs are:\".format(name))\n            print_inputs(args)\n            print(\"-------------------\")\n            raise e\n        failed = False\n        if was_cl:\n            if isinstance(result, ):\n                if result.dim() == 4 and not result.is_contiguous(memory_format=):\n                    print(\n                        \"`{}` got channels_last input, but output is not channels_last:\".format(name),\n                        result.shape,\n                        result.stride(),\n                        result.device,\n                        result.dtype,\n                    )\n                    failed = True\n        if failed and True:\n            print(\"`{}` inputs are:\".format(name))\n            print_inputs(args)\n            raise Exception(\"Operator `{}` lost channels_last property\".format(name))\n        return result\n\n    return check_cl\n\n\nold_attrs = dict()\n\n\ndef attribute(m):\n    old_attrs[m] = dict()\n    for i in dir(m):\n        e = getattr(m, i)\n        exclude_functions = [\"is_cuda\", \"has_names\", \"numel\", \"stride\", \"Tensor\", \"is_contiguous\", \"__class__\"]\n        if i not in exclude_functions and not i.startswith(\"_\") and \"__call__\" in dir(e):\n            try:\n                old_attrs[m][i] = e\n                setattr(m, i, check_wrapper(e))\n            except Exception as e:\n                print(i)\n                print(e)\n\n\nattribute()\nattribute(torch.nn.functional)\nattribute(torch)",
            "def preprocess(x, y):\n    return x.view(-1, 1, 28, 28), y\n\n\nclass WrappedDataLoader:\n    def __init__(self, dl, func):\n        self.dl = dl\n        self.func = func\n\n    def __len__(self):\n        return len(self.dl)\n\n    def __iter__(self):\n        batches = iter(self.dl)\n        for b in batches:\n            yield (self.func(*b))\n\ntrain_dl, valid_dl = get_data(, , bs)\ntrain_dl = WrappedDataLoader(train_dl, preprocess)\nvalid_dl = WrappedDataLoader(valid_dl, preprocess)",
            "def preprocess(x, y):\n    return x.view(-1, 1, 28, 28).to(), y.to()\n\n\ntrain_dl, valid_dl = get_data(, , bs)\ntrain_dl = WrappedDataLoader(train_dl, preprocess)\nvalid_dl = WrappedDataLoader(valid_dl, preprocess)",
            "def train_loop(dataloader, model, , ):\n    size = len(dataloader.dataset)\n    for batch, (X, y) in enumerate(dataloader):\n        # Compute prediction and loss\n        pred = model(X)\n        loss = (pred, y)\n\n        # Backpropagation\n        ()\n        loss.backward()\n        ()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), (batch + 1) * len(X)\n            print(f\"loss: {loss:&gt;7f}  [{current:&gt;5d}/{size:&gt;5d}]\")\n\n\ndef test_loop(dataloader, model, ):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    test_loss, correct = 0, 0\n\n    with ():\n        for X, y in dataloader:\n            pred = model(X)\n            test_loss += (pred, y).item()\n            correct += (pred.argmax(1) == y).type().sum().item()\n\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss: {test_loss:&gt;8f} \\n\")"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Manifold learning->Comparison of Manifold Learning methods->Dataset preparation"
            ],
            [
                "torch->Frontend APIs->(beta) Channels Last Memory Format in PyTorch->Converting existing models"
            ],
            [
                "torch->Learning PyTorch->What is torch.nn really?->Wrapping DataLoader"
            ],
            [
                "torch->Learning PyTorch->What is torch.nn really?->Using your GPU"
            ],
            [
                "torch->Introduction to PyTorch->Optimizing Model Parameters->Full Implementation"
            ]
        ]
    },
    "1097968": {
        "jupyter_code_cell": "import pandas as pd\nn_train = len(X_train)\nn_validation = len(X_valid)\nn_test = len(X_test)\nimage_shape = X_train.shape[1:]\nn_classes = pd.read_csv(\"signnames.csv\").shape[0]\nprint(\"Number of training examples =\", n_train)\nprint(\"Number of testing examples =\", n_test)\nprint(\"Image data shape =\", image_shape)\nprint(\"Number of classes =\", n_classes)\nimport matplotlib.pyplot as plt\nimport random\nplt.figure(figsize=(20, 10))\nfor i in range(n_classes):\n    plt.subplot(4, 11, i+1)\n    img = X_train[y_train == i][0]\n    plt.axis('off')\n    plt.title(\"ClassId: \" + str(i))\n    plt.imshow(img)\nplt.show()",
        "matched_tutorial_code_inds": [
            1393,
            3410,
            3367,
            1838,
            2252
        ],
        "matched_tutorial_codes": [
            "import matplotlib.pyplot as plt\nimport numpy as np\nidx = 5\nprint(\"Question: \", dataset[\"train\"][idx][\"question\"])\nprint(\"Answers: \" ,dataset[\"train\"][idx][\"answers\"])\nim = np.asarray(dataset[\"train\"][idx][\"image\"].resize((500,500)))\nplt.imshow(im)\nplt.show()\n\n\n<img alt=\"flava finetuning tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_flava_finetuning_tutorial_001.png\" srcset=\"../_images/sphx_glr_flava_finetuning_tutorial_001.png\"/>",
            "import pandas as pd\nfrom sklearn.decomposition import \n\npca = (n_components=2).fit(X_train)\nscaled_pca = (n_components=2).fit(scaled_X_train)\nX_train_transformed = pca.transform(X_train)\nX_train_std_transformed = scaled_pca.transform(scaled_X_train)\n\nfirst_pca_component = (\n    pca.components_[0], index=X.columns, columns=[\"without scaling\"]\n)\nfirst_pca_component[\"with scaling\"] = scaled_pca.components_[0]\nfirst_pca_component.plot.bar(\n    title=\"Weights of the first principal component\", figsize=(6, 8)\n)\n\n_ = ()\n\n\n<img alt=\"Weights of the first principal component\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_scaling_importance_002.png\" srcset=\"../../_images/sphx_glr_plot_scaling_importance_002.png\"/>",
            "import pandas as pd\n\ncv_results = (search_cv.cv_results_)\ncv_results = cv_results.sort_values(\"mean_test_score\", ascending=False)\ncv_results[\n    [\n        \"mean_test_score\",\n        \"std_test_score\",\n        \"param_preprocessor__num__imputer__strategy\",\n        \"param_preprocessor__cat__selector__percentile\",\n        \"param_classifier__C\",\n    ]\n].head(5)\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "import scipy\nimport numpy as np\nfrom sklearn.model_selection import \nfrom sklearn.cluster import \nfrom sklearn.datasets import \nfrom sklearn.metrics import \n\nrng = (0)\nX, y = (random_state=rng)\nX = (X)\nX_train, X_test, _, y_test = (X, y, random_state=rng)\nkmeans = (n_init=\"auto\").fit(X_train)\nprint((kmeans.predict(X_test), y_test))",
            "import numpy as np\nfrom sklearn.metrics import \n\nada_discrete_err = ((n_estimators,))\nfor i, y_pred in enumerate(ada_discrete.staged_predict(X_test)):\n    ada_discrete_err[i] = (y_pred, y_test)\n\nada_discrete_err_train = ((n_estimators,))\nfor i, y_pred in enumerate(ada_discrete.staged_predict(X_train)):\n    ada_discrete_err_train[i] = (y_pred, y_train)\n\nada_real_err = ((n_estimators,))\nfor i, y_pred in enumerate(ada_real.staged_predict(X_test)):\n    ada_real_err[i] = (y_pred, y_test)\n\nada_real_err_train = ((n_estimators,))\nfor i, y_pred in enumerate(ada_real.staged_predict(X_train)):\n    ada_real_err_train[i] = (y_pred, y_train)"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Multimodality->TorchMultimodal Tutorial: Finetuning FLAVA->Steps"
            ],
            [
                "sklearn->Examples->Preprocessing->Importance of Feature Scaling->Effect of rescaling on a PCA dimensional reduction"
            ],
            [
                "sklearn->Examples->Pipelines and composite estimators->Column Transformer with Mixed Types"
            ],
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 0.23->Scalability and stability improvements to KMeans"
            ],
            [
                "sklearn->Examples->Ensemble methods->Discrete versus Real AdaBoost->Adaboost with discrete SAMME and real SAMME.R"
            ]
        ]
    },
    "2582": {
        "jupyter_code_cell": "Boy=df[df['Gender']=='Boy']\nBoy_performance=Boy.groupby(['State'])['Total'].mean()\nprint(Boy_performance.plot(kind='bar',figsize=(20,7)))\nGirl=df[df['Gender']=='Girl']\nGirl_performance=Girl.groupby(['State'])['Total'].mean()\nprint(Girl_performance.plot(kind='bar',figsize=(20,7)))\nplt.xlabel(\"Girls performance across States\")",
        "matched_tutorial_code_inds": [
            2055,
            2662,
            1744,
            746,
            6880
        ],
        "matched_tutorial_codes": [
            "Corr(X)\n[[ 1.    0.44 -0.06 -0.01]\n [ 0.44  1.   -0.01 -0.06]\n [-0.06 -0.01  1.    0.5 ]\n [-0.01 -0.06  0.5   1.  ]]\nCorr(Y)\n[[ 1.    0.47 -0.05  0.02]\n [ 0.47  1.   -0.01  0.03]\n [-0.05 -0.01  1.    0.47]\n [ 0.02  0.03  0.47  1.  ]]",
            "m, s, _ = (\n    (enet.coef_)[0],\n    enet.coef_[enet.coef_ != 0],\n    markerfmt=\"x\",\n    label=\"Elastic net coefficients\",\n)\n([m, s], color=\"#2ca02c\")\nm, s, _ = (\n    (lasso.coef_)[0],\n    lasso.coef_[lasso.coef_ != 0],\n    markerfmt=\"x\",\n    label=\"Lasso coefficients\",\n)\n([m, s], color=\"#ff7f0e\")\n(\n    (coef)[0],\n    coef[coef != 0],\n    label=\"true coefficients\",\n    markerfmt=\"bx\",\n)\n\n(loc=\"best\")\n(\n    \"Lasso $R^2$: %.3f, Elastic Net $R^2$: %.3f\" % (r2_score_lasso, r2_score_enet)\n)\n()\n\n\n<img alt=\"Lasso $R^2$: 0.658, Elastic Net $R^2$: 0.643\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_lasso_and_elasticnet_001.png\" srcset=\"../../_images/sphx_glr_plot_lasso_and_elasticnet_001.png\"/>",
            "X_train = textproc.cleantext(train_df,\n                       text_column='review',\n                       remove_stopwords=True,\n                       remove_punc=True)[0:2000]\n\nX_test = textproc.cleantext(test_df,\n                       text_column='review',\n                       remove_stopwords=True,\n                       remove_punc=True)[0:1000]\n\ny_train = train_df['sentiment'].to_numpy()[0:2000]\ny_test = test_df['sentiment'].to_numpy()[0:1000]",
            "Per-sample-grads without vmap &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7f53a48fc250&gt;\ncompute_sample_grads(data, targets)\n  67.80 ms\n  1 measurement, 100 runs , 1 thread\nPer-sample-grads with vmap &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7f53a491a8f0&gt;\nft_compute_sample_grad(params, buffers, data, targets)\n  7.30 ms\n  1 measurement, 100 runs , 1 thread\nPerformance delta: 828.9354 percent improvement with vmap",
            "/opt/hostedtoolcache/Python/3.10.8/x64/lib/python3.10/site-packages/statsmodels/base/model.py:2694: UserWarning: df_model + k_constant differs from nparams\n  warnings.warn(\"df_model + k_constant differs from nparams\")\n/opt/hostedtoolcache/Python/3.10.8/x64/lib/python3.10/site-packages/statsmodels/base/model.py:2696: UserWarning: df_resid differs from nobs - nparams\n  warnings.warn(\"df_resid differs from nobs - nparams\")"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Cross decomposition->Compare cross decomposition methods->Dataset based latent variables model"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Lasso and Elastic Net for Sparse Signals->Plot"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ],
            [
                "torch->Frontend APIs->Per-sample-gradients->Performance comparison"
            ],
            [
                "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 2: Negative Binomial Regression for Count Data->Usage Example"
            ]
        ]
    },
    "995746": {
        "jupyter_code_cell": "j=0\nplt.figure(figsize=(10,10))\nfor i in list_features[:-1]:\n    j+=1\n    plt.subplot(3, 4,j )\n    sns.barplot(x='quality',y=i,palette='Reds',data=df)\nplt.tight_layout()\nplt.savefig('./figures/features.png')\ncorrmat = df.corr()\nf, ax = plt.subplots(figsize=(10, 9))\nsns.heatmap(abs(corrmat), vmax=0.6, square=True,annot=True)\nplt.savefig('./figures/corrmat.png')",
        "matched_tutorial_code_inds": [
            4090,
            5998,
            5225,
            2741,
            2116
        ],
        "matched_tutorial_codes": [
            "g = sns.catplot(\n    data=titanic,\n    x=\"fare\", y=\"embark_town\", row=\"class\",\n    kind=\"box\", orient=\"h\",\n    sharex=False, margin_titles=True,\n    height=1.5, aspect=4,\n)\ng.set(xlabel=\"Fare\", ylabel=\"\")\ng.set_titles(row_template=\"{row_name} class\")\nfor ax in g.axes.flat:\n    ax.xaxis.set_major_formatter('${x:.0f}')\n",
            "res5 = combine_effects(eff, var_eff, method_re=\"chi2\", use_t=False)\nres5_df = res5.summary_frame()\nprint(\"method RE:\", res5.method_re)\nprint(res5.summary_frame())\nfig = res5.plot_forest()\nfig.set_figheight(8)\nfig.set_figwidth(6)",
            "x = np.arange(data.income.min(), data.income.max(), 50)\nget_y = lambda a, b: a + b * x\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nfor i in range(models.shape[0]):\n    y = get_y(models.a[i], models.b[i])\n    ax.plot(x, y, linestyle=\"dotted\", color=\"grey\")\n\ny = get_y(ols[\"a\"], ols[\"b\"])\n\nax.plot(x, y, color=\"red\", label=\"OLS\")\nax.scatter(data.income, data.foodexp, alpha=0.2)\nax.set_xlim((240, 3000))\nax.set_ylim((240, 2000))\nlegend = ax.legend()\nax.set_xlabel(\"Income\", fontsize=16)\nax.set_ylabel(\"Food expenditure\", fontsize=16)",
            "x_train = (0, 10, 100)\nrng = (0)\nx_train = (rng.choice(x_train, size=20, replace=False))\ny_train = f(x_train)\n\n# create 2D-array versions of these arrays to feed to transformers\nX_train = x_train[:, ]\nX_plot = x_plot[:, ]",
            "fa_estimator = (n_components=n_components, max_iter=20)\nfa_estimator.fit(faces_centered)\nplot_gallery(\"Factor Analysis (FA)\", fa_estimator.components_[:n_components])\n\n# --- Pixelwise variance\n(figsize=(3.2, 3.6), facecolor=\"white\", tight_layout=True)\nvec = fa_estimator.noise_variance_\nvmax = max(vec.max(), -vec.min())\n(\n    vec.reshape(image_shape),\n    cmap=plt.cm.gray,\n    interpolation=\"nearest\",\n    vmin=-vmax,\n    vmax=vmax,\n)\n(\"off\")\n(\"Pixelwise variance from \\n Factor Analysis (FA)\", size=16, wrap=True)\n(orientation=\"horizontal\", shrink=0.8, pad=0.03)\n()\n\n\n\n<img alt=\"Factor Analysis (FA)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_008.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_008.png\"/>\n<img alt=\"Pixelwise variance from   Factor Analysis (FA)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_009.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_009.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Visualizing categorical data->Showing additional dimensions",
                "seaborn->Plotting functions->Visualizing categorical data->Showing additional dimensions"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Meta-analysis of proportions->changing data to have positive random effects variance"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Quantile Regression->Visualizing the results->First plot"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Polynomial and Spline interpolation"
            ],
            [
                "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition->Factor Analysis components - FA"
            ]
        ]
    },
    "1292954": {
        "jupyter_code_cell": "credentials = configparser.ConfigParser()\ncredentials.read(os.path.join('..', 'credentials.ini'))\nauth = tweepy.OAuthHandler(credentials.get('twitter', 'consumer_key'), credentials.get('twitter', 'consumer_secret'))\nauth.set_access_token(credentials.get('twitter', 'access_token'), credentials.get('twitter', 'access_secret'))\napi = tweepy.API(auth) \nuser = 'EPFL_en'\nn = 20  ",
        "matched_tutorial_code_inds": [
            3291,
            23,
            3257,
            1065,
            2985
        ],
        "matched_tutorial_codes": [
            "nca = (max_iter=30, random_state=0)\nnca = nca.fit(X, y)\n\n(2)\nax2 = ()\nX_embedded = nca.transform(X)\nrelate_point(X_embedded, i, ax2)\n\nfor i in range(len(X)):\n    ax2.text(X_embedded[i, 0], X_embedded[i, 1], str(i), va=\"center\", ha=\"center\")\n    ax2.scatter(X_embedded[i, 0], X_embedded[i, 1], s=300, c=cm.Set1(y[[i]]), alpha=0.4)\n\nax2.set_title(\"NCA embedding\")\nax2.axes.get_xaxis().set_visible(False)\nax2.axes.get_yaxis().set_visible(False)\nax2.axis(\"equal\")\n()\n\n\n<img alt=\"NCA embedding\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_nca_illustration_002.png\" srcset=\"../../_images/sphx_glr_plot_nca_illustration_002.png\"/>",
            "face_dataset = FaceLandmarksDataset(csv_file='faces/face_landmarks.csv',\n                                    root_dir='faces/')\n\nfig = plt.figure()\n\nfor i in range(len(face_dataset)):\n    sample = face_dataset[i]\n\n    print(i, sample['image'].shape, sample['landmarks'].shape)\n\n    ax = plt.subplot(1, 4, i + 1)\n    plt.tight_layout()\n    ax.set_title('Sample #{}'.format(i))\n    ax.axis('off')\n    show_landmarks(**sample)\n\n    if i == 3:\n        plt.show()\n        break",
            "alphas = (-5, 1, 60)\nenet = (l1_ratio=0.7, max_iter=10000)\ntrain_errors = list()\ntest_errors = list()\nfor alpha in alphas:\n    enet.set_params(alpha=alpha)\n    enet.fit(X_train, y_train)\n    train_errors.append(enet.score(X_train, y_train))\n    test_errors.append(enet.score(X_test, y_test))\n\ni_alpha_optim = (test_errors)\nalpha_optim = alphas[i_alpha_optim]\nprint(\"Optimal regularization parameter : %s\" % alpha_optim)\n\n# Estimate the coef_ on full data with optimal regularization parameter\nenet.set_params(alpha=alpha_optim)\ncoef_ = enet.fit(X, y).coef_",
            "configs = Namespace()\n\n# The output directory for the fine-tuned model, $OUT_DIR.\nconfigs.output_dir = \"./MRPC/\"\n\n# The data directory for the MRPC task in the GLUE benchmark, $GLUE_DIR/$TASK_NAME.\nconfigs.data_dir = \"./glue_data/MRPC\"\n\n# The model name or path for the pre-trained model.\nconfigs.model_name_or_path = \"bert-base-uncased\"\n# The maximum length of an input sequence\nconfigs.max_seq_length = 128\n\n# Prepare GLUE task.\nconfigs.task_name = \"MRPC\".lower()\nconfigs.processor = processors[configs.task_name]()\nconfigs.output_mode = output_modes[configs.task_name]\nconfigs.label_list = configs.processor.get_labels()\nconfigs.model_type = \"bert\".lower()\nconfigs.do_lower_case = True\n\n# Set the device, batch size, topology, and caching flags.\nconfigs.device = \"cpu\"\nconfigs.per_gpu_eval_batch_size = 8\nconfigs.n_gpu = 0\nconfigs.local_rank = -1\nconfigs.overwrite_cache = False\n\n\n# Set random seed for reproducibility.\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\nset_seed(42)",
            "diabetes = ()\nX = (diabetes.data, columns=diabetes.feature_names)\ny = diabetes.target\n\ntree = ()\nmlp = (\n    (),\n    (hidden_layer_sizes=(100, 100), tol=1e-2, max_iter=500, random_state=0),\n)\ntree.fit(X, y)\nmlp.fit(X, y)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Nearest Neighbors->Neighborhood Components Analysis Illustration->Learning an embedding"
            ],
            [
                "torch->PyTorch Recipes->Developing Custom PyTorch Dataloaders->Part 1: The Dataset->1.3 Iterate through data samples"
            ],
            [
                "sklearn->Examples->Model Selection->Train error vs Test error->Compute train and test errors"
            ],
            [
                "torch->Model Optimization->(beta) Dynamic Quantization on BERT->2. Fine-tune the BERT model->2.1 Set global configurations"
            ],
            [
                "sklearn->Examples->Miscellaneous->Advanced Plotting With Partial Dependence->Train models on the diabetes dataset"
            ]
        ]
    },
    "1188870": {
        "jupyter_code_cell": "data[2:9]\ndata[4995:]",
        "matched_tutorial_code_inds": [
            5605,
            2122,
            192,
            5260,
            3939
        ],
        "matched_tutorial_codes": [
            "data[:3]",
            "data = ()\nX = ().fit_transform(data[\"data\"])\nfeature_names = data[\"feature_names\"]",
            "data = [[1, 2],[3, 4]]\n = (data)",
            "params.iloc[57:62]",
            "anagrams = sns.load_dataset(\"anagrams\")\nanagrams\n"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Weighted GLM: Poisson response data->Load data"
            ],
            [
                "sklearn->Examples->Decomposition->Factor Analysis (with rotation) to visualize patterns"
            ],
            [
                "torch->Introduction to PyTorch->Tensors->Initializing a Tensor"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Rolling Least Squares"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data",
                "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data"
            ]
        ]
    },
    "129968": {
        "jupyter_code_cell": "ggplot(airports) + aes(x = 'altitude', y= '..count..') + geom_point(stat = 'bin', bins = 40)\nggplot(airports) + aes(x = 'altitude') + geom_histogram(bins = 40)",
        "matched_tutorial_code_inds": [
            3792,
            4064,
            6804,
            6089,
            5365
        ],
        "matched_tutorial_codes": [
            "sns.countplot(x='cut', data=df)\nsns.despine()\nplt.tight_layout()",
            "sns.relplot(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\nsns.rugplot(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\n",
            "endog = macrodata['infl']\nendog.plot(figsize=(15, 5))",
            "fig = res_glob.plot_predict(start=714, end=732)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_40_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_40_0.png\"/>",
            "fig = beanplot(age, jitter=True)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_plots_boxplots_9_0.png\" src=\"../../../_images/examples_notebooks_generated_plots_boxplots_9_0.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Distribution visualization in other settings->Plotting joint and marginal distributions",
                "seaborn->Plotting functions->Visualizing distributions of data->Distribution visualization in other settings->Plotting joint and marginal distributions"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions->Industrial Production"
            ],
            [
                "statsmodels->Examples->Plotting->Box Plots->Bean Plots"
            ]
        ]
    },
    "1515095": {
        "jupyter_code_cell": "temp_dataset['majOpinWriter'] = temp_dataset['majOpinWriter'].fillna(temp_dataset['majOpinWriter'].median())\ntemp_dataset['lawMinorPresent'] = numpy.where(temp_dataset.lawMinor.isnull(), 0, 1)\ntemp_dataset['lawMinor'] = temp_dataset['lawMinor'].fillna(0)",
        "matched_tutorial_code_inds": [
            1620,
            6527,
            5295,
            6568,
            5516
        ],
        "matched_tutorial_codes": [
            "pixel_intensity_distribution = ndimage.histogram(\n    xray_image, min=np.min(xray_image), max=np.max(xray_image), bins=256\n)\n\nplt.plot(pixel_intensity_distribution)\nplt.title(\"Pixel intensity distribution\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/00c12e163a30744c62700ec414e2d514c5c4eae10256e7cd0cf87553d0f4d34c.png\" src=\"../_images/00c12e163a30744c62700ec414e2d514c5c4eae10256e7cd0cf87553d0f4d34c.png\"/>",
            "initial_obs_cov = np.cov(y.T)\ninitial_state_cov_diag = [0.01] * mod.k_states\n\n# Update H and Q\nmod.update_variances(initial_obs_cov, initial_state_cov_diag)\n\n# Perform Kalman filtering and smoothing\n# (the [] is just an empty list that in some models might contain\n# additional parameters. Here, we don't have any additional parameters\n# so we just pass an empty list)\ninitial_res = mod.smooth([])",
            "covb = res_ols.cov_params()\nprediction_var = res_ols.mse_resid + (X * np.dot(covb, X.T).T).sum(1)\nprediction_std = np.sqrt(prediction_var)\ntppf = stats.t.ppf(0.975, res_ols.df_resid)",
            "y_pre = y.iloc[:-5]\ny_pre.plot(figsize=(15, 3), title='Inflation');\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_news_9_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_news_9_0.png\"/>",
            "modfd_logit = OrderedModel.from_formula(\"apply ~ 1 + pared + public + gpa + C(dummy)\", data_student,\n                                      distr='logit')\nresfd_logit = modfd_logit.fit(method='bfgs')\nprint(resfd_logit.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Applications->X-ray image processing->Apply masks to X-rays with np.where()"
            ],
            [
                "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Application: Bayesian analysis of a TVP-VAR model by MCMC->Preliminary investigation with ad-hoc parameters in H, Q"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Weighted Least Squares->OLS vs.\u00a0WLS"
            ],
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Simple univariate example: AR(1)->Step 1: fitting the model on the available dataset"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model"
            ]
        ]
    },
    "50668": {
        "jupyter_code_cell": "import statsmodels.api as sm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt",
        "matched_tutorial_code_inds": [
            5710,
            6399,
            6455,
            1505,
            5484
        ],
        "matched_tutorial_codes": [
            "import statsmodels.api as sm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO",
            "import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt",
            "import numpy as np\nfrom scipy import stats\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport statsmodels.api as sm",
            "import numpy as np\nimport pandas as pd\nimport scipy.stats as stats\n\nfrom statsmodels.miscmodels.ordinal_model import OrderedModel"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Generalized Linear Models->Quasi-binomial regression"
            ],
            [
                "statsmodels->Examples->State space models->VARMAX: Introduction",
                "statsmodels->Examples->State space models->Trends and cycles in unemployment"
            ],
            [
                "statsmodels->Examples->State space models->Statespace ARMA: Sunspots Data"
            ],
            [
                "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy->What you\u2019ll need",
                "statsmodels->Examples->Robust Regression->Comparing OLS and RLM"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression"
            ]
        ]
    },
    "124443": {
        "jupyter_code_cell": "review_df.hist(column='stars')\nbiz_df.hist(column='stars')",
        "matched_tutorial_code_inds": [
            1743,
            6804,
            6209,
            4155,
            4150
        ],
        "matched_tutorial_codes": [
            "train_df = textproc.txt_to_df(imdb_train)\ntest_df = textproc.txt_to_df(imdb_test)",
            "endog = macrodata['infl']\nendog.plot(figsize=(15, 5))",
            "res_fedfunds.summary()",
            "sinplot()\nsns.despine()\n",
            "sns.set_theme()\nsinplot()\n"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Markov switching dynamic regression->Federal funds rate with switching intercept"
            ],
            [
                "seaborn->User guide and tutorial->Controlling figure aesthetics->Removing axes spines",
                "seaborn->Figure aesthetics->Controlling figure aesthetics->Removing axes spines"
            ],
            [
                "seaborn->User guide and tutorial->Controlling figure aesthetics",
                "seaborn->Figure aesthetics->Controlling figure aesthetics"
            ]
        ]
    },
    "570824": {
        "jupyter_code_cell": "feature_df = pd.merge(feature_df.reset_index(), patient_df['age'].reset_index(), \n                      on=\"icustay_id\").set_index(['icustay_id', 'time'])\nfeature_df.describe()",
        "matched_tutorial_code_inds": [
            3616,
            3861,
            5381,
            5399,
            4140
        ],
        "matched_tutorial_codes": [
            "hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()",
            "df = dd.read_parquet(\"data/indiv-*.parquet\", engine='pyarrow', columns=['occupation'])\n\nmost_common = df.occupation.value_counts().nlargest(100)\nmost_common.compute().sort_values(ascending=False)",
            "spector_data = sm.datasets.spector.load()\nspector_data.exog = sm.add_constant(spector_data.exog, prepend=False)",
            "rand_data = sm.datasets.randhie.load()\nrand_exog = rand_data.exog\nrand_exog = sm.add_constant(rand_exog, prepend=False)",
            "g = sns.PairGrid(iris, hue=\"species\")\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\ng.add_legend()\n"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing"
            ],
            [
                "pandas_toms_blog->Scaling->Using Dask"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Data"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Poisson"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships"
            ]
        ]
    },
    "273867": {
        "jupyter_code_cell": "from sklearn.model_selection import train_test_split\ncorpus = np.array(df.clean_posts)\nlabels = np.array(df.type)\ntrain_corpus, test_corpus, train_labels, test_labels  = train_test_split(corpus, labels, test_size=0.3, random_state=42)\ntrain_labels_IE = [entry[0] for entry in train_labels]\ntrain_labels_NS = [entry[1] for entry in train_labels]\ntrain_labels_TF = [entry[2] for entry in train_labels]\ntrain_labels_JP = [entry[3] for entry in train_labels]",
        "matched_tutorial_code_inds": [
            2787,
            3533,
            2917,
            2504,
            1778
        ],
        "matched_tutorial_codes": [
            "from sklearn.linear_model import \n\n\nmask_train = df_train[\"ClaimAmount\"]  0\nmask_test = df_test[\"ClaimAmount\"]  0\n\nglm_sev = (alpha=10.0, solver=\"newton-cholesky\")\n\nglm_sev.fit(\n    X_train[mask_train.values],\n    df_train.loc[mask_train, \"AvgClaimAmount\"],\n    sample_weight=df_train.loc[mask_train, \"ClaimNb\"],\n)\n\nscores = score_estimator(\n    glm_sev,\n    X_train[mask_train.values],\n    X_test[mask_test.values],\n    df_train[mask_train],\n    df_test[mask_test],\n    target=\"AvgClaimAmount\",\n    weights=\"ClaimNb\",\n)\nprint(\"Evaluation of GammaRegressor on target AvgClaimAmount\")\nprint(scores)",
            "from sklearn.datasets import \n\ncategories = [\n    \"alt.atheism\",\n    \"comp.graphics\",\n    \"comp.sys.ibm.pc.hardware\",\n    \"misc.forsale\",\n    \"rec.autos\",\n    \"sci.space\",\n    \"talk.religion.misc\",\n]\n\nprint(\"Loading 20 newsgroups training data\")\nraw_data, _ = (subset=\"train\", categories=categories, return_X_y=True)\ndata_size_mb = sum(len(s.encode(\"utf-8\")) for s in raw_data) / 1e6\nprint(f\"{len(raw_data)} documents - {data_size_mb:.3f}MB\")",
            "from sklearn.datasets import \nfrom sklearn.model_selection import \n\nX, y = (\n    \"titanic\", version=1, as_frame=True, return_X_y=True, parser=\"pandas\"\n)\nrng = (seed=42)\nX[\"random_cat\"] = rng.randint(3, size=X.shape[0])\nX[\"random_num\"] = rng.randn(X.shape[0])\n\ncategorical_columns = [\"pclass\", \"sex\", \"embarked\", \"random_cat\"]\nnumerical_columns = [\"age\", \"sibsp\", \"parch\", \"fare\", \"random_num\"]\n\nX = X[categorical_columns + numerical_columns]\nX_train, X_test, y_train, y_test = (X, y, stratify=y, random_state=42)",
            "from sklearn.datasets import \n\nX, y = (\n    n_samples=500,\n    n_features=15,\n    n_informative=3,\n    n_redundant=2,\n    n_repeated=0,\n    n_classes=8,\n    n_clusters_per_class=1,\n    class_sep=0.8,\n    random_state=0,\n)",
            "from sklearn.inspection import PartialDependenceDisplay\n\nfig, ax = (figsize=(14, 4), constrained_layout=True)\n_ = (\n    model,\n    X,\n    features=[\"age\", \"sex\", (\"pclass\", \"sex\")],\n    categorical_features=categorical_features,\n    ax=ax,\n)\n\n\n<img alt=\"plot release highlights 1 2 0\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_release_highlights_1_2_0_003.png\" srcset=\"../../_images/sphx_glr_plot_release_highlights_1_2_0_003.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Generalized Linear Models->Tweedie regression on insurance claims->Severity Model -  Gamma distribution"
            ],
            [
                "sklearn->Examples->Working with text documents->FeatureHasher and DictVectorizer Comparison->Load Data"
            ],
            [
                "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Data Loading and Feature Engineering"
            ],
            [
                "sklearn->Examples->Feature Selection->Recursive feature elimination with cross-validation->Data generation"
            ],
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 1.2->New and enhanced displays"
            ]
        ]
    },
    "1438729": {
        "jupyter_code_cell": "data.get_hospitals_by_product('product_1807')\ndata.plot_product('product_1842')",
        "matched_tutorial_code_inds": [
            161,
            3793,
            3784,
            4025,
            3994
        ],
        "matched_tutorial_codes": [
            "compare.trim_significant_figures()\ncompare.colorize()\ncompare.print()",
            "sns.barplot(x='cut', y='price', data=df)\nsns.despine()\nplt.tight_layout()",
            "import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "tips = sns.load_dataset(\"tips\")\nsns.displot(tips, x=\"size\")\n",
            "tips = sns.load_dataset(\"tips\")\nsns.relplot(data=tips, x=\"total_bill\", y=\"tip\")\n"
        ],
        "matched_tutorial_paths": [
            [
                "torch->PyTorch Recipes->PyTorch Benchmark->Steps->5. Comparing benchmark results"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size",
                "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Relating variables with scatter plots",
                "seaborn->Plotting functions->Visualizing statistical relationships->Relating variables with scatter plots"
            ]
        ]
    },
    "1026224": {
        "jupyter_code_cell": "ncs.keys()\ngeo = ncs['15-OCT']['Data']['geolocation']\ngeo.variables.keys()\nlat =geo.variables.get('latitude')\nlon =geo.variables.get('longitude')\nlat.shape\nlats = np.array(lat)\nlons = np.array(lon)",
        "matched_tutorial_code_inds": [
            2824,
            3784,
            2090,
            4013,
            660
        ],
        "matched_tutorial_codes": [
            "feature_names = model[:-1].get_feature_names_out()\n\ncoefs = (\n    model[-1].regressor_.coef_,\n    columns=[\"Coefficients\"],\n    index=feature_names,\n)\n\ncoefs\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "iris = ()\nX = iris.data\ny = iris.target\nX_train, X_test, y_train, y_test = (X, y, random_state=0)\n\nclf = (max_leaf_nodes=3, random_state=0)\nclf.fit(X_train, y_train)",
            "palette = sns.cubehelix_palette(light=.8, n_colors=6)\nsns.relplot(\n    data=dots, kind=\"line\",\n    x=\"time\", y=\"firing_rate\",\n    hue=\"coherence\", style=\"choice\", palette=palette,\n)\n",
            "interp = (rn18)\n(input)\nprint(interp.summary(True))"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples"
            ],
            [
                "sklearn->Examples->Decision Trees->Understanding the decision tree structure->Train tree classifier"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Emphasizing continuity with line plots->Plotting subsets of data with semantic mappings",
                "seaborn->Plotting functions->Visualizing statistical relationships->Emphasizing continuity with line plots->Plotting subsets of data with semantic mappings"
            ],
            [
                "torch->Code Transforms with FX->(beta) Building a Simple CPU Performance Profiler with FX->Investigating the Performance of ResNet18"
            ]
        ]
    },
    "96976": {
        "jupyter_code_cell": "plt.figure(figsize=(6, 5))\nfor i, t in enumerate(num):\n    w = X[i]\n    plt.text(w[0] + .5, w[1] + .5, str(t), color=plt.cm.Dark2(t / 4.),\n             fontdict={'weight': 'bold', 'size': 30})\nplt.axis([0, np.max(X[:, 0]), 0, np.max(X[:, 1])])\nplt.axis('off')\nimport pandas as pd\ncountry_data = pd.read_csv('datasets/WDI_Data.csv', encoding = \"ISO-8859-1\")",
        "matched_tutorial_code_inds": [
            5890,
            550,
            401,
            4691,
            4795
        ],
        "matched_tutorial_codes": [
            "plt.figure(figsize=(6, 6))\nsymbols = [\"D\", \"^\"]\ncolors = [\"r\", \"g\", \"blue\"]\nfactor_groups = salary_table.groupby([\"E\", \"M\"])\nfor values, group in factor_groups:\n    i, j = values\n    plt.scatter(group[\"X\"], group[\"S\"], marker=symbols[j], color=colors[i - 1], s=144)\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "plt.figure(figsize=(10, 10))\nplt.subplot(2, 2, 1)\nplt.plot(logs[\"reward\"])\nplt.title(\"training rewards (average)\")\nplt.subplot(2, 2, 2)\nplt.plot(logs[\"step_count\"])\nplt.title(\"Max step count (training)\")\nplt.subplot(2, 2, 3)\nplt.plot(logs[\"eval reward (sum)\"])\nplt.title(\"Return (test)\")\nplt.subplot(2, 2, 4)\nplt.plot(logs[\"eval step_count\"])\nplt.title(\"Max step count (test)\")\nplt.show()\n\n\n<img alt=\"training rewards (average), Max step count (training), Return (test), Max step count (test)\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_reinforcement_ppo_001.png\" srcset=\"../_images/sphx_glr_reinforcement_ppo_001.png\"/>",
            "plt.figure(figsize=(5,5))\nplt.plot(epsilons, accuracies, \"*-\")\nplt.yticks(np.arange(0, 1.1, step=0.1))\nplt.xticks(np.arange(0, .35, step=0.05))\nplt.title(\"Accuracy vs Epsilon\")\nplt.xlabel(\"Epsilon\")\nplt.ylabel(\"Accuracy\")\nplt.show()\n\n\n<img alt=\"Accuracy vs Epsilon\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_fgsm_tutorial_001.png\" srcset=\"../_images/sphx_glr_fgsm_tutorial_001.png\"/>",
            "plt.rcParams.update({'figure.autolayout': True})\n\nfig, ax = plt.subplots()\nax.barh(group_names, group_data)\nlabels = ax.get_xticklabels()\nplt.setp(labels, rotation=45, horizontalalignment='right')\n\n\n<img alt=\"lifecycle\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_lifecycle_006.png\" srcset=\"../../_images/sphx_glr_lifecycle_006.png, ../../_images/sphx_glr_lifecycle_006_2_0x.png 2.0x\"/>",
            "plt.rcParams['figure.constrained_layout.use'] = True\nfig, axs = plt.subplots(2, 2, figsize=(3, 3))\nfor ax in axs.flat:\n    example_plot(ax)\n\n\n<img alt=\"Title, Title, Title, Title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_constrainedlayout_guide_018.png\" srcset=\"../../_images/sphx_glr_constrainedlayout_guide_018.png, ../../_images/sphx_glr_constrainedlayout_guide_018_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "torch->Reinforcement Learning->Reinforcement Learning (PPO) with TorchRL Tutorial->Results"
            ],
            [
                "torch->Image and Video->Adversarial Example Generation->Results->Accuracy vs Epsilon"
            ],
            [
                "matplotlib->Tutorials->Introductory->The Lifecycle of a Plot->Customizing the plot"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Constrained Layout Guide->rcParams"
            ]
        ]
    },
    "1111978": {
        "jupyter_code_cell": "rglob_dict = {\n    'wt': rglob_wt, \n    'ras': rglob_ras, \n    'braf':rglob_braf, \n    'egfr': rglob_egfr\n}\npanel_all = cnr.PerturbationPanel(\n    REF_NODES, PERTURBATIONS, INHIBITOR_TARGETS, [], rglob_dict\n)\npanel_ras_wt = cnr.PerturbationPanel(\n    REF_NODES, PERTURBATIONS, INHIBITOR_TARGETS, [], \n    {key: val for key, val in rglob_dict.items() if key in {'wt', 'ras'}} \n)\npanel_braf_wt = cnr.PerturbationPanel(\n    REF_NODES, PERTURBATIONS, INHIBITOR_TARGETS, [],\n    {key: val for key, val in rglob_dict.items() if key in {'wt', 'braf'}} \n)\npanel_egfr_wt = cnr.PerturbationPanel(\n    REF_NODES, PERTURBATIONS, INHIBITOR_TARGETS, [],\n    {key: val for key, val in rglob_dict.items() if key in {'wt', 'egfr'}} \n)\nETA = 0.005",
        "matched_tutorial_code_inds": [
            2124,
            5902,
            5915,
            220,
            3079
        ],
        "matched_tutorial_codes": [
            "n_comps = 2\n\nmethods = [\n    (\"PCA\", ()),\n    (\"Unrotated FA\", ()),\n    (\"Varimax FA\", (rotation=\"varimax\")),\n]\nfig, axes = (ncols=len(methods), figsize=(10, 8), sharey=True)\n\nfor ax, (method, fa) in zip(axes, methods):\n    fa.set_params(n_components=n_comps)\n    fa.fit(X)\n\n    components = fa.components_.T\n    print(\"\\n\\n %s :\\n\" % method)\n    print(components)\n\n    vmax = np.abs(components).max()\n    ax.imshow(components, cmap=\"RdBu_r\", vmax=vmax, vmin=-vmax)\n    ax.set_yticks((len(feature_names)))\n    ax.set_yticklabels(feature_names)\n    ax.set_title(str(method))\n    ax.set_xticks([0, 1])\n    ax.set_xticklabels([\"Comp. 1\", \"Comp. 2\"])\nfig.suptitle(\"Factors\")\n()\n()\n\n\n<img alt=\"Factors, PCA, Unrotated FA, Varimax FA\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_varimax_fa_002.png\" srcset=\"../../_images/sphx_glr_plot_varimax_fa_002.png\"/>",
            "resid = lm.resid\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    group_num = i * 2 + j - 1  # for plotting purposes\n    x = [group_num] * len(group)\n    plt.scatter(\n        x,\n        resid[group.index],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\nplt.xlabel(\"Group\")\nplt.ylabel(\"Residuals\")",
            "resid = interM_lm32.get_influence().summary_frame()[\"standard_resid\"]\n\nplt.figure(figsize=(6, 6))\nresid = resid.reindex(X.index)\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X.loc[idx],\n        resid.loc[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\nplt.xlabel(\"X[~[32]]\")\nplt.ylabel(\"standardized resids\")",
            "labels_map = {\n    0: \"T-Shirt\",\n    1: \"Trouser\",\n    2: \"Pullover\",\n    3: \"Dress\",\n    4: \"Coat\",\n    5: \"Sandal\",\n    6: \"Shirt\",\n    7: \"Sneaker\",\n    8: \"Bag\",\n    9: \"Ankle Boot\",\n}\nfigure = plt.figure(figsize=(8, 8))\ncols, rows = 3, 3\nfor i in range(1, cols * rows + 1):\n    sample_idx = (len(), size=(1,)).item()\n    ,  = [sample_idx]\n    figure.add_subplot(rows, cols, i)\n    plt.title(labels_map[])\n    plt.axis(\"off\")\n    plt.imshow(.squeeze(), cmap=\"gray\")\nplt.show()\n\n\n<img alt=\"Pullover, Sandal, Bag, Bag, Sneaker, Bag, Pullover, Sandal, Sandal\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_data_tutorial_001.png\" srcset=\"../../_images/sphx_glr_data_tutorial_001.png\"/>",
            "rfc = (n_estimators=10, random_state=42)\nrfc.fit(X_train, y_train)\nax = ()\nrfc_disp = (rfc, X_test, y_test, ax=ax, alpha=0.8)\nsvc_disp.plot(ax=ax, alpha=0.8)\n()\n\n\n<img alt=\"plot roc curve visualization api\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_roc_curve_visualization_api_002.png\" srcset=\"../../_images/sphx_glr_plot_roc_curve_visualization_api_002.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Decomposition->Factor Analysis (with rotation) to visualize patterns"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "torch->Introduction to PyTorch->Datasets & DataLoaders->Iterating and Visualizing the Dataset"
            ],
            [
                "sklearn->Examples->Miscellaneous->ROC Curve with Visualization API->Training a Random Forest and Plotting the ROC Curve"
            ]
        ]
    },
    "523898": {
        "jupyter_code_cell": "dist_TS.distributionType\nout = dist_TS.fit_normal()",
        "matched_tutorial_code_inds": [
            6878,
            4510,
            4509,
            6850,
            6793
        ],
        "matched_tutorial_codes": [
            "mod = NBin(y, X)\nres = mod.fit()",
            "stats.ks_2samp(rvs1, rvs3)\nKstestResult(statistic=0.114, pvalue=0.00299005061044668)  # random",
            "stats.ks_2samp(rvs1, rvs2)\nKstestResult(statistic=0.026, pvalue=0.9959527565364388)  # random",
            "mod = sm.tsa.SARIMAX(endog4)\nres = mod.fit()",
            "ypred = olsres.predict(X)\nprint(ypred)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 2: Negative Binomial Regression for Count Data->Usage Example"
            ],
            [
                "scipy->Statistics (scipy.stats)->Comparing two samples->Kolmogorov-Smirnov test for two samples ks_2samp"
            ],
            [
                "scipy->Statistics (scipy.stats)->Comparing two samples->Kolmogorov-Smirnov test for two samples ks_2samp"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes"
            ],
            [
                "statsmodels->Examples->User Notes->Prediction->In-sample prediction"
            ]
        ]
    },
    "1423447": {
        "jupyter_code_cell": "import pandas as pd\nimport numpy as np\nfrom pprint import pprint\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\ndf = pd.read_csv('/home/nbangs/Notebooks/data/mcnulty/sahie_2008.csv')",
        "matched_tutorial_code_inds": [
            5228,
            3781,
            6687,
            1738,
            6270
        ],
        "matched_tutorial_codes": [
            "%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom pandas_datareader.data import DataReader\n\nnp.set_printoptions(suppress=True)",
            "import os\nimport feather\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nif int(os.environ.get(\"MODERN_PANDAS_EPUB\", 0)):\n    import prep # noqa",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pandas_datareader as pdr\nimport seaborn as sns\n\nplt.rc(\"figure\", figsize=(16, 8))\nplt.rc(\"font\", size=15)\nplt.rc(\"lines\", linewidth=3)\nsns.set_style(\"darkgrid\")",
            "# Importing the necessary packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pooch\nimport string\nimport re\nimport zipfile\nimport os\n\n# Creating the random instance\nrng = np.random.default_rng()",
            "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom pandas.plotting import register_matplotlib_converters\n\nregister_matplotlib_converters()\nsns.set_style(\"darkgrid\")"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Recursive Least Squares"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples"
            ],
            [
                "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Imports"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition"
            ]
        ]
    },
    "1496843": {
        "jupyter_code_cell": "care_categories = ['hospital_care', 'pharmaceuticals', 'mental_care', 'GP_capitation', 'GP_fee_for_service', 'GP_other', 'dental_care', 'physiotherapy', 'maternity_care', 'obstetrics', 'geriatrics']\nfor variable in care_categories:\n    df_25_70[variable] = df_25_70[variable]/df_25_70['number_citizens']\ny = df_25_70.target\nsubset_care_categories = ['physiotherapy', 'obstetrics', 'geriatrics', 'pharmaceuticals']\nX = df_25_70[subset_care_categories]",
        "matched_tutorial_code_inds": [
            2873,
            2088,
            2875,
            5355,
            2298
        ],
        "matched_tutorial_codes": [
            "features_names = [\"experience\", \"parent hourly wage\", \"college degree\"]\n\nregressor_without_ability = ()\nregressor_without_ability.fit(X_train[features_names], y_train)\ny_pred_without_ability = regressor_without_ability.predict(X_test[features_names])\nR2_without_ability = (y_test, y_pred_without_ability)\n\nprint(f\"R2 score without ability: {R2_without_ability:.3f}\")",
            "train_scores = [clf.score(X_train, y_train) for clf in clfs]\ntest_scores = [clf.score(X_test, y_test) for clf in clfs]\n\nfig, ax = ()\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker=\"o\", label=\"test\", drawstyle=\"steps-post\")\nax.legend()\n()\n\n\n<img alt=\"Accuracy vs alpha for training and testing sets\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cost_complexity_pruning_003.png\" srcset=\"../../_images/sphx_glr_plot_cost_complexity_pruning_003.png\"/>",
            "model_coef = (regressor_without_ability.coef_, index=features_names)\ncoef = (\n    [true_coef[features_names], model_coef],\n    keys=[\"Coefficients of true generative model\", \"Model coefficients\"],\n    axis=1,\n)\nax = coef.plot.barh()\nax.set_xlabel(\"Coefficient values\")\n_ = ax.set_title(\"Coefficients of the linear regression excluding the ability feature\")\n()\n()\n\n\n<img alt=\"Coefficients of the linear regression excluding the ability feature\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_causal_interpretation_003.png\" srcset=\"../../_images/sphx_glr_plot_causal_interpretation_003.png\"/>",
            "weights = rob_crime_model.weights\nidx = weights  0\nX = rob_crime_model.model.exog[idx.values]\nww = weights[idx] / weights[idx].mean()\nhat_matrix_diag = ww * (X * np.linalg.pinv(X).T).sum(1)\nresid = rob_crime_model.resid\nresid2 = resid ** 2\nresid2 /= resid2.sum()\nnobs = int(idx.sum())\nhm = hat_matrix_diag.mean()\nrm = resid2.mean()",
            "feature_importance = reg.feature_importances_\nsorted_idx = (feature_importance)\npos = (sorted_idx.shape[0]) + 0.5\nfig = (figsize=(12, 6))\n(1, 2, 1)\n(pos, feature_importance[sorted_idx], align=\"center\")\n(pos, (diabetes.feature_names)[sorted_idx])\n(\"Feature Importance (MDI)\")\n\nresult = (\n    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_idx = result.importances_mean.argsort()\n(1, 2, 2)\n(\n    result.importances[sorted_idx].T,\n    vert=False,\n    labels=(diabetes.feature_names)[sorted_idx],\n)\n(\"Permutation Importance (test set)\")\nfig.tight_layout()\n()\n\n\n<img alt=\"Feature Importance (MDI), Permutation Importance (test set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Inspection->Failure of Machine Learning to infer causal effects->Income prediction with partial observations"
            ],
            [
                "sklearn->Examples->Decision Trees->Post pruning decision trees with cost complexity pruning->Accuracy vs alpha for training and testing sets"
            ],
            [
                "sklearn->Examples->Inspection->Failure of Machine Learning to infer causal effects->Income prediction with partial observations"
            ],
            [
                "statsmodels->Examples->Plotting->Regression Plots->Statewide Crime 2009 Dataset->Using robust regression to correct for outliers."
            ],
            [
                "sklearn->Examples->Ensemble methods->Gradient Boosting regression->Plot feature importance"
            ]
        ]
    },
    "1047259": {
        "jupyter_code_cell": "def smartdownsample(img,n):\n    img_2D = img.reshape(28,28)\n    no_samples = len(img_2D)//n\n    img_binned = np.zeros([no_samples,no_samples])\n    for i in range(no_samples):\n        for j in range(no_samples):\n            img_binned[i,j] = img_2D[i*n:(i+1)*n, j*n:(j+1)*n].sum()\n    return img_binned            ",
        "matched_tutorial_code_inds": [
            380,
            1711,
            2376,
            5724,
            1656
        ],
        "matched_tutorial_codes": [
            "def imshow(inp, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\n\n# Get a batch of training data\n,  = next(iter(dataloaders['train']))\n\n# Make a grid from batch\n = ()\n\nimshow(, title=[class_names[x] for x in ])\n\n\n<img alt=\"['bees', 'bees', 'ants', 'bees']\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_transfer_learning_tutorial_001.png\" srcset=\"../_images/sphx_glr_transfer_learning_tutorial_001.png\"/>",
            "def frame_preprocessing(observation_frame):\n    # Crop the frame.\n    observation_frame = observation_frame[35:195]\n    # Downsample the frame by a factor of 2.\n    observation_frame = observation_frame[::2, ::2, 0]\n    # Remove the background and apply other enhancements.\n    observation_frame[observation_frame == 144] = 0  # Erase the background (type 1).\n    observation_frame[observation_frame == 109] = 0  # Erase the background (type 2).\n    observation_frame[observation_frame != 0] = 1  # Set the items (rackets, ball) to 1.\n    # Return the preprocessed frame as a 1D floating-point array.\n    return observation_frame.astype(float)",
            "def plot_gallery(images, titles, h, w, n_row=3, n_col=4):\n    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n    (figsize=(1.8 * n_col, 2.4 * n_row))\n    (bottom=0, left=0.01, right=0.99, top=0.90, hspace=0.35)\n    for i in range(n_row * n_col):\n        (n_row, n_col, i + 1)\n        (images[i].reshape((h, w)), cmap=plt.cm.gray)\n        (titles[i], size=12)\n        (())\n        (())",
            "def plot_weights(support, weights_func, xlabels, xticks):\n    fig = plt.figure(figsize=(12, 8))\n    ax = fig.add_subplot(111)\n    ax.plot(support, weights_func(support))\n    ax.set_xticks(xticks)\n    ax.set_xticklabels(xlabels, fontsize=16)\n    ax.set_ylim(-0.1, 1.1)\n    return ax",
            "def julia(mesh, c=-1, num_iter=10, radius=2):\n\n    z = mesh.copy()\n    diverge_len = np.zeros(z.shape)\n\n    for i in range(num_iter):\n        conv_mask = np.abs(z) &lt; radius\n        z[conv_mask] = np.square(z[conv_mask]) + c\n        diverge_len[conv_mask] += 1\n\n    return diverge_len"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->Load Data->Visualize a few images"
            ],
            [
                "numpy->Articles->Deep reinforcement learning with Pong from pixels->Preprocess frames (the observation)"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Faces recognition example using eigenfaces and SVMs"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression"
            ],
            [
                "numpy->NumPy Applications->Plotting Fractals->Julia set"
            ]
        ]
    },
    "560286": {
        "jupyter_code_cell": "plt.figure(figsize=(16,8))\nplt.plot(corp_it.index, corp_it['amount'])\nplt.ylabel('USD ($)')\nplt.xlabel('Date')\nplt.title('Corporate IT Spending')\nplt.show()\nplt.clf()\nheights = [] \nfor i in categories:\n    temp_df = corp_it.loc[corp_it['category'] == i]\n    heights.append((i,sum(temp_df['amount'])))\ntemp_df = None",
        "matched_tutorial_code_inds": [
            5890,
            550,
            401,
            3772,
            4346
        ],
        "matched_tutorial_codes": [
            "plt.figure(figsize=(6, 6))\nsymbols = [\"D\", \"^\"]\ncolors = [\"r\", \"g\", \"blue\"]\nfactor_groups = salary_table.groupby([\"E\", \"M\"])\nfor values, group in factor_groups:\n    i, j = values\n    plt.scatter(group[\"X\"], group[\"S\"], marker=symbols[j], color=colors[i - 1], s=144)\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "plt.figure(figsize=(10, 10))\nplt.subplot(2, 2, 1)\nplt.plot(logs[\"reward\"])\nplt.title(\"training rewards (average)\")\nplt.subplot(2, 2, 2)\nplt.plot(logs[\"step_count\"])\nplt.title(\"Max step count (training)\")\nplt.subplot(2, 2, 3)\nplt.plot(logs[\"eval reward (sum)\"])\nplt.title(\"Return (test)\")\nplt.subplot(2, 2, 4)\nplt.plot(logs[\"eval step_count\"])\nplt.title(\"Max step count (test)\")\nplt.show()\n\n\n<img alt=\"training rewards (average), Max step count (training), Return (test), Max step count (test)\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_reinforcement_ppo_001.png\" srcset=\"../_images/sphx_glr_reinforcement_ppo_001.png\"/>",
            "plt.figure(figsize=(5,5))\nplt.plot(epsilons, accuracies, \"*-\")\nplt.yticks(np.arange(0, 1.1, step=0.1))\nplt.xticks(np.arange(0, .35, step=0.05))\nplt.title(\"Accuracy vs Epsilon\")\nplt.xlabel(\"Epsilon\")\nplt.ylabel(\"Accuracy\")\nplt.show()\n\n\n<img alt=\"Accuracy vs Epsilon\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_fgsm_tutorial_001.png\" srcset=\"../_images/sphx_glr_fgsm_tutorial_001.png\"/>",
            "plt.figure(figsize=(8, 5))\n(wins.win_pct\n    .unstack()\n    .assign(**{'Home Win % - Away %': lambda x: x.home_team - x.away_team,\n               'Overall %': lambda x: (x.home_team + x.away_team) / 2})\n     .pipe((sns.regplot, 'data'), x='Overall %', y='Home Win % - Away %')\n)\nsns.despine()\nplt.tight_layout()",
            "plt.figure()\n plt.imshow(deriv)\n plt.gray()\n plt.title('Output of spline edge filter')\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code displays two plots. The first plot is a normal grayscale photo of a raccoon climbing on a palm plant. The second plot has the 2-D spline filter applied to the photo and is completely grey except the edges of the photo have been emphasized, especially on the raccoon fur and palm fronds.\"' class=\"plot-directive\" src=\"../_images/signal-1_01_00.png\"/>\n</figure>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "torch->Reinforcement Learning->Reinforcement Learning (PPO) with TorchRL Tutorial->Results"
            ],
            [
                "torch->Image and Video->Adversarial Example Generation->Results->Accuracy vs Epsilon"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team"
            ],
            [
                "scipy->Signal Processing (scipy.signal)->B-splines"
            ]
        ]
    },
    "1385211": {
        "jupyter_code_cell": "print('Model Accuracy: ', rf.score(X_test_2, y_test))\nrf_pred = rf.predict(X_test_2)\nprint('Model AUC Score: ', roc_auc_score(y_test, rf_pred))\nrf = RandomForestClassifier(class_weight='balanced', random_state= 42)\nrf.fit(X_train_1, y_train)",
        "matched_tutorial_code_inds": [
            4503,
            4506,
            1960,
            1039,
            4305
        ],
        "matched_tutorial_codes": [
            "print('normal skewtest teststat = %6.3f pvalue = %6.4f' % stats.skewtest(x))\nnormal skewtest teststat =  2.785 pvalue = 0.0054  # random\n print('normal kurtosistest teststat = %6.3f pvalue = %6.4f' % stats.kurtosistest(x))\nnormal kurtosistest teststat =  4.757 pvalue = 0.0000  # random",
            "print('normaltest teststat = %6.3f pvalue = %6.4f' %\n...       stats.normaltest(stats.t.rvs(10, size=100)))\nnormaltest teststat =  4.698 pvalue = 0.0955  # random\n print('normaltest teststat = %6.3f pvalue = %6.4f' %\n...              stats.normaltest(stats.norm.rvs(size=1000)))\nnormaltest teststat =  0.613 pvalue = 0.7361  # random",
            "print(f\"Homogeneity: {(labels_true, labels):.3f}\")\nprint(f\"Completeness: {(labels_true, labels):.3f}\")\nprint(f\"V-measure: {(labels_true, labels):.3f}\")\nprint(f\"Adjusted Rand Index: {(labels_true, labels):.3f}\")\nprint(\n    \"Adjusted Mutual Information:\"\n    f\" {(labels_true, labels):.3f}\"\n)\nprint(f\"Silhouette Coefficient: {(X, labels):.3f}\")",
            "print(\n    \"Sparsity in conv1.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Sparsity in conv2.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Sparsity in fc1.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Sparsity in fc2.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Sparsity in fc3.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Global sparsity: {:.2f}%\".format(\n        100. * float(\n            ( == 0)\n            + ( == 0)\n            + ( == 0)\n            + ( == 0)\n            + ( == 0)\n        )\n        / float(\n            .nelement()\n            + .nelement()\n            + .nelement()\n            + .nelement()\n            + .nelement()\n        )\n    )\n)",
            "print(b_ub - (A_ub @ x).flatten())  # this is equivalent to result.slack\n[ 6.52747190e-10, -2.26730279e-09]  # may vary\n print(b_eq - (A_eq @ x).flatten())  # this is equivalent to result.con\n[ 9.78840831e-09, 1.04662945e-08]]  # may vary\n print([0 &lt;= result.x[0], 0 &lt;= result.x[1] &lt;= 6.0, result.x[2] &lt;= 0.5, -3.0 &lt;= result.x[3]])\n[True, True, True, True]"
        ],
        "matched_tutorial_paths": [
            [
                "scipy->Statistics (scipy.stats)->Analysing one sample->Special tests for normal distributions"
            ],
            [
                "scipy->Statistics (scipy.stats)->Analysing one sample->Special tests for normal distributions"
            ],
            [
                "sklearn->Examples->Clustering->Demo of DBSCAN clustering algorithm->Compute DBSCAN"
            ],
            [
                "torch->Model Optimization->Pruning Tutorial->Global pruning"
            ],
            [
                "scipy->Optimization (scipy.optimize)->Linear programming (linprog)->Linear programming example"
            ]
        ]
    },
    "1517204": {
        "jupyter_code_cell": "from extract_bottleneck_features import *\ndef VGG16_predict_breed(img_path):\n    bottleneck_feature = extract_VGG16(path_to_tensor(img_path))\n    predicted_vector = VGG16_model.predict(bottleneck_feature)\n    return dog_names[np.argmax(predicted_vector)]\ntrain_tensors_a = paths_to_tensor(train_files).astype('float32')\nvalid_tensors_a = paths_to_tensor(valid_files).astype('float32')\ntest_tensors_a = paths_to_tensor(test_files).astype('float32')",
        "matched_tutorial_code_inds": [
            1206,
            2018,
            511,
            3375,
            2308
        ],
        "matched_tutorial_codes": [
            "from torch.profiler import , record_function, ProfilerActivity\nactivities = []\nif device == 'cuda':\n    activities.append()\n\nwith (activities=activities, record_shapes=False) as :\n    with record_function(\" Non-Compilied Causal Attention\"):\n        for _ in range(25):\n            model()\nprint(().table(sort_by=\"cuda_time_total\", row_limit=10))\n\n\nwith (activities=activities, record_shapes=False) as :\n    with record_function(\"Compiled Causal Attention\"):\n        for _ in range(25):\n            ()\nprint(().table(sort_by=\"cuda_time_total\", row_limit=10))\n\n# For even more insights, you can export the trace and use ``chrome://tracing`` to view the results\n# prof.export_chrome_trace(\"compiled_causal_attention_trace.json\").",
            "from sklearn.cluster import \nimport matplotlib.pyplot as plt\n\nlabels = (graph, n_clusters=4, eigen_solver=\"arpack\")\nlabel_im = (mask.shape, -1.0)\nlabel_im[mask] = labels\n\nfig, axs = (nrows=1, ncols=2, figsize=(10, 5))\naxs[0].matshow(img)\naxs[1].matshow(label_im)\n\n()\n\n\n<img alt=\"plot segmentation toy\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_segmentation_toy_001.png\" srcset=\"../../_images/sphx_glr_plot_segmentation_toy_001.png\"/>",
            "from timeit import default_timer as timer\nNUM_EPOCHS = 18\n\nfor epoch in range(1, NUM_EPOCHS+1):\n    start_time = timer()\n    train_loss = train_epoch(transformer, optimizer)\n    end_time = timer()\n    val_loss = evaluate(transformer)\n    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n\n\n# function to generate output sequence using greedy algorithm\ndef greedy_decode(model, src, src_mask, max_len, start_symbol):\n    src = src.to(DEVICE)\n    src_mask = src_mask.to(DEVICE)\n\n    memory = model.encode(src, src_mask)\n    ys = (1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n    for i in range(max_len-1):\n        memory = memory.to(DEVICE)\n        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n                    .type(torch.bool)).to(DEVICE)\n        out = model.decode(ys, memory, tgt_mask)\n        out = out.transpose(0, 1)\n        prob = model.generator(out[:, -1])\n        _, next_word = (prob, dim=1)\n        next_word = next_word.item()\n\n        ys = ([ys,\n                        (1, 1).type_as(src.data).fill_(next_word)], dim=0)\n        if next_word == EOS_IDX:\n            break\n    return ys\n\n\n# actual function to translate input sentence into target language\ndef translate(model: , src_sentence: str):\n    model.eval()\n    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n    num_tokens = src.shape[0]\n    src_mask = ((num_tokens, num_tokens)).type(torch.bool)\n    tgt_tokens = greedy_decode(\n        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"&lt;bos&gt;\", \"\").replace(\"&lt;eos&gt;\", \"\")",
            "from sklearn.metrics import , \n\n\ndef compute_score(y_true, y_pred):\n    return {\n        \"R2\": f\"{(y_true, y_pred):.3f}\",\n        \"MedAE\": f\"{(y_true, y_pred):.3f}\",\n    }",
            "from sklearn.ensemble import \nfrom sklearn.inspection import PartialDependenceDisplay\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nrng = (0)\n\nn_samples = 1000\nf_0 = rng.rand(n_samples)\nf_1 = rng.rand(n_samples)\nX = [f_0, f_1]\nnoise = rng.normal(loc=0.0, scale=0.01, size=n_samples)\n\n# y is positively correlated with f_0, and negatively correlated with f_1\ny = 5 * f_0 + (10 *  * f_0) - 5 * f_1 - (10 *  * f_1) + noise"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Model Optimization->Using SDPA with torch.compile"
            ],
            [
                "sklearn->Examples->Clustering->Spectral clustering for image segmentation->Plotting four circles"
            ],
            [
                "torch->Text->Language Translation with nn.Transformer and torchtext->Collation"
            ],
            [
                "sklearn->Examples->Pipelines and composite estimators->Effect of transforming the targets in regression model->Synthetic example"
            ],
            [
                "sklearn->Examples->Ensemble methods->Monotonic Constraints"
            ]
        ]
    },
    "122427": {
        "jupyter_code_cell": "import seaborn as sns\nf, ax = plt.subplots(figsize=(5, 8))\ncorr = k.corr()\nsns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            square=True, ax=ax)\nplt.show()\nk.info()",
        "matched_tutorial_code_inds": [
            3003,
            2006,
            3434,
            2963,
            3181
        ],
        "matched_tutorial_codes": [
            "import matplotlib.pyplot as plt\n\nsv_ind = svr.best_estimator_.support_\n(\n    X[sv_ind],\n    y[sv_ind],\n    c=\"r\",\n    s=50,\n    label=\"SVR support vectors\",\n    zorder=2,\n    edgecolors=(0, 0, 0),\n)\n(X[:100], y[:100], c=\"k\", label=\"data\", zorder=1, edgecolors=(0, 0, 0))\n(\n    X_plot,\n    y_svr,\n    c=\"r\",\n    label=\"SVR (fit: %.3fs, predict: %.3fs)\" % (svr_fit, svr_predict),\n)\n(\n    X_plot, y_kr, c=\"g\", label=\"KRR (fit: %.3fs, predict: %.3fs)\" % (kr_fit, kr_predict)\n)\n(\"data\")\n(\"target\")\n(\"SVR versus Kernel Ridge\")\n_ = ()\n\n\n<img alt=\"SVR versus Kernel Ridge\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_kernel_ridge_regression_001.png\" srcset=\"../../_images/sphx_glr_plot_kernel_ridge_regression_001.png\"/>",
            "import matplotlib.pyplot as plt\n\n(figsize=(4.2, 4))\nfor i, patch in enumerate(kmeans.cluster_centers_):\n    (9, 9, i + 1)\n    (patch.reshape(patch_size), cmap=plt.cm.gray, interpolation=\"nearest\")\n    (())\n    (())\n\n\n(\n    \"Patches of faces\\nTrain time %.1fs on %d patches\" % (dt, 8 * len(faces.images)),\n    fontsize=16,\n)\n(0.08, 0.02, 0.92, 0.85, 0.08, 0.23)\n\n()\n\n\n<img alt=\"Patches of faces Train time 1.3s on 3200 patches\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_dict_face_patches_001.png\" srcset=\"../../_images/sphx_glr_plot_dict_face_patches_001.png\"/>",
            "import matplotlib.pyplot as plt\n\nf = (figsize=(7, 5))\nfor index, image_index in enumerate(uncertainty_index):\n    image = images[image_index]\n\n    sub = f.add_subplot(2, 5, index + 1)\n    sub.imshow(image, cmap=plt.cm.gray_r)\n    ([])\n    ([])\n    sub.set_title(\n        \"predict: %i\\ntrue: %i\" % (lp_model.transduction_[image_index], y[image_index])\n    )\n\nf.suptitle(\"Learning with small amount of labeled data\")\n()\n\n\n<img alt=\"Learning with small amount of labeled data, predict: 1 true: 2, predict: 2 true: 2, predict: 8 true: 8, predict: 1 true: 8, predict: 1 true: 8, predict: 1 true: 8, predict: 3 true: 3, predict: 8 true: 8, predict: 2 true: 2, predict: 7 true: 2\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_label_propagation_digits_002.png\" srcset=\"../../_images/sphx_glr_plot_label_propagation_digits_002.png\"/>",
            "import matplotlib.pyplot as plt\n\nfig, ax = (figsize=(7, 7))\nax.scatter(\n    [\n        results[\"LSVM\"][\"time\"],\n    ],\n    [\n        results[\"LSVM\"][\"score\"],\n    ],\n    label=\"Linear SVM\",\n    c=\"green\",\n    marker=\"^\",\n)\n\nax.scatter(\n    [\n        results[\"LSVM + PS(250)\"][\"time\"],\n    ],\n    [\n        results[\"LSVM + PS(250)\"][\"score\"],\n    ],\n    label=\"Linear SVM + PolynomialCountSketch\",\n    c=\"blue\",\n)\n\nfor n_components in N_COMPONENTS:\n    ax.scatter(\n        [\n            results[f\"LSVM + PS({n_components})\"][\"time\"],\n        ],\n        [\n            results[f\"LSVM + PS({n_components})\"][\"score\"],\n        ],\n        c=\"blue\",\n    )\n    ax.annotate(\n        f\"n_comp.={n_components}\",\n        (\n            results[f\"LSVM + PS({n_components})\"][\"time\"],\n            results[f\"LSVM + PS({n_components})\"][\"score\"],\n        ),\n        xytext=(-30, 10),\n        textcoords=\"offset pixels\",\n    )\n\nax.scatter(\n    [\n        results[\"KSVM\"][\"time\"],\n    ],\n    [\n        results[\"KSVM\"][\"score\"],\n    ],\n    label=\"Kernel SVM\",\n    c=\"red\",\n    marker=\"x\",\n)\n\nax.set_xlabel(\"Training time (s)\")\nax.set_ylabel(\"Accuracy (%)\")\nax.legend()\n()\n\n\n<img alt=\"plot scalable poly kernels\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_scalable_poly_kernels_001.png\" srcset=\"../../_images/sphx_glr_plot_scalable_poly_kernels_001.png\"/>",
            "import matplotlib.pyplot as plt\nfrom sklearn.metrics import PredictionErrorDisplay\n\nfig, axs = (ncols=2, figsize=(8, 4))\n(\n    y,\n    y_pred=y_pred,\n    kind=\"actual_vs_predicted\",\n    subsample=100,\n    ax=axs[0],\n    random_state=0,\n)\naxs[0].set_title(\"Actual vs. Predicted values\")\n(\n    y,\n    y_pred=y_pred,\n    kind=\"residual_vs_predicted\",\n    subsample=100,\n    ax=axs[1],\n    random_state=0,\n)\naxs[1].set_title(\"Residuals vs. Predicted Values\")\nfig.suptitle(\"Plotting cross-validated predictions\")\n()\n()\n\n\n<img alt=\"Plotting cross-validated predictions, Actual vs. Predicted values, Residuals vs. Predicted Values\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cv_predict_001.png\" srcset=\"../../_images/sphx_glr_plot_cv_predict_001.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Miscellaneous->Comparison of kernel ridge regression and SVR->Look at the results"
            ],
            [
                "sklearn->Examples->Clustering->Online learning of a dictionary of parts of faces->Plot the results"
            ],
            [
                "sklearn->Examples->Semi Supervised Classification->Label Propagation digits: Demonstrating performance->Plot the most uncertain predictions"
            ],
            [
                "sklearn->Examples->Kernel Approximation->Scalable learning with polynomial kernel approximation->Comparing the results"
            ],
            [
                "sklearn->Examples->Model Selection->Plotting Cross-Validated Predictions",
                "sklearn->Examples->Model Selection->Plotting Learning Curves and Checking Models' Scalability"
            ]
        ]
    },
    "1149662": {
        "jupyter_code_cell": "transit_norm = transit.copy()\ntransit_norm['weekly_riders_per_10k'] = transit_norm.weekly_riders / 10000\ntransit_norm['monthly_income_per_k'] = transit_norm.monthly_income / 1000\ntransit_norm['city_pop_per_100k'] = transit_norm.city_pop / 100000\ntransit_norm = transit_norm.drop(['city', 'weekly_riders', 'monthly_income', 'city_pop'], axis=1)\ntransit_graph = transit_norm.drop(['city_pop_per_100k'], axis=1)\ntransit_graph.weekly_riders_per_10k = transit_graph.weekly_riders_per_10k.round()\nfrom plotly.tools import FigureFactory as FF\niplot(FF.create_scatterplotmatrix(transit_graph,width=1200, size=10, height=1200, diag='box'));",
        "matched_tutorial_code_inds": [
            5917,
            27,
            1615,
            6639,
            2979
        ],
        "matched_tutorial_codes": [
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "transformed_dataset = FaceLandmarksDataset(csv_file='faces/face_landmarks.csv',\n                                           root_dir='faces/',\n                                           transform=transforms.Compose([\n                                               Rescale(256),\n                                               RandomCrop(224),\n                                               ToTensor()\n                                           ]))\n\nfor i in range(len(transformed_dataset)):\n    sample = transformed_dataset[i]\n\n    print(i, sample['image'].size(), sample['landmarks'].size())\n\n    if i == 3:\n        break",
            "fourier_gaussian = ndimage.fourier_gaussian(xray_image, sigma=0.05)\n\nx_prewitt = ndimage.prewitt(fourier_gaussian, axis=0)\ny_prewitt = ndimage.prewitt(fourier_gaussian, axis=1)\n\nxray_image_canny = np.hypot(x_prewitt, y_prewitt)\n\nxray_image_canny *= 255.0 / np.max(xray_image_canny)\n\nprint(\"The data type - \", xray_image_canny.dtype)",
            "model_heuristic = ETSModel(oil, initialization_method=\"heuristic\")\nfit_heuristic = model_heuristic.fit()\noil.plot(label=\"data\")\nfit.fittedvalues.plot(label=\"estimated\")\nfit_heuristic.fittedvalues.plot(label=\"heuristic\", linestyle=\"--\")\nplt.ylabel(\"Annual oil production in Saudi Arabia (Mt)\")\n\n# obtained from R\nparams = [0.99989969, 0.11888177503085334, 0.80000197, 36.46466837, 34.72584983]\nyhat = model.smooth(params).fittedvalues\nyhat.plot(label=\"with R params\", linestyle=\":\")\n\nplt.legend()",
            "sr_lle, sr_err = (\n    sr_points, n_neighbors=12, n_components=2\n)\n\nsr_tsne = (n_components=2, perplexity=40, random_state=0).fit_transform(\n    sr_points\n)\n\nfig, axs = (figsize=(8, 8), nrows=2)\naxs[0].scatter(sr_lle[:, 0], sr_lle[:, 1], c=sr_color)\naxs[0].set_title(\"LLE Embedding of Swiss Roll\")\naxs[1].scatter(sr_tsne[:, 0], sr_tsne[:, 1], c=sr_color)\n_ = axs[1].set_title(\"t-SNE Embedding of Swiss Roll\")\n\n\n<img alt=\"LLE Embedding of Swiss Roll, t-SNE Embedding of Swiss Roll\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_swissroll_002.png\" srcset=\"../../_images/sphx_glr_plot_swissroll_002.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "torch->PyTorch Recipes->Developing Custom PyTorch Dataloaders->Part 2: Data Tranformations->2.3 Iterate through the dataset"
            ],
            [
                "numpy->NumPy Applications->X-ray image processing->Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters->The Canny filter"
            ],
            [
                "statsmodels->Examples->State space models->ETS models->Simple exponential smoothing"
            ],
            [
                "sklearn->Examples->Manifold learning->Swiss Roll And Swiss-Hole Reduction->Swiss Roll"
            ]
        ]
    },
    "1125729": {
        "jupyter_code_cell": "fig, ax = plt.subplots()\nfor key, grp in data.groupby(['participant_name']):\n    ax = grp.plot(ax=ax, kind='line', x='trade_number', y='happiness_rating', label=key)\nplt.legend(loc='best')\nplt.xlabel('Number of trades')\nplt.ylabel('Happiness Rating')\nplt.title('Happiness of individuals with candy selection vs. number of candy trades')\nplt.show()\ndata.boxplot('happiness_rating', by='trade_number', figsize=(12, 8))",
        "matched_tutorial_code_inds": [
            4692,
            4645,
            6453,
            6715,
            4690
        ],
        "matched_tutorial_codes": [
            "fig, ax = plt.subplots()\nax.barh(group_names, group_data)\nlabels = ax.get_xticklabels()\nplt.setp(labels, rotation=45, horizontalalignment='right')\nax.set(xlim=[-10000, 140000], xlabel='Total Revenue', ylabel='Company',\n       title='Company Revenue')\n\n\n<img alt=\"Company Revenue\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_lifecycle_007.png\" srcset=\"../../_images/sphx_glr_lifecycle_007.png, ../../_images/sphx_glr_lifecycle_007_2_0x.png 2.0x\"/>",
            "fig, ax = plt.subplots(figsize=(5, 2.7), layout='constrained')\ndates = np.arange(np.datetime64('2021-11-15'), np.datetime64('2021-12-25'),\n                  np.timedelta64(1, 'h'))\ndata = np.cumsum(np.random.randn(len(dates)))\nax.plot(dates, data)\ncdf = mpl.dates.ConciseDateFormatter(ax.xaxis.get_major_locator())\nax.xaxis.set_major_formatter(cdf)\n\n\n<img alt=\"quick start\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_quick_start_014.png\" srcset=\"../../_images/sphx_glr_quick_start_014.png, ../../_images/sphx_glr_quick_start_014_2_0x.png 2.0x\"/>",
            "fig, ax = plt.subplots(figsize=(10,4))\n\n# Plot the results\ndf['lff'].plot(ax=ax, style='k.', label='Observations')\npredict.predicted_mean.plot(ax=ax, label='One-step-ahead Prediction')\npredict_ci = predict.conf_int(alpha=0.05)\npredict_index = np.arange(len(predict_ci))\nax.fill_between(predict_index[2:], predict_ci.iloc[2:, 0], predict_ci.iloc[2:, 1], alpha=0.1)\n\nforecast.predicted_mean.plot(ax=ax, style='r', label='Forecast')\nforecast_ci = forecast.conf_int()\nforecast_index = np.arange(len(predict_ci), len(predict_ci) + len(forecast_ci))\nax.fill_between(forecast_index, forecast_ci.iloc[:, 0], forecast_ci.iloc[:, 1], alpha=0.1)\n\n# Cleanup the image\nax.set_ylim((4, 8));\nlegend = ax.legend(loc='lower left');\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_local_linear_trend_11_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_local_linear_trend_11_0.png\"/>",
            "fig, ax = plt.subplots()\npca_model.loadings.plot.scatter(x=\"comp_00\", y=\"comp_01\", ax=ax)\nax.set_xlabel(\"PC 1\", size=17)\nax.set_ylabel(\"PC 2\", size=17)\ndta.index[pca_model.loadings.iloc[:, 1]  0.2].values",
            "fig, ax = plt.subplots()\nax.barh(group_names, group_data)\nlabels = ax.get_xticklabels()\nplt.setp(labels, rotation=45, horizontalalignment='right')\n\n\n<img alt=\"lifecycle\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_lifecycle_005.png\" srcset=\"../../_images/sphx_glr_lifecycle_005.png, ../../_images/sphx_glr_lifecycle_005_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Introductory->The Lifecycle of a Plot->Customizing the plot"
            ],
            [
                "matplotlib->Tutorials->Introductory->Quick start guide->Axis scales and ticks->Plotting dates and strings"
            ],
            [
                "statsmodels->Examples->State space models->State space modeling: Local Linear Trends"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "matplotlib->Tutorials->Introductory->The Lifecycle of a Plot->Customizing the plot"
            ]
        ]
    },
    "103124": {
        "jupyter_code_cell": "print \"Non zeros percent in each feture:\"\ny = ((df[:-1] !=0).sum() / row_num)[:-1]\nx = df.columns[:-1]\nplt.figure(figsize=(10,3))\nplt.title('Percent of non-zeros values for each feature')\nplt.ylabel('Percent')\nwidth = 1/1.5\nplt.bar(x, y, width, color=\"darkblue\")\nplt.show()\nprint (y)\ndf[(df == 0).astype(int).sum(axis=1)==10].groupby(['target']).agg(['count'])",
        "matched_tutorial_code_inds": [
            2372,
            2481,
            2374,
            1521,
            6655
        ],
        "matched_tutorial_codes": [
            "print(\"Fitting the classifier to the training set\")\nt0 = ()\nparam_grid = {\n    \"C\": loguniform(1e3, 1e5),\n    \"gamma\": loguniform(1e-4, 1e-1),\n}\nclf = (\n    (kernel=\"rbf\", class_weight=\"balanced\"), param_grid, n_iter=10\n)\nclf = clf.fit(X_train_pca, y_train)\nprint(\"done in %0.3fs\" % (() - t0))\nprint(\"Best estimator found by grid search:\")\nprint(clf.best_estimator_)",
            "print(\"Computing the principal singular vectors using randomized_svd\")\nt0 = ()\nU, s, V = randomized_svd(X, 5, n_iter=3)\nprint(\"done in %0.3fs\" % (() - t0))\n\n# print the names of the wikipedia related strongest components of the\n# principal singular vector which should be similar to the highest eigenvector\nprint(\"Top wikipedia pages according to principal singular vectors\")\n([names[i] for i in np.abs(U.T[0]).argsort()[-10:]])\n([names[i] for i in np.abs(V[0]).argsort()[-10:]])",
            "print(\"Predicting people's names on the test set\")\nt0 = ()\ny_pred = clf.predict(X_test_pca)\nprint(\"done in %0.3fs\" % (() - t0))\n\nprint((y_test, y_pred, target_names=target_names))\n(\n    clf, X_test_pca, y_test, display_labels=target_names, xticks_rotation=\"vertical\"\n)\n()\n()\n\n\n<img alt=\"plot face recognition\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_face_recognition_001.png\" srcset=\"../../_images/sphx_glr_plot_face_recognition_001.png\"/>",
            "print(\"Rate of semiconductors added on a chip every 2 years:\")\nprint(\n    \"\\tx{:.2f} +/- {:.2f} semiconductors per chip\".format(\n        np.exp((A) * 2), 2 * A * np.exp(2 * A) * 0.006\n    )\n)",
            "for i in range(simulated.shape[1]):\n    simulated.iloc[:, i].plot(label=\"_\", color=\"gray\", alpha=0.1)\ndf[\"mean\"].plot(label=\"mean prediction\")\ndf[\"pi_lower\"].plot(linestyle=\"--\", color=\"tab:blue\", label=\"95% interval\")\ndf[\"pi_upper\"].plot(linestyle=\"--\", color=\"tab:blue\", label=\"_\")\npred.endog.plot(label=\"data\")\nplt.legend()"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Examples based on real world datasets->Faces recognition example using eigenfaces and SVMs"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Wikipedia principal eigenvector->Computing Principal Singular Vector using Randomized SVD"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Faces recognition example using eigenfaces and SVMs"
            ],
            [
                "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy->Calculating the historical growth curve for transistors"
            ],
            [
                "statsmodels->Examples->State space models->ETS models->Predictions"
            ]
        ]
    },
    "63594": {
        "jupyter_code_cell": "hint(\"Cleaning train set...\")\nX = train['comment_text'].apply(lambda c: preprocess(c))\nhint(\"Cleaning test set...\")\nX_ = test['comment_text'].apply(lambda c: preprocess(c))\nhint(\"Done\")\nfrom keras.preprocessing import text as ktxt, sequence\nvocab_max = 100000\nhint(\"Fitting the tokenizer...\")\ntokenizer = ktxt.Tokenizer(num_words=vocab_max)\ntokenizer.fit_on_texts(X)\nhint(\"Tokenizing...\")\nX = tokenizer.texts_to_sequences(X)\nX_ = tokenizer.texts_to_sequences(X_)\nhint(\"Padding the sequences...\")\nmax_comment_length = 200  \nX = sequence.pad_sequences(X, maxlen=max_comment_length)\nX_ = sequence.pad_sequences(X_, maxlen=max_comment_length)\nhint(\"Done\")",
        "matched_tutorial_code_inds": [
            423,
            2860,
            6253,
            2864,
            2844
        ],
        "matched_tutorial_codes": [
            "with (use_cuda=False) as :\n     = model()\nwith (use_cuda=False) as :\n     = scripted_model()\nwith (use_cuda=False) as :\n     = scripted_quantized_model()\nwith (use_cuda=False) as :\n     = optimized_scripted_quantized_model()\nwith (use_cuda=False) as :\n     = ptl()\n\nprint(\"original model: {:.2f}ms\".format(/1000))\nprint(\"scripted model: {:.2f}ms\".format(/1000))\nprint(\"scripted &amp; quantized model: {:.2f}ms\".format(/1000))\nprint(\"scripted &amp; quantized &amp; optimized model: {:.2f}ms\".format(/1000))\nprint(\"lite model: {:.2f}ms\".format(/1000))",
            "mae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}\n\n_, ax = (figsize=(5, 5))\ndisplay = (\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n)\nax.set_title(\"Ridge model, optimum regularization\")\nfor name, score in scores.items():\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\nax.legend(loc=\"upper left\")\n()\n\n\n<img alt=\"Ridge model, optimum regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_012.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_012.png\"/>",
            "fit1 = SimpleExpSmoothing(oildata, initialization_method=\"heuristic\").fit(\n    smoothing_level=0.2, optimized=False\n)\nfcast1 = fit1.forecast(3).rename(r\"$\\alpha=0.2$\")\nfit2 = SimpleExpSmoothing(oildata, initialization_method=\"heuristic\").fit(\n    smoothing_level=0.6, optimized=False\n)\nfcast2 = fit2.forecast(3).rename(r\"$\\alpha=0.6$\")\nfit3 = SimpleExpSmoothing(oildata, initialization_method=\"estimated\").fit()\nfcast3 = fit3.forecast(3).rename(r\"$\\alpha=%s$\" % fit3.model.params[\"smoothing_level\"])\n\nplt.figure(figsize=(12, 8))\nplt.plot(oildata, marker=\"o\", color=\"black\")\nplt.plot(fit1.fittedvalues, marker=\"o\", color=\"blue\")\n(line1,) = plt.plot(fcast1, marker=\"o\", color=\"blue\")\nplt.plot(fit2.fittedvalues, marker=\"o\", color=\"red\")\n(line2,) = plt.plot(fcast2, marker=\"o\", color=\"red\")\nplt.plot(fit3.fittedvalues, marker=\"o\", color=\"green\")\n(line3,) = plt.plot(fcast3, marker=\"o\", color=\"green\")\nplt.legend([line1, line2, line3], [fcast1.name, fcast2.name, fcast3.name])",
            "mae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}\n\n_, ax = (figsize=(6, 6))\ndisplay = (\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n)\nax.set_title(\"Lasso model, optimum regularization\")\nfor name, score in scores.items():\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\nax.legend(loc=\"upper left\")\n()\n\n\n<img alt=\"Lasso model, optimum regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_015.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_015.png\"/>",
            "mae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}\n\n_, ax = (figsize=(5, 5))\ndisplay = (\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n)\nax.set_title(\"Ridge model, small regularization\")\nfor name, score in scores.items():\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\nax.legend(loc=\"upper left\")\n()\n\n\n<img alt=\"Ridge model, small regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_009.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_009.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Image and Video->Optimizing Vision Transformer Model for Deployment->Comparing Inference Speed",
                "torch->Model Optimization->Optimizing Vision Transformer Model for Deployment->Comparing Inference Speed"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Simple Exponential Smoothing"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ]
        ]
    },
    "563488": {
        "jupyter_code_cell": "plt.imshow(markers)\nl = 14\ntemp=markers.copy()\ntemp[markers!=l]=-1\nplt.imshow(temp)",
        "matched_tutorial_code_inds": [
            6158,
            6170,
            5233,
            4721,
            6197
        ],
        "matched_tutorial_codes": [
            "fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nax.plot(arma_t.generate_sample(nsample=50))",
            "fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nax = cpi.plot(ax=ax)\nax.legend()",
            "print(res.recursive_coefficients.filtered[0])\nres.plot_recursive_coefficient(range(mod.k_exog), alpha=None, figsize=(10, 6))",
            "fig2 = plt.figure()\nax2 = fig2.add_axes([0.15, 0.1, 0.7, 0.3])",
            "fig = plt.figure(figsize=(12, 10))\nax = fig.add_subplot(111)\nbk_cycles.plot(ax=ax, style=[\"r--\", \"b-\"])"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data->Simulated ARMA(4,1): Model Identification is Difficult"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data->Exercise: How good of in-sample prediction can you do for another series, say, CPI->Hint:"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 1: Copper"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Artist tutorial"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Time Series Filters->Baxter-King approximate band-pass filter: Inflation and Unemployment->Explore the hypothesis that inflation and unemployment are counter-cyclical."
            ]
        ]
    },
    "128770": {
        "jupyter_code_cell": "svc_best = SVC().set_params(**rs_svc.best_params_).fit(X_train_scaled, y_train)\nparam_grid = dict(gamma=gamma_range, C=C_range)\ngs_svc = GridSearchCV(SVC(), param_grid, scoring='accuracy', \n                      cv=5, n_jobs=-1).fit(X, y)\ngs_svc.best_params_",
        "matched_tutorial_code_inds": [
            3078,
            2973,
            2971,
            2119,
            2120
        ],
        "matched_tutorial_codes": [
            "svc_disp = (svc, X_test, y_test)\n()\n\n\n<img alt=\"plot roc curve visualization api\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_roc_curve_visualization_api_001.png\" srcset=\"../../_images/sphx_glr_plot_roc_curve_visualization_api_001.png\"/>",
            "t_sne = (\n    n_components=n_components,\n    perplexity=30,\n    init=\"random\",\n    n_iter=250,\n    random_state=0,\n)\nS_t_sne = t_sne.fit_transform(S_points)\n\nplot_2d(S_t_sne, S_color, \"T-distributed Stochastic  \\n Neighbor Embedding\")\n\n\n<img alt=\"T-distributed Stochastic    Neighbor Embedding\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_compare_methods_006.png\" srcset=\"../../_images/sphx_glr_plot_compare_methods_006.png\"/>",
            "md_scaling = (\n    n_components=n_components,\n    max_iter=50,\n    n_init=4,\n    random_state=0,\n    normalized_stress=False,\n)\nS_scaling = md_scaling.fit_transform(S_points)\n\nplot_2d(S_scaling, S_color, \"Multidimensional scaling\")\n\n\n<img alt=\"Multidimensional scaling\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_compare_methods_004.png\" srcset=\"../../_images/sphx_glr_plot_compare_methods_004.png\"/>",
            "dict_pos_code_estimator = (\n    n_components=n_components,\n    alpha=0.1,\n    max_iter=50,\n    batch_size=3,\n    fit_algorithm=\"cd\",\n    random_state=rng,\n    positive_code=True,\n)\ndict_pos_code_estimator.fit(faces_centered)\nplot_gallery(\n    \"Dictionary learning - positive code\",\n    dict_pos_code_estimator.components_[:n_components],\n    cmap=plt.cm.RdBu,\n)\n\n\n<img alt=\"Dictionary learning - positive code\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_012.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_012.png\"/>",
            "dict_pos_estimator = (\n    n_components=n_components,\n    alpha=0.1,\n    max_iter=50,\n    batch_size=3,\n    fit_algorithm=\"cd\",\n    random_state=rng,\n    positive_dict=True,\n    positive_code=True,\n)\ndict_pos_estimator.fit(faces_centered)\nplot_gallery(\n    \"Dictionary learning - positive dictionary & code\",\n    dict_pos_estimator.components_[:n_components],\n    cmap=plt.cm.RdBu,\n)\n\n\n<img alt=\"Dictionary learning - positive dictionary & code\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_013.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_013.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Miscellaneous->ROC Curve with Visualization API->Plotting the ROC Curve"
            ],
            [
                "sklearn->Examples->Manifold learning->Comparison of Manifold Learning methods->Define algorithms for the manifold learning->T-distributed Stochastic Neighbor Embedding"
            ],
            [
                "sklearn->Examples->Manifold learning->Comparison of Manifold Learning methods->Define algorithms for the manifold learning->Multidimensional scaling"
            ],
            [
                "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition: Dictionary learning->Dictionary learning - positive code"
            ],
            [
                "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition: Dictionary learning->Dictionary learning - positive dictionary & code"
            ]
        ]
    },
    "1358369": {
        "jupyter_code_cell": "with open('advs.txt', 'r') as a:\n    texto = a.read()",
        "matched_tutorial_code_inds": [
            3784,
            1167,
            5030,
            577,
            798
        ],
        "matched_tutorial_codes": [
            "import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "import traceback as tb\ntry:\n    (f1)\nexcept:\n    tb.print_exc()",
            "import mpl_toolkits.axisartist as AA\nfig = plt.figure()\nfig.add_axes([0.1, 0.1, 0.8, 0.8], axes_class=AA.Axes)",
            "with open(\"../_static/img/sample_file.jpeg\", 'rb') as f:\n    image_bytes = f.read()\n    print(get_prediction(image_bytes=image_bytes))",
            "with torch.autograd.profiler.profile() as prof:\n    ens(x)\nprof.export_chrome_trace('parallel.json')"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples"
            ],
            [
                "torch->Model Optimization->torch.compile Tutorial->Comparison to TorchScript and FX Tracing"
            ],
            [
                "matplotlib->Tutorials->Toolkits->The axisartist toolkit->axisartist"
            ],
            [
                "torch->Deploying PyTorch Models in Production->Deploying PyTorch in Python via a REST API with Flask->Inference->Prediction"
            ],
            [
                "torch->Frontend APIs->Dynamic Parallelism in TorchScript->Aside: Visualizing Parallelism"
            ]
        ]
    },
    "29495": {
        "jupyter_code_cell": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.set_option('display.float_format', '{:.4f}'.format)\npd.options.display.max_columns = 999\npd.options.display.max_rows = 999\nsns.set()",
        "matched_tutorial_code_inds": [
            5559,
            6174,
            6687,
            6049,
            6657
        ],
        "matched_tutorial_codes": [
            "import numpy as np\nimport pylab\nimport seaborn as sns\nimport statsmodels.api as sm\n\nsns.set_style(\"darkgrid\")\npylab.rc(\"figure\", figsize=(16, 8))\npylab.rc(\"font\", size=14)",
            "import numpy as np\nimport pandas as pd\n\nfrom statsmodels.graphics.tsaplots import plot_predict\nfrom statsmodels.tsa.arima_process import arma_generate_sample\nfrom statsmodels.tsa.arima.model import ARIMA\n\nnp.random.seed(12345)",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pandas_datareader as pdr\nimport seaborn as sns\n\nplt.rc(\"figure\", figsize=(16, 8))\nplt.rc(\"font\", size=15)\nplt.rc(\"lines\", linewidth=3)\nsns.set_style(\"darkgrid\")",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom scipy import stats\n\nsns.set_style(\"darkgrid\")\nsns.mpl.rc(\"figure\", figsize=(8, 8))",
            "import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\ndta = sm.datasets.macrodata.load_pandas().data\ndta.index = pd.date_range(start='1959Q1', end='2009Q4', freq='Q')"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Nonparametric Statistics->Lowess Regression"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Artificial Data"
            ],
            [
                "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Imports"
            ],
            [
                "statsmodels->Examples->Statistics->Copulas"
            ],
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale"
            ]
        ]
    },
    "1219618": {
        "jupyter_code_cell": "df['release_date'] = pd.to_datetime(df['release_date'])\ncols_list = list(df.columns.values)\ncols_list",
        "matched_tutorial_code_inds": [
            6804,
            5425,
            6187,
            1484,
            3159
        ],
        "matched_tutorial_codes": [
            "endog = macrodata['infl']\nendog.plot(figsize=(15, 5))",
            "respondent1000 = dta.iloc[1000]\nprint(respondent1000)",
            "dta.index = index\ndel dta[\"year\"]\ndel dta[\"quarter\"]",
            "china_masked = nbcases_ma[locations[:, 1] == \"China\"].sum(axis=0)\nchina_masked",
            "class_of_interest = \"virginica\"\nclass_id = (label_binarizer.classes_ == class_of_interest)[0]\nclass_id"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Time Series Filters"
            ],
            [
                "numpy->NumPy Features->Masked Arrays->Missing data"
            ],
            [
                "sklearn->Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->One-vs-Rest multiclass ROC->ROC curve showing a specific class"
            ]
        ]
    },
    "85609": {
        "jupyter_code_cell": "scipy.stats.chi2.isf(0.05, 1) \npvalue",
        "matched_tutorial_code_inds": [
            5433,
            6653,
            3935,
            5461,
            5908
        ],
        "matched_tutorial_codes": [
            "affair_mod.fittedvalues[1000]",
            "df = pred.summary_frame(alpha=0.05)\ndf",
            "sns.relplot(data=flights_wide, kind=\"line\")\n",
            "glm_mod.model.data.orig_endog.sum(1)",
            "interM_lm.model.data.orig_exog[:5]"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data"
            ],
            [
                "statsmodels->Examples->State space models->ETS models->Predictions"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ]
        ]
    },
    "694743": {
        "jupyter_code_cell": "type(example)\nexample.loc['Name']",
        "matched_tutorial_code_inds": [
            3541,
            5605,
            4611,
            3582,
            1418
        ],
        "matched_tutorial_codes": [
            "type(vectorizer.vocabulary_)",
            "data[:3]",
            "sio.whosmat('octave_a.mat')\n[('a', (1, 3, 4), 'double')]",
            "count_vect.vocabulary_.get(u'algorithm')\n4690",
            "dtype('float64')"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Working with text documents->FeatureHasher and DictVectorizer Comparison->DictVectorizer"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Weighted GLM: Poisson response data->Load data"
            ],
            [
                "scipy->File IO (scipy.io)->MATLAB files->How do I start?"
            ],
            [
                "sklearn->Tutorials->Working With Text Data->Extracting features from text files->Tokenizing text with scikit-learn"
            ],
            [
                "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Content->Shape, axis and array properties"
            ]
        ]
    },
    "1325610": {
        "jupyter_code_cell": "import os\nimport re\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport datetime as dt\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble.partial_dependence import plot_partial_dependence\nfrom sklearn.ensemble.partial_dependence import partial_dependence\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nimport datetime as dt\nfrom sklearn.ensemble import RandomForestClassifier as rfc\nfrom sklearn.metrics import roc_auc_score, roc_curve, auc",
        "matched_tutorial_code_inds": [
            1250,
            1215,
            1333,
            1088,
            3823
        ],
        "matched_tutorial_codes": [
            "import os\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom transformers import AutoTokenizer, GPT2TokenizerFast\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nimport functools\nfrom torch.optim.lr_scheduler import StepLR\nimport torch.nn.functional as F\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data.distributed import DistributedSampler\nfrom transformers.models.t5.modeling_t5 import T5Block\n\nfrom torch.distributed.algorithms._checkpoint.checkpoint_wrapper import (\n checkpoint_wrapper,\n CheckpointImpl,\n apply_activation_checkpointing_wrapper)\n\nfrom torch.distributed.fsdp import (\n    FullyShardedDataParallel as FSDP,\n    MixedPrecision,\n    BackwardPrefetch,\n    ShardingStrategy,\n    FullStateDictConfig,\n    StateDictType,\n)\nfrom torch.distributed.fsdp.wrap import (\n    transformer_auto_wrap_policy,\n    enable_wrap,\n    wrap,\n)\nfrom functools import partial\nfrom torch.utils.data import DataLoader\nfrom pathlib import Path\nfrom summarization_dataset import *\nfrom transformers.models.t5.modeling_t5 import T5Block\nfrom typing import Type\nimport time\nimport tqdm\nfrom datetime import datetime",
            "import os\nimport sys\nimport tempfile\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.multiprocessing as mp\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\n# On Windows platform, the torch.distributed package only\n# supports Gloo backend, FileStore and TcpStore.\n# For FileStore, set init_method parameter in init_process_group\n# to a local file. Example as follow:\n# init_method=\"file:///f:/libtmp/some_file\"\n# dist.init_process_group(\n#    \"gloo\",\n#    rank=rank,\n#    init_method=init_method,\n#    world_size=world_size)\n# For TcpStore, same way as on Linux.\n\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n\n    # initialize the process group\n    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n\ndef cleanup():\n    dist.destroy_process_group()",
            "import sys\nimport os\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport tempfile\nfrom torch.nn import TransformerEncoder, \n\nclass PositionalEncoding():\n\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = (p=dropout)\n\n        pe = (max_len, d_model)\n        position = (0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = ((0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = (position * div_term)\n        pe[:, 1::2] = (position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.pe = nn.Parameter(pe, requires_grad=False)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)",
            "import os\nimport sys\nimport time\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\nimport torchvision\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\n\n# Set up warnings\nimport warnings\nwarnings.filterwarnings(\n    action='ignore',\n    category=DeprecationWarning,\n    module=r'.*'\n)\nwarnings.filterwarnings(\n    action='default',\n    module=r'torch.ao.quantization'\n)\n\n# Specify random seed for repeatable results\ntorch.manual_seed(191009)",
            "import os\nimport io\nimport glob\nimport zipfile\nfrom utils import download_timeseries\n\nimport statsmodels.api as sm\n\n\ndef download_many(start, end):\n    months = pd.period_range(start, end=end, freq='M')\n    # We could easily parallelize this loop.\n    for i, month in enumerate(months):\n        download_timeseries(month)\n\n\ndef time_to_datetime(df, columns):\n    '''\n    Combine all time items into datetimes.\n\n    2014-01-01,1149.0 -&gt; 2014-01-01T11:49:00\n    '''\n    def converter(col):\n        timepart = (col.astype(str)\n                       .str.replace('\\.0$', '')  # NaNs force float dtype\n                       .str.pad(4, fillchar='0'))\n        return  pd.to_datetime(df['fl_date'] + ' ' +\n                               timepart.str.slice(0, 2) + ':' +\n                               timepart.str.slice(2, 4),\n                               errors='coerce')\n        return datetime_part\n    df[columns] = df[columns].apply(converter)\n    return df\n\n\ndef read_one(fp):\n    df = (pd.read_csv(fp, encoding='latin1')\n            .rename(columns=str.lower)\n            .drop('unnamed: 6', axis=1)\n            .pipe(time_to_datetime, ['dep_time', 'arr_time', 'crs_arr_time',\n                                     'crs_dep_time'])\n            .assign(fl_date=lambda x: pd.to_datetime(x['fl_date'])))\n    return df"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Parallel and Distributed Training->Advanced Model Training with Fully Sharded Data Parallel (FSDP)->Fine-tuning HF T5"
            ],
            [
                "torch->Parallel and Distributed Training->Getting Started with Distributed Data Parallel->Basic Use Case"
            ],
            [
                "torch->Parallel and Distributed Training->Training Transformer models using Distributed Data Parallel and Pipeline Parallelism->Define the model"
            ],
            [
                "torch->Model Optimization->(beta) Static Quantization with Eager Mode in PyTorch"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ]
        ]
    },
    "941578": {
        "jupyter_code_cell": "list1\nvis_dataset = pd.concat([features, target], axis = 1)",
        "matched_tutorial_code_inds": [
            1043,
            2801,
            3157,
            3588,
            4426
        ],
        "matched_tutorial_codes": [
            "model = ()\nfoobar_unstructured(, name='bias')\n\nprint()",
            "X = survey.data[survey.feature_names]\nX.describe(include=\"all\")\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "label_binarizer.transform([\"virginica\"])",
            "text_clf.fit(twenty_train.data, twenty_train.target)\nPipeline(...)",
            "points[tri.simplices[i, 1]]\narray([ 0. ,  1.1])"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Model Optimization->Pruning Tutorial->Extending torch.nn.utils.prune with custom pruning functions"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "sklearn->Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->One-vs-Rest multiclass ROC"
            ],
            [
                "sklearn->Tutorials->Working With Text Data->Building a pipeline"
            ],
            [
                "scipy->Spatial data structures and algorithms (scipy.spatial)->Delaunay triangulations"
            ]
        ]
    },
    "286998": {
        "jupyter_code_cell": "combined = getSentiment(\"This camera worked qute well, I am really happy with its image quality and eas-of-use.\")\ncombined\ndf = pd.read_pickle(\"Electronics_meta.pickle\")\ndf.head(1)",
        "matched_tutorial_code_inds": [
            6491,
            12,
            3805,
            6061,
            3657
        ],
        "matched_tutorial_codes": [
            "# Assign better names for our seasonal terms\ntrue_seasonal_10_3 = terms[0]\ntrue_seasonal_100_2 = terms[1]\ntrue_sum = true_seasonal_10_3 + true_seasonal_100_2\n<br/>",
            "# Equates to one random 28x28 image\nrandom_data = ((1, 1, 28, 28))\n\nmy_nn = Net()\nresult = my_nn(random_data)\nprint (result)",
            "gs = web.DataReader(\"GS\", data_source='yahoo', start='2006-01-01',\n                    end='2010-01-01')\ngs.head().round(2)",
            "mod = AutoReg(housing, 3, old_names=False)\nres = mod.fit()\nprint(res.summary())",
            "# With indecies\ntemp = weather['tmpf']\n\nc = (temp - 32) * 5 / 9\nc.to_frame()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->Seasonality in Time Series Data->Comparison of filtered estimates"
            ],
            [
                "torch->PyTorch Recipes->Defining a Neural Network in PyTorch->Steps->4. [Optional] Pass data through your model to test"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions"
            ],
            [
                "pandas_toms_blog->Indexes->Flavors->Indexes for Easier Arithmetic, Analysis"
            ]
        ]
    },
    "1242186": {
        "jupyter_code_cell": "X.shape\nfull_PCA = sklearn.decomposition.PCA()",
        "matched_tutorial_code_inds": [
            2880,
            1791,
            6878,
            2496,
            3440
        ],
        "matched_tutorial_codes": [
            "X_train.info()",
            "LogisticRegression()\n\n<br/>\n<br/>",
            "mod = NBin(y, X)\nres = mod.fit()",
            "LinearSVC()\n\n<br/>\n<br/>",
            "LabelSpreading(alpha=0.8, kernel='knn')\n\n<br/>\n<br/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Inspection->Partial Dependence and Individual Conditional Expectation Plots->Bike sharing dataset preprocessing",
                "sklearn->Examples->Pipelines and composite estimators->Column Transformer with Mixed Types"
            ],
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 1.1->get_feature_names_out Available in all Transformers",
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 0.23->Rich visual representation of estimators",
                "sklearn->Examples->Miscellaneous->Displaying Pipelines->Displaying a Pipeline with a Preprocessing Step and Classifier",
                "sklearn->Examples->Miscellaneous->Displaying estimators and complex pipelines->Rich HTML representation",
                "sklearn->Examples->Pipelines and composite estimators->Column Transformer with Mixed Types",
                "sklearn->Examples->Pipelines and composite estimators->Column Transformer with Mixed Types",
                "sklearn->Examples->Pipelines and composite estimators->Column Transformer with Mixed Types"
            ],
            [
                "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 2: Negative Binomial Regression for Count Data->Usage Example"
            ],
            [
                "sklearn->Examples->Feature Selection->Pipeline ANOVA SVM"
            ],
            [
                "sklearn->Examples->Semi Supervised Classification->Label Propagation learning a complex structure"
            ]
        ]
    },
    "1241726": {
        "jupyter_code_cell": "quandl.get(['NSE/OIL.1', 'WIKI/AAPL.4'])\nmix = quandl.get(['NSE/OIL.1', 'WIKI/AAPL.4'])\nmix.plot(subplots=True)",
        "matched_tutorial_code_inds": [
            5381,
            5399,
            3951,
            6460,
            4066
        ],
        "matched_tutorial_codes": [
            "spector_data = sm.datasets.spector.load()\nspector_data.exog = sm.add_constant(spector_data.exog, prepend=False)",
            "rand_data = sm.datasets.randhie.load()\nrand_exog = rand_data.exog\nrand_exog = sm.add_constant(rand_exog, prepend=False)",
            "flights_array = flights_wide.to_numpy()\nsns.relplot(data=flights_array, kind=\"line\")\n",
            "arma_mod20 = sm.tsa.statespace.SARIMAX(dta, order=(2,0,0), trend='c').fit(disp=False)\nprint(arma_mod20.params)",
            "g = sns.PairGrid(penguins)\ng.map_upper(sns.histplot)\ng.map_lower(sns.kdeplot, fill=True)\ng.map_diag(sns.histplot, kde=True)\n"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Data"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Poisson"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing wide-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing wide-form data"
            ],
            [
                "statsmodels->Examples->State space models->Statespace ARMA: Sunspots Data->Sunspots Data"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions",
                "seaborn->Plotting functions->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions"
            ]
        ]
    },
    "567587": {
        "jupyter_code_cell": "df.index.duplicated()\ndf[df.index.duplicated()]",
        "matched_tutorial_code_inds": [
            2801,
            3716,
            1495,
            3802,
            3706
        ],
        "matched_tutorial_codes": [
            "X = survey.data[survey.feature_names]\nX.describe(include=\"all\")\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "idx = idx[idx.get_level_values(0) &lt;= idx.get_level_values(1)]",
            "china_total.mask\ninvalid = china_total[china_total.mask]\ninvalid",
            "scores = pd.DataFrame(est.cv_results_)\nscores.head()",
            "delays.nsmallest(5).sort_values()"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization"
            ],
            [
                "numpy->NumPy Features->Masked Arrays->Fitting Data"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization"
            ]
        ]
    },
    "1225934": {
        "jupyter_code_cell": "xls2Sums= xls2Hotcodeclean.sum()\nxls2Sums\nxls2Hotcodeclean.columns",
        "matched_tutorial_code_inds": [
            6360,
            3864,
            5909,
            1438,
            6354
        ],
        "matched_tutorial_codes": [
            "delta_hat, rho_hat = sarima_res.params[:2]\ndelta_hat + rho_hat * y[0]",
            "most_common = df.occupation.value_counts().nlargest(100)\nmost_common",
            "interM_lm.model.exog\ninterM_lm.model.exog_names",
            "img_array_transposed = np.transpose(img_array, (2, 0, 1))\nimg_array_transposed.shape",
            "delta_hat, rho_hat = arima_res.params[:2]\ndelta_hat + rho_hat * (y[0] - delta_hat)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Reconstructing residuals, fitted values and forecasts in SARIMAX and ARIMA"
            ],
            [
                "pandas_toms_blog->Scaling->Dask"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Approximation->Applying to all colors"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Reconstructing residuals, fitted values and forecasts in SARIMAX and ARIMA"
            ]
        ]
    },
    "1179458": {
        "jupyter_code_cell": "print(\"P-Vales: \", zip(['Income', 'Age' ,'Education', 'Cards', 'Race_Asian', 'Race_Caucasian', 'Gender_Female'], \n                       lm.pvalues[1:]))\n0.234",
        "matched_tutorial_code_inds": [
            5229,
            4483,
            530,
            4500,
            3559
        ],
        "matched_tutorial_codes": [
            "print(sm.datasets.copper.DESCRLONG)\n\ndta = sm.datasets.copper.load_pandas().data\ndta.index = pd.date_range(\"1951-01-01\", \"1975-01-01\", freq=\"AS\")\nendog = dta[\"WORLDCONSUMPTION\"]\n\n# To the regressors in the dataset, we add a column of ones for an intercept\nexog = sm.add_constant(\n    dta[[\"COPPERPRICE\", \"INCOMEINDEX\", \"ALUMPRICE\", \"INVENTORYINDEX\"]]\n)",
            "print('mean = %6.4f, variance = %6.4f, skew = %6.4f, kurtosis = %6.4f' %\n...       normdiscrete.stats(moments='mvsk'))\nmean = -0.0000, variance = 6.3302, skew = 0.0000, kurtosis = -0.0076",
            "print(\"normalization constant shape:\", .transform[0].loc.shape)",
            "print('tail prob. of normal at 1%%, 5%% and 10%% %8.4f %8.4f %8.4f' %\n...       tuple(stats.norm.sf([crit01, crit05, crit10])*100))\ntail prob. of normal at 1%, 5% and 10%   0.2857   3.4957   8.5003",
            "print(digits.data)\n[[ 0.   0.   5. ...   0.   0.   0.]\n [ 0.   0.   0. ...  10.   0.   0.]\n [ 0.   0.   0. ...  16.   9.   0.]\n ...\n [ 0.   0.   1. ...   6.   0.   0.]\n [ 0.   0.   2. ...  12.   0.   0.]\n [ 0.   0.  10. ...  12.   1.   0.]]"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 1: Copper"
            ],
            [
                "scipy->Statistics (scipy.stats)->Building specific distributions->Subclassing rv_discrete"
            ],
            [
                "torch->Reinforcement Learning->Reinforcement Learning (PPO) with TorchRL Tutorial->Define an environment->Normalization"
            ],
            [
                "scipy->Statistics (scipy.stats)->Analysing one sample->Tails of the distribution"
            ],
            [
                "sklearn->Tutorials->An introduction to machine learning with scikit-learn->Loading an example dataset"
            ]
        ]
    },
    "1121415": {
        "jupyter_code_cell": "outputs = awe2d.forward(model,src,outparam,bc,hpc)\nprint('The outputs are:')\nfor out in outputs.keys():\n    print('- {}'.format(out))\nif 'sub_volume_boundary' in outputs.keys():\n    print('There are {} sub_volume_boundary outputs'.format(outputs['sub_volume_boundary'].shape[0]))",
        "matched_tutorial_code_inds": [
            1677,
            1679,
            1659,
            1661,
            1669
        ],
        "matched_tutorial_codes": [
            "output = general_julia(mesh, f=g, num_iter=15, radius=2.1)\nkwargs = {'title': 'g(z) = sin(tan(z^2))', 'cmap': 'plasma_r'}\n\nplot_fractal(output, **kwargs);\n\n\n\n\n<img alt=\"../_images/5526a4060ed968ec299d39a61bbae12254e1e2536b7ad21d8175edd94edf1bfd.png\" src=\"../_images/5526a4060ed968ec299d39a61bbae12254e1e2536b7ad21d8175edd94edf1bfd.png\"/>",
            "output = general_julia(small_mesh, f=h, num_iter=10, radius=2.1)\nkwargs = {'title': 'h(z) = tan(z^2) + sin(tan(z^2))', 'figsize': (7, 7), 'extent': [-1, 1, -1, 1], 'cmap': 'jet'}\n\nplot_fractal(output, **kwargs);\n\n\n\n\n<img alt=\"../_images/dbc0e399684e1cc4e4eeff0527f59ad37af23e35431abf58dcee0f7e2bdd370a.png\" src=\"../_images/dbc0e399684e1cc4e4eeff0527f59ad37af23e35431abf58dcee0f7e2bdd370a.png\"/>",
            "output = julia(mesh, num_iter=15)\nkwargs = {'title': 'f(z) = z^2 -1'}\n\nplot_fractal(output, **kwargs);\n\n\n\n\n<img alt=\"../_images/6f5ebf5f94a5bdba07a281f894d39c1f30729b755d1481c4cc3727674555ee16.png\" src=\"../_images/6f5ebf5f94a5bdba07a281f894d39c1f30729b755d1481c4cc3727674555ee16.png\"/>",
            "output = julia(mesh, c=-0.75 + 0.4j, num_iter=20)\nkwargs = {'title': 'f(z) = z^2 - \\dfrac{3}{4} + 0.4i', 'cmap': 'Greens_r'}\n\nplot_fractal(output, **kwargs);\n\n\n\n\n<img alt=\"../_images/bfb120678f65f1892658c8e3121fc93f5e95b61f9b4acd4ff184d041d5903eac.png\" src=\"../_images/bfb120678f65f1892658c8e3121fc93f5e95b61f9b4acd4ff184d041d5903eac.png\"/>",
            "output = newton_fractal(mesh, p, p.deriv(), num_iter=15, r=2)\nkwargs = {'title': 'f(z) = z - \\dfrac{(z^8 + 15z^4 - 16)}{(8z^7 + 60z^3)}', 'cmap': 'copper'}\n\nplot_fractal(output, **kwargs)\n\n\n\n\n<img alt=\"../_images/a48a6fcf739b31c8b3af199ca2f5dc75c0eb09a8a51543af4a9c037a9cd59be7.png\" src=\"../_images/a48a6fcf739b31c8b3af199ca2f5dc75c0eb09a8a51543af4a9c037a9cd59be7.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Applications->Plotting Fractals->Creating your own fractals"
            ],
            [
                "numpy->NumPy Applications->Plotting Fractals->Creating your own fractals"
            ],
            [
                "numpy->NumPy Applications->Plotting Fractals->Julia set"
            ],
            [
                "numpy->NumPy Applications->Plotting Fractals->Julia set"
            ],
            [
                "numpy->NumPy Applications->Plotting Fractals->Generalizing the Julia set->Newton Fractals"
            ]
        ]
    },
    "170297": {
        "jupyter_code_cell": "observed_survival_rate_child = observed_survived_child/total_child\nobserved_survival_rate_child\nadult_dropna_df = dropna_titanic_df[(dropna_titanic_df.IsAdult == 'Adult')]\ntotal_adult = adult_dropna_df['PassengerId'].count()\ntotal_adult",
        "matched_tutorial_code_inds": [
            5641,
            5643,
            5645,
            5630,
            5917
        ],
        "matched_tutorial_codes": [
            "glm = smf.glm(\n    \"affairs ~ rate_marriage + yrs_married\", data=data, family=sm.families.Poisson()\n)\nres_o2 = glm.fit()\n# print(res_f2.summary())\nres_o2.pearson_chi2 - res_o.pearson_chi2, res_o2.deviance - res_o.deviance, res_o2.llf - res_o.llf",
            "glm = smf.glm(\n    \"affairs ~ rate_marriage + yrs_married\",\n    data=dc,\n    family=sm.families.Poisson(),\n    freq_weights=np.asarray(dc[\"freq\"]),\n)\nres_f2 = glm.fit()\n# print(res_f2.summary())\nres_f2.pearson_chi2 - res_f.pearson_chi2, res_f2.deviance - res_f.deviance, res_f2.llf - res_f.llf",
            "glm = smf.glm(\n    \"affairs_sum ~ rate_marriage + yrs_married\",\n    data=df_a,\n    family=sm.families.Poisson(),\n    exposure=np.asarray(df_a[\"affairs_count\"]),\n)\nres_e2 = glm.fit()\nres_e2.pearson_chi2 - res_e.pearson_chi2, res_e2.deviance - res_e.deviance, res_e2.llf - res_e.llf",
            "glm = smf.glm(\n    \"affairs_sum ~ rate_marriage + age + yrs_married\",\n    data=df_a,\n    family=sm.families.Poisson(),\n    exposure=np.asarray(df_a[\"affairs_count\"]),\n)\nres_e = glm.fit()\nprint(res_e.summary())",
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->original observations and frequency weights"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->original observations and frequency weights"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->aggregated data: exposure and var_weights"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->aggregated or averaged data (unique values of explanatory variables)->using exposure"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ]
        ]
    },
    "624962": {
        "jupyter_code_cell": "midnights = pd.date_range(data.index[0],data.index[-1],\n                         freq = 'D',normalize = True)\nmidnights\nnew_index = midnights.union(data.index)\nnew_index",
        "matched_tutorial_code_inds": [
            6848,
            3676,
            3802,
            6804,
            1475
        ],
        "matched_tutorial_codes": [
            "index = pd.DatetimeIndex([\n    '2000-01-01 10:08am', '2000-01-01 11:32am',\n    '2000-01-01 5:32pm', '2000-01-02 6:15am'])\nendog4 = pd.Series([0.2, 0.5, -0.1, 0.1], index=index)\nprint(endog4.index)",
            "m = pd.merge(flights, daily.reset_index().rename(columns={'date': 'fl_date', 'station': 'origin'}),\n             on=['fl_date', 'origin']).set_index(idx_cols).sort_index()\n\nm.head()",
            "scores = pd.DataFrame(est.cv_results_)\nscores.head()",
            "endog = macrodata['infl']\nendog.plot(figsize=(15, 5))",
            "totals_row = 35\nlocations = np.delete(locations, (totals_row), axis=0)\nnbcases = np.delete(nbcases, (totals_row), axis=0)\n\nchina_total = nbcases[locations[:, 1] == \"China\"].sum(axis=0)\nchina_total"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes"
            ],
            [
                "pandas_toms_blog->Indexes->Merging->The merge version"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example"
            ],
            [
                "numpy->NumPy Features->Masked Arrays->Exploring the data"
            ]
        ]
    },
    "97209": {
        "jupyter_code_cell": "std_width = np.std(width)\nstd_height = np.std(height)\nmean_width = np.mean(width)\nmean_height = np.mean(height)\nexpected_uncert_on_product = np.sqrt(((std_width / mean_width)**2 + (std_height / mean_height)**2)                                     * (mean_width*mean_height)**2)\nprint(\"mean(w) * mean(h) = %.3f, propagation of uncertainty for w*h = %.3f \" %(mean_width*mean_height, expected_uncert_on_product))\nplt.scatter(area, width*height)\nplt.xlabel('area')\nplt.ylabel('width*height')\nplt.xlim(0.1,.25)\nplt.ylim(0.1,.25)",
        "matched_tutorial_code_inds": [
            1620,
            4507,
            3231,
            4224,
            4497
        ],
        "matched_tutorial_codes": [
            "pixel_intensity_distribution = ndimage.histogram(\n    xray_image, min=np.min(xray_image), max=np.max(xray_image), bins=256\n)\n\nplt.plot(pixel_intensity_distribution)\nplt.title(\"Pixel intensity distribution\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/00c12e163a30744c62700ec414e2d514c5c4eae10256e7cd0cf87553d0f4d34c.png\" src=\"../_images/00c12e163a30744c62700ec414e2d514c5c4eae10256e7cd0cf87553d0f4d34c.png\"/>",
            "rvs1 = stats.norm.rvs(loc=5, scale=10, size=500)\n rvs2 = stats.norm.rvs(loc=5, scale=10, size=500)\n stats.ttest_ind(rvs1, rvs2)\nTtest_indResult(statistic=-0.5489036175088705, pvalue=0.5831943748663959)  # random",
            "t_stat_uncorrected = (differences) / ((differences, ddof=1) / n)\np_val_uncorrected = .sf(np.abs(t_stat_uncorrected), df)\n\nprint(\n    f\"Uncorrected t-value: {t_stat_uncorrected:.3f}\\n\"\n    f\"Uncorrected p-value: {p_val_uncorrected:.3f}\"\n)",
            "print(I(4))\n(0.2500000000043577, 1.29830334693681e-08)\n print(I(3))\n(0.33333333325010883, 1.3888461883425516e-08)\n print(I(2))\n(0.4999999999985751, 1.3894083651858995e-08)",
            "crit01, crit05, crit10 = stats.t.ppf([1-0.01, 1-0.05, 1-0.10], 10)\n print('critical values from ppf at 1%%, 5%% and 10%% %8.4f %8.4f %8.4f' % (crit01, crit05, crit10))\ncritical values from ppf at 1%, 5% and 10%   2.7638   1.8125   1.3722\n print('critical values from isf at 1%%, 5%% and 10%% %8.4f %8.4f %8.4f' % tuple(stats.t.isf([0.01,0.05,0.10],10)))\ncritical values from isf at 1%, 5% and 10%   2.7638   1.8125   1.3722"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Applications->X-ray image processing->Apply masks to X-rays with np.where()"
            ],
            [
                "scipy->Statistics (scipy.stats)->Comparing two samples->Comparing means"
            ],
            [
                "sklearn->Examples->Model Selection->Statistical comparison of models using grid search->Comparing two models: frequentist approach"
            ],
            [
                "scipy->Integration (scipy.integrate)->General multiple integration (dblquad, tplquad, nquad)"
            ],
            [
                "scipy->Statistics (scipy.stats)->Analysing one sample->Tails of the distribution"
            ]
        ]
    },
    "906330": {
        "jupyter_code_cell": "objects = genre_list\ny_pos = np.arange(len(objects))\nnbOfMovies = [object[1] for object in topFiveGenres]\nplt.bar(y_pos, nbOfMovies, align='center', alpha=0.5, width=0.2)\nplt.xticks(y_pos, objects)\nplt.ylabel('Count in all movies')\nplt.title('Genres')\nplt.show()\nimport seaborn as sns\ncmap = sns.diverging_palette(220, 10, as_cmap=True)",
        "matched_tutorial_code_inds": [
            2298,
            5355,
            2873,
            3698,
            5180
        ],
        "matched_tutorial_codes": [
            "feature_importance = reg.feature_importances_\nsorted_idx = (feature_importance)\npos = (sorted_idx.shape[0]) + 0.5\nfig = (figsize=(12, 6))\n(1, 2, 1)\n(pos, feature_importance[sorted_idx], align=\"center\")\n(pos, (diabetes.feature_names)[sorted_idx])\n(\"Feature Importance (MDI)\")\n\nresult = (\n    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_idx = result.importances_mean.argsort()\n(1, 2, 2)\n(\n    result.importances[sorted_idx].T,\n    vert=False,\n    labels=(diabetes.feature_names)[sorted_idx],\n)\n(\"Permutation Importance (test set)\")\nfig.tight_layout()\n()\n\n\n<img alt=\"Feature Importance (MDI), Permutation Importance (test set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\"/>",
            "weights = rob_crime_model.weights\nidx = weights  0\nX = rob_crime_model.model.exog[idx.values]\nww = weights[idx] / weights[idx].mean()\nhat_matrix_diag = ww * (X * np.linalg.pinv(X).T).sum(1)\nresid = rob_crime_model.resid\nresid2 = resid ** 2\nresid2 /= resid2.sum()\nnobs = int(idx.sum())\nhm = hat_matrix_diag.mean()\nrm = resid2.mean()",
            "features_names = [\"experience\", \"parent hourly wage\", \"college degree\"]\n\nregressor_without_ability = ()\nregressor_without_ability.fit(X_train[features_names], y_train)\ny_pred_without_ability = regressor_without_ability.predict(X_test[features_names])\nR2_without_ability = (y_test, y_pred_without_ability)\n\nprint(f\"R2 score without ability: {R2_without_ability:.3f}\")",
            "files = glob.glob('weather/*.csv')\ncolumns = ['station', 'date', 'tmpf', 'relh', 'sped', 'mslp',\n           'p01i', 'vsby', 'gust_mph', 'skyc1', 'skyc2', 'skyc3']\n\n# init empty DataFrame, like you might for a list\nweather = pd.DataFrame(columns=columns)\n\nfor fp in files:\n    city = pd.read_csv(fp, columns=columns)\n    weather.append(city)",
            "pred_ols2 = res2.get_prediction()\niv_l = pred_ols.summary_frame()[\"obs_ci_lower\"]\niv_u = pred_ols.summary_frame()[\"obs_ci_upper\"]\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nax.plot(x, y, \"o\", label=\"Data\")\nax.plot(x, y_true, \"b-\", label=\"True\")\nax.plot(x, res2.fittedvalues, \"r--.\", label=\"Predicted\")\nax.plot(x, iv_u, \"r--\")\nax.plot(x, iv_l, \"r--\")\nlegend = ax.legend(loc=\"best\")\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_ols_26_0.png\" src=\"../../../_images/examples_notebooks_generated_ols_26_0.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Ensemble methods->Gradient Boosting regression->Plot feature importance"
            ],
            [
                "statsmodels->Examples->Plotting->Regression Plots->Statewide Crime 2009 Dataset->Using robust regression to correct for outliers."
            ],
            [
                "sklearn->Examples->Inspection->Failure of Machine Learning to infer causal effects->Income prediction with partial observations"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->OLS with dummy variables"
            ]
        ]
    },
    "158789": {
        "jupyter_code_cell": "ax = sns.barplot(x=\"WWID\", y=\"Post_Q29\", hue=\"MatchSequence\", data=df)\nsns.plt.ylim(0, 10)\nplt.legend(bbox_to_anchor=(1.04, 1),loc=1, borderaxespad=0.) \nax = sns.barplot(x=\"WWID\", y=\"Pre_Q30\", hue=\"MatchSequence\",data=df,palette=\"husl\")\nsns.plt.ylim(0, 10)\nplt.legend(bbox_to_anchor=(1.04, 1),loc=1, borderaxespad=0.) ",
        "matched_tutorial_code_inds": [
            3888,
            2678,
            2662,
            2635,
            5059
        ],
        "matched_tutorial_codes": [
            "ax = occupation_avg.sort_values(ascending=False).plot.barh(color='k', width=0.9);\nlim = ax.get_ylim()\nax.vlines(total_avg, *lim, color='C1', linewidth=3)\nax.legend(['Average donation'])\nax.set(xlabel=\"Donation Amount\", title=\"Average Dontation by Occupation\")\nsns.despine()",
            "ax = results.plot()\nax.vlines(\n    alpha_aic,\n    results[\"AIC criterion\"].min(),\n    results[\"AIC criterion\"].max(),\n    label=\"alpha: AIC estimate\",\n    linestyles=\"--\",\n    color=\"tab:blue\",\n)\nax.vlines(\n    alpha_bic,\n    results[\"BIC criterion\"].min(),\n    results[\"BIC criterion\"].max(),\n    label=\"alpha: BIC estimate\",\n    linestyle=\"--\",\n    color=\"tab:orange\",\n)\nax.set_xlabel(r\"$\\alpha$\")\nax.set_ylabel(\"criterion\")\nax.set_xscale(\"log\")\nax.legend()\n_ = ax.set_title(\n    f\"Information-criterion for model selection (training time {fit_time:.2f}s)\"\n)\n\n\n<img alt=\"Information-criterion for model selection (training time 0.01s)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_lasso_model_selection_001.png\" srcset=\"../../_images/sphx_glr_plot_lasso_model_selection_001.png\"/>",
            "m, s, _ = (\n    (enet.coef_)[0],\n    enet.coef_[enet.coef_ != 0],\n    markerfmt=\"x\",\n    label=\"Elastic net coefficients\",\n)\n([m, s], color=\"#2ca02c\")\nm, s, _ = (\n    (lasso.coef_)[0],\n    lasso.coef_[lasso.coef_ != 0],\n    markerfmt=\"x\",\n    label=\"Lasso coefficients\",\n)\n([m, s], color=\"#ff7f0e\")\n(\n    (coef)[0],\n    coef[coef != 0],\n    label=\"true coefficients\",\n    markerfmt=\"bx\",\n)\n\n(loc=\"best\")\n(\n    \"Lasso $R^2$: %.3f, Elastic Net $R^2$: %.3f\" % (r2_score_lasso, r2_score_enet)\n)\n()\n\n\n<img alt=\"Lasso $R^2$: 0.658, Elastic Net $R^2$: 0.643\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_lasso_and_elasticnet_001.png\" srcset=\"../../_images/sphx_glr_plot_lasso_and_elasticnet_001.png\"/>",
            "ax = (\n    data=full_data, x=\"input_feature\", y=\"target\", color=\"black\", alpha=0.75\n)\nax.plot(X_plot, y_plot, color=\"black\", label=\"Ground Truth\")\nax.plot(X_plot, y_brr, color=\"red\", label=\"BayesianRidge with polynomial features\")\nax.plot(X_plot, y_ard, color=\"navy\", label=\"ARD with polynomial features\")\nax.fill_between(\n    X_plot.ravel(),\n    y_ard - y_ard_std,\n    y_ard + y_ard_std,\n    color=\"navy\",\n    alpha=0.3,\n)\nax.fill_between(\n    X_plot.ravel(),\n    y_brr - y_brr_std,\n    y_brr + y_brr_std,\n    color=\"red\",\n    alpha=0.3,\n)\nax.legend()\n_ = ax.set_title(\"Polynomial fit of a non-linear feature\")\n\n\n<img alt=\"Polynomial fit of a non-linear feature\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_ard_003.png\" srcset=\"../../_images/sphx_glr_plot_ard_003.png\"/>",
            "ax1 = SubplotHost(fig, 1, 2, 2, grid_helper=grid_helper)\n\n# A parasite axes with given transform\nax2 = ax1.get_aux_axes(tr, \"equal\")\n# note that ax2.transData == tr + ax1.transData\n# Anything you draw in ax2 will match the ticks and grids of ax1.\n\n\n<figure class=\"align-center\">\n<img alt=\"../../_images/sphx_glr_demo_curvelinear_grid_001.png\" src=\"../../_images/sphx_glr_demo_curvelinear_grid_001.png\"/>\n</figure>"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Scaling->Dask"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Lasso model selection: AIC-BIC / cross-validation->Selecting Lasso via an information criterion"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Lasso and Elastic Net for Sparse Signals->Plot"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Comparing Linear Bayesian Regressors->Bayesian regressions with polynomial feature expansion->Plotting polynomial regressions with std errors of the scores"
            ],
            [
                "matplotlib->Tutorials->Toolkits->The axisartist toolkit->GridHelper"
            ]
        ]
    },
    "1001878": {
        "jupyter_code_cell": "%matplotlib inline \nfrom matplotlib import rcParams \nfrom bs4 import BeautifulSoup\nfrom collections import defaultdict \nfrom imdb import IMDb\nfrom pyquery import PyQuery as pq\nimport numpy as np\nimport pandas as pd\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport cPickle as pickle\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\nimport io \nimport time\nimport requests\nimport sklearn\nimport warnings\nwarnings.filterwarnings('ignore')\nyears = xrange(2009,2015)\npages = xrange(1,9)\nyear_pagetxt = {}\nfor year in years: \n    pagestext = {}\n    for page in pages: \n        r = requests.get(\"http://www.boxofficemojo.com/yearly/chart/?page=%s&view=releasedate&view2=domestic&yr=%s&p=.htm\"%(page, year))\n        pagestext[page] = r.text\n        time.sleep(1)\n    year_pagetxt[year] = pagestext",
        "matched_tutorial_code_inds": [
            6518,
            3697,
            3745,
            6539,
            3804
        ],
        "matched_tutorial_codes": [
            "%matplotlib inline\n\nfrom importlib import reload\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\nfrom scipy.stats import invwishart, invgamma\n\n# Get the macro dataset\ndta = sm.datasets.macrodata.load_pandas().data\ndta.index = pd.date_range('1959Q1', '2009Q3', freq='QS')",
            "%matplotlib inline\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nif int(os.environ.get(\"MODERN_PANDAS_EPUB\", 0)):\n    import prep # noqa\n\nsns.set_style('ticks')\nsns.set_context('talk')\npd.options.display.max_rows = 10",
            "%matplotlib inline\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nif int(os.environ.get(\"MODERN_PANDAS_EPUB\", 0)):\n    import prep # noqa\n\npd.options.display.max_rows = 10\nsns.set(style='ticks', context='talk')",
            "%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pymc3 as pm\nimport statsmodels.api as sm\nimport theano\nimport theano.tensor as tt\nfrom pandas.plotting import register_matplotlib_converters\nfrom pandas_datareader.data import DataReader\n\nplt.style.use(\"seaborn\")\nregister_matplotlib_converters()",
            "%matplotlib inline\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport pandas_datareader.data as web\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style='ticks', context='talk')\n\nif int(os.environ.get(\"MODERN_PANDAS_EPUB\", 0)):\n    import prep # noqa"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing"
            ],
            [
                "pandas_toms_blog->Fast Pandas"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX estimation with Bayesian methods->Introduction->1. Import external dependencies"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries"
            ]
        ]
    },
    "985676": {
        "jupyter_code_cell": "close_px['AAPL'].plot()\nkk.line_tidy(close_px['AAPL'])",
        "matched_tutorial_code_inds": [
            5345,
            5334,
            5233,
            5885,
            3829
        ],
        "matched_tutorial_codes": [
            "fig = sm.graphics.plot_partregress_grid(crime_model)\nfig.tight_layout(pad=1.0)",
            "fig = sm.graphics.plot_partregress_grid(prestige_model)\nfig.tight_layout(pad=1.0)",
            "print(res.recursive_coefficients.filtered[0])\nres.plot_recursive_coefficient(range(mod.k_exog), alpha=None, figsize=(10, 6))",
            "_ = plt.boxplot([scales[0][0], scales[0][1], scales[1][0], scales[1][1]])\nplt.ylabel(\"Estimated scale\")",
            "ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Plotting->Regression Plots->Statewide Crime 2009 Dataset->Partial Regression Plots (Crime Data)"
            ],
            [
                "statsmodels->Examples->Plotting->Regression Plots->Duncan\u2019s Prestige Dataset->Partial Regression Plots (Duncan)"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 1: Copper"
            ],
            [
                "statsmodels->Examples->Generalized Estimating Equations->GEE Score Tests"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ]
        ]
    },
    "15264": {
        "jupyter_code_cell": "sns.set()\nsns.distplot(df['temperature'], bins=20, fit=norm)",
        "matched_tutorial_code_inds": [
            3792,
            3793,
            4125,
            4157,
            4003
        ],
        "matched_tutorial_codes": [
            "sns.countplot(x='cut', data=df)\nsns.despine()\nplt.tight_layout()",
            "sns.barplot(x='cut', y='price', data=df)\nsns.despine()\nplt.tight_layout()",
            "g = sns.FacetGrid(tips, col=\"time\")\ng.map(sns.histplot, \"tip\")\n",
            "sns.set_style(\"whitegrid\")\nsns.boxplot(data=data, palette=\"deep\")\nsns.despine(left=True)\n",
            "fmri = sns.load_dataset(\"fmri\")\nsns.relplot(data=fmri, x=\"timepoint\", y=\"signal\", kind=\"line\")\n"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Conditional small multiples",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Conditional small multiples"
            ],
            [
                "seaborn->User guide and tutorial->Controlling figure aesthetics->Removing axes spines",
                "seaborn->Figure aesthetics->Controlling figure aesthetics->Removing axes spines"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty",
                "seaborn->Plotting functions->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty"
            ]
        ]
    },
    "1425563": {
        "jupyter_code_cell": "chisquare_survival(titanic_df_clean,'Pclass')\nsns.boxplot(x='Survived',y='Fare',data=titanic_df_clean,fliersize=0,palette=['r','g'])\nplt.xticks([0,1],['Died','Survived'])\nplt.ylim(0,150)\nplt.xlabel('Survival status')\nplt.ylabel('Fare paid (USD)')\nplt.title(\"Chart 3: Boxplots of fare paid by survival status (excluding outliers)\")",
        "matched_tutorial_code_inds": [
            6714,
            4574,
            2825,
            4099,
            3078
        ],
        "matched_tutorial_codes": [
            "make_plot(dta.index[idx[:5]])\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_24_0.png\" src=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_24_0.png\"/>",
            "geometric_transform(a, shift_func, extra_keywords = {'s0': 0.5, 's1': 0.5})\narray([[ 0.    ,  0.    ,  0.    ],\n       [ 0.    ,  1.3625,  2.7375],\n       [ 0.    ,  4.8125,  6.1875],\n       [ 0.    ,  8.2625,  9.6375]])",
            "coefs.plot.barh(figsize=(9, 7))\n(\"Ridge model, small regularization\")\n(x=0, color=\".5\")\n(\"Raw coefficient values\")\n(left=0.3)\n\n\n<img alt=\"Ridge model, small regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_003.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_003.png\"/>",
            "plot_errorbars(lambda x: (x.min(), x.max()))\n",
            "svc_disp = (svc, X_test, y_test)\n()\n\n\n<img alt=\"plot roc curve visualization api\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_roc_curve_visualization_api_001.png\" srcset=\"../../_images/sphx_glr_plot_roc_curve_visualization_api_001.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "scipy->Multidimensional image processing (scipy.ndimage)->Interpolation functions->Interpolation functions"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "seaborn->User guide and tutorial->Statistical estimation and error bars->Measures of estimate uncertainty->Custom error bars",
                "seaborn->Statistical operations->Statistical estimation and error bars->Measures of estimate uncertainty->Custom error bars"
            ],
            [
                "sklearn->Examples->Miscellaneous->ROC Curve with Visualization API->Plotting the ROC Curve"
            ]
        ]
    },
    "543699": {
        "jupyter_code_cell": "from sklearn.metrics import classification_report,confusion_matrix\npred = knn.predict(X_test)\nprint(confusion_matrix(y_test,pred))",
        "matched_tutorial_code_inds": [
            3145,
            2497,
            2303,
            1880,
            2771
        ],
        "matched_tutorial_codes": [
            "from sklearn.metrics import \n\ny_pred = grid_search.predict(X_test)\nprint((y_test, y_pred))",
            "from sklearn.metrics import \n\ny_pred = anova_svm.predict(X_test)\nprint((y_test, y_pred))",
            "from sklearn.ensemble import \n\nclf = (max_samples=100, random_state=0)\nclf.fit(X_train)",
            "from sklearn.ensemble import \n\nclf = (n_estimators=25)\nclf.fit(X_train_valid, y_train_valid)",
            "from sklearn import linear_model\n\nols = ()\n_ = ols.fit(X_train, y_train)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Model Selection->Custom refit strategy of a grid search with cross-validation->Tuning hyper-parameters"
            ],
            [
                "sklearn->Examples->Feature Selection->Pipeline ANOVA SVM"
            ],
            [
                "sklearn->Examples->Ensemble methods->IsolationForest example->Training of the model"
            ],
            [
                "sklearn->Examples->Calibration->Probability Calibration for 3-class classification->Fitting and calibration"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Sparsity Example: Fitting only features 1  and 2"
            ]
        ]
    },
    "1402926": {
        "jupyter_code_cell": "df = pd.read_pickle('After_filling_Nans')\ntemp_df = df.drop_duplicates(['Hotel_Name'])\nlen(temp_df)",
        "matched_tutorial_code_inds": [
            3703,
            6061,
            6659,
            3829,
            6664
        ],
        "matched_tutorial_codes": [
            "df = pd.read_csv(\"data/347136217_T_ONTIME.csv\")\ndelays = df['DEP_DELAY']",
            "mod = AutoReg(housing, 3, old_names=False)\nres = mod.fit()\nprint(res.summary())",
            "mod = LocalLevel(dta.infl)\nres = mod.fit(disp=False)\nprint(res.summary())",
            "ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "mod_conc = LocalLevelConcentrated(dta.infl)\nres_conc = mod_conc.fit(disp=False)\nprint(res_conc.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions"
            ],
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Typical approach"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Concentrating out the scale"
            ]
        ]
    },
    "1144631": {
        "jupyter_code_cell": "yahoo.drop('Adj Close', axis = 1, inplace=True)\nyahoo['day_range'] = yahoo['High'] - yahoo['Low']\nyahoo['intraday_return'] = (yahoo['Close'] - yahoo['Open'])/yahoo['Open']",
        "matched_tutorial_code_inds": [
            6704,
            6844,
            6846,
            3827,
            6703
        ],
        "matched_tutorial_codes": [
            "ax = dta.mean().plot(grid=False)\nax.set_xlabel(\"Year\", size=17)\nax.set_ylabel(\"Fertility rate\", size=17)\nax.set_xlim(0, 51)",
            "# Quarterly frequency, using a DatetimeIndex\nindex = pd.date_range(start='2000', periods=4, freq='QS')\nendog2 = pd.Series([1, 2, 3, 4], index=index)\nprint(endog2.index)",
            "# Monthly frequency, using a DatetimeIndex\nindex = pd.date_range(start='2000', periods=4, freq='M')\nendog3 = pd.Series([1, 2, 3, 4], index=index)\nprint(endog3.index)",
            "daily = df.fl_date.value_counts().sort_index()\ny = daily.resample('MS').mean()\ny.head()",
            "columns = list(map(str, range(1960, 2012)))\ndata.set_index(\"Country Name\", inplace=True)\ndta = data[columns]\ndta = dta.dropna()\ndta.head()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ]
        ]
    },
    "313536": {
        "jupyter_code_cell": "%matplotlib inline\nimport xarray as xa\nimport netCDF4 as nc\nimport pandas as pd\nimport numpy as np\nimport datetime\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom matplotlib.dates import YearLocator, WeekdayLocator, MonthLocator, DayLocator, HourLocator, DateFormatter\nimport matplotlib.ticker as ticker\nimport matplotlib.dates as mdates\nfrom matplotlib.ticker import NullFormatter  \nimport cmocean\nimport gmt\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors as mcolors\nimport collections",
        "matched_tutorial_code_inds": [
            6539,
            6518,
            3745,
            3697,
            3804
        ],
        "matched_tutorial_codes": [
            "%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pymc3 as pm\nimport statsmodels.api as sm\nimport theano\nimport theano.tensor as tt\nfrom pandas.plotting import register_matplotlib_converters\nfrom pandas_datareader.data import DataReader\n\nplt.style.use(\"seaborn\")\nregister_matplotlib_converters()",
            "%matplotlib inline\n\nfrom importlib import reload\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\nfrom scipy.stats import invwishart, invgamma\n\n# Get the macro dataset\ndta = sm.datasets.macrodata.load_pandas().data\ndta.index = pd.date_range('1959Q1', '2009Q3', freq='QS')",
            "%matplotlib inline\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nif int(os.environ.get(\"MODERN_PANDAS_EPUB\", 0)):\n    import prep # noqa\n\npd.options.display.max_rows = 10\nsns.set(style='ticks', context='talk')",
            "%matplotlib inline\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nif int(os.environ.get(\"MODERN_PANDAS_EPUB\", 0)):\n    import prep # noqa\n\nsns.set_style('ticks')\nsns.set_context('talk')\npd.options.display.max_rows = 10",
            "%matplotlib inline\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport pandas_datareader.data as web\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style='ticks', context='talk')\n\nif int(os.environ.get(\"MODERN_PANDAS_EPUB\", 0)):\n    import prep # noqa"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->SARIMAX estimation with Bayesian methods->Introduction->1. Import external dependencies"
            ],
            [
                "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data"
            ],
            [
                "pandas_toms_blog->Fast Pandas"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries"
            ]
        ]
    },
    "58274": {
        "jupyter_code_cell": "import matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport math\nimport operator",
        "matched_tutorial_code_inds": [
            5161,
            6117,
            5407,
            3993,
            4668
        ],
        "matched_tutorial_codes": [
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\nnp.random.seed(9876789)",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom statsmodels.tsa.arima.model import ARIMA",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom statsmodels.formula.api import logit",
            "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n",
            "import matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships",
                "seaborn->Plotting functions->Visualizing statistical relationships"
            ],
            [
                "matplotlib->Tutorials->Introductory->Image tutorial->Startup commands"
            ]
        ]
    },
    "50548": {
        "jupyter_code_cell": "for keys in tokenized_sents.iterkeys():\n    filtered_sents = [token for token in tokenized_sents[keys] if token not in stopwords_list ]\nimport nltk\nstopwords_list_570 = []\nwith open('./stopwords_en.txt') as f:\n    stopwords_list_570 = f.read().splitlines()",
        "matched_tutorial_code_inds": [
            6655,
            6258,
            3167,
            1085,
            2162
        ],
        "matched_tutorial_codes": [
            "for i in range(simulated.shape[1]):\n    simulated.iloc[:, i].plot(label=\"_\", color=\"gray\", alpha=0.1)\ndf[\"mean\"].plot(label=\"mean prediction\")\ndf[\"pi_lower\"].plot(linestyle=\"--\", color=\"tab:blue\", label=\"95% interval\")\ndf[\"pi_upper\"].plot(linestyle=\"--\", color=\"tab:blue\", label=\"_\")\npred.endog.plot(label=\"data\")\nplt.legend()",
            "for fit in [fit2, fit4]:\n    pd.DataFrame(np.c_[fit.level, fit.trend]).rename(\n        columns={0: \"level\", 1: \"slope\"}\n    ).plot(subplots=True)\nplt.show()\nprint(\n    \"Figure 7.4: Level and slope components for Holt\u2019s linear trend method and the additive damped trend method.\"\n)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_0.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_0.png\"/>\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_1.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_1.png\"/>",
            "for i in range(n_classes):\n    fpr[i], tpr[i], _ = (y_onehot_test[:, i], y_score[:, i])\n    roc_auc[i] = (fpr[i], tpr[i])\n\nfpr_grid = (0.0, 1.0, 1000)\n\n# Interpolate all ROC curves at these points\nmean_tpr = (fpr_grid)\n\nfor i in range(n_classes):\n    mean_tpr += (fpr_grid, fpr[i], tpr[i])  # linear interpolation\n\n# Average it and compute AUC\nmean_tpr /= n_classes\n\nfpr[\"macro\"] = fpr_grid\ntpr[\"macro\"] = mean_tpr\nroc_auc[\"macro\"] = (fpr[\"macro\"], tpr[\"macro\"])\n\nprint(f\"Macro-averaged One-vs-Rest ROC AUC score:\\n{roc_auc['macro']:.2f}\")",
            "for param in model_ft.parameters():\n  param.requires_grad = True\n\nmodel_ft.to(device)  # We can fine-tune on GPU if available\n\ncriterion = nn.CrossEntropyLoss()\n\n# Note that we are training everything, so the learning rate is lower\n# Notice the smaller learning rate\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=1e-3, momentum=0.9, weight_decay=0.1)\n\n# Decay LR by a factor of 0.3 every several epochs\nexp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=5, gamma=0.3)\n\nmodel_ft_tuned = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n                             num_epochs=25, device=device)",
            "for pipe in (hist_dropped, hist_one_hot, hist_ordinal, hist_native):\n    pipe.set_params(\n        histgradientboostingregressor__max_depth=3,\n        histgradientboostingregressor__max_iter=15,\n    )\n\ndropped_result = (hist_dropped, X, y, cv=n_cv_folds, scoring=scoring)\none_hot_result = (hist_one_hot, X, y, cv=n_cv_folds, scoring=scoring)\nordinal_result = (hist_ordinal, X, y, cv=n_cv_folds, scoring=scoring)\nnative_result = (hist_native, X, y, cv=n_cv_folds, scoring=scoring)\n\nplot_results(\"Gradient Boosting on Ames Housing (few and small trees)\")\n\n()\n\n\n<img alt=\"Gradient Boosting on Ames Housing (few and small trees), Fit times (s), Mean Absolute Percentage Error\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_002.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->ETS models->Predictions"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Method->Plots of Seasonally Adjusted Data"
            ],
            [
                "sklearn->Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->One-vs-Rest multiclass ROC->ROC curve using the OvR macro-average"
            ],
            [
                "torch->Model Optimization->(beta) Quantized Transfer Learning for Computer Vision Tutorial->Part 2. Finetuning the Quantizable Model->Finetuning the model"
            ],
            [
                "sklearn->Examples->Ensemble methods->Categorical Feature Support in Gradient Boosting->Limiting the number of splits"
            ]
        ]
    },
    "967198": {
        "jupyter_code_cell": "accuracy1 = model1.score(aX_train, ay_train)\naccuracy2 = model2.score(bX_train, by_train)\naccuracy3 = model3.score(cX_train, cy_train)\nprint('Accuracy')\nprint('model1:', accuracy1)\nprint('model2:',accuracy2)\nprint('model3:',accuracy3)\na_preds = model1.predict_proba(a_test)\nb_preds = model2.predict_proba(b_test)\nc_preds = model3.predict_proba(c_test)",
        "matched_tutorial_code_inds": [
            2860,
            2864,
            2844,
            6090,
            5643
        ],
        "matched_tutorial_codes": [
            "mae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}\n\n_, ax = (figsize=(5, 5))\ndisplay = (\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n)\nax.set_title(\"Ridge model, optimum regularization\")\nfor name, score in scores.items():\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\nax.legend(loc=\"upper left\")\n()\n\n\n<img alt=\"Ridge model, optimum regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_012.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_012.png\"/>",
            "mae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}\n\n_, ax = (figsize=(6, 6))\ndisplay = (\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n)\nax.set_title(\"Lasso model, optimum regularization\")\nfor name, score in scores.items():\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\nax.legend(loc=\"upper left\")\n()\n\n\n<img alt=\"Lasso model, optimum regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_015.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_015.png\"/>",
            "mae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}\n\n_, ax = (figsize=(5, 5))\ndisplay = (\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n)\nax.set_title(\"Ridge model, small regularization\")\nfor name, score in scores.items():\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\nax.legend(loc=\"upper left\")\n()\n\n\n<img alt=\"Ridge model, small regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_009.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_009.png\"/>",
            "res_ar5 = AutoReg(ind_prod, 5, old_names=False).fit()\npredictions = pd.DataFrame(\n    {\n        \"AR(5)\": res_ar5.predict(start=714, end=726),\n        \"AR(13)\": res.predict(start=714, end=726),\n        \"Restr. AR(13)\": res_glob.predict(start=714, end=726),\n    }\n)\n_, ax = plt.subplots()\nax = predictions.plot(ax=ax)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_42_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_42_0.png\"/>",
            "glm = smf.glm(\n    \"affairs ~ rate_marriage + yrs_married\",\n    data=dc,\n    family=sm.families.Poisson(),\n    freq_weights=np.asarray(dc[\"freq\"]),\n)\nres_f2 = glm.fit()\n# print(res_f2.summary())\nres_f2.pearson_chi2 - res_f.pearson_chi2, res_f2.deviance - res_f.deviance, res_f2.llf - res_f.llf"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions->Industrial Production"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->original observations and frequency weights"
            ]
        ]
    },
    "1438081": {
        "jupyter_code_cell": "transactions = %contiamo query query:sql:48590597:411:g71GXzJjsx4Uvad11ouKjoYbQUNNPy-qRMKkBNZfyx4\ncustomers = %contiamo query query:sql:48590597:441:MG5W2dMjXzYgsHsgdQYzmhv44dxEQX2Lodu5Uh2Hx_s\napplications = %contiamo query query:sql:48590597:442:-gz3nbw1fdmtSXkD4zGNA-cVa7s6sQtRn8upCSn6uys            \ndf1 = pd.DataFrame ({\n        'Age' : customers['Field age'],\n        'Customer id' : customers['Field customer id']\n    })",
        "matched_tutorial_code_inds": [
            1523,
            2446,
            2463,
            3893,
            632
        ],
        "matched_tutorial_codes": [
            "transistor_count_predicted = np.exp(B) * np.exp(A * year)\ntransistor_Moores_law = Moores_law(year)\nplt.style.use(\"fivethirtyeight\")\nplt.semilogy(year, transistor_count, \"s\", label=\"MOS transistor count\")\nplt.semilogy(year, transistor_count_predicted, label=\"linear regression\")\n\n\nplt.plot(year, transistor_Moores_law, label=\"Moore's Law\")\nplt.title(\n    \"MOS transistor count per microprocessor\\n\"\n    + \"every two years \\n\"\n    + \"Transistor count was x{:.2f} higher\".format(np.exp(A * 2))\n)\nplt.xlabel(\"year introduced\")\nplt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\nplt.ylabel(\"# of transistors\\nper microprocessor\")",
            "last_hours = slice(-96, None)\nfig, ax = (figsize=(12, 4))\nfig.suptitle(\"Predictions by linear models\")\nax.plot(\n    y.iloc[test_0].values[last_hours],\n    \"x-\",\n    alpha=0.2,\n    label=\"Actual demand\",\n    color=\"black\",\n)\nax.plot(naive_linear_predictions[last_hours], \"x-\", label=\"Ordinal time features\")\nax.plot(\n    cyclic_cossin_linear_predictions[last_hours],\n    \"x-\",\n    label=\"Trigonometric time features\",\n)\nax.plot(\n    cyclic_spline_linear_predictions[last_hours],\n    \"x-\",\n    label=\"Spline-based time features\",\n)\nax.plot(\n    one_hot_linear_predictions[last_hours],\n    \"x-\",\n    label=\"One-hot time features\",\n)\n_ = ax.legend()\n\n\n<img alt=\"Predictions by linear models\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_006.png\" srcset=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_006.png\"/>",
            "last_hours = slice(-96, None)\nfig, ax = (figsize=(12, 4))\nfig.suptitle(\"Predictions by non-linear regression models\")\nax.plot(\n    y.iloc[test_0].values[last_hours],\n    \"x-\",\n    alpha=0.2,\n    label=\"Actual demand\",\n    color=\"black\",\n)\nax.plot(\n    gbrt_predictions[last_hours],\n    \"x-\",\n    label=\"Gradient Boosted Trees\",\n)\nax.plot(\n    one_hot_poly_predictions[last_hours],\n    \"x-\",\n    label=\"One-hot + polynomial kernel\",\n)\nax.plot(\n    cyclic_spline_poly_predictions[last_hours],\n    \"x-\",\n    label=\"Splines + polynomial kernel\",\n)\n_ = ax.legend()\n\n\n<img alt=\"Predictions by non-linear regression models\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_007.png\" srcset=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_007.png\"/>",
            "indiv = indiv[(indiv.transaction_dt &gt;= pd.Timestamp(\"2007-01-01\")) &amp;\n              (indiv.transaction_dt &lt;= pd.Timestamp(\"2018-01-01\"))]\n\ndf2 = dd.merge(indiv, cm.reset_index(), on='cmte_id')\ndf2\n<strong>Dask DataFrame Structure:</strong>Dask Name: merge, 141 tasks",
            "img_out_y = Image.fromarray(np.uint8((img_out_y[0] * 255.0).clip(0, 255)[0]), mode='L')\n\n# get the output image follow post-processing step from PyTorch implementation\nfinal_img = Image.merge(\n    \"YCbCr\", [\n        img_out_y,\n        img_cb.resize(img_out_y.size, Image.BICUBIC),\n        img_cr.resize(img_out_y.size, Image.BICUBIC),\n    ]).convert(\"RGB\")\n\n# Save the image, we will compare this with the output image from mobile device\nfinal_img.save(\"./_static/img/cat_superres_with_ort.jpg\")\n\n\n\n<img alt=\"output\\_cat\" src=\"../_images/cat_superres_with_ort.jpg\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy->Calculating the historical growth curve for transistors"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Qualitative analysis of the impact of features on linear model predictions"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Modeling non-linear feature interactions with kernels"
            ],
            [
                "pandas_toms_blog->Scaling->Joining"
            ],
            [
                "torch->Deploying PyTorch Models in Production->(optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime->Running the model on an image using ONNX Runtime"
            ]
        ]
    },
    "402972": {
        "jupyter_code_cell": "df[df.index.duplicated()]\nprint(\"Size before dropping dubs =\", df.size)\ndf = df[~df.index.duplicated(keep='first')]\nprint(\"Size after dropping dubs =\", df.size)",
        "matched_tutorial_code_inds": [
            3616,
            3827,
            6704,
            3831,
            6569
        ],
        "matched_tutorial_codes": [
            "hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()",
            "daily = df.fl_date.value_counts().sort_index()\ny = daily.resample('MS').mean()\ny.head()",
            "ax = dta.mean().plot(grid=False)\nax.set_xlabel(\"Year\", size=17)\nax.set_ylabel(\"Fertility rate\", size=17)\nax.set_xlim(0, 51)",
            "X = (pd.concat([y.shift(i) for i in range(6)], axis=1,\n               keys=['y'] + ['L%s' % i for i in range(1, 6)])\n       .dropna())\nX.head()",
            "mod_pre = sm.tsa.arima.ARIMA(y_pre, order=(1, 0, 0), trend='n')\nres_pre = mod_pre.fit()\nprint(res_pre.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Simple univariate example: AR(1)->Step 1: fitting the model on the available dataset"
            ]
        ]
    },
    "334960": {
        "jupyter_code_cell": "X = sm.add_constant(x_train)\nprint(X)\ntoyregr_sm = sm.OLS(y_train, X)\nresults_sm = toyregr_sm.fit()\nbeta0_sm = results_sm.params[0]\nbeta1_sm = results_sm.params[1]\nprint(\"(beta0, beta1) = (%f, %f)\" %(beta0_sm, beta1_sm))\ntoyregr_skl = linear_model.LinearRegression()\nresults_skl = toyregr_skl.fit(x_train,y_train)\nbeta0_skl = toyregr_skl.intercept_\nbeta1_skl = toyregr_skl.coef_[0]\nprint(\"(beta0, beta1) = (%f, %f)\" %(beta0_skl, beta1_skl))",
        "matched_tutorial_code_inds": [
            2826,
            6795,
            3290,
            3148,
            2683
        ],
        "matched_tutorial_codes": [
            "X_train_preprocessed = (\n    model[:-1].transform(X_train), columns=feature_names\n)\n\nX_train_preprocessed.std(axis=0).plot.barh(figsize=(9, 7))\n(\"Feature ranges\")\n(\"Std. dev. of feature values\")\n(left=0.3)\n\n\n<img alt=\"Feature ranges\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_004.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_004.png\"/>",
            "x1n = np.linspace(20.5, 25, 10)\nXnew = np.column_stack((x1n, np.sin(x1n), (x1n - 5) ** 2))\nXnew = sm.add_constant(Xnew)\nynewpred = olsres.predict(Xnew)  # predict out of sample\nprint(ynewpred)",
            "X, y = (\n    n_samples=9,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_classes=3,\n    n_clusters_per_class=1,\n    class_sep=1.0,\n    random_state=0,\n)\n\n(1)\nax = ()\nfor i in range(X.shape[0]):\n    ax.text(X[i, 0], X[i, 1], str(i), va=\"center\", ha=\"center\")\n    ax.scatter(X[i, 0], X[i, 1], s=300, c=cm.Set1(y[[i]]), alpha=0.4)\n\nax.set_title(\"Original points\")\nax.axes.get_xaxis().set_visible(False)\nax.axes.get_yaxis().set_visible(False)\nax.axis(\"equal\")  # so that boundaries are displayed correctly as circles\n\n\ndef link_thickness_i(X, i):\n    diff_embedded = X[i] - X\n    dist_embedded = (\"ij,ij-i\", diff_embedded, diff_embedded)\n    dist_embedded[i] = \n\n    # compute exponentiated distances (use the log-sum-exp trick to\n    # avoid numerical instabilities\n    exp_dist_embedded = (-dist_embedded - (-dist_embedded))\n    return exp_dist_embedded\n\n\ndef relate_point(X, i, ax):\n    pt_i = X[i]\n    for j, pt_j in enumerate(X):\n        thickness = link_thickness_i(X, i)\n        if i != j:\n            line = ([pt_i[0], pt_j[0]], [pt_i[1], pt_j[1]])\n            ax.plot(*line, c=cm.Set1(y[j]), linewidth=5 * thickness[j])\n\n\ni = 3\nrelate_point(X, i, ax)\n()\n\n\n<img alt=\"Original points\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_nca_illustration_001.png\" srcset=\"../../_images/sphx_glr_plot_nca_illustration_001.png\"/>",
            "X, y = (n_samples=8000, random_state=42)\n\n# The scorers can be either one of the predefined metric strings or a scorer\n# callable, like the one returned by make_scorer\nscoring = {\"AUC\": \"roc_auc\", \"Accuracy\": ()}\n\n# Setting refit='AUC', refits an estimator on the whole dataset with the\n# parameter setting that has the best cross-validated AUC score.\n# That estimator is made available at ``gs.best_estimator_`` along with\n# parameters like ``gs.best_score_``, ``gs.best_params_`` and\n# ``gs.best_index_``\ngs = (\n    (random_state=42),\n    param_grid={\"min_samples_split\": range(2, 403, 20)},\n    scoring=scoring,\n    refit=\"AUC\",\n    n_jobs=2,\n    return_train_score=True,\n)\ngs.fit(X, y)\nresults = gs.cv_results_",
            "X, y = (n_samples=200, n_features=5000, random_state=0)\n# create a copy of X in sparse format\nX_sp = (X)\n\nalpha = 1\nsparse_lasso = (alpha=alpha, fit_intercept=False, max_iter=1000)\ndense_lasso = (alpha=alpha, fit_intercept=False, max_iter=1000)\n\nt0 = ()\nsparse_lasso.fit(X_sp, y)\nprint(f\"Sparse Lasso done in {(() - t0):.3f}s\")\n\nt0 = ()\ndense_lasso.fit(X, y)\nprint(f\"Dense Lasso done in {(() - t0):.3f}s\")\n\n# compare the regression coefficients\ncoeff_diff = (sparse_lasso.coef_ - dense_lasso.coef_)\nprint(f\"Distance between coefficients : {coeff_diff:.2e}\")\n\n#"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "statsmodels->Examples->User Notes->Prediction->Create a new sample of explanatory variables Xnew, predict and plot"
            ],
            [
                "sklearn->Examples->Nearest Neighbors->Neighborhood Components Analysis Illustration->Original points"
            ],
            [
                "sklearn->Examples->Model Selection->Demonstration of multi-metric evaluation on cross_val_score and GridSearchCV->Running GridSearchCV using multiple evaluation metrics"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Lasso on dense and sparse data->Comparing the two Lasso implementations on Dense data"
            ]
        ]
    },
    "1421575": {
        "jupyter_code_cell": "dfdate.plot(y='Usage', figsize=(20,5))\ndfweekdaytot = dftot\ndfweekdaytot['Day'] = dftot.index.weekday",
        "matched_tutorial_code_inds": [
            3793,
            5703,
            4067,
            4908,
            3784
        ],
        "matched_tutorial_codes": [
            "sns.barplot(x='cut', y='price', data=df)\nsns.despine()\nplt.tight_layout()",
            "summ_df = infl.summary_frame()\nsumm_df.sort_values(\"cooks_d\", ascending=False)[:10]",
            "tips = sns.load_dataset(\"tips\")\nsns.catplot(data=tips, x=\"day\", y=\"total_bill\")\n",
            "copper = mpl.colormaps['copper'].resampled(8)\n\nprint('copper(range(8))', copper(range(8)))\nprint('copper(np.linspace(0, 1, 8))', copper(np.linspace(0, 1, 8)))",
            "import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Influence Measures for GLMs->get the influence measures"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing categorical data->Categorical scatterplots",
                "seaborn->Plotting functions->Visualizing categorical data->Categorical scatterplots"
            ],
            [
                "matplotlib->Tutorials->Colors->Creating Colormaps in Matplotlib->Getting colormaps and accessing their values->LinearSegmentedColormap"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples"
            ]
        ]
    },
    "287224": {
        "jupyter_code_cell": "deviation = np.abs(trips_per_track_id[\"trip_time_in_minutes\"]) - np.abs(x.iloc[:])\nabsolute_deviation = np.abs(deviation)\nprint(\"Maximum Absolute Deviation: \" + str(max(absolute_deviation)))\nfig , ax = plt.subplots(figsize=(12,5))\nax.bar(range(x.shape[0]), absolute_deviation)\nax.set_title(\"Absolute deviation in trip time from tracks & trackspoints\")\nplt.show();",
        "matched_tutorial_code_inds": [
            1620,
            4501,
            2298,
            5915,
            1685
        ],
        "matched_tutorial_codes": [
            "pixel_intensity_distribution = ndimage.histogram(\n    xray_image, min=np.min(xray_image), max=np.max(xray_image), bins=256\n)\n\nplt.plot(pixel_intensity_distribution)\nplt.title(\"Pixel intensity distribution\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/00c12e163a30744c62700ec414e2d514c5c4eae10256e7cd0cf87553d0f4d34c.png\" src=\"../_images/00c12e163a30744c62700ec414e2d514c5c4eae10256e7cd0cf87553d0f4d34c.png\"/>",
            "quantiles = [0.0, 0.01, 0.05, 0.1, 1-0.10, 1-0.05, 1-0.01, 1.0]\n crit = stats.t.ppf(quantiles, 10)\n crit\narray([       -inf, -2.76376946, -1.81246112, -1.37218364,  1.37218364,\n        1.81246112,  2.76376946,         inf])\n n_sample = x.size\n freqcount = np.histogram(x, bins=crit)[0]\n tprob = np.diff(quantiles)\n nprob = np.diff(stats.norm.cdf(crit))\n tch, tpval = stats.chisquare(freqcount, tprob*n_sample)\n nch, npval = stats.chisquare(freqcount, nprob*n_sample)\n print('chisquare for t:      chi2 = %6.2f pvalue = %6.4f' % (tch, tpval))\nchisquare for t:      chi2 =  2.30 pvalue = 0.8901  # random\n print('chisquare for normal: chi2 = %6.2f pvalue = %6.4f' % (nch, npval))\nchisquare for normal: chi2 = 64.60 pvalue = 0.0000  # random",
            "feature_importance = reg.feature_importances_\nsorted_idx = (feature_importance)\npos = (sorted_idx.shape[0]) + 0.5\nfig = (figsize=(12, 6))\n(1, 2, 1)\n(pos, feature_importance[sorted_idx], align=\"center\")\n(pos, (diabetes.feature_names)[sorted_idx])\n(\"Feature Importance (MDI)\")\n\nresult = (\n    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_idx = result.importances_mean.argsort()\n(1, 2, 2)\n(\n    result.importances[sorted_idx].T,\n    vert=False,\n    labels=(diabetes.feature_names)[sorted_idx],\n)\n(\"Permutation Importance (test set)\")\nfig.tight_layout()\n()\n\n\n<img alt=\"Feature Importance (MDI), Permutation Importance (test set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\"/>",
            "resid = interM_lm32.get_influence().summary_frame()[\"standard_resid\"]\n\nplt.figure(figsize=(6, 6))\nresid = resid.reindex(X.index)\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X.loc[idx],\n        resid.loc[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\nplt.xlabel(\"X[~[32]]\")\nplt.ylabel(\"standardized resids\")",
            "pollutant_data = np.loadtxt(\"air-quality-data.csv\", dtype=float, delimiter=\",\",\n                            skiprows=1, usecols=range(1, 8))\npollutants_A = pollutant_data[:, 0:5]\npollutants_B = pollutant_data[:, 5:]\n\nprint(pollutants_A.shape)\nprint(pollutants_B.shape)"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Applications->X-ray image processing->Apply masks to X-rays with np.where()"
            ],
            [
                "scipy->Statistics (scipy.stats)->Analysing one sample->Tails of the distribution"
            ],
            [
                "sklearn->Examples->Ensemble methods->Gradient Boosting regression->Plot feature importance"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Building the dataset"
            ]
        ]
    },
    "389299": {
        "jupyter_code_cell": "sns.clustermap(dayHour,cmap='coolwarm')\ndayMonth = df.groupby(by=['Dayofweek','Month']).count()['Reason'].unstack()\ndayMonth.head()",
        "matched_tutorial_code_inds": [
            5703,
            3784,
            3610,
            3794,
            3834
        ],
        "matched_tutorial_codes": [
            "summ_df = infl.summary_frame()\nsumm_df.sort_values(\"cooks_d\", ascending=False)[:10]",
            "import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "first = df.groupby('unique_carrier').first()\nfirst.ix[10:15, ['fl_date', 'tail_num']]",
            "sns.jointplot(x='carat', y='price', data=df, size=8, alpha=.25,\n              color='k', marker='.')\nplt.tight_layout()",
            "ax = res_lagged.params.drop(['Intercept', 'trend']).plot.bar(rot=0)\nplt.ylabel('Coefficeint')\nsns.despine()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Generalized Linear Models->Influence Measures for GLMs->get the influence measures"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ]
        ]
    },
    "1226615": {
        "jupyter_code_cell": "adata = sc.read(results_file)\nrcParams['axes.linewidth'] = 0.5\nsc.settings.set_figure_params(dpi=150)\n_, pos = sc.pl.aga_graph(\n    adata,\n    title='abstracted graph and lineage tree',\n    root='neoblast 1',\n    layout='rt_circular',\n    node_size_scale=0.5,\n    node_size_power=0.9,\n    max_edge_width=0.7,\n    fontsize=3.5,\n    return_pos=True)\ntheta = np.radians(150)\nc, s = np.cos(theta), np.sin(theta)\nr = np.array([[c, -s], [s, c]])\npos = r.dot(pos.T).T",
        "matched_tutorial_code_inds": [
            4654,
            2116,
            2119,
            2120,
            23
        ],
        "matched_tutorial_codes": [
            "data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>",
            "fa_estimator = (n_components=n_components, max_iter=20)\nfa_estimator.fit(faces_centered)\nplot_gallery(\"Factor Analysis (FA)\", fa_estimator.components_[:n_components])\n\n# --- Pixelwise variance\n(figsize=(3.2, 3.6), facecolor=\"white\", tight_layout=True)\nvec = fa_estimator.noise_variance_\nvmax = max(vec.max(), -vec.min())\n(\n    vec.reshape(image_shape),\n    cmap=plt.cm.gray,\n    interpolation=\"nearest\",\n    vmin=-vmax,\n    vmax=vmax,\n)\n(\"off\")\n(\"Pixelwise variance from \\n Factor Analysis (FA)\", size=16, wrap=True)\n(orientation=\"horizontal\", shrink=0.8, pad=0.03)\n()\n\n\n\n<img alt=\"Factor Analysis (FA)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_008.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_008.png\"/>\n<img alt=\"Pixelwise variance from   Factor Analysis (FA)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_009.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_009.png\"/>",
            "dict_pos_code_estimator = (\n    n_components=n_components,\n    alpha=0.1,\n    max_iter=50,\n    batch_size=3,\n    fit_algorithm=\"cd\",\n    random_state=rng,\n    positive_code=True,\n)\ndict_pos_code_estimator.fit(faces_centered)\nplot_gallery(\n    \"Dictionary learning - positive code\",\n    dict_pos_code_estimator.components_[:n_components],\n    cmap=plt.cm.RdBu,\n)\n\n\n<img alt=\"Dictionary learning - positive code\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_012.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_012.png\"/>",
            "dict_pos_estimator = (\n    n_components=n_components,\n    alpha=0.1,\n    max_iter=50,\n    batch_size=3,\n    fit_algorithm=\"cd\",\n    random_state=rng,\n    positive_dict=True,\n    positive_code=True,\n)\ndict_pos_estimator.fit(faces_centered)\nplot_gallery(\n    \"Dictionary learning - positive dictionary & code\",\n    dict_pos_estimator.components_[:n_components],\n    cmap=plt.cm.RdBu,\n)\n\n\n<img alt=\"Dictionary learning - positive dictionary & code\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_013.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_013.png\"/>",
            "face_dataset = FaceLandmarksDataset(csv_file='faces/face_landmarks.csv',\n                                    root_dir='faces/')\n\nfig = plt.figure()\n\nfor i in range(len(face_dataset)):\n    sample = face_dataset[i]\n\n    print(i, sample['image'].shape, sample['landmarks'].shape)\n\n    ax = plt.subplot(1, 4, i + 1)\n    plt.tight_layout()\n    ax.set_title('Sample #{}'.format(i))\n    ax.axis('off')\n    show_landmarks(**sample)\n\n    if i == 3:\n        plt.show()\n        break"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings"
            ],
            [
                "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition->Factor Analysis components - FA"
            ],
            [
                "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition: Dictionary learning->Dictionary learning - positive code"
            ],
            [
                "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition: Dictionary learning->Dictionary learning - positive dictionary & code"
            ],
            [
                "torch->PyTorch Recipes->Developing Custom PyTorch Dataloaders->Part 1: The Dataset->1.3 Iterate through data samples"
            ]
        ]
    },
    "548289": {
        "jupyter_code_cell": "scipy.stats.mstats.normaltest(temp, axis=0)\nscipy.stats.ttest_1samp(temp, 98.6, axis=0)",
        "matched_tutorial_code_inds": [
            5548,
            2309,
            3802,
            3071,
            4464
        ],
        "matched_tutorial_codes": [
            "kde = sm.nonparametric.KDEUnivariate(obs_dist)\nkde.fit()",
            "gbdt_no_cst = ()\ngbdt_no_cst.fit(X, y)",
            "scores = pd.DataFrame(est.cv_results_)\nscores.head()",
            "ir = (out_of_bounds=\"clip\")\ny_ = ir.fit_transform(x, y)\n\nlr = ()\nlr.fit(x[:, ], y)  # x needs to be 2d for LinearRegression",
            "from scipy.stats import gamma\n gamma.numargs\n1\n gamma.shapes\n'a'"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->A more difficult case"
            ],
            [
                "sklearn->Examples->Ensemble methods->Monotonic Constraints"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "sklearn->Examples->Miscellaneous->Isotonic Regression"
            ],
            [
                "scipy->Statistics (scipy.stats)->Random variables->Shape parameters"
            ]
        ]
    },
    "991195": {
        "jupyter_code_cell": "del my_title, rs\nrelease_dates.head()\npd.Series(pd.DatetimeIndex(pd.merge(    cast.loc[cast.name == 'Tom Cruise'].loc[:,['title']],    release_dates.loc[release_dates.country == 'USA'],    on='title').date).month).value_counts().sort_index().plot.bar()",
        "matched_tutorial_code_inds": [
            2446,
            2463,
            6714,
            6401,
            6422
        ],
        "matched_tutorial_codes": [
            "last_hours = slice(-96, None)\nfig, ax = (figsize=(12, 4))\nfig.suptitle(\"Predictions by linear models\")\nax.plot(\n    y.iloc[test_0].values[last_hours],\n    \"x-\",\n    alpha=0.2,\n    label=\"Actual demand\",\n    color=\"black\",\n)\nax.plot(naive_linear_predictions[last_hours], \"x-\", label=\"Ordinal time features\")\nax.plot(\n    cyclic_cossin_linear_predictions[last_hours],\n    \"x-\",\n    label=\"Trigonometric time features\",\n)\nax.plot(\n    cyclic_spline_linear_predictions[last_hours],\n    \"x-\",\n    label=\"Spline-based time features\",\n)\nax.plot(\n    one_hot_linear_predictions[last_hours],\n    \"x-\",\n    label=\"One-hot time features\",\n)\n_ = ax.legend()\n\n\n<img alt=\"Predictions by linear models\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_006.png\" srcset=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_006.png\"/>",
            "last_hours = slice(-96, None)\nfig, ax = (figsize=(12, 4))\nfig.suptitle(\"Predictions by non-linear regression models\")\nax.plot(\n    y.iloc[test_0].values[last_hours],\n    \"x-\",\n    alpha=0.2,\n    label=\"Actual demand\",\n    color=\"black\",\n)\nax.plot(\n    gbrt_predictions[last_hours],\n    \"x-\",\n    label=\"Gradient Boosted Trees\",\n)\nax.plot(\n    one_hot_poly_predictions[last_hours],\n    \"x-\",\n    label=\"One-hot + polynomial kernel\",\n)\nax.plot(\n    cyclic_spline_poly_predictions[last_hours],\n    \"x-\",\n    label=\"Splines + polynomial kernel\",\n)\n_ = ax.legend()\n\n\n<img alt=\"Predictions by non-linear regression models\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_007.png\" srcset=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_007.png\"/>",
            "make_plot(dta.index[idx[:5]])\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_24_0.png\" src=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_24_0.png\"/>",
            "exog = endog['dln_consump']\nmod = sm.tsa.VARMAX(endog[['dln_inv', 'dln_inc']], order=(2,0), trend='n', exog=exog)\nres = mod.fit(maxiter=1000, disp=False)\nprint(res.summary())",
            "dusphci = usphci.diff()[1:].values\ndef compute_coincident_index(mod, res):\n    # Estimate W(1)\n    spec = res.specification\n    design = mod.ssm['design']\n    transition = mod.ssm['transition']\n    ss_kalman_gain = res.filter_results.kalman_gain[:,:,-1]\n    k_states = ss_kalman_gain.shape[0]\n\n    W1 = np.linalg.inv(np.eye(k_states) - np.dot(\n        np.eye(k_states) - np.dot(ss_kalman_gain, design),\n        transition\n    )).dot(ss_kalman_gain)[0]\n\n    # Compute the factor mean vector\n    factor_mean = np.dot(W1, dta.loc['1972-02-01':, 'dln_indprod':'dln_emp'].mean())\n\n    # Normalize the factors\n    factor = res.factors.filtered[0]\n    factor *= np.std(usphci.diff()[1:]) / np.std(factor)\n\n    # Compute the coincident index\n    coincident_index = np.zeros(mod.nobs+1)\n    # The initial value is arbitrary; here it is set to\n    # facilitate comparison\n    coincident_index[0] = usphci.iloc[0] * factor_mean / dusphci.mean()\n    for t in range(0, mod.nobs):\n        coincident_index[t+1] = coincident_index[t] + factor[t] + factor_mean\n\n    # Attach dates\n    coincident_index = pd.Series(coincident_index, index=dta.index).iloc[1:]\n\n    # Normalize to use the same base year as USPHCI\n    coincident_index *= (usphci.loc['1992-07-01'] / coincident_index.loc['1992-07-01'])\n\n    return coincident_index"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Qualitative analysis of the impact of features on linear model predictions"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Modeling non-linear feature interactions with kernels"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "statsmodels->Examples->State space models->VARMAX: Introduction->Example 1: VAR"
            ],
            [
                "statsmodels->Examples->State space models->Dynamic Factor Models: Application->Coincident Index"
            ]
        ]
    },
    "225031": {
        "jupyter_code_cell": "cik_lookup = {\n    'AMZN': '0001018724',\n    'BMY': '0000014272',   \n    'CNP': '0001130310',\n    'CVX': '0000093410',\n    'FL': '0000850209',\n    'FRT': '0000034903',\n    'HON': '0000773840'}\nadditional_cik = {\n    'AEP': '0000004904',\n    'AXP': '0000004962',\n    'BA': '0000012927', \n    'BK': '0001390777',\n    'CAT': '0000018230',\n    'DE': '0000315189',\n    'DIS': '0001001039',\n    'DTE': '0000936340',\n    'ED': '0001047862',\n    'EMR': '0000032604',\n    'ETN': '0001551182',\n    'GE': '0000040545',\n    'IBM': '0000051143',\n    'IP': '0000051434',\n    'JNJ': '0000200406',\n    'KO': '0000021344',\n    'LLY': '0000059478',\n    'MCD': '0000063908',\n    'MO': '0000764180',\n    'MRK': '0000310158',\n    'MRO': '0000101778',\n    'PCG': '0001004980',\n    'PEP': '0000077476',\n    'PFE': '0000078003',\n    'PG': '0000080424',\n    'PNR': '0000077360',\n    'SYY': '0000096021',\n    'TXN': '0000097476',\n    'UTX': '0000101829',\n    'WFC': '0000072971',\n    'WMT': '0000104169',\n    'WY': '0000106535',\n    'XOM': '0000034088'}\nsec_api = project_helper.SecAPI()",
        "matched_tutorial_code_inds": [
            220,
            5802,
            5800,
            6646,
            2120
        ],
        "matched_tutorial_codes": [
            "labels_map = {\n    0: \"T-Shirt\",\n    1: \"Trouser\",\n    2: \"Pullover\",\n    3: \"Dress\",\n    4: \"Coat\",\n    5: \"Sandal\",\n    6: \"Shirt\",\n    7: \"Sneaker\",\n    8: \"Bag\",\n    9: \"Ankle Boot\",\n}\nfigure = plt.figure(figsize=(8, 8))\ncols, rows = 3, 3\nfor i in range(1, cols * rows + 1):\n    sample_idx = (len(), size=(1,)).item()\n    ,  = [sample_idx]\n    figure.add_subplot(rows, cols, i)\n    plt.title(labels_map[])\n    plt.axis(\"off\")\n    plt.imshow(.squeeze(), cmap=\"gray\")\nplt.show()\n\n\n<img alt=\"Pullover, Sandal, Bag, Bag, Sneaker, Bag, Pullover, Sandal, Sandal\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_data_tutorial_001.png\" srcset=\"../../_images/sphx_glr_data_tutorial_001.png\"/>",
            "student_resid   unadj_p  fdr_bh(p)\nminister                 3.134519  0.003177   0.142974\nreporter                -2.397022  0.021170   0.476332\ncontractor               2.043805  0.047433   0.596233\ninsurance.agent         -1.930919  0.060428   0.596233\nmachinist                1.887047  0.066248   0.596233\nstore.clerk             -1.760491  0.085783   0.616782\nconductor               -1.704032  0.095944   0.616782\nfactory.owner            1.602429  0.116738   0.656653\nmail.carrier            -1.433249  0.159369   0.796844\nstreetcar.motorman      -1.104485  0.275823   0.999436\ncarpenter                1.068858  0.291386   0.999436\ncoal.miner               1.018527  0.314400   0.999436\nbartender               -0.902422  0.372104   0.999436\nbookkeeper              -0.902388  0.372122   0.999436\nsoda.clerk              -0.883095  0.382334   0.999436\nchemist                  0.826578  0.413261   0.999436\nRR.engineer              0.808922  0.423229   0.999436\nprofessor                0.768277  0.446725   0.999436\nelectrician              0.731949  0.468363   0.999436\ngas.stn.attendant       -0.666596  0.508764   0.999436\nauto.repairman           0.522735  0.603972   0.999436\nwatchman                -0.513502  0.610357   0.999436\nbanker                   0.508388  0.613906   0.999436\nmachine.operator         0.499922  0.619802   0.999436\ndentist                 -0.498082  0.621088   0.999436\nwaiter                  -0.475972  0.636621   0.999436\nshoe.shiner             -0.429357  0.669912   0.999436\nwelfare.worker          -0.411406  0.682918   0.999436\nplumber                 -0.377954  0.707414   0.999436\nphysician                0.355687  0.723898   0.999436\npilot                    0.340920  0.734905   0.999436\nengineer                 0.306225  0.760983   0.999436\naccountant               0.303900  0.762741   0.999436\nlawyer                  -0.303082  0.763360   0.999436\nundertaker              -0.187339  0.852319   0.999436\nbarber                   0.173805  0.862874   0.999436\nstore.manager            0.142425  0.887442   0.999436\ntruck.driver            -0.129227  0.897810   0.999436\ncook                     0.127207  0.899399   0.999436\njanitor                 -0.079890  0.936713   0.999436\npoliceman                0.078847  0.937538   0.999436\narchitect                0.072256  0.942750   0.999436\nteacher                  0.050510  0.959961   0.999436\ntaxi.driver              0.023322  0.981507   0.999436\nauthor                   0.000711  0.999436   0.999436",
            "student_resid   unadj_p  sidak(p)\nminister                 3.134519  0.003177  0.133421\nreporter                -2.397022  0.021170  0.618213\ncontractor               2.043805  0.047433  0.887721\ninsurance.agent         -1.930919  0.060428  0.939485\nmachinist                1.887047  0.066248  0.954247\nstore.clerk             -1.760491  0.085783  0.982331\nconductor               -1.704032  0.095944  0.989315\nfactory.owner            1.602429  0.116738  0.996250\nmail.carrier            -1.433249  0.159369  0.999595\nstreetcar.motorman      -1.104485  0.275823  1.000000\ncarpenter                1.068858  0.291386  1.000000\ncoal.miner               1.018527  0.314400  1.000000\nbartender               -0.902422  0.372104  1.000000\nbookkeeper              -0.902388  0.372122  1.000000\nsoda.clerk              -0.883095  0.382334  1.000000\nchemist                  0.826578  0.413261  1.000000\nRR.engineer              0.808922  0.423229  1.000000\nprofessor                0.768277  0.446725  1.000000\nelectrician              0.731949  0.468363  1.000000\ngas.stn.attendant       -0.666596  0.508764  1.000000\nauto.repairman           0.522735  0.603972  1.000000\nwatchman                -0.513502  0.610357  1.000000\nbanker                   0.508388  0.613906  1.000000\nmachine.operator         0.499922  0.619802  1.000000\ndentist                 -0.498082  0.621088  1.000000\nwaiter                  -0.475972  0.636621  1.000000\nshoe.shiner             -0.429357  0.669912  1.000000\nwelfare.worker          -0.411406  0.682918  1.000000\nplumber                 -0.377954  0.707414  1.000000\nphysician                0.355687  0.723898  1.000000\npilot                    0.340920  0.734905  1.000000\nengineer                 0.306225  0.760983  1.000000\naccountant               0.303900  0.762741  1.000000\nlawyer                  -0.303082  0.763360  1.000000\nundertaker              -0.187339  0.852319  1.000000\nbarber                   0.173805  0.862874  1.000000\nstore.manager            0.142425  0.887442  1.000000\ntruck.driver            -0.129227  0.897810  1.000000\ncook                     0.127207  0.899399  1.000000\njanitor                 -0.079890  0.936713  1.000000\npoliceman                0.078847  0.937538  1.000000\narchitect                0.072256  0.942750  1.000000\nteacher                  0.050510  0.959961  1.000000\ntaxi.driver              0.023322  0.981507  1.000000\nauthor                   0.000711  0.999436  1.000000",
            "austourists_data = [\n    30.05251300,\n    19.14849600,\n    25.31769200,\n    27.59143700,\n    32.07645600,\n    23.48796100,\n    28.47594000,\n    35.12375300,\n    36.83848500,\n    25.00701700,\n    30.72223000,\n    28.69375900,\n    36.64098600,\n    23.82460900,\n    29.31168300,\n    31.77030900,\n    35.17787700,\n    19.77524400,\n    29.60175000,\n    34.53884200,\n    41.27359900,\n    26.65586200,\n    28.27985900,\n    35.19115300,\n    42.20566386,\n    24.64917133,\n    32.66733514,\n    37.25735401,\n    45.24246027,\n    29.35048127,\n    36.34420728,\n    41.78208136,\n    49.27659843,\n    31.27540139,\n    37.85062549,\n    38.83704413,\n    51.23690034,\n    31.83855162,\n    41.32342126,\n    42.79900337,\n    55.70835836,\n    33.40714492,\n    42.31663797,\n    45.15712257,\n    59.57607996,\n    34.83733016,\n    44.84168072,\n    46.97124960,\n    60.01903094,\n    38.37117851,\n    46.97586413,\n    50.73379646,\n    61.64687319,\n    39.29956937,\n    52.67120908,\n    54.33231689,\n    66.83435838,\n    40.87118847,\n    51.82853579,\n    57.49190993,\n    65.25146985,\n    43.06120822,\n    54.76075713,\n    59.83447494,\n    73.25702747,\n    47.69662373,\n    61.09776802,\n    66.05576122,\n]\nindex = pd.date_range(\"1999-03-01\", \"2015-12-01\", freq=\"3MS\")\naustourists = pd.Series(austourists_data, index=index)\naustourists.plot()\nplt.ylabel(\"Australian Tourists\")",
            "dict_pos_estimator = (\n    n_components=n_components,\n    alpha=0.1,\n    max_iter=50,\n    batch_size=3,\n    fit_algorithm=\"cd\",\n    random_state=rng,\n    positive_dict=True,\n    positive_code=True,\n)\ndict_pos_estimator.fit(faces_centered)\nplot_gallery(\n    \"Dictionary learning - positive dictionary & code\",\n    dict_pos_estimator.components_[:n_components],\n    cmap=plt.cm.RdBu,\n)\n\n\n<img alt=\"Dictionary learning - positive dictionary & code\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_013.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_013.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Introduction to PyTorch->Datasets & DataLoaders->Iterating and Visualizing the Dataset"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Duncan\u2019s Occupational Prestige data - M-estimation for outliers"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Duncan\u2019s Occupational Prestige data - M-estimation for outliers"
            ],
            [
                "statsmodels->Examples->State space models->ETS models->Holt-Winters\u2019 seasonal method"
            ],
            [
                "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition: Dictionary learning->Dictionary learning - positive dictionary & code"
            ]
        ]
    },
    "836642": {
        "jupyter_code_cell": "import pandas as pd \nimport matplotlib.pyplot as plt \nfrom mpl_toolkits.basemap import Basemap ",
        "matched_tutorial_code_inds": [
            6035,
            3993,
            5161,
            6117,
            4934
        ],
        "matched_tutorial_codes": [
            "import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom statsmodels.stats.mediation import Mediation",
            "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\nnp.random.seed(9876789)",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom statsmodels.tsa.arima.model import ARIMA",
            "import numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom colorspacious import cspace_converter"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->Mediation analysis with duration data"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships",
                "seaborn->Plotting functions->Visualizing statistical relationships"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data"
            ],
            [
                "matplotlib->Tutorials->Colors->Choosing Colormaps in Matplotlib->Classes of colormaps"
            ]
        ]
    },
    "391995": {
        "jupyter_code_cell": "df.head()\nplt.figure(figsize=(10,6))\ndf[df['credit.policy'] == 0]['fico'].hist(bins=30, alpha=0.5,label='Credit.Policy=0', color='blue')\ndf[df['credit.policy'] == 1]['fico'].hist(bins=30, alpha=0.5, label='Credit.Policy=1', color='red')\nplt.legend()\nplt.xlabel('FICO')",
        "matched_tutorial_code_inds": [
            3863,
            3773,
            4905,
            5900,
            2996
        ],
        "matched_tutorial_codes": [
            "df.visualize(rankdir='LR')",
            "df = df.assign(away_strength=df['away_team'].map(win_percent),\n               home_strength=df['home_team'].map(win_percent),\n               point_diff=df['home_points'] - df['away_points'],\n               rest_diff=df['home_rest'] - df['away_rest'])\ndf.head()",
            "viridis.colors [[0.267004 0.004874 0.329415 1.      ]\n [0.275191 0.194905 0.496005 1.      ]\n [0.212395 0.359683 0.55171  1.      ]\n [0.153364 0.497    0.557724 1.      ]\n [0.122312 0.633153 0.530398 1.      ]\n [0.288921 0.758394 0.428426 1.      ]\n [0.626579 0.854645 0.223353 1.      ]\n [0.993248 0.906157 0.143936 1.      ]]\nviridis(range(8)) [[0.267004 0.004874 0.329415 1.      ]\n [0.275191 0.194905 0.496005 1.      ]\n [0.212395 0.359683 0.55171  1.      ]\n [0.153364 0.497    0.557724 1.      ]\n [0.122312 0.633153 0.530398 1.      ]\n [0.288921 0.758394 0.428426 1.      ]\n [0.626579 0.854645 0.223353 1.      ]\n [0.993248 0.906157 0.143936 1.      ]]\nviridis(np.linspace(0, 1, 8)) [[0.267004 0.004874 0.329415 1.      ]\n [0.275191 0.194905 0.496005 1.      ]\n [0.212395 0.359683 0.55171  1.      ]\n [0.153364 0.497    0.557724 1.      ]\n [0.122312 0.633153 0.530398 1.      ]\n [0.288921 0.758394 0.428426 1.      ]\n [0.626579 0.854645 0.223353 1.      ]\n [0.993248 0.906157 0.143936 1.      ]]",
            "df_infl = infl.summary_frame()",
            "tree_disp.plot(line_kw={\"label\": \"Decision Tree\"})\nmlp_disp.plot(\n    line_kw={\"label\": \"Multi-layer Perceptron\", \"color\": \"red\"}, ax=tree_disp.axes_\n)\ntree_disp.figure_.set_size_inches(10, 6)\ntree_disp.axes_[0, 0].legend()\ntree_disp.axes_[0, 1].legend()\n()\n\n\n<img alt=\"plot partial dependence visualization api\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_partial_dependence_visualization_api_005.png\" srcset=\"../../_images/sphx_glr_plot_partial_dependence_visualization_api_005.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Scaling->Dask"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team"
            ],
            [
                "matplotlib->Tutorials->Colors->Creating Colormaps in Matplotlib->Getting colormaps and accessing their values->ListedColormap"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "sklearn->Examples->Miscellaneous->Advanced Plotting With Partial Dependence->Plotting partial dependence of the two models together"
            ]
        ]
    },
    "1492338": {
        "jupyter_code_cell": "plt.plot(range(4), np.cumsum(pca.explained_variance_ratio_))\nplt.title(\"Cumulative variance explained by PCA\")\nplt.xlabel(\"number of component\")\nplt.ylabel(\"Variance explained\")\nprint(\"Cumulative variance for each component\", np.cumsum(pca.explained_variance_ratio_))\npca = PCA(n_components=2, random_state=2)\nreduced_data = pca.fit_transform(X_train_scaled)\nreduced_samples = pca.transform(samples_scaled)\nreduced_data_df = pd.DataFrame(reduced_data, columns = [\"PCA 1\", \"PCA 2\"])",
        "matched_tutorial_code_inds": [
            4372,
            4374,
            1432,
            4652,
            4827
        ],
        "matched_tutorial_codes": [
            "plt.semilogy(f, Pper_spec)\n plt.xlabel('frequency [Hz]')\n plt.ylabel('PSD')\n plt.grid()\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code displays a single X-Y log-linear plot with the power spectral density on the Y axis vs frequency on the X axis. A single blue trace shows a noise floor with a power level of 1e-3 with a single peak at 1270 Hz up to a power of 1. The noise floor measurements appear noisy and oscillate down to 1e-7.\"' class=\"plot-directive\" src=\"../_images/signal-8.png\"/>\n</figure>",
            "plt.semilogy(f, Pwelch_spec)\n plt.xlabel('frequency [Hz]')\n plt.ylabel('PSD')\n plt.grid()\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code displays a single X-Y log-linear plot with the power spectral density on the Y axis vs frequency on the X axis. A single blue trace shows a smooth noise floor at a power level of 6e-2 with a single peak up to a power level of 2 at 1270 Hz.\"' class=\"plot-directive\" src=\"../_images/signal-9.png\"/>\n</figure>",
            "plt.plot(s)\nplt.show()\n\n\n\n\n<img alt=\"../_images/b3c5a64421a25e9b8f34f2d5e71f750a53e3ef7b513a82271d088e5c12a26308.png\" src=\"../_images/b3c5a64421a25e9b8f34f2d5e71f750a53e3ef7b513a82271d088e5c12a26308.png\"/>",
            "plt.plot([1, 2, 3, 4], [1, 4, 9, 16], 'ro')\nplt.axis([0, 6, 0, 20])\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_003.png\" srcset=\"../../_images/sphx_glr_pyplot_003.png, ../../_images/sphx_glr_pyplot_003_2_0x.png 2.0x\"/>",
            "plt.close('all')\narr = np.arange(100).reshape((10, 10))\nfig = plt.figure(figsize=(4, 4))\nim = plt.imshow(arr, interpolation=\"none\")\n\nplt.colorbar(im)\n\nplt.tight_layout()\n\n\n<img alt=\"tight layout guide\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_tight_layout_guide_014.png\" srcset=\"../../_images/sphx_glr_tight_layout_guide_014.png, ../../_images/sphx_glr_tight_layout_guide_014_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "scipy->Signal Processing (scipy.signal)->Spectral Analysis->Periodogram Measurements"
            ],
            [
                "scipy->Signal Processing (scipy.signal)->Spectral Analysis->Spectral Analysis using Welch\u00e2\u0080\u0099s Method"
            ],
            [
                "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Approximation"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Introduction to pyplot->Formatting the style of your plot"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Tight Layout guide->Colorbar"
            ]
        ]
    },
    "413568": {
        "jupyter_code_cell": "pylab.rcParams['figure.figsize'] = (6.0, 6.0)\nloans_df[loans_df.country == 'Chile'].sector.value_counts().plot.bar(color='cornflowerblue');\nplt.title(\"Loan Count by Sector in Chile\")\nplt.xlabel(\"Sector\")\nplt.ylabel(\"Loan Count\")\ntime_to_fund = (loans_df.funded_time - loans_df.posted_time)\ntime_to_fund_in_days = (time_to_fund.astype('timedelta64[s]')/(3600 * 24))\nloans_df = loans_df.assign(time_to_fund=time_to_fund)\nloans_df = loans_df.assign(time_to_fund_in_days=time_to_fund_in_days)",
        "matched_tutorial_code_inds": [
            4796,
            4795,
            4691,
            4820,
            4827
        ],
        "matched_tutorial_codes": [
            "plt.rcParams['figure.constrained_layout.use'] = False\nfig = plt.figure(layout=\"constrained\")\n\ngs1 = gridspec.GridSpec(2, 1, figure=fig)\nax1 = fig.add_subplot(gs1[0])\nax2 = fig.add_subplot(gs1[1])\n\nexample_plot(ax1)\nexample_plot(ax2)\n\n\n<img alt=\"Title, Title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_constrainedlayout_guide_019.png\" srcset=\"../../_images/sphx_glr_constrainedlayout_guide_019.png, ../../_images/sphx_glr_constrainedlayout_guide_019_2_0x.png 2.0x\"/>",
            "plt.rcParams['figure.constrained_layout.use'] = True\nfig, axs = plt.subplots(2, 2, figsize=(3, 3))\nfor ax in axs.flat:\n    example_plot(ax)\n\n\n<img alt=\"Title, Title, Title, Title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_constrainedlayout_guide_018.png\" srcset=\"../../_images/sphx_glr_constrainedlayout_guide_018.png, ../../_images/sphx_glr_constrainedlayout_guide_018_2_0x.png 2.0x\"/>",
            "plt.rcParams.update({'figure.autolayout': True})\n\nfig, ax = plt.subplots()\nax.barh(group_names, group_data)\nlabels = ax.get_xticklabels()\nplt.setp(labels, rotation=45, horizontalalignment='right')\n\n\n<img alt=\"lifecycle\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_lifecycle_006.png\" srcset=\"../../_images/sphx_glr_lifecycle_006.png, ../../_images/sphx_glr_lifecycle_006_2_0x.png 2.0x\"/>",
            "plt.close('all')\nfig = plt.figure()\n\nax1 = plt.subplot2grid((3, 3), (0, 0))\nax2 = plt.subplot2grid((3, 3), (0, 1), colspan=2)\nax3 = plt.subplot2grid((3, 3), (1, 0), colspan=2, rowspan=2)\nax4 = plt.subplot2grid((3, 3), (1, 2), rowspan=2)\n\nexample_plot(ax1)\nexample_plot(ax2)\nexample_plot(ax3)\nexample_plot(ax4)\n\nplt.tight_layout()\n\n\n<img alt=\"Title, Title, Title, Title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_tight_layout_guide_007.png\" srcset=\"../../_images/sphx_glr_tight_layout_guide_007.png, ../../_images/sphx_glr_tight_layout_guide_007_2_0x.png 2.0x\"/>",
            "plt.close('all')\narr = np.arange(100).reshape((10, 10))\nfig = plt.figure(figsize=(4, 4))\nim = plt.imshow(arr, interpolation=\"none\")\n\nplt.colorbar(im)\n\nplt.tight_layout()\n\n\n<img alt=\"tight layout guide\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_tight_layout_guide_014.png\" srcset=\"../../_images/sphx_glr_tight_layout_guide_014.png, ../../_images/sphx_glr_tight_layout_guide_014_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Intermediate->Constrained Layout Guide->Use with GridSpec"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Constrained Layout Guide->rcParams"
            ],
            [
                "matplotlib->Tutorials->Introductory->The Lifecycle of a Plot->Customizing the plot"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Tight Layout guide->Simple Example"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Tight Layout guide->Colorbar"
            ]
        ]
    },
    "197186": {
        "jupyter_code_cell": "from time import time\nt0 = time()\ndendrogram = g.community_walktrap()\nclusters = dendrogram.as_clustering()\nmembership = clusters.membership\nnodesDF[\"cluster\"] = membership\nprint(\"Modularity:\", np.round(clusters.modularity,2))\nprint(\"%s : %.2fs\" % (\"Community walk trap clustering\\n\", time() - t0))\ndisplay(nodesDF.sample(5))\ndf_to_json_save(nodesDF, edgesDF)\nfrom sklearn.cluster import spectral_clustering\nobj = spectral_clustering(np.matrix(A.data), 5)",
        "matched_tutorial_code_inds": [
            1849,
            288,
            3241,
            5889,
            1864
        ],
        "matched_tutorial_codes": [
            "from tempfile import \nfrom sklearn.neighbors import \nfrom sklearn.manifold import \nfrom sklearn.pipeline import \n\nX, y = (random_state=0)\n\nwith (prefix=\"sklearn_cache_\") as tmpdir:\n    estimator = (\n        (n_neighbors=10, mode=\"distance\"),\n        (n_neighbors=10, metric=\"precomputed\"),\n        memory=tmpdir,\n    )\n    estimator.fit(X)\n\n    # We can decrease the number of neighbors and the graph will not be\n    # recomputed.\n    estimator.set_params(isomap__n_neighbors=5)\n    estimator.fit(X)",
            "from matplotlib import pyplot\nimport numpy as np\n\npyplot.imshow([0].reshape((28, 28)), cmap=\"gray\")\nprint(.shape)\n\n\n<img alt=\"nn tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_nn_tutorial_001.png\" srcset=\"../_images/sphx_glr_nn_tutorial_001.png\"/>",
            "from itertools import \nfrom math import \n\nn_comparisons = (len(model_scores)) / (\n    (2) * (len(model_scores) - 2)\n)\npairwise_t_test = []\n\nfor model_i, model_k in (range(len(model_scores)), 2):\n    model_i_scores = model_scores.iloc[model_i].values\n    model_k_scores = model_scores.iloc[model_k].values\n    differences = model_i_scores - model_k_scores\n    t_stat, p_val = compute_corrected_ttest(differences, df, n_train, n_test)\n    p_val *= n_comparisons  # implement Bonferroni correction\n    # Bonferroni can output p-values higher than 1\n    p_val = 1 if p_val  1 else p_val\n    pairwise_t_test.append(\n        [model_scores.index[model_i], model_scores.index[model_k], t_stat, p_val]\n    )\n\npairwise_comp_df = (\n    pairwise_t_test, columns=[\"model_1\", \"model_2\", \"t_stat\", \"p_val\"]\n).round(3)\npairwise_comp_df\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "from urllib.request import urlopen\nimport numpy as np\n\nnp.set_printoptions(precision=4, suppress=True)\n\nimport pandas as pd\n\npd.set_option(\"display.width\", 100)\nimport matplotlib.pyplot as plt\nfrom statsmodels.formula.api import ols\nfrom statsmodels.graphics.api import interaction_plot, abline_plot\nfrom statsmodels.stats.anova import anova_lm\n\ntry:\n    salary_table = pd.read_csv(\"salary.table\")\nexcept:  # recent pandas can read URL without urlopen\n    url = \"http://stats191.stanford.edu/data/salary.table\"\n    fh = urlopen(url)\n    salary_table = pd.read_table(fh)\n    salary_table.to_csv(\"salary.table\")\n\nE = salary_table.E\nM = salary_table.M\nX = salary_table.X\nS = salary_table.S",
            "from collections import \nimport operator\nfrom time import \n\nimport numpy as np\n\nfrom sklearn.cluster import \nfrom sklearn.cluster import \nfrom sklearn.datasets import \nfrom sklearn.feature_extraction.text import \nfrom sklearn.metrics.cluster import \n\n\ndef number_normalizer(tokens):\n    \"\"\"Map all numeric tokens to a placeholder.\n\n    For many applications, tokens that begin with a number are not directly\n    useful, but the fact that such a token exists can be relevant.  By applying\n    this form of dimensionality reduction, some methods may perform better.\n    \"\"\"\n    return (\"#NUMBER\" if token[0].isdigit() else token for token in tokens)\n\n\nclass NumberNormalizingVectorizer():\n    def build_tokenizer(self):\n        tokenize = super().build_tokenizer()\n        return lambda doc: list(number_normalizer(tokenize(doc)))\n\n\n# exclude 'comp.os.ms-windows.misc'\ncategories = [\n    \"alt.atheism\",\n    \"comp.graphics\",\n    \"comp.sys.ibm.pc.hardware\",\n    \"comp.sys.mac.hardware\",\n    \"comp.windows.x\",\n    \"misc.forsale\",\n    \"rec.autos\",\n    \"rec.motorcycles\",\n    \"rec.sport.baseball\",\n    \"rec.sport.hockey\",\n    \"sci.crypt\",\n    \"sci.electronics\",\n    \"sci.med\",\n    \"sci.space\",\n    \"soc.religion.christian\",\n    \"talk.politics.guns\",\n    \"talk.politics.mideast\",\n    \"talk.politics.misc\",\n    \"talk.religion.misc\",\n]\nnewsgroups = (categories=categories)\ny_true = newsgroups.target\n\nvectorizer = NumberNormalizingVectorizer(stop_words=\"english\", min_df=5)\ncocluster = (\n    n_clusters=len(categories), svd_method=\"arpack\", random_state=0\n)\nkmeans = (\n    n_clusters=len(categories), batch_size=20000, random_state=0, n_init=3\n)\n\nprint(\"Vectorizing...\")\nX = vectorizer.fit_transform(newsgroups.data)\n\nprint(\"Coclustering...\")\nstart_time = ()\ncocluster.fit(X)\ny_cocluster = cocluster.row_labels_\nprint(\n    \"Done in {:.2f}s. V-measure: {:.4f}\".format(\n        () - start_time, (y_cocluster, y_true)\n    )\n)\n\nprint(\"MiniBatchKMeans...\")\nstart_time = ()\ny_kmeans = kmeans.fit_predict(X)\nprint(\n    \"Done in {:.2f}s. V-measure: {:.4f}\".format(\n        () - start_time, (y_kmeans, y_true)\n    )\n)\n\nfeature_names = vectorizer.get_feature_names_out()\ndocument_names = list(newsgroups.target_names[i] for i in newsgroups.target)\n\n\ndef bicluster_ncut(i):\n    rows, cols = cocluster.get_indices(i)\n    if not ((rows) and (cols)):\n        import sys\n\n        return sys.float_info.max\n    row_complement = ((cocluster.rows_[i]))[0]\n    col_complement = ((cocluster.columns_[i]))[0]\n    # Note: the following is identical to X[rows[:, np.newaxis],\n    # cols].sum() but much faster in scipy &lt;= 0.16\n    weight = X[rows][:, cols].sum()\n    cut = X[row_complement][:, cols].sum() + X[rows][:, col_complement].sum()\n    return cut / weight\n\n\ndef most_common(d):\n    \"\"\"Items of a defaultdict(int) with the highest values.\n\n    Like Counter.most_common in Python =2.7.\n    \"\"\"\n    return sorted(d.items(), key=(1), reverse=True)\n\n\nbicluster_ncuts = list(bicluster_ncut(i) for i in range(len(newsgroups.target_names)))\nbest_idx = (bicluster_ncuts)[:5]\n\nprint()\nprint(\"Best biclusters:\")\nprint(\"----------------\")\nfor idx, cluster in enumerate(best_idx):\n    n_rows, n_cols = cocluster.get_shape(cluster)\n    cluster_docs, cluster_words = cocluster.get_indices(cluster)\n    if not len(cluster_docs) or not len(cluster_words):\n        continue\n\n    # categories\n    counter = (int)\n    for i in cluster_docs:\n        counter[document_names[i]] += 1\n    cat_string = \", \".join(\n        \"{:.0f}% {}\".format(float(c) / n_rows * 100, name)\n        for name, c in most_common(counter)[:3]\n    )\n\n    # words\n    out_of_cluster_docs = cocluster.row_labels_ != cluster\n    out_of_cluster_docs = (out_of_cluster_docs)[0]\n    word_col = X[:, cluster_words]\n    word_scores = (\n        word_col[cluster_docs, :].sum(axis=0)\n        - word_col[out_of_cluster_docs, :].sum(axis=0)\n    )\n    word_scores = word_scores.ravel()\n    important_words = list(\n        feature_names[cluster_words[i]] for i in word_scores.argsort()[:-11:-1]\n    )\n\n    print(\"bicluster {} : {} documents, {} words\".format(idx, n_rows, n_cols))\n    print(\"categories   : {}\".format(cat_string))\n    print(\"words        : {}\\n\".format(\", \".join(important_words)))"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 0.22->Precomputed sparse nearest neighbors graph"
            ],
            [
                "torch->Learning PyTorch->What is torch.nn really?->MNIST data setup"
            ],
            [
                "sklearn->Examples->Model Selection->Statistical comparison of models using grid search->Pairwise comparison of all models: frequentist approach"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "sklearn->Examples->Biclustering->Biclustering documents with the Spectral Co-clustering algorithm"
            ]
        ]
    },
    "304587": {
        "jupyter_code_cell": "monthlyrain.plot(kind='bar')\nplt.ylabel('mm/month')\nplt.xlabel('month');\nmonthlyrain.to_csv('sample_data/rotterdam_monthly_rainfall_2012.csv')\nmonthlyrain.to_csv('sample_data/rotterdam_monthly_rainfall_2012.txt')\nmonthlyrain.to_excel('sample_data/rotterdam_monthly_rainfall_2012.xls')",
        "matched_tutorial_code_inds": [
            5715,
            5720,
            6317,
            6704,
            3702
        ],
        "matched_tutorial_codes": [
            "plt.clf()\nplt.grid(True)\nplt.plot(result1.predict(linear=True), result1.resid_pearson, \"o\")\nplt.xlabel(\"Linear predictor\")\nplt.ylabel(\"Residual\")",
            "plt.clf()\nplt.grid(True)\nplt.plot(result2.predict(linear=True), result2.resid_pearson, \"o\")\nplt.xlabel(\"Linear predictor\")\nplt.ylabel(\"Residual\")",
            "# Fit the model\nmod = sm.tsa.statespace.SARIMAX(data['ln_wpi'], trend='c', order=(1,1,1))\nres = mod.fit(disp=False)\nprint(res.summary())",
            "ax = dta.mean().plot(grid=False)\nax.set_xlabel(\"Year\", size=17)\nax.set_ylabel(\"Fertility rate\", size=17)\nax.set_xlim(0, 51)",
            "plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Generalized Linear Models->Quasi-binomial regression"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Quasi-binomial regression"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Example 2: Arima with additive seasonal effects"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ]
        ]
    },
    "343435": {
        "jupyter_code_cell": "plt.figure(figsize=(10,5))\nplt.title('HorsePower Vs Milage')\nplt.xlabel('Horse Power')\nplt.ylabel('Milage')\nplt.style.use('seaborn-darkgrid')\nplt.scatter(df1['hp'], df1['milage'], alpha=0.7,marker='o', s=df.milage*5, c=['r','g','b','y'], edgecolors='black')\nplt.subplots(figsize=(10,7))\nsns.heatmap(df1.corr(), annot=True, cmap='viridis')",
        "matched_tutorial_code_inds": [
            5890,
            401,
            550,
            4346,
            4691
        ],
        "matched_tutorial_codes": [
            "plt.figure(figsize=(6, 6))\nsymbols = [\"D\", \"^\"]\ncolors = [\"r\", \"g\", \"blue\"]\nfactor_groups = salary_table.groupby([\"E\", \"M\"])\nfor values, group in factor_groups:\n    i, j = values\n    plt.scatter(group[\"X\"], group[\"S\"], marker=symbols[j], color=colors[i - 1], s=144)\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "plt.figure(figsize=(5,5))\nplt.plot(epsilons, accuracies, \"*-\")\nplt.yticks(np.arange(0, 1.1, step=0.1))\nplt.xticks(np.arange(0, .35, step=0.05))\nplt.title(\"Accuracy vs Epsilon\")\nplt.xlabel(\"Epsilon\")\nplt.ylabel(\"Accuracy\")\nplt.show()\n\n\n<img alt=\"Accuracy vs Epsilon\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_fgsm_tutorial_001.png\" srcset=\"../_images/sphx_glr_fgsm_tutorial_001.png\"/>",
            "plt.figure(figsize=(10, 10))\nplt.subplot(2, 2, 1)\nplt.plot(logs[\"reward\"])\nplt.title(\"training rewards (average)\")\nplt.subplot(2, 2, 2)\nplt.plot(logs[\"step_count\"])\nplt.title(\"Max step count (training)\")\nplt.subplot(2, 2, 3)\nplt.plot(logs[\"eval reward (sum)\"])\nplt.title(\"Return (test)\")\nplt.subplot(2, 2, 4)\nplt.plot(logs[\"eval step_count\"])\nplt.title(\"Max step count (test)\")\nplt.show()\n\n\n<img alt=\"training rewards (average), Max step count (training), Return (test), Max step count (test)\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_reinforcement_ppo_001.png\" srcset=\"../_images/sphx_glr_reinforcement_ppo_001.png\"/>",
            "plt.figure()\n plt.imshow(deriv)\n plt.gray()\n plt.title('Output of spline edge filter')\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code displays two plots. The first plot is a normal grayscale photo of a raccoon climbing on a palm plant. The second plot has the 2-D spline filter applied to the photo and is completely grey except the edges of the photo have been emphasized, especially on the raccoon fur and palm fronds.\"' class=\"plot-directive\" src=\"../_images/signal-1_01_00.png\"/>\n</figure>",
            "plt.rcParams.update({'figure.autolayout': True})\n\nfig, ax = plt.subplots()\nax.barh(group_names, group_data)\nlabels = ax.get_xticklabels()\nplt.setp(labels, rotation=45, horizontalalignment='right')\n\n\n<img alt=\"lifecycle\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_lifecycle_006.png\" srcset=\"../../_images/sphx_glr_lifecycle_006.png, ../../_images/sphx_glr_lifecycle_006_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "torch->Image and Video->Adversarial Example Generation->Results->Accuracy vs Epsilon"
            ],
            [
                "torch->Reinforcement Learning->Reinforcement Learning (PPO) with TorchRL Tutorial->Results"
            ],
            [
                "scipy->Signal Processing (scipy.signal)->B-splines"
            ],
            [
                "matplotlib->Tutorials->Introductory->The Lifecycle of a Plot->Customizing the plot"
            ]
        ]
    },
    "1282212": {
        "jupyter_code_cell": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nplt.style.use('ggplot')",
        "matched_tutorial_code_inds": [
            3993,
            5161,
            5061,
            6092,
            4934
        ],
        "matched_tutorial_codes": [
            "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\nnp.random.seed(9876789)",
            "import matplotlib.pyplot as plt\nfig = plt.figure()\nax = fig.add_subplot(projection='3d')",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nplt.rc(\"figure\", figsize=(16, 9))\nplt.rc(\"font\", size=16)",
            "import numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom colorspacious import cspace_converter"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships",
                "seaborn->Plotting functions->Visualizing statistical relationships"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares"
            ],
            [
                "matplotlib->Tutorials->Toolkits->The mplot3d toolkit"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Deterministic Terms"
            ],
            [
                "matplotlib->Tutorials->Colors->Choosing Colormaps in Matplotlib->Classes of colormaps"
            ]
        ]
    },
    "934165": {
        "jupyter_code_cell": "movies.head()\ndel movies['color']\ndel movies['num_critic_for_reviews']\ndel movies['director_facebook_likes']\ndel movies['actor_3_facebook_likes']\ndel movies['actor_1_facebook_likes']\ndel movies['genres']\ndel movies['num_user_for_reviews']\ndel movies['language']\ndel movies['country']\ndel movies['actor_2_facebook_likes']\ndel movies['aspect_ratio']\ndel movies['movie_facebook_likes']\ndel movies['num_voted_users']\ndel movies['cast_total_facebook_likes']\ndel movies['plot_keywords']\ndel movies['movie_imdb_link']\nmovies.head()",
        "matched_tutorial_code_inds": [
            1523,
            2446,
            5917,
            2463,
            2676
        ],
        "matched_tutorial_codes": [
            "transistor_count_predicted = np.exp(B) * np.exp(A * year)\ntransistor_Moores_law = Moores_law(year)\nplt.style.use(\"fivethirtyeight\")\nplt.semilogy(year, transistor_count, \"s\", label=\"MOS transistor count\")\nplt.semilogy(year, transistor_count_predicted, label=\"linear regression\")\n\n\nplt.plot(year, transistor_Moores_law, label=\"Moore's Law\")\nplt.title(\n    \"MOS transistor count per microprocessor\\n\"\n    + \"every two years \\n\"\n    + \"Transistor count was x{:.2f} higher\".format(np.exp(A * 2))\n)\nplt.xlabel(\"year introduced\")\nplt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\nplt.ylabel(\"# of transistors\\nper microprocessor\")",
            "last_hours = slice(-96, None)\nfig, ax = (figsize=(12, 4))\nfig.suptitle(\"Predictions by linear models\")\nax.plot(\n    y.iloc[test_0].values[last_hours],\n    \"x-\",\n    alpha=0.2,\n    label=\"Actual demand\",\n    color=\"black\",\n)\nax.plot(naive_linear_predictions[last_hours], \"x-\", label=\"Ordinal time features\")\nax.plot(\n    cyclic_cossin_linear_predictions[last_hours],\n    \"x-\",\n    label=\"Trigonometric time features\",\n)\nax.plot(\n    cyclic_spline_linear_predictions[last_hours],\n    \"x-\",\n    label=\"Spline-based time features\",\n)\nax.plot(\n    one_hot_linear_predictions[last_hours],\n    \"x-\",\n    label=\"One-hot time features\",\n)\n_ = ax.legend()\n\n\n<img alt=\"Predictions by linear models\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_006.png\" srcset=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_006.png\"/>",
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "last_hours = slice(-96, None)\nfig, ax = (figsize=(12, 4))\nfig.suptitle(\"Predictions by non-linear regression models\")\nax.plot(\n    y.iloc[test_0].values[last_hours],\n    \"x-\",\n    alpha=0.2,\n    label=\"Actual demand\",\n    color=\"black\",\n)\nax.plot(\n    gbrt_predictions[last_hours],\n    \"x-\",\n    label=\"Gradient Boosted Trees\",\n)\nax.plot(\n    one_hot_poly_predictions[last_hours],\n    \"x-\",\n    label=\"One-hot + polynomial kernel\",\n)\nax.plot(\n    cyclic_spline_poly_predictions[last_hours],\n    \"x-\",\n    label=\"Splines + polynomial kernel\",\n)\n_ = ax.legend()\n\n\n<img alt=\"Predictions by non-linear regression models\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_007.png\" srcset=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_007.png\"/>",
            "lasso_lars_ic.set_params(lassolarsic__criterion=\"bic\").fit(X, y)\nresults[\"BIC criterion\"] = lasso_lars_ic[-1].criterion_\nalpha_bic = lasso_lars_ic[-1].alpha_"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy->Calculating the historical growth curve for transistors"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Qualitative analysis of the impact of features on linear model predictions"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Modeling non-linear feature interactions with kernels"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Lasso model selection: AIC-BIC / cross-validation->Selecting Lasso via an information criterion"
            ]
        ]
    },
    "1212139": {
        "jupyter_code_cell": "wellness_seg.head()",
        "matched_tutorial_code_inds": [
            5524,
            3695,
            6352,
            5555,
            3760
        ],
        "matched_tutorial_codes": [
            "resf_logit.predict()",
            "flights.dep_time.head()",
            "arima_res.predict(0, 2)",
            "kde.evaluate(-1)",
            "rest.unstack().head()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model"
            ],
            [
                "pandas_toms_blog->Indexes->Merging->The merge version"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Reconstructing residuals, fitted values and forecasts in SARIMAX and ARIMA"
            ],
            [
                "statsmodels->Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->The KDE is a distribution"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Stack / Unstack"
            ]
        ]
    },
    "1363790": {
        "jupyter_code_cell": "mpld3.enable_notebook()\ndef plot_Gl(df, good, alpha=0.3):\n    x = 'ln_G_50'\n    y = 'ln_l_50'\n    bad = ~good\n    plt.scatter(df[good][x], df[good][y], color='b', alpha=alpha)\n    plt.scatter(df[bad][x], df[bad][y], color='r', alpha=alpha);\n    x = np.linspace(Glim, 4, 100)\n    y = x + y_int\n    plt.plot(x, y, 'k:')\n    plt.plot([Glim, Glim], [0, y_int-Glim], 'k:')\n    plt.ylim(ymin=1)\ngood = good_mask(subdf, has_truth=False, max_unc=0.5)\nbad = ~good\nprint('{} classified as good; {} bad.'.format(good.sum(), bad.sum()))\nplot_Gl(subdf, good);\nscatter_compare(subdf[good]);",
        "matched_tutorial_code_inds": [
            4705,
            5005,
            6714,
            6563,
            112
        ],
        "matched_tutorial_codes": [
            "plt.style.use('ggplot')",
            "matplotlib.use('pgf')",
            "make_plot(dta.index[idx[:5]])\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_24_0.png\" src=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_24_0.png\"/>",
            "pm.summary(trace_uc)",
            "torch.backends.cudnn.benchmark = True"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Introductory->Customizing Matplotlib with style sheets and rcParams->Using style sheets"
            ],
            [
                "matplotlib->Tutorials->Text->Text rendering with XeLaTeX/LuaLaTeX via the ``pgf`` backend"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX estimation with Bayesian methods->Appendix A. Application to UnobservedComponents models"
            ],
            [
                "torch->PyTorch Recipes->Performance Tuning Guide->GPU specific optimizations->Enable cuDNN auto-tuner"
            ]
        ]
    },
    "439350": {
        "jupyter_code_cell": "import numpy as np\nimport pandas as pd\nimport mmlspark\ndataFile = \"AdultCensusIncome.csv\"\nimport os, urllib\nif not os.path.isfile(dataFile):\n    urllib.request.urlretrieve(\"https://mmlspark.azureedge.net/datasets/\"+dataFile, dataFile)\ndata = spark.createDataFrame(pd.read_csv(dataFile, dtype={\" hours-per-week\": np.float64}))\ndata = data.select([\" education\", \" marital-status\", \" hours-per-week\", \" income\"])\ntrain, test = data.randomSplit([0.75, 0.25], seed=123)\ntrain.limit(10).toPandas()",
        "matched_tutorial_code_inds": [
            3062,
            3260,
            2867,
            6279,
            2528
        ],
        "matched_tutorial_codes": [
            "import numpy as np\nfrom sklearn.datasets import , , \nfrom sklearn.preprocessing import \nimport pandas as pd\n\nrng = (42)\n\n\ndef preprocess_dataset(dataset_name):\n\n    # loading and vectorization\n    print(f\"Loading {dataset_name} data\")\n    if dataset_name in [\"http\", \"smtp\", \"SA\", \"SF\"]:\n        dataset = (subset=dataset_name, percent10=True, random_state=rng)\n        X = dataset.data\n        y = dataset.target\n        lb = ()\n\n        if dataset_name == \"SF\":\n            idx = rng.choice(X.shape[0], int(X.shape[0] * 0.1), replace=False)\n            X = X[idx]  # reduce the sample size\n            y = y[idx]\n            x1 = lb.fit_transform(X[:, 1].astype(str))\n            X = [X[:, :1], x1, X[:, 2:]]\n        elif dataset_name == \"SA\":\n            idx = rng.choice(X.shape[0], int(X.shape[0] * 0.1), replace=False)\n            X = X[idx]  # reduce the sample size\n            y = y[idx]\n            x1 = lb.fit_transform(X[:, 1].astype(str))\n            x2 = lb.fit_transform(X[:, 2].astype(str))\n            x3 = lb.fit_transform(X[:, 3].astype(str))\n            X = [X[:, :1], x1, x2, x3, X[:, 4:]]\n        y = (y != b\"normal.\").astype(int)\n    if dataset_name == \"forestcover\":\n        dataset = ()\n        X = dataset.data\n        y = dataset.target\n        idx = rng.choice(X.shape[0], int(X.shape[0] * 0.1), replace=False)\n        X = X[idx]  # reduce the sample size\n        y = y[idx]\n\n        # inliers are those with attribute 2\n        # outliers are those with attribute 4\n        s = (y == 2) + (y == 4)\n        X = X[s, :]\n        y = y[s]\n        y = (y != 2).astype(int)\n    if dataset_name in [\"glass\", \"wdbc\", \"cardiotocography\"]:\n        dataset = (\n            name=dataset_name, version=1, as_frame=False, parser=\"pandas\"\n        )\n        X = dataset.data\n        y = dataset.target\n\n        if dataset_name == \"glass\":\n            s = y == \"tableware\"\n            y = s.astype(int)\n        if dataset_name == \"wdbc\":\n            s = y == \"2\"\n            y = s.astype(int)\n            X_mal, y_mal = X[s], y[s]\n            X_ben, y_ben = X[~s], y[~s]\n\n            # downsampled to 39 points (9.8% outliers)\n            idx = rng.choice(y_mal.shape[0], 39, replace=False)\n            X_mal2 = X_mal[idx]\n            y_mal2 = y_mal[idx]\n            X = ((X_ben, X_mal2), axis=0)\n            y = ((y_ben, y_mal2), axis=0)\n        if dataset_name == \"cardiotocography\":\n            s = y == \"3\"\n            y = s.astype(int)\n    # 0 represents inliers, and 1 represents outliers\n    y = (y, dtype=\"category\")\n    return (X, y)",
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import \nfrom sklearn.preprocessing import \nfrom sklearn.linear_model import \nfrom sklearn.model_selection import \n\n\ndef true_fun(X):\n    return (1.5 *  * X)\n\n\n(0)\n\nn_samples = 30\ndegrees = [1, 4, 15]\n\nX = ((n_samples))\ny = true_fun(X) + (n_samples) * 0.1\n\n(figsize=(14, 5))\nfor i in range(len(degrees)):\n    ax = (1, len(degrees), i + 1)\n    (ax, xticks=(), yticks=())\n\n    polynomial_features = (degree=degrees[i], include_bias=False)\n    linear_regression = ()\n    pipeline = (\n        [\n            (\"polynomial_features\", polynomial_features),\n            (\"linear_regression\", linear_regression),\n        ]\n    )\n    pipeline.fit(X[:, ], y)\n\n    # Evaluate the models using crossvalidation\n    scores = (\n        pipeline, X[:, ], y, scoring=\"neg_mean_squared_error\", cv=10\n    )\n\n    X_test = (0, 1, 100)\n    (X_test, pipeline.predict(X_test[:, ]), label=\"Model\")\n    (X_test, true_fun(X_test), label=\"True function\")\n    (X, y, edgecolor=\"b\", s=20, label=\"Samples\")\n    (\"x\")\n    (\"y\")\n    ((0, 1))\n    ((-2, 2))\n    (loc=\"best\")\n    (\n        \"Degree {}\\nMSE = {:.2e}(+/- {:.2e})\".format(\n            degrees[i], -scores.mean(), scores.std()\n        )\n    )\n()",
            "import numpy as np\nimport pandas as pd\n\nn_samples = 10_000\nrng = (32)\n\nexperiences = rng.normal(20, 10, size=n_samples).astype(int)\nexperiences[experiences &lt; 0] = 0\nabilities = rng.normal(0, 0.15, size=n_samples)\nparent_hourly_wages = 50 * rng.beta(2, 8, size=n_samples)\nparent_hourly_wages[parent_hourly_wages &lt; 0] = 0\ncollege_degrees = (\n    9 * abilities + 0.02 * parent_hourly_wages + rng.randn(n_samples)  0.7\n).astype(int)\n\ntrue_coef = (\n    {\n        \"college degree\": 2.0,\n        \"ability\": 5.0,\n        \"experience\": 0.2,\n        \"parent hourly wage\": 1.0,\n    }\n)\nhourly_wages = (\n    true_coef[\"experience\"] * experiences\n    + true_coef[\"parent hourly wage\"] * parent_hourly_wages\n    + true_coef[\"college degree\"] * college_degrees\n    + true_coef[\"ability\"] * abilities\n    + rng.normal(0, 1, size=n_samples)\n)\n\nhourly_wages[hourly_wages &lt; 0] = 0",
            "import numpy as np\n\nrs = np.random.RandomState(0xA4FD94BC)\ntau = 2000\nt = np.arange(tau)\nperiod = int(0.05 * tau)\nseasonal = period + ((period % 2) == 0)  # Ensure odd\ne = 0.25 * rs.standard_normal(tau)\ny = np.cos(t / tau * 2 * np.pi) + 0.25 * np.sin(t / period * 2 * np.pi) + e\nplt.plot(y)\nplt.title(\"Simulated Data\")\nxlim = plt.gca().set_xlim(0, tau)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_stl_decomposition_15_0.png\" src=\"../../../_images/examples_notebooks_generated_stl_decomposition_15_0.png\"/>",
            "import pandas as pd\n\ndf = (grid_search.cv_results_)[\n    [\"param_n_components\", \"param_covariance_type\", \"mean_test_score\"]\n]\ndf[\"mean_test_score\"] = -df[\"mean_test_score\"]\ndf = df.rename(\n    columns={\n        \"param_n_components\": \"Number of components\",\n        \"param_covariance_type\": \"Type of covariance\",\n        \"mean_test_score\": \"BIC score\",\n    }\n)\ndf.sort_values(by=\"BIC score\").head()\n\n\n\n\n\n\n\n\n<br/>\n<br/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Miscellaneous->Evaluation of outlier detection estimators->Define a data preprocessing function"
            ],
            [
                "sklearn->Examples->Model Selection->Underfitting vs. Overfitting"
            ],
            [
                "sklearn->Examples->Inspection->Failure of Machine Learning to infer causal effects->The dataset: simulated hourly wages"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition->Performance"
            ],
            [
                "sklearn->Examples->Gaussian Mixture Models->Gaussian Mixture Model Selection->Plot the BIC scores"
            ]
        ]
    },
    "289985": {
        "jupyter_code_cell": "model = linear_model.LinearRegression()\nresults = model.fit(X_train, y_train)\nprint (results.intercept_, results.coef_)\nuber_pred = model.predict(X_test) ",
        "matched_tutorial_code_inds": [
            6607,
            5164,
            2702,
            6628,
            2273
        ],
        "matched_tutorial_codes": [
            "mod = TVRegression(y_t, x_t, w_t)\nres = mod.fit()\n\nprint(res.summary())",
            "model = sm.OLS(y, X)\nresults = model.fit()\nprint(results.summary())",
            "reg_ols = ()\ny_pred_ols = reg_ols.fit(X_train, y_train).predict(X_test)\nr2_score_ols = (y_test, y_pred_ols)\nprint(\"OLS R2 score\", r2_score_ols)",
            "mod = TVRegression(y_t, x_t, w_t)\nres_mle = mod.fit(disp=False)\nprint(res_mle.summary())",
            "from sklearn.linear_model import \nfrom sklearn.pipeline import \n\nrt_model = (random_tree_embedding, (max_iter=1000))\nrt_model.fit(X_train_linear, y_train_linear)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->Statespace: Custom Models->Model 1: time-varying coefficients->And then estimate it with our custom model class"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->OLS estimation"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Non-negative least squares"
            ],
            [
                "statsmodels->Examples->State space models->Statespace: Custom Models->Bonus: pymc3 for fast Bayesian estimation"
            ],
            [
                "sklearn->Examples->Ensemble methods->Feature transformations with ensembles of trees"
            ]
        ]
    },
    "931479": {
        "jupyter_code_cell": "tls.embed('https://plot.ly/~crimenghini/16')\ntrain_forest = forest.fit(X_1, y_1_train)\npredict_1 = train_forest.predict(X_test)\ntrain_forest = forest.fit(X_2, y_2_train)\npredict_2 = train_forest.predict(X_test)\ntrain_forest = forest.fit(X_3, y_3_train)\npredict_3 = train_forest.predict(X_test)\ntrain_forest = forest.fit(X_4, y_4_train)\npredict_4 = train_forest.predict(X_test)\ntrain_forest = forest.fit(X_5, y_5_train)\npredict_5 = train_forest.predict(X_test)",
        "matched_tutorial_code_inds": [
            4021,
            4085,
            2973,
            4087,
            2996
        ],
        "matched_tutorial_codes": [
            "sns.relplot(\n    data=fmri.query(\"region == 'frontal'\"), kind=\"line\",\n    x=\"timepoint\", y=\"signal\", hue=\"event\", style=\"event\",\n    col=\"subject\", col_wrap=5,\n    height=3, aspect=.75, linewidth=2.5,\n)\n",
            "sns.catplot(data=titanic, x=\"deck\", kind=\"count\", palette=\"ch:.25\")\n",
            "t_sne = (\n    n_components=n_components,\n    perplexity=30,\n    init=\"random\",\n    n_iter=250,\n    random_state=0,\n)\nS_t_sne = t_sne.fit_transform(S_points)\n\nplot_2d(S_t_sne, S_color, \"T-distributed Stochastic  \\n Neighbor Embedding\")\n\n\n<img alt=\"T-distributed Stochastic    Neighbor Embedding\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_compare_methods_006.png\" srcset=\"../../_images/sphx_glr_plot_compare_methods_006.png\"/>",
            "sns.catplot(data=titanic, x=\"sex\", y=\"survived\", hue=\"class\", kind=\"point\")\n",
            "tree_disp.plot(line_kw={\"label\": \"Decision Tree\"})\nmlp_disp.plot(\n    line_kw={\"label\": \"Multi-layer Perceptron\", \"color\": \"red\"}, ax=tree_disp.axes_\n)\ntree_disp.figure_.set_size_inches(10, 6)\ntree_disp.axes_[0, 0].legend()\ntree_disp.axes_[0, 1].legend()\n()\n\n\n<img alt=\"plot partial dependence visualization api\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_partial_dependence_visualization_api_005.png\" srcset=\"../../_images/sphx_glr_plot_partial_dependence_visualization_api_005.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Showing multiple relationships with facets",
                "seaborn->Plotting functions->Visualizing statistical relationships->Showing multiple relationships with facets"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing categorical data->Estimating central tendency->Bar plots",
                "seaborn->Plotting functions->Visualizing categorical data->Estimating central tendency->Bar plots"
            ],
            [
                "sklearn->Examples->Manifold learning->Comparison of Manifold Learning methods->Define algorithms for the manifold learning->T-distributed Stochastic Neighbor Embedding"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing categorical data->Estimating central tendency->Point plots",
                "seaborn->Plotting functions->Visualizing categorical data->Estimating central tendency->Point plots"
            ],
            [
                "sklearn->Examples->Miscellaneous->Advanced Plotting With Partial Dependence->Plotting partial dependence of the two models together"
            ]
        ]
    },
    "1272029": {
        "jupyter_code_cell": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import VarianceThreshold\ndata = pd.read_csv('../datasets/santander.csv')\ndata.shape",
        "matched_tutorial_code_inds": [
            6174,
            6657,
            5559,
            6892,
            3252
        ],
        "matched_tutorial_codes": [
            "import numpy as np\nimport pandas as pd\n\nfrom statsmodels.graphics.tsaplots import plot_predict\nfrom statsmodels.tsa.arima_process import arma_generate_sample\nfrom statsmodels.tsa.arima.model import ARIMA\n\nnp.random.seed(12345)",
            "import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\ndta = sm.datasets.macrodata.load_pandas().data\ndta.index = pd.date_range(start='1959Q1', end='2009Q4', freq='Q')",
            "import numpy as np\nimport pylab\nimport seaborn as sns\nimport statsmodels.api as sm\n\nsns.set_style(\"darkgrid\")\npylab.rc(\"figure\", figsize=(16, 8))\npylab.rc(\"font\", size=14)",
            "import pandas as pd\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm\nfrom statsmodels.tsa.ar_model import AutoReg, ar_select_order\n\nplt.rc(\"figure\", figsize=(16, 8))\nplt.rc(\"font\", size=14)",
            "import numpy as np\n\nn_uncorrelated_features = 20\nrng = (seed=0)\n# Use same number of samples as in iris and 20 features\nX_rand = rng.normal(size=(X.shape[0], n_uncorrelated_features))"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Artificial Data"
            ],
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale"
            ],
            [
                "statsmodels->Examples->Nonparametric Statistics->Lowess Regression"
            ],
            [
                "statsmodels->Examples->User Notes->Dates in Time-Series Models"
            ],
            [
                "sklearn->Examples->Model Selection->Test with permutations the significance of a classification score->Dataset"
            ]
        ]
    },
    "928393": {
        "jupyter_code_cell": "logit_mod_country = sm.Logit(users_country['converted'],    users_country[['intercept', 'ab_page', 'US', 'UK']])\nresults_country = logit_mod_country.fit()\nresults_country.summary()\nUS_coeff = 0.0408\nUK_coeff = 0.0506\nprint('Exponentiating the US coefficient {} yields {:.3f}'.     format(US_coeff, np.exp(US_coeff)))\nprint('Exponentiating the UK coefficient {} yields {:.3f}'.     format(UK_coeff, np.exp(UK_coeff)))",
        "matched_tutorial_code_inds": [
            5917,
            5845,
            5641,
            2460,
            5643
        ],
        "matched_tutorial_codes": [
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "huber_t = sm.RLM(data.endog, data.exog, M=sm.robust.norms.HuberT())\nhub_results = huber_t.fit()\nprint(hub_results.params)\nprint(hub_results.bse)\nprint(\n    hub_results.summary(\n        yname=\"y\", xname=[\"var_%d\" % i for i in range(len(hub_results.params))]\n    )\n)",
            "glm = smf.glm(\n    \"affairs ~ rate_marriage + yrs_married\", data=data, family=sm.families.Poisson()\n)\nres_o2 = glm.fit()\n# print(res_f2.summary())\nres_o2.pearson_chi2 - res_o.pearson_chi2, res_o2.deviance - res_o.deviance, res_o2.llf - res_o.llf",
            "one_hot_poly_pipeline = (\n    (\n        transformers=[\n            (\"categorical\", one_hot_encoder, categorical_columns),\n            (\"one_hot_time\", one_hot_encoder, [\"hour\", \"weekday\", \"month\"]),\n        ],\n        remainder=\"passthrough\",\n    ),\n    (kernel=\"poly\", degree=2, n_components=300, random_state=0),\n    (alphas=alphas),\n)\nevaluate(one_hot_poly_pipeline, X, y, cv=ts_cv)",
            "glm = smf.glm(\n    \"affairs ~ rate_marriage + yrs_married\",\n    data=dc,\n    family=sm.families.Poisson(),\n    freq_weights=np.asarray(dc[\"freq\"]),\n)\nres_f2 = glm.fit()\n# print(res_f2.summary())\nres_f2.pearson_chi2 - res_f.pearson_chi2, res_f2.deviance - res_f.deviance, res_f2.llf - res_f.llf"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "statsmodels->Examples->Robust Regression->Comparing OLS and RLM->Estimation"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->original observations and frequency weights"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Modeling non-linear feature interactions with kernels"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->original observations and frequency weights"
            ]
        ]
    },
    "1418017": {
        "jupyter_code_cell": "ari_lm.model_wo_gt.plot_predictions(figsize=(20, 8), title=\"Predictions of baseline model for 1 week lag scenario\")\nhom_var, hom_lags = \"CASES\", [1, 2, 6]\nhom_ts = pd.read_csv(\"./DBs/Hom/HomCDMX4RM.csv\", parse_dates=[\"TMS\"], index_col=\"TMS\")[[hom_var]]\nhom_gt = gt_ins.load(\"./DBs/Hom/HomCDMXHorariosSeM2004-2018.csv\", var=\"GI\")\nhom_dr = DataRegularizer(data=hom_ts, gt=hom_gt, lags=hom_lags, var_list=[], tf=(\"2009-01\", \"2017-01\"))\nhom_dr.proceed()\nhom_lm = GTModelComparison(data=hom_dr.df, train=hom_dr.train, gt_vars=hom_dr.gt_vars)\nhom_lm.run_all()",
        "matched_tutorial_code_inds": [
            5248,
            6352,
            5819,
            5524,
            6850
        ],
        "matched_tutorial_codes": [
            "res.plot_cusum()",
            "arima_res.predict(0, 2)",
            "student_resid   unadj_p  sidak(p)\n16      -2.049393  0.046415  0.892872\n13      -2.035329  0.047868  0.900286\n33       1.905847  0.063216  0.953543\n18      -1.575505  0.122304  0.997826\n1        1.522185  0.135118  0.998911\n3        1.522185  0.135118  0.998911\n21      -1.450418  0.154034  0.999615\n17      -1.426675  0.160731  0.999735\n29       1.388520  0.171969  0.999859\n14      -1.374733  0.176175  0.999889\n35       1.346543  0.185023  0.999933\n34      -1.272159  0.209999  0.999985\n28      -1.186946  0.241618  0.999998\n20      -1.150621  0.256103  0.999999\n44       1.134779  0.262612  0.999999\n39       1.091886  0.280826  1.000000\n19       1.064878  0.292740  1.000000\n6       -1.026873  0.310093  1.000000\n30      -1.009096  0.318446  1.000000\n22      -0.979768  0.332557  1.000000\n8        0.961218  0.341695  1.000000\n5        0.913802  0.365801  1.000000\n11       0.871997  0.387943  1.000000\n12       0.856685  0.396261  1.000000\n46      -0.833923  0.408829  1.000000\n10       0.743920  0.460879  1.000000\n42       0.727179  0.470968  1.000000\n15      -0.689258  0.494280  1.000000\n43       0.688272  0.494895  1.000000\n7        0.655712  0.515424  1.000000\n40      -0.646396  0.521381  1.000000\n26      -0.640978  0.524862  1.000000\n25      -0.545561  0.588123  1.000000\n32       0.472819  0.638680  1.000000\n37       0.472819  0.638680  1.000000\n38       0.462187  0.646225  1.000000\n0        0.430686  0.668799  1.000000\n31       0.341726  0.734184  1.000000\n36       0.318911  0.751303  1.000000\n4        0.307890  0.759619  1.000000\n9        0.235114  0.815211  1.000000\n41       0.187732  0.851950  1.000000\n2       -0.182093  0.856346  1.000000\n23      -0.156014  0.876736  1.000000\n27      -0.147406  0.883485  1.000000\n24       0.065195  0.948314  1.000000\n45       0.045675  0.963776  1.000000",
            "resf_logit.predict()",
            "mod = sm.tsa.SARIMAX(endog4)\nres = mod.fit()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 2: Quantity theory of money"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Reconstructing residuals, fitted values and forecasts in SARIMAX and ARIMA"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Hertzprung Russell data for Star Cluster CYG 0B1 - Leverage Points"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes"
            ]
        ]
    },
    "286983": {
        "jupyter_code_cell": "Image(filename=\"geomagia_screenshot.png\")\nImage(filename=\"geomagia_refs.png\")",
        "matched_tutorial_code_inds": [
            4716,
            5032,
            161,
            4064,
            4717
        ],
        "matched_tutorial_codes": [
            "ani.save(filename=\"/tmp/pillow_example.gif\", writer=\"pillow\")\nani.save(filename=\"/tmp/pillow_example.apng\", writer=\"pillow\")",
            "ax.axis[\"right\"].set_visible(False)\nax.axis[\"top\"].set_visible(False)\n\n\n<figure class=\"align-center\">\n<img alt=\"../../_images/sphx_glr_simple_axisline3_001.png\" src=\"../../_images/sphx_glr_simple_axisline3_001.png\"/>\n</figure>",
            "compare.trim_significant_figures()\ncompare.colorize()\ncompare.print()",
            "sns.relplot(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\nsns.rugplot(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\n",
            "ani.save(filename=\"/tmp/html_example.html\", writer=\"html\")\nani.save(filename=\"/tmp/html_example.htm\", writer=\"html\")\nani.save(filename=\"/tmp/html_example.png\", writer=\"html\")"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Introductory->Animations using Matplotlib->Animation Writers->Saving Animations"
            ],
            [
                "matplotlib->Tutorials->Toolkits->The axisartist toolkit->axisartist"
            ],
            [
                "torch->PyTorch Recipes->PyTorch Benchmark->Steps->5. Comparing benchmark results"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Distribution visualization in other settings->Plotting joint and marginal distributions",
                "seaborn->Plotting functions->Visualizing distributions of data->Distribution visualization in other settings->Plotting joint and marginal distributions"
            ],
            [
                "matplotlib->Tutorials->Introductory->Animations using Matplotlib->Animation Writers->Saving Animations"
            ]
        ]
    },
    "452562": {
        "jupyter_code_cell": "if 'losses2_pred_gen_total' not in comb.columns:\n    for i in comb.index:\n        comb.ix[i, 'losses2_pred_gen_total'] = -32257 + 0.156*comb.ix[i, 'gen_tot']\n        comb.ix[i,'losses2_resid_gen_total'] = comb.ix[i,'losses2'] - comb.ix[i,'losses2_pred_gen_total']\ncomb.iloc[:,-5:].head()\ncomb[['gen_tot','losses2', 'losses2_pred_gen_total', 'losses2_resid_gen_total']].corr()",
        "matched_tutorial_code_inds": [
            1334,
            519,
            677,
            789,
            603
        ],
        "matched_tutorial_codes": [
            "if sys.platform == 'win32':\n    print('Windows platform is not supported for pipeline parallelism')\n    sys.exit(0)\nif () &lt; 4:\n    print('Need at least four GPU devices for this tutorial')\n    sys.exit(0)\n\nclass Encoder():\n    def __init__(self, ntoken, ninp, dropout=0.5):\n        super(Encoder, self).__init__()\n        self.pos_encoder = PositionalEncoding(ninp, dropout)\n        self.encoder = (ntoken, ninp)\n        self.ninp = ninp\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, src):\n        # Need (S, N) format for encoder.\n        src = src.t()\n        src = self.encoder(src) * math.sqrt(self.ninp)\n        return self.pos_encoder(src)\n\nclass Decoder():\n    def __init__(self, ntoken, ninp):\n        super(Decoder, self).__init__()\n        self.decoder = (ninp, ntoken)\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.1\n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, inp):\n        # Need batch dimension first for output of pipeline.\n        return self.decoder(inp).permute(1, 0, 2)",
            "if ():\n    num_episodes = 600\nelse:\n    num_episodes = 50\n\nfor i_episode in range(num_episodes):\n    # Initialize the environment and get it's state\n    , info = env.reset()\n     = (, dtype=, =).unsqueeze(0)\n    for t in count():\n         = select_action()\n        observation, , terminated, truncated, _ = env.step(.item())\n         = ([], =)\n        done = terminated or truncated\n\n        if terminated:\n             = None\n        else:\n             = (observation, dtype=, =).unsqueeze(0)\n\n        # Store the transition in memory\n        memory.push(, , , )\n\n        # Move to the next state\n         = \n\n        # Perform one step of the optimization (on the policy network)\n        optimize_model()\n\n        # Soft update of the target network's weights\n        # \u03b8\u2032 \u2190 \u03c4 \u03b8 + (1 \u2212\u03c4 )\u03b8\u2032\n        target_net_state_dict = ()\n        policy_net_state_dict = ()\n        for key in policy_net_state_dict:\n            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n        (target_net_state_dict)\n\n        if done:\n            episode_durations.append(t + 1)\n            plot_durations()\n            break\n\nprint('Complete')\nplot_durations(show_result=True)\nplt.ioff()\nplt.show()\n\n\n<img alt=\"Result\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_reinforcement_q_learning_001.png\" srcset=\"../_images/sphx_glr_reinforcement_q_learning_001.png\"/>",
            "if () &gt;= 7603:\n     = (8, 4, 3).cuda().half()\n     = (memory_format=)  # Module parameters need to be channels last\n\n    input = (1, 10, (2, 8, 4, 4), dtype=, requires_grad=True)\n    input = input.to(device=\"cuda\", memory_format=, dtype=)\n\n     = (input)\n    print(.is_contiguous(memory_format=))  # Ouputs: True",
            "if (batch_index % kCheckpointEvery == 0) {\n  // Checkpoint the model and optimizer state.\n  torch::save(generator, \"generator-checkpoint.pt\");\n  torch::save(generator_optimizer, \"generator-optimizer-checkpoint.pt\");\n  torch::save(discriminator, \"discriminator-checkpoint.pt\");\n  torch::save(discriminator_optimizer, \"discriminator-optimizer-checkpoint.pt\");\n  // Sample the generator and save the images.\n  torch::Tensor samples = generator-&gt;forward(torch::randn({8, kNoiseSize, 1, 1}, device));\n  torch::save((samples + 1.0) / 2.0, torch::str(\"dcgan-sample-\", checkpoint_counter, \".pt\"));\n  std::cout &lt;&lt; \"\\n-&gt; checkpoint \" &lt;&lt; ++checkpoint_counter &lt;&lt; '\\n';\n}",
            "class MyRNNLoop():\n    def __init__(self):\n        super(, self).__init__()\n        self.cell = ((scripted_gate), (, ))\n\n    def forward(self, xs):\n        , y = (3, 4), (3, 4)\n        for i in range(xs.size(0)):\n            y,  = self.cell(xs[i], )\n        return y, \n\nrnn_loop = (())\nprint()"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Parallel and Distributed Training->Training Transformer models using Distributed Data Parallel and Pipeline Parallelism->Define the model"
            ],
            [
                "torch->Reinforcement Learning->Reinforcement Learning (DQN) Tutorial->Training->Training loop"
            ],
            [
                "torch->Frontend APIs->(beta) Channels Last Memory Format in PyTorch->Memory Format API"
            ],
            [
                "torch->Frontend APIs->Using the PyTorch C++ Frontend->Checkpointing and Recovering the Training State"
            ],
            [
                "torch->Deploying PyTorch Models in Production->Introduction to TorchScript->Using Scripting to Convert Modules->Mixing Scripting and Tracing"
            ]
        ]
    },
    "1408963": {
        "jupyter_code_cell": "cm = confusion_matrix(y_deploy, y_pred_TS)\nnp.set_printoptions(precision=2)\nprint('Confusion matrix, without normalization')\nprint(cm)\nplt.figure(1)\nplt.subplot(2,2,1)\nplot_confusion_matrix(cm)\ncm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\nprint('Normalized confusion matrix')\nprint(cm_normalized)\nplt.subplot(2,2,2)\nplot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\nplt.show()\ncm = confusion_matrix(y_deploy, y_pred_HG)\nnp.set_printoptions(precision=2)\nprint('Confusion matrix, without normalization')\nprint(cm)\nplt.figure(1)\nplt.subplot(2,2,1)\nplot_confusion_matrix(cm)\ncm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\nprint('Normalized confusion matrix')\nprint(cm_normalized)\nplt.subplot(2,2,2)\nplot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\nplt.show()",
        "matched_tutorial_code_inds": [
            2662,
            5643,
            2601,
            2860,
            2844
        ],
        "matched_tutorial_codes": [
            "m, s, _ = (\n    (enet.coef_)[0],\n    enet.coef_[enet.coef_ != 0],\n    markerfmt=\"x\",\n    label=\"Elastic net coefficients\",\n)\n([m, s], color=\"#2ca02c\")\nm, s, _ = (\n    (lasso.coef_)[0],\n    lasso.coef_[lasso.coef_ != 0],\n    markerfmt=\"x\",\n    label=\"Lasso coefficients\",\n)\n([m, s], color=\"#ff7f0e\")\n(\n    (coef)[0],\n    coef[coef != 0],\n    label=\"true coefficients\",\n    markerfmt=\"bx\",\n)\n\n(loc=\"best\")\n(\n    \"Lasso $R^2$: %.3f, Elastic Net $R^2$: %.3f\" % (r2_score_lasso, r2_score_enet)\n)\n()\n\n\n<img alt=\"Lasso $R^2$: 0.658, Elastic Net $R^2$: 0.643\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_lasso_and_elasticnet_001.png\" srcset=\"../../_images/sphx_glr_plot_lasso_and_elasticnet_001.png\"/>",
            "glm = smf.glm(\n    \"affairs ~ rate_marriage + yrs_married\",\n    data=dc,\n    family=sm.families.Poisson(),\n    freq_weights=np.asarray(dc[\"freq\"]),\n)\nres_f2 = glm.fit()\n# print(res_f2.summary())\nres_f2.pearson_chi2 - res_f.pearson_chi2, res_f2.deviance - res_f.deviance, res_f2.llf - res_f.llf",
            "vmin, vmax = (-log_marginal_likelihood).min(), 50\nlevel = (((vmin), (vmax), num=50), decimals=1)\n(\n    length_scale_grid,\n    noise_level_grid,\n    -log_marginal_likelihood,\n    levels=level,\n    norm=(vmin=vmin, vmax=vmax),\n)\n()\n(\"log\")\n(\"log\")\n(\"Length-scale\")\n(\"Noise-level\")\n(\"Log-marginal-likelihood\")\n()\n\n\n<img alt=\"Log-marginal-likelihood\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_noisy_005.png\" srcset=\"../../_images/sphx_glr_plot_gpr_noisy_005.png\"/>",
            "mae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}\n\n_, ax = (figsize=(5, 5))\ndisplay = (\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n)\nax.set_title(\"Ridge model, optimum regularization\")\nfor name, score in scores.items():\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\nax.legend(loc=\"upper left\")\n()\n\n\n<img alt=\"Ridge model, optimum regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_012.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_012.png\"/>",
            "mae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}\n\n_, ax = (figsize=(5, 5))\ndisplay = (\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n)\nax.set_title(\"Ridge model, small regularization\")\nfor name, score in scores.items():\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\nax.legend(loc=\"upper left\")\n()\n\n\n<img alt=\"Ridge model, small regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_009.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_009.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Generalized Linear Models->Lasso and Elastic Net for Sparse Signals->Plot"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->original observations and frequency weights"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) with noise-level estimation->Optimisation of kernel hyperparameters in GPR"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ]
        ]
    },
    "842247": {
        "jupyter_code_cell": "df_states = pandas.read_csv('correlatesofstatepolicyprojectv1_10.csv',low_memory=False)\ndf_ineq = df_states[~df_states['incshare_top1'].isnull()]\nany(df_ineq['incshare_top1'].isnull())",
        "matched_tutorial_code_inds": [
            5143,
            6460,
            6818,
            5571,
            6218
        ],
        "matched_tutorial_codes": [
            "sqtab = sm.stats.SquareTable.from_data(data[['left', 'right']])\nprint(sqtab.summary())",
            "arma_mod20 = sm.tsa.statespace.SARIMAX(dta, order=(2,0,0), trend='c').fit(disp=False)\nprint(arma_mod20.params)",
            "fcast_res3 = res.get_forecast('2010Q2')\nprint(fcast_res3.summary_frame())",
            "glm_binom = sm.GLM(data.endog, data.exog, family=sm.families.Binomial())\nres = glm_binom.fit()\nprint(res.summary())",
            "res_fedfunds2.smoothed_marginal_probabilities[0].plot(\n    title=\"Probability of being in the high regime\", figsize=(12, 3)\n)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->User Guide->Statistics and Tools->Contingency tables->Symmetry and homogeneity"
            ],
            [
                "statsmodels->Examples->State space models->Statespace ARMA: Sunspots Data->Sunspots Data"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example->Specifying the number of forecasts"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Generalized Linear Models Overview->GLM: Binomial response data->Fit and summary"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Markov switching dynamic regression->Federal funds rate with switching intercept and lagged dependent variable"
            ]
        ]
    },
    "852877": {
        "jupyter_code_cell": "wrangle_clean.shape\nnull_rows = wrangle_clean[pd.isnull(wrangle_clean).any(axis=1)]\nnull_rows",
        "matched_tutorial_code_inds": [
            1484,
            5539,
            5466,
            5425,
            3159
        ],
        "matched_tutorial_codes": [
            "china_masked = nbcases_ma[locations[:, 1] == \"China\"].sum(axis=0)\nchina_masked",
            "kde = sm.nonparametric.KDEUnivariate(obs_dist)\nkde.fit()  # Estimate the densities",
            "means25 = exog.mean()\nprint(means25)",
            "respondent1000 = dta.iloc[1000]\nprint(respondent1000)",
            "class_of_interest = \"virginica\"\nclass_id = (label_binarizer.classes_ == class_of_interest)[0]\nclass_id"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Features->Masked Arrays->Missing data"
            ],
            [
                "statsmodels->Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->Fitting with the default arguments"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data"
            ],
            [
                "sklearn->Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->One-vs-Rest multiclass ROC->ROC curve showing a specific class"
            ]
        ]
    },
    "307032": {
        "jupyter_code_cell": "train = drinks.sample(frac=0.75, random_state=1)    \ntest = drinks[~drinks.index.isin(train.index)]      \nd = pd.DataFrame({'capital':['Montgomery', 'Juneau', 'Phoenix'], 'state':['AL', 'AK', 'AZ']})\nd.head(2)",
        "matched_tutorial_code_inds": [
            2463,
            2932,
            3893,
            2088,
            2446
        ],
        "matched_tutorial_codes": [
            "last_hours = slice(-96, None)\nfig, ax = (figsize=(12, 4))\nfig.suptitle(\"Predictions by non-linear regression models\")\nax.plot(\n    y.iloc[test_0].values[last_hours],\n    \"x-\",\n    alpha=0.2,\n    label=\"Actual demand\",\n    color=\"black\",\n)\nax.plot(\n    gbrt_predictions[last_hours],\n    \"x-\",\n    label=\"Gradient Boosted Trees\",\n)\nax.plot(\n    one_hot_poly_predictions[last_hours],\n    \"x-\",\n    label=\"One-hot + polynomial kernel\",\n)\nax.plot(\n    cyclic_spline_poly_predictions[last_hours],\n    \"x-\",\n    label=\"Splines + polynomial kernel\",\n)\n_ = ax.legend()\n\n\n<img alt=\"Predictions by non-linear regression models\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_007.png\" srcset=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_007.png\"/>",
            "result = (\n    rf, X_train, y_train, n_repeats=10, random_state=42, n_jobs=2\n)\n\nsorted_importances_idx = result.importances_mean.argsort()\nimportances = (\n    result.importances[sorted_importances_idx].T,\n    columns=X.columns[sorted_importances_idx],\n)\nax = importances.plot.box(vert=False, whis=10)\nax.set_title(\"Permutation Importances (train set)\")\nax.axvline(x=0, color=\"k\", linestyle=\"--\")\nax.set_xlabel(\"Decrease in accuracy score\")\nax.figure.tight_layout()\n\n\n<img alt=\"Permutation Importances (train set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_permutation_importance_003.png\" srcset=\"../../_images/sphx_glr_plot_permutation_importance_003.png\"/>",
            "indiv = indiv[(indiv.transaction_dt &gt;= pd.Timestamp(\"2007-01-01\")) &amp;\n              (indiv.transaction_dt &lt;= pd.Timestamp(\"2018-01-01\"))]\n\ndf2 = dd.merge(indiv, cm.reset_index(), on='cmte_id')\ndf2\n<strong>Dask DataFrame Structure:</strong>Dask Name: merge, 141 tasks",
            "train_scores = [clf.score(X_train, y_train) for clf in clfs]\ntest_scores = [clf.score(X_test, y_test) for clf in clfs]\n\nfig, ax = ()\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker=\"o\", label=\"test\", drawstyle=\"steps-post\")\nax.legend()\n()\n\n\n<img alt=\"Accuracy vs alpha for training and testing sets\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cost_complexity_pruning_003.png\" srcset=\"../../_images/sphx_glr_plot_cost_complexity_pruning_003.png\"/>",
            "last_hours = slice(-96, None)\nfig, ax = (figsize=(12, 4))\nfig.suptitle(\"Predictions by linear models\")\nax.plot(\n    y.iloc[test_0].values[last_hours],\n    \"x-\",\n    alpha=0.2,\n    label=\"Actual demand\",\n    color=\"black\",\n)\nax.plot(naive_linear_predictions[last_hours], \"x-\", label=\"Ordinal time features\")\nax.plot(\n    cyclic_cossin_linear_predictions[last_hours],\n    \"x-\",\n    label=\"Trigonometric time features\",\n)\nax.plot(\n    cyclic_spline_linear_predictions[last_hours],\n    \"x-\",\n    label=\"Spline-based time features\",\n)\nax.plot(\n    one_hot_linear_predictions[last_hours],\n    \"x-\",\n    label=\"One-hot time features\",\n)\n_ = ax.legend()\n\n\n<img alt=\"Predictions by linear models\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_006.png\" srcset=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_006.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Modeling non-linear feature interactions with kernels"
            ],
            [
                "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)"
            ],
            [
                "pandas_toms_blog->Scaling->Joining"
            ],
            [
                "sklearn->Examples->Decision Trees->Post pruning decision trees with cost complexity pruning->Accuracy vs alpha for training and testing sets"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Qualitative analysis of the impact of features on linear model predictions"
            ]
        ]
    },
    "42295": {
        "jupyter_code_cell": "import ROOT\nimport pandas as pd\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport seaborn\nslact = ROOT.TChain('slacAnalyzer/eventTree')\nfor run_num in range(1930, 1939):\n    slact.Add('../rootOutputs/gm2slac_run0{}.root'.format(run_num))\nslact.SetBranchStatus(\"*\",0)\nfor used_branch in ['EventNum', 'IslandNum', 'XtalNum', 'Energy', 'Time']:\n    slact.SetBranchStatus('XtalHit_' + used_branch)\nslact.GetEntries()",
        "matched_tutorial_code_inds": [
            5256,
            1838,
            1110,
            4661,
            925
        ],
        "matched_tutorial_codes": [
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pandas_datareader as pdr\nimport seaborn\n\nimport statsmodels.api as sm\nfrom statsmodels.regression.rolling import RollingOLS\n\nseaborn.set_style(\"darkgrid\")\npd.plotting.register_matplotlib_converters()\n%matplotlib inline",
            "import scipy\nimport numpy as np\nfrom sklearn.model_selection import \nfrom sklearn.cluster import \nfrom sklearn.datasets import \nfrom sklearn.metrics import \n\nrng = (0)\nX, y = (random_state=rng)\nX = (X)\nX_train, X_test, _, y_test = (X, y, random_state=rng)\nkmeans = (n_init=\"auto\").fit(X_train)\nprint((kmeans.predict(X_test), y_test))",
            "import torch\nimport torch.nn.functional as F\nimport functorch\nfrom functorch.compile import memory_efficient_fusion\nfrom copy import deepcopy\nfrom typing import List\nimport time\nimport functools\nimport random\n\nrandom.seed(42)\n\nif torch.__version__ &lt; (1, 12, 0):\n    raise RuntimeError(\n        \"PyTorch &gt;= 1.12.0 required, but your environment uses torch=={}\".format(\n            torch.__version__\n        )\n    )\n\nmajor, minor, _ = functorch.__version__.split(\".\")\nif int(major) == 0 and int(minor) &lt; 2:\n    raise RuntimeError(\n        \"FuncTorch &gt;= 0.2.0 required, but your environment uses functorch=={}\".format(\n            functorch.__version__\n        )\n    )",
            "import matplotlib.pyplot as plt\nplt.figure(1)                # the first figure\nplt.subplot(211)             # the first subplot in the first figure\nplt.plot([1, 2, 3])\nplt.subplot(212)             # the second subplot in the first figure\nplt.plot([4, 5, 6])\n\n\nplt.figure(2)                # a second figure\nplt.plot([4, 5, 6])          # creates a subplot() by default\n\nplt.figure(1)                # first figure current;\n                             # subplot(212) still current\nplt.subplot(211)             # make subplot(211) in the first figure\n                             # current\nplt.title('Easy as 1, 2, 3') # subplot 211 title",
            "import torch.utils.cpp_extension\n\ntorch.utils.cpp_extension.load(\n    name=\"warp_perspective\",\n    sources=[\"op.cpp\"],\n    extra_ldflags=[\"-lopencv_core\", \"-lopencv_imgproc\"],\n    is_python_module=False,\n    verbose=True\n)\n\nprint(torch.ops.my_ops.warp_perspective)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Rolling Least Squares"
            ],
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 0.23->Scalability and stability improvements to KMeans"
            ],
            [
                "torch->Model Optimization->Getting Started - Accelerate Your Scripts with nvFuser->Importing Packages and Selecting a Device"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Working with multiple figures and axes"
            ],
            [
                "torch->Extending PyTorch->Extending TorchScript with Custom C++ Operators->Appendix A: More Ways of Building Custom Operators->Building with JIT compilation"
            ]
        ]
    },
    "595116": {
        "jupyter_code_cell": "pca_2d_plot_labels(pca, df, frame)\nframe.to_csv('order_segmentation_0.0.csv')",
        "matched_tutorial_code_inds": [
            5334,
            5345,
            3794,
            4139,
            5022
        ],
        "matched_tutorial_codes": [
            "fig = sm.graphics.plot_partregress_grid(prestige_model)\nfig.tight_layout(pad=1.0)",
            "fig = sm.graphics.plot_partregress_grid(crime_model)\nfig.tight_layout(pad=1.0)",
            "sns.jointplot(x='carat', y='price', data=df, size=8, alpha=.25,\n              color='k', marker='.')\nplt.tight_layout()",
            "g = sns.PairGrid(iris)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n",
            "inset_axes = zoomed_inset_axes(ax,\n                               0.5,  # zoom = 0.5\n                               loc='upper right')"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Plotting->Regression Plots->Duncan\u2019s Prestige Dataset->Partial Regression Plots (Duncan)"
            ],
            [
                "statsmodels->Examples->Plotting->Regression Plots->Statewide Crime 2009 Dataset->Partial Regression Plots (Crime Data)"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships"
            ],
            [
                "matplotlib->Tutorials->Toolkits->The axes_grid1 toolkit->axes_grid1->InsetLocator"
            ]
        ]
    },
    "796741": {
        "jupyter_code_cell": "temp = df.From_To.str.split('_', expand=True)\ntemp.columns = ['From', 'To']\ntemp['From'] = temp['From'].str.capitalize()\ntemp['To'] = temp['To'].str.capitalize()",
        "matched_tutorial_code_inds": [
            4751,
            6704,
            6664,
            3603,
            4750
        ],
        "matched_tutorial_codes": [
            "fig, ax = plt.subplots()\nline_up, = ax.plot([1, 2, 3], label='Line 2')\nline_down, = ax.plot([3, 2, 1], label='Line 1')\nax.legend([line_up, line_down], ['Line Up', 'Line Down'])",
            "ax = dta.mean().plot(grid=False)\nax.set_xlabel(\"Year\", size=17)\nax.set_ylabel(\"Fertility rate\", size=17)\nax.set_xlim(0, 51)",
            "mod_conc = LocalLevelConcentrated(dta.infl)\nres_conc = mod_conc.fit(disp=False)\nprint(res_conc.summary())",
            "zf = zipfile.ZipFile(\"data/flights.csv.zip\")\nfp = zf.extract(zf.filelist[0].filename, path='data/')\ndf = pd.read_csv(fp, parse_dates=[\"FL_DATE\"]).rename(columns=str.lower)\n\ndf.info()",
            "fig, ax = plt.subplots()\nline_up, = ax.plot([1, 2, 3], label='Line 2')\nline_down, = ax.plot([3, 2, 1], label='Line 1')\nax.legend(handles=[line_up, line_down])"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Intermediate->Legend guide->Controlling the legend entries"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Concentrating out the scale"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Get the Data"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Legend guide->Controlling the legend entries"
            ]
        ]
    },
    "1329174": {
        "jupyter_code_cell": "ax = sns.barplot(x=\"projectCount\", y=\"projectCount\", hue=\"turnover\", data=df, estimator=lambda x: len(x) / len(df) * 100)\nax.set(ylabel=\"Percent\")\nfig = plt.figure(figsize=(15,4),)\nax=sns.kdeplot(df.loc[(df['turnover'] == 0),'evaluation'] , color='b',shade=True,label='no turnover')\nax=sns.kdeplot(df.loc[(df['turnover'] == 1),'evaluation'] , color='r',shade=True, label='turnover')\nplt.title('Last evaluation')",
        "matched_tutorial_code_inds": [
            6403,
            2635,
            3853,
            3855,
            4514
        ],
        "matched_tutorial_codes": [
            "ax = res.impulse_responses(10, orthogonalized=True, impulse=[1, 0]).plot(figsize=(13,3))\nax.set(xlabel='t', title='Responses to a shock to `dln_inv`');\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_varmax_8_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_varmax_8_0.png\"/>",
            "ax = (\n    data=full_data, x=\"input_feature\", y=\"target\", color=\"black\", alpha=0.75\n)\nax.plot(X_plot, y_plot, color=\"black\", label=\"Ground Truth\")\nax.plot(X_plot, y_brr, color=\"red\", label=\"BayesianRidge with polynomial features\")\nax.plot(X_plot, y_ard, color=\"navy\", label=\"ARD with polynomial features\")\nax.fill_between(\n    X_plot.ravel(),\n    y_ard - y_ard_std,\n    y_ard + y_ard_std,\n    color=\"navy\",\n    alpha=0.3,\n)\nax.fill_between(\n    X_plot.ravel(),\n    y_brr - y_brr_std,\n    y_brr + y_brr_std,\n    color=\"red\",\n    alpha=0.3,\n)\nax.legend()\n_ = ax.set_title(\"Polynomial fit of a non-linear feature\")\n\n\n<img alt=\"Polynomial fit of a non-linear feature\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_ard_003.png\" srcset=\"../../_images/sphx_glr_plot_ard_003.png\"/>",
            "ax = y.plot(label='observed')\npred.predicted_mean.plot(ax=ax, label='Forecast', alpha=.7)\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.2)\nax.set_ylabel(\"Monthly Flights\")\nplt.legend()\nsns.despine()",
            "ax = y.plot(label='observed')\npred_dy.predicted_mean.plot(ax=ax, label='Forecast')\nax.fill_between(pred_dy_ci.index,\n                pred_dy_ci.iloc[:, 0],\n                pred_dy_ci.iloc[:, 1], color='k', alpha=.25)\nax.set_ylabel(\"Monthly Flights\")\n\n# Highlight the forecast area\nax.fill_betweenx(ax.get_ylim(), pd.Timestamp('2013-01-01'), y.index[-1],\n                 alpha=.1, zorder=-1)\nax.annotate('Dynamic $\\\\longrightarrow$', (pd.Timestamp('2013-02-01'), 550))\n\nplt.legend()\nsns.despine()",
            "ax.plot(x1, np.zeros(x1.shape), 'b+', ms=20)  # rug plot\n x_eval = np.linspace(-10, 10, num=200)\n ax.plot(x_eval, kde1(x_eval), 'k-', label=\"Scott's Rule\")\n ax.plot(x_eval, kde2(x_eval), 'r-', label=\"Silverman's Rule\")"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->VARMAX: Introduction->Example 1: VAR"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Comparing Linear Bayesian Regressors->Bayesian regressions with polynomial feature expansion->Plotting polynomial regressions with std errors of the scores"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Forecasting"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Forecasting"
            ],
            [
                "scipy->Statistics (scipy.stats)->Kernel density estimation->Univariate estimation"
            ]
        ]
    },
    "715128": {
        "jupyter_code_cell": "BBB_Train.head(n=5)\nBBB_Train[\"Rank_Clicks\"] = 0\nDDD_Train[\"Rank_Clicks\"] = 0\nFFF_Train[\"Rank_Clicks\"] = 0\nBBB_Test[\"Rank_Clicks\"] = 0\nDDD_Test[\"Rank_Clicks\"] = 0\nFFF_Test[\"Rank_Clicks\"] = 0",
        "matched_tutorial_code_inds": [
            4144,
            3703,
            5191,
            4066,
            4128
        ],
        "matched_tutorial_codes": [
            "g = sns.PairGrid(tips, hue=\"size\", palette=\"GnBu_d\")\ng.map(plt.scatter, s=50, edgecolor=\"white\")\ng.add_legend()\n",
            "df = pd.read_csv(\"data/347136217_T_ONTIME.csv\")\ndelays = df['DEP_DELAY']",
            "ols_model = sm.OLS(y, X)\nols_results = ols_model.fit()\nprint(ols_results.summary())",
            "g = sns.PairGrid(penguins)\ng.map_upper(sns.histplot)\ng.map_lower(sns.kdeplot, fill=True)\ng.map_diag(sns.histplot, kde=True)\n",
            "g = sns.FacetGrid(tips, col=\"day\", height=4, aspect=.5)\ng.map(sns.barplot, \"sex\", \"total_bill\", order=[\"Male\", \"Female\"])\n"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions",
                "seaborn->Plotting functions->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Conditional small multiples",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Conditional small multiples"
            ]
        ]
    },
    "226104": {
        "jupyter_code_cell": "row_idx = validation[:, 0] - 1\ncol_idx = validation[:, 1] - 1\naccuracy = np.mean(positive_rating[row_idx, col_idx] == validation[:, 2])\naccuracy",
        "matched_tutorial_code_inds": [
            5296,
            1965,
            6835,
            5871,
            2352
        ],
        "matched_tutorial_codes": [
            "pred_ols = res_ols.get_prediction()\niv_l_ols = pred_ols.summary_frame()[\"obs_ci_lower\"]\niv_u_ols = pred_ols.summary_frame()[\"obs_ci_upper\"]",
            "centers = [[1, 1], [-1, -1], [1, -1]]\nX, labels_true = (\n    n_samples=300, centers=centers, cluster_std=0.5, random_state=0\n)",
            "# Compute the root mean square error\nrmse = (flattened**2).mean(axis=1)**0.5\n\nprint(rmse)",
            "y = groups_re[groups_ix] + level1_re[level1_ix] + level2_re[level2_ix]\ny += np.sqrt(resid_var) * np.random.normal(size=n)",
            "coverage_fraction(\n    y_test, all_models[\"q 0.05\"].predict(X_test), all_models[\"q 0.95\"].predict(X_test)\n)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Weighted Least Squares->OLS vs.\u00a0WLS"
            ],
            [
                "sklearn->Examples->Clustering->Demo of affinity propagation clustering algorithm->Generate sample data"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example",
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example->Using extend"
            ],
            [
                "statsmodels->Examples->Generalized Estimating Equations->GEE Nested Covariance Structure"
            ],
            [
                "sklearn->Examples->Ensemble methods->Prediction Intervals for Gradient Boosting Regression->Calibration of the confidence interval"
            ]
        ]
    },
    "1076191": {
        "jupyter_code_cell": "data = Truth['RenovatablePriceFactor']\nsorted_data = np.sort(data)\nyvals=np.arange(len(sorted_data))/float(len(sorted_data)-1)\nplt.plot(sorted_data,yvals)\nplt.title(\"CDF of Renovatable Price Factor\")\nplt.show()\nk = 10 \ncols = Truth.corr().nlargest(k, 'RenovatablePriceFactor')['RenovatablePriceFactor'].index \ncm = np.corrcoef(Truth[cols].values.T)\nsns.set(font_scale=1.25)\nplt.subplots(figsize=[12,12])\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)",
        "matched_tutorial_code_inds": [
            5963,
            6081,
            6060,
            2298,
            4654
        ],
        "matched_tutorial_codes": [
            "data = [\n    [\"Carroll\", 94, 22, 60, 92, 20, 60],\n    [\"Grant\", 98, 21, 65, 92, 22, 65],\n    [\"Peck\", 98, 28, 40, 88, 26, 40],\n    [\"Donat\", 94, 19, 200, 82, 17, 200],\n    [\"Stewart\", 98, 21, 50, 88, 22, 45],\n    [\"Young\", 96, 21, 85, 92, 22, 85],\n]\ncolnames = [\"study\", \"mean_t\", \"sd_t\", \"n_t\", \"mean_c\", \"sd_c\", \"n_c\"]\nrownames = [i[0] for i in data]\ndframe1 = pd.DataFrame(data, columns=colnames)\nrownames",
            "data = pdr.get_data_fred(\"INDPRO\", \"1959-01-01\", \"2019-06-01\")\nind_prod = data.INDPRO.pct_change(12).dropna().asfreq(\"MS\")\n_, ax = plt.subplots(figsize=(16, 9))\nind_prod.plot(ax=ax)",
            "data = pdr.get_data_fred(\"HOUSTNSA\", \"1959-01-01\", \"2019-06-01\")\nhousing = data.HOUSTNSA.pct_change().dropna()\n# Scale by 100 to get percentages\nhousing = 100 * housing.asfreq(\"MS\")\nfig, ax = plt.subplots()\nax = housing.plot(ax=ax)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_6_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_6_0.png\"/>",
            "feature_importance = reg.feature_importances_\nsorted_idx = (feature_importance)\npos = (sorted_idx.shape[0]) + 0.5\nfig = (figsize=(12, 6))\n(1, 2, 1)\n(pos, feature_importance[sorted_idx], align=\"center\")\n(pos, (diabetes.feature_names)[sorted_idx])\n(\"Feature Importance (MDI)\")\n\nresult = (\n    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_idx = result.importances_mean.argsort()\n(1, 2, 2)\n(\n    result.importances[sorted_idx].T,\n    vert=False,\n    labels=(diabetes.feature_names)[sorted_idx],\n)\n(\"Permutation Importance (test set)\")\nfig.tight_layout()\n()\n\n\n<img alt=\"Feature Importance (MDI), Permutation Importance (test set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\"/>",
            "data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Example"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions->Industrial Production"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions"
            ],
            [
                "sklearn->Examples->Ensemble methods->Gradient Boosting regression->Plot feature importance"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings"
            ]
        ]
    },
    "54427": {
        "jupyter_code_cell": "var = con.variables\nprint('Number of variables in', dataset, ':', len(var))\ncon.variables.head()\ncols = ['B19083_001E']\ncols.extend(['NAME', 'GEOID'])",
        "matched_tutorial_code_inds": [
            5573,
            6069,
            6065,
            6860,
            6814
        ],
        "matched_tutorial_codes": [
            "print('Total number of trials:',  data.endog.iloc[:, 0].sum())\nprint('Parameters: ', res.params)\nprint('T-values: ', res.tvalues)",
            "sel = ar_select_order(housing, 13, seasonal=True, old_names=False)\nsel.ar_lags\nres = sel.model.fit()\nprint(res.summary())",
            "sel = ar_select_order(housing, 13, old_names=False)\nsel.ar_lags\nres = sel.model.fit()\nprint(res.summary())",
            "data = sm.datasets.spector.load_pandas()\nexog = data.exog\nendog = data.endog\nprint(sm.datasets.spector.NOTE)\nprint(data.exog.head())",
            "fcast_res2 = res.get_forecast(steps=2)\n# Note: since we did not specify the alpha parameter, the\n# confidence level is at the default, 95%\nprint(fcast_res2.summary_frame())"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Generalized Linear Models->Generalized Linear Models Overview->GLM: Binomial response data->Quantities of interest"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions->Seasonal Dummies"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions"
            ],
            [
                "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 1: Probit model"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example->Specifying the number of forecasts"
            ]
        ]
    },
    "944408": {
        "jupyter_code_cell": "flights['DepDelay'].describe()\nDepDelay = flights.loc[lambda df: flights['DepDelay'] < 29, :]\nArrDelay = flights.loc[lambda df: flights['ArrDelay'] < 27, :]\ndelayDep = DepDelay['DepDelay']\ndelayArr = ArrDelay['ArrDelay']",
        "matched_tutorial_code_inds": [
            3703,
            6146,
            3616,
            3829,
            6704
        ],
        "matched_tutorial_codes": [
            "df = pd.read_csv(\"data/347136217_T_ONTIME.csv\")\ndelays = df['DEP_DELAY']",
            "table = pd.DataFrame(data, columns=[\"lag\", \"AC\", \"Q\", \"Prob(Q)\"])\nprint(table.set_index(\"lag\"))",
            "hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()",
            "ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "ax = dta.mean().plot(grid=False)\nax.set_xlabel(\"Year\", size=17)\nax.set_ylabel(\"Fertility rate\", size=17)\nax.set_xlim(0, 51)"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ]
        ]
    },
    "921933": {
        "jupyter_code_cell": "print('Label: ', np.where(mnist.train.labels[0] ==1)[0][0])\nplt.imshow(np.reshape(mnist.train.images[i],(28,28)))\ncentroids = []\nfor i in range(10):\n    tmp = np.where((np.where(mnist.train.labels == 1)[1]) == i)[0]\n    tmp = np.average(mnist.train.images[tmp], axis=0)\n    centroids.append(tmp)",
        "matched_tutorial_code_inds": [
            5229,
            4483,
            4500,
            4487,
            4494
        ],
        "matched_tutorial_codes": [
            "print(sm.datasets.copper.DESCRLONG)\n\ndta = sm.datasets.copper.load_pandas().data\ndta.index = pd.date_range(\"1951-01-01\", \"1975-01-01\", freq=\"AS\")\nendog = dta[\"WORLDCONSUMPTION\"]\n\n# To the regressors in the dataset, we add a column of ones for an intercept\nexog = sm.add_constant(\n    dta[[\"COPPERPRICE\", \"INCOMEINDEX\", \"ALUMPRICE\", \"INVENTORYINDEX\"]]\n)",
            "print('mean = %6.4f, variance = %6.4f, skew = %6.4f, kurtosis = %6.4f' %\n...       normdiscrete.stats(moments='mvsk'))\nmean = -0.0000, variance = 6.3302, skew = 0.0000, kurtosis = -0.0076",
            "print('tail prob. of normal at 1%%, 5%% and 10%% %8.4f %8.4f %8.4f' %\n...       tuple(stats.norm.sf([crit01, crit05, crit10])*100))\ntail prob. of normal at 1%, 5% and 10%   0.2857   3.4957   8.5003",
            "print('chisquare for normdiscrete: chi2 = %6.3f pvalue = %6.4f' % (ch2, pval))\nchisquare for normdiscrete: chi2 = 12.466 pvalue = 0.4090  # random",
            "print('KS-statistic D = %6.3f pvalue = %6.4f' % stats.kstest(x, 't', (10,)))\nKS-statistic D =  0.016 pvalue = 0.9571  # random"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 1: Copper"
            ],
            [
                "scipy->Statistics (scipy.stats)->Building specific distributions->Subclassing rv_discrete"
            ],
            [
                "scipy->Statistics (scipy.stats)->Analysing one sample->Tails of the distribution"
            ],
            [
                "scipy->Statistics (scipy.stats)->Building specific distributions->Subclassing rv_discrete"
            ],
            [
                "scipy->Statistics (scipy.stats)->Analysing one sample->T-test and KS-test"
            ]
        ]
    },
    "640743": {
        "jupyter_code_cell": "from sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nlinreg.fit(X_train[cols], y_train)\ny_pred = linreg.predict(X_test[cols])\nr2_value = r2_score(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nprint(\"R-squared value for Linear Regression :\",r2_value)\nprint(\"RMSE :\",rmse)\nfrom sklearn.tree import DecisionTreeRegressor\ndtr = DecisionTreeRegressor()",
        "matched_tutorial_code_inds": [
            2503,
            2918,
            1841,
            1823,
            2308
        ],
        "matched_tutorial_codes": [
            "from sklearn.svm import \nfrom sklearn.datasets import \nfrom sklearn.feature_selection import \nimport matplotlib.pyplot as plt\n\n# Load the digits dataset\ndigits = ()\nX = digits.images.reshape((len(digits.images), -1))\ny = digits.target\n\n# Create the RFE object and rank each pixel\nsvc = (kernel=\"linear\", C=1)\nrfe = (estimator=svc, n_features_to_select=1, step=1)\nrfe.fit(X, y)\nranking = rfe.ranking_.reshape(digits.images[0].shape)\n\n# Plot pixel ranking\n(ranking, cmap=plt.cm.Blues)\n()\n(\"Ranking of pixels with RFE\")\n()",
            "from sklearn.ensemble import \nfrom sklearn.impute import \nfrom sklearn.compose import \nfrom sklearn.pipeline import \nfrom sklearn.preprocessing import \n\ncategorical_encoder = (\n    handle_unknown=\"use_encoded_value\", unknown_value=-1, encoded_missing_value=-1\n)\nnumerical_pipe = (strategy=\"mean\")\n\npreprocessing = (\n    [\n        (\"cat\", categorical_encoder, categorical_columns),\n        (\"num\", numerical_pipe, numerical_columns),\n    ],\n    verbose_feature_names_out=False,\n)\n\nrf = (\n    [\n        (\"preprocess\", preprocessing),\n        (\"classifier\", (random_state=42)),\n    ]\n)\nrf.fit(X_train, y_train)",
            "from sklearn.model_selection import \nfrom sklearn.datasets import \nfrom sklearn.linear_model import \nimport numpy as np\n\nn_samples, n_features = 1000, 20\nrng = (0)\nX, y = (n_samples, n_features, random_state=rng)\nsample_weight = rng.rand(n_samples)\nX_train, X_test, y_train, y_test, sw_train, sw_test = (\n    X, y, sample_weight, random_state=rng\n)\nreg = ()\nreg.fit(X_train, y_train, sample_weight=sw_train)\nprint(reg.score(X_test, y_test, sw_test))",
            "from sklearn.tree import \nfrom sklearn.model_selection import \nimport numpy as np\n\nn_samples, n_features = 1000, 20\nrng = (0)\nX = rng.randn(n_samples, n_features)\n# positive integer target correlated with X[:, 5] with many zeros:\ny = rng.poisson(lam=(X[:, 5]) / 2)\nX_train, X_test, y_train, y_test = (X, y, random_state=rng)\nregressor = (criterion=\"poisson\", random_state=0)\nregressor.fit(X_train, y_train)",
            "from sklearn.ensemble import \nfrom sklearn.inspection import PartialDependenceDisplay\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nrng = (0)\n\nn_samples = 1000\nf_0 = rng.rand(n_samples)\nf_1 = rng.rand(n_samples)\nX = [f_0, f_1]\nnoise = rng.normal(loc=0.0, scale=0.01, size=n_samples)\n\n# y is positively correlated with f_0, and negatively correlated with f_1\ny = 5 * f_0 + (10 *  * f_0) - 5 * f_1 - (10 *  * f_1) + noise"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Feature Selection->Recursive feature elimination"
            ],
            [
                "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Data Loading and Feature Engineering"
            ],
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 0.23->Sample-weight support for Lasso and ElasticNet"
            ],
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 0.24->New Poisson splitting criterion for DecisionTreeRegressor"
            ],
            [
                "sklearn->Examples->Ensemble methods->Monotonic Constraints"
            ]
        ]
    },
    "9180": {
        "jupyter_code_cell": "learning_rate = 0.01\ntraining_iters = 5000\nbatch_size = 128\ndisplay_step = 2\nn_input = 784 \nn_classes = 10 \ndropout = 0.75 \nx = tf.placeholder(tf.float32, [None, n_input])\ny = tf.placeholder(tf.float32, [None, n_classes])\nkeep_prob = tf.placeholder(tf.float32)",
        "matched_tutorial_code_inds": [
            1574,
            265,
            1534,
            1099,
            2943
        ],
        "matched_tutorial_codes": [
            "learning_rate = 0.005\nepochs = 20\nhidden_size = 100\npixels_per_image = 784\nnum_labels = 10",
            "learning_rate = 1e-3\nbatch_size = 64\nepochs = 5",
            "air-quality-data.csv\nmooreslaw_regression.npz\nmooreslaw-tutorial.md\npairing.md\nsave-load-arrays.md\n_static\ntext_preprocessing.py\ntransistor_data.csv\ntutorial-air-quality-analysis.md\ntutorial-deep-learning-on-mnist.md\ntutorial-deep-reinforcement-learning-with-pong-from-pixels.md\ntutorial-ma.md\ntutorial-nlp-from-scratch\ntutorial-nlp-from-scratch.md\ntutorial-plotting-fractals\ntutorial-plotting-fractals.md\ntutorial-static_equilibrium.md\ntutorial-style-guide.md\ntutorial-svd.md\ntutorial-x-ray-image-processing\ntutorial-x-ray-image-processing.md\nwho_covid_19_sit_rep_time_series.csv\nx_y-squared.csv\nx_y-squared.npz",
            "num_train_batches = 20\n\n# QAT takes time and one needs to train over a few epochs.\n# Train and check accuracy after each epoch\nfor nepoch in range(8):\n    train_one_epoch(qat_model, criterion, optimizer, data_loader, torch.device('cpu'), num_train_batches)\n    if nepoch &gt; 3:\n        # Freeze quantizer parameters\n        qat_model.apply(torch.ao.quantization.disable_observer)\n    if nepoch &gt; 2:\n        # Freeze batch norm mean and variance estimates\n        qat_model.apply(torch.nn.intrinsic.qat.freeze_bn_stats)\n\n    # Check the accuracy after each epoch\n    quantized_model = torch.ao.quantization.convert(qat_model.eval(), inplace=False)\n    quantized_model.eval()\n    top1, top5 = evaluate(quantized_model,criterion, data_loader_test, neval_batches=num_eval_batches)\n    print('Epoch %d :Evaluation accuracy on %d images, %2.2f'%(nepoch, num_eval_batches * eval_batch_size, top1.avg))",
            "train_result = (\n    rf, X_train, y_train, n_repeats=10, random_state=42, n_jobs=2\n)\ntest_results = (\n    rf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_importances_idx = train_result.importances_mean.argsort()"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Applications->Deep learning on MNIST->3. Build and train a small neural network from scratch->Compose the model and begin training and testing it"
            ],
            [
                "torch->Introduction to PyTorch->Optimizing Model Parameters->Hyperparameters"
            ],
            [
                "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy->Sharing your results as zipped arrays and a csv->Zipping the arrays into a file"
            ],
            [
                "torch->Model Optimization->(beta) Static Quantization with Eager Mode in PyTorch->5. Quantization-aware training"
            ],
            [
                "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)"
            ]
        ]
    },
    "1421037": {
        "jupyter_code_cell": "df.salt > 60\ndf[df.salt > 60]",
        "matched_tutorial_code_inds": [
            1495,
            2322,
            5901,
            3716,
            3706
        ],
        "matched_tutorial_codes": [
            "china_total.mask\ninvalid = china_total[china_total.mask]\ninvalid",
            "mask = y &lt; 5\nX = X[mask]\ny = y[mask]",
            "df_infl[:5]",
            "idx = idx[idx.get_level_values(0) &lt;= idx.get_level_values(1)]",
            "delays.nsmallest(5).sort_values()"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Features->Masked Arrays->Fitting Data"
            ],
            [
                "sklearn->Examples->Ensemble methods->Pixel importances with a parallel forest of trees->Loading the data and model fitting"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization"
            ]
        ]
    },
    "566972": {
        "jupyter_code_cell": "def process_pclass():\n    global combined\n    pclass_dummies = pd.get_dummies(combined['Pclass'],prefix=\"Pclass\")\n    combined = pd.concat([combined,pclass_dummies],axis=1)\n    combined.drop('Pclass',axis=1,inplace=True)\n    status('pclass')\nprocess_pclass()",
        "matched_tutorial_code_inds": [
            143,
            2966,
            35,
            312,
            1236
        ],
        "matched_tutorial_codes": [
            "def trace_handler(p):\n    output = p.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=10)\n    print(output)\n    p.export_chrome_trace(\"/tmp/trace_\" + str(p.step_num) + \".json\")\n\nwith (\n    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n    =(\n        wait=1,\n        warmup=1,\n        active=2),\n    on_trace_ready=trace_handler\n) as p:\n    for idx in range(8):\n        model(inputs)\n        p.step()",
            "def plot_3d(points, points_color, title):\n    x, y, z = points.T\n\n    fig, ax = (\n        figsize=(6, 6),\n        facecolor=\"white\",\n        tight_layout=True,\n        subplot_kw={\"projection\": \"3d\"},\n    )\n    fig.suptitle(title, size=16)\n    col = ax.scatter(x, y, z, c=points_color, s=50, alpha=0.8)\n    ax.view_init(azim=-60, elev=9)\n    ax.xaxis.set_major_locator((1))\n    ax.yaxis.set_major_locator((1))\n    ax.zaxis.set_major_locator((1))\n\n    fig.colorbar(col, ax=ax, orientation=\"horizontal\", shrink=0.6, aspect=60, pad=0.01)\n    ()\n\n\ndef plot_2d(points, points_color, title):\n    fig, ax = (figsize=(3, 3), facecolor=\"white\", constrained_layout=True)\n    fig.suptitle(title, size=16)\n    add_2d_scatter(ax, points, points_color)\n    ()\n\n\ndef add_2d_scatter(ax, points, points_color, title=None):\n    x, y = points.T\n    ax.scatter(x, y, c=points_color, s=50, alpha=0.8)\n    ax.set_title(title)\n    ax.xaxis.set_major_formatter(())\n    ax.yaxis.set_major_formatter(())\n\n\nplot_3d(S_points, S_color, \"Original S-curve samples\")\n\n\n<img alt=\"Original S-curve samples\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_compare_methods_001.png\" srcset=\"../../_images/sphx_glr_plot_compare_methods_001.png\"/>",
            "def print_size_of_model(model, label=\"\"):\n    (model.state_dict(), \"temp.p\")\n    size=os.path.getsize(\"temp.p\")\n    print(\"model: \",label,' \\t','Size (KB):', size/1e3)\n    os.remove('temp.p')\n    return size\n\n# compare the sizes\nf=print_size_of_model(float_lstm,\"fp32\")\nq=print_size_of_model(quantized_lstm,\"int8\")\nprint(\"{0:.2f} times smaller\".format(f/q))",
            "def fit():\n    for epoch in range(epochs):\n        for i in range((n - 1) // bs + 1):\n            start_i = i * bs\n            end_i = start_i + bs\n             = [start_i:end_i]\n             = [start_i:end_i]\n             = ()\n             = loss_func(, )\n\n            ()\n            with ():\n                for p in ():\n                    p -= p.grad * lr\n                ()\n\nfit()",
            "def setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n\n    # initialize the process group\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\ndef cleanup():\n    dist.destroy_process_group()"
        ],
        "matched_tutorial_paths": [
            [
                "torch->PyTorch Recipes->PyTorch Profiler->Steps->8. Using profiler to analyze long-running jobs"
            ],
            [
                "sklearn->Examples->Manifold learning->Comparison of Manifold Learning methods->Dataset preparation"
            ],
            [
                "torch->PyTorch Recipes->Dynamic Quantization->Steps->3. Look at Model Size"
            ],
            [
                "torch->Learning PyTorch->What is torch.nn really?->Refactor using nn.Module"
            ],
            [
                "torch->Parallel and Distributed Training->Getting Started with Fully Sharded Data Parallel(FSDP)->How to use FSDP"
            ]
        ]
    },
    "714393": {
        "jupyter_code_cell": "lognormal_mean = lognormal.mean()\nlognormal_stdev = lognormal.std()\nplt.axvline(x=lognormal_mean, color=\"g\", linestyle=\"solid\")\nplt.axvline(x=(lognormal_mean - lognormal_stdev), color=\"g\", linestyle=\"dashed\")\nplt.axvline(x=(lognormal_mean + lognormal_stdev), color=\"g\", linestyle=\"dashed\")\nplt.hist(lognormal)\nplt.show()\nlaplace = np.random.laplace(10, 2, 100)\nplt.hist(laplace)\nplt.show()",
        "matched_tutorial_code_inds": [
            5917,
            1095,
            3079,
            1615,
            2114
        ],
        "matched_tutorial_codes": [
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "per_channel_quantized_model = load_model(saved_model_dir + float_model_file)\nper_channel_quantized_model.eval()\nper_channel_quantized_model.fuse_model()\n# The old 'fbgemm' is still available but 'x86' is the recommended default.\nper_channel_quantized_model.qconfig = torch.ao.quantization.get_default_qconfig('x86')\nprint(per_channel_quantized_model.qconfig)\n\ntorch.ao.quantization.prepare(per_channel_quantized_model, inplace=True)\nevaluate(per_channel_quantized_model,criterion, data_loader, num_calibration_batches)\ntorch.ao.quantization.convert(per_channel_quantized_model, inplace=True)\ntop1, top5 = evaluate(per_channel_quantized_model, criterion, data_loader_test, neval_batches=num_eval_batches)\nprint('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\ntorch.jit.save(torch.jit.script(per_channel_quantized_model), saved_model_dir + scripted_quantized_model_file)",
            "rfc = (n_estimators=10, random_state=42)\nrfc.fit(X_train, y_train)\nax = ()\nrfc_disp = (rfc, X_test, y_test, ax=ax, alpha=0.8)\nsvc_disp.plot(ax=ax, alpha=0.8)\n()\n\n\n<img alt=\"plot roc curve visualization api\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_roc_curve_visualization_api_002.png\" srcset=\"../../_images/sphx_glr_plot_roc_curve_visualization_api_002.png\"/>",
            "fourier_gaussian = ndimage.fourier_gaussian(xray_image, sigma=0.05)\n\nx_prewitt = ndimage.prewitt(fourier_gaussian, axis=0)\ny_prewitt = ndimage.prewitt(fourier_gaussian, axis=1)\n\nxray_image_canny = np.hypot(x_prewitt, y_prewitt)\n\nxray_image_canny *= 255.0 / np.max(xray_image_canny)\n\nprint(\"The data type - \", xray_image_canny.dtype)",
            "batch_dict_estimator = (\n    n_components=n_components, alpha=0.1, max_iter=50, batch_size=3, random_state=rng\n)\nbatch_dict_estimator.fit(faces_centered)\nplot_gallery(\"Dictionary learning\", batch_dict_estimator.components_[:n_components])\n\n\n<img alt=\"Dictionary learning\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_006.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_006.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "torch->Model Optimization->(beta) Static Quantization with Eager Mode in PyTorch->4. Post-training static quantization"
            ],
            [
                "sklearn->Examples->Miscellaneous->ROC Curve with Visualization API->Training a Random Forest and Plotting the ROC Curve"
            ],
            [
                "numpy->NumPy Applications->X-ray image processing->Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters->The Canny filter"
            ],
            [
                "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition->Dictionary learning"
            ]
        ]
    },
    "200955": {
        "jupyter_code_cell": "gmail_regex = r'([a-zA-Z0-9]+)@(gmail)\\.(com)'\ntext  = 'email1@gmail.com, email2@yahoo.com, email3@gmail.com'\nre.findall(gmail_regex, text)\ngmail_regex = r'(([a-zA-Z0-9]+)@(gmail)\\.(com))'\ntext  = 'email1@gmail.com, email2@yahoo.com, email3@gmail.com'\nre.findall(gmail_regex, text)",
        "matched_tutorial_code_inds": [
            5643,
            5647,
            5645,
            5299,
            2997
        ],
        "matched_tutorial_codes": [
            "glm = smf.glm(\n    \"affairs ~ rate_marriage + yrs_married\",\n    data=dc,\n    family=sm.families.Poisson(),\n    freq_weights=np.asarray(dc[\"freq\"]),\n)\nres_f2 = glm.fit()\n# print(res_f2.summary())\nres_f2.pearson_chi2 - res_f.pearson_chi2, res_f2.deviance - res_f.deviance, res_f2.llf - res_f.llf",
            "glm = smf.glm(\n    \"affairs_mean ~ rate_marriage + yrs_married\",\n    data=df_a,\n    family=sm.families.Poisson(),\n    var_weights=np.asarray(df_a[\"affairs_count\"]),\n)\nres_a2 = glm.fit()\nres_a2.pearson_chi2 - res_a.pearson_chi2, res_a2.deviance - res_a.deviance, res_a2.llf - res_a.llf",
            "glm = smf.glm(\n    \"affairs_sum ~ rate_marriage + yrs_married\",\n    data=df_a,\n    family=sm.families.Poisson(),\n    exposure=np.asarray(df_a[\"affairs_count\"]),\n)\nres_e2 = glm.fit()\nres_e2.pearson_chi2 - res_e.pearson_chi2, res_e2.deviance - res_e.deviance, res_e2.llf - res_e.llf",
            "resid1 = res_ols.resid[w == 1.0]\nvar1 = resid1.var(ddof=int(res_ols.df_model) + 1)\nresid2 = res_ols.resid[w != 1.0]\nvar2 = resid2.var(ddof=int(res_ols.df_model) + 1)\nw_est = w.copy()\nw_est[w != 1.0] = np.sqrt(var2) / np.sqrt(var1)\nres_fwls = sm.WLS(y, X, 1.0 / ((w_est ** 2))).fit()\nprint(res_fwls.summary())",
            "tree_disp = (tree, X, [\"age\"])\nmlp_disp = (\n    mlp, X, [\"age\"], ax=tree_disp.axes_, line_kw={\"color\": \"red\"}\n)\n\n\n<img alt=\"plot partial dependence visualization api\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_partial_dependence_visualization_api_006.png\" srcset=\"../../_images/sphx_glr_plot_partial_dependence_visualization_api_006.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->original observations and frequency weights"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->aggregated data: exposure and var_weights"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->aggregated data: exposure and var_weights"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Weighted Least Squares->Feasible Weighted Least Squares (2-stage FWLS)"
            ],
            [
                "sklearn->Examples->Miscellaneous->Advanced Plotting With Partial Dependence->Plotting partial dependence for one feature"
            ]
        ]
    },
    "987800": {
        "jupyter_code_cell": "model_rf = ensemble.RandomForestRegressor(random_state=42)\nn_estimators = [20, 50, 100, 150, 200, 250]\nmax_depth = [3, 4, 5, 6, 7, 8, 9, 10]\nparam_grid = {'n_estimators': n_estimators, \n              'max_depth': max_depth}\ngs = GridSearchCV(estimator = model_rf, \n                  param_grid = param_grid, \n                  scoring = 'neg_mean_squared_error', \n                  cv=10)\ngs = gs.fit(X_treino, y_treino)\nprint(gs.best_params_)\nmodel_gb = ensemble.GradientBoostingRegressor(random_state=42)\nn_estimators = [20, 50, 100, 150, 200, 250]\nmax_depth = [3, 4, 5, 6, 7, 8, 9, 10]\nlearning_rate = [ 0.1, 0.05, 0.01, 0.001]\nparam_grid = {'n_estimators': n_estimators, \n              'max_depth': max_depth, \n              'learning_rate': learning_rate}\ngs = GridSearchCV(estimator = model_gb, \n                  param_grid = param_grid, \n                  scoring = 'neg_mean_squared_error', \n                  cv = 10)\ngs = gs.fit(X_treino, y_treino)\nprint(gs.best_params_)",
        "matched_tutorial_code_inds": [
            6639,
            5308,
            6253,
            3474,
            6444
        ],
        "matched_tutorial_codes": [
            "model_heuristic = ETSModel(oil, initialization_method=\"heuristic\")\nfit_heuristic = model_heuristic.fit()\noil.plot(label=\"data\")\nfit.fittedvalues.plot(label=\"estimated\")\nfit_heuristic.fittedvalues.plot(label=\"heuristic\", linestyle=\"--\")\nplt.ylabel(\"Annual oil production in Saudi Arabia (Mt)\")\n\n# obtained from R\nparams = [0.99989969, 0.11888177503085334, 0.80000197, 36.46466837, 34.72584983]\nyhat = model.smooth(params).fittedvalues\nyhat.plot(label=\"with R params\", linestyle=\":\")\n\nplt.legend()",
            "model1 = sm.MixedLM.from_formula(\n    \"y ~ 1\",\n    re_formula=\"1\",\n    vc_formula={\"group2\": \"0 + C(group2)\"},\n    groups=\"group1\",\n    data=df,\n)\nresult1 = model1.fit()\nprint(result1.summary())",
            "fit1 = SimpleExpSmoothing(oildata, initialization_method=\"heuristic\").fit(\n    smoothing_level=0.2, optimized=False\n)\nfcast1 = fit1.forecast(3).rename(r\"$\\alpha=0.2$\")\nfit2 = SimpleExpSmoothing(oildata, initialization_method=\"heuristic\").fit(\n    smoothing_level=0.6, optimized=False\n)\nfcast2 = fit2.forecast(3).rename(r\"$\\alpha=0.6$\")\nfit3 = SimpleExpSmoothing(oildata, initialization_method=\"estimated\").fit()\nfcast3 = fit3.forecast(3).rename(r\"$\\alpha=%s$\" % fit3.model.params[\"smoothing_level\"])\n\nplt.figure(figsize=(12, 8))\nplt.plot(oildata, marker=\"o\", color=\"black\")\nplt.plot(fit1.fittedvalues, marker=\"o\", color=\"blue\")\n(line1,) = plt.plot(fcast1, marker=\"o\", color=\"blue\")\nplt.plot(fit2.fittedvalues, marker=\"o\", color=\"red\")\n(line2,) = plt.plot(fcast2, marker=\"o\", color=\"red\")\nplt.plot(fit3.fittedvalues, marker=\"o\", color=\"green\")\n(line3,) = plt.plot(fcast3, marker=\"o\", color=\"green\")\nplt.legend([line1, line2, line3], [fcast1.name, fcast2.name, fcast3.name])",
            "model_l2 = (penalty=\"l2\", loss=\"squared_hinge\", dual=True)\nCs = (-4.5, -2, 10)\n\nlabels = [f\"fraction: {train_size}\" for train_size in train_sizes]\nresults = {\"C\": Cs}\nfor label, train_size in zip(labels, train_sizes):\n    cv = (train_size=train_size, test_size=0.3, n_splits=50, random_state=1)\n    train_scores, test_scores = (\n        model_l2, X, y, param_name=\"C\", param_range=Cs, cv=cv\n    )\n    results[label] = test_scores.mean(axis=1)\nresults = (results)",
            "mod_uc = sm.tsa.UnobservedComponents(\n    endog, 'rwalk',\n    cycle=True, stochastic_cycle=True, damped_cycle=True,\n)\n# Here the powell method gets close to the optimum\nres_uc = mod_uc.fit(method='powell', disp=False)\n# but to get to the highest loglikelihood we do a\n# second round using the L-BFGS method.\nres_uc = mod_uc.fit(res_uc.params, disp=False)\nprint(res_uc.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->ETS models->Simple exponential smoothing"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Nested analysis"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Simple Exponential Smoothing"
            ],
            [
                "sklearn->Examples->Support Vector Machines->Scaling the regularization parameter for SVCs->L2-penalty case"
            ],
            [
                "statsmodels->Examples->State space models->Trends and cycles in unemployment->Unobserved components with stochastic cycle (UC)"
            ]
        ]
    },
    "507966": {
        "jupyter_code_cell": "df_train.head(3)\ndef title(passenger):\n    name = passenger['Name']\n    start = name.find(',') + 1\n    end = name.find('.')\n    return name[start:end].strip()\ndf_train['title'] = df_train.apply(title, axis=1)\ndf_test['title'] = df_test.apply(title, axis=1)\nprint(df_train.groupby('title').size(), '\\n\\n',\n      df_test.groupby('title').size())",
        "matched_tutorial_code_inds": [
            4341,
            922,
            4905,
            3596,
            4333
        ],
        "matched_tutorial_codes": [
            "dst(dst(x, type=4, norm='ortho'), type=4, norm='ortho')\narray([ 1. ,  2. ,  1. , -1. ,  1.5])\n  # scaling factor 2*N = 10\n dst(dst(x, type=4), type=4)\narray([ 10.,  20.,  10., -10.,  15.])\n  # no scaling factor\n idst(dst(x, type=4), type=4)\narray([ 1. ,  2. ,  1. , -1. ,  1.5])",
            "find_package(OpenCV REQUIRED)\nadd_library(warp_perspective SHARED op.cpp)\ntarget_compile_features(warp_perspective PRIVATE cxx_range_for)\ntarget_link_libraries(warp_perspective PRIVATE \"${TORCH_LIBRARIES}\")\ntarget_link_libraries(warp_perspective PRIVATE opencv_core opencv_photo)",
            "viridis.colors [[0.267004 0.004874 0.329415 1.      ]\n [0.275191 0.194905 0.496005 1.      ]\n [0.212395 0.359683 0.55171  1.      ]\n [0.153364 0.497    0.557724 1.      ]\n [0.122312 0.633153 0.530398 1.      ]\n [0.288921 0.758394 0.428426 1.      ]\n [0.626579 0.854645 0.223353 1.      ]\n [0.993248 0.906157 0.143936 1.      ]]\nviridis(range(8)) [[0.267004 0.004874 0.329415 1.      ]\n [0.275191 0.194905 0.496005 1.      ]\n [0.212395 0.359683 0.55171  1.      ]\n [0.153364 0.497    0.557724 1.      ]\n [0.122312 0.633153 0.530398 1.      ]\n [0.288921 0.758394 0.428426 1.      ]\n [0.626579 0.854645 0.223353 1.      ]\n [0.993248 0.906157 0.143936 1.      ]]\nviridis(np.linspace(0, 1, 8)) [[0.267004 0.004874 0.329415 1.      ]\n [0.275191 0.194905 0.496005 1.      ]\n [0.212395 0.359683 0.55171  1.      ]\n [0.153364 0.497    0.557724 1.      ]\n [0.122312 0.633153 0.530398 1.      ]\n [0.288921 0.758394 0.428426 1.      ]\n [0.626579 0.854645 0.223353 1.      ]\n [0.993248 0.906157 0.143936 1.      ]]",
            "gs_clf.best_score_\n0.9...\n for param_name in sorted(parameters.keys()):\n...     print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))\n...\nclf__alpha: 0.001\ntfidf__use_idf: True\nvect__ngram_range: (1, 1)",
            "dct(dct(x, type=1, norm='ortho'), type=1, norm='ortho')\narray([ 1. ,  2. ,  1. , -1. ,  1.5])\n # Unnormalized round-trip via DCT-I: scaling factor 2*(N-1) = 8\n dct(dct(x, type=1), type=1)\narray([ 8. ,  16.,  8. , -8. ,  12.])\n # Normalized inverse: no scaling factor\n idct(dct(x, type=1), type=1)\narray([ 1. ,  2. ,  1. , -1. ,  1.5])"
        ],
        "matched_tutorial_paths": [
            [
                "scipy->Fourier Transforms (scipy.fft)->Discrete Sine Transforms->DST and IDST"
            ],
            [
                "torch->Extending PyTorch->Extending TorchScript with Custom C++ Operators->Using the TorchScript Custom Operator in C++"
            ],
            [
                "matplotlib->Tutorials->Colors->Creating Colormaps in Matplotlib->Getting colormaps and accessing their values->ListedColormap"
            ],
            [
                "sklearn->Tutorials->Working With Text Data->Parameter tuning using grid search"
            ],
            [
                "scipy->Fourier Transforms (scipy.fft)->Discrete Cosine Transforms->DCT and IDCT"
            ]
        ]
    },
    "1242106": {
        "jupyter_code_cell": "import pandas as pd\nawards2016=pd.read_csv('NSERC_GRT_FYR2016_AWARD.csv', encoding = 'ISO-8859-1')\nawards2016.head()\nawards2016.info()",
        "matched_tutorial_code_inds": [
            3703,
            3805,
            3603,
            1779,
            6061
        ],
        "matched_tutorial_codes": [
            "df = pd.read_csv(\"data/347136217_T_ONTIME.csv\")\ndelays = df['DEP_DELAY']",
            "gs = web.DataReader(\"GS\", data_source='yahoo', start='2006-01-01',\n                    end='2010-01-01')\ngs.head().round(2)",
            "zf = zipfile.ZipFile(\"data/flights.csv.zip\")\nfp = zf.extract(zf.filelist[0].filename, path='data/')\ndf = pd.read_csv(fp, parse_dates=[\"FL_DATE\"]).rename(columns=str.lower)\n\ndf.info()",
            "X, y = (\n    \"titanic\", version=1, as_frame=True, return_X_y=True, parser=\"pandas\"\n)\nX.head()\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "mod = AutoReg(housing, 3, old_names=False)\nres = mod.fit()\nprint(res.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Get the Data"
            ],
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 1.2->Faster parser in"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions"
            ]
        ]
    },
    "1312124": {
        "jupyter_code_cell": "def calculate_cook_distance(x_c, y_c, b0_c, b1_c):\n    D = []\n    predict_c = predict(x_c, b0_c, b1_c)\n    p = 2 \n    mse = (np.sum((predict_c - y_c)**2))/x_c.shape[0] \n    for i in range(x_c.shape[0]):\n        x_c_i = copy.deepcopy(x_c)\n        y_c_i = copy.deepcopy(y_c)\n        x_c_i = np.delete(x_c_i, i)\n        y_c_i = np.delete(y_c_i, i)\n        (R_c_i, b1_c_i, b0_c_i) = fit(x_c_i, y_c_i)\n        predict_c_i = predict(x_c_i, b0_c_i, b1_c_i)\n        predict_c_i = np.insert(predict_c_i, i, predict_c[i])\n        D.append(np.sum((predict_c - predict_c_i)**2) / (p*mse))\n    df = pd.DataFrame(np.array([x_c, y_c, D]).T, columns = ['x', 'y', 'D'])    \n    df = df.sort_values(by='D', ascending=False)\n    return df\ncalculate_cook_distance(x_c, y_c, b0_c, b1_c)\nx_c = copy.deepcopy(x)\ny_c = copy.deepcopy(y)\nx_c = np.append(x_c, 65)\ny_c = np.append(y_c, 65*SLOPE + INTERCEPT + 20)",
        "matched_tutorial_code_inds": [
            2398,
            3103,
            6276,
            35,
            2391
        ],
        "matched_tutorial_codes": [
            "def plot_accuracy(x, y, x_legend):\n    \"\"\"Plot accuracy as a function of x.\"\"\"\n    x = (x)\n    y = (y)\n    (\"Classification accuracy as a function of %s\" % x_legend)\n    (\"%s\" % x_legend)\n    (\"Accuracy\")\n    (True)\n    (x, y)\n\n\n[\"legend.fontsize\"] = 10\ncls_names = list(sorted(cls_stats.keys()))\n\n# Plot accuracy evolution\n()\nfor _, stats in sorted(cls_stats.items()):\n    # Plot accuracy evolution with #examples\n    accuracy, n_examples = zip(*stats[\"accuracy_history\"])\n    plot_accuracy(n_examples, accuracy, \"training examples (#)\")\n    ax = ()\n    ax.set_ylim((0.8, 1))\n(cls_names, loc=\"best\")\n\n()\nfor _, stats in sorted(cls_stats.items()):\n    # Plot accuracy evolution with runtime\n    accuracy, runtime = zip(*stats[\"runtime_history\"])\n    plot_accuracy(runtime, accuracy, \"runtime (s)\")\n    ax = ()\n    ax.set_ylim((0.8, 1))\n(cls_names, loc=\"best\")\n\n# Plot fitting times\n()\nfig = ()\ncls_runtime = [stats[\"total_fit_time\"] for cls_name, stats in sorted(cls_stats.items())]\n\ncls_runtime.append(total_vect_time)\ncls_names.append(\"Vectorization\")\nbar_colors = [\"b\", \"g\", \"r\", \"c\", \"m\", \"y\"]\n\nax = (111)\nrectangles = (range(len(cls_names)), cls_runtime, width=0.5, color=bar_colors)\n\nax.set_xticks((0, len(cls_names) - 1, len(cls_names)))\nax.set_xticklabels(cls_names, fontsize=10)\nymax = max(cls_runtime) * 1.2\nax.set_ylim((0, ymax))\nax.set_ylabel(\"runtime (s)\")\nax.set_title(\"Training Times\")\n\n\ndef autolabel(rectangles):\n    \"\"\"attach some text vi autolabel on rectangles.\"\"\"\n    for rect in rectangles:\n        height = rect.get_height()\n        ax.text(\n            rect.get_x() + rect.get_width() / 2.0,\n            1.05 * height,\n            \"%.4f\" % height,\n            ha=\"center\",\n            va=\"bottom\",\n        )\n        (()[1], rotation=30)\n\n\nautolabel(rectangles)\n()\n()\n\n# Plot prediction times\n()\ncls_runtime = []\ncls_names = list(sorted(cls_stats.keys()))\nfor cls_name, stats in sorted(cls_stats.items()):\n    cls_runtime.append(stats[\"prediction_time\"])\ncls_runtime.append(parsing_time)\ncls_names.append(\"Read/Parse\\n+Feat.Extr.\")\ncls_runtime.append(vectorizing_time)\ncls_names.append(\"Hashing\\n+Vect.\")\n\nax = (111)\nrectangles = (range(len(cls_names)), cls_runtime, width=0.5, color=bar_colors)\n\nax.set_xticks((0, len(cls_names) - 1, len(cls_names)))\nax.set_xticklabels(cls_names, fontsize=8)\n(()[1], rotation=30)\nymax = max(cls_runtime) * 1.2\nax.set_ylim((0, ymax))\nax.set_ylabel(\"runtime (s)\")\nax.set_title(\"Prediction Times (%d instances)\" % n_test_documents)\nautolabel(rectangles)\n()\n()\n\n\n\n<img alt=\"Classification accuracy as a function of training examples (#)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_out_of_core_classification_001.png\" srcset=\"../../_images/sphx_glr_plot_out_of_core_classification_001.png\"/>\n<img alt=\"Classification accuracy as a function of runtime (s)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_out_of_core_classification_002.png\" srcset=\"../../_images/sphx_glr_plot_out_of_core_classification_002.png\"/>\n<img alt=\"Training Times\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_out_of_core_classification_003.png\" srcset=\"../../_images/sphx_glr_plot_out_of_core_classification_003.png\"/>\n<img alt=\"Prediction Times (1000 instances)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_out_of_core_classification_004.png\" srcset=\"../../_images/sphx_glr_plot_out_of_core_classification_004.png\"/>",
            "def get_impute_iterative(X_missing, y_missing):\n    imputer = (\n        missing_values=,\n        add_indicator=True,\n        random_state=0,\n        n_nearest_features=3,\n        max_iter=1,\n        sample_posterior=True,\n    )\n    iterative_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n    return iterative_impute_scores.mean(), iterative_impute_scores.std()\n\n\nmses_california[4], stds_california[4] = get_impute_iterative(\n    X_miss_california, y_miss_california\n)\nmses_diabetes[4], stds_diabetes[4] = get_impute_iterative(\n    X_miss_diabetes, y_miss_diabetes\n)\nx_labels.append(\"Iterative Imputation\")\n\nmses_diabetes = mses_diabetes * -1\nmses_california = mses_california * -1",
            "def add_stl_plot(fig, res, legend):\n    \"\"\"Add 3 plots from a second STL fit\"\"\"\n    axs = fig.get_axes()\n    comps = [\"trend\", \"seasonal\", \"resid\"]\n    for ax, comp in zip(axs[1:], comps):\n        series = getattr(res, comp)\n        if comp == \"resid\":\n            ax.plot(series, marker=\"o\", linestyle=\"none\")\n        else:\n            ax.plot(series)\n            if comp == \"trend\":\n                ax.legend(legend, frameon=False)\n\n\nstl = STL(elec_equip, period=12, robust=True)\nres_robust = stl.fit()\nfig = res_robust.plot()\nres_non_robust = STL(elec_equip, period=12, robust=False).fit()\nadd_stl_plot(fig, res_non_robust, [\"Robust\", \"Non-robust\"])\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_stl_decomposition_10_0.png\" src=\"../../../_images/examples_notebooks_generated_stl_decomposition_10_0.png\"/>",
            "def print_size_of_model(model, label=\"\"):\n    (model.state_dict(), \"temp.p\")\n    size=os.path.getsize(\"temp.p\")\n    print(\"model: \",label,' \\t','Size (KB):', size/1e3)\n    os.remove('temp.p')\n    return size\n\n# compare the sizes\nf=print_size_of_model(float_lstm,\"fp32\")\nq=print_size_of_model(quantized_lstm,\"int8\")\nprint(\"{0:.2f} times smaller\".format(f/q))",
            "def _count_nonzero_coefficients(estimator):\n    a = estimator.coef_.toarray()\n    return (a)\n\n\nconfigurations = [\n    {\n        \"estimator\": ,\n        \"tuned_params\": {\n            \"penalty\": \"elasticnet\",\n            \"alpha\": 0.001,\n            \"loss\": \"modified_huber\",\n            \"fit_intercept\": True,\n            \"tol\": 1e-1,\n            \"n_iter_no_change\": 2,\n        },\n        \"changing_param\": \"l1_ratio\",\n        \"changing_param_values\": [0.25, 0.5, 0.75, 0.9],\n        \"complexity_label\": \"non_zero coefficients\",\n        \"complexity_computer\": _count_nonzero_coefficients,\n        \"prediction_performance_computer\": ,\n        \"prediction_performance_label\": \"Hamming Loss (Misclassification Ratio)\",\n        \"postfit_hook\": lambda x: x.sparsify(),\n        \"data\": classification_data,\n        \"n_samples\": 5,\n    },\n    {\n        \"estimator\": ,\n        \"tuned_params\": {\"C\": 1e3, \"gamma\": 2**-15},\n        \"changing_param\": \"nu\",\n        \"changing_param_values\": [0.05, 0.1, 0.2, 0.35, 0.5],\n        \"complexity_label\": \"n_support_vectors\",\n        \"complexity_computer\": lambda x: len(x.support_vectors_),\n        \"data\": regression_data,\n        \"postfit_hook\": lambda x: x,\n        \"prediction_performance_computer\": ,\n        \"prediction_performance_label\": \"MSE\",\n        \"n_samples\": 15,\n    },\n    {\n        \"estimator\": ,\n        \"tuned_params\": {\n            \"loss\": \"squared_error\",\n            \"learning_rate\": 0.05,\n            \"max_depth\": 2,\n        },\n        \"changing_param\": \"n_estimators\",\n        \"changing_param_values\": [10, 25, 50, 75, 100],\n        \"complexity_label\": \"n_trees\",\n        \"complexity_computer\": lambda x: x.n_estimators,\n        \"data\": regression_data,\n        \"postfit_hook\": lambda x: x,\n        \"prediction_performance_computer\": ,\n        \"prediction_performance_label\": \"MSE\",\n        \"n_samples\": 15,\n    },\n]"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Examples based on real world datasets->Out-of-core classification of text documents->Plot results"
            ],
            [
                "sklearn->Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score->Iterative imputation of the missing values"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition->Robust Fitting"
            ],
            [
                "torch->PyTorch Recipes->Dynamic Quantization->Steps->3. Look at Model Size"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Model Complexity Influence->Choose parameters"
            ]
        ]
    },
    "1232588": {
        "jupyter_code_cell": "input_digit = np.zeros(10).reshape(1, -1)\ninput_digit[:, number] = 1\nconv1 = conv1_model.predict([input_digit, input_noise])\nconv2 = conv2_model.predict([input_digit, input_noise])\nplt.rcParams['figure.figsize'] = (15, 15)\nc1 = np.concatenate([np.concatenate([conv1[:, :, :, j*16+i].squeeze() for i in range(16)], axis=1) for j in range(16)])\nc1img = Image.fromarray((c1 + 1) * 127.5)\nplt.imshow(c1img)",
        "matched_tutorial_code_inds": [
            4654,
            5917,
            1982,
            6795,
            3257
        ],
        "matched_tutorial_codes": [
            "data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>",
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "coef = ((size, size))\ncoef[0:roi_size, 0:roi_size] = -1.0\ncoef[-roi_size:, -roi_size:] = 1.0\n\nX = (n_samples, size**2)\nfor x in X:  # smooth data\n    x[:] = (x.reshape(size, size), sigma=1.0).ravel()\nX -= X.mean(axis=0)\nX /= X.std(axis=0)\n\ny = (X, coef.ravel())",
            "x1n = np.linspace(20.5, 25, 10)\nXnew = np.column_stack((x1n, np.sin(x1n), (x1n - 5) ** 2))\nXnew = sm.add_constant(Xnew)\nynewpred = olsres.predict(Xnew)  # predict out of sample\nprint(ynewpred)",
            "alphas = (-5, 1, 60)\nenet = (l1_ratio=0.7, max_iter=10000)\ntrain_errors = list()\ntest_errors = list()\nfor alpha in alphas:\n    enet.set_params(alpha=alpha)\n    enet.fit(X_train, y_train)\n    train_errors.append(enet.score(X_train, y_train))\n    test_errors.append(enet.score(X_test, y_test))\n\ni_alpha_optim = (test_errors)\nalpha_optim = alphas[i_alpha_optim]\nprint(\"Optimal regularization parameter : %s\" % alpha_optim)\n\n# Estimate the coef_ on full data with optimal regularization parameter\nenet.set_params(alpha=alpha_optim)\ncoef_ = enet.fit(X, y).coef_"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "sklearn->Examples->Clustering->Feature agglomeration vs. univariate selection"
            ],
            [
                "statsmodels->Examples->User Notes->Prediction->Create a new sample of explanatory variables Xnew, predict and plot"
            ],
            [
                "sklearn->Examples->Model Selection->Train error vs Test error->Compute train and test errors"
            ]
        ]
    },
    "455986": {
        "jupyter_code_cell": "modeldata = addDummyVariable(modeldata, 'Agency')\nmodeldata['Sex'].drop_duplicates()",
        "matched_tutorial_code_inds": [
            4139,
            5566,
            3951,
            5423,
            5031
        ],
        "matched_tutorial_codes": [
            "g = sns.PairGrid(iris)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n",
            "data = sm.datasets.star98.load()\ndata.exog = sm.add_constant(data.exog, prepend=False)",
            "flights_array = flights_wide.to_numpy()\nsns.relplot(data=flights_array, kind=\"line\")\n",
            "mfx = affair_mod.get_margeff()\nprint(mfx.summary())",
            "fig.add_subplot(111, axes_class=AA.Axes)\n# Given that 111 is the default, one can also do\nfig.add_subplot(axes_class=AA.Axes)"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Generalized Linear Models Overview->GLM: Binomial response data->Load Star98 data"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing wide-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing wide-form data"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data"
            ],
            [
                "matplotlib->Tutorials->Toolkits->The axisartist toolkit->axisartist"
            ]
        ]
    },
    "413255": {
        "jupyter_code_cell": "countries_raw = requests.get('http://api.worldbank.org/countries?per_page=12000&format=json')\ncountries_raw.text\ncountries_json = json.loads(countries_raw.text)\ncountries_json",
        "matched_tutorial_code_inds": [
            3784,
            4067,
            5423,
            5388,
            5429
        ],
        "matched_tutorial_codes": [
            "import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "tips = sns.load_dataset(\"tips\")\nsns.catplot(data=tips, x=\"day\", y=\"total_bill\")\n",
            "mfx = affair_mod.get_margeff()\nprint(mfx.summary())",
            "margeff = logit_res.get_margeff()\nprint(margeff.summary())",
            "mfx = affair_mod.get_margeff(atexog=resp)\nprint(mfx.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing categorical data->Categorical scatterplots",
                "seaborn->Plotting functions->Visualizing categorical data->Categorical scatterplots"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Logit Model"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data"
            ]
        ]
    },
    "869915": {
        "jupyter_code_cell": "model = smf.ols(formula = 'match_sum ~ amb_iRateMe_exp + amb_iMeasUp_2' , data = subsetdf_OPME1).fit()\nmodel.summary() \nfigure = plt.figure(figsize = (12, 8))\nfigure = sm.graphics.plot_regress_exog(model, 'amb_iRateMe_exp', fig = figure)",
        "matched_tutorial_code_inds": [
            6481,
            6485,
            6489,
            6487,
            6442
        ],
        "matched_tutorial_codes": [
            "model = sm.tsa.UnobservedComponents(series.values,\n                                    level='fixed intercept',\n                                    freq_seasonal=[{'period': 10,\n                                                    'harmonics': 3},\n                                                   {'period': 100,\n                                                    'harmonics': 2}])\nres_f = model.fit(disp=False)\nprint(res_f.summary())\n# The first state variable holds our estimate of the intercept\nprint(\"fixed intercept estimated as {0:.3f}\".format(res_f.smoother_results.smoothed_state[0,-1:][0]))\n\nres_f.plot_components()\nplt.show()\n<br/>",
            "model = sm.tsa.UnobservedComponents(series,\n                                    level='fixed intercept',\n                                    seasonal=10,\n                                    freq_seasonal=[{'period': 100,\n                                                    'harmonics': 2}])\nres_tf = model.fit(disp=False)\nprint(res_tf.summary())\n# The first state variable holds our estimate of the intercept\nprint(\"fixed intercept estimated as {0:.3f}\".format(res_tf.smoother_results.smoothed_state[0,-1:][0]))\n\nfig = res_tf.plot_components()\nfig.tight_layout(pad=1.0)",
            "model = sm.tsa.UnobservedComponents(series,\n                                    level='fixed intercept',\n                                    seasonal=100)\nres_lt = model.fit(disp=False)\nprint(res_lt.summary())\n# The first state variable holds our estimate of the intercept\nprint(\"fixed intercept estimated as {0:.3f}\".format(res_lt.smoother_results.smoothed_state[0,-1:][0]))\n\nfig = res_lt.plot_components()\nfig.tight_layout(pad=1.0)",
            "model = sm.tsa.UnobservedComponents(series,\n                                    level='fixed intercept',\n                                    freq_seasonal=[{'period': 100}])\nres_lf = model.fit(disp=False)\nprint(res_lf.summary())\n# The first state variable holds our estimate of the intercept\nprint(\"fixed intercept estimated as {0:.3f}\".format(res_lf.smoother_results.smoothed_state[0,-1:][0]))\n\nfig = res_lf.plot_components()\nfig.tight_layout(pad=1.0)",
            "mod_ucarima = sm.tsa.UnobservedComponents(endog, 'rwalk', autoregressive=4)\n# Here the powell method is used, since it achieves a\n# higher loglikelihood than the default L-BFGS method\nres_ucarima = mod_ucarima.fit(method='powell', disp=False)\nprint(res_ucarima.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->Seasonality in Time Series Data->Unobserved components (frequency domain modeling)"
            ],
            [
                "statsmodels->Examples->State space models->Seasonality in Time Series Data->Unobserved components (mixed time and frequency domain modeling)"
            ],
            [
                "statsmodels->Examples->State space models->Seasonality in Time Series Data->Unobserved components (lazy time domain seasonal modeling)"
            ],
            [
                "statsmodels->Examples->State space models->Seasonality in Time Series Data->Unobserved components (lazy frequency domain modeling)"
            ],
            [
                "statsmodels->Examples->State space models->Trends and cycles in unemployment->Unobserved components and ARIMA model (UC-ARIMA)"
            ]
        ]
    },
    "196279": {
        "jupyter_code_cell": "import math\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom sklearn import linear_model\nimport statsmodels.formula.api as smf\nfrom statsmodels.sandbox.regression.predstd import wls_prediction_std\n%matplotlib inline\npd.options.display.float_format = '{:.3f}'.format\nimport warnings\nwarnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")\ndataset = pd.read_excel(\"NYCCrime.xls\", header=4)",
        "matched_tutorial_code_inds": [
            5256,
            3384,
            3218,
            1838,
            6250
        ],
        "matched_tutorial_codes": [
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pandas_datareader as pdr\nimport seaborn\n\nimport statsmodels.api as sm\nfrom statsmodels.regression.rolling import RollingOLS\n\nseaborn.set_style(\"darkgrid\")\npd.plotting.register_matplotlib_converters()\n%matplotlib inline",
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import \nfrom sklearn.model_selection import \nfrom sklearn.pipeline import \nfrom sklearn.svm import \nfrom sklearn.decomposition import , \nfrom sklearn.feature_selection import , \nfrom sklearn.preprocessing import \n\nX, y = (return_X_y=True)\n\npipe = (\n    [\n        (\"scaling\", ()),\n        # the reduce_dim stage is populated by the param_grid\n        (\"reduce_dim\", \"passthrough\"),\n        (\"classify\", (dual=False, max_iter=10000)),\n    ]\n)\n\nN_FEATURES_OPTIONS = [2, 4, 8]\nC_OPTIONS = [1, 10, 100, 1000]\nparam_grid = [\n    {\n        \"reduce_dim\": [(iterated_power=7), (max_iter=1_000)],\n        \"reduce_dim__n_components\": N_FEATURES_OPTIONS,\n        \"classify__C\": C_OPTIONS,\n    },\n    {\n        \"reduce_dim\": [()],\n        \"reduce_dim__k\": N_FEATURES_OPTIONS,\n        \"classify__C\": C_OPTIONS,\n    },\n]\nreducer_labels = [\"PCA\", \"NMF\", \"KBest(mutual_info_classif)\"]\n\ngrid = (pipe, n_jobs=1, param_grid=param_grid)\ngrid.fit(X, y)",
            "import math\n\ncolumn_results = param_names + [\"mean_test_score\", \"mean_score_time\"]\n\ntransform_funcs = dict.fromkeys(column_results, lambda x: x)\n# Using a logarithmic scale for alpha\ntransform_funcs[\"alpha\"] = \n# L1 norms are mapped to index 1, and L2 norms to index 2\ntransform_funcs[\"norm\"] = lambda x: 2 if x == \"l2\" else 1\n# Unigrams are mapped to index 1 and bigrams to index 2\ntransform_funcs[\"ngram_range\"] = lambda x: x[1]\n\nfig = px.parallel_coordinates(\n    cv_results[column_results].apply(transform_funcs),\n    color=\"mean_test_score\",\n    color_continuous_scale=px.colors.sequential.Viridis_r,\n    labels=labels,\n)\nfig.update_layout(\n    title={\n        \"text\": \"Parallel coordinates plot of text classifier pipeline\",\n        \"y\": 0.99,\n        \"x\": 0.5,\n        \"xanchor\": \"center\",\n        \"yanchor\": \"top\",\n    }\n)\nfig\n\n\n\n <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script> <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n<script src=\"https://cdn.plot.ly/plotly-2.18.0.min.js\"></script>  <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c6acaacd-52bc-4061-a9e6-b279cc3bdbdb\")) {                    Plotly.newPlot(                        \"c6acaacd-52bc-4061-a9e6-b279cc3bdbdb\",                        [{\"dimensions\":[{\"label\":\"max_df\",\"values\":[1.0,0.6,0.6,1.0,0.2,0.2,0.2,1.0,0.2,0.4,1.0,0.4,0.6,0.2,0.6,0.8,0.8,0.8,0.4,0.8,0.2,0.4,0.8,0.4,0.6,1.0,0.2,0.8,0.8,1.0,0.2,0.8,0.4,0.6,0.2,1.0,0.6,0.8,0.8,0.2]},{\"label\":\"min_df\",\"values\":[3,3,10,10,3,1,1,5,5,10,5,10,3,1,10,3,5,1,5,1,1,5,1,10,1,1,1,1,10,5,5,10,1,3,5,1,3,3,3,1]},{\"label\":\"ngram_range\",\"values\":[2,2,1,2,2,2,2,2,2,2,2,2,2,1,2,2,1,1,2,2,1,2,2,2,1,2,2,2,1,1,2,2,2,2,1,1,2,2,2,2]},{\"label\":\"norm\",\"values\":[2,2,1,2,2,2,2,1,2,1,2,2,1,1,2,1,1,2,1,1,2,2,2,2,2,1,1,2,1,1,2,2,1,1,2,2,1,2,2,2]},{\"label\":\"alpha\",\"values\":[1.0,2.0,-2.0,-3.0,0.0,3.0,0.0,5.0,-3.0,-3.0,-6.0,2.0,2.0,-2.0,-2.0,-3.0,4.0,2.0,0.0,0.0,-6.0,-6.0,0.0,-6.0,0.0,0.0,6.0,4.0,-2.0,-3.0,-2.0,5.0,-6.0,5.0,4.0,4.0,0.0,6.0,-3.0,-1.0]},{\"label\":\"CV score (accuracy)\",\"values\":[0.6067319461444309,0.6114035087719298,0.7444308445532435,0.7385624915000679,0.7735890112879098,0.6988916088671291,0.7350537195702435,0.5775873793009656,0.7723922208622331,0.7327281381748946,0.76890384876921,0.6603835169318646,0.599734802121583,0.8156262749898001,0.73625050999592,0.7958044335645316,0.5729294165646674,0.5705902352781178,0.6661906704746362,0.5857405140758873,0.7841425268597851,0.7665714674282607,0.670875832993336,0.7304229566163472,0.7000271997824018,0.5810825513395892,0.6673806609547123,0.5775873793009656,0.7479464164286685,0.7770909832721339,0.7747314021487828,0.5845913232694139,0.8109547123623011,0.599734802121583,0.7280769753841969,0.5624303005575955,0.6195498436012512,0.5752685978512172,0.7829389364885082,0.8132666938664489]},{\"label\":\"CV Score time (s)\",\"values\":[0.04458394050598145,0.04707136154174805,0.023392200469970703,0.03561687469482422,0.04037842750549316,0.0492828369140625,0.04824423789978027,0.037982940673828125,0.0380521297454834,0.03654317855834961,0.039098358154296874,0.0368809700012207,0.04478645324707031,0.02125234603881836,0.03831219673156738,0.046416950225830075,0.021130466461181642,0.021540355682373048,0.0394838809967041,0.04886045455932617,0.022491979598999023,0.038994884490966795,0.048995256423950195,0.03816032409667969,0.02209053039550781,0.04859938621520996,0.04920797348022461,0.05251688957214355,0.019916343688964843,0.023452281951904297,0.038121604919433595,0.03604555130004883,0.04728374481201172,0.04159135818481445,0.020193862915039062,0.02047595977783203,0.04090127944946289,0.04113917350769043,0.04163060188293457,0.048309040069580075]}],\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"line\":{\"color\":[0.6067319461444309,0.6114035087719298,0.7444308445532435,0.7385624915000679,0.7735890112879098,0.6988916088671291,0.7350537195702435,0.5775873793009656,0.7723922208622331,0.7327281381748946,0.76890384876921,0.6603835169318646,0.599734802121583,0.8156262749898001,0.73625050999592,0.7958044335645316,0.5729294165646674,0.5705902352781178,0.6661906704746362,0.5857405140758873,0.7841425268597851,0.7665714674282607,0.670875832993336,0.7304229566163472,0.7000271997824018,0.5810825513395892,0.6673806609547123,0.5775873793009656,0.7479464164286685,0.7770909832721339,0.7747314021487828,0.5845913232694139,0.8109547123623011,0.599734802121583,0.7280769753841969,0.5624303005575955,0.6195498436012512,0.5752685978512172,0.7829389364885082,0.8132666938664489],\"coloraxis\":\"coloraxis\"},\"name\":\"\",\"type\":\"parcoords\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"CV score (accuracy)\"}},\"colorscale\":[[0.0,\"#fde725\"],[0.1111111111111111,\"#b5de2b\"],[0.2222222222222222,\"#6ece58\"],[0.3333333333333333,\"#35b779\"],[0.4444444444444444,\"#1f9e89\"],[0.5555555555555556,\"#26828e\"],[0.6666666666666666,\"#31688e\"],[0.7777777777777778,\"#3e4989\"],[0.8888888888888888,\"#482878\"],[1.0,\"#440154\"]]},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60},\"title\":{\"text\":\"Parallel coordinates plot of text classifier pipeline\",\"y\":0.99,\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"}},                        {\"responsive\": true}                    )                };                            </script> \n\n<br/>\n<br/>",
            "import scipy\nimport numpy as np\nfrom sklearn.model_selection import \nfrom sklearn.cluster import \nfrom sklearn.datasets import \nfrom sklearn.metrics import \n\nrng = (0)\nX, y = (random_state=rng)\nX = (X)\nX_train, X_test, _, y_test = (X, y, random_state=rng)\nkmeans = (n_init=\"auto\").fit(X_train)\nprint((kmeans.predict(X_test), y_test))",
            "import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n\n%matplotlib inline\n\ndata = [\n    446.6565,\n    454.4733,\n    455.663,\n    423.6322,\n    456.2713,\n    440.5881,\n    425.3325,\n    485.1494,\n    506.0482,\n    526.792,\n    514.2689,\n    494.211,\n]\nindex = pd.date_range(start=\"1996\", end=\"2008\", freq=\"A\")\noildata = pd.Series(data, index)\n\ndata = [\n    17.5534,\n    21.86,\n    23.8866,\n    26.9293,\n    26.8885,\n    28.8314,\n    30.0751,\n    30.9535,\n    30.1857,\n    31.5797,\n    32.5776,\n    33.4774,\n    39.0216,\n    41.3864,\n    41.5966,\n]\nindex = pd.date_range(start=\"1990\", end=\"2005\", freq=\"A\")\nair = pd.Series(data, index)\n\ndata = [\n    263.9177,\n    268.3072,\n    260.6626,\n    266.6394,\n    277.5158,\n    283.834,\n    290.309,\n    292.4742,\n    300.8307,\n    309.2867,\n    318.3311,\n    329.3724,\n    338.884,\n    339.2441,\n    328.6006,\n    314.2554,\n    314.4597,\n    321.4138,\n    329.7893,\n    346.3852,\n    352.2979,\n    348.3705,\n    417.5629,\n    417.1236,\n    417.7495,\n    412.2339,\n    411.9468,\n    394.6971,\n    401.4993,\n    408.2705,\n    414.2428,\n]\nindex = pd.date_range(start=\"1970\", end=\"2001\", freq=\"A\")\nlivestock2 = pd.Series(data, index)\n\ndata = [407.9979, 403.4608, 413.8249, 428.105, 445.3387, 452.9942, 455.7402]\nindex = pd.date_range(start=\"2001\", end=\"2008\", freq=\"A\")\nlivestock3 = pd.Series(data, index)\n\ndata = [\n    41.7275,\n    24.0418,\n    32.3281,\n    37.3287,\n    46.2132,\n    29.3463,\n    36.4829,\n    42.9777,\n    48.9015,\n    31.1802,\n    37.7179,\n    40.4202,\n    51.2069,\n    31.8872,\n    40.9783,\n    43.7725,\n    55.5586,\n    33.8509,\n    42.0764,\n    45.6423,\n    59.7668,\n    35.1919,\n    44.3197,\n    47.9137,\n]\nindex = pd.date_range(start=\"2005\", end=\"2010-Q4\", freq=\"QS-OCT\")\naust = pd.Series(data, index)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Rolling Least Squares"
            ],
            [
                "sklearn->Examples->Pipelines and composite estimators->Selecting dimensionality reduction with Pipeline and GridSearchCV->Illustration of Pipeline and GridSearchCV"
            ],
            [
                "sklearn->Examples->Model Selection->Sample pipeline for text feature extraction and evaluation->Pipeline with hyperparameter tuning"
            ],
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 0.23->Scalability and stability improvements to KMeans"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Loading data"
            ]
        ]
    },
    "1094427": {
        "jupyter_code_cell": "class MyStreamListener(tweepy.StreamListener):\n    def on_data(self, data):\n        data = json.loads(data)\n        tweet_id = int(data['id'])\n        tweet_created_at = data['created_at']\n        tweet_author = data['user']['screen_name']\n        author_location = data['user']['location']\n        author_followers = data['user']['followers_count'] if not None else 0\n        author_friends = data['user']['friends_count'] if not None else 0\n        hashtags = data['entities']['hashtags']\n        tweet_hashtags = []\n        for hashtag in hashtags:\n            tweet_hashtags.append(\"#\" + str(hashtag['text']))\n        tweet_hashtags = \",\".join(tweet_hashtags)\n        tweet_text = data['text']\n        in_reply_to = data['in_reply_to_screen_name']\n        tweet_lang = data['lang']\n        conn.execute('INSERT INTO tweets (id, created_at, author, author_location, author_followers, author_friends, hashtags, tweet, in_reply_to, lang, method) VALUES (?,?,?,?,?,?,?,?,?,?,?)', (tweet_id, tweet_created_at, tweet_author, author_location, author_followers, author_friends, tweet_hashtags, tweet_text, in_reply_to, tweet_lang, \"StreamingAPI\"))\n        conn.commit()\n        cursor = conn.cursor()\n        cursor.execute(\"select * from tweets\")\n        r = cursor.fetchall() \n        print(\"\\rTweet from \" + str(tweet_created_at[:-10]) + \" (\" + str(len(r)) +\")              \", end='')\ntwitter_stream = tweepy.Stream(auth, MyStreamListener())\nstarttime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\nprint(\"Running since \" + str(starttime))\nwhile True:\n    try:\n        twitter_stream.filter(track=streamingquery)\n    except KeyError:\n        pass\n    except sqlite3.IntegrityError: \n        pass\ndb_a = sqlite3.connect('livetweets.db')\ndb_b = sqlite3.connect('histtweets.db')\nb_cursor = db_b.cursor()\nb_cursor.execute('SELECT * FROM tweets')\noutput = b_cursor.fetchall()   \na_cursor = db_a.cursor()\nfor row in output:\n    try:\n        a_cursor.execute('INSERT INTO tweets VALUES (?,?,?,?,?,?,?,?,?,?,?)', row)\n    except sqlite3.IntegrityError: \n        pass\ndb_a.commit()\na_cursor.close()\nb_cursor.close()\nos.rename('livetweets.db', 'tweets.db')\nos.remove('histtweets.db')",
        "matched_tutorial_code_inds": [
            1305,
            1200,
            659,
            22,
            1304
        ],
        "matched_tutorial_codes": [
            "class DistResNet50(nn.Module):\n    def __init__(self, num_split, workers, *args, **kwargs):\n        super(DistResNet50, self).__init__()\n\n        self.num_split = num_split\n\n        # Put the first part of the ResNet50 on workers[0]\n        self.p1_rref = rpc.remote(\n            workers[0],\n            ResNetShard1,\n            args = (\"cuda:0\",) + args,\n            kwargs = kwargs\n        )\n\n        # Put the second part of the ResNet50 on workers[1]\n        self.p2_rref = rpc.remote(\n            workers[1],\n            ResNetShard2,\n            args = (\"cuda:1\",) + args,\n            kwargs = kwargs\n        )\n\n    def forward(self, xs):\n        out_futures = []\n        for x in iter(xs.split(self.num_split, dim=0)):\n            x_rref = RRef(x)\n            y_rref = self.p1_rref.remote().forward(x_rref)\n            z_fut = self.p2_rref.rpc_async().forward(y_rref)\n            out_futures.append(z_fut)\n\n        return torch.cat(torch.futures.wait_all(out_futures))\n\n    def parameter_rrefs(self):\n        remote_params = []\n        remote_params.extend(self.p1_rref.remote().parameter_rrefs().to_here())\n        remote_params.extend(self.p2_rref.remote().parameter_rrefs().to_here())\n        return remote_params",
            "class CausalSelfAttention():\n\n    def __init__(self, num_heads: int, embed_dimension: int, bias: bool=False, is_causal: bool=False, dropout:float=0.0):\n        super().__init__()\n        assert embed_dimension % num_heads == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = (embed_dimension, 3 * embed_dimension, bias=bias)\n        # output projection\n        self.c_proj = (embed_dimension, embed_dimension, bias=bias)\n        # regularization\n        self.dropout = dropout\n        self.resid_dropout = (dropout)\n        self.num_heads = num_heads\n        self.embed_dimension = embed_dimension\n        # Perform causal masking\n        self.is_causal = is_causal\n\n    def forward(self, ):\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        query_projected = self.c_attn()\n\n        batch_size = query_projected.size(0)\n        embed_dim = query_projected.size(2)\n        head_dim = embed_dim // (self.num_heads * 3)\n\n        , ,  = query_projected.chunk(3, -1)\n         = .view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n         = .view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n         = .view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n\n        if self.training:\n            dropout = self.dropout\n            is_causal = self.is_causal\n        else:\n            dropout = 0.0\n            is_causal = False\n\n        y = (, , , attn_mask=None, dropout_p=dropout, is_causal=is_causal)\n        y = y.transpose(1, 2).view(batch_size, -1, self.num_heads * head_dim)\n\n        y = self.resid_dropout(self.c_proj(y))\n        return y\n\n\nnum_heads = 8\nheads_per_dim = 64\nembed_dimension = num_heads * heads_per_dim\n = \nmodel = (num_heads=num_heads, embed_dimension=embed_dimension, bias=False, is_causal=True, dropout=0.1).to(\"cuda\").to().eval()\nprint(model)",
            "class ProfilingInterpreter():\n    def __init__(self, mod : ):\n        # Rather than have the user symbolically trace their model,\n        # we're going to do it in the constructor. As a result, the\n        # user can pass in any ``Module`` without having to worry about\n        # symbolic tracing APIs\n        gm = (mod)\n        super().__init__(gm)\n\n        # We are going to store away two things here:\n        #\n        # 1. A list of total runtimes for ``mod``. In other words, we are\n        #    storing away the time ``mod(...)`` took each time this\n        #    interpreter is called.\n        self.total_runtime_sec : List[float] = []\n        # 2. A map from ``Node`` to a list of times (in seconds) that\n        #    node took to run. This can be seen as similar to (1) but\n        #    for specific sub-parts of the model.\n        self.runtimes_sec : Dict[, List[float]] = {}\n\n    ######################################################################\n    # Next, let's override our first method: ``run()``. ``Interpreter``'s ``run``\n    # method is the top-level entrypoint for execution of the model. We will\n    # want to intercept this so that we can record the total runtime of the\n    # model.\n\n    def run(self, *args) -&gt; Any:\n        # Record the time we started running the model\n        t_start = time.time()\n        # Run the model by delegating back into Interpreter.run()\n        return_val = super().run(*args)\n        # Record the time we finished running the model\n        t_end = time.time()\n        # Store the total elapsed time this model execution took in the\n        # ProfilingInterpreter\n        self.total_runtime_sec.append(t_end - t_start)\n        return return_val\n\n    ######################################################################\n    # Now, let's override ``run_node``. ``Interpreter`` calls ``run_node`` each\n    # time it executes a single node. We will intercept this so that we\n    # can measure and record the time taken for each individual call in\n    # the model.\n\n    def run_node(self, n : ) -&gt; Any:\n        # Record the time we started running the op\n        t_start = time.time()\n        # Run the op by delegating back into Interpreter.run_node()\n        return_val = super().run_node(n)\n        # Record the time we finished running the op\n        t_end = time.time()\n        # If we don't have an entry for this node in our runtimes_sec\n        # data structure, add one with an empty list value.\n        self.runtimes_sec.setdefault(n, [])\n        # Record the total elapsed time for this single invocation\n        # in the runtimes_sec data structure\n        self.runtimes_sec[n].append(t_end - t_start)\n        return return_val\n\n    ######################################################################\n    # Finally, we are going to define a method (one which doesn't override\n    # any ``Interpreter`` method) that provides us a nice, organized view of\n    # the data we have collected.\n\n    def summary(self, should_sort : bool = False) -&gt; str:\n        # Build up a list of summary information for each node\n        node_summaries : List[List[Any]] = []\n        # Calculate the mean runtime for the whole network. Because the\n        # network may have been called multiple times during profiling,\n        # we need to summarize the runtimes. We choose to use the\n        # arithmetic mean for this.\n        mean_total_runtime = statistics.mean(self.total_runtime_sec)\n\n        # For each node, record summary statistics\n        for node, runtimes in self.runtimes_sec.items():\n            # Similarly, compute the mean runtime for ``node``\n            mean_runtime = statistics.mean(runtimes)\n            # For easier understanding, we also compute the percentage\n            # time each node took with respect to the whole network.\n            pct_total = mean_runtime / mean_total_runtime * 100\n            # Record the node's type, name of the node, mean runtime, and\n            # percent runtim\n            node_summaries.append(\n                [node.op, str(node), mean_runtime, pct_total])\n\n        # One of the most important questions to answer when doing performance\n        # profiling is \"Which op(s) took the longest?\". We can make this easy\n        # to see by providing sorting functionality in our summary view\n        if should_sort:\n            node_summaries.sort(key=lambda s: s[2], reverse=True)\n\n        # Use the ``tabulate`` library to create a well-formatted table\n        # presenting our summary information\n        headers : List[str] = [\n            'Op type', 'Op', 'Average runtime (s)', 'Pct total runtime'\n        ]\n        return tabulate.tabulate(node_summaries, headers=headers)",
            "class FaceLandmarksDataset(Dataset):\n    \"\"\"Face Landmarks dataset.\"\"\"\n\n    def __init__(self, csv_file, root_dir, transform=None):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.landmarks_frame = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.landmarks_frame)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        img_name = os.path.join(self.root_dir,\n                                self.landmarks_frame.iloc[idx, 0])\n        image = io.imread(img_name)\n        landmarks = self.landmarks_frame.iloc[idx, 1:]\n        landmarks = np.array([landmarks])\n        landmarks = landmarks.astype('float').reshape(-1, 2)\n        sample = {'image': image, 'landmarks': landmarks}\n\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample",
            "class ResNetShard1(ResNetBase):\n    def __init__(self, device, *args, **kwargs):\n        super(ResNetShard1, self).__init__(\n            Bottleneck, 64, num_classes=num_classes, *args, **kwargs)\n\n        self.device = device\n        self.seq = nn.Sequential(\n            nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False),\n            self._norm_layer(self.inplanes),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n            self._make_layer(64, 3),\n            self._make_layer(128, 4, stride=2)\n        ).to(self.device)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x_rref):\n        x = x_rref.to_here().to(self.device)\n        with self._lock:\n            out =  self.seq(x)\n        return out.cpu()\n\n\nclass ResNetShard2(ResNetBase):\n    def __init__(self, device, *args, **kwargs):\n        super(ResNetShard2, self).__init__(\n            Bottleneck, 512, num_classes=num_classes, *args, **kwargs)\n\n        self.device = device\n        self.seq = nn.Sequential(\n            self._make_layer(256, 6, stride=2),\n            self._make_layer(512, 3, stride=2),\n            nn.AdaptiveAvgPool2d((1, 1)),\n        ).to(self.device)\n\n        self.fc =  nn.Linear(512 * self._block.expansion, num_classes).to(self.device)\n\n    def forward(self, x_rref):\n        x = x_rref.to_here().to(self.device)\n        with self._lock:\n            out = self.fc(torch.flatten(self.seq(x), 1))\n        return out.cpu()"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Parallel and Distributed Training->Distributed Pipeline Parallelism Using RPC->Step 2: Stitch ResNet50 Model Shards Into One Module"
            ],
            [
                "torch->Model Optimization->(Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA)->Causal Self Attention"
            ],
            [
                "torch->Code Transforms with FX->(beta) Building a Simple CPU Performance Profiler with FX->Creating a Profiling Interpreter"
            ],
            [
                "torch->PyTorch Recipes->Developing Custom PyTorch Dataloaders->Part 1: The Dataset->1.2 Create a dataset class"
            ],
            [
                "torch->Parallel and Distributed Training->Distributed Pipeline Parallelism Using RPC->Step 1: Partition ResNet50 Model"
            ]
        ]
    },
    "126499": {
        "jupyter_code_cell": "notebookdir = os.getcwd()\nhs=hydroshare.hydroshare()\nhomedir = hs.getContentPath(os.environ[\"HS_RES_ID\"])\nos.chdir(homedir)\nprint('Data will be loaded from and save to:'+homedir)\nhs.getResourceFromHydroShare('ef2d82bf960144b4bfb1bae6242bcc7f')\nNAmer = hs.content['NAmer_dem_list.shp']\nhs.getResourceFromHydroShare('c532e0578e974201a0bc40a37ef2d284')\nsauk = hs.content['wbdhub12_17110006_WGS84_Basin.shp']\nhs.getResourceFromHydroShare('4aff8b10bc424250b3d7bac2188391e8', )\nelwha = hs.content[\"elwha_ws_bnd_wgs84.shp\"]\nhs.getResourceFromHydroShare('5c041d95ceb64dce8eb85d2a7db88ed7')\nriosalado = hs.content['UpperRioSalado_delineatedBoundary.shp']",
        "matched_tutorial_code_inds": [
            1092,
            1745,
            63,
            3194,
            6891
        ],
        "matched_tutorial_codes": [
            "data_path = '~/.data/imagenet'\nsaved_model_dir = 'data/'\nfloat_model_file = 'mobilenet_pretrained_float.pth'\nscripted_float_model_file = 'mobilenet_quantization_scripted.pth'\nscripted_quantized_model_file = 'mobilenet_quantization_scripted_quantized.pth'\n\ntrain_batch_size = 30\neval_batch_size = 50\n\ndata_loader, data_loader_test = prepare_data_loaders(data_path)\ncriterion = nn.CrossEntropyLoss()\nfloat_model = load_model(saved_model_dir + float_model_file).to('cpu')\n\n# Next, we'll \"fuse modules\"; this can both make the model faster by saving on memory access\n# while also improving numerical accuracy. While this can be used with any model, this is\n# especially common with quantized models.\n\nprint('\\n Inverted Residual Block: Before fusion \\n\\n', float_model.features[1].conv)\nfloat_model.eval()\n\n# Fuses modules\nfloat_model.fuse_model()\n\n# Note fusion of Conv+BN+Relu and Conv+Relu\nprint('\\n Inverted Residual Block: After fusion\\n\\n',float_model.features[1].conv)",
            "speech_data_path = 'tutorial-nlp-from-scratch/speeches.csv'\nspeech_df = pd.read_csv(speech_data_path)\nX_pred = textproc.cleantext(speech_df,\n                            text_column='speech',\n                            remove_stopwords=True,\n                            remove_punc=False)\nspeakers = speech_df['speaker'].to_numpy()",
            "model.qconfig = torch.quantization.get_default_qconfig('qnnpack')\ntorch.quantization.prepare(model, inplace=True)\n# Calibrate your model\ndef calibrate(model, calibration_data):\n    # Your calibration code here\n    return\ncalibrate(model, [])\ntorch.quantization.convert(model, inplace=True)",
            "display = (\n    recall=recall[\"micro\"],\n    precision=precision[\"micro\"],\n    average_precision=average_precision[\"micro\"],\n)\ndisplay.plot()\n_ = display.ax_.set_title(\"Micro-averaged over all classes\")\n\n\n<img alt=\"Micro-averaged over all classes\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_precision_recall_003.png\" srcset=\"../../_images/sphx_glr_plot_precision_recall_003.png\"/>",
            "url = 'https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/csv/COUNT/medpar.csv'\nmedpar = read.csv(url)\nf = los~factor(type)+hmo+white\n\nlibrary(MASS)\nmod = glm.nb(f, medpar)\ncoef(summary(mod))\n                 Estimate Std. Error   z value      Pr(|z|)\n(Intercept)    2.31027893 0.06744676 34.253370 3.885556e-257\nfactor(type)2  0.22124898 0.05045746  4.384861  1.160597e-05\nfactor(type)3  0.70615882 0.07599849  9.291748  1.517751e-20\nhmo           -0.06795522 0.05321375 -1.277024  2.015939e-01\nwhite         -0.12906544 0.06836272 -1.887951  5.903257e-02"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Model Optimization->(beta) Static Quantization with Eager Mode in PyTorch->3. Define dataset and data loaders->ImageNet Data"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ],
            [
                "torch->PyTorch Recipes->Pytorch Mobile Performance Recipes->Model preparation->2. Quantize your model"
            ],
            [
                "sklearn->Examples->Model Selection->Precision-Recall->In multi-label settings->Plot the micro-averaged Precision-Recall curve"
            ],
            [
                "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 2: Negative Binomial Regression for Count Data->Testing"
            ]
        ]
    },
    "1175442": {
        "jupyter_code_cell": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib as mpl\nimport numpy as np\nimport pandas as pd\nimport scipy as sp\nfrom scipy.optimize import curve_fit\nfrom scipy.signal import savgol_filter\nplt.style.use('default')\nplt.style.use('seaborn-paper')\ncurrent_palette = sns.color_palette(palette=None)\nsns.set_palette(palette=None)\nsns.palplot(current_palette)\nplot_width = 3.487\nplot_height = plot_width / 1.618\ndef dataparsing(input_file, output_file, data_width):\n    j = 0\n    with open(input_file, 'r') as f_i:    \n        for line, data in enumerate(f_i):\n            if line % data_width == 0:\n                j = j + 1\n            else:\n                with open(output_file + str(j) + '.txt','a+') as f_o:\n                    f_o.write(data)\ninput_file = 'Data\\\\4_uc_new device\\\\dev16\\\\Dev16_Vgs-sweep -2V to 1V -- Vds 0V to 1V.txt'\noutput_file = 'Data\\\\4_uc_new device\\\\dev16\\\\Dev16_RoomT_Vgs_sweep\\\\dev16_dataset'\ndata_width = 606\ndataparsing(input_file, output_file, data_width)",
        "matched_tutorial_code_inds": [
            5256,
            1393,
            1212,
            4752,
            4700
        ],
        "matched_tutorial_codes": [
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pandas_datareader as pdr\nimport seaborn\n\nimport statsmodels.api as sm\nfrom statsmodels.regression.rolling import RollingOLS\n\nseaborn.set_style(\"darkgrid\")\npd.plotting.register_matplotlib_converters()\n%matplotlib inline",
            "import matplotlib.pyplot as plt\nimport numpy as np\nidx = 5\nprint(\"Question: \", dataset[\"train\"][idx][\"question\"])\nprint(\"Answers: \" ,dataset[\"train\"][idx][\"answers\"])\nim = np.asarray(dataset[\"train\"][idx][\"image\"].resize((500,500)))\nplt.imshow(im)\nplt.show()\n\n\n<img alt=\"flava finetuning tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_flava_finetuning_tutorial_001.png\" srcset=\"../_images/sphx_glr_flava_finetuning_tutorial_001.png\"/>",
            "import matplotlib.pyplot as plt\nplt.switch_backend('Agg')\nimport numpy as np\nimport timeit\n\nnum_repeat = 10\n\nstmt = \"train(model)\"\n\nsetup = \"model = ModelParallelResNet50()\"\nmp_run_times = timeit.repeat(\n    stmt, setup, number=1, repeat=num_repeat, globals=globals())\nmp_mean, mp_std = np.mean(mp_run_times), np.std(mp_run_times)\n\nsetup = \"import torchvision.models as models;\" + \\\n        \"model = models.resnet50(num_classes=num_classes).to('cuda:0')\"\nrn_run_times = timeit.repeat(\n    stmt, setup, number=1, repeat=num_repeat, globals=globals())\nrn_mean, rn_std = np.mean(rn_run_times), np.std(rn_run_times)\n\n\ndef plot(means, stds, , fig_name):\n    fig, ax = plt.subplots()\n    ax.bar(np.arange(len(means)), means, yerr=stds,\n           align='center', alpha=0.5, ecolor='red', capsize=10, width=0.6)\n    ax.set_ylabel('ResNet50 Execution Time (Second)')\n    ax.set_xticks(np.arange(len(means)))\n    ax.set_xticklabels()\n    ax.yaxis.grid(True)\n    plt.tight_layout()\n    plt.savefig(fig_name)\n    plt.close(fig)\n\n\nplot([mp_mean, rn_mean],\n     [mp_std, rn_std],\n     ['Model Parallel', 'Single GPU'],\n     'mp_vs_rn.png')\n\n\n\n<img alt=\"\" src=\"../_images/mp_vs_rn.png\"/>",
            "import matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nred_patch = mpatches.Patch(color='red', label='The red data')\nax.legend(handles=[red_patch])\n\nplt.show()\n\n\n<img alt=\"legend guide\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_legend_guide_001.png\" srcset=\"../../_images/sphx_glr_legend_guide_001.png, ../../_images/sphx_glr_legend_guide_001_2_0x.png 2.0x\"/>",
            "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom cycler import cycler\nmpl.rcParams['lines.linewidth'] = 2\nmpl.rcParams['lines.linestyle'] = '--'\ndata = np.random.randn(50)\nplt.plot(data)\n\n\n<img alt=\"customizing\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_customizing_001.png\" srcset=\"../../_images/sphx_glr_customizing_001.png, ../../_images/sphx_glr_customizing_001_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Rolling Least Squares"
            ],
            [
                "torch->Multimodality->TorchMultimodal Tutorial: Finetuning FLAVA->Steps"
            ],
            [
                "torch->Parallel and Distributed Training->Single-Machine Model Parallel Best Practices->Apply Model Parallel to Existing Modules"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Legend guide->Creating artists specifically for adding to the legend (aka. Proxy artists)"
            ],
            [
                "matplotlib->Tutorials->Introductory->Customizing Matplotlib with style sheets and rcParams->Runtime rc settings"
            ]
        ]
    },
    "988965": {
        "jupyter_code_cell": "d = 0.5\nsel = dbars[['price']].iloc[:100]\nx = np.log(dbars.price).cumsum()\ncprint(x)\nx.plot()",
        "matched_tutorial_code_inds": [
            5703,
            3834,
            3852,
            5345,
            3610
        ],
        "matched_tutorial_codes": [
            "summ_df = infl.summary_frame()\nsumm_df.sort_values(\"cooks_d\", ascending=False)[:10]",
            "ax = res_lagged.params.drop(['Intercept', 'trend']).plot.bar(rot=0)\nplt.ylabel('Coefficeint')\nsns.despine()",
            "pred = res_seasonal.get_prediction(start='2001-03-01')\npred_ci = pred.conf_int()",
            "fig = sm.graphics.plot_partregress_grid(crime_model)\nfig.tight_layout(pad=1.0)",
            "first = df.groupby('unique_carrier').first()\nfirst.ix[10:15, ['fl_date', 'tail_num']]"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Generalized Linear Models->Influence Measures for GLMs->get the influence measures"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Forecasting"
            ],
            [
                "statsmodels->Examples->Plotting->Regression Plots->Statewide Crime 2009 Dataset->Partial Regression Plots (Crime Data)"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing"
            ]
        ]
    },
    "9974": {
        "jupyter_code_cell": "plot_learning_curves(ridge_reg, X, y,141)\nplot_learning_curves(ridge_reg2, X, y,142)\nplot_learning_curves(ridge_reg3, X, y,143)\nplot_learning_curves(ridge_reg4, X, y,141)\nplot_learning_curves(ridge_reg5, X, y,142)\nplot_learning_curves(ridge_reg6, X, y,143)",
        "matched_tutorial_code_inds": [
            5215,
            5720,
            5437,
            5715,
            5439
        ],
        "matched_tutorial_codes": [
            "glsar_model = sm.GLSAR(data.endog, data.exog, 1)\nglsar_results = glsar_model.iterative_fit(1)\nprint(glsar_results.summary())",
            "plt.clf()\nplt.grid(True)\nplt.plot(result2.predict(linear=True), result2.resid_pearson, \"o\")\nplt.xlabel(\"Linear predictor\")\nplt.ylabel(\"Residual\")",
            "fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nsupport = np.linspace(-6, 6, 1000)\nax.plot(support, stats.logistic.cdf(support), \"r-\", label=\"Logistic\")\nax.plot(support, stats.norm.cdf(support), label=\"Probit\")\nax.legend()",
            "plt.clf()\nplt.grid(True)\nplt.plot(result1.predict(linear=True), result1.resid_pearson, \"o\")\nplt.xlabel(\"Linear predictor\")\nplt.ylabel(\"Residual\")",
            "fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nsupport = np.linspace(-6, 6, 1000)\nax.plot(support, stats.logistic.pdf(support), \"r-\", label=\"Logistic\")\nax.plot(support, stats.norm.pdf(support), label=\"Probit\")\nax.legend()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Generalized Least Squares"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Quasi-binomial regression"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Exercise: Logit vs Probit"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Quasi-binomial regression"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Exercise: Logit vs Probit"
            ]
        ]
    },
    "314889": {
        "jupyter_code_cell": "RF.score(X_test, y_test)\nrf_all_features = pd.DataFrame(fit_all_rf.feature_importances_,\n                                   index = X.columns,\n                                    columns=['importance']).sort_values('importance',\n                                                                        ascending=False)",
        "matched_tutorial_code_inds": [
            1791,
            3169,
            2973,
            3035,
            2330
        ],
        "matched_tutorial_codes": [
            "LogisticRegression()\n\n<br/>\n<br/>",
            "macro_roc_auc_ovr = (\n    y_test,\n    y_score,\n    multi_class=\"ovr\",\n    average=\"macro\",\n)\n\nprint(f\"Macro-averaged One-vs-Rest ROC AUC score:\\n{macro_roc_auc_ovr:.2f}\")",
            "t_sne = (\n    n_components=n_components,\n    perplexity=30,\n    init=\"random\",\n    n_iter=250,\n    random_state=0,\n)\nS_t_sne = t_sne.fit_transform(S_points)\n\nplot_2d(S_t_sne, S_color, \"T-distributed Stochastic  \\n Neighbor Embedding\")\n\n\n<img alt=\"T-distributed Stochastic    Neighbor Embedding\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_compare_methods_006.png\" srcset=\"../../_images/sphx_glr_plot_compare_methods_006.png\"/>",
            "LogisticRegression(max_iter=500)\n\n<br/>\n<br/>",
            "X, y = (return_X_y=True)\n\n# Train classifiers\nreg1 = (random_state=1)\nreg2 = (random_state=1)\nreg3 = ()\n\nreg1.fit(X, y)\nreg2.fit(X, y)\nreg3.fit(X, y)\n\nereg = ([(\"gb\", reg1), (\"rf\", reg2), (\"lr\", reg3)])\nereg.fit(X, y)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 1.1->get_feature_names_out Available in all Transformers",
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 0.23->Rich visual representation of estimators",
                "sklearn->Examples->Miscellaneous->Displaying Pipelines->Displaying a Pipeline with a Preprocessing Step and Classifier",
                "sklearn->Examples->Miscellaneous->Displaying estimators and complex pipelines->Rich HTML representation",
                "sklearn->Examples->Pipelines and composite estimators->Column Transformer with Mixed Types",
                "sklearn->Examples->Pipelines and composite estimators->Column Transformer with Mixed Types",
                "sklearn->Examples->Pipelines and composite estimators->Column Transformer with Mixed Types"
            ],
            [
                "sklearn->Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->One-vs-Rest multiclass ROC->ROC curve using the OvR macro-average"
            ],
            [
                "sklearn->Examples->Manifold learning->Comparison of Manifold Learning methods->Define algorithms for the manifold learning->T-distributed Stochastic Neighbor Embedding"
            ],
            [
                "sklearn->Examples->Miscellaneous->Displaying Pipelines->Displaying a Complex Pipeline Chaining a Column Transformer"
            ],
            [
                "sklearn->Examples->Ensemble methods->Plot individual and voting regression predictions->Training classifiers"
            ]
        ]
    },
    "1195980": {
        "jupyter_code_cell": "plt.scatter(bos.DIS, bos.PRICE)\nplt.xlabel(\"Weighted Distance to Boston Employment Centers\")\nplt.ylabel(\"Housing Price\")\nplt.title(\"Relationship between DIS and Price\")\nplt.scatter(bos.LSTAT, bos.PRICE)\nplt.xlabel(\"% lower status of the population\")\nplt.ylabel(\"Housing Price\")\nplt.title(\"Relationship between LSTAT and Price\")",
        "matched_tutorial_code_inds": [
            5362,
            6253,
            6255,
            1039,
            6260
        ],
        "matched_tutorial_codes": [
            "plt.rcParams[\"figure.subplot.bottom\"] = 0.23  # keep labels visible\nplt.rcParams[\"figure.figsize\"] = (10.0, 8.0)  # make plot larger in notebook\nage = [data.exog[\"age\"][data.endog == id] for id in party_ID]\nfig = plt.figure()\nax = fig.add_subplot(111)\nplot_opts = {\n    \"cutoff_val\": 5,\n    \"cutoff_type\": \"abs\",\n    \"label_fontsize\": \"small\",\n    \"label_rotation\": 30,\n}\nsm.graphics.beanplot(age, ax=ax, labels=labels, plot_opts=plot_opts)\nax.set_xlabel(\"Party identification of respondent.\")\nax.set_ylabel(\"Age\")\n# plt.show()",
            "fit1 = SimpleExpSmoothing(oildata, initialization_method=\"heuristic\").fit(\n    smoothing_level=0.2, optimized=False\n)\nfcast1 = fit1.forecast(3).rename(r\"$\\alpha=0.2$\")\nfit2 = SimpleExpSmoothing(oildata, initialization_method=\"heuristic\").fit(\n    smoothing_level=0.6, optimized=False\n)\nfcast2 = fit2.forecast(3).rename(r\"$\\alpha=0.6$\")\nfit3 = SimpleExpSmoothing(oildata, initialization_method=\"estimated\").fit()\nfcast3 = fit3.forecast(3).rename(r\"$\\alpha=%s$\" % fit3.model.params[\"smoothing_level\"])\n\nplt.figure(figsize=(12, 8))\nplt.plot(oildata, marker=\"o\", color=\"black\")\nplt.plot(fit1.fittedvalues, marker=\"o\", color=\"blue\")\n(line1,) = plt.plot(fcast1, marker=\"o\", color=\"blue\")\nplt.plot(fit2.fittedvalues, marker=\"o\", color=\"red\")\n(line2,) = plt.plot(fcast2, marker=\"o\", color=\"red\")\nplt.plot(fit3.fittedvalues, marker=\"o\", color=\"green\")\n(line3,) = plt.plot(fcast3, marker=\"o\", color=\"green\")\nplt.legend([line1, line2, line3], [fcast1.name, fcast2.name, fcast3.name])",
            "fit1 = Holt(air, initialization_method=\"estimated\").fit(\n    smoothing_level=0.8, smoothing_trend=0.2, optimized=False\n)\nfcast1 = fit1.forecast(5).rename(\"Holt's linear trend\")\nfit2 = Holt(air, exponential=True, initialization_method=\"estimated\").fit(\n    smoothing_level=0.8, smoothing_trend=0.2, optimized=False\n)\nfcast2 = fit2.forecast(5).rename(\"Exponential trend\")\nfit3 = Holt(air, damped_trend=True, initialization_method=\"estimated\").fit(\n    smoothing_level=0.8, smoothing_trend=0.2\n)\nfcast3 = fit3.forecast(5).rename(\"Additive damped trend\")\n\nplt.figure(figsize=(12, 8))\nplt.plot(air, marker=\"o\", color=\"black\")\nplt.plot(fit1.fittedvalues, color=\"blue\")\n(line1,) = plt.plot(fcast1, marker=\"o\", color=\"blue\")\nplt.plot(fit2.fittedvalues, color=\"red\")\n(line2,) = plt.plot(fcast2, marker=\"o\", color=\"red\")\nplt.plot(fit3.fittedvalues, color=\"green\")\n(line3,) = plt.plot(fcast3, marker=\"o\", color=\"green\")\nplt.legend([line1, line2, line3], [fcast1.name, fcast2.name, fcast3.name])",
            "print(\n    \"Sparsity in conv1.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Sparsity in conv2.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Sparsity in fc1.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Sparsity in fc2.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Sparsity in fc3.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Global sparsity: {:.2f}%\".format(\n        100. * float(\n            ( == 0)\n            + ( == 0)\n            + ( == 0)\n            + ( == 0)\n            + ( == 0)\n        )\n        / float(\n            .nelement()\n            + .nelement()\n            + .nelement()\n            + .nelement()\n            + .nelement()\n        )\n    )\n)",
            "fit1 = SimpleExpSmoothing(livestock2, initialization_method=\"estimated\").fit()\nfcast1 = fit1.forecast(9).rename(\"SES\")\nfit2 = Holt(livestock2, initialization_method=\"estimated\").fit()\nfcast2 = fit2.forecast(9).rename(\"Holt's\")\nfit3 = Holt(livestock2, exponential=True, initialization_method=\"estimated\").fit()\nfcast3 = fit3.forecast(9).rename(\"Exponential\")\nfit4 = Holt(livestock2, damped_trend=True, initialization_method=\"estimated\").fit(\n    damping_trend=0.98\n)\nfcast4 = fit4.forecast(9).rename(\"Additive Damped\")\nfit5 = Holt(\n    livestock2, exponential=True, damped_trend=True, initialization_method=\"estimated\"\n).fit()\nfcast5 = fit5.forecast(9).rename(\"Multiplicative Damped\")\n\nax = livestock2.plot(color=\"black\", marker=\"o\", figsize=(12, 8))\nlivestock3.plot(ax=ax, color=\"black\", marker=\"o\", legend=False)\nfcast1.plot(ax=ax, color=\"red\", legend=True)\nfcast2.plot(ax=ax, color=\"green\", legend=True)\nfcast3.plot(ax=ax, color=\"blue\", legend=True)\nfcast4.plot(ax=ax, color=\"cyan\", legend=True)\nfcast5.plot(ax=ax, color=\"magenta\", legend=True)\nax.set_ylabel(\"Livestock, sheep in Asia (millions)\")\nplt.show()\nprint(\n    \"Figure 7.5: Forecasting livestock, sheep in Asia: comparing forecasting performance of non-seasonal methods.\"\n)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_14_0.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_14_0.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Plotting->Box Plots->Bean Plots"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Simple Exponential Smoothing"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Method"
            ],
            [
                "torch->Model Optimization->Pruning Tutorial->Global pruning"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Comparison"
            ]
        ]
    },
    "612806": {
        "jupyter_code_cell": "for index, row in cast.iterrows():\n    i=0\n    for item in row['cast']:\n        if i<5:\n            cast.at[index,item]=1\n        else:\n            break\n        i=i+1\ncast=cast.fillna(0)\ncast.shape",
        "matched_tutorial_code_inds": [
            6258,
            3167,
            6655,
            4423,
            2162
        ],
        "matched_tutorial_codes": [
            "for fit in [fit2, fit4]:\n    pd.DataFrame(np.c_[fit.level, fit.trend]).rename(\n        columns={0: \"level\", 1: \"slope\"}\n    ).plot(subplots=True)\nplt.show()\nprint(\n    \"Figure 7.4: Level and slope components for Holt\u2019s linear trend method and the additive damped trend method.\"\n)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_0.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_0.png\"/>\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_1.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_1.png\"/>",
            "for i in range(n_classes):\n    fpr[i], tpr[i], _ = (y_onehot_test[:, i], y_score[:, i])\n    roc_auc[i] = (fpr[i], tpr[i])\n\nfpr_grid = (0.0, 1.0, 1000)\n\n# Interpolate all ROC curves at these points\nmean_tpr = (fpr_grid)\n\nfor i in range(n_classes):\n    mean_tpr += (fpr_grid, fpr[i], tpr[i])  # linear interpolation\n\n# Average it and compute AUC\nmean_tpr /= n_classes\n\nfpr[\"macro\"] = fpr_grid\ntpr[\"macro\"] = mean_tpr\nroc_auc[\"macro\"] = (fpr[\"macro\"], tpr[\"macro\"])\n\nprint(f\"Macro-averaged One-vs-Rest ROC AUC score:\\n{roc_auc['macro']:.2f}\")",
            "for i in range(simulated.shape[1]):\n    simulated.iloc[:, i].plot(label=\"_\", color=\"gray\", alpha=0.1)\ndf[\"mean\"].plot(label=\"mean prediction\")\ndf[\"pi_lower\"].plot(linestyle=\"--\", color=\"tab:blue\", label=\"95% interval\")\ndf[\"pi_upper\"].plot(linestyle=\"--\", color=\"tab:blue\", label=\"_\")\npred.endog.plot(label=\"data\")\nplt.legend()",
            "for j, p in enumerate(points):\n...     plt.text(p[0]-0.03, p[1]+0.03, j, ha='right') # label the points\n for j, s in enumerate(tri.simplices):\n...     p = points[s].mean(axis=0)\n...     plt.text(p[0], p[1], '#%d' % j, ha='center') # label triangles\n plt.xlim(-0.5, 1.5); plt.ylim(-0.5, 1.5)\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code generates an X-Y plot with four green points annotated 0 through 3 roughly in the shape of a box. The box is outlined with a diagonal line between points 0 and 3 forming two adjacent triangles. The top triangle is annotated as #1 and the bottom triangle is annotated as #0.\"' class=\"plot-directive\" src=\"../_images/spatial-1.png\"/>\n</figure>",
            "for pipe in (hist_dropped, hist_one_hot, hist_ordinal, hist_native):\n    pipe.set_params(\n        histgradientboostingregressor__max_depth=3,\n        histgradientboostingregressor__max_iter=15,\n    )\n\ndropped_result = (hist_dropped, X, y, cv=n_cv_folds, scoring=scoring)\none_hot_result = (hist_one_hot, X, y, cv=n_cv_folds, scoring=scoring)\nordinal_result = (hist_ordinal, X, y, cv=n_cv_folds, scoring=scoring)\nnative_result = (hist_native, X, y, cv=n_cv_folds, scoring=scoring)\n\nplot_results(\"Gradient Boosting on Ames Housing (few and small trees)\")\n\n()\n\n\n<img alt=\"Gradient Boosting on Ames Housing (few and small trees), Fit times (s), Mean Absolute Percentage Error\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_002.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Method->Plots of Seasonally Adjusted Data"
            ],
            [
                "sklearn->Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->One-vs-Rest multiclass ROC->ROC curve using the OvR macro-average"
            ],
            [
                "statsmodels->Examples->State space models->ETS models->Predictions"
            ],
            [
                "scipy->Spatial data structures and algorithms (scipy.spatial)->Delaunay triangulations"
            ],
            [
                "sklearn->Examples->Ensemble methods->Categorical Feature Support in Gradient Boosting->Limiting the number of splits"
            ]
        ]
    },
    "1125698": {
        "jupyter_code_cell": "sns.distplot(iran_csv.ix[:,1])\nsns.distplot(turkey_csv.ix[:,1])\nSOLID_FUEL_COLUMN_INDEX = 2\na = sns.jointplot(iran_csv.ix[:,SOLID_FUEL_COLUMN_INDEX],\n              turkey_csv.ix[:,SOLID_FUEL_COLUMN_INDEX]).set_axis_labels(\n    \"IRAN: \" + iran_csv.columns[SOLID_FUEL_COLUMN_INDEX], \n    \"TURKEY: \" + turkey_csv.columns[SOLID_FUEL_COLUMN_INDEX])\na.savefig(\"output.png\")",
        "matched_tutorial_code_inds": [
            3400,
            3701,
            3669,
            5917,
            2383
        ],
        "matched_tutorial_codes": [
            "make_plot(5)\nmake_plot(6)\n\n\n\n<img alt=\"Data after power transformation (Yeo-Johnson), Full data, Zoom-in\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_all_scaling_006.png\" srcset=\"../../_images/sphx_glr_plot_all_scaling_006.png\"/>\n<img alt=\"Data after power transformation (Box-Cox), Full data, Zoom-in\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_all_scaling_007.png\" srcset=\"../../_images/sphx_glr_plot_all_scaling_007.png\"/>",
            "t_append = append_df()\nt_concat = concat_df()\n\ntimings = (pd.DataFrame({\"Append\": t_append, \"Concat\": t_concat})\n             .stack()\n             .reset_index()\n             .rename(columns={0: 'Time (s)',\n                              'level_1': 'Method'}))\ntimings.head()",
            "temp2 = temp.reset_index()\nsped2 = sped.reset_index()\n\n# Find rows where the operation is defined\ncommon_dates = pd.Index(temp2.date) &amp; sped2.date\npd.concat([\n    # concat to not lose date information\n    sped2.loc[sped2['date'].isin(common_dates), 'date'],\n    (sped2.loc[sped2.date.isin(common_dates), 'sped'] /\n     temp2.loc[temp2.date.isin(common_dates), 'tmpf'])],\n    axis=1).dropna(how='all')",
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "plot_digits(X_test, \"Uncorrupted test images\")\nplot_digits(\n    X_test_noisy, f\"Noisy test images\\nMSE: {((X_test - X_test_noisy) ** 2):.2f}\"\n)\n\n\n\n<img alt=\"Uncorrupted test images\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_digits_denoising_001.png\" srcset=\"../../_images/sphx_glr_plot_digits_denoising_001.png\"/>\n<img alt=\"Noisy test images MSE: 0.06\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_digits_denoising_002.png\" srcset=\"../../_images/sphx_glr_plot_digits_denoising_002.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Preprocessing->Compare the effect of different scalers on data with outliers->PowerTransformer"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ],
            [
                "pandas_toms_blog->Indexes->Flavors->Indexes for Alignment"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Image denoising using kernel PCA->Load the dataset via OpenML"
            ]
        ]
    },
    "236926": {
        "jupyter_code_cell": "L = 1\ns_array = np.linspace(0, L, 101)\nxi_p = L / 1000\ntheta_sq = avg_theta_sqr(150, s_array, L, xi_p)\nn_plot = [5, 10, 50, 100, 150]\ncolors = sns.color_palette('Blues_r', n_colors=len(n_plot)+2)\nfor i, n in enumerate(n_plot):\n    plt.plot(s_array, theta_sq[0:n, :].sum(axis=0) - s_array / xi_p,\n             label=str(n), color=colors[i])\nplt.xlabel(r'$s / L$')\nplt.ylabel(r'$\\left\\langle \\theta(s)^2 \\right\\rangle - s / \\xi_p$')\nplt.legend(title='$n$', ncol=2)\nplt.savefig(figdir + 'problem_10_01_04.png', bbox_inches='tight',\n            dpi=300)\ndf = pd.read_excel('../data/fig6.24.xls', sheet_name='tidy')\ndf.head()",
        "matched_tutorial_code_inds": [
            4928,
            4927,
            4915,
            4926,
            4930
        ],
        "matched_tutorial_codes": [
            "N = 100\nX, Y = np.mgrid[0:3:complex(0, N), 0:2:complex(0, N)]\nZ1 = (1 + np.sin(Y * 10.)) * X**2\n\nfig, ax = plt.subplots(2, 1, constrained_layout=True)\n\npcm = ax[0].pcolormesh(X, Y, Z1, norm=colors.PowerNorm(gamma=0.5),\n                       cmap='PuBu_r', shading='auto')\nfig.colorbar(pcm, ax=ax[0], extend='max')\nax[0].set_title('PowerNorm()')\n\npcm = ax[1].pcolormesh(X, Y, Z1, cmap='PuBu_r', shading='auto')\nfig.colorbar(pcm, ax=ax[1], extend='max')\nax[1].set_title('Normalize()')\nplt.show()\n\n\n<img alt=\"PowerNorm(), Normalize()\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_colormapnorms_004.png\" srcset=\"../../_images/sphx_glr_colormapnorms_004.png, ../../_images/sphx_glr_colormapnorms_004_2_0x.png 2.0x\"/>",
            "N = 100\nX, Y = np.mgrid[-3:3:complex(0, N), -2:2:complex(0, N)]\nZ1 = np.exp(-X**2 - Y**2)\nZ2 = np.exp(-(X - 1)**2 - (Y - 1)**2)\nZ = (Z1 - Z2) * 2\n\nfig, ax = plt.subplots(2, 1)\n\npcm = ax[0].pcolormesh(X, Y, Z,\n                       norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,\n                                              vmin=-1.0, vmax=1.0, base=10),\n                       cmap='RdBu_r', shading='auto')\nfig.colorbar(pcm, ax=ax[0], extend='both')\n\npcm = ax[1].pcolormesh(X, Y, Z, cmap='RdBu_r', vmin=-np.max(Z), shading='auto')\nfig.colorbar(pcm, ax=ax[1], extend='both')\nplt.show()\n\n\n<img alt=\"colormapnorms\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_colormapnorms_003.png\" srcset=\"../../_images/sphx_glr_colormapnorms_003.png, ../../_images/sphx_glr_colormapnorms_003_2_0x.png 2.0x\"/>",
            "N = 256\nvals = np.ones((N, 4))\nvals[:, 0] = np.linspace(90/256, 1, N)\nvals[:, 1] = np.linspace(40/256, 1, N)\nvals[:, 2] = np.linspace(40/256, 1, N)\nnewcmp = ListedColormap(vals)\nplot_examples([viridis, newcmp])\n\n\n<img alt=\"colormap manipulation\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_colormap-manipulation_005.png\" srcset=\"../../_images/sphx_glr_colormap-manipulation_005.png, ../../_images/sphx_glr_colormap-manipulation_005_2_0x.png 2.0x\"/>",
            "delta = 0.1\nx = np.arange(-3.0, 4.001, delta)\ny = np.arange(-4.0, 3.001, delta)\nX, Y = np.meshgrid(x, y)\nZ1 = np.exp(-X**2 - Y**2)\nZ2 = np.exp(-(X - 1)**2 - (Y - 1)**2)\nZ = (0.9*Z1 - 0.5*Z2) * 2\n\n# select a divergent colormap\ncmap = cm.coolwarm\n\nfig, (ax1, ax2) = plt.subplots(ncols=2)\npc = ax1.pcolormesh(Z, cmap=cmap)\nfig.colorbar(pc, ax=ax1)\nax1.set_title('Normalize()')\n\npc = ax2.pcolormesh(Z, norm=colors.CenteredNorm(), cmap=cmap)\nfig.colorbar(pc, ax=ax2)\nax2.set_title('CenteredNorm()')\n\nplt.show()\n\n\n<img alt=\"Normalize(), CenteredNorm()\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_colormapnorms_002.png\" srcset=\"../../_images/sphx_glr_colormapnorms_002.png, ../../_images/sphx_glr_colormapnorms_002_2_0x.png 2.0x\"/>",
            "N = 100\nX, Y = np.meshgrid(np.linspace(-3, 3, N), np.linspace(-2, 2, N))\nZ1 = np.exp(-X**2 - Y**2)\nZ2 = np.exp(-(X - 1)**2 - (Y - 1)**2)\nZ = ((Z1 - Z2) * 2)[:-1, :-1]\n\nfig, ax = plt.subplots(2, 2, figsize=(8, 6), constrained_layout=True)\nax = ax.flatten()\n\n# Default norm:\npcm = ax[0].pcolormesh(X, Y, Z, cmap='RdBu_r')\nfig.colorbar(pcm, ax=ax[0], orientation='vertical')\nax[0].set_title('Default norm')\n\n# Even bounds give a contour-like effect:\nbounds = np.linspace(-1.5, 1.5, 7)\nnorm = colors.BoundaryNorm(boundaries=bounds, ncolors=256)\npcm = ax[1].pcolormesh(X, Y, Z, norm=norm, cmap='RdBu_r')\nfig.colorbar(pcm, ax=ax[1], extend='both', orientation='vertical')\nax[1].set_title('BoundaryNorm: 7 boundaries')\n\n# Bounds may be unevenly spaced:\nbounds = np.array([-0.2, -0.1, 0, 0.5, 1])\nnorm = colors.BoundaryNorm(boundaries=bounds, ncolors=256)\npcm = ax[2].pcolormesh(X, Y, Z, norm=norm, cmap='RdBu_r')\nfig.colorbar(pcm, ax=ax[2], extend='both', orientation='vertical')\nax[2].set_title('BoundaryNorm: nonuniform')\n\n# With out-of-bounds colors:\nbounds = np.linspace(-1.5, 1.5, 7)\nnorm = colors.BoundaryNorm(boundaries=bounds, ncolors=256, extend='both')\npcm = ax[3].pcolormesh(X, Y, Z, norm=norm, cmap='RdBu_r')\n# The colorbar inherits the \"extend\" argument from BoundaryNorm.\nfig.colorbar(pcm, ax=ax[3], orientation='vertical')\nax[3].set_title('BoundaryNorm: extend=\"both\"')\nplt.show()\n\n\n<img alt=\"Default norm, BoundaryNorm: 7 boundaries, BoundaryNorm: nonuniform, BoundaryNorm: extend=\" both=\"\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_colormapnorms_005.png\" srcset=\"../../_images/sphx_glr_colormapnorms_005.png, ../../_images/sphx_glr_colormapnorms_005_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Colors->Colormap Normalization->Power-law"
            ],
            [
                "matplotlib->Tutorials->Colors->Colormap Normalization->Symmetric logarithmic"
            ],
            [
                "matplotlib->Tutorials->Colors->Creating Colormaps in Matplotlib->Creating listed colormaps"
            ],
            [
                "matplotlib->Tutorials->Colors->Colormap Normalization->Centered"
            ],
            [
                "matplotlib->Tutorials->Colors->Colormap Normalization->Discrete bounds"
            ]
        ]
    },
    "1239988": {
        "jupyter_code_cell": "from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.decomposition import PCA\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport keras\nfrom keras.layers import Dense\nfrom keras.layers import Softmax\nfrom keras.layers import Dropout\nfrom keras.models import Sequential",
        "matched_tutorial_code_inds": [
            1867,
            2308,
            3426,
            2345,
            2822
        ],
        "matched_tutorial_codes": [
            "from sklearn.calibration import CalibrationDisplay\nfrom sklearn.ensemble import \nfrom sklearn.linear_model import \nfrom sklearn.naive_bayes import \n\n# Create classifiers\nlr = ()\ngnb = ()\nsvc = NaivelyCalibratedLinearSVC(C=1.0)\nrfc = ()\n\nclf_list = [\n    (lr, \"Logistic\"),\n    (gnb, \"Naive Bayes\"),\n    (svc, \"SVC\"),\n    (rfc, \"Random forest\"),\n]",
            "from sklearn.ensemble import \nfrom sklearn.inspection import PartialDependenceDisplay\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nrng = (0)\n\nn_samples = 1000\nf_0 = rng.rand(n_samples)\nf_1 = rng.rand(n_samples)\nX = [f_0, f_1]\nnoise = rng.normal(loc=0.0, scale=0.01, size=n_samples)\n\n# y is positively correlated with f_0, and negatively correlated with f_1\ny = 5 * f_0 + (10 *  * f_0) - 5 * f_1 - (10 *  * f_1) + noise",
            "from sklearn.semi_supervised import \nfrom sklearn.metrics import \n\nlp_model = (gamma=0.25, max_iter=20)\nlp_model.fit(X, y_train)\npredicted_labels = lp_model.transduction_[unlabeled_set]\ntrue_labels = y[unlabeled_set]\n\nprint(\n    \"Label Spreading model: %d labeled & %d unlabeled points (%d total)\"\n    % (n_labeled_points, n_total_samples - n_labeled_points, n_total_samples)\n)",
            "from sklearn.ensemble import \nfrom sklearn.metrics import , \n\n\nall_models = {}\ncommon_params = dict(\n    learning_rate=0.05,\n    n_estimators=200,\n    max_depth=2,\n    min_samples_leaf=9,\n    min_samples_split=9,\n)\nfor alpha in [0.05, 0.5, 0.95]:\n    gbr = (loss=\"quantile\", alpha=alpha, **common_params)\n    all_models[\"q %1.2f\" % alpha] = gbr.fit(X_train, y_train)",
            "from sklearn.metrics import \nfrom sklearn.metrics import PredictionErrorDisplay\n\nmae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Calibration->Comparison of Calibration of Classifiers->Calibration curves"
            ],
            [
                "sklearn->Examples->Ensemble methods->Monotonic Constraints"
            ],
            [
                "sklearn->Examples->Semi Supervised Classification->Label Propagation digits: Demonstrating performance->Semi-supervised learning"
            ],
            [
                "sklearn->Examples->Ensemble methods->Prediction Intervals for Gradient Boosting Regression->Fitting non-linear quantile and least squares regressors"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ]
        ]
    },
    "1157290": {
        "jupyter_code_cell": "distances.sort_values()[:5]\ndef scotch_recommender(user_scotches, n_recs=5):\n    user_preferences = (scotch_props.loc[scotch_props.index.isin(user_scotches)]\n                                    .mean(axis=0))\n    distances = cdist(scotch_props, user_preferences.to_frame().T).squeeze()\n    distances = pd.Series(data=distances,\n                      index=scotch_props.index)\n    distances = distances[~distances.index.isin(user_scotches)]\n    return distances.sort_values()[:n_recs].index.tolist()",
        "matched_tutorial_code_inds": [
            6422,
            3561,
            4478,
            149,
            6714
        ],
        "matched_tutorial_codes": [
            "dusphci = usphci.diff()[1:].values\ndef compute_coincident_index(mod, res):\n    # Estimate W(1)\n    spec = res.specification\n    design = mod.ssm['design']\n    transition = mod.ssm['transition']\n    ss_kalman_gain = res.filter_results.kalman_gain[:,:,-1]\n    k_states = ss_kalman_gain.shape[0]\n\n    W1 = np.linalg.inv(np.eye(k_states) - np.dot(\n        np.eye(k_states) - np.dot(ss_kalman_gain, design),\n        transition\n    )).dot(ss_kalman_gain)[0]\n\n    # Compute the factor mean vector\n    factor_mean = np.dot(W1, dta.loc['1972-02-01':, 'dln_indprod':'dln_emp'].mean())\n\n    # Normalize the factors\n    factor = res.factors.filtered[0]\n    factor *= np.std(usphci.diff()[1:]) / np.std(factor)\n\n    # Compute the coincident index\n    coincident_index = np.zeros(mod.nobs+1)\n    # The initial value is arbitrary; here it is set to\n    # facilitate comparison\n    coincident_index[0] = usphci.iloc[0] * factor_mean / dusphci.mean()\n    for t in range(0, mod.nobs):\n        coincident_index[t+1] = coincident_index[t] + factor[t] + factor_mean\n\n    # Attach dates\n    coincident_index = pd.Series(coincident_index, index=dta.index).iloc[1:]\n\n    # Normalize to use the same base year as USPHCI\n    coincident_index *= (usphci.loc['1992-07-01'] / coincident_index.loc['1992-07-01'])\n\n    return coincident_index",
            "digits.images[0]\narray([[  0.,   0.,   5.,  13.,   9.,   1.,   0.,   0.],\n       [  0.,   0.,  13.,  15.,  10.,  15.,   5.,   0.],\n       [  0.,   3.,  15.,   2.,   0.,  11.,   8.,   0.],\n       [  0.,   4.,  12.,   0.,   0.,   8.,   8.,   0.],\n       [  0.,   5.,   8.,   0.,   0.,   9.,   8.,   0.],\n       [  0.,   4.,  11.,   0.,   1.,  12.,   7.,   0.],\n       [  0.,   2.,  14.,   5.,  10.,  12.,   0.,   0.],\n       [  0.,   0.,   6.,  13.,  10.,   0.,   0.,   0.]])",
            "deterministic.pdf(np.arange(-3, 3, 0.5))\narray([  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n         5.83333333e+04,   4.16333634e-12,   4.16333634e-12,\n         4.16333634e-12,   4.16333634e-12,   4.16333634e-12])",
            "num_threads = ()\nprint(f'Benchmarking on {num_threads} threads')\n\nt0 = (\n    stmt='batched_dot_mul_sum(x, x)',\n    setup='from __main__ import batched_dot_mul_sum',\n    globals={'x': x},\n    num_threads=num_threads,\n    label='Multithreaded batch dot',\n    sub_label='Implemented using mul and sum')\n\nt1 = (\n    stmt='batched_dot_bmm(x, x)',\n    setup='from __main__ import batched_dot_bmm',\n    globals={'x': x},\n    num_threads=num_threads,\n    label='Multithreaded batch dot',\n    sub_label='Implemented using bmm')\n\nprint(t0.timeit(100))\nprint(t1.timeit(100))\n\n\n\nOutput",
            "make_plot(dta.index[idx[:5]])\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_24_0.png\" src=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_24_0.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->Dynamic Factor Models: Application->Coincident Index"
            ],
            [
                "sklearn->Tutorials->An introduction to machine learning with scikit-learn->Loading an example dataset"
            ],
            [
                "scipy->Statistics (scipy.stats)->Building specific distributions->Making a continuous distribution, i.e., subclassing rv_continuous"
            ],
            [
                "torch->PyTorch Recipes->PyTorch Benchmark->Steps->3. Benchmarking with torch.utils.benchmark.Timer"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ]
        ]
    },
    "14967": {
        "jupyter_code_cell": "p = base_plot()\np.xaxis.axis_label = \"Fare, $\"\np.yaxis.axis_label = \"Tip, $\"\npipeline = ds.Pipeline(df, ds.Point(\"fare_amount\", \"tip_amount\"))\nInteractiveImage(p, pipeline)\nimport datashader as ds\nfrom datashader.bokeh_ext import InteractiveImage\nfrom bokeh.models import Range1d\np = base_plot()\np.xaxis.axis_label = \"Passengers\"\np.yaxis.axis_label = \"Tip, $\"\np.x_range = Range1d(-0.5, 6.5)\np.y_range = Range1d(0, 60)\npipeline = ds.Pipeline(df, ds.Point(\"passenger_count\", \"tip_amount\"), width_scale=0.035)\nInteractiveImage(p, pipeline)",
        "matched_tutorial_code_inds": [
            2860,
            2844,
            2864,
            4158,
            2981
        ],
        "matched_tutorial_codes": [
            "mae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}\n\n_, ax = (figsize=(5, 5))\ndisplay = (\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n)\nax.set_title(\"Ridge model, optimum regularization\")\nfor name, score in scores.items():\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\nax.legend(loc=\"upper left\")\n()\n\n\n<img alt=\"Ridge model, optimum regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_012.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_012.png\"/>",
            "mae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}\n\n_, ax = (figsize=(5, 5))\ndisplay = (\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n)\nax.set_title(\"Ridge model, small regularization\")\nfor name, score in scores.items():\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\nax.legend(loc=\"upper left\")\n()\n\n\n<img alt=\"Ridge model, small regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_009.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_009.png\"/>",
            "mae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}\n\n_, ax = (figsize=(6, 6))\ndisplay = (\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n)\nax.set_title(\"Lasso model, optimum regularization\")\nfor name, score in scores.items():\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\nax.legend(loc=\"upper left\")\n()\n\n\n<img alt=\"Lasso model, optimum regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_015.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_015.png\"/>",
            "f = plt.figure(figsize=(6, 6))\ngs = f.add_gridspec(2, 2)\n\nwith sns.axes_style(\"darkgrid\"):\n    ax = f.add_subplot(gs[0, 0])\n    sinplot(6)\n\nwith sns.axes_style(\"white\"):\n    ax = f.add_subplot(gs[0, 1])\n    sinplot(6)\n\nwith sns.axes_style(\"ticks\"):\n    ax = f.add_subplot(gs[1, 0])\n    sinplot(6)\n\nwith sns.axes_style(\"whitegrid\"):\n    ax = f.add_subplot(gs[1, 1])\n    sinplot(6)\n\nf.tight_layout()\n",
            "sh_lle, sh_err = (\n    sh_points, n_neighbors=12, n_components=2\n)\n\nsh_tsne = (\n    n_components=2, perplexity=40, init=\"random\", random_state=0\n).fit_transform(sh_points)\n\nfig, axs = (figsize=(8, 8), nrows=2)\naxs[0].scatter(sh_lle[:, 0], sh_lle[:, 1], c=sh_color)\naxs[0].set_title(\"LLE Embedding of Swiss-Hole\")\naxs[1].scatter(sh_tsne[:, 0], sh_tsne[:, 1], c=sh_color)\n_ = axs[1].set_title(\"t-SNE Embedding of Swiss-Hole\")\n\n\n<img alt=\"LLE Embedding of Swiss-Hole, t-SNE Embedding of Swiss-Hole\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_swissroll_004.png\" srcset=\"../../_images/sphx_glr_plot_swissroll_004.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "seaborn->User guide and tutorial->Controlling figure aesthetics->Temporarily setting figure style",
                "seaborn->Figure aesthetics->Controlling figure aesthetics->Temporarily setting figure style"
            ],
            [
                "sklearn->Examples->Manifold learning->Swiss Roll And Swiss-Hole Reduction->Swiss-Hole"
            ]
        ]
    },
    "723290": {
        "jupyter_code_cell": "applForm.save(\"applform1.csv\")\nif ('applForm' in locals()): del applForm",
        "matched_tutorial_code_inds": [
            647,
            3939,
            2527,
            2122,
            3206
        ],
        "matched_tutorial_codes": [
            "traced_model = (model)\nprint(traced_model.graph)",
            "anagrams = sns.load_dataset(\"anagrams\")\nanagrams\n",
            "GaussianMixture()\n\n<br/>\n<br/>",
            "data = ()\nX = ().fit_transform(data[\"data\"])\nfeature_names = data[\"feature_names\"]",
            "ComplementNB()\n\n<br/>\n<br/>"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Code Transforms with FX->(beta) Building a Convolution/Batch Norm fuser in FX->Fusing Convolution with Batch Norm"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data",
                "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data"
            ],
            [
                "sklearn->Examples->Gaussian Mixture Models->Gaussian Mixture Model Selection->Model training and selection"
            ],
            [
                "sklearn->Examples->Decomposition->Factor Analysis (with rotation) to visualize patterns"
            ],
            [
                "sklearn->Examples->Model Selection->Sample pipeline for text feature extraction and evaluation->Pipeline with hyperparameter tuning"
            ]
        ]
    },
    "714784": {
        "jupyter_code_cell": "plt.scatter(X2[:, 0], X2[:, 1], c=labels, cmap='rainbow')\nplt.colorbar()\nfig, ax = plt.subplots(1, 2, figsize=(14, 6))\npivoted.T[labels == 0].T.plot(legend=False, alpha=0.1, ax=ax[0]);\npivoted.T[labels == 1].T.plot(legend=False, alpha=0.1, ax=ax[1]);\nax[0].set_title('Purple Cluster')\nax[1].set_title('Red Cluster');",
        "matched_tutorial_code_inds": [
            1432,
            4652,
            1423,
            4372,
            4819
        ],
        "matched_tutorial_codes": [
            "plt.plot(s)\nplt.show()\n\n\n\n\n<img alt=\"../_images/b3c5a64421a25e9b8f34f2d5e71f750a53e3ef7b513a82271d088e5c12a26308.png\" src=\"../_images/b3c5a64421a25e9b8f34f2d5e71f750a53e3ef7b513a82271d088e5c12a26308.png\"/>",
            "plt.plot([1, 2, 3, 4], [1, 4, 9, 16], 'ro')\nplt.axis([0, 6, 0, 20])\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_003.png\" srcset=\"../../_images/sphx_glr_pyplot_003.png, ../../_images/sphx_glr_pyplot_003_2_0x.png 2.0x\"/>",
            "plt.imshow(img_gray, cmap=\"gray\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/7adf6740f755ce33a18fced959f7be249e3d4f46738c70cfd0eeff87ffe4ff20.png\" src=\"../_images/7adf6740f755ce33a18fced959f7be249e3d4f46738c70cfd0eeff87ffe4ff20.png\"/>",
            "plt.semilogy(f, Pper_spec)\n plt.xlabel('frequency [Hz]')\n plt.ylabel('PSD')\n plt.grid()\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code displays a single X-Y log-linear plot with the power spectral density on the Y axis vs frequency on the X axis. A single blue trace shows a noise floor with a power level of 1e-3 with a single peak at 1270 Hz up to a power of 1. The noise floor measurements appear noisy and oscillate down to 1e-7.\"' class=\"plot-directive\" src=\"../_images/signal-8.png\"/>\n</figure>",
            "plt.close('all')\nfig = plt.figure()\n\nax1 = plt.subplot(221)\nax2 = plt.subplot(223)\nax3 = plt.subplot(122)\n\nexample_plot(ax1)\nexample_plot(ax2)\nexample_plot(ax3)\n\nplt.tight_layout()\n\n\n<img alt=\"Title, Title, Title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_tight_layout_guide_006.png\" srcset=\"../../_images/sphx_glr_tight_layout_guide_006.png, ../../_images/sphx_glr_tight_layout_guide_006_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Approximation"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Introduction to pyplot->Formatting the style of your plot"
            ],
            [
                "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Content->Operations on an axis"
            ],
            [
                "scipy->Signal Processing (scipy.signal)->Spectral Analysis->Periodogram Measurements"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Tight Layout guide->Simple Example"
            ]
        ]
    },
    "815500": {
        "jupyter_code_cell": "ax = None\nfilter = (df['Primary Type']=='HOMICIDE')\nym = [i.strftime('%Y') for i in df['Date'][filter]]\ncounts = Counter(ym)\nax = plot_counter(counts,\n                  sample_frac=1,\n                  last_year=True,\n                  dot=True,\n                  color='b',\n                  label='all homicides',\n                  ax=ax)\nfilter = (df['Primary Type']=='HOMICIDE') & (df['Domestic'])\nym = [i.strftime('%Y') for i in df['Date'][filter]]\ncounts = Counter(ym)\nax = plot_counter(counts,\n                  sample_frac=1,\n                  last_year=True,\n                  dot=True,\n                  color='r',\n                  label='domestic homicides',\n                  ax=ax)\nfilter = (df['Primary Type']=='HOMICIDE') & (df['Domestic']==False)\nym = [i.strftime('%Y') for i in df['Date'][filter]]\ncounts = Counter(ym)\nax = plot_counter(counts,\n                  sample_frac=1,\n                  last_year=True,\n                  dot=True,\n                  color='g',\n                  label='non-domestic homicides',\n                  ax=ax)\nplt.legend(loc=0)\nax.set_title('Chicago Homicide Rate')\nym = [i.strftime('%Y') for i in df['Date'][df['Domestic']]]\ncounts = Counter(ym)\nax = plot_counter(counts, sample_frac=1, last_year=True)\nax.set_title('Chicago Domestic Violence Rate')",
        "matched_tutorial_code_inds": [
            2315,
            2123,
            2635,
            3888,
            2446
        ],
        "matched_tutorial_codes": [
            "fig, ax = ()\ndisp = (\n    gbdt_no_cst,\n    X,\n    features=[0, 1],\n    feature_names=(\n        \"First feature\",\n        \"Second feature\",\n    ),\n    line_kw={\"linewidth\": 4, \"label\": \"unconstrained\", \"color\": \"tab:blue\"},\n    ax=ax,\n)\n(\n    gbdt_with_monotonic_cst,\n    X,\n    features=[0, 1],\n    line_kw={\"linewidth\": 4, \"label\": \"constrained\", \"color\": \"tab:orange\"},\n    ax=disp.axes_,\n)\n\nfor f_idx in (0, 1):\n    disp.axes_[0, f_idx].plot(\n        X[:, f_idx], y, \"o\", alpha=0.3, zorder=-1, color=\"tab:green\"\n    )\n    disp.axes_[0, f_idx].set_ylim(-6, 6)\n\n()\nfig.suptitle(\"Monotonic constraints effect on partial dependences\")\n()\n\n\n<img alt=\"Monotonic constraints effect on partial dependences\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_monotonic_constraints_001.png\" srcset=\"../../_images/sphx_glr_plot_monotonic_constraints_001.png\"/>",
            "ax = ()\n\nim = ax.imshow((X.T), cmap=\"RdBu_r\", vmin=-1, vmax=1)\n\nax.set_xticks([0, 1, 2, 3])\nax.set_xticklabels(list(feature_names), rotation=90)\nax.set_yticks([0, 1, 2, 3])\nax.set_yticklabels(list(feature_names))\n\n(im).ax.set_ylabel(\"$r$\", rotation=0)\nax.set_title(\"Iris feature correlation matrix\")\n()\n\n\n<img alt=\"Iris feature correlation matrix\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_varimax_fa_001.png\" srcset=\"../../_images/sphx_glr_plot_varimax_fa_001.png\"/>",
            "ax = (\n    data=full_data, x=\"input_feature\", y=\"target\", color=\"black\", alpha=0.75\n)\nax.plot(X_plot, y_plot, color=\"black\", label=\"Ground Truth\")\nax.plot(X_plot, y_brr, color=\"red\", label=\"BayesianRidge with polynomial features\")\nax.plot(X_plot, y_ard, color=\"navy\", label=\"ARD with polynomial features\")\nax.fill_between(\n    X_plot.ravel(),\n    y_ard - y_ard_std,\n    y_ard + y_ard_std,\n    color=\"navy\",\n    alpha=0.3,\n)\nax.fill_between(\n    X_plot.ravel(),\n    y_brr - y_brr_std,\n    y_brr + y_brr_std,\n    color=\"red\",\n    alpha=0.3,\n)\nax.legend()\n_ = ax.set_title(\"Polynomial fit of a non-linear feature\")\n\n\n<img alt=\"Polynomial fit of a non-linear feature\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_ard_003.png\" srcset=\"../../_images/sphx_glr_plot_ard_003.png\"/>",
            "ax = occupation_avg.sort_values(ascending=False).plot.barh(color='k', width=0.9);\nlim = ax.get_ylim()\nax.vlines(total_avg, *lim, color='C1', linewidth=3)\nax.legend(['Average donation'])\nax.set(xlabel=\"Donation Amount\", title=\"Average Dontation by Occupation\")\nsns.despine()",
            "last_hours = slice(-96, None)\nfig, ax = (figsize=(12, 4))\nfig.suptitle(\"Predictions by linear models\")\nax.plot(\n    y.iloc[test_0].values[last_hours],\n    \"x-\",\n    alpha=0.2,\n    label=\"Actual demand\",\n    color=\"black\",\n)\nax.plot(naive_linear_predictions[last_hours], \"x-\", label=\"Ordinal time features\")\nax.plot(\n    cyclic_cossin_linear_predictions[last_hours],\n    \"x-\",\n    label=\"Trigonometric time features\",\n)\nax.plot(\n    cyclic_spline_linear_predictions[last_hours],\n    \"x-\",\n    label=\"Spline-based time features\",\n)\nax.plot(\n    one_hot_linear_predictions[last_hours],\n    \"x-\",\n    label=\"One-hot time features\",\n)\n_ = ax.legend()\n\n\n<img alt=\"Predictions by linear models\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_006.png\" srcset=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_006.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Ensemble methods->Monotonic Constraints"
            ],
            [
                "sklearn->Examples->Decomposition->Factor Analysis (with rotation) to visualize patterns"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Comparing Linear Bayesian Regressors->Bayesian regressions with polynomial feature expansion->Plotting polynomial regressions with std errors of the scores"
            ],
            [
                "pandas_toms_blog->Scaling->Dask"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Qualitative analysis of the impact of features on linear model predictions"
            ]
        ]
    },
    "1157750": {
        "jupyter_code_cell": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nimport scipy.stats as stats\nfrom scipy.stats import chi2\nfrom scipy.stats import shapiro\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols",
        "matched_tutorial_code_inds": [
            6306,
            5220,
            5287,
            2984,
            6478
        ],
        "matched_tutorial_codes": [
            "import numpy as np\nimport pandas as pd\nfrom scipy.stats import norm\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport requests\nfrom io import BytesIO",
            "import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\n\ndata = sm.datasets.engel.load_pandas().data\ndata.head()",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom statsmodels.iolib.table import SimpleTable, default_txt_fmt\n\nnp.random.seed(1024)",
            "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import \nfrom sklearn.neural_network import \nfrom sklearn.preprocessing import \nfrom sklearn.pipeline import \nfrom sklearn.tree import \nfrom sklearn.inspection import PartialDependenceDisplay",
            "import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\nplt.rc(\"figure\", figsize=(16,8))\nplt.rc(\"font\", size=14)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Quantile Regression->Setup"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Weighted Least Squares"
            ],
            [
                "sklearn->Examples->Miscellaneous->Advanced Plotting With Partial Dependence"
            ],
            [
                "statsmodels->Examples->State space models->Seasonality in Time Series Data"
            ]
        ]
    },
    "1515110": {
        "jupyter_code_cell": "for i in range(len(to_predict)):\n    alg = RandomForestClassifier(random_state=1, n_estimators=120, min_samples_split=4, min_samples_leaf=2)\n    scores = cross_validation.cross_val_score(alg, srdf[predictors], srdf[to_predict[i]], cv=3)\n    print (to_predict[i], scores.mean())\nalg = LogisticRegression(random_state=1)\nscores = cross_validation.cross_val_score(alg, frs[predictors], frs[\"finalmajor_full\"])\nprint(scores.mean())",
        "matched_tutorial_code_inds": [
            6655,
            6258,
            2162,
            3167,
            1085
        ],
        "matched_tutorial_codes": [
            "for i in range(simulated.shape[1]):\n    simulated.iloc[:, i].plot(label=\"_\", color=\"gray\", alpha=0.1)\ndf[\"mean\"].plot(label=\"mean prediction\")\ndf[\"pi_lower\"].plot(linestyle=\"--\", color=\"tab:blue\", label=\"95% interval\")\ndf[\"pi_upper\"].plot(linestyle=\"--\", color=\"tab:blue\", label=\"_\")\npred.endog.plot(label=\"data\")\nplt.legend()",
            "for fit in [fit2, fit4]:\n    pd.DataFrame(np.c_[fit.level, fit.trend]).rename(\n        columns={0: \"level\", 1: \"slope\"}\n    ).plot(subplots=True)\nplt.show()\nprint(\n    \"Figure 7.4: Level and slope components for Holt\u2019s linear trend method and the additive damped trend method.\"\n)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_0.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_0.png\"/>\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_1.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_1.png\"/>",
            "for pipe in (hist_dropped, hist_one_hot, hist_ordinal, hist_native):\n    pipe.set_params(\n        histgradientboostingregressor__max_depth=3,\n        histgradientboostingregressor__max_iter=15,\n    )\n\ndropped_result = (hist_dropped, X, y, cv=n_cv_folds, scoring=scoring)\none_hot_result = (hist_one_hot, X, y, cv=n_cv_folds, scoring=scoring)\nordinal_result = (hist_ordinal, X, y, cv=n_cv_folds, scoring=scoring)\nnative_result = (hist_native, X, y, cv=n_cv_folds, scoring=scoring)\n\nplot_results(\"Gradient Boosting on Ames Housing (few and small trees)\")\n\n()\n\n\n<img alt=\"Gradient Boosting on Ames Housing (few and small trees), Fit times (s), Mean Absolute Percentage Error\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_002.png\"/>",
            "for i in range(n_classes):\n    fpr[i], tpr[i], _ = (y_onehot_test[:, i], y_score[:, i])\n    roc_auc[i] = (fpr[i], tpr[i])\n\nfpr_grid = (0.0, 1.0, 1000)\n\n# Interpolate all ROC curves at these points\nmean_tpr = (fpr_grid)\n\nfor i in range(n_classes):\n    mean_tpr += (fpr_grid, fpr[i], tpr[i])  # linear interpolation\n\n# Average it and compute AUC\nmean_tpr /= n_classes\n\nfpr[\"macro\"] = fpr_grid\ntpr[\"macro\"] = mean_tpr\nroc_auc[\"macro\"] = (fpr[\"macro\"], tpr[\"macro\"])\n\nprint(f\"Macro-averaged One-vs-Rest ROC AUC score:\\n{roc_auc['macro']:.2f}\")",
            "for param in model_ft.parameters():\n  param.requires_grad = True\n\nmodel_ft.to(device)  # We can fine-tune on GPU if available\n\ncriterion = nn.CrossEntropyLoss()\n\n# Note that we are training everything, so the learning rate is lower\n# Notice the smaller learning rate\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=1e-3, momentum=0.9, weight_decay=0.1)\n\n# Decay LR by a factor of 0.3 every several epochs\nexp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=5, gamma=0.3)\n\nmodel_ft_tuned = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n                             num_epochs=25, device=device)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->ETS models->Predictions"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Method->Plots of Seasonally Adjusted Data"
            ],
            [
                "sklearn->Examples->Ensemble methods->Categorical Feature Support in Gradient Boosting->Limiting the number of splits"
            ],
            [
                "sklearn->Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->One-vs-Rest multiclass ROC->ROC curve using the OvR macro-average"
            ],
            [
                "torch->Model Optimization->(beta) Quantized Transfer Learning for Computer Vision Tutorial->Part 2. Finetuning the Quantizable Model->Finetuning the model"
            ]
        ]
    },
    "964580": {
        "jupyter_code_cell": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nData = pd.read_csv('Salary_Data.csv')\nX = Data.iloc[:, :1].values\ny = Data.iloc[:, 1].values",
        "matched_tutorial_code_inds": [
            5220,
            6174,
            5287,
            2037,
            5559
        ],
        "matched_tutorial_codes": [
            "import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\n\ndata = sm.datasets.engel.load_pandas().data\ndata.head()",
            "import numpy as np\nimport pandas as pd\n\nfrom statsmodels.graphics.tsaplots import plot_predict\nfrom statsmodels.tsa.arima_process import arma_generate_sample\nfrom statsmodels.tsa.arima.model import ARIMA\n\nnp.random.seed(12345)",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom statsmodels.iolib.table import SimpleTable, default_txt_fmt\n\nnp.random.seed(1024)",
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.linalg import , \n\nfrom sklearn.covariance import , \n\n(0)",
            "import numpy as np\nimport pylab\nimport seaborn as sns\nimport statsmodels.api as sm\n\nsns.set_style(\"darkgrid\")\npylab.rc(\"figure\", figsize=(16, 8))\npylab.rc(\"font\", size=14)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Quantile Regression->Setup"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Artificial Data"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Weighted Least Squares"
            ],
            [
                "sklearn->Examples->Covariance estimation->Ledoit-Wolf vs OAS estimation"
            ],
            [
                "statsmodels->Examples->Nonparametric Statistics->Lowess Regression"
            ]
        ]
    },
    "1221941": {
        "jupyter_code_cell": "college.apply(second_most, axis='columns')\ndef second_most(s):\n    ugds, s = s.iloc[0], s.iloc[1:]\n    s = s.sort_values(ascending=False)\n    second_pct = s.iloc[1]\n    second_pop = (second_pct * ugds).astype(int)\n    return second_pop",
        "matched_tutorial_code_inds": [
            4105,
            3937,
            6714,
            3588,
            1418
        ],
        "matched_tutorial_codes": [
            "sns.lmplot(x=\"size\", y=\"tip\", data=tips, x_estimator=np.mean);\n",
            "sns.relplot(data=flights_wide.transpose(), kind=\"line\")\n",
            "make_plot(dta.index[idx[:5]])\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_24_0.png\" src=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_24_0.png\"/>",
            "text_clf.fit(twenty_train.data, twenty_train.target)\nPipeline(...)",
            "dtype('float64')"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Estimating regression fits->Functions for drawing linear regression models",
                "seaborn->Statistical operations->Estimating regression fits->Functions for drawing linear regression models"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "sklearn->Tutorials->Working With Text Data->Building a pipeline"
            ],
            [
                "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Content->Shape, axis and array properties"
            ]
        ]
    },
    "131740": {
        "jupyter_code_cell": "gscv.best_estimator_.fit(x_train, y_train)\ny_pred = gscv.best_estimator_.predict(x_test)",
        "matched_tutorial_code_inds": [
            3363,
            2309,
            3563,
            3802,
            6878
        ],
        "matched_tutorial_codes": [
            "search_cv.fit(X_train, y_train)\n\nprint(\"Best params:\")\nprint(search_cv.best_params_)",
            "gbdt_no_cst = ()\ngbdt_no_cst.fit(X, y)",
            "clf.fit(digits.data[:-1], digits.target[:-1])\nSVC(C=100.0, gamma=0.001)",
            "scores = pd.DataFrame(est.cv_results_)\nscores.head()",
            "mod = NBin(y, X)\nres = mod.fit()"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Pipelines and composite estimators->Column Transformer with Mixed Types"
            ],
            [
                "sklearn->Examples->Ensemble methods->Monotonic Constraints"
            ],
            [
                "sklearn->Tutorials->An introduction to machine learning with scikit-learn->Learning and predicting"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 2: Negative Binomial Regression for Count Data->Usage Example"
            ]
        ]
    },
    "1046703": {
        "jupyter_code_cell": "teams=['MI','KKR','RCB','DC','CSK','RR','DD','GL','KXIP','SRH','RPS','KTK','PW']\nplay=delivery[delivery['is_super_over']==1].batting_team.unique()\nplay=list(play)\nprint('Teams who haven\"t ever played a super over are:' ,list(set(teams)-set(play)))\nmlt.subplots(figsize=(10,6))\nump=pd.concat([matches['umpire1'],matches['umpire2']]) \nax=ump.value_counts().head(10).plot.bar(width=0.8,color=sns.color_palette('summer',10))\nfor p in ax.patches:\n    ax.annotate(format(p.get_height()), (p.get_x()+0.15, p.get_height()+0.25))\nmlt.show()",
        "matched_tutorial_code_inds": [
            220,
            3646,
            6401,
            3267,
            6712
        ],
        "matched_tutorial_codes": [
            "labels_map = {\n    0: \"T-Shirt\",\n    1: \"Trouser\",\n    2: \"Pullover\",\n    3: \"Dress\",\n    4: \"Coat\",\n    5: \"Sandal\",\n    6: \"Shirt\",\n    7: \"Sneaker\",\n    8: \"Bag\",\n    9: \"Ankle Boot\",\n}\nfigure = plt.figure(figsize=(8, 8))\ncols, rows = 3, 3\nfor i in range(1, cols * rows + 1):\n    sample_idx = (len(), size=(1,)).item()\n    ,  = [sample_idx]\n    figure.add_subplot(rows, cols, i)\n    plt.title(labels_map[])\n    plt.axis(\"off\")\n    plt.imshow(.squeeze(), cmap=\"gray\")\nplt.show()\n\n\n<img alt=\"Pullover, Sandal, Bag, Bag, Sneaker, Bag, Pullover, Sandal, Sandal\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_data_tutorial_001.png\" srcset=\"../../_images/sphx_glr_data_tutorial_001.png\"/>",
            "stations = pd.io.json.json_normalize(js['features']).id\nurl = (\"http://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?\"\n       \"&amp;data=tmpf&amp;data=relh&amp;data=sped&amp;data=mslp&amp;data=p01i&amp;data=vsby&amp;data=gust_mph&amp;data=skyc1&amp;data=skyc2&amp;data=skyc3\"\n       \"&amp;tz=Etc/UTC&amp;format=comma&amp;latlon=no\"\n       \"&amp;{start:year1=%Y&amp;month1=%m&amp;day1=%d}\"\n       \"&amp;{end:year2=%Y&amp;month2=%m&amp;day2=%d}&amp;{stations}\")\nstations = \"&amp;\".join(\"station=%s\" % s for s in stations)\nstart = pd.Timestamp('2014-01-01')\nend=pd.Timestamp('2014-01-31')\n\nweather = (pd.read_csv(url.format(start=start, end=end, stations=stations),\n                       comment=\"#\"))",
            "exog = endog['dln_consump']\nmod = sm.tsa.VARMAX(endog[['dln_inv', 'dln_inc']], order=(2,0), trend='n', exog=exog)\nres = mod.fit(maxiter=1000, disp=False)\nprint(res.summary())",
            "cvs = [\n    ,\n    ,\n    ,\n    ,\n    ,\n    ,\n    ,\n    ,\n]\n\n\nfor cv in cvs:\n    this_cv = cv(n_splits=n_splits)\n    fig, ax = (figsize=(6, 3))\n    plot_cv_indices(this_cv, X, y, groups, ax, n_splits)\n\n    ax.legend(\n        [(color=cmap_cv(0.8)), (color=cmap_cv(0.02))],\n        [\"Testing set\", \"Training set\"],\n        loc=(1.02, 0.8),\n    )\n    # Make the legend fit\n    ()\n    fig.subplots_adjust(right=0.7)\n()\n\n\n\n<img alt=\"KFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_006.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_006.png\"/>\n<img alt=\"GroupKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_007.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_007.png\"/>\n<img alt=\"ShuffleSplit\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_008.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_008.png\"/>\n<img alt=\"StratifiedKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_009.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_009.png\"/>\n<img alt=\"StratifiedGroupKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_010.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_010.png\"/>\n<img alt=\"GroupShuffleSplit\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_011.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_011.png\"/>\n<img alt=\"StratifiedShuffleSplit\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_012.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_012.png\"/>\n<img alt=\"TimeSeriesSplit\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_013.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_013.png\"/>",
            "labels = dta.index[idx[-5:]]\nmake_plot(labels)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_20_0.png\" src=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_20_0.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Introduction to PyTorch->Datasets & DataLoaders->Iterating and Visualizing the Dataset"
            ],
            [
                "pandas_toms_blog->Indexes"
            ],
            [
                "statsmodels->Examples->State space models->VARMAX: Introduction->Example 1: VAR"
            ],
            [
                "sklearn->Examples->Model Selection->Visualizing cross-validation behavior in scikit-learn->Visualize cross-validation indices for many CV objects"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ]
        ]
    },
    "74527": {
        "jupyter_code_cell": "toxic_df.loc[:5, [\"comment_text\", \"tokens\", \"glove_aggregate\"]]\nfrom encoder import encoder",
        "matched_tutorial_code_inds": [
            3336,
            3994,
            3610,
            5283,
            6770
        ],
        "matched_tutorial_codes": [
            "subset_feature = [\"embarked\", \"sex\", \"pclass\", \"age\", \"fare\"]\nX_train, X_test = X_train[subset_feature], X_test[subset_feature]",
            "tips = sns.load_dataset(\"tips\")\nsns.relplot(data=tips, x=\"total_bill\", y=\"tip\")\n",
            "first = df.groupby('unique_carrier').first()\nfirst.ix[10:15, ['fl_date', 'tail_num']]",
            "name = [\"F statistic\", \"p-value\"]\ntest = sms.het_goldfeldquandt(results.resid, results.model.exog)\nlzip(name, test)",
            "df = dta.data[[\"Lottery\", \"Literacy\", \"Wealth\", \"Region\"]].dropna()\ndf.head()"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Pipelines and composite estimators->Column Transformer with Mixed Types"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Relating variables with scatter plots",
                "seaborn->Plotting functions->Visualizing statistical relationships->Relating variables with scatter plots"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Regression Diagnostics->Heteroskedasticity tests"
            ],
            [
                "statsmodels->Examples->User Notes->Formulas->OLS regression using formulas"
            ]
        ]
    },
    "156597": {
        "jupyter_code_cell": "__author__ = 'geomars'\nimport os.path\nimport pandas as pd\nimport numpy as np\nimport datetime as dt\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt \nimport matplotlib.dates as mdates\nimport matplotlib.ticker as ticker\n%matplotlib inline \nimport seaborn as sns\nsns.set(color_codes=True)\nraw = pd.read_csv(\"../datasets/data_indonesia.csv\", sep=',',                  dtype = None, error_bad_lines=False,                  encoding='utf-8', keep_default_na=False)                  \ndf = pd.DataFrame(raw)\ndf.dropna(inplace=True)\ndf.drop_duplicates(inplace=True)",
        "matched_tutorial_code_inds": [
            3240,
            5917,
            3127,
            1475,
            3291
        ],
        "matched_tutorial_codes": [
            "cred_intervals = []\nintervals = [0.5, 0.75, 0.95]\n\nfor interval in intervals:\n    cred_interval = list(t_post.interval(interval))\n    cred_intervals.append([interval, cred_interval[0], cred_interval[1]])\n\ncred_int_df = (\n    cred_intervals, columns=[\"interval\", \"lower value\", \"upper value\"]\n).set_index(\"interval\")\ncred_int_df\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "rng = (0)\nX, y = (n_samples=1000, random_state=rng)\n\ngammas = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7]\nCs = [1, 10, 100, 1e3, 1e4, 1e5]\nparam_grid = {\"gamma\": gammas, \"C\": Cs}\n\nclf = (random_state=rng)\n\ntic = ()\ngsh = (\n    estimator=clf, param_grid=param_grid, factor=2, random_state=rng\n)\ngsh.fit(X, y)\ngsh_time = () - tic\n\ntic = ()\ngs = (estimator=clf, param_grid=param_grid)\ngs.fit(X, y)\ngs_time = () - tic",
            "totals_row = 35\nlocations = np.delete(locations, (totals_row), axis=0)\nnbcases = np.delete(nbcases, (totals_row), axis=0)\n\nchina_total = nbcases[locations[:, 1] == \"China\"].sum(axis=0)\nchina_total",
            "nca = (max_iter=30, random_state=0)\nnca = nca.fit(X, y)\n\n(2)\nax2 = ()\nX_embedded = nca.transform(X)\nrelate_point(X_embedded, i, ax2)\n\nfor i in range(len(X)):\n    ax2.text(X_embedded[i, 0], X_embedded[i, 1], str(i), va=\"center\", ha=\"center\")\n    ax2.scatter(X_embedded[i, 0], X_embedded[i, 1], s=300, c=cm.Set1(y[[i]]), alpha=0.4)\n\nax2.set_title(\"NCA embedding\")\nax2.axes.get_xaxis().set_visible(False)\nax2.axes.get_yaxis().set_visible(False)\nax2.axis(\"equal\")\n()\n\n\n<img alt=\"NCA embedding\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_nca_illustration_002.png\" srcset=\"../../_images/sphx_glr_plot_nca_illustration_002.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Model Selection->Statistical comparison of models using grid search->Comparing two models: Bayesian approach->Region of Practical Equivalence"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "sklearn->Examples->Model Selection->Comparison between grid search and successive halving"
            ],
            [
                "numpy->NumPy Features->Masked Arrays->Exploring the data"
            ],
            [
                "sklearn->Examples->Nearest Neighbors->Neighborhood Components Analysis Illustration->Learning an embedding"
            ]
        ]
    },
    "870398": {
        "jupyter_code_cell": "df3 = pd.read_csv('indicator_forest coverage.csv')\ndf3.head()\ndf3.dropna(axis=1, how='all', inplace=True)",
        "matched_tutorial_code_inds": [
            5703,
            3861,
            5381,
            3784,
            4138
        ],
        "matched_tutorial_codes": [
            "summ_df = infl.summary_frame()\nsumm_df.sort_values(\"cooks_d\", ascending=False)[:10]",
            "df = dd.read_parquet(\"data/indiv-*.parquet\", engine='pyarrow', columns=['occupation'])\n\nmost_common = df.occupation.value_counts().nlargest(100)\nmost_common.compute().sort_values(ascending=False)",
            "spector_data = sm.datasets.spector.load()\nspector_data.exog = sm.add_constant(spector_data.exog, prepend=False)",
            "import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "iris = sns.load_dataset(\"iris\")\ng = sns.PairGrid(iris)\ng.map(sns.scatterplot)\n"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Generalized Linear Models->Influence Measures for GLMs->get the influence measures"
            ],
            [
                "pandas_toms_blog->Scaling->Using Dask"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Data"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships"
            ]
        ]
    },
    "1245411": {
        "jupyter_code_cell": "pyplt.hist(wyoming_permit_dat['long_gun'])\npyplt.title('Histogram for Wyoming Permit checks')\npyplt.xlabel('Value')\npyplt.ylabel('Frequency')\npyplt.show()\nhandgun_array = np.array(wyoming_permit_dat['handgun'])\nlong_gun_array = np.array(wyoming_permit_dat['long_gun'])\nz, p = wtests.ztest(handgun_array, long_gun_array)\nprint(\"The z-value comes out to be {}\".format(z))\nprint(\"The p-value comes out to be {}\".format(p))",
        "matched_tutorial_code_inds": [
            4652,
            4827,
            4374,
            4372,
            1503
        ],
        "matched_tutorial_codes": [
            "plt.plot([1, 2, 3, 4], [1, 4, 9, 16], 'ro')\nplt.axis([0, 6, 0, 20])\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_003.png\" srcset=\"../../_images/sphx_glr_pyplot_003.png, ../../_images/sphx_glr_pyplot_003_2_0x.png 2.0x\"/>",
            "plt.close('all')\narr = np.arange(100).reshape((10, 10))\nfig = plt.figure(figsize=(4, 4))\nim = plt.imshow(arr, interpolation=\"none\")\n\nplt.colorbar(im)\n\nplt.tight_layout()\n\n\n<img alt=\"tight layout guide\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_tight_layout_guide_014.png\" srcset=\"../../_images/sphx_glr_tight_layout_guide_014.png, ../../_images/sphx_glr_tight_layout_guide_014_2_0x.png 2.0x\"/>",
            "plt.semilogy(f, Pwelch_spec)\n plt.xlabel('frequency [Hz]')\n plt.ylabel('PSD')\n plt.grid()\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code displays a single X-Y log-linear plot with the power spectral density on the Y axis vs frequency on the X axis. A single blue trace shows a smooth noise floor at a power level of 6e-2 with a single peak up to a power level of 2 at 1270 Hz.\"' class=\"plot-directive\" src=\"../_images/signal-9.png\"/>\n</figure>",
            "plt.semilogy(f, Pper_spec)\n plt.xlabel('frequency [Hz]')\n plt.ylabel('PSD')\n plt.grid()\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code displays a single X-Y log-linear plot with the power spectral density on the Y axis vs frequency on the X axis. A single blue trace shows a noise floor with a power level of 1e-3 with a single peak at 1270 Hz up to a power of 1. The noise floor measurements appear noisy and oscillate down to 1e-7.\"' class=\"plot-directive\" src=\"../_images/signal-8.png\"/>\n</figure>",
            "plt.plot(t, china_total)\nplt.plot(t[china_total.mask], model(t)[china_total.mask], \"--\", color=\"orange\")\nplt.plot(7, model(7), \"r*\")\nplt.xticks([0, 7, 13], dates[[0, 7, 13]])\nplt.yticks([0, model(7), 10000, 17500])\nplt.legend([\"Mainland China\", \"Cubic estimate\", \"7 days after start\"])\nplt.title(\n    \"COVID-19 cumulative cases from Jan 21 to Feb 3 2020 - Mainland China\\n\"\n    \"Cubic estimate for 7 days after start\"\n)"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Introduction to pyplot->Formatting the style of your plot"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Tight Layout guide->Colorbar"
            ],
            [
                "scipy->Signal Processing (scipy.signal)->Spectral Analysis->Spectral Analysis using Welch\u00e2\u0080\u0099s Method"
            ],
            [
                "scipy->Signal Processing (scipy.signal)->Spectral Analysis->Periodogram Measurements"
            ],
            [
                "numpy->NumPy Features->Masked Arrays->Fitting Data"
            ]
        ]
    },
    "360337": {
        "jupyter_code_cell": "gaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\nscore_gaussian = gaussian.score(X_test,y_test)\nprint('The accuracy of Gaussian Naive Bayes is', score_gaussian)\nfrom sklearn.svm import SVC\nsvc = SVC(gamma=0.22)\nsvc.fit(X_train, y_train)\nscore_svc = svc.score(X_test,y_test)\nprint('The accuracy of SVC is', score_svc)",
        "matched_tutorial_code_inds": [
            2844,
            2860,
            2864,
            6253,
            2462
        ],
        "matched_tutorial_codes": [
            "mae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}\n\n_, ax = (figsize=(5, 5))\ndisplay = (\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n)\nax.set_title(\"Ridge model, small regularization\")\nfor name, score in scores.items():\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\nax.legend(loc=\"upper left\")\n()\n\n\n<img alt=\"Ridge model, small regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_009.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_009.png\"/>",
            "mae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}\n\n_, ax = (figsize=(5, 5))\ndisplay = (\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n)\nax.set_title(\"Ridge model, optimum regularization\")\nfor name, score in scores.items():\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\nax.legend(loc=\"upper left\")\n()\n\n\n<img alt=\"Ridge model, optimum regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_012.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_012.png\"/>",
            "mae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}\n\n_, ax = (figsize=(6, 6))\ndisplay = (\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n)\nax.set_title(\"Lasso model, optimum regularization\")\nfor name, score in scores.items():\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\nax.legend(loc=\"upper left\")\n()\n\n\n<img alt=\"Lasso model, optimum regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_015.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_015.png\"/>",
            "fit1 = SimpleExpSmoothing(oildata, initialization_method=\"heuristic\").fit(\n    smoothing_level=0.2, optimized=False\n)\nfcast1 = fit1.forecast(3).rename(r\"$\\alpha=0.2$\")\nfit2 = SimpleExpSmoothing(oildata, initialization_method=\"heuristic\").fit(\n    smoothing_level=0.6, optimized=False\n)\nfcast2 = fit2.forecast(3).rename(r\"$\\alpha=0.6$\")\nfit3 = SimpleExpSmoothing(oildata, initialization_method=\"estimated\").fit()\nfcast3 = fit3.forecast(3).rename(r\"$\\alpha=%s$\" % fit3.model.params[\"smoothing_level\"])\n\nplt.figure(figsize=(12, 8))\nplt.plot(oildata, marker=\"o\", color=\"black\")\nplt.plot(fit1.fittedvalues, marker=\"o\", color=\"blue\")\n(line1,) = plt.plot(fcast1, marker=\"o\", color=\"blue\")\nplt.plot(fit2.fittedvalues, marker=\"o\", color=\"red\")\n(line2,) = plt.plot(fcast2, marker=\"o\", color=\"red\")\nplt.plot(fit3.fittedvalues, marker=\"o\", color=\"green\")\n(line3,) = plt.plot(fcast3, marker=\"o\", color=\"green\")\nplt.legend([line1, line2, line3], [fcast1.name, fcast2.name, fcast3.name])",
            "gbrt_pipeline.fit(X.iloc[train_0], y.iloc[train_0])\ngbrt_predictions = gbrt_pipeline.predict(X.iloc[test_0])\n\none_hot_poly_pipeline.fit(X.iloc[train_0], y.iloc[train_0])\none_hot_poly_predictions = one_hot_poly_pipeline.predict(X.iloc[test_0])\n\ncyclic_spline_poly_pipeline.fit(X.iloc[train_0], y.iloc[train_0])\ncyclic_spline_poly_predictions = cyclic_spline_poly_pipeline.predict(X.iloc[test_0])"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Simple Exponential Smoothing"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Modeling non-linear feature interactions with kernels"
            ]
        ]
    },
    "921659": {
        "jupyter_code_cell": "stats.gmean(x)\nnp.ptp(data)",
        "matched_tutorial_code_inds": [
            5454,
            4509,
            6140,
            4510,
            4087
        ],
        "matched_tutorial_codes": [
            "stats.binom(5, 1.0 / 6).pmf(2)",
            "stats.ks_2samp(rvs1, rvs2)\nKstestResult(statistic=0.026, pvalue=0.9959527565364388)  # random",
            "stats.normaltest(resid)",
            "stats.ks_2samp(rvs1, rvs3)\nKstestResult(statistic=0.114, pvalue=0.00299005061044668)  # random",
            "sns.catplot(data=titanic, x=\"sex\", y=\"survived\", hue=\"class\", kind=\"point\")\n"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution"
            ],
            [
                "scipy->Statistics (scipy.stats)->Comparing two samples->Kolmogorov-Smirnov test for two samples ks_2samp"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data",
                "statsmodels->Examples->State space models->Statespace ARMA: Sunspots Data->Sunspots Data"
            ],
            [
                "scipy->Statistics (scipy.stats)->Comparing two samples->Kolmogorov-Smirnov test for two samples ks_2samp"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing categorical data->Estimating central tendency->Point plots",
                "seaborn->Plotting functions->Visualizing categorical data->Estimating central tendency->Point plots"
            ]
        ]
    },
    "628170": {
        "jupyter_code_cell": "col1 = Column(name='col1',\n             dtype=np.int,\n             unique=False,\n             validators=[v.funcs.not_null, v.funcs.positive, bt_2_and_10],\n             recoders=None)\ncol3 = Column(name='col3',\n             dtype=np.int,\n             unique=True,\n             validators=[v.funcs.not_null, v.funcs.positive, bt_2_and_10],\n             recoders=None)\ncol4 = Column(name='col4',\n             dtype=str,\n             unique=False,\n             validators=[v.funcs.upper, length_is_one, valid_sex],\n             recoders=None)\ncol4.validators",
        "matched_tutorial_code_inds": [
            6269,
            5299,
            2662,
            153,
            4837
        ],
        "matched_tutorial_codes": [
            "states1 = pd.DataFrame(\n    np.c_[fit1.level, fit1.trend, fit1.season],\n    columns=[\"level\", \"slope\", \"seasonal\"],\n    index=aust.index,\n)\nstates2 = pd.DataFrame(\n    np.c_[fit2.level, fit2.trend, fit2.season],\n    columns=[\"level\", \"slope\", \"seasonal\"],\n    index=aust.index,\n)\nfig, [[ax1, ax4], [ax2, ax5], [ax3, ax6]] = plt.subplots(3, 2, figsize=(12, 8))\nstates1[[\"level\"]].plot(ax=ax1)\nstates1[[\"slope\"]].plot(ax=ax2)\nstates1[[\"seasonal\"]].plot(ax=ax3)\nstates2[[\"level\"]].plot(ax=ax4)\nstates2[[\"slope\"]].plot(ax=ax5)\nstates2[[\"seasonal\"]].plot(ax=ax6)\nplt.show()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_22_0.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_22_0.png\"/>",
            "resid1 = res_ols.resid[w == 1.0]\nvar1 = resid1.var(ddof=int(res_ols.df_model) + 1)\nresid2 = res_ols.resid[w != 1.0]\nvar2 = resid2.var(ddof=int(res_ols.df_model) + 1)\nw_est = w.copy()\nw_est[w != 1.0] = np.sqrt(var2) / np.sqrt(var1)\nres_fwls = sm.WLS(y, X, 1.0 / ((w_est ** 2))).fit()\nprint(res_fwls.summary())",
            "m, s, _ = (\n    (enet.coef_)[0],\n    enet.coef_[enet.coef_ != 0],\n    markerfmt=\"x\",\n    label=\"Elastic net coefficients\",\n)\n([m, s], color=\"#2ca02c\")\nm, s, _ = (\n    (lasso.coef_)[0],\n    lasso.coef_[lasso.coef_ != 0],\n    markerfmt=\"x\",\n    label=\"Lasso coefficients\",\n)\n([m, s], color=\"#ff7f0e\")\n(\n    (coef)[0],\n    coef[coef != 0],\n    label=\"true coefficients\",\n    markerfmt=\"bx\",\n)\n\n(loc=\"best\")\n(\n    \"Lasso $R^2$: %.3f, Elastic Net $R^2$: %.3f\" % (r2_score_lasso, r2_score_enet)\n)\n()\n\n\n<img alt=\"Lasso $R^2$: 0.658, Elastic Net $R^2$: 0.643\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_lasso_and_elasticnet_001.png\" srcset=\"../../_images/sphx_glr_plot_lasso_and_elasticnet_001.png\"/>",
            "t0 = (\n    stmt='batched_dot_mul_sum(x, x)',\n    setup='from __main__ import batched_dot_mul_sum',\n    globals={'x': x})\n\nt1 = (\n    stmt='batched_dot_bmm(x, x)',\n    setup='from __main__ import batched_dot_bmm',\n    globals={'x': x})\n\n# Run only once since benchmark module does warmup for us\nprint(t0.timeit(100))\nprint(t1.timeit(100))\n\n\n\nOutput",
            "inner = [['innerA'],\n         ['innerB']]\nouter = [['upper left',  inner],\n          ['lower left', 'lower right']]\n\nfig, axd = plt.subplot_mosaic(outer, layout=\"constrained\")\nfor k in axd:\n    annotate_axes(axd[k], f'axd[\"{k}\"]')\n\n\n<img alt=\"arranging axes\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_arranging_axes_008.png\" srcset=\"../../_images/sphx_glr_arranging_axes_008.png, ../../_images/sphx_glr_arranging_axes_008_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Winters Seasonal->The Internals"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Weighted Least Squares->Feasible Weighted Least Squares (2-stage FWLS)"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Lasso and Elastic Net for Sparse Signals->Plot"
            ],
            [
                "torch->PyTorch Recipes->PyTorch Benchmark->Steps->3. Benchmarking with torch.utils.benchmark.Timer"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Arranging multiple Axes in a Figure->High-level methods for making grids->Nested Axes layouts"
            ]
        ]
    },
    "835435": {
        "jupyter_code_cell": "from cg_mapping import *\nimport numpy as np\nimport pandas as pd\nimport mdtraj\nimport itertools\ntraj = mdtraj.load(\"npt_b.xtc\", top=\"npt_b.pdb\")\nbeadtypes = set([a.name for a in traj.topology.atoms])",
        "matched_tutorial_code_inds": [
            2770,
            3151,
            2366,
            969,
            2056
        ],
        "matched_tutorial_codes": [
            "from sklearn import datasets\nimport numpy as np\n\nX, y = (return_X_y=True)\nindices = (0, 1)\n\nX_train = X[:-20, indices]\nX_test = X[-20:, indices]\ny_train = y[:-20]\ny_test = y[-20:]",
            "from sklearn.ensemble import \nfrom sklearn.pipeline import \nfrom sklearn.svm import \n\nclassifiers = {\n    \"Linear SVM\": ((), (C=0.025)),\n    \"Random Forest\": (\n        max_depth=5, n_estimators=10, max_features=1\n    ),\n}",
            "from time import \nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import \nfrom sklearn.model_selection import \nfrom sklearn.datasets import \nfrom sklearn.metrics import \nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.preprocessing import \nfrom sklearn.decomposition import \nfrom sklearn.svm import \nfrom sklearn.utils.fixes import loguniform",
            "from functools import partial\nimport numpy as np\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import \nimport torchvision\nimport torchvision.transforms as transforms\nfrom ray import tune\nfrom ray.tune import CLIReporter\nfrom ray.tune.schedulers import ASHAScheduler",
            "from sklearn.cross_decomposition import \n\nplsca = (n_components=2)\nplsca.fit(X_train, Y_train)\nX_train_r, Y_train_r = plsca.transform(X_train, Y_train)\nX_test_r, Y_test_r = plsca.transform(X_test, Y_test)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Generalized Linear Models->Sparsity Example: Fitting only features 1  and 2"
            ],
            [
                "sklearn->Examples->Model Selection->Detection error tradeoff (DET) curve->Define the classifiers"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Faces recognition example using eigenfaces and SVMs"
            ],
            [
                "torch->Model Optimization->Hyperparameter tuning with Ray Tune->Setup / Imports"
            ],
            [
                "sklearn->Examples->Cross decomposition->Compare cross decomposition methods->Canonical (symmetric) PLS->Transform data"
            ]
        ]
    },
    "836066": {
        "jupyter_code_cell": "import requests\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom matplotlib import patches as patch\n%matplotlib inline\nIRIS_FILE_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\nfile = requests.get(IRIS_FILE_URL).text",
        "matched_tutorial_code_inds": [
            5256,
            3271,
            6250,
            6909,
            2674
        ],
        "matched_tutorial_codes": [
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pandas_datareader as pdr\nimport seaborn\n\nimport statsmodels.api as sm\nfrom statsmodels.regression.rolling import RollingOLS\n\nseaborn.set_style(\"darkgrid\")\npd.plotting.register_matplotlib_converters()\n%matplotlib inline",
            "import joblib\nimport numpy as np\nfrom scipy.sparse import \nfrom sklearn.base import , \nfrom sklearn.datasets import \nfrom sklearn.utils import \n\n\nclass NMSlibTransformer(, ):\n    \"\"\"Wrapper for using nmslib as sklearn's KNeighborsTransformer\"\"\"\n\n    def __init__(self, n_neighbors=5, metric=\"euclidean\", method=\"sw-graph\", n_jobs=-1):\n        self.n_neighbors = n_neighbors\n        self.method = method\n        self.metric = metric\n        self.n_jobs = n_jobs\n\n    def fit(self, X):\n        self.n_samples_fit_ = X.shape[0]\n\n        # see more metric in the manual\n        # https://github.com/nmslib/nmslib/tree/master/manual\n        space = {\n            \"euclidean\": \"l2\",\n            \"cosine\": \"cosinesimil\",\n            \"l1\": \"l1\",\n            \"l2\": \"l2\",\n        }[self.metric]\n\n        self.nmslib_ = nmslib.init(method=self.method, space=space)\n        self.nmslib_.addDataPointBatch(X.copy())\n        self.nmslib_.createIndex()\n        return self\n\n    def transform(self, X):\n        n_samples_transform = X.shape[0]\n\n        # For compatibility reasons, as each sample is considered as its own\n        # neighbor, one extra neighbor will be computed.\n        n_neighbors = self.n_neighbors + 1\n\n        if self.n_jobs &lt; 0:\n            # Same handling as done in joblib for negative values of n_jobs:\n            # in particular, `n_jobs == -1` means \"as many threads as CPUs\".\n            num_threads = joblib.cpu_count() + self.n_jobs + 1\n        else:\n            num_threads = self.n_jobs\n\n        results = self.nmslib_.knnQueryBatch(\n            X.copy(), k=n_neighbors, num_threads=num_threads\n        )\n        indices, distances = zip(*results)\n        indices, distances = (indices), (distances)\n\n        indptr = (0, n_samples_transform * n_neighbors + 1, n_neighbors)\n        kneighbors_graph = (\n            (distances.ravel(), indices.ravel(), indptr),\n            shape=(n_samples_transform, self.n_samples_fit_),\n        )\n\n        return kneighbors_graph\n\n\ndef load_mnist(n_samples):\n    \"\"\"Load MNIST, shuffle the data, and return only n_samples.\"\"\"\n    mnist = (\"mnist_784\", as_frame=False, parser=\"pandas\")\n    X, y = (mnist.data, mnist.target, random_state=2)\n    return X[:n_samples] / 255, y[:n_samples]",
            "import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n\n%matplotlib inline\n\ndata = [\n    446.6565,\n    454.4733,\n    455.663,\n    423.6322,\n    456.2713,\n    440.5881,\n    425.3325,\n    485.1494,\n    506.0482,\n    526.792,\n    514.2689,\n    494.211,\n]\nindex = pd.date_range(start=\"1996\", end=\"2008\", freq=\"A\")\noildata = pd.Series(data, index)\n\ndata = [\n    17.5534,\n    21.86,\n    23.8866,\n    26.9293,\n    26.8885,\n    28.8314,\n    30.0751,\n    30.9535,\n    30.1857,\n    31.5797,\n    32.5776,\n    33.4774,\n    39.0216,\n    41.3864,\n    41.5966,\n]\nindex = pd.date_range(start=\"1990\", end=\"2005\", freq=\"A\")\nair = pd.Series(data, index)\n\ndata = [\n    263.9177,\n    268.3072,\n    260.6626,\n    266.6394,\n    277.5158,\n    283.834,\n    290.309,\n    292.4742,\n    300.8307,\n    309.2867,\n    318.3311,\n    329.3724,\n    338.884,\n    339.2441,\n    328.6006,\n    314.2554,\n    314.4597,\n    321.4138,\n    329.7893,\n    346.3852,\n    352.2979,\n    348.3705,\n    417.5629,\n    417.1236,\n    417.7495,\n    412.2339,\n    411.9468,\n    394.6971,\n    401.4993,\n    408.2705,\n    414.2428,\n]\nindex = pd.date_range(start=\"1970\", end=\"2001\", freq=\"A\")\nlivestock2 = pd.Series(data, index)\n\ndata = [407.9979, 403.4608, 413.8249, 428.105, 445.3387, 452.9942, 455.7402]\nindex = pd.date_range(start=\"2001\", end=\"2008\", freq=\"A\")\nlivestock3 = pd.Series(data, index)\n\ndata = [\n    41.7275,\n    24.0418,\n    32.3281,\n    37.3287,\n    46.2132,\n    29.3463,\n    36.4829,\n    42.9777,\n    48.9015,\n    31.1802,\n    37.7179,\n    40.4202,\n    51.2069,\n    31.8872,\n    40.9783,\n    43.7725,\n    55.5586,\n    33.8509,\n    42.0764,\n    45.6423,\n    59.7668,\n    35.1919,\n    44.3197,\n    47.9137,\n]\nindex = pd.date_range(start=\"2005\", end=\"2010-Q4\", freq=\"QS-OCT\")\naust = pd.Series(data, index)",
            "import numpy as np\nfrom scipy.stats.distributions import norm\nfrom statsmodels.base.distributed_estimation import DistributedModel\n\n\ndef _exog_gen(exog, partitions):\n    \"\"\"partitions exog data\"\"\"\n\n    n_exog = exog.shape[0]\n    n_part = np.ceil(n_exog / partitions)\n\n    ii = 0\n    while ii &lt; n_exog:\n        jj = int(min(ii + n_part, n_exog))\n        yield exog[ii:jj, :]\n        ii += int(n_part)\n\n\ndef _endog_gen(endog, partitions):\n    \"\"\"partitions endog data\"\"\"\n\n    n_endog = endog.shape[0]\n    n_part = np.ceil(n_endog / partitions)\n\n    ii = 0\n    while ii &lt; n_endog:\n        jj = int(min(ii + n_part, n_endog))\n        yield endog[ii:jj]\n        ii += int(n_part)",
            "import time\nfrom sklearn.preprocessing import \nfrom sklearn.linear_model import \nfrom sklearn.pipeline import \n\nstart_time = ()\nlasso_lars_ic = ((), (criterion=\"aic\")).fit(X, y)\nfit_time = () - start_time"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Rolling Least Squares"
            ],
            [
                "sklearn->Examples->Nearest Neighbors->Approximate nearest neighbors in TSNE"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Loading data"
            ],
            [
                "statsmodels->Examples->User Notes->Distributed Estimations"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Lasso model selection: AIC-BIC / cross-validation->Selecting Lasso via an information criterion"
            ]
        ]
    },
    "521639": {
        "jupyter_code_cell": "file=\"Losses_Ultra.tree\"\ntry:\n    f=open(file, 'r')\nexcept IOError:\n    print (\"Unknown file: \"+file)\n    sys.exit()\nline = \"\"\nfor l in f:\n    line += l.strip()\nf.close()\ntreeToAnnotate = Tree( line )\nrenumberNodes(treeToAnnotate, leafList2NodeIdRef)\nprint (treeToAnnotate.write( format=2) )\nfile=\"Transfers_Ultra.tree\"\ntry:\n    f=open(file, 'r')\nexcept IOError:\n    print (\"Unknown file: \"+file)\n    sys.exit()\nline = \"\"\nfor l in f:\n    line += l.strip()\nf.close()\ntreeToAnnotate = Tree( line )\nrenumberNodes(treeToAnnotate, leafList2NodeIdRef)\nprint (treeToAnnotate.write( format=2) )",
        "matched_tutorial_code_inds": [
            6253,
            2601,
            2093,
            1160,
            2096
        ],
        "matched_tutorial_codes": [
            "fit1 = SimpleExpSmoothing(oildata, initialization_method=\"heuristic\").fit(\n    smoothing_level=0.2, optimized=False\n)\nfcast1 = fit1.forecast(3).rename(r\"$\\alpha=0.2$\")\nfit2 = SimpleExpSmoothing(oildata, initialization_method=\"heuristic\").fit(\n    smoothing_level=0.6, optimized=False\n)\nfcast2 = fit2.forecast(3).rename(r\"$\\alpha=0.6$\")\nfit3 = SimpleExpSmoothing(oildata, initialization_method=\"estimated\").fit()\nfcast3 = fit3.forecast(3).rename(r\"$\\alpha=%s$\" % fit3.model.params[\"smoothing_level\"])\n\nplt.figure(figsize=(12, 8))\nplt.plot(oildata, marker=\"o\", color=\"black\")\nplt.plot(fit1.fittedvalues, marker=\"o\", color=\"blue\")\n(line1,) = plt.plot(fcast1, marker=\"o\", color=\"blue\")\nplt.plot(fit2.fittedvalues, marker=\"o\", color=\"red\")\n(line2,) = plt.plot(fcast2, marker=\"o\", color=\"red\")\nplt.plot(fit3.fittedvalues, marker=\"o\", color=\"green\")\n(line3,) = plt.plot(fcast3, marker=\"o\", color=\"green\")\nplt.legend([line1, line2, line3], [fcast1.name, fcast2.name, fcast3.name])",
            "vmin, vmax = (-log_marginal_likelihood).min(), 50\nlevel = (((vmin), (vmax), num=50), decimals=1)\n(\n    length_scale_grid,\n    noise_level_grid,\n    -log_marginal_likelihood,\n    levels=level,\n    norm=(vmin=vmin, vmax=vmax),\n)\n()\n(\"log\")\n(\"log\")\n(\"Length-scale\")\n(\"Noise-level\")\n(\"Log-marginal-likelihood\")\n()\n\n\n<img alt=\"Log-marginal-likelihood\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_noisy_005.png\" srcset=\"../../_images/sphx_glr_plot_gpr_noisy_005.png\"/>",
            "n_nodes = clf.tree_.node_count\nchildren_left = clf.tree_.children_left\nchildren_right = clf.tree_.children_right\nfeature = clf.tree_.feature\nthreshold = clf.tree_.threshold\n\nnode_depth = (shape=n_nodes, dtype=)\nis_leaves = (shape=n_nodes, dtype=bool)\nstack = [(0, 0)]  # start with the root node id (0) and its depth (0)\nwhile len(stack)  0:\n    # `pop` ensures each node is only visited once\n    node_id, depth = stack.pop()\n    node_depth[node_id] = depth\n\n    # If the left and right child of a node is not the same we have a split\n    # node\n    is_split_node = children_left[node_id] != children_right[node_id]\n    # If a split node, append left and right children and depth to `stack`\n    # so we can loop through them\n    if is_split_node:\n        stack.append((children_left[node_id], depth + 1))\n        stack.append((children_right[node_id], depth + 1))\n    else:\n        is_leaves[node_id] = True\n\nprint(\n    \"The binary tree structure has {n} nodes and has \"\n    \"the following tree structure:\\n\".format(n=n_nodes)\n)\nfor i in range(n_nodes):\n    if is_leaves[i]:\n        print(\n            \"{space}node={node} is a leaf node.\".format(\n                space=node_depth[i] * \"\\t\", node=i\n            )\n        )\n    else:\n        print(\n            \"{space}node={node} is a split node: \"\n            \"go to node {left} if X[:, {feature}] &lt;= {threshold} \"\n            \"else to node {right}.\".format(\n                space=node_depth[i] * \"\\t\",\n                node=i,\n                left=children_left[i],\n                feature=feature[i],\n                threshold=threshold[i],\n                right=children_right[i],\n            )\n        )",
            "eager_times = []\ncompile_times = []\nfor i in range(N_ITERS):\n    inp = generate_data(16)[0]\n    _, eager_time = timed(lambda: evaluate(model, inp))\n    eager_times.append(eager_time)\n    print(f\"eager eval time {i}: {eager_time}\")\n\nprint(\"~\" * 10)\n\ncompile_times = []\nfor i in range(N_ITERS):\n    inp = generate_data(16)[0]\n    _, compile_time = timed(lambda: evaluate_opt(model, inp))\n    compile_times.append(compile_time)\n    print(f\"compile eval time {i}: {compile_time}\")\nprint(\"~\" * 10)\n\nimport numpy as np\neager_med = np.median(eager_times)\ncompile_med = np.median(compile_times)\nspeedup = eager_med / compile_med\nprint(f\"(eval) eager median: {eager_med}, compile median: {compile_med}, speedup: {speedup}x\")\nprint(\"~\" * 10)",
            "node_indicator = clf.decision_path(X_test)\nleaf_id = clf.apply(X_test)\n\nsample_id = 0\n# obtain ids of the nodes `sample_id` goes through, i.e., row `sample_id`\nnode_index = node_indicator.indices[\n    node_indicator.indptr[sample_id] : node_indicator.indptr[sample_id + 1]\n]\n\nprint(\"Rules used to predict sample {id}:\\n\".format(id=sample_id))\nfor node_id in node_index:\n    # continue to the next node if it is a leaf node\n    if leaf_id[sample_id] == node_id:\n        continue\n\n    # check if value of the split feature for sample 0 is below threshold\n    if X_test[sample_id, feature[node_id]] &lt;= threshold[node_id]:\n        threshold_sign = \"&lt;=\"\n    else:\n        threshold_sign = \"\"\n\n    print(\n        \"decision node {node} : (X_test[{sample}, {feature}] = {value}) \"\n        \"{inequality} {threshold})\".format(\n            node=node_id,\n            sample=sample_id,\n            feature=feature[node_id],\n            value=X_test[sample_id, feature[node_id]],\n            inequality=threshold_sign,\n            threshold=threshold[node_id],\n        )\n    )"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Simple Exponential Smoothing"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) with noise-level estimation->Optimisation of kernel hyperparameters in GPR"
            ],
            [
                "sklearn->Examples->Decision Trees->Understanding the decision tree structure->Tree structure"
            ],
            [
                "torch->Model Optimization->torch.compile Tutorial->Demonstrating Speedups"
            ],
            [
                "sklearn->Examples->Decision Trees->Understanding the decision tree structure->Decision path"
            ]
        ]
    },
    "1234230": {
        "jupyter_code_cell": "men_bot = mensrights.groupby(\"author\").apply(duplicate_post)\nfem_bot = feminism.groupby(\"author\").apply(duplicate_post)\nmen_bot.shape[0], men_bot.sum(), fem_bot.shape[0], fem_bot.sum()",
        "matched_tutorial_code_inds": [
            5397,
            5338,
            6231,
            5381,
            5779
        ],
        "matched_tutorial_codes": [
            "mlogit_mod = sm.MNLogit(anes_data.endog, anes_exog)\nmlogit_res = mlogit_mod.fit()\nprint(mlogit_res.params)",
            "fig = sm.graphics.plot_regress_exog(prestige_model, \"education\")\nfig.tight_layout(pad=1.0)",
            "res_areturns.smoothed_marginal_probabilities[0].plot(\n    title=\"Probability of being in a low-variance regime\", figsize=(12, 3)\n)",
            "spector_data = sm.datasets.spector.load()\nspector_data.exog = sm.add_constant(spector_data.exog, prepend=False)",
            "huber = sm.robust.scale.Huber()\nloc, scale = huber(fat_tails)\nprint(loc, scale)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Multinomial Logit"
            ],
            [
                "statsmodels->Examples->Plotting->Regression Plots->Duncan\u2019s Prestige Dataset->Single Variable Regression Diagnostics"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Markov switching dynamic regression->Switching variances"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Data"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Scale Estimators"
            ]
        ]
    },
    "1468231": {
        "jupyter_code_cell": "endpoint = 'https://wikimedia.org/api/rest_v1/metrics/legacy/pagecounts/aggregate/{project}/{access-site}/{granularity}/{start}/{end}'\nheaders={'User-Agent' : 'https://github.com/anqiwang0827', 'From' : 'anqiw2@uw.edu'}\nparams = {'project' : 'en.wikipedia.org',\n            'access-site' : 'desktop-site',\n            'granularity' : 'monthly',\n            'start' : '2008010100',\n            'end' : '2016080100'\n            }\npagecounts_desktop_site_call = requests.get(endpoint.format(**params))\npagecounts_desktop_site = pagecounts_desktop_site_call.json()\npagecounts_desktop_site_200801_201607 = json.dumps(pagecounts_desktop_site)\nwith open('pagecounts_desktop-site_200801-201607.json', 'w') as file5:\n     json.dump(pagecounts_desktop_site_200801_201607, file5)\ndf_pv_mw = pd.DataFrame(pageview_mobileweb)\ndf_pv_ma = pd.DataFrame(pageview_mobileapp)\ndf_pv_dk = pd.DataFrame(pageview_desktop)\ndf_pc_ms = pd.DataFrame(pagecounts_mobile_site)\ndf_pc_ds = pd.DataFrame(pagecounts_desktop_site)\ndf_pv_mw_items = df_pv_mw['items']\ndf_pv_ma_items = df_pv_ma['items']\ndf_pv_dk_items = df_pv_dk['items']\ndf_pc_ms_items = df_pc_ms['items']\ndf_pc_ds_items = df_pc_ds['items']",
        "matched_tutorial_code_inds": [
            6891,
            2753,
            1302,
            3244,
            3257
        ],
        "matched_tutorial_codes": [
            "url = 'https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/csv/COUNT/medpar.csv'\nmedpar = read.csv(url)\nf = los~factor(type)+hmo+white\n\nlibrary(MASS)\nmod = glm.nb(f, medpar)\ncoef(summary(mod))\n                 Estimate Std. Error   z value      Pr(|z|)\n(Intercept)    2.31027893 0.06744676 34.253370 3.885556e-257\nfactor(type)2  0.22124898 0.05045746  4.384861  1.160597e-05\nfactor(type)3  0.70615882 0.07599849  9.291748  1.517751e-20\nhmo           -0.06795522 0.05321375 -1.277024  2.015939e-01\nwhite         -0.12906544 0.06836272 -1.887951  5.903257e-02",
            "quantiles = [0.05, 0.5, 0.95]\npredictions = {}\nout_bounds_predictions = (y_true_mean, dtype=)\nfor quantile in quantiles:\n    qr = (quantile=quantile, alpha=0, solver=solver)\n    y_pred = qr.fit(X, y_pareto).predict(X)\n    predictions[quantile] = y_pred\n\n    if quantile == min(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred = y_pareto\n        )\n    elif quantile == max(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred &lt;= y_pareto\n        )",
            "processes = []\nworld_size = args.world_size\nif args.rank == 0:\n    p = mp.Process(target=run_parameter_server, args=(0, world_size))\n    p.start()\n    processes.append(p)\nelse:\n    # Get data to train on\n    train_loader = torch.utils.data.DataLoader(\n        datasets.MNIST('../data', train=True, download=True,\n                       transform=transforms.Compose([\n                           transforms.ToTensor(),\n                           transforms.Normalize((0.1307,), (0.3081,))\n                       ])),\n        batch_size=32, shuffle=True,)\n    test_loader = torch.utils.data.DataLoader(\n        datasets.MNIST(\n            '../data',\n            train=False,\n            transform=transforms.Compose([\n                    transforms.ToTensor(),\n                    transforms.Normalize((0.1307,), (0.3081,))\n                        ])),\n        batch_size=32,\n        shuffle=True,\n    )\n    # start training worker on this node\n    p = mp.Process(\n        target=run_worker,\n        args=(\n            args.rank,\n            world_size, args.num_gpus,\n            train_loader,\n            test_loader))\n    p.start()\n    processes.append(p)\n\nfor p in processes:\n    p.join()",
            "rng = (0)\n\nX, y = (n_samples=400, n_features=12, random_state=rng)\n\nclf = (n_estimators=20, random_state=rng)\n\nparam_dist = {\n    \"max_depth\": [3, None],\n    \"max_features\": (1, 6),\n    \"min_samples_split\": (2, 11),\n    \"bootstrap\": [True, False],\n    \"criterion\": [\"gini\", \"entropy\"],\n}\n\nrsh = (\n    estimator=clf, param_distributions=param_dist, factor=2, random_state=rng\n)\nrsh.fit(X, y)",
            "alphas = (-5, 1, 60)\nenet = (l1_ratio=0.7, max_iter=10000)\ntrain_errors = list()\ntest_errors = list()\nfor alpha in alphas:\n    enet.set_params(alpha=alpha)\n    enet.fit(X_train, y_train)\n    train_errors.append(enet.score(X_train, y_train))\n    test_errors.append(enet.score(X_test, y_test))\n\ni_alpha_optim = (test_errors)\nalpha_optim = alphas[i_alpha_optim]\nprint(\"Optimal regularization parameter : %s\" % alpha_optim)\n\n# Estimate the coef_ on full data with optimal regularization parameter\nenet.set_params(alpha=alpha_optim)\ncoef_ = enet.fit(X, y).coef_"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 2: Negative Binomial Regression for Count Data->Testing"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Quantile regression->Fitting a QuantileRegressor"
            ],
            [
                "torch->Parallel and Distributed Training->Implementing a Parameter Server Using Distributed RPC Framework"
            ],
            [
                "sklearn->Examples->Model Selection->Successive Halving Iterations"
            ],
            [
                "sklearn->Examples->Model Selection->Train error vs Test error->Compute train and test errors"
            ]
        ]
    },
    "333615": {
        "jupyter_code_cell": "I_min, conf_int_lo = get_lo_conf_index(randomforest)\nplt.plot(randomforest.cv_results_['mean_test_score'])\nplt.title('Random forest: effect of number of trees on \\ntraining data performance(10-fold cross-validation)')\nplt.xlabel('Hyper-parameter index (lower values imply fewer trees)')\nplt.ylabel('ROC AUC')\nplt.axhline(randomforest.best_score_, color='k', linestyle='dashed')\nplt.axhline(conf_int_lo, color='r', linestyle='dashed')\nplt.axvline(I_min, color='r')\ndef bootstrap_statistic(stat_function, y_test, y_hat):\n    np.random.seed(3743)\n    N = 10**3\n    result = np.empty(N)\n    for n in xrange(N):\n        I = np.random.choice(len(y_test), len(y_test))\n        result[n] = stat_function(y_test[I], y_hat[I])\n    return result",
        "matched_tutorial_code_inds": [
            2564,
            4541,
            4396,
            4394,
            4497
        ],
        "matched_tutorial_codes": [
            "mean_prediction, std_prediction = gaussian_process.predict(X, return_std=True)\n\n(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\n(X_train, y_train, label=\"Observations\")\n(X, mean_prediction, label=\"Mean prediction\")\n(\n    X.ravel(),\n    mean_prediction - 1.96 * std_prediction,\n    mean_prediction + 1.96 * std_prediction,\n    alpha=0.5,\n    label=r\"95% confidence interval\",\n)\n()\n(\"$x$\")\n(\"$f(x)$\")\n_ = (\"Gaussian process regression on noise-free dataset\")\n\n\n<img alt=\"Gaussian process regression on noise-free dataset\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_noisy_targets_002.png\" srcset=\"../../_images/sphx_glr_plot_gpr_noisy_targets_002.png\"/>",
            "stat, pvalue, mgc_dict = multiscale_graphcorr(x, y)\n print(\"MGC test statistic: \", round(stat, 1))\nMGC test statistic:  0.2  # random\n print(\"P-value: \", round(pvalue, 1))\nP-value:  0.0\n mgc_plot(x, y, \"Spiral\", mgc_dict, only_mgc=True)\n\n\n<figure class=\"align-center\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/mgc_plot4.png\"/>\n</figure>",
            "evals_small, evecs_small = eigsh(X, 3, which='SM', tol=1E-2)\n evals_all[:3]\narray([0.00053181, 0.00298319, 0.01387821])\n evals_small\narray([0.00053181, 0.00298319, 0.01387821])\n np.dot(evecs_small.T, evecs_all[:,:3])\narray([[ 0.99999999  0.00000024 -0.00000049],    # may vary (signs)\n       [-0.00000023  0.99999999  0.00000056],\n       [ 0.00000031 -0.00000037  0.99999852]])",
            "evals_large, evecs_large = eigsh(X, 3, which='LM')\n print(evals_all[-3:])\n[29.22435321 30.05590784 30.58591252]\n print(evals_large)\n[29.22435321 30.05590784 30.58591252]\n print(np.dot(evecs_large.T, evecs_all[:,-3:]))\narray([[-1.  0.  0.],       # may vary (signs)\n       [ 0.  1.  0.],\n       [-0.  0. -1.]])",
            "crit01, crit05, crit10 = stats.t.ppf([1-0.01, 1-0.05, 1-0.10], 10)\n print('critical values from ppf at 1%%, 5%% and 10%% %8.4f %8.4f %8.4f' % (crit01, crit05, crit10))\ncritical values from ppf at 1%, 5% and 10%   2.7638   1.8125   1.3722\n print('critical values from isf at 1%%, 5%% and 10%% %8.4f %8.4f %8.4f' % tuple(stats.t.isf([0.01,0.05,0.10],10)))\ncritical values from isf at 1%, 5% and 10%   2.7638   1.8125   1.3722"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian Processes regression: basic introductory example->Example with noise-free target"
            ],
            [
                "scipy->Statistics (scipy.stats)->Kernel density estimation->Multiscale Graph Correlation (MGC)"
            ],
            [
                "scipy->Sparse eigenvalue problems with ARPACK->Examples"
            ],
            [
                "scipy->Sparse eigenvalue problems with ARPACK->Examples"
            ],
            [
                "scipy->Statistics (scipy.stats)->Analysing one sample->Tails of the distribution"
            ]
        ]
    },
    "683966": {
        "jupyter_code_cell": "FP_df\ncols_to_visualize = [\"GPA\", \"weight\", \"life_rewarding\", \"healthy_feeling\"]\nfor col in cols_to_visualize:\n    plt.title(col + \" histogram\")\n    plt.hist(QOFL_df[col])\n    plt.show()\npd.plotting.scatter_matrix(QOFL_df[cols_to_visualize])\nplt.suptitle(\"Scatter matrix of \" + ', '.join(cols_to_visualize))\nplt.show()",
        "matched_tutorial_code_inds": [
            5383,
            1516,
            2701,
            2703,
            3176
        ],
        "matched_tutorial_codes": [
            "GPA  TUCE  PSI  const\n0  2.66  20.0  0.0    1.0\n1  2.89  22.0  0.0    1.0\n2  3.28  24.0  0.0    1.0\n3  2.92  12.0  0.0    1.0\n4  4.00  21.0  0.0    1.0\n0    0.0\n1    0.0\n2    0.0\n3    0.0\n4    1.0\nName: GRADE, dtype: float64",
            "model = sm.OLS(yi, Z)",
            "NNLS R2 score 0.8225220806196526",
            "OLS R2 score 0.7436926291700356",
            "ovo_tpr = (fpr_grid)\n\nfig, ax = (figsize=(6, 6))\nfor ix, (label_a, label_b) in enumerate(pair_list):\n    ovo_tpr += mean_tpr[ix]\n    (\n        fpr_grid,\n        mean_tpr[ix],\n        label=f\"Mean {label_a} vs {label_b} (AUC = {pair_scores[ix]:.2f})\",\n    )\n\novo_tpr /= sum(1 for pair in enumerate(pair_list))\n\n(\n    fpr_grid,\n    ovo_tpr,\n    label=f\"One-vs-One macro-average (AUC = {macro_roc_auc_ovo:.2f})\",\n    linestyle=\":\",\n    linewidth=4,\n)\n([0, 1], [0, 1], \"k--\", label=\"chance level (AUC = 0.5)\")\n(\"square\")\n(\"False Positive Rate\")\n(\"True Positive Rate\")\n(\"Extension of Receiver Operating Characteristic\\nto One-vs-One multiclass\")\n()\n()\n\n\n<img alt=\"Extension of Receiver Operating Characteristic to One-vs-One multiclass\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_roc_007.png\" srcset=\"../../_images/sphx_glr_plot_roc_007.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Data"
            ],
            [
                "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy->Calculating the historical growth curve for transistors"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Non-negative least squares"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Non-negative least squares"
            ],
            [
                "sklearn->Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->One-vs-One multiclass ROC->Plot all OvO ROC curves together"
            ]
        ]
    },
    "82504": {
        "jupyter_code_cell": "tab = pd.crosstab(train['Deck'], train['Survived'])\nprint(tab)\ndummy = tab.div(tab.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True)\ndummy = plt.xlabel('Deck')\ndummy = plt.ylabel('Percentage')\nstats.binom_test(x=12,n=12+35,p=24/(24.+35.))",
        "matched_tutorial_code_inds": [
            2861,
            2865,
            5613,
            4800,
            4090
        ],
        "matched_tutorial_codes": [
            "coefs = (\n    model[-1].regressor_.coef_,\n    columns=[\"Coefficients importance\"],\n    index=feature_names,\n)\ncoefs.plot.barh(figsize=(9, 7))\n(\"Ridge model, with regularization, normalized variables\")\n(\"Raw coefficient values\")\n(x=0, color=\".5\")\n(left=0.3)\n\n\n<img alt=\"Ridge model, with regularization, normalized variables\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_013.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_013.png\"/>",
            "coefs = (\n    model[-1].regressor_.coef_,\n    columns=[\"Coefficients importance\"],\n    index=feature_names,\n)\ncoefs.plot(kind=\"barh\", figsize=(9, 7))\n(\"Lasso model, optimum regularization, normalized variables\")\n(x=0, color=\".5\")\n(left=0.3)\n\n\n<img alt=\"Lasso model, optimum regularization, normalized variables\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_016.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_016.png\"/>",
            "gr = data[\"affairs rate_marriage age yrs_married\".split()].groupby(\n    \"rate_marriage age yrs_married\".split()\n)\ndf_a = gr.agg([\"mean\", \"sum\", \"count\"])\n\n\ndef merge_tuple(tpl):\n    if isinstance(tpl, tuple) and len(tpl)  1:\n        return \"_\".join(map(str, tpl))\n    else:\n        return tpl\n\n\ndf_a.columns = df_a.columns.map(merge_tuple)\ndf_a.reset_index(inplace=True)\nprint(df_a.shape)\ndf_a.head()",
            "fig = plt.figure(layout=\"constrained\")\nsfigs = fig.subfigures(1, 2, width_ratios=[1, 2])\n\naxs_left = sfigs[0].subplots(2, 1)\nfor ax in axs_left.flat:\n    example_plot(ax)\n\naxs_right = sfigs[1].subplots(2, 2)\nfor ax in axs_right.flat:\n    pcm = ax.pcolormesh(arr, **pc_kwargs)\n    ax.set_xlabel('x-label')\n    ax.set_ylabel('y-label')\n    ax.set_title('title')\nfig.colorbar(pcm, ax=axs_right)\nfig.suptitle('Nested plots using subfigures')\n\n\n<img alt=\"Nested plots using subfigures, Title, Title, title, title, title, title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_constrainedlayout_guide_023.png\" srcset=\"../../_images/sphx_glr_constrainedlayout_guide_023.png, ../../_images/sphx_glr_constrainedlayout_guide_023_2_0x.png 2.0x\"/>",
            "g = sns.catplot(\n    data=titanic,\n    x=\"fare\", y=\"embark_town\", row=\"class\",\n    kind=\"box\", orient=\"h\",\n    sharex=False, margin_titles=True,\n    height=1.5, aspect=4,\n)\ng.set(xlabel=\"Fare\", ylabel=\"\")\ng.set_titles(row_template=\"{row_name} class\")\nfor ax in g.axes.flat:\n    ax.xaxis.set_major_formatter('${x:.0f}')\n"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Condensing and Aggregating observations->Dataset with unique explanatory variables (exog)"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Constrained Layout Guide->Use with GridSpec"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing categorical data->Showing additional dimensions",
                "seaborn->Plotting functions->Visualizing categorical data->Showing additional dimensions"
            ]
        ]
    },
    "581134": {
        "jupyter_code_cell": "pd.read_csv('AAII.csv', nrows = 5)\ndf_AAII = pd.read_csv('AAII.csv')\ndef convert_to_float(df_AAII):\n    sentimentList = [\"Bullish\", \"Neutral\", \"Bearish\"]\n    for i in range(len(sentimentList)):\n        sentimentType = sentimentList[i]\n        df_AAII[sentimentType] = df_AAII[sentimentType].replace('%','',regex=True).astype('float')/100\n    return df_AAII\ndf_AAII_float = convert_to_float(df_AAII)",
        "matched_tutorial_code_inds": [
            256,
            4546,
            149,
            4835,
            5712
        ],
        "matched_tutorial_codes": [
            "tensor([[0.0795, 0.1649, 0.2479],\n        [0.0795, 0.1649, 0.2479],\n        [0.0795, 0.1649, 0.2479],\n        [0.0795, 0.1649, 0.2479],\n        [0.0795, 0.1649, 0.2479]])\ntensor([0.0795, 0.1649, 0.2479])",
            "engine.reset()\n engine.random(5)\narray([[0.22166437, 0.07980522],  # random\n       [0.72166437, 0.93165708],\n       [0.47166437, 0.41313856],\n       [0.97166437, 0.19091633],\n       [0.01853937, 0.74647189]])",
            "num_threads = ()\nprint(f'Benchmarking on {num_threads} threads')\n\nt0 = (\n    stmt='batched_dot_mul_sum(x, x)',\n    setup='from __main__ import batched_dot_mul_sum',\n    globals={'x': x},\n    num_threads=num_threads,\n    label='Multithreaded batch dot',\n    sub_label='Implemented using mul and sum')\n\nt1 = (\n    stmt='batched_dot_bmm(x, x)',\n    setup='from __main__ import batched_dot_bmm',\n    globals={'x': x},\n    num_threads=num_threads,\n    label='Multithreaded batch dot',\n    sub_label='Implemented using bmm')\n\nprint(t0.timeit(100))\nprint(t1.timeit(100))\n\n\n\nOutput",
            "gs_kw = dict(width_ratios=[1.4, 1], height_ratios=[1, 2])\nfig, axd = plt.subplot_mosaic([['upper left', 'right'],\n                               ['lower left', 'right']],\n                              gridspec_kw=gs_kw, figsize=(5.5, 3.5),\n                              layout=\"constrained\")\nfor k in axd:\n    annotate_axes(axd[k], f'axd[\"{k}\"]', fontsize=14)\nfig.suptitle('plt.subplot_mosaic()')\n\n\n<img alt=\"plt.subplot_mosaic()\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_arranging_axes_006.png\" srcset=\"../../_images/sphx_glr_arranging_axes_006.png, ../../_images/sphx_glr_arranging_axes_006_2_0x.png 2.0x\"/>",
            "df = pd.read_csv(raw, header=None)\ndf = df.melt()\ndf[\"site\"] = 1 + np.floor(df.index / 10).astype(int)\ndf[\"variety\"] = 1 + (df.index % 10)\ndf = df.rename(columns={\"value\": \"blotch\"})\ndf = df.drop(\"variable\", axis=1)\ndf[\"blotch\"] /= 100"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Introduction to PyTorch->Automatic Differentiation with torch.autograd->Computing Gradients"
            ],
            [
                "scipy->Statistics (scipy.stats)->Quasi-Monte Carlo->Using a QMC engine"
            ],
            [
                "torch->PyTorch Recipes->PyTorch Benchmark->Steps->3. Benchmarking with torch.utils.benchmark.Timer"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Arranging multiple Axes in a Figure->High-level methods for making grids->Variable widths or heights in a grid"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Quasi-binomial regression"
            ]
        ]
    },
    "183655": {
        "jupyter_code_cell": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ntips = sns.load_dataset('tips')\nsns.lmplot(x = 'total_bill', y = 'tip',hue='sex', data = tips, palette='Set1')\nplt.show()",
        "matched_tutorial_code_inds": [
            6270,
            6687,
            6049,
            6174,
            5287
        ],
        "matched_tutorial_codes": [
            "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom pandas.plotting import register_matplotlib_converters\n\nregister_matplotlib_converters()\nsns.set_style(\"darkgrid\")",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pandas_datareader as pdr\nimport seaborn as sns\n\nplt.rc(\"figure\", figsize=(16, 8))\nplt.rc(\"font\", size=15)\nplt.rc(\"lines\", linewidth=3)\nsns.set_style(\"darkgrid\")",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom scipy import stats\n\nsns.set_style(\"darkgrid\")\nsns.mpl.rc(\"figure\", figsize=(8, 8))",
            "import numpy as np\nimport pandas as pd\n\nfrom statsmodels.graphics.tsaplots import plot_predict\nfrom statsmodels.tsa.arima_process import arma_generate_sample\nfrom statsmodels.tsa.arima.model import ARIMA\n\nnp.random.seed(12345)",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom statsmodels.iolib.table import SimpleTable, default_txt_fmt\n\nnp.random.seed(1024)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition"
            ],
            [
                "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Imports"
            ],
            [
                "statsmodels->Examples->Statistics->Copulas"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Artificial Data"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Weighted Least Squares"
            ]
        ]
    },
    "189019": {
        "jupyter_code_cell": "fig, ax = plt.subplots()\ncolors = sb.color_palette(palette='colorblind', n_colors=2)\nax.bar(age_bins[:-1], all_by_age_perc.values, \n       width=2.5, color=colors[0], label='All passengers')\nax.bar(age_bins[:-1]+2.5, survived_by_age_perc.values, \n       width=2.5, color=colors[1], label='Survived')\nax.set_xticks(age_bins[:-1]+2.5)\nax.set_xticklabels(bin_labels)\nax.set_xlabel('Age')\nax.set_ylabel('Percentage of all passengers')\nax.legend()\n_ = ax.set_xlim(-2, 77)\nsurvival_by_age_sex = df.groupby(['Age bins', 'Sex'])['Survived'].mean()",
        "matched_tutorial_code_inds": [
            4645,
            6453,
            1617,
            4958,
            6419
        ],
        "matched_tutorial_codes": [
            "fig, ax = plt.subplots(figsize=(5, 2.7), layout='constrained')\ndates = np.arange(np.datetime64('2021-11-15'), np.datetime64('2021-12-25'),\n                  np.timedelta64(1, 'h'))\ndata = np.cumsum(np.random.randn(len(dates)))\nax.plot(dates, data)\ncdf = mpl.dates.ConciseDateFormatter(ax.xaxis.get_major_locator())\nax.xaxis.set_major_formatter(cdf)\n\n\n<img alt=\"quick start\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_quick_start_014.png\" srcset=\"../../_images/sphx_glr_quick_start_014.png, ../../_images/sphx_glr_quick_start_014_2_0x.png 2.0x\"/>",
            "fig, ax = plt.subplots(figsize=(10,4))\n\n# Plot the results\ndf['lff'].plot(ax=ax, style='k.', label='Observations')\npredict.predicted_mean.plot(ax=ax, label='One-step-ahead Prediction')\npredict_ci = predict.conf_int(alpha=0.05)\npredict_index = np.arange(len(predict_ci))\nax.fill_between(predict_index[2:], predict_ci.iloc[2:, 0], predict_ci.iloc[2:, 1], alpha=0.1)\n\nforecast.predicted_mean.plot(ax=ax, style='r', label='Forecast')\nforecast_ci = forecast.conf_int()\nforecast_index = np.arange(len(predict_ci), len(predict_ci) + len(forecast_ci))\nax.fill_between(forecast_index, forecast_ci.iloc[:, 0], forecast_ci.iloc[:, 1], alpha=0.1)\n\n# Cleanup the image\nax.set_ylim((4, 8));\nlegend = ax.legend(loc='lower left');\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_local_linear_trend_11_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_local_linear_trend_11_0.png\"/>",
            "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(20, 15))\n\naxes[0].set_title(\"Original\")\naxes[0].imshow(xray_image, cmap=\"gray\")\naxes[1].set_title(\"Canny (edges) - prism\")\naxes[1].imshow(xray_image_canny, cmap=\"prism\")\naxes[2].set_title(\"Canny (edges) - nipy_spectral\")\naxes[2].imshow(xray_image_canny, cmap=\"nipy_spectral\")\naxes[3].set_title(\"Canny (edges) - terrain\")\naxes[3].imshow(xray_image_canny, cmap=\"terrain\")\nfor i in axes:\n    i.axis(\"off\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/e7ee670442f8d404bfdcfdb117f2063747c3554aad2d41cd1dbc99b51c61c9ae.png\" src=\"../_images/e7ee670442f8d404bfdcfdb117f2063747c3554aad2d41cd1dbc99b51c61c9ae.png\"/>",
            "fig, axs = plt.subplots(2, 2, figsize=(8, 5), tight_layout=True)\nfor n, ax in enumerate(axs.flat):\n    ax.plot(x1*10., y1)\n\nformatter = matplotlib.ticker.FormatStrFormatter('%1.1f')\nlocator = matplotlib.ticker.MaxNLocator(nbins='auto', steps=[1, 4, 10])\naxs[0, 1].xaxis.set_major_locator(locator)\naxs[0, 1].xaxis.set_major_formatter(formatter)\n\nformatter = matplotlib.ticker.FormatStrFormatter('%1.5f')\nlocator = matplotlib.ticker.AutoLocator()\naxs[1, 0].xaxis.set_major_formatter(formatter)\naxs[1, 0].xaxis.set_major_locator(locator)\n\nformatter = matplotlib.ticker.FormatStrFormatter('%1.5f')\nlocator = matplotlib.ticker.MaxNLocator(nbins=4)\naxs[1, 1].xaxis.set_major_formatter(formatter)\naxs[1, 1].xaxis.set_major_locator(locator)\n\nplt.show()\n\n\n<img alt=\"text intro\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_text_intro_014.png\" srcset=\"../../_images/sphx_glr_text_intro_014.png, ../../_images/sphx_glr_text_intro_014_2_0x.png 2.0x\"/>",
            "fig, ax = plt.subplots(figsize=(13,3))\n\n# Plot the factor\ndates = endog.index._mpl_repr()\nax.plot(dates, res.factors.filtered[0], label='Factor')\nax.legend()\n\n# Retrieve and also plot the NBER recession indicators\nrec = DataReader('USREC', 'fred', start=start, end=end)\nylim = ax.get_ylim()\nax.fill_between(dates[:-3], ylim[0], ylim[1], rec.values[:-4,0], facecolor='k', alpha=0.1);\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_dfm_coincident_19_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_dfm_coincident_19_0.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Introductory->Quick start guide->Axis scales and ticks->Plotting dates and strings"
            ],
            [
                "statsmodels->Examples->State space models->State space modeling: Local Linear Trends"
            ],
            [
                "numpy->NumPy Applications->X-ray image processing->Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters->The Canny filter"
            ],
            [
                "matplotlib->Tutorials->Text->Text in Matplotlib Plots->Ticks and ticklabels->Tick Locators and Formatters"
            ],
            [
                "statsmodels->Examples->State space models->Dynamic Factor Models: Application->Estimates->Estimated factors"
            ]
        ]
    },
    "1227801": {
        "jupyter_code_cell": "import numpy as np\nimport pandas as pd\n%matplotlib inline\ndf = pd.read_csv(\"vgsales.csv\")\ndf.head()\nfrom sklearn.preprocessing import LabelEncoder\nnumber = LabelEncoder()\ndf['Platform'] = number.fit_transform(df['Platform'].astype('str'))\ndf['Genre'] = number.fit_transform(df['Genre'].astype('str'))\ndf['Publisher'] = number.fit_transform(df['Publisher'].astype('str'))",
        "matched_tutorial_code_inds": [
            5256,
            4661,
            1957,
            2103,
            4700
        ],
        "matched_tutorial_codes": [
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pandas_datareader as pdr\nimport seaborn\n\nimport statsmodels.api as sm\nfrom statsmodels.regression.rolling import RollingOLS\n\nseaborn.set_style(\"darkgrid\")\npd.plotting.register_matplotlib_converters()\n%matplotlib inline",
            "import matplotlib.pyplot as plt\nplt.figure(1)                # the first figure\nplt.subplot(211)             # the first subplot in the first figure\nplt.plot([1, 2, 3])\nplt.subplot(212)             # the second subplot in the first figure\nplt.plot([4, 5, 6])\n\n\nplt.figure(2)                # a second figure\nplt.plot([4, 5, 6])          # creates a subplot() by default\n\nplt.figure(1)                # first figure current;\n                             # subplot(212) still current\nplt.subplot(211)             # make subplot(211) in the first figure\n                             # current\nplt.title('Easy as 1, 2, 3') # subplot 211 title",
            "import matplotlib.pyplot as plt\n\n(X[:, 0], X[:, 1])\n()\n\n\n<img alt=\"plot dbscan\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_dbscan_001.png\" srcset=\"../../_images/sphx_glr_plot_dbscan_001.png\"/>",
            "import matplotlib.pyplot as plt\n\n()\n\nmodels = [X, S, S_, H]\nnames = [\n    \"Observations (mixed signal)\",\n    \"True Sources\",\n    \"ICA recovered signals\",\n    \"PCA recovered signals\",\n]\ncolors = [\"red\", \"steelblue\", \"orange\"]\n\nfor ii, (model, name) in enumerate(zip(models, names), 1):\n    (4, 1, ii)\n    (name)\n    for sig, color in zip(model.T, colors):\n        (sig, color=color)\n\n()\n()\n\n\n<img alt=\"Observations (mixed signal), True Sources, ICA recovered signals, PCA recovered signals\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_ica_blind_source_separation_001.png\" srcset=\"../../_images/sphx_glr_plot_ica_blind_source_separation_001.png\"/>",
            "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom cycler import cycler\nmpl.rcParams['lines.linewidth'] = 2\nmpl.rcParams['lines.linestyle'] = '--'\ndata = np.random.randn(50)\nplt.plot(data)\n\n\n<img alt=\"customizing\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_customizing_001.png\" srcset=\"../../_images/sphx_glr_customizing_001.png, ../../_images/sphx_glr_customizing_001_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Rolling Least Squares"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Working with multiple figures and axes"
            ],
            [
                "sklearn->Examples->Clustering->Demo of DBSCAN clustering algorithm->Data generation"
            ],
            [
                "sklearn->Examples->Decomposition->Blind source separation using FastICA->Plot results"
            ],
            [
                "matplotlib->Tutorials->Introductory->Customizing Matplotlib with style sheets and rcParams->Runtime rc settings"
            ]
        ]
    },
    "779342": {
        "jupyter_code_cell": "player_age['Best_WS'] = player_age.max(axis=1)\nplayer_age['Best_Age'] = player_age.idxmax(axis=1)\nplayer_age\nplayer_age.mean()",
        "matched_tutorial_code_inds": [
            3829,
            5470,
            6146,
            3703,
            6704
        ],
        "matched_tutorial_codes": [
            "ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "means75 = exog.mean()\nmeans75[\"LOWINC\"] = exog[\"LOWINC\"].quantile(0.75)\nprint(means75)",
            "table = pd.DataFrame(data, columns=[\"lag\", \"AC\", \"Q\", \"Prob(Q)\"])\nprint(table.set_index(\"lag\"))",
            "df = pd.read_csv(\"data/347136217_T_ONTIME.csv\")\ndelays = df['DEP_DELAY']",
            "ax = dta.mean().plot(grid=False)\nax.set_xlabel(\"Year\", size=17)\nax.set_ylabel(\"Fertility rate\", size=17)\nax.set_xlim(0, 51)"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ]
        ]
    },
    "955497": {
        "jupyter_code_cell": "plt.figure(figsize=(10,10))\nk_means = KMeans(n_clusters=1)\nk_means.fit(X)\ncentroids = k_means.cluster_centers_\nprint(centroids[0][0]/1)\nplt.scatter(user_date, user_stars, c=\"green\")\nplt.scatter(centroids[:,0], centroids[:,1], \n            marker=\"x\", s=300,c=\"red\",linewidth=4)\nplt.show()\nplt.figure(figsize=(30,10))\nplt.hist(business_df['categories'].head(350))\nplt.show()",
        "matched_tutorial_code_inds": [
            5362,
            4346,
            4351,
            4691,
            4795
        ],
        "matched_tutorial_codes": [
            "plt.rcParams[\"figure.subplot.bottom\"] = 0.23  # keep labels visible\nplt.rcParams[\"figure.figsize\"] = (10.0, 8.0)  # make plot larger in notebook\nage = [data.exog[\"age\"][data.endog == id] for id in party_ID]\nfig = plt.figure()\nax = fig.add_subplot(111)\nplot_opts = {\n    \"cutoff_val\": 5,\n    \"cutoff_type\": \"abs\",\n    \"label_fontsize\": \"small\",\n    \"label_rotation\": 30,\n}\nsm.graphics.beanplot(age, ax=ax, labels=labels, plot_opts=plot_opts)\nax.set_xlabel(\"Party identification of respondent.\")\nax.set_ylabel(\"Age\")\n# plt.show()",
            "plt.figure()\n plt.imshow(deriv)\n plt.gray()\n plt.title('Output of spline edge filter')\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code displays two plots. The first plot is a normal grayscale photo of a raccoon climbing on a palm plant. The second plot has the 2-D spline filter applied to the photo and is completely grey except the edges of the photo have been emphasized, especially on the raccoon fur and palm fronds.\"' class=\"plot-directive\" src=\"../_images/signal-1_01_00.png\"/>\n</figure>",
            "plt.figure()\n plt.imshow(image_new)\n plt.gray()\n plt.title('Filtered image')\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code displays two plots. The first plot is the familiar photo of a raccoon climbing on a palm. The second plot has the FIR filter applied and has the two copies of the photo superimposed due to the twin peaks manually set in the filter kernel definition.\"' class=\"plot-directive\" src=\"../_images/signal-2_01_00.png\"/>\n</figure>",
            "plt.rcParams.update({'figure.autolayout': True})\n\nfig, ax = plt.subplots()\nax.barh(group_names, group_data)\nlabels = ax.get_xticklabels()\nplt.setp(labels, rotation=45, horizontalalignment='right')\n\n\n<img alt=\"lifecycle\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_lifecycle_006.png\" srcset=\"../../_images/sphx_glr_lifecycle_006.png, ../../_images/sphx_glr_lifecycle_006_2_0x.png 2.0x\"/>",
            "plt.rcParams['figure.constrained_layout.use'] = True\nfig, axs = plt.subplots(2, 2, figsize=(3, 3))\nfor ax in axs.flat:\n    example_plot(ax)\n\n\n<img alt=\"Title, Title, Title, Title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_constrainedlayout_guide_018.png\" srcset=\"../../_images/sphx_glr_constrainedlayout_guide_018.png, ../../_images/sphx_glr_constrainedlayout_guide_018_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Plotting->Box Plots->Bean Plots"
            ],
            [
                "scipy->Signal Processing (scipy.signal)->B-splines"
            ],
            [
                "scipy->Signal Processing (scipy.signal)->Filtering->Convolution/Correlation"
            ],
            [
                "matplotlib->Tutorials->Introductory->The Lifecycle of a Plot->Customizing the plot"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Constrained Layout Guide->rcParams"
            ]
        ]
    },
    "1406337": {
        "jupyter_code_cell": "y = data_copy['Enter'].values\ndata_copy = data_copy.drop(['Enter'],axis=1)\nX = data_copy.values\ndata_copy.head()\nXtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.50)\nprint(Xtrain.shape)\nprint(Xtest.shape)",
        "matched_tutorial_code_inds": [
            2064,
            5355,
            5180,
            2298,
            2806
        ],
        "matched_tutorial_codes": [
            "y = X.dot(pca.components_[1]) + rng.normal(size=n_samples) / 2\n\nfig, axes = (1, 2, figsize=(10, 3))\n\naxes[0].scatter(X.dot(pca.components_[0]), y, alpha=0.3)\naxes[0].set(xlabel=\"Projected data onto first PCA component\", ylabel=\"y\")\naxes[1].scatter(X.dot(pca.components_[1]), y, alpha=0.3)\naxes[1].set(xlabel=\"Projected data onto second PCA component\", ylabel=\"y\")\n()\n()\n\n\n<img alt=\"plot pcr vs pls\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_pcr_vs_pls_002.png\" srcset=\"../../_images/sphx_glr_plot_pcr_vs_pls_002.png\"/>",
            "weights = rob_crime_model.weights\nidx = weights  0\nX = rob_crime_model.model.exog[idx.values]\nww = weights[idx] / weights[idx].mean()\nhat_matrix_diag = ww * (X * np.linalg.pinv(X).T).sum(1)\nresid = rob_crime_model.resid\nresid2 = resid ** 2\nresid2 /= resid2.sum()\nnobs = int(idx.sum())\nhm = hat_matrix_diag.mean()\nrm = resid2.mean()",
            "pred_ols2 = res2.get_prediction()\niv_l = pred_ols.summary_frame()[\"obs_ci_lower\"]\niv_u = pred_ols.summary_frame()[\"obs_ci_upper\"]\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nax.plot(x, y, \"o\", label=\"Data\")\nax.plot(x, y_true, \"b-\", label=\"True\")\nax.plot(x, res2.fittedvalues, \"r--.\", label=\"Predicted\")\nax.plot(x, iv_u, \"r--\")\nax.plot(x, iv_l, \"r--\")\nlegend = ax.legend(loc=\"best\")\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_ols_26_0.png\" src=\"../../../_images/examples_notebooks_generated_ols_26_0.png\"/>",
            "feature_importance = reg.feature_importances_\nsorted_idx = (feature_importance)\npos = (sorted_idx.shape[0]) + 0.5\nfig = (figsize=(12, 6))\n(1, 2, 1)\n(pos, feature_importance[sorted_idx], align=\"center\")\n(pos, (diabetes.feature_names)[sorted_idx])\n(\"Feature Importance (MDI)\")\n\nresult = (\n    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_idx = result.importances_mean.argsort()\n(1, 2, 2)\n(\n    result.importances[sorted_idx].T,\n    vert=False,\n    labels=(diabetes.feature_names)[sorted_idx],\n)\n(\"Permutation Importance (test set)\")\nfig.tight_layout()\n()\n\n\n<img alt=\"Feature Importance (MDI), Permutation Importance (test set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\"/>",
            "train_dataset = X_train.copy()\ntrain_dataset.insert(0, \"WAGE\", y_train)\n_ = (train_dataset, kind=\"reg\", diag_kind=\"kde\")\n\n\n<img alt=\"plot linear model coefficient interpretation\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_001.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_001.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Cross decomposition->Principal Component Regression vs Partial Least Squares Regression->The data"
            ],
            [
                "statsmodels->Examples->Plotting->Regression Plots->Statewide Crime 2009 Dataset->Using robust regression to correct for outliers."
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->OLS with dummy variables"
            ],
            [
                "sklearn->Examples->Ensemble methods->Gradient Boosting regression->Plot feature importance"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ]
        ]
    },
    "492969": {
        "jupyter_code_cell": "dataset = sm.datasets.co2.load_pandas()\nobs = dataset.data\nobs.head()\nobs = (obs\n       .resample('D')\n       .mean()\n       .interpolate('linear'))\nobs.head(10)",
        "matched_tutorial_code_inds": [
            5588,
            5399,
            5205,
            6860,
            6169
        ],
        "matched_tutorial_codes": [
            "data2 = sm.datasets.scotland.load()\ndata2.exog = sm.add_constant(data2.exog, prepend=False)\nprint(data2.exog.head())\nprint(data2.endog.head())",
            "rand_data = sm.datasets.randhie.load()\nrand_exog = rand_data.exog\nrand_exog = sm.add_constant(rand_exog, prepend=False)",
            "data = sm.datasets.longley.load()\ndata.exog = sm.add_constant(data.exog)\nprint(data.exog.head())",
            "data = sm.datasets.spector.load_pandas()\nexog = data.exog\nendog = data.endog\nprint(sm.datasets.spector.NOTE)\nprint(data.exog.head())",
            "macrodta = sm.datasets.macrodata.load_pandas().data\nmacrodta.index = pd.Index(sm.tsa.datetools.dates_from_range(\"1959Q1\", \"2009Q3\"))\ncpi = macrodta[\"cpi\"]"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Generalized Linear Models->Generalized Linear Models Overview->GLM: Gamma for proportional count response->Load Scottish Parliament Voting data"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Poisson"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Generalized Least Squares"
            ],
            [
                "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 1: Probit model"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data->Exercise: How good of in-sample prediction can you do for another series, say, CPI"
            ]
        ]
    },
    "111722": {
        "jupyter_code_cell": "from sqlalchemy import create_engine\nimport pandas as pd\nengine = create_engine('mysql+pymysql://python:python@localhost:3306/smarth2o')\ndf = pd.read_sql_query(\"\"\"\n SELECT 'xml' as source ,min(a.reading_date_time) as min_date_reading,max(a.reading_date_time) as max_date_reading ,count(*) as count FROM smarth2o.meter_reading_reimported a\n union all\n SELECT 'production' as source ,min(a.reading_date_time) as min_date_reading,max(a.reading_date_time) as max_date_reading,count(*) as count FROM smarth2o.meter_reading a \"\"\",engine)\ndf",
        "matched_tutorial_code_inds": [
            6410,
            5010,
            4986,
            4976,
            5476
        ],
        "matched_tutorial_codes": [
            "from pandas_datareader.data import DataReader\n\n# Get the datasets from FRED\nstart = '1979-01-01'\nend = '2014-12-01'\nindprod = DataReader('IPMAN', 'fred', start=start, end=end)\nincome = DataReader('W875RX1', 'fred', start=start, end=end)\nsales = DataReader('CMRMTSPL', 'fred', start=start, end=end)\nemp = DataReader('PAYEMS', 'fred', start=start, end=end)\n# dta = pd.concat((indprod, income, sales, emp), axis=1)\n# dta.columns = ['indprod', 'income', 'sales', 'emp']",
            "from matplotlib.backends.backend_pgf import PdfPages\nimport matplotlib.pyplot as plt\n\nwith PdfPages('multipage.pdf', metadata={'author': 'Me'}) as pdf:\n\n    fig1, ax1 = plt.subplots()\n    ax1.plot([1, 5, 3])\n    pdf.savefig(fig1)\n\n    fig2, ax2 = plt.subplots()\n    ax2.plot([1, 5, 3])\n    pdf.savefig(fig2)",
            "from matplotlib.text import OffsetFrom\n\nfig, ax = plt.subplots(figsize=(3, 3))\nan1 = ax.annotate(\"Test 1\", xy=(0.5, 0.5), xycoords=\"data\",\n                  va=\"center\", ha=\"center\",\n                  bbox=dict(boxstyle=\"round\", fc=\"w\"))\n\noffset_from = OffsetFrom(an1, (0.5, 0))\nan2 = ax.annotate(\"Test 2\", xy=(0.1, 0.1), xycoords=\"data\",\n                  xytext=(0, -10), textcoords=offset_from,\n                  # xytext is offset points from \"xy=(0.5, 0), xycoords=an1\"\n                  va=\"top\", ha=\"center\",\n                  bbox=dict(boxstyle=\"round\", fc=\"w\"),\n                  arrowprops=dict(arrowstyle=\"-\"))\n\n\n<img alt=\"annotations\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_annotations_021.png\" srcset=\"../../_images/sphx_glr_annotations_021.png, ../../_images/sphx_glr_annotations_021_2_0x.png 2.0x\"/>",
            "from matplotlib.offsetbox import AnchoredText\n\nfig, ax = plt.subplots(figsize=(3, 3))\nat = AnchoredText(\"Figure 1a\",\n                  prop=dict(size=15), frameon=True, loc='upper left')\nat.patch.set_boxstyle(\"round,pad=0.,rounding_size=0.2\")\nax.add_artist(at)\n\n\n<img alt=\"annotations\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_annotations_011.png\" srcset=\"../../_images/sphx_glr_annotations_011.png, ../../_images/sphx_glr_annotations_011_2_0x.png 2.0x\"/>",
            "from statsmodels.graphics.api import abline_plot\n\nfig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111, ylabel=\"Observed Values\", xlabel=\"Fitted Values\")\nax.scatter(yhat, y)\ny_vs_yhat = sm.OLS(y, sm.add_constant(yhat, prepend=True)).fit()\nfig = abline_plot(model_results=y_vs_yhat, ax=ax)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_discrete_choice_example_55_0.png\" src=\"../../../_images/examples_notebooks_generated_discrete_choice_example_55_0.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->Dynamic Factor Models: Application->Macroeconomic data"
            ],
            [
                "matplotlib->Tutorials->Text->Text rendering with XeLaTeX/LuaLaTeX via the ``pgf`` backend->Multi-Page PDF Files"
            ],
            [
                "matplotlib->Tutorials->Text->Annotations->Coordinate systems for annotations"
            ],
            [
                "matplotlib->Tutorials->Text->Annotations->Advanced annotation->Placing Artist at anchored Axes locations"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution"
            ]
        ]
    },
    "1223118": {
        "jupyter_code_cell": "sentences.shape\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',    level=logging.INFO)\nfrom gensim.models import Word2Vec",
        "matched_tutorial_code_inds": [
            2090,
            3423,
            3583,
            4139,
            2165
        ],
        "matched_tutorial_codes": [
            "iris = ()\nX = iris.data\ny = iris.target\nX_train, X_test, y_train, y_test = (X, y, random_state=0)\n\nclf = (max_leaf_nodes=3, random_state=0)\nclf.fit(X_train, y_train)",
            "from sklearn import datasets\nimport numpy as np\n\ndigits = ()\nrng = (2)\nindices = (len(digits.data))\nrng.shuffle(indices)",
            "from sklearn.feature_extraction.text import TfidfTransformer\n tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n X_train_tf = tf_transformer.transform(X_train_counts)\n X_train_tf.shape\n(2257, 35788)",
            "g = sns.PairGrid(iris)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n",
            "from sklearn.compose import \n\ncat_selector = (dtype_include=object)\nnum_selector = (dtype_include=)\ncat_selector(X)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Decision Trees->Understanding the decision tree structure->Train tree classifier"
            ],
            [
                "sklearn->Examples->Semi Supervised Classification->Label Propagation digits: Demonstrating performance->Data generation"
            ],
            [
                "sklearn->Tutorials->Working With Text Data->Extracting features from text files->From occurrences to frequencies"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships"
            ],
            [
                "sklearn->Examples->Ensemble methods->Combine predictors using stacking->Make pipeline to preprocess the data"
            ]
        ]
    },
    "934727": {
        "jupyter_code_cell": "y = df['brain-weight'].values\nX = df['head-size'].values\nX = X[:, np.newaxis]\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.33, \n                                                    random_state=123)",
        "matched_tutorial_code_inds": [
            2064,
            2298,
            5355,
            5180,
            3190
        ],
        "matched_tutorial_codes": [
            "y = X.dot(pca.components_[1]) + rng.normal(size=n_samples) / 2\n\nfig, axes = (1, 2, figsize=(10, 3))\n\naxes[0].scatter(X.dot(pca.components_[0]), y, alpha=0.3)\naxes[0].set(xlabel=\"Projected data onto first PCA component\", ylabel=\"y\")\naxes[1].scatter(X.dot(pca.components_[1]), y, alpha=0.3)\naxes[1].set(xlabel=\"Projected data onto second PCA component\", ylabel=\"y\")\n()\n()\n\n\n<img alt=\"plot pcr vs pls\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_pcr_vs_pls_002.png\" srcset=\"../../_images/sphx_glr_plot_pcr_vs_pls_002.png\"/>",
            "feature_importance = reg.feature_importances_\nsorted_idx = (feature_importance)\npos = (sorted_idx.shape[0]) + 0.5\nfig = (figsize=(12, 6))\n(1, 2, 1)\n(pos, feature_importance[sorted_idx], align=\"center\")\n(pos, (diabetes.feature_names)[sorted_idx])\n(\"Feature Importance (MDI)\")\n\nresult = (\n    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_idx = result.importances_mean.argsort()\n(1, 2, 2)\n(\n    result.importances[sorted_idx].T,\n    vert=False,\n    labels=(diabetes.feature_names)[sorted_idx],\n)\n(\"Permutation Importance (test set)\")\nfig.tight_layout()\n()\n\n\n<img alt=\"Feature Importance (MDI), Permutation Importance (test set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\"/>",
            "weights = rob_crime_model.weights\nidx = weights  0\nX = rob_crime_model.model.exog[idx.values]\nww = weights[idx] / weights[idx].mean()\nhat_matrix_diag = ww * (X * np.linalg.pinv(X).T).sum(1)\nresid = rob_crime_model.resid\nresid2 = resid ** 2\nresid2 /= resid2.sum()\nnobs = int(idx.sum())\nhm = hat_matrix_diag.mean()\nrm = resid2.mean()",
            "pred_ols2 = res2.get_prediction()\niv_l = pred_ols.summary_frame()[\"obs_ci_lower\"]\niv_u = pred_ols.summary_frame()[\"obs_ci_upper\"]\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nax.plot(x, y, \"o\", label=\"Data\")\nax.plot(x, y_true, \"b-\", label=\"True\")\nax.plot(x, res2.fittedvalues, \"r--.\", label=\"Predicted\")\nax.plot(x, iv_u, \"r--\")\nax.plot(x, iv_l, \"r--\")\nlegend = ax.legend(loc=\"best\")\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_ols_26_0.png\" src=\"../../../_images/examples_notebooks_generated_ols_26_0.png\"/>",
            "y_score = classifier.decision_function(X_test)\n\ndisplay = (y_test, y_score, name=\"LinearSVC\")\n_ = display.ax_.set_title(\"2-class Precision-Recall curve\")\n\n\n<img alt=\"2-class Precision-Recall curve\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_precision_recall_002.png\" srcset=\"../../_images/sphx_glr_plot_precision_recall_002.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Cross decomposition->Principal Component Regression vs Partial Least Squares Regression->The data"
            ],
            [
                "sklearn->Examples->Ensemble methods->Gradient Boosting regression->Plot feature importance"
            ],
            [
                "statsmodels->Examples->Plotting->Regression Plots->Statewide Crime 2009 Dataset->Using robust regression to correct for outliers."
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->OLS with dummy variables"
            ],
            [
                "sklearn->Examples->Model Selection->Precision-Recall->In binary classification settings->Plot the Precision-Recall curve"
            ]
        ]
    },
    "858120": {
        "jupyter_code_cell": "print(np.mean(y == np.where(sigmoid(X@theta_optimized)>=0.5,1,0))*100,\"%\")\nx1 = np.linspace(start = chip_data[\"Microchip Test 1\"].min(), \n                 stop = chip_data[\"Microchip Test 1\"].max(), num = 100)\nx2 = np.linspace(start = chip_data[\"Microchip Test 2\"].min(), \n                 stop = chip_data[\"Microchip Test 2\"].max(), num = 100)",
        "matched_tutorial_code_inds": [
            134,
            954,
            728,
            530,
            5229
        ],
        "matched_tutorial_codes": [
            "print(prof.key_averages(group_by_input_shape=True).table(sort_by=\"cpu_time_total\", row_limit=10))\n\n# (omitting some columns)\n# ---------------------------------  ------------  -------------------------------------------\n#                              Name     CPU total                                 Input Shapes\n# ---------------------------------  ------------  -------------------------------------------\n#                   model_inference      57.503ms                                           []\n#                      aten::conv2d       8.008ms      [5,64,56,56], [64,64,3,3], [], ..., []]\n#                 aten::convolution       7.956ms     [[5,64,56,56], [64,64,3,3], [], ..., []]\n#                aten::_convolution       7.909ms     [[5,64,56,56], [64,64,3,3], [], ..., []]\n#          aten::mkldnn_convolution       7.834ms     [[5,64,56,56], [64,64,3,3], [], ..., []]\n#                      aten::conv2d       6.332ms    [[5,512,7,7], [512,512,3,3], [], ..., []]\n#                 aten::convolution       6.303ms    [[5,512,7,7], [512,512,3,3], [], ..., []]\n#                aten::_convolution       6.273ms    [[5,512,7,7], [512,512,3,3], [], ..., []]\n#          aten::mkldnn_convolution       6.233ms    [[5,512,7,7], [512,512,3,3], [], ..., []]\n#                      aten::conv2d       4.751ms  [[5,256,14,14], [256,256,3,3], [], ..., []]\n# ---------------------------------  ------------  -------------------------------------------\n# Self CPU time total: 57.549ms",
            "print(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total', row_limit=5))\n\n\"\"\"\n(Some columns are omitted)\n\n-------------  ------------  ------------  ------------  ---------------------------------\n         Name    Self CPU %      Self CPU  Self CPU Mem   Source Location\n-------------  ------------  ------------  ------------  ---------------------------------\n MASK INDICES        87.88%        5.212s    -953.67 Mb  /mnt/xarfuse/.../torch/au\n                                                         &lt;ipython-input-...&gt;(10): forward\n                                                         /mnt/xarfuse/.../torch/nn\n                                                         &lt;ipython-input-...&gt;(9): &lt;module&gt;\n                                                         /mnt/xarfuse/.../IPython/\n\n  aten::copy_        12.07%     715.848ms           0 b  &lt;ipython-input-...&gt;(12): forward\n                                                         /mnt/xarfuse/.../torch/nn\n                                                         &lt;ipython-input-...&gt;(9): &lt;module&gt;\n                                                         /mnt/xarfuse/.../IPython/\n                                                         /mnt/xarfuse/.../IPython/\n\n  LINEAR PASS         0.01%     350.151us         -20 b  /mnt/xarfuse/.../torch/au\n                                                         &lt;ipython-input-...&gt;(7): forward\n                                                         /mnt/xarfuse/.../torch/nn\n                                                         &lt;ipython-input-...&gt;(9): &lt;module&gt;\n                                                         /mnt/xarfuse/.../IPython/\n\n  aten::addmm         0.00%     293.342us           0 b  /mnt/xarfuse/.../torch/nn\n                                                         /mnt/xarfuse/.../torch/nn\n                                                         /mnt/xarfuse/.../torch/nn\n                                                         &lt;ipython-input-...&gt;(8): forward\n                                                         /mnt/xarfuse/.../torch/nn\n\n   aten::mean         0.00%     235.095us           0 b  &lt;ipython-input-...&gt;(11): forward\n                                                         /mnt/xarfuse/.../torch/nn\n                                                         &lt;ipython-input-...&gt;(9): &lt;module&gt;\n                                                         /mnt/xarfuse/.../IPython/\n                                                         /mnt/xarfuse/.../IPython/\n\n-----------------------------  ------------  ---------- ----------------------------------\nSelf CPU time total: 5.931s\n\n\"\"\"",
            "print([p.size(0) for p in params.values()]) # show the leading 'num_models' dimension\n\nassert .shape == (num_models, 64, 1, 28, 28) # verify minibatch has leading dimension of size 'num_models'\n\nfrom torch import \n\n = (fmodel)(params, buffers, )\n\n# verify the vmap predictions match the\nassert (, (predictions_diff_minibatch_loop), atol=1e-3, rtol=1e-5)",
            "print(\"normalization constant shape:\", .transform[0].loc.shape)",
            "print(sm.datasets.copper.DESCRLONG)\n\ndta = sm.datasets.copper.load_pandas().data\ndta.index = pd.date_range(\"1951-01-01\", \"1975-01-01\", freq=\"AS\")\nendog = dta[\"WORLDCONSUMPTION\"]\n\n# To the regressors in the dataset, we add a column of ones for an intercept\nexog = sm.add_constant(\n    dta[[\"COPPERPRICE\", \"INCOMEINDEX\", \"ALUMPRICE\", \"INVENTORYINDEX\"]]\n)"
        ],
        "matched_tutorial_paths": [
            [
                "torch->PyTorch Recipes->PyTorch Profiler->Steps->3. Using profiler to analyze execution time"
            ],
            [
                "torch->Model Optimization->Profiling your PyTorch Module->Print profiler results"
            ],
            [
                "torch->Frontend APIs->Model ensembling->Using vmap to vectorize the ensemble"
            ],
            [
                "torch->Reinforcement Learning->Reinforcement Learning (PPO) with TorchRL Tutorial->Define an environment->Normalization"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 1: Copper"
            ]
        ]
    },
    "1218605": {
        "jupyter_code_cell": "time_grouping = '5min'\ndataframe = pd.read_csv('ignored_assets/paxout_table.csv', engine='python', nrows=5000)\ndataframe['time_bucket'] = pd.to_datetime(dataframe['time_bucket'])\ndataframe = dataframe.set_index('time_bucket')\ndataframe['y'] = dataframe.sum(axis=1)\ndataframe['ds'] = dataframe.index.round(time_grouping)\ndataframe = dataframe[['y','ds']].groupby('ds').sum()\ndataset_orig = dataframe.values\ndataset_orig = dataset_orig.astype('float32')\ndataframe = dataframe.reset_index()\ndataframe['y'] = np.log(dataframe['y'])\ndataframe.head()",
        "matched_tutorial_code_inds": [
            1475,
            6492,
            2753,
            23,
            2951
        ],
        "matched_tutorial_codes": [
            "totals_row = 35\nlocations = np.delete(locations, (totals_row), axis=0)\nnbcases = np.delete(nbcases, (totals_row), axis=0)\n\nchina_total = nbcases[locations[:, 1] == \"China\"].sum(axis=0)\nchina_total",
            "time_s = np.s_[:50]  # After this they basically agree\nfig1 = plt.figure()\nax1 = fig1.add_subplot(111)\nidx = np.asarray(series.index)\nh1, = ax1.plot(idx[time_s], res_f.freq_seasonal[0].filtered[time_s], label='Double Freq. Seas')\nh2, = ax1.plot(idx[time_s], res_tf.seasonal.filtered[time_s], label='Mixed Domain Seas')\nh3, = ax1.plot(idx[time_s], true_seasonal_10_3[time_s], label='True Seasonal 10(3)')\nplt.legend([h1, h2, h3], ['Double Freq. Seasonal','Mixed Domain Seasonal','Truth'], loc=2)\nplt.title('Seasonal 10(3) component')\nplt.show()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_seasonal_21_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_seasonal_21_0.png\"/>",
            "quantiles = [0.05, 0.5, 0.95]\npredictions = {}\nout_bounds_predictions = (y_true_mean, dtype=)\nfor quantile in quantiles:\n    qr = (quantile=quantile, alpha=0, solver=solver)\n    y_pred = qr.fit(X, y_pareto).predict(X)\n    predictions[quantile] = y_pred\n\n    if quantile == min(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred = y_pareto\n        )\n    elif quantile == max(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred &lt;= y_pareto\n        )",
            "face_dataset = FaceLandmarksDataset(csv_file='faces/face_landmarks.csv',\n                                    root_dir='faces/')\n\nfig = plt.figure()\n\nfor i in range(len(face_dataset)):\n    sample = face_dataset[i]\n\n    print(i, sample['image'].shape, sample['landmarks'].shape)\n\n    ax = plt.subplot(1, 4, i + 1)\n    plt.tight_layout()\n    ax.set_title('Sample #{}'.format(i))\n    ax.axis('off')\n    show_landmarks(**sample)\n\n    if i == 3:\n        plt.show()\n        break",
            "cluster_ids = (dist_linkage, 1, criterion=\"distance\")\ncluster_id_to_feature_ids = (list)\nfor idx, cluster_id in enumerate(cluster_ids):\n    cluster_id_to_feature_ids[cluster_id].append(idx)\nselected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n\nX_train_sel = X_train[:, selected_features]\nX_test_sel = X_test[:, selected_features]\n\nclf_sel = (n_estimators=100, random_state=42)\nclf_sel.fit(X_train_sel, y_train)\nprint(\n    \"Accuracy on test data with features removed: {:.2f}\".format(\n        clf_sel.score(X_test_sel, y_test)\n    )\n)"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Features->Masked Arrays->Exploring the data"
            ],
            [
                "statsmodels->Examples->State space models->Seasonality in Time Series Data->Comparison of filtered estimates"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Quantile regression->Fitting a QuantileRegressor"
            ],
            [
                "torch->PyTorch Recipes->Developing Custom PyTorch Dataloaders->Part 1: The Dataset->1.3 Iterate through data samples"
            ],
            [
                "sklearn->Examples->Inspection->Permutation Importance with Multicollinear or Correlated Features->Handling Multicollinear Features"
            ]
        ]
    },
    "1469607": {
        "jupyter_code_cell": "print(train.shape)\nprint(test.shape)\nimport matplotlib.pyplot as plt\nplt.hist(train['is_iceberg'])",
        "matched_tutorial_code_inds": [
            5233,
            5854,
            5345,
            4138,
            2090
        ],
        "matched_tutorial_codes": [
            "print(res.recursive_coefficients.filtered[0])\nres.plot_recursive_coefficient(range(mod.k_exog), alpha=None, figsize=(10, 6))",
            "resrlm = sm.RLM(y2, X).fit()\nprint(resrlm.params)\nprint(resrlm.bse)",
            "fig = sm.graphics.plot_partregress_grid(crime_model)\nfig.tight_layout(pad=1.0)",
            "iris = sns.load_dataset(\"iris\")\ng = sns.PairGrid(iris)\ng.map(sns.scatterplot)\n",
            "iris = ()\nX = iris.data\ny = iris.target\nX_train, X_test, y_train, y_test = (X, y, random_state=0)\n\nclf = (max_leaf_nodes=3, random_state=0)\nclf.fit(X_train, y_train)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 1: Copper"
            ],
            [
                "statsmodels->Examples->Robust Regression->Comparing OLS and RLM->Comparing OLS and RLM->Example 1: quadratic function with linear truth"
            ],
            [
                "statsmodels->Examples->Plotting->Regression Plots->Statewide Crime 2009 Dataset->Partial Regression Plots (Crime Data)"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships"
            ],
            [
                "sklearn->Examples->Decision Trees->Understanding the decision tree structure->Train tree classifier"
            ]
        ]
    },
    "1415295": {
        "jupyter_code_cell": "alphas = [1e-4, 6e-4, 1e-3, 5e-3]\ncv_lasso = [rmse_cv(Lasso(alpha = alpha, max_iter=150000), X_train, y) for alpha in alphas]\npd.Series(cv_lasso, index = alphas).plot()\nmodel_lasso = Lasso(alpha=5e-4, max_iter=1150000).fit(X_train, y)",
        "matched_tutorial_code_inds": [
            3257,
            6795,
            3267,
            5263,
            3079
        ],
        "matched_tutorial_codes": [
            "alphas = (-5, 1, 60)\nenet = (l1_ratio=0.7, max_iter=10000)\ntrain_errors = list()\ntest_errors = list()\nfor alpha in alphas:\n    enet.set_params(alpha=alpha)\n    enet.fit(X_train, y_train)\n    train_errors.append(enet.score(X_train, y_train))\n    test_errors.append(enet.score(X_test, y_test))\n\ni_alpha_optim = (test_errors)\nalpha_optim = alphas[i_alpha_optim]\nprint(\"Optimal regularization parameter : %s\" % alpha_optim)\n\n# Estimate the coef_ on full data with optimal regularization parameter\nenet.set_params(alpha=alpha_optim)\ncoef_ = enet.fit(X, y).coef_",
            "x1n = np.linspace(20.5, 25, 10)\nXnew = np.column_stack((x1n, np.sin(x1n), (x1n - 5) ** 2))\nXnew = sm.add_constant(Xnew)\nynewpred = olsres.predict(Xnew)  # predict out of sample\nprint(ynewpred)",
            "cvs = [\n    ,\n    ,\n    ,\n    ,\n    ,\n    ,\n    ,\n    ,\n]\n\n\nfor cv in cvs:\n    this_cv = cv(n_splits=n_splits)\n    fig, ax = (figsize=(6, 3))\n    plot_cv_indices(this_cv, X, y, groups, ax, n_splits)\n\n    ax.legend(\n        [(color=cmap_cv(0.8)), (color=cmap_cv(0.02))],\n        [\"Testing set\", \"Training set\"],\n        loc=(1.02, 0.8),\n    )\n    # Make the legend fit\n    ()\n    fig.subplots_adjust(right=0.7)\n()\n\n\n\n<img alt=\"KFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_006.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_006.png\"/>\n<img alt=\"GroupKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_007.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_007.png\"/>\n<img alt=\"ShuffleSplit\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_008.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_008.png\"/>\n<img alt=\"StratifiedKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_009.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_009.png\"/>\n<img alt=\"StratifiedGroupKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_010.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_010.png\"/>\n<img alt=\"GroupShuffleSplit\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_011.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_011.png\"/>\n<img alt=\"StratifiedShuffleSplit\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_012.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_012.png\"/>\n<img alt=\"TimeSeriesSplit\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_013.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_013.png\"/>",
            "exog_vars = [\"Mkt-RF\", \"SMB\", \"HML\"]\nexog = sm.add_constant(factors[exog_vars])\nrols = RollingOLS(endog, exog, window=60)\nrres = rols.fit()\nfig = rres.plot_recursive_coefficient(variables=exog_vars, figsize=(14, 18))\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_rolling_ls_12_0.png\" src=\"../../../_images/examples_notebooks_generated_rolling_ls_12_0.png\"/>",
            "rfc = (n_estimators=10, random_state=42)\nrfc.fit(X_train, y_train)\nax = ()\nrfc_disp = (rfc, X_test, y_test, ax=ax, alpha=0.8)\nsvc_disp.plot(ax=ax, alpha=0.8)\n()\n\n\n<img alt=\"plot roc curve visualization api\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_roc_curve_visualization_api_002.png\" srcset=\"../../_images/sphx_glr_plot_roc_curve_visualization_api_002.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Model Selection->Train error vs Test error->Compute train and test errors"
            ],
            [
                "statsmodels->Examples->User Notes->Prediction->Create a new sample of explanatory variables Xnew, predict and plot"
            ],
            [
                "sklearn->Examples->Model Selection->Visualizing cross-validation behavior in scikit-learn->Visualize cross-validation indices for many CV objects"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Rolling Least Squares"
            ],
            [
                "sklearn->Examples->Miscellaneous->ROC Curve with Visualization API->Training a Random Forest and Plotting the ROC Curve"
            ]
        ]
    },
    "1134174": {
        "jupyter_code_cell": "clothingFile.shape\nprint(clothingFile['RESP'] != 0).sum()",
        "matched_tutorial_code_inds": [
            161,
            127,
            4251,
            1458,
            4316
        ],
        "matched_tutorial_codes": [
            "compare.trim_significant_figures()\ncompare.colorize()\ncompare.print()",
            "print(delta.transform(extract_fn_name).filter(lambda fn: \"TensorIterator\" in fn))\n\n\n\n<strong>Instruction count delta (filter)</strong>",
            "print(res.x)\n[1.         1.         1.         1.         0.99999999]",
            "load_xy = np.load(\"x_y-squared.npz\")\n\nprint(load_xy.files)",
            "res.success\nTrue\n res.x\narray([1., 1., 0., 1., 1., 1., 0., 0.])"
        ],
        "matched_tutorial_paths": [
            [
                "torch->PyTorch Recipes->PyTorch Benchmark->Steps->5. Comparing benchmark results"
            ],
            [
                "torch->PyTorch Recipes->Timer quick start->6. A/B testing with Callgrind"
            ],
            [
                "scipy->Optimization (scipy.optimize)->Unconstrained minimization of multivariate scalar functions (minimize)->Nelder-Mead Simplex algorithm (method='Nelder-Mead')"
            ],
            [
                "numpy->NumPy Features->Saving and sharing your NumPy arrays->Remove the saved arrays and load them back with NumPy\u2019s"
            ],
            [
                "scipy->Optimization (scipy.optimize)->Mixed integer linear programming->Knapsack problem example"
            ]
        ]
    },
    "1116941": {
        "jupyter_code_cell": "Image('orig_purity.png')\ncolor_spec = {'var' : 'g', 'vartas' : '#ee55aa', 'varrotated' : '#55aaee', 'varclosest' : '#aa55ee'}\ncontext_data = all_data[all_data.k_type.isin(color_spec.keys())]\nwith sns.axes_style('darkgrid'):\n    g = sns.FacetGrid(context_data,col=\"pooltype\", size=5,hue=\"k_type\",palette=color_spec, sharey=True)\n    g.map(sns.pointplot, \"num_clusters\", \"purities\")\n    g.add_legend()\n    plt.savefig('purities.png')",
        "matched_tutorial_code_inds": [
            4589,
            2743,
            2527,
            275,
            3120
        ],
        "matched_tutorial_codes": [
            "image = np.arange(4 * 6).reshape(4, 6)\n mask = np.array([[0,1,1,0,0,0],[0,1,1,0,1,0],[0,0,0,1,1,1],[0,0,0,0,1,0]])\n labels = label(mask)[0]\n slices = find_objects(labels)",
            "fig, axes = (ncols=2, figsize=(16, 5))\npft = (degree=3).fit(X_train)\naxes[0].plot(x_plot, pft.transform(X_plot))\naxes[0].legend(axes[0].lines, [f\"degree {n}\" for n in range(4)])\naxes[0].set_title(\"PolynomialFeatures\")\n\nsplt = (n_knots=4, degree=3).fit(X_train)\naxes[1].plot(x_plot, splt.transform(X_plot))\naxes[1].legend(axes[1].lines, [f\"spline {n}\" for n in range(6)])\naxes[1].set_title(\"SplineTransformer\")\n\n# plot knots of spline\nknots = splt.bsplines_[0].t\naxes[1].vlines(knots[3:-3], ymin=0, ymax=0.8, linestyles=\"dashed\")\n()\n\n\n<img alt=\"PolynomialFeatures, SplineTransformer\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_polynomial_interpolation_002.png\" srcset=\"../../_images/sphx_glr_plot_polynomial_interpolation_002.png\"/>",
            "GaussianMixture()\n\n<br/>\n<br/>",
            "VGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace=True)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace=True)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace=True)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace=True)\n    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): ReLU(inplace=True)\n    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace=True)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace=True)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): ReLU(inplace=True)\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (27): ReLU(inplace=True)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace=True)\n    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n  (classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=4096, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Linear(in_features=4096, out_features=1000, bias=True)\n  )\n)",
            "fig, axs = (nrows=3, ncols=2, figsize=(15, 12))\n\nfor ax, (n, weight) in zip(axs.ravel(), enumerate(weights)):\n\n    X, y = (\n        **common_params,\n        weights=[weight, 1 - weight],\n    )\n    prevalence = y.mean()\n    populations[\"prevalence\"].append(prevalence)\n    populations[\"X\"].append(X)\n    populations[\"y\"].append(y)\n\n    # down-sample for plotting\n    rng = (1)\n    plot_indices = rng.choice((X.shape[0]), size=500, replace=True)\n    X_plot, y_plot = X[plot_indices], y[plot_indices]\n\n    # plot fixed decision boundary of base model with varying prevalence\n    disp = (\n        estimator,\n        X_plot,\n        response_method=\"predict\",\n        alpha=0.5,\n        ax=ax,\n    )\n    scatter = disp.ax_.scatter(X_plot[:, 0], X_plot[:, 1], c=y_plot, edgecolor=\"k\")\n    disp.ax_.set_title(f\"prevalence = {y_plot.mean():.2f}\")\n    disp.ax_.legend(*scatter.legend_elements())\n\n\n<img alt=\"prevalence = 0.22, prevalence = 0.34, prevalence = 0.45, prevalence = 0.60, prevalence = 0.76, prevalence = 0.88\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_likelihood_ratios_001.png\" srcset=\"../../_images/sphx_glr_plot_likelihood_ratios_001.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "scipy->Multidimensional image processing (scipy.ndimage)->Object measurements"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Polynomial and Spline interpolation"
            ],
            [
                "sklearn->Examples->Gaussian Mixture Models->Gaussian Mixture Model Selection->Model training and selection"
            ],
            [
                "torch->Introduction to PyTorch->Save and Load the Model->Saving and Loading Model Weights"
            ],
            [
                "sklearn->Examples->Model Selection->Class Likelihood Ratios to measure classification performance->Invariance with respect to prevalence"
            ]
        ]
    },
    "526388": {
        "jupyter_code_cell": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.patches as mpatches",
        "matched_tutorial_code_inds": [
            3993,
            6092,
            6270,
            6789,
            5287
        ],
        "matched_tutorial_codes": [
            "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nplt.rc(\"figure\", figsize=(16, 9))\nplt.rc(\"font\", size=16)",
            "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom pandas.plotting import register_matplotlib_converters\n\nregister_matplotlib_converters()\nsns.set_style(\"darkgrid\")",
            "import numpy as np\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm\n\nplt.rc(\"figure\", figsize=(16, 8))\nplt.rc(\"font\", size=14)",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom statsmodels.iolib.table import SimpleTable, default_txt_fmt\n\nnp.random.seed(1024)"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships",
                "seaborn->Plotting functions->Visualizing statistical relationships"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Deterministic Terms"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition"
            ],
            [
                "statsmodels->Examples->User Notes->Prediction"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Weighted Least Squares"
            ]
        ]
    },
    "432589": {
        "jupyter_code_cell": "def backward(model, xs, hs, errs):\n    dW2 = hs.T @ errs\n    dh = errs @ model['W2'].T\n    dh[hs <= 0] = 0\n    dW1 = xs.T @ dh\n    return dict(W1=dW1, W2=dW2)\ndef sgd(model, X_train, y_train, minibatch_size):\n    for iter in range(n_iter):\n        print('Iteration {}'.format(iter))\n        X_train, y_train = shuffle(X_train, y_train)\n        for i in range(0, X_train.shape[0], minibatch_size):\n            X_train_mini = X_train[i:i + minibatch_size]\n            y_train_mini = y_train[i:i + minibatch_size]\n            model = sgd_step(model, X_train_mini, y_train_mini)\n    return model",
        "matched_tutorial_code_inds": [
            347,
            2283,
            353,
            507,
            648
        ],
        "matched_tutorial_codes": [
            "def preprocess(x, y):\n    return x.view(-1, 1, 28, 28), y\n\n\nclass WrappedDataLoader:\n    def __init__(self, dl, func):\n        self.dl = dl\n        self.func = func\n\n    def __len__(self):\n        return len(self.dl)\n\n    def __iter__(self):\n        batches = iter(self.dl)\n        for b in batches:\n            yield (self.func(*b))\n\ntrain_dl, valid_dl = get_data(, , bs)\ntrain_dl = WrappedDataLoader(train_dl, preprocess)\nvalid_dl = WrappedDataLoader(valid_dl, preprocess)",
            "def gbdt_apply(X, model):\n    return model.apply(X)[:, :, 0]\n\n\ngbdt_leaves_yielder = (\n    gbdt_apply, kw_args={\"model\": gradient_boosting}\n)\n\ngbdt_model = (\n    gbdt_leaves_yielder,\n    (handle_unknown=\"ignore\"),\n    (max_iter=1000),\n)\ngbdt_model.fit(X_train_linear, y_train_linear)",
            "def preprocess(x, y):\n    return x.view(-1, 1, 28, 28).to(), y.to()\n\n\ntrain_dl, valid_dl = get_data(, , bs)\ntrain_dl = WrappedDataLoader(train_dl, preprocess)\nvalid_dl = WrappedDataLoader(valid_dl, preprocess)",
            "def generate_square_subsequent_mask(sz):\n    mask = ((((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n    return mask\n\n\ndef create_mask(src, tgt):\n    src_seq_len = src.shape[0]\n    tgt_seq_len = tgt.shape[0]\n\n    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n    src_mask = ((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n\n    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask",
            "def fuse_conv_bn_eval(conv, bn):\n    \"\"\"\n    Given a conv Module `A` and an batch_norm module `B`, returns a conv\n    module `C` such that C(x) == B(A(x)) in inference mode.\n    \"\"\"\n    assert(not (conv.training or bn.training)), \"Fusion only for eval!\"\n    fused_conv = copy.deepcopy(conv)\n\n    fused_conv.weight, fused_conv.bias = \\\n        fuse_conv_bn_weights(fused_conv.weight, fused_conv.bias,\n                             bn.running_mean, bn.running_var, bn.eps, bn.weight, bn.bias)\n\n    return fused_conv\n\ndef fuse_conv_bn_weights(conv_w, conv_b, bn_rm, bn_rv, bn_eps, bn_w, bn_b):\n    if conv_b is None:\n        conv_b = (bn_rm)\n    if bn_w is None:\n        bn_w = (bn_rm)\n    if bn_b is None:\n        bn_b = (bn_rm)\n    bn_var_rsqrt = (bn_rv + bn_eps)\n\n    conv_w = conv_w * (bn_w * bn_var_rsqrt).reshape([-1] + [1] * (len(conv_w.shape) - 1))\n    conv_b = (conv_b - bn_rm) * bn_var_rsqrt * bn_w + bn_b\n\n    return torch.nn.Parameter(conv_w), torch.nn.Parameter(conv_b)"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Learning PyTorch->What is torch.nn really?->Wrapping DataLoader"
            ],
            [
                "sklearn->Examples->Ensemble methods->Feature transformations with ensembles of trees"
            ],
            [
                "torch->Learning PyTorch->What is torch.nn really?->Using your GPU"
            ],
            [
                "torch->Text->Language Translation with nn.Transformer and torchtext->Seq2Seq Network using Transformer"
            ],
            [
                "torch->Code Transforms with FX->(beta) Building a Convolution/Batch Norm fuser in FX->Fusing Convolution with Batch Norm"
            ]
        ]
    },
    "104880": {
        "jupyter_code_cell": "m3=train['Electrical'].mode()[0]\nall_data['Electrical'] = all_data['Electrical'].fillna(m3)\nm4=train['KitchenQual'].mode()[0]\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(m4)\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(train['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(train['Exterior2nd'].mode()[0])\nall_data['SaleType'] = all_data['SaleType'].fillna(train['SaleType'].mode()[0])",
        "matched_tutorial_code_inds": [
            2662,
            5299,
            2601,
            2860,
            2844
        ],
        "matched_tutorial_codes": [
            "m, s, _ = (\n    (enet.coef_)[0],\n    enet.coef_[enet.coef_ != 0],\n    markerfmt=\"x\",\n    label=\"Elastic net coefficients\",\n)\n([m, s], color=\"#2ca02c\")\nm, s, _ = (\n    (lasso.coef_)[0],\n    lasso.coef_[lasso.coef_ != 0],\n    markerfmt=\"x\",\n    label=\"Lasso coefficients\",\n)\n([m, s], color=\"#ff7f0e\")\n(\n    (coef)[0],\n    coef[coef != 0],\n    label=\"true coefficients\",\n    markerfmt=\"bx\",\n)\n\n(loc=\"best\")\n(\n    \"Lasso $R^2$: %.3f, Elastic Net $R^2$: %.3f\" % (r2_score_lasso, r2_score_enet)\n)\n()\n\n\n<img alt=\"Lasso $R^2$: 0.658, Elastic Net $R^2$: 0.643\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_lasso_and_elasticnet_001.png\" srcset=\"../../_images/sphx_glr_plot_lasso_and_elasticnet_001.png\"/>",
            "resid1 = res_ols.resid[w == 1.0]\nvar1 = resid1.var(ddof=int(res_ols.df_model) + 1)\nresid2 = res_ols.resid[w != 1.0]\nvar2 = resid2.var(ddof=int(res_ols.df_model) + 1)\nw_est = w.copy()\nw_est[w != 1.0] = np.sqrt(var2) / np.sqrt(var1)\nres_fwls = sm.WLS(y, X, 1.0 / ((w_est ** 2))).fit()\nprint(res_fwls.summary())",
            "vmin, vmax = (-log_marginal_likelihood).min(), 50\nlevel = (((vmin), (vmax), num=50), decimals=1)\n(\n    length_scale_grid,\n    noise_level_grid,\n    -log_marginal_likelihood,\n    levels=level,\n    norm=(vmin=vmin, vmax=vmax),\n)\n()\n(\"log\")\n(\"log\")\n(\"Length-scale\")\n(\"Noise-level\")\n(\"Log-marginal-likelihood\")\n()\n\n\n<img alt=\"Log-marginal-likelihood\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_noisy_005.png\" srcset=\"../../_images/sphx_glr_plot_gpr_noisy_005.png\"/>",
            "mae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}\n\n_, ax = (figsize=(5, 5))\ndisplay = (\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n)\nax.set_title(\"Ridge model, optimum regularization\")\nfor name, score in scores.items():\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\nax.legend(loc=\"upper left\")\n()\n\n\n<img alt=\"Ridge model, optimum regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_012.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_012.png\"/>",
            "mae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}\n\n_, ax = (figsize=(5, 5))\ndisplay = (\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n)\nax.set_title(\"Ridge model, small regularization\")\nfor name, score in scores.items():\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\nax.legend(loc=\"upper left\")\n()\n\n\n<img alt=\"Ridge model, small regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_009.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_009.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Generalized Linear Models->Lasso and Elastic Net for Sparse Signals->Plot"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Weighted Least Squares->Feasible Weighted Least Squares (2-stage FWLS)"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) with noise-level estimation->Optimisation of kernel hyperparameters in GPR"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ]
        ]
    },
    "1298185": {
        "jupyter_code_cell": "import numpy as np \nimport pandas as pd \nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import GridSearchCV\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))",
        "matched_tutorial_code_inds": [
            2700,
            3270,
            3436,
            2810,
            3080
        ],
        "matched_tutorial_codes": [
            "from sklearn.linear_model import \n\nreg_nnls = (positive=True)\ny_pred_nnls = reg_nnls.fit(X_train, y_train).predict(X_test)\nr2_score_nnls = (y_test, y_pred_nnls)\nprint(\"NNLS R2 score\", r2_score_nnls)",
            "import sys\n\ntry:\n    import nmslib\nexcept ImportError:\n    print(\"The package 'nmslib' is required to run this example.\")\n    ()\n\ntry:\n    from pynndescent import PyNNDescentTransformer\nexcept ImportError:\n    print(\"The package 'pynndescent' is required to run this example.\")\n    ()",
            "import numpy as np\nfrom sklearn.datasets import \n\nn_samples = 200\nX, y = (n_samples=n_samples, shuffle=False)\nouter, inner = 0, 1\nlabels = (n_samples, -1.0)\nlabels[0] = outer\nlabels[-1] = inner",
            "from sklearn.pipeline import \nfrom sklearn.linear_model import \nfrom sklearn.compose import \n\nmodel = (\n    preprocessor,\n    (\n        regressor=(alpha=1e-10), func=, inverse_func=\n    ),\n)",
            "import sys\nfrom time import \nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.random_projection import \nfrom sklearn.random_projection import \nfrom sklearn.datasets import \nfrom sklearn.datasets import \nfrom sklearn.metrics.pairwise import euclidean_distances"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Generalized Linear Models->Non-negative least squares"
            ],
            [
                "sklearn->Examples->Nearest Neighbors->Approximate nearest neighbors in TSNE"
            ],
            [
                "sklearn->Examples->Semi Supervised Classification->Label Propagation learning a complex structure"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "sklearn->Examples->Miscellaneous->The Johnson-Lindenstrauss bound for embedding with random projections"
            ]
        ]
    },
    "1447973": {
        "jupyter_code_cell": "import pandas as pd\nimport csv\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom sklearn.utils import shuffle",
        "matched_tutorial_code_inds": [
            6399,
            6455,
            5710,
            5407,
            5484
        ],
        "matched_tutorial_codes": [
            "import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt",
            "import numpy as np\nfrom scipy import stats\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm",
            "import statsmodels.api as sm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom statsmodels.formula.api import logit",
            "import numpy as np\nimport pandas as pd\nimport scipy.stats as stats\n\nfrom statsmodels.miscmodels.ordinal_model import OrderedModel"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->VARMAX: Introduction",
                "statsmodels->Examples->State space models->Trends and cycles in unemployment"
            ],
            [
                "statsmodels->Examples->State space models->Statespace ARMA: Sunspots Data"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Quasi-binomial regression"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression"
            ]
        ]
    },
    "1350574": {
        "jupyter_code_cell": "sales_per_agent_query2 = \"\"\"\nSELECT \n    e1.first_name || \" \" || e1.last_name sales_agent,\n    e1.hire_date,\n    ROUND(SUM(i.total),2) total_sales,\n    ROUND(SUM(i.total),2)/(julianday('now') - julianday(e1.hire_date)) sales_per_workday\nFROM employee e1\nINNER JOIN customer c ON e1.employee_id = c.support_rep_id\nINNER JOIN invoice i ON i.customer_id = c.customer_id\nGROUP BY e1.employee_id\nHAVING e1.title = \"Sales Support Agent\"\nORDER BY total_sales DESC\n\"\"\"\nsales_per_agent2 = run_query(sales_per_agent_query2)\nprint(sales_per_agent2)\nsales_per_agent2.plot.bar(x=\"sales_agent\",y=\"sales_per_workday\")\ncountry_sales_query = \"\"\"\nWITH \n    total_customers AS (\n        SELECT country nation, COUNT(*) num_of_customers \n        FROM customer \n        GROUP BY country\n     ), \n    total_value AS(\n        SELECT c.country nation, SUM(i.total) total_sales, AVG(i.total) avg_sales\n        FROM invoice i \n        INNER JOIN customer c ON c.customer_id = i.customer_id\n        GROUP BY nation\n      ),\n    multi_customer_countries AS (\n        SELECT \n            tc.nation, \n            tc.num_of_customers, \n            tv.total_sales total_sales_country,\n            CAST(tv.total_sales as float)/CAST(tc.num_of_customers as Float) average_value_per_customer,\n            tv.avg_sales average_order_value \n        FROM total_customers tc\n        INNER join total_value tv ON tv.nation = tc.nation\n        WHERE tc.num_of_customers > 1\n      ), \n    other_countries AS (\n        SELECT \n            CASE\n                WHEN tc.num_of_customers = 1 THEN \"Other\"\n            END\n            as nation,\n            COUNT(*) num_of_customers,\n            SUM(tv.total_sales) total_sales_country,\n            CAST(SUM(tv.total_sales) AS float)/CAST(COUNT(*) as FLOAT) average_value_per_customer,\n            AVG(tv.avg_sales) avg_order_value\n        FROM total_customers tc\n        INNER join total_value tv ON tv.nation = tc.nation\n        WHERE tc.num_of_customers = 1\n      ),\n    final_sales as (\n        SELECT * FROM multi_customer_countries\n        UNION\n        SELECT * FROM other_countries\n      )\nSELECT nation, num_of_customers, total_sales_country, average_order_value, average_value_per_customer\nFROM (\n  SELECT\n      fs.*,\n      CASE\n          WHEN fs.nation = \"Other\" THEN 1\n          ELSE 0\n      end AS sort\n  FROM final_sales fs\n  )\norder by sort ASC, total_sales_country DESC\n\"\"\"\nrun_query(country_sales_query)",
        "matched_tutorial_code_inds": [
            5917,
            220,
            5915,
            2951,
            2861
        ],
        "matched_tutorial_codes": [
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "labels_map = {\n    0: \"T-Shirt\",\n    1: \"Trouser\",\n    2: \"Pullover\",\n    3: \"Dress\",\n    4: \"Coat\",\n    5: \"Sandal\",\n    6: \"Shirt\",\n    7: \"Sneaker\",\n    8: \"Bag\",\n    9: \"Ankle Boot\",\n}\nfigure = plt.figure(figsize=(8, 8))\ncols, rows = 3, 3\nfor i in range(1, cols * rows + 1):\n    sample_idx = (len(), size=(1,)).item()\n    ,  = [sample_idx]\n    figure.add_subplot(rows, cols, i)\n    plt.title(labels_map[])\n    plt.axis(\"off\")\n    plt.imshow(.squeeze(), cmap=\"gray\")\nplt.show()\n\n\n<img alt=\"Pullover, Sandal, Bag, Bag, Sneaker, Bag, Pullover, Sandal, Sandal\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_data_tutorial_001.png\" srcset=\"../../_images/sphx_glr_data_tutorial_001.png\"/>",
            "resid = interM_lm32.get_influence().summary_frame()[\"standard_resid\"]\n\nplt.figure(figsize=(6, 6))\nresid = resid.reindex(X.index)\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X.loc[idx],\n        resid.loc[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\nplt.xlabel(\"X[~[32]]\")\nplt.ylabel(\"standardized resids\")",
            "cluster_ids = (dist_linkage, 1, criterion=\"distance\")\ncluster_id_to_feature_ids = (list)\nfor idx, cluster_id in enumerate(cluster_ids):\n    cluster_id_to_feature_ids[cluster_id].append(idx)\nselected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n\nX_train_sel = X_train[:, selected_features]\nX_test_sel = X_test[:, selected_features]\n\nclf_sel = (n_estimators=100, random_state=42)\nclf_sel.fit(X_train_sel, y_train)\nprint(\n    \"Accuracy on test data with features removed: {:.2f}\".format(\n        clf_sel.score(X_test_sel, y_test)\n    )\n)",
            "coefs = (\n    model[-1].regressor_.coef_,\n    columns=[\"Coefficients importance\"],\n    index=feature_names,\n)\ncoefs.plot.barh(figsize=(9, 7))\n(\"Ridge model, with regularization, normalized variables\")\n(\"Raw coefficient values\")\n(x=0, color=\".5\")\n(left=0.3)\n\n\n<img alt=\"Ridge model, with regularization, normalized variables\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_013.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_013.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "torch->Introduction to PyTorch->Datasets & DataLoaders->Iterating and Visualizing the Dataset"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "sklearn->Examples->Inspection->Permutation Importance with Multicollinear or Correlated Features->Handling Multicollinear Features"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ]
        ]
    },
    "799682": {
        "jupyter_code_cell": "test.head()\ny_train = np.log1p(train['price'])\ntrain['category_name'] = train['category_name'].fillna('Other').astype(str)\ntrain['brand_name'] = train['brand_name'].fillna('missing').astype(str)\ntrain['shipping'] = train['shipping'].astype(str)\ntrain['item_condition_id'] = train['item_condition_id'].astype(str)\ntrain['item_description'] = train['item_description'].fillna('None')",
        "matched_tutorial_code_inds": [
            2947,
            6714,
            2973,
            2972,
            2598
        ],
        "matched_tutorial_codes": [
            "data = ()\nX, y = data.data, data.target\nX_train, X_test, y_train, y_test = (X, y, random_state=42)\n\nclf = (n_estimators=100, random_state=42)\nclf.fit(X_train, y_train)\nprint(\"Accuracy on test data: {:.2f}\".format(clf.score(X_test, y_test)))",
            "make_plot(dta.index[idx[:5]])\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_24_0.png\" src=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_24_0.png\"/>",
            "t_sne = (\n    n_components=n_components,\n    perplexity=30,\n    init=\"random\",\n    n_iter=250,\n    random_state=0,\n)\nS_t_sne = t_sne.fit_transform(S_points)\n\nplot_2d(S_t_sne, S_color, \"T-distributed Stochastic  \\n Neighbor Embedding\")\n\n\n<img alt=\"T-distributed Stochastic    Neighbor Embedding\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_compare_methods_006.png\" srcset=\"../../_images/sphx_glr_plot_compare_methods_006.png\"/>",
            "spectral = (\n    n_components=n_components, n_neighbors=n_neighbors\n)\nS_spectral = spectral.fit_transform(S_points)\n\nplot_2d(S_spectral, S_color, \"Spectral Embedding\")\n\n\n<img alt=\"Spectral Embedding\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_compare_methods_005.png\" srcset=\"../../_images/sphx_glr_plot_compare_methods_005.png\"/>",
            "kernel = 1.0 * (length_scale=1e-1, length_scale_bounds=(1e-2, 1e3)) + (\n    noise_level=1e-2, noise_level_bounds=(1e-10, 1e1)\n)\ngpr = (kernel=kernel, alpha=0.0)\ngpr.fit(X_train, y_train)\ny_mean, y_std = gpr.predict(X, return_std=True)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Inspection->Permutation Importance with Multicollinear or Correlated Features->Random Forest Feature Importance on Breast Cancer Data"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "sklearn->Examples->Manifold learning->Comparison of Manifold Learning methods->Define algorithms for the manifold learning->T-distributed Stochastic Neighbor Embedding"
            ],
            [
                "sklearn->Examples->Manifold learning->Comparison of Manifold Learning methods->Define algorithms for the manifold learning->Spectral embedding for non-linear dimensionality reduction"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) with noise-level estimation->Optimisation of kernel hyperparameters in GPR"
            ]
        ]
    },
    "447074": {
        "jupyter_code_cell": "q_values\nq_value_observations",
        "matched_tutorial_code_inds": [
            5909,
            3425,
            5519,
            5015,
            3764
        ],
        "matched_tutorial_codes": [
            "interM_lm.model.exog\ninterM_lm.model.exog_names",
            "y_train = (y)\ny_train[unlabeled_set] = -1",
            "modfd_logit.k_constant",
            "text.usetex : true\nfont.family : Helvetica",
            "df['home_win'] = df.home_points &gt; df.away_points"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "sklearn->Examples->Semi Supervised Classification->Label Propagation digits: Demonstrating performance->Data generation"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model"
            ],
            [
                "matplotlib->Tutorials->Text->Text rendering with LaTeX"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 1: Create an outcome variable"
            ]
        ]
    },
    "1191469": {
        "jupyter_code_cell": "data = train\ngrouped = data.groupby('TripType')\ntem_b = grouped['HEALTH AND BEAUTY AIDS', 'SHOES', 'MENS WEAR', 'SWIMWEAR/OUTERWEAR', 'LADIES SOCKS'].agg([np.sum])\ntem_b.plot(kind='bar', figsize = (15,12.5))",
        "matched_tutorial_code_inds": [
            6060,
            6081,
            5712,
            4654,
            5963
        ],
        "matched_tutorial_codes": [
            "data = pdr.get_data_fred(\"HOUSTNSA\", \"1959-01-01\", \"2019-06-01\")\nhousing = data.HOUSTNSA.pct_change().dropna()\n# Scale by 100 to get percentages\nhousing = 100 * housing.asfreq(\"MS\")\nfig, ax = plt.subplots()\nax = housing.plot(ax=ax)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_6_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_6_0.png\"/>",
            "data = pdr.get_data_fred(\"INDPRO\", \"1959-01-01\", \"2019-06-01\")\nind_prod = data.INDPRO.pct_change(12).dropna().asfreq(\"MS\")\n_, ax = plt.subplots(figsize=(16, 9))\nind_prod.plot(ax=ax)",
            "df = pd.read_csv(raw, header=None)\ndf = df.melt()\ndf[\"site\"] = 1 + np.floor(df.index / 10).astype(int)\ndf[\"variety\"] = 1 + (df.index % 10)\ndf = df.rename(columns={\"value\": \"blotch\"})\ndf = df.drop(\"variable\", axis=1)\ndf[\"blotch\"] /= 100",
            "data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>",
            "data = [\n    [\"Carroll\", 94, 22, 60, 92, 20, 60],\n    [\"Grant\", 98, 21, 65, 92, 22, 65],\n    [\"Peck\", 98, 28, 40, 88, 26, 40],\n    [\"Donat\", 94, 19, 200, 82, 17, 200],\n    [\"Stewart\", 98, 21, 50, 88, 22, 45],\n    [\"Young\", 96, 21, 85, 92, 22, 85],\n]\ncolnames = [\"study\", \"mean_t\", \"sd_t\", \"n_t\", \"mean_c\", \"sd_c\", \"n_c\"]\nrownames = [i[0] for i in data]\ndframe1 = pd.DataFrame(data, columns=colnames)\nrownames"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions->Industrial Production"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Quasi-binomial regression"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Example"
            ]
        ]
    },
    "1152178": {
        "jupyter_code_cell": "x = np.array(range(1,11)).reshape(10, 1)\nX = np.concatenate((np.ones((10, 1)), x, x ** 2, x ** 3), axis = 1)\nX\nbeta = np.array([5000, 2.3, 1.3, 10]).reshape(1, 4)\ny = np.dot(beta, X.T).T\ny = y + 2000*np.random.random((10, 1))\ny",
        "matched_tutorial_code_inds": [
            6795,
            4474,
            93,
            2741,
            1982
        ],
        "matched_tutorial_codes": [
            "x1n = np.linspace(20.5, 25, 10)\nXnew = np.column_stack((x1n, np.sin(x1n), (x1n - 5) ** 2))\nXnew = sm.add_constant(Xnew)\nynewpred = olsres.predict(Xnew)  # predict out of sample\nprint(ynewpred)",
            "x = np.arange(4) * 2\n x\narray([0, 2, 4, 6])\n prb = hypergeom.cdf(x, M, n, N)\n prb\narray([  1.03199174e-04,   5.21155831e-02,   6.08359133e-01,\n         9.89783282e-01])\n hypergeom.ppf(prb, M, n, N)\narray([ 0.,  2.,  4.,  6.])",
            "x = (-5, 5, 0.1).view(-1, 1)\ny = -5 * x + 0.1 * (x.size())\n\nmodel = (1, 1)\ncriterion = ()\noptimizer = (model.parameters(), lr = 0.1)\n\ndef train_model(iter):\n    for epoch in range(iter):\n        y1 = model(x)\n        loss = criterion(y1, y)\n        writer.add_scalar(\"Loss/train\", loss, epoch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\ntrain_model(10)\nwriter.flush()",
            "x_train = (0, 10, 100)\nrng = (0)\nx_train = (rng.choice(x_train, size=20, replace=False))\ny_train = f(x_train)\n\n# create 2D-array versions of these arrays to feed to transformers\nX_train = x_train[:, ]\nX_plot = x_plot[:, ]",
            "coef = ((size, size))\ncoef[0:roi_size, 0:roi_size] = -1.0\ncoef[-roi_size:, -roi_size:] = 1.0\n\nX = (n_samples, size**2)\nfor x in X:  # smooth data\n    x[:] = (x.reshape(size, size), sigma=1.0).ravel()\nX -= X.mean(axis=0)\nX /= X.std(axis=0)\n\ny = (X, coef.ravel())"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->User Notes->Prediction->Create a new sample of explanatory variables Xnew, predict and plot"
            ],
            [
                "scipy->Statistics (scipy.stats)->Random variables->Specific points for discrete distributions"
            ],
            [
                "torch->PyTorch Recipes->How to use TensorBoard with PyTorch->Log scalars"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Polynomial and Spline interpolation"
            ],
            [
                "sklearn->Examples->Clustering->Feature agglomeration vs. univariate selection"
            ]
        ]
    },
    "31474": {
        "jupyter_code_cell": "print(pokemon_df['Type 1'].value_counts())\ndef to_upper_case(series):\n    return series.str.upper();\nprint(pokemon_df[['Type 1']].apply(to_upper_case).head())\nprint(pokemon_df['Type 1'].value_counts().head())\nprint(pokemon_df['Type 1'].map(lambda type1: type1.upper()).head())",
        "matched_tutorial_code_inds": [
            1039,
            2386,
            4483,
            2791,
            4494
        ],
        "matched_tutorial_codes": [
            "print(\n    \"Sparsity in conv1.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Sparsity in conv2.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Sparsity in fc1.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Sparsity in fc2.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Sparsity in fc3.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Global sparsity: {:.2f}%\".format(\n        100. * float(\n            ( == 0)\n            + ( == 0)\n            + ( == 0)\n            + ( == 0)\n            + ( == 0)\n        )\n        / float(\n            .nelement()\n            + .nelement()\n            + .nelement()\n            + .nelement()\n            + .nelement()\n        )\n    )\n)",
            "plot_digits(X_test, \"Uncorrupted test images\")\nplot_digits(\n    X_reconstructed_pca,\n    f\"PCA reconstruction\\nMSE: {((X_test - X_reconstructed_pca) ** 2):.2f}\",\n)\nplot_digits(\n    X_reconstructed_kernel_pca,\n    \"Kernel PCA reconstruction\\n\"\n    f\"MSE: {((X_test - X_reconstructed_kernel_pca) ** 2):.2f}\",\n)\n\n\n\n<img alt=\"Uncorrupted test images\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_digits_denoising_003.png\" srcset=\"../../_images/sphx_glr_plot_digits_denoising_003.png\"/>\n<img alt=\"PCA reconstruction MSE: 0.01\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_digits_denoising_004.png\" srcset=\"../../_images/sphx_glr_plot_digits_denoising_004.png\"/>\n<img alt=\"Kernel PCA reconstruction MSE: 0.03\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_digits_denoising_005.png\" srcset=\"../../_images/sphx_glr_plot_digits_denoising_005.png\"/>",
            "print('mean = %6.4f, variance = %6.4f, skew = %6.4f, kurtosis = %6.4f' %\n...       normdiscrete.stats(moments='mvsk'))\nmean = -0.0000, variance = 6.3302, skew = 0.0000, kurtosis = -0.0076",
            "print(\n    \"Mean AvgClaim Amount per policy:              %.2f \"\n    % df_train[\"AvgClaimAmount\"].mean()\n)\nprint(\n    \"Mean AvgClaim Amount | NbClaim  0:           %.2f\"\n    % df_train[\"AvgClaimAmount\"][df_train[\"AvgClaimAmount\"]  0].mean()\n)\nprint(\n    \"Predicted Mean AvgClaim Amount | NbClaim  0: %.2f\"\n    % glm_sev.predict(X_train).mean()\n)\nprint(\n    \"Predicted Mean AvgClaim Amount (dummy) | NbClaim  0: %.2f\"\n    % dummy_sev.predict(X_train).mean()\n)",
            "print('KS-statistic D = %6.3f pvalue = %6.4f' % stats.kstest(x, 't', (10,)))\nKS-statistic D =  0.016 pvalue = 0.9571  # random"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Model Optimization->Pruning Tutorial->Global pruning"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Image denoising using kernel PCA->Reconstruct and denoise test images"
            ],
            [
                "scipy->Statistics (scipy.stats)->Building specific distributions->Subclassing rv_discrete"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Tweedie regression on insurance claims->Severity Model -  Gamma distribution"
            ],
            [
                "scipy->Statistics (scipy.stats)->Analysing one sample->T-test and KS-test"
            ]
        ]
    },
    "811437": {
        "jupyter_code_cell": "import matplotlib.pyplot as plt\nplt.plot(range(1, len(train_auc)+1), train_auc, color='blue', label='Train auc')\nplt.plot(range(1, len(train_auc)+1), val_auc, color='red', label='Val auc')\nplt.legend(loc=\"best\")\nplt.xlabel('#Batches')\nplt.ylabel('Auc')\nplt.tight_layout()\nplt.savefig('./output/fig-out-of-core.png', dpi=300)\nplt.show()\nimport _pickle as pkl\npkl.dump(hashvec, open('output/hashvec.pkl', 'wb'))\npkl.dump(clf, open('output/clf-sgd.pkl', 'wb'))\nhashvec = pkl.load(open('output/hashvec.pkl', 'rb'))\nclf = pkl.load(open('output/clf-sgd.pkl', 'rb'))\ndf_test = pd.read_csv('datasets/test.csv')\nprint('test auc: %.3f' % roc_auc_score(df_test['sentiment'],            clf.predict_proba(hashvec.transform(df_test['review']))[:,1]))",
        "matched_tutorial_code_inds": [
            1393,
            1212,
            1926,
            4661,
            2667
        ],
        "matched_tutorial_codes": [
            "import matplotlib.pyplot as plt\nimport numpy as np\nidx = 5\nprint(\"Question: \", dataset[\"train\"][idx][\"question\"])\nprint(\"Answers: \" ,dataset[\"train\"][idx][\"answers\"])\nim = np.asarray(dataset[\"train\"][idx][\"image\"].resize((500,500)))\nplt.imshow(im)\nplt.show()\n\n\n<img alt=\"flava finetuning tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_flava_finetuning_tutorial_001.png\" srcset=\"../_images/sphx_glr_flava_finetuning_tutorial_001.png\"/>",
            "import matplotlib.pyplot as plt\nplt.switch_backend('Agg')\nimport numpy as np\nimport timeit\n\nnum_repeat = 10\n\nstmt = \"train(model)\"\n\nsetup = \"model = ModelParallelResNet50()\"\nmp_run_times = timeit.repeat(\n    stmt, setup, number=1, repeat=num_repeat, globals=globals())\nmp_mean, mp_std = np.mean(mp_run_times), np.std(mp_run_times)\n\nsetup = \"import torchvision.models as models;\" + \\\n        \"model = models.resnet50(num_classes=num_classes).to('cuda:0')\"\nrn_run_times = timeit.repeat(\n    stmt, setup, number=1, repeat=num_repeat, globals=globals())\nrn_mean, rn_std = np.mean(rn_run_times), np.std(rn_run_times)\n\n\ndef plot(means, stds, , fig_name):\n    fig, ax = plt.subplots()\n    ax.bar(np.arange(len(means)), means, yerr=stds,\n           align='center', alpha=0.5, ecolor='red', capsize=10, width=0.6)\n    ax.set_ylabel('ResNet50 Execution Time (Second)')\n    ax.set_xticks(np.arange(len(means)))\n    ax.set_xticklabels()\n    ax.yaxis.grid(True)\n    plt.tight_layout()\n    plt.savefig(fig_name)\n    plt.close(fig)\n\n\nplot([mp_mean, rn_mean],\n     [mp_std, rn_std],\n     ['Model Parallel', 'Single GPU'],\n     'mp_vs_rn.png')\n\n\n\n<img alt=\"\" src=\"../_images/mp_vs_rn.png\"/>",
            "import matplotlib.pyplot as plt\n\n(figsize=(5, 5))\n(rescaled_coins, cmap=plt.cm.gray)\nfor l in range(n_clusters):\n    (\n        label == l,\n        colors=[\n            plt.cm.nipy_spectral(l / float(n_clusters)),\n        ],\n    )\n(\"off\")\n()\n\n\n<img alt=\"plot coin ward segmentation\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_coin_ward_segmentation_001.png\" srcset=\"../../_images/sphx_glr_plot_coin_ward_segmentation_001.png\"/>",
            "import matplotlib.pyplot as plt\nplt.figure(1)                # the first figure\nplt.subplot(211)             # the first subplot in the first figure\nplt.plot([1, 2, 3])\nplt.subplot(212)             # the second subplot in the first figure\nplt.plot([4, 5, 6])\n\n\nplt.figure(2)                # a second figure\nplt.plot([4, 5, 6])          # creates a subplot() by default\n\nplt.figure(1)                # first figure current;\n                             # subplot(212) still current\nplt.subplot(211)             # make subplot(211) in the first figure\n                             # current\nplt.title('Easy as 1, 2, 3') # subplot 211 title",
            "import numpy as np\n\naic_criterion = zou_et_al_criterion_rescaling(\n    lasso_lars_ic[-1].criterion_,\n    n_samples,\n    lasso_lars_ic[-1].noise_variance_,\n)\n\nindex_alpha_path_aic = (\n    lasso_lars_ic[-1].alphas_ == lasso_lars_ic[-1].alpha_\n)[0]"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Multimodality->TorchMultimodal Tutorial: Finetuning FLAVA->Steps"
            ],
            [
                "torch->Parallel and Distributed Training->Single-Machine Model Parallel Best Practices->Apply Model Parallel to Existing Modules"
            ],
            [
                "sklearn->Examples->Clustering->A demo of structured Ward hierarchical clustering on an image of coins->Plot the results on an image"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Working with multiple figures and axes"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Lasso model selection via information criteria"
            ]
        ]
    },
    "271159": {
        "jupyter_code_cell": "def train_and_test(trainin,features,target='SalePrice',kf=10):\n    lrm = LinearRegression()\n    mean_s_errors = cross_val_score(lrm,\n                                    trainin[features],\n                                    trainin[target],\n                                    scoring='neg_mean_squared_error',\n                                    cv=kf)\n    r_ms_errors = [abs(m)**(1/2) for m in mean_s_errors]\n    avg_rms_error = np.mean(r_ms_errors)\n    return r_ms_errors, avg_rms_error\nnum_val_counts = train.isnull().sum()\nnum_val_counts[num_val_counts > 0.25*train.shape[0]]",
        "matched_tutorial_code_inds": [
            3103,
            3101,
            3102,
            3100,
            6276
        ],
        "matched_tutorial_codes": [
            "def get_impute_iterative(X_missing, y_missing):\n    imputer = (\n        missing_values=,\n        add_indicator=True,\n        random_state=0,\n        n_nearest_features=3,\n        max_iter=1,\n        sample_posterior=True,\n    )\n    iterative_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n    return iterative_impute_scores.mean(), iterative_impute_scores.std()\n\n\nmses_california[4], stds_california[4] = get_impute_iterative(\n    X_miss_california, y_miss_california\n)\nmses_diabetes[4], stds_diabetes[4] = get_impute_iterative(\n    X_miss_diabetes, y_miss_diabetes\n)\nx_labels.append(\"Iterative Imputation\")\n\nmses_diabetes = mses_diabetes * -1\nmses_california = mses_california * -1",
            "def get_impute_knn_score(X_missing, y_missing):\n    imputer = (missing_values=, add_indicator=True)\n    knn_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n    return knn_impute_scores.mean(), knn_impute_scores.std()\n\n\nmses_california[2], stds_california[2] = get_impute_knn_score(\n    X_miss_california, y_miss_california\n)\nmses_diabetes[2], stds_diabetes[2] = get_impute_knn_score(\n    X_miss_diabetes, y_miss_diabetes\n)\nx_labels.append(\"KNN Imputation\")",
            "def get_impute_mean(X_missing, y_missing):\n    imputer = (missing_values=, strategy=\"mean\", add_indicator=True)\n    mean_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n    return mean_impute_scores.mean(), mean_impute_scores.std()\n\n\nmses_california[3], stds_california[3] = get_impute_mean(\n    X_miss_california, y_miss_california\n)\nmses_diabetes[3], stds_diabetes[3] = get_impute_mean(X_miss_diabetes, y_miss_diabetes)\nx_labels.append(\"Mean Imputation\")",
            "def get_impute_zero_score(X_missing, y_missing):\n\n    imputer = (\n        missing_values=, add_indicator=True, strategy=\"constant\", fill_value=0\n    )\n    zero_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n    return zero_impute_scores.mean(), zero_impute_scores.std()\n\n\nmses_california[1], stds_california[1] = get_impute_zero_score(\n    X_miss_california, y_miss_california\n)\nmses_diabetes[1], stds_diabetes[1] = get_impute_zero_score(\n    X_miss_diabetes, y_miss_diabetes\n)\nx_labels.append(\"Zero imputation\")",
            "def add_stl_plot(fig, res, legend):\n    \"\"\"Add 3 plots from a second STL fit\"\"\"\n    axs = fig.get_axes()\n    comps = [\"trend\", \"seasonal\", \"resid\"]\n    for ax, comp in zip(axs[1:], comps):\n        series = getattr(res, comp)\n        if comp == \"resid\":\n            ax.plot(series, marker=\"o\", linestyle=\"none\")\n        else:\n            ax.plot(series)\n            if comp == \"trend\":\n                ax.legend(legend, frameon=False)\n\n\nstl = STL(elec_equip, period=12, robust=True)\nres_robust = stl.fit()\nfig = res_robust.plot()\nres_non_robust = STL(elec_equip, period=12, robust=False).fit()\nadd_stl_plot(fig, res_non_robust, [\"Robust\", \"Non-robust\"])\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_stl_decomposition_10_0.png\" src=\"../../../_images/examples_notebooks_generated_stl_decomposition_10_0.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score->Iterative imputation of the missing values"
            ],
            [
                "sklearn->Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score->kNN-imputation of the missing values"
            ],
            [
                "sklearn->Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score->Impute missing values with mean"
            ],
            [
                "sklearn->Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score->Replace missing values by 0"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition->Robust Fitting"
            ]
        ]
    },
    "633273": {
        "jupyter_code_cell": "db_name = 'streeteasy_db.sqlite'\ntrain_table_name = 'train_data'\ntest_table_name = 'test_data'\ncon = sqlite3.connect(db_name)\ndf_train = pd.read_sql(\"SELECT * FROM %s\" % (train_table_name),con)\ndf_test = pd.read_sql(\"SELECT * FROM %s\" % (test_table_name),con)\ndf_test = df_test[~df_test['data_id'].isin(df_train['data_id'])]\ndf_train['test_set'] = 0\ndf_test['test_set'] = 1\ndf = pd.concat([df_train,df_test],axis = 0)\nunit_type_list = df['unit_type'].unique()\nnhood_list_names = df['neighborhood'].unique()\ndf = standard_formatting(df)\ndf = df.drop('index',1)",
        "matched_tutorial_code_inds": [
            1092,
            137,
            3746,
            3698,
            6891
        ],
        "matched_tutorial_codes": [
            "data_path = '~/.data/imagenet'\nsaved_model_dir = 'data/'\nfloat_model_file = 'mobilenet_pretrained_float.pth'\nscripted_float_model_file = 'mobilenet_quantization_scripted.pth'\nscripted_quantized_model_file = 'mobilenet_quantization_scripted_quantized.pth'\n\ntrain_batch_size = 30\neval_batch_size = 50\n\ndata_loader, data_loader_test = prepare_data_loaders(data_path)\ncriterion = nn.CrossEntropyLoss()\nfloat_model = load_model(saved_model_dir + float_model_file).to('cpu')\n\n# Next, we'll \"fuse modules\"; this can both make the model faster by saving on memory access\n# while also improving numerical accuracy. While this can be used with any model, this is\n# especially common with quantized models.\n\nprint('\\n Inverted Residual Block: Before fusion \\n\\n', float_model.features[1].conv)\nfloat_model.eval()\n\n# Fuses modules\nfloat_model.fuse_model()\n\n# Note fusion of Conv+BN+Relu and Conv+Relu\nprint('\\n Inverted Residual Block: After fusion\\n\\n',float_model.features[1].conv)",
            "model = ()\ninputs = (5, 3, 224, 224)\n\nwith (activities=[ProfilerActivity.CPU],\n        profile_memory=True, record_shapes=True) as prof:\n    model(inputs)\n\nprint(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))\n\n# (omitting some columns)\n# ---------------------------------  ------------  ------------  ------------\n#                              Name       CPU Mem  Self CPU Mem    # of Calls\n# ---------------------------------  ------------  ------------  ------------\n#                       aten::empty      94.79 Mb      94.79 Mb           121\n#     aten::max_pool2d_with_indices      11.48 Mb      11.48 Mb             1\n#                       aten::addmm      19.53 Kb      19.53 Kb             1\n#               aten::empty_strided         572 b         572 b            25\n#                     aten::resize_         240 b         240 b             6\n#                         aten::abs         480 b         240 b             4\n#                         aten::add         160 b         160 b            20\n#               aten::masked_select         120 b         112 b             1\n#                          aten::ne         122 b          53 b             6\n#                          aten::eq          60 b          30 b             2\n# ---------------------------------  ------------  ------------  ------------\n# Self CPU time total: 53.064ms\n\nprint(prof.key_averages().table(sort_by=\"cpu_memory_usage\", row_limit=10))\n\n# (omitting some columns)\n# ---------------------------------  ------------  ------------  ------------\n#                              Name       CPU Mem  Self CPU Mem    # of Calls\n# ---------------------------------  ------------  ------------  ------------\n#                       aten::empty      94.79 Mb      94.79 Mb           121\n#                  aten::batch_norm      47.41 Mb           0 b            20\n#      aten::_batch_norm_impl_index      47.41 Mb           0 b            20\n#           aten::native_batch_norm      47.41 Mb           0 b            20\n#                      aten::conv2d      47.37 Mb           0 b            20\n#                 aten::convolution      47.37 Mb           0 b            20\n#                aten::_convolution      47.37 Mb           0 b            20\n#          aten::mkldnn_convolution      47.37 Mb           0 b            20\n#                  aten::max_pool2d      11.48 Mb           0 b             1\n#     aten::max_pool2d_with_indices      11.48 Mb      11.48 Mb             1\n# ---------------------------------  ------------  ------------  ------------\n# Self CPU time total: 53.064ms",
            "fp = 'data/nba.csv'\n\nif not os.path.exists(fp):\n    tables = pd.read_html(\"http://www.basketball-reference.com/leagues/NBA_2016_games.html\")\n    games = tables[0]\n    games.to_csv(fp)\nelse:\n    games = pd.read_csv(fp)\ngames.head()",
            "files = glob.glob('weather/*.csv')\ncolumns = ['station', 'date', 'tmpf', 'relh', 'sped', 'mslp',\n           'p01i', 'vsby', 'gust_mph', 'skyc1', 'skyc2', 'skyc3']\n\n# init empty DataFrame, like you might for a list\nweather = pd.DataFrame(columns=columns)\n\nfor fp in files:\n    city = pd.read_csv(fp, columns=columns)\n    weather.append(city)",
            "url = 'https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/csv/COUNT/medpar.csv'\nmedpar = read.csv(url)\nf = los~factor(type)+hmo+white\n\nlibrary(MASS)\nmod = glm.nb(f, medpar)\ncoef(summary(mod))\n                 Estimate Std. Error   z value      Pr(|z|)\n(Intercept)    2.31027893 0.06744676 34.253370 3.885556e-257\nfactor(type)2  0.22124898 0.05045746  4.384861  1.160597e-05\nfactor(type)3  0.70615882 0.07599849  9.291748  1.517751e-20\nhmo           -0.06795522 0.05321375 -1.277024  2.015939e-01\nwhite         -0.12906544 0.06836272 -1.887951  5.903257e-02"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Model Optimization->(beta) Static Quantization with Eager Mode in PyTorch->3. Define dataset and data loaders->ImageNet Data"
            ],
            [
                "torch->PyTorch Recipes->PyTorch Profiler->Steps->4. Using profiler to analyze memory consumption"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ],
            [
                "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 2: Negative Binomial Regression for Count Data->Testing"
            ]
        ]
    },
    "514077": {
        "jupyter_code_cell": "knn['node-caps'].mode().values[0]\ndef knn_impute(data, k_neighbors=3):\n    for i, row in data.iterrows():\n        if row.isnull().any():\n            knn = impute_neighbors(row)\n            for i, v in row.iteritems():\n                if isinstance(v, float) and isnan(v):\n                    val = knn[i].mode().values[0]\n                    data.set_value(row.name, i, val)\nfin = knn_impute(data)",
        "matched_tutorial_code_inds": [
            6714,
            4905,
            3561,
            6422,
            6492
        ],
        "matched_tutorial_codes": [
            "make_plot(dta.index[idx[:5]])\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_24_0.png\" src=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_24_0.png\"/>",
            "viridis.colors [[0.267004 0.004874 0.329415 1.      ]\n [0.275191 0.194905 0.496005 1.      ]\n [0.212395 0.359683 0.55171  1.      ]\n [0.153364 0.497    0.557724 1.      ]\n [0.122312 0.633153 0.530398 1.      ]\n [0.288921 0.758394 0.428426 1.      ]\n [0.626579 0.854645 0.223353 1.      ]\n [0.993248 0.906157 0.143936 1.      ]]\nviridis(range(8)) [[0.267004 0.004874 0.329415 1.      ]\n [0.275191 0.194905 0.496005 1.      ]\n [0.212395 0.359683 0.55171  1.      ]\n [0.153364 0.497    0.557724 1.      ]\n [0.122312 0.633153 0.530398 1.      ]\n [0.288921 0.758394 0.428426 1.      ]\n [0.626579 0.854645 0.223353 1.      ]\n [0.993248 0.906157 0.143936 1.      ]]\nviridis(np.linspace(0, 1, 8)) [[0.267004 0.004874 0.329415 1.      ]\n [0.275191 0.194905 0.496005 1.      ]\n [0.212395 0.359683 0.55171  1.      ]\n [0.153364 0.497    0.557724 1.      ]\n [0.122312 0.633153 0.530398 1.      ]\n [0.288921 0.758394 0.428426 1.      ]\n [0.626579 0.854645 0.223353 1.      ]\n [0.993248 0.906157 0.143936 1.      ]]",
            "digits.images[0]\narray([[  0.,   0.,   5.,  13.,   9.,   1.,   0.,   0.],\n       [  0.,   0.,  13.,  15.,  10.,  15.,   5.,   0.],\n       [  0.,   3.,  15.,   2.,   0.,  11.,   8.,   0.],\n       [  0.,   4.,  12.,   0.,   0.,   8.,   8.,   0.],\n       [  0.,   5.,   8.,   0.,   0.,   9.,   8.,   0.],\n       [  0.,   4.,  11.,   0.,   1.,  12.,   7.,   0.],\n       [  0.,   2.,  14.,   5.,  10.,  12.,   0.,   0.],\n       [  0.,   0.,   6.,  13.,  10.,   0.,   0.,   0.]])",
            "dusphci = usphci.diff()[1:].values\ndef compute_coincident_index(mod, res):\n    # Estimate W(1)\n    spec = res.specification\n    design = mod.ssm['design']\n    transition = mod.ssm['transition']\n    ss_kalman_gain = res.filter_results.kalman_gain[:,:,-1]\n    k_states = ss_kalman_gain.shape[0]\n\n    W1 = np.linalg.inv(np.eye(k_states) - np.dot(\n        np.eye(k_states) - np.dot(ss_kalman_gain, design),\n        transition\n    )).dot(ss_kalman_gain)[0]\n\n    # Compute the factor mean vector\n    factor_mean = np.dot(W1, dta.loc['1972-02-01':, 'dln_indprod':'dln_emp'].mean())\n\n    # Normalize the factors\n    factor = res.factors.filtered[0]\n    factor *= np.std(usphci.diff()[1:]) / np.std(factor)\n\n    # Compute the coincident index\n    coincident_index = np.zeros(mod.nobs+1)\n    # The initial value is arbitrary; here it is set to\n    # facilitate comparison\n    coincident_index[0] = usphci.iloc[0] * factor_mean / dusphci.mean()\n    for t in range(0, mod.nobs):\n        coincident_index[t+1] = coincident_index[t] + factor[t] + factor_mean\n\n    # Attach dates\n    coincident_index = pd.Series(coincident_index, index=dta.index).iloc[1:]\n\n    # Normalize to use the same base year as USPHCI\n    coincident_index *= (usphci.loc['1992-07-01'] / coincident_index.loc['1992-07-01'])\n\n    return coincident_index",
            "time_s = np.s_[:50]  # After this they basically agree\nfig1 = plt.figure()\nax1 = fig1.add_subplot(111)\nidx = np.asarray(series.index)\nh1, = ax1.plot(idx[time_s], res_f.freq_seasonal[0].filtered[time_s], label='Double Freq. Seas')\nh2, = ax1.plot(idx[time_s], res_tf.seasonal.filtered[time_s], label='Mixed Domain Seas')\nh3, = ax1.plot(idx[time_s], true_seasonal_10_3[time_s], label='True Seasonal 10(3)')\nplt.legend([h1, h2, h3], ['Double Freq. Seasonal','Mixed Domain Seasonal','Truth'], loc=2)\nplt.title('Seasonal 10(3) component')\nplt.show()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_seasonal_21_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_seasonal_21_0.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "matplotlib->Tutorials->Colors->Creating Colormaps in Matplotlib->Getting colormaps and accessing their values->ListedColormap"
            ],
            [
                "sklearn->Tutorials->An introduction to machine learning with scikit-learn->Loading an example dataset"
            ],
            [
                "statsmodels->Examples->State space models->Dynamic Factor Models: Application->Coincident Index"
            ],
            [
                "statsmodels->Examples->State space models->Seasonality in Time Series Data->Comparison of filtered estimates"
            ]
        ]
    },
    "1271827": {
        "jupyter_code_cell": "columnObjectType = ['imdb_id', 'original_title', 'cast', 'homepage', 'director', 'tagline',\n                    'keywords','overview','genres','production_companies','release_date']\nfor i in columnObjectType:\n    print(type(df_mov[i][0]))\npd.set_option('display.float_format', lambda x: '%.2f' % x) \ndf_mov.describe()",
        "matched_tutorial_code_inds": [
            3698,
            4654,
            3266,
            1767,
            23
        ],
        "matched_tutorial_codes": [
            "files = glob.glob('weather/*.csv')\ncolumns = ['station', 'date', 'tmpf', 'relh', 'sped', 'mslp',\n           'p01i', 'vsby', 'gust_mph', 'skyc1', 'skyc2', 'skyc3']\n\n# init empty DataFrame, like you might for a list\nweather = pd.DataFrame(columns=columns)\n\nfor fp in files:\n    city = pd.read_csv(fp, columns=columns)\n    weather.append(city)",
            "data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>",
            "cvs = [, , ]\n\nfor cv in cvs:\n    fig, ax = (figsize=(6, 3))\n    plot_cv_indices(cv(n_splits), X, y, groups, ax, n_splits)\n    ax.legend(\n        [(color=cmap_cv(0.8)), (color=cmap_cv(0.02))],\n        [\"Testing set\", \"Training set\"],\n        loc=(1.02, 0.8),\n    )\n    # Make the legend fit\n    ()\n    fig.subplots_adjust(right=0.7)\n\n\n\n<img alt=\"StratifiedKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_003.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_003.png\"/>\n<img alt=\"GroupKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_004.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_004.png\"/>\n<img alt=\"StratifiedGroupKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_005.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_005.png\"/>",
            "x_axis = []\ndata = {'positive sentiment': [], 'negative sentiment': []}\nfor speaker in predictions:\n    # The speakers will be used to label the x-axis in our plot\n    x_axis.append(speaker)\n    # number of paras with positive sentiment\n    no_pos_paras = len(predictions[speaker]['pos_paras'])\n    # number of paras with negative sentiment\n    no_neg_paras = len(predictions[speaker]['neg_paras'])\n    # Obtain percentage of paragraphs with positive predicted sentiment\n    pos_perc = no_pos_paras / (no_pos_paras + no_neg_paras)\n    # Store positive and negative percentages\n    data['positive sentiment'].append(pos_perc*100)\n    data['negative sentiment'].append(100*(1-pos_perc))\n\nindex = pd.Index(x_axis, name='speaker')\ndf = pd.DataFrame(data, index=index)\nax = df.plot(kind='bar', stacked=True)\nax.set_ylabel('percentage')\nax.legend(title='labels', bbox_to_anchor=(1, 1), loc='upper left')\nplt.show()",
            "face_dataset = FaceLandmarksDataset(csv_file='faces/face_landmarks.csv',\n                                    root_dir='faces/')\n\nfig = plt.figure()\n\nfor i in range(len(face_dataset)):\n    sample = face_dataset[i]\n\n    print(i, sample['image'].shape, sample['landmarks'].shape)\n\n    ax = plt.subplot(1, 4, i + 1)\n    plt.tight_layout()\n    ax.set_title('Sample #{}'.format(i))\n    ax.axis('off')\n    show_landmarks(**sample)\n\n    if i == 3:\n        plt.show()\n        break"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings"
            ],
            [
                "sklearn->Examples->Model Selection->Visualizing cross-validation behavior in scikit-learn->Define a function to visualize cross-validation behavior"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Sentiment Analysis on the Speech Data"
            ],
            [
                "torch->PyTorch Recipes->Developing Custom PyTorch Dataloaders->Part 1: The Dataset->1.3 Iterate through data samples"
            ]
        ]
    },
    "692862": {
        "jupyter_code_cell": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom __future__ import division\nfrom IPython.display import HTML\nHTML('''<script>\ncode_show=true; \nfunction code_toggle() {\n if (code_show){\n $('div.input').hide();\n } else {\n $('div.input').show();\n }\n code_show = !code_show\n} \n$( document ).ready(code_toggle);\n</script>\n<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')",
        "matched_tutorial_code_inds": [
            4700,
            3384,
            4895,
            3284,
            3119
        ],
        "matched_tutorial_codes": [
            "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom cycler import cycler\nmpl.rcParams['lines.linewidth'] = 2\nmpl.rcParams['lines.linestyle'] = '--'\ndata = np.random.randn(50)\nplt.plot(data)\n\n\n<img alt=\"customizing\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_customizing_001.png\" srcset=\"../../_images/sphx_glr_customizing_001.png, ../../_images/sphx_glr_customizing_001_2_0x.png 2.0x\"/>",
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import \nfrom sklearn.model_selection import \nfrom sklearn.pipeline import \nfrom sklearn.svm import \nfrom sklearn.decomposition import , \nfrom sklearn.feature_selection import , \nfrom sklearn.preprocessing import \n\nX, y = (return_X_y=True)\n\npipe = (\n    [\n        (\"scaling\", ()),\n        # the reduce_dim stage is populated by the param_grid\n        (\"reduce_dim\", \"passthrough\"),\n        (\"classify\", (dual=False, max_iter=10000)),\n    ]\n)\n\nN_FEATURES_OPTIONS = [2, 4, 8]\nC_OPTIONS = [1, 10, 100, 1000]\nparam_grid = [\n    {\n        \"reduce_dim\": [(iterated_power=7), (max_iter=1_000)],\n        \"reduce_dim__n_components\": N_FEATURES_OPTIONS,\n        \"classify__C\": C_OPTIONS,\n    },\n    {\n        \"reduce_dim\": [()],\n        \"reduce_dim__k\": N_FEATURES_OPTIONS,\n        \"classify__C\": C_OPTIONS,\n    },\n]\nreducer_labels = [\"PCA\", \"NMF\", \"KBest(mutual_info_classif)\"]\n\ngrid = (pipe, n_jobs=1, param_grid=param_grid)\ngrid.fit(X, y)",
            "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nth = np.linspace(0, 2*np.pi, 128)\n\n\ndef demo(sty):\n    mpl.style.use(sty)\n    fig, ax = plt.subplots(figsize=(3, 3))\n\n    ax.set_title('style: {!r}'.format(sty), color='C0')\n\n    ax.plot(th, np.cos(th), 'C1', label='C1')\n    ax.plot(th, np.sin(th), 'C2', label='C2')\n    ax.legend()\n\n\ndemo('default')\ndemo('seaborn-v0_8')\n\n\n\n<img alt=\"style: 'default'\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_colors_002.png\" srcset=\"../../_images/sphx_glr_colors_002.png, ../../_images/sphx_glr_colors_002_2_0x.png 2.0x\"/>\n<img alt=\"style: 'seaborn-v0_8'\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_colors_003.png\" srcset=\"../../_images/sphx_glr_colors_003.png, ../../_images/sphx_glr_colors_003_2_0x.png 2.0x\"/>",
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import \nfrom sklearn import datasets\nfrom sklearn.neighbors import \nfrom sklearn.inspection import DecisionBoundaryDisplay\n\nn_neighbors = 15\n\n# import some data to play with\niris = ()\n# we only take the first two features. We could avoid this ugly\n# slicing by using a two-dim dataset\nX = iris.data[:, :2]\ny = iris.target\n\n# Create color maps\ncmap_light = ([\"orange\", \"cyan\", \"cornflowerblue\"])\ncmap_bold = ([\"darkorange\", \"c\", \"darkblue\"])\n\nfor shrinkage in [None, 0.2]:\n    # we create an instance of Neighbours Classifier and fit the data.\n    clf = (shrink_threshold=shrinkage)\n    clf.fit(X, y)\n    y_pred = clf.predict(X)\n    print(shrinkage, (y == y_pred))\n\n    _, ax = ()\n    (\n        clf, X, cmap=cmap_light, ax=ax, response_method=\"predict\"\n    )\n\n    # Plot also the training points\n    (X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor=\"k\", s=20)\n    (\"3-Class classification (shrink_threshold=%r)\" % shrinkage)\n    (\"tight\")\n\n()",
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom collections import \n\npopulations = (list)\ncommon_params = {\n    \"n_samples\": 10_000,\n    \"n_features\": 2,\n    \"n_informative\": 2,\n    \"n_redundant\": 0,\n    \"random_state\": 0,\n}\nweights = (0.1, 0.8, 6)\nweights = weights[::-1]\n\n# fit and evaluate base model on balanced classes\nX, y = (**common_params, weights=[0.5, 0.5])\nestimator = ().fit(X, y)\nlr_base = extract_score((estimator, X, y, scoring=scoring, cv=10))\npos_lr_base, pos_lr_base_std = lr_base[\"positive\"].values\nneg_lr_base, neg_lr_base_std = lr_base[\"negative\"].values"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Introductory->Customizing Matplotlib with style sheets and rcParams->Runtime rc settings"
            ],
            [
                "sklearn->Examples->Pipelines and composite estimators->Selecting dimensionality reduction with Pipeline and GridSearchCV->Illustration of Pipeline and GridSearchCV"
            ],
            [
                "matplotlib->Tutorials->Colors->Specifying colors->\"CN\" color selection"
            ],
            [
                "sklearn->Examples->Nearest Neighbors->Nearest Centroid Classification"
            ],
            [
                "sklearn->Examples->Model Selection->Class Likelihood Ratios to measure classification performance->Invariance with respect to prevalence"
            ]
        ]
    },
    "140574": {
        "jupyter_code_cell": "table = your_name_history('John')\ntable.plot()\nyearly_data.plot(kind=\"bar\")\nplt.axis('off')",
        "matched_tutorial_code_inds": [
            4067,
            3923,
            5703,
            6061,
            5423
        ],
        "matched_tutorial_codes": [
            "tips = sns.load_dataset(\"tips\")\nsns.catplot(data=tips, x=\"day\", y=\"total_bill\")\n",
            "tips = sns.load_dataset(\"tips\")\ng = sns.relplot(data=tips, x=\"total_bill\", y=\"tip\")\ng.ax.axline(xy1=(10, 2), slope=.2, color=\"b\", dashes=(5, 2))\n",
            "summ_df = infl.summary_frame()\nsumm_df.sort_values(\"cooks_d\", ascending=False)[:10]",
            "mod = AutoReg(housing, 3, old_names=False)\nres = mod.fit()\nprint(res.summary())",
            "mfx = affair_mod.get_margeff()\nprint(mfx.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Visualizing categorical data->Categorical scatterplots",
                "seaborn->Plotting functions->Visualizing categorical data->Categorical scatterplots"
            ],
            [
                "seaborn->User guide and tutorial->Overview of seaborn plotting functions->Figure-level vs. axes-level functions->Figure-level functions own their figure",
                "seaborn->API Overview->Overview of seaborn plotting functions->Figure-level vs. axes-level functions->Figure-level functions own their figure"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Influence Measures for GLMs->get the influence measures"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data"
            ]
        ]
    },
    "1192209": {
        "jupyter_code_cell": "monthly_max = df_clean[['dew_point_faren', 'dry_bulb_faren']].resample('M').max()\nmonthly_max.plot(kind = 'hist', bins = 8, alpha = 0.5, subplots = True)\nplt.show()\naugust_max = df_climate.loc['2010-Aug', 'Temperature'].max()\nprint(august_max)\naugust_2011 = df_clean.loc['2011-Aug', 'dry_bulb_faren'].resample('D').max()\naugust_2011_high = august_2011[august_2011 > august_max]\naugust_2011_high.plot(kind = 'hist', bins = 25, normed = True, cumulative = True)\nplt.show()",
        "matched_tutorial_code_inds": [
            6337,
            1523,
            5956,
            5917,
            6700
        ],
        "matched_tutorial_codes": [
            "intercept = [\n    ar0_res.params[0],\n    sarimax_res.params[0],\n    arima_res.params[0],\n    autoreg_res.params[0],\n]\nrho_hat = [0] + [r.params[1] for r in (sarimax_res, arima_res, autoreg_res)]\nlong_run = [\n    ar0_res.params[0],\n    sarimax_res.params[0] / (1 - sarimax_res.params[1]),\n    arima_res.params[0],\n    autoreg_res.params[0] / (1 - autoreg_res.params[1]),\n]\ncols = [\"AR(0)\", \"SARIMAX\", \"ARIMA\", \"AutoReg\"]\npd.DataFrame(\n    [intercept, rho_hat, long_run],\n    columns=cols,\n    index=[\"delta-or-phi\", \"rho\", \"long-run mean\"],\n)",
            "transistor_count_predicted = np.exp(B) * np.exp(A * year)\ntransistor_Moores_law = Moores_law(year)\nplt.style.use(\"fivethirtyeight\")\nplt.semilogy(year, transistor_count, \"s\", label=\"MOS transistor count\")\nplt.semilogy(year, transistor_count_predicted, label=\"linear regression\")\n\n\nplt.plot(year, transistor_Moores_law, label=\"Moore's Law\")\nplt.title(\n    \"MOS transistor count per microprocessor\\n\"\n    + \"every two years \\n\"\n    + \"Transistor count was x{:.2f} higher\".format(np.exp(A * 2))\n)\nplt.xlabel(\"year introduced\")\nplt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\nplt.ylabel(\"# of transistors\\nper microprocessor\")",
            "kidney_lm = ols(\"np.log(Days+1) ~ C(Duration) * C(Weight)\", data=kt).fit()\n\ntable10 = anova_lm(kidney_lm)\n\nprint(\n    anova_lm(ols(\"np.log(Days+1) ~ C(Duration) + C(Weight)\", data=kt).fit(), kidney_lm)\n)\nprint(\n    anova_lm(\n        ols(\"np.log(Days+1) ~ C(Duration)\", data=kt).fit(),\n        ols(\"np.log(Days+1) ~ C(Duration) + C(Weight, Sum)\", data=kt).fit(),\n    )\n)\nprint(\n    anova_lm(\n        ols(\"np.log(Days+1) ~ C(Weight)\", data=kt).fit(),\n        ols(\"np.log(Days+1) ~ C(Duration) + C(Weight, Sum)\", data=kt).fit(),\n    )\n)",
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "ln_pce = np.log(pce.PCE)\nforecasts = {\"ln PCE\": ln_pce}\nfor year in range(1995, 2020, 3):\n    sub = ln_pce[: str(year)]\n    res = ThetaModel(sub).fit()\n    fcast = res.forecast(12)\n    forecasts[str(year)] = fcast\nforecasts = pd.DataFrame(forecasts)\nax = forecasts[\"1995\":].plot(legend=False)\nchildren = ax.get_children()\nchildren[0].set_linewidth(4)\nchildren[0].set_alpha(0.3)\nchildren[0].set_color(\"#000000\")\nax.set_title(\"ln PCE\")\nplt.tight_layout(pad=1.0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_theta-model_22_0.png\" src=\"../../../_images/examples_notebooks_generated_theta-model_22_0.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Comparing trends and exogenous variables in SARIMAX, ARIMA and AutoReg"
            ],
            [
                "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy->Calculating the historical growth curve for transistors"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA->Two-way ANOVA"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Personal Consumption Expenditure"
            ]
        ]
    },
    "453480": {
        "jupyter_code_cell": "df.sort_values('AVG_ATTENDANCE')\nteam_wins = df.groupby('WINNING_TEAM').count()['YEAR'].to_frame().reset_index()\nteam_wins.columns = ['team', 'wins']\nteam_wins",
        "matched_tutorial_code_inds": [
            3610,
            3827,
            3608,
            5703,
            3751
        ],
        "matched_tutorial_codes": [
            "first = df.groupby('unique_carrier').first()\nfirst.ix[10:15, ['fl_date', 'tail_num']]",
            "daily = df.fl_date.value_counts().sort_index()\ny = daily.resample('MS').mean()\ny.head()",
            "first = df.groupby('airline_id')[['fl_date', 'unique_carrier']].first()\nfirst.head()",
            "summ_df = infl.summary_frame()\nsumm_df.sort_values(\"cooks_d\", ascending=False)[:10]",
            "tidy['rest'] = tidy.sort_values('date').groupby('team').date.diff().dt.days - 1\ntidy.dropna().head()"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Influence Measures for GLMs->get the influence measures"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data"
            ]
        ]
    },
    "593940": {
        "jupyter_code_cell": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\ntest = pd.read_csv('C:\\Users\\gydoy\\Documents\\EE379K\\KaggleMidterm\\\\test_final.csv')\ntrain = pd.read_csv('C:\\Users\\gydoy\\Documents\\EE379K\\KaggleMidterm\\\\train_final.csv')\ntrain_Y = train['Y']\ntrain_X = train.drop(['id', 'Y', 'F25', 'F26', 'F27'], axis=1)\ntest_X = test.drop(['id'], axis=1)\nprint(\"done\")\nfrom pandas.plotting import scatter_matrix\nfig = plt.figure()\nscatter_matrix(train_X, alpha=0.2, figsize=(24, 24), diagonal='kde')\nplt.show()",
        "matched_tutorial_code_inds": [
            2765,
            4700,
            5149,
            2517,
            3410
        ],
        "matched_tutorial_codes": [
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import \nfrom sklearn.datasets import \n\n# we create 50 separable points\nX, Y = (n_samples=50, centers=2, random_state=0, cluster_std=0.60)\n\n# fit the model\nclf = (loss=\"hinge\", alpha=0.01, max_iter=200)\n\nclf.fit(X, Y)\n\n# plot the line, the points, and the nearest vectors to the plane\nxx = (-1, 5, 10)\nyy = (-1, 5, 10)\n\nX1, X2 = (xx, yy)\nZ = (X1.shape)\nfor (i, j), val in (X1):\n    x1 = val\n    x2 = X2[i, j]\n    p = clf.decision_function([[x1, x2]])\n    Z[i, j] = p[0]\nlevels = [-1.0, 0.0, 1.0]\nlinestyles = [\"dashed\", \"solid\", \"dashed\"]\ncolors = \"k\"\n(X1, X2, Z, levels, colors=colors, linestyles=linestyles)\n(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired, edgecolor=\"black\", s=20)\n\n(\"tight\")\n()",
            "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom cycler import cycler\nmpl.rcParams['lines.linewidth'] = 2\nmpl.rcParams['lines.linestyle'] = '--'\ndata = np.random.randn(50)\nplt.plot(data)\n\n\n<img alt=\"customizing\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_customizing_001.png\" srcset=\"../../_images/sphx_glr_customizing_001.png, ../../_images/sphx_glr_customizing_001_2_0x.png 2.0x\"/>",
            "import pyarrow as pa\nimport pyarrow.parquet as pq\nimport statsmodels.formula.api as smf\n\nclass DataSet(dict):\n    def __init__(self, path):\n        self.parquet = pq.ParquetFile(path)\n\n    def __getitem__(self, key):\n        try:\n            return self.parquet.read([key]).to_pandas()[key]\n        except:\n            raise KeyError\n\nLargeData = DataSet('LargeData.parquet')\n\nres = smf.ols('Profit ~ Sugar + Power + Women', data=LargeData).fit()",
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import \nfrom sklearn import mixture\n\nn_samples = 300\n\n# generate random sample, two components\n(0)\n\n# generate spherical data centered on (20, 20)\nshifted_gaussian = (n_samples, 2) + ([20, 20])\n\n# generate zero centered stretched Gaussian data\nC = ([[0.0, -0.7], [3.5, 0.7]])\nstretched_gaussian = ((n_samples, 2), C)\n\n# concatenate the two datasets into the final training set\nX_train = ([shifted_gaussian, stretched_gaussian])\n\n# fit a Gaussian Mixture Model with two components\nclf = (n_components=2, covariance_type=\"full\")\nclf.fit(X_train)\n\n# display predicted scores by the model as a contour plot\nx = (-20.0, 30.0)\ny = (-20.0, 40.0)\nX, Y = (x, y)\nXX = ([X.ravel(), Y.ravel()]).T\nZ = -clf.score_samples(XX)\nZ = Z.reshape(X.shape)\n\nCS = (\n    X, Y, Z, norm=(vmin=1.0, vmax=1000.0), levels=(0, 3, 10)\n)\nCB = (CS, shrink=0.8, extend=\"both\")\n(X_train[:, 0], X_train[:, 1], 0.8)\n\n(\"Negative log-likelihood predicted by a GMM\")\n(\"tight\")\n()",
            "import pandas as pd\nfrom sklearn.decomposition import \n\npca = (n_components=2).fit(X_train)\nscaled_pca = (n_components=2).fit(scaled_X_train)\nX_train_transformed = pca.transform(X_train)\nX_train_std_transformed = scaled_pca.transform(scaled_X_train)\n\nfirst_pca_component = (\n    pca.components_[0], index=X.columns, columns=[\"without scaling\"]\n)\nfirst_pca_component[\"with scaling\"] = scaled_pca.components_[0]\nfirst_pca_component.plot.bar(\n    title=\"Weights of the first principal component\", figsize=(6, 8)\n)\n\n_ = ()\n\n\n<img alt=\"Weights of the first principal component\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_scaling_importance_002.png\" srcset=\"../../_images/sphx_glr_plot_scaling_importance_002.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Generalized Linear Models->SGD: Maximum margin separating hyperplane"
            ],
            [
                "matplotlib->Tutorials->Introductory->Customizing Matplotlib with style sheets and rcParams->Runtime rc settings"
            ],
            [
                "statsmodels->User Guide->Statistics and Tools->Working with Large Data Sets->Subsetting your data"
            ],
            [
                "sklearn->Examples->Gaussian Mixture Models->Density Estimation for a Gaussian mixture"
            ],
            [
                "sklearn->Examples->Preprocessing->Importance of Feature Scaling->Effect of rescaling on a PCA dimensional reduction"
            ]
        ]
    },
    "797747": {
        "jupyter_code_cell": "names = pandas.read_csv('~/palaeovalleys/case_study_sites_small.csv', delimiter = ',')\nprint(names)\nnum = 48\nStudysite = names.ix[num]\nprint(Studysite)",
        "matched_tutorial_code_inds": [
            6065,
            1746,
            6076,
            5399,
            5588
        ],
        "matched_tutorial_codes": [
            "sel = ar_select_order(housing, 13, old_names=False)\nsel.ar_lags\nres = sel.model.fit()\nprint(res.summary())",
            "glove = data.fetch('glove.6B.50d.zip')\nemb_path = textproc.unzipper(glove, 'glove.6B.300d.txt')\nemb_matrix = textproc.loadGloveModel(emb_path)",
            "sel = ar_select_order(yoy_housing, 13, glob=True, old_names=False)\nsel.ar_lags\nres = sel.model.fit()\nprint(res.summary())",
            "rand_data = sm.datasets.randhie.load()\nrand_exog = rand_data.exog\nrand_exog = sm.add_constant(rand_exog, prepend=False)",
            "data2 = sm.datasets.scotland.load()\ndata2.exog = sm.add_constant(data2.exog, prepend=False)\nprint(data2.exog.head())\nprint(data2.endog.head())"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions->Seasonal Dynamics"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Poisson"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Generalized Linear Models Overview->GLM: Gamma for proportional count response->Load Scottish Parliament Voting data"
            ]
        ]
    },
    "1151276": {
        "jupyter_code_cell": "forecast_score('Napa', napa_model_fit)\nsf_train_log = np.log(sf_train)\nsf_train_resid = decomposing(sf_train_log)\nsf_train_p = 2\nsf_train_q = 2\nsf_model_fit = arima_summary(sf_train_log, sf_train_resid, sf_train_p, 2, sf_train_q)",
        "matched_tutorial_code_inds": [
            2460,
            1244,
            3474,
            2446,
            2463
        ],
        "matched_tutorial_codes": [
            "one_hot_poly_pipeline = (\n    (\n        transformers=[\n            (\"categorical\", one_hot_encoder, categorical_columns),\n            (\"one_hot_time\", one_hot_encoder, [\"hour\", \"weekday\", \"month\"]),\n        ],\n        remainder=\"passthrough\",\n    ),\n    (kernel=\"poly\", degree=2, n_components=300, random_state=0),\n    (alphas=alphas),\n)\nevaluate(one_hot_poly_pipeline, X, y, cv=ts_cv)",
            "my_auto_wrap_policy = functools.partial(\n        size_based_auto_wrap_policy, min_num_params=20000\n    )\ntorch.cuda.set_device(rank)\nmodel = Net().to(rank)\n\nmodel = FSDP(model,\n    fsdp_auto_wrap_policy=my_auto_wrap_policy)",
            "model_l2 = (penalty=\"l2\", loss=\"squared_hinge\", dual=True)\nCs = (-4.5, -2, 10)\n\nlabels = [f\"fraction: {train_size}\" for train_size in train_sizes]\nresults = {\"C\": Cs}\nfor label, train_size in zip(labels, train_sizes):\n    cv = (train_size=train_size, test_size=0.3, n_splits=50, random_state=1)\n    train_scores, test_scores = (\n        model_l2, X, y, param_name=\"C\", param_range=Cs, cv=cv\n    )\n    results[label] = test_scores.mean(axis=1)\nresults = (results)",
            "last_hours = slice(-96, None)\nfig, ax = (figsize=(12, 4))\nfig.suptitle(\"Predictions by linear models\")\nax.plot(\n    y.iloc[test_0].values[last_hours],\n    \"x-\",\n    alpha=0.2,\n    label=\"Actual demand\",\n    color=\"black\",\n)\nax.plot(naive_linear_predictions[last_hours], \"x-\", label=\"Ordinal time features\")\nax.plot(\n    cyclic_cossin_linear_predictions[last_hours],\n    \"x-\",\n    label=\"Trigonometric time features\",\n)\nax.plot(\n    cyclic_spline_linear_predictions[last_hours],\n    \"x-\",\n    label=\"Spline-based time features\",\n)\nax.plot(\n    one_hot_linear_predictions[last_hours],\n    \"x-\",\n    label=\"One-hot time features\",\n)\n_ = ax.legend()\n\n\n<img alt=\"Predictions by linear models\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_006.png\" srcset=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_006.png\"/>",
            "last_hours = slice(-96, None)\nfig, ax = (figsize=(12, 4))\nfig.suptitle(\"Predictions by non-linear regression models\")\nax.plot(\n    y.iloc[test_0].values[last_hours],\n    \"x-\",\n    alpha=0.2,\n    label=\"Actual demand\",\n    color=\"black\",\n)\nax.plot(\n    gbrt_predictions[last_hours],\n    \"x-\",\n    label=\"Gradient Boosted Trees\",\n)\nax.plot(\n    one_hot_poly_predictions[last_hours],\n    \"x-\",\n    label=\"One-hot + polynomial kernel\",\n)\nax.plot(\n    cyclic_spline_poly_predictions[last_hours],\n    \"x-\",\n    label=\"Splines + polynomial kernel\",\n)\n_ = ax.legend()\n\n\n<img alt=\"Predictions by non-linear regression models\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_007.png\" srcset=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_007.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Modeling non-linear feature interactions with kernels"
            ],
            [
                "torch->Parallel and Distributed Training->Getting Started with Fully Sharded Data Parallel(FSDP)->How to use FSDP"
            ],
            [
                "sklearn->Examples->Support Vector Machines->Scaling the regularization parameter for SVCs->L2-penalty case"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Qualitative analysis of the impact of features on linear model predictions"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Modeling non-linear feature interactions with kernels"
            ]
        ]
    },
    "621120": {
        "jupyter_code_cell": "salary = pd.read_csv('salary.csv')\nsalary.head()\nsalary.tail()\nsalary.rename(columns = {'2016-17':'Salary'}, inplace = True)\nclean_salaries = salary[['Player','Salary']]\nclean_salaries.head()\nclean_salaries.tail()",
        "matched_tutorial_code_inds": [
            6061,
            3889,
            3861,
            6704,
            6659
        ],
        "matched_tutorial_codes": [
            "mod = AutoReg(housing, 3, old_names=False)\nres = mod.fit()\nprint(res.summary())",
            "daily = (\n    indiv[['transaction_dt', 'transaction_amt']].dropna()\n        .set_index('transaction_dt')['transaction_amt']\n        .resample(\"D\")\n        .sum()\n).compute()\ndaily",
            "df = dd.read_parquet(\"data/indiv-*.parquet\", engine='pyarrow', columns=['occupation'])\n\nmost_common = df.occupation.value_counts().nlargest(100)\nmost_common.compute().sort_values(ascending=False)",
            "ax = dta.mean().plot(grid=False)\nax.set_xlabel(\"Year\", size=17)\nax.set_ylabel(\"Fertility rate\", size=17)\nax.set_xlim(0, 51)",
            "mod = LocalLevel(dta.infl)\nres = mod.fit(disp=False)\nprint(res.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions"
            ],
            [
                "pandas_toms_blog->Scaling->Dask"
            ],
            [
                "pandas_toms_blog->Scaling->Using Dask"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Typical approach"
            ]
        ]
    },
    "1439804": {
        "jupyter_code_cell": "net.set(parcels.node_id_walk, variable = parcels.total_du)\nprint('ok')\nresults = net.aggregate(1000, type='sum', decay='linear')\nprint('ok')",
        "matched_tutorial_code_inds": [
            6085,
            6083,
            6076,
            5799,
            5818
        ],
        "matched_tutorial_codes": [
            "sel = ar_select_order(ind_prod, 13, \"bic\", glob=True, old_names=False)\nsel.ar_lags\nres_glob = sel.model.fit()\nprint(res.summary())",
            "sel = ar_select_order(ind_prod, 13, \"bic\", old_names=False)\nres = sel.model.fit()\nprint(res.summary())",
            "sel = ar_select_order(yoy_housing, 13, glob=True, old_names=False)\nsel.ar_lags\nres = sel.model.fit()\nprint(res.summary())",
            "sidak = ols_model.outlier_test(\"sidak\")\nsidak.sort_values(\"unadj_p\", inplace=True)\nprint(sidak)",
            "sidak2 = ols_model.outlier_test(\"sidak\")\nsidak2.sort_values(\"unadj_p\", inplace=True)\nprint(sidak2)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions->Industrial Production"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions->Industrial Production"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions->Seasonal Dynamics"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Duncan\u2019s Occupational Prestige data - M-estimation for outliers"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Hertzprung Russell data for Star Cluster CYG 0B1 - Leverage Points"
            ]
        ]
    },
    "995072": {
        "jupyter_code_cell": "from sklearn import metrics\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7) \nestimator = LogisticRegression() \nestimator.fit(X_train, y_train)\npredicted = estimator.predict(X_test)\nfrom sklearn.cross_validation import cross_val_score\naccuracy = cross_val_score(estimator, X_test, y_test, cv=10,scoring='accuracy')\nprint ('accuracy', accuracy, '\\n')\nprint ('mean of scores =', np.mean(accuracy), '\\n')\nfrom nltk import ConfusionMatrix \nprint ('ConfusionMatrix \\n', ConfusionMatrix(list(y_test), list(predicted))) \nprint ('Sensitivity \\n', metrics.recall_score(y_test, predicted, pos_label=1)) \nimport matplotlib.pyplot as plt \nprobs = estimator.predict_proba(X_test)[:, 1] \nplt.hist(probs) \nplt.show()\nprint ('accuracy score =', metrics.accuracy_score(y_test, predicted)) \nfig, ax = plt.subplots(figsize=(12,7))\nfpr, tpr, thresholds = metrics.roc_curve(y_test, probs, pos_label=1) \nplt.plot(fpr, tpr)\nplt.plot([0, 1], [0, 1], color='grey', ls='dashed',\n             alpha=0.9, linewidth=4)\nplt.xlim([0.0, 1.01]) \nplt.ylim([0.0, 1.01]) \nplt.xlabel('False Positive Rate') \nplt.ylabel('True Positive Rate)') \nplt.show()\nprint('Baseline =', np.sum((y_test == 0)/ len(y_test)))\nprint ('auc =',metrics.roc_auc_score(y_test, probs))\ntp = np.sum((y_test == 1) & (predicted == 1))\nfp = np.sum((y_test == 0) & (predicted == 1))\ntn = np.sum((y_test == 0) & (predicted == 0))\nfn = np.sum((y_test == 1) & (predicted == 0))\nprint(tn, fp, fn, tp)",
        "matched_tutorial_code_inds": [
            3178,
            2787,
            1893,
            2012,
            1823
        ],
        "matched_tutorial_codes": [
            "from sklearn.datasets import \nfrom matplotlib import pyplot as plt\nfrom sklearn.svm import \nfrom sklearn.model_selection import , , \nimport numpy as np\n\n# Number of random trials\nNUM_TRIALS = 30\n\n# Load the dataset\niris = ()\nX_iris = iris.data\ny_iris = iris.target\n\n# Set up possible values of parameters to optimize over\np_grid = {\"C\": [1, 10, 100], \"gamma\": [0.01, 0.1]}\n\n# We will use a Support Vector Classifier with \"rbf\" kernel\nsvm = (kernel=\"rbf\")\n\n# Arrays to store scores\nnon_nested_scores = (NUM_TRIALS)\nnested_scores = (NUM_TRIALS)\n\n# Loop for each trial\nfor i in range(NUM_TRIALS):\n\n    # Choose cross-validation techniques for the inner and outer loops,\n    # independently of the dataset.\n    # E.g \"GroupKFold\", \"LeaveOneOut\", \"LeaveOneGroupOut\", etc.\n    inner_cv = (n_splits=4, shuffle=True, random_state=i)\n    outer_cv = (n_splits=4, shuffle=True, random_state=i)\n\n    # Non_nested parameter search and scoring\n    clf = (estimator=svm, param_grid=p_grid, cv=outer_cv)\n    clf.fit(X_iris, y_iris)\n    non_nested_scores[i] = clf.best_score_\n\n    # Nested CV with parameter optimization\n    clf = (estimator=svm, param_grid=p_grid, cv=inner_cv)\n    nested_score = (clf, X=X_iris, y=y_iris, cv=outer_cv)\n    nested_scores[i] = nested_score.mean()\n\nscore_difference = non_nested_scores - nested_scores\n\nprint(\n    \"Average difference of {:6f} with std. dev. of {:6f}.\".format(\n        score_difference.mean(), score_difference.std()\n    )\n)\n\n# Plot scores on each trial for nested and non-nested CV\n()\n(211)\n(non_nested_scores_line,) = (non_nested_scores, color=\"r\")\n(nested_line,) = (nested_scores, color=\"b\")\n(\"score\", fontsize=\"14\")\n(\n    [non_nested_scores_line, nested_line],\n    [\"Non-Nested CV\", \"Nested CV\"],\n    bbox_to_anchor=(0, 0.4, 0.5, 0),\n)\n(\n    \"Non-Nested and Nested Cross Validation on Iris Dataset\",\n    x=0.5,\n    y=1.1,\n    fontsize=\"15\",\n)\n\n# Plot bar chart of the difference.\n(212)\ndifference_plot = (range(NUM_TRIALS), score_difference)\n(\"Individual Trial #\")\n(\n    [difference_plot],\n    [\"Non-Nested CV - Nested CV Score\"],\n    bbox_to_anchor=(0, 1, 0.8, 0),\n)\n(\"score difference\", fontsize=\"14\")\n\n()",
            "from sklearn.linear_model import \n\n\nmask_train = df_train[\"ClaimAmount\"]  0\nmask_test = df_test[\"ClaimAmount\"]  0\n\nglm_sev = (alpha=10.0, solver=\"newton-cholesky\")\n\nglm_sev.fit(\n    X_train[mask_train.values],\n    df_train.loc[mask_train, \"AvgClaimAmount\"],\n    sample_weight=df_train.loc[mask_train, \"ClaimNb\"],\n)\n\nscores = score_estimator(\n    glm_sev,\n    X_train[mask_train.values],\n    X_test[mask_test.values],\n    df_train[mask_train],\n    df_test[mask_test],\n    target=\"AvgClaimAmount\",\n    weights=\"ClaimNb\",\n)\nprint(\"Evaluation of GammaRegressor on target AvgClaimAmount\")\nprint(scores)",
            "from sklearn.calibration import \nfrom sklearn.metrics import \nfrom sklearn.naive_bayes import \n\n# With no calibration\nclf = ()\nclf.fit(X_train, y_train)  # GaussianNB itself does not support sample-weights\nprob_pos_clf = clf.predict_proba(X_test)[:, 1]\n\n# With isotonic calibration\nclf_isotonic = (clf, cv=2, method=\"isotonic\")\nclf_isotonic.fit(X_train, y_train, sample_weight=sw_train)\nprob_pos_isotonic = clf_isotonic.predict_proba(X_test)[:, 1]\n\n# With sigmoid calibration\nclf_sigmoid = (clf, cv=2, method=\"sigmoid\")\nclf_sigmoid.fit(X_train, y_train, sample_weight=sw_train)\nprob_pos_sigmoid = clf_sigmoid.predict_proba(X_test)[:, 1]\n\nprint(\"Brier score losses: (the smaller the better)\")\n\nclf_score = (y_test, prob_pos_clf, sample_weight=sw_test)\nprint(\"No calibration: %1.3f\" % clf_score)\n\nclf_isotonic_score = (y_test, prob_pos_isotonic, sample_weight=sw_test)\nprint(\"With isotonic calibration: %1.3f\" % clf_isotonic_score)\n\nclf_sigmoid_score = (y_test, prob_pos_sigmoid, sample_weight=sw_test)\nprint(\"With sigmoid calibration: %1.3f\" % clf_sigmoid_score)",
            "from sklearn.datasets import \nfrom sklearn.cluster import \nfrom sklearn.metrics import , \n\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport numpy as np\n\n# Generating the sample data from make_blobs\n# This particular setting has one distinct cluster and 3 clusters placed close\n# together.\nX, y = (\n    n_samples=500,\n    n_features=2,\n    centers=4,\n    cluster_std=1,\n    center_box=(-10.0, 10.0),\n    shuffle=True,\n    random_state=1,\n)  # For reproducibility\n\nrange_n_clusters = [2, 3, 4, 5, 6]\n\nfor n_clusters in range_n_clusters:\n    # Create a subplot with 1 row and 2 columns\n    fig, (ax1, ax2) = (1, 2)\n    fig.set_size_inches(18, 7)\n\n    # The 1st subplot is the silhouette plot\n    # The silhouette coefficient can range from -1, 1 but in this example all\n    # lie within [-0.1, 1]\n    ax1.set_xlim([-0.1, 1])\n    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n    # plots of individual clusters, to demarcate them clearly.\n    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n\n    # Initialize the clusterer with n_clusters value and a random generator\n    # seed of 10 for reproducibility.\n    clusterer = (n_clusters=n_clusters, n_init=\"auto\", random_state=10)\n    cluster_labels = clusterer.fit_predict(X)\n\n    # The silhouette_score gives the average value for all the samples.\n    # This gives a perspective into the density and separation of the formed\n    # clusters\n    silhouette_avg = (X, cluster_labels)\n    print(\n        \"For n_clusters =\",\n        n_clusters,\n        \"The average silhouette_score is :\",\n        silhouette_avg,\n    )\n\n    # Compute the silhouette scores for each sample\n    sample_silhouette_values = (X, cluster_labels)\n\n    y_lower = 10\n    for i in range(n_clusters):\n        # Aggregate the silhouette scores for samples belonging to\n        # cluster i, and sort them\n        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        color = cm.nipy_spectral(float(i) / n_clusters)\n        ax1.fill_betweenx(\n            (y_lower, y_upper),\n            0,\n            ith_cluster_silhouette_values,\n            facecolor=color,\n            edgecolor=color,\n            alpha=0.7,\n        )\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title(\"The silhouette plot for the various clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\n    # 2nd Plot showing the actual clusters formed\n    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n    ax2.scatter(\n        X[:, 0], X[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n    )\n\n    # Labeling the clusters\n    centers = clusterer.cluster_centers_\n    # Draw white circles at cluster centers\n    ax2.scatter(\n        centers[:, 0],\n        centers[:, 1],\n        marker=\"o\",\n        c=\"white\",\n        alpha=1,\n        s=200,\n        edgecolor=\"k\",\n    )\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\n\n    ax2.set_title(\"The visualization of the clustered data.\")\n    ax2.set_xlabel(\"Feature space for the 1st feature\")\n    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\n    (\n        \"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\"\n        % n_clusters,\n        fontsize=14,\n        fontweight=\"bold\",\n    )\n\n()",
            "from sklearn.tree import \nfrom sklearn.model_selection import \nimport numpy as np\n\nn_samples, n_features = 1000, 20\nrng = (0)\nX = rng.randn(n_samples, n_features)\n# positive integer target correlated with X[:, 5] with many zeros:\ny = rng.poisson(lam=(X[:, 5]) / 2)\nX_train, X_test, y_train, y_test = (X, y, random_state=rng)\nregressor = (criterion=\"poisson\", random_state=0)\nregressor.fit(X_train, y_train)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Model Selection->Nested versus non-nested cross-validation"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Tweedie regression on insurance claims->Severity Model -  Gamma distribution"
            ],
            [
                "sklearn->Examples->Calibration->Probability calibration of classifiers->Gaussian Naive-Bayes"
            ],
            [
                "sklearn->Examples->Clustering->Selecting the number of clusters with silhouette analysis on KMeans clustering"
            ],
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 0.24->New Poisson splitting criterion for DecisionTreeRegressor"
            ]
        ]
    },
    "1354219": {
        "jupyter_code_cell": "print(\"Accuracy Score:\"+str(acc_perceptron))\nprint(\"Precision:\"+str(average_precision_score(y_val,y_pred)))\nprint(\"Recall:\"+str(recall_score(y_val,y_pred)))\nprint(\"Confusion Matrix:\")\ncnf_matrix = confusion_matrix(y_pred, y_val)\nprint(cnf_matrix)\nperceptron = Perceptron()\nscores = cross_val_score(perceptron, predictors, target, cv=10)\nprint(scores.mean())\ny_pred1=cross_val_predict(perceptron,predictors,target,cv=10)",
        "matched_tutorial_code_inds": [
            1960,
            3578,
            2374,
            1995,
            1521
        ],
        "matched_tutorial_codes": [
            "print(f\"Homogeneity: {(labels_true, labels):.3f}\")\nprint(f\"Completeness: {(labels_true, labels):.3f}\")\nprint(f\"V-measure: {(labels_true, labels):.3f}\")\nprint(f\"Adjusted Rand Index: {(labels_true, labels):.3f}\")\nprint(\n    \"Adjusted Mutual Information:\"\n    f\" {(labels_true, labels):.3f}\"\n)\nprint(f\"Silhouette Coefficient: {(X, labels):.3f}\")",
            "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3]))\nFrom: sd345@city.ac.uk (Michael Collier)\nSubject: Converting images to HP LaserJet III?\nNntp-Posting-Host: hampton\n\n print(twenty_train.target_names[twenty_train.target[0]])\ncomp.graphics",
            "print(\"Predicting people's names on the test set\")\nt0 = ()\ny_pred = clf.predict(X_test_pca)\nprint(\"done in %0.3fs\" % (() - t0))\n\nprint((y_test, y_pred, target_names=target_names))\n(\n    clf, X_test_pca, y_test, display_labels=target_names, xticks_rotation=\"vertical\"\n)\n()\n()\n\n\n<img alt=\"plot face recognition\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_face_recognition_001.png\" srcset=\"../../_images/sphx_glr_plot_face_recognition_001.png\"/>",
            "print(\"Compute structured hierarchical clustering...\")\nst = ()\nward = (\n    n_clusters=6, connectivity=connectivity, linkage=\"ward\"\n).fit(X)\nelapsed_time = () - st\nlabel = ward.labels_\nprint(f\"Elapsed time: {elapsed_time:.2f}s\")\nprint(f\"Number of points: {label.size}\")",
            "print(\"Rate of semiconductors added on a chip every 2 years:\")\nprint(\n    \"\\tx{:.2f} +/- {:.2f} semiconductors per chip\".format(\n        np.exp((A) * 2), 2 * A * np.exp(2 * A) * 0.006\n    )\n)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Clustering->Demo of DBSCAN clustering algorithm->Compute DBSCAN"
            ],
            [
                "sklearn->Tutorials->Working With Text Data->Loading the 20 newsgroups dataset"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Faces recognition example using eigenfaces and SVMs"
            ],
            [
                "sklearn->Examples->Clustering->Hierarchical clustering: structured vs unstructured ward->Compute clustering"
            ],
            [
                "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy->Calculating the historical growth curve for transistors"
            ]
        ]
    },
    "156032": {
        "jupyter_code_cell": "relevent_ratings_wide_df = relevent_ratings_df.pivot(index='userId', columns = 'movieId', values = 'rating')\nsns.heatmap(relevent_ratings_wide_df)\nrating_train, rating_test = train_test_split(relevent_ratings_df, test_size = 0.2, random_state = 42) ",
        "matched_tutorial_code_inds": [
            3079,
            1615,
            5917,
            2114,
            1523
        ],
        "matched_tutorial_codes": [
            "rfc = (n_estimators=10, random_state=42)\nrfc.fit(X_train, y_train)\nax = ()\nrfc_disp = (rfc, X_test, y_test, ax=ax, alpha=0.8)\nsvc_disp.plot(ax=ax, alpha=0.8)\n()\n\n\n<img alt=\"plot roc curve visualization api\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_roc_curve_visualization_api_002.png\" srcset=\"../../_images/sphx_glr_plot_roc_curve_visualization_api_002.png\"/>",
            "fourier_gaussian = ndimage.fourier_gaussian(xray_image, sigma=0.05)\n\nx_prewitt = ndimage.prewitt(fourier_gaussian, axis=0)\ny_prewitt = ndimage.prewitt(fourier_gaussian, axis=1)\n\nxray_image_canny = np.hypot(x_prewitt, y_prewitt)\n\nxray_image_canny *= 255.0 / np.max(xray_image_canny)\n\nprint(\"The data type - \", xray_image_canny.dtype)",
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "batch_dict_estimator = (\n    n_components=n_components, alpha=0.1, max_iter=50, batch_size=3, random_state=rng\n)\nbatch_dict_estimator.fit(faces_centered)\nplot_gallery(\"Dictionary learning\", batch_dict_estimator.components_[:n_components])\n\n\n<img alt=\"Dictionary learning\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_006.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_006.png\"/>",
            "transistor_count_predicted = np.exp(B) * np.exp(A * year)\ntransistor_Moores_law = Moores_law(year)\nplt.style.use(\"fivethirtyeight\")\nplt.semilogy(year, transistor_count, \"s\", label=\"MOS transistor count\")\nplt.semilogy(year, transistor_count_predicted, label=\"linear regression\")\n\n\nplt.plot(year, transistor_Moores_law, label=\"Moore's Law\")\nplt.title(\n    \"MOS transistor count per microprocessor\\n\"\n    + \"every two years \\n\"\n    + \"Transistor count was x{:.2f} higher\".format(np.exp(A * 2))\n)\nplt.xlabel(\"year introduced\")\nplt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\nplt.ylabel(\"# of transistors\\nper microprocessor\")"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Miscellaneous->ROC Curve with Visualization API->Training a Random Forest and Plotting the ROC Curve"
            ],
            [
                "numpy->NumPy Applications->X-ray image processing->Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters->The Canny filter"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition->Dictionary learning"
            ],
            [
                "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy->Calculating the historical growth curve for transistors"
            ]
        ]
    },
    "1314247": {
        "jupyter_code_cell": "scipy.stats.ks_2samp(newF, newM)\nnewM_1 = np.random.choice(newM,len(newF),replace=False,p=None)",
        "matched_tutorial_code_inds": [
            5770,
            4266,
            1539,
            3908,
            6878
        ],
        "matched_tutorial_codes": [
            "np.random.seed(12345)\nfat_tails = stats.t(6).rvs(40)",
            "from scipy.optimize import LinearConstraint\n linear_constraint = LinearConstraint([[1, 2], [2, 1]], [-np.inf, 1], [1, 1])",
            "np.savetxt(\"mooreslaw_regression.csv\", X=output, delimiter=\",\", header=head)",
            "sns.displot(data=tips, kind=\"ecdf\", x=\"total_bill\", col=\"time\", hue=\"smoker\", rug=True)\n",
            "mod = NBin(y, X)\nres = mod.fit()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Scale Estimators"
            ],
            [
                "scipy->Optimization (scipy.optimize)->Constrained minimization of multivariate scalar functions (minimize)->Trust-Region Constrained Algorithm (method='trust-constr')->Defining Linear Constraints:"
            ],
            [
                "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy->Sharing your results as zipped arrays and a csv->Creating your own comma separated value file"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics->Distributional representations"
            ],
            [
                "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 2: Negative Binomial Regression for Count Data->Usage Example"
            ]
        ]
    },
    "23861": {
        "jupyter_code_cell": "postcodeData = pandas.read_csv('demo_by_postcode_output.csv',sep=',')\nprint(postcodeData.head(5)) \ncenter = [-37, 145]\nzoom = 7\ndemo_map = Map(\n    center=center,\n    zoom=zoom\n)\ndemo_map",
        "matched_tutorial_code_inds": [
            4017,
            6848,
            1475,
            3904,
            1143
        ],
        "matched_tutorial_codes": [
            "healthexp = sns.load_dataset(\"healthexp\").sort_values(\"Year\")\nsns.relplot(\n    data=healthexp, kind=\"line\",\n    x=\"Spending_USD\", y=\"Life_Expectancy\", hue=\"Country\",\n    sort=False\n)\n",
            "index = pd.DatetimeIndex([\n    '2000-01-01 10:08am', '2000-01-01 11:32am',\n    '2000-01-01 5:32pm', '2000-01-02 6:15am'])\nendog4 = pd.Series([0.2, 0.5, -0.1, 0.1], index=index)\nprint(endog4.index)",
            "totals_row = 35\nlocations = np.delete(locations, (totals_row), axis=0)\nnbcases = np.delete(nbcases, (totals_row), axis=0)\n\nchina_total = nbcases[locations[:, 1] == \"China\"].sum(axis=0)\nchina_total",
            "dots = sns.load_dataset(\"dots\")\nsns.relplot(\n    data=dots, kind=\"line\",\n    x=\"time\", y=\"firing_rate\", col=\"align\",\n    hue=\"choice\", size=\"coherence\", style=\"choice\",\n    facet_kws=dict(sharex=False),\n)\n",
            "total_trials = 48  # total evaluation budget\n\nfrom ax.modelbridge.dispatch_utils import choose_generation_strategy\n\ngs = choose_generation_strategy(\n    search_space=experiment.search_space,\n    optimization_config=experiment.optimization_config,\n    num_trials=total_trials,\n  )"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Emphasizing continuity with line plots->Controlling sorting and orientation",
                "seaborn->Plotting functions->Visualizing statistical relationships->Emphasizing continuity with line plots->Controlling sorting and orientation"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes"
            ],
            [
                "numpy->NumPy Features->Masked Arrays->Exploring the data"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics"
            ],
            [
                "torch->Model Optimization->Multi-Objective NAS with Ax->Choosing the GenerationStrategy"
            ]
        ]
    },
    "329950": {
        "jupyter_code_cell": "df.plot(x='SerDates', y='RH', figsize=(14,10))\nplt.title('Boise Daily Relative Humidity: 2008-2018',fontsize=16)\nplt.xlabel('$Time$ $[years]$',fontsize=16)\nplt.ylabel('$RH$',fontsize=16)\nplt.show()\nRH = df['RH'].values\nnp.mean(RH, axis=0)",
        "matched_tutorial_code_inds": [
            5712,
            3773,
            3801,
            6267,
            6265
        ],
        "matched_tutorial_codes": [
            "df = pd.read_csv(raw, header=None)\ndf = df.melt()\ndf[\"site\"] = 1 + np.floor(df.index / 10).astype(int)\ndf[\"variety\"] = 1 + (df.index % 10)\ndf = df.rename(columns={\"value\": \"blotch\"})\ndf = df.drop(\"variable\", axis=1)\ndf[\"blotch\"] /= 100",
            "df = df.assign(away_strength=df['away_team'].map(win_percent),\n               home_strength=df['home_team'].map(win_percent),\n               point_diff=df['home_points'] - df['away_points'],\n               rest_diff=df['home_rest'] - df['away_rest'])\ndf.head()",
            "df = sns.load_dataset('titanic')\n\nclf = RandomForestClassifier()\nparam_grid = dict(max_depth=[1, 2, 5, 10, 20, 30, 40],\n                  min_samples_split=[2, 5, 10],\n                  min_samples_leaf=[2, 3, 5])\nest = GridSearchCV(clf, param_grid=param_grid, n_jobs=4)\n\ny = df['survived']\nX = df.drop(['survived', 'who', 'alive'], axis=1)\n\nX = pd.get_dummies(X, drop_first=True)\nX = X.fillna(value=X.median())\nest.fit(X, y);",
            "df = pd.DataFrame(\n    np.c_[aust, fit2.level, fit2.trend, fit2.season, fit2.fittedvalues],\n    columns=[r\"$y_t$\", r\"$l_t$\", r\"$b_t$\", r\"$s_t$\", r\"$\\hat{y}_t$\"],\n    index=aust.index,\n)\ndf.append(fit2.forecast(8).rename(r\"$\\hat{y}_t$\").to_frame(), sort=True)",
            "df = pd.DataFrame(\n    np.c_[aust, fit1.level, fit1.trend, fit1.season, fit1.fittedvalues],\n    columns=[r\"$y_t$\", r\"$l_t$\", r\"$b_t$\", r\"$s_t$\", r\"$\\hat{y}_t$\"],\n    index=aust.index,\n)\ndf.append(fit1.forecast(8).rename(r\"$\\hat{y}_t$\").to_frame(), sort=True)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Generalized Linear Models->Quasi-binomial regression"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Winters Seasonal->The Internals"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Winters Seasonal->The Internals"
            ]
        ]
    },
    "646955": {
        "jupyter_code_cell": "y = df[\"Hired\"]\nX = df[features]\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(X,y)\nfrom IPython.display import Image  \nfrom sklearn.externals.six import StringIO  \nimport pydotplus \ndot_data = StringIO()  \ntree.export_graphviz(clf, \n                     out_file=dot_data,  \n                     feature_names=features)  \ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(graph.create_pdf())  ",
        "matched_tutorial_code_inds": [
            2064,
            3071,
            6795,
            5355,
            2124
        ],
        "matched_tutorial_codes": [
            "y = X.dot(pca.components_[1]) + rng.normal(size=n_samples) / 2\n\nfig, axes = (1, 2, figsize=(10, 3))\n\naxes[0].scatter(X.dot(pca.components_[0]), y, alpha=0.3)\naxes[0].set(xlabel=\"Projected data onto first PCA component\", ylabel=\"y\")\naxes[1].scatter(X.dot(pca.components_[1]), y, alpha=0.3)\naxes[1].set(xlabel=\"Projected data onto second PCA component\", ylabel=\"y\")\n()\n()\n\n\n<img alt=\"plot pcr vs pls\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_pcr_vs_pls_002.png\" srcset=\"../../_images/sphx_glr_plot_pcr_vs_pls_002.png\"/>",
            "ir = (out_of_bounds=\"clip\")\ny_ = ir.fit_transform(x, y)\n\nlr = ()\nlr.fit(x[:, ], y)  # x needs to be 2d for LinearRegression",
            "x1n = np.linspace(20.5, 25, 10)\nXnew = np.column_stack((x1n, np.sin(x1n), (x1n - 5) ** 2))\nXnew = sm.add_constant(Xnew)\nynewpred = olsres.predict(Xnew)  # predict out of sample\nprint(ynewpred)",
            "weights = rob_crime_model.weights\nidx = weights  0\nX = rob_crime_model.model.exog[idx.values]\nww = weights[idx] / weights[idx].mean()\nhat_matrix_diag = ww * (X * np.linalg.pinv(X).T).sum(1)\nresid = rob_crime_model.resid\nresid2 = resid ** 2\nresid2 /= resid2.sum()\nnobs = int(idx.sum())\nhm = hat_matrix_diag.mean()\nrm = resid2.mean()",
            "n_comps = 2\n\nmethods = [\n    (\"PCA\", ()),\n    (\"Unrotated FA\", ()),\n    (\"Varimax FA\", (rotation=\"varimax\")),\n]\nfig, axes = (ncols=len(methods), figsize=(10, 8), sharey=True)\n\nfor ax, (method, fa) in zip(axes, methods):\n    fa.set_params(n_components=n_comps)\n    fa.fit(X)\n\n    components = fa.components_.T\n    print(\"\\n\\n %s :\\n\" % method)\n    print(components)\n\n    vmax = np.abs(components).max()\n    ax.imshow(components, cmap=\"RdBu_r\", vmax=vmax, vmin=-vmax)\n    ax.set_yticks((len(feature_names)))\n    ax.set_yticklabels(feature_names)\n    ax.set_title(str(method))\n    ax.set_xticks([0, 1])\n    ax.set_xticklabels([\"Comp. 1\", \"Comp. 2\"])\nfig.suptitle(\"Factors\")\n()\n()\n\n\n<img alt=\"Factors, PCA, Unrotated FA, Varimax FA\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_varimax_fa_002.png\" srcset=\"../../_images/sphx_glr_plot_varimax_fa_002.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Cross decomposition->Principal Component Regression vs Partial Least Squares Regression->The data"
            ],
            [
                "sklearn->Examples->Miscellaneous->Isotonic Regression"
            ],
            [
                "statsmodels->Examples->User Notes->Prediction->Create a new sample of explanatory variables Xnew, predict and plot"
            ],
            [
                "statsmodels->Examples->Plotting->Regression Plots->Statewide Crime 2009 Dataset->Using robust regression to correct for outliers."
            ],
            [
                "sklearn->Examples->Decomposition->Factor Analysis (with rotation) to visualize patterns"
            ]
        ]
    },
    "745719": {
        "jupyter_code_cell": "fig, ax = plt.subplots(figsize=(15,7))\ny_pred_proba2 = forest.predict_proba(X_test)[::,1]\nfpr2, tpr2, _ = metrics.roc_curve(ytst,  y_pred_proba2)\nauc2 = metrics.roc_auc_score(ytst, y_pred_proba2)\nplt.plot(fpr, tpr, label=\"LogisticRegression AUC = \"+str(auc), linewidth=4, color='green')\nplt.plot(fpr2, tpr2, label=\"RandomForestClassifier AUC = \"+str(auc2), linewidth=4, color='#ff5b00')\nplt.title('ROC AUC curve for binary classification', size = 20, color='r')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlabel('False Positive Rate', fontsize=18)\nplt.ylabel('True Positive Rate', fontsize=18)\nplt.legend(loc=4, fontsize = 'xx-large')\nplt.show()\nfeatures_sub2 = df_undersampling.iloc[:, 1:].values\ntarget_sub2 = df_undersampling.iloc[:, 0:1]\nss = StandardScaler()\nfeaturesS_sub2 = ss.fit_transform(features_sub2)\npredictions_RF_model = forest.predict(featuresS_sub2)\npredictions_percentage2 = forest.predict_proba(featuresS_sub2)[:, 0]",
        "matched_tutorial_code_inds": [
            6453,
            5580,
            6423,
            6428,
            6419
        ],
        "matched_tutorial_codes": [
            "fig, ax = plt.subplots(figsize=(10,4))\n\n# Plot the results\ndf['lff'].plot(ax=ax, style='k.', label='Observations')\npredict.predicted_mean.plot(ax=ax, label='One-step-ahead Prediction')\npredict_ci = predict.conf_int(alpha=0.05)\npredict_index = np.arange(len(predict_ci))\nax.fill_between(predict_index[2:], predict_ci.iloc[2:, 0], predict_ci.iloc[2:, 1], alpha=0.1)\n\nforecast.predicted_mean.plot(ax=ax, style='r', label='Forecast')\nforecast_ci = forecast.conf_int()\nforecast_index = np.arange(len(predict_ci), len(predict_ci) + len(forecast_ci))\nax.fill_between(forecast_index, forecast_ci.iloc[:, 0], forecast_ci.iloc[:, 1], alpha=0.1)\n\n# Cleanup the image\nax.set_ylim((4, 8));\nlegend = ax.legend(loc='lower left');\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_local_linear_trend_11_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_local_linear_trend_11_0.png\"/>",
            "fig, ax = plt.subplots()\nax.scatter(yhat, y)\nline_fit = sm.OLS(y, sm.add_constant(yhat, prepend=True)).fit()\nabline_plot(model_results=line_fit, ax=ax)\n\n\nax.set_title('Model Fit Plot')\nax.set_ylabel('Observed values')\nax.set_xlabel('Fitted values');\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_glm_23_0.png\" src=\"../../../_images/examples_notebooks_generated_glm_23_0.png\"/>",
            "fig, ax = plt.subplots(figsize=(13,3))\n\n# Compute the index\ncoincident_index = compute_coincident_index(mod, res)\n\n# Plot the factor\ndates = endog.index._mpl_repr()\nax.plot(dates, coincident_index, label='Coincident index')\nax.plot(usphci.index._mpl_repr(), usphci, label='USPHCI')\nax.legend(loc='lower right')\n\n# Retrieve and also plot the NBER recession indicators\nylim = ax.get_ylim()\nax.fill_between(dates[:-3], ylim[0], ylim[1], rec.values[:-4,0], facecolor='k', alpha=0.1);\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_dfm_coincident_26_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_dfm_coincident_26_0.png\"/>",
            "fig, ax = plt.subplots(figsize=(13,3))\n\n# Compute the index\nextended_coincident_index = compute_coincident_index(extended_mod, extended_res)\n\n# Plot the factor\ndates = endog.index._mpl_repr()\nax.plot(dates, coincident_index, '-', linewidth=1, label='Basic model')\nax.plot(dates, extended_coincident_index, '--', linewidth=3, label='Extended model')\nax.plot(usphci.index._mpl_repr(), usphci, label='USPHCI')\nax.legend(loc='lower right')\nax.set(title='Coincident indices, comparison')\n\n# Retrieve and also plot the NBER recession indicators\nylim = ax.get_ylim()\nax.fill_between(dates[:-3], ylim[0], ylim[1], rec.values[:-4,0], facecolor='k', alpha=0.1);\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_dfm_coincident_35_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_dfm_coincident_35_0.png\"/>",
            "fig, ax = plt.subplots(figsize=(13,3))\n\n# Plot the factor\ndates = endog.index._mpl_repr()\nax.plot(dates, res.factors.filtered[0], label='Factor')\nax.legend()\n\n# Retrieve and also plot the NBER recession indicators\nrec = DataReader('USREC', 'fred', start=start, end=end)\nylim = ax.get_ylim()\nax.fill_between(dates[:-3], ylim[0], ylim[1], rec.values[:-4,0], facecolor='k', alpha=0.1);\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_dfm_coincident_19_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_dfm_coincident_19_0.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->State space modeling: Local Linear Trends"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Generalized Linear Models Overview->GLM: Binomial response data->Plots"
            ],
            [
                "statsmodels->Examples->State space models->Dynamic Factor Models: Application->Coincident Index"
            ],
            [
                "statsmodels->Examples->State space models->Dynamic Factor Models: Application->Appendix 1: Extending the dynamic factor model"
            ],
            [
                "statsmodels->Examples->State space models->Dynamic Factor Models: Application->Estimates->Estimated factors"
            ]
        ]
    },
    "735119": {
        "jupyter_code_cell": "input_dirs = ['data']\nadict = {}\nsample_name = []\nfor iFold, folder in enumerate(input_dirs):\n    for file in os.listdir(folder):\n        if file.endswith('counts.tsv.gz'):\n            s = file.split('.')[0]\n            sample_name.append(s)\n            print('___________________')\n            print(s)\n            npz_filename = folder + '/' + s + '.counts.npz'\n            if os.path.isfile(npz_filename):\n                print('loading from npz')\n                Etmp = ssp.load_npz(npz_filename)\n            else:\n                print('loading from tsv')\n                Etmp = hf.load_text(hf.file_opener(folder + '/' + file))\n                print('saving npz')\n                ssp.save_npz(folder + '/' + s + '.counts.npz', Etmp, compressed=True)\n            adict[s] = sc.AnnData(Etmp)\n            adict[s].var_names = pd.read_csv(folder + '/genes.txt', header=None, sep='\\t')[0]\n            adict[s].var_names_make_unique()\n            adict[s].uns['min_tot'] = 500\n            adict[s].obs['Timepoint'] = s.split('_')[0]\n            adict[s].obs['Genotype'] = s.split('_')[1]\n            print('{} barcodes, {} genes'.format(adict[s].shape[0], adict[s].shape[1]))\n            del Etmp\nsample_name",
        "matched_tutorial_code_inds": [
            3257,
            2753,
            23,
            1767,
            3173
        ],
        "matched_tutorial_codes": [
            "alphas = (-5, 1, 60)\nenet = (l1_ratio=0.7, max_iter=10000)\ntrain_errors = list()\ntest_errors = list()\nfor alpha in alphas:\n    enet.set_params(alpha=alpha)\n    enet.fit(X_train, y_train)\n    train_errors.append(enet.score(X_train, y_train))\n    test_errors.append(enet.score(X_test, y_test))\n\ni_alpha_optim = (test_errors)\nalpha_optim = alphas[i_alpha_optim]\nprint(\"Optimal regularization parameter : %s\" % alpha_optim)\n\n# Estimate the coef_ on full data with optimal regularization parameter\nenet.set_params(alpha=alpha_optim)\ncoef_ = enet.fit(X, y).coef_",
            "quantiles = [0.05, 0.5, 0.95]\npredictions = {}\nout_bounds_predictions = (y_true_mean, dtype=)\nfor quantile in quantiles:\n    qr = (quantile=quantile, alpha=0, solver=solver)\n    y_pred = qr.fit(X, y_pareto).predict(X)\n    predictions[quantile] = y_pred\n\n    if quantile == min(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred = y_pareto\n        )\n    elif quantile == max(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred &lt;= y_pareto\n        )",
            "face_dataset = FaceLandmarksDataset(csv_file='faces/face_landmarks.csv',\n                                    root_dir='faces/')\n\nfig = plt.figure()\n\nfor i in range(len(face_dataset)):\n    sample = face_dataset[i]\n\n    print(i, sample['image'].shape, sample['landmarks'].shape)\n\n    ax = plt.subplot(1, 4, i + 1)\n    plt.tight_layout()\n    ax.set_title('Sample #{}'.format(i))\n    ax.axis('off')\n    show_landmarks(**sample)\n\n    if i == 3:\n        plt.show()\n        break",
            "x_axis = []\ndata = {'positive sentiment': [], 'negative sentiment': []}\nfor speaker in predictions:\n    # The speakers will be used to label the x-axis in our plot\n    x_axis.append(speaker)\n    # number of paras with positive sentiment\n    no_pos_paras = len(predictions[speaker]['pos_paras'])\n    # number of paras with negative sentiment\n    no_neg_paras = len(predictions[speaker]['neg_paras'])\n    # Obtain percentage of paragraphs with positive predicted sentiment\n    pos_perc = no_pos_paras / (no_pos_paras + no_neg_paras)\n    # Store positive and negative percentages\n    data['positive sentiment'].append(pos_perc*100)\n    data['negative sentiment'].append(100*(1-pos_perc))\n\nindex = pd.Index(x_axis, name='speaker')\ndf = pd.DataFrame(data, index=index)\nax = df.plot(kind='bar', stacked=True)\nax.set_ylabel('percentage')\nax.legend(title='labels', bbox_to_anchor=(1, 1), loc='upper left')\nplt.show()",
            "pair_scores = []\nmean_tpr = dict()\n\nfor ix, (label_a, label_b) in enumerate(pair_list):\n\n    a_mask = y_test == label_a\n    b_mask = y_test == label_b\n    ab_mask = (a_mask, b_mask)\n\n    a_true = a_mask[ab_mask]\n    b_true = b_mask[ab_mask]\n\n    idx_a = (label_binarizer.classes_ == label_a)[0]\n    idx_b = (label_binarizer.classes_ == label_b)[0]\n\n    fpr_a, tpr_a, _ = (a_true, y_score[ab_mask, idx_a])\n    fpr_b, tpr_b, _ = (b_true, y_score[ab_mask, idx_b])\n\n    mean_tpr[ix] = (fpr_grid)\n    mean_tpr[ix] += (fpr_grid, fpr_a, tpr_a)\n    mean_tpr[ix] += (fpr_grid, fpr_b, tpr_b)\n    mean_tpr[ix] /= 2\n    mean_score = (fpr_grid, mean_tpr[ix])\n    pair_scores.append(mean_score)\n\n    fig, ax = (figsize=(6, 6))\n    (\n        fpr_grid,\n        mean_tpr[ix],\n        label=f\"Mean {label_a} vs {label_b} (AUC = {mean_score :.2f})\",\n        linestyle=\":\",\n        linewidth=4,\n    )\n    (\n        a_true,\n        y_score[ab_mask, idx_a],\n        ax=ax,\n        name=f\"{label_a} as positive class\",\n    )\n    (\n        b_true,\n        y_score[ab_mask, idx_b],\n        ax=ax,\n        name=f\"{label_b} as positive class\",\n    )\n    ([0, 1], [0, 1], \"k--\", label=\"chance level (AUC = 0.5)\")\n    (\"square\")\n    (\"False Positive Rate\")\n    (\"True Positive Rate\")\n    (f\"{target_names[idx_a]} vs {label_b} ROC curves\")\n    ()\n    ()\n\nprint(f\"Macro-averaged One-vs-One ROC AUC score:\\n{(pair_scores):.2f}\")\n\n\n\n<img alt=\"setosa vs versicolor ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_004.png\" srcset=\"../../_images/sphx_glr_plot_roc_004.png\"/>\n<img alt=\"setosa vs virginica ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_005.png\" srcset=\"../../_images/sphx_glr_plot_roc_005.png\"/>\n<img alt=\"versicolor vs virginica ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_006.png\" srcset=\"../../_images/sphx_glr_plot_roc_006.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Model Selection->Train error vs Test error->Compute train and test errors"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Quantile regression->Fitting a QuantileRegressor"
            ],
            [
                "torch->PyTorch Recipes->Developing Custom PyTorch Dataloaders->Part 1: The Dataset->1.3 Iterate through data samples"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Sentiment Analysis on the Speech Data"
            ],
            [
                "sklearn->Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->One-vs-One multiclass ROC->ROC curve using the OvO macro-average"
            ]
        ]
    },
    "1036708": {
        "jupyter_code_cell": "train_df.corr().at['Survived', 'Parch']\ndef relatives_count(row): return row['SibSp'] + row['Parch']\ntrain_df['Relatives'] = train_df.apply(relatives_count, axis=1)\ntest_df['Relatives'] = test_df.apply(relatives_count, axis=1)",
        "matched_tutorial_code_inds": [
            6492,
            4905,
            6714,
            922,
            6493
        ],
        "matched_tutorial_codes": [
            "time_s = np.s_[:50]  # After this they basically agree\nfig1 = plt.figure()\nax1 = fig1.add_subplot(111)\nidx = np.asarray(series.index)\nh1, = ax1.plot(idx[time_s], res_f.freq_seasonal[0].filtered[time_s], label='Double Freq. Seas')\nh2, = ax1.plot(idx[time_s], res_tf.seasonal.filtered[time_s], label='Mixed Domain Seas')\nh3, = ax1.plot(idx[time_s], true_seasonal_10_3[time_s], label='True Seasonal 10(3)')\nplt.legend([h1, h2, h3], ['Double Freq. Seasonal','Mixed Domain Seasonal','Truth'], loc=2)\nplt.title('Seasonal 10(3) component')\nplt.show()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_seasonal_21_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_seasonal_21_0.png\"/>",
            "viridis.colors [[0.267004 0.004874 0.329415 1.      ]\n [0.275191 0.194905 0.496005 1.      ]\n [0.212395 0.359683 0.55171  1.      ]\n [0.153364 0.497    0.557724 1.      ]\n [0.122312 0.633153 0.530398 1.      ]\n [0.288921 0.758394 0.428426 1.      ]\n [0.626579 0.854645 0.223353 1.      ]\n [0.993248 0.906157 0.143936 1.      ]]\nviridis(range(8)) [[0.267004 0.004874 0.329415 1.      ]\n [0.275191 0.194905 0.496005 1.      ]\n [0.212395 0.359683 0.55171  1.      ]\n [0.153364 0.497    0.557724 1.      ]\n [0.122312 0.633153 0.530398 1.      ]\n [0.288921 0.758394 0.428426 1.      ]\n [0.626579 0.854645 0.223353 1.      ]\n [0.993248 0.906157 0.143936 1.      ]]\nviridis(np.linspace(0, 1, 8)) [[0.267004 0.004874 0.329415 1.      ]\n [0.275191 0.194905 0.496005 1.      ]\n [0.212395 0.359683 0.55171  1.      ]\n [0.153364 0.497    0.557724 1.      ]\n [0.122312 0.633153 0.530398 1.      ]\n [0.288921 0.758394 0.428426 1.      ]\n [0.626579 0.854645 0.223353 1.      ]\n [0.993248 0.906157 0.143936 1.      ]]",
            "make_plot(dta.index[idx[:5]])\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_24_0.png\" src=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_24_0.png\"/>",
            "find_package(OpenCV REQUIRED)\nadd_library(warp_perspective SHARED op.cpp)\ntarget_compile_features(warp_perspective PRIVATE cxx_range_for)\ntarget_link_libraries(warp_perspective PRIVATE \"${TORCH_LIBRARIES}\")\ntarget_link_libraries(warp_perspective PRIVATE opencv_core opencv_photo)",
            "time_s = np.s_[:50]  # After this they basically agree\nfig2 = plt.figure()\nax2 = fig2.add_subplot(111)\nh21, = ax2.plot(idx[time_s], res_f.freq_seasonal[1].filtered[time_s], label='Double Freq. Seas')\nh22, = ax2.plot(idx[time_s], res_tf.freq_seasonal[0].filtered[time_s], label='Mixed Domain Seas')\nh23, = ax2.plot(idx[time_s], true_seasonal_100_2[time_s], label='True Seasonal 100(2)')\nplt.legend([h21, h22, h23], ['Double Freq. Seasonal','Mixed Domain Seasonal','Truth'], loc=2)\nplt.title('Seasonal 100(2) component')\nplt.show()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_seasonal_22_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_seasonal_22_0.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->Seasonality in Time Series Data->Comparison of filtered estimates"
            ],
            [
                "matplotlib->Tutorials->Colors->Creating Colormaps in Matplotlib->Getting colormaps and accessing their values->ListedColormap"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "torch->Extending PyTorch->Extending TorchScript with Custom C++ Operators->Using the TorchScript Custom Operator in C++"
            ],
            [
                "statsmodels->Examples->State space models->Seasonality in Time Series Data->Comparison of filtered estimates"
            ]
        ]
    },
    "280085": {
        "jupyter_code_cell": "df.shape\ndf.info()",
        "matched_tutorial_code_inds": [
            3802,
            3785,
            5236,
            4155,
            4150
        ],
        "matched_tutorial_codes": [
            "scores = pd.DataFrame(est.cv_results_)\nscores.head()",
            "df.info()",
            "print(res.cusum)\nfig = res.plot_cusum()",
            "sinplot()\nsns.despine()\n",
            "sns.set_theme()\nsinplot()\n"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 1: Copper"
            ],
            [
                "seaborn->User guide and tutorial->Controlling figure aesthetics->Removing axes spines",
                "seaborn->Figure aesthetics->Controlling figure aesthetics->Removing axes spines"
            ],
            [
                "seaborn->User guide and tutorial->Controlling figure aesthetics",
                "seaborn->Figure aesthetics->Controlling figure aesthetics"
            ]
        ]
    },
    "872298": {
        "jupyter_code_cell": "from sklearn.linear_model import LinearRegression,Ridge,Lasso\nfrom sklearn.model_selection import GridSearchCV\nlModel = LinearRegression()\nlModel.fit(X = X_train,y = np.log1p(y_train))\npreds = lModel.predict(X= X_validate)\nprint (\"RMSLE Value For Linear Regression In Validation: \",rmsle(np.exp(np.log1p(y_validate)),np.exp(preds),False))\npredsTest = lModel.predict(X=dataTest)\nfig,(ax1,ax2)= plt.subplots(ncols=2)\nfig.set_size_inches(20,5)\nsn.distplot(yLabels,ax=ax1,bins=100)\nsn.distplot(np.exp(predsTest),ax=ax2,bins=100)\nax1.set(title=\"Training Set Distribution\")\nax2.set(title=\"Test Set Distribution\")",
        "matched_tutorial_code_inds": [
            1857,
            3086,
            3000,
            3014,
            1844
        ],
        "matched_tutorial_codes": [
            "from sklearn.datasets import \nfrom sklearn.svm import \nfrom sklearn.metrics import \n\nX, y = (n_classes=4, n_informative=16)\nclf = (decision_function_shape=\"ovo\", probability=True).fit(X, y)\nprint((y, clf.predict_proba(X), multi_class=\"ovo\"))",
            "from sklearn.datasets import \nfrom sklearn.preprocessing import \nfrom sklearn.pipeline import \nfrom sklearn.linear_model import \nfrom sklearn.model_selection import \n\nX, y = (data_id=1464, return_X_y=True, parser=\"pandas\")\nX_train, X_test, y_train, y_test = (X, y, stratify=y)\n\nclf = ((), (random_state=0))\nclf.fit(X_train, y_train)",
            "from sklearn.model_selection import \nfrom sklearn.svm import \nfrom sklearn.kernel_ridge import \n\ntrain_size = 100\n\nsvr = (\n    (kernel=\"rbf\", gamma=0.1),\n    param_grid={\"C\": [1e0, 1e1, 1e2, 1e3], \"gamma\": (-2, 2, 5)},\n)\n\nkr = (\n    (kernel=\"rbf\", gamma=0.1),\n    param_grid={\"alpha\": [1e0, 0.1, 1e-2, 1e-3], \"gamma\": (-2, 2, 5)},\n)",
            "from sklearn.pipeline import \nfrom sklearn.preprocessing import , \nfrom sklearn.linear_model import \n\nsteps = [\n    (\"standard_scaler\", ()),\n    (\"polynomial\", (degree=3)),\n    (\"classifier\", (C=2.0)),\n]\npipe = (steps)\npipe  # click on the diagram below to see the details of each step",
            "from sklearn.datasets import \nfrom sklearn.svm import \nfrom sklearn.linear_model import \nfrom sklearn.preprocessing import \nfrom sklearn.pipeline import \nfrom sklearn.ensemble import \nfrom sklearn.model_selection import \n\nX, y = (return_X_y=True)\nestimators = [\n    (\"rf\", (n_estimators=10, random_state=42)),\n    (\"svr\", ((), (random_state=42))),\n]\nclf = (estimators=estimators, final_estimator=())\nX_train, X_test, y_train, y_test = (X, y, stratify=y, random_state=42)\nclf.fit(X_train, y_train).score(X_test, y_test)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 0.22->ROC AUC now supports multiclass classification"
            ],
            [
                "sklearn->Examples->Miscellaneous->Visualizations with Display Objects->Load Data and train model"
            ],
            [
                "sklearn->Examples->Miscellaneous->Comparison of kernel ridge regression and SVR->Construct the kernel-based regression models"
            ],
            [
                "sklearn->Examples->Miscellaneous->Displaying Pipelines->Displaying a Pipeline Chaining Multiple Preprocessing Steps & Classifier"
            ],
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 0.22->Stacking Classifier and Regressor"
            ]
        ]
    },
    "1126720": {
        "jupyter_code_cell": "query_all = '''Select {},{} from {}'''.format(response,columns,table)\ndf_all    = con.select_ipc_gpu(query_all,device_id=0)\nprint('number of rows: ', len(df_all))\nrm_cols = set(['PRIVATE_CUSTOMER','ENGINE_POWER','ENGINE_POWER_KW_0','HORSE_POWER','HORSE_POWER_0','HORSE_POWER_1', 'MODEL_CODE'])\nfor col in rm_cols:\n    df_all.drop_column(col)",
        "matched_tutorial_code_inds": [
            6891,
            3698,
            5902,
            1092,
            4654
        ],
        "matched_tutorial_codes": [
            "url = 'https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/csv/COUNT/medpar.csv'\nmedpar = read.csv(url)\nf = los~factor(type)+hmo+white\n\nlibrary(MASS)\nmod = glm.nb(f, medpar)\ncoef(summary(mod))\n                 Estimate Std. Error   z value      Pr(|z|)\n(Intercept)    2.31027893 0.06744676 34.253370 3.885556e-257\nfactor(type)2  0.22124898 0.05045746  4.384861  1.160597e-05\nfactor(type)3  0.70615882 0.07599849  9.291748  1.517751e-20\nhmo           -0.06795522 0.05321375 -1.277024  2.015939e-01\nwhite         -0.12906544 0.06836272 -1.887951  5.903257e-02",
            "files = glob.glob('weather/*.csv')\ncolumns = ['station', 'date', 'tmpf', 'relh', 'sped', 'mslp',\n           'p01i', 'vsby', 'gust_mph', 'skyc1', 'skyc2', 'skyc3']\n\n# init empty DataFrame, like you might for a list\nweather = pd.DataFrame(columns=columns)\n\nfor fp in files:\n    city = pd.read_csv(fp, columns=columns)\n    weather.append(city)",
            "resid = lm.resid\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    group_num = i * 2 + j - 1  # for plotting purposes\n    x = [group_num] * len(group)\n    plt.scatter(\n        x,\n        resid[group.index],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\nplt.xlabel(\"Group\")\nplt.ylabel(\"Residuals\")",
            "data_path = '~/.data/imagenet'\nsaved_model_dir = 'data/'\nfloat_model_file = 'mobilenet_pretrained_float.pth'\nscripted_float_model_file = 'mobilenet_quantization_scripted.pth'\nscripted_quantized_model_file = 'mobilenet_quantization_scripted_quantized.pth'\n\ntrain_batch_size = 30\neval_batch_size = 50\n\ndata_loader, data_loader_test = prepare_data_loaders(data_path)\ncriterion = nn.CrossEntropyLoss()\nfloat_model = load_model(saved_model_dir + float_model_file).to('cpu')\n\n# Next, we'll \"fuse modules\"; this can both make the model faster by saving on memory access\n# while also improving numerical accuracy. While this can be used with any model, this is\n# especially common with quantized models.\n\nprint('\\n Inverted Residual Block: Before fusion \\n\\n', float_model.features[1].conv)\nfloat_model.eval()\n\n# Fuses modules\nfloat_model.fuse_model()\n\n# Note fusion of Conv+BN+Relu and Conv+Relu\nprint('\\n Inverted Residual Block: After fusion\\n\\n',float_model.features[1].conv)",
            "data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 2: Negative Binomial Regression for Count Data->Testing"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "torch->Model Optimization->(beta) Static Quantization with Eager Mode in PyTorch->3. Define dataset and data loaders->ImageNet Data"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings"
            ]
        ]
    },
    "683396": {
        "jupyter_code_cell": "K1=1\nK2=5\nK3=7\nseed = 99\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test =train_test_split(df[df.columns[0:df.columns.shape[0]-1]],df['class'],test_size=0.3,random_state=seed)",
        "matched_tutorial_code_inds": [
            3310,
            4928,
            4927,
            3453,
            6795
        ],
        "matched_tutorial_codes": [
            "Iteration 1, loss = 0.44139186\nIteration 2, loss = 0.19174891\nIteration 3, loss = 0.13983521\nIteration 4, loss = 0.11378556\nIteration 5, loss = 0.09443967\nIteration 6, loss = 0.07846529\nIteration 7, loss = 0.06506307\nIteration 8, loss = 0.05534985\nTraining set score: 0.986429\nTest set score: 0.953061\n\n\n\n<br/>",
            "N = 100\nX, Y = np.mgrid[0:3:complex(0, N), 0:2:complex(0, N)]\nZ1 = (1 + np.sin(Y * 10.)) * X**2\n\nfig, ax = plt.subplots(2, 1, constrained_layout=True)\n\npcm = ax[0].pcolormesh(X, Y, Z1, norm=colors.PowerNorm(gamma=0.5),\n                       cmap='PuBu_r', shading='auto')\nfig.colorbar(pcm, ax=ax[0], extend='max')\nax[0].set_title('PowerNorm()')\n\npcm = ax[1].pcolormesh(X, Y, Z1, cmap='PuBu_r', shading='auto')\nfig.colorbar(pcm, ax=ax[1], extend='max')\nax[1].set_title('Normalize()')\nplt.show()\n\n\n<img alt=\"PowerNorm(), Normalize()\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_colormapnorms_004.png\" srcset=\"../../_images/sphx_glr_colormapnorms_004.png, ../../_images/sphx_glr_colormapnorms_004_2_0x.png 2.0x\"/>",
            "N = 100\nX, Y = np.mgrid[-3:3:complex(0, N), -2:2:complex(0, N)]\nZ1 = np.exp(-X**2 - Y**2)\nZ2 = np.exp(-(X - 1)**2 - (Y - 1)**2)\nZ = (Z1 - Z2) * 2\n\nfig, ax = plt.subplots(2, 1)\n\npcm = ax[0].pcolormesh(X, Y, Z,\n                       norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,\n                                              vmin=-1.0, vmax=1.0, base=10),\n                       cmap='RdBu_r', shading='auto')\nfig.colorbar(pcm, ax=ax[0], extend='both')\n\npcm = ax[1].pcolormesh(X, Y, Z, cmap='RdBu_r', vmin=-np.max(Z), shading='auto')\nfig.colorbar(pcm, ax=ax[1], extend='both')\nplt.show()\n\n\n<img alt=\"colormapnorms\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_colormapnorms_003.png\" srcset=\"../../_images/sphx_glr_colormapnorms_003.png, ../../_images/sphx_glr_colormapnorms_003_2_0x.png 2.0x\"/>",
            "C_2d_range = [1e-2, 1, 1e2]\ngamma_2d_range = [1e-1, 1, 1e1]\nclassifiers = []\nfor C in C_2d_range:\n    for gamma in gamma_2d_range:\n        clf = (C=C, gamma=gamma)\n        clf.fit(X_2d, y_2d)\n        classifiers.append((C, gamma, clf))",
            "x1n = np.linspace(20.5, 25, 10)\nXnew = np.column_stack((x1n, np.sin(x1n), (x1n - 5) ** 2))\nXnew = sm.add_constant(Xnew)\nynewpred = olsres.predict(Xnew)  # predict out of sample\nprint(ynewpred)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Neural Networks->Visualization of MLP weights on MNIST"
            ],
            [
                "matplotlib->Tutorials->Colors->Colormap Normalization->Power-law"
            ],
            [
                "matplotlib->Tutorials->Colors->Colormap Normalization->Symmetric logarithmic"
            ],
            [
                "sklearn->Examples->Support Vector Machines->RBF SVM parameters->Train classifiers"
            ],
            [
                "statsmodels->Examples->User Notes->Prediction->Create a new sample of explanatory variables Xnew, predict and plot"
            ]
        ]
    },
    "656749": {
        "jupyter_code_cell": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns",
        "matched_tutorial_code_inds": [
            6035,
            6117,
            1646,
            5407,
            5161
        ],
        "matched_tutorial_codes": [
            "import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom statsmodels.stats.mediation import Mediation",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom statsmodels.tsa.arima.model import ARIMA",
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom statsmodels.formula.api import logit",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\nnp.random.seed(9876789)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->Mediation analysis with duration data"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data"
            ],
            [
                "numpy->NumPy Applications->Plotting Fractals->What you\u2019ll need"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares"
            ]
        ]
    },
    "1499311": {
        "jupyter_code_cell": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt",
        "matched_tutorial_code_inds": [
            6399,
            2799,
            6455,
            5863,
            5484
        ],
        "matched_tutorial_codes": [
            "import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt",
            "import numpy as np\nimport scipy as sp\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns",
            "import numpy as np\nfrom scipy import stats\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm",
            "import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm",
            "import numpy as np\nimport pandas as pd\nimport scipy.stats as stats\n\nfrom statsmodels.miscmodels.ordinal_model import OrderedModel"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->VARMAX: Introduction",
                "statsmodels->Examples->State space models->Trends and cycles in unemployment"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models"
            ],
            [
                "statsmodels->Examples->State space models->Statespace ARMA: Sunspots Data"
            ],
            [
                "statsmodels->Examples->Generalized Estimating Equations->GEE Nested Covariance Structure",
                "statsmodels->Examples->User Notes->Least squares fitting of models to data"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression"
            ]
        ]
    },
    "1406454": {
        "jupyter_code_cell": "subgraphs = [graph for graph in nx.connected_component_subgraphs(non_empty_graph)\n                   if len(graph.nodes) > 2\n                   and sum(node_name.startswith(\"code=\") for node_name in graph.nodes) > 1]\nlen(subgraphs)\n[len(graph.nodes) for graph in subgraphs]",
        "matched_tutorial_code_inds": [
            2861,
            2951,
            2845,
            2865,
            2460
        ],
        "matched_tutorial_codes": [
            "coefs = (\n    model[-1].regressor_.coef_,\n    columns=[\"Coefficients importance\"],\n    index=feature_names,\n)\ncoefs.plot.barh(figsize=(9, 7))\n(\"Ridge model, with regularization, normalized variables\")\n(\"Raw coefficient values\")\n(x=0, color=\".5\")\n(left=0.3)\n\n\n<img alt=\"Ridge model, with regularization, normalized variables\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_013.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_013.png\"/>",
            "cluster_ids = (dist_linkage, 1, criterion=\"distance\")\ncluster_id_to_feature_ids = (list)\nfor idx, cluster_id in enumerate(cluster_ids):\n    cluster_id_to_feature_ids[cluster_id].append(idx)\nselected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n\nX_train_sel = X_train[:, selected_features]\nX_test_sel = X_test[:, selected_features]\n\nclf_sel = (n_estimators=100, random_state=42)\nclf_sel.fit(X_train_sel, y_train)\nprint(\n    \"Accuracy on test data with features removed: {:.2f}\".format(\n        clf_sel.score(X_test_sel, y_test)\n    )\n)",
            "coefs = (\n    model[-1].regressor_.coef_,\n    columns=[\"Coefficients importance\"],\n    index=feature_names,\n)\ncoefs.plot.barh(figsize=(9, 7))\n(\"Ridge model, small regularization, normalized variables\")\n(\"Raw coefficient values\")\n(x=0, color=\".5\")\n(left=0.3)\n\n\n<img alt=\"Ridge model, small regularization, normalized variables\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_010.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_010.png\"/>",
            "coefs = (\n    model[-1].regressor_.coef_,\n    columns=[\"Coefficients importance\"],\n    index=feature_names,\n)\ncoefs.plot(kind=\"barh\", figsize=(9, 7))\n(\"Lasso model, optimum regularization, normalized variables\")\n(x=0, color=\".5\")\n(left=0.3)\n\n\n<img alt=\"Lasso model, optimum regularization, normalized variables\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_016.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_016.png\"/>",
            "one_hot_poly_pipeline = (\n    (\n        transformers=[\n            (\"categorical\", one_hot_encoder, categorical_columns),\n            (\"one_hot_time\", one_hot_encoder, [\"hour\", \"weekday\", \"month\"]),\n        ],\n        remainder=\"passthrough\",\n    ),\n    (kernel=\"poly\", degree=2, n_components=300, random_state=0),\n    (alphas=alphas),\n)\nevaluate(one_hot_poly_pipeline, X, y, cv=ts_cv)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "sklearn->Examples->Inspection->Permutation Importance with Multicollinear or Correlated Features->Handling Multicollinear Features"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Modeling non-linear feature interactions with kernels"
            ]
        ]
    },
    "1476769": {
        "jupyter_code_cell": "replaced = {91:0,93:0}\ndf_age_filtered.replace({'# Days Used Alcohol Past Month':replaced,'# Days Binge Drank Past Month':replaced,\n                        '# Days Used Marijuana Past Month':replaced,'# Days Used Cocaine Past Month':replaced,\n                        '# Days Used Hallucinogens Past Month':replaced},inplace=True)\nreplaced = {1:'1-11',2:'12-49',3:'50-99',4:'100-299',5:'300-365',6:'0'}\ndf_age_filtered.replace({'# Days Used Alcohol Past Year (Range)':replaced,'# Days Used Marijuana Past Year (Range)':replaced,\n                        '# Days Used Cocaine Past Year (Range)':replaced,'# Days Used Hallucinogens Past Year (Range)':replaced},\n                       inplace=True)",
        "matched_tutorial_code_inds": [
            2601,
            5643,
            5981,
            2951,
            5984
        ],
        "matched_tutorial_codes": [
            "vmin, vmax = (-log_marginal_likelihood).min(), 50\nlevel = (((vmin), (vmax), num=50), decimals=1)\n(\n    length_scale_grid,\n    noise_level_grid,\n    -log_marginal_likelihood,\n    levels=level,\n    norm=(vmin=vmin, vmax=vmax),\n)\n()\n(\"log\")\n(\"log\")\n(\"Length-scale\")\n(\"Noise-level\")\n(\"Log-marginal-likelihood\")\n()\n\n\n<img alt=\"Log-marginal-likelihood\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_noisy_005.png\" srcset=\"../../_images/sphx_glr_plot_gpr_noisy_005.png\"/>",
            "glm = smf.glm(\n    \"affairs ~ rate_marriage + yrs_married\",\n    data=dc,\n    family=sm.families.Poisson(),\n    freq_weights=np.asarray(dc[\"freq\"]),\n)\nres_f2 = glm.fit()\n# print(res_f2.summary())\nres_f2.pearson_chi2 - res_f.pearson_chi2, res_f2.deviance - res_f.deviance, res_f2.llf - res_f.llf",
            "res2_DL = combine_effects(eff, var_eff, method_re=\"dl\", use_t=True, row_names=rownames)\nprint(\"method RE:\", res2_DL.method_re)\nprint(res2_DL.summary_frame())\nfig = res2_DL.plot_forest()\nfig.set_figheight(6)\nfig.set_figwidth(6)",
            "cluster_ids = (dist_linkage, 1, criterion=\"distance\")\ncluster_id_to_feature_ids = (list)\nfor idx, cluster_id in enumerate(cluster_ids):\n    cluster_id_to_feature_ids[cluster_id].append(idx)\nselected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n\nX_train_sel = X_train[:, selected_features]\nX_test_sel = X_test[:, selected_features]\n\nclf_sel = (n_estimators=100, random_state=42)\nclf_sel.fit(X_train_sel, y_train)\nprint(\n    \"Accuracy on test data with features removed: {:.2f}\".format(\n        clf_sel.score(X_test_sel, y_test)\n    )\n)",
            "res2_PM = combine_effects(eff, var_eff, method_re=\"pm\", use_t=True, row_names=rownames)\nprint(\"method RE:\", res2_PM.method_re)\nprint(res2_PM.summary_frame())\nfig = res2_PM.plot_forest()\nfig.set_figheight(6)\nfig.set_figwidth(6)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) with noise-level estimation->Optimisation of kernel hyperparameters in GPR"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->original observations and frequency weights"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Example Kacker interlaboratory mean"
            ],
            [
                "sklearn->Examples->Inspection->Permutation Importance with Multicollinear or Correlated Features->Handling Multicollinear Features"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Example Kacker interlaboratory mean"
            ]
        ]
    },
    "1488721": {
        "jupyter_code_cell": "for i in range(0,10000):\n    lastPrice = appleData['price'][251]\n    twentyChanges = np.random.normal(mu, sigma, 20)\n    nextTwenty = []\n    for each in twentyChanges:\n        lastPrice = lastPrice + lastPrice*each\n        nextTwenty.append(lastPrice)\n    finalPrice.append(nextTwenty[19])\nnp.percentile(finalPrice,1)",
        "matched_tutorial_code_inds": [
            6258,
            6655,
            3167,
            1085,
            58
        ],
        "matched_tutorial_codes": [
            "for fit in [fit2, fit4]:\n    pd.DataFrame(np.c_[fit.level, fit.trend]).rename(\n        columns={0: \"level\", 1: \"slope\"}\n    ).plot(subplots=True)\nplt.show()\nprint(\n    \"Figure 7.4: Level and slope components for Holt\u2019s linear trend method and the additive damped trend method.\"\n)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_0.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_0.png\"/>\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_1.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_1.png\"/>",
            "for i in range(simulated.shape[1]):\n    simulated.iloc[:, i].plot(label=\"_\", color=\"gray\", alpha=0.1)\ndf[\"mean\"].plot(label=\"mean prediction\")\ndf[\"pi_lower\"].plot(linestyle=\"--\", color=\"tab:blue\", label=\"95% interval\")\ndf[\"pi_upper\"].plot(linestyle=\"--\", color=\"tab:blue\", label=\"_\")\npred.endog.plot(label=\"data\")\nplt.legend()",
            "for i in range(n_classes):\n    fpr[i], tpr[i], _ = (y_onehot_test[:, i], y_score[:, i])\n    roc_auc[i] = (fpr[i], tpr[i])\n\nfpr_grid = (0.0, 1.0, 1000)\n\n# Interpolate all ROC curves at these points\nmean_tpr = (fpr_grid)\n\nfor i in range(n_classes):\n    mean_tpr += (fpr_grid, fpr[i], tpr[i])  # linear interpolation\n\n# Average it and compute AUC\nmean_tpr /= n_classes\n\nfpr[\"macro\"] = fpr_grid\ntpr[\"macro\"] = mean_tpr\nroc_auc[\"macro\"] = (fpr[\"macro\"], tpr[\"macro\"])\n\nprint(f\"Macro-averaged One-vs-Rest ROC AUC score:\\n{roc_auc['macro']:.2f}\")",
            "for param in model_ft.parameters():\n  param.requires_grad = True\n\nmodel_ft.to(device)  # We can fine-tune on GPU if available\n\ncriterion = nn.CrossEntropyLoss()\n\n# Note that we are training everything, so the learning rate is lower\n# Notice the smaller learning rate\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=1e-3, momentum=0.9, weight_decay=0.1)\n\n# Decay LR by a factor of 0.3 every several epochs\nexp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=5, gamma=0.3)\n\nmodel_ft_tuned = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n                             num_epochs=25, device=device)",
            "for epoch in range(2):  # loop over the dataset multiple times\n\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 2000 == 1999:    # print every 2000 mini-batches\n            print('[%d, %5d] loss: %.3f' %\n                  (epoch + 1, i + 1, running_loss / 2000))\n            running_loss = 0.0\n\nprint('Finished Training')"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Method->Plots of Seasonally Adjusted Data"
            ],
            [
                "statsmodels->Examples->State space models->ETS models->Predictions"
            ],
            [
                "sklearn->Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->One-vs-Rest multiclass ROC->ROC curve using the OvR macro-average"
            ],
            [
                "torch->Model Optimization->(beta) Quantized Transfer Learning for Computer Vision Tutorial->Part 2. Finetuning the Quantizable Model->Finetuning the model"
            ],
            [
                "torch->PyTorch Recipes->Zeroing out gradients in PyTorch->Steps->5. Zero the gradients while training the network"
            ]
        ]
    },
    "176258": {
        "jupyter_code_cell": "seaborn.factorplot(data = tips,\n                  x = \"sex\",\n                  y = \"tip\",\n                  kind = \"swarm\",\n                   hue = \"smoker\",\n                  col = \"day\",\n                  col_wrap = 2,\n                  size = 6)\ntips[\"tip_bracket\"] = pandas.cut(x = tips[\"tip\"],bins=5)",
        "matched_tutorial_code_inds": [
            3933,
            3936,
            4087,
            4075,
            3907
        ],
        "matched_tutorial_codes": [
            "sns.relplot(data=flights, x=\"year\", y=\"passengers\", hue=\"month\", kind=\"line\")\n",
            "sns.relplot(data=flights, x=\"month\", y=\"passengers\", hue=\"year\", kind=\"line\")\n",
            "sns.catplot(data=titanic, x=\"sex\", y=\"survived\", hue=\"class\", kind=\"point\")\n",
            "sns.catplot(data=tips, x=\"day\", y=\"total_bill\", hue=\"smoker\", kind=\"box\")\n",
            "sns.displot(data=tips, x=\"total_bill\", col=\"time\", kde=True)\n"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Long-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Long-form data"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing categorical data->Estimating central tendency->Point plots",
                "seaborn->Plotting functions->Visualizing categorical data->Estimating central tendency->Point plots"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing categorical data->Comparing distributions->Boxplots",
                "seaborn->Plotting functions->Visualizing categorical data->Comparing distributions->Boxplots"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics->Distributional representations"
            ]
        ]
    },
    "767468": {
        "jupyter_code_cell": "fig = plt.figure(figsize=(16,4))\nfig.suptitle('Saskatoon house listings and sales', fontsize=14, fontweight='bold')\nax = fig.add_subplot(111)\nax.set_xlabel('Year')\nax.set_ylabel('# of listings/sales')\nplt.plot(listings['date_time'], listings['Listings'], 'b-', label='Listings')\nplt.plot(listings['date_time'], listings['Sales'], 'r-', label='Sales')\nplt.legend(loc='upper left')\nplt.show()\nlistings_sales_differences = listings['Listings'] - listings['Sales']\nprint('Index  Difference')\nprint(listings_sales_differences.sort_values(ascending=False).head())",
        "matched_tutorial_code_inds": [
            5477,
            4840,
            5550,
            5541,
            5856
        ],
        "matched_tutorial_codes": [
            "fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(\n    111,\n    title=\"Residual Dependence Plot\",\n    xlabel=\"Fitted Values\",\n    ylabel=\"Pearson Residuals\",\n)\nax.scatter(yhat, stats.zscore(glm_mod.resid_pearson))\nax.axis(\"tight\")\nax.plot([0.0, 1.0], [0.0, 0.0], \"k-\")",
            "fig = plt.figure(layout=None, facecolor='0.9')\ngs = fig.add_gridspec(nrows=3, ncols=3, left=0.05, right=0.75,\n                      hspace=0.1, wspace=0.05)\nax0 = fig.add_subplot(gs[:-1, :])\nannotate_axes(ax0, 'ax0')\nax1 = fig.add_subplot(gs[-1, :-1])\nannotate_axes(ax1, 'ax1')\nax2 = fig.add_subplot(gs[-1, -1])\nannotate_axes(ax2, 'ax2')\nfig.suptitle('Manual gridspec with right=0.75')\n\n\n<img alt=\"Manual gridspec with right=0.75\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_arranging_axes_011.png\" srcset=\"../../_images/sphx_glr_arranging_axes_011.png, ../../_images/sphx_glr_arranging_axes_011_2_0x.png 2.0x\"/>",
            "fig = plt.figure(figsize=(12, 5))\nax = fig.add_subplot(111)\nax.hist(obs_dist, bins=20, density=True, edgecolor=\"k\", zorder=4, alpha=0.5)\nax.plot(kde.support, kde.density, lw=3, zorder=7)\n# Plot the samples\nax.scatter(\n    obs_dist,\n    np.abs(np.random.randn(obs_dist.size)) / 50,\n    marker=\"x\",\n    color=\"red\",\n    zorder=20,\n    label=\"Data samples\",\n    alpha=0.5,\n)\nax.grid(True, zorder=-5)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_kernel_density_28_0.png\" src=\"../../../_images/examples_notebooks_generated_kernel_density_28_0.png\"/>",
            "fig = plt.figure(figsize=(12, 5))\nax = fig.add_subplot(111)\n\n# Plot the histrogram\nax.hist(\n    obs_dist,\n    bins=20,\n    density=True,\n    label=\"Histogram from samples\",\n    zorder=5,\n    edgecolor=\"k\",\n    alpha=0.5,\n)\n\n# Plot the KDE as fitted using the default arguments\nax.plot(kde.support, kde.density, lw=3, label=\"KDE from samples\", zorder=10)\n\n# Plot the true distribution\ntrue_values = (\n    stats.norm.pdf(loc=dist1_loc, scale=dist1_scale, x=kde.support) * weight1\n    + stats.norm.pdf(loc=dist2_loc, scale=dist2_scale, x=kde.support) * weight2\n)\nax.plot(kde.support, true_values, lw=3, label=\"True distribution\", zorder=15)\n\n# Plot the samples\nax.scatter(\n    obs_dist,\n    np.abs(np.random.randn(obs_dist.size)) / 40,\n    marker=\"x\",\n    color=\"red\",\n    zorder=20,\n    label=\"Samples\",\n    alpha=0.5,\n)\n\nax.legend(loc=\"best\")\nax.grid(True, zorder=-5)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_kernel_density_12_0.png\" src=\"../../../_images/examples_notebooks_generated_kernel_density_12_0.png\"/>",
            "fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nax.plot(x1, y2, \"o\", label=\"data\")\nax.plot(x1, y_true2, \"b-\", label=\"True\")\npred_ols = res.get_prediction()\niv_l = pred_ols.summary_frame()[\"obs_ci_lower\"]\niv_u = pred_ols.summary_frame()[\"obs_ci_upper\"]\n\nax.plot(x1, res.fittedvalues, \"r-\", label=\"OLS\")\nax.plot(x1, iv_u, \"r--\")\nax.plot(x1, iv_l, \"r--\")\nax.plot(x1, resrlm.fittedvalues, \"g.-\", label=\"RLM\")\nax.legend(loc=\"best\")"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Plot fitted values vs Pearson residuals"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Arranging multiple Axes in a Figure->Low-level and advanced grid methods->Manual adjustments to a GridSpec layout"
            ],
            [
                "statsmodels->Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->A more difficult case"
            ],
            [
                "statsmodels->Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->Fitting with the default arguments"
            ],
            [
                "statsmodels->Examples->Robust Regression->Comparing OLS and RLM->Comparing OLS and RLM->Example 1: quadratic function with linear truth"
            ]
        ]
    },
    "306288": {
        "jupyter_code_cell": "datagen=ImageDataGenerator(rotation_range=10,zoom_range=0.1,width_shift_range=0.1,height_shift_range=0.1)\ndatagen.fit(X_train)\nepoch=2\nbatch=100\nsp_epoch=X_train.shape[0]",
        "matched_tutorial_code_inds": [
            6594,
            1712,
            2248,
            5571,
            5400
        ],
        "matched_tutorial_codes": [
            "mod_pre = sm.tsa.DynamicFactor(y_pre, exog=const_pre, k_factors=1, factor_order=6)\nres_pre = mod_pre.fit()\nprint(res_pre.summary())",
            "preprocessed_random_frame = frame_preprocessing(random_frame)\nplt.imshow(preprocessed_random_frame, cmap=\"gray\")\nprint(preprocessed_random_frame.shape)",
            "ada_real = (\n    estimator=dt_stump,\n    learning_rate=learning_rate,\n    n_estimators=n_estimators,\n    algorithm=\"SAMME.R\",\n)\nada_real.fit(X_train, y_train)",
            "glm_binom = sm.GLM(data.endog, data.exog, family=sm.families.Binomial())\nres = glm_binom.fit()\nprint(res.summary())",
            "poisson_mod = sm.Poisson(rand_data.endog, rand_exog)\npoisson_res = poisson_mod.fit(method=\"newton\")\nprint(poisson_res.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor"
            ],
            [
                "numpy->Articles->Deep reinforcement learning with Pong from pixels->Preprocess frames (the observation)"
            ],
            [
                "sklearn->Examples->Ensemble methods->Discrete versus Real AdaBoost->Adaboost with discrete SAMME and real SAMME.R"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Generalized Linear Models Overview->GLM: Binomial response data->Fit and summary"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Poisson"
            ]
        ]
    },
    "1036325": {
        "jupyter_code_cell": "matching_propensity = nx.max_weight_matching(G)\nmatching_propensity = {k:v for (k,v) in matching_propensity.items() if \"NSW\" in k}\nlen(matching_propensity)\ndef observe_matching(matching, verbose):\n    sum_differences = 0\n    for (a, b) in matching.items():\n        propensity_score_a = data_propensity.loc[a][\"propensity\"]\n        propensity_score_b = data_propensity.loc[b][\"propensity\"]\n        diff = abs(propensity_score_a - propensity_score_b)\n        if verbose:\n            print(\"Difference between {} and {} is {}\".format(a, b, diff))\n        sum_differences += diff\n    print(\"Mean difference in prop score is {}\".format(sum_differences / len(matching)))\nobserve_matching(matching_propensity, False)",
        "matched_tutorial_code_inds": [
            2951,
            5643,
            5911,
            2096,
            5917
        ],
        "matched_tutorial_codes": [
            "cluster_ids = (dist_linkage, 1, criterion=\"distance\")\ncluster_id_to_feature_ids = (list)\nfor idx, cluster_id in enumerate(cluster_ids):\n    cluster_id_to_feature_ids[cluster_id].append(idx)\nselected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n\nX_train_sel = X_train[:, selected_features]\nX_test_sel = X_test[:, selected_features]\n\nclf_sel = (n_estimators=100, random_state=42)\nclf_sel.fit(X_train_sel, y_train)\nprint(\n    \"Accuracy on test data with features removed: {:.2f}\".format(\n        clf_sel.score(X_test_sel, y_test)\n    )\n)",
            "glm = smf.glm(\n    \"affairs ~ rate_marriage + yrs_married\",\n    data=dc,\n    family=sm.families.Poisson(),\n    freq_weights=np.asarray(dc[\"freq\"]),\n)\nres_f2 = glm.fit()\n# print(res_f2.summary())\nres_f2.pearson_chi2 - res_f.pearson_chi2, res_f2.deviance - res_f.deviance, res_f2.llf - res_f.llf",
            "infl = interM_lm.get_influence()\nresid = infl.resid_studentized_internal\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        resid[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\nplt.xlabel(\"X\")\nplt.ylabel(\"standardized resids\")",
            "node_indicator = clf.decision_path(X_test)\nleaf_id = clf.apply(X_test)\n\nsample_id = 0\n# obtain ids of the nodes `sample_id` goes through, i.e., row `sample_id`\nnode_index = node_indicator.indices[\n    node_indicator.indptr[sample_id] : node_indicator.indptr[sample_id + 1]\n]\n\nprint(\"Rules used to predict sample {id}:\\n\".format(id=sample_id))\nfor node_id in node_index:\n    # continue to the next node if it is a leaf node\n    if leaf_id[sample_id] == node_id:\n        continue\n\n    # check if value of the split feature for sample 0 is below threshold\n    if X_test[sample_id, feature[node_id]] &lt;= threshold[node_id]:\n        threshold_sign = \"&lt;=\"\n    else:\n        threshold_sign = \"\"\n\n    print(\n        \"decision node {node} : (X_test[{sample}, {feature}] = {value}) \"\n        \"{inequality} {threshold})\".format(\n            node=node_id,\n            sample=sample_id,\n            feature=feature[node_id],\n            value=X_test[sample_id, feature[node_id]],\n            inequality=threshold_sign,\n            threshold=threshold[node_id],\n        )\n    )",
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Inspection->Permutation Importance with Multicollinear or Correlated Features->Handling Multicollinear Features"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->original observations and frequency weights"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "sklearn->Examples->Decision Trees->Understanding the decision tree structure->Decision path"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ]
        ]
    },
    "1160265": {
        "jupyter_code_cell": "X_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\nX_train /= 255\nX_test /= 255\nprint(train_label.shape)",
        "matched_tutorial_code_inds": [
            3306,
            5191,
            5312,
            631,
            5319
        ],
        "matched_tutorial_codes": [
            "Y_pred = raw_pixel_classifier.predict(X_test)\nprint(\n    \"Logistic regression using raw pixel features:\\n%s\\n\"\n    % ((Y_test, Y_pred))\n)",
            "ols_model = sm.OLS(y, X)\nols_results = ols_model.fit()\nprint(ols_results.summary())",
            "oo = np.ones(df.shape[0])\nmodel2 = sm.MixedLM(df.y, oo, exog_re=oo, groups=df.group1, exog_vc=vcs)\nresult2 = model2.fit()\nprint(result2.summary())",
            "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(img_y)}\nort_outs = ort_session.run(None, ort_inputs)\nimg_out_y = ort_outs[0]",
            "oo = np.ones(df.shape[0])\nmodel4 = sm.MixedLM(df.y, oo[:, None], exog_re=None, groups=oo, exog_vc=vcs)\nresult4 = model4.fit()\nprint(result4.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Neural Networks->Restricted Boltzmann Machine features for digit classification->Evaluation"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Nested analysis"
            ],
            [
                "torch->Deploying PyTorch Models in Production->(optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime->Running the model on an image using ONNX Runtime"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Crossed analysis"
            ]
        ]
    },
    "805831": {
        "jupyter_code_cell": "reload(vocab_index_descriptions)\nreload(datasets)\nvocab_index_descriptions.vocab_index_descriptions('%s/vocab.csv' % MIMIC_3_DIR,\n                                                  '%s/description_vectors.vocab' % MIMIC_3_DIR)\nY = 50",
        "matched_tutorial_code_inds": [
            3400,
            5956,
            5917,
            2114,
            1615
        ],
        "matched_tutorial_codes": [
            "make_plot(5)\nmake_plot(6)\n\n\n\n<img alt=\"Data after power transformation (Yeo-Johnson), Full data, Zoom-in\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_all_scaling_006.png\" srcset=\"../../_images/sphx_glr_plot_all_scaling_006.png\"/>\n<img alt=\"Data after power transformation (Box-Cox), Full data, Zoom-in\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_all_scaling_007.png\" srcset=\"../../_images/sphx_glr_plot_all_scaling_007.png\"/>",
            "kidney_lm = ols(\"np.log(Days+1) ~ C(Duration) * C(Weight)\", data=kt).fit()\n\ntable10 = anova_lm(kidney_lm)\n\nprint(\n    anova_lm(ols(\"np.log(Days+1) ~ C(Duration) + C(Weight)\", data=kt).fit(), kidney_lm)\n)\nprint(\n    anova_lm(\n        ols(\"np.log(Days+1) ~ C(Duration)\", data=kt).fit(),\n        ols(\"np.log(Days+1) ~ C(Duration) + C(Weight, Sum)\", data=kt).fit(),\n    )\n)\nprint(\n    anova_lm(\n        ols(\"np.log(Days+1) ~ C(Weight)\", data=kt).fit(),\n        ols(\"np.log(Days+1) ~ C(Duration) + C(Weight, Sum)\", data=kt).fit(),\n    )\n)",
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "batch_dict_estimator = (\n    n_components=n_components, alpha=0.1, max_iter=50, batch_size=3, random_state=rng\n)\nbatch_dict_estimator.fit(faces_centered)\nplot_gallery(\"Dictionary learning\", batch_dict_estimator.components_[:n_components])\n\n\n<img alt=\"Dictionary learning\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_006.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_006.png\"/>",
            "fourier_gaussian = ndimage.fourier_gaussian(xray_image, sigma=0.05)\n\nx_prewitt = ndimage.prewitt(fourier_gaussian, axis=0)\ny_prewitt = ndimage.prewitt(fourier_gaussian, axis=1)\n\nxray_image_canny = np.hypot(x_prewitt, y_prewitt)\n\nxray_image_canny *= 255.0 / np.max(xray_image_canny)\n\nprint(\"The data type - \", xray_image_canny.dtype)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Preprocessing->Compare the effect of different scalers on data with outliers->PowerTransformer"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA->Two-way ANOVA"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition->Dictionary learning"
            ],
            [
                "numpy->NumPy Applications->X-ray image processing->Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters->The Canny filter"
            ]
        ]
    },
    "606503": {
        "jupyter_code_cell": "titanic.info()\ntitanic.drop([\"PassengerId\",\"Name\",\"Ticket\",\"Cabin\"], axis=1, inplace=True)",
        "matched_tutorial_code_inds": [
            4124,
            3994,
            4025,
            3784,
            4067
        ],
        "matched_tutorial_codes": [
            "tips = sns.load_dataset(\"tips\")\ng = sns.FacetGrid(tips, col=\"time\")\n",
            "tips = sns.load_dataset(\"tips\")\nsns.relplot(data=tips, x=\"total_bill\", y=\"tip\")\n",
            "tips = sns.load_dataset(\"tips\")\nsns.displot(tips, x=\"size\")\n",
            "import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "tips = sns.load_dataset(\"tips\")\nsns.catplot(data=tips, x=\"day\", y=\"total_bill\")\n"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Conditional small multiples",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Conditional small multiples"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Relating variables with scatter plots",
                "seaborn->Plotting functions->Visualizing statistical relationships->Relating variables with scatter plots"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size",
                "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing categorical data->Categorical scatterplots",
                "seaborn->Plotting functions->Visualizing categorical data->Categorical scatterplots"
            ]
        ]
    },
    "476671": {
        "jupyter_code_cell": "header = ['first', 'last', 'gender', 'proportion']\ndf = pd.DataFrame(columns=header)\ndf_lt90 = pd.DataFrame(columns=header)",
        "matched_tutorial_code_inds": [
            3616,
            5703,
            5143,
            3852,
            3610
        ],
        "matched_tutorial_codes": [
            "hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()",
            "summ_df = infl.summary_frame()\nsumm_df.sort_values(\"cooks_d\", ascending=False)[:10]",
            "sqtab = sm.stats.SquareTable.from_data(data[['left', 'right']])\nprint(sqtab.summary())",
            "pred = res_seasonal.get_prediction(start='2001-03-01')\npred_ci = pred.conf_int()",
            "first = df.groupby('unique_carrier').first()\nfirst.ix[10:15, ['fl_date', 'tail_num']]"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Influence Measures for GLMs->get the influence measures"
            ],
            [
                "statsmodels->User Guide->Statistics and Tools->Contingency tables->Symmetry and homogeneity"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Forecasting"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing"
            ]
        ]
    },
    "1315501": {
        "jupyter_code_cell": "from sklearn.cross_validation import train_test_split\nitrain, itest = train_test_split(xrange(len(Xarray)), train_size=0.7)\nmask=np.ones(len(Xarray), dtype='int')\nmask[itrain]=1\nmask[itest]=0\nmask = (mask==1)\ndef make_xy(X_col, y_col, vectorizer):\n    X = vectorizer.fit_transform(X_col)\n    y = y_col\n    return X, y",
        "matched_tutorial_code_inds": [
            2355,
            2848,
            2018,
            2863,
            2931
        ],
        "matched_tutorial_codes": [
            "from sklearn.base import clone\n\nalpha = 0.95\nneg_mean_pinball_loss_95p_scorer = (\n    ,\n    alpha=alpha,\n    greater_is_better=False,  # maximize the negative loss\n)\nsearch_95p = clone(search_05p).set_params(\n    estimator__alpha=alpha,\n    scoring=neg_mean_pinball_loss_95p_scorer,\n)\nsearch_95p.fit(X_train, y_train)\n(search_95p.best_params_)",
            "from sklearn.linear_model import \n\nalphas = (-10, 10, 21)  # alpha values to be chosen from by cross-validation\nmodel = (\n    preprocessor,\n    (\n        regressor=(alphas=alphas),\n        func=,\n        inverse_func=,\n    ),\n)\nmodel.fit(X_train, y_train)",
            "from sklearn.cluster import \nimport matplotlib.pyplot as plt\n\nlabels = (graph, n_clusters=4, eigen_solver=\"arpack\")\nlabel_im = (mask.shape, -1.0)\nlabel_im[mask] = labels\n\nfig, axs = (nrows=1, ncols=2, figsize=(10, 5))\naxs[0].matshow(img)\naxs[1].matshow(label_im)\n\n()\n\n\n<img alt=\"plot segmentation toy\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_segmentation_toy_001.png\" srcset=\"../../_images/sphx_glr_plot_segmentation_toy_001.png\"/>",
            "from sklearn.linear_model import \n\nalphas = (-10, 10, 21)  # alpha values to be chosen from by cross-validation\nmodel = (\n    preprocessor,\n    (\n        regressor=(alphas=alphas, max_iter=100_000),\n        func=,\n        inverse_func=,\n    ),\n)\n\n_ = model.fit(X_train, y_train)",
            "from sklearn.inspection import \n\nresult = (\n    rf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\n\nsorted_importances_idx = result.importances_mean.argsort()\nimportances = (\n    result.importances[sorted_importances_idx].T,\n    columns=X.columns[sorted_importances_idx],\n)\nax = importances.plot.box(vert=False, whis=10)\nax.set_title(\"Permutation Importances (test set)\")\nax.axvline(x=0, color=\"k\", linestyle=\"--\")\nax.set_xlabel(\"Decrease in accuracy score\")\nax.figure.tight_layout()\n\n\n<img alt=\"Permutation Importances (test set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_permutation_importance_002.png\" srcset=\"../../_images/sphx_glr_plot_permutation_importance_002.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Ensemble methods->Prediction Intervals for Gradient Boosting Regression->Tuning the hyper-parameters of the quantile regressors"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "sklearn->Examples->Clustering->Spectral clustering for image segmentation->Plotting four circles"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)"
            ]
        ]
    },
    "324533": {
        "jupyter_code_cell": "elizabethSentences = [sent for sent in pride.sents if 'Elizabeth' in sent.string]\nelizabethSentences[3]",
        "matched_tutorial_code_inds": [
            3947,
            5423,
            3994,
            3951,
            1795
        ],
        "matched_tutorial_codes": [
            "flights_wide_list = [col for _, col in flights_wide.items()]\nsns.relplot(data=flights_wide_list, kind=\"line\")\n",
            "mfx = affair_mod.get_margeff()\nprint(mfx.summary())",
            "tips = sns.load_dataset(\"tips\")\nsns.relplot(data=tips, x=\"total_bill\", y=\"tip\")\n",
            "flights_array = flights_wide.to_numpy()\nsns.relplot(data=flights_array, kind=\"line\")\n",
            "encoded = enc.transform(([[\"dog\"], [\"snake\"], [\"cat\"], [\"rabbit\"]]))\n(encoded, columns=enc.get_feature_names_out())\n\n\n\n\n\n\n\n\n<br/>\n<br/>"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing wide-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing wide-form data"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Relating variables with scatter plots",
                "seaborn->Plotting functions->Visualizing statistical relationships->Relating variables with scatter plots"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing wide-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing wide-form data"
            ],
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 1.1->Grouping infrequent categories in OneHotEncoder"
            ]
        ]
    },
    "1403304": {
        "jupyter_code_cell": "dc_idx = dc.set_index('START_DATE')\nprint pd.crosstab(dc.OFFENSE, dc_idx.index.weekday, colnames=['Day of Week'], margins=True)\nprint ''\nprint pd.crosstab(dc.OFFENSE, dc_idx.index.weekday, colnames=['Day of Week']).apply(percentConv, axis=1)\nplt.figure(figsize=(30,10))\nsns.set(font_scale=2)\nplt_test = dc[dc.TIME_TO_REPORT < 86400][dc.TIME_TO_REPORT > 0]\nsns.boxplot(x=plt_test.OFFENSE, y=plt_test.TIME_TO_REPORT/3600.0, hue=plt_test.SHIFT)\nplt.legend(loc='upper right')",
        "matched_tutorial_code_inds": [
            4654,
            2446,
            4090,
            5917,
            28
        ],
        "matched_tutorial_codes": [
            "data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>",
            "last_hours = slice(-96, None)\nfig, ax = (figsize=(12, 4))\nfig.suptitle(\"Predictions by linear models\")\nax.plot(\n    y.iloc[test_0].values[last_hours],\n    \"x-\",\n    alpha=0.2,\n    label=\"Actual demand\",\n    color=\"black\",\n)\nax.plot(naive_linear_predictions[last_hours], \"x-\", label=\"Ordinal time features\")\nax.plot(\n    cyclic_cossin_linear_predictions[last_hours],\n    \"x-\",\n    label=\"Trigonometric time features\",\n)\nax.plot(\n    cyclic_spline_linear_predictions[last_hours],\n    \"x-\",\n    label=\"Spline-based time features\",\n)\nax.plot(\n    one_hot_linear_predictions[last_hours],\n    \"x-\",\n    label=\"One-hot time features\",\n)\n_ = ax.legend()\n\n\n<img alt=\"Predictions by linear models\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_006.png\" srcset=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_006.png\"/>",
            "g = sns.catplot(\n    data=titanic,\n    x=\"fare\", y=\"embark_town\", row=\"class\",\n    kind=\"box\", orient=\"h\",\n    sharex=False, margin_titles=True,\n    height=1.5, aspect=4,\n)\ng.set(xlabel=\"Fare\", ylabel=\"\")\ng.set_titles(row_template=\"{row_name} class\")\nfor ax in g.axes.flat:\n    ax.xaxis.set_major_formatter('${x:.0f}')\n",
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "dataloader = DataLoader(transformed_dataset, batch_size=4,\n                        shuffle=True, num_workers=4)\n\n\n# Helper function to show a batch\ndef show_landmarks_batch(sample_batched):\n    \"\"\"Show image with landmarks for a batch of samples.\"\"\"\n    images_batch, landmarks_batch = \\\n            sample_batched['image'], sample_batched['landmarks']\n    batch_size = len(images_batch)\n    im_size = images_batch.size(2)\n\n    grid = utils.make_grid(images_batch)\n    plt.imshow(grid.numpy().transpose((1, 2, 0)))\n\n    for i in range(batch_size):\n        plt.scatter(landmarks_batch[i, :, 0].numpy() + i * im_size,\n                    landmarks_batch[i, :, 1].numpy(),\n                    s=10, marker='.', c='r')\n\n        plt.title('Batch from dataloader')\n\nfor i_batch, sample_batched in enumerate(dataloader):\n    print(i_batch, sample_batched['image'].size(),\n          sample_batched['landmarks'].size())\n\n    # observe 4th batch and stop.\n    if i_batch == 3:\n        plt.figure()\n        show_landmarks_batch(sample_batched)\n        plt.axis('off')\n        plt.ioff()\n        plt.show()\n        break"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Qualitative analysis of the impact of features on linear model predictions"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing categorical data->Showing additional dimensions",
                "seaborn->Plotting functions->Visualizing categorical data->Showing additional dimensions"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "torch->PyTorch Recipes->Developing Custom PyTorch Dataloaders->Part 3: The Dataloader"
            ]
        ]
    },
    "850846": {
        "jupyter_code_cell": "titanic_ds = pd.read_csv('./titanic-data.csv')\ntitanic_ds.describe()\ntitanic_ds.isnull().sum()",
        "matched_tutorial_code_inds": [
            5381,
            3784,
            5447,
            5388,
            3861
        ],
        "matched_tutorial_codes": [
            "spector_data = sm.datasets.spector.load()\nspector_data.exog = sm.add_constant(spector_data.exog, prepend=False)",
            "import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "dta = sm.datasets.star98.load_pandas().data\nprint(dta.columns)",
            "margeff = logit_res.get_margeff()\nprint(margeff.summary())",
            "df = dd.read_parquet(\"data/indiv-*.parquet\", engine='pyarrow', columns=['occupation'])\n\nmost_common = df.occupation.value_counts().nlargest(100)\nmost_common.compute().sort_values(ascending=False)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Data"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Logit Model"
            ],
            [
                "pandas_toms_blog->Scaling->Using Dask"
            ]
        ]
    },
    "603955": {
        "jupyter_code_cell": "air.gender.value_counts(ascending=False).plot(kind='pie', autopct='%.2f', figsize=(6,6),title='Distribution of Gender',fontsize=10)\nair.signup_method.value_counts(ascending=False).plot(kind='pie', autopct='%.2f',figsize=(6,6),title='Distribution of Signup Method',fontsize=12)",
        "matched_tutorial_code_inds": [
            2944,
            2088,
            2943,
            3400,
            6264
        ],
        "matched_tutorial_codes": [
            "train_importances = (\n    train_result.importances[sorted_importances_idx].T,\n    columns=X.columns[sorted_importances_idx],\n)\ntest_importances = (\n    test_results.importances[sorted_importances_idx].T,\n    columns=X.columns[sorted_importances_idx],\n)",
            "train_scores = [clf.score(X_train, y_train) for clf in clfs]\ntest_scores = [clf.score(X_test, y_test) for clf in clfs]\n\nfig, ax = ()\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker=\"o\", label=\"test\", drawstyle=\"steps-post\")\nax.legend()\n()\n\n\n<img alt=\"Accuracy vs alpha for training and testing sets\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cost_complexity_pruning_003.png\" srcset=\"../../_images/sphx_glr_plot_cost_complexity_pruning_003.png\"/>",
            "train_result = (\n    rf, X_train, y_train, n_repeats=10, random_state=42, n_jobs=2\n)\ntest_results = (\n    rf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_importances_idx = train_result.importances_mean.argsort()",
            "make_plot(5)\nmake_plot(6)\n\n\n\n<img alt=\"Data after power transformation (Yeo-Johnson), Full data, Zoom-in\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_all_scaling_006.png\" srcset=\"../../_images/sphx_glr_plot_all_scaling_006.png\"/>\n<img alt=\"Data after power transformation (Box-Cox), Full data, Zoom-in\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_all_scaling_007.png\" srcset=\"../../_images/sphx_glr_plot_all_scaling_007.png\"/>",
            "fit1 = ExponentialSmoothing(\n    aust,\n    seasonal_periods=4,\n    trend=\"add\",\n    seasonal=\"add\",\n    initialization_method=\"estimated\",\n).fit()\nfit2 = ExponentialSmoothing(\n    aust,\n    seasonal_periods=4,\n    trend=\"add\",\n    seasonal=\"mul\",\n    initialization_method=\"estimated\",\n).fit()"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)"
            ],
            [
                "sklearn->Examples->Decision Trees->Post pruning decision trees with cost complexity pruning->Accuracy vs alpha for training and testing sets"
            ],
            [
                "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)"
            ],
            [
                "sklearn->Examples->Preprocessing->Compare the effect of different scalers on data with outliers->PowerTransformer"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Winters Seasonal->The Internals"
            ]
        ]
    },
    "602423": {
        "jupyter_code_cell": "df['game_type'] = df[['TNF', 'Sunday_AM', 'Sunday_PM', 'SNF', 'MNF', 'England', 'Saturday']].idxmax(axis=1)\ndef time_btwn(x):\n    week = x[0]\n    team = x[1]\n    season = x[2]\n    game_t = {'TNF':0, 'Sunday_AM':3, 'Sunday_PM':3, 'SNF':3, 'MNF':4, 'England':3, 'Saturday':2}\n    try: \n        week_after = df[((df['Week']==week) & (df['Season']==season)) & ((df['Home_team']==team) | (df['Away_team']==team))]['game_type'].iloc[0]\n        try:\n            week_before = df[((df['Week']==week-1) & (df['Season']==season)) & ((df['Home_team']==team) | (df['Away_team']==team))]['game_type'].iloc[0]\n            weekly = 7\n        except:\n            week_before = df[((df['Week']==week-2) & (df['Season']==season)) & ((df['Home_team']==team) | (df['Away_team']==team))]['game_type'].iloc[0]\n            weekly = 14\n        tb = game_t[week_after] + weekly - game_t[week_before]\n    except:\n        tb = 7 \n    return tb\ndf['Home_team_days_after_last_game'] = df[['Week','Home_team','Season']].apply(time_btwn, axis=1)\ndf['Away_team_days_after_last_game'] = df[['Week','Away_team','Season']].apply(time_btwn, axis=1)\nfor row in range(len(df[(df['Week']==2) & (df['Season']==2017)])):\n    if (df['Home_team'][row]=='MIA') | (df['Home_team'][row]=='TAM') | (df['Away_team'][row]=='MIA') | (df['Away_team'][row]=='TAM'):\n        df = df.drop(row)",
        "matched_tutorial_code_inds": [
            3769,
            3773,
            2446,
            3765,
            4654
        ],
        "matched_tutorial_codes": [
            "win_percent = (\n    # Use sum(games) / sum(games) instead of mean\n    # since I don't know if teams play the same\n    # number of games at home as away\n    wins.groupby(level='team', as_index=True)\n        .apply(lambda x: x.n_wins.sum() / x.n_games.sum())\n)\nwin_percent.head()",
            "df = df.assign(away_strength=df['away_team'].map(win_percent),\n               home_strength=df['home_team'].map(win_percent),\n               point_diff=df['home_points'] - df['away_points'],\n               rest_diff=df['home_rest'] - df['away_rest'])\ndf.head()",
            "last_hours = slice(-96, None)\nfig, ax = (figsize=(12, 4))\nfig.suptitle(\"Predictions by linear models\")\nax.plot(\n    y.iloc[test_0].values[last_hours],\n    \"x-\",\n    alpha=0.2,\n    label=\"Actual demand\",\n    color=\"black\",\n)\nax.plot(naive_linear_predictions[last_hours], \"x-\", label=\"Ordinal time features\")\nax.plot(\n    cyclic_cossin_linear_predictions[last_hours],\n    \"x-\",\n    label=\"Trigonometric time features\",\n)\nax.plot(\n    cyclic_spline_linear_predictions[last_hours],\n    \"x-\",\n    label=\"Spline-based time features\",\n)\nax.plot(\n    one_hot_linear_predictions[last_hours],\n    \"x-\",\n    label=\"One-hot time features\",\n)\n_ = ax.legend()\n\n\n<img alt=\"Predictions by linear models\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_006.png\" srcset=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_006.png\"/>",
            "wins = (\n    pd.melt(df.reset_index(),\n            id_vars=['game_id', 'date', 'home_win'],\n            value_name='team', var_name='is_home',\n            value_vars=['home_team', 'away_team'])\n   .assign(win=lambda x: x.home_win == (x.is_home == 'home_team'))\n   .groupby(['team', 'is_home'])\n   .win\n   .agg(['sum', 'count', 'mean'])\n   .rename(columns=dict(sum='n_wins',\n                        count='n_games',\n                        mean='win_pct'))\n)\nwins.head()",
            "data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Qualitative analysis of the impact of features on linear model predictions"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings"
            ]
        ]
    },
    "1006052": {
        "jupyter_code_cell": "SMS_train_data = list(SMS_spamham_data['SMS Message'])\nSMS_train_target = list(SMS_spamham_data['Spam or Ham'])\nlb = preprocessing.LabelBinarizer()\nSMS_train_target = np.array([number[0] for number in lb.fit_transform(SMS_train_target)])\nsms_count_vect = CountVectorizer(decode_error = 'replace')\nsms_train_counts = sms_count_vect.fit_transform(SMS_train_data)\nsms_tfidf_transformer = TfidfTransformer()\nsms_train_tfidf = sms_tfidf_transformer.fit_transform(sms_train_counts)",
        "matched_tutorial_code_inds": [
            5917,
            1523,
            6795,
            1615,
            1525
        ],
        "matched_tutorial_codes": [
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "transistor_count_predicted = np.exp(B) * np.exp(A * year)\ntransistor_Moores_law = Moores_law(year)\nplt.style.use(\"fivethirtyeight\")\nplt.semilogy(year, transistor_count, \"s\", label=\"MOS transistor count\")\nplt.semilogy(year, transistor_count_predicted, label=\"linear regression\")\n\n\nplt.plot(year, transistor_Moores_law, label=\"Moore's Law\")\nplt.title(\n    \"MOS transistor count per microprocessor\\n\"\n    + \"every two years \\n\"\n    + \"Transistor count was x{:.2f} higher\".format(np.exp(A * 2))\n)\nplt.xlabel(\"year introduced\")\nplt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\nplt.ylabel(\"# of transistors\\nper microprocessor\")",
            "x1n = np.linspace(20.5, 25, 10)\nXnew = np.column_stack((x1n, np.sin(x1n), (x1n - 5) ** 2))\nXnew = sm.add_constant(Xnew)\nynewpred = olsres.predict(Xnew)  # predict out of sample\nprint(ynewpred)",
            "fourier_gaussian = ndimage.fourier_gaussian(xray_image, sigma=0.05)\n\nx_prewitt = ndimage.prewitt(fourier_gaussian, axis=0)\ny_prewitt = ndimage.prewitt(fourier_gaussian, axis=1)\n\nxray_image_canny = np.hypot(x_prewitt, y_prewitt)\n\nxray_image_canny *= 255.0 / np.max(xray_image_canny)\n\nprint(\"The data type - \", xray_image_canny.dtype)",
            "transistor_count2017 = transistor_count[year == 2017]\nprint(\n    transistor_count2017.max(), transistor_count2017.min(), transistor_count2017.mean()\n)\ny = np.linspace(2016.5, 2017.5)\nyour_model2017 = np.exp(B) * np.exp(A * y)\nMoore_Model2017 = Moores_law(y)\n\nplt.plot(\n    2017 * np.ones(np.sum(year == 2017)),\n    transistor_count2017,\n    \"ro\",\n    label=\"2017\",\n    alpha=0.2,\n)\nplt.plot(2017, transistor_count2017.mean(), \"g+\", markersize=20, mew=6)\n\nplt.plot(y, your_model2017, label=\"Your prediction\")\nplt.plot(y, Moore_Model2017, label=\"Moores law\")\nplt.ylabel(\"# of transistors\\nper microprocessor\")\nplt.legend()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy->Calculating the historical growth curve for transistors"
            ],
            [
                "statsmodels->Examples->User Notes->Prediction->Create a new sample of explanatory variables Xnew, predict and plot"
            ],
            [
                "numpy->NumPy Applications->X-ray image processing->Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters->The Canny filter"
            ],
            [
                "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy->Calculating the historical growth curve for transistors"
            ]
        ]
    },
    "667164": {
        "jupyter_code_cell": "import pandas as pd\nimport matplotlib.pyplot as plt, matplotlib.image as mpimg\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm\nfrom sklearn.model_selection import KFold",
        "matched_tutorial_code_inds": [
            2984,
            6892,
            6306,
            5220,
            2037
        ],
        "matched_tutorial_codes": [
            "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import \nfrom sklearn.neural_network import \nfrom sklearn.preprocessing import \nfrom sklearn.pipeline import \nfrom sklearn.tree import \nfrom sklearn.inspection import PartialDependenceDisplay",
            "import pandas as pd\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm\nfrom statsmodels.tsa.ar_model import AutoReg, ar_select_order\n\nplt.rc(\"figure\", figsize=(16, 8))\nplt.rc(\"font\", size=14)",
            "import numpy as np\nimport pandas as pd\nfrom scipy.stats import norm\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport requests\nfrom io import BytesIO",
            "import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\n\ndata = sm.datasets.engel.load_pandas().data\ndata.head()",
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.linalg import , \n\nfrom sklearn.covariance import , \n\n(0)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Miscellaneous->Advanced Plotting With Partial Dependence"
            ],
            [
                "statsmodels->Examples->User Notes->Dates in Time-Series Models"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Quantile Regression->Setup"
            ],
            [
                "sklearn->Examples->Covariance estimation->Ledoit-Wolf vs OAS estimation"
            ]
        ]
    },
    "501233": {
        "jupyter_code_cell": "data.dropna(how='any', inplace= True)\ndata.shape\ndata.drop(['is_collaborative', 'is_public','decade_1910'], axis =1 , inplace= True)\ndata1 = data.copy()\nX = data1.drop(['followers'], axis = 1)\nscaler = MinMaxScaler().fit(X)\ndata1 = scaler.transform(X)\ndata1 = pd.DataFrame(data1, columns= X.columns)\ndata1['followers'] = data['followers']\ndata3 = data1.copy()\ndata3['followers'] = np.log(1 + data3['followers'])\ny = (data1['followers'])\nX = data1.drop(['followers'], axis = 1)\ny2 = (data3['followers'])\nX2 = data3.drop(['followers'], axis = 1)\nX2.shape, y2.shape\nX_train, X_test, y_train, y_test = train_test_split(X2, y2, test_size=0.33, random_state=42)\nX = sm.add_constant(X)\nmodel = sm.OLS(y,X)\nresults = model.fit()\nresults.summary()",
        "matched_tutorial_code_inds": [
            3904,
            5194,
            2096,
            2753,
            5611
        ],
        "matched_tutorial_codes": [
            "dots = sns.load_dataset(\"dots\")\nsns.relplot(\n    data=dots, kind=\"line\",\n    x=\"time\", y=\"firing_rate\", col=\"align\",\n    hue=\"choice\", size=\"coherence\", style=\"choice\",\n    facet_kws=dict(sharex=False),\n)\n",
            "norm_x = X.values\nfor i, name in enumerate(X):\n    if name == \"const\":\n        continue\n    norm_x[:, i] = X[name] / np.linalg.norm(X[name])\nnorm_xtx = np.dot(norm_x.T, norm_x)",
            "node_indicator = clf.decision_path(X_test)\nleaf_id = clf.apply(X_test)\n\nsample_id = 0\n# obtain ids of the nodes `sample_id` goes through, i.e., row `sample_id`\nnode_index = node_indicator.indices[\n    node_indicator.indptr[sample_id] : node_indicator.indptr[sample_id + 1]\n]\n\nprint(\"Rules used to predict sample {id}:\\n\".format(id=sample_id))\nfor node_id in node_index:\n    # continue to the next node if it is a leaf node\n    if leaf_id[sample_id] == node_id:\n        continue\n\n    # check if value of the split feature for sample 0 is below threshold\n    if X_test[sample_id, feature[node_id]] &lt;= threshold[node_id]:\n        threshold_sign = \"&lt;=\"\n    else:\n        threshold_sign = \"\"\n\n    print(\n        \"decision node {node} : (X_test[{sample}, {feature}] = {value}) \"\n        \"{inequality} {threshold})\".format(\n            node=node_id,\n            sample=sample_id,\n            feature=feature[node_id],\n            value=X_test[sample_id, feature[node_id]],\n            inequality=threshold_sign,\n            threshold=threshold[node_id],\n        )\n    )",
            "quantiles = [0.05, 0.5, 0.95]\npredictions = {}\nout_bounds_predictions = (y_true_mean, dtype=)\nfor quantile in quantiles:\n    qr = (quantile=quantile, alpha=0, solver=solver)\n    y_pred = qr.fit(X, y_pareto).predict(X)\n    predictions[quantile] = y_pred\n\n    if quantile == min(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred = y_pareto\n        )\n    elif quantile == max(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred &lt;= y_pareto\n        )",
            "data2 = data.copy()\ndata2[\"const\"] = 1\ndc = (\n    data2[\"affairs rate_marriage age yrs_married const\".split()]\n    .groupby(\"affairs rate_marriage age yrs_married\".split())\n    .count()\n)\ndc.reset_index(inplace=True)\ndc.rename(columns={\"const\": \"freq\"}, inplace=True)\nprint(dc.shape)\ndc.head()"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity->Condition number"
            ],
            [
                "sklearn->Examples->Decision Trees->Understanding the decision tree structure->Decision path"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Quantile regression->Fitting a QuantileRegressor"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Condensing and Aggregating observations->Dataset with unique observations"
            ]
        ]
    },
    "1114155": {
        "jupyter_code_cell": "np.random.seed(42)\nmu, sigma = 0, 1\nx = mu + sigma * np.random.randn(100000)\npd.Series(x).plot(kind='hist', bins=50,\n                  color='#000000', alpha=0.5, normed=True)\nplt.title('Sampling Distribution Example')\nplt.xlabel('ATE Estimates')\nplt.ylabel('Density')\nplt.tick_params(axis='both',\n                top='off', bottom='off',\n                left='off', right='off')\n(((x - x.mean()) ** 2).mean()) ** 0.5",
        "matched_tutorial_code_inds": [
            5833,
            4630,
            4795,
            4796,
            3916
        ],
        "matched_tutorial_codes": [
            "np.random.seed(12345)\nnobs = 200\nbeta_true = np.array([3, 1, 2.5, 3, -4])\nX = np.random.uniform(-20, 20, size=(nobs, len(beta_true) - 1))\n# stack a constant in front\nX = sm.add_constant(X, prepend=True)  # np.c_[np.ones(nobs), X]\nmc_iter = 500\ncontaminate = 0.25  # percentage of response variables to contaminate",
            "np.random.seed(19680801)  # seed the random number generator.\ndata = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nfig, ax = plt.subplots(figsize=(5, 2.7), layout='constrained')\nax.scatter('a', 'b', c='c', s='d', data=data)\nax.set_xlabel('entry a')\nax.set_ylabel('entry b')\n\n\n<img alt=\"quick start\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_quick_start_002.png\" srcset=\"../../_images/sphx_glr_quick_start_002.png, ../../_images/sphx_glr_quick_start_002_2_0x.png 2.0x\"/>",
            "plt.rcParams['figure.constrained_layout.use'] = True\nfig, axs = plt.subplots(2, 2, figsize=(3, 3))\nfor ax in axs.flat:\n    example_plot(ax)\n\n\n<img alt=\"Title, Title, Title, Title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_constrainedlayout_guide_018.png\" srcset=\"../../_images/sphx_glr_constrainedlayout_guide_018.png, ../../_images/sphx_glr_constrainedlayout_guide_018_2_0x.png 2.0x\"/>",
            "plt.rcParams['figure.constrained_layout.use'] = False\nfig = plt.figure(layout=\"constrained\")\n\ngs1 = gridspec.GridSpec(2, 1, figure=fig)\nax1 = fig.add_subplot(gs1[0])\nax2 = fig.add_subplot(gs1[1])\n\nexample_plot(ax1)\nexample_plot(ax2)\n\n\n<img alt=\"Title, Title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_constrainedlayout_guide_019.png\" srcset=\"../../_images/sphx_glr_constrainedlayout_guide_019.png, ../../_images/sphx_glr_constrainedlayout_guide_019_2_0x.png 2.0x\"/>",
            "sns.set_theme(style=\"ticks\", font_scale=1.25)\ng = sns.relplot(\n    data=penguins,\n    x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"body_mass_g\",\n    palette=\"crest\", marker=\"x\", s=100,\n)\ng.set_axis_labels(\"Bill length (mm)\", \"Bill depth (mm)\", labelpad=10)\ng.legend.set_title(\"Body mass (g)\")\ng.figure.set_size_inches(6.5, 4.5)\ng.ax.margins(.15)\ng.despine(trim=True)\n"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Exercise: Breakdown points of M-estimator"
            ],
            [
                "matplotlib->Tutorials->Introductory->Quick start guide->Types of inputs to plotting functions"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Constrained Layout Guide->rcParams"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Constrained Layout Guide->Use with GridSpec"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->Opinionated defaults and flexible customization"
            ]
        ]
    }
}