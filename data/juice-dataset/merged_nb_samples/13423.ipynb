{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f16a8e29",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce8a8f9",
   "metadata": {},
   "source": [
    "We would be solving classification problems using logistic regression. In the first exercise, we would be learning an algorithm to classify students admission decision based on their test scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c15e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fe34d8",
   "metadata": {},
   "source": [
    "Loading the data into a numpy ndarray object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0716bc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('ex2data1.txt', delimiter = ',')\n",
    "X = data[:,:2]\n",
    "y = data[:,2]\n",
    "y = y.reshape(len(y), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bb70a0",
   "metadata": {},
   "source": [
    "To get a feel of the data set, we plot of scatter diagram which would give us an idea if the decision boundary is linear or non-linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a03f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[data[:,2]==0][:,0], data[data[:,2]==0][:,1], marker = 'o', label = 'Not Admited') #Not admitted\n",
    "plt.scatter(data[data[:,2]==1][:,0], data[data[:,2]==1][:,1], marker = 'x', label = 'Admitted') #Admitted\n",
    "plt.xlabel('Exam 1 Scores')\n",
    "plt.ylabel('Exam 2 Scores')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e45a35",
   "metadata": {},
   "source": [
    "It seems as linear decision boundary is quite possible from a plot displayed above. \n",
    "We would apply a linear function to the sigmoid logistic function to enable us learn the parameters for the decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d47930d",
   "metadata": {},
   "source": [
    "### The Sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d166ec57",
   "metadata": {},
   "source": [
    "The sigmoid logistic function which is uniquely S shaped gives us a tool for classifying our data between classes, say positive and negative based on the output from the a decision boundary function which could be linear as in most cases or non-linear. The general implementation of the logistic function approaches 1 for very positive values of x and 0 for very negative values of x cutting the x-axis at y = 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34872998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''\n",
    "    input: a value or an array of values.\n",
    "    Returns: The output of the sigmoid function operated on the input value.\n",
    "    '''\n",
    "    \n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e35bc0f",
   "metadata": {},
   "source": [
    "To perform a test plot of the sigmoid function using random generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d63cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.linspace(-10, 10, 100) #Generate an arbitrary list of 100 values from -20 to 20\n",
    "b = sigmoid(a)\n",
    "plt.plot(a, b)\n",
    "plt.xlabel(\"X\")\n",
    "plt.yticks([0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0])\n",
    "plt.grid()\n",
    "plt.ylabel(\"Sigmoid(X)\")\n",
    "plt.title(\"A plot of the sigmoid function\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60543552",
   "metadata": {},
   "source": [
    "Implementing the logistic regression cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aef8ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(theta, X, y):\n",
    "    '''\n",
    "    Inputs: \n",
    "    theta - The parameter vector for the decision boundary.\n",
    "    X - The feature matrix for the features.\n",
    "    y - The labels corresponding to the features.\n",
    "    \n",
    "    Returns: The cost\n",
    "    '''\n",
    "    theta = theta.reshape((theta.shape[0],1))\n",
    "    m = len(y)\n",
    "    h = sigmoid(np.matmul(X, theta))\n",
    "    cost = (1/m) * np.sum(-y * np.log(h) - (1 - y) * np.log(1 - h))\n",
    "    return cost\n",
    "\n",
    "def grad(theta, X, y):\n",
    "    '''\n",
    "    theta - The parameter vector for the decision boundary.\n",
    "    X - The feature matrix for the features.\n",
    "    y - The labels corresponding to the features.\n",
    "    \n",
    "    Returns: The gradient or differential values corresponding to the theta parameters.\n",
    "    '''\n",
    "    theta = theta.reshape((theta.shape[0],1))\n",
    "    m = len(y)\n",
    "    h = sigmoid(np.matmul(X, theta))\n",
    "    grad = (1/m) * sum(((h - y) * X))\n",
    "    \n",
    "    return grad#.reshape(theta.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b96922a",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b59951",
   "metadata": {},
   "source": [
    "We would apply an optimization algorithm implemented in the Scipy library to minimize the cost function, thus obtaining the optimal value of the decision boundary parameter vector theta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56907df0",
   "metadata": {},
   "source": [
    "We used the Truncated Newton algorithm, TNC from the optimize class in the Scipy library to minimize the cost function and thus obtain the optimal learning paramers. It is also possible to use other minimization methods, although this works well for our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2552e902",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.ones((X.shape[0],3))\n",
    "X[:,1:3] = data[:,:2]\n",
    "theta = np.zeros(X.shape[1])\n",
    "thetaOpt = optimize.minimize(fun = cost, x0 = theta, jac = grad, args = (X,y), method = 'TNC')\n",
    "thetaOpt = thetaOpt.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438cb3f9",
   "metadata": {},
   "source": [
    "Now that we have obtained a trained logistic regression model by learning the model parameters, we now plot the decision boundary. The decision boundary is linear based on the 2 features.\n",
    "    theta0 + theta1.X1 + theta2X2 = 0 is the equation for this straight line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bda6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotX = np.array([min(X[:,1] - 2), max(X[:,1]) + 2])\n",
    "plotY = (thetaOpt[1] * plotX + thetaOpt[0])/ -thetaOpt[2]\n",
    "plt.plot(plotX, plotY)\n",
    "plt.scatter(data[data[:,2]==0][:,0], data[data[:,2]==0][:,1], marker = 'o', label = 'Not Admited') #Not admitted\n",
    "plt.scatter(data[data[:,2]==1][:,0], data[data[:,2]==1][:,1], marker = 'x', label = 'Admitted') #Admitted\n",
    "plt.xlabel('Exam 1 Scores')\n",
    "plt.ylabel('Exam 2 Scores')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf86c61",
   "metadata": {},
   "source": [
    "Finally, we would determine the accuracy on our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f7e63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros(X.shape[1])\n",
    "y = y.reshape(len(y), 1)\n",
    "thetaOpt = optimize.minimize(fun = cost, x0 = theta, jac = grad, args = (X,y), method = 'L-BFGS-B')\n",
    "thetaOpt = thetaOpt.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d0ed69",
   "metadata": {},
   "source": [
    "The vector thetaOpt represents the parameters for the model which we have learnt via or 6th power logistic regression model. We now attempt to plot the decision boundary for a set of points within the axis of our initial scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7703939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is similar to process which was used in creating the extra features above.\n",
    "def mapfeature(X1, X2, n):\n",
    "    '''\n",
    "    Returns: An array obtained using the input creating extra features to power n\n",
    "    '''\n",
    "    X = [1,X1, X2]\n",
    "    for i in range(1,n+1):\n",
    "        for j in range(i+1):\n",
    "            X.append(X1**(i-j) * X2**(j))\n",
    "    \n",
    "    return np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9c36f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.linspace(-1,1.5,50)\n",
    "v = np.linspace(-1,1.5,50)\n",
    "z = np.zeros([len(u),len(v)])\n",
    "for i in range(len(u)):\n",
    "    for j in range(len(v)):\n",
    "        z[i,j]= np.matmul(mapfeature(u[i],v[j],6),thetaOpt)\n",
    "\n",
    "z = z.T\n",
    "plt.scatter(data[data[:,2]==0][:,0], data[data[:,2]==0][:,1], marker= 's', label='Failed')\n",
    "plt.scatter(data[data[:,2]==1][:,0], data[data[:,2]==1][:,1], marker= 'o', label='Passed')\n",
    "plt.contour(u,v,z)\n",
    "plt.title('Contour point of decision boundary for logistic regression without regularization')\n",
    "plt.xlabel('Test Case 1')\n",
    "plt.ylabel('Test Case 2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fe5022",
   "metadata": {},
   "source": [
    "Notice that the decision boundary fits the data very accurately, however there is a tendency that this model would not generalize on new data. It seems that the model memorized the training set; this is problem of over-fitting and to overcome this we would apply regularization to simplify the model to reduce its variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf543054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_reg(theta, X, y, lambd):\n",
    "    '''\n",
    "    Inputs: \n",
    "    theta - The parameter vector for the decision boundary.\n",
    "    X - The feature matrix for the features.\n",
    "    y - The labels corresponding to the features.\n",
    "    lambd - Regularization coefficient\n",
    "    \n",
    "    Returns: The cost\n",
    "    '''\n",
    "    theta = theta.reshape((theta.shape[0],1))\n",
    "    m = len(y)\n",
    "    h = sigmoid(np.matmul(X, theta))\n",
    "    cost = (1/m) * np.sum(-y * np.log(h) - (1 - y) * np.log(1 - h))\n",
    "    cost += (lambd/(2*m)) * np.power(theta,2) #this takes into account the regularization term, thus increasing the cost or penalty for the selected parameters\n",
    "    return cost\n",
    "\n",
    "def grad_reg(theta, X, y, lambd):\n",
    "    '''\n",
    "    theta - The parameter vector for the decision boundary.\n",
    "    X - The feature matrix for the features.\n",
    "    y - The labels corresponding to the features.\n",
    "    lambd - Regularization coefficient\n",
    "    \n",
    "    Returns: The gradient or differential values corresponding to the theta parameters.\n",
    "    '''\n",
    "    theta = theta.reshape((theta.shape[0],1))\n",
    "    m = len(y)\n",
    "    h = sigmoid(np.matmul(X, theta))\n",
    "    grad = (1/m) * sum(((h - y) * X))\n",
    "    theta[0,0] = 0\n",
    "    theta = theta.flatten()\n",
    "    grad += (lambd/m) * theta # To account for regularization. The constant term is usually not regularized.\n",
    "    \n",
    "    return grad\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168f0877",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros(X.shape[1])\n",
    "lambd = 0.001\n",
    "y = y.reshape(len(y), 1)\n",
    "thetaOpt = optimize.minimize(fun = cost_reg, x0 = theta, jac = grad_reg, args = (X,y, lambd), method = 'L-BFGS-B')\n",
    "thetaOpt = thetaOpt.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f85fb5",
   "metadata": {},
   "source": [
    "Notice that the new decision boundary, using a small amount of regularization (lambda of 0.001) tends create a less complex model and would most likely reduce the amount of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3a2f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.linspace(-1,1.5,50)\n",
    "v = np.linspace(-1,1.5,50)\n",
    "z = np.zeros([len(u),len(v)])\n",
    "for i in range(len(u)):\n",
    "    for j in range(len(v)):\n",
    "        z[i,j]= np.matmul(mapfeature(u[i],v[j],6),thetaOpt)\n",
    "\n",
    "z = z.T\n",
    "plt.scatter(data[data[:,2]==0][:,0], data[data[:,2]==0][:,1], marker= 's', label='Failed')\n",
    "plt.scatter(data[data[:,2]==1][:,0], data[data[:,2]==1][:,1], marker= 'o', label='Passed')\n",
    "plt.contour(u,v,z, label = 'Decision Boundary')\n",
    "plt.xlabel('Test Case 1')\n",
    "plt.ylabel('Test Case 2')\n",
    "plt.title('Logistic regression with regularization parameter of 0.001')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba784ca",
   "metadata": {},
   "source": [
    "If the value of lambda is zero then the model is without regularization, having a high variance as with the first plot. However, if the amount of regularization becomes too high, the model becomes too simple and the model would underfit the training set, thus having a high bias as we would see below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60591ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros(X.shape[1])\n",
    "lambd = 1\n",
    "y = y.reshape(len(y), 1)\n",
    "thetaOpt = optimize.minimize(fun = cost_reg, x0 = theta, jac = grad_reg, args = (X,y, lambd), method = 'L-BFGS-B')\n",
    "thetaOpt = thetaOpt.x\n",
    "\n",
    "u = np.linspace(-1,1.5,50)\n",
    "v = np.linspace(-1,1.5,50)\n",
    "z = np.zeros([len(u),len(v)])\n",
    "for i in range(len(u)):\n",
    "    for j in range(len(v)):\n",
    "        z[i,j]= np.matmul(mapfeature(u[i],v[j],6),thetaOpt)\n",
    "\n",
    "z = z.T\n",
    "plt.scatter(data[data[:,2]==0][:,0], data[data[:,2]==0][:,1], marker= 's', label='Failed')\n",
    "plt.scatter(data[data[:,2]==1][:,0], data[data[:,2]==1][:,1], marker= 'o', label='Passed')\n",
    "plt.contour(u,v,z, label = 'Decision Boundary')\n",
    "plt.xlabel('Test Case 1')\n",
    "plt.ylabel('Test Case 2')\n",
    "plt.title('Logistic regression with regularization parameter of 1')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c4060b",
   "metadata": {},
   "source": [
    "The prediction accuracy of model using our training set is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153c2640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict (X, y, theta):\n",
    "    t = sigmoid(np.matmul(X,theta))\n",
    "    ypred = np.array([1 if i >= 0.5 else 0 for i in t]) #gives the predicted values\n",
    "    y = y.flatten()\n",
    "    acc = (ypred == y)\n",
    "    acc = acc.astype(int)\n",
    "    predictAccuracy = np.mean(acc)\n",
    "    return predictAccuracy * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b21468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(X, y, thetaOpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215614fe",
   "metadata": {},
   "source": [
    "The prediction accuracy of using the training data is 89.0%. Although this is not a good measure of the model since it is based on the same data with which it was developed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e050445e",
   "metadata": {},
   "source": [
    "### Non-Linear Logistic regression (With regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676122e6",
   "metadata": {},
   "source": [
    "We would now use logistic regression for the classification of a case that cannot be seperated using a linear decision boundary. We would attempt to predict if microchips from a fabrication process passes QA tests based on the test result from two test cases in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69af1c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('ex2data2.txt', delimiter=',')\n",
    "X = data[:,:2]\n",
    "y = data[:,2]\n",
    "plt.scatter(data[data[:,2]==0][:,0], data[data[:,2]==0][:,1], marker= 's', label='Failed')\n",
    "plt.scatter(data[data[:,2]==1][:,0], data[data[:,2]==1][:,1], marker= 'o', label='Passed')\n",
    "plt.xlabel('Test Case 1')\n",
    "plt.ylabel('Test Case 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6e6836",
   "metadata": {},
   "source": [
    "Clearly, the data cannot be seperated with a linear decision boundary. Perhaps something similar to an elipse might do a good job of seperating the failed and passed microchip samples, lets find out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d7e8d5",
   "metadata": {},
   "source": [
    "We would create extra features which would add extra complexity to the model in order to fit the training data properly. Lets create polynomials to the sixth power using the two training features we already have.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed5798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for i in range(1,7):\n",
    "    for j in range(i+1):\n",
    "        new = np.power(X[:,0],(i-j)) * np.power(X[:,1], j)\n",
    "        X =  np.hstack((X, new.reshape(len(y), 1)))\n",
    "        #a.append([(i-j),j])\n",
    "X = np.hstack((np.zeros((len(y),1)), X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedaa6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
