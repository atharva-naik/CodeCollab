{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c656888",
   "metadata": {},
   "source": [
    "Set some basic config parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2383d28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 100 # how big is each word vector\n",
    "max_features = 10000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 100 # max number of words in a comment to use\n",
    "# consider increase to 200 to 300 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4066934",
   "metadata": {},
   "source": [
    "Read in our data and replace missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17e3e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test = pd.read_csv(TEST_DATA_FILE)\n",
    "\n",
    "COMMENT = 'comment_text'\n",
    "train[COMMENT].fillna(\"unknown\", inplace=True)\n",
    "test[COMMENT].fillna(\"unknown\", inplace=True)\n",
    "# fasttext preprocessing\n",
    "train[COMMENT] = train[COMMENT].str.lower().str.replace('https?:\\/\\/[^\\s]*','').str.replace(\"’\",\"'\").str.replace(\"''\",' ').str.replace(\"'\",\" ' \").str.replace('“','\"').str.replace('”','\"').str.replace(':',' : ').str.replace('.',' . ').str.replace(',',' , ').str.replace('[',' [ ').str.replace(']',' ] ').str.replace('(',' ( ').str.replace(')',' ) ').str.replace('!',' ! ').str.replace('?',' ? ').str.replace(';',' ').str.replace(':',' ').str.replace('-',' - ').str.replace('=', ' ').str.replace('=', ' ').str.replace('*', ' ').str.replace('|', ' ').str.replace('«', ' ').str.replace('\\d', ' ').str.replace('\\n', ' ').str.replace('\\s\\s+',' ').str.replace('\\n','NEWLINE ').str.strip().str.replace(\"[^\\s\\d\\w)(;:\\-+_?!\\]\\[,*,'\\\"]+\",\"\")\n",
    "test[COMMENT] = test[COMMENT].str.lower().str.replace('https?:\\/\\/[^\\s]*','').str.replace(\"’\",\"'\").str.replace(\"''\",' ').str.replace(\"'\",\" ' \").str.replace('“','\"').str.replace('”','\"').str.replace(':',' : ').str.replace('.',' . ').str.replace(',',' , ').str.replace('[',' [ ').str.replace(']',' ] ').str.replace('(',' ( ').str.replace(')',' ) ').str.replace('!',' ! ').str.replace('?',' ? ').str.replace(';',' ').str.replace(':',' ').str.replace('-',' - ').str.replace('=', ' ').str.replace('=', ' ').str.replace('*', ' ').str.replace('|', ' ').str.replace('«', ' ').str.replace('\\d', ' ').str.replace('\\n', ' ').str.replace('\\s\\s+',' ').str.replace('\\n','NEWLINE ').str.strip().str.replace(\"[^\\s\\d\\w)(;:\\-+_?!\\]\\[,*,'\\\"]+\",\"\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"_na_\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"_na_\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef424c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO test average, max, min comments words\n",
    "test_train = train.copy()\n",
    "test_train['tokens'] = test_train[COMMENT].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3ec240",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train['tokens_len'] = test_train['tokens'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83bf888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "print('mean:',test_train['tokens_len'].mean(), 'min:',test_train['tokens_len'].min(), 'max:',test_train['tokens_len'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec2e667",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train['tokens_len'].sample(n=3000).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea93b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train['tokens_len'].sample(n=3000).plot(kind='density')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac09b91",
   "metadata": {},
   "source": [
    "Standard keras preprocessing, to turn each comment into a list of word indexes of equal length (with truncation or padding as needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f128ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "# TODO filters=''\n",
    "# Tokenizer(num_words=max_features, filters='')\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ae2c0f",
   "metadata": {},
   "source": [
    "Read the wiki user comments word vectors (space delimited strings) into a dictionary from word->vector."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
