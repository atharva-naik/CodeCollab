{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22f3e048",
   "metadata": {},
   "source": [
    "# Data Wrangling - Perth, Australia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9313e4d",
   "metadata": {},
   "source": [
    "The dataset was downloaded from Mapzen: https://mapzen.com/data/metro-extracts/metro/perth_australia/\n",
    "\n",
    "This is a metro extract of Perth, WA, Australia. Size of 254MB with the remaining sample file to be ~50MB.\n",
    "\n",
    "I've chosen Perth as I have been living there briefly when I was a student, and have some fond memories of it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b07184a",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4e12ff",
   "metadata": {},
   "source": [
    "- code from the course Case Sudy\n",
    "- blog posts from discussions.udacity.com\n",
    "- for the conversion from XLM to CSV and CSV to SQL I used the following GitHub as a reference: https://gist.github.com/swwelch/f1144229848b407e0a5d13fcb7fbbd6f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4d915e",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8870f0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide code cells\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bec4570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up libraries\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import codecs\n",
    "\n",
    "import cerberus\n",
    "import schema\n",
    "\n",
    "import os\n",
    "from hurry.filesize import size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e2584f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load 'schema.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145d17b5",
   "metadata": {},
   "source": [
    "Once downloaded and unzipped, the OSM file for Perth, Australia has a size 254 MB.\n",
    "The requirement for this project is a size of 50 MB. Hence I am using the code provided in the Project Overview to create a sample file, by iterating over the lines by k buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f762bd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "from pprint import pprint\n",
    "import xml.etree.ElementTree as ET  # Use cElementTree or lxml if too slow\n",
    "\n",
    "OSM_FILE = \"perth_australia.osm\"\n",
    "SAMPLE_FILE = \"perth_australia_sample.osm\"\n",
    "\n",
    "k = 5\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\n",
    "\n",
    "    Reference:\n",
    "    http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python\n",
    "    \"\"\"\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "        \n",
    "        \n",
    "with open(SAMPLE_FILE, 'wb') as output:\n",
    "    output.write(b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "    output.write(b'<osm>\\n  ')\n",
    "\n",
    "    # Write every kth top level element\n",
    "    for i, element in enumerate(get_element(OSM_FILE)):\n",
    "        if i % k == 0:\n",
    "            output.write(ET.tostring(element, encoding='utf-8'))\n",
    "\n",
    "    output.write(b'</osm>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7776c592",
   "metadata": {},
   "source": [
    "The resulting sample file has a size of 51.5 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74305aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "db = sqlite3.connect(\"PerthWA.db\")\n",
    "c = db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdb659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.execute('''DROP TABLE IF EXISTS nodes''')\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222ac39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.execute('''\n",
    "CREATE TABLE nodes (\n",
    "    id INTEGER PRIMARY KEY NOT NULL,\n",
    "    lat REAL,\n",
    "    lon REAL,\n",
    "    user TEXT,\n",
    "    uid INTEGER,\n",
    "    version INTEGER,\n",
    "    changeset INTEGER,\n",
    "    timestamp TEXT\n",
    ");\n",
    "''')\n",
    "\n",
    "c.execute('''\n",
    "CREATE TABLE nodes_tags (\n",
    "    id INTEGER,\n",
    "    key TEXT,\n",
    "    value TEXT,\n",
    "    type TEXT,\n",
    "    FOREIGN KEY (id) REFERENCES nodes(id)\n",
    ");\n",
    "''')\n",
    "\n",
    "c.execute('''\n",
    "CREATE TABLE ways (\n",
    "    id INTEGER PRIMARY KEY NOT NULL,\n",
    "    user TEXT,\n",
    "    uid INTEGER,\n",
    "    version TEXT,\n",
    "    changeset INTEGER,\n",
    "    timestamp TEXT\n",
    ");\n",
    "''')\n",
    "\n",
    "c.execute('''\n",
    "CREATE TABLE ways_tags (\n",
    "    id INTEGER NOT NULL,\n",
    "    key TEXT NOT NULL,\n",
    "    value TEXT NOT NULL,\n",
    "    type TEXT,\n",
    "    FOREIGN KEY (id) REFERENCES ways(id)\n",
    ");\n",
    "''')\n",
    "\n",
    "c.execute('''\n",
    "CREATE TABLE ways_nodes (\n",
    "    id INTEGER NOT NULL,\n",
    "    node_id INTEGER NOT NULL,\n",
    "    position INTEGER NOT NULL,\n",
    "    FOREIGN KEY (id) REFERENCES ways(id),\n",
    "    FOREIGN KEY (node_id) REFERENCES nodes(id)\n",
    ");\n",
    "''')\n",
    "\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac6caf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the csv file as a dictionary, format the data as a list of tuples:\n",
    "with open('nodes.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db = [(i['id'], i['lat'], i['lon'], i['user'].decode(\"utf-8\"), i['uid'], i['version'], i['changeset'], i['timestamp']) for i in dr]\n",
    "    \n",
    "# insert the formatted data\n",
    "c.executemany(\"INSERT INTO nodes(id, lat, lon, user, uid, version, changeset, timestamp) VALUES (?, ?, ?, ?, ?, ?, ?, ?);\", to_db)\n",
    "# commit the changes\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd209621",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nodes_tags.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db = [(i['id'], i['key'], i['value'].decode(\"utf-8\"), i['type']) for i in dr]\n",
    "    \n",
    "# insert the formatted data\n",
    "c.executemany(\"INSERT INTO nodes_tags(id, key, value,type) VALUES (?, ?, ?, ?);\", to_db)\n",
    "# commit the changes\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7858831",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ways.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db = [(i['id'], i['user'].decode(\"utf-8\"), i['uid'], i['version'], i['changeset'], i['timestamp']) for i in dr]\n",
    "    \n",
    "# insert the formatted data\n",
    "c.executemany(\"INSERT INTO ways(id, user, uid, version, changeset, timestamp) VALUES (?, ?, ?, ?, ?, ?);\", to_db)\n",
    "# commit the changes\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c756f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ways_nodes.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db = [(i['id'], i['node_id'], i['position']) for i in dr]\n",
    "    \n",
    "# insert the formatted data\n",
    "c.executemany(\"INSERT INTO ways_nodes(id, node_id, position) VALUES (?, ?, ?);\", to_db)\n",
    "# commit the changes\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4533df5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ways_tags.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db = [(i['id'], i['key'], i['value'].decode(\"utf-8\"), i['type']) for i in dr]\n",
    "    \n",
    "# insert the formatted data\n",
    "c.executemany(\"INSERT INTO ways_tags(id, key, value, type) VALUES (?, ?, ?, ?);\", to_db)\n",
    "# commit the changes\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d1b7b1",
   "metadata": {},
   "source": [
    "### Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a241497c",
   "metadata": {},
   "source": [
    "Files sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c278f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the list of files and their size\n",
    "dirpath = '.'\n",
    "\n",
    "files_list = []\n",
    "for path, dirs, files in os.walk(dirpath):\n",
    "    files_list.extend([(filename, size(os.path.getsize(os.path.join(path, filename)))) for filename in files])\n",
    "\n",
    "for filename, size in files_list:\n",
    "    print '{:.<40s}: {:5s}'.format(filename,size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bc078f",
   "metadata": {},
   "source": [
    "Count of ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ffca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening file in filename\n",
    "filename = open(\"perth_australia_sample.osm\", \"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23a1378",
   "metadata": {},
   "source": [
    "## Data Audit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8342941f",
   "metadata": {},
   "source": [
    "### Tag types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f4d045",
   "metadata": {},
   "source": [
    "Count of each of the tags in the OSM data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd569ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterative parsing from the problem set in the course\n",
    "\"\"\"\n",
    "Your task is to use the iterative parsing to process the map file and find out not only what tags are there, but also how many, \n",
    "to get the feeling on how much of which data you can expect to have in the map.\n",
    "Fill out the count_tags function. It should return a dictionary with the tag name as the key and number of times this tag can be\n",
    "encountered in the map as value.\n",
    "\"\"\"\n",
    "def count_tags(samplefile):\n",
    "    tags = {}\n",
    "    for event, element in ET.iterparse(samplefile):\n",
    "        if element.tag not in tags.keys():\n",
    "            tags[element.tag] = 1\n",
    "        else:\n",
    "            tags[element.tag] += 1\n",
    "    return tags\n",
    "\n",
    "count_tags(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a8aae7",
   "metadata": {},
   "source": [
    "We would like to change the data model and expand the \"addr:street\" type of keys to a dictionary like this:\n",
    "{\"address\": {\"street\": \"Some value\"}}\n",
    "So, we have to see if we have such tags, and if we have any tags with problematic characters.\n",
    "\n",
    "Below we have a count of each of four tag categories in a dictionary:\n",
    "  \"lower\", for tags that contain only lowercase letters and are valid,\n",
    "  \"lower_colon\", for otherwise valid tags with a colon in their names,\n",
    "  \"problemchars\", for tags with problematic characters, and\n",
    "  \"other\", for other tags that do not fall into the other three categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de9d74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag types from the problem set in the course\n",
    "\"\"\"\n",
    "Your task is to explore the data a bit more. Before you process the data and add it into your database, you should check the\n",
    "\"k\" value for each \"<tag>\" and see if there are any potential problems.\n",
    "\n",
    "We have provided you with 3 regular expressions to check for certain patterns in the tags. As we saw in the quiz earlier, \n",
    "we would like to change the data model and expand the \"addr:street\" type of keys to a dictionary like this:\n",
    "{\"address\": {\"street\": \"Some value\"}}\n",
    "So, we have to see if we have such tags, and if we have any tags with problematic characters.\n",
    "\n",
    "Please complete the function 'key_type', such that we have a count of each of four tag categories in a dictionary:\n",
    "  \"lower\", for tags that contain only lowercase letters and are valid,\n",
    "  \"lower_colon\", for otherwise valid tags with a colon in their names,\n",
    "  \"problemchars\", for tags with problematic characters, and\n",
    "  \"other\", for other tags that do not fall into the other three categories.\n",
    "\"\"\"\n",
    "OSMFILE = \"perth_australia_sample.osm\"\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        k = element.attrib['k']\n",
    "        if problemchars.search(k):\n",
    "            keys['problemchars'] += 1\n",
    "        elif lower_colon.search(k):\n",
    "            keys['lower_colon'] += 1\n",
    "        elif lower.search(k):\n",
    "            keys['lower'] += 1\n",
    "        else:\n",
    "            keys['other'] += 1\n",
    "    return keys\n",
    "\n",
    "\n",
    "\n",
    "def process_map(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(osm_file):\n",
    "        keys = key_type(element, keys)\n",
    "    osm_file.close()\n",
    "    return keys\n",
    "\n",
    "process_map(OSMFILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac96497",
   "metadata": {},
   "source": [
    "### Explore users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df41b47b",
   "metadata": {},
   "source": [
    "Below is a set of how many unique users have contributed to the map in this particular area:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcfe16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a regex for the street names as street_type_re \n",
    "# Create a default dictionary of standardized names\n",
    "# Audit the file to find alternate names\n",
    "\n",
    "OSMFILE = \"perth_australia_sample.osm\"\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "street_types = defaultdict(set)\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Highway\", \"Way\", \"Freeway\", \"Crossing\", \"Mall\", \"Loop\", \"Circle\", \"Crescent\", \"Gate\", \"Close\",\n",
    "           \"Mews\", \"Parade\", \"Terrace\"]\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "        if elem.tag == \"way\" or elem.tag == \"node\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types\n",
    "\n",
    "\n",
    "# Run audit and print results\n",
    "st_types = audit(OSMFILE)\n",
    "pprint.pprint(dict(st_types))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59098fae",
   "metadata": {},
   "source": [
    "From the output of this audit, the data for Perth is actually pretty clean. There are only a couple of abbreviations to change (St and Ct) as well as a tag 'street' to change into 'Street' and 'Boulevarde' which is a typo for 'Boulevard'. I also find 'Subiaco' and 'Caversham', which are suburbs' names, 'Morrison' which is a Mall, 'Gelderland' which should be \"Gelderland Entrance\". Both Fairfield Garden and Connaught Garden should also have Garden spelled 'Gardens'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad80c255",
   "metadata": {},
   "source": [
    "However, while the 50MB portion of the initial osm file looks pretty clean, we can have other cases of abbreviated names in the full dataset. Therefore, I'll use an extensive mapping of common abbreviations to update the names. The streetnames are returned below after correction, with the format \"name => better_name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491b3319",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"ST\": \"Street\",\n",
    "            \"st\": \"Street\",\n",
    "            \"Rd.\": \"Road\",\n",
    "            \"Rd\": \"Road\",\n",
    "            \"RD\": \"Road\",\n",
    "            \"Ave\": \"Avenue\",\n",
    "            \"Ave.\": \"Avenue\",\n",
    "            \"Blvd\": \"Boulevard\",\n",
    "            \"BLVD\": \"Boulevard\",\n",
    "            \"Cir\": \"Circle\",\n",
    "            \"Ct\": \"Court\",\n",
    "            \"Dr\": \"Drive\",\n",
    "            \"Garden\": \"Gardens\",\n",
    "            \"Trl\": \"Trail\",\n",
    "            \"Ter\": \"Terrace\",\n",
    "            \"Pl\": \"Place\",\n",
    "            \"Pkwy\": \"Parkway\",\n",
    "            \"Bnd\": \"Bend\",\n",
    "            \"Mnr\": \"Manor\",\n",
    "            \"Ln\": \"Lane\",\n",
    "            \"street\": \"Street\",\n",
    "            \"AVE\": \"Avenue\",\n",
    "            \"Blvd.\": \"Boulevard\",\n",
    "            \"Cirlce\": \"Circle\",\n",
    "            \"DRIVE\": \"Drive\",\n",
    "            \"Cv\": \"Cove\",\n",
    "            \"Dr.\": \"Drive\",\n",
    "            \"Druve\": \"Drive\",\n",
    "            \"Holw\": \"Hollow\",\n",
    "            \"Hwy\": \"Highway\",\n",
    "            \"HWY\": \"Highway\",\n",
    "            \"Pt\": \"Point\",\n",
    "            \"Trce\": \"Trace\",\n",
    "            \"ave\": \"Avenue\",\n",
    "            \"Cres\": \"Crescent\"\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70275be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_name(name, mapping):\n",
    "    \"\"\" Substitutes incorrect abbreviation with correct one. \"\"\"\n",
    "    m = street_type_re.search(name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        temp= 0\n",
    "        try:\n",
    "            temp = int(street_type)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        if street_type not in expected and temp == 0:\n",
    "            try:\n",
    "                name = re.sub(street_type_re, mapping[street_type], name)\n",
    "            except:\n",
    "                pass\n",
    "    return name\n",
    "\n",
    "for st_type, ways in st_types.iteritems():\n",
    "    for name in ways:\n",
    "        better_name = update_name(name, mapping)\n",
    "        print name, \"=>\", better_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b729e10b",
   "metadata": {},
   "source": [
    "### Audit and correct postcodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4cd5a2",
   "metadata": {},
   "source": [
    "Next I'll look into the postcodes: Perth postcodes are 4 digit-numbers starting by 6. Using similar functions than the section on streetnames, we can look for unusual postcodes as printed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7acc59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a group of auditing functions for postal codes\n",
    "def audit_postcode(post_code, digits):\n",
    "    \"\"\" Checks if postal code is incompatible and adds it to the list if so. \"\"\"\n",
    "    if len(digits) != 4 or digits[0] != '6':\n",
    "        post_code.append(digits)\n",
    "\n",
    "\n",
    "def is_postalcode(elem):\n",
    "    \"\"\" Returns a Boolean value.\"\"\"\n",
    "    return (elem.attrib['k'] == \"addr:postcode\")\n",
    "\n",
    "\n",
    "def audit(osmfile):\n",
    "    \"\"\" Iterates and returns list of inconsistent postal codes found in the document. \"\"\"\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    post_code = []\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_postalcode(tag):\n",
    "                    audit_postcode(post_code, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return post_code\n",
    "\n",
    "# Run audit and print results\n",
    "postal_codes = audit(OSMFILE)\n",
    "print postal_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095f5aec",
   "metadata": {},
   "source": [
    "Here WA stands for Western Australia. While those are correct postcodes, we can remove the letters and spaces to harmonize their format with the rest of the dataset. After a similar update process than for the street names, the unusual postcodes are corrected as per below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3ecb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_zipcode(post_code):    \n",
    "    if post_code[0:2] == 'WA' or post_code[0:2] == 'Wa' or post_code[0:2] == 'wa':\n",
    "        post_code = post_code[3:].strip()\n",
    "    return post_code\n",
    "\n",
    "for code in postal_codes:\n",
    "    better_code = update_zipcode(code)\n",
    "    print code, \"=>\", better_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac8f29d",
   "metadata": {},
   "source": [
    "### Convert XLM to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7f9ec3",
   "metadata": {},
   "source": [
    "Now we can transform the data into CSV files ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad168d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing for Database from the problem set in the course\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "SCHEMA = schema.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ebdff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if input element is a \"node\" or a \"way\" then clean, shape and parse to corresponding dictionary.\n",
    "\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "    \n",
    "    if element.tag == 'node':\n",
    "        for field in node_attr_fields:\n",
    "            node_attribs[field] = element.attrib[field]\n",
    "    \n",
    "    if element.tag == 'way':\n",
    "        for field in way_attr_fields:\n",
    "            way_attribs[field] = element.attrib[field]\n",
    "        \n",
    "        position = 0\n",
    "        temp = {}\n",
    "        for tag in element.iter(\"nd\"):\n",
    "            temp['id'] = element.attrib[\"id\"]\n",
    "            temp['node_id'] = tag.attrib[\"ref\"]\n",
    "            temp['position'] = position\n",
    "            position += 1\n",
    "            way_nodes.append(temp.copy())\n",
    "\n",
    "    temp = {}\n",
    "    for tag in element.iter(\"tag\"):\n",
    "        temp['id'] = element.attrib[\"id\"]\n",
    "        if \":\" in tag.attrib[\"k\"]:\n",
    "            newKey = re.split(\":\",tag.attrib[\"k\"],1)\n",
    "            temp['key'] = newKey[1]\n",
    "            if temp['key'] == 'postcode':\n",
    "                temp['value'] = update_zipcode(tag.attrib[\"v\"])\n",
    "            elif temp['key'] == 'street':\n",
    "                temp['value'] = update_name(tag.attrib[\"v\"],mapping)\n",
    "            else:\n",
    "                temp['value'] = tag.attrib[\"v\"]\n",
    "            temp[\"type\"] = newKey[0]\n",
    "        else:\n",
    "            temp['key'] = tag.attrib[\"k\"]\n",
    "            if temp['key'] == 'postcode':\n",
    "                temp['value'] = update_zipcode(tag.attrib[\"v\"])\n",
    "            elif temp['key'] == 'street':\n",
    "                temp['value'] = update_name(tag.attrib[\"v\"],mapping)\n",
    "            else:\n",
    "                temp['value'] = tag.attrib[\"v\"]\n",
    "            temp[\"type\"] = default_tag_type\n",
    "        tags.append(temp.copy())  \n",
    "        \n",
    "    \n",
    "    if element.tag == 'node':\n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    elif element.tag == 'way':\n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb26c0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_strings = (\n",
    "            \"{0}: {1}\".format(k, v if isinstance(v, str) else \", \".join(v))\n",
    "            for k, v in errors.iteritems()\n",
    "        )\n",
    "        raise cerberus.ValidationError(\n",
    "            message_string.format(field, \"\\n\".join(error_strings))\n",
    "        )\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1ca028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "         codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bf5187",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_map(OSMFILE, validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c660e1c1",
   "metadata": {},
   "source": [
    "## Analyze the data with SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603c537f",
   "metadata": {},
   "source": [
    "### Import CSV into SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e05766e",
   "metadata": {},
   "source": [
    "... and import those CSV files into an SQLite3 Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9601c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of ways\n",
    "query = \"SELECT COUNT(*) FROM ways;\"\n",
    "c.execute(query)\n",
    "c.fetchall()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26545aea",
   "metadata": {},
   "source": [
    "Count of nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4a161c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of nodes\n",
    "query = \"SELECT COUNT(*) FROM nodes;\"\n",
    "c.execute(query)\n",
    "c.fetchall()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec61cc09",
   "metadata": {},
   "source": [
    "Top 5 contributing users and their number of contributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ad65ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 5 contributing users and their number of contributions\n",
    "query = \"SELECT e.user, COUNT(*) as num \\\n",
    "FROM (SELECT user FROM nodes UNION ALL SELECT user FROM ways) e GROUP BY e.user ORDER BY num DESC LIMIT 5;\"\n",
    "c.execute(query)\n",
    "c.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e470c89",
   "metadata": {},
   "source": [
    "10 most popular fast food chains and their count of restaurants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9d88d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 most popular fast food chains and their count of restaurants\n",
    "query = \"SELECT nodes_tags.value, COUNT(*) as num \\\n",
    "FROM nodes_tags JOIN (SELECT DISTINCT(id) FROM nodes_tags WHERE value='fast_food') as r \\\n",
    "    ON nodes_tags.id=r.id WHERE nodes_tags.key='name' \\\n",
    "GROUP BY nodes_tags.value ORDER BY num DESC LIMIT 10;\"\n",
    "c.execute(query)\n",
    "c.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa2e5e5",
   "metadata": {},
   "source": [
    "10 most represented types of amenities and their occurence count:"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
