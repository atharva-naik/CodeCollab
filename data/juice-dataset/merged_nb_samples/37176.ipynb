{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "893cc2c9",
   "metadata": {},
   "source": [
    "<img src=\"ReinforcementLearningMAZE_S1.png\",width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1477c273",
   "metadata": {},
   "source": [
    "State: $S = \\{0, \\ldots,10\\}$\n",
    "\n",
    "The agent can be located in one of the states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e72e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "StateSize = 11\n",
    "setS = range(StateSize)\n",
    "print setS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a9b6c7",
   "metadata": {},
   "source": [
    "#### Display one of the state figures randomly\n",
    "\n",
    "The following code shows the state figures randomly. Try the following code repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9626b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "# def displayCurState(curState):\n",
    "#     StateFigureName = \"ReinforcementLearningMAZE_S%d.png\" % curState\n",
    "#     print StateFigureName\n",
    "#     # Image(filename='ReinforcementLearningMAZE_S1.png', width=500) \n",
    "#     Image(filename=StateFigureName, width=500) \n",
    "\n",
    "# Randomly display one of the state figure\n",
    "curState = np.floor(np.random.rand()*len(setS))\n",
    "print curState\n",
    "\n",
    "# displayCurState(curState)\n",
    "\n",
    "StateFigureName = \"ReinforcementLearningMAZE_S%d.png\" % curState\n",
    "print StateFigureName\n",
    "# Image(filename='ReinforcementLearningMAZE_S1.png', width=500) \n",
    "Image(filename=StateFigureName, width=500) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848582f4",
   "metadata": {},
   "source": [
    "## Environment Parameters\n",
    "\n",
    "Now first is first. Insert the transition probabilities for each action. The actions consist of \n",
    "\n",
    "left (action code = 0)\n",
    "\n",
    "right (=1)\n",
    "\n",
    "up (=2)\n",
    "\n",
    "down (=3).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90742b42",
   "metadata": {},
   "source": [
    "The variable SignalRatio ($=p$) is defined. If the agent is located at 0 (current state is 0), with the action right (=1), the agent aims to move to the state 1, but it moves to the state 4 with probability $\\frac{1 - p}{3}$. With probability $p$ and additional $\\frac{2}{3}(1 - p)$, the agent moves to the state 1.\n",
    "\n",
    "Try $p = 0.9$, $p = 0.4$, $p = 0.2$, $p = 0.99$, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb86cb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "actionNum = 4    # left, right, up, down\n",
    "SignalRatio = 0.9\n",
    "\n",
    "# Transition progabilities of action 1 (Left)\n",
    "action1 = np.zeros([StateSize, StateSize])   # left\n",
    "action1[0,0] = SignalRatio + (1 - SignalRatio)/3\n",
    "action1[0,1] = (1 - SignalRatio)/3\n",
    "action1[0,4] = (1 - SignalRatio)/3\n",
    "action1[1,0] = SignalRatio + (1 - SignalRatio)/3*2\n",
    "action1[1,2] = (1 - SignalRatio)/3\n",
    "action1[2,1] = SignalRatio + (1 - SignalRatio)/3\n",
    "action1[2,3] = (1 - SignalRatio)/3\n",
    "action1[2,5] = (1 - SignalRatio)/3\n",
    "action1[3,:] = 1./(StateSize - 1)\n",
    "action1[3,3] = 0\n",
    "action1[4,4] = SignalRatio + (1 - SignalRatio)/3\n",
    "action1[4,0] = (1 - SignalRatio)/3\n",
    "action1[4,7] = (1 - SignalRatio)/3\n",
    "action1[5,5] = SignalRatio\n",
    "action1[5,2] = (1 - SignalRatio)/3\n",
    "action1[5,6] = (1 - SignalRatio)/3\n",
    "action1[5,9] = (1 - SignalRatio)/3\n",
    "action1[6,:] = 1./(StateSize - 1)\n",
    "action1[6,6] = 0\n",
    "action1[7,7] = SignalRatio + (1 - SignalRatio)/3\n",
    "action1[7,4] = (1 - SignalRatio)/3\n",
    "action1[7,8] = (1 - SignalRatio)/3\n",
    "action1[8,7] = SignalRatio + (1 - SignalRatio)/3*2\n",
    "action1[8,9] = (1 - SignalRatio)/3\n",
    "action1[9,8] = SignalRatio + (1 - SignalRatio)/3\n",
    "action1[9,5] = (1 - SignalRatio)/3\n",
    "action1[9,10] = (1 - SignalRatio)/3\n",
    "action1[10,9] = SignalRatio + (1 - SignalRatio)/3\n",
    "action1[10,6] = (1 - SignalRatio)/3\n",
    "action1[10,10] = (1 - SignalRatio)/3\n",
    "\n",
    "# Transition progabilities of action 2 (Right)\n",
    "action2 = np.zeros([StateSize, StateSize])   # right\n",
    "action2[0,1] = SignalRatio + (1 - SignalRatio)/3*2\n",
    "action2[0,4] = (1 - SignalRatio)/3\n",
    "action2[1,2] = SignalRatio + (1 - SignalRatio)/3*2\n",
    "action2[1,0] = (1 - SignalRatio)/3\n",
    "action2[2,3] = SignalRatio + (1 - SignalRatio)/3\n",
    "action2[2,1] = (1 - SignalRatio)/3\n",
    "action2[2,5] = (1 - SignalRatio)/3\n",
    "action2[3,:] = 1./(StateSize - 1)\n",
    "action2[3,3] = 0\n",
    "action2[4,4] = SignalRatio + (1 - SignalRatio)/3\n",
    "action2[4,0] = (1 - SignalRatio)/3\n",
    "action2[4,7] = (1 - SignalRatio)/3\n",
    "action2[5,6] = SignalRatio + (1 - SignalRatio)/3\n",
    "action2[5,2] = (1 - SignalRatio)/3\n",
    "action2[5,9] = (1 - SignalRatio)/3\n",
    "action2[6,:] = 1./(StateSize - 1)\n",
    "action2[6,6] = 0\n",
    "action2[7,8] = SignalRatio + (1 - SignalRatio)/3*2\n",
    "action2[7,4] = (1 - SignalRatio)/3\n",
    "action2[8,9] = SignalRatio + (1 - SignalRatio)/3*2\n",
    "action2[8,7] = (1 - SignalRatio)/3\n",
    "action2[9,10] = SignalRatio + (1 - SignalRatio)/3\n",
    "action2[9,5] = (1 - SignalRatio)/3\n",
    "action2[9,8] = (1 - SignalRatio)/3\n",
    "action2[10,10] = SignalRatio + (1 - SignalRatio)/3\n",
    "action2[10,6] = (1 - SignalRatio)/3\n",
    "action2[10,9] = (1 - SignalRatio)/3\n",
    "\n",
    "# Transition progabilities of action 3 (Up)\n",
    "action3 = np.zeros([StateSize, StateSize])   # up\n",
    "action3[0,0] = SignalRatio + (1 - SignalRatio)/3\n",
    "action3[0,1] = (1 - SignalRatio)/3\n",
    "action3[0,4] = (1 - SignalRatio)/3\n",
    "action3[1,1] = SignalRatio + (1 - SignalRatio)/3\n",
    "action3[1,0] = (1 - SignalRatio)/3\n",
    "action3[1,2] = (1 - SignalRatio)/3\n",
    "action3[2,2] = SignalRatio\n",
    "action3[2,1] = (1 - SignalRatio)/3\n",
    "action3[2,3] = (1 - SignalRatio)/3\n",
    "action3[2,5] = (1 - SignalRatio)/3\n",
    "action3[3,:] = 1./(StateSize - 1)\n",
    "action3[3,3] = 0\n",
    "action3[4,0] = SignalRatio + (1 - SignalRatio)/3*2\n",
    "action3[4,7] = (1 - SignalRatio)/3\n",
    "action3[5,2] = SignalRatio + (1 - SignalRatio)/3\n",
    "action3[5,6] = (1 - SignalRatio)/3\n",
    "action3[5,9] = (1 - SignalRatio)/3\n",
    "action3[6,:] = 1./(StateSize - 1)\n",
    "action3[6,6] = 0\n",
    "action3[7,4] = SignalRatio + (1 - SignalRatio)/3*2\n",
    "action3[7,8] = (1 - SignalRatio)/3\n",
    "action3[8,8] = SignalRatio + (1 - SignalRatio)/3\n",
    "action3[8,7] = (1 - SignalRatio)/3\n",
    "action3[8,9] = (1 - SignalRatio)/3\n",
    "action3[9,5] = SignalRatio + (1 - SignalRatio)/3\n",
    "action3[9,8] = (1 - SignalRatio)/3\n",
    "action3[9,10] = (1 - SignalRatio)/3\n",
    "action3[10,6] = SignalRatio + (1 - SignalRatio)/3*2\n",
    "action3[10,9] = (1 - SignalRatio)/3\n",
    "\n",
    "# Transition progabilities of action 4 (Down)\n",
    "action4 = np.zeros([StateSize, StateSize])   # down\n",
    "action4[0,4] = SignalRatio + (1 - SignalRatio)/3*2\n",
    "action4[0,1] = (1 - SignalRatio)/3\n",
    "action4[1,1] = SignalRatio + (1 - SignalRatio)/3\n",
    "action4[1,0] = (1 - SignalRatio)/3\n",
    "action4[1,2] = (1 - SignalRatio)/3\n",
    "action4[2,5] = SignalRatio + (1 - SignalRatio)/3\n",
    "action4[2,1] = (1 - SignalRatio)/3\n",
    "action4[2,3] = (1 - SignalRatio)/3\n",
    "action4[3,:] = 1./(StateSize - 1)\n",
    "action4[3,3] = 0\n",
    "action4[4,7] = SignalRatio + (1 - SignalRatio)/3*2\n",
    "action4[4,0] = (1 - SignalRatio)/3\n",
    "action4[5,9] = SignalRatio + (1 - SignalRatio)/3\n",
    "action4[5,2] = (1 - SignalRatio)/3\n",
    "action4[5,6] = (1 - SignalRatio)/3\n",
    "action4[6,:] = 1./(StateSize - 1)\n",
    "action4[6,6] = 0\n",
    "action4[7,7] = SignalRatio + (1 - SignalRatio)/3\n",
    "action4[7,4] = (1 - SignalRatio)/3\n",
    "action4[7,8] = (1 - SignalRatio)/3\n",
    "action4[8,8] = SignalRatio + (1 - SignalRatio)/3\n",
    "action4[8,7] = (1 - SignalRatio)/3\n",
    "action4[8,9] = (1 - SignalRatio)/3\n",
    "action4[9,9] = SignalRatio + (1 - SignalRatio)/3\n",
    "action4[9,8] = (1 - SignalRatio)/3\n",
    "action4[9,10] = (1 - SignalRatio)/3\n",
    "action4[10,10] = SignalRatio + (1 - SignalRatio)/3\n",
    "action4[10,6] = (1 - SignalRatio)/3\n",
    "action4[10,9] = (1 - SignalRatio)/3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8189377a",
   "metadata": {},
   "source": [
    "Now we can test the transitions by the actions using the transition probabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11028b4",
   "metadata": {},
   "source": [
    "#### Determine the initial state randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6172592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "currentState = np.floor(np.random.random()*StateSize)\n",
    "currentState = int(currentState)\n",
    "print currentState"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a120d7b4",
   "metadata": {},
   "source": [
    "#### Reward initialize and set the discount factor $\\gamma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bca4300",
   "metadata": {},
   "outputs": [],
   "source": [
    "curTotalReward = 0\n",
    "gammaVal = .9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b65606e",
   "metadata": {},
   "source": [
    "#### Test State change and try to obtain more rewards through actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216b2f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "accMat = np.zeros([StateSize,StateSize])\n",
    "for icnt1 in range(StateSize):\n",
    "    for icnt2 in range(0, icnt1 + 1):\n",
    "        accMat[icnt1,icnt2] = 1\n",
    "\n",
    "def left(State):\n",
    "    stateVec = np.zeros([StateSize,1])\n",
    "    stateVec[State] = 1\n",
    "    transitionVec = np.dot(action1.T,stateVec)   # left action\n",
    "    accuVec = accMat.dot(transitionVec)\n",
    "    thresholdVal = np.random.random()\n",
    "    State = np.sum((accuVec - thresholdVal) < 0)\n",
    "    print transitionVec.T\n",
    "    print \"left\"\n",
    "    return State\n",
    "def right(State):\n",
    "    stateVec = np.zeros([StateSize,1])\n",
    "    stateVec[State] = 1\n",
    "    transitionVec = np.dot(action2.T,stateVec)   # right action\n",
    "    accuVec = accMat.dot(transitionVec)\n",
    "    thresholdVal = np.random.random()\n",
    "    State = np.sum((accuVec - thresholdVal) < 0)\n",
    "    print transitionVec.T\n",
    "    print \"right\"\n",
    "    return State\n",
    "def up(State):\n",
    "    stateVec = np.zeros([StateSize,1])\n",
    "    stateVec[State] = 1\n",
    "    transitionVec = np.dot(action3.T,stateVec)   # up action\n",
    "    accuVec = accMat.dot(transitionVec)\n",
    "    thresholdVal = np.random.random()\n",
    "    State = np.sum((accuVec - thresholdVal) < 0)\n",
    "    print transitionVec.T\n",
    "    print \"up\"\n",
    "    return State\n",
    "def down(State):\n",
    "    stateVec = np.zeros([StateSize,1])\n",
    "    stateVec[State] = 1\n",
    "    transitionVec = np.dot(action4.T,stateVec)   # down action\n",
    "    accuVec = accMat.dot(transitionVec)\n",
    "    thresholdVal = np.random.random()\n",
    "    State = np.sum((accuVec - thresholdVal) < 0)\n",
    "    print transitionVec.T\n",
    "    print \"down\"\n",
    "    return State\n",
    "\n",
    "PerformAction = {0: left,\n",
    "          1: right,\n",
    "          2: up,\n",
    "          3: down}\n",
    "\n",
    "print currentState\n",
    "# action = 0 # left\n",
    "# action = 1 # right\n",
    "# action = 2 # up\n",
    "# action = 3 # down\n",
    "#################################################################\n",
    "### Change the action and Try to obtain more reward!! ###########\n",
    "action = 1\n",
    "#################################################################\n",
    "currentState = PerformAction[action](currentState)\n",
    "\n",
    "if currentState == 3:\n",
    "    curTotalReward = curTotalReward + 1\n",
    "elif currentState == 6:\n",
    "    curTotalReward = curTotalReward - 1\n",
    "else:\n",
    "    curTotalReward = curTotalReward*gammaVal\n",
    "print \"current Reward: %f\" % curTotalReward\n",
    "\n",
    "StateFigureName = \"ReinforcementLearningMAZE_S%d.png\" % currentState\n",
    "Image(filename=StateFigureName, width=500) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a73095",
   "metadata": {},
   "source": [
    "#### Take Reward, Q value function, and policy $\\pi(a|s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49061d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "Reward = np.zeros(StateSize)\n",
    "Reward[3] = 1\n",
    "Reward[6] = -1\n",
    "#Reward = np.ones(StateSize)*1\n",
    "#Reward[3] = 2\n",
    "#Reward[6] = 0\n",
    "\n",
    "# Q value initialization\n",
    "QVal = np.zeros([StateSize, actionNum])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f21e24",
   "metadata": {},
   "source": [
    "#### Policy update\n",
    "\n",
    "The policy is the strategy of choosing actions among the action set $A = \\{Left (=0), Right (=1), Up (=2), Down (=3)\\}$.\n",
    "\n",
    "$a = \\arg\\max_{a'\\in A} Q(s,a')$\n",
    "\n",
    "\n",
    "\n",
    "Probability of choosing action $a$:\n",
    "\\begin{eqnarray}\n",
    "\\pi(a|s) \\leftarrow \\delta\\left(a - \\arg\\max_{a'\\in A} Q(s,a')\\right)\n",
    "\\end{eqnarray}\n",
    "\n",
    "The variable PolicyPiMat in the following code is the $State \\times Action$ matrix with elements of probabilities of choosing the action at each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539bbf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices(a, func):\n",
    "    return [i for (i, val) in enumerate(a) if func(val)]\n",
    "\n",
    "# policy pi can be changed to probabilites later\n",
    "policyPi = np.argmax(QVal, axis=1)\n",
    "\n",
    "PolicyPiMat = np.zeros([StateSize,actionNum])\n",
    "for iact in range(actionNum):\n",
    "    curActionPolicy = np.zeros(StateSize)   # initialize\n",
    "    StatesOfCurAction = indices(policyPi, lambda x: x == iact)  # states having current action as its maximum Q(s,a)\n",
    "    curActionPolicy[StatesOfCurAction] = 1\n",
    "    PolicyPiMat.T[iact] = curActionPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61754d6e",
   "metadata": {},
   "source": [
    "#### Value function update\n",
    "\n",
    "With the actions generated from the policy distribution $\\pi(a|s)$ and the state transition probability distribution $p(s'|s,a)$, we consider the generated trajectory $h = \\{(s_1,a_1), (s_2,a_2), \\ldots\\}$ with infinite length. Then the value function $Q(s,a)$ with given policy $\\pi$ is the expectation of the total reward $r(s,a)$ on the trajectory discounted with $\\gamma$.\n",
    "\\begin{eqnarray}\n",
    "Q^\\pi(s,a) &=& \\mathbb{E}_{p^\\pi(h)}\\left[R(h)|s_1 = s, a_1 = a\\right] \\\\\n",
    "R(h) &=& r(s_1,a_1) + \\gamma r(s_2,a_2) + \\gamma^2 r(s_3,a_3) + \\ldots \\\\\n",
    "&=& \\sum_{i = 1}^\\infty r(s_i,a_i)\\gamma^{i - 1}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "\n",
    "The solution $Q$-function with policy $\\pi$ satisfies the following Bellman equation for all state $s$ and action $a$:\n",
    "\\begin{eqnarray}\n",
    "Q^\\pi(s,a) &=& r(s,a) + \\gamma\\mathbb{E}_{\\pi(a'|s')p(s'|s,a)}\\left[Q^\\pi(s',a')\\right] \\\\\n",
    "\\pi(a|s) &=& \\delta\\left(a - \\arg\\max_{a'\\in A} Q(s,a')\\right)\n",
    "\\end{eqnarray}\n",
    "with transition probabilities $p(s'|s,a)$ and the discount factor $\\gamma$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b53e83",
   "metadata": {},
   "source": [
    "#### Q-Learning\n",
    "\n",
    "Q-learning adopts the iterative process of putting the right-hand side to the left-hand side. At each $n=1,\\ldots,N$, the $\\widehat{Q}_n$ and $\\pi_n$ is updated to $\\widehat{Q}_{n + 1}$ and $\\pi_{n + 1}$ using\n",
    "\\begin{eqnarray}\n",
    "\\widehat{Q}_{n+1}(s,a) &\\leftarrow& r(s,a) + \\gamma\\mathbb{E}_{\\pi(a'|s')p(s'|s,a)}\\left[\\widehat{Q}_n(s',a')\\right] \\\\\n",
    "\\pi_{n+1}(a|s) &\\leftarrow& \\delta\\left(a - \\arg\\max_{a'\\in A} Q_n(s,a')\\right)\n",
    "\\end{eqnarray}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df87331",
   "metadata": {},
   "source": [
    "#### Perform Q-Learning efficiently\n",
    "\n",
    "By the following deriviation, the iterative update can be performed efficiently:\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}_{\\pi(a'|s')p(s'|s,a)}\\left[\\widehat{Q}_n(s',a')\\right] &=& \\sum_{a'}\\sum_{s'}\\pi(a'|s')p(s'|s,a)Q(s',a') \\\\\n",
    "&=& \\sum_{s'}p(s'|s,a)\\underbrace{\\sum_{a'}\\pi(a'|s')Q(s',a')}_{\\text{For each} \\ s' \\text{, calculate this scalar}. }.\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d1be9e",
   "metadata": {},
   "source": [
    "\n",
    "Let $g(s') \\equiv \\sum_{a'}\\pi(a'|s')Q(s',a')$ where the vector $\\mathbf{g}$ with $s'$-element is $\\mathbf{g}_{s'} = g(s')$:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\left(\\begin{array}{c} \\ \\\\ \\mathbf{g} \\\\ \\ \\end{array}\\right) = \n",
    "\\left[\\left(\\begin{array}{ccc} \\ & \\ & \\ \\\\ \\ & \\mathbf{\\pi}_{|S|\\times|A|} & \\ \\\\ \\ & \\ & \\ \\end{array}\\right) \\odot \\left(\\begin{array}{ccc} \\ & \\ & \\ \\\\ \\ & \\mathbf{Q_{|S|\\times|A|}} & \\ \\\\ \\ & \\ & \\ \\end{array}\\right)\\right]\\mathbf{1}_{|A|}\n",
    "\\end{eqnarray}\n",
    "with $\\odot$, the elementwise multiplication of two matrices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec735da",
   "metadata": {},
   "source": [
    "With another definition of the matrix $F_{|S|\\times|A|}$ with $(s,a)$-element $F_{s,a} = \\mathbb{E}[Q(s,a)]$, we can obtain the elementwise equation:\n",
    "\\begin{eqnarray}\n",
    "F_{s,a} = \\sum_{s'} p(s'|s,a)g(s'),\n",
    "\\end{eqnarray}\n",
    "and the following matrix equation:\n",
    "\\begin{eqnarray}\n",
    "\\left(\\begin{array}{ccc} \\ & \\ & \\ \\\\ \\ & F_{|S|\\times|A|} & \\ \\\\ \\ & \\ & \\ \\end{array}\\right) = \\left(\\left(\\begin{array}{c} \\ \\\\ \\mathbf{f}^{a = 1} \\\\ \\ \\end{array}\\right)\\left(\\begin{array}{c} \\ \\\\ \\mathbf{f}^{a = 2} \\\\ \\ \\end{array}\\right)\\ldots\\left(\\begin{array}{c} \\ \\\\ \\mathbf{f}^{a = A} \\\\ \\ \\end{array}\\right)\\right)\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f92189b",
   "metadata": {},
   "source": [
    "with vectors satisfying\n",
    "\\begin{eqnarray}\n",
    "\\left(\\begin{array}{c} \\ \\\\ \\mathbf{f}^a \\\\ \\ \\end{array}\\right) = \\left(\\begin{array}{ccc} \\ & \\ & \\ \\\\ \\ & T^a \\\\ \\ & \\ & \\ \\end{array}\\right) \\left(\\begin{array}{c} \\ \\\\ \\mathbf{g} \\\\ \\ \\end{array}\\right)\n",
    "\\end{eqnarray}\n",
    "\n",
    "with transition matrix $T^a$ for each action $a$ having elements $T^a_{s,s'} = p(s'|s,a)$\n",
    "\\begin{eqnarray}\n",
    "\\left(\\begin{array}{ccc} \\ & \\ & \\ \\\\ \\ & T^a_{s,s'} \\\\ \\ & \\ & \\ \\end{array}\\right) = \\left(\\begin{array}{ccc} p(s'=1|s=1,a) & p(s'=2|s=1,a) & \\cdots \\\\ p(s'=1|s=2,a) & \\ddots \\\\ \\vdots & \\ & \\ \\end{array}\\right).\n",
    "\\end{eqnarray}\n",
    "\n",
    "The element of $\\mathbf{f}^a$ is $\\mathbf{f}^a_s$ and the element of $\\mathbf{g}$ is $\\mathbf{g}_{s'}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df9431a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# g vector update\n",
    "gvec = np.sum(np.multiply(PolicyPiMat,QVal), axis=1)    # elementwise multiplication 1 (Good)\n",
    "#gvec = np.multiply(PolicyPiMat,QVal).dot(np.ones(actionNum)).T    # elementwise multiplication 2 (Good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3babecbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def left():\n",
    "    return action1\n",
    "def right():\n",
    "    return action2\n",
    "def up():\n",
    "    return action3\n",
    "def down():\n",
    "    return action4\n",
    "\n",
    "actionTransitionMat = {0: left,\n",
    "          1: right,\n",
    "          2: up,\n",
    "          3: down}\n",
    "\n",
    "EQVal = np.zeros([StateSize,actionNum])\n",
    "for iact in range(actionNum):\n",
    "#    print actionTransitionMat[iact]()\n",
    "    EQVal[:,iact] = actionTransitionMat[iact]().dot(gvec)\n",
    "\n",
    "#print EQVal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42ca5cb",
   "metadata": {},
   "source": [
    "The variable EQVal is the matrix $F$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddea59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print np.matrix(Reward).T\n",
    "# print np.tile(np.matrix(Reward).T, [1,4])\n",
    "\n",
    "#QVal = np.tile(np.matrix(Reward).T, [1,4]) + gammaVal*EQVal\n",
    "QVal = np.tile(Reward, [4,1]).T + gammaVal*EQVal\n",
    "#print QVal\n",
    "#print type(QVal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83efb2c2",
   "metadata": {},
   "source": [
    "Now one iteration is finished."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a9890e",
   "metadata": {},
   "source": [
    "#### perform many times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbde8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochNum = 100\n",
    "\n",
    "def left():\n",
    "    return action1\n",
    "def right():\n",
    "    return action2\n",
    "def up():\n",
    "    return action3\n",
    "def down():\n",
    "    return action4\n",
    "\n",
    "for iepoch in range(epochNum):\n",
    "    ####### Policy update #######\n",
    "    policyPi = np.argmax(QVal, axis=1)\n",
    "\n",
    "    PolicyPiMat = np.zeros([StateSize,actionNum])\n",
    "    for iact in range(actionNum):\n",
    "        curActionPolicy = np.zeros(StateSize)   # initialize\n",
    "        StatesOfCurAction = indices(policyPi, lambda x: x == iact)  # states having current action as its maximum Q(s,a)\n",
    "        curActionPolicy[StatesOfCurAction] = 1\n",
    "        PolicyPiMat.T[iact] = curActionPolicy\n",
    "\n",
    "    ######## Value function update #######\n",
    "    gvec = np.sum(np.multiply(PolicyPiMat,QVal), axis=1)    # elementwise multiplication\n",
    "    actionTransitionMat = {0: left,\n",
    "                          1: right,\n",
    "                          2: up,\n",
    "                          3: down}\n",
    "    EQVal = np.zeros([StateSize,actionNum])\n",
    "    for iact in range(actionNum):\n",
    "        EQVal[:,iact] = actionTransitionMat[iact]().dot(gvec)\n",
    "\n",
    "    QVal = np.tile(Reward, [4,1]).T + gammaVal*EQVal\n",
    "\n",
    "print QVal\n",
    "print np.argmax(QVal,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db13c487",
   "metadata": {},
   "source": [
    "#### Move agent using MDP policy"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
