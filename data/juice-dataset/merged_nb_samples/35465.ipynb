{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbe464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_phase2surrogate = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc1e0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "[turn_surrogate,convo_surrogate] = align.calculate_baseline_alignment(\n",
    "                                    input_files=INPUT_FILES, \n",
    "                                    maxngram=MAXNGRAM,\n",
    "                                    use_pretrained_vectors=USE_PRETRAINED_VECTORS,\n",
    "                                    pretrained_input_file=PRETRAINED_INPUT_FILE,\n",
    "                                    semantic_model_input_file=SEMANTIC_MODEL_INPUT_FILE,\n",
    "                                    output_file_directory=ANALYSIS_READY,\n",
    "                                    add_stanford_tags=ADD_STANFORD_TAGS,\n",
    "                                    ignore_duplicates=IGNORE_DUPLICATES,\n",
    "                                    high_sd_cutoff=HIGH_SD_CUTOFF,\n",
    "                                    low_n_cutoff=LOW_N_CUTOFF,\n",
    "                                    surrogate_file_directory=SURROGATE_TRANSCRIPTS,\n",
    "                                    all_surrogates=False,\n",
    "                                    keep_original_turn_order=True,\n",
    "                                    id_separator='\\-',\n",
    "                                    dyad_label='time',\n",
    "                                    condition_label='cond')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfff8223",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_phase2surrogate = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5089fe5",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649e88fa",
   "metadata": {},
   "source": [
    "# ALIGN output overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9951f9",
   "metadata": {},
   "source": [
    "## Speed calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27efdd13",
   "metadata": {},
   "source": [
    "As promised, let's take a look at how long it takes to run each section. Time is given in seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdea2e9",
   "metadata": {},
   "source": [
    "**Phase 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cd19d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_phase1 - start_phase1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffd6134",
   "metadata": {},
   "source": [
    "**Phase 2, real data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3624af",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_phase2real - start_phase2real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a802a67e",
   "metadata": {},
   "source": [
    "**Phase 2, surrogate data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b1f4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_phase2surrogate - start_phase2surrogate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2859ba92",
   "metadata": {},
   "source": [
    "**All phases:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f4a8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_phase2surrogate - start_phase1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f337de",
   "metadata": {},
   "source": [
    "## Printouts!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fee820",
   "metadata": {},
   "source": [
    "And that's it! Before we go, let's take a look at the output from the real data analyzed at the turn level for each conversation (`turn_real`) and at the conversation level for each dyad (`convo_real`). We'll then look at our surrogate data, analyzed both at the turn level (`turn_surrogate`) and at the conversation level (`convo_surrogate`). In our next step, we would then take these data and plug them into our statistical model of choice, but we'll stop here for the sake of our tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f96c91",
   "metadata": {},
   "source": [
    "Given appropriately prepared transcript files, ALIGN can be run in 3 high-level functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af505dc",
   "metadata": {},
   "source": [
    "**`prepare_transcripts`**: Pre-process each standardized \n",
    "conversation, checking it conforms to the requirements. \n",
    "Each utterance is tokenized and lemmatized and has \n",
    "POS tags added."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e948efd",
   "metadata": {},
   "source": [
    "**`calculate_alignment`**: Generates turn-level and \n",
    "conversation-level alignment scores (lexical, \n",
    "conceptual, and syntactic) across a range of \n",
    "*n*-gram sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f5c48c",
   "metadata": {},
   "source": [
    "**`calculate_baseline_alignment`**: Generate a surrogate corpus\n",
    "and run alignment analysis (using identical specifications \n",
    "from `calculate_alignment`) on it to produce a baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9ad762",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53a647f",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0c1cda",
   "metadata": {},
   "source": [
    "Import packages we'll need to run ALIGN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a49ad5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import align, os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179e2533",
   "metadata": {},
   "source": [
    "Import `time` so that we can get a sense of how\n",
    "long the ALIGN pipeline takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57247a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf84f76",
   "metadata": {},
   "source": [
    "Import `warnings` to flag us if required files aren't provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c426506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6fc0d5",
   "metadata": {},
   "source": [
    "## Install additional NTLK packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8e4b0c",
   "metadata": {},
   "source": [
    "Download some addition `nltk` packages for `align` to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cd201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d3a9b6",
   "metadata": {},
   "source": [
    "## Specify ALIGN path settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46591b6b",
   "metadata": {},
   "source": [
    "ALIGN will need to know where the raw transcripts are stored, where to store the processed data, and where to read in any additional files needed for optional ALIGN parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff5ad21",
   "metadata": {},
   "source": [
    "### Required directories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6e78b5",
   "metadata": {},
   "source": [
    "For the sake of this tutorial, specify a base path that will serve as our jumping-off point for our saved data. All of the CHILDES data and shipped data will be called from the package directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf0367f",
   "metadata": {},
   "source": [
    "**`BASE_PATH`**: Containing directory for this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a143fbda",
   "metadata": {},
   "source": [
    "## Run preparation phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef15d825",
   "metadata": {},
   "source": [
    "First, we prepare our transcripts by reading in individual `.txt`\n",
    "files for each conversation, clean up undesired text and turns,\n",
    "spell-check, tokenize, lemmatize, and add POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667491d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_phase1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e1aed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_store = align.prepare_transcripts(\n",
    "                        input_files=TRANSCRIPTS,\n",
    "                        output_file_directory=PREPPED_TRANSCRIPTS,\n",
    "                        minwords=2,\n",
    "                        use_filler_list=None,\n",
    "                        training_dictionary=None,\n",
    "                        add_stanford_tags=False,\n",
    "                        save_concatenated_dataframe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11478805",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_phase1 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecca1af0",
   "metadata": {},
   "source": [
    "# Phase 2: Calculate alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51009746",
   "metadata": {},
   "source": [
    "## For real data: Alignment calculation settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9665cad3",
   "metadata": {},
   "source": [
    "There are a number of parameters that we can set for the\n",
    "`calculate_alignment()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f4a89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print align.calculate_alignment.__doc__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b476b6fe",
   "metadata": {},
   "source": [
    "For the sake of this tutorial, we'll keep everything as\n",
    "defaults. Among other parameters, this means that we'll:\n",
    "* use only unigrams and bigrams for our *n*-grams\n",
    " (`maxngram=2`),\n",
    "* use pretrained vectors instead of creating our own\n",
    " semantic space, since our tutorial corpus is quite\n",
    " small (`use_pretrained_vectors=True` and\n",
    " `pretrained_file_directory=PRETRAINED_INPUT_FILE`),\n",
    "* ignore exact lexical duplicates when calculating\n",
    " syntactic alignment,\n",
    "* we'll rely only on the Penn POS tagger \n",
    " (`add_stanford_tags=False`), and\n",
    "* implement high- and low-frequency cutoffs to clean\n",
    " our transcript data (`high_sd_cutoff=3` and \n",
    " `low_n_cutoff=1`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e5fb52",
   "metadata": {},
   "source": [
    "Whenever we calculate a baseline level of alignment,\n",
    "we need to include the same parameter values for any\n",
    "parameters that are present in both `calculate_alignment()`\n",
    "(this step) and `calculate_baseline_alignment()`\n",
    "(next step). As a result, we'll specify these here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab87bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set standards to be used for real and surrogate\n",
    "INPUT_FILES = PREPPED_TRANSCRIPTS\n",
    "MAXNGRAM = 2\n",
    "USE_PRETRAINED_VECTORS = True\n",
    "SEMANTIC_MODEL_INPUT_FILE = os.path.join(CHILDES_EXAMPLE,\n",
    "                                         'align_concatenated_dataframe.txt')\n",
    "PRETRAINED_FILE_DRIRECTORY = PRETRAINED_INPUT_FILE\n",
    "ADD_STANFORD_TAGS = False\n",
    "IGNORE_DUPLICATES = True\n",
    "HIGH_SD_CUTOFF = 3\n",
    "LOW_N_CUTOFF = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ddd779",
   "metadata": {},
   "source": [
    "## For real data: Run alignment calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e0820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_phase2real = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d9fde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "[turn_real,convo_real] = align.calculate_alignment(\n",
    "                            input_files=INPUT_FILES,\n",
    "                            maxngram=MAXNGRAM,   \n",
    "                            use_pretrained_vectors=USE_PRETRAINED_VECTORS,\n",
    "                            pretrained_input_file=PRETRAINED_INPUT_FILE,\n",
    "                            semantic_model_input_file=SEMANTIC_MODEL_INPUT_FILE,\n",
    "                            output_file_directory=ANALYSIS_READY,\n",
    "                            add_stanford_tags=ADD_STANFORD_TAGS,\n",
    "                            ignore_duplicates=IGNORE_DUPLICATES,\n",
    "                            high_sd_cutoff=HIGH_SD_CUTOFF,\n",
    "                            low_n_cutoff=LOW_N_CUTOFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce14c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_phase2real = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950403f4",
   "metadata": {},
   "source": [
    "## For surrogate data: Alignment calculation settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88933af",
   "metadata": {},
   "source": [
    "For the surrogate or baseline data, we have many of the same\n",
    "parameters for `calculate_baseline_alignment()` as we do for\n",
    "`calculate_alignment()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fe5580",
   "metadata": {},
   "outputs": [],
   "source": [
    "print align.calculate_baseline_alignment.__doc__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e621ba5",
   "metadata": {},
   "source": [
    "As mentioned above, when calculating the baseline, it is **vital** \n",
    "to include the *same* parameter values for any parameters that \n",
    "are included  in both `calculate_alignment()` and \n",
    "`calculate_baseline_alignment()`. As a result, we re-use those\n",
    "values here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd833fc6",
   "metadata": {},
   "source": [
    "We demonstrate other possible uses for labels by setting \n",
    "`dyad_label = time`, allowing us to compare alignment over \n",
    "time across the same speakers. We also demonstrate how to \n",
    "generate a subset of surrogate pairings rather than all \n",
    "possible pairings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7d230c",
   "metadata": {},
   "source": [
    "In addition to the parameters that we're re-using from\n",
    "the `calculate_alignment()` values (see above), we'll \n",
    "keep most parameters at their defaults by:\n",
    "* preserving the turn order when creating surrogate\n",
    " pairs (`keep_original_turn_order=True`),\n",
    "* specifying condition with `cond` prefix \n",
    " (`condition_label='cond'`), and\n",
    "* using a hyphen to separate the condition and\n",
    " dyad identifiers (`id_separator='\\-'`).\n",
    " \n",
    "However, we will also change some of these defaults,\n",
    "including:\n",
    "* generating only a subset of surrogate data equal\n",
    " to the size of the real data (`all_surrogates=False`)\n",
    " and\n",
    "* specifying that we'll be shuffling the baseline data\n",
    " by time instead of by dyad (`dyad_label='time'`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f7dedd",
   "metadata": {},
   "source": [
    "## For surrogate data: Run alignment calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c366f90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSCRIPTS = align.datasets.CHILDES_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3895f8",
   "metadata": {},
   "source": [
    "**`PREPPED_TRANSCRIPTS`**: Set variable for folder name \n",
    "(as string) for relative location of folder into which \n",
    "prepared transcript files will be saved. (We'll create\n",
    "a default directory if one doesn't already exist.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ea5621",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPPED_TRANSCRIPTS = os.path.join(CHILDES_EXAMPLE,\n",
    "                                   'childes-prepped/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9965287a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(PREPPED_TRANSCRIPTS):\n",
    "    os.makedirs(PREPPED_TRANSCRIPTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5356bf3",
   "metadata": {},
   "source": [
    "**`ANALYSIS_READY`**: Set variable for folder name \n",
    "(as string) for relative location of folder into \n",
    "which analysis-ready dataframe files will be saved.\n",
    "(We'll create a default directory if one doesn't\n",
    "already exist.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92b5825",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANALYSIS_READY = os.path.join(CHILDES_EXAMPLE,\n",
    "                              'childes-analysis/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147402cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(ANALYSIS_READY):\n",
    "    os.makedirs(ANALYSIS_READY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970d6dd1",
   "metadata": {},
   "source": [
    "**`SURROGATE_TRANSCRIPTS`**: Set variable for folder name \n",
    "(as string) for relative location of folder into which all\n",
    "prepared surrogate transcript files will be saved. (We'll\n",
    "create a default directory if one doesn't already exist.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99982b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "SURROGATE_TRANSCRIPTS = os.path.join(CHILDES_EXAMPLE,\n",
    "                                     'childes-surrogate/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250dee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(SURROGATE_TRANSCRIPTS):\n",
    "    os.makedirs(SURROGATE_TRANSCRIPTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e04030",
   "metadata": {},
   "source": [
    "### Paths for optional parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bbe30c",
   "metadata": {},
   "source": [
    "**`OPTIONAL_PATHS`**: If using Stanford POS tagger or\n",
    "pretrained vectors, the path to these files. If these\n",
    "files are provided in other locations, be sure to\n",
    "change the file paths for them. (We'll create a default\n",
    "directory if one doesn't already exist.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2957da8",
   "metadata": {},
   "source": [
    "* Each input text file needs to contain a single conversation organized in an `N x 2` matrix\n",
    "    * Text file must be tab-delimited.\n",
    "* Each row must correspond to a single conversational turn from a speaker.\n",
    "    * Rows must be temporally ordered based on their occurrence in the conversation.\n",
    "    * Rows must alternate between speakers.\n",
    "* Speaker identifier and content for each turn are divided across two columns.\n",
    "    * Column 1 must have the header `participant`.\n",
    "        * Each cell specifies the speaker.\n",
    "        * Each speaker must have a unique label (e.g., `P1` and `P2`, `0` and `1`).\n",
    "    * Column 2 must have the header `content`.\n",
    "        * Each cell corresponds to the transcribed utterance from the speaker.\n",
    "        * Each cell must end with a newline character: `\\n`\n",
    "* See `examples` directory in Github repository for an example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54f8efe",
   "metadata": {},
   "source": [
    "## Filename conventions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a335ee",
   "metadata": {},
   "source": [
    "* Each conversation text file must be regularly formatted, including a prefix for dyad and a prefix for conversation prior to the identifier for each that are separated by a unique character. By default, ALIGN looks for patterns that follow this convention: `dyad1-condA.txt`\n",
    "    * However, users may choose to include any label for dyad or condition so long as the two labels are distinct from one another and are not subsets of any possible dyad or condition labels. Users may also use any character as a separator so long as it does not occur anywhere else in the filename.\n",
    "    * The chosen file format **must** be used when saving **all** files for this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d66c30f",
   "metadata": {},
   "source": [
    "## Highest-level functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e36ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3b2c75",
   "metadata": {},
   "source": [
    "**`CHILDES_EXAMPLE`**: Subdirectories for output and other\n",
    "files for this tutorial. (We'll create a default directory\n",
    "if one doesn't already exist.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4480f53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHILDES_EXAMPLE = os.path.join(BASE_PATH,\n",
    "                              'CHILDES/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e3f6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(CHILDES_EXAMPLE):\n",
    "    os.makedirs(CHILDES_EXAMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b64f46",
   "metadata": {},
   "source": [
    "**`TRANSCRIPTS`**: Path to raw transcript files. Automatically provided by `align`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1d1ebe",
   "metadata": {},
   "source": [
    "The Stanford POS tagger **will not be used** by \n",
    "default in this example. However, you may use them\n",
    "by uncommenting and providing the requested file \n",
    "paths in the cells in this section and then changing \n",
    "the relevant parameters in the ALIGN calls below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7db5e6",
   "metadata": {},
   "source": [
    "If desired, we could use the Standford part-of-speech \n",
    "tagger along with the Penn part-of-speech tagger\n",
    "(which is always used in ALIGN). To do so, the files\n",
    "will need to be downloaded separately: \n",
    "https://nlp.stanford.edu/software/tagger.shtml#Download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65c7054",
   "metadata": {},
   "source": [
    "**`STANFORD_POS_PATH`**: If using Stanford POS tagger\n",
    "with the Penn POS tagger, path to Stanford directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780ac19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STANFORD_POS_PATH = os.path.join(OPTIONAL_PATHS,\n",
    "#                                  'stanford-postagger-full-2017-06-09/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd50518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not STANFORD_POS_PATH:\n",
    "#     warnings.warn('Stanford POS directory not found at the specified '\n",
    "#                   'location. Please update the file path or comment '\n",
    "#                   'out the `STANFORD_POS_PATH` information.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0891fc8a",
   "metadata": {},
   "source": [
    "**`STANFORD_LANGUAGE`**: If using Stanford tagger,\n",
    "set language model to be used for POS tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2860f4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STANFORD_LANGUAGE = os.path.join(OPTIONAL_PATHS,\n",
    "#                                  'english-left3words-distsim.tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb3dc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not STANFORD_LANGUAGE:\n",
    "#     warnings.warn('Stanford tagger language not found at the specified '\n",
    "#                   'location. Please update the file path or comment '\n",
    "#                   'out the `STANFORD_POS_PATH` information.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d84b04",
   "metadata": {},
   "source": [
    "#### Google News pretrained vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0a1394",
   "metadata": {},
   "source": [
    "The Google News pretrained vectors **will be used**\n",
    "by default in this example. The file is available for\n",
    "download here: https://code.google.com/archive/p/word2vec/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7328370",
   "metadata": {},
   "source": [
    "If desired, researchers may choose to read in pretrained\n",
    "`word2vec` vectors rather than creating a semantic space\n",
    "from the corpus provided. This may be especially useful \n",
    "for small corpora (i.e., fewer than 30k unique words),\n",
    "although the choice of semantic space corpus should be\n",
    "made with careful consideration about the nature of the\n",
    "linguistic context (for further discussion, see Duran, \n",
    "Paxton, & Fusaroli, *submitted*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddb1d9b",
   "metadata": {},
   "source": [
    "**`PRETRAINED_INPUT_FILE`**: If using pretrained vectors, path\n",
    "to pretrained vector files. You may choose to download the file\n",
    "directly to this path or change the path to a different one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976b4a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_INPUT_FILE = os.path.join(OPTIONAL_PATHS,\n",
    "                            'GoogleNews-vectors-negative300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b3369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not PRETRAINED_INPUT_FILE:\n",
    "    warnings.warn('Google News vector not found at the specified '\n",
    "                  'location. Please update the file path or comment '\n",
    "                  'out the `PRETRAINED_INPUT_FILE` information.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123c2bb1",
   "metadata": {},
   "source": [
    "# Phase 1: Prepare transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e046d0",
   "metadata": {},
   "source": [
    "In Phase 1, we take our raw transcripts and get them ready\n",
    "for later ALIGN analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f7d8c9",
   "metadata": {},
   "source": [
    "## Preparation settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4de96c",
   "metadata": {},
   "source": [
    "There are a number of parameters that we can set for the\n",
    "`prepare_transcripts()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ec9f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print align.prepare_transcripts.__doc__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cc7c9f",
   "metadata": {},
   "source": [
    "For the sake of this demonstration, we'll keep everything as\n",
    "defaults. Among other parameters, this means that:\n",
    "* any turns fewer than 2 words will be removed from the corpus\n",
    " (`minwords=2`),\n",
    "* we'll be using regex to strip out any filler words\n",
    " (e.g., \"uh,\" \"um,\" \"huh\"; `use_filler_list=None`),\n",
    "* we'll be using the Project Gutenberg corpus to create our \n",
    " spell-checker algorithm (`training_dictionary=None`),\n",
    "* we'll rely only on the Penn POS tagger \n",
    " (`add_stanford_tags=False`), and\n",
    "* our data will be saved both as individual conversation files\n",
    " and as a master dataframe of all conversation outputs\n",
    " (`save_concatenated_dataframe=True`)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
