{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cd522a9",
   "metadata": {},
   "source": [
    "# Policy And Value Iteration for FrozenLake-v0\n",
    "Select problems from CMU's Deep RL HW1: https://katefvision.github.io/10703_hw1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a250fd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import time \n",
    "\n",
    "# To display seaborn heatmaps\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# So I can use the code that I wrote\n",
    "%run rl.py lake_envs.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79f9908",
   "metadata": {},
   "source": [
    "**Policy Iteration - Deterministic 8x8 Frozen Lake**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170efb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a - Policy Iteration \n",
    "gamma = 0.9 \n",
    "max_iterations = 1000\n",
    "tolerance = 1e-3 \n",
    "\n",
    "env = gym.make('Deterministic-8x8-FrozenLake-v0')\n",
    "env.reset()\n",
    "\n",
    "start = time.time()\n",
    "optimal_policy, optimal_value, eval_count, improve_count = policy_iteration(env, gamma, max_iterations, tolerance)\n",
    "end = time.time()\n",
    "print(\"{0:.5f}\".format(end - start) + ' s;', eval_count, 'policy evaluation steps;', improve_count, 'policy improvement steps;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595bd02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#b - Optimal Policy using Policy Iteration\n",
    "optimal_policy_map = print_policy(optimal_policy, lake_envs.action_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dfdc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#c- Value Function Color Plot - Policy Iteration \n",
    "print_value_function(optimal_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc51f37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#d - Value Iteration\n",
    "env.reset()\n",
    "start = time.time()\n",
    "optimal_value, iter_count = value_iteration(env, gamma, max_iterations, tolerance)\n",
    "end = time.time()\n",
    "print(\"{0:.5f}\".format(end - start) + ' s;', iter_count, 'value iteration steps;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4271b39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#e - Value Function Color Plot - Value Iteration \n",
    "print_value_function(optimal_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fe3980",
   "metadata": {},
   "source": [
    "Value Iteration was faster at 0.00702s, and required 64 iterations. Policy Iteration was slower at 0.03909s, and also required 64 iterations - 59 from policy evaluation and 5 from policy improvement. The value functions for policy and value iteration appear to be the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57224fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#h - Optimal Policy using Value Iteration\n",
    "\n",
    "#Extract optimal policy function\n",
    "optimal_policy_func = value_function_to_policy(env, gamma, optimal_value)\n",
    "\n",
    "#Using policy improvement on optimal value function to get optimal policy\n",
    "optimal_policy = improve_policy(env, gamma, optimal_value, optimal_policy_func)\n",
    "\n",
    "#Print policy map \n",
    "optimal_policy = print_policy(optimal_policy_func, lake_envs.action_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b7769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#i - Agent Executing Optimal Policy \n",
    "execute_policy(env, gamma, optimal_policy_func)\n",
    "print('This makes sense because the agent does not received any reward until the final step, which is discounted. Furthermore, the numbers are different because we are only considering cumulative reward and not state value in this case.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76693d63",
   "metadata": {},
   "source": [
    "**Value Iteration - Stochastic 4x4 Frozen Lake** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794a48a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a - Value Iteration\n",
    "env = gym.make('Stochastic-4x4-FrozenLake-v0')\n",
    "env.reset()\n",
    "\n",
    "start = time.time()\n",
    "optimal_value, iter_count = value_iteration(env, gamma, max_iterations, tolerance)\n",
    "end = time.time()\n",
    "print(\"{0:.5f}\".format(end - start) + ' s;', iter_count, 'value iteration steps;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7504ad74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#b - Value Function Color Plot - Value Iteration \n",
    "print_value_function(optimal_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462206a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#c - Optimal Policy using Value Iteration\n",
    "\n",
    "#Extract optimal policy function\n",
    "optimal_policy_func = value_function_to_policy(env, gamma, optimal_value)\n",
    "\n",
    "#Using policy improvement on optimal value function to get optimal policy\n",
    "optimal_policy = improve_policy(env, gamma, optimal_value, optimal_policy_func)\n",
    "\n",
    "#Print policy map \n",
    "optimal_policy = print_policy(optimal_policy_func, lake_envs.action_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f630d93",
   "metadata": {},
   "source": [
    "**d**: For being in some state s, the environment dynamics differ, which will affect what the optimal action is. For example, in the deterministic case, when choosing to go up, the agent will go up with 1.0 probability. However, in the stochastic case, choosing up allows for a 2/3 chance of going left or right.  "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
