{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a9384bf",
   "metadata": {},
   "source": [
    "----\n",
    "## Technical Report Notebook 2\n",
    "----\n",
    "### Tweet Text - Natural Language Processing and EDA\n",
    "\n",
    "This section of the EDA will analyse the tweet **text** for both events: Brisbane/27th Nov 2014 + Sydney/25th April 2015 and continue cleaning the dataset where appropriate.\n",
    "The EDA will compare words in the twitter text for both events throughout the 24 hour period and analyse the key words that surround the term \"hail\".\n",
    "\n",
    "The notebook will then extend the analysis onto Topic Modelling (using latent Dirichlet allocation) and sentiment analysis.\n",
    "\n",
    "----\n",
    "\n",
    "### 2.i. Twitter Text EDA - Data Cleaning\n",
    "\n",
    "Firstly I will clean and vectorise the full tweet text data prior to analysis..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59abbe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "stops = list(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3ed86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note these terms were added into stopwords list after the topic modelling (later in the report) as they\n",
    "# were distorting the groupings\n",
    "stops.append('youtube')\n",
    "stops.append('video')\n",
    "stops.append('via')\n",
    "stops.append('de')\n",
    "stops.append('done')\n",
    "stops.append('amp')\n",
    "stops.append('like')\n",
    "stops.append('keep')\n",
    "stops.append('go')\n",
    "stops.append('gt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81c4f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use function from capstone part2 to clean twitter data\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "\n",
    "    #Remove links from tweet\n",
    "    link_remove = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "    \n",
    "    #Remove non-letters from tweet    \n",
    "    letters = re.sub(\"[^a-zA-Z]\", \" \", link_remove) \n",
    "    \n",
    "    #Convert to lower case, split into individual words\n",
    "    words = letters.lower().split()                             \n",
    "    \n",
    "    #Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    \n",
    "    #Join the words back into one string separated by space and return the result.\n",
    "    return (\" \".join(meaningful_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccde6f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the final CSV for use in the Tweet Text Analysis:\n",
    "\n",
    "hail_data_final = pd.read_csv(\"./hail_data_final.csv\")\n",
    "hail_data_final.drop('Unnamed: 0',axis=1)\n",
    "hail_data_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac5c88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold the full parsed twitter dataset\n",
    "\n",
    "clean_hail_data = []\n",
    "\n",
    "# Loop over each tweet and append the cleanded tweet:\n",
    "\n",
    "for tweet in hail_data_final[\"Text\"]:\n",
    "    clean_hail_data.append(clean_tweet(tweet))\n",
    "    \n",
    "print(clean_hail_data[:10])\n",
    "print(len(clean_hail_data))\n",
    "\n",
    "hail_data_final['tweet_words'] = clean_hail_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3205aff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the CountVectorizer\n",
    "\n",
    "# In this updated version I also investigated 2 word ngrams to pick up any damage related terms\n",
    "# such as \"golf-ball\" and \"tennis-ball\", however too many of the bigrams were obvious duplicates of the single terms\n",
    "# so I reverted back to single words only.\n",
    "\n",
    "print (\"Vectorising! Please wait...\\n\")\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",tokenizer = None,stop_words = 'english',max_features = 1000) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(clean_hail_data)\n",
    "\n",
    "columns = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3308d740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit cvec and converting the result to a DataFrame.\n",
    "\n",
    "words_df = pd.DataFrame(vectorizer.transform(clean_hail_data).todense(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "words_df.fillna(0)\n",
    "\n",
    "print (words_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e493015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the final hail_tweet_text dataframe\n",
    "\n",
    "hail_tweet_text = pd.concat([hail_data_final, words_df], axis=1)\n",
    "hail_tweet_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91ba6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print top-20 words found in full dataset:\n",
    "\n",
    "hail_tweet_text.ix[:,26:-1].sum().nlargest(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e9f17a",
   "metadata": {},
   "source": [
    "#### Why is the term \"Gemini\" so prevalent?\n",
    "The term gemini is appearing an unusually high number of times...  Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42978c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out words relating to 'Gemini'\n",
    "\n",
    "hail_tweet_text_gemini = hail_tweet_text[hail_tweet_text['gemini']==1]\n",
    "hail_tweet_text_gemini.ix[:,26:-1].sum().nlargest(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d03d49d",
   "metadata": {},
   "source": [
    "**Conclusion:** After investigation, it appears this term is appearing with 3 key words (storm, talking and today). After investigation it appears it is related to horoscopes although the source is not Twittascope, which was identified in Capstone part-2. This is clearly unreliable data and would distort the use of the word storm in modelling (which could be a critical predictor of hail), so we will remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3570b98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Damage frequency plot (count vs time) for both events\n",
    "\n",
    "sns.factorplot(x=\"posted_hour\", y=\"damage\", hue=\"Cat_ID\", data=hail_tweet_text,\n",
    "                   size=12,aspect=2, kind=\"bar\",ci=None)\n",
    "plt.title('Hail Tweets: \"Damage\" frequency over time for Both Events',size=35)\n",
    "plt.xlabel('Hour of Day (GMT, +9 hours for Aus local time)')\n",
    "plt.ylabel('Tweet Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a66c50",
   "metadata": {},
   "source": [
    "---\n",
    "#### Key Words - Event Comparison\n",
    "\n",
    "_Analysing the key words that surround the term \"hail\" for both of the events._\n",
    "\n",
    "In this analysis I will only look at the twitter data that was posted during the actual hours of the hail storm, from Hour-6 through to Hour-10 (this is a conservative range)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf830c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data into actual storm hours\n",
    "\n",
    "hail_tt_storm_hours = hail_tweet_text[(hail_tweet_text['posted_hour'] >=6) &\\\n",
    "                                        (hail_tweet_text['posted_hour'] <=10) &\\\n",
    "                                        (hail_tweet_text['hail'] > 0)]\n",
    "\n",
    "print (\"Storm hours data shape:\",hail_tt_storm_hours.shape)\n",
    "\n",
    "print (hail_tt_storm_hours.ix[:,26:-1].sum().nlargest(21))\n",
    "\n",
    "# Create data of top 20 terms (exlcluding hail)\n",
    "\n",
    "df = hail_tt_storm_hours.groupby(['Cat_ID'])[['storm','rain','weather'\n",
    "                                        ,'hit','severe','golf','ball','lightning','thunder','damage','storms'\n",
    "                                        ,'crazy','snow','australia','sized','news']].sum()\n",
    "df = df.transpose()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6714de74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a comparison chart for the key words used in conjuction with the word 'hail' during of the hailstorm hours:\n",
    "\n",
    "sns.set(font_scale=1)\n",
    "fig = plt.figure(figsize=(16,12)) # Create matplotlib figure\n",
    "\n",
    "ax = fig.add_subplot(111) # Create matplotlib axes\n",
    "ax2 = ax.twiny() # Create another axes that shares the same x-axis as ax.\n",
    "\n",
    "df[144].plot(kind='barh', color='red', ax=ax, fontsize=12,legend=True, position=1)\n",
    "df[154].plot(kind='barh', color='blue', ax=ax2, fontsize=12, legend=True,position=0)\n",
    "\n",
    "ax.set_ylabel('Hail key term',size=12)\n",
    "ax.set_xlabel('Term count over hail period',size=12)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd013530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation plot of key words used with hail\n",
    "\n",
    "hail_tt_storm_hours_terms = hail_tt_storm_hours[['storm','rain','weather','hit','severe','golf','ball','lightning','thunder','damage','storms','crazy','snow','australia','sized','news']]\n",
    "\n",
    "hail_tt_storm_hours_corr = hail_tt_storm_hours_terms.corr()\n",
    "\n",
    "f, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "ax = sns.heatmap(data=hail_tt_storm_hours_corr, square=True, ax=ax)\n",
    "plt.title('\"Hail\" Tweets: Correlation plot of key words',size=16)\n",
    "ax.set_xticklabels(ax.xaxis.get_ticklabels(), fontsize=14, rotation=45)\n",
    "ax.set_yticklabels(ax.yaxis.get_ticklabels(), fontsize=14, rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045c1d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally I will experiment with another visualisation method - a wordcloud.\n",
    "# This is a package created by amueller on Github: https://github.com/amueller/word_cloud\n",
    "\n",
    "from os import path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "d = path.dirname('./images/')\n",
    "\n",
    "# read the mask image\n",
    "cloud_mask = np.array(Image.open(path.join(d, \"hail.png\")))\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "text = \"\".join(hail_tt_storm_hours['tweet_words'])\n",
    "\n",
    "wc = WordCloud(width=800, height=400,scale=16, background_color=\"white\", max_words=1000, mask=cloud_mask,collocations=False).generate(text)\n",
    "\n",
    "# show\n",
    "plt.figure(figsize=(18, 16))\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26e651d",
   "metadata": {},
   "source": [
    "#### Key hail words insight: \n",
    "\n",
    "Storm is the most prevalent word but the two interesting terms used more in the Brisbane event (Cat_ID 144) were 'golf' and 'ball'. As mentioned in previous capstone deliveries, this term is a key indicator of damaging hail. We know already that the Brisbane event was much costlier from an insurance loss perspective, the prevalence of this term in this event is an **excellent indicator of severity!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd689c70",
   "metadata": {},
   "source": [
    "### Geographic Analysis of Tweet Data\n",
    "\n",
    "This analysis looks at the geographical distribution of the twitter data over the two 24 hour time periods. \n",
    "\n",
    "Some key limitations of this analysis include:\n",
    "- The coordinate data was minimal in the original data as it relies on twitter users activating location services on their twitter apps to store coordinate data. \n",
    "- Ideally the actual tweet location would be used from the location services, however as a proxy, the registered location of the user was geocoded externally (using Google's Geocoder API) and appended to the twitter data.\n",
    "- Only 2234 records successfully geocoded (1.7% of total records).\n",
    "- Various geocoding resolutions resulted, ranging from city/suburb to country level, so the accuracy of the location is inconsistent.\n",
    "\n",
    "Despite these limitations, plotting the (known) registered locations of twitter users gives a reasonable geospatial representation of the global tweets over the two sample days.\n",
    "\n",
    "This analysis uses Folium to create an interactive map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b43b302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of tweets with coordinates (and hail only)\n",
    "hail_tweet_text_geo = pd.read_csv(\"./hail_tweet_text_geo.csv\")\n",
    "print (len(hail_tweet_text_geo))\n",
    "hail_tweet_text_geo = hail_tweet_text_geo[hail_tweet_text_geo['longitude']>=-180]\n",
    "# hail_tweet_text_geo = hail_tweet_text_geo[hail_tweet_text_geo['hail']>=0]\n",
    "                                                                         \n",
    "latitude = hail_tweet_text_geo['latitude']\n",
    "longitude = hail_tweet_text_geo['longitude']\n",
    "print (len(latitude))\n",
    "\n",
    "import folium\n",
    "from folium import plugins\n",
    "import pandas as pd\n",
    "\n",
    "WORLD_COORDINATES = (0, 0)\n",
    "\n",
    "# create empty map zoomed in on WORLD\n",
    "\n",
    "map1 = folium.Map(location=WORLD_COORDINATES, zoom_start=2)\n",
    "\n",
    "map1.add_child(plugins.HeatMap(zip(latitude, longitude), radius = 10))\n",
    "\n",
    "map1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401d39e5",
   "metadata": {},
   "source": [
    "#### Heat Map of All Tweets (over both 24-hour date samples):\n",
    "\n",
    "![map all tweets](images/map_all_tweets.png)\n",
    "\n",
    "#### Heat Map of Hail Tweets\n",
    "\n",
    "![map hail tweets](images/map_hail_tweets.png)\n",
    "\n",
    "_Note: Using geographic analysis, we can see there is a large concentration in tweets in Texas, US. After investigating this cluster, we can conclude another hail event took place in San Antonio on the 25th April 2015. This event did not seem to be significant and little evidence of this location is picked up in the actual tweet text data._\n",
    "\n",
    "#### Heat Map of Earthquake Tweets\n",
    "\n",
    "![map eq tweets](images/map_eq_tweets.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6849072",
   "metadata": {},
   "source": [
    "## 2.iii. LDA - Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05296b89",
   "metadata": {},
   "source": [
    "I will now investigate the different groupings of words within the dataset. Latent Dirichlet Allocation will help me discover the hidden semantic clusters in the twitter data within subsets relating to different criteria.\n",
    "\n",
    "The groups I will investigate include:\n",
    "      - The Full Dataset\n",
    "      - Brisbane Hail Data (Cat_ID = 144)\n",
    "      - Brisbane Hail Data (Cat_ID = 144) in the known hail storm timeframe (6 - 10 hours)\n",
    "      - Sydney Hail Data (Cat_ID = 154)\n",
    "      - Sydney Hail Data (Cat_ID = 154) in the known hail storm timeframe (6 - 10 hours)\n",
    "  \n",
    "Firstly I will run through each of the LDA processes for the full dataset. Then I will create an LDA function to parse my remaining critera separately.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34a6557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all records with the term gemini:\n",
    "\n",
    "hail_tweet_text = hail_tweet_text[hail_tweet_text['gemini']==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55174a2d",
   "metadata": {},
   "source": [
    "### 2.ii. Bag-of-Words Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9873da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print top-20 word occurrences for full dataset:\n",
    "\n",
    "hail_tweet_text.ix[:,26:-1].sum().nlargest(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88366148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation plot of top-20 words from full dataset:\n",
    "\n",
    "hail_tweet_text_top20 = hail_tweet_text[['storm' ,'damage','thunder','lightning','flood','nepal',\n",
    "                                         'hail','earthquake','rain','brisbane','pm','new','quake','hits',\n",
    "                                         'big','warning' ,'news','snow','weather','game','posted_hour']]\n",
    "\n",
    "corr_tt20 = hail_tweet_text_top20.corr()\n",
    "\n",
    "f, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "ax = sns.heatmap(data=corr_tt20, square=True, ax=ax)\n",
    "plt.title('\"Hail\" Tweets: Correlation plot of key words',size=16)\n",
    "ax.set_xticklabels(ax.xaxis.get_ticklabels(), fontsize=14, rotation=45)\n",
    "ax.set_yticklabels(ax.yaxis.get_ticklabels(), fontsize=14, rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fe32b9",
   "metadata": {},
   "source": [
    "#### Word Correlation Discussion: _Earthquake!?_\n",
    "\n",
    "The most significant correlation found is with an interesting new set of words introducued with the secondary dataset on 25-April-2015...\n",
    "\n",
    "The highest correlated words are: **quake, hits, big and Nepal...** We can now deduce that the twitter data also picked up another major natural catastrophe on this date: the Nepal Earthquake!\n",
    "\n",
    "The additional discovery of the earthquake event provides validation of the searching method, showing that this analysis can not only predict hail but also other natural disasters. This increases the scope of the project as well as the potential applications.\n",
    "\n",
    "_I will now analyse the word counts in greater detail by investigating the distribution of word counts..._\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### _Word Count Distributions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c50cfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the count distrubtion of all words in the Twitter data, then filter to greater levels.\n",
    "\n",
    "hist_counts = pd.Series(words_df.ix[:,26:-1].sum(),index=words_df.columns)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(16,8))\n",
    "    \n",
    "hist_counts.plot(kind=\"hist\", bins=20, ax=axes[0], title=\"Histogram - All\",color='green')\n",
    "    \n",
    "# Filtering all words with a count > 1,000\n",
    "hist_counts[hist_counts > 1000].plot(kind=\"hist\", bins=20, ax=axes[1], title=\"Histogram - Counts > 1,000\",color='orange')\n",
    "    \n",
    "# Filtering all words with a count > 10,000\n",
    "hist_counts[hist_counts > 10000].plot(kind=\"hist\", bins=20, ax=axes[2], title=\"Histogram - Counts > 10,000\",color='red')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca443e3",
   "metadata": {},
   "source": [
    "These histograms indicate a strong positive skew of word counts, as expected. When we filter counts above 1,000 and 10,000 we observe groupings in greater detail between 10,000 and 30,000, which implies some words may have been tweeted together often.\n",
    "\n",
    "Our key investiagtion is into hail, so now I will look specifically at this term and how it differs between datasets...\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72914f05",
   "metadata": {},
   "source": [
    "#### Hail Frequency Distribution Over Time\n",
    "\n",
    "Remember, the two datasets used are from two 24 hour periods where know hail events occurred...\n",
    "\n",
    "- Cat_ID 144 = Brisbane Hail on 27th Nov 2014\n",
    "- Cat_ID 154 = Sydney Hail on 25th April 2015\n",
    "\n",
    "I will now compare the key words in the twitter text for both events through each day's duration. First of all, I will review the key word **'hail'**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a690814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency plot of the use of the term 'hail' over time:\n",
    "\n",
    "sns.set(font_scale=2.5)\n",
    "sns.factorplot(x=\"posted_hour\", y=\"hail\", hue=\"Cat_ID\", data=hail_tweet_text,\n",
    "                   size=12,aspect=2, kind=\"bar\",palette=\"bright\",ci=None)\n",
    "plt.title('Hail Tweets: \"Hail\" frequency over time for Both Events',size=35)\n",
    "plt.xlabel('Hour of Day (GMT, +9 hours for Aus local time)')\n",
    "plt.ylabel('Tweet Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628cd5e5",
   "metadata": {},
   "source": [
    "Key insights:\n",
    "1. Significant peaks at the same same 6-10 hours - this is in line with the known hail event activity time during the two days:\n",
    "  - Brisbane Hail (144) on 27th Nov 14 occurred after 3pm\n",
    "  - Sydney Hail on 25th April 15 occurred at a similar time. _Hail events are known to impact the East Coast of Australia later in the day in the summer months_\n",
    "- Brisbane event: what occurring at 16/17 hours? First I will check another key term..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e50c52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot to check the occurrence of 'hailstorm' for the two events:\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.factorplot(x=\"posted_hour\", y=\"hailstorm\", hue=\"Cat_ID\", data=hail_tweet_text,\n",
    "                   size=12,aspect=2, kind=\"bar\",palette=\"bright\",ci=None)\n",
    "plt.title('Hail Tweets: \"hailstorm\" frequency over time for Both Events',size=35)\n",
    "plt.xlabel('Hour of Day (GMT, +9 hours for Aus local time)')\n",
    "plt.ylabel('Tweet Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d57bd0b",
   "metadata": {},
   "source": [
    "**Comment:** The 'hailstorm' frequency does not provide any further explanation... What about other words used during this hour?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd26308b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, matutils\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# Setup vectorisation\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=stops,max_features = 1000)\n",
    "X = vectorizer.fit_transform(hail_tweet_text['tweet_words'])\n",
    "\n",
    "# Count the terms in the vectorised vocabulary\n",
    "\n",
    "from collections import Counter\n",
    "dict(Counter(vectorizer.vocabulary_).most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb26900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe of words\n",
    "\n",
    "docs = pd.DataFrame(X.todense(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "#Setup dictionary for gensim\n",
    "\n",
    "vocab = {v: k for k, v in vectorizer.vocabulary_.items()}\n",
    "\n",
    "dict(Counter(vocab).most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a8633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the frequency of words\n",
    "\n",
    "frequency = defaultdict(int)\n",
    "\n",
    "for text in hail_tweet_text['tweet_words']:\n",
    "    for token in text.split():\n",
    "        frequency[token] += 1\n",
    "        \n",
    "dict(Counter(frequency).most_common(5))\n",
    "\n",
    "# Remove single words and stopwords\n",
    "\n",
    "texts = [[token for token in text.split() if frequency[token] > 1 and token not in stops]\n",
    "          for text in hail_tweet_text['tweet_words']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c36147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create gensim dictionary object\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "dict(Counter(dictionary).most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f10c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create corpus matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpus[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4365ed53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform LDA modelling\n",
    "\n",
    "lda = models.LdaModel(\n",
    "    matutils.Sparse2Corpus(X, documents_columns=False),\n",
    "    num_topics  =  3, # found to be optimal\n",
    "    passes      =  5, # passes reduced due to runtime\n",
    "    id2word     =  vocab)\n",
    "\n",
    "# Detection of opportunity:\n",
    "lda.print_topics(num_topics=3, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af1cf29",
   "metadata": {},
   "source": [
    "#### Initial Results:\n",
    "\n",
    "For the initial topic modelling I selected three grouping of 10 words. Generally the words collected effectively group into known events during the two selected time periods:\n",
    "\n",
    "- Group 0: **Storm** - A general clustering of terms related to storms: lightning, thunder, time, day, today, etc.\n",
    "- Group 1: **The Nepal Earthquake** - As observed in the words EDA, the Nepal Earthquake is another major catastrophe that has been identified in the 25th April data. Here we observe _injuries, magnitude, causing, hits_ and other key words, however some other meterological terms slip in too.\n",
    "- Group 2: **The [Sydney] hail event** - location information, damage, storm and it's characteristics clearly identify this group.\n",
    "\n",
    "These topics classify the data well and if smaller groups of words were used, the topics would likely be more definitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aa52b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a correlation plot function to review our topic modelling groups:\n",
    "\n",
    "def corr_plot(df):\n",
    "    pref_corr = df.corr()\n",
    "    mask = np.zeros_like(pref_corr, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8,7))\n",
    "\n",
    "    ax = sns.heatmap(pref_corr, mask=mask)\n",
    "\n",
    "    ax.set_xticklabels(ax.xaxis.get_ticklabels(), fontsize=12, rotation=45)\n",
    "    ax.set_yticklabels(ax.yaxis.get_ticklabels(), fontsize=12, rotation=0)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168b6b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note this group (and subsequent groups) may differ to notebook results because of variation in running the model\n",
    "\n",
    "hail_group = hail_tweet_text[['storm','flood','hail','rain','sydneystorm',\n",
    "                                          'sydney','pm','severe','weather','warning']]\n",
    "corr_plot(hail_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fb691c",
   "metadata": {},
   "source": [
    "**Discussion:** \n",
    "\n",
    "Of the terms collected in the \"Sydney Hail\" group, very few have any significant correlations with each other. 'Sydney' - 'hail', and 'pm' - 'warning', are the only two significant correlations.\n",
    "\n",
    "I will now analyse groups to see if hail storm topics become more pronounced..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b872da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a topic modelling function to use for all groups I want to investigate\n",
    "\n",
    "def topic_modelling(twitter_data):\n",
    "    \n",
    "    vectorizer = CountVectorizer(stop_words=stops,max_features = 1000)\n",
    "    X = vectorizer.fit_transform(twitter_data)\n",
    "    \n",
    "    vectorizer.get_feature_names()\n",
    "    \n",
    "    docs = pd.DataFrame(X.todense(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "    vocab = {v: k for k, v in vectorizer.vocabulary_.items()}\n",
    "    \n",
    "    frequency = defaultdict(int)\n",
    "\n",
    "    for text in twitter_data:\n",
    "        for token in text.split():\n",
    "            frequency[token] += 1\n",
    "        \n",
    "    texts = [[token for token in text.split() if frequency[token] > 1 and token not in stops]\n",
    "          for text in twitter_data]\n",
    "\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    \n",
    "    lda = models.LdaModel(\n",
    "    matutils.Sparse2Corpus(X, documents_columns=False),\n",
    "    num_topics  =  5,\n",
    "    passes      =  5,\n",
    "    id2word     =  vocab)\n",
    "\n",
    "    return lda.print_topics(num_topics=5, num_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fee805",
   "metadata": {},
   "source": [
    "### Brisbane Hail Topic Modelling\n",
    "\n",
    "#### A. Full Day Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857b2902",
   "metadata": {},
   "outputs": [],
   "source": [
    "hail_tweet_text_bris = hail_tweet_text[hail_tweet_text['Cat_ID']==144]\n",
    "print (hail_tweet_text_bris.shape)\n",
    "\n",
    "topic_modelling(hail_tweet_text_bris['tweet_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c28ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation plot:\n",
    "\n",
    "bris_group1 = hail_tweet_text_bris[['storm','power','damage','clean','hail',]]\n",
    "corr_plot(bris_group1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea35785e",
   "metadata": {},
   "source": [
    "#### B. Brisbane Hail Storm Hours Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8982e6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brisbane Event data (27th Nov 15); Known hours of hailstorm (Hour 6 to 10)\n",
    "\n",
    "hail_tweet_text_bris_storm = hail_tweet_text_bris[(hail_tweet_text_bris['posted_hour'] >=6) &\\\n",
    "                                            (hail_tweet_text_bris['posted_hour'] <=10)]\n",
    "\n",
    "topic_modelling(hail_tweet_text_bris_storm['tweet_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e8228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bris_group2 = hail_tweet_text_bris_storm[['damage','lightning','storm','flood','iphone']]\n",
    "corr_plot(bris_group2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fabd9ed",
   "metadata": {},
   "source": [
    "### Sydney Hail Topic Modelling\n",
    "\n",
    "#### A. Full Day Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c4892e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hail_tweet_text_syd = hail_tweet_text[hail_tweet_text['Cat_ID']==154]\n",
    "hail_tweet_text_syd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616aa77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sydney Hail data (25th April 15)\n",
    "\n",
    "topic_modelling(hail_tweet_text_syd['tweet_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f59bd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "syd_group1 = hail_tweet_text_syd[['storm','hail','sydneystorm','thunder','lightning']]\n",
    "corr_plot(syd_group1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16e86e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sydney Hail data (25th April 15); Known hours of hailstorm (Hour 6 to 10)\n",
    "\n",
    "hail_tweet_text_syd_storm = hail_tweet_text_syd[(hail_tweet_text_syd['posted_hour'] >=6) &\\\n",
    "                                            (hail_tweet_text_syd['posted_hour'] <=10)]\n",
    "\n",
    "topic_modelling(hail_tweet_text_syd_storm['tweet_words'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cede06",
   "metadata": {},
   "source": [
    "**NRL \"purplepride\"** is a hashtag for the NRL team Melbourne Storm (they lost Manly Sea Eagles on 25th April 2015!) They did not _win_ and were not _heroes_, but they did play a _game_ - the words are typical to sport sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4e86ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "syd_group2 = hail_tweet_text_syd_storm[['storm','damage','flood','time','warning']]\n",
    "corr_plot(syd_group2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1a5a02",
   "metadata": {},
   "source": [
    "**Insights of Grouped Topic Modelling:**\n",
    "\n",
    "The topic modelling groups (the two hail events and their storm durations) improve the recognition of terms likely to be tweeted during hail events (e.g.: meteorology and damage)\n",
    "No major difference in types of words produced when comparing the Brisbane Hail to the Sydney Hail event, apart from a few terms relating to 'damage' and 'power' in the Brisbane event, and the correlation plots show little improvement between variables than previously observed.\n",
    "Additional popular events were picked up through this process, including a basketball game between the Indiana Spurs and San Antonia Pacers and Thanksgiving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22db0ea9",
   "metadata": {},
   "source": [
    "## 2.iv. Sentiment Analysis:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd89c5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using textblob package to perform sentiment scoring\n",
    "\n",
    "from textblob import TextBlob, Word\n",
    "\n",
    "def detect_sentiment(text):\n",
    "    return TextBlob(text).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe712d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new DataFrame column for sentiment\n",
    "hail_tweet_text['sentiment'] = hail_tweet_text.Text.apply(detect_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff79eff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of sentiment distribtuion for all, hail and earthquake tweets\n",
    "\n",
    "hail_tweets = hail_tweet_text.loc[hail_tweet_text['hail'] >= 1]\n",
    "eq_tweets = hail_tweet_text.loc[hail_tweet_text['earthquake'] >= 1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,10))\n",
    "sns.distplot(hail_tweet_text['sentiment'],hist=False,kde_kws={\"lw\": 1.5, \"alpha\": 0.6, \"label\": \"all data\",\"color\": \"grey\"})\n",
    "sns.distplot(hail_tweets['sentiment'],hist=False,kde_kws={\"lw\": 2, \"alpha\": 0.8, \"label\": \"hail tweets\",\"color\": \"b\"})\n",
    "sns.distplot(eq_tweets['sentiment'],hist=False,kde_kws={\"lw\": 1.5, \"alpha\": 0.8, \"label\": \"earthquake tweets\",\"color\": \"r\"})\n",
    "ax.set_xlabel('sentiment score', fontsize=16)\n",
    "ax.set_ylabel('frequency', fontsize=16)\n",
    "plt.title(\"Tweet Sentiment Score by Event Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b41c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of log-frequency sentiment distributions of all, earthquake and hail tweets\n",
    "\n",
    "hail_tweets = hail_tweet_text.loc[hail_tweet_text['hail'] >= 1]\n",
    "eq_tweets = hail_tweet_text.loc[hail_tweet_text['earthquake'] >= 1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,10))\n",
    "sns.distplot(hail_tweet_text['sentiment'],hist=False,kde_kws={\"lw\": 4, \"alpha\": 0.3,\"label\": \"all data\",\"color\": \"grey\"})\n",
    "sns.distplot(hail_tweets['sentiment'],hist=False,kde_kws={\"lw\": 1.5, \"alpha\": 0.7, \"label\": \"hail tweets\", \"color\": \"b\"})\n",
    "sns.distplot(eq_tweets['sentiment'],hist=False,kde_kws={\"lw\": 1.5, \"alpha\": 0.7, \"label\": \"earthquake tweets\",\"color\": \"r\"})\n",
    "ax.set_xlabel('sentiment score', fontsize=16)\n",
    "ax.set_ylabel('frequency', fontsize=16)\n",
    "plt.title(\"Tweet Sentiment Score by Event Type (Log Frequency)\")\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c640047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud of negative sentiment scores for hail\n",
    "\n",
    "# creating negative sentiment tweet variable for hail tweets\n",
    "hail_tweets_neg_sent = hail_tweets[hail_tweets['sentiment']<=-0.8]\n",
    "\n",
    "d = path.dirname('./images/')\n",
    "\n",
    "# read the mask image\n",
    "cloud_mask = np.array(Image.open(path.join(d, \"cloud.jpg\")))\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "text = \"\".join(hail_tweets_neg_sent['tweet_words'])\n",
    "\n",
    "wc = WordCloud(width=800, height=400,scale=16, background_color=\"white\", max_words=1000, \n",
    "               collocations=False).generate(text)\n",
    "# generate word cloud\n",
    "\n",
    "# display word cloud\n",
    "plt.figure(figsize=(18, 16))\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b604cabe",
   "metadata": {},
   "source": [
    "**Sentiment Analysis Discussion**\n",
    "\n",
    "The sentiment analysis suggests that at more negative scores, the relative frequency of earthquake and (lesser so) hail tweets are greater than all-tweets combined. The inverse is seen at positive sentiment scores. The above wordcloud provides insight into some of the colourful language used in the negative hail tweets.\n",
    "\n",
    "Interestingly there appears to be a higher frequency of positive tweets for the earthquake event than for the hail events.this could be people tweeting to record their safety and generally, in very severe natural catastrophes, people tend to tweet very emotive language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc7fb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigation for top 20 words used at hour 17 within the Brisbane/Nov 2014 hail data:\n",
    "\n",
    "hail_tweet_text_bris = hail_tweet_text[hail_tweet_text['Cat_ID']==144]\n",
    "\n",
    "hail_tweet_text_bris17 = hail_tweet_text[hail_tweet_text['posted_hour']==17]\n",
    "print (hail_tweet_text_bris17.ix[:,26:-1].sum().nlargest(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b9856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of source type at hour 17 within the Brisbane/Nov 2014 hail data:\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "hail_tweet_text_bris17.source.value_counts().nlargest(10).plot(kind='bar',color='red')\n",
    "plt.tick_params(labelsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a13f8e4",
   "metadata": {},
   "source": [
    " #### Brisbane Hail - Hour 17 Discussion?\n",
    "\n",
    "Inconclusive results:\n",
    "- Nothing conclusive to indicate the additional signature at hour 17 - nothing is observed from other words and source information...\n",
    "- It is unclear from this analysis why there is an additional signature later in the day. Possibly international recognition of the event was causing the term to be tweeted?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41c44d6",
   "metadata": {},
   "source": [
    "### \"Damage\" frequency plot for Both Events:"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
