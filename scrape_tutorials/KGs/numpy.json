{
    "NumPy Features": {
        "Linear algebra on n-dimensional arrays": [
            {
                "Prerequisites": [
                    [
                        "Before reading this tutorial, you should know a bit of Python. If you would like to refresh your memory, take a look at the .",
                        "markdown"
                    ],
                    [
                        "If you want to be able to run the examples in this tutorial, you should also have  and  installed on your computer.",
                        "markdown"
                    ]
                ]
            },
            {
                "Learner profile": [
                    [
                        "This tutorial is for people who have a basic understanding of linear algebra and arrays in NumPy and want to understand how n-dimensional (\\(n&gt;=2\\)) arrays are represented and can be manipulated. In particular, if you don\u2019t know how to apply common functions to n-dimensional arrays (without using for-loops), or if you want to understand axis and shape properties for n-dimensional arrays, this tutorial might be of help.",
                        "markdown"
                    ]
                ]
            },
            {
                "Learning Objectives": [
                    [
                        "After this tutorial, you should be able to:",
                        "markdown"
                    ],
                    [
                        "Understand the difference between one-, two- and n-dimensional arrays in NumPy;",
                        "markdown"
                    ],
                    [
                        "Understand how to apply some linear algebra operations to n-dimensional arrays without using for-loops;",
                        "markdown"
                    ],
                    [
                        "Understand axis and shape properties for n-dimensional arrays.",
                        "markdown"
                    ]
                ]
            },
            {
                "Content": [
                    [
                        "In this tutorial, we will use a  from linear algebra, the Singular Value Decomposition, to generate a compressed approximation of an image. We\u2019ll use the face image from the  module:",
                        "markdown"
                    ],
                    [
                        "from scipy import misc\n\nimg = misc.face()",
                        "code"
                    ],
                    [
                        "/tmp/ipykernel_399/2202046956.py:3: DeprecationWarning: scipy.misc.face has been deprecated in SciPy v1.10.0; and will be completely removed in SciPy v1.12.0. Dataset methods have moved into the scipy.datasets module. Use scipy.datasets.face instead.\n  img = misc.face()",
                        "code"
                    ],
                    [
                        "<strong>Note</strong>: If you prefer, you can use your own image as you work through this tutorial. In order to transform your image into a NumPy array that can be manipulated, you can use the imread function from the  submodule. Alternatively, you can use the  function from the imageio library. Be aware that if you use your own image, you\u2019ll likely need to adapt the steps below. For more information on how images are treated when converted to NumPy arrays, see  from the scikit-image documentation.",
                        "markdown"
                    ],
                    [
                        "Now, img is a NumPy array, as we can see when using the type function:",
                        "markdown"
                    ],
                    [
                        "type(img)",
                        "code"
                    ],
                    [
                        "numpy.ndarray",
                        "code"
                    ],
                    [
                        "We can see the image using the  function &amp; the special iPython command, %matplotlib inline to display plots inline:",
                        "markdown"
                    ],
                    [
                        "import matplotlib.pyplot as plt\n\n%matplotlib inline",
                        "code"
                    ],
                    [
                        "plt.imshow(img)\nplt.show()\n\n\n\n\n<img alt=\"../_images/0802995ab45723028c194f181d4e07f2e90f6c49c7f49d85ee3de0f218782122.png\" src=\"../_images/0802995ab45723028c194f181d4e07f2e90f6c49c7f49d85ee3de0f218782122.png\"/>",
                        "code"
                    ],
                    {
                        "Shape, axis and array properties": [
                            [
                                "Note that, in linear algebra, the dimension of a vector refers to the number of entries in an array. In NumPy, it instead defines the number of axes. For example, a 1D array is a vector such as [1, 2, 3], a 2D array is a matrix, and so forth.",
                                "markdown"
                            ],
                            [
                                "First, let\u2019s check for the shape of the data in our array. Since this image is two-dimensional (the pixels in the image form a rectangle), we might expect a two-dimensional array to represent it (a matrix). However, using the shape property of this NumPy array gives us a different result:",
                                "markdown"
                            ],
                            [
                                "img.shape",
                                "code"
                            ],
                            [
                                "(768, 1024, 3)",
                                "code"
                            ],
                            [
                                "The output is a  with three elements, which means that this is a three-dimensional array. In fact, since this is a color image, and we have used the imread function to read it, the data is organized in three 2D arrays, representing color channels (in this case, red, green and blue - RGB). You can see this by looking at the shape above: it indicates that we have an array of 3 matrices, each having shape 768x1024.",
                                "markdown"
                            ],
                            [
                                "Furthermore, using the ndim property of this array, we can see that",
                                "markdown"
                            ],
                            [
                                "img.ndim",
                                "code"
                            ],
                            [
                                "3",
                                "code"
                            ],
                            [
                                "NumPy refers to each dimension as an <em>axis</em>. Because of how imread works, the <em>first index in the 3rd axis</em> is the red pixel data for our image. We can access this by using the syntax",
                                "markdown"
                            ],
                            [
                                "img[:, :, 0]",
                                "code"
                            ],
                            [
                                "array([[121, 138, 153, ..., 119, 131, 139],\n       [ 89, 110, 130, ..., 118, 134, 146],\n       [ 73,  94, 115, ..., 117, 133, 144],\n       ...,\n       [ 87,  94, 107, ..., 120, 119, 119],\n       [ 85,  95, 112, ..., 121, 120, 120],\n       [ 85,  97, 111, ..., 120, 119, 118]], dtype=uint8)",
                                "code"
                            ],
                            [
                                "From the output above, we can see that every value in img[:, :, 0] is an integer value between 0 and 255, representing the level of red in each corresponding image pixel (keep in mind that this might be different if you\nuse your own image instead of ).",
                                "markdown"
                            ],
                            [
                                "As expected, this is a 768x1024 matrix:",
                                "markdown"
                            ],
                            [
                                "img[:, :, 0].shape",
                                "code"
                            ],
                            [
                                "(768, 1024)",
                                "code"
                            ],
                            [
                                "Since we are going to perform linear algebra operations on this data, it might be more interesting to have real numbers between 0 and 1 in each entry of the matrices to represent the RGB values. We can do that by setting",
                                "markdown"
                            ],
                            [
                                "img_array = img / 255",
                                "code"
                            ],
                            [
                                "This operation, dividing an array by a scalar, works because of NumPy\u2019s . (Note that in real-world applications, it would be better to use, for example, the  utility function from scikit-image).",
                                "markdown"
                            ],
                            [
                                "You can check that the above works by doing some tests; for example, inquiring\nabout maximum and minimum values for this array:",
                                "markdown"
                            ],
                            [
                                "img_array.max(), img_array.min()",
                                "code"
                            ],
                            [
                                "(1.0, 0.0)",
                                "code"
                            ],
                            [
                                "or checking the type of data in the array:",
                                "markdown"
                            ],
                            [
                                "img_array.dtype",
                                "code"
                            ],
                            [
                                "dtype('float64')",
                                "code"
                            ],
                            [
                                "Note that we can assign each color channel to a separate matrix using the slice syntax:",
                                "markdown"
                            ],
                            [
                                "red_array = img_array[:, :, 0]\ngreen_array = img_array[:, :, 1]\nblue_array = img_array[:, :, 2]",
                                "code"
                            ]
                        ]
                    },
                    {
                        "Operations on an axis": [
                            [
                                "It is possible to use methods from linear algebra to approximate an existing set of data. Here, we will use the  to try to rebuild an image that uses less singular value information than the original one, while still retaining some of its features.",
                                "markdown"
                            ],
                            [
                                "<strong>Note</strong>: We will use NumPy\u2019s linear algebra module, , to perform the operations in this tutorial. Most of the linear algebra functions in this module can also be found in , and users are encouraged to use the  module for real-world applications. However, some functions in the  module, such as the SVD function, only support 2D arrays. For more information on this, check the .",
                                "markdown"
                            ],
                            [
                                "To proceed, import the linear algebra submodule from NumPy:",
                                "markdown"
                            ],
                            [
                                "from numpy import linalg",
                                "code"
                            ],
                            [
                                "In order to extract information from a given matrix, we can use the SVD to obtain 3 arrays which can be multiplied to obtain the original matrix. From the theory of linear algebra, given a matrix \\(A\\), the following product can be computed:\n\n\\[U \\Sigma V^T = A\\]",
                                "markdown"
                            ],
                            [
                                "where \\(U\\) and \\(V^T\\) are square and \\(\\Sigma\\) is the same size as \\(A\\). \\(\\Sigma\\) is a diagonal matrix and contains the  of \\(A\\), organized from largest to smallest. These values are always non-negative and can be used as an indicator of the \u201cimportance\u201d of some features represented by the matrix \\(A\\).",
                                "markdown"
                            ],
                            [
                                "Let\u2019s see how this works in practice with just one matrix first. Note that according to ,\nit is possible to obtain a fairly reasonable grayscale version of our color image if we apply the formula\n\n\\[Y = 0.2126 R + 0.7152 G + 0.0722 B\\]",
                                "markdown"
                            ],
                            [
                                "where \\(Y\\) is the array representing the grayscale image, and \\(R\\), \\(G\\) and \\(B\\) are the red, green and blue channel arrays we had originally. Notice we can use the @ operator (the matrix multiplication operator for NumPy arrays, see ) for this:",
                                "markdown"
                            ],
                            [
                                "img_gray = img_array @ [0.2126, 0.7152, 0.0722]",
                                "code"
                            ],
                            [
                                "Now, img_gray has shape",
                                "markdown"
                            ],
                            [
                                "img_gray.shape",
                                "code"
                            ],
                            [
                                "(768, 1024)",
                                "code"
                            ],
                            [
                                "To see if this makes sense in our image, we should use a colormap from matplotlib corresponding to the color we wish to see in out image (otherwise, matplotlib will default to a colormap that does not correspond to the real data).",
                                "markdown"
                            ],
                            [
                                "In our case, we are approximating the grayscale portion of the image, so we will use the colormap gray:",
                                "markdown"
                            ],
                            [
                                "plt.imshow(img_gray, cmap=\"gray\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/7adf6740f755ce33a18fced959f7be249e3d4f46738c70cfd0eeff87ffe4ff20.png\" src=\"../_images/7adf6740f755ce33a18fced959f7be249e3d4f46738c70cfd0eeff87ffe4ff20.png\"/>",
                                "code"
                            ],
                            [
                                "Now, applying the  function to this matrix, we obtain the following decomposition:",
                                "markdown"
                            ],
                            [
                                "U, s, Vt = linalg.svd(img_gray)",
                                "code"
                            ],
                            [
                                "<strong>Note</strong> If you are using your own image, this command might take a while to run, depending on the size of your image and your hardware. Don\u2019t worry, this is normal! The SVD can be a pretty intensive computation.",
                                "markdown"
                            ],
                            [
                                "Let\u2019s check that this is what we expected:",
                                "markdown"
                            ],
                            [
                                "U.shape, s.shape, Vt.shape",
                                "code"
                            ],
                            [
                                "((768, 768), (768,), (1024, 1024))",
                                "code"
                            ],
                            [
                                "Note that s has a particular shape: it has only one dimension. This means that some linear algebra functions that expect 2d arrays might not work. For example, from the theory, one might expect s and Vt to be\ncompatible for multiplication. However, this is not true as s does not have a second axis. Executing",
                                "markdown"
                            ],
                            [
                                "s @ Vt",
                                "code"
                            ],
                            [
                                "results in a ValueError. This happens because having a one-dimensional array for s, in this case, is much more economic in practice than building a diagonal matrix with the same data. To reconstruct the original matrix, we can rebuild the diagonal matrix \\(\\Sigma\\) with the elements of s in its diagonal and with the appropriate dimensions for multiplying: in our case, \\(\\Sigma\\) should be 768x1024 since U is 768x768 and Vt is 1024x1024. In order to add the singular values to the diagonal of Sigma, we will use the  function from NumPy:",
                                "markdown"
                            ],
                            [
                                "import numpy as np\n\nSigma = np.zeros((U.shape[1], Vt.shape[0]))\nnp.fill_diagonal(Sigma, s)",
                                "code"
                            ],
                            [
                                "Now, we want to check if the reconstructed U @ Sigma @ Vt is close to the original img_gray matrix.",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "Approximation": [
                    [
                        "The  module includes a norm function, which computes the norm of a vector or matrix represented in a NumPy array. For example, from the SVD explanation above, we would expect the norm of the difference between img_gray and the reconstructed SVD product to be small. As expected, you should see something like",
                        "markdown"
                    ],
                    [
                        "linalg.norm(img_gray - U @ Sigma @ Vt)",
                        "code"
                    ],
                    [
                        "1.4383100782204338e-12",
                        "code"
                    ],
                    [
                        "(The actual result of this operation might be different depending on your architecture and linear algebra setup. Regardless, you should see a small number.)",
                        "markdown"
                    ],
                    [
                        "We could also have used the  function to make sure the reconstructed product is, in fact, <em>close</em> to our original matrix (the difference between the two arrays is small):",
                        "markdown"
                    ],
                    [
                        "np.allclose(img_gray, U @ Sigma @ Vt)",
                        "code"
                    ],
                    [
                        "True",
                        "code"
                    ],
                    [
                        "To see if an approximation is reasonable, we can check the values in s:",
                        "markdown"
                    ],
                    [
                        "plt.plot(s)\nplt.show()\n\n\n\n\n<img alt=\"../_images/b3c5a64421a25e9b8f34f2d5e71f750a53e3ef7b513a82271d088e5c12a26308.png\" src=\"../_images/b3c5a64421a25e9b8f34f2d5e71f750a53e3ef7b513a82271d088e5c12a26308.png\"/>",
                        "code"
                    ],
                    [
                        "In the graph, we can see that although we have 768 singular values in s, most of those (after the 150th entry or so) are pretty small. So it might make sense to use only the information related to the first (say, 50) <em>singular values</em> to build a more economical approximation to our image.",
                        "markdown"
                    ],
                    [
                        "The idea is to consider all but the first k singular values in Sigma (which are the same as in s) as zeros, keeping U and Vt intact, and computing the product of these matrices as the approximation.",
                        "markdown"
                    ],
                    [
                        "For example, if we choose",
                        "markdown"
                    ],
                    [
                        "k = 10",
                        "code"
                    ],
                    [
                        "we can build the approximation by doing",
                        "markdown"
                    ],
                    [
                        "approx = U @ Sigma[:, :k] @ Vt[:k, :]",
                        "code"
                    ],
                    [
                        "Note that we had to use only the first k rows of Vt, since all other rows would be multiplied by the zeros corresponding to the singular values we eliminated from this approximation.",
                        "markdown"
                    ],
                    [
                        "plt.imshow(approx, cmap=\"gray\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/d1cf34104d6d0e0bdcfc9bf043db57d8d175cc23af1fb41a44dd536e3d9161df.png\" src=\"../_images/d1cf34104d6d0e0bdcfc9bf043db57d8d175cc23af1fb41a44dd536e3d9161df.png\"/>",
                        "code"
                    ],
                    [
                        "Now, you can go ahead and repeat this experiment with other values of k, and each of your experiments should give you a slightly better (or worse) image depending on the value you choose.",
                        "markdown"
                    ],
                    {
                        "Applying to all colors": [
                            [
                                "Now we want to do the same kind of operation, but to all three colors. Our first instinct might be to repeat the same operation we did above to each color matrix individually. However, NumPy\u2019s <em>broadcasting</em> takes care of this\nfor us.",
                                "markdown"
                            ],
                            [
                                "If our array has more than two dimensions, then the SVD can be applied to all axes at once. However, the linear algebra functions in NumPy expect to see an array of the form (n, M, N), where the first axis n represents the number of MxN matrices in the stack.",
                                "markdown"
                            ],
                            [
                                "In our case,",
                                "markdown"
                            ],
                            [
                                "img_array.shape",
                                "code"
                            ],
                            [
                                "(768, 1024, 3)",
                                "code"
                            ],
                            [
                                "so we need to permutate the axis on this array to get a shape like (3, 768, 1024). Fortunately, the  function can do that for us:",
                                "markdown"
                            ],
                            [
                                "np.transpose(x, axes=(i, j, k))",
                                "code"
                            ],
                            [
                                "indicates that the axis will be reordered such that the final shape of the transposed array will be reordered according to the indices (i, j, k).",
                                "markdown"
                            ],
                            [
                                "Let\u2019s see how this goes for our array:",
                                "markdown"
                            ],
                            [
                                "img_array_transposed = np.transpose(img_array, (2, 0, 1))\nimg_array_transposed.shape",
                                "code"
                            ],
                            [
                                "(3, 768, 1024)",
                                "code"
                            ],
                            [
                                "Now we are ready to apply the SVD:",
                                "markdown"
                            ],
                            [
                                "U, s, Vt = linalg.svd(img_array_transposed)",
                                "code"
                            ],
                            [
                                "Finally, to obtain the full approximated image, we need to reassemble these matrices into the approximation. Now, note that",
                                "markdown"
                            ],
                            [
                                "U.shape, s.shape, Vt.shape",
                                "code"
                            ],
                            [
                                "((3, 768, 768), (3, 768), (3, 1024, 1024))",
                                "code"
                            ],
                            [
                                "To build the final approximation matrix, we must understand how multiplication across different axes works.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "Products with n-dimensional arrays": [
                            [
                                "If you have worked before with only one- or two-dimensional arrays in NumPy, you might use  and  (or the @ operator) interchangeably. However, for n-dimensional arrays, they work in very different ways. For more details, check the documentation on .",
                                "markdown"
                            ],
                            [
                                "Now, to build our approximation, we first need to make sure that our singular values are ready for multiplication, so we build our Sigma matrix similarly to what we did before. The Sigma array must have dimensions (3, 768, 1024). In order to add the singular values to the diagonal of Sigma, we will again use the  function, using each of the 3 rows in s as the diagonal for each of the 3 matrices in Sigma:",
                                "markdown"
                            ],
                            [
                                "Sigma = np.zeros((3, 768, 1024))\nfor j in range(3):\n    np.fill_diagonal(Sigma[j, :, :], s[j, :])",
                                "code"
                            ],
                            [
                                "Now, if we wish to rebuild the full SVD (with no approximation), we can do",
                                "markdown"
                            ],
                            [
                                "reconstructed = U @ Sigma @ Vt",
                                "code"
                            ],
                            [
                                "Note that",
                                "markdown"
                            ],
                            [
                                "reconstructed.shape",
                                "code"
                            ],
                            [
                                "(3, 768, 1024)",
                                "code"
                            ],
                            [
                                "The reconstructed image should be indistinguishable from the original one, except for differences due to floating point errors from the reconstruction. Recall that our original image consisted of floating point values in the range [0., 1.]. The accumulation of floating point error from the reconstruction can result in values slightly outside this original range:",
                                "markdown"
                            ],
                            [
                                "reconstructed.min(), reconstructed.max()",
                                "code"
                            ],
                            [
                                "(-4.870236158804886e-15, 1.0000000000000047)",
                                "code"
                            ],
                            [
                                "Since imshow expects values in the range, we can use clip to excise the floating point error:",
                                "markdown"
                            ],
                            [
                                "reconstructed = np.clip(reconstructed, 0, 1)\nplt.imshow(np.transpose(reconstructed, (1, 2, 0)))\nplt.show()\n\n\n\n\n<img alt=\"../_images/01d5082a5df7d6454455fe9df2a92b95e8eb97f54c36b0b3c73e01d0106c45b5.png\" src=\"../_images/01d5082a5df7d6454455fe9df2a92b95e8eb97f54c36b0b3c73e01d0106c45b5.png\"/>",
                                "code"
                            ],
                            [
                                "In fact, imshow peforms this clipping under-the-hood, so if you skip the first line in the previous code cell, you might see a warning message saying \"Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\"",
                                "markdown"
                            ],
                            [
                                "Now, to do the approximation, we must choose only the first k singular values for each color channel. This can be done using the following syntax:",
                                "markdown"
                            ],
                            [
                                "approx_img = U @ Sigma[..., :k] @ Vt[..., :k, :]",
                                "code"
                            ],
                            [
                                "You can see that we have selected only the first k components of the last axis for Sigma (this means that we have used only the first k columns of each of the three matrices in the stack), and that we have selected only the first k components in the second-to-last axis of Vt (this means we have selected only the first k rows from every matrix in the stack Vt and all columns). If you are unfamiliar with the ellipsis syntax, it is a\nplaceholder for other axes. For more details, see the documentation on .",
                                "markdown"
                            ],
                            [
                                "Now,",
                                "markdown"
                            ],
                            [
                                "approx_img.shape",
                                "code"
                            ],
                            [
                                "(3, 768, 1024)",
                                "code"
                            ],
                            [
                                "which is not the right shape for showing the image. Finally, reordering the axes back to our original shape of (768, 1024, 3), we can see our approximation:",
                                "markdown"
                            ],
                            [
                                "plt.imshow(np.transpose(approx_img, (1, 2, 0)))\nplt.show()",
                                "code"
                            ],
                            [
                                "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n<img alt=\"../_images/f548b24149bd161ccb169bb0651751737d236a8e5bce3a5eb5bc4b63a2cae891.png\" src=\"../_images/f548b24149bd161ccb169bb0651751737d236a8e5bce3a5eb5bc4b63a2cae891.png\"/>",
                                "code"
                            ],
                            [
                                "Even though the image is not as sharp, using a small number of k singular values (compared to the original set of 768 values), we can recover many of the distinguishing features from this image.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "Final words": [
                            [
                                "Of course, this is not the best method to <em>approximate</em> an image. However, there is, in fact, a result in linear algebra that says that the approximation we built above is the best we can get to the original matrix in\nterms of the norm of the difference. For more information, see <em>G. H. Golub and C. F. Van Loan, Matrix Computations, Baltimore, MD, Johns Hopkins University Press, 1985</em>.",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "Further reading": [
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Saving and sharing your NumPy arrays": [
            {
                "What you\u2019ll learn": [
                    [
                        "You\u2019ll save your NumPy arrays as zipped files and human-readable\ncomma-delimited files i.e. *.csv. You will also learn to load both of these\nfile types back into NumPy workspaces.",
                        "markdown"
                    ]
                ]
            },
            {
                "What you\u2019ll do": [
                    [
                        "You\u2019ll learn two ways of saving and reading files\u2013as compressed and as\ntext files\u2013that will serve most of your storage needs in NumPy.",
                        "markdown"
                    ],
                    [
                        "You\u2019ll create two 1D arrays and one 2D array",
                        "markdown"
                    ],
                    [
                        "You\u2019ll save these arrays to files",
                        "markdown"
                    ],
                    [
                        "You\u2019ll remove variables from your workspace",
                        "markdown"
                    ],
                    [
                        "You\u2019ll load the variables from your saved file",
                        "markdown"
                    ],
                    [
                        "You\u2019ll compare zipped binary files to human-readable delimited files",
                        "markdown"
                    ],
                    [
                        "You\u2019ll finish with the skills of saving, loading, and sharing NumPy arrays",
                        "markdown"
                    ]
                ]
            },
            {
                "What you\u2019ll need": [
                    [
                        "NumPy",
                        "markdown"
                    ],
                    [
                        "read-write access to your working directory",
                        "markdown"
                    ],
                    [
                        "Load the necessary functions using the following command.",
                        "markdown"
                    ],
                    [
                        "import numpy as np",
                        "code"
                    ],
                    [
                        "In this tutorial, you will use the following Python, IPython magic, and NumPy functions:",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            },
            {
                "Create your arrays": [
                    [
                        "Now that you have imported the NumPy library, you can make a couple of\narrays; let\u2019s start with two 1D arrays, x and y, where y = x**2.You\nwill assign x to the integers from 0 to 9 using\n.",
                        "markdown"
                    ],
                    [
                        "x = np.arange(10)\ny = x ** 2\nprint(x)\nprint(y)",
                        "code"
                    ],
                    [
                        "[0 1 2 3 4 5 6 7 8 9]\n[ 0  1  4  9 16 25 36 49 64 81]",
                        "code"
                    ]
                ]
            },
            {
                "Save your arrays with NumPy\u2019s": [
                    [
                        "Now you have two arrays in your workspace,",
                        "markdown"
                    ],
                    [
                        "x: [0 1 2 3 4 5 6 7 8 9]",
                        "markdown"
                    ],
                    [
                        "y: [ 0\u00a0 1\u00a0 4\u00a0 9 16 25 36 49 64 81]",
                        "markdown"
                    ],
                    [
                        "The first thing you will do is save them to a file as zipped arrays\nusing\n.\nYou will use two options to label the arrays in the file,",
                        "markdown"
                    ],
                    [
                        "x_axis = x: this option is assigning the name x_axis to the variable x",
                        "markdown"
                    ],
                    [
                        "y_axis = y: this option is assigning the name y_axis to the variable y",
                        "markdown"
                    ],
                    [
                        "np.savez(\"x_y-squared.npz\", x_axis=x, y_axis=y)",
                        "code"
                    ]
                ]
            },
            {
                "Remove the saved arrays and load them back with NumPy\u2019s": [
                    [
                        "In your current working directory, you should have a new file with the\nname x_y-squared.npz. This file is a zipped binary of the two arrays,\nx and y. Let\u2019s clear the workspace and load the values back in. This\nx_y-squared.npz file contains two \nfiles. The NPY format is a . You cannot read\nthe numbers in a standard text editor or spreadsheet.",
                        "markdown"
                    ],
                    [
                        "remove x and y from the workspaec with ",
                        "markdown"
                    ],
                    [
                        "load the arrays into the workspace in a dictionary with ",
                        "markdown"
                    ],
                    [
                        "To see what variables are in the workspace, use the Jupyter/IPython\n\u201cmagic\u201d command\n.",
                        "markdown"
                    ],
                    [
                        "del x, y",
                        "code"
                    ],
                    [
                        "%whos",
                        "code"
                    ],
                    [
                        "Variable   Type      Data/Info\n------------------------------\nnp         module    &lt;module 'numpy' from '/ho&lt;...&gt;kages/numpy/__init__.py'&gt;",
                        "code"
                    ],
                    [
                        "load_xy = np.load(\"x_y-squared.npz\")\n\nprint(load_xy.files)",
                        "code"
                    ],
                    [
                        "['x_axis', 'y_axis']",
                        "code"
                    ],
                    [
                        "whos",
                        "code"
                    ],
                    [
                        "Variable   Type       Data/Info\n-------------------------------\nload_xy    NpzFile    &lt;numpy.lib.npyio.NpzFile &lt;...&gt;object at 0x7f7cfce98880&gt;\nnp         module     &lt;module 'numpy' from '/ho&lt;...&gt;kages/numpy/__init__.py'&gt;",
                        "code"
                    ]
                ]
            },
            {
                "Reassign the NpzFile arrays to x and y": [
                    [
                        "You\u2019ve now created the dictionary with an NpzFile-type. The\nincluded files are x_axis and y_axis that you defined in your\nsavez command. You can reassign x and y to the load_xy files.",
                        "markdown"
                    ],
                    [
                        "x = load_xy[\"x_axis\"]\ny = load_xy[\"y_axis\"]\nprint(x)\nprint(y)",
                        "code"
                    ],
                    [
                        "[0 1 2 3 4 5 6 7 8 9]\n[ 0  1  4  9 16 25 36 49 64 81]",
                        "code"
                    ]
                ]
            },
            {
                "Success": [
                    [
                        "You have created, saved, deleted, and loaded the variables x and y using savez and load. Nice work.",
                        "markdown"
                    ]
                ]
            },
            {
                "Another option: saving to human-readable csv": [
                    [
                        "Let\u2019s consider another scenario, you want to share x and y with\nother people or other programs. You may need human-readable text file\nthat is easier to share. Next, you use the\n\nto save x and y in a comma separated value file, x_y-squared.csv.\nThe resulting csv is composed of ASCII characters. You can load the file\nback into NumPy or read it with other programs.",
                        "markdown"
                    ]
                ]
            },
            {
                "Rearrange the data into a single 2D array": [
                    [
                        "First, you have to create a single 2D array from your two 1D arrays. The\ncsv-filetype is a spreadsheet-style dataset. The csv arranges numbers in\nrows\u2013separated by new lines\u2013and columns\u2013separated by commas. If the\ndata is more complex e.g. multiple 2D arrays or higher dimensional\narrays, it is better to use savez. Here, you use\ntwo NumPy functions to format the data:",
                        "markdown"
                    ],
                    [
                        ": this function appends arrays together into a 2D array",
                        "markdown"
                    ],
                    [
                        ": this function forces the 1D array into a 2D column vector with 10 rows and 1 column.",
                        "markdown"
                    ],
                    [
                        "array_out = np.block([x[:, np.newaxis], y[:, np.newaxis]])\nprint(\"the output array has shape \", array_out.shape, \" with values:\")\nprint(array_out)",
                        "code"
                    ],
                    [
                        "the output array has shape  (10, 2)  with values:\n[[ 0  0]\n [ 1  1]\n [ 2  4]\n [ 3  9]\n [ 4 16]\n [ 5 25]\n [ 6 36]\n [ 7 49]\n [ 8 64]\n [ 9 81]]",
                        "code"
                    ]
                ]
            },
            {
                "Save the data to csv file using": [
                    [
                        "You use savetxt with a three options to make your file easier to read:",
                        "markdown"
                    ],
                    [
                        "X = array_out: this option tells savetxt to save your 2D array, array_out, to the file x_y-squared.csv",
                        "markdown"
                    ],
                    [
                        "header = 'x, y': this option writes a header before any data that labels the columns of the csv",
                        "markdown"
                    ],
                    [
                        "delimiter = ',': this option tells savetxt to place a comma between each column in the file",
                        "markdown"
                    ],
                    [
                        "np.savetxt(\"x_y-squared.csv\", X=array_out, header=\"x, y\", delimiter=\",\")",
                        "code"
                    ],
                    [
                        "Open the file, x_y-squared.csv, and you\u2019ll see the following:",
                        "markdown"
                    ],
                    [
                        "# x, y\n0.000000000000000000e+00,0.000000000000000000e+00\n1.000000000000000000e+00,1.000000000000000000e+00\n2.000000000000000000e+00,4.000000000000000000e+00\n3.000000000000000000e+00,9.000000000000000000e+00\n4.000000000000000000e+00,1.600000000000000000e+01\n5.000000000000000000e+00,2.500000000000000000e+01\n6.000000000000000000e+00,3.600000000000000000e+01\n7.000000000000000000e+00,4.900000000000000000e+01\n8.000000000000000000e+00,6.400000000000000000e+01\n9.000000000000000000e+00,8.100000000000000000e+01",
                        "code"
                    ]
                ]
            },
            {
                "Our arrays as a csv file": [
                    [
                        "There are two features that you shoud notice here:",
                        "markdown"
                    ],
                    [
                        "NumPy uses # to ignore headings when using loadtxt. If you\u2019re using\n\nwith other csv files, you can skip header rows with skiprows = &lt;number_of_header_lines&gt;.",
                        "markdown"
                    ],
                    [
                        "The integers were written in scientific notation. <em>You can</em> specify\nthe format of the text using the savetxt option, , but it\nwill still be written with ASCII characters. In general, you cannot\npreserve the type of ASCII numbers as float or int.",
                        "markdown"
                    ],
                    [
                        "Now, delete x and y again and assign them to your columns in x-y_squared.csv.",
                        "markdown"
                    ],
                    [
                        "del x, y",
                        "code"
                    ],
                    [
                        "load_xy = np.loadtxt(\"x_y-squared.csv\", delimiter=\",\")",
                        "code"
                    ],
                    [
                        "load_xy.shape",
                        "code"
                    ],
                    [
                        "(10, 2)",
                        "code"
                    ],
                    [
                        "x = load_xy[:, 0]\ny = load_xy[:, 1]\nprint(x)\nprint(y)",
                        "code"
                    ],
                    [
                        "[0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n[ 0.  1.  4.  9. 16. 25. 36. 49. 64. 81.]",
                        "code"
                    ]
                ]
            },
            {
                "Success, but remember your types": [
                    [
                        "When you saved the arrays to the csv file, you did not preserve the\nint type. When loading the arrays back into your workspace the default process will be to load the csv file as a 2D floating point array e.g. load_xy.dtype == 'float64' and load_xy.shape == (10, 2).",
                        "markdown"
                    ]
                ]
            },
            {
                "Wrapping up": [
                    [
                        "In conclusion, you can create, save, and load arrays in NumPy. Saving arrays makes sharing your work and collaboration much easier. There are other ways Python can save data to files, such as , but savez and savetxt will serve most of your storage needs for future NumPy work and sharing with other people, respectively.",
                        "markdown"
                    ],
                    [
                        "<strong>Next steps</strong>: you can import data with missing values from  or learn more about general NumPy IO with .",
                        "markdown"
                    ]
                ]
            }
        ],
        "Masked Arrays": [
            {
                "What you\u2019ll do": [
                    [
                        "Use the masked arrays module from NumPy to analyze COVID-19 data and deal with missing values.",
                        "markdown"
                    ]
                ]
            },
            {
                "What you\u2019ll learn": [
                    [
                        "You\u2019ll understand what are masked arrays and how they can be created",
                        "markdown"
                    ],
                    [
                        "You\u2019ll see how to access and modify data for masked arrays",
                        "markdown"
                    ],
                    [
                        "You\u2019ll be able to decide when the use of masked arrays is appropriate in some of your applications",
                        "markdown"
                    ]
                ]
            },
            {
                "What you\u2019ll need": [
                    [
                        "Basic familiarity with Python. If you would like to refresh your memory, take a look at the .",
                        "markdown"
                    ],
                    [
                        "Basic familiarity with NumPy",
                        "markdown"
                    ],
                    [
                        "To run the plots on your computer, you need .",
                        "markdown"
                    ]
                ]
            },
            {
                "What are masked arrays?": [
                    [
                        "Consider the following problem. You have a dataset with missing or invalid entries. If you\u2019re doing any kind of processing on this data, and want to <em>skip</em> or flag these unwanted entries without just deleting them, you may have to use conditionals or filter your data somehow. The  module provides some of the same functionality of  with added structure to ensure invalid entries are not used in computation.",
                        "markdown"
                    ],
                    [
                        "From the :\n<blockquote>",
                        "markdown"
                    ],
                    [
                        "A masked array is the combination of a standard  and a <strong>mask</strong>. A mask is either nomask, indicating that no value of the associated array is invalid, or an array of booleans that determines for each element of the associated array whether the value is valid or not. When an element of the mask is False, the corresponding element of the associated array is valid and is said to be unmasked. When an element of the mask is True, the corresponding element of the associated array is said to be masked (invalid).\n</blockquote>",
                        "markdown"
                    ],
                    [
                        "We can think of a  as a combination of:",
                        "markdown"
                    ],
                    [
                        "Data, as a regular numpy.ndarray of any shape or datatype;",
                        "markdown"
                    ],
                    [
                        "A boolean mask with the same shape as the data;",
                        "markdown"
                    ],
                    [
                        "A fill_value, a value that may be used to replace the invalid entries in order to return a standard numpy.ndarray.",
                        "markdown"
                    ]
                ]
            },
            {
                "When can they be useful?": [
                    [
                        "There are a few situations where masked arrays can be more useful than just eliminating the invalid entries of an array:",
                        "markdown"
                    ],
                    [
                        "When you want to preserve the values you masked for later processing, without copying the array;",
                        "markdown"
                    ],
                    [
                        "When you have to handle many arrays, each with their own mask. If the mask is part of the array, you avoid bugs and the code is possibly more compact;",
                        "markdown"
                    ],
                    [
                        "When you have different flags for missing or invalid values, and wish to preserve these flags without replacing them in the original dataset, but exclude them from computations;",
                        "markdown"
                    ],
                    [
                        "If you can\u2019t avoid or eliminate missing values, but don\u2019t want to deal with  values in your operations.",
                        "markdown"
                    ],
                    [
                        "Masked arrays are also a good idea since the numpy.ma module also comes with a specific implementation of most , which means that you can still apply fast vectorized functions and operations on masked data. The output is then a masked array. We\u2019ll see some examples of how this works in practice below.",
                        "markdown"
                    ]
                ]
            },
            {
                "Using masked arrays to see COVID-19 data": [
                    [
                        "From  it is possible to download a dataset with initial data about the COVID-19 outbreak in the beginning of 2020. We are going to look at a small subset of this data, contained in the file who_covid_19_sit_rep_time_series.csv. <em>(Note that this file has been replaced with a version without missing data sometime in late 2020.)</em>",
                        "markdown"
                    ],
                    [
                        "import numpy as np\nimport os\n\n# The os.getcwd() function returns the current folder; you can change\n# the filepath variable to point to the folder where you saved the .csv file\nfilepath = os.getcwd()\nfilename = os.path.join(filepath, \"who_covid_19_sit_rep_time_series.csv\")",
                        "code"
                    ],
                    [
                        "The data file contains data of different types and is organized as follows:",
                        "markdown"
                    ],
                    [
                        "The first row is a header line that (mostly) describes the data in each column that follow in the rows below, and beginning in the fourth column, the header is the date of the observation.",
                        "markdown"
                    ],
                    [
                        "The second through seventh row contain summary data that is of a different type than that which we are going to examine, so we will need to exclude that from the data with which we will work.",
                        "markdown"
                    ],
                    [
                        "The numerical data we wish to work with begins at column 4, row 8, and extends from there to the rightmost column and the lowermost row.",
                        "markdown"
                    ],
                    [
                        "Let\u2019s explore the data inside this file for the first 14 days of records. To gather data from the .csv file, we will use the  function, making sure we select only the columns with actual numbers instead of the first four columns which contain location data. We also skip the first 6\nrows of this file, since they contain other data we are not interested in. Separately, we will extract the information about dates and location for this data.",
                        "markdown"
                    ],
                    [
                        "# Note we are using skip_header and usecols to read only portions of the\n# data file into each variable.\n# Read just the dates for columns 4-18 from the first row\ndates = np.genfromtxt(\n    filename,\n    dtype=np.unicode_,\n    delimiter=\",\",\n    max_rows=1,\n    usecols=range(4, 18),\n    encoding=\"utf-8-sig\",\n)\n# Read the names of the geographic locations from the first two\n# columns, skipping the first six rows\nlocations = np.genfromtxt(\n    filename,\n    dtype=np.unicode_,\n    delimiter=\",\",\n    skip_header=6,\n    usecols=(0, 1),\n    encoding=\"utf-8-sig\",\n)\n# Read the numeric data from just the first 14 days\nnbcases = np.genfromtxt(\n    filename,\n    dtype=np.int_,\n    delimiter=\",\",\n    skip_header=6,\n    usecols=range(4, 18),\n    encoding=\"utf-8-sig\",\n)",
                        "code"
                    ],
                    [
                        "Included in the numpy.genfromtxt function call, we have selected the  for each subset of the data (either an integer - numpy.int_ - or a string of characters - numpy.unicode_). We have also used the encoding argument to select utf-8-sig as the encoding for the file (read more about encoding in the . You can read more about the numpy.genfromtxt function from the  or from the .",
                        "markdown"
                    ]
                ]
            },
            {
                "Exploring the data": [
                    [
                        "First of all, we can plot the whole set of data we have and see what it looks like. In order to get a readable plot, we select only a few of the dates to show in our . Note also that in our plot command, we use nbcases.T (the transpose of the nbcases array) since this means we will plot each row of the file as a separate line. We choose to plot a dashed line (using the '--' line style). See the  documentation for more info on this.",
                        "markdown"
                    ],
                    [
                        "import matplotlib.pyplot as plt\n\nselected_dates = [0, 3, 11, 13]\nplt.plot(dates, nbcases.T, \"--\")\nplt.xticks(selected_dates, dates[selected_dates])\nplt.title(\"COVID-19 cumulative cases from Jan 21 to Feb 3 2020\")",
                        "code"
                    ],
                    [
                        "Text(0.5, 1.0, 'COVID-19 cumulative cases from Jan 21 to Feb 3 2020')\n\n\n<img alt=\"../_images/61642c5f2f4ca1f6e8c5ff0631b418879f272fc201422517e443413c935adc67.png\" src=\"../_images/61642c5f2f4ca1f6e8c5ff0631b418879f272fc201422517e443413c935adc67.png\"/>",
                        "code"
                    ],
                    [
                        "The graph has a strange shape from January 24th to February 1st. It would be interesting to know where this data comes from. If we look at the locations array we extracted from the .csv file, we can see that we have two columns, where the first would contain regions and the second would contain the name of the country. However, only the first few rows contain data for the the first column (province names in China). Following that, we only have country names. So it would make sense to group all the data from China into a single row. For this, we\u2019ll select from the nbcases array only the rows for which the second entry of the locations array corresponds to China. Next, we\u2019ll use the  function to sum all the selected rows (axis=0). Note also that row 35 corresponds to the total counts for the whole country for each date. Since we want to calculate the sum ourselves from the provinces data, we have to remove that row first from both locations and nbcases:",
                        "markdown"
                    ],
                    [
                        "totals_row = 35\nlocations = np.delete(locations, (totals_row), axis=0)\nnbcases = np.delete(nbcases, (totals_row), axis=0)\n\nchina_total = nbcases[locations[:, 1] == \"China\"].sum(axis=0)\nchina_total",
                        "code"
                    ],
                    [
                        "array([  247,   288,   556,   817,   -22,   -22,   -15,   -10,    -9,\n          -7,    -4, 11820, 14410, 17237])",
                        "code"
                    ],
                    [
                        "Something\u2019s wrong with this data - we are not supposed to have negative values in a cumulative data set. What\u2019s going on?",
                        "markdown"
                    ]
                ]
            },
            {
                "Missing data": [
                    [
                        "Looking at the data, here\u2019s what we find: there is a period with <strong>missing data</strong>:",
                        "markdown"
                    ],
                    [
                        "nbcases",
                        "code"
                    ],
                    [
                        "array([[  258,   270,   375, ...,  7153,  9074, 11177],\n       [   14,    17,    26, ...,   520,   604,   683],\n       [   -1,     1,     1, ...,   422,   493,   566],\n       ...,\n       [   -1,    -1,    -1, ...,    -1,    -1,    -1],\n       [   -1,    -1,    -1, ...,    -1,    -1,    -1],\n       [   -1,    -1,    -1, ...,    -1,    -1,    -1]])",
                        "code"
                    ],
                    [
                        "All the -1 values we are seeing come from numpy.genfromtxt attempting to read missing data from the original .csv file. Obviously, we\ndon\u2019t want to compute missing data as -1 - we just want to skip this value so it doesn\u2019t interfere in our analysis. After importing the numpy.ma module, we\u2019ll create a new array, this time masking the invalid values:",
                        "markdown"
                    ],
                    [
                        "from numpy import ma\n\nnbcases_ma = ma.masked_values(nbcases, -1)",
                        "code"
                    ],
                    [
                        "If we look at the nbcases_ma masked array, this is what we have:",
                        "markdown"
                    ],
                    [
                        "nbcases_ma",
                        "code"
                    ],
                    [
                        "masked_array(\n  data=[[258, 270, 375, ..., 7153, 9074, 11177],\n        [14, 17, 26, ..., 520, 604, 683],\n        [--, 1, 1, ..., 422, 493, 566],\n        ...,\n        [--, --, --, ..., --, --, --],\n        [--, --, --, ..., --, --, --],\n        [--, --, --, ..., --, --, --]],\n  mask=[[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [ True, False, False, ..., False, False, False],\n        ...,\n        [ True,  True,  True, ...,  True,  True,  True],\n        [ True,  True,  True, ...,  True,  True,  True],\n        [ True,  True,  True, ...,  True,  True,  True]],\n  fill_value=-1)",
                        "code"
                    ],
                    [
                        "We can see that this is a different kind of array. As mentioned in the introduction, it has three attributes (data, mask and fill_value).\nKeep in mind that the mask attribute has a True value for elements corresponding to <strong>invalid</strong> data (represented by two dashes in the data attribute).",
                        "markdown"
                    ],
                    [
                        "Let\u2019s try and see what the data looks like excluding the first row (data from the Hubei province in China) so we can look at the missing data more\nclosely:",
                        "markdown"
                    ],
                    [
                        "plt.plot(dates, nbcases_ma[1:].T, \"--\")\nplt.xticks(selected_dates, dates[selected_dates])\nplt.title(\"COVID-19 cumulative cases from Jan 21 to Feb 3 2020\")",
                        "code"
                    ],
                    [
                        "Text(0.5, 1.0, 'COVID-19 cumulative cases from Jan 21 to Feb 3 2020')\n\n\n<img alt=\"../_images/6828fab516c9327c5f752f80a0b0d251c3f205860d47ac3c821a3ad7d1c76ce3.png\" src=\"../_images/6828fab516c9327c5f752f80a0b0d251c3f205860d47ac3c821a3ad7d1c76ce3.png\"/>",
                        "code"
                    ],
                    [
                        "Now that our data has been masked, let\u2019s try summing up all the cases in China:",
                        "markdown"
                    ],
                    [
                        "china_masked = nbcases_ma[locations[:, 1] == \"China\"].sum(axis=0)\nchina_masked",
                        "code"
                    ],
                    [
                        "masked_array(data=[278, 309, 574, 835, 10, 10, 17, 22, 23, 25, 28, 11821,\n                   14411, 17238],\n             mask=[False, False, False, False, False, False, False, False,\n                   False, False, False, False, False, False],\n       fill_value=999999)",
                        "code"
                    ],
                    [
                        "Note that china_masked is a masked array, so it has a different data structure than a regular NumPy array. Now, we can access its data directly by using the .data attribute:",
                        "markdown"
                    ],
                    [
                        "china_total = china_masked.data\nchina_total",
                        "code"
                    ],
                    [
                        "array([  278,   309,   574,   835,    10,    10,    17,    22,    23,\n          25,    28, 11821, 14411, 17238])",
                        "code"
                    ],
                    [
                        "That is better: no more negative values. However, we can still see that for some days, the cumulative number of cases seems to go down (from 835 to 10, for example), which does not agree with the definition of \u201ccumulative data\u201d. If we look more closely at the data, we can see that in the period where there was missing data in mainland China, there was valid data for Hong Kong, Taiwan, Macau and \u201cUnspecified\u201d regions of China. Maybe we can remove those from the total sum of cases in China, to get a better understanding of the data.",
                        "markdown"
                    ],
                    [
                        "First, we\u2019ll identify the indices of locations in mainland China:",
                        "markdown"
                    ],
                    [
                        "china_mask = (\n    (locations[:, 1] == \"China\")\n    &amp; (locations[:, 0] != \"Hong Kong\")\n    &amp; (locations[:, 0] != \"Taiwan\")\n    &amp; (locations[:, 0] != \"Macau\")\n    &amp; (locations[:, 0] != \"Unspecified*\")\n)",
                        "code"
                    ],
                    [
                        "Now, china_mask is an array of boolean values (True or False); we can check that the indices are what we wanted with the  method for masked arrays:",
                        "markdown"
                    ],
                    [
                        "china_mask.nonzero()",
                        "code"
                    ],
                    [
                        "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n        17, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28, 29, 31, 33]),)",
                        "code"
                    ],
                    [
                        "Now we can correctly sum entries for mainland China:",
                        "markdown"
                    ],
                    [
                        "china_total = nbcases_ma[china_mask].sum(axis=0)\nchina_total",
                        "code"
                    ],
                    [
                        "masked_array(data=[278, 308, 440, 446, --, --, --, --, --, --, --, 11791,\n                   14380, 17205],\n             mask=[False, False, False, False,  True,  True,  True,  True,\n                    True,  True,  True, False, False, False],\n       fill_value=999999)",
                        "code"
                    ],
                    [
                        "We can replace the data with this information and plot a new graph, focusing on Mainland China:",
                        "markdown"
                    ],
                    [
                        "plt.plot(dates, china_total.T, \"--\")\nplt.xticks(selected_dates, dates[selected_dates])\nplt.title(\"COVID-19 cumulative cases from Jan 21 to Feb 3 2020 - Mainland China\")",
                        "code"
                    ],
                    [
                        "Text(0.5, 1.0, 'COVID-19 cumulative cases from Jan 21 to Feb 3 2020 - Mainland China')\n\n\n<img alt=\"../_images/c73ad892cb741c9f3dc94135672ec45191a1d62af9283d1c9e99f90c125d3c1b.png\" src=\"../_images/c73ad892cb741c9f3dc94135672ec45191a1d62af9283d1c9e99f90c125d3c1b.png\"/>",
                        "code"
                    ],
                    [
                        "It\u2019s clear that masked arrays are the right solution here. We cannot represent the missing data without mischaracterizing the evolution of the curve.",
                        "markdown"
                    ]
                ]
            },
            {
                "Fitting Data": [
                    [
                        "One possibility we can think of is to interpolate the missing data to estimate the number of cases in late January. Observe that we can select the masked elements using the .mask attribute:",
                        "markdown"
                    ],
                    [
                        "china_total.mask\ninvalid = china_total[china_total.mask]\ninvalid",
                        "code"
                    ],
                    [
                        "masked_array(data=[--, --, --, --, --, --, --],\n             mask=[ True,  True,  True,  True,  True,  True,  True],\n       fill_value=999999,\n            dtype=int64)",
                        "code"
                    ],
                    [
                        "We can also access the valid entries by using the logical negation for this mask:",
                        "markdown"
                    ],
                    [
                        "valid = china_total[~china_total.mask]\nvalid",
                        "code"
                    ],
                    [
                        "masked_array(data=[278, 308, 440, 446, 11791, 14380, 17205],\n             mask=[False, False, False, False, False, False, False],\n       fill_value=999999)",
                        "code"
                    ],
                    [
                        "Now, if we want to create a very simple approximation for this data, we should take into account the valid entries around the invalid ones. So first let\u2019s select the dates for which the data is valid. Note that we can use the mask from the china_total masked array to index the dates array:",
                        "markdown"
                    ],
                    [
                        "dates[~china_total.mask]",
                        "code"
                    ],
                    [
                        "array(['1/21/20', '1/22/20', '1/23/20', '1/24/20', '2/1/20', '2/2/20',\n       '2/3/20'], dtype='&lt;U7')",
                        "code"
                    ],
                    [
                        "Finally, we can use the\n\npackage to create a cubic polynomial model that fits the data as best as possible:",
                        "markdown"
                    ],
                    [
                        "t = np.arange(len(china_total))\nmodel = np.polynomial.Polynomial.fit(t[~china_total.mask], valid, deg=3)\nplt.plot(t, china_total)\nplt.plot(t, model(t), \"--\")",
                        "code"
                    ],
                    [
                        "[&lt;matplotlib.lines.Line2D at 0x7fb70eec3a30&gt;]\n\n\n<img alt=\"../_images/7bb5f74f01083f9cef8de19d4ca821b3542f52c0aef5966bbe500f6f26854902.png\" src=\"../_images/7bb5f74f01083f9cef8de19d4ca821b3542f52c0aef5966bbe500f6f26854902.png\"/>",
                        "code"
                    ],
                    [
                        "This plot is not so readable since the lines seem to be over each other, so let\u2019s summarize in a more elaborate plot. We\u2019ll plot the real data when\navailable, and show the cubic fit for unavailable data, using this fit to compute an estimate to the observed number of cases on January 28th 2020, 7 days after the beginning of the records:",
                        "markdown"
                    ],
                    [
                        "plt.plot(t, china_total)\nplt.plot(t[china_total.mask], model(t)[china_total.mask], \"--\", color=\"orange\")\nplt.plot(7, model(7), \"r*\")\nplt.xticks([0, 7, 13], dates[[0, 7, 13]])\nplt.yticks([0, model(7), 10000, 17500])\nplt.legend([\"Mainland China\", \"Cubic estimate\", \"7 days after start\"])\nplt.title(\n    \"COVID-19 cumulative cases from Jan 21 to Feb 3 2020 - Mainland China\\n\"\n    \"Cubic estimate for 7 days after start\"\n)",
                        "code"
                    ],
                    [
                        "Text(0.5, 1.0, 'COVID-19 cumulative cases from Jan 21 to Feb 3 2020 - Mainland China\\nCubic estimate for 7 days after start')\n\n\n<img alt=\"../_images/a6f22fa34b666874eedfdbab1f4491a703ebfc1e441ced449b895c36529a7533.png\" src=\"../_images/a6f22fa34b666874eedfdbab1f4491a703ebfc1e441ced449b895c36529a7533.png\"/>",
                        "code"
                    ]
                ]
            },
            {
                "In practice": [
                    [
                        "Adding -1 to missing data is not a problem with numpy.genfromtxt; in this particular case, substituting the missing value with 0 might have been fine, but we\u2019ll see later that this is far from a general solution. Also, it is possible to call the numpy.genfromtxt function using the usemask parameter. If usemask=True, numpy.genfromtxt automatically returns a masked array.",
                        "markdown"
                    ]
                ]
            },
            {
                "Further reading": [
                    [
                        "Topics not covered in this tutorial can be found in the documentation:",
                        "markdown"
                    ],
                    [
                        " vs. ",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    {
                        "Reference": [
                            [
                                "Ensheng Dong, Hongru Du, Lauren Gardner, <em>An interactive web-based dashboard to track COVID-19 in real time</em>, The Lancet Infectious Diseases, Volume 20, Issue 5, 2020, Pages 533-534, ISSN 1473-3099, .",
                                "markdown"
                            ]
                        ]
                    }
                ]
            }
        ]
    },
    "NumPy Applications": {
        "Determining Moore\u2019s Law with real data in NumPy": [
            [
                "<img alt=\"Scatter plot of MOS transistor count per microprocessor every two years as a demonstration of Moore's Law.\" src=\"../_images/01-mooreslaw-tutorial-intro.png\"/>",
                "markdown"
            ],
            [
                "<em>The number of transistors reported per a given chip plotted on a log scale in the y axis with the date of introduction on the linear scale x-axis. The blue data points are from a . The red line is an ordinary least squares prediction and the orange line is Moore\u2019s law.</em>",
                "markdown"
            ],
            {
                "What you\u2019ll do": [
                    [
                        "In 1965, engineer Gordon Moore\n that\ntransistors on a chip would double every two years in the coming decade\n[,\n].\nYou\u2019ll compare Moore\u2019s prediction against actual transistor counts in\nthe 53 years following his prediction. You will determine the best-fit constants to describe the exponential growth of transistors on semiconductors compared to Moore\u2019s Law.",
                        "markdown"
                    ]
                ]
            },
            {
                "Skills you\u2019ll learn": [
                    [
                        "Load data from a  file",
                        "markdown"
                    ],
                    [
                        "Perform linear regression and predict exponential growth using ordinary least squares",
                        "markdown"
                    ],
                    [
                        "You\u2019ll compare exponential growth constants between models",
                        "markdown"
                    ],
                    [
                        "Share your analysis in a file:",
                        "markdown"
                    ],
                    [
                        "as NumPy zipped files *.npz",
                        "markdown"
                    ],
                    [
                        "as a *.csv file",
                        "markdown"
                    ],
                    [
                        "Assess the amazing progress semiconductor manufacturers have made in the last five decades",
                        "markdown"
                    ]
                ]
            },
            {
                "What you\u2019ll need": [
                    [
                        "<strong>1.</strong> These packages:",
                        "markdown"
                    ],
                    [
                        "NumPy",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        " ordinary linear regression",
                        "markdown"
                    ],
                    [
                        "imported with the following commands",
                        "markdown"
                    ],
                    [
                        "import matplotlib.pyplot as plt\nimport numpy as np\nimport statsmodels.api as sm",
                        "code"
                    ],
                    [
                        "<strong>2.</strong> Since this is an exponential growth law you need a little background in doing math with  and .",
                        "markdown"
                    ],
                    [
                        "You\u2019ll use these NumPy, Matplotlib, and statsmodels functions:",
                        "markdown"
                    ],
                    [
                        ": this function loads text into a NumPy array",
                        "markdown"
                    ],
                    [
                        ": this function takes the natural log of all elements in a NumPy array",
                        "markdown"
                    ],
                    [
                        ": this function takes the exponential of all elements in a NumPy array",
                        "markdown"
                    ],
                    [
                        ": this is a minimal function definition for creating a function model",
                        "markdown"
                    ],
                    [
                        ": this function will plot x-y data onto a figure with a linear x-axis and \\(\\log_{10}\\) y-axis\n: this function will plot x-y data on linear axes",
                        "markdown"
                    ],
                    [
                        ": find fitting parameters and standard errors using the statsmodels ordinary least squares model",
                        "markdown"
                    ],
                    [
                        "slicing arrays: view parts of the data loaded into the workspace, slice the arrays e.g. x[:10] for the first 10 values in the array, x",
                        "markdown"
                    ],
                    [
                        "boolean array indexing: to view parts of the data that match a given condition use boolean operations to index an array",
                        "markdown"
                    ],
                    [
                        ": to combine arrays into 2D arrays",
                        "markdown"
                    ],
                    [
                        ": to change a 1D vector to a row or column vector",
                        "markdown"
                    ],
                    [
                        " and : these two functions will save your arrays in zipped array format and text, respectively",
                        "markdown"
                    ]
                ]
            },
            {
                "Building Moore\u2019s law as an exponential function": [
                    [
                        "Your empirical model assumes that the number of transistors per\nsemiconductor follows an exponential growth,",
                        "markdown"
                    ],
                    [
                        "\\(\\log(\\text{transistor_count})= f(\\text{year}) = A\\cdot \\text{year}+B,\\)",
                        "markdown"
                    ],
                    [
                        "where \\(A\\) and \\(B\\) are fitting constants. You use semiconductor\nmanufacturers\u2019 data to find the fitting constants.",
                        "markdown"
                    ],
                    [
                        "You determine these constants for Moore\u2019s law by specifying the\nrate for added transistors, 2, and giving an initial number of transistors for a given year.",
                        "markdown"
                    ],
                    [
                        "You state Moore\u2019s law in an exponential form as follows,",
                        "markdown"
                    ],
                    [
                        "\\(\\text{transistor_count}= e^{A_M\\cdot \\text{year} +B_M}.\\)",
                        "markdown"
                    ],
                    [
                        "Where \\(A_M\\) and \\(B_M\\) are constants that double the number of transistors every two years and start at 2250 transistors in 1971,",
                        "markdown"
                    ],
                    [
                        "\\(\\dfrac{\\text{transistor_count}(\\text{year} +2)}{\\text{transistor_count}(\\text{year})} = 2 = \\dfrac{e^{B_M}e^{A_M \\text{year} + 2A_M}}{e^{B_M}e^{A_M \\text{year}}} = e^{2A_M} \\rightarrow A_M = \\frac{\\log(2)}{2}\\)",
                        "markdown"
                    ],
                    [
                        "\\(\\log(2250) = \\frac{\\log(2)}{2}\\cdot 1971 + B_M \\rightarrow B_M = \\log(2250)-\\frac{\\log(2)}{2}\\cdot 1971\\)",
                        "markdown"
                    ],
                    [
                        "so Moore\u2019s law stated as an exponential function is",
                        "markdown"
                    ],
                    [
                        "\\(\\log(\\text{transistor_count})= A_M\\cdot \\text{year}+B_M,\\)",
                        "markdown"
                    ],
                    [
                        "where",
                        "markdown"
                    ],
                    [
                        "\\(A_M=0.3466\\)",
                        "markdown"
                    ],
                    [
                        "\\(B_M=-675.4\\)",
                        "markdown"
                    ],
                    [
                        "Since the function represents Moore\u2019s law, define it as a Python\nfunction using",
                        "markdown"
                    ],
                    [
                        "A_M = np.log(2) / 2\nB_M = np.log(2250) - A_M * 1971\nMoores_law = lambda year: np.exp(B_M) * np.exp(A_M * year)",
                        "code"
                    ],
                    [
                        "In 1971, there were 2250 transistors on the Intel 4004 chip. Use\nMoores_law to check the number of semiconductors Gordon Moore would expect\nin 1973.",
                        "markdown"
                    ],
                    [
                        "ML_1971 = Moores_law(1971)\nML_1973 = Moores_law(1973)\nprint(\"In 1973, G. Moore expects {:.0f} transistors on Intels chips\".format(ML_1973))\nprint(\"This is x{:.2f} more transistors than 1971\".format(ML_1973 / ML_1971))",
                        "code"
                    ],
                    [
                        "In 1973, G. Moore expects 4500 transistors on Intels chips\nThis is x2.00 more transistors than 1971",
                        "code"
                    ]
                ]
            },
            {
                "Loading historical manufacturing data to your workspace": [
                    [
                        "Now, make a prediction based upon the historical data for\nsemiconductors per chip. The \neach year is in the transistor_data.csv file. Before loading a *.csv\nfile into a NumPy array, its a good idea to inspect the structure of the\nfile first. Then, locate the columns of interest and save them to a\nvariable. Save two columns of the file to the array, data.",
                        "markdown"
                    ],
                    [
                        "Here, print out the first 10 rows of transistor_data.csv. The columns are",
                        "markdown"
                    ],
                    [
                        "! head transistor_data.csv",
                        "code"
                    ],
                    [
                        "Processor,MOS transistor count,Date of Introduction,Designer,MOSprocess,Area\nIntel 4004 (4-bit  16-pin),2250,1971,Intel,\"10,000\u00a0nm\",12\u00a0mm\u00b2\nIntel 8008 (8-bit  18-pin),3500,1972,Intel,\"10,000\u00a0nm\",14\u00a0mm\u00b2\nNEC \u03bcCOM-4 (4-bit  42-pin),2500,1973,NEC,\"7,500\u00a0nm\",?\nIntel 4040 (4-bit  16-pin),3000,1974,Intel,\"10,000\u00a0nm\",12\u00a0mm\u00b2\nMotorola 6800 (8-bit  40-pin),4100,1974,Motorola,\"6,000\u00a0nm\",16\u00a0mm\u00b2\nIntel 8080 (8-bit  40-pin),6000,1974,Intel,\"6,000\u00a0nm\",20\u00a0mm\u00b2\nTMS 1000 (4-bit  28-pin),8000,1974,Texas Instruments,\"8,000\u00a0nm\",11\u00a0mm\u00b2\nMOS Technology 6502 (8-bit  40-pin),4528,1975,MOS Technology,\"8,000\u00a0nm\",21\u00a0mm\u00b2\nIntersil IM6100 (12-bit  40-pin; clone of PDP-8),4000,1975,Intersil,,",
                        "code"
                    ],
                    [
                        "You don\u2019t need the columns that specify <strong>Processor</strong>, <strong>Designer</strong>,\n<strong>MOSprocess</strong>, or <strong>Area</strong>. That leaves the second and third columns,\n<strong>MOS transistor count</strong> and <strong>Date of Introduction</strong>, respectively.",
                        "markdown"
                    ],
                    [
                        "Next, you load these two columns into a NumPy array using\n.\nThe extra options below will put the data in the desired format:",
                        "markdown"
                    ],
                    [
                        "delimiter = ',': specify delimeter as a comma \u2018,\u2019 (this is the default behavior)",
                        "markdown"
                    ],
                    [
                        "usecols = [1,2]: import the second and third columns from the csv",
                        "markdown"
                    ],
                    [
                        "skiprows = 1: do not use the first row, because its a header row",
                        "markdown"
                    ],
                    [
                        "data = np.loadtxt(\"transistor_data.csv\", delimiter=\",\", usecols=[1, 2], skiprows=1)",
                        "code"
                    ],
                    [
                        "You loaded the entire history of semiconducting into a NumPy array named\ndata. The first column is the <strong>MOS transistor count</strong> and the second\ncolumn is the <strong>Date of Introduction</strong> in a four-digit year.",
                        "markdown"
                    ],
                    [
                        "Next, make the data easier to read and manage by assigning the two\ncolumns to variables, year and transistor_count. Print out the first\n10 values by slicing the year and transistor_count arrays with\n[:10]. Print these values out to check that you have the saved the\ndata to the correct variables.",
                        "markdown"
                    ],
                    [
                        "year = data[:, 1]  # grab the second column and assign\ntransistor_count = data[:, 0]  # grab the first column and assign\n\nprint(\"year:\\t\\t\", year[:10])\nprint(\"trans. cnt:\\t\", transistor_count[:10])",
                        "code"
                    ],
                    [
                        "year:\t\t [1971. 1972. 1973. 1974. 1974. 1974. 1974. 1975. 1975. 1975.]\ntrans. cnt:\t [2250. 3500. 2500. 3000. 4100. 6000. 8000. 4528. 4000. 5000.]",
                        "code"
                    ],
                    [
                        "You are creating a function that predicts the transistor count given a\nyear. You have an <em>independent variable</em>, year, and a <em>dependent\nvariable</em>, transistor_count. Transform the independent variable to\nlog-scale,",
                        "markdown"
                    ],
                    [
                        "\\(y_i = \\log(\\) transistor_count[i] \\(),\\)",
                        "markdown"
                    ],
                    [
                        "resulting in a linear equation,",
                        "markdown"
                    ],
                    [
                        "\\(y_i = A\\cdot \\text{year} +B\\).",
                        "markdown"
                    ],
                    [
                        "yi = np.log(transistor_count)",
                        "code"
                    ]
                ]
            },
            {
                "Calculating the historical growth curve for transistors": [
                    [
                        "Your model assume that yi is a function of year. Now, find the best-fit model that minimizes the difference between \\(y_i\\) and \\(A\\cdot \\text{year} +B, \\) as such",
                        "markdown"
                    ],
                    [
                        "\\(\\min \\sum|y_i - (A\\cdot \\text{year}_i + B)|^2.\\)",
                        "markdown"
                    ],
                    [
                        "This  can be\nsuccinctly represented as arrays as such",
                        "markdown"
                    ],
                    [
                        "\\(\\sum|\\mathbf{y}-\\mathbf{Z} [A,~B]^T|^2,\\)",
                        "markdown"
                    ],
                    [
                        "where \\(\\mathbf{y}\\) are the observations of the log of the number of\ntransistors in a 1D array and \\(\\mathbf{Z}=[\\text{year}_i^1,~\\text{year}_i^0]\\) are the\npolynomial terms for \\(\\text{year}_i\\) in the first and second columns. By\ncreating this set of regressors in the \\(\\mathbf{Z}-\\)matrix you set\nup an ordinary least squares statistical model. Some clever\nNumPy array features will build \\(\\mathbf{Z}\\)",
                        "markdown"
                    ],
                    [
                        "year[:,np.newaxis] : takes the 1D array with shape (179,) and turns it into a 2D column vector with shape (179,1)",
                        "markdown"
                    ],
                    [
                        "**[1, 0] : stacks two columns, in the first column is year**1 and the second column is year**0 == 1",
                        "markdown"
                    ],
                    [
                        "Z = year[:, np.newaxis] ** [1, 0]",
                        "code"
                    ],
                    [
                        "Now that you have the created a matrix of regressors, \\(\\mathbf{Z},\\) and\nthe observations are in vector, \\(\\mathbf{y},\\) you can use these\nvariables to build the an ordinary least squares model with\n.",
                        "markdown"
                    ],
                    [
                        "model = sm.OLS(yi, Z)",
                        "code"
                    ],
                    [
                        "Now, you can view the fitting constants, \\(A\\) and \\(B\\), and their standard\nerrors.  Run the\n and print the\n to view results as such,",
                        "markdown"
                    ],
                    [
                        "results = model.fit()\nprint(results.summary())",
                        "code"
                    ],
                    [
                        "                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.949\nModel:                            OLS   Adj. R-squared:                  0.949\nMethod:                 Least Squares   F-statistic:                     3309.\nDate:                Fri, 17 Mar 2023   Prob (F-statistic):          1.75e-116\nTime:                        17:07:33   Log-Likelihood:                -273.43\nNo. Observations:                 179   AIC:                             550.9\nDf Residuals:                     177   BIC:                             557.2\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nx1             0.3416      0.006     57.521      0.000       0.330       0.353\nconst       -666.3264     11.890    -56.042      0.000    -689.790    -642.862\n==============================================================================\nOmnibus:                      128.297   Durbin-Watson:                   1.600\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             1184.322\nSkew:                          -2.637   Prob(JB):                    6.73e-258\nKurtosis:                      14.444   Cond. No.                     2.84e+05\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.84e+05. This might indicate that there are\nstrong multicollinearity or other numerical problems.",
                        "code"
                    ],
                    [
                        "The <strong>OLS Regression Results</strong> summary gives a lot of information about\nthe regressors, \\(\\mathbf{Z},\\) and observations, \\(\\mathbf{y}.\\) The most\nimportant outputs for your current analysis are",
                        "markdown"
                    ],
                    [
                        "=================================\n                 coef    std err\n---------------------------------\nx1             0.3416      0.006\nconst       -666.3264     11.890\n=================================",
                        "code"
                    ],
                    [
                        "where x1 is slope, \\(A=0.3416\\), const is the intercept,\n\\(B=-666.364\\), and std error gives the precision of constants\n\\(A=0.342\\pm 0.006~\\dfrac{\\log(\\text{transistors}/\\text{chip})}{\\text{years}}\\) and \\(B=-666\\pm\n12~\\log(\\text{transistors}/\\text{chip}),\\) where the units are in\n\\(\\log(\\text{transistors}/\\text{chip})\\). You created an exponential growth model.\nTo get the constants, save them to an array AB with\nresults.params and assign \\(A\\) and \\(B\\) to x1 and constant.",
                        "markdown"
                    ],
                    [
                        "AB = results.params\nA = AB[0]\nB = AB[1]",
                        "code"
                    ],
                    [
                        "Did manufacturers double the transistor count every two years? You have\nthe final formula,",
                        "markdown"
                    ],
                    [
                        "\\(\\dfrac{\\text{transistor_count}(\\text{year} +2)}{\\text{transistor_count}(\\text{year})} = xFactor =\n\\dfrac{e^{B}e^{A( \\text{year} + 2)}}{e^{B}e^{A \\text{year}}} = e^{2A}\\)",
                        "markdown"
                    ],
                    [
                        "where increase in number of transistors is \\(xFactor,\\) number of years is\n2, and \\(A\\) is the best fit slope on the semilog function. The error in\nyour\nprediction, \\(\\Delta(xFactor),\\) comes from the precision of your constant\n\\(A,\\) which you calculated as the standard error \\(\\Delta A= 0.006\\).",
                        "markdown"
                    ],
                    [
                        "\\(\\Delta (xFactor) = \\frac{\\partial}{\\partial A}(e^{2A})\\Delta A = 2Ae^{2A}\\Delta A\\)",
                        "markdown"
                    ],
                    [
                        "print(\"Rate of semiconductors added on a chip every 2 years:\")\nprint(\n    \"\\tx{:.2f} +/- {:.2f} semiconductors per chip\".format(\n        np.exp((A) * 2), 2 * A * np.exp(2 * A) * 0.006\n    )\n)",
                        "code"
                    ],
                    [
                        "Rate of semiconductors added on a chip every 2 years:\n\tx1.98 +/- 0.01 semiconductors per chip",
                        "code"
                    ],
                    [
                        "Based upon your least-squares regression model, the number of\nsemiconductors per chip increased by a factor of \\(1.98\\pm 0.01\\) every two\nyears. You have a model that predicts the number of semiconductors each\nyear. Now compare your model to the actual manufacturing reports.  Plot\nthe linear regression results and all of the transistor counts.",
                        "markdown"
                    ],
                    [
                        "Here, use\n\nto plot the number of transistors on a log-scale and the year on a\nlinear scale. You have defined a three arrays to get to a final model",
                        "markdown"
                    ],
                    [
                        "\\(y_i = \\log(\\text{transistor_count}),\\)",
                        "markdown"
                    ],
                    [
                        "\\(y_i = A \\cdot \\text{year} + B,\\)",
                        "markdown"
                    ],
                    [
                        "and",
                        "markdown"
                    ],
                    [
                        "\\(\\log(\\text{transistor_count}) = A\\cdot \\text{year} + B,\\)",
                        "markdown"
                    ],
                    [
                        "your variables, transistor_count, year, and yi all have the same\ndimensions, (179,). NumPy arrays need the same dimensions to make a\nplot. The predicted number of transistors is now",
                        "markdown"
                    ],
                    [
                        "\\(\\text{transistor_count}_{\\text{predicted}} = e^Be^{A\\cdot \\text{year}}\\).",
                        "markdown"
                    ],
                    [
                        "In the next plot, use the\n\nstyle sheet.\nThe style sheet replicates\nhttps://fivethirtyeight.com elements. Change the matplotlib style with\n.",
                        "markdown"
                    ],
                    [
                        "transistor_count_predicted = np.exp(B) * np.exp(A * year)\ntransistor_Moores_law = Moores_law(year)\nplt.style.use(\"fivethirtyeight\")\nplt.semilogy(year, transistor_count, \"s\", label=\"MOS transistor count\")\nplt.semilogy(year, transistor_count_predicted, label=\"linear regression\")\n\n\nplt.plot(year, transistor_Moores_law, label=\"Moore's Law\")\nplt.title(\n    \"MOS transistor count per microprocessor\\n\"\n    + \"every two years \\n\"\n    + \"Transistor count was x{:.2f} higher\".format(np.exp(A * 2))\n)\nplt.xlabel(\"year introduced\")\nplt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\nplt.ylabel(\"# of transistors\\nper microprocessor\")",
                        "code"
                    ],
                    [
                        "Text(0, 0.5, '# of transistors\\nper microprocessor')\n\n\n<img alt=\"../_images/9407c374bb2daa0f7320ee0351424aa51d56711ed1ce5e5880db6bc269fbc956.png\" src=\"../_images/9407c374bb2daa0f7320ee0351424aa51d56711ed1ce5e5880db6bc269fbc956.png\"/>",
                        "code"
                    ],
                    [
                        "<em>A scatter plot of MOS transistor count per microprocessor every two years with a red line for the ordinary least squares prediction and an orange line for Moore\u2019s law.</em>",
                        "markdown"
                    ],
                    [
                        "The linear regression captures the increase in the number of transistors\nper semiconductors each year.  In 2015, semiconductor manufacturers\nclaimed they could not keep up with Moore\u2019s law anymore. Your analysis\nshows that since 1971, the average increase in transistor count was\nx1.98 every 2 years, but Gordon Moore predicted it would be x2\nevery 2 years. That is an amazing prediction.",
                        "markdown"
                    ],
                    [
                        "Consider the year 2017. Compare the data to your linear regression\nmodel and Gordon Moore\u2019s prediction. First, get the\ntransistor counts from the year 2017. You can do this with a Boolean\ncomparator,",
                        "markdown"
                    ],
                    [
                        "year == 2017.",
                        "markdown"
                    ],
                    [
                        "Then, make a prediction for 2017 with Moores_law defined above\nand plugging in your best fit constants into your function",
                        "markdown"
                    ],
                    [
                        "\\(\\text{transistor_count} = e^{B}e^{A\\cdot \\text{year}}\\).",
                        "markdown"
                    ],
                    [
                        "A great way to compare these measurements is to compare your prediction\nand Moore\u2019s prediction to the average transistor count and look at the\nrange of reported values for that year. Use the\n\noption,\n,\nto increase the transparency of the data. The more opaque the points\nappear, the more reported values lie on that measurement. The green \\(+\\)\nis the average reported transistor count for 2017. Plot your predictions\nfor $\\pm\\frac{1}{2}~years.",
                        "markdown"
                    ],
                    [
                        "transistor_count2017 = transistor_count[year == 2017]\nprint(\n    transistor_count2017.max(), transistor_count2017.min(), transistor_count2017.mean()\n)\ny = np.linspace(2016.5, 2017.5)\nyour_model2017 = np.exp(B) * np.exp(A * y)\nMoore_Model2017 = Moores_law(y)\n\nplt.plot(\n    2017 * np.ones(np.sum(year == 2017)),\n    transistor_count2017,\n    \"ro\",\n    label=\"2017\",\n    alpha=0.2,\n)\nplt.plot(2017, transistor_count2017.mean(), \"g+\", markersize=20, mew=6)\n\nplt.plot(y, your_model2017, label=\"Your prediction\")\nplt.plot(y, Moore_Model2017, label=\"Moores law\")\nplt.ylabel(\"# of transistors\\nper microprocessor\")\nplt.legend()",
                        "code"
                    ],
                    [
                        "19200000000.0 250000000.0 7050000000.0",
                        "code"
                    ],
                    [
                        "&lt;matplotlib.legend.Legend at 0x7f83255cf040&gt;\n\n\n<img alt=\"../_images/397cfee4652c323437ff609ff34c7638ced66ba5e23cab7b83228681d94716a6.png\" src=\"../_images/397cfee4652c323437ff609ff34c7638ced66ba5e23cab7b83228681d94716a6.png\"/>",
                        "code"
                    ],
                    [
                        "The result is that your model is close to the mean, but Gordon\nMoore\u2019s prediction is closer to the maximum number of transistors per\nmicroprocessor produced in 2017. Even though semiconductor manufacturers\nthought that the growth would slow, once in 1975 and now again\napproaching 2025, manufacturers are still producing semiconductors every 2 years that\nnearly double the number of transistors.",
                        "markdown"
                    ],
                    [
                        "The linear regression model is much better at predicting the\naverage than extreme values because it satisfies the condition to\nminimize \\(\\sum |y_i - A\\cdot \\text{year}[i]+B|^2\\).",
                        "markdown"
                    ]
                ]
            },
            {
                "Sharing your results as zipped arrays and a csv": [
                    [
                        "The last step, is to share your findings. You created\nnew arrays that represent a linear regression model and Gordon Moore\u2019s\nprediction. You started this process by importing a csv file into a NumPy\narray using np.loadtxt, to save your model use two approaches",
                        "markdown"
                    ],
                    [
                        ": save NumPy arrays for other Python sessions",
                        "markdown"
                    ],
                    [
                        ": save a csv file with the original data and your predicted data",
                        "markdown"
                    ],
                    {
                        "Zipping the arrays into a file": [
                            [
                                "Using np.savez, you can save thousands of arrays and give them names. The\nfunction np.load will load the arrays back into the workspace as a\ndictionary. You\u2019ll save a five arrays so the next user will have the year,\ntransistor count, predicted transistor count,  Gordon Moore\u2019s\npredicted count, and fitting constants. Add one more variable that other users can use to\nunderstand the model, notes.",
                                "markdown"
                            ],
                            [
                                "notes = \"the arrays in this file are the result of a linear regression model\\n\"\nnotes += \"the arrays include\\nyear: year of manufacture\\n\"\nnotes += \"transistor_count: number of transistors reported by manufacturers in a given year\\n\"\nnotes += \"transistor_count_predicted: linear regression model = exp({:.2f})*exp({:.2f}*year)\\n\".format(\n    B, A\n)\nnotes += \"transistor_Moores_law: Moores law =exp({:.2f})*exp({:.2f}*year)\\n\".format(\n    B_M, A_M\n)\nnotes += \"regression_csts: linear regression constants A and B for log(transistor_count)=A*year+B\"\nprint(notes)",
                                "code"
                            ],
                            [
                                "the arrays in this file are the result of a linear regression model\nthe arrays include\nyear: year of manufacture\ntransistor_count: number of transistors reported by manufacturers in a given year\ntransistor_count_predicted: linear regression model = exp(-666.33)*exp(0.34*year)\ntransistor_Moores_law: Moores law =exp(-675.38)*exp(0.35*year)\nregression_csts: linear regression constants A and B for log(transistor_count)=A*year+B",
                                "code"
                            ],
                            [
                                "np.savez(\n    \"mooreslaw_regression.npz\",\n    notes=notes,\n    year=year,\n    transistor_count=transistor_count,\n    transistor_count_predicted=transistor_count_predicted,\n    transistor_Moores_law=transistor_Moores_law,\n    regression_csts=AB,\n)",
                                "code"
                            ],
                            [
                                "results = np.load(\"mooreslaw_regression.npz\")",
                                "code"
                            ],
                            [
                                "print(results[\"regression_csts\"][1])",
                                "code"
                            ],
                            [
                                "-666.3264063536255",
                                "code"
                            ],
                            [
                                "! ls",
                                "code"
                            ],
                            [
                                "air-quality-data.csv\nmooreslaw_regression.npz\nmooreslaw-tutorial.md\npairing.md\nsave-load-arrays.md\n_static\ntext_preprocessing.py\ntransistor_data.csv\ntutorial-air-quality-analysis.md\ntutorial-deep-learning-on-mnist.md\ntutorial-deep-reinforcement-learning-with-pong-from-pixels.md\ntutorial-ma.md\ntutorial-nlp-from-scratch\ntutorial-nlp-from-scratch.md\ntutorial-plotting-fractals\ntutorial-plotting-fractals.md\ntutorial-static_equilibrium.md\ntutorial-style-guide.md\ntutorial-svd.md\ntutorial-x-ray-image-processing\ntutorial-x-ray-image-processing.md\nwho_covid_19_sit_rep_time_series.csv\nx_y-squared.csv\nx_y-squared.npz",
                                "code"
                            ],
                            [
                                "The benefit of np.savez is you can save hundreds of arrays with\ndifferent shapes and types. Here, you saved 4 arrays that are double\nprecision floating point numbers shape = (179,), one array that was\ntext, and one array of double precision floating point numbers shape =\n(2,). This is the preferred method for saving NumPy arrays for use in\nanother analysis.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "Creating your own comma separated value file": [
                            [
                                "If you want to share data and view the results in a table, then you have to\ncreate a text file. Save the data using np.savetxt. This\nfunction is more limited than np.savez. Delimited files, like csv\u2019s,\nneed 2D arrays.",
                                "markdown"
                            ],
                            [
                                "Prepare the data for export by creating a new 2D array whose columns\ncontain the data of interest.",
                                "markdown"
                            ],
                            [
                                "Use the header option to describe the data and the columns of\nthe file. Define another variable that contains file\ninformation as head.",
                                "markdown"
                            ],
                            [
                                "head = \"the columns in this file are the result of a linear regression model\\n\"\nhead += \"the columns include\\nyear: year of manufacture\\n\"\nhead += \"transistor_count: number of transistors reported by manufacturers in a given year\\n\"\nhead += \"transistor_count_predicted: linear regression model = exp({:.2f})*exp({:.2f}*year)\\n\".format(\n    B, A\n)\nhead += \"transistor_Moores_law: Moores law =exp({:.2f})*exp({:.2f}*year)\\n\".format(\n    B_M, A_M\n)\nhead += \"year:, transistor_count:, transistor_count_predicted:, transistor_Moores_law:\"\nprint(head)",
                                "code"
                            ],
                            [
                                "the columns in this file are the result of a linear regression model\nthe columns include\nyear: year of manufacture\ntransistor_count: number of transistors reported by manufacturers in a given year\ntransistor_count_predicted: linear regression model = exp(-666.33)*exp(0.34*year)\ntransistor_Moores_law: Moores law =exp(-675.38)*exp(0.35*year)\nyear:, transistor_count:, transistor_count_predicted:, transistor_Moores_law:",
                                "code"
                            ],
                            [
                                "Build a single 2D array to export to csv. Tabular data is inherently two\ndimensional. You need to organize your data to fit this 2D structure.\nUse year, transistor_count, transistor_count_predicted, and\ntransistor_Moores_law as the first through fourth columns,\nrespectively. Put the calculated constants in the header since they do\nnot fit the (179,) shape. The\n\nfunction appends arrays together to create a new, larger array. Arrange\nthe 1D vectors as columns using\n\ne.g.",
                                "markdown"
                            ],
                            [
                                "&gt;&gt;&gt; year.shape\n(179,)\n&gt;&gt;&gt; year[:,np.newaxis].shape\n(179,1)",
                                "code"
                            ],
                            [
                                "output = np.block(\n    [\n        year[:, np.newaxis],\n        transistor_count[:, np.newaxis],\n        transistor_count_predicted[:, np.newaxis],\n        transistor_Moores_law[:, np.newaxis],\n    ]\n)",
                                "code"
                            ],
                            [
                                "Creating the mooreslaw_regression.csv with np.savetxt, use three\noptions to create the desired file format:",
                                "markdown"
                            ],
                            [
                                "X = output : use output block to write the data into the file",
                                "markdown"
                            ],
                            [
                                "delimiter = ',' : use commas to separate columns in the file",
                                "markdown"
                            ],
                            [
                                "header = head : use the header head defined above",
                                "markdown"
                            ],
                            [
                                "np.savetxt(\"mooreslaw_regression.csv\", X=output, delimiter=\",\", header=head)",
                                "code"
                            ],
                            [
                                "! head mooreslaw_regression.csv",
                                "code"
                            ],
                            [
                                "# the columns in this file are the result of a linear regression model\n# the columns include\n# year: year of manufacture\n# transistor_count: number of transistors reported by manufacturers in a given year\n# transistor_count_predicted: linear regression model = exp(-666.33)*exp(0.34*year)\n# transistor_Moores_law: Moores law =exp(-675.38)*exp(0.35*year)\n# year:, transistor_count:, transistor_count_predicted:, transistor_Moores_law:\n1.971000000000000000e+03,2.250000000000000000e+03,1.130514785642205879e+03,2.249999999999916326e+03\n1.972000000000000000e+03,3.500000000000000000e+03,1.590908400344028905e+03,3.181980515339620069e+03\n1.973000000000000000e+03,2.500000000000000000e+03,2.238793840141975579e+03,4.500000000000097316e+03",
                                "code"
                            ]
                        ]
                    }
                ]
            },
            {
                "Wrapping up": [
                    [
                        "In conclusion, you have compared historical data for semiconductor\nmanufacturers to Moore\u2019s law and created a linear regression model to\nfind the average number of transistors added to each microprocessor\nevery two years. Gordon Moore predicted the number of transistors would\ndouble every two years from 1965 through 1975, but the average growth\nhas maintained a consistent increase of \\(\\times 1.98 \\pm 0.01\\) every two\nyears from 1971 through 2019.  In 2015, Moore revised his prediction to\nsay Moore\u2019s law should hold until 2025.\n[].\nYou can share these results as a zipped NumPy array file,\nmooreslaw_regression.npz, or as another csv,\nmooreslaw_regression.csv.  The amazing progress in semiconductor\nmanufacturing has enabled new industries and computational power. This\nanalysis should give you a small insight into how incredible this growth\nhas been over the last half-century.",
                        "markdown"
                    ]
                ]
            },
            {
                "References": [
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        ".",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Deep learning on MNIST": [
            [
                "This tutorial demonstrates how to build a simple  (with one hidden layer) and train it from scratch with NumPy to recognize handwritten digit images.",
                "markdown"
            ],
            [
                "Your deep learning model \u2014 one of the most basic artificial neural networks that resembles the original  \u2014 will learn to classify digits from 0 to 9 from the  dataset. The dataset contains 60,000 training and 10,000 test images and corresponding labels. Each training and test image is of size 784 (or 28x28 pixels) \u2014 this will be your input for the neural network.",
                "markdown"
            ],
            [
                "Based on the image inputs and their labels (), your neural network will be trained to learn their features using forward propagation and backpropagation ( differentiation). The final output of the network is a vector of 10 scores \u2014 one for each handwritten digit image. You will also evaluate how good your model is at classifying the images on the test set.",
                "markdown"
            ],
            [
                "<img alt=\"Diagram showing operations detailed in this tutorial (The input imageis passed into a Hidden layer that creates a weighted sum of outputs.The weighted sum is passed to the Non-linearity, then regularization andinto the output layer. The output layer creates a prediction which canthen be compared to existing data. The errors are used to calculate theloss function and update weights in the hidden layer and outputlayer.)\" src=\"../_images/tutorial-deep-learning-on-mnist.png\"/>",
                "markdown"
            ],
            [
                "This tutorial was adapted from the work by  (with the author\u2019s permission).",
                "markdown"
            ],
            {
                "Prerequisites": [
                    [
                        "The reader should have some knowledge of Python, NumPy array manipulation, and linear algebra. In addition, you should be familiar with main concepts of .",
                        "markdown"
                    ],
                    [
                        "To refresh the memory, you can take the  and  tutorials.",
                        "markdown"
                    ],
                    [
                        "You are advised to read the  paper published in 2015 by Yann LeCun, Yoshua Bengio, and Geoffrey Hinton, who are regarded as some of the pioneers of the field. You should also consider reading Andrew Trask\u2019s , which teaches deep learning with NumPy.",
                        "markdown"
                    ],
                    [
                        "In addition to NumPy, you will be utilizing the following Python standard modules for data loading and processing:",
                        "markdown"
                    ],
                    [
                        " for URL handling",
                        "markdown"
                    ],
                    [
                        " for URL opening",
                        "markdown"
                    ],
                    [
                        " for gzip file decompression",
                        "markdown"
                    ],
                    [
                        " to work with the pickle file format",
                        "markdown"
                    ],
                    [
                        "as well as:",
                        "markdown"
                    ],
                    [
                        " for data visualization",
                        "markdown"
                    ],
                    [
                        "This tutorial can be run locally in an isolated environment, such as  or . You can use  to run each notebook cell. Don\u2019t forget to  and .",
                        "markdown"
                    ]
                ]
            },
            {
                "Table of contents": [
                    [
                        "Load the MNIST dataset",
                        "markdown"
                    ],
                    [
                        "Preprocess the dataset",
                        "markdown"
                    ],
                    [
                        "Build and train a small neural network from scratch",
                        "markdown"
                    ],
                    [
                        "Next steps",
                        "markdown"
                    ]
                ]
            },
            {
                "1. Load the MNIST dataset": [
                    [
                        "In this section, you will download the zipped MNIST dataset files originally stored in . Then, you will transform them into 4 files of NumPy array type using built-in Python modules. Finally, you will split the arrays into training and test sets.",
                        "markdown"
                    ],
                    [
                        "<strong>1.</strong> Define a variable to store the training/test image/label names of the MNIST dataset in a list:",
                        "markdown"
                    ],
                    [
                        "data_sources = {\n    \"training_images\": \"train-images-idx3-ubyte.gz\",  # 60,000 training images.\n    \"test_images\": \"t10k-images-idx3-ubyte.gz\",  # 10,000 test images.\n    \"training_labels\": \"train-labels-idx1-ubyte.gz\",  # 60,000 training labels.\n    \"test_labels\": \"t10k-labels-idx1-ubyte.gz\",  # 10,000 test labels.\n}",
                        "code"
                    ],
                    [
                        "<strong>2.</strong> Load the data. First check if the data is stored locally; if not, then\ndownload it.",
                        "markdown"
                    ],
                    [
                        "import requests\nimport os\n\ndata_dir = \"../_data\"\nos.makedirs(data_dir, exist_ok=True)\n\nbase_url = \"https://github.com/rossbar/numpy-tutorial-data-mirror/blob/main/\"\n\nfor fname in data_sources.values():\n    fpath = os.path.join(data_dir, fname)\n    if not os.path.exists(fpath):\n        print(\"Downloading file: \" + fname)\n        resp = requests.get(base_url + fname, stream=True, **request_opts)\n        resp.raise_for_status()  # Ensure download was succesful\n        with open(fpath, \"wb\") as fh:\n            for chunk in resp.iter_content(chunk_size=128):\n                fh.write(chunk)",
                        "code"
                    ],
                    [
                        "Downloading file: train-images-idx3-ubyte.gz",
                        "code"
                    ],
                    [
                        "Downloading file: t10k-images-idx3-ubyte.gz",
                        "code"
                    ],
                    [
                        "Downloading file: train-labels-idx1-ubyte.gz",
                        "code"
                    ],
                    [
                        "Downloading file: t10k-labels-idx1-ubyte.gz",
                        "code"
                    ],
                    [
                        "<strong>3.</strong> Decompress the 4 files and create 4 , saving them into a dictionary. Each original image is of size 28x28 and neural networks normally expect a 1D vector input; therefore, you also need to reshape the images by multiplying 28 by 28 (784).",
                        "markdown"
                    ],
                    [
                        "import gzip\nimport numpy as np\n\nmnist_dataset = {}\n\n# Images\nfor key in (\"training_images\", \"test_images\"):\n    with gzip.open(os.path.join(data_dir, data_sources[key]), \"rb\") as mnist_file:\n        mnist_dataset[key] = np.frombuffer(\n            mnist_file.read(), np.uint8, offset=16\n        ).reshape(-1, 28 * 28)\n# Labels\nfor key in (\"training_labels\", \"test_labels\"):\n    with gzip.open(os.path.join(data_dir, data_sources[key]), \"rb\") as mnist_file:\n        mnist_dataset[key] = np.frombuffer(mnist_file.read(), np.uint8, offset=8)",
                        "code"
                    ],
                    [
                        "<strong>4.</strong> Split the data into training and test sets using the standard notation of x for data and y for labels, calling the training and test set images x_train and x_test, and the labels y_train and y_test:",
                        "markdown"
                    ],
                    [
                        "x_train, y_train, x_test, y_test = (\n    mnist_dataset[\"training_images\"],\n    mnist_dataset[\"training_labels\"],\n    mnist_dataset[\"test_images\"],\n    mnist_dataset[\"test_labels\"],\n)",
                        "code"
                    ],
                    [
                        "<strong>5.</strong> You can confirm that the shape of the image arrays is (60000, 784) and (10000, 784) for training and test sets, respectively, and the labels \u2014 (60000,) and (10000,):",
                        "markdown"
                    ],
                    [
                        "print(\n    \"The shape of training images: {} and training labels: {}\".format(\n        x_train.shape, y_train.shape\n    )\n)\nprint(\n    \"The shape of test images: {} and test labels: {}\".format(\n        x_test.shape, y_test.shape\n    )\n)",
                        "code"
                    ],
                    [
                        "The shape of training images: (60000, 784) and training labels: (60000,)\nThe shape of test images: (10000, 784) and test labels: (10000,)",
                        "code"
                    ],
                    [
                        "<strong>6.</strong> And you can inspect some images using Matplotlib:",
                        "markdown"
                    ],
                    [
                        "import matplotlib.pyplot as plt\n\n# Take the 60,000th image (indexed at 59,999) from the training set,\n# reshape from (784, ) to (28, 28) to have a valid shape for displaying purposes.\nmnist_image = x_train[59999, :].reshape(28, 28)\n# Set the color mapping to grayscale to have a black background.\nplt.imshow(mnist_image, cmap=\"gray\")\n# Display the image.\nplt.show()\n\n\n\n\n<img alt=\"../_images/6e96473e0d5890e8b54a3796e049afb704ba9c4b0fcf60945de18f4cf0454c8f.png\" src=\"../_images/6e96473e0d5890e8b54a3796e049afb704ba9c4b0fcf60945de18f4cf0454c8f.png\"/>",
                        "code"
                    ],
                    [
                        "# Display 5 random images from the training set.\nnum_examples = 5\nseed = 147197952744\nrng = np.random.default_rng(seed)\n\nfig, axes = plt.subplots(1, num_examples)\nfor sample, ax in zip(rng.choice(x_train, size=num_examples, replace=False), axes):\n    ax.imshow(sample.reshape(28, 28), cmap=\"gray\")\n\n\n\n\n<img alt=\"../_images/8484ad8185328c820925db98e28702162d6c6d279eb7cd636dfc9e2d94b68215.png\" src=\"../_images/8484ad8185328c820925db98e28702162d6c6d279eb7cd636dfc9e2d94b68215.png\"/>",
                        "code"
                    ],
                    [
                        "<em>Above are five images taken from the MNIST training set. Various hand-drawn\nArabic numerals are shown, with exact values chosen randomly with each run of the code.</em>\n<blockquote>",
                        "markdown"
                    ],
                    [
                        "<strong>Note:</strong> You can also visualize a sample image as an array by printing x_train[59999]. Here, 59999 is your 60,000th training image sample (0 would be your first). Your output will be quite long and should contain an array of 8-bit integers:",
                        "markdown"
                    ],
                    [
                        "...\n         0,   0,  38,  48,  48,  22,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,  62,  97, 198, 243, 254, 254, 212,  27,   0,   0,   0,   0,\n...\n\n\n</blockquote>",
                        "code"
                    ],
                    [
                        "# Display the label of the 60,000th image (indexed at 59,999) from the training set.\ny_train[59999]",
                        "code"
                    ],
                    [
                        "8",
                        "code"
                    ]
                ]
            },
            {
                "2. Preprocess the data": [
                    [
                        "Neural networks can work with inputs that are in a form of tensors (multidimensional arrays) of floating-point type. When preprocessing the data, you should consider the following processes:  and .",
                        "markdown"
                    ],
                    [
                        "Since the MNIST data is already vectorized and the arrays are of dtype uint8, your next challenge is to convert them to a floating-point format, such as float64 ():",
                        "markdown"
                    ],
                    [
                        "<em>Normalizing</em> the image data: a  procedure that can speed up the neural network training process by standardizing the .",
                        "markdown"
                    ],
                    [
                        "<em></em> of the image labels.",
                        "markdown"
                    ],
                    [
                        "In practice, you can use different types of floating-point precision depending on your goals and you can find more information about that in the  and  blog posts.",
                        "markdown"
                    ],
                    {
                        "Convert the image data to the floating-point format": [
                            [
                                "The images data contain 8-bit integers encoded in the [0, 255] interval with color values between 0 and 255.",
                                "markdown"
                            ],
                            [
                                "You will normalize them into floating-point arrays in the [0, 1] interval by dividing them by 255.",
                                "markdown"
                            ],
                            [
                                "<strong>1.</strong> Check that the vectorized image data has type uint8:",
                                "markdown"
                            ],
                            [
                                "print(\"The data type of training images: {}\".format(x_train.dtype))\nprint(\"The data type of test images: {}\".format(x_test.dtype))",
                                "code"
                            ],
                            [
                                "The data type of training images: uint8\nThe data type of test images: uint8",
                                "code"
                            ],
                            [
                                "<strong>2.</strong> Normalize the arrays by dividing them by 255 (and thus promoting the data type from uint8 to float64) and then assign the train and test image data variables \u2014 x_train and x_test \u2014 to training_images and train_labels, respectively.\nTo reduce the model training and evaluation time in this example, only a subset\nof the training and test images will be used.\nBoth training_images and test_images will contain only 1,000 samples each out\nof the complete datasets of 60,000 and 10,000 images, respectively.\nThese values can be controlled by changing the  training_sample and\ntest_sample below, up to their maximum values of 60,000 and 10,000.",
                                "markdown"
                            ],
                            [
                                "training_sample, test_sample = 1000, 1000\ntraining_images = x_train[0:training_sample] / 255\ntest_images = x_test[0:test_sample] / 255",
                                "code"
                            ],
                            [
                                "<strong>3.</strong> Confirm that the image data has changed to the floating-point format:",
                                "markdown"
                            ],
                            [
                                "print(\"The data type of training images: {}\".format(training_images.dtype))\nprint(\"The data type of test images: {}\".format(test_images.dtype))",
                                "code"
                            ],
                            [
                                "The data type of training images: float64\nThe data type of test images: float64\n\n\n\n\n<blockquote>",
                                "code"
                            ],
                            [
                                "<strong>Note:</strong> You can also check that normalization was successful by printing training_images[0] in a notebook cell. Your long output should contain an array of floating-point numbers:",
                                "markdown"
                            ],
                            [
                                "...\n       0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n       0.07058824, 0.49411765, 0.53333333, 0.68627451, 0.10196078,\n       0.65098039, 1.        , 0.96862745, 0.49803922, 0.        ,\n...\n\n\n</blockquote>",
                                "code"
                            ]
                        ]
                    },
                    {
                        "Convert the labels to floating point through categorical/one-hot encoding": [
                            [
                                "You will use one-hot encoding to embed each digit label as an all-zero vector with np.zeros() and place 1 for a label index. As a result, your label data will be arrays with 1.0 (or 1.) in the position of each image label.",
                                "markdown"
                            ],
                            [
                                "Since there are 10 labels (from 0 to 9) in total, your arrays will look similar to this:",
                                "markdown"
                            ],
                            [
                                "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])",
                                "code"
                            ],
                            [
                                "<strong>1.</strong> Confirm that the image label data are integers with dtype uint8:",
                                "markdown"
                            ],
                            [
                                "print(\"The data type of training labels: {}\".format(y_train.dtype))\nprint(\"The data type of test labels: {}\".format(y_test.dtype))",
                                "code"
                            ],
                            [
                                "The data type of training labels: uint8\nThe data type of test labels: uint8",
                                "code"
                            ],
                            [
                                "<strong>2.</strong> Define a function that performs one-hot encoding on arrays:",
                                "markdown"
                            ],
                            [
                                "def one_hot_encoding(labels, dimension=10):\n    # Define a one-hot variable for an all-zero vector\n    # with 10 dimensions (number labels from 0 to 9).\n    one_hot_labels = labels[..., None] == np.arange(dimension)[None]\n    # Return one-hot encoded labels.\n    return one_hot_labels.astype(np.float64)",
                                "code"
                            ],
                            [
                                "<strong>3.</strong> Encode the labels and assign the values to new variables:",
                                "markdown"
                            ],
                            [
                                "training_labels = one_hot_encoding(y_train[:training_sample])\ntest_labels = one_hot_encoding(y_test[:test_sample])",
                                "code"
                            ],
                            [
                                "<strong>4.</strong> Check that the data type has changed to floating point:",
                                "markdown"
                            ],
                            [
                                "print(\"The data type of training labels: {}\".format(training_labels.dtype))\nprint(\"The data type of test labels: {}\".format(test_labels.dtype))",
                                "code"
                            ],
                            [
                                "The data type of training labels: float64\nThe data type of test labels: float64",
                                "code"
                            ],
                            [
                                "<strong>5.</strong> Examine a few encoded labels:",
                                "markdown"
                            ],
                            [
                                "print(training_labels[0])\nprint(training_labels[1])\nprint(training_labels[2])",
                                "code"
                            ],
                            [
                                "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]",
                                "code"
                            ],
                            [
                                "\u2026and compare to the originals:",
                                "markdown"
                            ],
                            [
                                "print(y_train[0])\nprint(y_train[1])\nprint(y_train[2])",
                                "code"
                            ],
                            [
                                "5\n0\n4",
                                "code"
                            ],
                            [
                                "You have finished preparing the dataset.",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "3. Build and train a small neural network from scratch": [
                    [
                        "In this section you will familiarize yourself with some high-level concepts of the basic building blocks of a deep learning model. You can refer to the original  research publication for more information.",
                        "markdown"
                    ],
                    [
                        "Afterwards, you will construct the building blocks of a simple deep learning model in Python and NumPy and train it to learn to identify handwritten digits from the MNIST dataset with a certain level of accuracy.",
                        "markdown"
                    ],
                    {
                        "Neural network building blocks with NumPy": [
                            [
                                "<em>Layers</em>: These building blocks work as data filters \u2014 they process data and learn representations from inputs to better predict the target outputs.",
                                "markdown"
                            ],
                            [
                                "You will use 1 hidden layer in your model to pass the inputs forward (<em>forward propagation</em>) and propagate the gradients/error derivatives of a loss function backward (<em>backpropagation</em>). These are input, hidden and output layers.",
                                "markdown"
                            ],
                            [
                                "In the hidden (middle) and output (last) layers, the neural network model will compute the weighted sum of inputs. To compute this process, you will use NumPy\u2019s matrix multiplication function (the \u201cdot multiply\u201d or np.dot(layer, weights)).\n<blockquote>",
                                "markdown"
                            ],
                            [
                                "<strong>Note:</strong> For simplicity, the bias term is omitted in this example (there is no np.dot(layer, weights) + bias).\n</blockquote>",
                                "markdown"
                            ],
                            [
                                "<em>Weights</em>: These are important adjustable parameters that the neural network fine-tunes by forward and backward propagating the data. They are optimized through a process called . Before the model training starts, the weights are randomly initialized with NumPy\u2019s .",
                                "markdown"
                            ],
                            [
                                "The optimal weights should produce the highest prediction accuracy and the lowest error on the training and test sets.",
                                "markdown"
                            ],
                            [
                                "<em>Activation function</em>: Deep learning models are capable of determining non-linear relationships between inputs and outputs and these  are usually applied to the output of each layer.",
                                "markdown"
                            ],
                            [
                                "You will use a  to the hidden layer\u2019s output (for example, relu(np.dot(layer, weights)).",
                                "markdown"
                            ],
                            [
                                "<em>Regularization</em>: This  helps prevent the neural network model from .",
                                "markdown"
                            ],
                            [
                                "In this example, you will use a method called dropout \u2014  \u2014 that randomly sets a number of features in a layer to 0s. You will define it with NumPy\u2019s  method and apply it to the hidden layer of the network.",
                                "markdown"
                            ],
                            [
                                "<em>Loss function</em>: The computation determines the quality of predictions by comparing the image labels (the truth) with the predicted values in the final layer\u2019s output.",
                                "markdown"
                            ],
                            [
                                "For simplicity, you will use a basic total squared error using NumPy\u2019s np.sum() function (for example, np.sum((final_layer_output - image_labels) ** 2)).",
                                "markdown"
                            ],
                            [
                                "<em>Accuracy</em>: This metric measures the accuracy of the network\u2019s ability to predict on the data it hasn\u2019t seen.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "Model architecture and training summary": [
                            [
                                "Here is a summary of the neural network model architecture and the training process:",
                                "markdown"
                            ],
                            [
                                "<img alt=\"Diagram showing operations detailed in this tutorial (The input imageis passed into a Hidden layer that creates a weighted sum of outputs.The weighted sum is passed to the Non-linearity, then regularization andinto the output layer. The output layer creates a prediction which canthen be compared to existing data. The errors are used to calculate theloss function and update weights in the hidden layer and outputlayer.)\" src=\"../_images/tutorial-deep-learning-on-mnist.png\"/>",
                                "markdown"
                            ],
                            [
                                "<em>The input layer</em>:",
                                "markdown"
                            ],
                            [
                                "It is the input for the network \u2014 the previously preprocessed data that is loaded from training_images into layer_0.",
                                "markdown"
                            ],
                            [
                                "<em>The hidden (middle) layer</em>:",
                                "markdown"
                            ],
                            [
                                "layer_1 takes the output from the previous layer and performs matrix-multiplication of the input by weights (weights_1) with NumPy\u2019s np.dot()).",
                                "markdown"
                            ],
                            [
                                "Then, this output is passed through the ReLU activation function for non-linearity and then dropout is applied to help with overfitting.",
                                "markdown"
                            ],
                            [
                                "<em>The output (last) layer</em>:",
                                "markdown"
                            ],
                            [
                                "layer_2 ingests the output from layer_1 and repeats the same \u201cdot multiply\u201d process with weights_2.",
                                "markdown"
                            ],
                            [
                                "The final output returns 10 scores for each of the 0-9 digit labels. The network model ends with a size 10 layer \u2014 a 10-dimensional vector.",
                                "markdown"
                            ],
                            [
                                "<em>Forward propagation, backpropagation, training loop</em>:",
                                "markdown"
                            ],
                            [
                                "In the beginning of model training, your network randomly initializes the weights and feeds the input data forward through the hidden and output layers. This process is the forward pass or forward propagation.",
                                "markdown"
                            ],
                            [
                                "Then, the network propagates the \u201csignal\u201d from the loss function back through the hidden layer and adjusts the weights values with the help of the learning rate parameter (more on that later).\n\n\n<blockquote>",
                                "markdown"
                            ],
                            [
                                "<strong>Note:</strong> In more technical terms, you:",
                                "markdown"
                            ],
                            [
                                "Measure the error by comparing the real label of an image (the truth) with the prediction of the model.",
                                "markdown"
                            ],
                            [
                                "Differentiate the loss function.",
                                "markdown"
                            ],
                            [
                                "Ingest the  with the respect to the output, and backpropagate them with the respect to the inputs through the layer(s).",
                                "markdown"
                            ],
                            [
                                "Since the network contains tensor operations and weight matrices, backpropagation uses the .",
                                "markdown"
                            ],
                            [
                                "With each iteration (epoch) of the neural network training, this forward and backward propagation cycle adjusts the weights, which is reflected in the accuracy and error metrics. As you train the model, your goal is to minimize the error and maximize the accuracy on the training data, where the model learns from, as well as the test data, where you evaluate the model.\n</blockquote>",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "Compose the model and begin training and testing it": [
                            [
                                "Having covered the main deep learning concepts and the neural network architecture, let\u2019s write the code.",
                                "markdown"
                            ],
                            [
                                "<strong>1.</strong> We\u2019ll start by creating a new random number generator, providing a seed\nfor reproducibility:",
                                "markdown"
                            ],
                            [
                                "seed = 884736743\nrng = np.random.default_rng(seed)",
                                "code"
                            ],
                            [
                                "<strong>2.</strong> For the hidden layer, define the ReLU activation function for forward propagation and ReLU\u2019s derivative that will be used during backpropagation:",
                                "markdown"
                            ],
                            [
                                "# Define ReLU that returns the input if it's positive and 0 otherwise.\ndef relu(x):\n    return (x &gt;= 0) * x\n\n\n# Set up a derivative of the ReLU function that returns 1 for a positive input\n# and 0 otherwise.\ndef relu2deriv(output):\n    return output &gt;= 0",
                                "code"
                            ],
                            [
                                "<strong>3.</strong> Set certain default values of , such as:",
                                "markdown"
                            ],
                            [
                                ": learning_rate \u2014 helps limit the magnitude of weight updates to prevent them from overcorrecting.",
                                "markdown"
                            ],
                            [
                                "<em>Epochs (iterations)</em>: epochs \u2014 the number of complete passes \u2014 forward and backward propagations \u2014 of the data through the network. This parameter can positively or negatively affect the results. The higher the iterations, the longer the learning process may take. Because this is a computationally intensive task, we have chosen a very low number of epochs (20). To get meaningful results, you should choose a much larger number.",
                                "markdown"
                            ],
                            [
                                "<em>Size of the hidden (middle) layer in a network</em>: hidden_size \u2014 different sizes of the hidden layer can affect the results during training and testing.",
                                "markdown"
                            ],
                            [
                                "<em>Size of the input:</em> pixels_per_image \u2014 you have established that the image input is 784 (28x28) (in pixels).",
                                "markdown"
                            ],
                            [
                                "<em>Number of labels</em>: num_labels \u2014 indicates the output number for the output layer where the predictions occur for 10 (0 to 9) handwritten digit labels.",
                                "markdown"
                            ],
                            [
                                "learning_rate = 0.005\nepochs = 20\nhidden_size = 100\npixels_per_image = 784\nnum_labels = 10",
                                "code"
                            ],
                            [
                                "<strong>4.</strong> Initialize the weight vectors that will be used in the hidden and output layers with random values:",
                                "markdown"
                            ],
                            [
                                "weights_1 = 0.2 * rng.random((pixels_per_image, hidden_size)) - 0.1\nweights_2 = 0.2 * rng.random((hidden_size, num_labels)) - 0.1",
                                "code"
                            ],
                            [
                                "<strong>5.</strong> Set up the neural network\u2019s learning experiment with a training loop and start the training process.\nNote that the model is evaluated against the test set at each epoch to track\nits performance over the training epochs.",
                                "markdown"
                            ],
                            [
                                "Start the training process:",
                                "markdown"
                            ],
                            [
                                "# To store training and test set losses and accurate predictions\n# for visualization.\nstore_training_loss = []\nstore_training_accurate_pred = []\nstore_test_loss = []\nstore_test_accurate_pred = []\n\n# This is a training loop.\n# Run the learning experiment for a defined number of epochs (iterations).\nfor j in range(epochs):\n\n    #################\n    # Training step #\n    #################\n\n    # Set the initial loss/error and the number of accurate predictions to zero.\n    training_loss = 0.0\n    training_accurate_predictions = 0\n\n    # For all images in the training set, perform a forward pass\n    # and backpropagation and adjust the weights accordingly.\n    for i in range(len(training_images)):\n        # Forward propagation/forward pass:\n        # 1. The input layer:\n        #    Initialize the training image data as inputs.\n        layer_0 = training_images[i]\n        # 2. The hidden layer:\n        #    Take in the training image data into the middle layer by\n        #    matrix-multiplying it by randomly initialized weights.\n        layer_1 = np.dot(layer_0, weights_1)\n        # 3. Pass the hidden layer's output through the ReLU activation function.\n        layer_1 = relu(layer_1)\n        # 4. Define the dropout function for regularization.\n        dropout_mask = rng.integers(low=0, high=2, size=layer_1.shape)\n        # 5. Apply dropout to the hidden layer's output.\n        layer_1 *= dropout_mask * 2\n        # 6. The output layer:\n        #    Ingest the output of the middle layer into the the final layer\n        #    by matrix-multiplying it by randomly initialized weights.\n        #    Produce a 10-dimension vector with 10 scores.\n        layer_2 = np.dot(layer_1, weights_2)\n\n        # Backpropagation/backward pass:\n        # 1. Measure the training error (loss function) between the actual\n        #    image labels (the truth) and the prediction by the model.\n        training_loss += np.sum((training_labels[i] - layer_2) ** 2)\n        # 2. Increment the accurate prediction count.\n        training_accurate_predictions += int(\n            np.argmax(layer_2) == np.argmax(training_labels[i])\n        )\n        # 3. Differentiate the loss function/error.\n        layer_2_delta = training_labels[i] - layer_2\n        # 4. Propagate the gradients of the loss function back through the hidden layer.\n        layer_1_delta = np.dot(weights_2, layer_2_delta) * relu2deriv(layer_1)\n        # 5. Apply the dropout to the gradients.\n        layer_1_delta *= dropout_mask\n        # 6. Update the weights for the middle and input layers\n        #    by multiplying them by the learning rate and the gradients.\n        weights_1 += learning_rate * np.outer(layer_0, layer_1_delta)\n        weights_2 += learning_rate * np.outer(layer_1, layer_2_delta)\n\n    # Store training set losses and accurate predictions.\n    store_training_loss.append(training_loss)\n    store_training_accurate_pred.append(training_accurate_predictions)\n\n    ###################\n    # Evaluation step #\n    ###################\n\n    # Evaluate model performance on the test set at each epoch.\n\n    # Unlike the training step, the weights are not modified for each image\n    # (or batch). Therefore the model can be applied to the test images in a\n    # vectorized manner, eliminating the need to loop over each image\n    # individually:\n\n    results = relu(test_images @ weights_1) @ weights_2\n\n    # Measure the error between the actual label (truth) and prediction values.\n    test_loss = np.sum((test_labels - results) ** 2)\n\n    # Measure prediction accuracy on test set\n    test_accurate_predictions = np.sum(\n        np.argmax(results, axis=1) == np.argmax(test_labels, axis=1)\n    )\n\n    # Store test set losses and accurate predictions.\n    store_test_loss.append(test_loss)\n    store_test_accurate_pred.append(test_accurate_predictions)\n\n    # Summarize error and accuracy metrics at each epoch\n    print(\n        (\n            f\"Epoch: {j}\\n\"\n            f\"  Training set error: {training_loss / len(training_images):.3f}\\n\"\n            f\"  Training set accuracy: {training_accurate_predictions / len(training_images)}\\n\"\n            f\"  Test set error: {test_loss / len(test_images):.3f}\\n\"\n            f\"  Test set accuracy: {test_accurate_predictions / len(test_images)}\"\n        )\n    )",
                                "code"
                            ],
                            [
                                "Epoch: 0\n  Training set error: 0.898\n  Training set accuracy: 0.397\n  Test set error: 0.680\n  Test set accuracy: 0.582",
                                "code"
                            ],
                            [
                                "Epoch: 1\n  Training set error: 0.656\n  Training set accuracy: 0.633\n  Test set error: 0.607\n  Test set accuracy: 0.641",
                                "code"
                            ],
                            [
                                "Epoch: 2\n  Training set error: 0.592\n  Training set accuracy: 0.68\n  Test set error: 0.569\n  Test set accuracy: 0.679",
                                "code"
                            ],
                            [
                                "Epoch: 3\n  Training set error: 0.556\n  Training set accuracy: 0.7\n  Test set error: 0.541\n  Test set accuracy: 0.708",
                                "code"
                            ],
                            [
                                "Epoch: 4\n  Training set error: 0.534\n  Training set accuracy: 0.732\n  Test set error: 0.526\n  Test set accuracy: 0.729",
                                "code"
                            ],
                            [
                                "Epoch: 5\n  Training set error: 0.515\n  Training set accuracy: 0.715\n  Test set error: 0.500\n  Test set accuracy: 0.739",
                                "code"
                            ],
                            [
                                "Epoch: 6\n  Training set error: 0.495\n  Training set accuracy: 0.748\n  Test set error: 0.487\n  Test set accuracy: 0.753",
                                "code"
                            ],
                            [
                                "Epoch: 7\n  Training set error: 0.483\n  Training set accuracy: 0.769\n  Test set error: 0.486\n  Test set accuracy: 0.747",
                                "code"
                            ],
                            [
                                "Epoch: 8\n  Training set error: 0.473\n  Training set accuracy: 0.776\n  Test set error: 0.473\n  Test set accuracy: 0.752",
                                "code"
                            ],
                            [
                                "Epoch: 9\n  Training set error: 0.460\n  Training set accuracy: 0.788\n  Test set error: 0.462\n  Test set accuracy: 0.762",
                                "code"
                            ],
                            [
                                "Epoch: 10\n  Training set error: 0.465\n  Training set accuracy: 0.769\n  Test set error: 0.462\n  Test set accuracy: 0.767",
                                "code"
                            ],
                            [
                                "Epoch: 11\n  Training set error: 0.443\n  Training set accuracy: 0.801\n  Test set error: 0.456\n  Test set accuracy: 0.775",
                                "code"
                            ],
                            [
                                "Epoch: 12\n  Training set error: 0.448\n  Training set accuracy: 0.795\n  Test set error: 0.455\n  Test set accuracy: 0.772",
                                "code"
                            ],
                            [
                                "Epoch: 13\n  Training set error: 0.438\n  Training set accuracy: 0.787\n  Test set error: 0.453\n  Test set accuracy: 0.778",
                                "code"
                            ],
                            [
                                "Epoch: 14\n  Training set error: 0.446\n  Training set accuracy: 0.791\n  Test set error: 0.450\n  Test set accuracy: 0.779",
                                "code"
                            ],
                            [
                                "Epoch: 15\n  Training set error: 0.441\n  Training set accuracy: 0.788\n  Test set error: 0.452\n  Test set accuracy: 0.772",
                                "code"
                            ],
                            [
                                "Epoch: 16\n  Training set error: 0.437\n  Training set accuracy: 0.786\n  Test set error: 0.453\n  Test set accuracy: 0.772",
                                "code"
                            ],
                            [
                                "Epoch: 17\n  Training set error: 0.436\n  Training set accuracy: 0.794\n  Test set error: 0.449\n  Test set accuracy: 0.778",
                                "code"
                            ],
                            [
                                "Epoch: 18\n  Training set error: 0.433\n  Training set accuracy: 0.801\n  Test set error: 0.450\n  Test set accuracy: 0.774",
                                "code"
                            ],
                            [
                                "Epoch: 19\n  Training set error: 0.429\n  Training set accuracy: 0.785\n  Test set error: 0.436\n  Test set accuracy: 0.784",
                                "code"
                            ],
                            [
                                "The training process may take many minutes, depending on a number of factors, such as the processing power of the machine you are running the experiment on and the number of epochs. To reduce the waiting time, you can change the epoch (iteration) variable from 100 to a lower number, reset the runtime (which will reset the weights), and run the notebook cells again.",
                                "markdown"
                            ],
                            [
                                "After executing the cell above, you can visualize the training and test set errors and accuracy for an instance of this training process.",
                                "markdown"
                            ],
                            [
                                "# The training set metrics.\ny_training_error = [\n    store_training_loss[i] / float(len(training_images))\n    for i in range(len(store_training_loss))\n]\nx_training_error = range(1, len(store_training_loss) + 1)\ny_training_accuracy = [\n    store_training_accurate_pred[i] / float(len(training_images))\n    for i in range(len(store_training_accurate_pred))\n]\nx_training_accuracy = range(1, len(store_training_accurate_pred) + 1)\n\n# The test set metrics.\ny_test_error = [\n    store_test_loss[i] / float(len(test_images)) for i in range(len(store_test_loss))\n]\nx_test_error = range(1, len(store_test_loss) + 1)\ny_test_accuracy = [\n    store_training_accurate_pred[i] / float(len(training_images))\n    for i in range(len(store_training_accurate_pred))\n]\nx_test_accuracy = range(1, len(store_test_accurate_pred) + 1)\n\n# Display the plots.\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\naxes[0].set_title(\"Training set error, accuracy\")\naxes[0].plot(x_training_accuracy, y_training_accuracy, label=\"Training set accuracy\")\naxes[0].plot(x_training_error, y_training_error, label=\"Training set error\")\naxes[0].set_xlabel(\"Epochs\")\naxes[1].set_title(\"Test set error, accuracy\")\naxes[1].plot(x_test_accuracy, y_test_accuracy, label=\"Test set accuracy\")\naxes[1].plot(x_test_error, y_test_error, label=\"Test set error\")\naxes[1].set_xlabel(\"Epochs\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/a155f32ad6a3518292224002121b7167dd36f5c2c99a5ac7ac1d53357125a043.png\" src=\"../_images/a155f32ad6a3518292224002121b7167dd36f5c2c99a5ac7ac1d53357125a043.png\"/>",
                                "code"
                            ],
                            [
                                "<em>The training and testing error is shown above in the left and right\nplots, respectively. As the number of Epochs increases, the total error\ndecreases and the accuracy increases.</em>",
                                "markdown"
                            ],
                            [
                                "The accuracy rates that your model reaches during training and testing may be somewhat plausible but you may also find the error rates to be quite high.",
                                "markdown"
                            ],
                            [
                                "To reduce the error during training and testing, you can consider changing the simple loss function to, for example, categorical . Other possible solutions are discussed below.",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "Next steps": [
                    [
                        "You have learned how to build and train a simple feed-forward neural network from scratch using just NumPy to classify handwritten MNIST digits.",
                        "markdown"
                    ],
                    [
                        "To further enhance and optimize your neural network model, you can consider one of a mixture of the following:",
                        "markdown"
                    ],
                    [
                        "Increase the training sample size from 1,000 to a higher number (up to 60,000).",
                        "markdown"
                    ],
                    [
                        "Use .",
                        "markdown"
                    ],
                    [
                        "Alter the architecture by introducing more hidden layers to make the network .",
                        "markdown"
                    ],
                    [
                        "Combine the  loss function with a  activation function in the last layer.",
                        "markdown"
                    ],
                    [
                        "Introduce convolutional layers: replace the feedforward network with a  architecture.",
                        "markdown"
                    ],
                    [
                        "Use a higher epoch size to train longer and add more regularization techniques, such as , to prevent .",
                        "markdown"
                    ],
                    [
                        "Introduce a  for an unbiased valuation of the model fit.",
                        "markdown"
                    ],
                    [
                        "Apply  for faster and more stable training.",
                        "markdown"
                    ],
                    [
                        "Tune other parameters, such as the learning rate and hidden layer size.",
                        "markdown"
                    ],
                    [
                        "Building a neural network from scratch with NumPy is a great way to learn more about NumPy and about deep learning. However, for real-world applications you should use specialized frameworks \u2014 such as , ,  or  \u2014 that provide NumPy-like APIs, have built-in  and GPU support, and are designed for high-performance numerical computing and machine learning.",
                        "markdown"
                    ],
                    [
                        "Finally, when developing a machine learning model, you should think about potential ethical issues and apply practices to avoid or mitigate those:",
                        "markdown"
                    ],
                    [
                        "Document a trained model with a Model Card - see the  by Margaret Mitchell et al..",
                        "markdown"
                    ],
                    [
                        "Document a dataset with a Datasheet - see the ) by Timnit Gebru et al..",
                        "markdown"
                    ],
                    [
                        "Consider the impact of your model - who is affected by it, who does it benefit - see  and  by Pratyusha Kalluri.",
                        "markdown"
                    ],
                    [
                        "For more resources, see  and the .",
                        "markdown"
                    ],
                    [
                        "(Credit to  for demonstrating how to download MNIST without the use of external libraries.)",
                        "markdown"
                    ]
                ]
            }
        ],
        "X-ray image processing": [
            [
                "This tutorial demonstrates how to read and process X-ray images with NumPy,\nimageio, Matplotlib and SciPy. You will learn how to load medical images, focus\non certain parts, and visually compare them using the\n,\n,\n, and\n filters for edge\ndetection.",
                "markdown"
            ],
            [
                "X-ray image analysis can be part of your data analysis and\n\nwhen, for example, you\u2019re building an algorithm that helps\n\nas part of a \n.\nIn the healthcare industry, medical image processing and analysis is\nparticularly important when images are estimated to account for\n of all\nmedical data.",
                "markdown"
            ],
            [
                "You\u2019ll be working with radiology images from the\n\ndataset provided by the .\nChestX-ray8 contains over 100,000 de-identified X-ray images in the PNG format\nfrom more than 30,000 patients. You can find ChestX-ray8\u2019s files on NIH\u2019s public\nBox  in the /images\nfolder. (For more details, refer to the research\n\npublished at CVPR (a computer vision conference) in 2017.)",
                "markdown"
            ],
            [
                "For your convenience, a small number of PNG images have been saved to this\ntutorial\u2019s repository under tutorial-x-ray-image-processing/, since\nChestX-ray8 contains gigabytes of data and you may find it challenging to\ndownload it in batches.",
                "markdown"
            ],
            [
                "<img alt=\"A series of 9 x-ray images of the same region of a patient's chest is shown with different types of image processing filters applied to each image. Each x-ray shows different types of biological detail.\" src=\"../_images/tutorial-x-ray-image-processing.png\"/>",
                "markdown"
            ],
            {
                "Prerequisites": [
                    [
                        "The reader should have some knowledge of Python, NumPy arrays, and Matplotlib.\nTo refresh the memory, you can take the\n and Matplotlib\n tutorials,\nand the NumPy .",
                        "markdown"
                    ],
                    [
                        "The following packages are used in this tutorial:",
                        "markdown"
                    ],
                    [
                        " for reading and writing image data. The\nhealthcare industry usually works with the\n format for medical imaging and\n should be\nwell-suited for reading that format. For simplicity, in this tutorial you\u2019ll be\nworking with PNG files.",
                        "markdown"
                    ],
                    [
                        " for data visualization.",
                        "markdown"
                    ],
                    [
                        " for multi-dimensional image processing via\n.",
                        "markdown"
                    ],
                    [
                        "This tutorial can be run locally in an isolated environment, such as\n or\n.\nYou can use  to run\neach notebook cell.",
                        "markdown"
                    ]
                ]
            },
            {
                "Table of contents": [
                    [
                        "Examine an X-ray with imageio",
                        "markdown"
                    ],
                    [
                        "Combine images into a multi-dimensional array to demonstrate progression",
                        "markdown"
                    ],
                    [
                        "Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and\nCanny filters",
                        "markdown"
                    ],
                    [
                        "Apply masks to X-rays with np.where()",
                        "markdown"
                    ],
                    [
                        "Compare the results",
                        "markdown"
                    ]
                ]
            },
            {
                "Examine an X-ray with imageio": [
                    [
                        "Let\u2019s begin with a simple example using just one X-ray image from the\nChestX-ray8 dataset.",
                        "markdown"
                    ],
                    [
                        "The file \u2014 00000011_001.png \u2014 has been downloaded for you and saved in the\n/tutorial-x-ray-image-processing folder.",
                        "markdown"
                    ],
                    [
                        "<strong>1.</strong> Load the image with imageio:",
                        "markdown"
                    ],
                    [
                        "import os\nimport imageio\n\nDIR = \"tutorial-x-ray-image-processing\"\n\nxray_image = imageio.v3.imread(os.path.join(DIR, \"00000011_001.png\"))",
                        "code"
                    ],
                    [
                        "<strong>2.</strong> Check that its shape is 1024x1024 pixels and that the array is made up of\n8-bit integers:",
                        "markdown"
                    ],
                    [
                        "print(xray_image.shape)\nprint(xray_image.dtype)",
                        "code"
                    ],
                    [
                        "(1024, 1024)\nuint8",
                        "code"
                    ],
                    [
                        "<strong>3.</strong> Import matplotlib and display the image in a grayscale colormap:",
                        "markdown"
                    ],
                    [
                        "import matplotlib.pyplot as plt\n\nplt.imshow(xray_image, cmap=\"gray\")\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/5490847b3fbf08084e057be70bfdb737a09a1f14559a86ccf7408a5abd0812b0.png\" src=\"../_images/5490847b3fbf08084e057be70bfdb737a09a1f14559a86ccf7408a5abd0812b0.png\"/>",
                        "code"
                    ]
                ]
            },
            {
                "Combine images into a multidimensional array to demonstrate progression": [
                    [
                        "In the next example, instead of 1 image you\u2019ll use 9 X-ray 1024x1024-pixel\nimages from the ChestX-ray8 dataset that have been downloaded and extracted\nfrom one of the dataset files. They are numbered from ...000.png to\n...008.png and let\u2019s assume they belong to the same patient.",
                        "markdown"
                    ],
                    [
                        "<strong>1.</strong> Import NumPy, read in each of the X-rays, and create a three-dimensional\narray where the first dimension corresponds to image number:",
                        "markdown"
                    ],
                    [
                        "import numpy as np\nnum_imgs = 9\n\ncombined_xray_images_1 = np.array(\n    [imageio.v3.imread(os.path.join(DIR, f\"00000011_00{i}.png\")) for i in range(num_imgs)]\n)",
                        "code"
                    ],
                    [
                        "<strong>2.</strong> Check the shape of the new X-ray image array containing 9 stacked images:",
                        "markdown"
                    ],
                    [
                        "combined_xray_images_1.shape",
                        "code"
                    ],
                    [
                        "(9, 1024, 1024)",
                        "code"
                    ],
                    [
                        "Note that the shape in the first dimension matches num_imgs, so the\ncombined_xray_images_1 array can be interpreted as a stack of 2D images.",
                        "markdown"
                    ],
                    [
                        "<strong>3.</strong> You can now display the \u201chealth progress\u201d by plotting each of frames next\nto each other using Matplotlib:",
                        "markdown"
                    ],
                    [
                        "fig, axes = plt.subplots(nrows=1, ncols=num_imgs, figsize=(30, 30))\n\nfor img, ax in zip(combined_xray_images_1, axes):\n    ax.imshow(img, cmap='gray')\n    ax.axis('off')\n\n\n\n\n<img alt=\"../_images/c0ad74a0a105f09de501f2b79e2c983fed7f9bf2d17ba96732f2e0dd6e56d9f7.png\" src=\"../_images/c0ad74a0a105f09de501f2b79e2c983fed7f9bf2d17ba96732f2e0dd6e56d9f7.png\"/>",
                        "code"
                    ],
                    [
                        "<strong>4.</strong> In addition, it can be helpful to show the progress as an animation.\nLet\u2019s create a GIF file with imageio.mimwrite() and display the result in the\nnotebook:",
                        "markdown"
                    ],
                    [
                        "GIF_PATH = os.path.join(DIR, \"xray_image.gif\")\nimageio.mimwrite(GIF_PATH, combined_xray_images_1, format= \".gif\", fps=1)",
                        "code"
                    ],
                    [
                        "Which gives us:\n<img alt=\"An animated gif repeatedly cycles through a series of 8 x-rays, showing the same viewpoint of the patient's chest at different points in time. The patient's bones and internal organs can be visually compared from frame to frame.\" src=\"../_images/xray_image.gif\"/>",
                        "markdown"
                    ]
                ]
            },
            {
                "Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters": [
                    [
                        "When processing biomedical data, it can be useful to emphasize the 2D\n to focus on particular\nfeatures in an image. To do that, using\n can be\nparticularly helpful when detecting the change of color pixel intensity.",
                        "markdown"
                    ],
                    {
                        "The Laplace filter with Gaussian second derivatives": [
                            [
                                "Let\u2019s start with an n-dimensional\n filter\n(\u201cLaplacian-Gaussian\u201d) that uses\n second\nderivatives. This Laplacian method focuses on pixels with rapid intensity change\nin values and is combined with Gaussian smoothing to\n. Let\u2019s examine\nhow it can be useful in analyzing 2D X-ray images.",
                                "markdown"
                            ],
                            [
                                "The implementation of the Laplacian-Gaussian filter is relatively\nstraightforward: 1) import the ndimage module from SciPy; and 2) call\n\nwith a sigma (scalar) parameter, which affects the standard deviations of the\nGaussian filter (you\u2019ll use 1 in the example below):",
                                "markdown"
                            ],
                            [
                                "from scipy import ndimage\n\nxray_image_laplace_gaussian = ndimage.gaussian_laplace(xray_image, sigma=1)",
                                "code"
                            ],
                            [
                                "Display the original X-ray and the one with the Laplacian-Gaussian filter:",
                                "markdown"
                            ],
                            [
                                "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 10))\n\naxes[0].set_title(\"Original\")\naxes[0].imshow(xray_image, cmap=\"gray\")\naxes[1].set_title(\"Laplacian-Gaussian (edges)\")\naxes[1].imshow(xray_image_laplace_gaussian, cmap=\"gray\")\nfor i in axes:\n    i.axis(\"off\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/f9ee3ca99096fac8657226c65e072aea438d46085f1b0591e937158f05e4cf59.png\" src=\"../_images/f9ee3ca99096fac8657226c65e072aea438d46085f1b0591e937158f05e4cf59.png\"/>",
                                "code"
                            ]
                        ]
                    },
                    {
                        "The Gaussian gradient magnitude method": [
                            [
                                "Another method for edge detection that can be useful is the\n (gradient) filter.\nIt computes the multidimensional gradient magnitude with Gaussian derivatives\nand helps by remove\n\nimage components.",
                                "markdown"
                            ],
                            [
                                "<strong>1.</strong> Call \nwith a sigma (scalar) parameter (for standard deviations; you\u2019ll use 2 in the\nexample below):",
                                "markdown"
                            ],
                            [
                                "x_ray_image_gaussian_gradient = ndimage.gaussian_gradient_magnitude(xray_image, sigma=2)",
                                "code"
                            ],
                            [
                                "<strong>2.</strong> Display the original X-ray and the one with the Gaussian gradient filter:",
                                "markdown"
                            ],
                            [
                                "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 10))\n\naxes[0].set_title(\"Original\")\naxes[0].imshow(xray_image, cmap=\"gray\")\naxes[1].set_title(\"Gaussian gradient (edges)\")\naxes[1].imshow(x_ray_image_gaussian_gradient, cmap=\"gray\")\nfor i in axes:\n    i.axis(\"off\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/219df502b351733336051b1eb47e5866d63e2487d77bf04ac960164fbb5ed167.png\" src=\"../_images/219df502b351733336051b1eb47e5866d63e2487d77bf04ac960164fbb5ed167.png\"/>",
                                "code"
                            ]
                        ]
                    },
                    {
                        "The Sobel-Feldman operator (the Sobel filter)": [
                            [
                                "To find regions of high spatial frequency (the edges or the edge maps) along the\nhorizontal and vertical axes of a 2D X-ray image, you can use the\n\ntechnique. The Sobel filter applies two 3x3 kernel matrices \u2014 one for each axis\n\u2014 onto the X-ray through a .\nThen, these two points (gradients) are combined using the\n to\nproduce a gradient magnitude.",
                                "markdown"
                            ],
                            [
                                "<strong>1.</strong> Use the Sobel filters \u2014 ()\n\u2014 on x- and y-axes of the X-ray. Then, calculate the distance between x and\ny (with the Sobel filters applied to them) using the\n and\nNumPy\u2019s \nto obtain the magnitude. Finally, normalize the rescaled image for the pixel\nvalues to be between 0 and 255.",
                                "markdown"
                            ],
                            [
                                "follows the output_channel = 255.0 * (input_channel - min_value) / (max_value - min_value)\n. Because you\u2019re\nusing a grayscale image, you need to normalize just one channel.",
                                "markdown"
                            ],
                            [
                                "x_sobel = ndimage.sobel(xray_image, axis=0)\ny_sobel = ndimage.sobel(xray_image, axis=1)\n\nxray_image_sobel = np.hypot(x_sobel, y_sobel)\n\nxray_image_sobel *= 255.0 / np.max(xray_image_sobel)",
                                "code"
                            ],
                            [
                                "<strong>2.</strong> Change the new image array data type to the 32-bit floating-point format\nfrom float16 to \nwith Matplotlib:",
                                "markdown"
                            ],
                            [
                                "print(\"The data type - before: \", xray_image_sobel.dtype)\n\nxray_image_sobel = xray_image_sobel.astype(\"float32\")\n\nprint(\"The data type - after: \", xray_image_sobel.dtype)",
                                "code"
                            ],
                            [
                                "The data type - before:  float16\nThe data type - after:  float32",
                                "code"
                            ],
                            [
                                "<strong>3.</strong> Display the original X-ray and the one with the Sobel \u201cedge\u201d filter\napplied. Note that both the grayscale and CMRmap colormaps are used to help\nemphasize the edges:",
                                "markdown"
                            ],
                            [
                                "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 15))\n\naxes[0].set_title(\"Original\")\naxes[0].imshow(xray_image, cmap=\"gray\")\naxes[1].set_title(\"Sobel (edges) - grayscale\")\naxes[1].imshow(xray_image_sobel, cmap=\"gray\")\naxes[2].set_title(\"Sobel (edges) - CMRmap\")\naxes[2].imshow(xray_image_sobel, cmap=\"CMRmap\")\nfor i in axes:\n    i.axis(\"off\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/22e7655b5c372239084971ff47d3820119a4fc404d070d8e3d7dbb9b13ce9d16.png\" src=\"../_images/22e7655b5c372239084971ff47d3820119a4fc404d070d8e3d7dbb9b13ce9d16.png\"/>",
                                "code"
                            ]
                        ]
                    },
                    {
                        "The Canny filter": [
                            [
                                "You can also consider using another well-known filter for edge detection called\nthe .",
                                "markdown"
                            ],
                            [
                                "First, you apply a \nfilter to remove the noise in an image. In this example, you\u2019re using using the\n filter which\nsmoothens the X-ray through a \nprocess. Next, you apply the \non each of the 2 axes of the image to help detect some of the edges \u2014 this will\nresult in 2 gradient values. Similar to the Sobel filter, the Prewitt operator\nalso applies two 3x3 kernel matrices \u2014 one for each axis \u2014 onto the X-ray\nthrough a .\nIn the end, you compute the magnitude between the two gradients using the\n and\n\nthe images, as before.",
                                "markdown"
                            ],
                            [
                                "<strong>1.</strong> Use SciPy\u2019s Fourier filters \u2014 \n\u2014 with a small sigma value to remove some of the noise from the X-ray. Then,\ncalculate two gradients using .\nNext, measure the distance between the gradients using NumPy\u2019s np.hypot().\nFinally, \nthe rescaled image, as before.",
                                "markdown"
                            ],
                            [
                                "fourier_gaussian = ndimage.fourier_gaussian(xray_image, sigma=0.05)\n\nx_prewitt = ndimage.prewitt(fourier_gaussian, axis=0)\ny_prewitt = ndimage.prewitt(fourier_gaussian, axis=1)\n\nxray_image_canny = np.hypot(x_prewitt, y_prewitt)\n\nxray_image_canny *= 255.0 / np.max(xray_image_canny)\n\nprint(\"The data type - \", xray_image_canny.dtype)",
                                "code"
                            ],
                            [
                                "The data type -  float64",
                                "code"
                            ],
                            [
                                "<strong>2.</strong> Plot the original X-ray image and the ones with the edges detected with\nthe help of the Canny filter technique. The edges can be emphasized using the\nprism, nipy_spectral, and terrain Matplotlib colormaps.",
                                "markdown"
                            ],
                            [
                                "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(20, 15))\n\naxes[0].set_title(\"Original\")\naxes[0].imshow(xray_image, cmap=\"gray\")\naxes[1].set_title(\"Canny (edges) - prism\")\naxes[1].imshow(xray_image_canny, cmap=\"prism\")\naxes[2].set_title(\"Canny (edges) - nipy_spectral\")\naxes[2].imshow(xray_image_canny, cmap=\"nipy_spectral\")\naxes[3].set_title(\"Canny (edges) - terrain\")\naxes[3].imshow(xray_image_canny, cmap=\"terrain\")\nfor i in axes:\n    i.axis(\"off\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/e7ee670442f8d404bfdcfdb117f2063747c3554aad2d41cd1dbc99b51c61c9ae.png\" src=\"../_images/e7ee670442f8d404bfdcfdb117f2063747c3554aad2d41cd1dbc99b51c61c9ae.png\"/>",
                                "code"
                            ]
                        ]
                    }
                ]
            },
            {
                "Apply masks to X-rays with np.where()": [
                    [
                        "To screen out only certain pixels in X-ray images to help detect particular\nfeatures, you can apply masks with NumPy\u2019s\n\nthat returns x when True and y when False.",
                        "markdown"
                    ],
                    [
                        "Identifying regions of interest \u2014 certain sets of pixels in an image \u2014 can be\nuseful and masks serve as boolean arrays of the same shape as the original\nimage.",
                        "markdown"
                    ],
                    [
                        "<strong>1.</strong> Retrieve some basics statistics about the pixel values in the original\nX-ray image you\u2019ve been working with:",
                        "markdown"
                    ],
                    [
                        "print(\"The data type of the X-ray image is: \", xray_image.dtype)\nprint(\"The minimum pixel value is: \", np.min(xray_image))\nprint(\"The maximum pixel value is: \", np.max(xray_image))\nprint(\"The average pixel value is: \", np.mean(xray_image))\nprint(\"The median pixel value is: \", np.median(xray_image))",
                        "code"
                    ],
                    [
                        "The data type of the X-ray image is:  uint8\nThe minimum pixel value is:  0\nThe maximum pixel value is:  255\nThe average pixel value is:  172.52233219146729\nThe median pixel value is:  195.0",
                        "code"
                    ],
                    [
                        "<strong>2.</strong> The array data type is uint8 and the minimum/maximum value results\nsuggest that all 256 colors (from 0 to 255) are used in the X-ray. Let\u2019s\nvisualize the <em>pixel intensity distribution</em> of the original raw X-ray image\nwith ndimage.histogram() and Matplotlib:",
                        "markdown"
                    ],
                    [
                        "pixel_intensity_distribution = ndimage.histogram(\n    xray_image, min=np.min(xray_image), max=np.max(xray_image), bins=256\n)\n\nplt.plot(pixel_intensity_distribution)\nplt.title(\"Pixel intensity distribution\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/00c12e163a30744c62700ec414e2d514c5c4eae10256e7cd0cf87553d0f4d34c.png\" src=\"../_images/00c12e163a30744c62700ec414e2d514c5c4eae10256e7cd0cf87553d0f4d34c.png\"/>",
                        "code"
                    ],
                    [
                        "As the pixel intensity distribution suggests, there are many low (between around\n0 and 20) and very high (between around 200 and 240) pixel values.",
                        "markdown"
                    ],
                    [
                        "<strong>3.</strong> You can create different conditional masks with NumPy\u2019s np.where() \u2014\nfor example, let\u2019s have only those values of the image with the pixels exceeding\na certain threshold:",
                        "markdown"
                    ],
                    [
                        "# The threshold is \"greater than 150\"\n# Return the original image if true, `0` otherwise\nxray_image_mask_noisy = np.where(xray_image &gt; 150, xray_image, 0)\n\nplt.imshow(xray_image_mask_noisy, cmap=\"gray\")\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/976c52ef7b0c1a1d25ff407f99a98e8290bc74a7eebe2564e88373b58fc3cee1.png\" src=\"../_images/976c52ef7b0c1a1d25ff407f99a98e8290bc74a7eebe2564e88373b58fc3cee1.png\"/>",
                        "code"
                    ],
                    [
                        "# The threshold is \"greater than 150\"\n# Return `1` if true, `0` otherwise\nxray_image_mask_less_noisy = np.where(xray_image &gt; 150, 1, 0)\n\nplt.imshow(xray_image_mask_less_noisy, cmap=\"gray\")\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/2b2a2a73c09768a3e924a35f2e10a716c3171b763df137f3c1688aa222499701.png\" src=\"../_images/2b2a2a73c09768a3e924a35f2e10a716c3171b763df137f3c1688aa222499701.png\"/>",
                        "code"
                    ]
                ]
            },
            {
                "Compare the results": [
                    [
                        "Let\u2019s display some of the results of processed X-ray images you\u2019ve worked with\nso far:",
                        "markdown"
                    ],
                    [
                        "fig, axes = plt.subplots(nrows=1, ncols=9, figsize=(30, 30))\n\naxes[0].set_title(\"Original\")\naxes[0].imshow(xray_image, cmap=\"gray\")\naxes[1].set_title(\"Laplace-Gaussian (edges)\")\naxes[1].imshow(xray_image_laplace_gaussian, cmap=\"gray\")\naxes[2].set_title(\"Gaussian gradient (edges)\")\naxes[2].imshow(x_ray_image_gaussian_gradient, cmap=\"gray\")\naxes[3].set_title(\"Sobel (edges) - grayscale\")\naxes[3].imshow(xray_image_sobel, cmap=\"gray\")\naxes[4].set_title(\"Sobel (edges) - hot\")\naxes[4].imshow(xray_image_sobel, cmap=\"hot\")\naxes[5].set_title(\"Canny (edges) - prism)\")\naxes[5].imshow(xray_image_canny, cmap=\"prism\")\naxes[6].set_title(\"Canny (edges) - nipy_spectral)\")\naxes[6].imshow(xray_image_canny, cmap=\"nipy_spectral\")\naxes[7].set_title(\"Mask (&gt; 150, noisy)\")\naxes[7].imshow(xray_image_mask_noisy, cmap=\"gray\")\naxes[8].set_title(\"Mask (&gt; 150, less noisy)\")\naxes[8].imshow(xray_image_mask_less_noisy, cmap=\"gray\")\nfor i in axes:\n    i.axis(\"off\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/a181aad23123a912880c01644de71ffbbac292fb5c3338ce09893b614bed1188.png\" src=\"../_images/a181aad23123a912880c01644de71ffbbac292fb5c3338ce09893b614bed1188.png\"/>",
                        "code"
                    ]
                ]
            },
            {
                "Next steps": [
                    [
                        "If you want to use your own samples, you can use\n\nor search for various other ones on the \ndatabase. Openi contains many biomedical images and it can be especially helpful\nif you have low bandwidth and/or are restricted by the amount of data you can\ndownload.",
                        "markdown"
                    ],
                    [
                        "To learn more about image processing in the context of biomedical image data or\nsimply edge detection, you may find the following material useful:",
                        "markdown"
                    ],
                    [
                        " with Scikit-Image and pydicom (Radiology Data Quest)",
                        "markdown"
                    ],
                    [
                        " (Scipy Lecture Notes)",
                        "markdown"
                    ],
                    [
                        " (presentation, DataCamp)",
                        "markdown"
                    ],
                    [
                        " (Maker Portal)",
                        "markdown"
                    ],
                    [
                        " with deep learning (a Kaggle-hosted Jupyter notebook)",
                        "markdown"
                    ],
                    [
                        " (lecture slides, CS6670: Computer Vision, Cornell University)",
                        "markdown"
                    ],
                    [
                        " and NumPy (Towards Data Science)",
                        "markdown"
                    ],
                    [
                        " with Scikit-Image (Data Carpentry)",
                        "markdown"
                    ],
                    [
                        " (lecture slides, 16-385 Computer Vision, Carnegie Mellon University)",
                        "markdown"
                    ]
                ]
            }
        ],
        "Determining Static Equilibrium in NumPy": [
            [
                "When analyzing physical structures, it is crucial to understand the mechanics keeping them stable. Applied forces on a floor, a beam, or any other structure, create reaction forces and moments. These reactions are the structure resisting movement without breaking. In cases where structures do not move despite having forces applied to them,  states that both the acceleration and sum of forces in all directions in the system must be zero. You can represent and solve this concept with NumPy arrays.",
                "markdown"
            ],
            {
                "What you\u2019ll do:": [
                    [
                        "In this tutorial, you will use NumPy to create vectors and moments using NumPy arrays",
                        "markdown"
                    ],
                    [
                        "Solve problems involving cables and floors holding up structures",
                        "markdown"
                    ],
                    [
                        "Write NumPy matrices to isolate unkowns",
                        "markdown"
                    ],
                    [
                        "Use NumPy functions to perform linear algebra operations",
                        "markdown"
                    ]
                ]
            },
            {
                "What you\u2019ll learn:": [
                    [
                        "How to represent points, vectors, and moments with NumPy.",
                        "markdown"
                    ],
                    [
                        "How to find the ",
                        "markdown"
                    ],
                    [
                        "Using NumPy to compute matrix calculations",
                        "markdown"
                    ]
                ]
            },
            {
                "What you\u2019ll need:": [
                    [
                        "NumPy",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "imported with the following comands:",
                        "markdown"
                    ],
                    [
                        "import numpy as np\nimport matplotlib.pyplot as plt",
                        "code"
                    ],
                    [
                        "In this tutorial you will use the following NumPy tools:",
                        "markdown"
                    ],
                    [
                        " : this function determines the measure of vector magnitude",
                        "markdown"
                    ],
                    [
                        " : this function takes two matrices and produces the cross product",
                        "markdown"
                    ]
                ]
            },
            {
                "Solving equilibrium with Newton\u2019s second law": [
                    [
                        "Your model consists of a beam under a sum of forces and moments. You can start analyzing this system with Newton\u2019s second law:\n\n\\[\\sum{\\text{force}} = \\text{mass} \\times \\text{acceleration}.\\]",
                        "markdown"
                    ],
                    [
                        "In order to simplify the examples looked at, assume they are static, with acceleration \\(=0\\). Due to our system existing in three dimensions, consider forces being applied in each of these dimensions. This means that you can represent these forces as vectors. You come to the same conclusion for , which result from forces being applied a certain distance away from an object\u2019s center of mass.",
                        "markdown"
                    ],
                    [
                        "Assume that the force \\(F\\) is represented as a three-dimensional vector\n\n\\[F = (F_x, F_y, F_z)\\]",
                        "markdown"
                    ],
                    [
                        "where each of the three components represent the magnitude of the force being applied in each corresponding direction. Assume also that each component in the vector\n\n\\[r = (r_x, r_y, r_z)\\]",
                        "markdown"
                    ],
                    [
                        "is the distance between the point where each component of the force is applied and the centroid of the system. Then, the moment can be computed by\n\n\\[r \\times F = (r_x, r_y, r_z) \\times (F_x, F_y, F_z).\\]",
                        "markdown"
                    ],
                    [
                        "Start with some simple examples of force vectors",
                        "markdown"
                    ],
                    [
                        "forceA = np.array([1, 0, 0])\nforceB = np.array([0, 1, 0])\nprint(\"Force A =\", forceA)\nprint(\"Force B =\", forceB)",
                        "code"
                    ],
                    [
                        "Force A = [1 0 0]\nForce B = [0 1 0]",
                        "code"
                    ],
                    [
                        "This defines forceA as being a vector with magnitude of 1 in the \\(x\\) direction and forceB as magnitude 1 in the \\(y\\) direction.",
                        "markdown"
                    ],
                    [
                        "It may be helpful to visualize these forces in order to better understand how they interact with each other.\nMatplotlib is a library with visualization tools that can be utilized for this purpose.\nQuiver plots will be used to demonstrate , but the library can also be used for .",
                        "markdown"
                    ],
                    [
                        "fig = plt.figure()\n\nd3 = fig.add_subplot(projection=\"3d\")\n\nd3.set_xlim(-1, 1)\nd3.set_ylim(-1, 1)\nd3.set_zlim(-1, 1)\n\nx, y, z = np.array([0, 0, 0])  # defining the point of application.  Make it the origin\n\nu, v, w = forceA  # breaking the force vector into individual components\nd3.quiver(x, y, z, u, v, w, color=\"r\", label=\"forceA\")\n\nu, v, w = forceB\nd3.quiver(x, y, z, u, v, w, color=\"b\", label=\"forceB\")\n\nplt.legend()\nplt.show()\n\n\n\n\n<img alt=\"../_images/76371f22e077e6a6f6ba349058d00c6213034bd336d0032bc8c19bae94a6b60c.png\" src=\"../_images/76371f22e077e6a6f6ba349058d00c6213034bd336d0032bc8c19bae94a6b60c.png\"/>",
                        "code"
                    ],
                    [
                        "There are two forces emanating from a single point. In order to simplify this problem, you can add them together to find the sum of forces. Note that both forceA and forceB are three-dimensional vectors, represented by NumPy as arrays with three components. Because NumPy is meant to simplify and optimize operations between vectors, you can easily compute the sum of these two vectors as follows:",
                        "markdown"
                    ],
                    [
                        "forceC = forceA + forceB\nprint(\"Force C =\", forceC)",
                        "code"
                    ],
                    [
                        "Force C = [1 1 0]",
                        "code"
                    ],
                    [
                        "Force C now acts as a single force that represents both A and B.\nYou can plot it to see the result.",
                        "markdown"
                    ],
                    [
                        "fig = plt.figure()\n\nd3 = fig.add_subplot(projection=\"3d\")\n\nd3.set_xlim(-1, 1)\nd3.set_ylim(-1, 1)\nd3.set_zlim(-1, 1)\n\nx, y, z = np.array([0, 0, 0])\n\nu, v, w = forceA\nd3.quiver(x, y, z, u, v, w, color=\"r\", label=\"forceA\")\nu, v, w = forceB\nd3.quiver(x, y, z, u, v, w, color=\"b\", label=\"forceB\")\nu, v, w = forceC\nd3.quiver(x, y, z, u, v, w, color=\"g\", label=\"forceC\")\n\nplt.legend()\nplt.show()\n\n\n\n\n<img alt=\"../_images/815f2a13f3ea03fdf90492e2ed93ccfe51644b6ade838d8fa0f67e8d5e60a981.png\" src=\"../_images/815f2a13f3ea03fdf90492e2ed93ccfe51644b6ade838d8fa0f67e8d5e60a981.png\"/>",
                        "code"
                    ],
                    [
                        "However, the goal is equilibrium.\nThis means that you want your sum of forces to be \\((0, 0, 0)\\) or else your object will experience acceleration.\nTherefore, there needs to be another force that counteracts the prior ones.",
                        "markdown"
                    ],
                    [
                        "You can write this problem as \\(A+B+R=0\\), with \\(R\\) being the reaction force that solves the problem.",
                        "markdown"
                    ],
                    [
                        "In this example this would mean:\n\n\\[(1, 0, 0) + (0, 1, 0) + (R_x, R_y, R_z) = (0, 0, 0)\\]",
                        "markdown"
                    ],
                    [
                        "Broken into \\(x\\), \\(y\\), and \\(z\\) components this gives you:\n\n\\[\\begin{split}\\begin{cases}\n1+0+R_x=0\\\\\n0+1+R_y=0\\\\\n0+0+R_z=0\n\\end{cases}\\end{split}\\]",
                        "markdown"
                    ],
                    [
                        "solving for \\(R_x\\), \\(R_y\\), and \\(R_z\\) gives you a vector \\(R\\) of \\((-1, -1, 0)\\).",
                        "markdown"
                    ],
                    [
                        "If plotted, the forces seen in prior examples should be nullified.\nOnly if there is no force remaining is the system considered to be in equilibrium.",
                        "markdown"
                    ],
                    [
                        "R = np.array([-1, -1, 0])\n\nfig = plt.figure()\n\nd3.set_xlim(-1, 1)\nd3.set_ylim(-1, 1)\nd3.set_zlim(-1, 1)\n\nd3 = fig.add_subplot(projection=\"3d\")\n\nx, y, z = np.array([0, 0, 0])\n\nu, v, w = forceA + forceB + R  # add them all together for sum of forces\nd3.quiver(x, y, z, u, v, w)\n\nplt.show()\n\n\n\n\n<img alt=\"../_images/716bd88df28c59c340b26ca7e38ba4b0092ef349670d4b541a6067a0ebeb1a7c.png\" src=\"../_images/716bd88df28c59c340b26ca7e38ba4b0092ef349670d4b541a6067a0ebeb1a7c.png\"/>",
                        "code"
                    ],
                    [
                        "The empty graph signifies that there are no outlying forces. This denotes a system in equilibrium.",
                        "markdown"
                    ]
                ]
            },
            {
                "Solving Equilibrium as a sum of moments": [
                    [
                        "Next let\u2019s move to a more complicated application.\nWhen forces are not all applied at the same point, moments are created.",
                        "markdown"
                    ],
                    [
                        "Similar to forces, these moments must all sum to zero, otherwise rotational acceleration will be experienced.  Similar to the sum of forces, this creates a linear equation for each of the three coordinate directions in space.",
                        "markdown"
                    ],
                    [
                        "A simple example of this would be from a force applied to a stationary pole secured in the ground.\nThe pole does not move, so it must apply a reaction force.\nThe pole also does not rotate, so it must also be creating a reaction moment.\nSolve for both the reaction force and moments.",
                        "markdown"
                    ],
                    [
                        "Lets say a 5N force is applied perpendicularly 2m above the base of the pole.",
                        "markdown"
                    ],
                    [
                        "f = 5  # Force in newtons\nL = 2  # Length of the pole\n\nR = 0 - f\nM = 0 - f * L\nprint(\"Reaction force =\", R)\nprint(\"Reaction moment =\", M)",
                        "code"
                    ],
                    [
                        "Reaction force = -5\nReaction moment = -10",
                        "code"
                    ]
                ]
            },
            {
                "Finding values with physical properties": [
                    [
                        "Let\u2019s say that instead of a force acting perpendicularly to the beam, a force was applied to our pole through a wire that was also attached to the ground.\nGiven the tension in this cord, all you need to solve this problem are the physical locations of these objects.",
                        "markdown"
                    ],
                    [
                        "<img alt=\"Image representing the problem\" src=\"../_images/static_eqbm-fig01.png\"/>",
                        "markdown"
                    ],
                    [
                        "In response to the forces acting upon the pole, the base generated reaction forces in the x and y directions, as well as a reaction moment.",
                        "markdown"
                    ],
                    [
                        "Denote the base of the pole as the origin.\nNow, say the cord is attached to the ground 3m in the x direction and attached to the pole 2m up, in the z direction.",
                        "markdown"
                    ],
                    [
                        "Define these points in space as NumPy arrays, and then use those arrays to find directional vectors.",
                        "markdown"
                    ],
                    [
                        "poleBase = np.array([0, 0, 0])\ncordBase = np.array([3, 0, 0])\ncordConnection = np.array([0, 0, 2])\n\npoleDirection = cordConnection - poleBase\nprint(\"Pole direction =\", poleDirection)\ncordDirection = cordBase - cordConnection\nprint(\"Cord direction =\", cordDirection)",
                        "code"
                    ],
                    [
                        "Pole direction = [0 0 2]\nCord direction = [ 3  0 -2]",
                        "code"
                    ],
                    [
                        "In order to use these vectors in relation to forces you need to convert them into unit vectors.\nUnit vectors have a magnitude of one, and convey only the direction of the forces.",
                        "markdown"
                    ],
                    [
                        "cordUnit = cordDirection / np.linalg.norm(cordDirection)\nprint(\"Cord unit vector =\", cordUnit)",
                        "code"
                    ],
                    [
                        "Cord unit vector = [ 0.83205029  0.         -0.5547002 ]",
                        "code"
                    ],
                    [
                        "You can then multiply this direction with the magnitude of the force in order to find the force vector.",
                        "markdown"
                    ],
                    [
                        "Let\u2019s say the cord has a tension of 5N:",
                        "markdown"
                    ],
                    [
                        "cordTension = 5\nforceCord = cordUnit * cordTension\nprint(\"Force from the cord =\", forceCord)",
                        "code"
                    ],
                    [
                        "Force from the cord = [ 4.16025147  0.         -2.77350098]",
                        "code"
                    ],
                    [
                        "In order to find the moment you need the cross product of the force vector and the radius.",
                        "markdown"
                    ],
                    [
                        "momentCord = np.cross(forceCord, poleDirection)\nprint(\"Moment from the cord =\", momentCord)",
                        "code"
                    ],
                    [
                        "Moment from the cord = [ 0.         -8.32050294  0.        ]",
                        "code"
                    ],
                    [
                        "Now all you need to do is find the reaction force and moment.",
                        "markdown"
                    ],
                    [
                        "equilibrium = np.array([0, 0, 0])\nR = equilibrium - forceCord\nM = equilibrium - momentCord\nprint(\"Reaction force =\", R)\nprint(\"Reaction moment =\", M)",
                        "code"
                    ],
                    [
                        "Reaction force = [-4.16025147  0.          2.77350098]\nReaction moment = [0.         8.32050294 0.        ]",
                        "code"
                    ],
                    {
                        "Another Example": [
                            [
                                "Let\u2019s look at a slightly more complicated model.  In this example you will be observing a beam with two cables and an applied force.  This time you need to find both the tension in the cords and the reaction forces of the beam. <em>(Source: , Problem 4.106)</em>",
                                "markdown"
                            ],
                            [
                                "<img alt=\"image.png\" src=\"../_images/problem4.png\"/>",
                                "markdown"
                            ],
                            [
                                "Define distance <em>a</em> as 3 meters",
                                "markdown"
                            ],
                            [
                                "As before, start by defining the location of each relevant point as an array.",
                                "markdown"
                            ],
                            [
                                "A = np.array([0, 0, 0])\nB = np.array([0, 3, 0])\nC = np.array([0, 6, 0])\nD = np.array([1.5, 0, -3])\nE = np.array([1.5, 0, 3])\nF = np.array([-3, 0, 2])",
                                "code"
                            ],
                            [
                                "From these equations, you start by determining vector directions with unit vectors.",
                                "markdown"
                            ],
                            [
                                "AB = B - C\nAC = C - A\nBD = D - B\nBE = E - B\nCF = F - C\n\nUnitBD = BD / np.linalg.norm(BD)\nUnitBE = BE / np.linalg.norm(BE)\nUnitCF = CF / np.linalg.norm(CF)\n\nRadBD = np.cross(AB, UnitBD)\nRadBE = np.cross(AB, UnitBE)\nRadCF = np.cross(AC, UnitCF)",
                                "code"
                            ],
                            [
                                "This lets you represent the tension (T) and reaction (R) forces acting on the system as\n\n\\[\\begin{split}\\left[\n\\begin{array}\n~1/3 &amp; 1/3 &amp; 1 &amp; 0 &amp; 0\\\\\n-2/3 &amp; -2/3 &amp; 0 &amp; 1 &amp; 0\\\\\n-2/3 &amp; 2/3 &amp; 0 &amp; 0 &amp; 1\\\\\n\\end{array}\n\\right]\n\\left[\n\\begin{array}\n~T_{BD}\\\\\nT_{BE}\\\\\nR_{x}\\\\\nR_{y}\\\\\nR_{z}\\\\\n\\end{array}\n\\right]\n=\n\\left[\n\\begin{array}\n~195\\\\\n390\\\\\n-130\\\\\n\\end{array}\n\\right]\\end{split}\\]",
                                "markdown"
                            ],
                            [
                                "and the moments as\n\n\\[\\begin{split}\\left[\n\\begin{array}\n~2 &amp; -2\\\\\n1 &amp; 1\\\\\n\\end{array}\n\\right]\n\\left[\n\\begin{array}\n~T_{BD}\\\\\nT_{BE}\\\\\n\\end{array}\n\\right]\n=\n\\left[\n\\begin{array}\n~780\\\\\n1170\\\\\n\\end{array}\n\\right]\\end{split}\\]",
                                "markdown"
                            ],
                            [
                                "Where \\(T\\) is the tension in the respective cord and \\(R\\) is the reaction force in a respective direction. Then you just have six equations:",
                                "markdown"
                            ],
                            [
                                "\\(\\sum F_{x} = 0 = T_{BE}/3+T_{BD}/3-195+R_{x}\\)",
                                "markdown"
                            ],
                            [
                                "\\(\\sum F_{y} = 0 = (-\\frac{2}{3})T_{BE}-\\frac{2}{3}T_{BD}-390+R_{y}\\)",
                                "markdown"
                            ],
                            [
                                "\\(\\sum F_{z} = 0 = (-\\frac{2}{3})T_{BE}+\\frac{2}{3}T_{BD}+130+R_{z}\\)",
                                "markdown"
                            ],
                            [
                                "\\(\\sum M_{x} = 0 = 780+2T_{BE}-2T_{BD}\\)",
                                "markdown"
                            ],
                            [
                                "\\(\\sum M_{z} = 0 = 1170-T_{BE}-T_{BD}\\)",
                                "markdown"
                            ],
                            [
                                "You now have five unknowns with five equations, and can solve for:",
                                "markdown"
                            ],
                            [
                                "\\(\\ T_{BD} = 780N\\)",
                                "markdown"
                            ],
                            [
                                "\\(\\ T_{BE} = 390N\\)",
                                "markdown"
                            ],
                            [
                                "\\(\\ R_{x} = -195N\\)",
                                "markdown"
                            ],
                            [
                                "\\(\\ R_{y} = 1170N\\)",
                                "markdown"
                            ],
                            [
                                "\\(\\ R_{z} = 130N\\)",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "Wrapping up": [
                    [
                        "You have learned how to use arrays to represent points, forces, and moments in three dimensional space. Each entry in an array can be used to represent a physical property broken into directional components. These can then be easily manipulated with NumPy functions.",
                        "markdown"
                    ],
                    {
                        "Additional Applications": [
                            [
                                "This same process can be applied to kinetic problems or in any number of dimensions. The examples done in this tutorial assumed three dimensional problems in static equilibrium. These methods can easily be used in more varied problems. More or less dimensions require larger or smaller arrays to represent. In systems experiencing acceleration, velocity and acceleration can be similarly be represented as vectors as well.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "References": [
                            [
                                "",
                                "markdown"
                            ],
                            [
                                "",
                                "markdown"
                            ]
                        ]
                    }
                ]
            }
        ],
        "Plotting Fractals": [
            [
                "<img alt=\"Fractal picture\" src=\"../_images/fractal.png\"/>",
                "markdown"
            ],
            [
                "Fractals are beautiful, compelling mathematical forms that can be oftentimes created from a relatively simple set of instructions. In nature they can be found in various places, such as coastlines, seashells, and ferns, and even were used in creating certain types of antennas. The mathematical idea of fractals was known for quite some time, but they really began to be truly appreciated in the 1970\u2019s as advancements in computer graphics and some accidental discoveries lead researchers like  to stumble upon the truly mystifying visualizations that fractals possess.",
                "markdown"
            ],
            [
                "Today we will learn how to plot these beautiful visualizations and will start to do a bit of exploring for ourselves as we gain familiarity of the mathematics behind fractals and will use the ever powerful NumPy universal functions to perform the necessary calculations efficiently.",
                "markdown"
            ],
            {
                "What you\u2019ll do": [
                    [
                        "Write a function for plotting various Julia sets",
                        "markdown"
                    ],
                    [
                        "Create a visualization of the Mandelbrot set",
                        "markdown"
                    ],
                    [
                        "Write a function that computes Newton fractals",
                        "markdown"
                    ],
                    [
                        "Experiment with variations of general fractal types",
                        "markdown"
                    ]
                ]
            },
            {
                "What you\u2019ll learn": [
                    [
                        "A better intuition for how fractals work mathematically",
                        "markdown"
                    ],
                    [
                        "A basic understanding about NumPy universal functions and Boolean Indexing",
                        "markdown"
                    ],
                    [
                        "The basics of working with complex numbers in NumPy",
                        "markdown"
                    ],
                    [
                        "How to create your own unique fractal visualizations",
                        "markdown"
                    ]
                ]
            },
            {
                "What you\u2019ll need": [
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "make_axis_locatable function from mpl_toolkits API",
                        "markdown"
                    ],
                    [
                        "which can be imported as follows:",
                        "markdown"
                    ],
                    [
                        "import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable",
                        "code"
                    ],
                    [
                        "Some familiarity with Python, NumPy and matplotlib",
                        "markdown"
                    ],
                    [
                        "An idea of elementary mathematical functions, such as , ,  etc",
                        "markdown"
                    ],
                    [
                        "A very basic understanding of  would be useful",
                        "markdown"
                    ],
                    [
                        "Knowledge of  may be helpful",
                        "markdown"
                    ]
                ]
            },
            {
                "Warmup": [
                    [
                        "To gain some intuition for what fractals are, we will begin with an example.",
                        "markdown"
                    ],
                    [
                        "Consider the following equation:",
                        "markdown"
                    ],
                    [
                        "\\(f(z) = z^2 -1 \\)",
                        "markdown"
                    ],
                    [
                        "where z is a complex number (i.e of the form \\(a + bi\\) )",
                        "markdown"
                    ],
                    [
                        "For our convenience, we will write a Python function for it",
                        "markdown"
                    ],
                    [
                        "def f(z):\n    return np.square(z) - 1",
                        "code"
                    ],
                    [
                        "Note that the square function we used is an example of a <strong></strong>; we will come back to the significance of this decision shortly.",
                        "markdown"
                    ],
                    [
                        "To gain some intuition for the behaviour of the function, we can try plugging in some different values.",
                        "markdown"
                    ],
                    [
                        "For \\(z = 0\\), we would expect to get \\(-1\\):",
                        "markdown"
                    ],
                    [
                        "f(0)",
                        "code"
                    ],
                    [
                        "-1",
                        "code"
                    ],
                    [
                        "Since we used a universal function in our design, we can compute multiple inputs at the same time:",
                        "markdown"
                    ],
                    [
                        "z = [4, 1-0.2j, 1.6]\nf(z)",
                        "code"
                    ],
                    [
                        "array([15.  +0.j , -0.04-0.4j,  1.56+0.j ])",
                        "code"
                    ],
                    [
                        "Some values grow, some values shrink, some don\u2019t experience much change.",
                        "markdown"
                    ],
                    [
                        "To see the behaviour of the function on a larger scale, we can apply the function to a subset of the complex plane and plot the result. To create our subset (or mesh), we can make use of the  function.",
                        "markdown"
                    ],
                    [
                        "x, y = np.meshgrid(np.linspace(-10, 10, 20), np.linspace(-10, 10, 20))\nmesh = x + (1j * y)  # Make mesh of complex plane",
                        "code"
                    ],
                    [
                        "Now we will apply our function to each value contained in the mesh. Since we used a universal function in our design, this means that we can pass in the entire mesh all at once. This is extremely convenient for two reasons: It reduces the amount of code needed to be written and greatly increases the efficiency (as universal functions make use of system level C programming in their computations).",
                        "markdown"
                    ],
                    [
                        "Here we plot the absolute value (or modulus) of each element in the mesh after one \u201citeration\u201d of the function using a :",
                        "markdown"
                    ],
                    [
                        "output = np.abs(f(mesh))  # Take the absolute value of the output (for plotting)\n\nfig = plt.figure()\nax = plt.axes(projection='3d')\n\nax.scatter(x, y, output, alpha=0.2)\n\nax.set_xlabel('Real axis')\nax.set_ylabel('Imaginary axis')\nax.set_zlabel('Absolute value')\nax.set_title('One Iteration: $ f(z) = z^2 - 1$');\n\n\n\n\n<img alt=\"../_images/7a6c141a418bba32990998c7d201a565b436c3d4fa0f254d8fccd4499f26e895.png\" src=\"../_images/7a6c141a418bba32990998c7d201a565b436c3d4fa0f254d8fccd4499f26e895.png\"/>",
                        "code"
                    ],
                    [
                        "This gives us a rough idea of what one iteration of the function does. Certain areas (notably in the areas closest to \\((0,0i)\\)) remain rather small while other areas grow quite considerably. Note that we lose information about the output by taking the absolute value, but it is the only way for us to be able to make a plot.",
                        "markdown"
                    ],
                    [
                        "Let\u2019s see what happens when we apply 2 iterations to the mesh:",
                        "markdown"
                    ],
                    [
                        "output = np.abs(f(f(mesh)))\n\nax = plt.axes(projection='3d')\n\nax.scatter(x, y, output, alpha=0.2)\n\nax.set_xlabel('Real axis')\nax.set_ylabel('Imaginary axis')\nax.set_zlabel('Absolute value')\nax.set_title('Two Iterations: $ f(z) = z^2 - 1$');\n\n\n\n\n<img alt=\"../_images/7a222133db32eae7f71b9575aaf81b60e1c64824261ea8a992d3e03e3ef9be9b.png\" src=\"../_images/7a222133db32eae7f71b9575aaf81b60e1c64824261ea8a992d3e03e3ef9be9b.png\"/>",
                        "code"
                    ],
                    [
                        "Once again, we see that values around the origin remain small, and values with a larger absolute value (or modulus) \u201cexplode\u201d.",
                        "markdown"
                    ],
                    [
                        "From first impression, its behaviour appears to be normal, and may even seem mundane. Fractals tend to have more to them then what meets the eye; the exotic behavior shows itself when we begin applying more iterations.",
                        "markdown"
                    ],
                    [
                        "Consider three complex numbers:",
                        "markdown"
                    ],
                    [
                        "\\(z_1 = 0.4 + 0.4i \\),",
                        "markdown"
                    ],
                    [
                        "\\(z_2 = z_1 + 0.1\\),",
                        "markdown"
                    ],
                    [
                        "\\(z_3 = z_1 + 0.1i\\)",
                        "markdown"
                    ],
                    [
                        "Given the shape of our first two plots, we would expect that these values would remain near the origin as we apply iterations to them. Let us see what happens when we apply 10 iterations to each value:",
                        "markdown"
                    ],
                    [
                        "selected_values = np.array([0.4 + 0.4j, 0.41 + 0.4j, 0.4 + 0.41j])\nnum_iter = 9\n\noutputs = np.zeros((num_iter+1, selected_values.shape[0]), dtype=complex)\noutputs[0] = selected_values\n\nfor i in range(num_iter):\n    outputs[i+1] = f(outputs[i])  # Apply 10 iterations, save each output\n\nfig, axes = plt.subplots(1, selected_values.shape[0], figsize=(16, 6))\naxes[1].set_xlabel('Real axis')\naxes[0].set_ylabel('Imaginary axis')\n\nfor ax, data in zip(axes, outputs.T):\n    cycle = ax.scatter(data.real, data.imag, c=range(data.shape[0]), alpha=0.6)\n    ax.set_title(f'Mapping of iterations on {data[0]}')\n\nfig.colorbar(cycle, ax=axes, location=\"bottom\", label='Iteration');\n\n\n\n\n<img alt=\"../_images/0f42c59cef601c9805113d0bede6a59560a1d564c6ca29b224d80e8f39313a0e.png\" src=\"../_images/0f42c59cef601c9805113d0bede6a59560a1d564c6ca29b224d80e8f39313a0e.png\"/>",
                        "code"
                    ],
                    [
                        "To our surprise, the behaviour of the function did not come close to matching our hypothesis. This is a prime example of the chaotic behaviour fractals possess. In the first two plots, the value \u201cexploded\u201d on the last iteration, jumping way beyond the region that it was contained in previously. The third plot on the other hand remained bounded to a small region close to the origin, yielding completely different behaviour despite the tiny change in value.",
                        "markdown"
                    ],
                    [
                        "This leads us to an extremely important question: <strong>How many iterations can be applied to each value before they diverge (\u201cexplode\u201d)?</strong>",
                        "markdown"
                    ],
                    [
                        "As we saw from the first two plots, the further the values were from the origin, the faster they generally exploded. Although the behaviour is uncertain for smaller values (like \\(z_1, z_2, z_3\\)), we can assume that if a value surpasses a certain distance from the origin (say 2) that it is doomed to diverge. We will call this threshold the <strong>radius</strong>.",
                        "markdown"
                    ],
                    [
                        "This allows us to quantify the behaviour of the function for a particular value without having to perform as many computations. Once the radius is surpassed, we are allowed to stop iterating, which gives us a way of answering the question we posed. If we tally how many computations were applied before divergence, we gain insight into the behaviour of the function that would be hard to keep track of otherwise.",
                        "markdown"
                    ],
                    [
                        "Of course, we can do much better and design a function that performs the procedure on an entire mesh.",
                        "markdown"
                    ],
                    [
                        "def divergence_rate(mesh, num_iter=10, radius=2):\n\n    z = mesh.copy()\n    diverge_len = np.zeros(mesh.shape)  # Keep tally of the number of iterations\n\n    # Iterate on element if and only if |element| &lt; radius (Otherwise assume divergence)\n    for i in range(num_iter):\n        conv_mask = np.abs(z) &lt; radius\n        diverge_len[conv_mask] += 1\n        z[conv_mask] = f(z[conv_mask])\n\n    return diverge_len",
                        "code"
                    ],
                    [
                        "The behaviour of this function may look confusing at first glance, so it will help to explain some of the notation.",
                        "markdown"
                    ],
                    [
                        "Our goal is to iterate over each value in the mesh and to tally the number of iterations before the value diverges. Since some values will diverge quicker than others, we need a procedure that only iterates over values that have an absolute value that is sufficiently small enough. We also want to stop tallying values once they surpass the radius. For this, we can use <strong></strong>, a NumPy feature that when paired with universal functions is unbeatable. Boolean Indexing allows for operations to be performed conditionally on a NumPy array without having to resort to looping over and checking for each array value individually.",
                        "markdown"
                    ],
                    [
                        "In our case, we use a loop to apply iterations to our function \\(f(z) = z^2 -1 \\) and keep tally. Using Boolean indexing, we only apply the iterations to values that have an absolute value less than 2.",
                        "markdown"
                    ],
                    [
                        "With that out of the way, we can go about plotting our first fractal! We will use the  function to create a colour-coded visualization of the tallies.",
                        "markdown"
                    ],
                    [
                        "x, y = np.meshgrid(np.linspace(-2, 2, 400), np.linspace(-2, 2, 400))\nmesh = x + (1j * y)\n\noutput = divergence_rate(mesh)\n\nfig = plt.figure(figsize=(5, 5))\nax = plt.axes()\n\nax.set_title('$f(z) = z^2 -1$')\nax.set_xlabel('Real axis')\nax.set_ylabel('Imaginary axis')\n\nim = ax.imshow(output, extent=[-2, 2, -2, 2])\ndivider = make_axes_locatable(ax)\ncax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\nplt.colorbar(im, cax=cax, label='Number of iterations');\n\n\n\n\n<img alt=\"../_images/2d354fd0ad367540f3138acc05ce34b19938b301fb66cec55761b453b1fe33ef.png\" src=\"../_images/2d354fd0ad367540f3138acc05ce34b19938b301fb66cec55761b453b1fe33ef.png\"/>",
                        "code"
                    ],
                    [
                        "What this stunning visual conveys is the complexity of the function\u2019s behaviour. The yellow region represents values that remain small, while the purple region represents the divergent values. The beautiful pattern that arises on the border of the converging and diverging values is even more fascinating when you realize that it is created from such a simple function.",
                        "markdown"
                    ]
                ]
            },
            {
                "Julia set": [
                    [
                        "What we just explored was an example of a fractal visualization of a specific Julia Set.",
                        "markdown"
                    ],
                    [
                        "Consider the function \\(f(z) = z^2 + c\\) where \\(c\\) is a complex number. The <strong>filled-in Julia set</strong> of \\(c\\) is the set of all complex numbers z in which the function converges at \\(f(z)\\). Likewise, the boundary of the filled-in Julia set is what we call the <strong>Julia set</strong>. In our above visualization, we can see that the yellow region represents an approximation of the filled-in Julia set for \\(c = -1\\) and the greenish-yellow border would contain the Julia set.",
                        "markdown"
                    ],
                    [
                        "To gain access to a wider range of \u201cJulia fractals\u201d, we can write a function that allows for different values of \\(c\\) to be passed in:",
                        "markdown"
                    ],
                    [
                        "def julia(mesh, c=-1, num_iter=10, radius=2):\n\n    z = mesh.copy()\n    diverge_len = np.zeros(z.shape)\n\n    for i in range(num_iter):\n        conv_mask = np.abs(z) &lt; radius\n        z[conv_mask] = np.square(z[conv_mask]) + c\n        diverge_len[conv_mask] += 1\n\n    return diverge_len",
                        "code"
                    ],
                    [
                        "To make our lives easier, we will create a couple meshes that we will reuse throughout the rest of the examples:",
                        "markdown"
                    ],
                    [
                        "x, y = np.meshgrid(np.linspace(-1, 1, 400), np.linspace(-1, 1, 400))\nsmall_mesh = x + (1j * y)\n\nx, y = np.meshgrid(np.linspace(-2, 2, 400), np.linspace(-2, 2, 400))\nmesh = x + (1j * y)",
                        "code"
                    ],
                    [
                        "We will also write a function that we will use to create our fractal plots:",
                        "markdown"
                    ],
                    [
                        "def plot_fractal(fractal, title='Fractal', figsize=(6, 6), cmap='rainbow', extent=[-2, 2, -2, 2]):\n\n    plt.figure(figsize=figsize)\n    ax = plt.axes()\n\n    ax.set_title(f'${title}$')\n    ax.set_xlabel('Real axis')\n    ax.set_ylabel('Imaginary axis')\n\n    im = ax.imshow(fractal, extent=extent, cmap=cmap)\n    divider = make_axes_locatable(ax)\n    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n    plt.colorbar(im, cax=cax, label='Number of iterations')",
                        "code"
                    ],
                    [
                        "Using our newly defined functions, we can make a quick plot of the first fractal again:",
                        "markdown"
                    ],
                    [
                        "output = julia(mesh, num_iter=15)\nkwargs = {'title': 'f(z) = z^2 -1'}\n\nplot_fractal(output, **kwargs);\n\n\n\n\n<img alt=\"../_images/6f5ebf5f94a5bdba07a281f894d39c1f30729b755d1481c4cc3727674555ee16.png\" src=\"../_images/6f5ebf5f94a5bdba07a281f894d39c1f30729b755d1481c4cc3727674555ee16.png\"/>",
                        "code"
                    ],
                    [
                        "We also can explore some different Julia sets by experimenting with different values of \\(c\\). It can be surprising how much influence it has on the shape of the fractal.",
                        "markdown"
                    ],
                    [
                        "For example, setting \\(c = \\frac{\\pi}{10}\\) gives us a very elegant cloud shape, while setting c = \\(-\\frac{3}{4} + 0.4i\\) yields a completely different pattern.",
                        "markdown"
                    ],
                    [
                        "output = julia(mesh, c=np.pi/10, num_iter=20)\nkwargs = {'title': 'f(z) = z^2 + \\dfrac{\\pi}{10}', 'cmap': 'plasma'}\n\nplot_fractal(output, **kwargs);\n\n\n\n\n<img alt=\"../_images/5ea538e3ccf0cf0acc5f746184d99282ac5e09450cab6cde73c6106e5a8cef33.png\" src=\"../_images/5ea538e3ccf0cf0acc5f746184d99282ac5e09450cab6cde73c6106e5a8cef33.png\"/>",
                        "code"
                    ],
                    [
                        "output = julia(mesh, c=-0.75 + 0.4j, num_iter=20)\nkwargs = {'title': 'f(z) = z^2 - \\dfrac{3}{4} + 0.4i', 'cmap': 'Greens_r'}\n\nplot_fractal(output, **kwargs);\n\n\n\n\n<img alt=\"../_images/bfb120678f65f1892658c8e3121fc93f5e95b61f9b4acd4ff184d041d5903eac.png\" src=\"../_images/bfb120678f65f1892658c8e3121fc93f5e95b61f9b4acd4ff184d041d5903eac.png\"/>",
                        "code"
                    ]
                ]
            },
            {
                "Mandelbrot set": [
                    [
                        "Closely related to the Julia set is the famous <strong>Mandelbrot set</strong>, which has a slightly different definition. Once again, we define \\(f(z) = z^2 + c\\) where \\(c\\) is a complex number, but this time our focus is on our choice of \\(c\\). We say that \\(c\\) is an element of the Mandelbrot set if f converges at \\(z = 0\\). An equivalent definition is to say that \\(c\\) is an element of the Mandelbrot set if \\(f(c)\\) can be iterated infinitely and not \u2018explode\u2019. We will tweak our Julia function slightly (and rename it appropriately) so that we can plot a visualization of the Mandelbrot set, which possesses an elegant fractal pattern.",
                        "markdown"
                    ],
                    [
                        "def mandelbrot(mesh, num_iter=10, radius=2):\n\n    c = mesh.copy()\n    z = np.zeros(mesh.shape, dtype=np.complex128)\n    diverge_len = np.zeros(z.shape)\n\n    for i in range(num_iter):\n        conv_mask = np.abs(z) &lt; radius\n        z[conv_mask] = np.square(z[conv_mask]) + c[conv_mask]\n        diverge_len[conv_mask] += 1\n\n    return diverge_len",
                        "code"
                    ],
                    [
                        "output = mandelbrot(mesh, num_iter=50)\nkwargs = {'title': 'Mandelbrot \\ set', 'cmap': 'hot'}\n\nplot_fractal(output, **kwargs);\n\n\n\n\n<img alt=\"../_images/20dd449f2a6134d4842d5337133c4150c22d59046f8a6076cf707296b245644c.png\" src=\"../_images/20dd449f2a6134d4842d5337133c4150c22d59046f8a6076cf707296b245644c.png\"/>",
                        "code"
                    ]
                ]
            },
            {
                "Generalizing the Julia set": [
                    [
                        "We can generalize our Julia function even further by giving it a parameter for which universal function we would like to pass in. This would allow us to plot fractals of the form \\(f(z) = g(z) + c\\) where g is a universal function selected by us.",
                        "markdown"
                    ],
                    [
                        "def general_julia(mesh, c=-1, f=np.square, num_iter=100, radius=2):\n\n    z = mesh.copy()\n    diverge_len = np.zeros(z.shape)\n\n    for i in range(num_iter):\n        conv_mask = np.abs(z) &lt; radius\n        z[conv_mask] = f(z[conv_mask]) + c\n        diverge_len[conv_mask] += 1\n\n    return diverge_len",
                        "code"
                    ],
                    [
                        "One cool set of fractals that can be plotted using our general Julia function are ones of the form \\(f(z) = z^n + c\\) for some positive integer \\(n\\). A very cool pattern which emerges is that the number of regions that \u2018stick out\u2019 matches the degree in which we raise the function to while iterating over it.",
                        "markdown"
                    ],
                    [
                        "fig, axes = plt.subplots(2, 3, figsize=(8, 8))\nbase_degree = 2\n\nfor deg, ax in enumerate(axes.ravel()):\n    degree = base_degree + deg\n    power = lambda z: np.power(z, degree)  # Create power function for current degree\n\n    diverge_len = general_julia(mesh, f=power, num_iter=15)\n    ax.imshow(diverge_len, extent=[-2, 2, -2, 2], cmap='binary')\n    ax.set_title(f'$f(z) = z^{degree} -1$')\n\nfig.tight_layout();\n\n\n\n\n<img alt=\"../_images/73d82e6263ce699c9f5574fbe8c3a85d913e2de43ba2fe4b07e6bbfaec8cd810.png\" src=\"../_images/73d82e6263ce699c9f5574fbe8c3a85d913e2de43ba2fe4b07e6bbfaec8cd810.png\"/>",
                        "code"
                    ],
                    [
                        "Needless to say, there is a large amount of exploring that can be done by fiddling with the inputted function, value of \\(c\\), number of iterations, radius and even the density of the mesh and choice of colours.",
                        "markdown"
                    ],
                    {
                        "Newton Fractals": [
                            [
                                "Newton fractals are a specific class of fractals, where iterations involve adding or subtracting the ratio of a function (often a polynomial) and its derivative to the input values. Mathematically, it can be expressed as:",
                                "markdown"
                            ],
                            [
                                "\\(z := z - \\frac{f(z)}{f'(z)}\\)",
                                "markdown"
                            ],
                            [
                                "We will define a general version of the fractal which will allow for different variations to be plotted by passing in our functions of choice.",
                                "markdown"
                            ],
                            [
                                "def newton_fractal(mesh, f, df, num_iter=10, r=2):\n\n    z = mesh.copy()\n    diverge_len = np.zeros(z.shape)\n\n    for i in range(num_iter):\n        conv_mask = np.abs(z) &lt; r\n        pz = f(z[conv_mask])\n        dp = df(z[conv_mask])\n        z[conv_mask] = z[conv_mask] - pz/dp\n        diverge_len[conv_mask] += 1\n\n    return diverge_len",
                                "code"
                            ],
                            [
                                "Now we can experiment with some different functions. For polynomials, we can create our plots quite effortlessly using the , which has built in functionality for computing derivatives.",
                                "markdown"
                            ],
                            [
                                "For example, let\u2019s try a higher-degree polynomial:",
                                "markdown"
                            ],
                            [
                                "p = np.polynomial.Polynomial([-16, 0, 0, 0, 15, 0, 0, 0, 1])\np\n\n\n\n\n\n\\[x \\mapsto \\text{-16.0}\\color{LightGray}{ + \\text{0.0}\\,x}\\color{LightGray}{ + \\text{0.0}\\,x^{2}}\\color{LightGray}{ + \\text{0.0}\\,x^{3}} + \\text{15.0}\\,x^{4}\\color{LightGray}{ + \\text{0.0}\\,x^{5}}\\color{LightGray}{ + \\text{0.0}\\,x^{6}}\\color{LightGray}{ + \\text{0.0}\\,x^{7}} + \\text{1.0}\\,x^{8}\\]",
                                "code"
                            ],
                            [
                                "which has the derivative:",
                                "markdown"
                            ],
                            [
                                "p.deriv()\n\n\n\n\n\n\\[x \\mapsto \\color{LightGray}{\\text{0.0}}\\color{LightGray}{ + \\text{0.0}\\,x}\\color{LightGray}{ + \\text{0.0}\\,x^{2}} + \\text{60.0}\\,x^{3}\\color{LightGray}{ + \\text{0.0}\\,x^{4}}\\color{LightGray}{ + \\text{0.0}\\,x^{5}}\\color{LightGray}{ + \\text{0.0}\\,x^{6}} + \\text{8.0}\\,x^{7}\\]",
                                "code"
                            ],
                            [
                                "output = newton_fractal(mesh, p, p.deriv(), num_iter=15, r=2)\nkwargs = {'title': 'f(z) = z - \\dfrac{(z^8 + 15z^4 - 16)}{(8z^7 + 60z^3)}', 'cmap': 'copper'}\n\nplot_fractal(output, **kwargs)\n\n\n\n\n<img alt=\"../_images/a48a6fcf739b31c8b3af199ca2f5dc75c0eb09a8a51543af4a9c037a9cd59be7.png\" src=\"../_images/a48a6fcf739b31c8b3af199ca2f5dc75c0eb09a8a51543af4a9c037a9cd59be7.png\"/>",
                                "code"
                            ],
                            [
                                "Beautiful! Let\u2019s try another one:",
                                "markdown"
                            ],
                            [
                                "f(z) = \\(tan^2(z)\\)",
                                "markdown"
                            ],
                            [
                                "\\(\\frac{df}{dz} = 2 \\cdot tan(z) sec^2(z) =\\frac{2 \\cdot tan(z)}{cos^2(z)}\\)",
                                "markdown"
                            ],
                            [
                                "This makes \\(\\frac{f(z)}{f'(z)} =  tan^2(z) \\cdot \\frac{cos^2(z)}{2 \\cdot tan(z)} = \\frac{tan(z)\\cdot cos^2(z)}{2} = \\frac{sin(z)\\cdot cos(z)}{2}\\)",
                                "markdown"
                            ],
                            [
                                "def f_tan(z):\n    return np.square(np.tan(z))\n\n\ndef d_tan(z):\n    return 2*np.tan(z) / np.square(np.cos(z))",
                                "code"
                            ],
                            [
                                "output = newton_fractal(mesh, f_tan, d_tan, num_iter=15, r=50)\nkwargs = {'title': 'f(z) = z - \\dfrac{sin(z)cos(z)}{2}', 'cmap': 'binary'}\n\nplot_fractal(output, **kwargs);\n\n\n\n\n<img alt=\"../_images/f3be0b5fb4d9cc46faffa70cb89005b898d8adbc4c2860cb2f113d61de3cee97.png\" src=\"../_images/f3be0b5fb4d9cc46faffa70cb89005b898d8adbc4c2860cb2f113d61de3cee97.png\"/>",
                                "code"
                            ],
                            [
                                "Note that you sometimes have to play with the radius in order to get a neat looking fractal.",
                                "markdown"
                            ],
                            [
                                "Finally, we can go a little bit wild with our function selection",
                                "markdown"
                            ],
                            [
                                "\\(f(z) = \\sum_{i=1}^{10} sin^i(z)\\)",
                                "markdown"
                            ],
                            [
                                "\\(\\frac{df}{dz} = \\sum_{i=1}^{10} i \\cdot sin^{i-1}(z) \\cdot cos(z)\\)",
                                "markdown"
                            ],
                            [
                                "def sin_sum(z, n=10):\n    total = np.zeros(z.size, dtype=z.dtype)\n    for i in range(1, n+1):\n        total += np.power(np.sin(z), i)\n    return total\n\n\ndef d_sin_sum(z, n=10):\n    total = np.zeros(z.size, dtype=z.dtype)\n    for i in range(1, n+1):\n        total += i * np.power(np.sin(z), i-1) * np.cos(z)\n    return total",
                                "code"
                            ],
                            [
                                "We will denote this one \u2018Wacky fractal\u2019, as its equation would not be fun to try and put in the title.",
                                "markdown"
                            ],
                            [
                                "output = newton_fractal(small_mesh, sin_sum, d_sin_sum, num_iter=10, r=1)\nkwargs = {'title': 'Wacky \\ fractal', 'figsize': (6, 6), 'extent': [-1, 1, -1, 1], 'cmap': 'terrain'}\n\nplot_fractal(output, **kwargs)\n\n\n\n\n<img alt=\"../_images/054b639a3d49da873297a98e8a06288bab3e08761da084c9251a4d4f24163e04.png\" src=\"../_images/054b639a3d49da873297a98e8a06288bab3e08761da084c9251a4d4f24163e04.png\"/>",
                                "code"
                            ],
                            [
                                "It is truly fascinating how distinct yet similar these fractals are with each other. This leads us to the final section.",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "Creating your own fractals": [
                    [
                        "What makes fractals more exciting is how much there is to explore once you become familiar with the basics. Now we will wrap up our tutorial by exploring some of the different ways one can experiment in creating unique fractals. I encourage you to try some things out on your own (if you have not done so already).",
                        "markdown"
                    ],
                    [
                        "One of the first places to experiment would be with the function for the generalized Julia set, where we can try passing in different functions as parameters.",
                        "markdown"
                    ],
                    [
                        "Let\u2019s start by choosing",
                        "markdown"
                    ],
                    [
                        "\\(f(z) = tan(z^2)\\)",
                        "markdown"
                    ],
                    [
                        "def f(z):\n    return np.tan(np.square(z))",
                        "code"
                    ],
                    [
                        "output = general_julia(mesh, f=f, num_iter=15, radius=2.1)\nkwargs = {'title': 'f(z) = tan(z^2)', 'cmap': 'gist_stern'}\n\nplot_fractal(output, **kwargs);\n\n\n\n\n<img alt=\"../_images/4f49f2bcbdd0b60c30e8d4aca75da994995a9d5cb1d4e929e9a1751e8c8bdb4f.png\" src=\"../_images/4f49f2bcbdd0b60c30e8d4aca75da994995a9d5cb1d4e929e9a1751e8c8bdb4f.png\"/>",
                        "code"
                    ],
                    [
                        "What happens if we compose our defined function inside of a sine function?",
                        "markdown"
                    ],
                    [
                        "Let\u2019s try defining",
                        "markdown"
                    ],
                    [
                        "\\(g(z) = sin(f(z)) = sin(tan(z^2))\\)",
                        "markdown"
                    ],
                    [
                        "def g(z):\n    return np.sin(f(z))",
                        "code"
                    ],
                    [
                        "output = general_julia(mesh, f=g, num_iter=15, radius=2.1)\nkwargs = {'title': 'g(z) = sin(tan(z^2))', 'cmap': 'plasma_r'}\n\nplot_fractal(output, **kwargs);\n\n\n\n\n<img alt=\"../_images/5526a4060ed968ec299d39a61bbae12254e1e2536b7ad21d8175edd94edf1bfd.png\" src=\"../_images/5526a4060ed968ec299d39a61bbae12254e1e2536b7ad21d8175edd94edf1bfd.png\"/>",
                        "code"
                    ],
                    [
                        "Next, let\u2019s create a function that applies both f and g to the inputs each iteration and adds the result together:",
                        "markdown"
                    ],
                    [
                        "\\(h(z) = f(z) + g(z) = tan(z^2) + sin(tan(z^2))\\)",
                        "markdown"
                    ],
                    [
                        "def h(z):\n    return f(z) + g(z)",
                        "code"
                    ],
                    [
                        "output = general_julia(small_mesh, f=h, num_iter=10, radius=2.1)\nkwargs = {'title': 'h(z) = tan(z^2) + sin(tan(z^2))', 'figsize': (7, 7), 'extent': [-1, 1, -1, 1], 'cmap': 'jet'}\n\nplot_fractal(output, **kwargs);\n\n\n\n\n<img alt=\"../_images/dbc0e399684e1cc4e4eeff0527f59ad37af23e35431abf58dcee0f7e2bdd370a.png\" src=\"../_images/dbc0e399684e1cc4e4eeff0527f59ad37af23e35431abf58dcee0f7e2bdd370a.png\"/>",
                        "code"
                    ],
                    [
                        "You can even create beautiful fractals through your own errors. Here is one that got created accidently by making a mistake in computing the derivative of a Newton fractal:",
                        "markdown"
                    ],
                    [
                        "def accident(z):\n    return z - (2 * np.power(np.tan(z), 2) / (np.sin(z) * np.cos(z)))",
                        "code"
                    ],
                    [
                        "output = general_julia(mesh, f=accident, num_iter=15, c=0, radius=np.pi)\nkwargs = {'title': 'Accidental \\ fractal', 'cmap': 'Blues'}\n\nplot_fractal(output, **kwargs);\n\n\n\n\n<img alt=\"../_images/352104137d4eb764db51b8dd2d5e9fd7e3f16db6117e52b1033a7e9f92f71309.png\" src=\"../_images/352104137d4eb764db51b8dd2d5e9fd7e3f16db6117e52b1033a7e9f92f71309.png\"/>",
                        "code"
                    ],
                    [
                        "Needless to say, there are a nearly endless supply of interesting fractal creations that can be made just by playing around with various combinations of NumPy universal functions and by tinkering with the parameters.",
                        "markdown"
                    ]
                ]
            },
            {
                "In conclusion": [
                    [
                        "We learned a lot about generating fractals today. We saw how complicated fractals requiring many iterations could be computed efficiently using universal functions. We also took advantage of boolean indexing, which allowed for less computations to be made without having to individually verify each value. Finally, we learned a lot about fractals themselves. As a recap:",
                        "markdown"
                    ],
                    [
                        "Fractal images are created by iterating a function over a set of values, and keeping tally of how long it takes for each value to pass a certain threshold",
                        "markdown"
                    ],
                    [
                        "The colours in the image correspond to the tally counts of the values",
                        "markdown"
                    ],
                    [
                        "The filled-in Julia set for \\(c\\) consists of all complex numbers z in which \\(f(z) = z^2 + c\\) converges",
                        "markdown"
                    ],
                    [
                        "The Julia set for \\(c\\) is the set of complex numbers that make up the boundary of the filled-in Julia set",
                        "markdown"
                    ],
                    [
                        "The Mandelbrot set is all values \\(c\\) in which \\(f(z) = z^2 + c\\) converges at 0",
                        "markdown"
                    ],
                    [
                        "Newton fractals use functions of the form \\(f(z) = z - \\frac{p(z)}{p'(z)}\\)",
                        "markdown"
                    ],
                    [
                        "The fractal images can vary as you adjust the number of iterations, radius of convergence, mesh size, colours, function choice and parameter choice",
                        "markdown"
                    ]
                ]
            },
            {
                "On your own": [
                    [
                        "Play around with the parameters of the generalized Julia set function, try playing with the constant value, number of iterations, function choice, radius, and colour choice.",
                        "markdown"
                    ],
                    [
                        "Visit the \u201cList of fractals by Hausdorff dimension\u201d Wikipedia page (linked in the Further reading section) and try writing a function for a fractal not mentioned in this tutorial.",
                        "markdown"
                    ]
                ]
            },
            {
                "Further reading": [
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Analyzing the impact of the lockdown on air quality in Delhi, India": [
            [
                "<img alt=\"A grid showing the India Gate in smog above and clear air below\" src=\"../_images/11-delhi-aqi.jpg\"/>",
                "markdown"
            ],
            {
                "What you\u2019ll do": [
                    [
                        "Calculate Air Quality Indices (AQI) and perform paired Student\u2019s t-test on them.",
                        "markdown"
                    ]
                ]
            },
            {
                "What you\u2019ll learn": [
                    [
                        "You\u2019ll learn the concept of moving averages",
                        "markdown"
                    ],
                    [
                        "You\u2019ll learn how to calculate Air Quality Index (AQI)",
                        "markdown"
                    ],
                    [
                        "You\u2019ll learn how to perform a paired Student\u2019s t-test and find the t and p values",
                        "markdown"
                    ],
                    [
                        "You\u2019ll learn how to interpret these values",
                        "markdown"
                    ]
                ]
            },
            {
                "What you\u2019ll need": [
                    [
                        " installed in your environment",
                        "markdown"
                    ],
                    [
                        "Basic understanding of statistical terms like population, sample, mean, standard deviation etc.",
                        "markdown"
                    ]
                ]
            },
            {
                "The problem of air pollution": [
                    [
                        "Air pollution is one of the most prominent types of pollution we face that has an immediate effect on our daily lives. The\nCOVID-19 pandemic resulted in lockdowns in different parts of the world; offering a rare opportunity to study the effect of\nhuman activity (or lack thereof) on air pollution. In this tutorial, we will study the air quality in Delhi, one of the\nworst affected cities by air pollution, before and during the lockdown from March to June 2020. For this, we will first compute\nthe Air Quality Index for each hour from the collected pollutant measurements. Next, we will sample these indices and perform\na  on them. It will statistically show us that the air quality improved due to the lockdown, supporting our intuition.",
                        "markdown"
                    ],
                    [
                        "Let\u2019s start by importing the necessary libraries into our environment.",
                        "markdown"
                    ],
                    [
                        "import numpy as np\nfrom numpy.random import default_rng\nfrom scipy import stats",
                        "code"
                    ]
                ]
            },
            {
                "Building the dataset": [
                    [
                        "We will use a condensed version of the  dataset. This dataset contains air quality data and AQI (Air Quality Index) at hourly and daily level of various stations across multiple cities in India. The condensed version available with this tutorial contains hourly pollutant measurements for Delhi\nfrom May 31, 2019 to June 30, 2020. It has measurements of the standard pollutants that are required for Air Quality Index calculation and a few other important ones:\nParticulate Matter (PM 2.5 and PM 10), nitrogen dioxide (NO2), ammonia (NH3), sulfur dioxide (SO2), carbon monoxide (CO), ozone (O3), oxides of nitrogen (NOx), nitric oxide (NO), benzene, toluene, and xylene.",
                        "markdown"
                    ],
                    [
                        "Let\u2019s print out the first few rows to have a glimpse of our dataset.",
                        "markdown"
                    ],
                    [
                        "! head air-quality-data.csv",
                        "code"
                    ],
                    [
                        "Datetime,PM2.5,PM10,NO2,NH3,SO2,CO,O3,NOx,NO,Benzene,Toluene,Xylene\n2019-05-31 00:00:00,103.26,305.46,94.71,31.43,30.16,3.0,18.06,178.31,152.73,13.65,83.47,2.54\n2019-05-31 01:00:00,104.47,309.14,74.66,34.08,27.02,1.69,18.65,106.5,79.98,11.35,76.79,2.91\n2019-05-31 02:00:00,90.0,314.02,48.11,32.6,18.12,0.83,28.27,48.45,25.27,5.66,32.91,1.59\n2019-05-31 03:00:00,78.01,356.14,45.45,30.21,16.78,0.79,27.47,44.22,21.5,3.6,21.41,0.78\n2019-05-31 04:00:00,80.19,372.9,45.23,28.68,16.41,0.76,26.92,44.06,22.15,4.5,23.39,0.62\n2019-05-31 05:00:00,83.59,389.97,39.49,27.71,17.42,0.76,28.71,39.33,21.04,3.25,23.59,0.56\n2019-05-31 06:00:00,79.04,371.64,39.61,26.87,16.91,0.84,29.26,43.11,24.37,3.12,15.27,0.46\n2019-05-31 07:00:00,77.32,361.88,42.63,27.26,17.86,0.96,27.07,48.22,28.81,3.32,14.42,0.41\n2019-05-31 08:00:00,84.3,377.77,42.49,28.41,20.19,0.98,33.05,48.22,27.76,3.4,14.53,0.4",
                        "code"
                    ],
                    [
                        "For the purpose of this tutorial, we are only concerned with standard pollutants required for calculating the AQI, viz., PM 2.5, PM 10, NO2, NH3, SO2, CO, and O3. So, we will only import these particular columns with . We\u2019ll then  and create two sets: pollutants_A with PM 2.5, PM 10, NO2, NH3, and SO2, and pollutants_B with CO and O3. The\ntwo sets will be processed slightly differently, as we\u2019ll see later on.",
                        "markdown"
                    ],
                    [
                        "pollutant_data = np.loadtxt(\"air-quality-data.csv\", dtype=float, delimiter=\",\",\n                            skiprows=1, usecols=range(1, 8))\npollutants_A = pollutant_data[:, 0:5]\npollutants_B = pollutant_data[:, 5:]\n\nprint(pollutants_A.shape)\nprint(pollutants_B.shape)",
                        "code"
                    ],
                    [
                        "(9528, 5)\n(9528, 2)",
                        "code"
                    ],
                    [
                        "Our dataset might contain missing values, denoted by NaN, so let\u2019s do a quick check with .",
                        "markdown"
                    ],
                    [
                        "np.all(np.isfinite(pollutant_data))",
                        "code"
                    ],
                    [
                        "True",
                        "code"
                    ],
                    [
                        "With this, we have successfully imported the data and checked that it is complete. Let\u2019s move on to the AQI calculations!",
                        "markdown"
                    ]
                ]
            },
            {
                "Calculating the Air Quality Index": [
                    [
                        "We will calculate the AQI using  adopted by the  of India.  To summarize the steps:",
                        "markdown"
                    ],
                    [
                        "Collect 24-hourly average concentration values for the standard pollutants; 8-hourly in case of CO and O3.",
                        "markdown"
                    ],
                    [
                        "Calculate the sub-indices for these pollutants with the formula:\n\n\\[\n    Ip = \\dfrac{\\text{IHi \u2013 ILo}}{\\text{BPHi \u2013 BPLo}}\\cdot{\\text{Cp \u2013 BPLo}} + \\text{ILo}\n    \\]",
                        "markdown"
                    ],
                    [
                        "Where,",
                        "markdown"
                    ],
                    [
                        "Ip = sub-index of pollutant p<br/>\nCp = averaged concentration of pollutant p<br/>\nBPHi = concentration breakpoint i.e. greater than or equal to Cp<br/>\nBPLo = concentration breakpoint i.e. less than or equal to Cp<br/>\nIHi = AQI value corresponding to BPHi<br/>\nILo = AQI value corresponding to BPLo",
                        "markdown"
                    ],
                    [
                        "The maximum sub-index at any given time is the Air Quality Index.",
                        "markdown"
                    ],
                    [
                        "The Air Quality Index is calculated with the help of breakpoint ranges as shown in the chart below.",
                        "markdown"
                    ],
                    [
                        "<img alt=\"Chart of the breakpoint ranges\" src=\"../_images/11-breakpoints.png\"/>",
                        "markdown"
                    ],
                    [
                        "Let\u2019s create two arrays to store the AQI ranges and breakpoints so that we can use them later for our calculations.",
                        "markdown"
                    ],
                    [
                        "AQI = np.array([0, 51, 101, 201, 301, 401, 501])\n\nbreakpoints = {\n    'PM2.5': np.array([0, 31, 61, 91, 121, 251]),\n    'PM10': np.array([0, 51, 101, 251, 351, 431]),\n    'NO2': np.array([0, 41, 81, 181, 281, 401]),\n    'NH3': np.array([0, 201, 401, 801, 1201, 1801]),\n    'SO2': np.array([0, 41, 81, 381, 801, 1601]),\n    'CO': np.array([0, 1.1, 2.1, 10.1, 17.1, 35]),\n    'O3': np.array([0, 51, 101, 169, 209, 749])\n}",
                        "code"
                    ],
                    {
                        "Moving averages": [
                            [
                                "For the first step, we have to compute  for pollutants_A over a window of 24 hours and pollutants_B over a\nwindow of 8 hours. We will write a simple function moving_mean using  and  to achieve this.",
                                "markdown"
                            ],
                            [
                                "To make sure both the sets are of the same length, we will truncate the pollutants_B_8hr_avg according to the length of\npollutants_A_24hr_avg. This will also ensure we have concentrations for all the pollutants over the same period of time.",
                                "markdown"
                            ],
                            [
                                "def moving_mean(a, n):\n    ret = np.cumsum(a, dtype=float, axis=0)\n    ret[n:] = ret[n:] - ret[:-n]\n    return ret[n - 1:] / n\n\npollutants_A_24hr_avg = moving_mean(pollutants_A, 24)\npollutants_B_8hr_avg = moving_mean(pollutants_B, 8)[-(pollutants_A_24hr_avg.shape[0]):]",
                                "code"
                            ],
                            [
                                "Now, we can join both sets with  to form a single data set of all the averaged concentrations. Note that we have to join our arrays column-wise so we pass the\naxis=1 parameter.",
                                "markdown"
                            ],
                            [
                                "pollutants = np.concatenate((pollutants_A_24hr_avg, pollutants_B_8hr_avg), axis=1)",
                                "code"
                            ]
                        ]
                    },
                    {
                        "Sub-indices": [
                            [
                                "The subindices for each pollutant are calculated according to the linear relationship between the AQI and standard breakpoint ranges with the formula as above:\n\n\\[\nIp = \\dfrac{\\text{IHi \u2013 ILo}}{\\text{BPHi \u2013 BPLo}}\\cdot{\\text{Cp \u2013 BPLo}} + \\text{ILo}\n\\]",
                                "markdown"
                            ],
                            [
                                "The compute_indices function first fetches the correct upper and lower bounds of AQI categories and breakpoint concentrations for the input concentration and pollutant with the help of arrays AQI and breakpoints we created above. Then, it feeds these values into the formula to calculate the sub-index.",
                                "markdown"
                            ],
                            [
                                "def compute_indices(pol, con):\n    bp = breakpoints[pol]\n    \n    if pol == 'CO':\n        inc = 0.1\n    else:\n        inc = 1\n    \n    if bp[0] &lt;= con &lt; bp[1]:\n        Bl = bp[0]\n        Bh = bp[1] - inc\n        Ih = AQI[1] - inc\n        Il = AQI[0]\n\n    elif bp[1] &lt;= con &lt; bp[2]:\n        Bl = bp[1]\n        Bh = bp[2] - inc\n        Ih = AQI[2] - inc\n        Il = AQI[1]\n\n    elif bp[2] &lt;= con &lt; bp[3]:\n        Bl = bp[2]\n        Bh = bp[3] - inc\n        Ih = AQI[3] - inc\n        Il = AQI[2]\n\n    elif bp[3] &lt;= con &lt; bp[4]:\n        Bl = bp[3]\n        Bh = bp[4] - inc\n        Ih = AQI[4] - inc\n        Il = AQI[3]\n\n    elif bp[4] &lt;= con &lt; bp[5]:\n        Bl = bp[4]\n        Bh = bp[5] - inc\n        Ih = AQI[5] - inc\n        Il = AQI[4]\n\n    elif bp[5] &lt;= con:\n        Bl = bp[5]\n        Bh = bp[5] + bp[4] - (2 * inc)\n        Ih = AQI[6]\n        Il = AQI[5]\n\n    else:\n        print(\"Concentration out of range!\")\n        \n    return ((Ih - Il) / (Bh - Bl)) * (con - Bl) + Il",
                                "code"
                            ],
                            [
                                "We will use  to utilize the concept of vectorization. This simply means we don\u2019t have loop over each element of the pollutant array ourselves.  is one of the key advantages of NumPy.",
                                "markdown"
                            ],
                            [
                                "vcompute_indices = np.vectorize(compute_indices)",
                                "code"
                            ],
                            [
                                "By calling our vectorized function vcompute_indices for each pollutant, we get the sub-indices. To get back an array with the original shape, we use .",
                                "markdown"
                            ],
                            [
                                "sub_indices = np.stack((vcompute_indices('PM2.5', pollutants[..., 0]),\n                        vcompute_indices('PM10', pollutants[..., 1]),\n                        vcompute_indices('NO2', pollutants[..., 2]),\n                        vcompute_indices('NH3', pollutants[..., 3]),\n                        vcompute_indices('SO2', pollutants[..., 4]),\n                        vcompute_indices('CO', pollutants[..., 5]),\n                        vcompute_indices('O3', pollutants[..., 6])), axis=1)",
                                "code"
                            ]
                        ]
                    },
                    {
                        "Air quality indices": [
                            [
                                "Using , we find out the maximum sub-index for each period, which is our Air Quality Index!",
                                "markdown"
                            ],
                            [
                                "aqi_array = np.max(sub_indices, axis=1)",
                                "code"
                            ],
                            [
                                "With this, we have the AQI for every hour from June 1, 2019 to June 30, 2020. Note that even though we started out with\nthe data from 31st May, we truncated that during the moving averages step.",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "Paired Student\u2019s t-test on the AQIs": [
                    [
                        "Hypothesis testing is a form of descriptive statistics used to help us make decisions with the data. From the calculated AQI data, we want to find out if there was a statistically significant difference in average AQI before and after the lockdown was imposed. We will use the left-tailed,  to compute two test statistics- the  and the . We will then compare these with the corresponding critical values to make a decision.",
                        "markdown"
                    ],
                    [
                        "<img alt=\"Normal distribution plot showing area of rejection in one-tailed test (left tailed)\" src=\"../_images/11-one-tailed-test.svg\"/>",
                        "markdown"
                    ],
                    {
                        "Sampling": [
                            [
                                "We will now import the datetime column from our original dataset into a  array. We will use this array to index the AQI array and obtain subsets of the dataset.",
                                "markdown"
                            ],
                            [
                                "datetime = np.loadtxt(\"air-quality-data.csv\", dtype='M8[h]', delimiter=\",\",\n                         skiprows=1, usecols=(0, ))[-(pollutants_A_24hr_avg.shape[0]):]",
                                "code"
                            ],
                            [
                                "Since total lockdown commenced in Delhi from March 24, 2020, the after-lockdown subset is of the period March 24, 2020 to June 30, 2020. The before-lockdown subset is for the same length of time before 24th March.",
                                "markdown"
                            ],
                            [
                                "after_lock = aqi_array[np.where(datetime &gt;= np.datetime64('2020-03-24T00'))]\n\nbefore_lock = aqi_array[np.where(datetime &lt;= np.datetime64('2020-03-21T00'))][-(after_lock.shape[0]):]\n\nprint(after_lock.shape)\nprint(before_lock.shape)",
                                "code"
                            ],
                            [
                                "(2376,)\n(2376,)",
                                "code"
                            ],
                            [
                                "To make sure our samples are <em>approximately</em> normally distributed, we take samples of size n = 30. before_sample and after_sample are the set of random observations drawn before and after the total lockdown. We use  to generate the samples.",
                                "markdown"
                            ],
                            [
                                "rng = default_rng()\n\nbefore_sample = rng.choice(before_lock, size=30, replace=False)\nafter_sample = rng.choice(after_lock, size=30, replace=False)",
                                "code"
                            ]
                        ]
                    },
                    {
                        "Defining the hypothesis": [
                            [
                                "Let us assume that there is no significant difference between the sample means before and after the lockdown. This will be the null hypothesis. The alternative hypothesis would be that there <em>is</em> a significant difference between the means and the AQI <em>improved</em>. Mathematically,",
                                "markdown"
                            ],
                            [
                                "\\(H_{0}: \\mu_\\text{after-before} = 0\\) <br/>\n\\(H_{a}: \\mu_\\text{after-before} &lt; 0\\)",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "Calculating the test statistics": [
                            [
                                "We will use the t statistic to evaluate our hypothesis and even calculate the p value from it. The formula for the t statistic is:\n\n\\[\nt = \\frac{\\mu_\\text{after-before}}{\\sqrt{\\sigma^{2}/n}}\n\\]",
                                "markdown"
                            ],
                            [
                                "where,",
                                "markdown"
                            ],
                            [
                                "\\(\\mu_\\text{after-before}\\) = mean differences of samples <br/>\n\\(\\sigma^{2}\\) = variance of mean differences <br/>\n\\(n\\) = sample size",
                                "markdown"
                            ],
                            [
                                "def t_test(x, y):\n    diff = y - x\n    var = np.var(diff, ddof=1)\n    num = np.mean(diff)\n    denom = np.sqrt(var / len(x))\n    return np.divide(num, denom)\n\nt_value = t_test(before_sample, after_sample)",
                                "code"
                            ],
                            [
                                "For the p value, we will use SciPy\u2019s stats.distributions.t.cdf() function. It takes two arguments- the t statistic and the degrees of freedom (dof). The formula for dof is n - 1.",
                                "markdown"
                            ],
                            [
                                "dof = len(before_sample) - 1\n\np_value = stats.distributions.t.cdf(t_value, dof)\n\nprint(\"The t value is {} and the p value is {}.\".format(t_value, p_value))",
                                "code"
                            ],
                            [
                                "The t value is -5.877365898842091 and the p value is 1.115499699062372e-06.",
                                "code"
                            ]
                        ]
                    }
                ]
            },
            {
                "What do the t and p values mean?": [
                    [
                        "We will now compare the calculated test statistics with the critical test statistics. The critical t value is calculated by looking up the .",
                        "markdown"
                    ],
                    [
                        "<img alt=\"Table of selected t values at different confidence levels. T value for 29 dof at 95% confidence level is highlighted with a yellow square\" src=\"../_images/11-t-table.png\"/>",
                        "markdown"
                    ],
                    [
                        "From the table above, the critical value is 1.699 for 29 dof at a confidence level of 95%. Since we are using the left tailed test, our critical value is -1.699. Clearly, the calculated t value is less than the critical value so we can safely reject the null hypothesis.",
                        "markdown"
                    ],
                    [
                        "The critical p value, denoted by \\(\\alpha\\), is usually chosen to be 0.05, corresponding to a confidence level of 95%. If the calculated p value is less than \\(\\alpha\\), then the null hypothesis can be safely rejected. Clearly, our p value is much less than \\(\\alpha\\), so we can reject the null hypothesis.",
                        "markdown"
                    ],
                    [
                        "Note that this does not mean we can accept the alternative hypothesis. It only tells us that there is not enough evidence to reject \\(H_{a}\\). In other words, we fail to reject the alternative hypothesis so, it <em>may</em> be true.",
                        "markdown"
                    ]
                ]
            },
            {
                "In practice\u2026": [
                    [
                        "The  library is preferable to use for time-series data analysis.",
                        "markdown"
                    ],
                    [
                        "The SciPy stats module provides the  function which can be used to get the t statistic and p value.",
                        "markdown"
                    ],
                    [
                        "In real life, data are generally not normally distributed. There are tests for such non-normal data like the .",
                        "markdown"
                    ]
                ]
            },
            {
                "Further reading": [
                    [
                        "There are a host of statistical tests you can choose according to the characteristics of the given data. Read more about them at\n.",
                        "markdown"
                    ],
                    [
                        "There are various versions of the  that you can adopt according to your needs.",
                        "markdown"
                    ]
                ]
            }
        ]
    },
    "Articles": {
        "Deep reinforcement learning with Pong from pixels": [
            [
                "Caution",
                "markdown"
            ],
            [
                "This article is not currently tested due to licensing/installation issues with\nthe underlying gym and atari-py dependencies.\nHelp improve this article by developing an example with reduced dependency\nfootprint!",
                "markdown"
            ],
            [
                "This tutorial demonstrates how to implement a deep reinforcement learning (RL) agent from scratch using a policy gradient method that learns to play the  video game using screen pixels as inputs with NumPy. Your Pong agent will obtain experience on the go using an  as its .",
                "markdown"
            ],
            [
                "Pong is a 2D game from 1972 where two players use \u201crackets\u201d to play a form of table tennis. Each player moves the racket up and down the screen and tries to hit a ball in their opponent\u2019s direction by touching it. The goal is to hit the ball such that it goes past the opponent\u2019s racket (they miss their shot). According to the rules, if a player reaches 21 points, they win. In Pong, the RL agent that learns to play against an opponent is displayed on the right.",
                "markdown"
            ],
            [
                "<img alt=\"Diagram showing operations detailed in this tutorial\" src=\"../_images/tutorial-deep-reinforcement-learning-with-pong-from-pixels.png\"/>",
                "markdown"
            ],
            [
                "This example is based on the  developed by  for the  in 2017 at UC Berkeley. His  from 2016 also provides more background on the mechanics and theory used in Pong RL.",
                "markdown"
            ],
            {
                "Prerequisites": [
                    [
                        "<strong>OpenAI Gym</strong>: To help with the game environment, you will use  \u2014 an open-source Python interface  that helps perform RL tasks while supporting many simulation environments.",
                        "markdown"
                    ],
                    [
                        "<strong>Python and NumPy</strong>: The reader should have some knowledge of Python, NumPy array manipulation, and linear algebra.",
                        "markdown"
                    ],
                    [
                        "<strong>Deep learning and deep RL</strong>: You should be familiar with main concepts of , which are explained in the  paper published in 2015 by Yann LeCun, Yoshua Bengio, and Geoffrey Hinton, who are regarded as some of the pioneers of the field. The tutorial will try to guide you through the main concepts of deep RL and you will find various literature with links to original sources for your convenience.",
                        "markdown"
                    ],
                    [
                        "<strong>Jupyter notebook environments</strong>: Because RL experiments can require high computing power, you can run the tutorial on the cloud for free using  or  (which offers free limited GPU and TPU acceleration).",
                        "markdown"
                    ],
                    [
                        "<strong>Matplotlib</strong>: For plotting images. Check out the  guide to set it up in your environment.",
                        "markdown"
                    ],
                    [
                        "This tutorial can also be run locally in an isolated environment, such as  and .",
                        "markdown"
                    ]
                ]
            },
            {
                "Table of contents": [
                    [
                        "A note on RL and deep RL",
                        "markdown"
                    ],
                    [
                        "Deep RL glossary",
                        "markdown"
                    ],
                    [
                        "Set up Pong",
                        "markdown"
                    ],
                    [
                        "Preprocess frames (the observation)",
                        "markdown"
                    ],
                    [
                        "Create the policy (the neural network) and the forward pass",
                        "markdown"
                    ],
                    [
                        "Set up the update step (backpropagation)",
                        "markdown"
                    ],
                    [
                        "Define the discounted rewards (expected return) function",
                        "markdown"
                    ],
                    [
                        "Train the agent for 3 episodes",
                        "markdown"
                    ],
                    [
                        "Next steps",
                        "markdown"
                    ],
                    [
                        "Appendix",
                        "markdown"
                    ],
                    [
                        "Notes on RL and deep RL",
                        "markdown"
                    ],
                    [
                        "How to set up video playback in your Jupyter notebook",
                        "markdown"
                    ],
                    {
                        "A note on RL and deep RL": [
                            [
                                "In , your agent learns from trial and error by interacting with an environment using a so-called policy to gain experience. After taking one action, the agent receives information about its reward (which it may or may not get) and the next observation of the environment. It can then proceed to take another action. This happens over a number of episodes and/or until the task is deemed to be complete.",
                                "markdown"
                            ],
                            [
                                "The agent\u2019s policy works by \u201cmapping\u201d the agent\u2019s observations to its actions \u2014 that is, assigning a presentation of what the agent observes with required actions. The overall goal is usually to optimize the agent\u2019s policy such that it maximizes the expected rewards from each observation.",
                                "markdown"
                            ],
                            [
                                "For detailed information about RL, there is an  by Richard Sutton and Andrew Barton.",
                                "markdown"
                            ],
                            [
                                "Check out the Appendix at the end of the tutorial for more information.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "Deep RL glossary": [
                            [
                                "Below is a concise glossary of deep RL terms you may find useful for the remaining part of the tutorial:",
                                "markdown"
                            ],
                            [
                                "In a finite-horizon world, such as a game of Pong, the learning agent can explore (and exploit) the <em>environment</em> over an <em>episode</em>. It usually takes many episodes for the agent to learn.",
                                "markdown"
                            ],
                            [
                                "The agent interacts with the <em>environment</em> using <em>actions</em>.",
                                "markdown"
                            ],
                            [
                                "After taking an action, the agent receives some feedback through a <em>reward</em> (if there is one), depending on which action it takes and the <em>state</em> it is in. The state contains information about the environment.",
                                "markdown"
                            ],
                            [
                                "The agent\u2019s <em>observation</em> is a partial observation of the state \u2014 this is the term this tutorial prefers (instead of <em>state</em>).",
                                "markdown"
                            ],
                            [
                                "The agent can choose an action based on cumulative <em>rewards</em> (also known as the <em>value function</em>) and the <em>policy</em>. The <em>cumulative reward function</em> estimates the quality of the observations the agent visits using its <em>policy</em>.",
                                "markdown"
                            ],
                            [
                                "The <em>policy</em> (defined by a neural network) outputs action choices (as (log) probabilities) that should maximize the cumulative rewards from the state the agent is in.",
                                "markdown"
                            ],
                            [
                                "The <em>expected return from an observation</em>, conditional to the action, is called the <em>action-value</em> function. To provide more weight to shorter-term rewards versus the longer-term ones, you usually use a <em>discount factor</em> (often a floating point number between 0.9 and 0.99).",
                                "markdown"
                            ],
                            [
                                "The sequence of actions and states (observations) during each policy \u201crun\u201d by the agent is sometimes referred to as a <em>trajectory</em> \u2014 such a sequence yields <em>rewards</em>.",
                                "markdown"
                            ],
                            [
                                "You will train your Pong agent through an \u201con-policy\u201d method using policy gradients \u2014 it\u2019s an algorithm belonging to a family of <em>policy-based</em> methods. Policy gradient methods typically update the parameters of the policy with respect to the long-term cumulative reward using  that is widely used in machine learning. And, since the goal is to maximize the function (the rewards), not minimize it, the process is also called <em>gradient ascent</em>. In other words, you use a policy for the agent to take actions and the objective is to maximize the rewards, which you do by computing the gradients and use them to update the parameters in the policy (neural) network.",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "Set up Pong": [
                    [
                        "<strong>1.</strong> First, you should install OpenAI Gym (using pip install gym[atari] - this package is currently not available on conda), and import NumPy, Gym and the necessary modules:",
                        "markdown"
                    ],
                    [
                        "import numpy as np\nimport gym",
                        "code"
                    ],
                    [
                        "Gym can monitor and save the output using the Monitor wrapper:",
                        "markdown"
                    ],
                    [
                        "from gym import wrappers\nfrom gym.wrappers import Monitor",
                        "code"
                    ],
                    [
                        "<strong>2.</strong> Instantiate a Gym environment for the game of Pong:",
                        "markdown"
                    ],
                    [
                        "env = gym.make(\"Pong-v0\")",
                        "code"
                    ],
                    [
                        "<strong>3.</strong> Let\u2019s review which actions are available in the Pong-v0 environment:",
                        "markdown"
                    ],
                    [
                        "print(env.action_space)",
                        "code"
                    ],
                    [
                        "print(env.get_action_meanings())",
                        "code"
                    ],
                    [
                        "There are 6 actions. However, LEFTFIRE is actually LEFT, RIGHTFIRE \u2014 RIGHT, and NOOP \u2014 FIRE.",
                        "markdown"
                    ],
                    [
                        "For simplicity, your policy network will have one output \u2014 a (log) probability for \u201cmoving up\u201d (indexed at 2 or RIGHT). The other available action will be indexed at 3 (\u201cmove down\u201d or LEFT).",
                        "markdown"
                    ],
                    [
                        "<strong>4.</strong> Gym can save videos of the agent\u2019s learning in an MP4 format \u2014 wrap Monitor() around the environment by running the following:",
                        "markdown"
                    ],
                    [
                        "env = Monitor(env, \"./video\", force=True)",
                        "code"
                    ],
                    [
                        "While you can perform all kinds of RL experiments in a Jupyter notebook, rendering images or videos of a Gym environment to visualize how your agent plays the game of Pong after training can be rather challenging. If you want to set up video playback in a notebook, you can find the details in the Appendix at the end of this tutorial.",
                        "markdown"
                    ]
                ]
            },
            {
                "Preprocess frames (the observation)": [
                    [
                        "In this section you will set up a function to preprocess the input data (game observation) to make it digestible for the neural network, which can only work with inputs that are in a form of tensors (multidimensional arrays) of floating-point type.",
                        "markdown"
                    ],
                    [
                        "Your agent will use the frames from the Pong game \u2014 pixels from screen frames \u2014 as input-observations for the policy network. The game observation tells the agent about where the ball is before it is fed (with a forward pass) into the neural network (the policy). This is similar to DeepMind\u2019s  method (which is further discussed in the Appendix).",
                        "markdown"
                    ],
                    [
                        "Pong screen frames are 210x160 pixels over 3 color dimensions (red, green and blue). The arrays are encoded with uint8 (or 8-bit integers), and these observations are stored on a Gym Box instance.",
                        "markdown"
                    ],
                    [
                        "<strong>1.</strong> Check the Pong\u2019s observations:",
                        "markdown"
                    ],
                    [
                        "print(env.observation_space)",
                        "code"
                    ],
                    [
                        "In Gym, the agent\u2019s actions and observations can be part of the Box (n-dimensional) or Discrete (fixed-range integers) classes.",
                        "markdown"
                    ],
                    [
                        "<strong>2.</strong> You can view a random observation \u2014 one frame \u2014 by:",
                        "markdown"
                    ],
                    [
                        "1) Setting the random `seed` before initialization (optional).\n\n2) Calling  Gym's `reset()` to reset the environment, which returns an initial observation.\n\n3) Using Matplotlib to display the `render`ed observation.",
                        "code"
                    ],
                    [
                        "(You can refer to the OpenAI Gym core  for more information about Gym\u2019s core classes and methods.)",
                        "markdown"
                    ],
                    [
                        "import matplotlib.pyplot as plt\n\nenv.seed(42)\nenv.reset()\nrandom_frame = env.render(mode=\"rgb_array\")\nprint(random_frame.shape)\nplt.imshow(random_frame)",
                        "code"
                    ],
                    [
                        "To feed the observations into the policy (neural) network, you need to convert them into 1D grayscale vectors with 6,400 (80x80x1) floating point arrays. (During training, you will use NumPy\u2019s  function to flatten these arrays.)",
                        "markdown"
                    ],
                    [
                        "<strong>3.</strong> Set up a helper function for frame (observation) preprocessing:",
                        "markdown"
                    ],
                    [
                        "def frame_preprocessing(observation_frame):\n    # Crop the frame.\n    observation_frame = observation_frame[35:195]\n    # Downsample the frame by a factor of 2.\n    observation_frame = observation_frame[::2, ::2, 0]\n    # Remove the background and apply other enhancements.\n    observation_frame[observation_frame == 144] = 0  # Erase the background (type 1).\n    observation_frame[observation_frame == 109] = 0  # Erase the background (type 2).\n    observation_frame[observation_frame != 0] = 1  # Set the items (rackets, ball) to 1.\n    # Return the preprocessed frame as a 1D floating-point array.\n    return observation_frame.astype(float)",
                        "code"
                    ],
                    [
                        "<strong>4.</strong> Preprocess the random frame from earlier to test the function \u2014 the input for the policy network is an 80x80 1D image:",
                        "markdown"
                    ],
                    [
                        "preprocessed_random_frame = frame_preprocessing(random_frame)\nplt.imshow(preprocessed_random_frame, cmap=\"gray\")\nprint(preprocessed_random_frame.shape)",
                        "code"
                    ]
                ]
            },
            {
                "Create the policy (the neural network) and the forward pass": [
                    [
                        "Next, you will define the policy as a simple feedforward network that uses a game observation as an input and outputs an action log probability:",
                        "markdown"
                    ],
                    [
                        "For the <em>input</em>, it will use the Pong video game frames \u2014 the preprocessed 1D vectors with 6,400 (80x80) floating point arrays.",
                        "markdown"
                    ],
                    [
                        "The <em>hidden layer</em> will compute the weighted sum of inputs using NumPy\u2019s dot product function  for the arrays and then apply a <em>non-linear activation function</em>, such as .",
                        "markdown"
                    ],
                    [
                        "Then, the <em>output layer</em> will perform the matrix-multiplication again of  weight parameters and the hidden layer\u2019s output (with ), and send that information through a  <em>activation function</em>.",
                        "markdown"
                    ],
                    [
                        "In the end, the policy network will output one action log probability (given that observation) for the agent \u2014 the probability for Pong action indexed in the environment at 2 (\u201cmoving the racket up\u201d).",
                        "markdown"
                    ],
                    [
                        "<strong>1.</strong> Let\u2019s instantiate certain parameters for the input, hidden, and output layers, and start setting up the network model.",
                        "markdown"
                    ],
                    [
                        "Start by creating a random number generator instance for the experiment\n(seeded for reproducibility):",
                        "markdown"
                    ],
                    [
                        "rng = np.random.default_rng(seed=12288743)",
                        "code"
                    ],
                    [
                        "Then:",
                        "markdown"
                    ],
                    [
                        "Set the input (observation) dimensionality - your preprocessed screen frames:",
                        "markdown"
                    ],
                    [
                        "D = 80 * 80",
                        "code"
                    ],
                    [
                        "Set the number of hidden layer neurons.",
                        "markdown"
                    ],
                    [
                        "H = 200",
                        "code"
                    ],
                    [
                        "Instantiate your policy (neural) network model as an empty dictionary.",
                        "markdown"
                    ],
                    [
                        "model = {}",
                        "code"
                    ],
                    [
                        "In a neural network, <em>weights</em> are important adjustable parameters that the network fine-tunes by forward and backward propagating the data.",
                        "markdown"
                    ],
                    [
                        "<strong>2.</strong> Using a technique called , set up the network model\u2019s initial weights with NumPy\u2019s  that returns random numbers over a standard Normal distribution, as well as :",
                        "markdown"
                    ],
                    [
                        "model[\"W1\"] = rng.standard_normal(size=(H, D)) / np.sqrt(D)\nmodel[\"W2\"] = rng.standard_normal(size=H) / np.sqrt(H)",
                        "code"
                    ],
                    [
                        "<strong>3.</strong> Your policy network starts by randomly initializing the weights and feeds the input data (frames) forward from the input layer through a hidden layer to the output layers. This process is called the <em>forward pass</em> or <em>forward propagation</em>, and is outlined in the function policy_forward():",
                        "markdown"
                    ],
                    [
                        "def policy_forward(x, model):\n    # Matrix-multiply the weights by the input in the one and only hidden layer.\n    h = np.dot(model[\"W1\"], x)\n    # Apply non-linearity with ReLU.\n    h[h &lt; 0] = 0\n    # Calculate the \"dot\" product in the outer layer.\n    # The input for the sigmoid function is called logit.\n    logit = np.dot(model[\"W2\"], h)\n    # Apply the sigmoid function (non-linear activation).\n    p = sigmoid(logit)\n    # Return a log probability for the action 2 (\"move up\")\n    # and the hidden \"state\" that you need for backpropagation.\n    return p, h",
                        "code"
                    ],
                    [
                        "Note that there are two <em>activation functions</em> for determining non-linear relationships between inputs and outputs. These  are applied to the output of the layers:",
                        "markdown"
                    ],
                    [
                        ": defined as h[h&lt;0] = 0 above. It returns 0 for negative inputs and the same value if it\u2019s positive.",
                        "markdown"
                    ],
                    [
                        ": defined below as sigmoid(). It \u201cwraps\u201d the last layer\u2019s output and returns an action log probability in the (0, 1) range.",
                        "markdown"
                    ],
                    [
                        "<strong>4.</strong> Define the sigmoid function separately with NumPy\u2019s  for computing exponentials:",
                        "markdown"
                    ],
                    [
                        "def sigmoid(x):\n    return 1.0 / (1.0 + np.exp(-x))",
                        "code"
                    ]
                ]
            },
            {
                "Set up the update step (backpropagation)": [
                    [
                        "During learning in your deep RL algorithm, you use the action log probabilities (given an observation) and the discounted returns (for example, +1 or -1 in Pong) and perform the <em>backward pass</em> or <em>backpropagation</em> to update the parameters \u2014 the policy network\u2019s weights.",
                        "markdown"
                    ],
                    [
                        "<strong>1.</strong> Let\u2019s define the backward pass function (policy_backward()) with the help of NumPy\u2019s modules for array multiplication \u2014  (matrix multiplication),  (outer product computation), and  (to flatten arrays into 1D arrays):",
                        "markdown"
                    ],
                    [
                        "def policy_backward(eph, epdlogp, model):\n    dW2 = np.dot(eph.T, epdlogp).ravel()\n    dh = np.outer(epdlogp, model[\"W2\"])\n    dh[eph &lt;= 0] = 0\n    dW1 = np.dot(dh.T, epx)\n    # Return new \"optimized\" weights for the policy network.\n    return {\"W1\": dW1, \"W2\": dW2}",
                        "code"
                    ],
                    [
                        "Using the intermediate hidden \u201cstates\u201d of the network (eph) and the gradients of action log probabilities (epdlogp) for an episode, the policy_backward function propagates the gradients back through the policy network and update the weights.",
                        "markdown"
                    ],
                    [
                        "<strong>2.</strong> When applying backpropagation during agent training, you will need to save several variables for each episode. Let\u2019s instantiate empty lists to store them:",
                        "markdown"
                    ],
                    [
                        "# All preprocessed observations for the episode.\nxs = []\n# All hidden \"states\" (from the network) for the episode.\nhs = []\n# All gradients of probability actions\n# (with respect to observations) for the episode.\ndlogps = []\n# All rewards for the episode.\ndrs = []",
                        "code"
                    ],
                    [
                        "You will reset these variables manually at the end of each episode during training after they are \u201cfull\u201d and reshape with NumPy\u2019s . This is demonstrated in the training stage towards the end of the tutorial.",
                        "markdown"
                    ],
                    [
                        "<strong>3.</strong> Next, to perform a gradient ascent when optimizing the agent\u2019s policy, it is common to use deep learning <em>optimizers</em> (you\u2019re performing optimization with gradients). In this example, you\u2019ll use  \u2014 an adaptive optimization . Let\u2019s set a discounting factor \u2014 a decay rate \u2014 for the optimizer:",
                        "markdown"
                    ],
                    [
                        "decay_rate = 0.99",
                        "code"
                    ],
                    [
                        "<strong>4.</strong> You will also need to store the gradients (with the help of NumPy\u2019s ) for the optimization step during training:",
                        "markdown"
                    ],
                    [
                        "First, save the update buffers that add up gradients over a batch:",
                        "markdown"
                    ],
                    [
                        "grad_buffer = {k: np.zeros_like(v) for k, v in model.items()}",
                        "code"
                    ],
                    [
                        "Second, store the RMSProp memory for the optimizer for gradient ascent:",
                        "markdown"
                    ],
                    [
                        "rmsprop_cache = {k: np.zeros_like(v) for k, v in model.items()}",
                        "code"
                    ]
                ]
            },
            {
                "Define the discounted rewards (expected return) function": [
                    [
                        "In this section, you will set up a function for computing discounted rewards (discount_rewards()) \u2014 the expected return from an observation \u2014 that uses a 1D array of rewards as inputs (with the help of NumPy\u2019s ) function.",
                        "markdown"
                    ],
                    [
                        "To provide more weight to shorter-term rewards over longer-term ones, you will use a <em>discount factor</em> (gamma) that is often a floating-point number between 0.9 and 0.99.",
                        "markdown"
                    ],
                    [
                        "gamma = 0.99\n\n\ndef discount_rewards(r, gamma):\n    discounted_r = np.zeros_like(r)\n    running_add = 0\n    # From the last reward to the first...\n    for t in reversed(range(0, r.size)):\n        # ...reset the reward sum\n        if r[t] != 0:\n            running_add = 0\n        # ...compute the discounted reward\n        running_add = running_add * gamma + r[t]\n        discounted_r[t] = running_add\n    return discounted_r",
                        "code"
                    ]
                ]
            },
            {
                "Train the agent for a number of episodes": [
                    [
                        "This section covers how to set up the training process during which your agent will be learning to play Pong using its policy.",
                        "markdown"
                    ],
                    [
                        "The pseudocode for the policy gradient method for Pong:",
                        "markdown"
                    ],
                    [
                        "Instantiate the policy \u2014 your neural network \u2014 and randomly initialize the weights in the policy network.",
                        "markdown"
                    ],
                    [
                        "Initialize a random observation.",
                        "markdown"
                    ],
                    [
                        "Randomly initialize the weights in the policy network.",
                        "markdown"
                    ],
                    [
                        "Repeat over a number of episodes:",
                        "markdown"
                    ],
                    [
                        "Input an observation into the policy network and output action probabilities for the agent (forward propagation).",
                        "markdown"
                    ],
                    [
                        "The agent takes an action for each observation, observes the received rewards and collects trajectories (over a predefined number of episodes or batch size) of state-action experiences.",
                        "markdown"
                    ],
                    [
                        "Compute the  (with a positive sign, since you need to maximize the rewards and not minimize the loss).",
                        "markdown"
                    ],
                    [
                        "For every batch of episodes:",
                        "markdown"
                    ],
                    [
                        "Calculate the gradients of your action log probabilities using the cross-entropy.",
                        "markdown"
                    ],
                    [
                        "Compute the cumulative return and, to provide more weight to shorter-term rewards versus the longer-term ones, use a discount factor discount.",
                        "markdown"
                    ],
                    [
                        "Multiply the gradients of the action log probabilities by the discounted rewards (the \u201cadvantage\u201d).",
                        "markdown"
                    ],
                    [
                        "Perform gradient ascent (backpropagation) to optimize the policy network\u2019s parameters (its weights).",
                        "markdown"
                    ],
                    [
                        "Maximize the probability of actions that lead to high rewards.",
                        "markdown"
                    ],
                    [
                        "<img alt=\"Diagram showing operations detailed in this tutorial\" src=\"../_images/tutorial-deep-reinforcement-learning-with-pong-from-pixels.png\"/>",
                        "markdown"
                    ],
                    [
                        "You can stop the training at any time or/and check saved MP4 videos of saved plays on your disk in the /video directory. You can set the maximum number of episodes that is more appropriate for your setup.",
                        "markdown"
                    ],
                    [
                        "<strong>1.</strong> For demo purposes, let\u2019s limit the number of episodes for training to 3. If you are using hardware acceleration (CPUs and GPUs), you can increase the number to 1,000 or beyond. For comparison, Andrej Karpathy\u2019s original experiment took about 8,000 episodes.",
                        "markdown"
                    ],
                    [
                        "max_episodes = 3",
                        "code"
                    ],
                    [
                        "<strong>2.</strong> Set the batch size and the learning rate values:",
                        "markdown"
                    ],
                    [
                        "The <em>batch size</em> dictates how often (in episodes) the model performs a parameter update. It is the number of times your agent can collect the state-action trajectories. At the end of the collection, you can perform the maximization of action-probability multiples.",
                        "markdown"
                    ],
                    [
                        "The  helps limit the magnitude of weight updates to prevent them from overcorrecting.",
                        "markdown"
                    ],
                    [
                        "batch_size = 3\nlearning_rate = 1e-4",
                        "code"
                    ],
                    [
                        "<strong>3.</strong> Set the game rendering default variable for Gym\u2019s render method (it is used to display the observation and is optional but can be useful during debugging):",
                        "markdown"
                    ],
                    [
                        "render = False",
                        "code"
                    ],
                    [
                        "<strong>4.</strong> Set the agent\u2019s initial (random) observation by calling reset():",
                        "markdown"
                    ],
                    [
                        "observation = env.reset()",
                        "code"
                    ],
                    [
                        "<strong>5.</strong> Initialize the previous observation:",
                        "markdown"
                    ],
                    [
                        "prev_x = None",
                        "code"
                    ],
                    [
                        "<strong>6.</strong> Initialize the reward variables and the episode count:",
                        "markdown"
                    ],
                    [
                        "running_reward = None\nreward_sum = 0\nepisode_number = 0",
                        "code"
                    ],
                    [
                        "<strong>7.</strong> To simulate motion between the frames, set the single input frame (x) for the policy network as the difference between the current and previous preprocessed frames:",
                        "markdown"
                    ],
                    [
                        "def update_input(prev_x, cur_x, D):\n    if prev_x is not None:\n        x = cur_x - prev_x\n    else:\n        x = np.zeros(D)\n    return x",
                        "code"
                    ],
                    [
                        "<strong>8.</strong> Finally, start the training loop, using the functions you have predefined:",
                        "markdown"
                    ],
                    [
                        ":tags: [output_scroll]\n\nwhile episode_number &lt; max_episodes:\n    # (For rendering.)\n    if render:\n        env.render()\n\n    # 1. Preprocess the observation (a game frame) and flatten with NumPy's `ravel()`.\n    cur_x = frame_preprocessing(observation).ravel()\n\n    # 2. Instantiate the observation for the policy network\n    x = update_input(prev_x, cur_x, D)\n    prev_x = cur_x\n\n    # 3. Perform the forward pass through the policy network using the observations\n    # (preprocessed frames as inputs) and store the action log probabilities\n    # and hidden \"states\" (for backpropagation) during the course of each episode.\n    aprob, h = policy_forward(x, model)\n    # 4. Let the action indexed at `2` (\"move up\") be that probability\n    # if it's higher than a randomly sampled value\n    # or use action `3` (\"move down\") otherwise.\n    action = 2 if rng.uniform() &lt; aprob else 3\n\n    # 5. Cache the observations and hidden \"states\" (from the network)\n    # in separate variables for backpropagation.\n    xs.append(x)\n    hs.append(h)\n\n    # 6. Compute the gradients of action log probabilities:\n    # - If the action was to \"move up\" (index `2`):\n    y = 1 if action == 2 else 0\n    # - The cross-entropy:\n    # `y*log(aprob) + (1 - y)*log(1-aprob)`\n    # or `log(aprob)` if y = 1, else: `log(1 - aprob)`.\n    # (Recall: you used the sigmoid function (`1/(1+np.exp(-x)`) to output\n    # `aprob` action probabilities.)\n    # - Then the gradient: `y - aprob`.\n    # 7. Append the gradients of your action log probabilities.\n    dlogps.append(y - aprob)\n    # 8. Take an action and update the parameters with Gym's `step()`\n    # function; obtain a new observation.\n    observation, reward, done, info = env.step(action)\n    # 9. Update the total sum of rewards.\n    reward_sum += reward\n    # 10. Append the reward for the previous action.\n    drs.append(reward)\n\n    # After an episode is finished:\n    if done:\n        episode_number += 1\n        # 11. Collect and reshape stored values with `np.vstack()` of:\n        # - Observation frames (inputs),\n        epx = np.vstack(xs)\n        # - hidden \"states\" (from the network),\n        eph = np.vstack(hs)\n        # - gradients of action log probabilities,\n        epdlogp = np.vstack(dlogps)\n        # - and received rewards for the past episode.\n        epr = np.vstack(drs)\n\n        # 12. Reset the stored variables for the new episode:\n        xs = []\n        hs = []\n        dlogps = []\n        drs = []\n\n        # 13. Discount the rewards for the past episode using the helper\n        # function you defined earlier...\n        discounted_epr = discount_rewards(epr, gamma)\n        # ...and normalize them because they have high variance\n        # (this is explained below.)\n        discounted_epr -= np.mean(discounted_epr)\n        discounted_epr /= np.std(discounted_epr)\n\n        # 14. Multiply the discounted rewards by the gradients of the action\n        # log probabilities (the \"advantage\").\n        epdlogp *= discounted_epr\n        # 15. Use the gradients to perform backpropagation and gradient ascent.\n        grad = policy_backward(eph, epdlogp, model)\n        # 16. Save the policy gradients in a buffer.\n        for k in model:\n            grad_buffer[k] += grad[k]\n        # 17. Use the RMSProp optimizer to perform the policy network\n        # parameter (weight) update at every batch size\n        # (by default: every 10 episodes).\n        if episode_number % batch_size == 0:\n            for k, v in model.items():\n                # The gradient.\n                g = grad_buffer[k]\n                # Use the RMSProp discounting factor.\n                rmsprop_cache[k] = (\n                    decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g ** 2\n                )\n                # Update the policy network with a learning rate\n                # and the RMSProp optimizer using gradient ascent\n                # (hence, there's no negative sign)\n                model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n                # Reset the gradient buffer at the end.\n                grad_buffer[k] = np.zeros_like(v)\n\n        # 18. Measure the total discounted reward.\n        running_reward = (\n            reward_sum\n            if running_reward is None\n            else running_reward * 0.99 + reward_sum * 0.01\n        )\n        print(\n            \"Resetting the Pong environment. Episode total reward: {} Running mean: {}\".format(\n                reward_sum, running_reward\n            )\n        )\n\n        # 19. Set the agent's initial observation by calling Gym's `reset()` function\n        # for the next episode and setting the reward sum back to 0.\n        reward_sum = 0\n        observation = env.reset()\n        prev_x = None\n\n    # 20. Display the output during training.\n    if reward != 0:\n        print(\n            \"Episode {}: Game finished. Reward: {}...\".format(episode_number, reward)\n            + (\"\" if reward == -1 else \" POSITIVE REWARD!\")\n        )",
                        "code"
                    ],
                    [
                        "A few notes:",
                        "markdown"
                    ],
                    [
                        "If you have previously run an experiment and want to repeat it, your Monitor instance may still be running, which may throw an error the next time you try to traini the agent. Therefore, you should first shut down Monitor by calling env.close() by uncommenting and running the cell below:",
                        "markdown"
                    ],
                    [
                        "# env.close()",
                        "code"
                    ],
                    [
                        "In Pong, if a player doesn\u2019t hit the ball back, they receive a negative reward (-1) and the other player gets a +1 reward. The rewards that the agent receives by playing Pong have a significant variance. Therefore, it\u2019s best practice to normalize them with the same mean (using ) and standard deviation (using NumPy\u2019s ).",
                        "markdown"
                    ],
                    [
                        "When using only NumPy, the deep RL training process, including backpropagation, spans several lines of code that may appear quite long. One of the main reasons for this is you\u2019re not using a deep learning framework with an automatic differentiation library that usually simplifies such experiments. This tutorial shows how to perform everything from scratch but you can also use one of many Python-based frameworks with \u201cautodiff\u201d and \u201cautograd\u201d, which you will learn about at the end of the tutorial.",
                        "markdown"
                    ]
                ]
            },
            {
                "Next steps": [
                    [
                        "You may notice that training an RL agent takes a long time if you increase the number of episodes from 100 to 500 or 1,000+, depending on the hardware \u2014 CPUs and GPUs \u2014 you are using for this task.",
                        "markdown"
                    ],
                    [
                        "Policy gradient methods can learn a task if you give them a lot of time, and optimization in RL is a challenging problem. Training agents to learn to play Pong or any other task can be sample-inefficient and require a lot of episodes. You may also notice in your training output that even after hundreds of episodes, the rewards may have high variance.",
                        "markdown"
                    ],
                    [
                        "In addition, like in many deep learning-based algorithms, you should take into account a large amount of parameters that your policy has to learn. In Pong, this number adds up to 1 million or more with 200 nodes in the hidden layer of the network and the input dimension being of size 6,400 (80x80). Therefore, adding more CPUs and GPUs to assist with training can always be an option.",
                        "markdown"
                    ],
                    [
                        "You can use a much more advanced policy gradient-based algorithm that can help speed up training, improve the sensitivity to parameters, and resolve other issues. For example, there are \u201cself-play\u201d methods, such as  developed by  et al in 2017, which were  to train the  agent over 10 months to play Dota 2 at a competitive level. Of course, if you apply these methods to smaller Gym environments, it should take hours, not months to train.",
                        "markdown"
                    ],
                    [
                        "In general, there are many RL challenges and possible solutions and you can explore some of them in  by , Sam Ritter, , Zeb Kurth-Nelson, , and  (2019).",
                        "markdown"
                    ],
                    [
                        "If you want to learn more about deep RL, you should check out the following free educational material:",
                        "markdown"
                    ],
                    [
                        ": developed by OpenAI.",
                        "markdown"
                    ],
                    [
                        "Deep RL lectures taught by practitioners at  and .",
                        "markdown"
                    ],
                    [
                        "RL  taught by  (DeepMind, UCL).",
                        "markdown"
                    ],
                    [
                        "Building a neural network from scratch with NumPy is a great way to learn more about NumPy and about deep learning. However, for real-world applications you should use specialized frameworks \u2014 such as , ,  or  \u2014 that provide NumPy-like APIs, have built-in  and GPU support, and are designed for high-performance numerical computing and machine learning.",
                        "markdown"
                    ]
                ]
            },
            {
                "Appendix": [
                    {
                        "Notes on RL and deep RL": [
                            [
                                "In  deep learning for tasks, such as image recognition, language translation, or text classification, you\u2019re more likely to use a lot of labeled data. However, in RL, agents typically don\u2019t receive direct explicit feedback indicating correct or wrong actions \u2014 they rely on other signals, such as rewards.",
                                "markdown"
                            ],
                            [
                                "<em>Deep RL</em> combines RL with . The field had its first major success in more complex environments, such as video games, in 2013 \u2014 a year after the  breakthrough in computer vision. Volodymyr Mnih and colleagues at DeepMind published a research paper called  (and  in 2015) that showed that they were able to train an agent that could play several classic games from the Arcade Learning Environment at a human-level. Their RL algorithm \u2014 called a deep Q-network (DQN) \u2014 used  in a neural network that approximated  and used .",
                                "markdown"
                            ],
                            [
                                "Unlike the simple policy gradient method that you used in this example, DQN uses a type of \u201coff-policy\u201d <em>value-based</em> method (that approximates Q learning), while the original  uses policy gradients and .",
                                "markdown"
                            ],
                            [
                                "Policy gradients <em>with function approximation</em>, such as neural networks, were  in 2000 by Richard Sutton et al. They were influenced by a number of previous works, including statistical gradient-following algorithms, such as  (Ronald Williams, 1992), as well as  (Geoffrey Hinton, 1986), which helps deep learning algorithms learn. RL with neural-network function approximation were introduced in the 1990s in research by Gerald Tesauro (, 1995), who worked with IBM on an agent that learned to  in 1992, and Long-Ji Lin (, 1993).",
                                "markdown"
                            ],
                            [
                                "Since 2013, researchers have come up with many notable approaches for learning to solve complex tasks using deep RL, such as  for the game of Go (David Silver et al, 2016),  that mastered Go, Chess, and Shogi with self-play (David Silver et al, 2017-2018),  for Dota 2 with  (OpenAI, 2019), and  for StarCraft 2 that used an  algorithm with , , and  (Oriol Vinyals et al, 2019). In addition, there have been other experiments, such as deep RL for  by engineers at Electronic Arts/DICE.",
                                "markdown"
                            ],
                            [
                                "One of the reasons why video games are popular in deep RL research is that, unlike real-world experiments, such as RL with  (  et al, 2006), virtual simulations can offer safer testing environments.",
                                "markdown"
                            ],
                            [
                                "If you\u2019re interested in learning about the implications of deep RL on other fields, such as neuroscience, you can refer to a  by  et al (2020).",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "How to set up video playback in your Jupyter notebook": [
                            [
                                "If you\u2019re using  \u2014 a free Jupyter notebook-based tool \u2014 you can set up the Docker image and add freeglut3-dev, xvfb, and x11-utils to the apt.txt configuration file to install the initial dependencies. Then, to binder/environment.yml under channels, add gym, pyvirtualdisplay and anything else you may need, such as python=3.7, pip, and jupyterlab. Check the following  for more information.",
                                "markdown"
                            ],
                            [
                                "If you\u2019re using  (another free Jupyter notebook-based tool), you can enable video playback of the game environments installing and setting up /, , , , , and other dependencies, as described further below.",
                                "markdown"
                            ],
                            [
                                "If you\u2019re using Google Colaboratory, run the following commands in the notebook cells to help with video playback:",
                                "markdown"
                            ],
                            [
                                "# Install Xvfb and X11 dependencies.\n!apt-get install -y xvfb x11-utils &gt; /dev/null 2&gt;&amp;1\n# To work with videos, install FFmpeg.\n!apt-get install -y ffmpeg &gt; /dev/null 2&gt;&amp;1\n# Install PyVirtualDisplay for visual feedback and other libraries/dependencies.\n!pip install pyvirtualdisplay PyOpenGL PyOpenGL-accelerate &gt; /dev/null 2&gt;&amp;1",
                                "code"
                            ],
                            [
                                "Then, add this Python code:",
                                "markdown"
                            ],
                            [
                                "# Import the virtual display module.\nfrom pyvirtualdisplay import Display\n# Import ipythondisplay and HTML from IPython for image and video rendering.\nfrom IPython import display as ipythondisplay\nfrom IPython.display import HTML\n\n# Initialize the virtual buffer at 400x300 (adjustable size).\n# With Xvfb, you should set `visible=False`.\ndisplay = Display(visible=False, size=(400, 300))\ndisplay.start()\n\n# Check that no display is present.\n# If no displays are present, the expected output is `:0`.\n!echo $DISPLAY\n\n# Define a helper function to display videos in Jupyter notebooks:.\n# (Source: https://star-ai.github.io/Rendering-OpenAi-Gym-in-Colaboratory/)\n\nimport sys\nimport math\nimport glob\nimport io\nimport base64\n\ndef show_any_video(mp4video=0):\n    mp4list = glob.glob('video/*.mp4')\n    if len(mp4list) &gt; 0:\n        mp4 = mp4list[mp4video]\n        video = io.open(mp4, 'r+b').read()\n        encoded = base64.b64encode(video)\n        ipythondisplay.display(HTML(data='''&lt;video alt=\"test\" autoplay\n                                            loop controls style=\"height: 400px;\"&gt;\n                                            &lt;source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /&gt;\n                                            &lt;/video&gt;'''.format(encoded.decode('ascii'))))\n\n    else:\n        print('Could not find the video!')",
                                "code"
                            ],
                            [
                                "If you want to view the last (very quick) gameplay inside a Jupyter notebook and implemented the show_any_video() function earlier, run this inside a cell:",
                                "markdown"
                            ],
                            [
                                "show_any_video(-1)",
                                "code"
                            ],
                            [
                                "If you\u2019re following the instructions in this tutorial in a local environment on Linux or macOS, you can add most of the code into one <strong>Python (.py)</strong> file. Then, you can run your Gym experiment through python your-code.py in your terminal. To enable rendering, you can use the command-line interface by following the  (make sure you have Gym and Xvfb installed, as described in the guide).",
                                "markdown"
                            ]
                        ]
                    }
                ]
            }
        ],
        "Sentiment Analysis on notable speeches of the last decade": [
            [
                "Caution",
                "markdown"
            ],
            [
                "This article is not currently tested. Help improve this tutorial by making it\nfully executable!",
                "markdown"
            ],
            [
                "This tutorial demonstrates how to build a simple  from scratch in NumPy to perform sentiment analysis on a socially relevant and ethically acquired dataset.",
                "markdown"
            ],
            [
                "Your deep learning model (the LSTM) is a form of a Recurrent Neural Network and will learn to classify a piece of text as positive or negative from the IMDB reviews dataset. The dataset contains 50,000 movie reviews and corresponding labels. Based on the numeric representations of these reviews and their corresponding labels  the neural network will be trained to learn the sentiment using forward propagation and backpropagation through time since we are dealing with sequential data here. The output will be a vector containing the probabilities that the text samples are positive.",
                "markdown"
            ],
            [
                "Today, Deep Learning is getting adopted in everyday life and now it is more important to ensure that decisions that have been taken using AI are not reflecting discriminatory behavior towards a set of populations. It is important to take fairness into consideration while consuming the output from AI. Throughout the tutorial we\u2019ll try to question all the steps in our pipeline from an ethics point of view.",
                "markdown"
            ],
            {
                "Prerequisites": [
                    [
                        "You are expected to be familiar with the Python programming language and array manipulation with NumPy. In addition, some understanding of Linear Algebra and Calculus is recommended. You should also be familiar with how Neural Networks work. For reference, you can visit the ,  and  tutorials.",
                        "markdown"
                    ],
                    [
                        "To get a refresher on Deep Learning basics, You should consider reading , which is an interactive deep learning book with multi-framework code, math, and discussions. You can also go through the  to understand how a basic neural network is implemented from scratch.",
                        "markdown"
                    ],
                    [
                        "In addition to NumPy, you will be utilizing the following Python standard modules for data loading and processing:",
                        "markdown"
                    ],
                    [
                        " for handling dataframes",
                        "markdown"
                    ],
                    [
                        " for data visualization",
                        "markdown"
                    ],
                    [
                        " to download and cache datasets",
                        "markdown"
                    ],
                    [
                        "This tutorial can be run locally in an isolated environment, such as  or . You can use  to run each notebook cell.",
                        "markdown"
                    ]
                ]
            },
            {
                "Table of contents": [
                    [
                        "Data Collection",
                        "markdown"
                    ],
                    [
                        "Preprocess the datasets",
                        "markdown"
                    ],
                    [
                        "Build and train a LSTM network from scratch",
                        "markdown"
                    ],
                    [
                        "Perform sentiment analysis on collected speeches",
                        "markdown"
                    ],
                    [
                        "Next steps",
                        "markdown"
                    ]
                ]
            },
            {
                "1. Data Collection": [
                    [
                        "Before you begin there are a few pointers you should always keep in mind before choosing the data you wish to train your model on:",
                        "markdown"
                    ],
                    [
                        "<strong>Identifying Data Bias</strong> - Bias is an inherent component of the human thought process. Therefore data sourced from human activities reflects that bias. Some ways in which this bias tends to occur in Machine Learning datasets are:",
                        "markdown"
                    ],
                    [
                        "<em>Bias in historical data</em>: Historical data are often skewed towards, or against, particular groups.\nData can also be severely imbalanced with limited information on protected groups.",
                        "markdown"
                    ],
                    [
                        "<em>Bias in data collection mechanisms</em>: Lack of representativeness introduces inherent biases in the data collection process.",
                        "markdown"
                    ],
                    [
                        "<em>Bias towards observable outcomes</em>: In some scenarios, we have the information about True Outcomes only for a certain section of the population. In the absence of information on all outcomes, one cannot even measure fairness",
                        "markdown"
                    ],
                    [
                        "<strong>Preserving human anonymity for sensitive data</strong>:  identified a list of sensitive topics that need to be handled with extra care. We present the same below along with a few additions:",
                        "markdown"
                    ],
                    [
                        "personal daily routines (including location data);",
                        "markdown"
                    ],
                    [
                        "individual details about impairment and/or medical records;",
                        "markdown"
                    ],
                    [
                        "emotional accounts of pain and chronic illness;",
                        "markdown"
                    ],
                    [
                        "financial information about income and/or welfare payments;",
                        "markdown"
                    ],
                    [
                        "discrimination and abuse episodes;",
                        "markdown"
                    ],
                    [
                        "criticism/praise of individual providers of healthcare and support services;",
                        "markdown"
                    ],
                    [
                        "suicidal thoughts;",
                        "markdown"
                    ],
                    [
                        "criticism/praise of a power structure especially if it compromises their safety;",
                        "markdown"
                    ],
                    [
                        "personally-identifying information (even if anonymized in some way) including things like fingerprints or voice.\n\n\n\n<blockquote>",
                        "markdown"
                    ],
                    [
                        "While it can be difficult taking consent from so many people especially on on-line platforms, the necessity of it depends upon the sensitivity of the topics your data includes and other indicators like whether the platform the data was obtained from allows users to operate under pseudonyms. If the website has a policy that forces the use of a real name, then the users need to be asked for consent.\n</blockquote>",
                        "markdown"
                    ],
                    [
                        "In this section, you will be collecting two different datasets: the IMDb movie reviews dataset, and a collection of 10 speeches curated for this tutorial including activists from different countries around the world, different times, and different topics. The former would be used to train the deep learning model while the latter will be used to perform sentiment analysis on.",
                        "markdown"
                    ],
                    {
                        "Collecting the IMDb reviews dataset": [
                            [
                                "IMDb Reviews Dataset is a large movie review dataset collected and prepared by Andrew L. Maas from the popular movie rating service, IMDb. The IMDb Reviews dataset is used for binary sentiment classification, whether a review is positive or negative. It contains 25,000 movie reviews for training and 25,000 for testing. All these 50,000 reviews are labeled data that may be used for supervised deep learning. For ease of reproducibility, we\u2019ll be sourcing  the data from .\n<blockquote>",
                                "markdown"
                            ],
                            [
                                "The IMDb platform allows the usage of their public datasets for personal and non-commercial use. We did our best to ensure that these reviews do not contain any of the aforementioned sensitive topics pertaining to the reviewer.\n</blockquote>",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "Collecting and loading the speech transcripts": [
                            [
                                "We have chosen speeches by activists around the globe talking about issues like climate change, feminism, lgbtqa+ rights and racism. These were sourced from newspapers, the official website of the United Nations and the archives of established universities as cited in the table below. A CSV file was created containing the transcribed speeches, their speaker and the source the speeches were obtained from.\nWe made sure to include different demographics in our data and included a range of different topics, most of which focus on social and/or ethical issues.\n\n<!-- #region -->",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "2. Preprocess the datasets\n<blockquote>": [
                    [
                        "Preprocessing data is an extremely crucial step before building any Deep learning model, however in an attempt to keep the tutorial focused on building the model, we will not dive deep into the code for preprocessing. Given below is a brief overview of all the steps we undertake to clean our data and convert it to its numeric representation.\n</blockquote>",
                        "markdown"
                    ],
                    [
                        "<strong>Text Denoising</strong> : Before converting your text into vectors, it is important to clean it and remove all unhelpful parts a.k.a the noise from your data by converting all characters to lowercase, removing html tags, brackets and stop words (words that don\u2019t add much meaning to a sentence). Without this step the dataset is often a cluster of words that the computer doesn\u2019t understand.",
                        "markdown"
                    ],
                    [
                        "<strong>Converting words to vectors</strong> : A word embedding is a learned representation for text where words that have the same meaning have a similar representation. Individual words are represented as real-valued vectors in a predefined vector space. GloVe is an unsupervised algorithm developed by Stanford for generating word embeddings by generating global word-word co-occurence matrix from a corpus. You can download the zipped files containing the embeddings from https://nlp.stanford.edu/projects/glove/. Here you can choose any of the four options for different sizes or training datasets. We have chosen the least memory consuming embedding file.\n\n<blockquote>",
                        "markdown"
                    ],
                    [
                        "The GloVe word embeddings include sets that were trained on billions of tokens, some up to 840 billion tokens. These algorithms exhibit stereotypical biases, such as gender bias which can be traced back to the original training data. For example certain occupations seem to be more biased towards a particular gender, reinforcing problematic stereotypes. The nearest solution to this problem are some de-biasing algorithms as the one presented in https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/reports/6835575.pdf which one can use on embeddings of their choice to mitigate bias, if present.\n</blockquote>\n<!-- #endregion -->",
                        "markdown"
                    ],
                    [
                        "You\u2019ll start with importing the necessary packages to build our Deep Learning network.",
                        "markdown"
                    ],
                    [
                        "# Importing the necessary packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pooch\nimport string\nimport re\nimport zipfile\nimport os\n\n# Creating the random instance\nrng = np.random.default_rng()",
                        "code"
                    ],
                    [
                        "Next, you\u2019ll define set of text preprocessing helper functions.",
                        "markdown"
                    ],
                    [
                        "class TextPreprocess:\n    \"\"\"Text Preprocessing for a Natural Language Processing model.\"\"\"\n\n    def txt_to_df(self, file):\n        \"\"\"Function to convert a txt file to pandas dataframe.\n\n        Parameters\n        ----------\n        file : str\n            Path to the txt file.\n\n        Returns\n        -------\n        Pandas dataframe\n            txt file converted to a dataframe.\n\n        \"\"\"\n        with open(imdb_train, 'r') as in_file:\n            stripped = (line.strip() for line in in_file)\n            reviews = {}\n            for line in stripped:\n                lines = [splits for splits in line.split(\"\\t\") if splits != \"\"]\n                reviews[lines[1]] = float(lines[0])\n        df = pd.DataFrame(reviews.items(), columns=['review', 'sentiment'])\n        df = df.sample(frac=1).reset_index(drop=True)\n        return df\n\n    def unzipper(self, zipped, to_extract):\n        \"\"\"Function to extract a file from a zipped folder.\n\n        Parameters\n        ----------\n        zipped : str\n            Path to the zipped folder.\n\n        to_extract: str\n            Path to the file to be extracted from the zipped folder\n\n        Returns\n        -------\n        str\n            Path to the extracted file.\n\n        \"\"\"\n        fh = open(zipped, 'rb')\n        z = zipfile.ZipFile(fh)\n        outdir = os.path.split(zipped)[0]\n        z.extract(to_extract, outdir)\n        fh.close()\n        output_file = os.path.join(outdir, to_extract)\n        return output_file\n\n    def cleantext(self, df, text_column=None,\n                  remove_stopwords=True, remove_punc=True):\n        \"\"\"Function to clean text data.\n\n        Parameters\n        ----------\n        df : pandas dataframe\n            The dataframe housing the input data.\n        text_column : str\n            Column in dataframe whose text is to be cleaned.\n        remove_stopwords : bool\n            if True, remove stopwords from text\n        remove_punc : bool\n            if True, remove punctuation symbols from text\n\n        Returns\n        -------\n        Numpy array\n            Cleaned text.\n\n        \"\"\"\n        # converting all characters to lowercase\n        df[text_column] = df[text_column].str.lower()\n\n        # List of stopwords taken from https://gist.github.com/sebleier/554280\n        stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\",\n                     \"all\", \"am\", \"an\", \"and\", \"any\", \"are\",\n                     \"as\", \"at\", \"be\", \"because\",\n                     \"been\", \"before\", \"being\", \"below\",\n                     \"between\", \"both\", \"but\", \"by\", \"could\",\n                     \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\",\n                     \"each\", \"few\", \"for\", \"from\", \"further\",\n                     \"had\", \"has\", \"have\", \"having\", \"he\",\n                     \"he'd\", \"he'll\", \"he's\", \"her\", \"here\",\n                     \"here's\", \"hers\", \"herself\", \"him\",\n                     \"himself\", \"his\", \"how\", \"how's\", \"i\",\n                     \"i'd\", \"i'll\", \"i'm\", \"i've\",\n                     \"if\", \"in\", \"into\",\n                     \"is\", \"it\", \"it's\", \"its\",\n                     \"itself\", \"let's\", \"me\", \"more\",\n                     \"most\", \"my\", \"myself\", \"nor\", \"of\",\n                     \"on\", \"once\", \"only\", \"or\",\n                     \"other\", \"ought\", \"our\", \"ours\",\n                     \"ourselves\", \"out\", \"over\", \"own\", \"same\",\n                     \"she\", \"she'd\", \"she'll\", \"she's\", \"should\",\n                     \"so\", \"some\", \"such\", \"than\", \"that\",\n                     \"that's\", \"the\", \"their\", \"theirs\", \"them\",\n                     \"themselves\", \"then\", \"there\", \"there's\",\n                     \"these\", \"they\", \"they'd\", \"they'll\",\n                     \"they're\", \"they've\", \"this\", \"those\",\n                     \"through\", \"to\", \"too\", \"under\", \"until\", \"up\",\n                     \"very\", \"was\", \"we\", \"we'd\", \"we'll\",\n                     \"we're\", \"we've\", \"were\", \"what\",\n                     \"what's\", \"when\", \"when's\",\n                     \"where\", \"where's\",\n                     \"which\", \"while\", \"who\", \"who's\",\n                     \"whom\", \"why\", \"why's\", \"with\",\n                     \"would\", \"you\", \"you'd\", \"you'll\",\n                     \"you're\", \"you've\",\n                     \"your\", \"yours\", \"yourself\", \"yourselves\"]\n\n        def remove_stopwords(data, column):\n            data[f'{column} without stopwords'] = data[column].apply(\n                lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\n            return data\n\n        def remove_tags(string):\n            result = re.sub('&lt;*&gt;', '', string)\n            return result\n\n        # remove html tags and brackets from text\n        if remove_stopwords:\n            data_without_stopwords = remove_stopwords(df, text_column)\n            data_without_stopwords[f'clean_{text_column}'] = data_without_stopwords[f'{text_column} without stopwords'].apply(\n                lambda cw: remove_tags(cw))\n        if remove_punc:\n            data_without_stopwords[f'clean_{text_column}'] = data_without_stopwords[f'clean_{text_column}'].str.replace(\n                '[{}]'.format(string.punctuation), ' ', regex=True)\n\n        X = data_without_stopwords[f'clean_{text_column}'].to_numpy()\n\n        return X\n\n\n    def sent_tokeniser(self, x):\n        \"\"\"Function to split text into sentences.\n\n        Parameters\n        ----------\n        x : str\n            piece of text\n\n        Returns\n        -------\n        list\n            sentences with punctuation removed.\n\n        \"\"\"\n        sentences = re.split(r'(?&lt;!\\w\\.\\w.)(?&lt;![A-Z][a-z]\\.)(?&lt;=\\.|\\?)\\s', x)\n        sentences.pop()\n        sentences_cleaned = [re.sub(r'[^\\w\\s]', '', x) for x in sentences]\n        return sentences_cleaned\n\n    def word_tokeniser(self, text):\n        \"\"\"Function to split text into tokens.\n\n        Parameters\n        ----------\n        x : str\n            piece of text\n\n        Returns\n        -------\n        list\n            words with punctuation removed.\n\n        \"\"\"\n        tokens = re.split(r\"([-\\s.,;!?])+\", text)\n        words = [x for x in tokens if (\n            x not in '- \\t\\n.,;!?\\\\' and '\\\\' not in x)]\n        return words\n\n    def loadGloveModel(self, emb_path):\n        \"\"\"Function to read from the word embedding file.\n\n        Returns\n        -------\n        Dict\n            mapping from word to corresponding word embedding.\n\n        \"\"\"\n        print(\"Loading Glove Model\")\n        File = emb_path\n        f = open(File, 'r')\n        gloveModel = {}\n        for line in f:\n            splitLines = line.split()\n            word = splitLines[0]\n            wordEmbedding = np.array([float(value) for value in splitLines[1:]])\n            gloveModel[word] = wordEmbedding\n        print(len(gloveModel), \" words loaded!\")\n        return gloveModel\n\n    def text_to_paras(self, text, para_len):\n        \"\"\"Function to split text into paragraphs.\n\n        Parameters\n        ----------\n        text : str\n            piece of text\n\n        para_len : int\n            length of each paragraph\n\n        Returns\n        -------\n        list\n            paragraphs of specified length.\n\n        \"\"\"\n        # split the speech into a list of words\n        words = text.split()\n        # obtain the total number of paragraphs\n        no_paras = int(np.ceil(len(words)/para_len))\n        # split the speech into a list of sentences\n        sentences = self.sent_tokeniser(text)\n        # aggregate the sentences into paragraphs\n        k, m = divmod(len(sentences), no_paras)\n        agg_sentences = [sentences[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(no_paras)]\n        paras = np.array([' '.join(sents) for sents in agg_sentences])\n\n        return paras",
                        "code"
                    ],
                    [
                        " is a Python package made by scientists that manages downloading data files over HTTP and storing them in a local directory. We use this to set up a download manager which contains all of the information needed to fetch the data files in our registry and store them in the specified cache folder.",
                        "markdown"
                    ],
                    [
                        "data = pooch.create(\n    # folder where the data will be stored in the\n    # default cache folder of your Operating System\n    path=pooch.os_cache(\"numpy-nlp-tutorial\"),\n    # Base URL of the remote data store\n    base_url=\"\",\n    # The cache file registry. A dictionary with all files managed by this pooch.\n    # The keys are the file names and values are their respective hash codes which\n    # ensure we download the same, uncorrupted file each time.\n    registry={\n        \"imdb_train.txt\": \"6a38ea6ab5e1902cc03f6b9294ceea5e8ab985af991f35bcabd301a08ea5b3f0\",\n         \"imdb_test.txt\": \"7363ef08ad996bf4233b115008d6d7f9814b7cc0f4d13ab570b938701eadefeb\",\n        \"glove.6B.50d.zip\": \"617afb2fe6cbd085c235baf7a465b96f4112bd7f7ccb2b2cbd649fed9cbcf2fb\",\n    },\n    # Now specify custom URLs for some of the files in the registry.\n    urls={\n        \"imdb_train.txt\": \"doi:10.5281/zenodo.4117827/imdb_train.txt\",\n        \"imdb_test.txt\": \"doi:10.5281/zenodo.4117827/imdb_test.txt\",\n        \"glove.6B.50d.zip\": 'https://nlp.stanford.edu/data/glove.6B.zip'\n    }\n)",
                        "code"
                    ],
                    [
                        "Download the IMDb training and testing data files:",
                        "markdown"
                    ],
                    [
                        "imdb_train = data.fetch('imdb_train.txt')\nimdb_test = data.fetch('imdb_test.txt')",
                        "code"
                    ],
                    [
                        "Instantiate the TextPreprocess class to perform various operations on our datasets:",
                        "markdown"
                    ],
                    [
                        "textproc = TextPreprocess()",
                        "code"
                    ],
                    [
                        "Convert each IMDb file to a pandas dataframe for a more convenient preprocessing of the datasets:",
                        "markdown"
                    ],
                    [
                        "train_df = textproc.txt_to_df(imdb_train)\ntest_df = textproc.txt_to_df(imdb_test)",
                        "code"
                    ],
                    [
                        "Now, you will clean the dataframes obtained above by removing occurrences of stopwords and punctuation marks. You will also retrieve the sentiment values from each dataframe to obtain the target variables:",
                        "markdown"
                    ],
                    [
                        "X_train = textproc.cleantext(train_df,\n                       text_column='review',\n                       remove_stopwords=True,\n                       remove_punc=True)[0:2000]\n\nX_test = textproc.cleantext(test_df,\n                       text_column='review',\n                       remove_stopwords=True,\n                       remove_punc=True)[0:1000]\n\ny_train = train_df['sentiment'].to_numpy()[0:2000]\ny_test = test_df['sentiment'].to_numpy()[0:1000]",
                        "code"
                    ],
                    [
                        "The same process is applicable on the collected speeches:\n<blockquote>",
                        "markdown"
                    ],
                    [
                        "Since we will be performing paragraph wise sentiment analysis on each speech further ahead in the tutorial, we\u2019ll need the punctuation marks to split the text into paragraphs, hence we refrain from removing their punctuation marks at this stage\n</blockquote>",
                        "markdown"
                    ],
                    [
                        "speech_data_path = 'tutorial-nlp-from-scratch/speeches.csv'\nspeech_df = pd.read_csv(speech_data_path)\nX_pred = textproc.cleantext(speech_df,\n                            text_column='speech',\n                            remove_stopwords=True,\n                            remove_punc=False)\nspeakers = speech_df['speaker'].to_numpy()",
                        "code"
                    ],
                    [
                        "You will now download the GloVe embeddings, unzip them and build a dictionary mapping each word and word embedding. This will act as a cache for when you need to replace each word with its respective word embedding.",
                        "markdown"
                    ],
                    [
                        "glove = data.fetch('glove.6B.50d.zip')\nemb_path = textproc.unzipper(glove, 'glove.6B.300d.txt')\nemb_matrix = textproc.loadGloveModel(emb_path)",
                        "code"
                    ]
                ]
            },
            {
                "3. Build the Deep Learning Model\u00b6": [
                    [
                        "It is time to start implementing our LSTM! You will have to first familiarize yourself with some high-level concepts of the basic building blocks of a deep learning model. You can refer to the  for the same.",
                        "markdown"
                    ],
                    [
                        "You will then learn how a Recurrent Neural Network differs from a plain Neural Network and what makes it so suitable for processing sequential data. Afterwards, you will construct the building blocks of a simple deep learning model in Python and NumPy and train it to learn to classify the sentiment of a piece of text as positive or negative with a certain level of accuracy",
                        "markdown"
                    ],
                    {
                        "Introduction to a Long Short Term Memory Network": [
                            [
                                "In a  (MLP), the information only moves in one direction \u2014 from the input layer, through the hidden layers, to the output layer. The information moves straight through the network and never takes the previous nodes into account at a later stage. Because it only considers the current input, the features learned are not shared across different positions of the sequence. Moreover, it cannot process sequences with varying lengths.",
                                "markdown"
                            ],
                            [
                                "Unlike an MLP, the RNN was designed to work with sequence prediction problems.RNNs introduce state variables to store past information, together with the current inputs, to determine the current outputs. Since an RNN shares the learned features with all the data points in a sequence regardless of its length, it is capable of processing sequences with varying lengths.",
                                "markdown"
                            ],
                            [
                                "The problem with an RNN however, is that it cannot retain long-term memory because the influence of a given input on the hidden layer, and therefore on the network output, either decays or blows up exponentially as it cycles around the network\u2019s recurrent connections. This shortcoming is referred to as the vanishing gradient problem. Long Short-Term Memory (LSTM) is an RNN architecture specifically designed to address the .",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "Overview of the Model Architecture": [
                            [
                                "<img alt=\"Overview of the model architecture, showing a series of animated boxes. There are five identical boxes labeled A and receiving as input one of the words in the phrase &quot;life's a box of chocolates&quot;. Each box is highlighted in turn, representing the memory blocks of the LSTM network as information passes through them, ultimately reaching a &quot;Positive&quot; output value.\" src=\"../_images/lstm.gif\"/>",
                                "markdown"
                            ],
                            [
                                "In the above gif, the rectangles labeled \\(A\\) are called Cells and they are the <strong>Memory Blocks</strong> of our LSTM network. They are responsible for choosing what to remember in a sequence and pass on that information to the next cell via two states called the hidden state \\(H_{t}\\) and the cell state \\(C_{t}\\) where \\(t\\) indicates the time-step. Each Cell has dedicated gates which are responsible for storing, writing or reading the information passed to an LSTM. You will now look closely at the architecture of the network by implementing each mechanism happening inside of it.",
                                "markdown"
                            ],
                            [
                                "Lets start with writing a function to randomly initialize the parameters which will be learned while our model trains",
                                "markdown"
                            ],
                            [
                                "def initialise_params(hidden_dim, input_dim):\n    # forget gate\n    Wf = rng.standard_normal(size=(hidden_dim, hidden_dim + input_dim))\n    bf = rng.standard_normal(size=(hidden_dim, 1))\n    # input gate\n    Wi = rng.standard_normal(size=(hidden_dim, hidden_dim + input_dim))\n    bi = rng.standard_normal(size=(hidden_dim, 1))\n    # candidate memory gate\n    Wcm = rng.standard_normal(size=(hidden_dim, hidden_dim + input_dim))\n    bcm = rng.standard_normal(size=(hidden_dim, 1))\n    # output gate\n    Wo = rng.standard_normal(size=(hidden_dim, hidden_dim + input_dim))\n    bo = rng.standard_normal(size=(hidden_dim, 1))\n\n    # fully connected layer for classification\n    W2 = rng.standard_normal(size=(1, hidden_dim))\n    b2 = np.zeros((1, 1))\n\n    parameters = {\n        \"Wf\": Wf,\n        \"bf\": bf,\n        \"Wi\": Wi,\n        \"bi\": bi,\n        \"Wcm\": Wcm,\n        \"bcm\": bcm,\n        \"Wo\": Wo,\n        \"bo\": bo,\n        \"W2\": W2,\n        \"b2\": b2\n    }\n    return parameters",
                                "code"
                            ]
                        ]
                    },
                    {
                        "Forward Propagation": [
                            [
                                "Now that you have your initialized parameters, you can pass the input data in a forward direction through the network. Each layer accepts the input data, processes it and passes it to the successive layer. This process is called Forward Propagation. You will undertake the following mechanism to implement it:",
                                "markdown"
                            ],
                            [
                                "Loading the word embeddings of the input data",
                                "markdown"
                            ],
                            [
                                "Passing the embeddings to an LSTM",
                                "markdown"
                            ],
                            [
                                "Perform all the gate mechanisms in every memory block of the LSTM to obtain the final hidden state",
                                "markdown"
                            ],
                            [
                                "Passing the final hidden state through a fully connected layer to obtain the probability with which the sequence is positive",
                                "markdown"
                            ],
                            [
                                "Storing all the calculated values in a cache to utilize during backpropagation",
                                "markdown"
                            ],
                            [
                                " belongs to the family of non-linear activation functions. It helps the network to update or forget the data. If the sigmoid of a value results in 0, the information is considered forgotten. Similarly, the information stays if it is 1.",
                                "markdown"
                            ],
                            [
                                "def sigmoid(x):\n    n = np.exp(np.fmin(x, 0))\n    d = (1 + np.exp(-np.abs(x)))\n    return n / d",
                                "code"
                            ],
                            [
                                "The <strong>Forget Gate</strong> takes the current word embedding and the previous hidden state concatenated together as input. and decides what parts of the old memory cell content need attention and which can be ignored.",
                                "markdown"
                            ],
                            [
                                "def fp_forget_gate(concat, parameters):\n    ft = sigmoid(np.dot(parameters['Wf'], concat)\n                 + parameters['bf'])\n    return ft",
                                "code"
                            ],
                            [
                                "The <strong>Input Gate</strong> takes the current word embedding and the previous hidden state concatenated together as input. and governs how much of the new data we take into account via the <strong>Candidate Memory Gate</strong> which utilizes the  to regulate the values flowing through the network.",
                                "markdown"
                            ],
                            [
                                "def fp_input_gate(concat, parameters):\n    it = sigmoid(np.dot(parameters['Wi'], concat)\n                 + parameters['bi'])\n    cmt = np.tanh(np.dot(parameters['Wcm'], concat)\n                  + parameters['bcm'])\n    return it, cmt",
                                "code"
                            ],
                            [
                                "Finally we have the <strong>Output Gate</strong> which takes information from the current word embedding, previous hidden state and the cell state which has been updated with information from the forget and input gates to update the value of the hidden state.",
                                "markdown"
                            ],
                            [
                                "def fp_output_gate(concat, next_cs, parameters):\n    ot = sigmoid(np.dot(parameters['Wo'], concat)\n                 + parameters['bo'])\n    next_hs = ot * np.tanh(next_cs)\n    return ot, next_hs",
                                "code"
                            ],
                            [
                                "The following image summarizes each gate mechanism in the memory block of a LSTM network:\n<blockquote>",
                                "markdown"
                            ],
                            [
                                "Image has been modified from  source\n</blockquote>",
                                "markdown"
                            ],
                            [
                                "<img alt='Diagram showing three sections of a memory block, labeled \"Forget gate\", \"Input gate\" and \"Output gate\". Each gate contains several subparts, representing the operations performed at that stage of the process.' src=\"../_images/mem_block.png\"/>",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "But how do you obtain sentiment from the LSTM\u2019s output?": [
                            [
                                "The hidden state you obtain from the output gate of the last memory block in a sequence is considered to be a representation of all the information contained in a sequence. To classify this information into various classes (2 in our case, positive and negative) we use a <strong>Fully Connected layer</strong> which firstly maps this information to a predefined output size (1 in our case). Then, an activation function such as the sigmoid converts this output to a value between 0 and 1. We\u2019ll consider values greater than 0.5 to be indicative of a positive sentiment.",
                                "markdown"
                            ],
                            [
                                "def fp_fc_layer(last_hs, parameters):\n    z2 = (np.dot(parameters['W2'], last_hs)\n          + parameters['b2'])\n    a2 = sigmoid(z2)\n    return a2",
                                "code"
                            ],
                            [
                                "Now you will put all these functions together to summarize the <strong>Forward Propagation</strong> step in our model architecture:",
                                "markdown"
                            ],
                            [
                                "def forward_prop(X_vec, parameters, input_dim):\n\n    hidden_dim = parameters['Wf'].shape[0]\n    time_steps = len(X_vec)\n\n    # Initialise hidden and cell state before passing to first time step\n    prev_hs = np.zeros((hidden_dim, 1))\n    prev_cs = np.zeros(prev_hs.shape)\n\n    # Store all the intermediate and final values here\n    caches = {'lstm_values': [], 'fc_values': []}\n\n    # Hidden state from the last cell in the LSTM layer is calculated.\n    for t in range(time_steps):\n        # Retrieve word corresponding to current time step\n        x = X_vec[t]\n        # Retrieve the embedding for the word and reshape it to make the LSTM happy\n        xt = emb_matrix.get(x, rng.random(size=(input_dim, 1)))\n        xt = xt.reshape((input_dim, 1))\n\n        # Input to the gates is concatenated previous hidden state and current word embedding\n        concat = np.vstack((prev_hs, xt))\n\n        # Calculate output of the forget gate\n        ft = fp_forget_gate(concat, parameters)\n\n        # Calculate output of the input gate\n        it, cmt = fp_input_gate(concat, parameters)\n        io = it * cmt\n\n        # Update the cell state\n        next_cs = (ft * prev_cs) + io\n\n        # Calculate output of the output gate\n        ot, next_hs = fp_output_gate(concat, next_cs, parameters)\n\n        # store all the values used and calculated by\n        # the LSTM in a cache for backward propagation.\n        lstm_cache = {\n        \"next_hs\": next_hs,\n        \"next_cs\": next_cs,\n        \"prev_hs\": prev_hs,\n        \"prev_cs\": prev_cs,\n        \"ft\": ft,\n        \"it\" : it,\n        \"cmt\": cmt,\n        \"ot\": ot,\n        \"xt\": xt,\n        }\n        caches['lstm_values'].append(lstm_cache)\n\n        # Pass the updated hidden state and cell state to the next time step\n        prev_hs = next_hs\n        prev_cs = next_cs\n\n    # Pass the LSTM output through a fully connected layer to\n    # obtain probability of the sequence being positive\n    a2 = fp_fc_layer(next_hs, parameters)\n\n    # store all the values used and calculated by the\n    # fully connected layer in a cache for backward propagation.\n    fc_cache = {\n    \"a2\" : a2,\n    \"W2\" : parameters['W2']\n    }\n    caches['fc_values'].append(fc_cache)\n    return caches",
                                "code"
                            ]
                        ]
                    },
                    {
                        "Backpropagation": [
                            [
                                "After each forward pass through the network, you will implement the backpropagation through time algorithm to accumulate gradients of each parameter over the time steps. Backpropagation through a LSTM is not as straightforward as through other common Deep Learning architectures, due to the special way its underlying layers interact. Nonetheless, the approach is largely the same; identifying dependencies and applying the chain rule.",
                                "markdown"
                            ],
                            [
                                "Lets start with defining a function to initialize gradients of each parameter as arrays made up of zeros with same dimensions as the corresponding parameter",
                                "markdown"
                            ],
                            [
                                "# Initialise the gradients\ndef initialize_grads(parameters):\n    grads = {}\n    for param in parameters.keys():\n        grads[f'd{param}'] = np.zeros((parameters[param].shape))\n    return grads",
                                "code"
                            ],
                            [
                                "Now, for each gate and the fully connected layer, we define a function to calculate the gradient of the loss with respect to the input passed and the parameters used. To understand the mathematics behind how the derivatives were calculated we suggest you to follow this helpful  by Christina Kouridi.",
                                "markdown"
                            ],
                            [
                                "Define a function to calculate the gradients in the <strong>Forget Gate</strong>:",
                                "markdown"
                            ],
                            [
                                "def bp_forget_gate(hidden_dim, concat, dh_prev, dc_prev, cache, gradients, parameters):\n    # dft = dL/da2 * da2/dZ2 * dZ2/dh_prev * dh_prev/dc_prev * dc_prev/dft\n    dft = ((dc_prev * cache[\"prev_cs\"] + cache[\"ot\"]\n           * (1 - np.square(np.tanh(cache[\"next_cs\"])))\n           * cache[\"prev_cs\"] * dh_prev) * cache[\"ft\"] * (1 - cache[\"ft\"]))\n    # dWf = dft * dft/dWf\n    gradients['dWf'] += np.dot(dft, concat.T)\n    # dbf = dft * dft/dbf\n    gradients['dbf'] += np.sum(dft, axis=1, keepdims=True)\n    # dh_f = dft * dft/dh_prev\n    dh_f = np.dot(parameters[\"Wf\"][:, :hidden_dim].T, dft)\n    return dh_f, gradients",
                                "code"
                            ],
                            [
                                "Define a function to calculate the gradients in the <strong>Input Gate</strong> and <strong>Candidate Memory Gate</strong>:",
                                "markdown"
                            ],
                            [
                                "def bp_input_gate(hidden_dim, concat, dh_prev, dc_prev, cache, gradients, parameters):\n    # dit = dL/da2 * da2/dZ2 * dZ2/dh_prev * dh_prev/dc_prev * dc_prev/dit\n    dit = ((dc_prev * cache[\"cmt\"] + cache[\"ot\"]\n           * (1 - np.square(np.tanh(cache[\"next_cs\"])))\n           * cache[\"cmt\"] * dh_prev) * cache[\"it\"] * (1 - cache[\"it\"]))\n    # dcmt = dL/da2 * da2/dZ2 * dZ2/dh_prev * dh_prev/dc_prev * dc_prev/dcmt\n    dcmt = ((dc_prev * cache[\"it\"] + cache[\"ot\"]\n            * (1 - np.square(np.tanh(cache[\"next_cs\"])))\n            * cache[\"it\"] * dh_prev) * (1 - np.square(cache[\"cmt\"])))\n    # dWi = dit * dit/dWi\n    gradients['dWi'] += np.dot(dit, concat.T)\n    # dWcm = dcmt * dcmt/dWcm\n    gradients['dWcm'] += np.dot(dcmt, concat.T)\n    # dbi = dit * dit/dbi\n    gradients['dbi'] += np.sum(dit, axis=1, keepdims=True)\n    # dWcm = dcmt * dcmt/dbcm\n    gradients['dbcm'] += np.sum(dcmt, axis=1, keepdims=True)\n    # dhi = dit * dit/dh_prev\n    dh_i = np.dot(parameters[\"Wi\"][:, :hidden_dim].T, dit)\n    # dhcm = dcmt * dcmt/dh_prev\n    dh_cm = np.dot(parameters[\"Wcm\"][:, :hidden_dim].T, dcmt)\n    return dh_i, dh_cm, gradients",
                                "code"
                            ],
                            [
                                "Define a function to calculate the gradients for the <strong>Output Gate</strong>:",
                                "markdown"
                            ],
                            [
                                "def bp_output_gate(hidden_dim, concat, dh_prev, dc_prev, cache, gradients, parameters):\n    # dot = dL/da2 * da2/dZ2 * dZ2/dh_prev * dh_prev/dot\n    dot = (dh_prev * np.tanh(cache[\"next_cs\"])\n           * cache[\"ot\"] * (1 - cache[\"ot\"]))\n    # dWo = dot * dot/dWo\n    gradients['dWo'] += np.dot(dot, concat.T)\n    # dbo = dot * dot/dbo\n    gradients['dbo'] += np.sum(dot, axis=1, keepdims=True)\n    # dho = dot * dot/dho\n    dh_o = np.dot(parameters[\"Wo\"][:, :hidden_dim].T, dot)\n    return dh_o, gradients",
                                "code"
                            ],
                            [
                                "Define a function to calculate the gradients for the <strong>Fully Connected Layer</strong>:",
                                "markdown"
                            ],
                            [
                                "def bp_fc_layer (target, caches, gradients):\n    # dZ2 = dL/da2 * da2/dZ2\n    predicted = np.array(caches['fc_values'][0]['a2'])\n    target = np.array(target)\n    dZ2 = predicted - target\n    # dW2 = dL/da2 * da2/dZ2 * dZ2/dW2\n    last_hs = caches['lstm_values'][-1][\"next_hs\"]\n    gradients['dW2'] = np.dot(dZ2, last_hs.T)\n    # db2 = dL/da2 * da2/dZ2 * dZ2/db2\n    gradients['db2'] = np.sum(dZ2)\n    # dh_last = dZ2 * W2\n    W2 = caches['fc_values'][0][\"W2\"]\n    dh_last = np.dot(W2.T, dZ2)\n    return dh_last, gradients",
                                "code"
                            ],
                            [
                                "Put all these functions together to summarize the <strong>Backpropagation</strong> step for our model:",
                                "markdown"
                            ],
                            [
                                "def backprop(y, caches, hidden_dim, input_dim, time_steps, parameters):\n\n    # Initialize gradients\n    gradients = initialize_grads(parameters)\n\n    # Calculate gradients for the fully connected layer\n    dh_last, gradients = bp_fc_layer(target, caches, gradients)\n\n    # Initialize gradients w.r.t previous hidden state and previous cell state\n    dh_prev = dh_last\n    dc_prev = np.zeros((dh_prev.shape))\n\n    # loop back over the whole sequence\n    for t in reversed(range(time_steps)):\n        cache = caches['lstm_values'][t]\n\n        # Input to the gates is concatenated previous hidden state and current word embedding\n        concat = np.concatenate((cache[\"prev_hs\"], cache[\"xt\"]), axis=0)\n\n        # Compute gates related derivatives\n        # Calculate derivative w.r.t the input and parameters of forget gate\n        dh_f, gradients = bp_forget_gate(hidden_dim, concat, dh_prev, dc_prev, cache, gradients, parameters)\n\n        # Calculate derivative w.r.t the input and parameters of input gate\n        dh_i, dh_cm, gradients = bp_input_gate(hidden_dim, concat, dh_prev, dc_prev, cache, gradients, parameters)\n\n        # Calculate derivative w.r.t the input and parameters of output gate\n        dh_o, gradients = bp_output_gate(hidden_dim, concat, dh_prev, dc_prev, cache, gradients, parameters)\n\n        # Compute derivatives w.r.t prev. hidden state and the prev. cell state\n        dh_prev = dh_f + dh_i + dh_cm + dh_o\n        dc_prev = (dc_prev * cache[\"ft\"] + cache[\"ot\"]\n                   * (1 - np.square(np.tanh(cache[\"next_cs\"])))\n                   * cache[\"ft\"] * dh_prev)\n\n    return gradients",
                                "code"
                            ]
                        ]
                    },
                    {
                        "Updating the Parameters": [
                            [
                                "We update the parameters through an optimization algorithm called  which is an extension to stochastic gradient descent that has recently seen broader adoption for deep learning applications in computer vision and natural language processing. Specifically, the algorithm calculates an exponential moving average of the gradient and the squared gradient, and the parameters beta1 and beta2 control the decay rates of these moving averages. Adam has shown increased convergence and robustness over other gradient descent algorithms and is often recommended as the default optimizer for training.",
                                "markdown"
                            ],
                            [
                                "Define a function to initialise the moving averages for each parameter",
                                "markdown"
                            ],
                            [
                                "# initialise the moving averages\ndef initialise_mav(hidden_dim, input_dim, params):\n    v = {}\n    s = {}\n    # Initialize dictionaries v, s\n    for key in params:\n        v['d' + key] = np.zeros(params[key].shape)\n        s['d' + key] = np.zeros(params[key].shape)\n    # Return initialised moving averages\n    return v, s",
                                "code"
                            ],
                            [
                                "Define a function to update the parameters",
                                "markdown"
                            ],
                            [
                                "# Update the parameters using Adam optimization\ndef update_parameters(parameters, gradients, v, s,\n                      learning_rate=0.01, beta1=0.9, beta2=0.999):\n    for key in parameters:\n        # Moving average of the gradients\n        v['d' + key] = (beta1 * v['d' + key]\n                        + (1 - beta1) * gradients['d' + key])\n\n        # Moving average of the squared gradients\n        s['d' + key] = (beta2 * s['d' + key]\n                        + (1 - beta2) * (gradients['d' + key] ** 2))\n\n        # Update parameters\n        parameters[key] = (parameters[key] - learning_rate\n                           * v['d' + key] / np.sqrt(s['d' + key] + 1e-8))\n    # Return updated parameters and moving averages\n    return parameters, v, s",
                                "code"
                            ]
                        ]
                    },
                    {
                        "Training the Network": [
                            [
                                "You will start by initializing all the parameters and hyperparameters being used in your network",
                                "markdown"
                            ],
                            [
                                "hidden_dim = 64\ninput_dim = emb_matrix['memory'].shape[0]\nlearning_rate = 0.001\nepochs = 10\nparameters = initialise_params(hidden_dim,\n                               input_dim)\nv, s = initialise_mav(hidden_dim,\n                      input_dim,\n                      parameters)",
                                "code"
                            ],
                            [
                                "To optimize your deep learning network, you need to calculate a loss based on how well the model is doing on the training data. Loss value implies how poorly or well a model behaves after each iteration of optimization. <br/>\nDefine a function to calculate the loss using ",
                                "markdown"
                            ],
                            [
                                "def loss_f(A, Y):\n    # define value of epsilon to prevent zero division error inside a log\n    epsilon = 1e-5\n    # Implement formula for negative log likelihood\n    loss = (- Y * np.log(A + epsilon)\n            - (1 - Y) * np.log(1 - A + epsilon))\n    # Return loss\n    return np.squeeze(loss)",
                                "code"
                            ],
                            [
                                "Set up the neural network\u2019s learning experiment with a training loop and start the training process. You will also evaluate the model\u2019s performance on the training dataset to see how well the model is <em>learning</em> and the testing dataset to see how well it is <em>generalizing</em>.\n<blockquote>",
                                "markdown"
                            ],
                            [
                                "Skip running this cell if you already have the trained parameters stored in a npy file\n</blockquote>",
                                "markdown"
                            ],
                            [
                                "# To store training losses\ntraining_losses = []\n# To store testing losses\ntesting_losses = []\n\n# This is a training loop.\n# Run the learning experiment for a defined number of epochs (iterations).\nfor epoch in range(epochs):\n    #################\n    # Training step #\n    #################\n    train_j = []\n    for sample, target in zip(X_train, y_train):\n        # split text sample into words/tokens\n        b = textproc.word_tokeniser(sample)\n\n        # Forward propagation/forward pass:\n        caches = forward_prop(b,\n                              parameters,\n                              input_dim)\n\n        # Backward propagation/backward pass:\n        gradients = backprop(target,\n                             caches,\n                             hidden_dim,\n                             input_dim,\n                             len(b),\n                             parameters)\n\n        # Update the weights and biases for the LSTM and fully connected layer\n        parameters, v, s = update_parameters(parameters,\n                                             gradients,\n                                             v,\n                                             s,\n                                             learning_rate=learning_rate,\n                                             beta1=0.999,\n                                             beta2=0.9)\n\n        # Measure the training error (loss function) between the actual\n        # sentiment (the truth) and the prediction by the model.\n        y_pred = caches['fc_values'][0]['a2'][0][0]\n        loss = loss_f(y_pred, target)\n        # Store training set losses\n        train_j.append(loss)\n\n    ###################\n    # Evaluation step #\n    ###################\n    test_j = []\n    for sample, target in zip(X_test, y_test):\n        # split text sample into words/tokens\n        b = textproc.word_tokeniser(sample)\n\n        # Forward propagation/forward pass:\n        caches = forward_prop(b,\n                              parameters,\n                              input_dim)\n\n        # Measure the testing error (loss function) between the actual\n        # sentiment (the truth) and the prediction by the model.\n        y_pred = caches['fc_values'][0]['a2'][0][0]\n        loss = loss_f(y_pred, target)\n\n        # Store testing set losses\n        test_j.append(loss)\n\n    # Calculate average of training and testing losses for one epoch\n    mean_train_cost = np.mean(train_j)\n    mean_test_cost = np.mean(test_j)\n    training_losses.append(mean_train_cost)\n    testing_losses.append(mean_test_cost)\n    print('Epoch {} finished. \\t  Training Loss : {} \\t  Testing Loss : {}'.\n          format(epoch + 1, mean_train_cost, mean_test_cost))\n\n# save the trained parameters to a npy file\nnp.save('tutorial-nlp-from-scratch/parameters.npy', parameters)",
                                "code"
                            ],
                            [
                                "It is a good practice to plot the training and testing losses as the learning curves are often helpful in diagnosing the behavior of a Machine Learning model.",
                                "markdown"
                            ],
                            [
                                "fig = plt.figure()\nax = fig.add_subplot(111)\n\n# plot the training loss\nax.plot(range(0, len(training_losses)), training_losses, label='training loss')\n# plot the testing loss\nax.plot(range(0, len(testing_losses)), testing_losses, label='testing loss')\n\n# set the x and y labels\nax.set_xlabel(\"epochs\")\nax.set_ylabel(\"loss\")\n\nplt.legend(title='labels', bbox_to_anchor=(1.0, 1), loc='upper left')\nplt.show()",
                                "code"
                            ]
                        ]
                    },
                    {
                        "Sentiment Analysis on the Speech Data": [
                            [
                                "Once your model is trained, you can use the updated parameters to start making our predictions. You can break each speech into paragraphs of uniform size before passing them to the Deep Learning model and predicting the sentiment of each paragraph",
                                "markdown"
                            ],
                            [
                                "# To store predicted sentiments\npredictions = {}\n\n# define the length of a paragraph\npara_len = 100\n\n# Retrieve trained values of the parameters\nif os.path.isfile('tutorial-nlp-from-scratch/parameters.npy'):\n    parameters = np.load('tutorial-nlp-from-scratch/parameters.npy', allow_pickle=True).item()\n\n# This is the prediction loop.\nfor index, text in enumerate(X_pred):\n    # split each speech into paragraphs\n    paras = textproc.text_to_paras(text, para_len)\n    # To store the network outputs\n    preds = []\n\n    for para in paras:\n        # split text sample into words/tokens\n        para_tokens = textproc.word_tokeniser(para)\n        # Forward Propagation\n        caches = forward_prop(para_tokens,\n                              parameters,\n                              input_dim)\n\n        # Retrieve the output of the fully connected layer\n        sent_prob = caches['fc_values'][0]['a2'][0][0]\n        preds.append(sent_prob)\n\n    threshold = 0.5\n    preds = np.array(preds)\n    # Mark all predictions &gt; threshold as positive and &lt; threshold as negative\n    pos_indices = np.where(preds &gt; threshold)  # indices where output &gt; 0.5\n    neg_indices = np.where(preds &lt; threshold)  # indices where output &lt; 0.5\n    # Store predictions and corresponding piece of text\n    predictions[speakers[index]] = {'pos_paras': paras[pos_indices[0]],\n                                    'neg_paras': paras[neg_indices[0]]}",
                                "code"
                            ],
                            [
                                "Visualizing the sentiment predictions:",
                                "markdown"
                            ],
                            [
                                "x_axis = []\ndata = {'positive sentiment': [], 'negative sentiment': []}\nfor speaker in predictions:\n    # The speakers will be used to label the x-axis in our plot\n    x_axis.append(speaker)\n    # number of paras with positive sentiment\n    no_pos_paras = len(predictions[speaker]['pos_paras'])\n    # number of paras with negative sentiment\n    no_neg_paras = len(predictions[speaker]['neg_paras'])\n    # Obtain percentage of paragraphs with positive predicted sentiment\n    pos_perc = no_pos_paras / (no_pos_paras + no_neg_paras)\n    # Store positive and negative percentages\n    data['positive sentiment'].append(pos_perc*100)\n    data['negative sentiment'].append(100*(1-pos_perc))\n\nindex = pd.Index(x_axis, name='speaker')\ndf = pd.DataFrame(data, index=index)\nax = df.plot(kind='bar', stacked=True)\nax.set_ylabel('percentage')\nax.legend(title='labels', bbox_to_anchor=(1, 1), loc='upper left')\nplt.show()",
                                "code"
                            ],
                            [
                                "In the plot above, you\u2019re shown what percentages of each speech are expected to carry a positive and negative  sentiment. Since this implementation prioritized simplicity and clarity over performance, we cannot expect these results to be very accurate. Moreover, while making the sentiment predictions for one paragraph we did not use the neighboring paragraphs for context which would have led to more accurate predictions. We encourage the reader to play around with the model and make some tweaks suggested in Next Steps and observe how the model performance changes.",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "Looking at our Neural Network from an ethical perspective\n<!-- #region -->": [
                    [
                        "It\u2019s crucial to understand that accurately identifying a text\u2019s sentiment is not easy primarily because of the complex ways in which humans express sentiment, using irony, sarcasm, humor, or, in social media, abbreviation. Moreover neatly placing text into two categories: \u2018positive\u2019 and \u2018negative\u2019 can be problematic because it is being done without any context. Words or abbreviations can convey very different sentiments depending on age and location, none of which we took into account while building our model.",
                        "markdown"
                    ],
                    [
                        "Along with data, there are also growing concerns that data processing algorithms are influencing policy and daily lives in ways that are not transparent and introduce biases. Certain biases such as the  are essential to help a Machine Learning model generalize better, for example the LSTM we built earlier is biased towards preserving contextual information over long sequences which makes it so suitable for processing sequential data. The problem arises when  creep into algorithmic predictions. Optimizing Machine algorithms via methods like  can then further amplify these biases by learning every bit of information in the data.",
                        "markdown"
                    ],
                    [
                        "There are also cases where bias is only in the output and not the inputs (data, algorithm). For example, in sentiment analysis . End users of sentiment analysis should be aware that its small gender biases can affect the conclusions drawn from it and apply correction factors when necessary. Hence, it is important that demands for algorithmic accountability should include the ability to test the outputs of a system, including the ability to drill down into different user groups by gender, ethnicity and other characteristics, to identify, and hopefully suggest corrections for, system output biases.\n<!-- #endregion -->",
                        "markdown"
                    ]
                ]
            },
            {
                "Next Steps": [
                    [
                        "You have learned how to build and train a simple Long Short Term Memory network from scratch using just NumPy to perform sentiment analysis.",
                        "markdown"
                    ],
                    [
                        "To further enhance and optimize your neural network model, you can consider one of a mixture of the following:",
                        "markdown"
                    ],
                    [
                        "Alter the architecture by introducing multiple LSTM layers to make the network deeper.",
                        "markdown"
                    ],
                    [
                        "Use a higher epoch size to train longer and add more regularization techniques, such as early stopping, to prevent overfitting.",
                        "markdown"
                    ],
                    [
                        "Introduce a validation set for an unbiased evaluation of the model fit.",
                        "markdown"
                    ],
                    [
                        "Apply batch normalization for faster and more stable training.",
                        "markdown"
                    ],
                    [
                        "Tune other parameters, such as the learning rate and hidden layer size.",
                        "markdown"
                    ],
                    [
                        "Initialize weights using  to prevent vanishing/exploding gradients instead of initializing them randomly.",
                        "markdown"
                    ],
                    [
                        "Replace LSTM with a  to use both left and right context for predicting sentiment.",
                        "markdown"
                    ],
                    [
                        "Nowadays, LSTMs have been replaced by the ( which uses  to tackle all the problems that plague an LSTM such as as lack of , lack of  and a long gradient chain for lengthy sequences",
                        "markdown"
                    ],
                    [
                        "Building a neural network from scratch with NumPy is a great way to learn more about NumPy and about deep learning. However, for real-world applications you should use specialized frameworks \u2014 such as PyTorch, JAX, TensorFlow or MXNet \u2014 that provide NumPy-like APIs, have built-in automatic differentiation and GPU support, and are designed for high-performance numerical computing and machine learning.",
                        "markdown"
                    ],
                    [
                        "Finally, to know more about how ethics come into play when developing a machine learning model, you can refer to the following resources :",
                        "markdown"
                    ],
                    [
                        "Data ethics resources by the Turing Institute. https://www.turing.ac.uk/research/data-ethics",
                        "markdown"
                    ],
                    [
                        "Considering how artificial intelligence shifts power, an  and  by Pratyusha Kalluri",
                        "markdown"
                    ],
                    [
                        "More ethics resources on  by Rachel Thomas and the",
                        "markdown"
                    ]
                ]
            }
        ]
    }
}