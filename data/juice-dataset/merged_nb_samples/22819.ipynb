{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbcacd84",
   "metadata": {},
   "source": [
    "# Subject: Advanced Data Analysis\n",
    "\n",
    "# Module: Geospatial Analysis\n",
    "\n",
    "## Session 5 - Geographic Analysis of Social Network Data \n",
    "\n",
    "### Demo 1 -  Sentiment analysis and Geospatial analysis on Carles Puigdemont tweets using Python "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7059a9c3",
   "metadata": {},
   "source": [
    "The requirements that we'll need to install are:\n",
    "\n",
    "- NumPy: This is the fundamental package for scientific computing with Python. Besides its obvious scientific uses, NumPy can also be used as an efficient multi-dimensional container of generic data.\n",
    "- Pandas: This is an open source library providing high-performance, easy-to-use data structures and data analysis tools.\n",
    "- Tweepy: This is an easy-to-use Python library for accessing the Twitter API.\n",
    "- Matplotlib: This is a Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms.\n",
    "- Seaborn: This is a Python visualization library based on matplotlib. It provides a high-level interface for drawing attractive statistical graphics.\n",
    "- Textblob: This is a Python library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks.\n",
    "\n",
    "https://textblob.readthedocs.io/en/dev/\n",
    "\n",
    "$ pip install -U textblob\n",
    "\n",
    "$ python -m textblob.download_corpora\n",
    "\n",
    "https://github.com/tweepy/tweepy\n",
    "\n",
    "$ pip install tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de98a345",
   "metadata": {},
   "source": [
    "## 1. Extracting twitter data (tweepy + pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55223788",
   "metadata": {},
   "source": [
    "### 1.1. Importing our libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e79f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General:\n",
    "import tweepy           # To consume Twitter's API\n",
    "import pandas as pd     # To handle data\n",
    "import numpy as np      # For number computing\n",
    "\n",
    "# For plotting and visualization:\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0900fef2",
   "metadata": {},
   "source": [
    "### 1.2. Creating a Twitter App"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2a4c44",
   "metadata": {},
   "source": [
    "In order to extract tweets for a posterior analysis, we need to access to our Twitter account and create an app. The website to do this is https://apps.twitter.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08c927f",
   "metadata": {},
   "source": [
    "From this app that we're creating we will save the following information:\n",
    "\n",
    "- Consumer Key (API Key)\n",
    "\n",
    "- Consumer Secret (API Secret)\n",
    "\n",
    "- Access Token\n",
    "\n",
    "- Access Token Secret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c82a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API's setup:\n",
    "def twitter_setup():\n",
    "    \"\"\"\n",
    "    Utility function to setup the Twitter's API\n",
    "    with our access keys provided.\n",
    "    \"\"\"\n",
    "    # Authentication and access using keys:\n",
    "    auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "    auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
    "    \n",
    "    # Return API with authentication:\n",
    "    api = tweepy.API(auth)\n",
    "    return api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f42a099",
   "metadata": {},
   "source": [
    "### 1.3. Tweets extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b8abf1",
   "metadata": {},
   "source": [
    "Now that we've created a function to setup the Twitter API, we can use this function to create an \"extractor\" object. After this, we will use Tweepy's function extractor.user_timeline(screen_name, count) to extract from screen_name's user the quantity of count tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df26ed8f",
   "metadata": {},
   "source": [
    "As it is mentioned in the title, I've chosen @KRLS as the user to extract data for a posterior analysis. The way to extract Twitter's data is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f72c10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create an extractor object:\n",
    "extractor = twitter_setup()\n",
    "\n",
    "# We create a tweet list as follows:\n",
    "tweets = extractor.user_timeline(screen_name=\"KRLS\", count=200)\n",
    "print(\"Number of tweets extracted: {}.\\n\".format(len(tweets)))\n",
    "\n",
    "# We print the most recent 5 tweets:\n",
    "print(\"5 recent tweets:\\n\")\n",
    "for tweet in tweets[:5]:\n",
    "    print(tweet.text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922f23f1",
   "metadata": {},
   "source": [
    "We now have an extractor and extracted data, which is listed in the tweets variable. I must mention at this point that each element in that list is a tweet object from Tweepy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab782c17",
   "metadata": {},
   "source": [
    "### 1.4. Creating a (pandas) DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aaedd4",
   "metadata": {},
   "source": [
    "We now have initial information to construct a pandas DataFrame, in order to manipulate the info in a very easy way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3f3f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a pandas dataframe as follows:\n",
    "data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])\n",
    "\n",
    "# We display the first 10 elements of the dataframe:\n",
    "display(data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034b0f4d",
   "metadata": {},
   "source": [
    "An interesting thing is the number if internal methods that the tweetstructure has in Tweepy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b97a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internal methods of a single tweet object:\n",
    "print(dir(tweets[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57adfd03",
   "metadata": {},
   "source": [
    "The interesting part from here is the quantity of metadata contained in a single tweet. If we want to obtain data such as the creation date, or the source of creation, we can access the info with this attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5a05a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob #TextBlob is a Python (2 and 3) library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more.\n",
    "import re #This module provides regular expression matching operations.Regular expressions use the backslash character ('\\') to indicate special forms or to allow special characters to be used without invoking their special meaning. \n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    '''\n",
    "    Utility function to clean the text in a tweet by removing \n",
    "    links and special characters using regex.\n",
    "    '''\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "\n",
    "def analize_sentiment(tweet):\n",
    "    '''\n",
    "    Utility function to classify the polarity of a tweet\n",
    "    using textblob.\n",
    "    '''\n",
    "    analysis = TextBlob(clean_tweet(tweet))\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return 1\n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0a5990",
   "metadata": {},
   "source": [
    "https://link.springer.com/chapter/10.1007/978-3-319-47602-5_40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea067622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a column with the result of the analysis:\n",
    "data['SA'] = np.array([ analize_sentiment(tweet) for tweet in data['Tweets'] ])\n",
    "\n",
    "# We display the updated dataframe with the new column:\n",
    "display(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b776b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb97a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b695641",
   "metadata": {},
   "source": [
    "### 2.2. Analyzing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1584ead",
   "metadata": {},
   "source": [
    "#### 2.2.1. Calculation of the percentage of the classified tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de259428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We construct lists with classified tweets:\n",
    "\n",
    "pos_tweets = [ tweet for index, tweet in enumerate(data['Tweets']) if data['SA'][index] > 0]\n",
    "neu_tweets = [ tweet for index, tweet in enumerate(data['Tweets']) if data['SA'][index] == 0]\n",
    "neg_tweets = [ tweet for index, tweet in enumerate(data['Tweets']) if data['SA'][index] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc10a757",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb33cc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "neu_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9565d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f309a0e",
   "metadata": {},
   "source": [
    "Now that we have the lists, we just print the percentages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857fdd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter App access keys for @user\n",
    "\n",
    "# Consume:\n",
    "CONSUMER_KEY    = 'Uu3D2hHGljVnU2vchDYmHZtGw'\n",
    "CONSUMER_SECRET = 'aUfG03L1ZUQjojGK1dQ6dKFC8nktUZQZ4eZDU3p23hEA8ZQus3'\n",
    "\n",
    "# Access:\n",
    "ACCESS_TOKEN  = '955416286477082625-85nByhWARuuQQJt2QyfFwublVbSE28L'\n",
    "ACCESS_SECRET = 'DWARKyhsQKwm1aVWiTsS8RjKQGy778iaERGHeKnauB9Sb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ddf4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['timestamp'] = data['timestamp'].dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87eb93d",
   "metadata": {},
   "source": [
    "### 2. Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d769b23f",
   "metadata": {},
   "source": [
    "### 2.1. Importing textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3313987",
   "metadata": {},
   "source": [
    "Anyway, getting back to the code we will just add an extra column to our data. This column will contain the sentiment analysis and we can plot the dataframe to see the update:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f20adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph of the Polarity by dates (2017 and 2018)\n",
    "df1.plot( kind='line', x='timestamp', y='Polarity',title='Polarity by date')\n",
    "axes = plt.gca()\n",
    "plt.xticks(rotation='vertical', fontsize=11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fd93d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph of the Polarity by hour for a specific day\n",
    "df1['Hour'] = pd.to_datetime(df1['Date'], format='%H:%M').dt.hour # to create a new column with the hour information\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5f6ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1[df1['timestamp'] == '2018-01-23']\n",
    "df2.plot( kind='line', x='Hour', y='Polarity',title='Polarity on 23/01/2018')\n",
    "axes = plt.gca()\n",
    "plt.xticks(rotation='vertical', fontsize=11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2332646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph of the Likes by dates (2017 and 2018)\n",
    "df1.plot( kind='line', x='timestamp', y='Likes',title='Likes by date')\n",
    "axes = plt.gca()\n",
    "plt.xticks(rotation='vertical', fontsize=11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39444264",
   "metadata": {},
   "source": [
    "## 3. Export the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007f6867",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data_twitter.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adc39d9",
   "metadata": {},
   "source": [
    "## 4. QuantumGIS processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0700ae8e",
   "metadata": {},
   "source": [
    "### Task: \n",
    "\n",
    "Because Carles Puigdemont has not activated the Twiter location-sharing mode, we need to simulate a creation of 200 random location points within the administrative boundaries of Brussels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d578a3",
   "metadata": {},
   "source": [
    " - Inside QuantumGIS lets calculate a shapefile with 200 random points inside Brussels. Use the function \"Random Points in layer bounds\" (Menu Vector, Research tools)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a4c7d0",
   "metadata": {},
   "source": [
    "- Add the longitude and latitude columns with the option \"Export/Add geometry columns\" (Menu Vector, Geometry tools)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbdca30",
   "metadata": {},
   "source": [
    "- Use the exported file csv file ('data_twitter.csv') and perform a join with the random points layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f308dc",
   "metadata": {},
   "source": [
    "## 5. Import the joined shapefile to Jupyter and create a geodataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66b0988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We print info from the first tweet:\n",
    "print(tweets[0].id)\n",
    "print(tweets[0].created_at)\n",
    "print(tweets[0].source)\n",
    "print(tweets[0].favorite_count)\n",
    "print(tweets[0].retweet_count)\n",
    "print(tweets[0].geo)\n",
    "print(tweets[0].coordinates)\n",
    "print(tweets[0].entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c52f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add relevant data:\n",
    "data['len']  = np.array([len(tweet.text) for tweet in tweets])\n",
    "data['ID']   = np.array([tweet.id for tweet in tweets])\n",
    "data['Date'] = np.array([tweet.created_at for tweet in tweets])\n",
    "data['Source'] = np.array([tweet.source for tweet in tweets])\n",
    "data['Likes']  = np.array([tweet.favorite_count for tweet in tweets])\n",
    "data['coordinates']    = np.array([tweet.coordinates for tweet in tweets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e319eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We convert the Date type:\n",
    "data['timestamp'] = pd.to_datetime(data['Date'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53097720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We print percentages:\n",
    "\n",
    "print(\"Percentage of positive tweets: {}%\".format(len(pos_tweets)*100/len(data['Tweets'])))\n",
    "print(\"Percentage of neutral tweets: {}%\".format(len(neu_tweets)*100/len(data['Tweets'])))\n",
    "print(\"Percentage de negative tweets: {}%\".format(len(neg_tweets)*100/len(data['Tweets'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61464d79",
   "metadata": {},
   "source": [
    "#### 2.2.2. Infographics of the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe1e8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "gdf = gpd.read_file('Random_points_twitter.shp')\n",
    "gdf = gdf.to_crs({'init': 'epsg:4326'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b87f961",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412ae8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156efd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We must convert the Polarity to string to be used as a map attribute\n",
    "gdf.data_twi_8 = gdf.data_twi_8.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e87bbe",
   "metadata": {},
   "source": [
    "## 6.Webmapping of Tweets with Folium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d60a08",
   "metadata": {},
   "source": [
    "### 6.1. Installation:\n",
    "\n",
    "https://github.com/python-visualization/folium\n",
    "\n",
    "$ pip install folium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21517e9",
   "metadata": {},
   "source": [
    "### 6.2. Creation of a webmap with twitter location by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7857f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0e00cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Twitter basemap specifying map center, zoom level, and using the CartoDB Positron tiles\n",
    "Twitter_map = folium.Map([45.955263, 8.935129], tiles='cartodbpositron', zoom_start = 5)\n",
    "Twitter_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d1202f",
   "metadata": {},
   "source": [
    "https://deparkes.co.uk/2016/06/10/folium-map-tiles/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d0ccd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas DataFrame with the Sentiment Analysis results\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac6eced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Pandas DataFrame with a new name of the field including the Sentiment Analysis results (SA)\n",
    "df1=data.rename(columns={'SA':'Polarity'})\n",
    "df1.head()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
