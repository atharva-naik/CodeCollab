{"markdowns": ["Lab 3.4: csvs, functions, numpy, and distributions Run the cell below to load the required packages and set up plotting in the notebook!", "Sales data For this lab we will be using a truncated version of some sales data that we will be looking at further down the line in more detail. The csv has about 200 rows of data and 4 columns. The relative path to the csv sales_info.csv is provided below. If you copied files over and moved them around, this might be different for you and you will have to figure out the correct relative path to enter.", "1. Loading the data Set up an empty list called rows . Using the pattern for loading csvs we learned earlier, add all of the rows in the csv file to the rows list. For your reference, the pattern is: python with open(my_csv_path, 'r') as f: reader = csv.reader(f) ... Beyond this, adding the rows in the csv file to the rows variable is up to you.", "2. Separate header and data The header of the csv is contained in the first index of the rows variable, as it is the first row in the csv file. Use python indexing to create two new variables: header which contains the 4 column names, and data which contains the list of lists, each sub-list representing a row from the csv. Lastly, print header to see the names of the columns.", "3. Create a dictionary with the data Use loops or list comprehensions to create a dictionary called sales_data , where the keys of the dictionary are the column names, and the values of the dictionary are lists of the data points of the column corresponding to that column name.", "3.A Print out the first 10 items of the 'volume_sold' column.", "4. Convert data from string to float As you can see, the data is still in string format (which is how it is read in from the csv). For each key:value pair in our sales_data dictionary, convert the values (column data) from string values to float values.", "5. Write function to print summary statistics Now write a function to print out summary statistics for the data. Your function should: - Accept two arguments: the column name and the data associated with that column - Print out information, clearly labeling each item when you print it: 1. Print out the column name 2. Print the mean of the data using np.mean() 3. Print out the median of the data using np.median() 4. Print out the mode of the rounded data using stats.mode() 5. Print out the variance of the data using np.var() 6. Print out the standard deviation of the data using np.std() Remember that you will need to convert the numeric data from these function to strings by wrapping them in the str() function.", "5.A Using your function, print the summary statistics for 'volume_sold'", "5.B Using your function, print the summary statistics for '2015_margin'", "5.C Using your function, print the summary statistics for '2015_q1_sales'", "5.D Using your function, print the summary statistics for '2016_q1_sales'", "6. Plot the distributions We've provided a plotting function below called distribution_plotter() . It takes two arguments, the name of the column and the data associated with that column. In individual cells, plot the distributions for each of the 4 columns. Do the data appear skewed? Symmetrical? If skewed, what would be your hypothesis for why?"], "topic": "data analysis"}
{"markdowns": ["Sentiment Classification & How To \"Frame Problems\" for a Neural Network by Andrew Trask - Twitter : @iamtrask - Blog : http://iamtrask.github.io", "What You Should Already Know - neural networks, forward and back-propagation - stochastic gradient descent - mean squared error - and train/test splits Where to Get Help if You Need it - Re-watch previous Udacity Lectures - Leverage the recommended Course Reading Material - [Grokking Deep Learning](https://www.manning.com/books/grokking-deep-learning) (40% Off: traskud17 ) - Shoot me a tweet @iamtrask Tutorial Outline: - Intro: The Importance of \"Framing a Problem\" - Curate a Dataset - Developing a \"Predictive Theory\" - PROJECT 1 : Quick Theory Validation - Transforming Text to Numbers - PROJECT 2 : Creating the Input/Output Data - Putting it all together in a Neural Network - PROJECT 3 : Building our Neural Network - Understanding Neural Noise - PROJECT 4 : Making Learning Faster by Reducing Noise - Analyzing Inefficiencies in our Network - PROJECT 5 : Making our Network Train and Run Faster - Further Noise Reduction - PROJECT 6 : Reducing Noise by Strategically Reducing the Vocabulary - Analysis: What's going on in the weights?", "Lesson: Curate a Dataset", "Lesson: Develop a Predictive Theory", "Project 1: Quick Theory Validation", "Transforming Text into Numbers", "Project 2: Creating the Input/Output Data"], "topic": "convolution"}
{"markdowns": ["Machine Learning 101++ in Python by Pieter Buteneers (@pieterbuteneers) and Bart De Vylder from CoScale 1. Imports Let's first start with importing all the necessary packages. Some imports will be repeated in the exercises but if you want to skip some parts you can just execute the imports below and start with any exercise.", "2. Linear Regression Linear Regression assumes a linear realationship between 2 variables. As an example we'll consider the historical page views of a web server and compare it to its CPU usage. We'll try to predict the CPU usage of the server based on the page views of the different pages. 2.1 Data import and inspection Let's import the data and take a look at it.", "The orange line on the plot above is the number of page views and the blue line is the CPU load that viewing this pages generates on the server. 2.2 Simple linear regression First, we're going to work with the total page views on the server, and compare it to the CPU usage. We can make use of a [PyPlot's scatter plot](http://matplotlib.org/api/pyplot_api.html matplotlib.pyplot.scatter) to understand the relation between the total page views and the CPU usage:", "There clearly is a strong correlation between the page views and the CPU usage. Because of this correlation we can build a model to predict the CPU usage from the total page views. If we use a linear model we get a formula like the following: $$ \\text{cpu_usage} = c_0 + c_1 \\text{total_page_views} $$ Since we don't know the exact values for $c_0$ and $c_1$ we will have to compute them. For that we'll make use of the [scikit-learn](http://scikit-learn.org/stable/) machine learning library for Python and use [least-squares linear regression](http://scikit-learn.org/stable/modules/linear_model.html ordinary-least-squares)", "Now we need to feed the data to the model to fit it. The [model.fit(X,y) method](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html sklearn.linear_model.LinearRegression.fit) in general takes a matrix X and vector y as arguments: X = [[x_11, x_12, x_13, ...], y = [y_1, [x_21, x_22, x_23, ...], y_2, [x_31, x_32, x_33, ...], y_3, ...] ...] and tries to find coefficients that allow to predict the y_i 's from the x_ij 's. In our case the matrix X will consist of only 1 column containing the total page views. Our total_page_views variable however, is still only a one-dimensional vector, so we need to [ np.reshape() ](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html) it into a two-dimensional array. Since there is only 1 feature the second dimension should be 1. Then we fit our model using the the total page views and cpu. The coefficients found are automatically stored in the simple_lin_model object.", "We can now inspect the coefficient $c_1$ and constant term (intercept) $c_0$ of the model:", "So this means that each additional page view adds about 0.11% CPU load to the server and all the other processes running on the server consume on average 0.72% CPU. Once the model is trained we can use it to [ predict ](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html sklearn.linear_model.LinearRegression.predict) the outcome for a given input (or array of inputs). Note that the predict function requires a 2-dimensional array similar to the fit function. What is the expected CPU usage when we have 880 page views per second?", "Now we plot the linear model together with our data to verify it captures the relationship correctly (the predict method can accept the entire total_page_views array at once).", "Our model can calculate the R2 [ score ](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html sklearn.linear_model.LinearRegression.score) indicating how well the linear model captures the data. A score of 1 means there is perfect linear correlation and the model can fit the data perfectly, a score of 0 (or lower) means that there is no correlation at all (and it does not make sense to try to model it that way). The score method takes the same arguments as the fit method.", "2.3 Extrapolation Now let's repeat this experiment with different but similar data. We will try to predict what the CPU usage will be if there will be 8 page views (per second).", "Now let's plot what you have done.", "Is this what you would expect? Can you see what's wrong? Let's plot the time series again to get a different view at the data.", "The spikes of CPU usage are actually backups that run at night and they can be ignored. So repeat the exercize again but ignore these data points. Hint: The selection variable should contain True where there is no backup going on and False when the backup occurs. This is an easy shortcut to do a selection of specific data points in numpy arrays.", "So what you should have learned from the previous exercise is that you should always look at your data and/or write scripts to inspect your data. Additionally extrapolation does not always work because there are no training examples in that area. 3. Multiple linear regression A server can host different pages and each of the page views will generate load on the CPU. This load will however not be the same for each page. Now let us consider the separate page views and build a linear model for that. The model we try to fit takes the form: $$\\text{cpu_usage} = c_0 + c_1 \\text{page_views}_1 + c_2 \\text{page_views}_2 + \\ldots + c_n \\text{page_views}_n$$ where the $\\text{page_views}_i$'s correspond the our different pages:", "We start again by creating a [ LinearRegression ](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html sklearn.linear_model.LinearRegression) model.", "Next we fit the model on the data, using multi_lin_model.fit(X,y) . In contrast to the case above our page_views variable already has the correct shape to pass as the X matrix: it has one column per page.", "Now, given the coefficients calculated by the model, which capture the contribution of each page view to the total CPU usage, we can start to answer some interesting questions. For example, which page view causes most CPU usage, on a per visit basis? For this we can generate a table of page names with their coefficients in descending order:", "From this table we see that 'resources/js/basket.js' consumes the most per CPU per view. It generates about 0.30% CPU load for each additional page view. 'products/science.html' on the other hand is much leaner and only consumes about 0.04% CPU per view. Now let us investigate the constant term again.", "As you can see this term is very similar to the result achieved in single linear regression, but it is not entirely the same. This means that these models are not perfect. However, they seem to be able to give a reliable estimate. 4. Non-linear Regression Sometimes linear relations don't cut it anymore, so you might want a more complex method. There are 2 approaches to this: Use a non-linear method (such as Neural Networks, Support Vector Machines, Random Forests and Gaussian Processes) Use non-linear features as pre-processing for a linear method Actually both methods are in essence identical and there is not always a clear distinction between the two. We will use the second approach in this section since it is easier to understand what is going on. Please note that it is very often not even necessary to use non-linear methods, since the linear methods can be extremely powerful on their own and they are quite often very stable and reliable (in contrast to non-linear methods). 4.1. Fitting a sine function with linear regression Als an example task we will try to fit a sine function. We will use the [ np.sine() ](https://docs.scipy.org/doc/numpy/reference/generated/numpy.sin.html) function to compute the sine of the elements in a numpy array. Let's first try this with linear regression.", "For training we will draw 10 samples of this function as our train set.", "Now let's try to fit this function with linear regression.", "As you can see this fit is not optimal. 4.2. Fitting a sine function using polynomial expansion One of the easiest ways to make your machine learning technique more intelligent is to extract relevant features from the data. These features can be anything that you can find that will make it easier for the metod to be able to fit the data. This means that as a machine learning engineer it is best to know and understand your data. As some of you might remember from math class is that you can create an approximation of any function (including a sine function) using a polynomial function with the [Taylor expansion](https://en.wikipedia.org/wiki/Taylor_series). So we will use that approach to learn a better fit. In this case we will create what we call features using a [polynomial expansion](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html). If you set the degree to 3 it will generate data of the 0d, 1st, 2nd and 3rd order (including cross products) as shown in the example below (change x and degree to see the different expansions of x to a certain degree).", "As you can see above this function transforms $x$ into [$x^0$, $x^1$, $x^2$, $x^3$] with $x^0=1$ and $x^1 = x$. If you have 2 inputs it will also take the cross products so that [$x_1$, $x_2$] is transformed into: [1, $x_1$, $x_2$, $x_1^2$, $x_1x_2$, $x_2^2$, $x_1^3$, $x_1^2x_2$, $x_1x_2^2$, $x_2^3$] as shown below.", "In this example we only have 1 input so the number of features is always the degree + 1 . Because of this polynomial features extraction finding of the coefficients of the polynomial becomes a linear problem, so similar to the previous exercise on multiple linear regression you can find the optimal weights as follows: $$y = c_0 + c_1 x + c_2 x^2 + c_3 x^3 + \\cdots + c_n x^n$$ So for multiple values of $x$ and $y$ you can minimize the error of this equation using linear regression. How this is done in practice is shown below.", "The more relevant these features are the better your model can fit the data. Now play with the degree of the polynomal expansion function below to create better features. Search for the optimal degree.", "Now let's test this on new and unseen data.", "If everything is correct your score is very close to 1. Which means that we have built a model that can fit this data (almost) perfectly. 4.3. Add noise to the equation Sadly all the data that we measure or gather doesn'thave the mathematical precision of the data we used here. Quite often our measurements contain noise. So let us repeat this process for data with more noise. Similarly as above, you have to choose the optimal degree of the polynomials.", "Now let's see what this results to in the test set.", "As you can clearly see, this result is not that good. Why do you think this is? Now plot the result to see the function you created.", "Is this what you expect? Now repeat the process below a couple of times for random noise.", "What did you observe? And what is the method learning? And how can you avoid this? Try to figure out a solution for this problem without changing the noise level.", "Check your solution a couple of times to make sure your solution works for different noise samples. 5. Over-fitting and Cross-Validation What you have experienced above is called over-fitting and happens when your model learns the noise that is inherrent in the data. This problem was caused because there were to many parameters in the model. So the model was too advanced so that it became capable of learning the noise in the data by heart. Reducing the number of parameters solves this problem. But how do you know how many parameters is optimal? (Another way to solve this problem is to use more data. Because if there are more data points in the data and if there is more noise, your model isn't able to learn all that noise anymore and you get a better result. Since it usually is not possible to gather more data we will not take this approach.) In the exercise above you had to set the number of polynomial functions to get a better result, but how can you estimate this in a reliable way without manually selection the optimal parameters? 5.1. Validation set A common way to solve this problem is through the use of a validation set. This means that you use a subset of the training data to train your model on, and another subset of the training data to validate your parameters. Based on the score of your model on this validation set you can select the optimal parameter. So use this approach to select the best number of polynomials for the noisy sine function.", "Now test this result on the test set with the following code.", "As you can see this approach works to select the optimal degree. Usually the test score is lower than the validation score, but in this case it is not because the test data doesn't contain noise. 5.2. Cross-Validation To improve this procedure you can repeat the process above for different train and validation sets so that the optimal parameter is less dependent on the way the data was selected. One basic strategy for this is leave-one-out cross validation, where each data point is left out of the train set once, and the model is then validated on this point. Now let's implement this. First make a 2-dimensional array results to store all your results using the [ np.ones() ](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ones.html) function: 1 dimension (row) for each validation set and 1 dimension (column) for each degree of the PolynomialFeatures() function. Then you loop over all the validation sets followed by a loop over all the degrees of the PolynomialFeatures() function you want to try out. Then set the result for that experiment in the right element of the results array. We will use the [mean squared error (MSE)](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) instead of R2 because that is more stable. Since the MSE measures the error, smaller values are better. Now that you have your results average them over all validation sets (using the [ np.mean() ](https://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html) function over the correct axis) so that you know the average error for each degree over all validation sets. Now find the the degree with the smallest error using the [ np.argmin() ](https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmin.html) function.", "Now let's have a look at the result.", "As you can see this automatic way of selecting the optimal degree has resulted in a good fit for the sine function. 5.3 Regularisation When you have too many parameters in your model, there is a risk of overfitting, i.e. your model learns the noise. To avoid this, techniques have been developed to make an estimation of this noise. One of these techniques is Ridge Regression. This linear regression technique has an additional parameter called the regularisation parameter. This parameter basically sets the standard deviation of the noise you want to remove. The effect in practice is that it makes sure the weights of linear regression remain small and thus less over-fitting. Since this is an additional parameter that needs to be set, it needs to be set using cross-validation as well. Luckily sklearn developed a method that does this for us in a computational efficient way called [ sklearn.linear_model.RidgeCV() ](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html)", "As you can see above, the result of Ridge Regression is not as good as reducing the number of features in this example. However it works a lot better than without regularisation (try that). In the example above you will notice that it makes the result a lot smoother and removes the unwanted spikes. It will actually make sure that if you have too many features you still get a reasonable result. So this means that it should be in your standard toolkit. The removal of the extra features can be automated using feature selection. A very short introduction to sklearn on the topic can be found [here](http://scikit-learn.org/stable/modules/feature_selection.html). Another method that is often used is [ sklearn.linear_model.LassoCV() ](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html sklearn.linear_model.LassoCV) which actually combines removal of features and estimation of the noise. It is however very dependent on the dataset which of the two methods performs best. Cross-validation should be applied to any parameter you set in your function and that without looking at the test set. Over-fitting is one of the biggest issues in machine learning and most of the research that is currently being done in machine learning is a search for techniques to avoid over-fitting. As a starting point we list a few of the techniques that you can use to avoid over-fitting: Use more data Artificially generate more data based on the original data Use a smaller model (with less parameters) Use less features (and thus less parameters) Use a regularisation parameter Artificially add noise to your model Only use linear models or make sure that the non-linearity in your model is closer to a linear function Combine multiple models that each over-fit in their own way into what is called an ensemble 5.4 Extrapolation Now let's extend the range of the optimal plot you achieved from -4 to 10. What do you see? Does it look like a sine function?", "As you can see, the extrapolation results for non-linear regression are even worse than for those of linear regression. This is because models only work well in the input space they have been trained in. A possible way to be able to extrapolate and to use a non-linear method is to use forecasting techniques. This part is optional for those interested and going through the tutorial quite fast. Otherwise continue to the final part on classification in exercise 7.", "6. Classification In classification the purpose is to separate 2 classes. As an example we will use the double spiral. It is a very common toy example in machine learning and allows you to visually show what is going on. As shown in the graph below the purpose is to separate the blue from the red dots.", "In a colored image this is easy to do, but when you remove the color it becomes much harder. Can you do the classification in the image below? In black the samples from the train set are shown and in yellow the samples from the validation set.", "As you can see classifying is very hard to do when you don't get the answer even if you saw the solution earlier. But you will see that machine learning algorithms can solve this quite well if they can learn from examples. 6.1 Linear classifier Let's try to do this with a linear classifier. A linear classifier is basically a form of linear regression where the output is set to 1 for all the data points of class 1 and to 0 for all the data points of class 0.", "Now let's plot the result.", "As you can see a linear classifier returns a linear decision boundary. 6.2 Non-linear classification Now let's do this better with a non-linear classifier using polynomials. Play with the degree of the polynomial expansion and look for the effect of the RidgeCV() and LassoCV() models. What gives you the best results?", "If everything went well you should get a validation/test accuracy very close to 0.8. 6.3 Random Forests An often used technique in machine learning are random forests. Basically they are [decision trees](https://en.wikipedia.org/wiki/Decision_tree_learning), or in programmers terms, if-then-else structures, like the one shown below. <img src=\"images/tree.png\" width=70%> Decision trees are know to over-fit a lot because they just learn the train set by heart and store it. Random forests on the other hand combine multiple different (randomly initialized) decision trees that all over-fit in their own way. But by combining their output using a voting mechanism, they tend to cancel out eachothers mistakes. This approach is called an [ensemble](https://en.wikipedia.org/wiki/Ensemble_learning) and can be used for any combination of machine learning techniques. A schematical representation of how such a random forest works is shown below. <img src=\"images/random_forest.jpg\"> Now let's try to use a random forest to solve the double spiral problem. (see [ sklearn.ensemble.RandomForestClassifier() ](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html))", "As you can see they are quite powerful right out of the box without any parameter tuning. But we can get the results even beter with some fine tuning. Try changing the min_samples_leaf parameter for values between 0 and 0.5."], "topic": "optimization problem"}
{"markdowns": ["COGS 108 - Assignment 4: Data Analysis", "Important Reminders You must submit this file ( A4_DataAnalysis.ipynb ) to TritonED to finish the homework. - This assignment has hidden tests: tests that are not visible here, but that will be run on your submitted assignment for grading. - This means passing all the tests you can see in the notebook here does not guarantee you have the right answer! - In particular many of the tests you can see simply check that the right variable names exist. Hidden tests check the actual values. - It is up to you to check the values, and make sure they seem reasonable. - A reminder to restart the kernel and re-run the code as a first line check if things seem to go weird. - For example, note that some cells can only be run once, because they re-write a variable (for example, your dataframe), and change it in a way that means a second execution will fail. - Also, running some cells out of order might change the dataframe in ways that may cause an error, which can be fixed by re-running.", "Notes - Assignment Outline Parts 1-6 of this assignment are modeled on being a minimal example of a project notebook. This mimics, and gets you working with, something like what you will need for your final project. Parts 7 & 8 break from the project narrative, and are OPTIONAL (UNGRADED). They serve instead as a couple of quick one-offs to get you working with some other methods that might be useful to incorporate into your project.", "Setup Data: the responses collected from a survery of the COGS 108 class. - There are 417 observations in the data, covering 10 different 'features'. Research Question: Do students in different majors have different heights? Background: Physical height has previously shown to correlate with career choice, and career success. More recently it has been demonstrated that these correlations can actually be explained by height in high school, as opposed to height in adulthood (1). It is currently unclear whether height correlates with choice of major in university. Reference: 1) http://economics.sas.upenn.edu/~apostlew/paper/pdf/short.pdf Hypothesis: We hypothesize that there will be a relation between height and chosen major.", "Part 1: Load & Clean the Data", "Fixing messy data makes up a large amount of the work of being a Data Scientist. The real world produces messy measurements and it is your job to find ways to standardize your data such that you can make useful analyses out of it. In this section, you will learn, and practice, how to successfully deal with unclean data.", "1a) Load the data Import datafile 'COGS108_IntroQuestionnaireData.csv' into a DataFrame called 'df'.", "Those column names are a bit excessive, so first let's rename them - code provided below to do so.", "Pandas has a very useful function for detecting missing data. This function is called 'isnull()'. If you have a dataframe called 'df', then calling 'df.isnull()' will return another dataframe of the same size as 'df' where every cell is either True of False. Each True or False is the answer to the question 'is the data in this cell null?'. So, False, means the cell is not null (and therefore, does have data). True means the cell is null (does not have data). This function is very useful because it allows us to find missing data very quickly in our dataframe. As an example, consider the code below.", "If you print out more, and scroll down, you'll see some rows with missing data. For example:", "Check an example, row 49, in which an entry has missing data", "Granted, the example above is not very informative. As you can see, the output of 'isnull()' is a dataframe where the values at each cell is either True or False. Most cells have the value of 'False'. We expect this to be the case since most people gave out answers to each question in our survey. However, some rows such as row 49 show that some people chose not to answer certain questions. In the case of row 49, it seems that someone did not give out an answer for 'What year (in school) are you?' However, what if wanted to use 'isnull()' to see all rows where our dataframe 'df' has missing values? In other words, what if we want to see the ACTUAL rows with missing values instead of this dataframe with True or False cells. For that, we need to write the following line of code: <br> <br> python df[df.isnull().any(axis=1)]", "1b) Find missing data Find all rows that have missing data in them. Save the ouput, as a dataframe, into a variable called 'rows_to_drop'. In other words, copy over and use the line of code that we gave out in the cell above.", "You need to run & read the following couple of cells - but don't have to add any code:", "Real world data is messy. As an example of it, we consider the data shown in rows_to_drop (below). If you've done everything correctly so far, you should see an unexpected response with emojis at index 357. These types of responses, although funny, are hard to parse when dealing with big datasets. We'll learn about solutions to these types of problems in the upcoming cells", "In the cell below, briefly explain below how 'df[df.isnull().any(axis=1)]' works, in a couple sentences. Include an explanation of what 'any(axis=1)' means and how it affects the code.", "df.isnull.any(axis=1) looks at the dataframe and checks for any empty/null values in the dataframe. Axis=1 specifies to drop the rows that have the null values, while axis=0 would drop columns.", "------", "1c) Drop the rows with NaN values Drop any rows with missing data, but only for the columns 'major', 'height', 'gender' and 'age'. These will be the data of primary interest for our analyses, so we drop missing data here. Note that there are other missing data (in other rows) but this is fine for our analyses, so we keep them. To do this, ese the pandas 'dropna' method, inplace, using the 'subset' arguments to specify columns.", "Now we have to standardize the data!", "Check all different values given for majors. It's a lot!", "---------", "We'll write a function performing some simple substring checking in order to group many responses together", "Applying the transformation", "Previewing the results of the previous transformation. It looks a lot better, though it's not perfect, but we'll run with this", "Next let's check the 'gender' column.", "Check the different responses received for gender, including how many of each response we have", "standardize_gender : Function to standardize the gender responses Note: for the purposes of the following analyses, we will keep self-reported gender for categories in which we have a sizable number of responses, in this case, those which correspond to 'female' and 'male'", "Apply the transformation, and drop any rows with missing gender information", "Now you will write some code to standardize some of the other data columns.", "1d) Standardize other columns Find, programatically, the number of unique responses to in the 'year' column. Save the result in a variable named 'num_unique_responses'. Hint: you can answer this question using the 'unique' method, used above.", "Print out all the different answers in 'year'", "The line of code above shows us the different values we got, to the question 'What year (in school) are you?'. As you can tell, it is a <b>mess</b>!. For example, if you are a junior student, then you might have answered: 3, three, third, 3rd year, junior, junior year, Junior, etc. That is an issue. We want to be able to analyze this data and, in order to do this successfully, we need to all answers with the same meaning to be written in the same way. Therefore, we're gonna have to transform answers such as '3, third, 3rd, junior, etc' into a single possible value. We'll do this for all values that mean the same.", "In the rest of Part 1, we will work on writing code, organized into functions that will allow us to transform similar respones into the same value. We will call this process: standardizing the data. The cell below provides an example for the kind of code you will need to write to answer this question. This example is separate from our actual data, and is a potential function we might use to standardize messy data - in this case, hypothetical data to the question 'What is your favourite major python version?'. Note some things used in this example that you need to use to standardize data: - string methods, such as 'lower' and 'strip' to transform strings - the 'replace' string method, to replace a set of characters with something else - if/else statements that check what's in our string (number, letters, etc) - type casting, for example using 'int()' to turn a variable into an integer - using 'np.nan' (which stands for 'not a number') to denote missing or unknown data", "1e) Standardize 'year' column Write a function named 'standardize_year' that takes in as input a string and returns an integer. The function will do the following (in the order specified): Note that for these detailed instructions, each line corresponds to one line of code you need to write. - 1) convert all characters of the string into lowercase - 2) strip the string of all leading and trailing whitespace - 3) replace any occurences of 'first' with '1' - 4) replace any occurences of 'second' with '2' - 5) replace any occurences of 'third' with '3' - 6) replace any occurences of 'fourth' with '4' - 7) replace any occurences of 'fifth' with '5' - 8) replace any occurences of 'sixth' with '6' - 9) replace any occurences of 'freshman' with '1' - 10) replace any occurences of 'sophomore' with '2' - 11) replace any occurences of 'junior' with '3' - 12) replace any occurences of 'senior' with 4' - 13) replace any occurences of 'year' with '' (remove it from the string) - 14) replace any occurences of 'th' with '' (remove it from the string) - 15) replace any occurences of 'rd' with '' (remove it from the string) - 16) replace any occurences of 'nd' with '' (remove it from the string) - 17) strip the string of all leading and trailing whitespace (again) - 18) If the resulting string is a number and it is less than 10, then cast it into an integer and return that value - 19) Else return np.nan to symbolize that the student's response was not a valid entry HINTS: you will need to use the functions 'lower()', 'strip()', isnumeric() and 'replace()'", "1f) Transform 'year' column Use 'standardize_year' to transform the data in column 'What year (in school) are you?'. Hint: use the apply function AND remember to save your output inside the dataframe", "Assuming that all is correct up to this point, the line below should show all values now found in df. It should look a lot better. With this data, we can now make insightful analyses. You should see an array with elements 1,2,3,4,5,6 and nan (not necessarily in that order). Note that if you check the data type of this column, you'll see that pandas converts these numbers to 'float', even though the applied function returns 'int', because 'np.nan' is considered a float. This is fine.", "Let's do it again. Let's take a look at the responses in the 'weight' column, and then standardize them.", "1g) Standardize 'weight' column Write a function named 'standardize_weight' that takes in as input a string and returns an integer. The function will do the following (in the order specified): - 1) convert all characters of the string into lowercase - 2) strip the string of all leading and trailing whitespace - 3) replace any occurences of 'lbs' with '' (remove it from the string) - 4) replace any occurences of 'lb' with '' (remove it from the string) - 5) replace any occurences of 'pounds' with '' (remove it from the string) - 6) If the string contains the substring 'kg', then: - 6.1) replace 'kg' with '' - 6.2) cast the string into an integer type using the function 'int()' - 6.3) multiply the resulting integer by 2 (an approximate conversion of kilograms to pounds) - 6.4) cast the resulting number back into a string with the function 'str()' - 7) Strip the string of its whitespaces (again) - 8) If the resulting string is numeric: cast it into an integer and return the resulting value - 9) Else: return np.nan", "1h) Transform 'weight' column Use 'standardize_weight' to transform the data in the 'weight' column. Hint: use the apply function AND remember to save your output inside the dataframe", "Now, let's see the result of our hard work . The code below should output all numbers (or nan).", "So far, you've gotten a taste of what it is like to deal with messy data. It's not easy, as you can tell. The last variable we need to standardize for the purposes of our analysis is 'height'. We will standardize that one for you. Do read the code below and try to understand what it is doing.", "It seems like we'll have to handle different measurement systems. Ugh, ok... Let's write a function that converts all those values to inches", "Applying the transformation and dropping invalid rows", "Check the height data, after applying our standardization", "Ensuring that the data types are correct - type cast age to int.", "Check that the dataframe has the right number of columns. If this doesn't pass - check your code in the section above.", "Part 2: Exploratory Data Vizualization First, we need to do some exploratory data visualization, to get a feel for the data. For plotting questions, do not change or move the 'plt.gcf()' lines.", "2a) Scatter Matrix Plot the data, using scatter_matrix, from Pandas. Assign it to a variable called 'fig'.", "2b) Bar Chart Plot a bar chart showing the number of students in each majors. Hints: you can use 'value_counts' to get the counts for each major. You can then use the 'plot' method from pandas for plotting - you don't need matplotlib.", "2c) Histogram for COGSCI Plot a histogram of the height data for all students who wrote 'COGSCI' as their major.", "2d) Histogram for COMPSCI Plot a histogram of the height data for all students who wrote 'COMPSCI' as their major.", "Part 3: Exploring The Data Beyond just plotting the data, we should check some other basic properties of the data. This serves both as a way to get a 'feel' for the data, and to look for any quirks or oddities about the data, that may indicate issues that need resolving. To do this, let's explore that data a bit (not limiting ourselves to only features that we plan to use - exploring the dataset as a whole can help us find any issues). Notes: - You answers should NOT be pandas objects (Series or DataFrames), extract answers so the variables are ints, floats or strings (as appropriate). - You must answer these questions programmatically: do not count / check and hard code particular values.", "3a) Number of majors How many different majors are in the dataset? Save this number to a variable 'n_majors'.", "3b) Range of 'age' What is the range (max value - min value) of ages in the dataset? Save this number to a variable 'r_age'", "3c) Most popular ice-cream flavour What is the most popular ice cream flavour? Save the ice cream name to the variable 'f_ice', and the number of people who like it to a variable 'n_ice'. Hint: you can get these values using the 'value_counts' method.", "3d) Unique favourite ice cream How many people have a unique favourite ice cream? (How many ice cream flavours are only 1 persons favourite?) Save this number to a variable 'u_ice'", "Part 4: Testing Distributions Soon, in the data analysis, we will want to run some statistical tests on our data. First, we should check the distributions! When using methods / statistical tests that make certain assumptions, it's always best to explicitly check if your data meet those assumptions (otherwise the results may be invalid). Let's test if our data are in fact normally distributed. See an example of how to test the disributions of data in the 'TestingDistributions' notebook in Tutorials.", "4a) Normal Test For each of 'h_co', and 'h_cs', use the 'normaltest' function to test for normality of the distribution. 'normaltest' returns two values, a test statistic and a p-value. Save these values as 'st_co', 'p_co', 'st_cs', and 'p_cs' respectively.", "4b) Are they normal? Have a look at the values returned. Based on these results, and using an alpha significance value of 0.01: Set boolean values (True, False) of whether each distribution can be considered to be normally distributed (set as True if the test supports it is normally distributed (or, more formally, we have not rejected the null hypothesis), and False if the test suggests the data is not normally distributed (we should reject the null hypothesis).", "Set boolean values, as specified above: - For the 'h_co' data, set a boolean value to the var 'is_n_co' - For the 'h_cs' data, set a boolean value to the var 'is_n_cs'", "CO data: plot the comparison of the data and a normal distribution (this code provided)", "CS data: plot the comparison of the data and a normal distribution (this code provided)", "Part 5: Data Analysis Now let's analyze the data, to address our research question. For the purposes of this analysis, let's assume we need at least 75 students per major to analyze the height data. This means we are only going to use data from people who wrote 'COGSCI' or 'COMPSCI' as their major.", "5a) Select the data Pull out the data we are going to use: - Save the height data for all 'COGSCI' majors to a variable called 'h_co' - Save the height data for all 'COMPSCI' majors to a variable called 'h_cs'", "5b) Mean height What is the average (mean) height for students from each major? Save these values to 'avg_h_co' for cogs students, and 'avg_h_cs' for cs students.", "Print out the average heights - this code provided", "Based on the cell above, it looks like there might indeed be a difference in the average height for students in cogs vs cs majors. Now we want to statistically test this difference. To do so, we will use a t-test.", "5c) T-test Use a t-test ('ttest_ind' function) to compare the two height distributions ('h_co' vs 'h_cs') 'ttest_ind' returns a t-statistic, and a p-value. Save these outputs to 't_val' and 'p_val' respectively.", "Check if statistical test passes significance, using an alpha value of 0.01. This code provided.", "Note: this test should pass significance. If it doesn't, double check your code up until this point. So - we've reached a conclusion! We're done right!? Nope. We have a first pass analysis, and an interim conclusion that happens to follow our hypothesis. Now let's try to break it. Let's explore some more You should always interrogate your findings, however they come out. What could be some alternate explanations, that would change our interpretations of the current analyses? In this case, we should be worried about confounding variables. We want to be able to say whether height relates to major specifically, but it could be the case that some other variable, that happens to differ between majors, better explains the differences in height. In this case, we also have data on gender. Let's check if differences in the gender ratio of the two majors can explain the difference in height.", "5d) Separate the genders Using 'value_counts' from pandas, extract the number of 'male' and 'female', separately for cogs and cs students. To do so, select from the df each major, separately, extract the gender column, and use the 'value_counts' method. - Save the counts for each gender for 'COGSCI' majors to a variable called 'g_co' - Save the counts for each gender for 'COMPSCI' majors to a variable called 'g_cs'", "5e) Ratio of women What is the ratio of women in each major? By ratio, we mean the proportion of students that are female, as a ratio. This will be value between 0.0 and 1.0 , calculated as F / ( M + F) - done separately for each major. You can use the g_co and g_cs variables to calculate these. - Save the ratio of women in COGSCI to a variable 'r_co' - Save the ratio of women in COMPSCI to a variable 'r_cs' Note: keep these numbers as ratios (they should be decimal numbers, less than 1)", "Make sure you print out and check the values of these ratios. They seem pretty different. We can actually ask, using a chi-squared test, whether this difference in gender-ratio between the majors is signficantly different. Code to do this is provided below.", "5f) Separate the majors Create a new dataframe, called df2 , which only includes data from COGSCI and COMPSCI majors. Hint: you can do this using the or operater | , with loc.", "5g) Pivot Tables Another way to look at these kinds of comparisons is pivot tables. Use the pandas pivot_table method to create pivot table, assign it to a variable pv . Set the values as height , and the indices as gender and major in the pivot table. Make sure you do this using df2 .", "Print out the pivot table you just created.Compare the average height values, split up by major and gender. Does it look like there are differences in heights by major, when spit up by major?", "Let's recap where we are: - Our initial hypothesis suggested there is a significant difference between heights of people in different majors. - However, further analyses suggested there may be a confounding variable, as there is also a significantly different gender balance between majors. Checking the average height, per major, split up by gender, suggests there may not be a difference between major, other than what is explained by gender. Now we want to statistically ask this question: is there still a different in height between majors, when controlling for differences in gender.", "Linear Models For the following question you will need to make some linear models, using Ordinary Least Squares (OLS). There is more than one way to do this in Python. For the purposes of this assignment, you must use the method that is outlined in the 'LinearModels' Tutorial, using patsy, and statsmodels. That is: - Create design matrices with 'patsy.dmatrices' - Iniliaize an OLS model with 'sm.OLS' - Fit the OLS model - Check the summary for results.", "5h) Predict height from major Create a linear model to predict height from major (using df2 as data). Use patsy.dmatrices to create the design matrices, calling the outputs outcome_1 , predictors_1 . Create an OLS model (sm.OLS) using outcome_1 and predictors_1 . Call it mod_1 . Fit the model, assigning it to res_1 .", "Print out the summary results of the model fitting", "5i) Analysis Based on the model you ran above (using alpha value of 0.01), does major significantly predict height? Set your answer as a boolean (True / False) to a variable called lm_1 .", "5j) Predict height from both major & gender Create a linear model to predict height from both major and gender (using df2 as data). Use patsy.dmatrices to create the design matrices, calling the outputs outcome_2 , predictors_2 . Create an OLS model (sm.OLS) using outcome_2 and predictors_2 . Call it mod_2 . Fit the model, assigning it to res_2 .", "5k) Analysis Based on the model you ran above (using alpha value of 0.01), does major significantly predict height? Set your answer as a boolean (True / False) to a variable called lm_2"], "topic": "addition"}
{"markdowns": ["Lab 1: Independent Component Analysis Machine Learning 2 (2017/2018) The lab exercises should be made in groups of two people. The deadline is Thursday, April 19, 23:59. Assignment should be submitted through BlackBoard! Make sure to include your and your teammates' names with the submission. Attach the .IPYNB (IPython Notebook) file containing your code and answers. Naming of the file should be \"studentid1\\_studentid2\\_lab \", for example, the attached file should be \"12345\\_12346\\_lab1.ipynb\". Only use underscores (\"\\_\") to connect ids, otherwise the files cannot be parsed. Notes on implementation: You should write your code and answers in an IPython Notebook: http://ipython.org/notebook.html. If you have problems, please ask. Use __one cell__ for code and markdown answers only! Put all code in the cell with the YOUR CODE HERE comment and overwrite the raise NotImplementedError() line. For theoretical questions, put your solution using LaTeX style formatting in the YOUR ANSWER HERE cell. Among the first lines of your notebook should be \"%pylab inline\". This imports all required modules, and your plots will appear inline. Large parts of you notebook will be graded automatically. Therefore it is important that your notebook can be run completely without errors and within a reasonable time limit. To test your notebook before submission, select Kernel -> Restart \\& Run All.", "Literature In this assignment, we will implement the Independent Component Analysis algorithm as described in chapter 34 of David MacKay's book \"Information Theory, Inference, and Learning Algorithms\", which is freely available here: http://www.inference.phy.cam.ac.uk/mackay/itila/book.html Read the ICA chapter carefuly before you continue! Notation $\\mathbf{X}$ is the $M \\times T$ data matrix, containing $M$ measurements at $T$ time steps. $\\mathbf{S}$ is the $S \\times T$ source matrix, containing $S$ source signal values at $T$ time steps. We will assume $S = M$. $\\mathbf{A}$ is the mixing matrix. We have $\\mathbf{X} = \\mathbf{A S}$. $\\mathbf{W}$ is the matrix we aim to learn. It is the inverse of $\\mathbf{A}$, up to indeterminacies (scaling and permutation of sources). $\\phi$ is an elementwise non-linearity or activation function, typically applied to elements of $\\mathbf{W X}$. Code In the following assignments, you can make use of the signal generators listed below.", "The following code generates some toy data to work with.", "1.1 Make mixtures (5 points) Write a function make_mixtures(S, A)' that takes a matrix of source signals $\\mathbf{S}$ and a mixing matrix $\\mathbf{A}$, and generates mixed signals $\\mathbf{X}$.", "1.2 Histogram (5 points) Write a function plot_histograms(X) that takes a data-matrix $\\mathbf{X}$ and plots one histogram for each signal (row) in $\\mathbf{X}$. You can use the np.histogram() (followed by plot ) or plt.hist() function. Plot histograms of the sources and the measurements.", "Which of these distributions (sources or measurements) tend to look more like Gaussians? Can you think of an explanation for this phenomenon? Why is this important for ICA?", "YOUR ANSWER HERE", "1.3 Implicit priors (20 points) As explained in MacKay's book, an activation function $\\phi$ used in the ICA learning algorithm corresponds to a prior distribution over sources. Specifically, $\\phi(a) = \\frac{d}{da} \\ln p(a)$. For each of the following activation functions, derive the source distribution they correspond to. $$\\phi_0(a) = -\\tanh(a)$$ $$\\phi_1(a) = -a + \\tanh(a)$$ $$\\phi_2(a) = -a^3$$ $$\\phi_3(a) = -\\frac{6a}{a^2 + 5}$$ Give your answer without the normalizing constant, so an answer of the form $p(a) \\propto \\verb+[answer]+$ is ok.", "To normalize, we make sure that: $$ k \\int_{-\\infty}^{\\infty} p(a) da = 1 $$thus k is the normalizing constant. $$ $$ Case 1: $$ \\int - \\tanh(a) da = - \\log (\\cosh(a)) $$ $$ p(a) =\\exp \\left[- \\log (\\cosh(a)) \\right] = \\frac{1}{\\cosh(a)} $$ $$ k \\int_{-\\infty}^{\\infty} \\frac{1}{\\cosh(a)} da = 1 = \\pi $$ $$ k = \\pi $$ Case 2: $$ \\int -a + \\tanh(a) da = \\log (\\cosh(a)) - \\frac{a^2}{2} $$ $$ p(a) =\\exp \\left[ \\log (\\cosh(a)) - \\frac{a^2}{2} \\right] = \\exp^{-0.5a^2} \\cosh(a) $$ $$ k \\int_{-\\infty}^{\\infty} \\exp^{-0.5a^2} \\cosh(a) da = 1 = k \\sqrt{2 e \\pi} $$ $$ k = \\frac{1}{\\sqrt{2 e \\pi}} $$ Case 3: $$ \\int - a^3 da = \\frac{-a^4}{4} $$ $$ p(a) =\\exp \\left[ \\frac{-a^4}{4} \\right] = \\exp^{\\frac{-a^4}{4}} $$ $$ k \\int_{-\\infty}^{\\infty}\\exp^{\\frac{-a^4}{4}} da = 1 = k \\frac{\\Gamma(0.25)}{\\sqrt{2}} $$ $$ k = \\frac{\\sqrt{2}}{\\Gamma(0.25)} $$ Case 4: $$ \\int \\frac{-6a}{a^2 + 5} da = -3 \\log (a^2 + 5) $$ $$ p(a) = \\exp \\left[ -3 \\log (a^2 + 5) \\right] = \\frac{1}{(a^2 + 5)^3} $$ $$ k \\int_{-\\infty}^{\\infty} \\frac{1}{(a^2 + 5)^3} da = 1 = k \\frac{3\\pi}{200\\sqrt{5}} $$ $$ k = \\frac{200\\sqrt{5}}{3\\pi} $$", "Plot the activation functions and the corresponding prior distributions, from $a = -5$ to $5$ (hint: use the lists defined in the cell above). Compare the shape of the priors to the histogram you plotted in the last question.", "Comparison", "All four priors have a shape that resembles a Gaussian function somehow. The sawtooth and triangle histogram are best represented by p_2, because it is the most square of all four priors. The sine and square wave can not be matched to any of the four priors while the histogram corresponding to 'random' could be matched by p_0 op p_3, since these priors best resemble a Gaussian shape.", "1.4 Whitening (15 points) Some ICA algorithms can only learn from whitened data. Write a method whiten(X) that takes a $M \\times T$ data matrix $\\mathbf{X}$ (where $M$ is the dimensionality and $T$ the number of examples) and returns a whitened matrix. If you forgot what whitening is or how to compute it, various good sources are available online, such as http://courses.media.mit.edu/2010fall/mas622j/whiten.pdf. Your function should also center the data before whitening.", "1.5 Interpret results of whitening (10 points) Make 3 figures, one for the sources, one for measurements and one for the whitened measurements. In each figure, make $5 \\times 5$ subplots with scatter plots for each pair of signals. Each axis represents a signal and each time-instance is plotted as a dot in this space. You can use the plt.scatter() function. Describe what you see. Now compute and visualize the covariance matrix of the sources, the measurements and the whitened measurements. You can visualize each covariance matrix using this code: python Dummy covariance matrix C; C = np.eye(5) ax = imshow(C, cmap='gray', interpolation='nearest')", "Are the signals independent after whitening?", "ANSWER: Yes, they are completely independent as the Xw plot indicates that there is no correlation between the datapoints. (Datapoints distributed in a circular pattern means no correlation)", "1.6 Covariance (5 points) Explain what a covariant algorithm is.", "ANSWER: A covariant algorithm has a learning algorithm in the form: $$\\Delta w_i = \\eta \\sum_{i'} M_{ii'} \\frac{\\partial L }{\\partial w_i}$$ Other than having a non-covariant (\"steepest descent) algorithm like: $$\\Delta w_i = \\eta \\frac{\\partial L }{\\partial w_i}$$ ... which does not contain the positive-definite $M$ matrix that makes the algorithm covariant. This M matrix is derived from the curvature and metrics of the log-likelihood, and allows for The covariant algorithm allows for consistency in dimensionality and not makes use of matrix inversions, which makes training faster by not having a exponential 'log-like' learning curve. Source: http://www.inference.org.uk/mackay/ica.pdf", "1.7 Independent Component Analysis (25 points) Implement the covariant ICA algorithm as described in MacKay. Write a function ICA(X, activation_function, learning_rate) , that returns the demixing matrix $\\mathbf{W}$. The input activation_function should accept a function such as lambda a: -tanh(a) . Update the gradient in batch mode, averaging the gradients over the whole dataset for each update. Make it efficient, so use matrix operations instead of loops where possible (loops are slow in interpreted languages such as python and matlab, whereas matrix operations are internally computed using fast C code). Experiment with the learning rate and the initialization of $\\mathbf{W}$. Your algorithm should be able to converge (i.e. np.linalg.norm(grad) < 1e-5 ) in within 10000 steps.", "1.8 Experiments (5 points) Run ICA on the provided signals using each activation function $\\phi_0, \\ldots, \\phi_3$ (or reuse W_estimates ). Use the found demixing matrix $\\mathbf{W}$ to reconstruct the signals and plot the retreived signals for each choice of activation function.", "1.9 Audio demixing (10 points) The 'cocktail party effect' refers to the ability humans have to attend to one speaker in a noisy room. We will now use ICA to solve a similar but somewhat idealized version of this problem. The code below loads 5 sound files and plots them. Use a random non-singular mixing matrix to mix the 5 sound files. You can listen to the results in your browser using play_signals , or save them to disk if this does not work for you. Plot histograms of the mixed audio and use your ICA implementation to de-mix these and reproduce the original source signals. As in the previous exercise, try each of the activation functions. Keep in mind that this problem is easier than the real cocktail party problem, because in real life there are often more sources than measurements (we have only two ears!), and the number of sources is unknown and variable. Also, mixing is not instantaneous in real life, because the sound from one source arrives at each ear at a different point in time. If you have time left, you can think of ways to deal with these issues.", "Report your results. Using which activation functions ICA recovers the sources?", "ANSWER: Activation functions $\\phi_0$ and $\\phi_3$ recover the source sounds correctly.", "1.10 Excess Kurtosis (15 points) The (excess) kurtosis is a measure of 'peakedness' of a distribution. It is defined as $$ \\verb+Kurt+[X] = \\frac{\\mu_4}{\\sigma^4} - 3 = \\frac{\\operatorname{E}[(X-{\\mu})^4]}{(\\operatorname{E}[(X-{\\mu})^2])^2} - 3 $$ Here, $\\mu_4$ is known as the fourth moment about the mean, and $\\sigma$ is the standard deviation. The '-3' term is introduced so that a Gaussian random variable has 0 excess kurtosis. We will now try to understand the performance of the various activation functions by considering the kurtosis of the corresponding priors, and comparing those to the empirical kurtosis of our data. 1.10.1 (10 points) First, compute analytically the kurtosis of the four priors that you derived from the activation functions before. You may find it helpful to use an online service such as [Wolfram Alpha](https://www.wolframalpha.com/) or [Integral Calculator](https://www.integral-calculator.com/) to (help you) evaluate the required integrals. Give your answer as both an exact expression as well as a numerical approximation (for example $\\frac{\\pi}{2} \\approx 1.571$).", "Note that all priors have a mean of 0, because they are symmetric and centered around x = 0. $$ $$ Case 1: $$ p(a) = \\frac{1}{\\pi cosh(a)} $$ $$ Kurt[X] = \\frac{\\mu_4}{\\sigma_4}-3 = \\frac{E[(x- \\mu)^4]} {E[(x-\\mu)^2]^2} =\\frac{E[x^4]} {E[x^2]^2} -3$$ $$ = \\frac{ \\int_{\\infty}^{\\infty} \\frac{a^4}{\\pi \\cosh(a)} da } { \\left( \\int_{\\infty}^{\\infty} \\frac{a^2}{\\pi \\cosh(a)} da \\right)^2 } -3 = \\frac{\\frac{5\\pi^4}{16}}{\\frac{\\pi^4}{16}}-3 = 5-3 = 2$$ Case 2: $$ p(a) = \\frac{1}{\\sqrt{2 \\pi e}} \\exp^{-0.5a^2} \\cosh(a) $$ $$ Kurt[X] = \\frac{\\int_{-\\infty}^{\\infty} \\frac{a^4}{\\sqrt{2 \\pi e}} \\exp^{-0.5a^2} \\cosh(a) da}{ \\left( \\int_{-\\infty}^{\\infty} \\frac{a^2}{\\sqrt{2 \\pi e}} \\exp^{-0.5a^2} \\cosh(a) da \\right)^2}-3 = \\frac{10}{4} - 3 = -0.5$$", "Case 3: $$ p(a) = \\frac{\\sqrt{2}}{\\Gamma(0.25)} e^{\\frac{-a^4}{4}} $$ $$ Kurt[X] = \\frac{ \\int_{-\\infty}^{\\infty}a^4 \\frac{\\sqrt{2}}{\\Gamma(0.25)} e^{\\frac{-a^4}{4}} da} {\\left( \\int_{-\\infty}^{\\infty}a^2 \\frac{\\sqrt{2}}{\\Gamma(0.25)} e^{\\frac{-a^4}{4}} da \\right)^2}- 3= \\frac{1}{\\left (\\frac{2\\operatorname{\\Gamma}\\left(\\frac{3}{4}\\right)}{\\operatorname{\\Gamma}\\left(\\frac{1}{4}\\right)} \\right)^2}-3 = \\frac{1}{0.67597^2} -3= - 0.8 115$$ Case 4: $$ p(a) = \\frac{200\\sqrt{5}}{3\\pi} \\frac{1}{(a^2 + 5)^3} $$ $$ Kurt[X] = \\frac { \\int_{-\\infty}^{\\infty} a^4 \\frac{200\\sqrt{5}}{3\\pi} \\frac{1}{(a^2 + 5)^3} da }{ \\left( \\int_{-\\infty}^{\\infty} a^2 \\frac{200\\sqrt{5}}{3\\pi} \\frac{1}{(a^2 + 5)^3} da \\right)^2 }-3 = \\frac{25}{\\left(\\frac{5}{3}\\right)^2} - 3 = 6$$"], "topic": "addition"}
{"markdowns": ["Instructions for the Assignment", "1. Unzip the whole zipped folder and not individual files 2. Don't rename the files. Still, if you wan't to rename them, copy to a new notebook 3. When uploading a) put your code in the unrenamed files b) copy both the unrenamed files with your code, even if you have done only 1 or done none, to a folder c) Rename the folder with your full ID d) zip the folder (.zip only) by right-cicking and \"Send to\" e) Upload the zipped file on to the portal f) If you get logged out during upload, log back in and check", "The Delivery Medoids Let's say you're the strategic head at Amazon India, who plan to introduce improve their delivery system to better serve the Indian market. The existing delivery network is crappy, and simply transports items from the seller's location to the customer's location. To improve upon the network, the company wants to you to come up with the number of warehouses along with strategic locations which will be used to improve upon delivery times. For this task, you are given a dataset of $N$ cities each having their latitude, longitude, and the average number of deliveries made to the city per month. The first and probably the easiest method that comes to your mind is to apply the K-means algorithm to form $k$ clusters out of the $N$ cities such that the centroids of each of the $k$ clusters represent the location of a warehouse. But think on it again. Is it really a good idea? Sooner or later, you realize that neither K Means nor Fuzzy C Means might be a good approach to the problem, simply because the centroids must _exist_. Common sense suggests that the warehouses must be built _within_ an existing city and not in a far off wasteland. _What do you do now_ ? You think about tweaking the K means algorithm to choose a centroid out of the existing data points, and that is exactly what this problem is all about. We'll also find a way find the optimal number of warehouses required. But first, let's start off.", "Let's see what this data looks like.", "One of the variants of the K means algorithm which might be useful in our case is the K medoids algorithm. Study the algorithm below. The K-Medoids Algorithm Let's say we have $N$ data points $X_1, \\ldots, X_N$ and wish to form $K$ clusters. Here's what the algorithm looks like. <ol> <li>Select initial centers $c_1,\\ldots c_K$ according to a given metric, then repeat: <ul> <li>Minimize over $C$: for each $i=1,\\ldots N$, find the cluster center $c_k$ closest to $X_i$, and let $C(i)=k$ <li>Minimize over $c_1,\\ldots c_K$: for each $k=1,\\ldots K$, assign $c_k = X^ _k$, which is the _medoid_ of points in cluster $k$. A medoid is the point $X_i$ in cluster $k$ that minimizes $$J = \\sum_{C(j)=k} \\|X_j-X_i\\|_2^2$$ </ul> <li>Stop when within-cluster variation doesn't change </ol> In words: Cluster (label) each point based on the closest center Replace each center by the medoid of points in its cluster Note that if $A$ and $B$ are both $D$ dimensional vectors, $\\| A - B \\|_2$ is the $L_2$ norm, so that $\\| A - B \\|_2^2$ is essentially the sum of squared errors. i.e, $$\\| A - B \\|_2^2 = (A_1 - B_1)^2 + (A_2 - B_2)^2 \\ldots (A_D - B_D)^2 $$", "Now that you got a gist of the algorithm, read the docstrings in red highlighted in red and complete the functions below. First, let's initialize the centres. It makes sense to initialize the centres with the $k$ most active cities in the dataset. Task 1 Complete the function below", "Let's plot the centres to see where exactly do they lie on a map.", "If you see that the locations marked in red are metropolitans, you're probably going in the right direction. Now that we have the initial centres, The next step is to associate every location with its closest centre. Since this is something that's been done repeatedly during the practise tests, the function is implemented below.", "Let's plot the clusters. Convince yourself that the function returns the closest centres indeed.", "The next step of the algorithm is to calculate a shift the existing centres to the new medoids. As described in the algorithm, you need to repeat the following for every cluster. 1. Calculate sum of squared L2 distances from every point in the cluster to every other point 2. The point which minimizes the above sum gets assigned as the new center Task 2 Complete the function below"], "topic": "file format"}
{"markdowns": ["Probability, Odds, and Odds ratios", "Probability: the number of ways that an event can occur divided by the total number of possible outcomes. The probability of drawing a red card from a standard deck of cards is 26/52 (50 percent). The probability of drawing a club from that deck is 13/52 (25 percent)", "What's the probability of getting heads in a fair coin flip?", "The odds for an event is the ratio of the number of ways the event can occur compared to the number of ways it does not occur. For example, using the same events as above, the odds for: drawing a red card from a standard deck of cards is 1:1; and drawing a club from that deck is 1:3.", "What's the odds of a fair coin flip?", "Suppose that 18 out of 20 patients in an experiment lost weight while using diet A, while 16 out of 20 lost weight using diet B.", "What's the probability of weight loss with diet A? What's the odds?", "What's the probablity of weight loss with diet B? What's the odds?", "What's the odds ratio?"], "topic": "constraint"}
{"markdowns": ["Week 11 Problem 2 If you are not using the Assignments tab on the course JupyterHub server to read this notebook, read [Activating the assignments tab](https://github.com/UI-DataScience/info490-fa16/blob/master/Week2/assignments/README.md). A few things you should keep in mind when working on assignments: 1. Make sure you fill in any place that says YOUR CODE HERE . Do not write your answer in anywhere else other than where it says YOUR CODE HERE . Anything you write anywhere else will be removed or overwritten by the autograder. 2. Before you submit your assignment, make sure everything runs as expected. Go to menubar, select _Kernel_, and restart the kernel and run all cells (_Restart & Run all_). 3. Do not change the title (i.e. file name) of this notebook. 4. Make sure that you save your work (in the menubar, select _File_ \u2192 _Save and CheckPoint_) 5. You are allowed to submit an assignment multiple times, but only the most recent submission will be graded.", "Lately, you have been studying on functional programming and it's high time that you got some practice with it! For this problem set, you will be focusing mainly on lambda functions, as well as map(), filter(), and reduce(). You will notice that I have already imported functools for you, so please bear that in mind. Problem 1. For this first problem, you will be making a function that will take a list of temperatures as well as a string that will convey if the temperatures inside are in Celcius or Fahrenheit. Using lambda functions, you should manipulate the lists so that the list of temperatures get converted to Kelvin. In order to do this, you should keep in mind the following equations: If you are converting from Celsius to Kelvin, you simply add 273.15 to the original temperature: $$Kelvin= Celsius + 273.15 $$ If you are converting from Fahrenheit to Kelvin, the easiest avenue is to first convert the temperature to Celsius, then convert the new Celsius temperatures to Kelvin, as was done above: $$Celsius = \\frac{5}{9}\\cdot(Fahrenheit-32) $$ $$Kelvin= Celsius + 273.15 $$ Note: you do not have to worry about c_or_f being anything but \"cel\" or \"far\"."], "topic": "addition"}
{"markdowns": ["> Jupyter slideshow: This notebook can be displayed as slides. To view it as a slideshow in your browser, type the following in the console: > > jupyter nbconvert [this_notebook.ipynb] --to slides --post serve > To toggle off the slideshow cell formatting, click the CellToolbar button, then View --> Cell Toolbar --> None .", "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\"> Regular Expressions _Authors: Alex Combs (NYC)_ --- <a id=\"learning-objectives\"></a> Learning Objectives After this lesson, you will be able to: - Define regular expressions. - Use regular expressions to match text. - Demonstrate how to use capturing and non-capturing groups. - Use advanced methods such as lookaheads.", "Lesson Guide - [Exploring regex ]( exploring-regex) - [The Most Famous Quote in regex-dom ]( most-famous-quote-in-regex-dom) - [So, What Does a Regular Expression Look Like?]( so-what-does-a-regular-expression-look-like) - [The History of Regular Expressions]( the-history-of-regular-expressions) - [Where are regex Implemented?]( where-is-regex-implemented) - [Basic Regular Expression Syntax]( basic-regular-expression-syntax) - [Literals]( literals) - [Character Classes]( character-classes) - [Character Classes Can Also Accept Certain Ranges]( character-classes-can-also-accept-certain-ranges) - [Character Class Negation]( character-class-negation) - [Exercise 1]( exercise-) - [What Happens if We Put Two Character Class Brackets Back to Back?]( what-happens-if-we-put-two-character-class-brackets-back-to-back) - [Shorthand for Character Classes]( shorthand-for-character-classes) - [Special Characters]( special-characters) - [Exercise 2]( exercise-2) - [The Dot]( the-dot) - [Anchors]( anchors) - [Exercise 3]( exercise-3) - [Modifiers]( modifiers) - [Quantifiers]( quantifiers) - [Greedy and Lazy Matching]( greedy-and-lazy-matching) - [Exercise 4]( exercise-4) - [Groups and Capturing]( groups-and-capturing) - [Exercise 5]( exercise-) - [Alternation]( alternation) - [Word Border]( word-border) - [Lookahead]( lookahead) - [Exercise 6]( exercise-5) - [ regex in Python and pandas ]( regex-in-python-and-pandas) - [ regex ' .search() Method]( regex-search-method) - [ regex ' .findall() method]( regex-findall-method) - [Using pandas ]( using-pandas) - [ str.contains ]( strcontains) - [ str.extract ]( strextract) - [Independent Practice]( independent-practice) - [Extra Practice]( extra-practice)", "<img src=\"../assets/regex1.png\"> <br> <center> as in </center> <br> <img src=\"../assets/regex2.png\"> <br> <center> not as in </center> <img src=\"../assets/regex3.png\">", "<a id=\"exploring-regex\"></a> Exploring regex --- [RegExr](http://regexr.com/) lets you explore regex . - Copy the text in the cell below into the body of the website linked above. - Make sure to click on flags in the upper right-hand corner, and check that g and m are clicked.", "1. This is a string 2. That is also a string 3. This is an illusion 4. THIS IS LOUD that isn't thus bob this is bob bob bob_ ralph_ bobbobbobbybobbob ababababab 6. tHiS iS CoFu SEd 777. THIS IS 100%-THE-BEST!!! 8888. this_is_a_fiiile.py hidden bob", "<a id=\"most-famous-quote-in-regex-dom\"></a> The Most Famous Quote in regex-dom >\"Some people, when confronted with a problem, think 'I know, I'll use regular expressions.' Now they have two problems.\" \u2014 Jamie Zawinski (Netscape engineer)", "<a id=\"so-what-does-a-regular-expression-look-like\"></a> So, What Does a Regular Expression Look Like?", "/^([a-z0-9_\\.-]+)@([\\da-z\\.-]+)\\.([a-z\\.]{2,6})$/", "<img src=\"../assets/regex4.png\">", "<a id=\"the-history-of-regular-expressions\"></a> The History of Regular Expressions --- Regular expressions and neural nets have a common ancestry in the work of McColloch and Pitts (1943) and their attempt to computationally represent a model of a neuron. This work was picked up by Steve Kleene (Mr. \\ ) and developed further into the idea of regular expressions. His idea was then popularized by its inclusion in Unix in the 1970s, in the form of [ grep ](http://opensourceforu.com/2012/06/beginners-guide-gnu-grep-basics-regular-expressions/). Its inclusion in PERL in the 1980s cemented its popularity. Here's [the story of Walter Pitts](http://nautil.us/issue/21/information/the-man-who-tried-to-redeem-the-world-with-logic).", "<a id=\"where-is-regex-implemented\"></a> Where are regex Implemented? --- There are any number of places where regex s can be run \u2014 from your text editor, to the bash shell, to Python, and even SQL. It is typically baked into the standard libary of programming languages. In Python, it can be imported like so: python import re", "<a id=\"basic-regular-expression-syntax\"></a> Basic Regular Expression Syntax --- <a id=\"literals\"></a> Literals Literals are essentially just what you think of as characters in a string. For example: a b c X Y Z 1 5 100 These are all considered literals.", "<a id=\"character-classes\"></a> Character Classes A character class is a set of characters matched as an \"or.\" [io] So, this class would run as \"match either i or o.\" You can include as many characters as you like in between the brackets. Character classes match only a single character.", "<a id=\"character-classes-can-also-accept-certain-ranges\"></a> Character Classes Can Also Accept Certain Ranges For example, the following will all work: [a-f] [a-z] [A-Z] [a-zA-Z] [1-4] [a-c1-3]", "<a id=\"character-class-negation\"></a> Character Class Negation We can also add negation to character classes. For example: [^a-z] This means match ANYTHING that is NOT a through z .", "<a id=\"exercise-\"></a> Exercise 1 --- <a id=\"what-happens-if-we-put-two-character-class-brackets-back-to-back\"></a> What Happens If We Put Two Character Class Brackets Back to Back? Using RegExr and the text snippet from earlier, match \"That\", \"that\" , and \"thus\" \u2014 but not \"This\" and \"this\" \u2014 using the following: - One literal - Two character classes (no negation) - One negation in a character class", "Solution [Tt]h[^i][st] Solution Breakdown: [Th] = _'T' or 't'_ h = _'h'_ [^i] = Anything that is _not_ 'i' [st] =_'s' or 't'_", "<a id=\"shorthand-for-character-classes\"></a> Shorthand for Character Classes --- \\w - Matches word characters (includes digits and underscores) \\W - Matches what \\w doesn't \u2014 non-word characters \\d - Matches all digit characters \\D - Matches all non-digit characters \\s - Matches whitespace (including tabs) \\S - Matches non-whitespace \\n - Matches new lines \\r - Matches carriage returns \\t - Matches tabs", "These can also be placed into brackets like so: [\\d\\t] [^\\d\\t]", "<a id=\"special-characters\"></a> Special Characters --- Certain characters must be escaped with a backslash: \" \\ .\" These include the following: . ? \\ { } ( ) [ ] + - & < >", "<a id=\"exercise-2\"></a> Exercise 2 --- Use RegExr and our text snippet to match all digits. Do this three ways: - First, with character classes - Second, with the shorthand character classes - Third, using only negation", "Solution 1. [0-9] 2. \\d 3. [^\\D] or [^a-zA-Z\\s\\%\\'!\\-\\._] >_The latter option of solution 3 is specific to our text block, as we explicitly specify the special characters to exclude._", "<a id=\"the-dot\"></a> The Dot --- The dot matches any single character.", "<a id=\"anchors\"></a> Anchors --- Anchors are used to denote the start and end of a line. ^ - Matches the start of the line $ - Matches the end of the line Example: bash ^Now - Matches the word \"Now\" when it occurs at the beginning of a line. country$ - Matches the word \"country\" when it occurs at the end of a line.", "<a id=\"exercise-3\"></a> Exercise 3 --- Use an anchor and a character class to find the bab and the bob at the end of the line, but not elsewhere.", "Solution b[oa]b$", "<a id=\"modifiers\"></a> Modifiers --- Modifiers control the following: g - Global match (matches every occurance in the text, rather than just the first) i - Case insensitivity m - Multiline (modifies how ^ and $ work)", "<a id=\"quantifiers\"></a> Quantifiers --- Quantfiers adjust how many items are matched. - Zero or more + - One or more ? - Zero or one {n} - Exactly 'n' number {n,} - Matches 'n' or more occurrences {n,m} - Between 'n' and 'm'", "<a id=\"greedy-and-lazy-matching\"></a> Greedy and Lazy Matching --- By nature, .+ and . are greedy matchers. This means they will match for as many characters as possible (i.e., the longest match). This can be flipped to lazy matching (the shortest match) by adding a question mark: ? .", "<a id=\"exercise-4\"></a> Exercise 4 --- 1. Find bob , but only if it occurs three times in a row without any spaces. 2. Find bob if it occurs twice in a row, with or without spaces.", "Solution 1. (bob){3} 2. (bob)( )?(bob) or (bob ?){2}", "<a id=\"groups-and-capturing\"></a> Groups and Capturing --- In regex , parentheses \u2014 () \u2014 denote groupings. These groups can then be quantified. Additionally, these groups can be designated as either \"capture\" or \"non-capture.\" To mark a group as a capture group, just put it in parenthesis \u2014 (match_phrase). To mark it as a non-capture group, punctuate it like so \u2014 (?:match_phrase). Each capture group is assigned a consecutive number that can be referenced (e.g., $1, $2... ).", "<a id=\"exercise-5\"></a> Exercise 5 --- 1. Run the following in RegExr: (bob.?) (bob.?) . 2. Then, click on \"list\" at the bottom to open the tab and try entering $1 . 3. Now, enter $2 instead \u2014 what is the difference? 4. Change the code to make the first one a non-capture group. 5. Enter $1 again \u2014 what has changed?", "<a id=\"alternation\"></a> Alternation --- The pipe character \u2014 | \u2014 can be used to denote an OR relation, just like in Python. For example, (bob|bab) or (b(o|a)b) .", "<a id=\"word-border\"></a> Word Border --- The word border \u2014 \\b \u2014 limits matches to those that mark the boundaries of words. These borders can be used on both the left and right sides of the match.", "<a id=\"lookahead\"></a> Lookahead --- There are two types of lookaheads: postive and negative. (?=match_text) \u2014 A postive lookahead says, \"only match the current pattern if it is followed by another pattern.\" (?!match_text) \u2014 A negative lookahead says the opposite. Examples: - that(?=guy) \u2014 Only match \"that\" if it is followed by \"guy.\" - these(?!guys) \u2014 Only match \"these\" if it is NOT follow by \"guys.\"", "<a id=\"exercise-6\"></a> Exercise 6 --- 1. Match bob only if it is followed by \"_\". 2. Match bob if it is followed by \"_\" or a new line character (Hint: How do we specify \"or\" in regex ?). 3. Match bob only if it isn't followed by a space or a new line character.", "Solution 1. (bob)(?=_) 2. (bob)(?=_|\\n) 3. (bob)(?!( |\\n))", "<a id=\"regex-in-python-and-pandas\"></a> Regex in Python and pandas --- Let's practice working with regex in Python and pandas using the string below."], "topic": "web page"}
{"markdowns": ["Calculus Solutions", "For each exercise, fill in the function according to its docstring.", "Derivatives", "Compute the following $$ \\frac{d}{dx}\\sin(x)e^x$$ $$ \\frac{\\partial}{\\partial x}\\sin(xy)e^x $$ $$ \\frac{\\partial^2}{\\partial x\\partial y}\\sin(xy)e^x $$"], "topic": "polynomial ring"}
{"markdowns": ["COGS 108 - Assignment 2: Data Exploration", "Important - Rename this file to 'A2_$ .ipynb' (filled in with your unique course ID) before you submit it. Submit it to TritonED. - This assignment has hidden tests: tests that are not visible here, but that will be run on your submitted assignment. - This means passing all the tests you can see in the notebook here does not guarantee you have the right answer! - Each coding question in this assignment only requires a small amount of code, about 1-3 lines. - If you find yourself writing much more than that, you might want to reconsider your approach. - A reminder that the Tutorials notebooks often contain similar examples to those used in the assignments. - This assignment also has some written answers, in which you will write a short response (in text, not code). - These questions are referred to as 'W ', and will be followed by a markdown cell in which you can write your answer. - These are short answer questions, prompting you to think about the approaches we are using. - You answers can and should be fairly brief, typically about 1-2 sentences.", "Part 1 - Data Wrangling For this assignment, you are provided with two data files: - age_steps.csv - name_income_id.json", "Part 2 - Data Cleaning Before analysis, data must be cleaned of missing or unusable data. Now that the datasets are joined into a single dataframe, the next steps are to remove or replace any bad data before moving onto any analyses.", "W1 - Missing Data You just used two different approaches for dealing with missing data: dropping the rows and replacing with the mean. For each approach, briefly describe below when you think it would be appropriate to use this approach. When is best to use one approach over the other?", "YOUR ANSWER HERE In the case of dropping rows, we consider the data to be neglegigble. That is, it would not skew our results, or greatly affect the final outcome we seek. In other cases, it may be that something like a piece of data concerning age may be indeterminate, or given a value such as -999 for missing age. Allowing such data would most likely skew our results, and since it cannot be determined with certainty, we exclude (drop) the row. With replacing rows with the mean, we consider the case of actually dropping the rows. Doing so may drop other pieces of data that are important to our analysis.", "Part 3: Data Visualization Here we will start exploring our data visually, using some plots. In particular, we will plot the distributions of each of our variables, in such a way as to let us get a sense of the data, and look for any problems. Pandas comes with some plotting capabilities. We will also be making histograms, which can be done with matplotlib. Here's an example of making histograms: http://matplotlib.org/1.2.1/examples/pylab_examples/histogram_demo.html This example has more than you need: - Titles, axis labels, etc are optional (but recommended) - You do no need to add a 'best fit' line. Notes: - Everywhere that we ask you to create a plot, make sure to leave the 'plt.gcf()' line at the end of the cell. - This is used to get a figure handle in order to programmatically evaluate your plots. - For all your histograms, use 25 bins", "W2 - What do you notice about data now that you've visualized it? For each of 'steps', 'income' and 'age', write a quick comment on each of anything that you notice from the above visualizations, that may need to be addressed before moving onto data analysis. In particular: - Are there outliers or bad values we need to fix? - Might we want to transform any of the distributions?", "YOUR ANSWER HERE Obvsering the two histograms between age and steps we see that the shape follows that of a normal distribution. If we think about the two pieces of data together, this makes since. We should expect persons of younger age and getting older to yield an increasing trend up to a certain point, and then steadily decreasing as they get older and older and moving around much less. No outliers or erroneous values need to be fixed. The income graph yields a decaying trend as observed above (i.e. it is not normal). A few outliers can be seen to the right. This may indicate some variability in the measurement or experimental error. Exluding the latter would change the form of the distrubtion.", "Part 4: Data Pre-Processing In the above sections, we did some basic data cleaning, and visualization. In practice, these two components of an analysis pipeline are often combined into an iterative approach, going back and forth between looking at the data, checking for issues, and then cleaning the data. In this next section we will continue with an iterative procedure of data cleaning and visualization, addressing some issues that we can start to see when visualizing the data.", "W3 - Transforming Distributions Once thing you might have noticed is that the income is not normally distributed. In the next steps, we will address this by log transforming the income data. First though, we will have to deal with any income values that are 0. Note that these values are not impossible values, but may, for example, reflect people who are unemployed. Why are are we log-transforming the data, and how does that help us? Why do we have to do something special with 0's? Hint: calculate log10 of zero, and see what the answer is.", "YOUR ANSWER HERE The log of 0 is undefined, so we must fix this. One way is to replace the 0 values with 1, where 1 would indicate \"0 income\" so to speak, and in this way, we can now use the log function on our data set. Log transforming can be used when data does not follow normal assumptions, so we transform it to obtain normality. In this way we reduce skewedness and can more readily establish linear relationships.", "Part 5 Outliers So far, we have dealt with the most clearly wrong or impossible data values. We might still might need to deal with more subtle outliers: values that seem abnormally high or low. These values may seem suspicious, and they may skew our analyses. In this step, we are going to deal with outlier values by removing all rows in which the data for the steps column exceeds +/- 3 standard deviations above or below the mean. Note that when you perform this kind of outlier removal can have large impacts. For example, the procedure we take here might give quite different results if we did it before removing the -1 values in steps.", "Part 6 - Basic Analyses Now that we have wrangled and cleaned our data, we can start doing some simple analyses. Here we will explore some basic descriptive summaries of our data, look into the inter-relations (correlations) between variables, and ask some simple questions about potentially interesting subsets of our data.", "Part 7 - Predictions A frequent goal of data analysis is to understand so that we can make predictions about future or unseen data points. Here we will explore some basic predictions, looking into whether we might be able to predict income from our other variables. Notes: - Use the polyfit function from numpy, in the way that we did in 'SectionMaterials/02-DataAnalysis - The predictions for income should be in dollars (so you'll have to 'un-log' the prediction)", "W4 - Why do you think (whichever model you answered) is better?", "YOUR ANSWER HERE Age seems like a far more likely indicator of income rather than number of steps taken. Also, looking at the correlation table in 6a, we can see that the correlation between income and age (0.134383) is much closer to 1 than the correlation between income and steps (0.001249). This indicates that income-age is a better model for prediction.", "8 - Revisiting Missing Values Originally, we dealt with missing values by either dropping rows that contained them or by replacing them with the mean value of that column. Another approach one can take is to predict missing values, based on the rest of the available data."], "topic": "addition"}
{"markdowns": ["<img style=\"float: left; padding-right: 10px; width: 45px\" src=\"iacs.png\"> S-109A Introduction to Data Science Lab 4: Regularization and Cross Validation Harvard University <br> Summer 2018 <br> Instructors: Pavlos Protopapas and Kevin Rader<br> Lab Instructors: David Sondak and Will Claybaugh ---", "<font color='red'> Run the cell below to properly highlight the exercises</font>", "Table of Contents <ol start=\"0\"> <li> Learning Goals </li> <li> Review of regularized regression </li> <li> Ridge regression with one predictor on a grid </li> <li> Ridge regression with polynomial features on a grid</li> <li> Cross-validation --- Multiple Estimates </li> <li> Cross-validation --- Finding the best regularization parameter </li> </ol>", "Learning Goals In this lab, you will work with some noisy data. You will use simple linear and ridge regressions to fit linear, high-order polynomial features to the dataset. You will attempt to figure out what degree polynomial fits the dataset the best and ultimately use cross validation to determine the best polynomial order. Finally, you will automate the cross validation process using sklearn in order to determine the best regularization paramter for the ridge regression analysis on your dataset. By the end of this lab, you should: Really understand regularized regression principles. Have a good grasp of working with ridge regression through the sklearn API Understand the effects of the regularization (a.k.a penalization) parameter on fits from ridge regression Understand the ideas behind cross-validation Why is it necessary? Why is it important? Basic implementation details. Be able to use sklearn objects to automate the cross validation process. This lab corresponds to lectures 4 and 5 and maps on to homework 4 (and beyond).", "Part 1: Review of regularized regression We briefly review the idea of regularization as introduced in lecture. Recall that in the ordinary least squares problem we find the regression coefficients $\\boldsymbol{\\beta}\\in\\mathbb{R}^{m}$ that minimize the loss function \\begin{align } L(\\boldsymbol{\\beta}) = \\frac{1}{n} \\sum_{i=1}^n \\|y_i - \\boldsymbol{\\beta}^T \\mathbf{x}_i\\|^2. \\end{align } Recall that we have $n$ observations. Here $y_i$ is the response variable for observation $i$ and $\\mathbf{x}_i\\in\\mathbb{R}^{m}$ is a vector from the predictor matrix corresponding to observation $i$. The general idea behind regularization is to penalize the loss function to account for possibly very large values of the coefficients $\\boldsymbol{\\beta}$. Instead of minimizing $L(\\boldsymbol{\\beta})$, we minimize the regularized loss function \\begin{align } L_{\\text{reg}}(\\boldsymbol{\\beta}) = L(\\boldsymbol{\\beta}) + \\lambda R(\\boldsymbol{\\beta}) \\end{align } where $R(\\boldsymbol{\\beta})$ is a penalty function and $\\lambda$ is a scalar that weighs the relative importance of this penalty. In this lab we will explore one regularized regression model: ridge regression. In ridge regression, the penalty function is the sum of the squares of the parameters, which is written as \\begin{align } L_{\\text{ridge}}(\\boldsymbol{\\beta}) = \\frac{1}{n} \\sum_{i=1}^n \\|y_i - \\boldsymbol{\\beta}^T \\mathbf{x}_i\\|^2 + \\lambda \\sum_{j=1}^m \\beta_{j}^{2}. \\end{align } In lecture, you also learned about LASSO regression in which the penalty function is the sum of the absolute values of the parameters. This is written as, \\begin{align } L_{\\text{LASSO}}(\\boldsymbol{\\beta}) = \\frac{1}{n} \\sum_{i=1}^n \\|y_i - \\boldsymbol{\\beta}^T \\mathbf{x}_i\\|^2 + \\lambda \\sum_{j=1}^m |\\beta_j|. \\end{align } In this lab, we will show how these optimization problems can be solved with sklearn to determine the model parameters $\\boldsymbol{\\beta}$. We will also show how to choose $\\lambda$ appropriately via cross-validation.", "Dataset You will work with a synthetic dataset contained in data/noisypopulation.csv . The data were generated from a specific function $f\\left(x\\right)$ (the actual form will not be revealed to you in this lab). Noise was added to the function to generate synthetic, noisy observations via $y = f\\left(x\\right) + \\epsilon$ where $\\epsilon$ was drawn from a random distribution. The idea here is that in real life the data you are working with often comes with noise. Even if you could make observations at every single value of $x$, the true function may still be obscured. Of course, the samples you actually take are usually a subset of all the possible observations. In this lab, we will refer to observations at every single value of $x$ as the population and the subset of observations as in-sample y or simply the observations . The dataset contains three columns: 1. f is the true function value 2. x is the predictor 3. y is the measured response.", "In this lab, we will try out some regression methods to fit the data and see how well our model matches the true function f .", "Let's take a quick look at the dataset. We will plot the true function value and the population.", "It is often the case that you just can't make observations at every single value of $x$. We will simulate this situation by making a random choice of $60$ points from the full $200$ points. We do it by choosing the indices randomly and then using these indices as a way of getting the appropriate samples.", "Note: If you are not familiar with the numpy sort method or the numpy random.choice() method, then please take a moment to look them up in the numpy documentation.", "Moving on, let's get the $60$ random samples from our dataset.", "Let's take one more look at our data to see which points we've selected.", "Now we do our favorite thing and split the sample data into training and testing sets. Note that here we are actually getting indices instead of the actual training and test set. This is okay and is another way of generating train-test splits.", "Great! At this point we've explored our data a little bit, selected a sample of the dataset, and done a train-test split on the sample dataset.", "<hr style='height:2px'>", "Let's move on to the data analysis. We'll begin with ridge regression. In particular we'll do ridge regression on a single predictor and compare it with simple linear regression.", "To start, let's fit the old classic, linear regression.", "But wait! Unlike statsmodels , we don't get confidence intervals for the betas. Fortunately, we can bootstrap to build the confidence intervals", "<div class=\"exercise\"><b>Exercise 1</b></div> 1. In the code below, two key steps of bootstrapping are missing. Fill in the code to draw sample indices with replacement and to fit the model to the bootstrap sample. You'll need numpy 's np.random.choice . Here's the [function documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.choice.html) in case you need it. 2. Visualize the results, and use numpy 's np.percentile : [function documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.percentile.html).", "From the above, we find that the bootstrap $90\\%$ confidence interval is well away from $0$. We can confidently say that $\\beta_{1}$ is not secretly $0$ and we're being fooled by randomness.", "Next we'll dive into ridge regression!", "Part 2: Ridge regression with one predictor on a grid To begin, we'll use sklearn to do simple linear regression on the sampled training data. We'll then do ridge regression with the same data, setting the penalty parameter $\\lambda$ to zero. Setting $\\lambda = 0$ reduces the ridge problem to the simple ordinary least squares problem, so we expect the results of these models to be identical. We will store the regression coefficients in a dataframe for easy comparison. The cell below provides some code to set up the dataframe ahead of time. Notice that we don't know the actual values in the pandas series, so we just set them to NaN . We will overwrite these later.", "We start with simple linear regression to get the ball rolling.", "We will use the above $\\boldsymbol\\beta$ coefficients as a benchmark for comparision to the ridge method. The same coefficients can be obtained with ridge regression, which we demonstrate now. For reference, here is the ridge regression documentation: [sklearn.linear_model.Ridge](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html).", "The snippet of code below implements the ridge regression with $\\lambda = 0$. Note: The weight $\\lambda$ is referred to as alpha in the documentation. Remark: $\\lambda$ goes by many names including, but not limited to: regularization parameter, penalization parameter, shrinking parameter, and weight. Regardless of these names, it is a hyperparameter. That is, you set it before you begin the training process. An algorithm can be very sensitive to its hyperparameters and we will discuss how a method for selecting the \"correct\" hyperparameter values later in this lab.", "The beta coefficients for linear and ridge regressions coincide for $\\lambda = 0$, as expected. We plot the data and fits.", "<div class=\"exercise\"><b>Exercise 2</b></div> Explore the effect of $\\lambda$ on ridge regression. Make a plot with of the ridge regression predictions with $\\lambda = 0, 5, 10, 100$. Be sure to include a legend. What happens for very large $\\lambda$ (e.g. $\\lambda \\to \\infty$)? Your plot should look something like the following plot (doesn't have to be exact): ![](ridge_lambda.png)", "Part 2 Recap That was nice, but we were just doing simple linear regression. We really want to do more interesting regression problems like multilinear regression. We will do so in the next section.", "Part 3: Ridge regression with polynomial features on a grid", "Now we'll make a more complex model by adding polynomial features. Instead of building the linear model $y = \\beta_0 + \\beta_1 x$, we build a polynomial model $y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\ldots \\beta_d x^d$ for some $d$ to be determined. This regression will be linear though, since we'll be treating $x^2, \\ldots, x^d$ themselves as predictors in the linear model. The design matrix $\\mathbf{X}$ contains columns corresponding to $1, x, x^2, \\ldots, x^d$. To build it, we use sklearn . (The particular design matrix is also known as the [ Vandermonde matrix](https://en.wikipedia.org/wiki/Vandermonde_matrix)). For example, if we have three observations<br><br> \\begin{align } \\left\\{\\left(x_{1}, y_{1}\\right), \\left(x_{2}, y_{2}\\right), \\left(x_{3}, y_{3}\\right)\\right\\} \\end{align }<br> and we want polynomial features up to and including degree $4$, the design matrix looks like<br><br> \\begin{align } X = \\begin{bmatrix} x_1^0 & x_1^1 & x_1^2 & x_1^3 & x_1^4\\\\ x_2^0 & x_2^1 & x_2^2 & x_2^3 & x_2^4\\\\ x_3^0 & x_3^1 & x_3^2 & x_3^3 & x_3^4\\\\ \\end{bmatrix} = \\begin{bmatrix} 1& x_1^1 & x_1^2 & x_1^3 & x_1^4\\\\ 1 & x_2^1 & x_2^2 & x_2^3 & x_2^4\\\\ 1 & x_3^1 & x_3^2 & x_3^3 & x_3^4\\\\ \\end{bmatrix}. \\end{align }", "<div class=\"exercise\"><b>Exercise 3</b></div> 1. Make a toy vector called toy , where \\begin{align } \\mathrm{toy} = \\begin{bmatrix} 0 \\\\ 2 \\\\ 5 \\\\ \\end{bmatrix}. \\end{align } 2. Build the feature matrix up to (and including) degree $4$. Confirm that the entries in the matrix are what you'd expect based on the above discussion. Note: You may use sklearn to build the matrix using PolynomialFeatures() ."], "topic": "data analysis"}
{"markdowns": ["Lab 4: optimization Task 1: 1D optimization In this task you will write a function that passes <code>fixed_lift</code> from Lab 3 to an optimizer to find $z_u^{(2)}$ that will give a desired lift for a given angle of attack. First import the necessary modules (note that you will be using the <code>minimize</code> method from SciPy).", "First we'll start with a simple function $x^2$:", "We know the optimum should be zero at $x=0$. If we pass <code>square</code> to the <code>minimize</code> optimizer, and give it a starting point of 0.5 (here we'll pretend this is our best guess at the location of the minimum), we should get an answer close to, but not exactly $x=0$:", "Now write a function <code>one_dim_opt(x0, alpha, target_cl)</code>that finds the value of $z_u^{(2)}$ that minimizes the output of <code>fixed_lift</code> for a given $\\alpha$ and target $C_L$. Unlike the $x^2$ example above, you'll need to pass two extra arguments to <code>fixed_lift</code>, which you are not optimizing. Use the syntax <code>minimize(..., ..., args = (alpha, target_cl))</code> to pass these additional arguments. You can find out more about scipy.optimize [here](https://docs.scipy.org/doc/scipy-0.18.1/reference/tutorial/optimize.html )."], "topic": "addition"}
{"markdowns": ["IPython Notebooks You can run a cell by pressing [shift] + [Enter] or by pressing the \"play\" button in the menu. You can get help on a function or object by pressing [shift] + [tab] after the opening parenthesis function( You can also get help by executing: function? We'll use the following standard imports. Execute this cell first:", "Exercise: are first-borns more likely to be late? This exercise is based on [lecture material by Allen Downey](https://github.com/AllenDowney/CompStats.git). License: [Creative Commons Attribution 4.0 International](http://creativecommons.org/licenses/by/4.0/)", "Are first babies more likely to be late? ---------------------------------------- Allen Downey wrote a popular blog post about this topic: http://allendowney.blogspot.com/2011/02/are-first-babies-more-likely-to-be-late.html We are going to investigate the question for ourselves, based on data from the National Survey of Family Growth (NSFG).", "Use the Pandas read_csv command to load data/2002FemPreg.csv.gz .", "- The variable outcome encodes the outcome of the pregnancy. Outcome 1 is a live birth. - The variable pregordr encodes for first pregnancies (==1) and others (>1). - The variables prglngth encodes for the length of pregnancy up to birth.", "Let's visualize the number of births over different weeks:", "And here is the total number of babies born up to a certain week:", "Now, create a Pandas dataframe containing only the three columns of interest. Hint: loc"], "topic": "addition"}
{"markdowns": ["___ <a href='http://www.pieriandata.com'> <img src='../Pierian_Data_Logo.png' /></a> ___", "Natural Language Processing Project Welcome to the NLP Project for this section of the course. In this NLP project you will be attempting to classify Yelp Reviews into 1 star or 5 star categories based off the text content in the reviews. This will be a simpler procedure than the lecture, since we will utilize the pipeline methods for more complex tasks. We will use the [Yelp Review Data Set from Kaggle](https://www.kaggle.com/c/yelp-recsys-2013). Each observation in this dataset is a review of a particular business by a particular user. The \"stars\" column is the number of stars (1 through 5) assigned by the reviewer to the business. (Higher stars is better.) In other words, it is the rating of the business by the person who wrote the review. The \"cool\" column is the number of \"cool\" votes this review received from other Yelp users. All reviews start with 0 \"cool\" votes, and there is no limit to how many \"cool\" votes a review can receive. In other words, it is a rating of the review itself, not a rating of the business. The \"useful\" and \"funny\" columns are similar to the \"cool\" column. Let's get started! Just follow the directions below!", "Imports Import the usual suspects. :)", "The Data Read the yelp.csv file and set it as a dataframe called yelp.", "Check the head, info , and describe methods on yelp.", "Create a new column called \"text length\" which is the number of words in the text column.", "EDA Let's explore the data Imports Import the data visualization libraries if you haven't done so already.", "Use FacetGrid from the seaborn library to create a grid of 5 histograms of text length based off of the star ratings. Reference the seaborn documentation for hints on this", "Create a boxplot of text length for each star category.", "Create a countplot of the number of occurrences for each type of star rating.", "Use groupby to get the mean values of the numerical columns, you should be able to create this dataframe with the operation:", "Use the corr() method on that groupby dataframe to produce this dataframe:", "Then use seaborn to create a heatmap based off that .corr() dataframe:", "NLP Classification Task Let's move on to the actual task. To make things a little easier, go ahead and only grab reviews that were either 1 star or 5 stars. Create a dataframe called yelp_class that contains the columns of yelp dataframe but for only the 1 or 5 star reviews.", "Create two objects X and y. X will be the 'text' column of yelp_class and y will be the 'stars' column of yelp_class. (Your features and target/labels)", "Import CountVectorizer and create a CountVectorizer object.", "Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.", "Train Test Split Let's split our data into training and testing data. Use train_test_split to split up the data into X_train, X_test, y_train, y_test. Use test_size=0.3 and random_state=101", "Training a Model Time to train a model! Import MultinomialNB and create an instance of the estimator and call is nb", "Now fit nb using the training data.", "Predictions and Evaluations Time to see how our model did! Use the predict method off of nb to predict labels from X_test.", "Create a confusion matrix and classification report using these predictions and y_test", "Great! Let's see what happens if we try to include TF-IDF to this process using a pipeline.", "Using Text Processing Import TfidfTransformer from sklearn.", "Import Pipeline from sklearn.", "Now create a pipeline with the following steps:CountVectorizer(), TfidfTransformer(),MultinomialNB()", "Using the Pipeline Time to use the pipeline! Remember this pipeline has all your pre-process steps in it already, meaning we'll need to re-split the original data (Remember that we overwrote X as the CountVectorized version. What we need is just the text", "Train Test Split Redo the train test split on the yelp_class object.", "Now fit the pipeline to the training data. Remember you can't use the same training data as last time because that data has already been vectorized. We need to pass in just the text and labels", "Predictions and Evaluation Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results."], "topic": "data analysis"}
{"markdowns": ["<img src=\"https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png\" style=\"float: left; margin: 10px;\"> Classification Review and Churn Case Study --- Week 4 | Lesson 4.1 LEARNING OBJECTIVES After this lesson, you will be able to: - Describe examples in which optimizing precision over recall is desired and vice versa - Graph and Analyze ROC and Precision-Recall Curves - Demonstrate an understanding of feature engineering by applying it to a data set - Demonstrate and apply different methods to balance classification labels - Analyze and explain learning curves and what they are communicating about Logistic Regression performance - Synthesis previously learned material to increase the predictive performance of Logistic Regression - Interpret the odds ratios of Logistic Regression", "1 Exploratory Data Analysis 1.1 Clean Data", "Head Observations 1. There are a mixture of data types (strings and numerical) 2. Some columns are likely not useful for modeling (state, account lenght, area code, and phone). 3. We have features binary values that need to be transformed into one's and zeros. 4. The churn column name needs to be reformated and it appears to have stings values."], "topic": "context"}
{"markdowns": ["Working with Spatial Data and Networks Lab Preparing data, extracting features, visualization and modelling", "In this lab we'll explore terrorism, as described in [this](https://www.kaggle.com/START-UMD/gtd) dataset. The full codebook, describing all variables, their values, the data collection process, and many more details, is located [here](http://start.umd.edu/gtd/downloads/Codebook.pdf). We'll perform our usual process: Reading and inspecting the data Validating the data Exploring the data Creating inferences and getting to conclusions", "1. Read the dataset (1 point) The dataset is provided in the data folder. Read it into Python. You'll likely get an encoding error, so you can use encoding = \"cp1252\" . Use the column eventid as the index, since this is a good unique identifier and it's used in the dataset (e.g. the column related contains the eventid of all related crimes). You'll see the dataset has a lot of columns. For this lab, we aren't going to need all of them, only a few. Select these columns (and rename them as described in the tests below): eventid iyear, imonth, iday country_txt region_txt multiple (1 if it's a part of a multiple incident and 0 otherwise) latitude, longitude targtype1_txt (first target type) weaptype1_text (weapon type) gname (perpetrator group name) nkill (total number of fatalities) related (IDs of the related incidents) You'll have a total of 12 columns (the eventid which will become an index is not considered a typical column in a dataframe) and just over 170 thousand records.", "2. Convert columns (1 point) Convert the \"year\", \"month\" and \"day\" columns into a single date. Call the new column \"date\". Note that some months and/or days contain 0 as their values. This means that the exact month or day is not present or known. In these cases, write \"not a time\" (NaT) values in the \"date\" column. Do not remove those records! Also, convert the \"multiple\" column into a Boolean one.", "3. Show how the number of attacks evolves with time (1 point) Group all incidents by year. Create a line plot showing how the attacks evolve. Create a function which accepts a dataframe and returns the years with minimum and maximum incidents. Use this function to see the numbers for your dataset.", "4. Filter terror attacks (1 point) Get only recent attacks (from 1 Jan 2000 onwards, inclusive). Save them in the variable recent_attacks . Create another variable which contains attacks before that. Call that older_attacks . We'll compare these later."], "topic": "data analysis"}
{"markdowns": ["Homework 3 Note: Do not import any libraries besides those listed below!", "Q1 (10 points)", "Load the file into NumPy array and print first five rows of the populations data.", "What is the mean and standard deviation of the populations of each species for the years in the period?", "In which years did each species have the largest population?", "Which species has the largest population for each year?", "Which years any of the populations is above 50000?"], "topic": "human"}
{"markdowns": ["KNN (K-Nearest Neighbors Classification)", "Load in the Wisconsin breast cancer dataset. The attributes below will be the columns of the dataset. Attribute Information: (class attribute has been moved to last column) Attribute Values -- ----------------------------------------- 1. Sample code number id number 2. Clump Thickness 1 - 10 3. Uniformity of Cell Size 1 - 10 4. Uniformity of Cell Shape 1 - 10 5. Marginal Adhesion 1 - 10 6. Single Epithelial Cell Size 1 - 10 7. Bare Nuclei 1 - 10 8. Bland Chromatin 1 - 10 9. Normal Nucleoli 1 - 10 10. Mitoses 1 - 10 11. Class: (2 for benign, 4 for malignant)", "The column names are taken from the dataset info file. Create an array with the column names and assign them as the header when loading the csv.", "The class field is coded as \"2\" for benign and \"4\" as malignant. - Let's recode this to a binary variable for classification, with \"1\" as malign and \"0\" as benign."], "topic": "nucleic acid"}
{"markdowns": ["<div style=\"background-image:url(images/meschede-seismic-waves.png); padding: 10px 30px 20px 30px; background-size:cover; background-opacity:50%; border-radius:5px\"> <p style=\"float:right; margin-top:20px; padding: 20px 20px 0px 20px; background:rgba(255,255,255,0.6); border-radius:10px;\"> <img width=\"400px\" src=images/obspy_logo_full_524x179px.png?raw=true> </p> <h1 style=\"color: 999\">IRIS-EarthScope Short Course</h1> <h5 style=\"color: FFF\">Bloomington/IN, August 2015</h5> <br/> <h2 style=\"color: EEE\">Python/ObsPy Introduction</h2> <br/> </div> <div align=\"right\">image by Matthias Meschede</div>", "Exercise -- The 2008 Mt. Carmel, Illinois, Earthquake and Aftershock Series Introduction from [\"The April 18, 2008 Illinois Earthquake: An ANSS Monitoring Success\" by Robert B. Herrmann, Mitch Withers, and Harley Benz, SRL 2008](http://srl.geoscienceworld.org/content/79/6/830.extract): \"The largest-magnitude earthquake in the past 20 years struck near Mt. Carmel in southeastern Illinois on Friday morning, 18 April 2008 at 09:36:59 UTC (04:37 CDT). The Mw 5.2 earthquake was felt over an area that spanned Chicago and Atlanta , with about 40,000 reports submitted to the U.S. Geological Survey (USGS) \u201cDid You Feel It?\u201d system. There were at least six felt aftershocks greater than magnitude 3 and 20 aftershocks with magnitudes greater than 2 located by regional and national seismic networks. Portable instrumentation was deployed by researchers of the University of Memphis and Indiana University (the first portable station was installed at about 23:00 UTC on 18 April). The portable seismographs were deployed both to capture near-source, high-frequency ground motions for significant aftershocks and to better understand structure along the active fault. [...]\" <img src=\"http://earthquake.usgs.gov/images/globes/40_-90.jpg\" alt=\"World map\" style=\"width: 250px;\"/> <img src=\"http://earthquake.usgs.gov/earthquakes/dyfi/events/us/2008qza6/us/us2008qza6_ciim_zoomout.jpg\" alt=\"Felt report map\" style=\"width: 700px;\"/> Web page hits at USGS/NEIC during 24 hours after the earthquake: - peak rate: 3,892 hits/second - 68 million hits in the 24 hours after the earthquake <img src=\"https://www.geophysik.uni-muenchen.de/~megies/.iris/.mtcarmel_figure1.jpg\" alt=\"USGS/NEIC web page hits\" style=\"width: 500px;\"/> Some links: - http://earthquake.usgs.gov/earthquakes/dyfi/events/us/2008qza6/us/index.html - http://earthquake.usgs.gov/earthquakes/eqinthenews/2008/us2008qza6/ summary - http://srl.geoscienceworld.org/content/79/6/830.extract - http://srl.geoscienceworld.org/content/82/5/735.short Again, please execute the following cell, it contains a few adjustments to the notebook.", "Use the search on the [ObsPy docs](https://docs.obspy.org/) for any functionality that you do not remember/know yet..! E.g. [searching for \"filter\"](https://docs.obspy.org/search.html?q=filter)...", "1. Download and visualize main shock Request information on stations recording close to the event from IRIS using the [ obspy.fdsn Client ](https://docs.obspy.org/packages/obspy.fdsn.html), print the requested station information.", "Download waveform data for the mainshock for one of the stations using the FDSN client (if you get an error, maybe try a different station and/or ask for help). Make the preview plot using ObsPy.", "Visualize a Spectrogram (if you have time, you can play around with the different parameters for the spectrogram). Working on a copy of the downloaded data, apply a filter, then trim the requested data to some interesting parts of the earthquake and plot the data again.", "Define a function plot_data(t) that fetches waveform data for this station and shows a preview plot of 20 seconds of data starting at a given time. It should take a UTCDateTime object as the single argument.", "Test your function by calling it for the time of the main shock", "2. Visualize aftershock and estimate magnitude Read file \" ./data/mtcarmel.mseed \". It contains data of stations from an aftershock network that was set up shortly after the main shock. Print the stream information and have a look at the network/station information, channel names time span of the data etc.. Make a preview plot.", "The strongest aftershock you see in the given recordings is at 2008-04-21T05:38:30 . Trim the data to this aftershock and make a preview plot and a spectrogram plot of it.", "Make a very simple approximation of the magnitude. Use the function provided below (after you execute the code box with the function you can call it anywhere in your code boxes). Demean the data and then determine the raw data's peak value of the event at one of the stations (e.g. using Python's max function or a NumPy method on the data array) and call the provided function for that value. (Note that this is usually done on the horizontal components.. we do it on vertical for simplicity here)", "Do the magnitude approximation in a for-loop for all stations in the Stream. Calculate a network magnitude as the average of all three stations.", "Define a function netmag(st) that returns a network magnitude approximation. It should take a Stream object (which is assumed to be trimmed to an event) as only argument. Use the provided mag_approx function and calculate the mean of all traces in the stream internally.", "Test your function on the cut out Stream object of the large aftershock from before.", "Advanced You can also download the station metadata using the FDSN client and extract poles and zeros information and directly use the estimateMagnitude function without using the hard-coded response information. 3. Detect aftershocks using triggering routines Read the 3-station data from file \"./data/mtcarmel.mseed\" again. Apply a bandpass filter, adjust it to the dominant event frequency range you have seen in the aftershock spectrogram before. Run a recursive STA/LTA trigger on the filtered data (see [ObsPy docs](https://docs.obspy.org/packages/autogen/obspy.core.stream.Stream.trigger.html)). The length of the sta window should be a bit longer than an arriving seismic phase, the lta window can be around ten to twenty times as long. Make a preview plot of the Stream object, now showing the characteristic function of the triggering. High trigger values indicate transient signals (of the frequency range of interest) that might be an event (or just a local noise burst on that station..). (play around with different values and check out the resulting characteristic function)", "We could now manually compare trigger values on the different stations to find small aftershocks, termed a network coincidence trigger. However, there is a convenience function in ObsPy's signal toolbox to do just that in only a few lines of code. Read the data again and apply a bandpass to the dominant frequencies of the events. Use the coincidenceTrigger function that returns a list of possible events (see the [ObsPy Tutorial](https://docs.obspy.org/tutorial/code_snippets/trigger_tutorial.html network-coincidence-trigger-example) for an example of a recursive STA/LTA network coincidence trigger). Print the length of the list and adjust the trigger-on/off thresholds so that you get around 5 suspected events. Print the first trigger in the list to show information on the suspected event."], "topic": "embedding"}
{"markdowns": ["Fraud Detection Algorithm for Banks As a data scientist at JPMorgan Chase & Co., you have been asked to design a fraud detection algorithm for identifying the cases of fraud in the bank database. You decide to use the Mamdani fuzzy inference process for the same. Input parameters: - Age of the account-holder - Salary of the account-holder - Pending Loan-Amount of the account holder Output parameters: - Probability of the current case being a fraud The definition for all the variables and their classes are as follows: Age of the account-holder - Young - Middle-aged - Old-age Salary of the account-holder - Low salary range - Medium salary range - High salary range Pending loan amount of the account holder - Low amount - Medium amount - High amount Fraud probability - Low probability - Medium probability - High probability The rules which are to be used for inference are as follows: 1. IF age==high AND salary==low THEN probability of fraud=high 2. IF age==low AND salary==high THEN probability of fraud=low 3. IF age==middle OR loan amount==middle THEN probability of fraud=medium", "Graded Components You will be implementing the following functions in this notebook - line(a, b, x) 1 Marks - up(a, b, x) 1.5 Mark - down(a, b, x) 1 Marks - triangle(a, b, x) 1.5 Mark - trapezoid(a, b, c, d, x) 1 Mark - def calculate_rule_strength(memberships) 1 Marks - def get_output_fuzzy_set(age, salary, loan_amount, X) 2 Marks - get_centroid(X, Y) 1 Mark Super Important: Whenever required to return an array in any function make sure its a numpy array. To convert a python list to a numpy array, use np.array(A) where A is a python list", "Task 1 Write a lambda to return $y$ for straight line $y(x)$ given $a, b, x$ where, $$y(a) = 0 $$ $$y(b) = 1 $$"], "topic": "addition"}
{"markdowns": ["Revisiting MLE", "As you recall, for linear regressions, we estimate parameters by deploying some kind of least square technique, the usual suspect being the ordinary least squares (OLS). However, a logistic regression breaks one of the cardinal assumptions of OLS, namely the normality of the target (dependent) variable, given the fact that logistics are binary. Therefore, you'll recall that we have to deploy the apparatus of the MLE to do parameter estimation for the right-hand side of the equation. Recall that MLE seek to find the maximum probability given the data at hand.", "Constructing the Logistic Regression", "As you recall, MLE's require a few ingredients, one is a well-defined likelihood function. Given, the fact that we are dealing with binary targets, we can easily model these using Bernoulli scheme: $$ P(x_i)^{r_i}(1-P(x_i)^{1-r_i})$$ Recall also that we seek to maximize the following: $$\\prod_{i=1}^n P(x_i)^{r_i}(1-P(x_i)^{1-r_i})$$, yet because off the annoying property of computing the derivatives numerically, it's best to transform this with the logarithm, which makes this into computing a derivative of a product chain to computing a derivative of a sum: $$\\sum_{i=1}^n [r_i ln(P(x_i)) + (1-r_i)ln(1-P(x_i))]$$ A much \"cleaner\" functional form with respect to computing a derivative. From here, the problem is transferred to solving a constraint optimization problem. Rarely will you have to do this by hand, and we did some simple finger exercises previously for you to get the feel of it, but in general, the software will handle these in the background for you. For those of you with the mathematical background, you should check up iterative numerical methods for least square problems.", "Some useful reserved words/functions in the Stan environment", "Remember The Docs are your friends, use them, study them, read them always, at night, in the mornings, while you're eating lunch, while you're taking care of your baby, while you're on a date... ALWAYS . http://pystan.readthedocs.io/en/latest/api.html", "Reintroducing the Polling data", "This case study will be based off of the data we used in Lab 1.4: http://www.stat.columbia.edu/~gelman/arm/examples/election88/, from the great one, Dr. Gelman, one of the foremost researchers in Bayesian analysis. Recall, this election was ultimately a major victory for the Grand Old Party (GOP/Republicans), and propelled George H.W. Bush (the father of George W. Bush) into the presidency. Import the data, and delete the Unnamed column", "Problem 1", "Load the data and check the first few observations to get a view of the data. Basically do what you did in the previous lab, you should be a champ at this by now !", "Make sure you have the same length of the data set (you will) so you know the file hasn't changed (at least with respect to length).", "Building a Logistic Regression in PyStan", "We're going use the old polling data differently today, we're going to build a bush-classifier. What does that mean? Since we have a column of data that is 1 for a polled for Bush, and 0 otherwise, we'll see if any of the data in the table are good predictors for someone who ended up polling for Papa Bush. The point of this exercise is not really to build a good predictor, but to gain more facility with the PyStan environment. So we're going to make some unrealstic assumptions in this first run so we can just get use to producing/constructing models with PyStan. For the PyStan model below, I want you to assume that all your parameters are normally distributed with 0 mean and variance 10. Realistic? No way. But it'll make the model specification go smoother. For those of you who want a challenge, after you get your first model working. Go back and think about what distributions you should use instead of normal to get a better fit.", "Problem 2", "Construct the PyStan Logistic Regression model", "Problem 3", "Build a dictionary for the 2-variate Pystan Logistic Regression and setup the Pystan model from the STAN C++ code written above, also print the model to observe the quartile and other summary stats. After, output the traceplot to observe the parameter graphs"], "topic": "constraint"}
{"markdowns": ["Basic Vector Operations", "In this notebook you will implement a number of basic vector operations using pure Python.", "Implement a function that computes the sum of two input vectors x and y :"], "topic": "vector space"}
{"markdowns": ["This notebook will be collected automatically at 6pm on Monday from /home/data_scientist/assignments/Week4 directory on the course JupyterHub server. If you work on this assignment on the course Jupyterhub server, just make sure that you save your work and instructors will pull your notebooks automatically after the deadline. If you work on this assignment locally, the only way to submit assignments is via Jupyterhub, and you have to place the notebook file in the correct directory with the correct file name before the deadline. 1. Make sure everything runs as expected. First, restart the kernel (in the menubar, select Kernel \u2192 Restart ) and then run all cells (in the menubar, select Cell \u2192 Run All ). 2. Make sure you fill in any place that says YOUR CODE HERE . Do not write your answer in anywhere else other than where it says YOUR CODE HERE . Anything you write anywhere else will be removed by the autograder. 3. Do not change the file path or the file name of this notebook. 4. Make sure that you save your work (in the menubar, select File \u2192 Save and CheckPoint )", "Problem 4.1. Decision Trees In this problem, we will use the Decision Trees algorithm to see if we can use machine learning techniques to predict departure delays at the O'Hare airport (ORD). A bit of introduction before we begin. You will see that this problem is not really about decision trees but data preparation. However, it is what practical data science is really about; the actual machine learning, especially with packages like scikit-learn, is a line of fit and predict . The rest is data munging.", "Suppose you want to include weather as training features. 2001.csv doesn't have weather information, so we have to gather the data ourselves. There are various weather APIs available, one of which is [Weather Underground](http://www.wunderground.com/). Their terms of service says I have to display their logo, so here it is: ![](http://www.wunderground.com/logos/images/wundergroundLogo_4c.jpg) After you sign up for an account and generate an API token, you can issue HTTP requests such as: http://api.wunderground.com/api/<token number>/history_20010101/conditions/q/KORD.json The above example will return a JSON with historical weather information on January 1, 2001 (20010101) at O'Hare (KORD). To save you the trouble of dealing with the Weather Underground API, I saved the JSON responses as .json files in /home/data_scientist/data/weather . shell $ ls /home/data_scientist/data/weather | head weather_kord_2001_0101.json weather_kord_2001_0102.json weather_kord_2001_0103.json weather_kord_2001_0104.json weather_kord_2001_0105.json weather_kord_2001_0106.json weather_kord_2001_0107.json weather_kord_2001_0108.json weather_kord_2001_0109.json weather_kord_2001_0110.json", "Each file contains exactly the same response you would get from the Weather Underground API, because I simply dumped the JSON responses to files. Here is the full code that generated these files: python def get_2001_json(date, year=2001): url = 'http://api.wunderground.com/api/e693d410bdf457e2/history_{0}{1}/conditions/q/KORD.json'.format(year, date) resp = requests.get(url) resp_json = resp.json() return resp_json def save_2001_json(date, dir_name='data', filename='weather_kord_2001_{}.json'): data = get_2001_json(date) path = os.path.join(dir_name, filename.format(date)) with open(path, 'w') as f: json.dump(data, f) dates = ['{0:0>2}{1:0>2}'.format(m, d) for m in [1, 3, 5, 7, 8, 10, 12] for d in range(1, 32)] dates.extend(['{0:0>2}{1:0>2}'.format(m, d) for m in [4, 6, 9, 11] for d in range(1, 31)]) dates.extend(['02{0:0>2}'.format(d) for d in range(1, 29)]) if not os.path.exists('data'): os.mkdir('data') for d in dates: save_2001_json(d) time.sleep(6) free plan limit: 10 calls/min Do not run this code to generate these files. We will use the files in /home/data_scientist/data/weather instead. Load JSON files - Write a function named from_json_to_dict() that takes a string in the format MMDD (M = Month, D = Day of month) and returns a dictoinary.", "Tests for from_json_to_dict() :", "Parse time and visibility from JSON - Write a function named from_dict_to_visibility() that takes a dictionary and returns a tuple of (Month, Day, Hour, Minute, Visibility) . We covered the json format in the previous course, so you know how to do this. Let's say you created a dictionary called data by reading the json file weather_kord_2001_0101.json . python >>> data = from_json_to_dict('0101') >>> print(data.keys() dict_keys(['response', 'current_observation', 'history']) You can peek into response and current_observation but they are not important for our purposes, so we look at history : python >>> print(data['history'].keys()) dict_keys(['observations', 'date', 'dailysummary', 'utcdate']) Here, observations is a list. python >>> print(type(data['history']['observations'])) <class 'list'> The first element looks like as follows: python >>> from pprint import pprint >>> pprint(data['history']['observations'][0]) {'conds': 'Overcast', 'date': {'hour': '00', 'mday': '01', 'min': '56', 'mon': '01', 'pretty': '12:56 AM CST on January 01, 2001', 'tzname': 'America/Chicago', 'year': '2001'}, 'dewpti': '10.9', 'dewptm': '-11.7', 'fog': '0', 'hail': '0', 'heatindexi': '-9999', 'heatindexm': '-9999', 'hum': '92', 'icon': 'cloudy', 'metar': 'METAR KORD 010656Z 36004KT 9SM BKN055 OVC095 M11/M12 A3034 RMK ' 'AO2 SLP285 T11061117 $', 'precipi': '-9999.00', 'precipm': '-9999.00', 'pressurei': '30.38', 'pressurem': '1028.5', 'rain': '0', 'snow': '0', 'tempi': '12.9', 'tempm': '-10.6', 'thunder': '0', 'tornado': '0', 'utcdate': {'hour': '06', 'mday': '01', 'min': '56', 'mon': '01', 'pretty': '6:56 AM GMT on January 01, 2001', 'tzname': 'UTC', 'year': '2001'}, 'visi': '9.0', 'vism': '14.5', 'wdird': '360', 'wdire': 'North', 'wgusti': '-9999.0', 'wgustm': '-9999.0', 'windchilli': '5.2', 'windchillm': '-14.9', 'wspdi': '4.6', 'wspdm': '7.4'}", "Tests for from_dict_to_visibility() :", "Process all 365 files We will use the functions from_json_to_dict() and from_dict_to_visibility() (in a loop) for all 365 days of the year. Let's first generate a list of dates in sequential order.", "- Write a function named collect_365_days() that takes a list of strings, iterates through the list, and uses from_json_to_dict() and from_dict_to_visibility() to return a list of 5-tuples (month, day, hour, minute, visibility) . Here's the output you should get: python >>> visibilities = collect_365_days(dates) >>> print(\"The length of visibilities is {}.\".format(len(visibilities))) >>> print(\"The first five elements of visibilities are {}\".format(visibilities[:5])) The length of visibilities is 10159. The first five elements of visibilities are [('01', '01', '00', '56', '9.0'), ('01', '01', '01', '56', '7.0'), ('01', '01', '02', '56', '10.0'), ('01', '01', '03', '56', '10.0'), ('01', '01', '04', '56', '9.0')]", "Now we will combine the weather data with our flights data. We import the following columns of 2001.csv : - Column 1: Month, 1-12 - Column 2: DayofMonth, 1-31 - Column 5: CRSDepTime, scheduled departure time (local, hhmm) - Column 8: UniqueCarrier, unique carrier code - Column 15: DepDelay, departure delay, in minutes - Column 16: Origin, origin IATA airport code", "We use only AA flights that departed from ORD (American Airlines is the largest airline using the O'Hare airport). We define a flight to be delayed if its departure delay is 15 minutes or more, the same definition used by the FAA (source: [Wikipedia](https://en.wikipedia.org/wiki/Flight_cancellation_and_delay)).", "Let's print the first few columns and see what we'll be working with. python >>> print(local.head(5)) Month DayofMonth CRSDepTime Delayed Month DayofMonth CRSDepTime Delayed 398444 1 1 1905 1 398445 1 2 1905 1 398446 1 3 1905 1 398447 1 4 1905 0 398448 1 5 1905 1", "Convert strings to numbers Now we want to match the Month and DayofMonth columns in local with the corresponding entries in visibilities and find the time in visibilities that is closes to the CRSDepTime . What would be the best way to about matching the times? Rahter than comparing three columns, I think it's better to combine the three numbers into one long number and compare just one column. Recall that we had a tuple of strings, while the data types in local is integer. python >>> print(local.CRSDepTime.dtype) int64 So let's convert the strings into integers in the form mmddHHMM , where m is month, d is day of month, H is hour, and M is minute. Let's create a data frame from tuple while we are at it so our function can do: python >>> print(visibilities[:3]) [('01', '01', '00', '56', '9.0'), ('01', '01', '01', '56', '7.0'), ('01', '01', '02', '56', '10.0')] python >>> time_visi = from_string_to_numbers(visibilities) >>> print(time_visi.head(3)) Time Visibility 0 1010056 9 1 1010156 7 2 1010256 10", "Create a Time column - Do the same for the local data frame. Put the result into a column named Time so we have python >>> time_delayed = combine_time(local) >>> print(time_delayed.head()) Month DayofMonth CRSDepTime Delayed Time 398444 1 1 1905 1 1011905 398445 1 2 1905 1 1021905 398446 1 3 1905 1 1031905 398447 1 4 1905 0 1041905 398448 1 5 1905 1 1051905", "Now we find the time closest to the departure time. The following code cell will take a few minutes because we are using iterrows() , which is essentially a for loop. When you are doing numerical operations with big data in Python, you should avoid for loops as much as possible, and this is why. It's slow. Maybe there's a clever way to do this in a vectorized way, but I couldn't figure it out. You don't have to write the match_visibility() function, but you should understand what it's doing. python >>> local_visi = match_visibility(time_delayed, time_visi) >>> print(local_visi.head()) Month DayofMonth CRSDepTime Delayed Time Visibility 398444 1 1 1905 1 1011905 10 398445 1 2 1905 1 1021905 9 398446 1 3 1905 1 1031905 5 398447 1 4 1905 0 1041905 7 398448 1 5 1905 1 1051905 10", "Now we will split the data set into training and test sets. We will train on two columns, CRSDepTime and Visibility , so let's drop those columns.", "Split This function is the same function from [Problem 3.1](https://github.com/UI-DataScience/info490-sp16/blob/master/Week3/assignments/w3p1.ipynb). You can copy-paste your answer. I'll try not to make you write this again in the future.", "We split local_visi into 80:20 training and test sets.", "In the following code cell, we test if the returned DataFrames have the correct columns and lengths.", "Train a Decision Trees model - Write a function named fit_and_predict() that trains a Decision Trees model. Use default parameters. Don't forget that we have to pass an instance of check_random_state() to the train_test_split() function for reproducibility."], "topic": "addition"}
{"markdowns": ["COGS 108 - Assignment 3: Data Privacy", "Important Reminders - Rename this file to 'A3_$ .ipynb', replacing with your unique ID (first letter of your last name, followed by the last 4 digits of your student ID number), before you submit it. Submit it to TritonED. - Do not change / update / delete any existing cells with 'assert' in them. These are the tests used to check your assignment. - Changing these will be flagged for attempted cheating. - This assignment has hidden tests: tests that are not visible here, but that will be run on your submitted file. - This means passing all the tests you can see in the notebook here does not guarantee you have the right answer!", "Overview We have discussed in lecture the importance and the mechanics of protecting individuals privacy when they are included in datasets. One method to do so is the Safe Harbor Method. The Safe Harbour method specifies how to protect individual's identities by telling us which tells us which information to remove from a dataset in order to avoid accidently disclosing personal information. In this assignment, we will explore web scraping, which can often include personally identifiable information, how identity can be decoded from badly anonymized datasets, and also explore using Safe Harbour to anonymize datasets properly. The topics covered in this assignment are mainly covered in the 'DataGathering' and 'DataPrivacy&Anonymization' Tutorial notebooks.", "Installing new packages In the first part of the assignment we will understand how we can scrape the web for data. You have to use the Beautiful Soup library in Python for scraping the data. The library is not installed in Anaconda version, therefore to install a new library for Anaconda, we can use the conda package manager, which the cell below does for you.", "Imports", "Part 1: Web Scraping", "Scraping Rules 1) If you are using another organizations website for scraping, make sure to check the website's terms & conditions. 2) Do not request data from the website too aggressively (quickly) with your program (also known as spamming), as this may break the website. Make sure your program behaves in a reasonable manner (i.e. acts like a human). One request for one webpage per second is good practice. 3) The layout of a website may change from time to time. Because of this, if you're scraping website, make sure to revisit the site and rewrite your code as needed.", "What is the error that you got, and why did you get it?", "YOUR ANSWER HERE", "Part 2: Identifying Data", "Data Files: - anon_user_dat.json - employee_info.json You will first be working with a file called 'anon_user_dat.json'. This file that contains information about some (fake) Tinder users. When creating an account, each Tinder user was asked to provide their first name, last name, work email (to verify the disclosed workplace), age, gender, phone and zip code. Before releasing this data, a data scientist cleaned the data to protect the privacy of Tinder's users by removing the obvious personal identifiers: phone , zip code, and IP address. However, the data scientist chose to keep each users' email addresses because when they visually skimmed a couple of the email addresses none of them seemed to have any of the user's actual names in them. This is where the data scientist made a huge mistake! We will take advantage of having the work email addresses by finding the employee information of different companies and matching that employee information with the information we have, in order to identify the names of the secret Tinder users!"], "topic": "addition"}
{"markdowns": ["___ <a href='http://www.pieriandata.com'> <img src='../../Pierian_Data_Logo.png' /></a> ___ Ecommerce Purchases Exercise - Solutions In this Exercise you will be given some Fake Data about some purchases done through Amazon! Just go ahead and follow the directions and try your best to answer the questions and complete the tasks. Feel free to reference the solutions. Most of the tasks can be solved in different ways. For the most part, the questions get progressively harder. Please excuse anything that doesn't make \"Real-World\" sense in the dataframe, all the data is fake and made-up. Also note that all of these questions can be answered with one line of code. ____ Import pandas and read in the Ecommerce Purchases csv file and set it to a DataFrame called ecom.", "Check the head of the DataFrame.", "How many rows and columns are there?", "What is the average Purchase Price?"], "topic": "addition"}
{"markdowns": ["Sentiment Analysis on Yelp Reviews Let's explore the Yelp reviews and perform a sentiment analysis: 1. Load reviews from data file ... there are in JSON format 2. Convert JSON records to Python tuples for earch row, extract only what we need 3. Maybe Look at stars rating 4. Create list of words (or bag-of-words) 4. Load sentiment dictionary file and convert into a useful format. 5. Assign sentiment values (pos and neg) to words of reviews 6. Aggregate over reviews and report sentiment analysis", "First Glance at Reviews", "Sentiment Dictionary", "Apply Sentiment Analysis", "Summary Statistics Using short cut calculation: $$mean = \\frac{1}{n} \\sum_{i=1}^n x_i$$ $$var = \\frac{1}{n-1}( \\sum_{i=1}^n x_i^2 - \\frac{ (\\sum_{i=1}^n x_i)^2}{n})$$ Suggestion https://stackoverflow.com/questions/39981312/spark-rdd-how-to-calculate-statistics-most-efficiently", "You can try reduceByKey. It's pretty straightforward if we only want to compute the min(): rdd.reduceByKey(lambda x,y: min(x,y)).collect() Out[84]: [('key3', 2.0), ('key2', 3.0), ('key1', 1.0)] To calculate the mean, you'll first need to create (value, 1) tuples which we use to calculate both the sum and count in the reduceByKey operation. Lastly we divide them by each other to arrive at the mean: meanRDD = (rdd .mapValues(lambda x: (x, 1)) .reduceByKey(lambda x, y: (x[0]+y[0], x[1]+y[1])) .mapValues(lambda x: x[0]/x[1])) meanRDD.collect() Out[85]: [('key3', 5.5), ('key2', 5.0), ('key1', 3.3333333333333335)] For the variance, you can use the formula (sumOfSquares/count) - (sum/count)^2, which we translate in the following way: varRDD = (rdd .mapValues(lambda x: (1, x, x x)) .reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1], x[2]+y[2])) .mapValues(lambda x: (x[2]/x[0] - (x[1]/x[0]) 2))) varRDD.collect() Out[106]: [('key3', 12.25), ('key2', 4.0), ('key1', 2.8888888888888875)] I used values of type double instead of int in the dummy data to accurately illustrate computing the average and variance: rdd = sc.parallelize([(\"key1\", 1.0), (\"key3\", 9.0), (\"key2\", 3.0), (\"key1\", 4.0), (\"key1\", 5.0), (\"key3\", 2.0), (\"key2\", 7.0)])"], "topic": "data analysis"}
{"markdowns": ["important terms: - principal component analysis (PCA) - high-dimensional data", "What is the value in using a PCA to plot in Two Dimensions? What are the possible issues?", "By reducing the dimensionality to 2D, it can help visualize the data in a different and easier way. E.g., some features might be redundant so reducing the dimension can simplify the plot. The problem with this is that we may lose information like relationships between features.", "Using PCA to Plot Multidimensional Data in Two Dimensions PCA is a complex and very powerful model typically used for dimensionality reduction. We will explore this model in greater detail later, but for now there is one application that is so useful that we will skip the details and just use it.", "High-Dimensional Data The Iris data we are looking at is an example of high-dimensional data. Actually, it is the smallest number of dimensions that we can really think of as \"high-dimensional\". You can easily imagine how to visualize data in one, two, or three dimensions, but as soon there is a fourth dimension, this becomes much more challenging.", "Here, we have used PCA to reduce the dimensionality of our dataset from 4 to 2. Obviously, we have lost information, but this is okay. The purpose of running this algorithm is not to generate predictions, but to help us to visualize the data. At this, it was successful!", "Coloring by Target"], "topic": "addition"}
{"markdowns": ["Data Visualization and Exploratory Data Analysis Lab Visualizing and exploring data. Data mining process as a whole", "Problem 1. Read the dataset (1 point) You'll be exploring data about people's income. Your task is to understand whether there are significant differences in the lifestyle of lower- vs. higher-income groups. Read the dataset located [here](https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data). The information file is [here](https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names). Save it into the variable income_data . Change the column names to what you like. The last column is related to the income class. Get acquainted with the information file well before starting work. You don't need to clean the dataset.", "Problem 2. High income (1 point) How many people have high income (over 50 000 USD per year)? Write a function to return the value. The function should accept the dataframe as a parameter. Work with that parameter.", "Problem 3. Capital gain: thresholding (1 point) Plot a histogram of the capital gain. You can see that there are many people with relatively low gains and a few people - with very high gains."], "topic": "data analysis"}
{"markdowns": ["[LEGALST-190] Lab 2-15 This lab will provide an introduction to extra plugins in the folium package: Search and Draw. Estimated Time: 30 minutes --- Table of Contents [The Data]( section data)<br> [Context]( section context)<br> 1 - [Search]( section 1)<br> 2 - [Draw]( section 2)<br> Dependencies:", "--- The Data <a id='data'></a> ---", "--- Plugins <a id='data'></a> Now that we're familiar with several different types of Folium mapping, we make our maps more functional with two additional plug-ins : Folium package components that add new features. Today, we'll work with Search and Draw . ---", "Search <a id='section 1'></a>", "Most map applications you've used likely contained a search function. Here's how to add one using Folium. Create a map of the United States. Next, create a Search object by calling its constructor function, then add the Search to your Map using add_too - just like you've done with many other Folium modules. Search(...) needs to be given some searchable GeoJSON-formatted data. Here, you can use states . Hint: Remember, the add_too function call looks something like <thing you want to add>.add_to(<place you want to add it to>)", "You can also set optional parameters such as the location of the bar. We loaded the code for Search in an earlier cell: look at the 'parameters' section to see what options you can change, then see what happens when you change geom_type , search_zoom , and position on your states search map. In addition to searching for whole geographic areas, we can also search for specific locations. Create another searchable map in the cell below, this time using [41.9, 12.5] for the map coordinates and rome for your search data. You'll want to set a high search_zoom . How many cafes can you find?", "Folium's Search is currently pretty limited, but it can be useful when you have the right set of GeoJSON data. 1. Go to http://geojson.io, a tool for creating and downloading GeoJSON (the interface might look familiar: they use Leaflet, the same software Folium is build around). 2. Create ten markers any where you'd like. Click on each marker and name them by adding a row with 'name' in the left column and the marker's name in the right column. 3. Save your markers in a GeoJSON file by using the save menu in the upper right. 4. Upload your GeoJSON file to datahub 5. Open your file in this notebook and create a searchable map using your markers. Hint: there's an example of code for opening a GeoJSON file earlier in this notebook.", "---", "Draw <a id='section 2'></a>", "A Draw tool can be a great way to interactively add, change, or highlight features on your map. Adding it is simple: 1. Create a Map 2. Create a Draw object by calling the Draw() constructor function 3. Add your Draw object to your map using add_to", "Folium's Draw has a lot of features. Let's try a few. - Draw a polygon representing the [Bermuda Triangle](https://en.wikipedia.org/wiki/Bermuda_Triangle) - The Deepwater Horizon oil spill in 2010 occurred in the nearby Gulf of Mexico. Look up the exact coordinates, add a marker for the center, and draw two circles: one representing the size of the spill at its smallest, and one at its largest.", "- The Proclaimers say [they would walk 500 miles and they would walk 500 more just to be the man who walked 1000 miles to fall down at your door](https://en.wikipedia.org/wiki/I%27m_Gonna_Be_(500_Miles). Create a map with a marker at the Proclaimers' birthplace in Edinburgh, Scotland. Then make a circle with a radius of 1000 miles (about 1600 kilometers) centered at Edinburgh to see if the Proclaimers can reach your door. - Vanessa Carlton lives in Nashville, Tennessee and says that [you know she'd walk 1000 miles if she could just see you](https://en.wikipedia.org/wiki/A_Thousand_Miles). Use the edit layers button to move your circle to be centered at Nashville and see if you're in Vanessa's range. Note: the circle changes size slightly when you move it south. Why?"], "topic": "data analysis"}
{"markdowns": ["___ <a href='http://www.pieriandata.com'> <img src='../Pierian_Data_Logo.png' /></a> ___ Random Forest Project - Solutions For this project we will be exploring publicly available data from [LendingClub.com](www.lendingclub.com). Lending Club connects people who need money (borrowers) with people who have money (investors). Hopefully, as an investor you would want to invest in people who showed a profile of having a high probability of paying you back. We will try to create a model that will help predict this. Lending club had a [very interesting year in 2016](https://en.wikipedia.org/wiki/Lending_Club 2016), so let's check out some of their data and keep the context in mind. This data is from before they even went public. We will use lending data from 2007-2010 and be trying to classify and predict whether or not the borrower paid back their loan in full. You can download the data from [here](https://www.lendingclub.com/info/download-data.action) or just use the csv already provided. It's recommended you use the csv provided as it has been cleaned of NA values. Here are what the columns represent: credit.policy: 1 if the customer meets the credit underwriting criteria of LendingClub.com, and 0 otherwise. purpose: The purpose of the loan (takes values \"credit_card\", \"debt_consolidation\", \"educational\", \"major_purchase\", \"small_business\", and \"all_other\"). int.rate: The interest rate of the loan, as a proportion (a rate of 11% would be stored as 0.11). Borrowers judged by LendingClub.com to be more risky are assigned higher interest rates. installment: The monthly installments owed by the borrower if the loan is funded. log.annual.inc: The natural log of the self-reported annual income of the borrower. dti: The debt-to-income ratio of the borrower (amount of debt divided by annual income). fico: The FICO credit score of the borrower. days.with.cr.line: The number of days the borrower has had a credit line. revol.bal: The borrower's revolving balance (amount unpaid at the end of the credit card billing cycle). revol.util: The borrower's revolving line utilization rate (the amount of the credit line used relative to total credit available). inq.last.6mths: The borrower's number of inquiries by creditors in the last 6 months. delinq.2yrs: The number of times the borrower had been 30+ days past due on a payment in the past 2 years. pub.rec: The borrower's number of derogatory public records (bankruptcy filings, tax liens, or judgments).", "Import Libraries Import the usual libraries for pandas and plotting. You can import sklearn later on.", "Get the Data Use pandas to read loan_data.csv as a dataframe called loans.", "Check out the info(), head(), and describe() methods on loans.", "Exploratory Data Analysis Let's do some data visualization! We'll use seaborn and pandas built-in plotting capabilities, but feel free to use whatever library you want. Don't worry about the colors matching, just worry about getting the main idea of the plot. Create a histogram of two FICO distributions on top of each other, one for each credit.policy outcome. Note: This is pretty tricky, feel free to reference the solutions. You'll probably need one line of code for each histogram, I also recommend just using pandas built in .hist()", "Create a similar figure, except this time select by the not.fully.paid column.", "Create a countplot using seaborn showing the counts of loans by purpose, with the color hue defined by not.fully.paid.", "Let's see the trend between FICO score and interest rate. Recreate the following jointplot.", "Create the following lmplots to see if the trend differed between not.fully.paid and credit.policy. Check the documentation for lmplot() if you can't figure out how to separate it into columns.", "Setting up the Data Let's get ready to set up our data for our Random Forest Classification Model! Check loans.info() again.", "Categorical Features Notice that the purpose column as categorical That means we need to transform them using dummy variables so sklearn will be able to understand them. Let's do this in one clean step using pd.get_dummies. Let's show you a way of dealing with these columns that can be expanded to multiple categorical features if necessary. Create a list of 1 element containing the string 'purpose'. Call this list cat_feats.", "Now use pd.get_dummies(loans,columns=cat_feats,drop_first=True) to create a fixed larger dataframe that has new feature columns with dummy variables. Set this dataframe as final_data.", "Train Test Split Now its time to split our data into a training set and a testing set! Use sklearn to split your data into a training set and a testing set as we've done in the past.", "Training a Decision Tree Model Let's start by training a single decision tree first! Import DecisionTreeClassifier", "Create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.", "Predictions and Evaluation of Decision Tree Create predictions from the test set and create a classification report and a confusion matrix.", "Training the Random Forest model Now its time to train our model! Create an instance of the RandomForestClassifier class and fit it to our training data from the previous step.", "Predictions and Evaluation Let's predict off the y_test values and evaluate our model. Predict the class of not.fully.paid for the X_test data.", "Now create a classification report from the results. Do you get anything strange or some sort of warning?"], "topic": "the concept"}
{"markdowns": ["CS 109A/STAT 121A/AC 209A/CSCI E-109A Standard Section 4: Predictor types and feature selection Harvard University <br/> Fall 2017 <br/> Section Leaders: Nathaniel Burbank, Albert Wu<br/> Instructors: Pavlos Protopapas, Kevin Rader, Rahul Dave, Margo Levine", "<center> Download this notebook from the CS109 repo or here: </center> <center> http://bit.ly/109_S4 </center> For this section, our goal is to continue discussing the complexities around different types of data features and thinking carefully about how different datatypes and collinearity issues can affect our models, whether our true goal is inference or prediction. Specifically, we will: 1. Review different ways to plot multiple axes on a single figure in Matplotlib 2. Discuss different variable types, and techniques of \u201cone-hot-encoding\u201d our factor variables 3. Build a variable selection function that performs an exhaustive feature search overall all possible combinations of predictors", "For this section we will be using the following packages:", "Part (1): Review plotting on multiple axes within a single matplot lib figure ![](https://i.imgur.com/XTzSuoR.png) source: http://matplotlib.org/faq/usage_faq.html Some of the code in the plots below in this section was also adapted from [this](http://matplotlib.org/faq/usage_faq.html) matplotlib tutorial.", "Plot y1 and y2 on single figure", "Plot y1 and y2 on side by side axes"], "topic": "data analysis"}
{"markdowns": ["Sentiment Classification & How To \"Frame Problems\" for a Neural Network by Andrew Trask - Twitter : @iamtrask - Blog : http://iamtrask.github.io", "What You Should Already Know - neural networks, forward and back-propagation - stochastic gradient descent - mean squared error - and train/test splits Where to Get Help if You Need it - Re-watch previous Udacity Lectures - Leverage the recommended Course Reading Material - [Grokking Deep Learning](https://www.manning.com/books/grokking-deep-learning) (40% Off: traskud17 ) - Shoot me a tweet @iamtrask Tutorial Outline: - Intro: The Importance of \"Framing a Problem\" - Curate a Dataset - Developing a \"Predictive Theory\" - PROJECT 1 : Quick Theory Validation - Transforming Text to Numbers - PROJECT 2 : Creating the Input/Output Data - Putting it all together in a Neural Network - PROJECT 3 : Building our Neural Network - Understanding Neural Noise - PROJECT 4 : Making Learning Faster by Reducing Noise - Analyzing Inefficiencies in our Network - PROJECT 5 : Making our Network Train and Run Faster - Further Noise Reduction - PROJECT 6 : Reducing Noise by Strategically Reducing the Vocabulary - Analysis: What's going on in the weights?", "Lesson: Curate a Dataset", "Lesson: Develop a Predictive Theory", "Project 1: Quick Theory Validation", "Transforming Text into Numbers", "Project 2: Creating the Input/Output Data"], "topic": "convolution"}
{"markdowns": ["<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\"> Postgres SQL Lab _Authors: Dave Yerrington (SF)_ --- In this lab you will practice executing SQL within your ipython notebook and practice some queries on the [Northwind sample database](https://northwinddatabase.codeplex.com/). You can access the data with this command: psql -h dsi.c20gkj5cvu3l.us-east-1.rds.amazonaws.com -p 5432 -U dsi_student northwind password: gastudents Alternatively you can use sqlalchemy to interface to the database:", "And generate dataframes from string queries using pandas .read_sql like so:", "1. Inspect the database If we were connected via console, it would be easy to list all tables using \\dt . We can also access table information by running a query on the information_schema.tables table. Write a SELECT statement that lists all the tables in the public schema of the northwind database, sorted alphabetically. .sql SELECT tablename FROM pg_catalog.pg_tables WHERE schemaname='public'", "2. Print schemas The table INFORMATION_SCHEMA.COLUMNS contains schema information on each schema. Query it to display schemas of all the public tables. In particular we are interested in the column names and data types. Make sure you only include public schemas to avoid cluttering your results with a bunch of postgres related stuff. Specifically, select columns table_name , data_type , and table_schema from the table only where table_schema is \"public\"."], "topic": "google chrome"}
{"markdowns": ["Dimensionality Reduction with PCA [Dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction) is the process of reducing the number of random variables (features) under consideration while minimizing information loss in our data. The process of dimensionality reduction has a variety of applications and is used throughout the domain of data mining. Here we will explore the concepts behind [Principal Component Analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis), and step through a couple of examples. - The first example is the canonical PCA example. - We will examine the handwritten digits dataset, specifically clustering by principal components. - We'll implement PCA on the Iris dataset. - More interestingly, we'll try out PCA for facial recognition on the [Labeled Faces in the Wild](http://vis-www.cs.umass.edu/lfw/) dataset. Learning Goals - Perform [ PCA with sklearn ](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) - Use PCA as a precursor for clustering algorithms - Use PCA as a precursor for classification algorithms - Understand the output of PCA in terms of reduced features and explained variance - Use PCA as dimensionality reduction for a face recognition (classification) problem Datasets - Random Manufactured: a 2-dimensional randomly sampled gaussian - [Iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set): the familiar flower type classification dataset - [Handwritten Digits Dataset](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html): sklearn 's built-in handwritten digits dataset - [Labeled Faces in the Wild Dataset](http://vis-www.cs.umass.edu/lfw/): Collection of headshot photographs of well-known people", "PCA Example - Random Data Make a random dataset We'll generate a dataset from a randomly sampled 2-D Gaussian distribution of known mean and covariance matrix so we can demonstrate and visualize some of the principles (pun intended) of PCA.", "Plot the Data Let's try plotting our data to take a look at it. Based on the gaussian parameters we set, the data should be centered near [1,2], should vary more in the y direction than the x direction (from the y and x variances), and there should be a negative correlation between x and y (from the negative covariance coefficient).", "Does the data look as you expect? Can you predict graphically where the first and second principal components might be (the directions along which the data varies most and 2nd most)?", "We would now like to analyze the directions in which the data varies most. For that, we place the point cloud in the center (0,0) and rotate a line through the data, such that the direction with most variance is parallel to the x-axis. Each succeding component in turn accounts for the highest variance possible that is orthoganal to existing components.", "Now, let's perform principal component analysis (PCA) to project the data into fewer dimensions. In PCA, the projection is defined by principal components (eigenvectors), each of which can be viewed as a linear combination of the original features that corresponds to a dimension in the projection. The projection is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to (i.e., uncorrelated with) the preceding components. Each principal component (eigenvector) is associated with an eigenvalue, which corresponds to the amount of the variance explained by that component. Dimensionality reduction is a one-way transformation that induces a loss of information. We can try to minimize the loss of information while retaining the benefits of dimensionality reduction by trying to find the number of principal components needed to effectively represent the original dataset. This number can often be determined by the \"elbow\" or \"knee\" point, which is considered to be the natural break between the useful principal components (or dimensions) and residual noise. We can find the elbow point by computing PCA on our dataset and observing the number of principal components after which the amount of variance explained displays a natural break or drop-off.", "Principal Components We'll use the [ sklearn.decomposition.PCA ](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) object type to create an object that can perform PCA with the specified number of components. Here we'll use n_components=2 to examine the first 2 principal components, then we'll call fit_transform() on our data to transform our data into our new 2-dimensional space.", "Let's check out the resulting eigenvectors from our decomposition. To do this we will call pca.components_ which returns an array of 2 (for 2 principal components) 2D vectors (for the original 2D space) which represent the linear coefficients that transform the original space into the PCA space.", "Explore the data Let's generate a few plots to demonstrate what the PCA is actually doing and provide some intuition. We'll plot the 1st and 2nd principal components overlaying the original data in the original data space. Then we'll plot the data in 1-D along both the 1st and 2nd principal components. Finally we'll plot the data in the full transformed 2-D PCA space.", "Questions: - What do you notice about these plots? - Do the orientations of the PCA axes make sense to you? - How many total PCA components could their be with this dataset? Notice that the PCA axes are perpendicular (orthogonal), a requirement since the PCA eigenvectors form an orthogonal basis.", "How much of the dataset's variation is explained by each component? Let's use the pca.explained_variance_ratio_ to print out how much variance is explained by each principal component.", "Questions: - What percentage of the variance in the original data is explained by the first/second Principal Components? - Does the total variance explained by the 2 combined make sense to you?", "Iris Dataset", "Let's get introduced to PCA in sklearn with our friendly Iris dataset. Load the dataset by a call to sklearn.datasets.load_iris() . Store the results as iris .", "Store the iris data as X and center it (subtract the mean) into a variable called X_centered .", "Perform a PCA on X_centered with 2 principal components by first creating a PCA object with n_components=2 and then calling fit_transform() on X_centered . Store the results in X_pca ."], "topic": "quantitative"}
{"markdowns": ["This notebook will be collected automatically at 6pm on Monday from /home/data_scientist/assignments/Week7 directory on the course JupyterHub server. If you work on this assignment on the course Jupyterhub server, just make sure that you save your work and instructors will pull your notebooks automatically after the deadline. If you work on this assignment locally, the only way to submit assignments is via Jupyterhub, and you have to place the notebook file in the correct directory with the correct file name before the deadline. 1. Make sure everything runs as expected. First, restart the kernel (in the menubar, select Kernel \u2192 Restart ) and then run all cells (in the menubar, select Cell \u2192 Run All ). 2. Make sure you fill in any place that says YOUR CODE HERE . Do not write your answer in anywhere else other than where it says YOUR CODE HERE . Anything you write anywhere else will be removed by the autograder. 3. Do not change the file path or the file name of this notebook. 4. Make sure that you save your work (in the menubar, select File \u2192 Save and CheckPoint )", "Problem 7.1. Text Analysis. In this problem, we perform basic text analysis tasks, such as accessing data, tokenizing a corpus, and computing token frequencies, on our course syllabus and on the NLTK Reuters corpus.", "In the first half of the problem, we use our [course syllabus](https://github.com/UI-DataScience/info490-sp16/blob/master/orientation/syllabus.md) as a sample text.", "Tokenize - Tokenize the text string syllabus_text . You should clean up the list of tokens by removing all puntuation tokens and keeping only tokens with one or more alphanumeric characters."], "topic": "addition"}
{"markdowns": ["Trend Lines in PyPlot", "Drawing a Trend Line requires that we compute a polynomial fitting for the time series. This is easily handled by the libraries in PyPlot. For this exercise, we're going to examine data taken from [google trends](www.trends.google.com). First, let's get our data loaded in and do some re-organizing. It's important to note that for this step, it's best if the data is evenly spaced , and retains an incremental integer index . Luckily, our data follows those guidelines, but if it didn't, at this point you should be able to use Pandas to resample and reindex.", "Great! Now, let's plot the data.", "Next we need to compute a line coefficient. This is where NumPy's polyfit method comes in handy. Let's look at the documentation [here](http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.polyfit.html) and implement it.", "Last step is to define the polynomial function using the line coefficiant. We can do this on a single dimension using NumPy's poly1d method."], "topic": "data analysis"}
{"markdowns": ["Comparing gene expression quantification strategies Introduction You've run STAR to align and create a Sequence Alignment Map (SAM) file, samtools to sort and index the SAM file and turn it into a binary SAM file (BAM), featureCounts to count the number of reads of features, and kallisto to perform all of the above using quasi-alignment and quantification. The purpose of this homework is to compare the gene expression quantification strategies (we'll call them \"Align-Count\" and \"Quasialign-Count\" ) Reading list - [What the FPKM](https://haroldpimentel.wordpress.com/2014/05/08/what-the-fpkm-a-review-rna-seq-expression-units/) - Explain difference between TPM/FPKM/FPKM units - [Pearson correlation](https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient) - linear correlation unit - [Spearman correlation](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient) - rank-based non-linear correlation unit Description of libraries in Python We'll be using five additional libraries in Python: 1. [ numpy ](http://www.numpy.org/) - (pronounced \"num-pie\") which is basis for most scientific packages. It's basically a nice-looking Python interface to C code. It's very fast. 2. [ pandas ](http://pandas.pydata.org) - This is the \"DataFrames in Python.\" (like R's nice dataframes) They're a super convenient form that's based on numpy so they're fast. And you can do convenient things like calculate mea n and variance very easily. 3. [ matplotlib ](http://matplotlib.org/) - This is the base plotting library in Python. 4. [ scipy ](http://www.scipy.org/) - (pronounced \"sigh-pie\") Contains 5. [ seaborn ](http://web.stanford.edu/~mwaskom/software/seaborn/index.html) - Statistical plotting library. To be completely honest, R's plotting and graphics capabilities are much better than Python's. However, Python is a really nice langauge to learn and use, it's very memory efficient, can be parallized well, and has a very robust machine learning library, scikit-learn , which has a very nice and consistent interface. So this is Python's answer to ggplot2 (very popular R library for plotting) to try and make plotting in Python nicer looking and to make statistical plots easier to do.", "Change directories to where we have our processed data", "Read the S10 sample kallisto file. The index_col=\"target_id\" tells us that column named \"target_id\" should be used as the \"index\" aka the row names. The head() command helps us look at the top of the dataframe and shows the first 5 rows by default.", "You can also use \" tail \" to show the last few rows: (also 5 by default)", "Exercise 1: using .head() Show the first 17 rows of s10_kallisto .", "We can see the number of rows and columns using .shape , which shows (nrows, ncols):", "This seems like a ton of genes, but don't worry, we'll filter on this. Read the S13 sample kallisto file.", "Exercise 2: How many rows and columns are in s13_kallisto? Use shape to show how many rows and columns are in s13_kallisto", "Let's plot their correlation to each other using jointplot in seaborn:", "Oh right -- we have expression data and the scales are enormous... Notice the 200,000 maximum on the y-scale. Let's add 1 to all values and take the log2 of the data. We add one because log(0) is undefined and then all our logged values start from zero too. This \"$\\log_2(TPM + 1)$\" is a very common transformation of expression data so it's easier to analyze. To do that, we'll create a new column in each of the s10_kallisto and s13_kallisto dataframes using the existing data. Here's an example of doing something similar but adding 1000.", "Exercise 3: Add a log2_tpm column to s13_kallisto", "Exercise 4: Plot the logged TPM correlation Use sns.jointplot to plot the correlation between the logged TPMs we just made.", "Interesting, we have quite a bit of correlation! What if we used rank-based, non-linear correlation such as spearman? We can specify a different statistical function with stat_func= and specifying spearmanr from scipy.stats .", "We'll now create a dataframe containing the two columns (\"Series\") of the separate s10_kallisto and s13_kallisto columns, using pd.concat to concatenate the two series, and rename them so the names are s13 and s10 , using the keys=['s10', 's13'] . The axis=1 means to glue along the columns (axis=0 is rows, axis=1 is columns), so that we stack horizontally, versus vertically. Otherwise we'd get a really tall series that can't tell the difference between s10 and s13.", "So we have a ton of genes where the expression is near zero. This is not that helpful so let's only use genes with expression greater than one in at least one sample. We'll do this using the [boolean](https://en.wikipedia.org/wiki/Boolean) (True/False) matrix we get from asking \" kallisto_log2_tpm > 1 \":", "If we use the convenient .sum() function on the dataframe, we'll get the number of \"expressed genes\" (defining \"Expressed genes\" as genes with log2(TPM+1) greater than 1) per sample:", "If we sum with axis=1 , then we get the number of samples with expression greater than one for each gene.", "We can now use this to get all genes expressed in at least one sample with \" >= 1 \"", "Now we can use the .loc[] notation to get the rows we want as a subset. Notice that this outputs the dataframe itself - we haven't assigned it to anything. This is equivalent to extracting your RNA and throwing it on the ground. You can look at it but you didn't put it anywhere useful so you can't do anything with it.", "Now let's actually make another dataframe with only the expressed genes.", "We'll plot the jointplot again, using a little different syntax. Since the data we want are two columns in the same dataframe, we can specify the names of the columns as \"x\" (first position) and \"y\" (second position) and then the dataframe in the third position.", "Exercise 5: Plot logged TPM of expressed genes using spearmanr as the statistical function", "Reading featureCounts", "Exercise 6: Read s10_featurecounts table 1. Skip the first row 2. Set the first column (0th column) as the index", "Calculate TPM from the reads counted by featureCounts Now, featureCounts outputs the actual number of reads mapped to each gene. You can tell because the datatype is integer, which would only be true if it was raw read counts, and not a transformed value like TPM, which has decimals (i.e. is a \"floating-point\" type) To get the transcripts per kilobase mapped (TPM) so we can compare to the kallisto output, we'll have to do some extra steps.", "Let's look at the distribution of the number of reads per feature using sns.distplot .", "For the next exercise ... remember that you can use convenient methods of \".sum()\" to get the sum of a column. This sums all the gene lengths in the data.", "Like with the log2 TPM, we'll create a new column based on the existing columns. This example assigns the big ole column name to a single variable so it's easier to work with, and creates a new column that's the sum of all lengths times the number of reads, divided by 2000 (2e3 = $2\\times 10^3$). Notice that you can use regular multiplication with \" \" and division with \" / \" (addition with \" + \" and \" - \" also work)", "Exercise 7: Calculate FPKM Using your knowledge about [FPKM](https://haroldpimentel.wordpress.com/2014/05/08/what-the-fpkm-a-review-rna-seq-expression-units/), add a column called 'fpkm' to s10_featurecounts that's the fragments per kilobase mapped. We're doing FPKM first because you can calculate the TPM from the FPKM easily. (Use the \"Length\" column provided rather than the \"effective length\" which is the length minus the read lengths. otherwise we'll get negative FPKMs!)", "Let's look at the new distribution of the FPKMs. Notice that the range is much smaller than the reads.", "Exercise 8: Calculate TPM Now add a column called 'tpm' which uses FPKM to calculate the transcripts per million. You'll need to read the [\"What the FPKM\"](https://haroldpimentel.wordpress.com/2014/05/08/what-the-fpkm-a-review-rna-seq-expression-units/) blog post in detail to get the equation for TPM. Hint: How would you sum all the FPKMs in the data?", "Let's look at this new distribution of TPM. Notice its range is also smaller than the FPKMs.", "If you plot FPKM vs TPM, you'll see that they're linearly related, just like the equation told us :)", "Exercise 9: Add a log2_tpm column Add a column called 'log2_tpm' where you do the log2(TPM+1) transformation that we did before.", "Compare kallisto and featureCounts Remember this kallisto dataframe we made? Let's take a look at it again.", "Notice that its row names (\" index \") have all this stuff in them. We really only need the gene ids - everything else is an annotation of the transcript (the length, where the UTR is in the sequence, where the coding region is in the sequence, etc). Here's an example of using split() on this string which is one of the IDs.", "Now since we're using DataFrames we can use this really convenient function map which applies the same function to each element of the vector. We'll only get the gene names by \"mapping\" a small function (\"lambda\") that splits each item in the row name on the pipe (\"|\") and takes the 1th item (remember we start counting from 0).", "We'll now copy the original dataframe and replace the \" index \" with this new one, so we can compare to featureCounts .", "Notice that we have some duplicate gene ids. This is because there were multiple transcripts per gene id. This next bit of code takes each gene ID, and for the ones that match, it'll sum the TPMs. This is legal to do because the total number of transcripts has not changed, we're merely summing per gene. The [ .groupby ](http://pandas.pydata.org/pandas-docs/stable/groupby.html) function is very useful every time you have one or more tables that have some common key (in this case gene ids) that you want to use. We won't go into it here but it may come in handy to you later.", "Now we'll use a convenient function called align which will unify the row names of the two columns (Series) we have: the kallisto log2 TPMs and the featurecounts log2 TPMs. We'll assign them to \" x \" and \" y \" for short (for now)", "So that the plot shows the name of the sample and the algorithm, we'll change the .name attribute of the series.", "What about using spearman correlation?", "Exercise 10: Why did kallisto and featureCounts give different results? Write 3-5 sentences describing why you think kallisto and FeatureCounts have similar, but different results, based on the algorithms used for mapping and (quasi-)alignment. Recall that with kallisto we mapped to protein-coding transcripts, and for FeatureCounts we first had to map with STAR (which has its own biases - like what?) and only after that we used the basic annotation, both from [GENCODE Mouse V8](http://www.gencodegenes.org/mouse_releases/8.html).", "Differential expression We'll now do some differential expression analyses to get a sense of how these different algorithms affect the results.", "Since the differences between genes is approximately normal, we can find the genes that are overexpressed in S10 (i.e. speceific to S10) by getting the ones that are 2 standard deviations greater than the mean. We need this line: kallisto_s10_specific_genes = pd.Series(kallisto_s10_specific.index[kallisto_s10_specific]) Becaus we want to create a new series where the values are the gene ids, rather than the index is the gene ids. This makes writing to a file easier.", "Exercise 11: Get S13-specific genes in kallisto Make a series called kallisto_s13_specific_genes which contains the genes that are specifically expressed in the sample S13. (Hint: specific to S13 means underexpressed in S10 - so similar to above, but 2 standard deviations less than the mean)"], "topic": "gene expression"}
{"markdowns": ["Before you turn this problem in, make sure everything runs as expected. First, restart the kernel (in the menubar, select Kernel$\\rightarrow$Restart) and then run all cells (in the menubar, select Cell$\\rightarrow$Run All). Make sure you fill in any place that says YOUR CODE HERE or \"YOUR ANSWER HERE\", as well as your name and collaborators below:", "---", "Combined Lab + Discussion 2: Pandas Overview This assignment should be completed by Wednesday September 5, 2018 at 11:59 PM.", "[Pandas](https://pandas.pydata.org/) is one of the most widely used Python libraries in data science. In this lab, you will learn commonly used data wrangling operations/tools in Pandas. We aim to give you familiarity with: Creating dataframes Slicing data frames (ie. selecting rows and columns) Filtering data (using boolean arrays) Data Aggregation/Grouping dataframes Merging dataframes In this lab, you are going to use several pandas methods like drop() , loc[] , groupby() . You may press shift+tab on the method parameters to see the documentation for that method.", "Just as a side note : Pandas operations can be confusing at times and the documentation is not great, but it is OK to be stumped when figuring out why a piece of code is not doing what it's supposed to. We don't expect you to memorize all the different Pandas functions, just know the basic ones like iloc[] , loc[] , slicing, and other general dataframe operations. Throughout the semester, you will have to search through Pandas documentation and experiment, but remember it is part of the learning experience and will help shape you as a data scientist!", "Setup", "Creating DataFrames & Basic Manipulations A [dataframe](http://pandas.pydata.org/pandas-docs/stable/dsintro.html dataframe) is a two-dimensional labeled data structure with columns of potentially different types. The pandas [ DataFrame function](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html) provides at least two syntaxes to create a data frame.", "Syntax 1: You can create a data frame by specifying the columns and values using a dictionary as shown below. The keys of the dictionary are the column names, and the values of the dictionary are lists containing the row entries.", "Syntax 2: You can also define a dataframe by specifying the rows like below. Each row corresponds to a distinct tuple, and the columns are specified separately.", "You can obtain the dimensions of a matrix by using the shape attribute dataframe.shape", "Question 1(a) You can add a column by dataframe['new column name'] = [data] . Please add a column called rank1 to the fruit_info table which contains a 1,2,3, or 4 based on your personal preference ordering for each fruit.", "Question 1(b) You can ALSO add a column by dataframe.loc[:, 'new column name'] = [data] . This way to modify an existing dataframe is preferred over the assignment above. In other words, it is best that you use loc[] . Although using loc[] is more verbose, it is faster. (However, this tradeoff is more likely to be valuable in production than during interactive use.) We will explain in more detail what loc[] does, but essentially, the first parameter is for the rows and second is for columns. The : means keep all rows and the new column name indicates the column you are modifying or in this case adding. Please add a column called rank2 to the fruit_info table which contains a 1,2,3, or 4 based on your personal preference ordering for each fruit.", "Question 2 Use the .drop() method to [drop](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html) the both the rank1 and rank2 columns you created. (Make sure to use the axis parameter correctly) Hint: Look through the documentation to see how you can drop multiple columns of a Pandas dataframe at once, it may involve a list.", "Question 3 Use the .rename() method to [rename](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rename.html) the columns of fruit_info_original so they begin with a capital letter. Set the inplace parameter correctly to change the fruit_info_original dataframe. ( Hint: in Question 2, drop creates and returns a new dataframe instead of changing fruit_info because inplace by default is False )", "Babyname datasets Now that we have learned the basics, let's move on to the babynames dataset. Let's clean and wrangle the following data frames for the remainder of the lab. First let's run the following cells to build the dataframe baby_names . The cells below download the data from the web and extract the data in a California region. There should be a total of 5933561 records.", "fetch_and_cache Helper The following function downloads and caches data in the data/ directory and returns the Path to the downloaded file", "Now, what in the world is the above fetch_and_cache function doing? Well, let's step through it and identify some of the key lines of the above function. In Python, whenever you want to check if a file exists in a certain path, it is not sufficient to just have the string representation of the path, you need to create a Path object usign the Path() constructor. Essentially, after the Path object is created for the directory, a directory is created at that path location using the mkdir() method. Then, within the directory, a path for the file itself is created and if the path has already been linked (a.k.a file has already been created and put in the directory), then a new one is not created and instead uses the cached version. The function exists() in the code above is one way to check if a file exists at a certain path when called on a path object. There is also another way this can be done using the os library in Python. If you decided to use the os library, you wouldn't need to create a Path object and rather pass in the the string representation of the path. Now, going back to the code, if the path hasn't been linked, then the file is downloaded and created at the path location. The benefit of this function is that not only can you force when you want a new file to be downloaded using the force parameter, but in cases when you don't need the file to be re-downloaded, you can use the cached version and save download time.", "Below we use fetch and cache to download the namesbystate.zip zip file. This might take a little while! Consider stretching.", "The following cell builds the final full baby_names DataFrame. Here is documentation for [pd.concat](https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.concat.html) if you want to know more about its functionality.", "Slicing Data Frames - selecting rows and columns", "Selection Using Label Column Selection To select a column of a DataFrame by column label, the safest and fastest way is to use the .loc [method](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.loc.html). General usage looks like frame.loc[rowname,colname] . (Reminder that the colon : means \"everything\"). For example, if we want the color column of the ex data frame, we would use : ex.loc[:, 'color'] - You can also slice across columns. For example, baby_names.loc[:, 'Name':] would give select the columns Name and the columns after. - Alternative: While .loc is invaluable when writing production code, it may be a little too verbose for interactive use. One recommended alternative is the [] method, which takes on the form frame['colname'] . Row Selection Similarly, if we want to select a row by its label, we can use the same .loc method. In this case, the \"label\" of each row refers to the index (ie. primary key) of the dataframe.", "The .loc actually uses the Pandas row index rather than row id/position of rows in the dataframe to perform the selection. Also, notice that if you write 2:5 with loc[] , contrary to normal Python slicing functionality, the end index is included, so you get the row with index 5.", "There is another Pandas slicing function called iloc[] which lets you slice the dataframe by row id and column id instead of by column name and row index (for loc[] ). This is really the main difference between the 2 functions and it is important that you remember the difference and why you might want to use one over the other. In addition, with iloc[] , the end index is NOT included, like with normal Python slicing. Here is an example of how we would get the 2nd, 3rd, and 4th rows with only the Name column of the baby_names dataframe using both iloc[] and loc[] . Observe the difference.", "Lastly, we can change the index of a dataframe using the set_index method.", "We can now lookup rows by name directly:", "However, if we still want to access rows by location we will need to use the integer loc ( iloc ) accessor:", "Question 4 Selecting multiple columns is easy. You just need to supply a list of column names. Select the Name and Year in that order from the baby_names table.", "As you may have noticed above, the .loc() method is a way to re-order the columns within a dataframe.", "Filtering Data", "Filtering with boolean arrays Filtering is the process of removing unwanted material. In your quest for cleaner data, you will undoubtedly filter your data at some point: whether it be for clearing up cases with missing values, culling out fishy outliers, or analyzing subgroups of your data set. Note that compound expressions have to be grouped with parentheses. Example usage looks like df[df[column name] < 5]] . For your reference, some commonly used comparison operators are given below. Symbol | Usage | Meaning ------ | ---------- | ------------------------------------- == | a == b | Does a equal b? <= | a <= b | Is a less than or equal to b? >= | a >= b | Is a greater than or equal to b? < | a < b | Is a less than b? & 62; | a & 62; b | Is a greater than b? ~ | ~p | Returns negation of p & 124; | p & 124; q | p OR q & | p & q | p AND q ^ | p ^ q | p XOR q (exclusive or)", "In the following we construct the DataFrame containing only names registered in California", "Question 5a Select the names in Year 2000 (for all baby_names) that have larger than 3000 counts. What do you notice? (If you use p & q to filter the dataframe, make sure to use df[df[(p) & (q)]] or df.loc[df[(p) & (q)]]) Remember that both slicing and using loc will achieve the same result, it is just that loc is typically faster in production. You are free to use whichever one you would like.", "Data Aggregration (Grouping Data Frames) Question 6a To count the number of instances of each unique value in a Series , we can use the value_counts() [method](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html) as df[\"col_name\"].value_counts() . Count the number of different names for each Year in CA (California). (You may use the ca DataFrame created above.) Note: We are not computing the number of babies but instead the number of names (rows in the table) for each year.", "Question 6b Count the number of different names for each gender in CA . Does the result help explaining the findings in Question 5?"], "topic": "addition"}
{"markdowns": ["Chaos game The term chaos game originally referred to a method of creating a fractal, using a polygon and an initial point selected at random inside it.The fractal is created by iteratively creating a sequence of points, starting with the initial random point, in which each point in the sequence is a given fraction of the distance between the previous point and one of the vertices of the polygon; the vertex is chosen at random in each iteration. Repeating this iterative process a large number of times, selecting the vertex at random on each iteration, and throwing out the first few points in the sequence, will often (but not always) produce a fractal shape. Using a regular triangle and the factor 1/2 will result in the Sierpinski triangle.", "Problem 1.2.1 Write a lambda to find the mid point of two given ndarrays.", "Problem 1.2.2 Write a lambda to choose a triangle vertex at random set the current point to be the midpoint between the previous current point and the randomly chosen vertex."], "topic": "polynomial ring"}
{"markdowns": ["<img src=\"../support_files/cropped-SummerWorkshop_Header.png\"> <h1 align=\"center\">Python Bootcamp</h1> <h3 align=\"center\">August 20-21, 2016</h3>", "<div style=\"border-left: 3px solid 000; padding: 1px; padding-left: 10px; background: F0FAFF; \"> <center><h1>Basic Python Part 2 - Control Flow and Functions</h1></center> <p>In this section we will go over the basic control flow features in Python -- for-loops, if-then blocks, and functions. <p> Documentation: [control flow](https://docs.python.org/2/tutorial/controlflow.html) </div>", "<div style=\"border-left: 3px solid 000; padding: 1px; padding-left: 10px; background: F0FAFF; \"> <h2> for -loops</h2> <p>Python has a number of ways to iterate, or \"loop\" over commands. The most common is the for loop, which allows a single block of code to be executed multiple times in sequence: </div>", "<div style=\"border-left: 3px solid 000; padding: 1px; padding-left: 10px; background: F0FAFF; \"> <p>The code above does the following: <ol> <li> range(6) creates a list of 6 consecutive integers starting at 0. <li> For each integer in the list, assign the integer to the variable i and then execute the code inside the for loop. This means that the statement print(i) is executed 6 times, each time with a different value assigned to i . <li> Print \"All done.' </ol> <p>If you're coming from almost any other programming language, the above code may look strange to you--how does Python know that print(i) is meant to be executed 6 times, whereas print \"All done.\" should only be executed once? Most languages would require you to explicitly mark the end of for loop (for example, by writing END , ENDFOR , or a closing brace } ). <p>The answer is that Python uses indentation to determine the beginning and end of code blocks. Every line with the same initial indentation is considered a part of the same code block. A different indentation level indicates another code block. This makes many newcomers uncomfortable at first! Never fear; we love this feature for its simplicity and cleanliness, and our text editors take care of the tedious work for us. <p>Let's see what would have happened if the print(i) statement were not correctly indented: </div>", "<div style=\"border-left: 3px solid 000; padding: 1px; padding-left: 10px; background: F0FAFF; \"> <p>The code in the previous cell gives an IndentationError , which tells us that we need to fix the indentation of the line that says print(i) . <p>How much should you indent? By convention, most Python code indents using 4 spaces. This is good practice. Technically, you can choose as much white space you would like as long as it is consistent , but please do not use tabs! This can confuse other people trying to use your code on a different platform or with a different editor. Most Python environments will by default or can be configured to convert the use of the tab key to 4 spaces. Many will also auto-indent what should be a new code block. <p>We will see many more examples of code indentation below. </div>", "<div style=\"background: DFF0D8; border-radius: 3px; padding: 10px;\"> <p> Exercise 2.1: Try typing the above for-loop into the cell below. Notice that when you press enter at the end of the first line, Jupyter will automatically begin an indented code block. </div>", "<div style=\"background: DFF0D8; border-radius: 3px; padding: 10px;\"> <p> Exercise 2.2: Print out the squares of the first ten integers. </div>", "<div style=\"background: DFF0D8; border-radius: 3px; padding: 10px;\"> <p> Exercise 2.3: Print out the sum of the first ten integers. </div>", "<div style=\"background: DFF0D8; border-radius: 3px; padding: 10px;\"> <p> Exercise 2.4: Add two equal length lists of numbers element-wise. </div>", "<div style=\"border-left: 3px solid 000; padding: 1px; padding-left: 10px; background: F0FAFF; \"> <h2>Conditionals: if...elif...else</h2> <p> if statements allow your code to make decisions based on the value of an object. </div>", "<div style=\"border-left: 3px solid 000; padding: 1px; padding-left: 10px; background: F0FAFF; \"> <p>Again, the body of the if code block is defined by indentation. <p>Note the use of the double-equal sign to indicate a test for equality. Remember: <ul> <li> A single-equal = is an assignment ; this changes the value of a variable. <li> A double-equal == is a question ; it returns True if the objects on either side are equal. </ul> </div>", "<div style=\"background: DFF0D8; border-radius: 3px; padding: 10px;\"> <p> Exercise 2.5: <p>Repeat the above code block with the if statement changed to </div> python if x=5: <div style=\"background: DFF0D8; border-radius: 3px; padding: 10px;\"> <p>What do you get? </div>", "<div style=\"border-left: 3px solid 000; padding: 1px; padding-left: 10px; background: F0FAFF; \"> The expression x==5 evaluates to a boolean value (either True or False ). </div>", "<div style=\"border-left: 3px solid 000; padding: 1px; padding-left: 10px; background: F0FAFF; \"> Any type of expression that can be evaluated to a boolean may be used as a condition in an if-block. For example: </div>", "<div style=\"border-left: 3px solid 000; padding: 1px; padding-left: 10px; background: F0FAFF; \"> One can also add elif and else blocks to an if statement. elif adds another conditional to be tested in case the first is false. The else block is run only if all other conditions are false. elif and else are both optional. </div>", "<div style=\"border-left: 3px solid 000; padding: 1px; padding-left: 10px; background: F0FAFF; \"> <h2>Mixing for-loops and conditionals</h2> <p> for loops also support break and continue statements. These follow the standard behavior used in many other languages: <ul> <li> break causes the currently-running loop to exit immediately--execution jumps to the first line after the end of the for-loop's block. <li> continue causes the current iteration of the for-loop to end--execution jumps back to the beginning of the for-loop's block, and the next iteration begins. </ul> </div>", "<div style=\"border-left: 3px solid 000; padding: 1px; padding-left: 10px; background: F0FAFF; \"> Note the use of nested indentation--we have an if block nested inside a for -loop. </div>", "<div style=\"background: DFF0D8; border-radius: 3px; padding: 10px;\"> <p> Exercise 2.6: Most code editors provide some means to automatically indent and de-indent whole blocks of code. In the cell above, select the last three lines and then press tab to increase their indentation, and shift-tab to decrease their indentation. </div>", "<div style=\"border-left: 3px solid 000; padding: 1px; padding-left: 10px; background: F0FAFF; \"> <h2>Iterating over lists, tuples, and dicts</h2> <p>A very common operation in programming is to loop over a list, performing the same action once for each item in the list. How does this work in Python? <p>For example, if you want to print each item in a list, one at a time, then you might try the following: </div>", "<div style=\"border-left: 3px solid 000; padding: 1px; padding-left: 10px; background: F0FAFF; \"> <p>Is there a better way? <p>When we originally used for i in range(6) , we said that range(6) just generates a list of integers, and that the for-loop simply executes its code block once for each item in the list. <p>In this case, we already have the list of items we want to iterate over in words_from_hello , so there is no need to introduce a second list of integers: </div>", "<div style=\"border-left: 3px solid 000; padding: 1px; padding-left: 10px; background: F0FAFF; \"> <p>In each iteration of the loop, the next succesive element in the list words_from_hello is assigned to the value word . The variable word can then be used inside the loop. <p>We can also iterate over tuple s and dict s. Iterating over a tuple works exactly as with list s: </div>", "<div style=\"border-left: 3px solid 000; padding: 1px; padding-left: 10px; background: F0FAFF; \"> <p>If we use a dict in a for-loop, then the behavior is to iterate over the keys of the dict : </div>", "<div style=\"border-left: 3px solid 000; padding: 1px; padding-left: 10px; background: F0FAFF; \"> <p>Are the values printed in the order you expected? Be careful! dict s are not ordered, so you cannot rely on the order of the sequence that is returned. <p>There are actually a few different ways to iterate over a dict : </div>", "<div style=\"border-left: 3px solid 000; padding: 1px; padding-left: 10px; background: F0FAFF; \"> <p>The last example is a nice combination of multiple assignment and iterators. The items() method generates a tuple for each loop iteration that contains the (key, value) pair, which is then split and assigned to the key and value variables in the for-loop. <p>It is quite common that we will want to iterate over a list of items and at the same time have access to the index (position) of each item within the list. This can be done simply using the tools shown above, but Python also provides a convenient function called enumerate() that makes this easier: </div>", "<div style=\"background: DFF0D8; border-radius: 3px; padding: 10px;\"> <p> Exercise 2.7: Create a dictionary with the following keys and values by iterating with a for loop. Start with an empty dict and add one key/value pair at a time. </div>", "<div style=\"border-left: 3px solid 000; padding: 1px; padding-left: 10px; background: F0FAFF; \"> <h2>List Comprehensions</h2> <p>Earlier, we said that a very common operation in programming is to loop over a list, performing the same action once for each item in the list. Python has a cool feature to simplify this process. For example, let's take a list of numbers and compute a new list containing the squares of those numbers: </div>", "<div style=\"border-left: 3px solid 000; padding: 1px; padding-left: 10px; background: F0FAFF; \"> A list comprehension performs the same operation as above, but condensed into a single line. </div>", "<div style=\"border-left: 3px solid 000; padding: 1px; padding-left: 10px; background: F0FAFF; \"> <p>List comprehensions can also be used to filter lists using if statements. For example, the following prints out the first 100 even valued perfect squares. </div>", "<div style=\"border-left: 3px solid 000; padding: 1px; padding-left: 10px; background: F0FAFF; \"> <p>List comprehensions are frequently-used by Python programmers, so although it is not necessary to use list comprehensions in your own code, you should still understand how to read them. </div>", "<div style=\"border-left: 3px solid 000; padding: 1px; padding-left: 10px; background: F0FAFF; \"> <h2>Functions</h2> <p>Code execution is often very repetitive; we have seen this above with for-loops. What happens when you need the same block of code to be executed from two different parts of your program? Beginning programmers are often tempted to copy-and-paste snippets of code, a practice that is highly discouraged. In the vast majority of cases, code duplication results in programs that are difficult to read, maintain, and debug. <p>To avoid code duplication, we define blocks of re-usable code called functions . These are one of the most ubiquitous and powerful features across all programming languages. We have already seen a few of Python's built-in functions above-- range() , len() , enumerate() , etc. In this section we will see how to define new functions for our own use. <p>In Python, we define new functions by using the following syntax: </div>", "<div style=\"border-left: 3px solid 000; padding: 1px; padding-left: 10px; background: F0FAFF; \"> <p>The first line (1) defines a new function called my_add_func , and (2) specifies that the function requires two arguments (inputs) called x and y . <p>In the last line we actually execute the function that we have just defined. When the Python interpreter encounters this line, it immediately jumps to the first line of my_add_func , with the value 3 assigned to x and 5 assigned to y . The function returns the sum x + y , which is then passed to the print() function. <p>The indented code block in between defines what will actually happen whenever the function is run. All functions must either return an object--the output of the function--or raise an exception if an error occurred. Any type of object may be returned, including tuples for returning multiple values: </div>", "<div style=\"border-left: 3px solid 000; padding: 1px; padding-left: 10px; background: F0FAFF; \"> <p>Another nice feature of Python is that it allows documentation to be embedded within the code. <p>We document functions by adding a string just below the declaration. This is called the \"docstring\" and generally contains information about the function. Calling \"help\" on a function returns the docstring. <p>Note that the triple-quote ''' below allows the string to span multiple lines. </div>", "<div style=\"border-left: 3px solid 000; padding: 1px; padding-left: 10px; background: F0FAFF; \"> A function's arguments can be given a default value by using argname=value in the declared list of arguments. These are called \"keyword arguments\", and they must appear after all other arguments. When we call the function, argument values can be specified many different ways, as we will see below: </div>", "<div style=\"border-left: 3px solid 000; padding: 1px; padding-left: 10px; background: F0FAFF; \"> You can also \"unpack\" the values inside lists and tuples to fill multiple arguments: </div>", "<div style=\"border-left: 3px solid 000; padding: 1px; padding-left: 10px; background: F0FAFF; \"> Alternatively, you can unpack a dictionary that contains the names and values to assign for multiple keyword arguments: </div>", "<div style=\"border-left: 3px solid 000; padding: 1px; padding-left: 10px; background: F0FAFF; \"> <h2>Function arguments are passed by reference</h2> <p>Let's revisit a point we made earlier about variable names. <p>A variable in Python is a reference to an object. If you have multiple variables that all reference the same object, then any modifications made to the object will appear to affect all of those variables. For example: </div>", "<div style=\"border-left: 3px solid 000; padding: 1px; padding-left: 10px; background: F0FAFF; \"> <p>What happened in the code above? We printed my_list twice, but got a different result the second time. <p>This is because we have two variables-- my_list and list_to_print --that both reference the same list. When we reversed list_to_print from inside print_backward() , it also affected my_list . <p>If we wanted to avoid this side-effect, we would need to somehow avoid modifying the original list. If the side-effect is intended , however, then it should be described in the docstring: </div>", "<div style=\"border-left: 3px solid 000; padding: 1px; padding-left: 10px; background: F0FAFF; \"> <h2>Variable Scope</h2> <p>When working with large programs, we might create so many variables that we lose track of which names have been used and which have not. To mitigate this problem, most programming languages use a concept called \"scope\" to confine most variable names to a smaller portion of the program. <p>In Python, the scope for any particular variable depends on the location where it is first defined. Most of the variables we have defined in this notebook so far are \"global\" variables, which means that we should be able to access them from anywhere else in the notebook. However, variables that are assigned inside a function (including the function's arguments) have a more restricted scope--these can only be accessed from within the function. Note that unlike with functions, this does not apply to the other control flow structures we have seen; for-loops and if-blocks do not have a local scope. <p>Let's see an example of local scoping in a function: </div>", "<div style=\"border-left: 3px solid 000; padding: 1px; padding-left: 10px; background: F0FAFF; \"> <p>The above example generates a NameError because the variable message only exists within the scope of the print_hi() function. By the time we try to print it again, message is no longer available. <p>The reverse of this example is when we try to access a global variable from within a function: </div>", "<div style=\"border-left: 3px solid 000; padding: 1px; padding-left: 10px; background: F0FAFF; \"> In this case, we are able to access the variable message from within the function, because the variable is defined in the global scope. However, this practice is generally discouraged because it can make your code more difficult to maintain and debug. </div>", "Exercises 2 <br> <div style=\"background: DFF0D8; border-radius: 3px; padding: 10px;\"> <p> Exercise 2.8: Write a function to 'flatten' a nested list. Can you do this with a list comprehension? (Caution: The answer for a list comprehension is very simple but can be quite tricky to work out.) </div> [['a','b'],['c','d']] -> ['a','b','c','d']", "<div style=\"background: DFF0D8; border-radius: 3px; padding: 10px;\"> <p> Exercise 2.9: Consider the function in the following code block. Does this function work as expected? What happens to the list passed in as my_list when you run it? Experiment with a list of numbers. </div> python def square_ith(i,my_list): '''Return the square of the ith element of a list''' x = my_list.pop(i) return x x X = [1,2,3,4,5] print square_ith(3,X) Should output 16"], "topic": "application programming interface"}
{"markdowns": ["Week6 Assignment - Data retrieval and dataset preprocessing. For this assignment, you will need the iris flowers dataset available on the course website, under Week6 files. This is a dataset of 150 observations of flowers, with petal lengths and widths, and sepal lengths and widths. The basic goal of this assignment is to preprocess the iris dataset in preparation for a machine-learning algorithm. In the first component, you will load the dataset into a Pandas dataframe. In the second component, you will impute missing values in one of the columns. In this case, you will assign the average value in the column to the missing data points. In the third component, you will create two new columns that will approximate the sepal and petal sizes: One will equal the petal length multiplied by the petal width, and the other will equal the sepal length multiplied by the sepal width. In the fourth component, you will normalize the sepal and petal sizes. In the fifth component, you will add a column with a boolean value representing whether a flower belongs to the setosa species. All the exercises are designed so that the solutions will need only one or a few lines of code. Do not hesitate to contact instuctors and TA via week6 channel on Slack if you get stuck. Join the channel first by clicking on Channels.", "Part A. Read in the iris dataset. In this component you will read the iris dataset into a Pandas data frame. Make sure you download the iris.csv file from the course website. Do not download or load a different instance of the iris dataset: use the one from the course website as we have modified it for this exercise, and when we test your code we will test it against our version of the dataset. Also, do not manually modify the iris dataset. If you prefer to load the dataset by its URL, you can do that. Pay attention which directory you save the file to so that you can load it by its path. Once downloaded, use Pandas to read the file into a data frame. Save the data frame to a variable named iris_data."], "topic": "data analysis"}
{"markdowns": ["If you are not using the Assignments tab on the course JupyterHub server to read this notebook, read [Activating the assignments tab](https://github.com/lcdm-uiuc/info490-sp17/blob/master/help/act_assign_tab.md). A few things you should keep in mind when working on assignments: 1. Make sure you fill in any place that says YOUR CODE HERE . Do not write your answer in anywhere else other than where it says YOUR CODE HERE . Anything you write anywhere else will be removed or overwritten by the autograder. 2. Before you submit your assignment, make sure everything runs as expected. Go to menubar, select _Kernel_, and restart the kernel and run all cells (_Restart & Run all_). 3. Do not change the title (i.e. file name) of this notebook. 4. Make sure that you save your work (in the menubar, select _File_ \u2192 _Save and CheckPoint_) 5. You are allowed to submit an assignment multiple times, but only the most recent submission will be graded.", "Problem 7.2. Text Classification. In this problem, we perform text classificatoin tasks by using the scikit learn machine learning libraries.", "We will be using a data set borrowed from [here](https://github.com/jacoxu/StackOverflow) that has been made available by Kaggle. It contains 20,000 instances of StackOverFlow post titles accompanied by labels in a separate file. For the purposes of this assignment, I have combined them in one file. Firstly, we load the contents of the file into a Pandas DataFrame.", "Splitting data set for training and testing We shall be making use of [StratifiedShuffleSplit](http://scikit-learn.org/0.17/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html) to split the data set into train and test sets.", "Logistic Regression (no pipeline, no stop words) Use CountVectorizer to create a document term matrix for the titles, and apply the Logistic Regression algorithm to classify which label the title belongs to. Do not use pipeline (yet). Do not use stop words (yet). Use default parameters for both CountVectorizer and LogisticRegression ."], "topic": "addition"}
{"markdowns": ["Connect Intensive - Machine Learning Nanodegree Lesson 02: Working with the Enron Data Set Objectives - Work through the Datasets and Questions lesson from [ud-120](https://www.udacity.com/course/intro-to-machine-learning--ud120) within the Jupyter Notebook environment - Introduce [the pickle module](https://docs.python.org/2/library/pickle.html) from the Python Standard Library for preserving data structures in Python - Practice loading data from different directories - Review stacking and unstacking in pandas to [reshape a DataFrame ](http://pandas.pydata.org/pandas-docs/stable/reshaping.html) - Explore and understand data sets, particularly those with many missing values Prerequisites - You should be familiar with pandas (lesson 01 in the [ConnectIntensive repo](https://github.com/nickypie/ConnectIntensive))", "Background Information Before you start working through this Jupyter Notebook, you should probably watch the following videos from ud-120 (they're all conveniently compiled in your Classroom under the Data Modeling section). - Introduction - What is a POI - Accuracy vs. Training Set Size - Downloading Enron Data - Types of Data (Quizzes 1-6) - Enron Dataset Mini-Project Video [Katie Malone](http://blog.udacity.com/2016/04/women-in-machine-learning-katie-malone.html) provides background on the Enron scandal, introduces the email corpus, defines a person-of-interest (POI), and poses an interesting ML question to motivate the project (\"Can we identify patterns in the emails of people who were POIs?\"). The videos provide excellent insight into the workflow for a Machine Learnist or Data Scientist. This Jupyter Notebook is intended to provide a friendly guide through the \"Datasets and Questions\" lesson... but if you're feeling pretty confident about your Python skills, consider going off-script! Try to work through the lesson on your own -- you may encounter some snags, and you can always refer back to this Notebook if you need a little push forward.", "Getting Started The Datasets and Questions lesson in the Data Modeling section draws from Enron finance and email data. These datasets are found in the [ ud120-projects repo](https://github.com/udacity/ud120-projects) on GitHub. Please fork and clone the ud120-projects repo to your local machine if you haven't done so already (if you need a refresher on how to fork and clone from the command line, [check out this link from GitHub](https://help.github.com/articles/fork-a-repo/)). Be sure to keep track of where you save the local clone of this repo on your machine! We'll need the location of that directory in just a bit.", "In a pickle Suppose you're working with Python and you assemble nice data structures ( e.g. dictionaries, lists, tuples, sets...) that you'll want to re-use in Python at a later time. [The pickle module](https://docs.python.org/2/library/pickle.html) is a fast, efficient way to preserve (or pickle) those data structures without you needing to worry about how to structure or organize your output file. One nice thing about using pickle is that the data structures you store can be arbitrarily complex: you can have nested data structures ( e.g. lists of tuples as the values in a dictionary) and pickle will know exactly how to serialize (or write) those structures to file! The next time you're in Python, you can un-pickle the data structures using pickle.load() and pick up right where you left off! For a better explanation of pickle than I could hope to put together, please check out [this reference on Serializing Python Objects](http://www.diveintopython3.net/serializing.html) from Dive Into Python 3. Run the cell below to import the pickle module. (Don't forget, shift + enter or shift + return runs the active cell in a Jupyter Notebook)", "The path to success Do you remember where you cloned the ud120-projects repo to your local machine? We need that information now! The ud120-projects directory contains a lot of folders and files, one of which is the Enron data within a Python dictionary, preserved using the pickle module. However, we need to tell this Jupyter Notebook where it can find the ud120-projects repo on our local machine. In the cell below, I have a [Magic Function](http://ipython.readthedocs.io/en/stable/interactive/tutorial.html magics-explained) %cd \"...\" that changes the working directory to the ud120-projects directory on my machine. Chances are, you didn't save the ud120-projects directory to the same place on your machine (although you may have, in which case, hello fellow nick!) Update the Magic Function %cd \"...\" in the cell below to reflect the correct path of the ud120-projects directory on your local machine. Then run the cell below to load the Enron data!", "From Dictionary to DataFrame At this point, the variable enron_data is a dictionary object. Dictionaries are not displayed as nicely as pandas DataFrame objects within the Jupyter Notebook environment. So let's convert enron_data to a DataFrame object! In the Jupyter Notebook lesson-01, we saw how to construct a DataFrame from a .csv file... we simply used the method pd.DataFrame.read_csv() . Fortunately, it's just as easy to create a DataFrame object from a dictionary object: we could use [the method pandas.DataFrame.from_dict() ](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.from_dict.html) or simply use [the constructor pandas.DataFrame() ](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html) -- either one works! Run the cell below to: - import pandas and display . - set some display options. - create a DataFrame object for the Enron data. - display the Enron data.", "Stacking, unstacking, and rearranging Oh no, it looks like we created our DataFrame the wrong way! Each row of the DataFrame should correspond to a unique instance or input, while each column of the DataFrame should correspond to a unique feature or variable. The functions [ pandas.DataFrame.stack() ](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.stack.html) and [ pandas.DataFrame.unstack() ](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.unstack.html) will come to the rescue! First, we need to stack the current column indices, moving them to the innermost level of the row index. Run the cell below to see the results of calling enron_df.stack()", "We see that the result of enron_df.stack() is a Series object, where the innermost (rightmost) level of the index is the person's name in the Enron data set, while the outermost (leftmost) level of the index is the feature. If we call unstack() on the resulting Series without specifying a level, we'll just revert to the original DataFrame . Run the cell below to see the result of calling enron_df.stack().unstack()", "The trick is, we need to unstack the outermost level of the index, but by default, the function will unstack the innermost level of the index. Run the cell below once to correctly stack and unstack the Enron DataFrame to move the instances (names) to rows and features (variables) to columns. Be careful! If you run this cell an even number of times, you will lose your changes to the DataFrame ... can you see why?", "Great! Now that our DataFrame has the features and instances in the correct orientation, we can start to explore the data. But before we dive into the exercises, I'll leave you with one last reference for [reshaping Series and DataFrame objects in pandas ](http://pandas.pydata.org/pandas-docs/stable/reshaping.html).", "Exercises Now it's your turn to play with pandas to answer questions using the Enron data set. If you're not sure how to do something, feel free to ask questions, look through [the pandas documentation](http://pandas.pydata.org/pandas-docs/stable/api.html), or refer to the code examples above! You can check your solutions to each of these exercises by entering your answer in the corresponding Quiz in the \"Datasets and Questions\" lesson. I put the corresponding quizzes in parenthesis after each exercise, so you know where to go to check your answers.", "Question 1 How many data points (people) are in the data set? (Quiz: Size of the Enron Dataset)", "Question 2 For each person, how many features are available? (Quiz: Features in the Enron Dataset)", "Question 3 How many Persons of Interest (POIs) are in the dataset? (Quiz: Finding POIs in the Enron Data)", "Question 4 We compiled a list of all POI names (in final_project/poi_names.txt ) and associated email addresses (in final_project/poi_email_addresses.py ). How many POI\u2019s were there total? Use the names file, not the email addresses, since many folks have more than one address and a few didn\u2019t work for Enron, so we don\u2019t have their emails. (Quiz: How Many POIs Exist?) Hint: Open up the poi_names.txt file to see the file format: - the first line is a link to a USA Today article - the second line is blank - subsequent lines have the format: (\u2022) Lastname, Firstname - the dot \u2022 is either \"y\" (for yes) or \"n\" (for no), describing if the emails for that POI are available", "Question 5 What might be a problem with having some POIs missing from our dataset? (Quiz: Problems with Incomplete Data) This is more of a \"free response\" thought question -- we don't really expect you to answer this using code.", "Question 6 What is the total value of the stock belonging to James Prentice? (Query The Dataset 1)"], "topic": "software"}
{"markdowns": ["COGS 108 - Assignment 4: Data Analysis", "Important Reminders You must submit this file ( A4_DataAnalysis.ipynb ) to TritonED to finish the homework. - This assignment has hidden tests: tests that are not visible here, but that will be run on your submitted assignment for grading. - This means passing all the tests you can see in the notebook here does not guarantee you have the right answer! - In particular many of the tests you can see simply check that the right variable names exist. Hidden tests check the actual values. - It is up to you to check the values, and make sure they seem reasonable. - A reminder to restart the kernel and re-run the code as a first line check if things seem to go weird. - For example, note that some cells can only be run once, because they re-write a variable (for example, your dataframe), and change it in a way that means a second execution will fail. - Also, running some cells out of order might change the dataframe in ways that may cause an error, which can be fixed by re-running.", "Notes - Assignment Outline Parts 1-6 of this assignment are modeled on being a minimal example of a project notebook. This mimics, and gets you working with, something like what you will need for your final project. Parts 7 & 8 break from the project narrative, and are OPTIONAL (UNGRADED). They serve instead as a couple of quick one-offs to get you working with some other methods that might be useful to incorporate into your project.", "Setup Data: the responses collected from a survery of the COGS 108 class. - There are 417 observations in the data, covering 10 different 'features'. Research Question: Do students in different majors have different heights? Background: Physical height has previously shown to correlate with career choice, and career success. More recently it has been demonstrated that these correlations can actually be explained by height in high school, as opposed to height in adulthood (1). It is currently unclear whether height correlates with choice of major in university. Reference: 1) http://economics.sas.upenn.edu/~apostlew/paper/pdf/short.pdf Hypothesis: We hypothesize that there will be a relation between height and chosen major.", "Part 1: Load & Clean the Data", "Fixing messy data makes up a large amount of the work of being a Data Scientist. The real world produces messy measurements and it is your job to find ways to standardize your data such that you can make useful analyses out of it. In this section, you will learn, and practice, how to successfully deal with unclean data.", "1a) Load the data Import datafile 'COGS108_IntroQuestionnaireData.csv' into a DataFrame called 'df'.", "Those column names are a bit excessive, so first let's rename them - code provided below to do so.", "Pandas has a very useful function for detecting missing data. This function is called 'isnull()'. If you have a dataframe called 'df', then calling 'df.isnull()' will return another dataframe of the same size as 'df' where every cell is either True of False. Each True or False is the answer to the question 'is the data in this cell null?'. So, False, means the cell is not null (and therefore, does have data). True means the cell is null (does not have data). This function is very useful because it allows us to find missing data very quickly in our dataframe. As an example, consider the code below.", "If you print out more, and scroll down, you'll see some rows with missing data. For example:", "Check an example, row 49, in which an entry has missing data", "Granted, the example above is not very informative. As you can see, the output of 'isnull()' is a dataframe where the values at each cell is either True or False. Most cells have the value of 'False'. We expect this to be the case since most people gave out answers to each question in our survey. However, some rows such as row 49 show that some people chose not to answer certain questions. In the case of row 49, it seems that someone did not give out an answer for 'What year (in school) are you?' However, what if wanted to use 'isnull()' to see all rows where our dataframe 'df' has missing values? In other words, what if we want to see the ACTUAL rows with missing values instead of this dataframe with True or False cells. For that, we need to write the following line of code: <br> <br> python df[df.isnull().any(axis=1)]", "1b) Find missing data Find all rows that have missing data in them. Save the ouput, as a dataframe, into a variable called 'rows_to_drop'. In other words, copy over and use the line of code that we gave out in the cell above.", "You need to run & read the following couple of cells - but don't have to add any code:", "Real world data is messy. As an example of it, we consider the data shown in rows_to_drop (below). If you've done everything correctly so far, you should see an unexpected response with emojis at index 357. These types of responses, although funny, are hard to parse when dealing with big datasets. We'll learn about solutions to these types of problems in the upcoming cells", "In the cell below, briefly explain below how 'df[df.isnull().any(axis=1)]' works, in a couple sentences. Include an explanation of what 'any(axis=1)' means and how it affects the code.", "df.isnull.any(axis=1) looks at the dataframe and checks for any empty/null values in the dataframe. Axis=1 specifies to drop the rows that have the null values, while axis=0 would drop columns.", "------", "1c) Drop the rows with NaN values Drop any rows with missing data, but only for the columns 'major', 'height', 'gender' and 'age'. These will be the data of primary interest for our analyses, so we drop missing data here. Note that there are other missing data (in other rows) but this is fine for our analyses, so we keep them. To do this, ese the pandas 'dropna' method, inplace, using the 'subset' arguments to specify columns.", "Now we have to standardize the data!", "Check all different values given for majors. It's a lot!", "---------", "We'll write a function performing some simple substring checking in order to group many responses together", "Applying the transformation", "Previewing the results of the previous transformation. It looks a lot better, though it's not perfect, but we'll run with this", "Next let's check the 'gender' column.", "Check the different responses received for gender, including how many of each response we have", "standardize_gender : Function to standardize the gender responses Note: for the purposes of the following analyses, we will keep self-reported gender for categories in which we have a sizable number of responses, in this case, those which correspond to 'female' and 'male'", "Apply the transformation, and drop any rows with missing gender information", "Now you will write some code to standardize some of the other data columns.", "1d) Standardize other columns Find, programatically, the number of unique responses to in the 'year' column. Save the result in a variable named 'num_unique_responses'. Hint: you can answer this question using the 'unique' method, used above.", "Print out all the different answers in 'year'", "The line of code above shows us the different values we got, to the question 'What year (in school) are you?'. As you can tell, it is a <b>mess</b>!. For example, if you are a junior student, then you might have answered: 3, three, third, 3rd year, junior, junior year, Junior, etc. That is an issue. We want to be able to analyze this data and, in order to do this successfully, we need to all answers with the same meaning to be written in the same way. Therefore, we're gonna have to transform answers such as '3, third, 3rd, junior, etc' into a single possible value. We'll do this for all values that mean the same.", "In the rest of Part 1, we will work on writing code, organized into functions that will allow us to transform similar respones into the same value. We will call this process: standardizing the data. The cell below provides an example for the kind of code you will need to write to answer this question. This example is separate from our actual data, and is a potential function we might use to standardize messy data - in this case, hypothetical data to the question 'What is your favourite major python version?'. Note some things used in this example that you need to use to standardize data: - string methods, such as 'lower' and 'strip' to transform strings - the 'replace' string method, to replace a set of characters with something else - if/else statements that check what's in our string (number, letters, etc) - type casting, for example using 'int()' to turn a variable into an integer - using 'np.nan' (which stands for 'not a number') to denote missing or unknown data", "1e) Standardize 'year' column Write a function named 'standardize_year' that takes in as input a string and returns an integer. The function will do the following (in the order specified): Note that for these detailed instructions, each line corresponds to one line of code you need to write. - 1) convert all characters of the string into lowercase - 2) strip the string of all leading and trailing whitespace - 3) replace any occurences of 'first' with '1' - 4) replace any occurences of 'second' with '2' - 5) replace any occurences of 'third' with '3' - 6) replace any occurences of 'fourth' with '4' - 7) replace any occurences of 'fifth' with '5' - 8) replace any occurences of 'sixth' with '6' - 9) replace any occurences of 'freshman' with '1' - 10) replace any occurences of 'sophomore' with '2' - 11) replace any occurences of 'junior' with '3' - 12) replace any occurences of 'senior' with 4' - 13) replace any occurences of 'year' with '' (remove it from the string) - 14) replace any occurences of 'th' with '' (remove it from the string) - 15) replace any occurences of 'rd' with '' (remove it from the string) - 16) replace any occurences of 'nd' with '' (remove it from the string) - 17) strip the string of all leading and trailing whitespace (again) - 18) If the resulting string is a number and it is less than 10, then cast it into an integer and return that value - 19) Else return np.nan to symbolize that the student's response was not a valid entry HINTS: you will need to use the functions 'lower()', 'strip()', isnumeric() and 'replace()'", "1f) Transform 'year' column Use 'standardize_year' to transform the data in column 'What year (in school) are you?'. Hint: use the apply function AND remember to save your output inside the dataframe", "Assuming that all is correct up to this point, the line below should show all values now found in df. It should look a lot better. With this data, we can now make insightful analyses. You should see an array with elements 1,2,3,4,5,6 and nan (not necessarily in that order). Note that if you check the data type of this column, you'll see that pandas converts these numbers to 'float', even though the applied function returns 'int', because 'np.nan' is considered a float. This is fine.", "Let's do it again. Let's take a look at the responses in the 'weight' column, and then standardize them.", "1g) Standardize 'weight' column Write a function named 'standardize_weight' that takes in as input a string and returns an integer. The function will do the following (in the order specified): - 1) convert all characters of the string into lowercase - 2) strip the string of all leading and trailing whitespace - 3) replace any occurences of 'lbs' with '' (remove it from the string) - 4) replace any occurences of 'lb' with '' (remove it from the string) - 5) replace any occurences of 'pounds' with '' (remove it from the string) - 6) If the string contains the substring 'kg', then: - 6.1) replace 'kg' with '' - 6.2) cast the string into an integer type using the function 'int()' - 6.3) multiply the resulting integer by 2 (an approximate conversion of kilograms to pounds) - 6.4) cast the resulting number back into a string with the function 'str()' - 7) Strip the string of its whitespaces (again) - 8) If the resulting string is numeric: cast it into an integer and return the resulting value - 9) Else: return np.nan", "1h) Transform 'weight' column Use 'standardize_weight' to transform the data in the 'weight' column. Hint: use the apply function AND remember to save your output inside the dataframe", "Now, let's see the result of our hard work . The code below should output all numbers (or nan).", "So far, you've gotten a taste of what it is like to deal with messy data. It's not easy, as you can tell. The last variable we need to standardize for the purposes of our analysis is 'height'. We will standardize that one for you. Do read the code below and try to understand what it is doing.", "It seems like we'll have to handle different measurement systems. Ugh, ok... Let's write a function that converts all those values to inches", "Applying the transformation and dropping invalid rows", "Check the height data, after applying our standardization", "Ensuring that the data types are correct - type cast age to int.", "Check that the dataframe has the right number of columns. If this doesn't pass - check your code in the section above.", "Part 2: Exploratory Data Vizualization First, we need to do some exploratory data visualization, to get a feel for the data. For plotting questions, do not change or move the 'plt.gcf()' lines.", "2a) Scatter Matrix Plot the data, using scatter_matrix, from Pandas. Assign it to a variable called 'fig'.", "2b) Bar Chart Plot a bar chart showing the number of students in each majors. Hints: you can use 'value_counts' to get the counts for each major. You can then use the 'plot' method from pandas for plotting - you don't need matplotlib.", "2c) Histogram for COGSCI Plot a histogram of the height data for all students who wrote 'COGSCI' as their major.", "2d) Histogram for COMPSCI Plot a histogram of the height data for all students who wrote 'COMPSCI' as their major.", "Part 3: Exploring The Data Beyond just plotting the data, we should check some other basic properties of the data. This serves both as a way to get a 'feel' for the data, and to look for any quirks or oddities about the data, that may indicate issues that need resolving. To do this, let's explore that data a bit (not limiting ourselves to only features that we plan to use - exploring the dataset as a whole can help us find any issues). Notes: - You answers should NOT be pandas objects (Series or DataFrames), extract answers so the variables are ints, floats or strings (as appropriate). - You must answer these questions programmatically: do not count / check and hard code particular values.", "3a) Number of majors How many different majors are in the dataset? Save this number to a variable 'n_majors'.", "3b) Range of 'age' What is the range (max value - min value) of ages in the dataset? Save this number to a variable 'r_age'", "3c) Most popular ice-cream flavour What is the most popular ice cream flavour? Save the ice cream name to the variable 'f_ice', and the number of people who like it to a variable 'n_ice'. Hint: you can get these values using the 'value_counts' method.", "3d) Unique favourite ice cream How many people have a unique favourite ice cream? (How many ice cream flavours are only 1 persons favourite?) Save this number to a variable 'u_ice'", "Part 4: Testing Distributions Soon, in the data analysis, we will want to run some statistical tests on our data. First, we should check the distributions! When using methods / statistical tests that make certain assumptions, it's always best to explicitly check if your data meet those assumptions (otherwise the results may be invalid). Let's test if our data are in fact normally distributed. See an example of how to test the disributions of data in the 'TestingDistributions' notebook in Tutorials.", "4a) Normal Test For each of 'h_co', and 'h_cs', use the 'normaltest' function to test for normality of the distribution. 'normaltest' returns two values, a test statistic and a p-value. Save these values as 'st_co', 'p_co', 'st_cs', and 'p_cs' respectively.", "4b) Are they normal? Have a look at the values returned. Based on these results, and using an alpha significance value of 0.01: Set boolean values (True, False) of whether each distribution can be considered to be normally distributed (set as True if the test supports it is normally distributed (or, more formally, we have not rejected the null hypothesis), and False if the test suggests the data is not normally distributed (we should reject the null hypothesis).", "Set boolean values, as specified above: - For the 'h_co' data, set a boolean value to the var 'is_n_co' - For the 'h_cs' data, set a boolean value to the var 'is_n_cs'", "CO data: plot the comparison of the data and a normal distribution (this code provided)", "CS data: plot the comparison of the data and a normal distribution (this code provided)", "Part 5: Data Analysis Now let's analyze the data, to address our research question. For the purposes of this analysis, let's assume we need at least 75 students per major to analyze the height data. This means we are only going to use data from people who wrote 'COGSCI' or 'COMPSCI' as their major.", "5a) Select the data Pull out the data we are going to use: - Save the height data for all 'COGSCI' majors to a variable called 'h_co' - Save the height data for all 'COMPSCI' majors to a variable called 'h_cs'", "5b) Mean height What is the average (mean) height for students from each major? Save these values to 'avg_h_co' for cogs students, and 'avg_h_cs' for cs students.", "Print out the average heights - this code provided", "Based on the cell above, it looks like there might indeed be a difference in the average height for students in cogs vs cs majors. Now we want to statistically test this difference. To do so, we will use a t-test.", "5c) T-test Use a t-test ('ttest_ind' function) to compare the two height distributions ('h_co' vs 'h_cs') 'ttest_ind' returns a t-statistic, and a p-value. Save these outputs to 't_val' and 'p_val' respectively.", "Check if statistical test passes significance, using an alpha value of 0.01. This code provided.", "Note: this test should pass significance. If it doesn't, double check your code up until this point. So - we've reached a conclusion! We're done right!? Nope. We have a first pass analysis, and an interim conclusion that happens to follow our hypothesis. Now let's try to break it. Let's explore some more You should always interrogate your findings, however they come out. What could be some alternate explanations, that would change our interpretations of the current analyses? In this case, we should be worried about confounding variables. We want to be able to say whether height relates to major specifically, but it could be the case that some other variable, that happens to differ between majors, better explains the differences in height. In this case, we also have data on gender. Let's check if differences in the gender ratio of the two majors can explain the difference in height.", "5d) Separate the genders Using 'value_counts' from pandas, extract the number of 'male' and 'female', separately for cogs and cs students. To do so, select from the df each major, separately, extract the gender column, and use the 'value_counts' method. - Save the counts for each gender for 'COGSCI' majors to a variable called 'g_co' - Save the counts for each gender for 'COMPSCI' majors to a variable called 'g_cs'", "5e) Ratio of women What is the ratio of women in each major? By ratio, we mean the proportion of students that are female, as a ratio. This will be value between 0.0 and 1.0 , calculated as F / ( M + F) - done separately for each major. You can use the g_co and g_cs variables to calculate these. - Save the ratio of women in COGSCI to a variable 'r_co' - Save the ratio of women in COMPSCI to a variable 'r_cs' Note: keep these numbers as ratios (they should be decimal numbers, less than 1)", "Make sure you print out and check the values of these ratios. They seem pretty different. We can actually ask, using a chi-squared test, whether this difference in gender-ratio between the majors is signficantly different. Code to do this is provided below.", "5f) Separate the majors Create a new dataframe, called df2 , which only includes data from COGSCI and COMPSCI majors. Hint: you can do this using the or operater | , with loc."], "topic": "addition"}
{"markdowns": ["Visualizing the Titanic Disaster", "Introduction: This exercise is based on the titanic Disaster dataset avaiable at [Kaggle](https://www.kaggle.com/c/titanic). To know more about the variables check [here](https://www.kaggle.com/c/titanic/data) Step 1. Import the necessary libraries", "Step 2. Import the dataset from this [address](https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/Visualization/Titanic_Desaster/train.csv).", "Step 3. Assign it to a variable titanic", "Step 4. Set PassengerId as the index", "Step 5. Create a pie chart presenting the male/female proportion", "Step 6. Create a scatterplot with the Fare payed and the Age, differ the plot color by gender"], "topic": "data analysis"}
{"markdowns": ["Ex1 - Filtering and Sorting Data", "This time we are going to pull data directly from the internet. Special thanks to: https://github.com/justmarkham for sharing the dataset and materials. Step 1. Import the necessary libraries", "Step 2. Import the dataset from this [address](https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv).", "Step 3. Assign it to a variable called chipo.", "Step 4. How many products cost more than $10.00?"], "topic": "data analysis"}
{"markdowns": ["Graph Analysis.", "Florentine Families Social Network In this assignment, we'll be looking at the social network of the [Florentine Families](https://en.wikipedia.org/wiki/Category:Families_of_Florence).", "- Use networkx.draw() to draw the social network of the Florentine families. ![](./images/FF_graph.png)", "Measures of centrality - Compute the degree centrality of each node (using [ degree_centrality() ](https://networkx.github.io/documentation/networkx-1.10/reference/generated/networkx.algorithms.centrality.degree_centrality.html networkx.algorithms.centrality.degree_centrality) ) , the [betweenness centrality](https://networkx.github.io/documentation/networkx-1.10/reference/generated/networkx.algorithms.centrality.betweenness_centrality.html) and [closeness centrality](https://networkx.github.io/documentation/networkx-1.10/reference/generated/networkx.algorithms.centrality.closeness_centrality.html networkx.algorithms.centrality.closeness_centrality).", "We see that two families ('Medici' and 'Guadagni') have the highest betweennness values. High betweenness values signify \"brokerage\" roles in the network. To visualize this, - Color each node according to whether it is a friend of the 'Medici' family node or the 'Guadagni' family node. If someone is a friend of both families, classify the node as a friend of the 'Guadagni' family. ![](./images/friends_graph.png) We could draw the nodes, edges, and labels individually using draw_networkx_nodes() , draw_networkx_edges() , and draw_networkx_labels() . But I think it's simpler to draw all nodes, edges, and labels by using draw() , and then overwrite the nodes with different colors by using draw_networkx_nodes() . In the following code cell, - Use networkx.draw() to draw a graph, - Use networkx.draw_networkx_nodes() to make the friends of node 'Medici' different colors, and - Use networkx.draw_networkx_nodes() to make the friends of node 'Guadagni' different colors. It is important to draw the graph in this specific order to pass the unit tests.", "Cliques Identify the cliques in which a given node is participating."], "topic": "data analysis"}
{"markdowns": ["Visualizing Chipotle's Data", "This time we are going to pull data directly from the internet. Special thanks to: https://github.com/justmarkham for sharing the dataset and materials. Step 1. Import the necessary libraries", "Step 2. Import the dataset from this [address](https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv).", "Step 3. Assign it to a variable called chipo.", "Step 4. See the first 10 entries", "Step 5. Create a histogram of the top 5 items bought", "Step 6. Create a scatterplot with the number of items orderered per order price Hint: Price should be in the X-axis and Items ordered in the Y-axis", "Step 7. BONUS: Create a question and a graph to answer your own question."], "topic": "data analysis"}
{"markdowns": ["Lab 4.3", "Setup your imports", "1. Pull the training set from the newsgroup data", "2. Create the vectorizer", "3. Create the Truncated Singular Value Decomposition", "4. Setup your k-means clustering", "5. Fit the vectorizer and SVD", "7. Fit the kmeans", "8. Check the performance of our kmeans test"], "topic": "data analysis"}
{"markdowns": ["Regression Models Lab Logistic regression: problem statement, derivation, usage", "Classification: Problem Statement In many real cases, the output variable is categorical, i.e. our model should return one of some predefined categories. There are a lot of examples: Classifying images Classifying diseases (is a given patient healthy or ill?) Any case of \"good / bad\" classification Anomaly detection (e.g. credit card fraud detection) Processes which involve creating catalogs, etc. We'll start with the simplest case: Only one input variable $x$ Binary output variable $y$, i.e. either $y = 0$ or $y = 1$", "1. Generate Some Sample Data (1 point) Let's imagine the simplest possible case. For example, we can think of $x$ as \"hours of studying time\" and $y$ as \"passed exam\" (0 of false, 1 if true). A class has 20 students. 12 of them studied between 1 and 3 hours and didn't pass the exam. The other 8 studied between 7 and 10 hours and passed the exam. Let's model this situation. First, to make our work easier, we can split it into two: for failed and passed students. Each student studied a random time, so let's choose this from a random uniform distribution (hint: np.random.uniform(min, max, size) ). Create the following: python failed_student_times = np.random.uniform(?, ?, ?) passed_student_times = np.random.uniform(?, ?, ?) all_times = np.concatenate([failed_student_times, passed_student_times]) Now, let's generate the outcome variable: we need 12 zeros, followed by 8 ones. python exam_result = ... Note: We don't need to use numpy arrays but they will give us many benefits later. One is that our code will be really easy and short, and another is that they are very fast (in terms of computation time).", "If we now plot the student performance, we'll get something like this:", "There's a very obvious distinction between the two classes. Once again, that's because we wanted so. How can we model the data? An example would be: python if x < 5: or x < 6, or something like that return 0 else: return 1 This model works but let's look at a more realistic scenario.", "2. Generate More Realistic Sample Data (1 point) Now, this was a really simple view. A more involved model would include some error. First, let's ensure the test results are repeatable, even with random variables:", "Let's reuse the previous example but this time: Generate 20 study times between 1 and 10. Choose each number randomly from a uniform distribution (hint: this is the same as what you did before). Generate the exam output variable like this: For each study time, if it's $\\le$ 3 hours, it should be 0, if it's $\\ge$ 7 hours, it should be 1. If the study time is between 3 and 7 hours, decide randomly whether it should be 0 or 1. How do we decide randomly between 0 and 1? A simple way would be to generate a random number between 0 and 1: np.random.random() . If that number is >= 0.5, say the student passed the exam and vice versa.", "Now the results loke more fuzzy. It's obvious that we can't model them with 100% accuracy. Still, let's try some things.", "3. Decide on a Modelling Function (2 points) We can see that our old approach can work somewhat. If we try to plot the prediction function, we'll see this:", "The red line is called \"decision boundary\". We can see that we misclassified some students but we are mostly correct. However, the function has problems. First of all, it's undefined at $x = 5$ (we don't know if it's 0 or 1). Second, it has \"sharp corners\", and matematicians hate functions with sharp corners :). We're looking for a function that kind of looks like our line. And there is such a function. It's called a sigmoid function. Its definition is like this: $$ \\sigma(z) = \\frac{1}{1+e^{-z}} $$ Implement the previous formula in code. Note: you can use np.exp(something) instead of np.e something - it's much more reliable."], "topic": "variance"}
{"markdowns": ["Week 14 Problem 2 If you are not using the Assignments tab on the course JupyterHub server to read this notebook, read [Activating the assignments tab](https://github.com/UI-DataScience/info490-fa16/blob/master/Week2/assignments/README.md). A few things you should keep in mind when working on assignments: 1. Make sure you fill in any place that says YOUR CODE HERE . Do not write your answer in anywhere else other than where it says YOUR CODE HERE . Anything you write anywhere else will be removed or overwritten by the autograder. 2. Before you submit your assignment, make sure everything runs as expected. Go to menubar, select _Kernel_, and restart the kernel and run all cells (_Restart & Run all_). 3. Do not change the title (i.e. file name) of this notebook. 4. Make sure that you save your work (in the menubar, select _File_ \u2192 _Save and CheckPoint_) 5. You are allowed to submit an assignment multiple times, but only the most recent submission will be graded.", "In this assignment, we will visualize the total number of flights and current temperature of the top 20 airports using the 2001 flight data. This week's assignment is one long, continuous problem, but I split it up into two sections for easier grading. Before you start coding, read the entire notebook first to understand the big picture. This is where we are going:", "The circles are the top 20 airports in the U.S. The size of each circle is proportional to the total number of arrivals and departures in 2001. The redder the color, the higher the temperature; the bluer the color, the lower the temperature. Thus, we will visualize three pieces of information in one single plot: the location of major airports, the size of the airports, and the current temperature. Problem 1. Recall that in [Problem 9.3](https://github.com/UI-DataScience/info490-fa16/blob/master/Week9/assignment/Problem_3.ipynb) we wrote a function named get_total_flights() that adds the number of departures and the number of arrivals in 2001: python dest_origin = pd.read_csv(\"2001.csv\", encoding='latin-1', usecols=('Dest', 'Origin')) flights = get_total_flights(dest_origin) And we also found which 20 airports had the most number of flights: python top20 = flights.sort(ascending=False, inplace=False)[:20] Suppose that we have stored the result (the data frame top20 ) in an SQL database named [top20.db](https://github.com/UI-DataScience/info490-fa15/blob/master/Week14/assignment/top20.db?raw=true). You can download top20.db from the course repository on Github:", "Function: read\\_top20() Your first task is to - Write a functoin named read_top20() that takes the file name ( top20.db ) and returns a Pandas data frame with the contents of the top20 table in the database."], "topic": "addition"}
